<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 01 Feb 2024 19:00:09 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Who is hiring? (February 2024) (102 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39217310</link>
            <guid>39217310</guid>
            <pubDate>Thu, 01 Feb 2024 16:00:08 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39217310">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="39217310">
      <td><span></span></td>      <td><center><a id="up_39217310" href="https://news.ycombinator.com/vote?id=39217310&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=39217310">Ask HN: Who is hiring? (February 2024)</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_39217310">102 points</span> by <a href="https://news.ycombinator.com/user?id=whoishiring">whoishiring</a> <span title="2024-02-01T16:00:08"><a href="https://news.ycombinator.com/item?id=39217310">2 hours ago</a></span> <span id="unv_39217310"></span> | <a href="https://news.ycombinator.com/hide?id=39217310&amp;goto=item%3Fid%3D39217310">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Who%20is%20hiring%3F%20(February%202024)&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=39217310&amp;auth=6ef02a504ca321855d1d6939f2aea7d8dd70929d">favorite</a> | <a href="https://news.ycombinator.com/item?id=39217310">88&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>Please state the location and include REMOTE, INTERNS and/or VISA
when that sort of candidate is welcome. When remote work is <i>not</i> an option,
include ONSITE.</p><p>Please only post if you personally are part of the hiring company—no
recruiting firms or job boards. One post per company. If it isn't a household name,
explain what your company does.</p><p>Commenters: please don't reply to job posts to complain about
something. It's off topic here.</p><p>Readers: please only email if you are personally interested in the job.</p><p>Searchers: try <a href="https://www.remotenbs.com/" rel="nofollow">https://www.remotenbs.com</a>, <a href="https://hnjobs.u-turn.dev/" rel="nofollow">https://hnjobs.u-turn.dev</a>, <a href="https://hnresumetojobs.com/" rel="nofollow">https://hnresumetojobs.com</a>,
<a href="https://hnhired.fly.dev/" rel="nofollow">https://hnhired.fly.dev</a>, <a href="https://kennytilton.github.io/whoishiring/" rel="nofollow">https://kennytilton.github.io/whoishiring/</a>, <a href="https://hnjobs.emilburzo.com/" rel="nofollow">https://hnjobs.emilburzo.com</a>.</p><p>Don't miss these other fine threads:</p><p><i>Who wants to be hired?</i> <a href="https://news.ycombinator.com/item?id=39217308">https://news.ycombinator.com/item?id=39217308</a></p><p><i>Freelancer? Seeking freelancer?</i> <a href="https://news.ycombinator.com/item?id=39217309">https://news.ycombinator.com/item?id=39217309</a></p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table><table>
            <tbody><tr id="39219725"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219725" href="https://news.ycombinator.com/vote?id=39219725&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Foodee | QA Automation Engineer (and various other roles)| Full Time, REMOTE (BC/ON in Canada only)<p>We connect high-quality, locally owned and operated restaurants to offices via catering and meal plans. Office managers love how we simplify the process, independent restaurants love how we bring them new business during their off hours, and employees love ordering food a few days a week and receiving it labelled and ready to eat!</p><p>Our dev group is growing fast and we're hiring for multiple roles. We're specifically interested in a Senior QA Automation Engineer [1] with experience at medium to large tech companies. If you care about the quality of software products that contribute to communities positively, we'd love to hear from you!</p><p>[1] <a href="https://foodee.bamboohr.com/careers/562" rel="nofollow">https://foodee.bamboohr.com/careers/562</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219697"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219697" href="https://news.ycombinator.com/vote?id=39219697&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Stryker | Freiburg im Breisgau, Germany | Multiple Roles | Full-Time | Hybrid (some onsite required) | German, English | <a href="https://www.stryker.com/de/de/index.html" rel="nofollow">https://www.stryker.com/de/de/index.html</a><p>At Stryker we are dedicated to improving lives, with a passion for researching and developing new medical device products. 
As a software engineer at Stryker, you will be proud of the work that you will be doing, using cutting-edge technologies to make 
healthcare better. Here, you will work in a supportive culture with other incredibly talented and intelligent people, creating 
industry-leading medical technology products. You will also have growth opportunities as we have a culture that supports your personal 
and professional development.</p><p>We are looking for computer vision engineers and generalist software engineers to work on advanced computer aided surgery systems.</p><p>Please apply here:</p><p>- <a href="https://stryker.wd1.myworkdayjobs.com/StrykerCareers/job/Freiburg-Germany/XMLNAME--Senior--Computer-Vision-SW-Engineer--m-f-d-_R510159-1" rel="nofollow">https://stryker.wd1.myworkdayjobs.com/StrykerCareers/job/Fre...</a></p><p>- <a href="https://stryker.wd1.myworkdayjobs.com/StrykerCareers/job/Freiburg-Germany/Senior-NET-C--Developer--m-f-d--for-medical-applications_R508386" rel="nofollow">https://stryker.wd1.myworkdayjobs.com/StrykerCareers/job/Fre...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219128"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219128" href="https://news.ycombinator.com/vote?id=39219128&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>City Innovate | <a href="https://cityinnovate.com/" rel="nofollow">https://cityinnovate.com</a> | Director of Engineering | REMOTE (USA only) | Full Time | $165-$185K<p>I’m the CEO and former CIO for the City of SF. City Innovate is a public benefit corporation helping modernize government agencies through document automation (think enterprise Google Docs - smart templates, section level permissioning, workflow). We have product-market-fit, are profitable, zero churn, and are growing quickly. We have some of the largest govt agencies in the world as our customers.</p><p>We’re looking for a Director of Engineering to lead our vision and strategy in the GovTech space, driving feature velocity, stability, and team entrepreneurship, all while maintaining a focus on social good. Bring your expertise in software development, architecture, and team management to guide our core Ruby on Rails application and future tech decisions.</p><p>Please apply here: <a href="https://apply.workable.com/city-innovate/j/96B79C27AE/" rel="nofollow">https://apply.workable.com/city-innovate/j/96B79C27AE/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219681"><td></td></tr>
            <tr id="39219656"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219656" href="https://news.ycombinator.com/vote?id=39219656&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Regrid | Full-time | REMOTE | US Eastern Time<p><a href="https://jobs.gusto.com/boards/regrid-map-your-future-c265c805-0902-4628-bd27-d013fdcfb5bc" rel="nofollow">https://jobs.gusto.com/boards/regrid-map-your-future-c265c80...</a></p><p>Regrid is a dynamic spatial data company building software and data products to deliver a nationwide dataset of 153+ million land parcels, 180+ million building footprints, and 300+ million addresses covering 99% of the US population. We offer our data in multiple formats to customers across private and public sector verticals as bulk data files &amp; through an API. Our Mapping as a Service product at <a href="https://app.regrid.com/" rel="nofollow">https://app.regrid.com</a> allows our customers to work with the parcel data we have collected. Our mobile apps give individuals and teams access to the latest parcel data and data collection tools in the field.</p><p>Data Analyst I or II (Processing,  $75,000 - $95,000)</p><p>Regrid maintains a geospatial dataset of over 158 million parcel polygons, plus rich associated information, all collected from thousands of sources. The core of this role will involve extract, transform, load (ETL) work organized around a repeating 2-week cycle: ingesting new data, transforming it into our schema, cleaning it to meet our standards, and packing it for deployment to our clients. We primarily work with open source geospatial tools like PostGIS, GDAL (ogr2ogr), and QGIS.</p><p>Data Analyst I or II (Client Services, $80,000 - $110,000)</p><p>Our customers are small, medium, and large businesses, nonprofits, NGOs, and government agencies, operating with a range of technical skills and in a variety of environments. They value our high quality support, including our fast response times, expertise, accurate and positive communication, and understanding, empathy, and respect for their needs and challenges. The primary product you will be supporting is a nationwide spatial dataset of 158 million parcel (property) polygons and associated attributes, as well as other companion data products.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219628"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219628" href="https://news.ycombinator.com/vote?id=39219628&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>SerpApi | <a href="https://serpapi.com/" rel="nofollow">https://serpapi.com</a> | Junior-to-Senior Fullstack Engineer | Illustrator and Graphic Designer | Based in Austin, TX but remote-first structure | Full-time | ONSITE or FULLY REMOTE | $150K - 180K a year 1099 for US or local avg + 20% for outside the US<p>SerpApi is the leading API to scrape and parse search engine results. We deeply support Google, Google Maps, Google Images, Bing, Baidu, and a lot more.</p><p>Our current stack is Ruby, Rails, MongoDB, and React.JS.
We are looking for more Junior and Senior FullStack Engineers.</p><p>We have an awesome work environment: We are a remote first company (before Covid!). We do continuous integration, continuous deployments, code reviews, code pairings, profit sharing, and most of communication is async via GitHub.</p><p>We value super strongly transparency, do open books, have a public roadmap, and contribute to the EFF.</p><p>Apply at: <a href="https://serpapi.com/careers" rel="nofollow">https://serpapi.com/careers</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219100"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219100" href="https://news.ycombinator.com/vote?id=39219100&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>HRL Laboratories | Senior Software Engineer; Principal Compiler Engineer | Common Lisp | ONSITE | Los Angeles, California | <a href="https://quantum.hrl.com/" rel="nofollow">https://quantum.hrl.com/</a><p>HRL Laboratories is a private, scientific laboratory that has been in business since 1960. We are best known for having created the first working laser.</p><p>We are doing R&amp;D on quantum computing, from fabrication to algorithms, all in-house, using a tech called exchange-only silicon dot qubits. We are looking for both senior engineering and management to help build out our compiler stack for a quantum programming language. We recently presented it, along with our language-oriented development approach, at the biggest physics conference: APS March Meeting. [1]</p><p>The quantum computing group uses Common Lisp, Coalton, Python, C++, and JavaScript to build a state-of-the-art operating system for physicists researching silicon dot quantum computers. Compiler development is done primarily in Common Lisp.</p><p>Those who are excellent programmers experienced with</p><p>- functional programming, and/or Lisp, and/or statically typed ML-descendants;</p><p>- compilers, machine architecture;</p><p>- and solid undergraduate mathematics and advanced computer science</p><p>are most qualified. We can train you on Lisp, Coalton, and quantum if the fundamentals are there. Knowledge of quantum computing is <i>not</i> necessary for most software engineering roles.</p><p>Interview process: phone call, "take-home" exercise, on-site interview (no whiteboarding), reference checks, and offer.</p><p>Candidates must be eligible to obtain and maintain a U.S. security clearance, and must be willing to relocate to southern California.</p><p>Compiler Engineer job: <a href="https://jobs.lever.co/dodmg/af802f7f-4e44-4457-9e49-14bc47bd8b31" rel="nofollow">https://jobs.lever.co/dodmg/af802f7f-4e44-4457-9e49-14bc47bd...</a></p><p>Other HRL Quantum roles: <a href="https://quantum.hrl.com/careers/" rel="nofollow">https://quantum.hrl.com/careers/</a></p><p>Reach out to the email in my profile with a letter and resume, or apply for a role directly through the website.</p><p>[1] <a href="https://youtu.be/F8TezGqCvE8" rel="nofollow">https://youtu.be/F8TezGqCvE8</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219546"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219546" href="https://news.ycombinator.com/vote?id=39219546&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><br><div>
                  <p><span>Interactive Brokers | Senior Python Developer | Mumbai India | Hybrid www.interactivebrokers.com You want to break into the finance industry? You want to learn about trading and investing? Are you tired of endless meetings and just want to code? If you love designing greenfield projects in Python, this is the job for you. I am the hiring manager for the team. We are a small technology team in a fast growing, multi-billion dollar company. Many of our larger clients are multi-national banks that you have heard of. We are seeking a self-starter that enjoys the craft of coding and developing innovative solutions. The ideal candidate is someone who can communicate well and design libraries and systems in Python from scratch. Experience with Object oriented design, design patterns, building systems, and SQL a plus. Positions are a hybrid model 3 days in office 2 days remote. IB has grown through tough economic times. We have continued to hire while others have cut staff. If you are looking for a great place to innovate, apply today. We have two positions open in our Mumbai India office. Apply Online here: <a href="https://boards.greenhouse.io/ibkr/jobs/6744937002" rel="nofollow">https://boards.greenhouse.io/ibkr/jobs/6744937002</a></span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39219547"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219547" href="https://news.ycombinator.com/vote?id=39219547&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><br><div>
                  <p><span>SAS | Software Developers | Cary, NC or REMOTE (US) | Full-time
We're the leader in analytics.  Our Decisioning Runtime R&amp;D team is looking for experienced developers.  You will be designing, crafting, and maintaining high quality software that empowers our users.
If Java or Go is your specialty and you have 8+ years experience, apply to be part of our team as a Senior Software Developer.  Your focus will be high-quality, testable, and scaleable mid-tier services. <a href="https://careers-sas.icims.com/jobs/35044/job" rel="nofollow">https://careers-sas.icims.com/jobs/35044/job</a>
If you're well versed in C, we have a position for you, too!  We're looking for a developer with 5+ years experience to design and develop scalable computational algorithms.  <a href="https://careers-sas.icims.com/jobs/34947/job" rel="nofollow">https://careers-sas.icims.com/jobs/34947/job</a></span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39219307"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219307" href="https://news.ycombinator.com/vote?id=39219307&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Benaco | full stack | 100% REMOTE (global) | <a href="https://benaco.com/" rel="nofollow">https://benaco.com</a><p>Founder-owned, bootstrapped Computer Vision startup.</p><p>Benaco creates high-quality 3D models out of photo and laser data. We bring photorealistic digital twins into browsers to save our customers on-site visits, from real estate to hazardous environments such as chemistry parks and nuclear waste cleanup sites.</p><p>Examples: <a href="https://benaco.com/go/example-real-estate" rel="nofollow">https://benaco.com/go/example-real-estate</a> <a href="https://benaco.com/go/example-church" rel="nofollow">https://benaco.com/go/example-church</a> <a href="https://benaco.com/go/example-industrial" rel="nofollow">https://benaco.com/go/example-industrial</a></p><p>Tasks:</p><pre><code>  * Computer Vision + Graphics, photogrammetry
  * Implementing academic papers
  * Browser frontend, web server
  * Low-level performance optimisations, dev tooling
  * Distributed systems, reliability engineering, server ops
  * B2B sales, customer interaction
</code></pre>
For this role we're especially looking for a good communicator who enjoys talking to our customers and helps us build the features they need.<p>Tech:</p><pre><code>  Haskell, C++, CUDA, Python with types
  TypeScript, React, WebGL, Linux, Postgres, Ceph, Nix
</code></pre>
We're a small, high-efficiency company. We value operational excellence and low overheads.<p>More details, open-source contributions, contact, on: <a href="https://discourse.haskell.org/t/benaco-offering-remote-engineering-job-3d-reconstruction/8695" rel="nofollow">https://discourse.haskell.org/t/benaco-offering-remote-engin...</a></p><p>Please mention you came from HN so we know where it's worth posting :)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219450"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219450" href="https://news.ycombinator.com/vote?id=39219450&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Fondant | AI-powered 3D tools | Founding Engineers | Full time, REMOTE<p>Fondant is on a mission to make 3D more accessible. We are building a platform that harnesses the latest advancements in generative AI/ML to create 3D assets instantly with a few words (or images). Traditional 3D tools are often intimidating (think Blender) and can take hours of YouTube tutorials to learn. We are leveraging the lessons learnt from building traditional tooling over the last decade + SOTA work on 3D reconstruction, NeRFs, gaussian splatting and image generation.</p><p>We have an early product + users + funding.</p><p>We are hiring for founding engineering roles for AI and Web/Frontend. If you can write good code, can work independently and thrive on solving fuzzy problems that often don't have answers on stack-overflow or ChatGPT/GPT4, you will be a good fit.</p><p>For ML/AI roles experience with ML infrastructure, diffusion models, 3D reconstruction, 3D priors (like Instant3D/LRM), NeRFs, gaussian splats, pytorch would be a huge plus.</p><p>For Web engineering/Frontend role, experience with Javascript, Typescript, WebGL and ThreeJS, shaders, game engines would be amazing.</p><p>If you're excited about pushing the boundaries of 3D technology, reach out at my username [at] fondant.design with a short blurb about yourself and link to a personal contribution (open-source code, research publication, demo etc.)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219056"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219056" href="https://news.ycombinator.com/vote?id=39219056&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>British Red Cross | Various platform engineering and service delivery roles | ONSITE at various UK locations (London, Paisley, Manchester) or REMOTE UK, requires UK RTW and Residency (non-negotiable even if you’re remote) | Full-time | <a href="https://careers.redcross.org.uk/" rel="nofollow">https://careers.redcross.org.uk/</a><p>Since 1870, the British Red Cross has been helping people in crisis, whoever and wherever they are, as part of the world’s largest humanitarian network. We're looking for technologists who want to use their skills to help people in crisis. We currently have 4 roles live with several more to come in the coming months</p><p>Live now:</p><p>-Senior Software Developer
-Platform Operations Manager
-Cyber Security Manager
-Application Development Manager</p><p>Upcoming:</p><p>-Operations Engineers
-Software Engineers
-Service Designers
-Front End Engineers
-Delivery and Engineering Managers</p><p>Some of our benefits:</p><p>-True Flexible working (flexi hours / days / compression / location)
-36 days annual leave plus the chance to purchase 5 extra days leave 
-Maternity, paternity, adoption, shared parental and careers leave
-Pension scheme</p><p>If we don’t have a role which suits you today, please keep an eye on the jobs page above or DM me on LinkedIn @globalgoat. You can read more about us and our culture here <a href="https://medium.com/digital-and-innovation-at-british-red-cross" rel="nofollow">https://medium.com/digital-and-innovation-at-british-red-cro...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217718"><td></td></tr>
            <tr id="39219441"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219441" href="https://news.ycombinator.com/vote?id=39219441&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Mio AI | AI Engineer | HYBRID SF / NY or REMOTE | Full-time | <a href="http://mio.ai/" rel="nofollow">http://mio.ai</a>
Mio AI is redefining how people create and consume media entertainment. We craft immersive experiences where creatives don't just write stories but co-create with artificial intelligence, shaping scenarios for a uniquely immersive journey.<p>With backing from industry leaders like New Wave (Mistral, BeReal, Amo), and experienced cofounders (former Tencent Games, TikTok, YC alumni) we're on a mission to redefine the entertainment landscape with the use of AI, making it immersive and empowering, putting people in control of their narrative experience</p><p>The Role:
Utilize the latest models to bring our user’s creativity to life. The role requires an ability to build multimodal systems involving language, diffusion, and TTS models.</p><p>contact: hi@mio.ai
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219465"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219465" href="https://news.ycombinator.com/vote?id=39219465&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Edge Impulse | Full-time | REMOTE | North America &amp; Europe | <a href="https://jobs.lever.co/edge-impulse" rel="nofollow">https://jobs.lever.co/edge-impulse</a><p>Edge Impulse is the enterprise platform for edge AI and TinyML; we make it easy to train and deploy machine learning for embedded applications. Our customers are mainly in the industrial, healthcare, and wearables spaces.</p><p>It's the best place I have ever worked. Our team includes several pioneers of edge AI. The company is a mix of the best parts of US and European culture, and are super open and transparent.</p><p>We are hiring a Technical Product Manager, Senior Full Stack Engineer, Customer Solutions Engineer, Solutions Engineering Manager, Sales Account Executive, and SDRs.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219405"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219405" href="https://news.ycombinator.com/vote?id=39219405&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Zilliz is hiring! We're looking for REMOTE and/or HYBRID roles in SF<p>Zilliz is the company behind Milvus (<a href="https://github.com/milvus-io/milvus">https://github.com/milvus-io/milvus</a>), the most widely adopted vector database. Vector databases are a crucial piece of any technology stack looking to take advantage of unstructured data. Most recently and notably, Retrieval Augmented Generation (RAG). For RAG, vector databases like Milvus are used as the tool to inject customized data. In other words, vector databases make things like customized chat bots, personalized product recommendations, and more possible.</p><p>We are hiring for Developer Advocates, Senior+ Level Engineers and Product people, and Talent Acquisition. Check out all the roles here: <a href="https://zilliz.com/careers" rel="nofollow">https://zilliz.com/careers</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219436"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219436" href="https://news.ycombinator.com/vote?id=39219436&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Vocode || Engineering (multiple roles) || SF/Remote || Full-time/Contract || <a href="https://vocode.dev/">https://vocode.dev</a>
We (Vocode) empower developers to connect the world with magical conversational AI experiences. Our hosted APIs and open source repo are used by thousands of engineers to create transformative experiences for customers around the world.<p>Backed by YC (W23), Google Ventures, Base10, and Gradient Ventures and some fantastic angels!</p><p>Details on openings here: <a href="https://vocode.notion.site/Vocode-Careers-47f27748419d48f9bcc3bbbb54118afa?pvs=4" rel="nofollow">https://vocode.notion.site/Vocode-Careers-47f27748419d48f9bc...</a></p><p>If interested, please reach out to hiring@vocode.dev!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219452"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219452" href="https://news.ycombinator.com/vote?id=39219452&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>*PSN Americas: Remote Front End Developer | Full-Time or Contract*<p>At PSN Americas, we use data analytics to enhance public safety. We're looking for a Front End Developer with expertise in:
- JavaScript
- D3
- Leaflet+Mapbox
- Python (Django is a bonus)</p><p>You'll develop intuitive interfaces, ensuring complex data is accessible and actionable. This role demands proficiency in Git and the ability to work during North American hours in our remote team setting.</p><p>We offer a competitive salary, benefits, and the chance to grow in a field where your work truly matters.</p><p>If making a tangible difference appeals to you, send your resume and a brief cover letter to careers@psnamericas.com. Please include links to relevant projects or your portfolio.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219240"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219240" href="https://news.ycombinator.com/vote?id=39219240&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Koddi | Currently seeking FT employees in the following locations: Ann Arbor, MI | Fort Worth, TX | Austin, TX | New York, NY or remote in the US Open roles: Senior Software Engineers (Go, Java, C, C++); Integration Engineer; Product Manager (adtech experience preferred); Jr Data Scientist. Must be US Citizen or Green Card holder and physically located in the US.
Passionate about development in leading technologies? Looking to become a major player on a diverse team? Want to make a big impact on an engineer-driven roadmap in your next career adventure? Koddi Engineers drive innovation by embracing challenges and deploying emerging technologies to solve complex problems in software development.<p>Koddi is a technology company that was born in 2013 from an opportunity to innovate in the adtech space. Our award-winning SaaS platform provides a robust network for brands to connect with consumers and drive revenue through native sponsored placements, metasearch, and programmatic media campaigns.</p><p>Based in Fort Worth, Texas, we’ve grown to become a diverse global team. Ranked by Forbes, Deloitte, and Inc. magazine as one of the fastest-growing companies in the nation, we’re growing rapidly and looking for innovative problem solvers to join our team.</p><p>Review all open roles at www.koddi.com/careers and apply directly, or send your resume to matthew.myller@koddi.com.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219406"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219406" href="https://news.ycombinator.com/vote?id=39219406&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Olo | Remote (US) (HQ in NYC) | Full-Time | <a href="https://www.olo.com/" rel="nofollow">https://www.olo.com/</a><p>Olo is a leading on-demand commerce platform powering the restaurant industry’s digital transformation. Our customers include many of the largest restaurant brands in the US, and we have a lot of exciting projects in the works.</p><p>We're hiring for multiple remote (US-based) roles in engineering and other departments. Engineering roles include Senior Machine Learning Engineer, Backend .NET Engineer, Security Engineer, and Senior Pen Tester.</p><p><a href="https://jobs.lever.co/olo?lever-via=WSee4m3bRz&amp;lever-social=job_site" rel="nofollow">https://jobs.lever.co/olo?lever-via=WSee4m3bRz&amp;lever-social=...</a></p><p>Salary info included on all job listings and great benefits!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219319"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219319" href="https://news.ycombinator.com/vote?id=39219319&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>ML6 | Machine Learning Engineer, Data Engineer | Python, TensorFlow, PyTorch, GCP, AWS, Azure | Full-time | Amsterdam, Berlin, Ghent (EU) On-site/hybrid<p>We are a Machine Learning consulting company that builds end-to-end Machine Learning solutions. By applying the latest AI research, we keep our clients at the forefront of innovation.</p><p>If you are interested check out: <a href="https://www.ml6.eu/resources/resource-library" rel="nofollow">https://www.ml6.eu/resources/resource-library</a> and <a href="https://www.ml6.eu/client-cases" rel="nofollow">https://www.ml6.eu/client-cases</a></p><p>Work on innovative projects for the biggest clients across Europe such as Randstad, ASML, FUNKE, and many more! Whether it’s about leveraging LLMs to improve customer support, building data lakes on cloud platforms to improve storage or implementing models using sensor data for quality control. You can find it all at ML6.</p><p>You will mostly work with Python and a range of ML frameworks such as TensorFlow, PyTorch, or HuggingFace Transformers to solve hard Machine Learning tasks and help bring these application into production by building data pipelines and cloud infrastructure on all of the major cloud providers (GCP, AWS, Azure).</p><p>We are looking for:</p><p>• (Senior) Data Engineer</p><p>• (Senior) Software Engineer</p><p>• (Senior) Machine Learning Engineer</p><p>• Alliance Manager – Azure</p><p>• AI Client Executive</p><p>• AI Client Partner</p><p>• AI Project Manager</p><p>• Talent Partner</p><p>Apply at: <a href="https://ml6.eu/join-us" rel="nofollow">https://ml6.eu/join-us</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219269"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219269" href="https://news.ycombinator.com/vote?id=39219269&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><br><div>
                  <p><span>Yendou | Full-time | Berlin, Germany | Onsite | <a href="https://yendou.io/" rel="nofollow">https://yendou.io</a><p>Yendou is the first CRM designed to help Clinical Operation teams to scale and accelerate the allocation of Clinical Trials to clinics. This year we will ship an AI Co-Pilot for Clinical Operation teams</p><p>Right now, most products in the healthcare space suck, and we are looking for great people to help us change that! If Yendou is successful, you will be able to tell your grandkids that you worked on something that saved human lives :)</p><p># Founding Software Engineer | Salary: €80-120k + Equity: 0.5% - 1.5%</p><pre><code>  * Experience with JavaScript or TypeScript ideally also with a modern web framework such as React.js, Next.js, Vue.js or Angular

  * Prior experience in LLM fine-tuning, evaluation, and/or prompting

  * Background in building AI agents, Co-Pilots or other LLM products
</code></pre>
Read more: <a href="https://yendou.notion.site/Founding-Software-Engineer-Berlin-34faf942cc654e1ba593a1175279257d" rel="nofollow">https://yendou.notion.site/Founding-Software-Engineer-Berlin...</a><p># Founding Product Designer | Salary: €75-110k + Equity: 0.5% - 1.5%</p><pre><code>  * 3+ years of experience at tech or product-driven companies

  * You want to build up and own the design processes as the first Designer

  * You aspire to build great products like: Linear, Vercel, or Attio
</code></pre>
Read more: <a href="https://yendou.notion.site/Founding-Product-Designer-Berlin-caba7f9b258e4fcbbca0e4fad6c1fbef" rel="nofollow">https://yendou.notion.site/Founding-Product-Designer-Berlin-...</a></span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39217427"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217427" href="https://news.ycombinator.com/vote?id=39217427&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Gro Intelligence | Staff Frontend Engineer | NYC/hybrid | Full-time | $175k-$225k base + equity | React, Typescript | <a href="https://grnh.se/a9034ec34us" rel="nofollow">https://grnh.se/a9034ec34us</a> |<p>At Gro Intelligence, our data analytics platform provides predictive insights to answer vital questions around climate risk, agriculture, and the food supply chain. We’re looking for a Staff Frontend Engineer who will develop user-centric, dynamic, and beautiful visualizations in order to help our customers find those answers. Our current code base is React, Javascript, and Typescript.</p><p>Apply here: <a href="https://grnh.se/a9034ec34us" rel="nofollow">https://grnh.se/a9034ec34us</a></p><p>See some of our existing apps here: <a href="https://www.gro-intelligence.com/platform/applications" rel="nofollow">https://www.gro-intelligence.com/platform/applications</a></p><p>If you love a good challenge, have deep expertise, and a desire to make a positive impact you can find out more about engineering life at Gro here: <a href="https://gro-intelligence.com/engineering-at-gro" rel="nofollow">https://gro-intelligence.com/engineering-at-gro</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217336"><td></td></tr>
            <tr id="39217451"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217451" href="https://news.ycombinator.com/vote?id=39217451&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Urlbox | TypeScript / Next.js / DevOps (k8s) | REMOTE (UK) | Full-time |  £30K to £60K | <a href="https://urlbox.com/" rel="nofollow">https://urlbox.com</a><p>Urlbox helps web developers render the web with precision. We've been focused on generating screenshots, images and PDFs from HTML or URLs for over a decade. Our customers include over 500 design or compliance led organisations. They depend on us to get the intricacies of browser rendering right so they can focus on their core products and services.</p><p>We're bootstrapped, profitable and ready to add a third full-time engineer to our team. Our stack is primarily TypeScript. It's a bonus if you're also interested in learning how to orchestrate and scale headless browsers on our Kubernetes clusters. There's also opportunities to create/maintain libraries and SDK's in a range of other languages.</p><p>We're excited to hear from people early in their tech career as well as more experienced folk.</p><p>Read more: <a href="https://urlbox.com/jobs/typescript-developer" rel="nofollow">https://urlbox.com/jobs/typescript-developer</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219352"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219352" href="https://news.ycombinator.com/vote?id=39219352&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Canopy Connect | $160-205k | Full-time | Remote | US &amp; Canada only | Product Engineer<p>Canopy Connect is single-click insurance data sharing (like "Plaid for insurance").</p><p>We serve over 3,500 companies (and growing rapidly!) including insurance agencies, lenders, insurtechs, fintechs, and insurance carriers to streamline acquiring verified insurance data from consumers and businesses.</p><p>We're hiring Product Engineers. We're a small, fully remote team based in the US and Canada. We present the best of both worlds - a nimble startup and small team (low strike price on equity and more ownership/impact), yet derisked because of our capital efficiency.</p><p>Apply here: <a href="https://www.usecanopy.com/company/careers" rel="nofollow">https://www.usecanopy.com/company/careers</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219059"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219059" href="https://news.ycombinator.com/vote?id=39219059&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Grafana Labs | Engineers, SREs, Managers | REMOTE | Full-time | <a href="https://grafana.com/about/careers/open-positions/" rel="nofollow">https://grafana.com/about/careers/open-positions/</a><p>We make that dashboard thing you've all seen... we also make and operate massively scalable distributed databases for various telemetry, do lots of OSS things (+1K Github repos!), and having become the leader at observability in general we're now focusing on things like application observability, correlating data / exemplars, and creating new and easier ways to explore observability data... also, those huge massively distributed databases :)</p><p>We have been hiring constantly, and mostly struggle to keep our Greenhouse page up to date.</p><p>In my team alone (I'm VP, Engineering for the databases Mimir, Loki, Tempo, Pyroscope) I need 2 EMs (1 in Europe, 1 in the USA/Canada or Europe), an SRE in the USA/Canada, an SRE in Europe, an Engineer for Mimir in the US/Canada, and a couple of EU engineers for Loki. Those are just my team, but lots of teams are hiring. Additionally we will continually open roles throughout the year.</p><p>Grafana team really would like a Principal Engineer who knows distributed systems and databases built on object storage... we have lots of expertise here throughout the company, we just need more people who can ramp up fast :D</p><p>The greenhouse page works, but if you don't see something I've mentioned feel free to email (in profile).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218265"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218265" href="https://news.ycombinator.com/vote?id=39218265&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Bertram Labs | Senior Software Engineer | Full time | Hybrid/onsite | Broomfield, CO USA<p>Bertram Labs is a dedicated team of software engineers and marketing professionals that work on projects for our portfolio companies. Our portfolio includes companies in Business Services, Consumer, and Industrials verticals. We enable technology and drive growth through digital marketing, e-commerce, big data and analytics, application development and internal and external platform optimization.</p><p>We are seeking a Senior Software Engineer to work across our portfolio companies.</p><p>The ideal candidate is an adaptable, individual contributor who brings best practices and evolving technical knowledge to solve business problems. The candidate we are looking for is independent and able to devise solutions on their own, but also is a solid team member who enjoys collaborating with other developers, UI/UX designers, and stakeholders alike.</p><p>We are interested in talking with you if you enjoy working across the application stack, have a passion for learning new technologies, and willing to dig into tough problems.</p><p>More details and the link to apply can be found at <a href="https://boards.greenhouse.io/bertramcapitalmanagement/jobs/7084295002" rel="nofollow">https://boards.greenhouse.io/bertramcapitalmanagement/jobs/7...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219276"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219276" href="https://news.ycombinator.com/vote?id=39219276&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Superhuman | React.js, Node.js, Electron, Swift, Kotlin, Golang, GCP | Fully remote, Full-time | US, Canada, Mexico<p>We’re building the fastest experience in the world for work email, team communication, and productivity platform of the future. (superhuman.com)</p><p>We’ve built the fastest email experience in the world, and soon-to-be team communication and collaboration so professionals end each day feeling happier, more productive, and closer to achieving their potential. There are estimated to be a billion professionals that spend 2-3 hours a day in email and they deserve superpowers. Teams have adopted us at Spotify, Netflix, Airbnb, Notion, Dropbox etc.</p><p>Hiring for:
Senior Product Manager</p><p>Apply at: <a href="https://superhuman.com/jobs" rel="nofollow">https://superhuman.com/jobs</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217831"><td></td></tr>
            <tr id="39219043"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219043" href="https://news.ycombinator.com/vote?id=39219043&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>SKULabs | Node.js React Developer | $120k-$160k | Full-Time | On-site | Miami, Florida USA<p>SKULabs helps ecommerce companies scale their operations. Since 2014, our founder has commanded SKULabs offer the best customer service and no annual contract inventory, warehouse, and order management solutions. Designed in-house for an in-house need and scaled to support publicly traded companies and startups alike, SKULabs products are a boon to any Shopify or Amazon seller.</p><p>Help us scale out our latest product, innovate with new concepts of your own, and work on what moves the needle for our customers and our company. This is not your typical role seeing as you'll be one of the first ~10 employees here.</p><p>We're profitable and our goal is to make three additional hires this year to help support our new and existing customers.</p><p>Starting pay for qualified developers is $120k. We are looking for those with real-world experience of at least 1 years in a Node.js / React role especially those with experience managing administrative type portals with large numbers of users and those who are experienced with directly improving their own work in response to feedback from their users like we have every day for the last 8 years.</p><p><a href="https://jobs.lever.co/skulabs/338dd628-5281-4423-bad7-6b68e715f1d2" rel="nofollow">https://jobs.lever.co/skulabs/338dd628-5281-4423-bad7-6b68e7...</a></p><p>We're also still looking for the right Community Manager / Content Producer
<a href="https://jobs.lever.co/skulabs" rel="nofollow">https://jobs.lever.co/skulabs</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219521"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219521" href="https://news.ycombinator.com/vote?id=39219521&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>ivision Assessment | Software Deconstruction Engineer (aka Penetration Tester aka Infosec Consultant) | REMOTE<p><a href="https://research.ivision.com/careers" rel="nofollow">https://research.ivision.com/careers</a> for background &amp; working style, <a href="https://research.ivision.com/job" rel="nofollow">https://research.ivision.com/job</a> for the job posting.</p><p>We are a team of ~15 security consultants who do more than hack. We combine engineering and consulting expertise to act as trusted advisors to the world’s largest companies, while actively seeking healthy and harmonious lifestyles.</p><p>No information security experience required! Software engineers do great here.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219205"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219205" href="https://news.ycombinator.com/vote?id=39219205&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Bluesky Social, PBC | Backend Developer | Full-time | Remote (US timezone overlap)<p>+ Scale our existing services to millions of users</p><p>+ Develop and maintain the AT Protocol implementation (See: atproto.com)</p><p>+ Create SDK and API tools for an ecosystem of protocol developers</p><p>+ Have experience with TypeScript and/or Go</p><p>+ Have a strong knowledge of data structures and software design principles</p><p>+ Have built scalable and high-performance server-side applications</p><p>Our tech stack includes TypeScript, Go, ScyllaDB, SQLite, Redis, memcached, Protobufs/gRPC.</p><p>To apply, email recruiting@blueskyweb.xyz with:</p><p>+ A resume or CV (link or PDF)</p><p>+ A cover letter that tells us why you care and how you can contribute (link or PDF)</p><p>+ A link to your GitHub, GitLab, or a portfolio of past work</p><p>More details: <a href="https://blueskyweb.xyz/join/backend-developer" rel="nofollow">https://blueskyweb.xyz/join/backend-developer</a></p><p>(For HN users, optionally feel free to CC jake+recruiting@blueskyweb.xyz and I'll try to take a look.)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39219229"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39219229" href="https://news.ycombinator.com/vote?id=39219229&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><br><div>
                  <p><span>I applied to BlueSky at least once, maybe twice months ago and didn't even get an automated rejection email. Bit disheartened to say the least.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39219297"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39219297" href="https://news.ycombinator.com/vote?id=39219297&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><br><div>
                  <p><span>Sorry about that. The team is very small and quite busy, and we've been doing everything manually. But as of last week we have someone that can dedicate time to recruiting using a proper applicant tracking system. Our process should be improving a lot.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="39219017"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219017" href="https://news.ycombinator.com/vote?id=39219017&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Skydio - Autonomy Infrastructure | Bay Area - San Mateo | Python &amp; C++<p>Skydio is the leading US drone company and the world leader in aerial autonomy. Our mission is to make the world more productive, creative, and safe with autonomous flight. Today tens of thousands of our robots are flown at scale to inspect critical infrastructure and provide life-saving intelligence, and we have one of the best robotics teams on the planet in support of these goals.</p><p>Our X10 drone features NVIDIA Orin and Qualcomm 865 processors with 85 TOPS of compute capability, along with onboard 360 trinocular camera coverage. We develop real-time 3D reconstruction and semantic AI so that our robot can understand the world around it and make intelligent decisions.</p><p>We’re looking for infrastructure engineers and a director for our autonomy team who love making code for flying robots and for making our engineering team more productive. Experience with Python and C++, performance optimization, systems infrastructure, first-principles thinking, 3D visualization, communication and logging. full-stack web apps, and build systems all welcome. Very strong software engineering and computer science fundamentals required.</p><p>I lead the autonomy team and you can reach me at { hayk at skydio dot com }.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218314"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218314" href="https://news.ycombinator.com/vote?id=39218314&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>ISO | Full Stack/Back-end Software Engineer | Fully Remote (US Only) | <a href="https://iso.io/" rel="nofollow">https://iso.io</a><p>We are a Series A, fully distributed company on a mission to build the first Transportation Performance Intelligence platform that provides shippers (e.g. Coke), brokers (e.g. Uber Freight), and carriers (e.g. Alex Trucking - there are 500,000+ in the US alone) with the ability to measure the total costs of transportation service, together. The global supply chain accounts for roughly 10% of Global GDP, and is also one of the largest contributors to significant global challenges like air pollution and food waste. By changing how organizations work together across the supply chain, we will play a critical role in tackling these challenges head on. We are a team of inclusive, mission-driven doers with experience building world-class product, engineering, operations, and sales &amp; marketing teams. We hope you’ll join our growth story!</p><p>We are excited to speak to product-minded engineers who are comfortable with either full stack or back-end software development to join our founding engineering team.</p><p>Full job description and apply here: <a href="https://www.iso.io/about/careers/" rel="nofollow">https://www.iso.io/about/careers/</a></p><p>Our interview process:</p><p>* Conversation with Head of Engineering</p><p>* Remote Technical Screen</p><p>* Virtual “onsite” interview including 3 technical sessions, group interview, and conversations with every team member (we are still a small team of 10 on the EPD side)</p><p>* Conversation w/ a co-founder</p><p>* Offer</p><p>Everything is done via Zoom and there are no takehomes.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219034"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219034" href="https://news.ycombinator.com/vote?id=39219034&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><br><div>
                  <p><span>Bold Penguin | Remote (US)* | Full Time | Software Engineers Bold Penguin products help insurance agencies successfully quote and bind small business insurance policies with the click of button! We are growing fast and expanding our teams. Use Linux, macOS, or Windows along with whatever tools you enjoy using (Slack, Zoom, your choice IDE, etc.)<p>If you don’t meet all of the qualifications please don’t worry. Apply anyway!
Current open roles:</p><pre><code>  Software engineer ~&gt; https://www.thegravityapp.com/shared/job?clientId=8a7883d0676d10a10167856a16554ede&amp;id=8a7883ac8c7f605d018cb2243d7755ad&amp;u=1706553505&amp;v=9&amp;token=eyJ1aWQiOjQzNjY3LCJwcm92aWRlciI6ImJvdW5jZSIsInR5cGUiOiJlbWFpbCJ9.GIfFH42OWXmv0-yZ0ywaD0k6nCE  
  Account Executive ~&gt; https://www.thegravityapp.com/shared/job?clientId=8a7883d0676d10a10167856a16554ede&amp;id=8a7887a18cf53356018cfefd09636835&amp;u=1705085581&amp;v=9&amp;token=eyJ1aWQiOjQzNjY3LCJwcm92aWRlciI6ImJvdW5jZSIsInR5cGUiOiJlbWFpbCJ9.8eY8wqZt8KRw_85oszyrco4eAgQ
  Customer Success Manager ~&gt; https://www.thegravityapp.com/shared/job?clientId=8a7883d0676d10a10167856a16554ede&amp;id=8a7887ac8c7f6022018cca9302ee3803&amp;u=1706542521&amp;v=9&amp;token=eyJ1aWQiOjQzNjY3LCJwcm92aWRlciI6ImJvdW5jZSIsInR5cGUiOiJlbWFpbCJ9.qK1pLxyK9L6jmnB7bwjT1FSYYw0
  Customer Success Advocate (Columbus) ~&gt; https://www.thegravityapp.com/shared/job?clientId=8a7883d0676d10a10167856a16554ede&amp;id=8a78859e8cf533ca018cffae8d5a7a27&amp;u=1706299698&amp;v=9&amp;token=eyJ1aWQiOjQzNjY3LCJwcm92aWRlciI6ImJvdW5jZSIsInR5cGUiOiJlbWFpbCJ9.RN4XhOKO5H9uZoYodI_3I7bb924
</code></pre>
* We are fully remote and headquartered in Columbus but have employees live within 150 miles of certain US cities which are listed on the job description.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39219000"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219000" href="https://news.ycombinator.com/vote?id=39219000&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Neurotone.ai || Founding Engineer || Fully Remote || Full-time || neurotone.com<p>Neurotone is developing an AI powered auditory training system to assist people with hearing disabilities get the most out of their hearing aids. We are utilizing the clinically proven LACE methodology to facilitate the best outcome for every patient, and we are partnered with a number of thought leaders in the audiology community to ensure we are delivering the most effective solution on the market.</p><p>We are looking for a founding/principal engineer and/or senior software engineer(s) to work alongside our CTO (based in Portugal) and help us deliver the next generation of our product, which will be a mobile-first application built on Expo (React Native) and Firebase. We are extremely passionate about technology, and our entire core team is composed of industry veterans with successful exits in previous ventures.</p><p>This is a proven product with existing revenue that we are excited to take to the next level, more importantly, this system can provide a dramatic improvement the quality of life for people suffering with auditory disabilities.</p><p>Please contact travis AT neurotone.com if interested. Thanks!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218015"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218015" href="https://news.ycombinator.com/vote?id=39218015&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Endress + Hauser | Switzerland (Basel) or Germany (Freiburg) | Senior Ruby Engineer | Full-time (flexible)<p>-- company</p><p>Endress + Hauser is a process and laboratory instrumentation and automation supplier. We offer world class instrumentation and all the services around them in almost all industries. Our products are used e.g. in potable water metering, chemical and pharmaceutical manufacturing contexts.
The company is still owned by the founders and takes pride in its social core values.</p><p>-- we</p><p>Are a small platform team (10 people with different roles) operating the central api for the digital offering (<a href="https://netilion.endress.com/" rel="nofollow">https://netilion.endress.com/</a>) around the industrial Internet of Things.
While we ingest measurements of connected instruments (the classic IoT). We also provide a digital mirror of the customers manufacturing plant including all documentation that occurs over the lifetime of the instruments and health-monitoring.</p><p>Our tech stack includes a large Ruby on Rails monolith handling the api part, but also time series databases, storage backends and some Amazon Lambdas.</p><p>-- job</p><p>We are looking for experienced (ruby) engineers that help us foster and build our platform team and api offering.</p><p>Feel free to mail me at oliver[point]wisler[at]endress.com 
(I'm a software engineer)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219054"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219054" href="https://news.ycombinator.com/vote?id=39219054&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>SelectIQ (YC W23) | Founding Software Engineer (Fullstack) | NYC or REMOTE (US) | Full-Time | <a href="https://selectiq.ai/">https://selectiq.ai/</a><p>SelectIQ works with clinical trial sites and their referring physicians to automate the patient referral process, massively speeding up enrollment.</p><p>Bringing new drugs to market is primarily rate limited on the speed of clinical trials, which are themselves bottlenecked by patient recruitment. 80% of clinical trials don’t meet their recruitment timeline, and being behind on recruitment can cost pharma companies millions of dollars per day.</p><p>We're looking to bring on our first engineering hire to help us build out the platform and build an amazing culture. More details and application can be found here: <a href="https://www.ycombinator.com/companies/selectiq/jobs/4aRBMcX-founding-engineer-full-stack-employee-1">https://www.ycombinator.com/companies/selectiq/jobs/4aRBMcX-...</a></p><p>Feel free to email me directly - jason@selectiq.ai
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217803"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217803" href="https://news.ycombinator.com/vote?id=39217803&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Mightier (<a href="http://mightier.com/" rel="nofollow">http://mightier.com</a>) | Video Game Developer | Boston, MA | Full Time | Hybrid or Remote<p>Mightier is a video game platform that uses biofeedback to help kids' mental health. Our intervention is clinically validated, with efficacy on par or greater than current treatments (<a href="https://www.mightier.com/science/" rel="nofollow">https://www.mightier.com/science/</a>). Currently we are integrating with the healthcare system to provide access via normal healthcare channels (<a href="https://www.marketplace.org/shows/marketplace-tech/in-one-state-a-video-gaming-app-for-kids-well-being-is-now-covered-by-medicaid/" rel="nofollow">https://www.marketplace.org/shows/marketplace-tech/in-one-st...</a>).</p><p>It only works if kids use it! We are hiring a software engineer to help our game team. Our software is built in C# using the Unity game engine. We pay close attention to our kids to stay close to who we are making this for. We pay close attention to our parents and clinicians to keep it grounded in the real world. Our ideal candidate has shipped at least one title.</p><p>The official posting, with information on how to apply, is on our careers page: <a href="https://www.mightier.com/careers/" rel="nofollow">https://www.mightier.com/careers/</a></p><p>Very excited to hear from anyone interested.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39218085"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39218085" href="https://news.ycombinator.com/vote?id=39218085&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><br><div>
                  <p><span>Not looking to apply, but having tried this out some years ago, I really love the approach. Using indie games to get kids to handle their emotions is great.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39218929"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218929" href="https://news.ycombinator.com/vote?id=39218929&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Lune | Software Engineers, Sales | London or REMOTE (London +/- 2 hours) | Full-Time | <a href="https://lune.co/" rel="nofollow">https://lune.co</a><p>Lune’s mission is to make every product and service climate-positive by default.</p><p>With the Lune API, we enable companies to seamlessly build emissions calculations and high-quality carbon removal into their product and services and make it part of the customer experience. In the future, everything we do will have a positive impact on the planet – powered by Lune.</p><p>If you’re a talented engineer or sales professional who learns quickly and cares about tackling the climate crisis, we’d love to work with you!</p><p>These roles are a unique opportunity to be part of Lune’s core team, to have a real impact on our mission, and to define and scale the company into the future.</p><p>As we grow, you’ll have the opportunity to take on new responsibilities and help build a great company while tackling the greatest challenge of our time.</p><p>Interested?</p><p>Tech stack: TypeScript, React, Nextjs, AWS, PostgreSQL, Terraform, Kubernetes.</p><p>Hiring for:</p><p>- Frontend/Backend/Fullstack Software Engineers (all levels)</p><p>- Technical writers (experienced)</p><p>- Head of sales</p><p>- Account executive: logistics and carbon markets</p><p>- Business development representative - logistics</p><p><a href="https://lune.crew.work/">https://lune.crew.work</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218081"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218081" href="https://news.ycombinator.com/vote?id=39218081&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Uncountable | SF &amp; Munich (In-Person) | Full-Stack Engineering
<a href="https://www.uncountable.com/hiring/hn" rel="nofollow">https://www.uncountable.com/hiring/hn</a>
Uncountable accelerates R&amp;D for industrial scientists across leading materials, chemicals, and life sciences organizations. With our SaaS solution, our customers get better products to the market in half the time. Uncountable was founded by MIT and Stanford engineers and has been profitable since 2016. Our team has grown from 20 to 50 over the last two years.<p>Full-Stack Engineers | $120k - $220k + Equity</p><p>---&gt; Uncountable is looking for engineers who can spearhead the development of the Uncountable Web Platform. The position is heavily product-driven and comes with challenges across the stack.</p><p>--&gt; Summer internships and working student positions are also available.</p><p>Learn more: <a href="https://www.uncountable.com/hiring/hn" rel="nofollow">https://www.uncountable.com/hiring/hn</a></p><p>Uncountable has offices in San Francisco, New York City or Munich.</p><p>Contact our CTO directly at jason@uncountable.com
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218843"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218843" href="https://news.ycombinator.com/vote?id=39218843&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Continua AI | ML and Systems Engineers | NYC, SF Bay Area, Seattle | <a href="http://continua.ai/" rel="nofollow">http://continua.ai</a><p>Stack: Python | C# | GCloud | HuggingFace</p><p>Continua was founded in April 2023 by a Distinguished Engineer from Google Research to reinvent how people interact with information, with services, and with each other.</p><p>We're very early; after building several prototypes in our first few months, we’ve developed conviction about an MVP, and we’re building like mad to bring our ideas to users. This is an opportunity to join and help set the product and technical direction of this brand-new company! ML engineers at Continua work on new approaches for knowledge representation, multimodality, and personalization under computational resource constraints. Systems engineers at Continua work on a wide variety of secure and reliable backend services and data pipelines to power our products.</p><p>We’re backed by tier-1 VCs, and we’ve already assembled a team of six engineers with backgrounds at Google, Microsoft, Slack, SpaceX, CourseHero, and Stash. We have a limited number of additional spots available on the team for engineers who are excited about building innovative products that leverage the latest advances in ML.</p><p>If you are passionate about building innovative new products, and you’re eager to work in a dynamic startup environment, we would love to hear from you. Please apply via  <a href="https://www.continua.ai/careers" rel="nofollow">https://www.continua.ai/careers</a> and let's embark on this exciting journey together!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218794"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218794" href="https://news.ycombinator.com/vote?id=39218794&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Railway | Full-stack Engineering (Product), Platform Engineering (Infra) | REMOTE (International) | <a href="https://railway.app/careers" rel="nofollow">https://railway.app/careers</a><p>Tired of trying to beat kube into shape? Does writing YAML to ship code fill you with utter dread? Dream of a future where deploying software is simple, and you don't need an army of infrastructure engineers to build that perfect janky bash script™ to make life easy?</p><p>We're Railway, and we think infrastructure can be better. So far we've built out a platform loved by hundreds of thousands of users who simply tell us "Give me Postgres", "Deploy this repo", and we make it happen</p><p>Fair warning! The problems are complex: home-rolled hypervisors, cut-above container orchestration, over/under/whateverlay networks, virtio device drivers, edge proxies, IAM that doesn't suck, kitchen sinks - we need to build it and we're looking for likeminded individuals who think this stuff is fun.</p><p>If that sounds like you, please apply at railway.app/careers. We have a number of roles, but are prioritizing the following two roles:</p><p>+Full-stack Engineer (<a href="https://blog.railway.app/p/team-spotlight-product-engineering" rel="nofollow">https://blog.railway.app/p/team-spotlight-product-engineerin...</a>)</p><p>+Infrastructure Engineer</p><p>See you soon, and happy shipping.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218957"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218957" href="https://news.ycombinator.com/vote?id=39218957&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Radar Labs | Software Engineers (full-stack, mobile) | ONSITE in New York City | Full Time <a href="https://radar.com/" rel="nofollow">https://radar.com</a> Radar Labs is location infrastructure as a SaaS platform. Over the past year Radar has been used from 100M+ devices doing 10,000 qps. We are are a small but mighty engineering team (13 engineers!). We're based in NYC with our HQ in Union Square.
There's a ton of opportunity to make an impact and work on a variety of things, as we're a pretty fluid team:<p>- Mobile (e.g. open-source mobile infrastructure, building a control system that automatically configures optimizing battery-life and location accuracy for different use-cases over time)</p><p>- Full-Stack (e.g. build tools to visualize and debug location data at scale)</p><p>What you'll do:</p><p>- Work on core Radar mobile infrastructure built with Kotlin, Objective-C, Swift, as well as wrapper SDKs written in React Native, Flutter and Capacitor.</p><p>- Work on full-stack features across the frontend and backend</p><p>- Embody our "Walk a mile" value by walking around to QA and debug the Radar SDK</p><p>- Push the limits of what is possible with location services on iOS and Android</p><p>- Have your work run on 100's of millions of devices</p><p>- Talk to Radar customers and prospects, hear their feedback, incorporate it into your work and make them successful</p><p>Check out our jobs page here: <a href="https://apply.workable.com/radarlabs/j/2BC5714908/" rel="nofollow">https://apply.workable.com/radarlabs/j/2BC5714908/</a></p><p>If you have any questions, feel free to reply here or you can e-mail me at tim@radar.com
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217850"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217850" href="https://news.ycombinator.com/vote?id=39217850&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Mindmonopol | Senior Frontend Engineer (Angular) | Remote (EU) | Full-time or part-time | English (German helpful but not required)<p>Mindmonopol is building a product that will enable consultants to perform data analytics tasks similar to a no-code fashion, enabling reusability of components. Our aim is to focus on tasks such as data migrations, year-end closing and other things that usually take a lot of formal work to verify and automate the formality by reusing proven methods.</p><p>Our frontend is in dire need of a senior engineer. We are specifically looking for someone with enough experience and know-how to be able to architect a system. Up to now, we had no frontend specialist (except a freelancer working 1 day a week) and it turned into a mess. We want to build the frontend on a solid foundation and are looking for someone that is willing to take ownership and incentive, who isn't shy to raise concerns. Ideally you would be willing to grow into a leadership position over time, but it isn't strictly required, right now our focus is on looking for someone with a strong technical background.</p><p>The frontend is built on Angular/AWS.</p><p>Contact us at info [at] mindmonopol [dot] de, the email address goes to the inbox of Markus, tell him you're coming from Leo.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218341"><td></td></tr>
            <tr id="39217871"><td></td></tr>
                <tr id="39217957"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39217957" href="https://news.ycombinator.com/vote?id=39217957&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><br><div>
                  <p><span>im sorry you had to find out this way, but there is already a very well-known, mercedes group-owned car brand called "smart car"</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39217628"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217628" href="https://news.ycombinator.com/vote?id=39217628&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Packfleet | iOS, TypeScript &amp; Rust | Full time | £70-110k + meaningful equity | Hybrid, Onsite | London, UK | <a href="https://packfleet.com/" rel="nofollow">https://packfleet.com/</a><p>Packfleet is a fast growing delivery startup based in London, founded by early employees of Monzo. We're out to make fully electric next-day deliveries the new normal, while improving every aspect of the delivery experience using modern tech.</p><p>We're a tech company and own our delivery technology end to end. That means we have lots of interesting product areas we're looking to develop further:</p><p>- Driver &amp; warehouse apps</p><p>- Routing &amp; optimization</p><p>- Tooling for live operations</p><p>- Tooling for merchants</p><p>- Public API and integrations</p><p>- Recipient app for tracking deliveries and more!</p><p>Roles currently available:</p><p>- Senior iOS engineer | <a href="https://apply.workable.com/packfleet/j/A95FEF8073/" rel="nofollow">https://apply.workable.com/packfleet/j/A95FEF8073/</a></p><p>- Senior product engineer | <a href="https://apply.workable.com/packfleet/j/09BC8EBB9B/" rel="nofollow">https://apply.workable.com/packfleet/j/09BC8EBB9B/</a></p><p>Apply at <a href="https://apply.workable.com/packfleet/" rel="nofollow">https://apply.workable.com/packfleet/</a> or email me on rico at packfleet.com.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218904"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218904" href="https://news.ycombinator.com/vote?id=39218904&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Resemble AI | San Francisco Bay Area (office in Santa Clara, CA) | Full-Time | Full-Stack Engineer, Frontend Engineer, Product Designer<p>Resemble AI creates high-quality synthetic voices that capture human emotion. We're a venture-backed high-growth startup that's looking to shake up an entire industry with state of the art AI. Our product changes the way that thousands of brands, media companies, creative agencies, and game studios create speech content. We believe that to build an enticing product and solid team is by encouraging innovation is by enabling continuous education. That's why every Friday is a day that you can use to work on anything you want, Resemble-related or not.</p><p>Recently, we open sourced a state of the art speech enhancement model: <a href="https://github.com/resemble-ai/resemble-enhance">https://github.com/resemble-ai/resemble-enhance</a></p><p>We're hiring for three roles:</p><p>Full Stack Engineer - Can you break the entire stack? You're the right person for this job. Work on our Rails app, with sprinkles of React, and Python for the deep learning. Everything is dockerized, and we use Kubernetes to deploy.</p><p>Frontend Engineer - We're hiring a Frontend Engineer proficient in React, TypeScript, and Ruby on Rails to shape our user experience. Join our team to develop user-friendly interfaces and collaborate on building exceptional web experiences.</p><p>Product Designer - As a Product Designer, you will lead the end-to-end design process, from concept to implementation, ensuring a seamless and delightful user experience. You will collaborate with cross-functional teams to define product vision, conduct user research, create visually compelling interfaces, and develop interactive prototypes.</p><p>If interested, reach out directly to me: zohaib [at] resemble.ai
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217619"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217619" href="https://news.ycombinator.com/vote?id=39217619&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Snowflake (Streamlit) | Bellevue/San Mateo | Open Source | Full-time | <a href="https://www.streamlit.io/" rel="nofollow">https://www.streamlit.io</a><p>We're hiring in many engineering roles on the Streamlit team, and many neighboring teams at Snowflake. At highest priority, I'm looking for an Engineering Manager to lead our Streamlit Open Source team. Streamlit is a popular open-source project that's changing the way people visualize data and build interactive apps. We're seeking someone who loves UI/UX + Data, a background in managing full-stack teams, and a drive to innovate in the open-source space.</p><p>Our ideal candidate has 3+ years of experience in leadership roles, a startup mentality, and high energy to champion Streamlit. This is a really special role for us. Proficiency in Python and React is a plus, along with experience in data science or interactive visualization tools.</p><p>Apply directly here: <a href="https://careers.snowflake.com/us/en/job/7118167002/Software-Engineering-Manager-Streamlit-Open-Source" rel="nofollow">https://careers.snowflake.com/us/en/job/7118167002/Software-...</a>, or drop me a line at jon.roes at snowflake.com.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217482"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217482" href="https://news.ycombinator.com/vote?id=39217482&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Sanctuary Computer | Senior Fullstack Developer | Hybrid (Remote and/or NYC) | Full-time<p>At Sanctuary Computer(<a href="https://www.sanctuary.computer/" rel="nofollow">https://www.sanctuary.computer/</a>) — we’re building a different type of technology shop. Based in NYC’s Chinatown (and remote), the way we approach building things is more like a restaurant than a traditional agency, with an emphasis in providing our clients with an outspoken sense of comfort and hospitality.</p><p>In this role, you’ll be joining our team as a senior developer who’s eager to practice Value Engineering while solving tough challenges. Responsibilities will include:</p><p>- Helping us to scope and estimate development work on client projects</p><p>- Drafting technical architecture plans for implementation</p><p>- Communicating directly with clients to align project scope with budget and timeline</p><p>- Owning the success of the project with a close eye on team velocity, project scope, budget, and timeline</p><p>- Supporting more junior team members through code review and advising</p><p>- Contributing to projects as an individual contributor and lead</p><p>Read more here: <a href="https://www.notion.so/garden3d/Senior-Full-stack-Developer-de17f758bfe74a9ea7929b0006fab7fc" rel="nofollow">https://www.notion.so/garden3d/Senior-Full-stack-Developer-d...</a></p><p>Please apply by emailing us at jobs@sanctuary.computer with the subject line: Senior Full-stack Developer
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219013"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219013" href="https://news.ycombinator.com/vote?id=39219013&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Octoscreen | Backend engineer | Remote (EU) | Full-time | <a href="https://www.octoscreen.de/" rel="nofollow">https://www.octoscreen.de</a><p>Octoscreen is helping clients to monitor locations and important information.
We ingest a lot of data from various sources and display the most important information for our clients.</p><p>We are looking for an Elixir developer who is also a generalist and can work with a diverse set of technologies.
The ideal candidate is someone who is following tech trends in the industry, keeps his/her knowledge up to date and is proactive in improving the product.</p><p>We don't do agile or any strict process. We focus on the product and the customer.
Small dev team (&lt;5 people).</p><p>Tech stack: Elixir, Phoenix LiveView, PostgreSQL, Docker, Linux.</p><p>EU only. Remote or office (Munich, Germany).</p><p>Contact: barna at octoscreen.de
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218924"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218924" href="https://news.ycombinator.com/vote?id=39218924&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Sheer Health (<a href="https://www.sheerhealth.com/" rel="nofollow">https://www.sheerhealth.com</a>) | New York, NY | Senior Site Reliability/Platform Engineer | Full-Time | ONSITE | $170-210k + equity<p>If you or a loved one has ever suffered from confusing/egregious medical bills or insurance communications, this is your chance to give the power back to others in a similar situation.</p><p>Our mission is to demystify health insurance, healthcare and medical billing. By utilizing cutting-edge technology and advanced data analytics, we deliver personalized recommendations on how to save money, maximize use of plan benefits, stay on top of reimbursements and avoid erroneous or inflated medical bills.</p><p>We're looking for a senior site reliability/platform engineer to help us build our a robust system with which to serve our members.</p><p>Drop us an email to hn ~at~ sheerhealth.com if this sounds interesting!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218939"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218939" href="https://news.ycombinator.com/vote?id=39218939&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Vanilla | Fully Remote (US Based) | Full-Time Positions Domain: Financial and Legal Tech Vanilla is a financial and legal technology company that is changing the way that people interact with their end-of-life planning. We are a series B company and are growing fast.
Full Stack Engineer (Mid to Senior Level)<p>- ReactJS/Typescript / Familiarity and interest in modern React ecosystem - GraphQL / Apollo</p><p>- Rails and Postgres</p><p>- Design / UI Focused</p><p>- Familiarity with Figma/Storybook/modern design tools</p><p>- Experience with AWS</p><p><a href="https://justvanilla.applytojob.com/apply/c07bUBBNlk/FullStack-Engineer-MidSenior" rel="nofollow">https://justvanilla.applytojob.com/apply/c07bUBBNlk/FullStac...</a></p><p>Jobs are fully remote but US based only please!</p><p><a href="https://www.justvanilla.com/careers" rel="nofollow">https://www.justvanilla.com/careers</a></p><p>Email kei.yasui@justvanilla.com if interested.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217448"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217448" href="https://news.ycombinator.com/vote?id=39217448&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>IVPN | Golang Developer | Remote (UTC-1 to UTC+3) | Full-time | <a href="https://www.ivpn.net/" rel="nofollow">https://www.ivpn.net</a><p>IVPN is a privacy-focused VPN service in operation since 2010. We have high ethical standards, regular security audits and a stellar reputation among security and privacy analysts.</p><p>We are looking for a Golang Developer to work on a new project, a privacy-friendly DNS solution (B2C).</p><p>Ideally you possess the following knowledge: 
- Strong experience with Golang. Minimum of 2 years experience in your last role.
- Experience with back-end development, including server, network, and hosting environments. 
- Strong networking knowledge, specifically relating to implementing and managing DNS.
- Fluent in SQL and able to create complex database schemas.
- Understanding of security best practices to safeguard user data.</p><p>We are looking for mid-level/senior candidates who can own the project from planning to release (with internal help, of course).</p><p>You can email me if you have any questions about the role: viktor@ivpn.net 
if you are ready, it's better to apply here: <a href="https://ivpn.recruitee.com/o/golang-developer" rel="nofollow">https://ivpn.recruitee.com/o/golang-developer</a></p><p>Don't forget to mention you found the role on HN.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217771"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217771" href="https://news.ycombinator.com/vote?id=39217771&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Orb | Software Engineers | San Francisco, CA (in-person, on-site) | Full Time<p>Orb is building billing infrastructure for modern companies. As companies grow and evolve, the ability to change their pricing and packaging is often limited by the sheer complexity of the engineering effort involved to make those changes. I experienced these challenges first hand and my desire to solve this problem is what led me to start Orb. Orb powers billing for the fastest growing developer companies, including Vercel, Replit, Materialize, Neon, and many more.</p><p>Join Orb to tackle technical challenges ranging from massive infra challenges (individual customers send us XM+ events/sec) to tricky data modeling problems in a new world of billing.</p><p>At Orb, we're looking to fill a handful of engineering roles including:</p><p>- Software Engineer, Customer Experience: <a href="https://jobs.ashbyhq.com/orb/68c8720a-5124-4611-97f6-56d8aa63419e">https://jobs.ashbyhq.com/orb/68c8720a-5124-4611-97f6-56d8aa6...</a></p><p>- Software Engineer, Full Stack: <a href="https://jobs.ashbyhq.com/orb/25d6b4f8-72b9-44ff-a48f-29a60c724cad">https://jobs.ashbyhq.com/orb/25d6b4f8-72b9-44ff-a48f-29a60c7...</a></p><p>- Software Engineer, Platform: <a href="https://jobs.ashbyhq.com/orb/8043aa64-dd69-4682-8ad3-8f6d654f2381">https://jobs.ashbyhq.com/orb/8043aa64-dd69-4682-8ad3-8f6d654...</a></p><p>- UI Engineer: <a href="https://jobs.ashbyhq.com/orb/bbb38cb8-6b71-4d3e-b8f6-32b0f153af7e">https://jobs.ashbyhq.com/orb/bbb38cb8-6b71-4d3e-b8f6-32b0f15...</a></p><p>You can find out more about the team and company here: <a href="https://www.withorb.com/about" rel="nofollow">https://www.withorb.com/about</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217361"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217361" href="https://news.ycombinator.com/vote?id=39217361&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>PlantingSpace | Full-time | Remote (EU time zone) with quarterly gatherings | <a href="https://planting.space/" rel="nofollow">https://planting.space</a><p>We are building an AI system that can accurately represent knowledge and handle uncertainty, to enable the discovery of insights and solve problems based on explainable reasoning. We envision applications to automate analysis and speed up research in domains such as Finance, Strategy Consulting, Engineering, Material Sciences, and more.</p><p>We are continuously looking for strong software engineers who are up for a challenge and a steep learning curve. You’ll be exposed to cutting edge research in Bayesian statistics, dynamical systems, information theory, category theory, and more.</p><p>Our current job openings include:</p><p>- Product Manager who has launched and released successful emerging technology products.</p><p>- NLP Engineers who have state of the art LLM and up to date transformer model experience.</p><p>Find more about our tech, our organisation, and ways of work on our website: <a href="https://planting.space/" rel="nofollow">https://planting.space/</a></p><p>To see a full list of openings, and to apply, check out our Join Us page: <a href="https://planting.space/joinus/" rel="nofollow">https://planting.space/joinus/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218172"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218172" href="https://news.ycombinator.com/vote?id=39218172&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Viator | Software Engineer all levels | Full time | Remote &amp; Hybrid On-site | Portugal, UK<p>Viator connects suppliers to travelers, creating the world's largest platform for travel experiences.</p><p>We are growing fast and have many positions to fill in Portugal and the UK.  We are looking for engineers at all levels for full-stack, backend and data teams.  Roles in Portugal require you to be in the office in Lisbon part time.  UK roles can be part time in an office in London or Oxford or fully remote. For the remote roles you still need to be based somewhere in the UK.</p><p>The full list of open roles is here: <a href="https://bit.ly/viator-jobs" rel="nofollow">https://bit.ly/viator-jobs</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218071"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218071" href="https://news.ycombinator.com/vote?id=39218071&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>OMMO | Dallas, TX, Visa, Full-time | ommo.co<p>Ommo is the pioneer of permanent magnet-based 3D tracking systems - building the foundation of human-computer interaction and digital transformation. Primary use case is surgical navigation then expanding to robotics, metaverse, and XR.</p><p>- Team of 20 from Apple, Samsung, LG, Intel, Huawei, Riot Games.</p><p>$14M raised, 100+ customers, winner of numerous awards, first product release is coming up this year!</p><p>- Salary + equity, flexible schedules, comprehensive benefits</p><p>Join us in tackling one of the most interesting, full-stack engineering challenge that has outsized real world impact and solid business foundation.</p><p>Hiring all roles at ommo.co/career:</p><p>- Engineering Project Manager - Onsite</p><p>- Algorithm / Signal Processing Engineer - Onsite (Hybrid possible)</p><p>- Sr. Firmware Engineer - Onsite</p><p>- Sr. Software Engineer - Onsite (Hybrid possible)</p><p>Questions? careers@ommo.co
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217388"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217388" href="https://news.ycombinator.com/vote?id=39217388&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Jam.dev | Staff Fullstack Engineer &amp; AI Product Engineer | Typescript/React | Remote (+ in person in SF, Austin, NYC) | Full-time<p>Dev tools company with 75,000+ users in less than 2 years. $10M in funding from Vercel CEO, GitHub CTO, Cloudflare CEO, etc.</p><p>We’re building a flight recorder for web apps – so anyone can report issues to engineers in a way that's actually debuggable (w/ console, network, websockets debugger, etc).</p><p>Small, senior team – several ex-engineering directors turned ICs (mostly ex-early Cloudflare). Looking for staff-level engineers with experience building highly performant front-end apps.</p><p>Stack: React/Typescript and MobX (MST) on the frontend, and Node/GraphQL across our backend.</p><p>The challenge ahead: Scaling. Usage 10x’ed last year, and our users are in 150+ countries, on all sorts of devices, network conditions, etc. Our bar for quality is high.</p><p>As a dev tool, developers at Jam are directly connected and involved with the product. Your usage of the product will directly inform the direction of Jam’s future.</p><p>Apply here (we read and respond to every submission):<a href="https://8wvzuv0oszm.typeform.com/work-at-jam" rel="nofollow">https://8wvzuv0oszm.typeform.com/work-at-jam</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218898"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218898" href="https://news.ycombinator.com/vote?id=39218898&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Chestnut | Founding Software Engineer | Remote (US only) or onsite hybrid in SF | Full-Time | <a href="https://jobs.ashbyhq.com/Chestnut">https://jobs.ashbyhq.com/Chestnut</a><p>Chestnut is a well funded (a16z) close-knit seed stage insurtech - building next-gen enterprise infrastructure for the insurance industry. If you are a passionate full stack Golang Developer with a track record of building high-quality applications and want to be part of a vibrant team that embraces innovation, we'd love to hear from you!</p><p>Golang, React, k8s, AWS</p><p><a href="https://jobs.ashbyhq.com/Chestnut/c6df5678-829e-4c98-9dbb-82fef53b1605">https://jobs.ashbyhq.com/Chestnut/c6df5678-829e-4c98-9dbb-82...</a></p><p>If interested, you can apply above or reach out to me directly: nick [at] chestnutfi.com
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217926"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217926" href="https://news.ycombinator.com/vote?id=39217926&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>The Trade Desk | Data Scientist | London, UK | Hybrid, on-site | Full-time<p>TTD empowers advertisers with a cutting-edge, data-driven platform for digital advertising and media buying.</p><p>We're looking for an experienced Data Scientist to join our London team.
In this role, you'll craft algorithms for our real-time bidding platform, tackling sparse data and low latency issues.
We're looking for someone with strong Python and Spark skills, a knack for creative problem-solving, and a product-focused approach.
We value teamwork, open communication, and a passion for continuous learning.</p><p>Learn more and apply here: <a href="https://careers.thetradedesk.com/jobs/4206630007/data-scientist" rel="nofollow">https://careers.thetradedesk.com/jobs/4206630007/data-scient...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217422"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217422" href="https://news.ycombinator.com/vote?id=39217422&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>ReciPal (<a href="https://www.recipal.com/" rel="nofollow">https://www.recipal.com</a>) | Founding Rails/Full Stack Software Engineer | REMOTE | Full-Time<p>We are hiring a full-time engineer who can hopefully be a technical stakeholder and business partner.</p><p>ReciPal is a profitable, growing, bootstrapped, and (have always been a) fully remote company (based in New York) with just about 4 full-time employees. We make simple software for food businesses - nutrition analysis and labeling, costing, and inventory management.</p><p>You'd be working directly with me, solo founder, and our senior engineer and hopefully take on a growing role over time to potentially lead engineering and become a business partner. It's a very low key work environment, but independent and plenty of responsibility and ownership for the right person.</p><p>Stack includes Rails 7, JavaScript, Postgres, Heroku.</p><p>Full job description:
<a href="https://docs.google.com/document/d/1At0_nQH5Vqvn3AQRjPLLUMi2TIypHVtb_IiEMV2FH0U/edit?usp=sharing" rel="nofollow">https://docs.google.com/document/d/1At0_nQH5Vqvn3AQRjPLLUMi2...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217556"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217556" href="https://news.ycombinator.com/vote?id=39217556&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>SingleStoreDB (formerly MemSQL) (<a href="https://singlestore.com/" rel="nofollow">https://singlestore.com</a>) | India, Portugal | Full Time | Remote
SingleStoreDB is a database focused on high performance and hybrid workloads (HTAP). Our customers include half of the top 10 US banks, 2 of the top 3 US telcos, and 12% of the Fortune 100.<p>Our product is a distributed, relational database that handles both transactions and real-time analytics at scale. Querying is done through standard SQL drivers and syntax, leveraging a broad ecosystem of drivers and applications.</p><p>We have been building SingleStore since 2011, and building a database-as-a-service product involves a bunch of interesting technical challenges such as: * Compilers and code generation</p><p>* Distributed computing and fault tolerance</p><p>* UI and User Experience</p><p>* Integrations with connectors and third-party tools (e.g., Business Intelligence tools like Tableau)</p><p>* Infrastructure that scales to dozens of thousands of database clusters across GCP, Azure and AWS</p><p>We have the following positions open:</p><p>* Partner Integration Engineer (Portugal, India)</p><p>* Software Engineer, Managed Services (Portugal, India)</p><p>Careers page with individual links for each open position: <a href="https://www.singlestore.com/careers/jobs/" rel="nofollow">https://www.singlestore.com/careers/jobs/</a>.</p><p>If you want to learn more about the engineering work we do, check out <a href="https://www.singlestore.com/blog/category/engineering/" rel="nofollow">https://www.singlestore.com/blog/category/engineering/</a>.</p><p>Feel free to email me directly via david at singlestore dot com.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218873"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218873" href="https://news.ycombinator.com/vote?id=39218873&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Blitz.gg | AI Research Engineer - Custom Model Development for Game Modding Platform | Full-Time | Hybrid/Remote/Onsite | Los Angeles, CA USA | <a href="https://blitz.gg/" rel="nofollow">https://blitz.gg</a><p>Job Description:
We are seeking a highly skilled and innovative AI Research Engineer to join our cutting-edge project in developing an AI-driven game mods platform. Our vision is to revolutionize the gaming experience by enabling dynamic and customized game asset generation through advanced AI technologies.</p><p>We are a $USD 440M games stats &amp; coaching platform company looking to expand into the AI gaming industry by making modding tools accessible to all!</p><p>Key Responsibilities:
- Design and develop AI models capable of generating and modifying game assets based on user prompts.
- Work closely with game mod engineers to integrate AI models into existing game assets from Unity and Unreal Engine platforms.
- Continuously research and implement the latest advancements in AI and machine learning to enhance model capabilities.
- Collaborate with cross-functional teams to understand user requirements and deliver solutions that meet high standards of creativity and functionality.</p><p>Requirements:
- Proven experience in AI research, particularly in creating and training custom models.
- Strong background in machine learning, deep learning, and AI technologies.
- Experience with game development, particularly in asset creation and modification, is highly desirable.
- Proficiency in programming languages relevant to AI and machine learning (Python, TensorFlow, PyTorch, etc.).
- Excellent problem-solving skills and creativity in applying AI to real-world applications.
- Bachelor's or Master's degree in Computer Science, Artificial Intelligence, or related fields; Ph.D. preferred.</p><p>What We Offer:
- A highly competitive salary package in excess of $500k/year base, plus a very generous total compensation package in terms of stocks and benefits.
- Opportunity to be a part of a groundbreaking project in AI and gaming.
- Professional growth and learning opportunities in the field of AI-driven gaming technologies.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218593"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218593" href="https://news.ycombinator.com/vote?id=39218593&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Full Stack Software Engineer - Airtime, Remote<p>Do you love music almost as much as technology - or vice versa? Python, JavaScript, PHP, Postgres and Redis? Do you have 5+ years of engineering experience with a track record of building consumer web applications or services? Time zones: UTC -7 to UTC +2? Then read on: Airtime Pro is looking for you!</p><p>Airtime Pro is a cloud-based platform for broadcasting streaming radio on the web, allowing users to upload music, build playlists, schedule content and broadcast online. Airtime Pro powers hundreds of stations around the world, from small college stations to community radio titans.</p><p>More details about the role <a href="https://www.sourcefabric.org/about/jobs/full-stack-software-engineer-airtime-pro-g4xra6ws" rel="nofollow">https://www.sourcefabric.org/about/jobs/full-stack-software-...</a></p><p>Apply here <a href="https://goo.gl/forms/VMhKTUnQTqwdTmgP2" rel="nofollow">https://goo.gl/forms/VMhKTUnQTqwdTmgP2</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217351"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217351" href="https://news.ycombinator.com/vote?id=39217351&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Aha! (<a href="https://www.aha.io/" rel="nofollow">https://www.aha.io</a>) | Rails / React / Devops | REMOTE<p>Aha! is the #1 tool for product managers to plan strategy and roadmaps. We serve more than 700,000 users worldwide. We are looking for:</p><p>* Javascript and geometry expert? Help us build our HTML canvas based whiteboard &amp; mockup tool.</p><p>* Experienced full-stack, front end and platform engineers to work on the Aha! product. Our application is built in Ruby on Rails, with React on the frontend for rich client-side experiences.</p><p>* Devops engineers with Ruby experience. We focus on the "dev" and all of our operations driven by code.</p><p>Aha! is profitable, you can work from anywhere in North or South America, and we offer excellent benefits. We use our own product to manage our work (which is especially rewarding) and we deploy continuously.</p><p>Our entire team has always been 100% remote - in North American timezones so we can collaborate during the work day.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                      <tr id="39218813"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218813" href="https://news.ycombinator.com/vote?id=39218813&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Head of Engineering @ Stay22 | Montreal, Canada, Hybrid, Full-time | stay22.com<p>You'll be working with me (CTO) and leading a team of a dozen engineers to transform the way &lt;scripts&gt;, &lt;a href&gt; and &lt;iframes&gt; are done in the travel industry. We use a lot of AI (specifically traditional ML and LLMs) to predict, extract, and even do critical thinking for decision making when generating travel offers.</p><p>Apply here or directly to me: <a href="https://stay22.breezy.hr/p/b1215478578c-head-of-engineering" rel="nofollow">https://stay22.breezy.hr/p/b1215478578c-head-of-engineering</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39219015"><td></td></tr>
            <tr id="39219365"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219365" href="https://news.ycombinator.com/vote?id=39219365&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>bloop (YC S21) | Software Engineer (Rust, Compiler) | Hybrid London, UK<p>We're building automatic code translation software, combining source to source compilers and LLMs. If you're into tree-sitter, ANTLR, ASTs or compilers we should speak. Rust proficiency is also a must.</p><p>Please email join [at] bloop [dot] ai with "HN Rust Compiler Engineer" in the subject line.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217715"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217715" href="https://news.ycombinator.com/vote?id=39217715&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Narrative I/O | Senior Backend Engineer | New York, NY | Full-time | REMOTE | $120k to $200k USD | <a href="https://narrative.io/" rel="nofollow">https://narrative.io/</a><p>Narrative has been building a data collaboration platform designed for simplicity and ease of use since being founded in 2016.</p><p>Our primary strength is functioning as a data marketplace where we differentiate ourselves by automatically standardizing data across suppliers, making data available via powerful query interfaces (both intuitive UIs and an SQL dialect) instead of via one-size-fits-all pre-packaged datasets, and giving data providers the ability to define row-level access and pricing policies. The same building blocks that power the marketplace enable other data collaboration use cases, like clean room data analysis and the automatic segmentation and delivery of data to other platforms. We're also building towards a future where users can leverage our collaboration tools without needing to push their data to us, instead our query engine will be embeddable where their data is already at rest.</p><p>We are a small, remote-first team looking for great developers who want to jump in and take major systems and user-facing features from design to launch. While the company's headquarters are in NYC, the development team currently includes engineers working from the US (California and New York), Canada (Alberta, British Columbia, and Québec), and Poland.</p><p>In brief, the technologies we use are:</p><p>- Scala, Spark, Apache Iceberg, and Apache Calcite in the backend. We love functional programming and make use of libraries like Cats, Cats-Effect, and FS2, but we keep a pragmatic stance on purity given our exposure to the Spark ecosystem.</p><p>- Our services are deployed on AWS and provisioned using Terraform. We leverage the usual suspects: Fargate, EMR, RDS, Lambda, etc. and we're starting to lean into Kubernetes. Being a small team, we think twice before rolling out custom infrastructure that we'll need to monitor and maintain.</p><p>- On the frontend, we use Typescript, VueJS, PrimeVue, Nuxt, and Vite.</p><p>Job postings and more on information about our team and culture are available at: <a href="https://jobs.narrative.io/" rel="nofollow">https://jobs.narrative.io/</a></p><p>Apply by sending your resume to hiring-dev@narrative.io.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218532"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218532" href="https://news.ycombinator.com/vote?id=39218532&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Fulcrum | Senior Full Stack Engineer | MSP | $145k - 168k+ + benefits and equity | Full-time, ONSITE | <a href="https://fulcrumpro.com/" rel="nofollow">https://fulcrumpro.com</a> Our mission is clear and staggeringly important: we're building the manufacturing operating system of the future from the ground up, one that’s engineered to create network effects as we grow. A beautiful, sophisticated quantum leap forward for the most important core industry in the world.
The market tells us we’re on the right track and we're growing. Currently seventeen developers. (Many from HN!). Series A-2: <a href="https://fulcrumpro.com/ignite" rel="nofollow">https://fulcrumpro.com/ignite</a><p>We’re serious about doing great work and we empower people to make it happen. Our favorite tools include C#, Angular, Vue, and serverless TypeScript. We’re primarily interested in finding intensely smart people with talent at their fingertips, regardless of what tech stack you come from. We value autonomy over routine, and empathy over ego. We want to make products that are so elegant that our users feel constantly amazed. If you have at least 2 years of professional experience building all parts of modern webapps, and are interested in helping us develop the next generation of software in a massive market, we’d love to hear from you. The future is ours to build.
Answers to your FAQs: Immediate 401(k), with 5% match after 90 days through Vanguard, immediate vesting. health, dental, and vision benefits. Unlimited vacation (15 day mandatory minimums). Remote and flexible work. Typical process: culture screen - tech screen - tech interview - offer. To get in touch: careers@fulcrumpro.com or apply at <a href="https://fulcrumpro.com/team-careers" rel="nofollow">https://fulcrumpro.com/team-careers</a></p><p>*
Manifesto
Manufacturing is important. It’s the fundamental industry on which everything else is built. We take it for granted. In 1989, 35% of the entire software industry was manufacturing systems. Back then, services, hand holding, customization, and consulting were necessary to get the systems to where they are today. What was the first massive burst in productivity from technology has stagnated into incremental diminishing returns and exponentially increasing complexity. We can only continue to add complexity for so much longer until the whole system collapses.
"Any intelligent fool can make things bigger and more complex. It takes a touch of genius and a lot of courage to move in the opposite direction."
Manufacturing is important. We must deliver a touch of technological genius to reintroduce simplicity, efficiency, and integration.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217613"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217613" href="https://news.ycombinator.com/vote?id=39217613&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>OpenRent | London, UK | Full-Time | ONSITE+PART REMOTE | <a href="https://www.openrent.co.uk/" rel="nofollow">https://www.openrent.co.uk</a><p>What sucked the last time you rented a house or flat? Come and fix it.</p><p>OpenRent is a force for good in an industry tarnished by rip-off agencies. Enabled by an unrelenting focus on technology, we now let more properties than any agency in the UK. In the last 12m we let over £50 billion worth of property, to over 6 million registered users, without ever charging any admin fees.</p><p>You'll be working on solving every aspect of the rental journey, from machine learning models to predict the right price of a property, to building the future of property management, all to help tenants find their dream home, and landlords their ideal tenant.</p><p>We're VC backed, profitable, and have plenty of ambition to maintain our fast growth in this absolutely massive market.</p><p>Roles currently available:</p><p>- Senior Web Designer | Equity | 75k-100k+ (based on experience) + Quarterly Bonus</p><p>- Senior Full Stack Engineer (C#) | Equity | £75k-£120k+ (based on experience) + Quarterly Bonus</p><p>All roles visible here: <a href="https://www.openrent.co.uk/jobs" rel="nofollow">https://www.openrent.co.uk/jobs</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217521"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217521" href="https://news.ycombinator.com/vote?id=39217521&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Tenzir | Remote - EU | open-core | Full-time | <a href="https://tenzir.com/" rel="nofollow">https://tenzir.com</a><p>Tenzir is hiring several key engineering roles to meet the needs in expanding the team. Our product: security data pipelines. From the data side, think of it as an Arrow-native, multi-schema ETL tool that offers optional storage in Parquet/Feather. From the security perspective, think of it as a solution for collecting, parsing, transforming, aggregating, and routing data. We typically sit between the data sources (endpoint, network, cloud) and sinks (SIEM, data lake).</p><p>Our open-source execution engine is C++20, our platform is SvelteKit and TypeScript. Experience with data-first frontend apps is a great plus.</p><p>Apply at <a href="https://tenzir.jobs.personio.de/" rel="nofollow">https://tenzir.jobs.personio.de</a> for the following positions:</p><p>- Fullstack Engineer</p><p>- Senior Frontend Engineer</p><p>- UI/UX Designer</p><p>- SOC Solution Engineer</p><p>We are based out of Hamburg, Germany, and hire across EU time zones, stretching all the way to India.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217390"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217390" href="https://news.ycombinator.com/vote?id=39217390&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>onXmaps, Inc | Montana or REMOTE, USA only | <a href="https://www.onxmaps.com/careers" rel="nofollow">https://www.onxmaps.com/careers</a> ABOUT – Are you an Engineer who loves the outdoors? Join onX! onX is a suite of digital navigation apps (Hunt, Offroad, and Backcountry) that empower millions of outdoor enthusiasts. If you’re passionate about writing great software, love playing outside, believe in protecting access to public lands, and want to dominate the off-pavement mobile GPS market – then join our team, where we empower millions of outdoor enthusiasts to explore the unknown!
We have multiple openings! View them here: <a href="https://www.onxmaps.com/join-our-team" rel="nofollow">https://www.onxmaps.com/join-our-team</a><p>Here are some of the technologies we work with:</p><p>3D (OpenGL, Metal, C++)</p><p>Data Automation (Python, PostgreSQL, GIS)</p><p>Android (Kotlin)</p><p>iOS (Swift, SwiftUI)</p><p>Backend (Go, Elixir, GraphQL, GCP, Kubernetes)</p><p>Web (Vue JS)</p><p>Quality (Cypress, XCUITest, Espresso)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217396"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217396" href="https://news.ycombinator.com/vote?id=39217396&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Oddity.ai | Rust Developer | Full-time | On-site Utrecht, the Netherlands<p>At Oddity.ai, we are leading the way in shaping the future of safety through the detection of high impact crimes on live video footage. Our advanced algorithm seamlessly integrates with observation center technology, earning the trust of law enforcement agencies, correctional facilities, cities, and numerous other entities. Our mission is to decrease violence in the world.</p><p>We are looking for a Rust Developer who can design, develop, and maintain robust and high-performance Rust applications.</p><p>Read more: <a href="https://oddity.ai/nl/careers/rust-developer/" rel="nofollow">https://oddity.ai/nl/careers/rust-developer/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217797"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217797" href="https://news.ycombinator.com/vote?id=39217797&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><br><div>
                  <p><span>Koyeb | Software Designer + Technical CSM  | Remote - Europe | Full-time |<p>At Koyeb, we make developers’ lives easier with the fastest way to deploy applications globally. The Koyeb Serverless Platform is completely managed: we take code, build it into containers, and run it inside of MicroVMs distributed across multiple continents.</p><p>Software Designer:  We are looking for our first Software Designer to help us shape the features we build for our users. This starts from the big picture of how the features operate together as a system, to fine-tuned UI details, all with the goal to create a friction-free developer experience.</p><p>Technical Customer Success Manager:  We are looking for our first customer success manager to help our professional customers deploy on the platform and adopt advanced features. This job is focused on helping our users, who are developers, adopt the product and quickly deploy their production applications on the Koyeb platform.</p><p>You can read more: <a href="https://www.koyeb.com/careers?#974212" rel="nofollow">https://www.koyeb.com/careers?#974212</a></p><pre><code>  We are a team of 16 product-minded people who have built a community of over 50,000 developers worldwide. We are a fully-remote distributed team and we hire all-around Europe.</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39219500"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39219500" href="https://news.ycombinator.com/vote?id=39219500&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Nuna (<a href="https://www.nuna.com/" rel="nofollow">https://www.nuna.com</a>) | San Francisco &amp; Remote friendly (US only)| Full-time | Visa Transfer
In the US, we spend an average of over $12,500 per person each year on healthcare -- that’s almost twice what other developed countries spend. Healthcare in the US costs a staggering $4 trillion dollars per year, almost 1/5 of our Nation’s entire economy. Yet with all this resource, our healthcare outcomes are poorer than other countries, people still can’t afford their healthcare, and our healthcare providers are burnt out.<p>Moreover, our healthcare is systemically unequal. People of color, lower income, and LGBTQ+ have demonstrably worse healthcare outcomes, a disparity grimly highlighted by the pandemic we’re living through now where people of color are three-times more likely to die from COVID-19.</p><p>How can this be, and how can we change it?</p><p>Nuna is tackling one of the most hardest problems in healthcare underlying the negative outcomes and disparities we see: how healthcare gets paid.</p><p>Today, hospitals only get paid when they do more-- more visits, more tests, more meds, more surgeries. Hopefully this helps patients get better, but regardless, the system gets paid. In fact, doing more is the only way to stay afloat.</p><p>But -- is this really the right set of incentives? Shouldn’t everyone get rewarded not just by doing more, but by when patients actually get better? Shouldn’t everyone have access to affordable, high quality care, and shouldn’t hospitals be rewarded when they deliver this care? And shouldn’t insurance companies get rewarded when they help ALL their patients get better?</p><p>Absolutely, yes. This concept is called Value-Based Care. In fact, healthcare as a whole has been trying to move in this direction for years, but making it all reality is deeply complex -- it is after all our healthcare. Nuna’s technology platform, our software apps, our vision, and our exceptionally talented team are collectively accelerating the healthcare system’s ability to make value-based care available to everyone.</p><p>In 2022, Nuna will power over $70B of healthcare payments for over 6.5M patients. We also leverage our data science and platform to direct patients to the best, culturally-matched, and accessible care providers for them. Additionally, we make it transparent and easy for both hospitals and insurers to see how they are performing in value-based care by spotlighting the patients or areas where they need to pay extra attention so that they can provide good care to all their patients and get rewarded.</p><p>Nuna is unique - we have brought together an exceptional team of over 200 people. We are the industry’s best in healthcare data, analytics, engineering, clinicians, and value based healthcare experts. We have joined forces to create a more equitable health system for everyone.</p><p>Our dreams and ambitions to change healthcare as we know it are big. If yours are too, we want to work with you.</p><p>Open positions include:</p><p>* Staff and Senior Health Data Engineer</p><p>* Senior Software Engineer - Program Development Kit Engine</p><p>* Staff Software Engineer, Data Serving</p><p>* Staff Software Engineer, Production Applications</p><p>Jobs Board: <a href="https://bit.ly/nuna-job-board" rel="nofollow">https://bit.ly/nuna-job-board</a></p><p>Frontend: React, Typescript</p><p>Backend: Django, Python, Kotlin, Scala</p><p>Cloud: AWS</p><p>Questions? Email: recruiting+hn@nuna.com
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217491"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217491" href="https://news.ycombinator.com/vote?id=39217491&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Powertools Technologies | Junior/Senior Engineer | Lisbon, Portugal | Full-time | ONSITE<p>&gt; Looking for a senior engineer for work on software related to Electronic Design Automation. Candidate should have some experience with EDA software (Cadence Virtuoso, Siemens Calibre, Synopsys Design Compiler, etc), ideally including plugin development. Experienced Software Developers are more than welcome to apply.</p><p>&gt; Looking for a junior engineer for work on software related to Electronic Design Automation and/or Software Development. Candidate should at least have (or graduate shortly) a 3 year university degree in engineering. Most suitably Electronic/Computer Engineering or Informatics. Software Developers are more than welcome to apply.</p><p>Site: <a href="https://www.powertools-tech.com/" rel="nofollow">https://www.powertools-tech.com</a> . Growing a small experienced team with international industrial and academic track, willing to train new hire in fairly uncommon skill set. Candidate should be capable of quality detail work, and have good communication abilities, to provide support to international design teams in fabless semiconductor companies.</p><p>Email your interest and CV to hr@powertools-tech.com, please.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39217326"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217326" href="https://news.ycombinator.com/vote?id=39217326&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Roboflow | Open Source Software Engineer, Web Designer / Developer, and more. | Full-time (Remote, SF, NYC) | <a href="https://roboflow.com/careers?ref=whoishiring0224">https://roboflow.com/careers?ref=whoishiring0224</a><p>Roboflow is the fastest way to use computer vision in production. We help developers give their software the sense of sight. Our end-to-end platform[1] provides tooling for image collection, annotation, dataset exploration and curation, training, and deployment.</p><p>Over 250k engineers (including engineers from 2/3 Fortune 100 companies) build with Roboflow. We now host the largest collection of open source computer vision datasets and pre-trained models[2]. We are pushing forward the CV ecosystem with open source projects like Autodistill[3] and Supervision[4]. And we've built one of the most comprehensive resources for software engineers to learn to use computer vision with our popular blog[5] and YouTube channel[6].</p><p>We have several openings available but are primarily looking for strong technical generalists who want to help us democratize computer vision and like to wear many hats and have an outsized impact. Our engineering culture is built on a foundation of autonomy &amp; we don't consider an engineer fully ramped until they can "choose their own loss function". At Roboflow, engineers aren't just responsible for building things but also for helping us figure out what we should build next. We're builders &amp; problem solvers; not just coders. (For this reason we also especially love hiring past and future founders.)</p><p>We're currently hiring full-stack engineers for our ML and web platform teams, a web developer to bridge our product and marketing teams, several technical roles on the sales &amp; field engineering teams, and our first applied machine learning researcher to help push forward the state of the art in computer vision.</p><p>[1]: <a href="https://roboflow.com/?ref=whoishiring0224">https://roboflow.com/?ref=whoishiring0224</a></p><p>[2]: <a href="https://roboflow.com/universe?ref=whoishiring0224">https://roboflow.com/universe?ref=whoishiring0224</a></p><p>[3]: <a href="https://github.com/autodistill/autodistill">https://github.com/autodistill/autodistill</a></p><p>[4]: <a href="https://github.com/roboflow/supervision">https://github.com/roboflow/supervision</a></p><p>[5]: <a href="https://blog.roboflow.com/?ref=whoishiring0224">https://blog.roboflow.com/?ref=whoishiring0224</a></p><p>[6]: <a href="https://www.youtube.com/@Roboflow" rel="nofollow">https://www.youtube.com/@Roboflow</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39217632"><td></td></tr>
                  <tr id="39217352"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39217352" href="https://news.ycombinator.com/vote?id=39217352&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>Temporal Technologies | Multiple positions in United States - WORK FROM HOME | FULL-TIME |<p>Temporal offers an entirely new way to build scalable and reliable applications. Temporal enables developers to focus on writing important business logic, and not on managing state or worrying about the underlying infrastructure. Sequoia Capital led our last round of funding and our team has experience from start-ups and larger companies like Microsoft, Google, Amazon, Uber, and more.</p><p>Temporal Investors Expand Funding: <a href="https://temporal.io/news/temporal-investors-expand-funding-with-usd75m-round" rel="nofollow">https://temporal.io/news/temporal-investors-expand-funding-w...</a></p><p>Temporal in 7 minutes: <a href="https://temporal.io/tldr" rel="nofollow">https://temporal.io/tldr</a></p><p>We're looking for senior level engineers for multiple roles - see here - <a href="https://www.temporal.io/careers" rel="nofollow">https://www.temporal.io/careers</a></p><p>FEATURED ROLES:</p><p>Senior Developer Success Engineer → <a href="https://grnh.se/40ad44c87us" rel="nofollow">https://grnh.se/40ad44c87us</a></p><p>Senior Staff Software Design Engineer (IAM, AUTH) → <a href="https://grnh.se/ab5b9bb47us" rel="nofollow">https://grnh.se/ab5b9bb47us</a></p><p>Senior Staff Distributed Systems Software Engineer → <a href="https://grnh.se/05f8fdd17us" rel="nofollow">https://grnh.se/05f8fdd17us</a></p><p>Senior Software Design Engineer → <a href="https://grnh.se/784c72127us" rel="nofollow">https://grnh.se/784c72127us</a></p><p>Staff Software Design Engineer → <a href="https://grnh.se/942b12a77us" rel="nofollow">https://grnh.se/942b12a77us</a></p><p>US benefits include: Flexible PTO, 12 Holidays + 2 Floating Holidays, 100% Premiums Coverage for Medical, Dental, and Vision, AD&amp;D, LT &amp; ST Disability and Life Insurance , Empower 401K Plan, Additional Perks for Learning &amp; Development, Lifestyle Spending, In-Home Office Setup, Professional Memberships, WFH Meals, Internet Stipend and more! Benefits outside the United States vary by country.</p><p>Apply here <a href="https://www.temporal.io/careers/" rel="nofollow">https://www.temporal.io/careers/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39218708"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39218708" href="https://news.ycombinator.com/vote?id=39218708&amp;how=up&amp;goto=item%3Fid%3D39217310"></a></center>    </td><td><p><span>BRINC Drones | <a href="https://brincdrones.com/" rel="nofollow">https://brincdrones.com/</a> | Onsite in Seattle | Full-time | $150k-$225k + equity<p>BRINC Drones helps first responders with products that quickly and effectively respond to emergencies. Our technology lets first responders get eyes and ears on what's happening in indoor structures when it's too dangerous for people to enter - whether it be a fire, medical emergency, violence, or search and rescue. In March 2023, we released the Lemur 2, our latest and most advanced drone, and we started shipping it to customers the same year.</p><p>In addition to launching more advanced drones, we're also expanding our software ecosystem into realtime monitoring and fleet management, and we recently shipped the first version of our first web app: LiveOps.  This app gives drone operators and first response teams access to real-time live-streaming drone data such as visual and thermal video feeds, 3D map data via point clouds, and a myriad of drone telemetry – it’s essentially a web-based drone fleet management software. But our vision isn’t just fleet management; LiveOps represents a critical part of BRINC’s transition from a company that just sells drones to a company that sells entire drone programs: we want to build out the entire drone ecosystem (both hardware and software) for our customers.</p><p>It’s an ambitious goal, but it’s one that we feel confident about – we’ve got a lot of interest from customers in what we’ve built so far, and we are seeking both a full-stack and a backend/platform engineer to join our engineering team and own the development of this web app and cloud ecosystem.</p><p>Job Postings:</p><p>- Senior/Principal Full-stack Engineer: <a href="https://app.dover.io/apply/brinc/a567bc8a-9d90-4961-9e30-e5f82f3d388d?rs=42706078" rel="nofollow">https://app.dover.io/apply/brinc/a567bc8a-9d90-4961-9e30-e5f...</a></p><p>- Senior/Principal Backend &amp; Platform Engineer: <a href="https://app.dover.io/apply/brinc/5aa2526c-472c-49b1-a1cf-8c5c5d97e904?rs=42706078" rel="nofollow">https://app.dover.io/apply/brinc/5aa2526c-472c-49b1-a1cf-8c5...</a></p><p>Some recent press:</p><p>- <a href="https://www.geekwire.com/2023/inside-drone-startup-brinc-with-the-23-year-old-founder-who-landed-openais-ceo-as-an-investor/" rel="nofollow">https://www.geekwire.com/2023/inside-drone-startup-brinc-wit...</a></p><p>- <a href="https://techcrunch.com/2023/03/02/brinc-lemur-2/" rel="nofollow">https://techcrunch.com/2023/03/02/brinc-lemur-2/</a></p><p>I'm the founding engineer for the cloud services engineering team at BRINC, so feel free to relay any questions about the role to me (dylan at brinc drones dot com, parse it yourself), but please apply through the job posting if you're interested in the role so that we can keep track of you!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My favourite Git commit (2019) (271 pts)]]></title>
            <link>https://dhwthompson.com/2019/my-favourite-git-commit</link>
            <guid>39217149</guid>
            <pubDate>Thu, 01 Feb 2024 15:46:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dhwthompson.com/2019/my-favourite-git-commit">https://dhwthompson.com/2019/my-favourite-git-commit</a>, See on <a href="https://news.ycombinator.com/item?id=39217149">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>I like Git commit messages.
Used well, I think they’re one of the most powerful tools available to document a codebase over its lifetime.
I’d like to illustrate that by showing you my favourite ever Git commit.</p>

<p>This commit is from my time at the Government Digital Service, working on GOV.UK.
It’s from a developer by the name of <a href="https://twitter.com/dancarley">Dan Carley</a>, and it has the rather unassuming name of “<a href="https://github.com/alphagov/govuk-puppet/commit/63b36f93bf75a848e2125008aa1e880c5861cf46">Convert template to US-ASCII to fix error</a>”.</p>

<p><img src="https://dhwthompson.com/assets/the-carley-commit.png" alt=""></p>

<p>A quick aside: one of <a href="https://gds.blog.gov.uk/2017/09/04/the-benefits-of-coding-in-the-open/">the benefits of coding in the open</a>, as practised at GDS, is that it’s possible to share examples like this outside the organisation that produced them.
I’m not sure who first introduced that idea to GDS –&nbsp;it was well-established by the time I joined –&nbsp;but I’m forever grateful to them.</p>

<h2 id="why-i-like-this-commit">Why I like this commit</h2>
<p>I’ve lost count of the number of times I’ve shared this as an example of what commit messages can do.
It’s fun because of the ratio of commit message to code change, but that’s not why I think it’s worth sharing.</p>

<p>In a different organisation, from a different developer, this entire commit message might have been <code>change whitespace</code>, or <code>fix bug</code>, or (depending on the team’s culture) some less than flattering opinions about the inventor of the non-breaking space.
Instead, Dan took the time to craft a really useful commit message for the benefit of those around him.
I’d like to step through a few of the ways I think this is a really good example.</p>

<h3 id="it-explains-the-reason-for-the-change">It explains the reason for the change</h3>
<p>The best commit messages I’ve seen don’t just explain <strong>what</strong> they’ve changed: they explain <strong>why</strong>.
In this instance:</p>

<div><pre><code>I introduced some tests in a feature branch to match the contents of
`/etc/nginx/router_routes.conf`. They worked fine when run with `bundle exec
rake spec` or `bundle exec rspec modules/router/spec`. But when run as
`bundle exec rake` each should block failed with:

    ArgumentError:
      invalid byte sequence in US-ASCII
</code></pre></div>

<p>Without this level of detail, we could hazard a guess that this commit fixed some kind of parsing error in some tool or other.
Thanks to the commit message, we know exactly which tool it was.</p>

<p>This kind of information can be really valuable to document, and is all too easy to lose as people forget the original context behind their work, move on to other teams, and eventually leave the organisation.</p>

<h3 id="its-searchable">It’s searchable</h3>
<p>One of the first things in this commit message is the error message that inspired the change:</p>

<div><pre><code>ArgumentError:
  invalid byte sequence in US-ASCII
</code></pre></div>

<p>Anyone else who comes across this error can search through the codebase, either
by running <code>git log --grep "invalid byte sequence"</code> or by <a href="https://github.com/alphagov/govuk-puppet/search?q=%22invalid+byte+sequence%22&amp;type=Commits">using GitHub’s commit search</a>.
In fact, from the looks of the search results, multiple people did so, and found out who had found this problem before, when they came across it, and what they did about it.</p>

<h3 id="it-tells-a-story">It tells a story</h3>
<p>This commit message goes into a lot of detail about what the problem looked like, what the process of investigating it looked like, and what the process of fixing it looked like.
For example:</p>

<div><pre><code>I eventually found that removing the `.with_content(//)` matchers made the
errors go away. That there weren't any weird characters in the spec file. And
that it could be reproduced by requiring Puppet in the same interpreter
</code></pre></div>

<p>This is one of the areas commit messages can really shine, because they’re documenting the change itself, rather than documenting a particular file, or function, or line of code.
This makes them a great place to document this kind of extra information about the journey the codebase has taken.</p>

<h3 id="it-makes-everyone-a-little-smarter">It makes everyone a little smarter</h3>
<p>One thing Dan did here that I really appreciate was to document the commands he ran at each stage.
This can be a great lightweight way to spread knowledge around a team.
By reading this commit message, someone can learn quite a few useful tips about the Unix toolset:</p>

<ul>
  <li>they can pass an <code>-exec</code> argument into <code>find</code> to run a command against each file found</li>
  <li>that adding a <code>\+</code> onto the end of this command does something interesting (it passes many filenames into a single <code>file</code> command, rather than running the command once per file)</li>
  <li><code>file --mime</code> can tell them the MIME type of a file</li>
  <li><code>iconv</code> exists</li>
</ul>

<p>The person who reviews this change can learn these things.
Anyone who finds this commit later can learn these things.
Over enough time and enough commits, this can become a really powerful multiplier for a team.</p>

<h3 id="it-builds-compassion-and-trust">It builds compassion and trust</h3>

<div><pre><code>Now the tests work! One hour of my life I won't get back..
</code></pre></div>

<p>This last paragraph adds an extra bit of human context.
Reading these words, it’s hard not to feel just a little bit of Dan’s frustration at having to spend an hour tracking down a sneaky bug, and satisfaction at fixing it.</p>

<p>Now imagine a similar message attached to a short-term hack, or a piece of prototype code that made its way into production and set down roots (as pieces of prototype code love to do).
A commit message like this makes it much easier to remember that every change has a human on the other end of it, making the best decision they could given the information they had at the time.</p>

<h2 id="good-commits-matter">Good commits matter</h2>

<p>I’ll admit this is an extreme example, and I wouldn’t expect all commits (especially ones of this size) to have this level of detail.
Still, I think it’s an excellent example of explaining the context behind a change, of helping others to learn, and of contributing to the team’s collective mental model of the codebase.</p>

<p>If you’re interested in learning a bit more about the benefits of good commit messages, and some of the tools that make it easier to structure your changes around them, I can recommend:</p>

<ul>
  <li><a href="https://blog.mocoso.co.uk/talks/2015/01/12/telling-stories-through-your-commits/">Telling stories through your commits</a> by Joel Chippindale</li>
  <li><a href="https://tekin.co.uk/2019/02/a-talk-about-revision-histories">A branch in time</a> by Tekin Süleyman</li>
</ul>


  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bard's latest updates: Access Gemini Pro globally and generate images (152 pts)]]></title>
            <link>https://blog.google/products/bard/google-bard-gemini-pro-image-generation/</link>
            <guid>39217046</guid>
            <pubDate>Thu, 01 Feb 2024 15:36:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/products/bard/google-bard-gemini-pro-image-generation/">https://blog.google/products/bard/google-bard-gemini-pro-image-generation/</a>, See on <a href="https://news.ycombinator.com/item?id=39217046">Hacker News</a></p>
<div id="readability-page-1" class="page"><article ng-init="drawerToggle = {'open': true}">

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Bard’s latest updates: Access Gemini Pro globally and generate images&quot;
  }">
      
      
        <p>
          Starting today, you can generate images in Bard in most countries, and use Gemini Pro in any language, country and territory Bard currently supports.
        </p>
      
    </div>

    

    
      







<div>
    <figure>
      <div>
  <p><img srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Hero_10_1.width-600.format-webp.webp 600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Hero_10_1.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Hero_10_1.width-1600.format-webp.webp 1600w" sizes="(max-width: 599px) 100vw, (max-width: 1023px) 600px, 1024px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Hero_10_1.width-1200.format-webp.webp" fetchpriority="high" alt="A graphic that reads &quot;Gemini Pro in Bard&quot;">
  </p>
</div>

      
    </figure>
  </div>


    

    
    <div data-reading-time="true" data-component="uni-drop-cap|uni-tombstone">

            

            
            
<!--article text-->

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Bard’s latest updates: Access Gemini Pro globally and generate images&quot;
         }"><p data-block-key="8t52c">Today, we’re bringing Bard’s latest capabilities — including Gemini Pro in Bard — to more languages and places. Plus, we’re introducing image generation to help more of your ideas come to life.</p><h2 data-block-key="d3rc2">Use Gemini Pro in all supported languages and places</h2><p data-block-key="1i5at">Last December, we brought <a href="https://blog.google/products/bard/google-bard-try-gemini-ai/" rt-link-type="external">Gemini Pro into Bard</a> in English, giving Bard more advanced understanding, reasoning, summarizing and coding abilities. Today Gemini Pro in Bard will be available in <a href="https://support.google.com/bard/answer/13575153?hl=en" rt-link-type="external">over 40 languages and more than 230 countries and territories</a>, so more people can collaborate with this faster, more capable version of Bard.</p><p data-block-key="2hj37">The Large Model Systems Organization, a leading evaluator of language models and chatbots across languages, <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard" rt-link-type="external">recently shared</a> that Bard with Gemini Pro is one of the most preferred chatbots available (with or without cost), <a href="https://twitter.com/lmsysorg/status/1750921228012122526?s=20" rt-link-type="external">noting</a> that it has made a “stunning leap” forward. And blind evaluations with our third-party raters identified Bard with Gemini Pro as one of the top-performing conversational AIs, compared to leading free and paid alternatives.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;Bard’s latest updates: Access Gemini Pro globally and generate images&quot;
        }">
  

  <p>

      
      
        
          <video tabindex="0" autoplay="" loop="" muted="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Bard_with_Gemini_Pro_v02_1.mp4" type="video/mp4" title="A GIF reads “Bard with Gemini Pro now in more languages and places. With Gemini Pro, Bard is more capable at understanding, summarizing, reasoning, planning. Try it today at bard.google.com.”" alt="Bard with Gemini Pro GIF">
            Video format not supported
          </video>
        
      
    
    </p>
    
  
    </div>
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Bard’s latest updates: Access Gemini Pro globally and generate images&quot;
         }"><h2 data-block-key="8t52c">Double-check your responses in more languages</h2><p data-block-key="abcns">Since we know people want the ability to corroborate Bard’s responses, we’re also expanding our <a href="https://blog.google/products/bard/google-bard-new-features-update-sept-2023/" rt-link-type="external">double-check feature</a>, which is already used by millions of people in English, to more than 40 languages. When you click on the “G” icon, Bard will evaluate whether there is content across the web to substantiate its response. If it can be evaluated, you can click the highlighted phrases and learn more about supporting or contradicting information found by Search.</p><h2 data-block-key="ffebd">Bring your ideas to life with image generation</h2><p data-block-key="bfke3">For an extra creative boost, you can now generate images in Bard in <a href="https://support-content-draft.corp.google.com/bard?p=b_gen_img" rt-link-type="external">English</a> in most countries around the world, at no cost. This new capability is powered by our updated <a href="https://blog.google/technology/ai/google-imagen-2" rt-link-type="external">Imagen 2 model</a>, which is designed to balance quality and speed, delivering high-quality, photorealistic outputs. Just type in a description — like “create an image of a dog riding a surfboard” — and Bard will generate custom, wide-ranging visuals to help bring your idea to life.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;Bard’s latest updates: Access Gemini Pro globally and generate images&quot;
        }">
  

  <p>

      
      
        
          <video tabindex="0" autoplay="" loop="" muted="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/BARD_Imagen_Desktop_Dog_Demo_1080x1080_v06.mp4" type="video/mp4" title="GIF of a prompt that says &quot;create an image of a dog riding a surfboard.&quot; Image results appear." alt="Bard Imagen">
            Video format not supported
          </video>
        
      
    
    </p>
    
  
    </div>
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Bard’s latest updates: Access Gemini Pro globally and generate images&quot;
         }">
        <p data-block-key="8t52c">Consistent with our <a href="https://ai.google/responsibility/principles/" rt-link-type="external">AI Principles</a>, image generation was designed with responsibility in mind. For instance, to ensure there’s a clear distinction between visuals created with Bard and original human artwork, Bard uses <a href="https://deepmind.google/technologies/synthid/" rt-link-type="external">SynthID</a> to embed digitally identifiable watermarks into the pixels of generated images.</p>
      </div>
  

  
    




  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Bard’s latest updates: Access Gemini Pro globally and generate images&quot;
         }"><p data-block-key="8t52c">Our technical guardrails and investments in the safety of training data seek to limit violent, offensive or sexually explicit content. Additionally, we apply filters designed to avoid the generation of images of named people. We’ll continue investing in new techniques to improve the safety and privacy protections of our models.</p><p data-block-key="fknvb">These updates make Bard an even more helpful and globally accessible AI collaborator for everything from big, creative projects to smaller, everyday tasks. Try it out today at <a href="http://bard.google.com/" rt-link-type="external">bard.google.com</a>.</p></div>
  


            
            

            
              




            
          </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Damn Small Linux 2024 (258 pts)]]></title>
            <link>https://www.damnsmalllinux.org/</link>
            <guid>39215846</guid>
            <pubDate>Thu, 01 Feb 2024 13:47:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.damnsmalllinux.org/">https://www.damnsmalllinux.org/</a>, See on <a href="https://news.ycombinator.com/item?id=39215846">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <tbody><tr>
      <td>
	<header>
        	<img id="logo" src="https://www.damnsmalllinux.org/img/logo.png" alt="Damn Small Linux">
        	
	</header>      
	 <br>

<nav>
    <a href="https://www.damnsmalllinux.org/">Home</a>
    <a href="https://www.damnsmalllinux.org/2024-download.html">Download DSL 2024</a>
    <a href="https://www.damnsmalllinux.org/forums">DSL Forums</a>
    <a href="https://www.damnsmalllinux.org/support.html">Support DSL</a>
    <a href="https://www.damnsmalllinux.org/old-index.html">Historic DSL</a>
</nav>

        <p>
          The New DSL 2024 has been reborn as a compact Linux distribution tailored for low-spec x86 computers. It packs a lot of applications into a small package. All the applications are chosen for their functionality, small size, and low dependencies. DSL 2024 also has many text-based applications that make it handy to use in a term window or TTY.
        </p>
	



        <p>
          DSL 2024 currently only ships with two window managers: Fluxbox and JWM. Both are lightweight, fairly intuitive, and easy to use.
        </p>
        <p>
          DSL has three X-based web browsers:
          </p><ul>
	    <li><strong>BadWolf</strong> (light, security-oriented, fully HTML5 compatible)</li>
            <li><strong>Dillo</strong> (super-light GUI browser)</li>
            <li><strong>Links2</strong> (text and light GUI browser)</li>
          </ul>
        
        <p>
          For office applications, DSL has:
          </p><ul>
            <li><strong>AbiWord</strong> word processor</li>
            <li><strong>Gnumeric</strong> spreadsheets</li>
            <li><strong>Sylpheed</strong> email client</li>
            <li><strong>Zathura</strong> PDF viewer</li>
          </ul>
        
        <p>
          For multimedia applications:
          </p><ul>
            <li><strong>MPV</strong> (video and audio)</li>
            <li><strong>XMMS</strong> (a lightweight audio player)</li>
          </ul>
        
        <p>
           Other applications:
         </p><ul> 
          <li><strong>mtPaint</strong> (graphics editing)</li>
          <li><strong>gFTP</strong> (FTP, SFTP, and SCP)</li>
          <li><strong>Leafpad</strong> (quick editing)</li>
          <li><strong>zzzFM</strong> (file manager lifted straight out of antiX)</li>
         </ul>
        
        <p>
          There are three GUI-based games picked because they are fun and relatively light.
        </p>
        <p>
          DSL 2024 is also loaded up with a whole bunch of handy term-based applications:
          </p><ul>
            <li><strong>Ranger</strong> file manager</li>
            <li><strong>VisiData</strong> a powerful CSV and spreadsheet tool</li>
            <li><strong>FZF</strong> fuzzy finder</li>
            <li><strong>Tmux</strong> terminal multiplexer</li>
            <li><strong>Mutt</strong> email client</li>
            <li><strong>Cmus</strong> music player</li>
            <li><strong>CDW</strong> CD burner</li>
            <li><strong>Htop</strong>, an interactive process viewer</li>
            <li><strong>SurfRaw</strong> (with wrapper) to search from the term</li>
            <li><strong>Weather App</strong></li>
            <li><strong>MPV</strong> video/audio player with wrapper</li>
            <li><strong>Vim</strong> and <strong>Nano</strong> for editing</li>
            <li>Five term-based games</li>
            <li>Two term-compatible web browsers: <strong>W3M</strong> and <strong>Links2</strong></li>
            <li>...and much more</li>
          </ul>
        
        <p>
          <strong>Why make a new DSL after all these years?</strong>
        </p>
        <p>
          Creating the original DSL, a versatile 50MB distribution, was a lot of fun and one of the things I am most proud of as a personal accomplishment. However, as a concept, it was in the right place at the right time, and the computer industry has changed a lot since then. While it would be possible to make a bootable Xwindows 50MB distribution today, it would be missing many drivers and have only a handful of very rudimentary applications. People would find such a distribution a fun toy or something to build upon, but it would not be usable for the average computer user out of the gate.
        </p>
        <p>
          Meanwhile, in 2024, nearly everyone has abandoned the sub-700MB size limit to run on computers old enough to not have a DVD and cannot boot off of a USB drive. This is completely understandable because applications, the kernel, and drivers have all mushroomed in their space requirements. Hats off to Puppy Linux for staying one of the few that still offer a full desktop environment in a small size.
        </p>
        <p>
          The new goal of DSL is to pack as much usable desktop distribution into an image small enough to fit on a single CD, or a hard limit of 700MB. This project is meant to service older computers and have them continue to be useful far into the future. Such a notion sits well with my values. I think of this project as my way of keeping otherwise usable hardware out of landfills.
        </p>
        <p>
          As with most things in the GNU/Linux community, this project continues to stand on the shoulders of giants. I am just one guy without a CS degree, so for now, this project is based on antiX 23 i386. AntiX is a fantastic distribution that I think shares much of the same spirit as the original DSL project. AntiX shares pedigree with MEPIS and also leans heavily on the geniuses at Debian. So, this project stands on the shoulders of giants. In other words, DSL 2024 is a humble little project!
        </p>
        <p>
          Though it may seem comparably ridiculous that 700MB is small in 2024 when DSL was 50MB in 2002, I’ve done a lot of hunting to find small footprint applications, and I had to do some tricks to get a workable desktop into the 700MB limit. To get the size down, the ISO only has en_US, en_BG, en_CA, and en_AU locales enabled, and I had to strip the source codes, many man pages, and documentation out. I do provide a download script that will restore all the missing files, and so far, it seems to be working well.
        </p>
        <p>
          Unlike the original DSL, this version has apt fully enabled. So if there is anything you feel is missing, it is very simple to get it installed. I also made an effort to leave as much of the antiX goodness enabled as possible. However, it must be said that DSL is a derivative work but also a reductive work. Some things from antiX may be broken or missing. If you find a bug, it is likely my fault.
        </p>
	<h2>Thank you section:</h2>

	<p>Thank you Debian and antiX for doing all the heavy lifting.</p>

	<p>Thank you <a href="http://www.ibiblio.org/"><img src="http://www.ibiblio.org/hosted/images/sm_hosted_trans.gif" width="80" height="24" alt="hosted by ibiblio"></a> for mirroring the DSL downloads all these years.</p>

	<p>Thank you GPedde at DeviantArt for the beautiful <a href="https://www.deviantart.com/gpedde/art/Maintenance-165229479">wallpaper</a>.

	</p><p>Finally, thank you to the users of DSL for your <a href="https://www.damnsmalllinux.org/forums">feedback</a> and <a href="https://www.damnsmalllinux.org/support.html">support</a>.</p>
      
      </td>
    </tr>
  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FreeBSD 4 Bug may be present in Playstation 4/5 (113 pts)]]></title>
            <link>https://wololo.net/2024/02/01/is-an-18-year-old-vulnerability-the-key-to-the-next-ps5-ps4-jailbreak/</link>
            <guid>39215309</guid>
            <pubDate>Thu, 01 Feb 2024 12:36:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wololo.net/2024/02/01/is-an-18-year-old-vulnerability-the-key-to-the-next-ps5-ps4-jailbreak/">https://wololo.net/2024/02/01/is-an-18-year-old-vulnerability-the-key-to-the-next-ps5-ps4-jailbreak/</a>, See on <a href="https://news.ycombinator.com/item?id=39215309">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>	
				<div>

<!--nok--><p><img decoding="async" fetchpriority="high" src="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/ps5_crash_test.jpg?resize=768%2C1024&amp;ssl=1" alt="" width="768" height="1024" srcset="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/ps5_crash_test.jpg?resize=768%2C1024&amp;ssl=1 768w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/ps5_crash_test.jpg?resize=225%2C300&amp;ssl=1 225w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/ps5_crash_test.jpg?w=1024&amp;ssl=1 1024w" sizes="(max-width: 768px) 100vw, 768px" data-recalc-dims="1"></p>
<p>What initially looked like a troll might turn out to be an actual PS4/PS5 vulnerability. While people shouldn’t get their hopes up just right now as this might fizzle, it’s more interesting than it initially looked on the surface.</p><!-- ezoic_video_placeholder-under_first_paragraph-854x480-999998-clearholder --><!-- ezoic_video_placeholder-under_first_paragraph-854x480-999998-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-under_first_paragraph-640x360-999998-clearholder --><!-- ezoic_video_placeholder-under_first_paragraph-640x360-999998-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-under_first_paragraph-426x240-999998-clearholder --><!-- ezoic_video_placeholder-under_first_paragraph-426x240-999998-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-under_first_paragraph-384x216-999998-clearholder --><!-- ezoic_video_placeholder-under_first_paragraph-384x216-999998-nonexxxclearxxxblock -->
<p>Discussions about an 18 year old CVE have been making the rounds of PS4/PS5 hacking discords, forums, and Twitter over the past few days. Some people have been claiming that a Network bug disclosed in 2006 could&nbsp; be leveraged to Jailbreak recent PS4 and PS5 firmwares, and that this might even be the vulnerability (or one of the vulnerabilities in a chain) used by <a href="https://wololo.net/tag">TheFloW</a> in <a href="https://wololo.net/2023/09/23/spoiler-alert-dont-update-theflow-sets-the-ps4-ps5-scene-on-fire-once-again/">one of his most recent PlayStation bug bounties</a>.</p>
<p>Early tests seem to <strong>indicate that a crash is indeed present in PS4 up to 11.00 included, and PS5 8.20 included</strong>. (Which would put the patch for this issue at <a href="https://wololo.net/2023/12/06/ps5-firmware-8-40-and-ps4-11-02-are-out-and-theres-something-interesting-in-there-dont-update/">firmwares PS5 8.40 and PS4 11.02</a>)</p>
<p>I’ll be the first to admit I initially dismissed the original message as a troll, but <strong>it seems the claim has legs</strong>: a PoC (proof of concept) file has been circulating, and multiple people confirm it crashes their PS4 and PS5, up to the firmwares mentioned above. Now, <strong>a crash in itself is certainly not a vulnerability (let alone an exploitable one)</strong>, but this means the claim is now leaving the “troll” territory and becomes worth looking into. And there’s more…</p>
<h2>CVE From 2006 and PS4/PS5 crash PoC</h2>
<p>Before people become overly excited, I’d like to say that <strong>this should still be considered as a rumor, or, at the very least, something that could fizzle, until more skilled devs actually dig into the issue</strong>. However, here’s what happened so far:</p><!-- ezoic_video_placeholder-under_second_paragraph-854x480-999997-clearholder --><!-- ezoic_video_placeholder-under_second_paragraph-854x480-999997-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-under_second_paragraph-640x360-999997-clearholder --><!-- ezoic_video_placeholder-under_second_paragraph-640x360-999997-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-under_second_paragraph-426x240-999997-clearholder --><!-- ezoic_video_placeholder-under_second_paragraph-426x240-999997-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-under_second_paragraph-384x216-999997-clearholder --><!-- ezoic_video_placeholder-under_second_paragraph-384x216-999997-nonexxxclearxxxblock -->
<ul>
<li>An old CVE From 2006 (<a href="https://www.cvedetails.com/cve/CVE-2006-4304/">CVE-2006-4304</a>) was <a href="https://wololo.net/2024/01/26/ps5-update-8-60-is-here-and-guess-what-dont-update/comment-page-1/#comment-6543161">initially reported</a> by some anonymous user to be a “PS4/PS5 KEX” a few days ago. Multiple people, including myself, initially dismissed the claim: “There’s no way a 18-year old FreeBSD 4 bug is present on the PS4 or the PS5, which are based on FreeBSD 9 or above”.</li>
</ul>
<p><img decoding="async" src="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/initial_claims.png?resize=603%2C339&amp;ssl=1" alt="" width="603" height="339" srcset="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/initial_claims.png?w=603&amp;ssl=1 603w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/initial_claims.png?resize=300%2C169&amp;ssl=1 300w" sizes="(max-width: 603px) 100vw, 603px" data-recalc-dims="1"></p>
<ul>
<li>However, several folks have tested the <a href="https://gist.github.com/RobbedColek/8b0bc9655adc6ad9564ad71453188e37">poc that’s been circulating</a> (Please note that @Colek who provided this PoC on their github is not the person who initially made the claims) and have confirmed that it crashes their PS4/PS5 consoles, up to some specific firmwares (apparently up to PS4 11.00 and PS5 8.20 included).</li>
</ul>
<p><img decoding="async" src="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/packet_ps4_ps5_crash.jpg?resize=1024%2C768&amp;ssl=1" alt="" width="1024" height="768" srcset="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/packet_ps4_ps5_crash-scaled.jpg?resize=1024%2C768&amp;ssl=1 1024w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/packet_ps4_ps5_crash-scaled.jpg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/packet_ps4_ps5_crash-scaled.jpg?resize=768%2C576&amp;ssl=1 768w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/packet_ps4_ps5_crash-scaled.jpg?resize=1536%2C1152&amp;ssl=1 1536w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/packet_ps4_ps5_crash-scaled.jpg?resize=2048%2C1536&amp;ssl=1 2048w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></p>

<div id="attachment_48289"><p><img aria-describedby="caption-attachment-48289" decoding="async" loading="lazy" src="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/ps5_crash_test.jpg?resize=768%2C1024&amp;ssl=1" alt="" width="768" height="1024" srcset="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/ps5_crash_test.jpg?resize=768%2C1024&amp;ssl=1 768w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/ps5_crash_test.jpg?resize=225%2C300&amp;ssl=1 225w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/ps5_crash_test.jpg?w=1024&amp;ssl=1 1024w" sizes="(max-width: 768px) 100vw, 768px" data-recalc-dims="1"></p><p id="caption-attachment-48289">Screenshots courtesy of hhk2003</p><!-- ezoic_video_placeholder-mid_content-640x360-999996-clearholder --><!-- ezoic_video_placeholder-mid_content-640x360-999996-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-mid_content-426x240-999996-clearholder --><!-- ezoic_video_placeholder-mid_content-426x240-999996-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-mid_content-384x216-999996-clearholder --><!-- ezoic_video_placeholder-mid_content-384x216-999996-nonexxxclearxxxblock --></div>
<p><strong>What the CVE is about</strong>:</p>
<blockquote><p><em>Buffer overflow in the sppp driver in FreeBSD 4.11 through 6.1, NetBSD 2.0 through 4.0 beta before 20060823, and OpenBSD 3.8 and 3.9 before 20060902 allows remote attackers to cause a denial of service (panic), obtain sensitive information, and possibly execute arbitrary code via crafted Link Control Protocol (LCP) packets with an option length that exceeds the overall length, which triggers the overflow in (1) pppoe and (2) ippp.</em></p></blockquote>
<p>This appears to be a bug in the Network stack of FreeBSD/NetBSD. Setting aside for a minute how crazy it would be that an 18 year old vulnerability is still present in modern consoles, at the very least this is an area of the code that we know TheFloW has been investigating a lot in the past (the only known PS5 Kernel exploit was from TheFloW and is related to the Network stack as well).</p>
<p><strong>The poc:</strong></p>
<pre>from scapy.all import PPPoED, Ether, sniff, sendp, srp1, hexdump

source = b"\xXX\xXX\xXX\xXX\xXX\xXX" # MAC address of your adapter on PC
destination = b"\xXX\xXX\xXX\xXX\xXX\xXX" # MAC address of LAN on your PS4
interface = "Realtek PCIe 2.5GbE Family Controller #2" # get via "ipconfig /all" or eth0 or similiar on Linux

packet = sniff(iface=interface, filter="pppoed", count=1)
tag_value = packet[PPPoED][0].tag_list[1].tag_value
payload = destination + source + b"\x88\x63\x11\x07\x00\x00\x00\x0c\x01\x03\x00\x08" + tag_value
sendp(payload, iface=interface)

packet = sniff(iface=interface, filter="pppoed", count=1)
payload = destination + source + b"\x88\x63\x11\x65\x00\x01\x00\x0c\x01\x03\x00\x08" + tag_value
sendp(payload, iface=interface)

packet = sniff(iface=interface, filter="pppoes", count=1)
payload = destination + source + b"\x88\x64\x11\x00\x00\x01\x00\x09\xc0\x21\x01\x01\x00\x07\xab\xff"
packet = srp1(Ether(payload), iface=interface)
print(hexdump(packet))

payload = destination + source + b"\x88\x64\x11\x00\x00\x01\x00\x09\xc0\x21\x01\x01\x00\x07\xab\xff" + b"\xff" * 256 # this number might be different, just a guess
for i in range(20):
sendp(payload, iface=interface)</pre><!-- ezoic_video_placeholder-long_content-854x480-999995-clearholder --><!-- ezoic_video_placeholder-long_content-854x480-999995-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-long_content-640x360-999995-clearholder --><!-- ezoic_video_placeholder-long_content-640x360-999995-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-long_content-426x240-999995-clearholder --><!-- ezoic_video_placeholder-long_content-426x240-999995-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-long_content-384x216-999995-clearholder --><!-- ezoic_video_placeholder-long_content-384x216-999995-nonexxxclearxxxblock -->
<h2>Verdict: Is there a new vulnerability on the PS4/PS5?</h2>
<p>A few thoughts come to mind:</p>
<ul>
<li>In my opinion, if a bug from a CVE has been fixed on some system, it should generally not crash that system after the patch. I understand that in some rare scenarios, a crash is the only way out, but in my experience, most bug fixes intend to handle an error case gracefully. <strong>A crash is not an exploit</strong>, but it is very interesting to see that this code actually crashes the PS4 and PS5
<ul>
<li>A likely explanation, rather than an old bug surfacing, is a new one (or similar one) that happens to be triggered by taking a similar path in the code base.</li>
<li>It is also possible that some bits and pieces of the PS4/PS5 Network stack are not based on FreeBSD, but some other PS4/PS5 specific code that might still have the issue due to not be part of FreeBSD patches.</li>
</ul>
</li>
<li><strong>It is particularly interesting to me that some more recent firmwares do not crash</strong>. This <strong>implies that Sony have fixed some issue, or modified some code in that area, in recent firmwares</strong> (which was actually <a href="https://wololo.net/2023/12/06/ps5-firmware-8-40-and-ps4-11-02-are-out-and-theres-something-interesting-in-there-dont-update/">strongly hinted at</a> when PS4 11.02 and PS5 8.40 were released). This is additional confirmation that this crash could be related to a recent vulnerability that was surfaced to Sony’s engineers (e.g. one of TheFloW’s bugs is not out of the question)</li>
<li>Some people have confirmed that the buggy code does, or did, exist, at some point, in the PS4’s codebase. SlidyBat over at the PS5 R&amp;D discord in particular says that there is indeed some function in the PS4 that seems to use the old buggy code:</li>
</ul>
<p><img decoding="async" loading="lazy" src="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_tests.png?resize=1012%2C491&amp;ssl=1" alt="" width="1012" height="491" srcset="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_tests.png?w=1012&amp;ssl=1 1012w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_tests.png?resize=300%2C146&amp;ssl=1 300w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_tests.png?resize=768%2C373&amp;ssl=1 768w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></p>
<p><img decoding="async" loading="lazy" src="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_code_change.png?resize=1024%2C295&amp;ssl=1" alt="" width="1024" height="295" srcset="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_code_change.png?resize=1024%2C295&amp;ssl=1 1024w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_code_change.png?resize=300%2C86&amp;ssl=1 300w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_code_change.png?resize=768%2C221&amp;ssl=1 768w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_code_change.png?w=1300&amp;ssl=1 1300w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></p>
<p><img decoding="async" loading="lazy" src="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_ps4_code.png?resize=1024%2C575&amp;ssl=1" alt="" width="1024" height="575" srcset="https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_ps4_code.png?resize=1024%2C575&amp;ssl=1 1024w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_ps4_code.png?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_ps4_code.png?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_ps4_code.png?resize=1536%2C863&amp;ssl=1 1536w, https://i0.wp.com/wololo.net/wagic/wp-content/uploads/2024/02/slidybat_ps4_code.png?w=1670&amp;ssl=1 1670w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"></p>
<h2>Download and test the PoC</h2><!-- ezoic_video_placeholder-longer_content-854x480-999994-clearholder --><!-- ezoic_video_placeholder-longer_content-854x480-999994-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-longer_content-640x360-999994-clearholder --><!-- ezoic_video_placeholder-longer_content-640x360-999994-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-longer_content-426x240-999994-clearholder --><!-- ezoic_video_placeholder-longer_content-426x240-999994-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-longer_content-384x216-999994-clearholder --><!-- ezoic_video_placeholder-longer_content-384x216-999994-nonexxxclearxxxblock -->
<p>It is of course <strong>too early to tell if this could lead to anything on the PS4 and PS5</strong>, but for sure this looks <strong>way more interesting than I initially thought </strong>it did, a few days ago. We’re at the very early stages of this, and although this won’t be useful to anyone without the right skills to dig further, it probably doesn’t hurt for people to try and run the PoC on their PS4 and PS5.</p>
<p>Feel free to give it a try and report in the comments whether this crashed your console (and which firmware you’re on).</p>
<p>Please understand that in its current stage, this will only crash your console, and it’s not particularly obvious to set up: you’ll need some basic scripting skills. If you can’t follow the instructions on how to edit the script and run it, I’d strongly suggest you don’t sweat it.</p>
<ol>
<li>Get the Poc here: <a href="https://gist.github.com/RobbedColek/8b0bc9655adc6ad9564ad71453188e37">https://gist.github.com/RobbedColek/8b0bc9655adc6ad9564ad71453188e37</a>
<ul>
<li>Requires Python 3 and Scapy.</li>
</ul>
</li>
<li>Connect your PS4/PS5 directly to PC via LAN cable.</li>
<li>Set up your PS4/PS5 for manual ethernet connection via PPPoE – insert any username/password.</li>
<li>Update the “source”, “destination”, and “interface” details in the script, according to your home PC and target console. &lt;– <strong>If you don’t know how to do that and can’t figure it out, stop right there, this thing isn’t for you and you will not be helpful</strong></li>
<li>Use “Test Internet Connection” on console.</li>
<li>Launch script.</li>
</ol><!-- ezoic_video_placeholder-longest_content-854x480-999993-clearholder --><!-- ezoic_video_placeholder-longest_content-854x480-999993-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-longest_content-640x360-999993-clearholder --><!-- ezoic_video_placeholder-longest_content-640x360-999993-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-longest_content-426x240-999993-clearholder --><!-- ezoic_video_placeholder-longest_content-426x240-999993-nonexxxclearxxxblock --><!-- ezoic_video_placeholder-longest_content-384x216-999993-clearholder --><!-- ezoic_video_placeholder-longest_content-384x216-999993-nonexxxclearxxxblock -->
<p dir="auto">If it worked, it will crash your console.</p>
<h2 dir="auto">Reference</h2>
<ul>
<li>Original claim: https://wololo.net/2024/01/26/ps5-update-8-60-is-here-and-guess-what-dont-update/comment-page-1/#comment-6543161</li>
<li>PoC for PS4/PS5: https://gist.github.com/RobbedColek/8b0bc9655adc6ad9564ad71453188e37</li>
<li>CVE: https://www.cvedetails.com/cve/CVE-2006-4304/</li>
<li>OpenBSD patch to fix the 2006 vulnerability: https://ftp.openbsd.org/pub/OpenBSD/patches/3.8/common/014_sppp.patch</li>
</ul>
<p><em>As always I’ve tried to be accurate and precise in this article. If you notice something is missing or wrong, please let me know in the comments. Thanks</em></p>



													</div><!--/.post-inner-->	
			</article><div>
				<p><img alt="" src="https://secure.gravatar.com/avatar/2701e5bfb18f0a34a907b73c452a5c42?s=128&amp;d=wavatar&amp;r=g" srcset="https://secure.gravatar.com/avatar/2701e5bfb18f0a34a907b73c452a5c42?s=256&amp;d=wavatar&amp;r=g 2x" height="128" width="128" loading="lazy" decoding="async"></p>
				<p>wololo</p>
				<p>We are constantly looking for guest bloggers at wololo.net. If you like to write, and have a strong interest in the console hacking scene, contact me either with a comment here, or in a PM on /talk!</p>
				
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The VAE Used for Stable Diffusion Is Flawed (217 pts)]]></title>
            <link>https://old.reddit.com/r/StableDiffusion/comments/1ag5h5s/the_vae_used_for_stable_diffusion_1x2x_and_other/</link>
            <guid>39215242</guid>
            <pubDate>Thu, 01 Feb 2024 12:25:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/StableDiffusion/comments/1ag5h5s/the_vae_used_for_stable_diffusion_1x2x_and_other/">https://old.reddit.com/r/StableDiffusion/comments/1ag5h5s/the_vae_used_for_stable_diffusion_1x2x_and_other/</a>, See on <a href="https://news.ycombinator.com/item?id=39215242">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h5><strong>Short summary for those who are technically inclined:</strong></h5>

<p>CompVis fucked up the KL divergence loss on the KL-F8 VAE that is used by SD1.x, SD2.x, SVD, DALL-E 3, and probably other models.  As a result, the latent space created by it has a massive KL divergence and is smuggling global information about the image through a few pixels.  If you are thinking of using it for training a new, trained-from-scratch foundation model, <strong>don't!</strong> (for the less technically inclined this <strong>does not</strong> mean switch out your VAE for your LoRAs or finetunes, you absolutely do not have the compute power to change the model to a whole new latent space, that would require effectively a full retrain's worth of training.)  SDXL is not subject to this issue because it has its own VAE, which as far as I can tell is trained correctly and does not exhibit the same issues.</p>

<h3>What is the VAE?</h3>

<p>A Variational Autoencoder, in the context of a latent diffusion model, is the eyes and the paintbrush of the model. It translates regular pixel-space images into latent images that are constructed to encode as much of the information about those images as possible into a form that is smaller and easier for the diffusion model to process.</p>

<p>Ideally, we want this "latent space" (as an alternative to <em>pixel space</em>) to be robust to noise (since we're using it with a <em>denoising</em> model), we want latent pixels to be very spatially related to the RGB pixels they represent, and most importantly of all, we want the model to be able to (mostly) accurately reconstruct the image from the latent.  Because of the first requirement, the VAE's encoder doesn't output just a tensor, it outputs a probability distribution that we then sample, and training with samples from this distribution helps the model to be less fragile if we get things a little bit wrong with operations on latents.  For the second requirement, we use Kullback-Leibler (KL) divergence as part of our loss objective: when training the model, we try to push it towards a point where the KL divergence between the latents and a standard Gaussian distribution is minimal -- this effectively ensures that the model's distribution trends toward being roughly equally certain about what each individual pixel should be.  For the third, we simply decode the latent and use any standard reconstruction loss function (LDM used LPIPS and L1 for this VAE).</p>

<h3>What is going on with KL-F8?</h3>

<p>First, I have to show you what a <em>good</em> latent space looks like.  Consider this image: <a href="https://i.imgur.com/DoYf4Ym.jpeg">https://i.imgur.com/DoYf4Ym.jpeg</a></p>

<p>Now, let's encode it using the SDXL encoder (after downscaling the image to shortest side 512) and look at the <em>log variance</em> of the latent distribution (please ignore the plot titles, I was testing something else when I discovered this): <a href="https://i.imgur.com/Dh80Zvr.png">https://i.imgur.com/Dh80Zvr.png</a></p>

<p>Notice how there are some lines, but overall the log variance is fairly consistent throughout the latent.  Let's see how the KL-F8 encoder handles this: <a href="https://i.imgur.com/pLn4Tpv.png">https://i.imgur.com/pLn4Tpv.png</a></p>

<p>This obviously looks very different in many ways, but the most important part right now is that black dot (hereafter referred to as the "black hole").  It's not a brain tumor, though it does look like one, and might as well be the machine-learning equivalent of one.  It's a spot where the VAE is trying to smuggle <strong>global information about the image</strong> through latent space.  This is exactly the problem that KL-divergence loss is supposed to prevent.  Somehow, it didn't.  I suspect this is due to underweighting of the KL loss term.</p>

<h3>What are the implications?</h3>

<p>Somewhat subtle, but significant.  Any latent diffusion model using this encoder is having to do a lot of extra work to get around the bad latent space.</p>

<p>The easiest one to demonstrate, is that the latent space is very fragile in the area of the black hole: <a href="https://i.imgur.com/8DSJYPP.png">https://i.imgur.com/8DSJYPP.png</a></p>

<p>In this image, I overwrote the mean of the latent distribution with random noise in a 3x3 area centered on the black hole, and then decoded it.  I then did the same on another 3x3 area as a control and decoded it.  The right side images are the difference between the altered and unaltered images.  <strong>Altering the latents at the black hole region makes changes across the whole image.</strong>  Altering latents anywhere else causes strictly local changes.  What we would want is strictly local changes.</p>

<p>The most substantial implication of this, is that these are the rules that the Stable Diffusion or other denoiser model has to play by, because this is the latent space it is aligned to.  So, of course, it learns to construct latents that smuggle information: <a href="https://i.imgur.com/WJsWG78.png">https://i.imgur.com/WJsWG78.png</a></p>

<p>This image was constructed by measuring the mean absolute error between the reconstruction of an unaltered latent and one where a <strong>single</strong> latent pixel was zeroed out.  Bright regions are ones where it is smuggling information.</p>

<p>This presents a number of huge issues for a denoiser model, because these latent pixels have a huge impact on the whole image and yet are treated as equal.  The model also has to spend a ton of its parameter space on managing this.</p>

<p>You can reproduce the effects on Stable Diffusion yourself using this code:</p>

<pre><code>import torch
from diffusers import StableDiffusionPipeline
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
from copy import deepcopy

pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, safety_checker=None).to("cuda")
pipe.vae.requires_grad_(False)
pipe.unet.requires_grad_(False)
pipe.text_encoder.requires_grad_(False)

def decode_latent(latent):
    image = pipe.vae.decode(latent / pipe.vae.config.scaling_factor, return_dict=False)
    image = pipe.image_processor.postprocess(image[0], output_type="np", do_denormalize=[True] * image[0].shape[0])
    return image[0]

prompt = "a photo of an astronaut riding a horse on mars"

latent = pipe(prompt, output_type="latent").images

original_image = decode_latent(latent)

plt.imshow(original_image)
plt.show()

divergence = np.zeros((64, 64))
for i in tqdm(range(64)):
    for j in range(64):
        latent_pert = deepcopy(latent)
        latent_pert[:, :, i, j] = 0
        md = np.mean(np.abs(original_image - decode_latent(latent_pert)))
        divergence[i, j] = md

plt.imshow(divergence)
plt.show()
</code></pre>

<h3>What is the prognosis?</h3>

<p>Still investigating this!  But I wanted to disclose this sooner rather than later, because I am confident in my findings and what they represent.</p>

<p>SD 1.x, SD 2.x, SVD, and DALL-E 3 (kek) and probably other models are likely affected by this.  You can't just switch them over to another VAE like SDXL's VAE without what might as well be a full retrain.</p>

<p>Let me be clear on this before going any further:  <strong>These models demonstrably work fine.  If it works, it works, and they work.</strong>  This is more of a discussion of the limits and if/when it is worth jumping ship to another model architecture.  I love model necromancy though, so let's talk about salvaging them.</p>

<p>Firstly though, if you are thinking of making a new, trained-from-scratch foundation model with the KL-F8 encoder, <strong>don't!</strong>  Probably tens of millions of dollars of compute have already gone towards models using this flawed encoder, don't add to that number!  At the very least, resume training on it and crank up that KL divergence loss term until the model behaves!  Better yet, do what Stability did and train a new one on a dataset that is better than OpenImages.</p>

<p>I think there is a good chance that the VAE could be fixed without altering the overall latent space too much, which would allow salvaging existing models.  Recall my comparison in that second to last image: even though the VAE was smuggling global features, the reconstruction still looked mostly fine without the smuggled features.  Training a VAE encoder would normally be an extremely bad idea if your expectation is to use the VAE on existing models aligned to it, because you'll be changing the latent space and the model will not be aligned to it anymore.  But if deleting the black hole doesn't destroy the image (which is the case here), it may very well be possible to tune the VAE to no longer smuggle global features while keeping the latent space at least similar enough to where existing models can be made compatible with it with at most a significantly shorter finetune than would normally be needed.  It may also be the case that you can already define a latent image within the decoder's space that is a close reconstruction of a given original without the smuggled features, which would make this task significantly easier.  Personally, I'm not ready to give up on SD1.5 until I have tried this and conclusively failed, because frankly rebuilding all existing tooling would suck, and model necromancy is fun, so I vote model necromancy!  This all needs actual testing though.</p>

<p>I suspect it may be possible to mitigate some of the effects of this within SD's training regimen by somehow scaling reconstruction loss on the latent image by the log variance of the latent.  The black hole is very well defined by the log variance: the VAE is <em>very certain</em> about what those pixels should be compared to other pixels and they accordingly have much more influence on the image that is reconstructed.  If we take the log variance as a proxy for the impact a given pixel has on the model, maybe you can better align the training objective of the denoiser model with the actual impact on latent reconstruction.  This is purely theoretical and needs to be tested first.  Maybe don't do this until I get a chance to try to fix the VAE, because that would just be further committing the model to the existing shitty latent space.</p>

<p>Failing this, people should recognize the limits of SD1.x and move to a new architecture.  It's over a year old, and this field moves fast.  Preferably one that still doesn't require a 3090 to run, please, I have one but not everyone does and what made SD1.5 so well supported was the fact that it could be run and trained on a much broader variety of hardware (being able to train a model in a decent amount of time with less than an A100-80GB would also be great too).  There are a lot of exciting new architectural changes proposed lately with things like Hourglass Diffusion Transformers and the new Karras paper from December to where a much, much better model with a similar compute footprint is certainly possible.  And we knew that SD1.5 would be fully obsolete one day.</p>

<p>I would like to thank my friends who helped me recognize and analyze this problem, and I would also like to thank the Glaze Team, because I accidentally discovered this while analyzing latent images perturbed by Nightshade and wouldn't have found it without them, because I guess nobody else ever had a reason to inspect the log variance of the latent distributions created by the VAE.  I'm definitely going to be performing more validation on models I try to use in my projects from now on after this, Jesus fucking Christ.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: filippo.io/mlkem768 – Post-Quantum Cryptography for the Go Ecosystem (236 pts)]]></title>
            <link>https://words.filippo.io/dispatches/mlkem768/</link>
            <guid>39214743</guid>
            <pubDate>Thu, 01 Feb 2024 11:10:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://words.filippo.io/dispatches/mlkem768/">https://words.filippo.io/dispatches/mlkem768/</a>, See on <a href="https://news.ycombinator.com/item?id=39214743">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <p><a href="https://pkg.go.dev/filippo.io/mlkem768?ref=words.filippo.io">filippo.io/mlkem768</a> is a pure-Go implementation of ML-KEM-768 optimized for correctness and readability. ML-KEM (formerly known as Kyber, renamed because we can’t have nice things) is a <a href="https://en.wikipedia.org/wiki/Post-quantum_cryptography?ref=words.filippo.io">post-quantum</a> key exchange mechanism in the process of being standardized by NIST and adopted by most of the industry.</p>
<p>The package amounts to <a href="https://github.com/FiloSottile/mlkem768/blob/main/mlkem768.go?ref=words.filippo.io">~500 lines of code</a>, plus 200 lines of comments, and 650 lines of tests. It has no dependencies except for golang.org/x/crypto/sha3. It’s meant for <a href="https://go.dev/cl/550215?ref=words.filippo.io">upstreaming</a> into the Go standard library (initially as an internal-only package used in an opt-in crypto/tls experiment) and was designed to provide high security assurance through ease of review, simplicity, and thorough testing.</p>
<p>I livecoded part of its development <a href="https://twitch.tv/filosottile?ref=words.filippo.io">on Twitch</a>, and you can watch <a href="https://www.youtube.com/watch?v=MyB7A93C-V0&amp;ref=words.filippo.io">the replay on YouTube</a>.</p>
<p>Unlike most other implementations, this code was not ported from the reference pq-crystals library, but written from scratch not having ever closely read other codebases. This was an intentional exercise in spec validation, to show it is possible to produce an interoperable implementation from the specification alone.</p>
<p>The <a href="https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.203.ipd.pdf?ref=words.filippo.io">FIPS 203 document</a> turned out to be an excellent implementation guide, with detailed pseudo-code, exhaustive definitions, and consistent type information. (This is something I would like to ask of any large specification document: define your types and use them and denote them!) To make the code both easier to review and better as a learning resource, function and variable names, and even operation ordering, are carefully picked to mirror the FIPS specification.</p>
<p>The specification actually requires fairly limited math background, but to facilitate the work of implementers, I wrote up <a href="https://words.filippo.io/dispatches/kyber-math/">Enough Polynomials and Linear Algebra to Implement Kyber</a>.</p>
<p>Beyond that, the only parts left as an exercise to the reader were</p>
<ol>
<li>implementing arithmetic modulo the prime 3329;</li>
<li>concretely implementing the compress and decompress functions mapping values [0, 3329) to and from [0, 2ᵈ); and</li>
<li>ensuring constant time operations.</li>
</ol>
<p>Modulo arithmetic was reasonably easy, as we all collectively learned a lot about finite field arithmetic through years of RSA and elliptic curve implementations. The small prime actually makes the task feel unnaturally simple.</p>
<p><a href="https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.203.ipd.pdf?ref=words.filippo.io#equation.4.5">Compression and decompression</a> turned out to be the most difficult part of the project. The specification defines them in abstract terms as fractions and rounding rules—“just” compute (2ᵈ/q)·x or (q/2ᵈ)·y and round to the closest integer—but in practice we need to implement them with constant time arithmetic and bitwise operations! In my <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/n8dsN_aMsa8/m/NHO2lNyWAAAJ?ref=words.filippo.io">public comments</a> I pointed out that having each implementation figure out a strategy is risky and redundant. I was more correct than I thought: it turned out that <a href="https://github.com/pq-crystals/kyber/commit/dda29cc63af721981ee2c831cf00822e69be3220?ref=words.filippo.io">the reference implementation and ~every implementation ported from it used a division</a> which depending on compiler optimizations and platform might result in a DIV instruction, which is variable-time even when the divisor is fixed. This package was unaffected, because it used <a href="https://www.nayuki.io/page/barrett-reduction-algorithm?ref=words.filippo.io">Barrett reduction</a> from the start, like BoringSSL.</p>
<p>You can read <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/n8dsN_aMsa8/m/NHO2lNyWAAAJ?ref=words.filippo.io">the rest of my formal public comments</a> on the pqc-forum mailing list.</p>
<p>Readability was a major goal of the implementation, and it was pursued <s>even</s> especially for complex functions like compression and decompression. A readable implementation has two purposes: first, it allows effective review, both during the code review process and later by interested researchers, improving security; second, it serves as an educational resource for the next generation of maintainers and cryptography engineers (or curious nerds). Reading the Go cryptography standard library is how I got started on the path that led me here, so it is especially important to me to preserve and improve it as a learning resource. It’s obviously subjective, but I believe this to be the most understandable public ML-KEM/Kyber implementation. Compare for example <a href="https://github.com/FiloSottile/mlkem768/blob/344d5ee2c575ca84613bbb119c1d8b1ef6699ea5/mlkem768.go?ref=words.filippo.io#L394-L442">our compression/decompression functions</a> with <a href="https://github.com/pq-crystals/kyber/blob/11d00ff1f20cfca1f72d819e5a45165c1e0a2816/ref/poly.c?ref=words.filippo.io#L9-L112">the reference implementation</a>.</p>
<p>Sometimes improving readability and reviewability means making code longer and less reusable: for example for ML-KEM-768 we need to serialize 1-, 4-, 10-, and 12-bit integers in a packed format. A universal 1-to-12 bit encoder and decoder is a pretty gnarly piece of code to write correctly, but each of those four sizes are actually pretty easy to write a dedicated encoder/decoder for.<sup><a href="#fn1" id="fnref1">[1]</a></sup> This is why we have <code>ringCompressAndEncode1/4/10</code> etc. instead of a single universal function. This also made it easy to work some special required checks into the 12-bit decoder.</p>
<p>This, by the way, was only possible because we targeted ML-KEM-768 specifically, or we’d have had to implement 5- and 11-bit encodings, as well. ML-KEM is specified at three security levels (-512, -768, and -1024). However, the Kyber team recommends using -768 over -512 for a more conservative security margin against novel cryptanalysis, while -1024 exists only for the same reasons 256-bit security levels exist: compliance and blind <a href="https://www.imperialviolet.org/2014/05/25/strengthmatching.html?ref=words.filippo.io">strength matching</a>. Most protocols being tested or standardized coalesced around ML-KEM-768, so targeting only that improves not only readability, but also security (because there are fewer moving parts), and performance (because we can optimize allocation sizes, iteration counts, and encoding algorithms) at little to no cost.</p>
<p>After readability, testing is the main component in this package’s high security assurance strategy. Besides checking that key generation, encapsulation, and decapsulation round-trip correctly, and maintaining a test coverage of 95%+, we</p>
<ul>
<li>ensure interoperability with <a href="https://github.com/FiloSottile/mlkem768/blob/344d5ee2c575ca84613bbb119c1d8b1ef6699ea5/testdata/vectors.json?ref=words.filippo.io">test vectors obtained from NIST and other implementations</a>;</li>
<li>exhaustively test every input combination for base field arithmetic operations (addition, subtraction, and multiplication modulo 3329) against expected values computed trivially with variable-time operations;</li>
<li>exhaustively test compression and decompression against math/big.Rat (contributed by David Buchanan);</li>
<li>test that pre-computed constants match their definition;</li>
<li>check that incorrect lengths (both long and short) cause the appropriate error for every input of every function;</li>
<li>run an extensive set of reusable test vectors we developed (see below);</li>
<li>run test vectors <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/aCAX-2QrUFw/m/hy5gwcESAAAJ?ref=words.filippo.io">provided by Sophie Schmieg</a> which will be eventually included in <a href="https://github.com/google/wycheproof?ref=words.filippo.io">Wycheproof</a>.</li>
</ul>
<p>Our test vectors are designed to be reusable by other implementations, and <a href="https://c2sp.org/CCTV/ML-KEM?ref=words.filippo.io">are published as part of the CCTV project</a> along with detailed <a href="https://github.com/C2SP/CCTV/blob/main/ML-KEM/README.md?ref=words.filippo.io#intermediate-values">intermediate values</a> for testing and debugging each intermediate step and partial algorithm, which we used during development. There are different sets of tests vectors, each designed to reach different edge cases.</p>
<ul>
<li>
<p><a href="https://github.com/C2SP/CCTV/blob/main/ML-KEM/README.md?ref=words.filippo.io#bad-encapsulation-keys">Negative test vectors</a> provide invalid encapsulation keys, where the coefficients are higher than 3329. These were often requested, since all the test vectors from the Kyber and NIST teams are for regular, correct inputs. These vectors individually test every value from 3329 to 2¹²-1 and every coefficient location, sharing the remaining coefficients so they compress from 1–3 MiB down to 12–28 KiB.</p>
</li>
<li>
<p><a href="https://github.com/C2SP/CCTV/blob/main/ML-KEM/README.md?ref=words.filippo.io#unlucky-ntt-sampling-vector">“Unlucky” vectors</a> require an unusually large number of XOF reads. Kyber samples a matrix from a portion of public keys<sup><a href="#fn2" id="fnref2">[2]</a></sup> with <em>rejection sampling</em>: it gets a random value between 0 and 2 ¹²-1 and checks if it’s less than 3329, if not, it tries again. The amount of bytes needed to sample a matrix depends on how lucky you get with the sampling, and that’s a random function of the public key component. These vectors are regular public keys and require reading more than 575 bytes from the SHAKE-128 XOF in SampleNTT, which would ordinarily happen with probability 2⁻³⁸. Sophie’s vectors were bruteforced further, and require up to 591 bytes.</p>
<blockquote>
<p>At this point I would like to thank our detection and response team for not killing my job(s) hashing vast amounts of random seeds and looking for zeroes in the output. — Sophie Schmieg</p>
</blockquote>
</li>
<li>
<p>Special vectors <a href="https://github.com/C2SP/CCTV/blob/main/ML-KEM/README.md?ref=words.filippo.io#strcmp-vectors">fail if strcmp is used</a> in ML-KEM.Decaps. In ML-KEM.Decaps the ciphertext is compared with the output of K-PKE.Encrypt for implicit rejection. If an implementation were to use <code>strcmp()</code> for that comparison it would fail to reject some ciphertexts if a zero byte terminates the comparison early. This one I hope is going to sit as a silent trap for years—who would use <code>strcmp()</code> in cryptographic code—and then ruthlessly kill a vulnerability, because of course someone will.</p>
</li>
<li>
<p><a href="https://github.com/C2SP/CCTV/blob/main/ML-KEM/README.md?ref=words.filippo.io#accumulated-pq-crystals-vectors">Accumulated vectors</a> (derived from the reference pq-crystals implementation) allow testing randomly reachable edge cases without checking in large amounts of data. The reference implementation of Kyber includes a <code>test_vectors.c</code> program that generates 300MB of random vectors. I had no intention of checking in the output or compiling C, but since they are just randomly generated vectors, we can regenerate them in our tests from the deterministic RNG (SHAKE-128 with an empty input) and check they hash to an expected value. We can even take it further, and produce hashes for a million random tests, beyond the 10k they generate.</p>
</li>
</ul>
<p>I am happy to report that none of the tests, many introduced after completion of the implementation, identified any issues in filippo.io/mlkem768. There is <a href="https://github.com/C2SP/CCTV/issues/7?ref=words.filippo.io">at least one reported instance</a> of the negative vectors identifying a defect in a major implementation, though.</p>
<p>Performance is not a primary goal (neither of this package nor of <a href="https://go.dev/design/cryptography-principles?ref=words.filippo.io">the Go cryptography packages</a>) but the package needs to be fast enough to be useful. Thankfully, ML-KEM is pretty fast, to the point that this simple implementation is competitive with our assembly-optimized P-256 and X25519 implementations.</p>
<p>To compare apples to apples, note that we need to compare the whole operation that each side needs to perform for key establishment: for ECDH, two scalar multiplications (one of them by the fixed base point); for KEMs, key generation and decapsulation on one side, and encapsulation on the other. ECDH is symmetrical, ML-KEM key establishment is not.</p>
<p>The ECDH benchmarks below already include the two scalar multiplications, while the mlkem768 benchmarks are split as key generation and decapsulation under “Alice” and encapsulation under “Bob”. Since decapsulation includes a full encryption (to check the resulting ciphertext matches the input), Alice takes a lot longer than Bob: the latter does an encryption, while the former does an encryption, a decryption, and a key generation.</p>
<p>All in all, “Bob” is as fast as our X25519 or P-256, while “Alice” takes less than twice. Compared to some of the fastest ML-KEM implementations out there (BoringSSL and libcrux), this package takes approximately double the time. For such a simple and unoptimized implementation, this is more than satisfactory.</p>
<pre><code>goos: darwin
goarch: arm64
cpu: Intel(R) Core(TM) i5-7400 CPU @ 3.00GHz
pkg: crypto/ecdh
                         │   sec/op    │
ECDH/P256-8                49.43µ ± 0%
ECDH/X25519-8              77.46µ ± 0%

pkg: filippo.io/mlkem768
                         │   sec/op    │
RoundTrip/Alice-8          109.4µ ± 0%
RoundTrip/Bob-8            56.19µ ± 0%

goos: linux
goarch: amd64
pkg: crypto/ecdh
                         │   sec/op    │
ECDH/P256-4                78.88µ ± 1%
ECDH/X25519-4              115.6µ ± 2%

pkg: filippo.io/mlkem768
                         │   sec/op    │
RoundTrip/Alice-4          223.8µ ± 2%
RoundTrip/Bob-4            114.7µ ± 1%
</code></pre>
<p>The performance wasn’t entirely free. In general, I followed high-performance Go programming patterns, trying for example to minimize heap allocations. Next, I <a href="https://go.dev/cl/544817/2?ref=words.filippo.io">reworked the x/crypto/sha3 package</a> so it could be used without any heap allocation thanks to <a href="https://words.filippo.io/efficient-go-apis-with-the-inliner/">the mid-stack inlining trick</a>. However, I haven’t merged those changes yet and they are not included in the benchmarks above, because they have a negative effect on Apple M2 processors. No idea why yet.</p>
<pre><code>goos: darwin
goarch: arm64
pkg: filippo.io/mlkem768
                  │   sec/op    │   sec/op     vs base                │
RoundTrip/Alice-8   109.4µ ± 0%   121.3µ ± 1%  +10.91% (p=0.000 n=10)
RoundTrip/Bob-8     56.19µ ± 0%   59.94µ ± 2%   +6.66% (p=0.000 n=10)

goos: linux
goarch: amd64
                  │   sec/op    │   sec/op     vs base               │
RoundTrip/Alice-4   223.8µ ± 2%   218.6µ ± 1%  -2.32% (p=0.000 n=10)
RoundTrip/Bob-4     114.7µ ± 1%   109.5µ ± 0%  -4.57% (p=0.000 n=10)
</code></pre>
<p>The one successful optimization was complaining about the confusing result above on the Gophers Slack <code>#performance</code> channel, which sniped Josh Bleecher Snyder into contributing <a href="https://github.com/FiloSottile/mlkem768/pulls?q=is%3Apr+author%3Ajosharian&amp;ref=words.filippo.io">a couple changes</a> :)</p>
<p>There is some low hanging fruit still: key generation and decapsulation both sample a matrix from the same value, and since the two are usually done sequentially on the Alice side, the matrix could be stored saving around 10% time. There might be an opportunity to save a copy in the sha3 read path, too. After that, it’s a matter of optimizing the field implementation.</p>
<p>If you got this far, you might want to follow me on Bluesky at <a href="https://bsky.app/profile/filippo.abyssdomain.expert?ref=words.filippo.io">@filippo.abyssdomain.expert</a> or on Mastodon at <a href="https://abyssdomain.expert/@filippo?ref=words.filippo.io">@filippo@abyssdomain.expert</a>.</p>
<h2 id="bonus-track-using-a-ml-kem-implementation-as-kyber-v3">Bonus track: using a ML-KEM implementation as Kyber v3</h2>
<p>NIST made a few small changes to the Round 3 submission of Kyber. They are summarized in Section 1.3 of the FIPS draft.</p>
<p>However, there are a few experimental protocols defined in terms of Kyber v3 (or “draft00”), including the main deployed PQ TLS key exchange. Do we have to make a separate package to support them?</p>
<p>Luckily, no we don’t.</p>
<p>One change adds some validation for an edge case (non-canonical coefficient encodings in public keys) that was undefined in Kyber. Honest implementations will not produce such keys, so we can reject them as specified in the FIPS draft. It will make it possible to fingerprint our implementation as Kyber-on-ML-KEM but will be otherwise harmless.</p>
<p>One change removed a hashing step applied to CSPRNG input. Since those bytes are random, it’s impossible for any party to tell the difference.</p>
<p>The final change is the major one, and the trickiest. The ciphertext used to be hashed into the shared secret. This difference would prevent interoperability. However, the mixing happens as an additional key derivation, which was entirely removed in ML-KEM, which instead returns the value K as-is. This means we can run ML-KEM to generate the shared secret K and then apply</p>
<pre><code>SHAKE-256(K || c)[:32]
</code></pre>
<p>to generate the Kyber shared secret. No need to break the ML-KEM abstraction.</p>
<p>There’s one wrinkle: both Kyber and ML-KEM perform implicit rejection in Decapsulate by hashing a secret with the ciphertext and returning that as the shared secret. If we do the key derivation above on top of ML-KEM, we’ll hash the ciphertext twice for implicit rejections. That’s ok, because the output of implicit rejection is unpredictable by design, not an interoperation target.</p>
<h2 id="the-picture">The picture</h2>
<p>In Berlin there's an old closed airport, <a href="https://en.wikipedia.org/wiki/Berlin_Tempelhof_Airport?ref=words.filippo.io">Tempelhof</a>, which is now a public park. Walking down the taxiways (pictured) or along the centrelines of the 09L/27R and 09R/27L crossed-out runways is kinda unsettling, at least for me. ("Should I be speaking with Ground or Tower? Can I enter this runway?") Fun fact, in 2010 a single-engine plane forgot to switch fuel tank and did an emergency landing on 27L. Closed runways are the best bad places to land, after all.</p>
<p><img src="https://words.filippo.io/content/images/2024/01/news---1--2--1.jpeg" alt="A cement taxiway pictured at sunset, from the middle of the yellow centreline. The airport terminal is visible on the horizon, and a patch of grass on the left." loading="lazy"></p>
<p>This work was funded by a Google <a href="https://bughunters.google.com/about/rules/5891381450768384/open-source-security-subsidies-rules?ref=words.filippo.io">Open Source Security Subsidy</a> and by my awesome clients—<a href="https://www.sigsum.org/?ref=words.filippo.io">Sigsum</a>, <a href="https://www.latacora.com/?ref=words.filippo.io">Latacora</a>, <a href="https://interchain.io/?ref=words.filippo.io">Interchain</a>, <a href="https://smallstep.com/?ref=words.filippo.io">Smallstep</a>, <a href="https://www.avalabs.org/?ref=words.filippo.io">Ava Labs</a>, <a href="https://goteleport.com/?ref=words.filippo.io">Teleport</a>, and <a href="https://tailscale.com/?ref=words.filippo.io">Tailscale</a>—who, through our retainer contracts, get face time and unlimited access to advice on Go and cryptography.</p>
<p>Here are a few words from some of them!</p>
<p>Latacora — <a href="https://www.latacora.com/blog/2023/12/22/case-for-password-hashing/?ref=words.filippo.io">We wrote about password hashing with delegation</a>, a somewhat less known password hashing primitive. It's a PBKDF with a special property, that allows offloading hashing computation to a potentially untrusted server. In this blog post, we describe this primitive and discuss its applicability in the context of End-to-End Encrypted (E2EE) backup systems.</p>
<p>Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. <a href="https://goteleport.com/identity-governance-security/?utm=filippo&amp;ref=words.filippo.io">Teleport Identity Governance &amp; Security</a> is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.</p>
<p>Ava Labs — We at <a href="https://www.avalabs.org/?ref=words.filippo.io">Ava Labs</a>, maintainer of <a href="https://github.com/ava-labs/avalanchego?ref=words.filippo.io">AvalancheGo</a> (the most widely used client for interacting with the <a href="https://www.avax.network/?ref=words.filippo.io">Avalanche Network</a>), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>The minimum common multiple of 1/4/10/12 with 8 is less than 64, so we can pack a few values in a uint64, and then serialize that. <a href="https://github.com/FiloSottile/mlkem768/blob/344d5ee2c575ca84613bbb119c1d8b1ef6699ea5/mlkem768.go?ref=words.filippo.io#L522-L627">The result</a> is IMHO pretty readable. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>IIUC the matrix could have been hardcoded but is instead derived from a seed in the key instead to bypass debate on how the hardcoded matrix was generated, and any backdoor concerns. My somewhat spicy opinion is that we’ll come to see this as a mistake, and a case of <a href="https://words.filippo.io/dispatches/parameters/">failing to define parameters</a>. If the matrix was hardcoded ML-KEM would be faster and simpler. For example, there would be no need for these tests at all, and <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/s-C-zIAeKfE/m/n9yqLja7AQAJ?ref=words.filippo.io">the matrix derivation typo in the spec draft</a> couldn’t have happened. Maybe some deployments can just specify and use a profile of ML-KEM that fixes the matrix seed. <a href="#fnref2">↩︎</a></p>
</li>
</ol>
</section>

        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Markov Chains Are the Original Language Models (136 pts)]]></title>
            <link>https://elijahpotter.dev/articles/markov_chains_are_the_original_language_models</link>
            <guid>39213410</guid>
            <pubDate>Thu, 01 Feb 2024 06:59:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://elijahpotter.dev/articles/markov_chains_are_the_original_language_models">https://elijahpotter.dev/articles/markov_chains_are_the_original_language_models</a>, See on <a href="https://news.ycombinator.com/item?id=39213410">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img src="https://elijahpotter.dev/images/old_car.webp" alt="An old car interior"></p><blockquote>
<p><strong>Heads Up:</strong> This article is a republished (with some tweaks on spelling, grammar and layout) version of
an article I wrote in my senior year of high school for my Linear Algebra class.
As such, the publish date is not quite correct.</p>
</blockquote>
<h2>The AI Buzz is Boring Now</h2>
<p>I've come to the conclusion that there are four stages to the current AI hype cycle in an individual person's brain, at least as it pertains to large language models.
At the very least, these are the stages I went through.</p>
<h3>Stage One: Amazement</h3>
<p>"Wow! This is so cool! I can converse with a computer just like a real person!"</p>
<p>This is where all the science fiction fantasies come to fruition.
The possibilities seem endless.
We can all kick back and relax now, right?</p>
<h3>Stage Two: Frustration</h3>
<p>"Hmm... This isn't as effective as I originally thought."</p>
<p>It seems like the brand-new technology is really only applicable to the kinds of work no one wants to do anyway.
What it <strong>is</strong> able to do doesn't provide too much value to you.
It gets information and logic wrong often enough that it cannot be trusted for just about anything.</p>
<h3>Stage Three: Confusion</h3>
<p>After stage two, you start to forget about it.
But the hype is inescapable.
Your friends bring it up.
Your parents ask you about it when you go home for the holidays.
Even your dentist tries to extol its virtues.</p>
<p>Even if you moved on it, no one else did.
Could that mean that you were wrong?</p>
<h3>Stage Four: Boredom</h3>
<p>At this point the rate of new language models appearing has become faster than rate of new JavaScript frameworks (and just as annoying).
You want to go back to your roots and start from scratch.
You want the freedom of knowing the whole stack from start to finish.
You don't want any of the ineffective magic.</p>
<p>This is where I am right now.
Want to go back to my roots.
Some people work on old cars, even though they are less efficient.
At the same time though, they are more fun to work on than new cars.
I've decided to look into Markov chains.</p>
<h2>Markov Chains</h2>
<p>Below is a demonstration of my implementation of auto-completion using Markov Chains.</p>
<p>Though it is written in Rust and compiled to WebAssembly, it is not particularly efficient. To find out why, continue down the page to my detailed explanation of the implementation.</p>
<h2>Controls</h2>
<p>You may use either "Choose Word" or your right arrow key [→] to let the system choose the next word. Alternatively, you can tap any of the [Possible Next Words] to do so yourself.</p>

<h2>Explanation</h2>
<p>Markov chains, named after their inventor, Andrey Markov, are often used to model sequences of probabilistic events. That is, systems that cannot be modeled deterministically.</p>
<h2>Example</h2>
<p>Alice is at the grocery store. For every hour she is there, she has a 70% chance of leaving and going to the planetarium. Conversely, she has a 30% chance of staying.
If Alice is already at the planetarium, she has a 10% chance of leaving and going to the grocery store and a 90% chance of staying.
We can represent these probabilities as a table, where each column belongs to a start location, and each row belongs to a end location:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Start at Grocery Store</td>
<td>Start at Planetarium</td>
</tr>
<tr>
<td>End at Grocery Store</td>
<td>30%</td>
<td>10%</td>
</tr>
<tr>
<td>End at Planetarium</td>
<td>70%</td>
<td>90%</td>
</tr>
</tbody>
</table>
<p>If we already know Alice's location for sure, we can simply perform table lookups to predict her most likely next move.
For example, we <em>know</em> she is at the grocery store right now. So by looking at row 2, column 1, we can be 70% confident she will be at the planetarium next hour.
However, this doesn't work if we aren't sure of her location, or we want to predict more than one hour in advance. How do we predict her next move if we aren't certain of her current location?
In the latter case, we might express her current location as another table.</p>
<table>
<thead>
<tr>
<th>Location</th>
<th>% Alice Present</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grocery Store</td>
<td>25%</td>
</tr>
<tr>
<td>Planetarium</td>
<td>75%</td>
</tr>
</tbody>
</table>
<p>How do we estimate Alice's location in this new plane of possibility? In particular, how likely will Alice be at the Planetarium next hour?
Since there is a 25% probability Alice is at the grocery store, we multiply that with the probility of her transitioning to the Planetarium: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>25</mn><mi mathvariant="normal">%</mi><mo>∗</mo><mn>75</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">25\% * 75\%</annotation></semantics></math></span></span>. Next, we add the result with the probability of being at the Planetarium multiplied with the probability of her staying: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>75</mn><mi mathvariant="normal">%</mi><mo>∗</mo><mn>90</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">75\% * 90\%</annotation></semantics></math></span></span>.
In full, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>25</mn><mi mathvariant="normal">%</mi><mo>∗</mo><mn>75</mn><mi mathvariant="normal">%</mi><mo>+</mo><mn>75</mn><mi mathvariant="normal">%</mi><mo>∗</mo><mn>90</mn><mi mathvariant="normal">%</mi><mo>=</mo><mn>85</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">25\% * 75\% + 75\% * 90\% = 85\%</annotation></semantics></math></span></span>.
To see the probabilities as a table:</p>
<table>
<thead>
<tr>
<th>Next Location</th>
<th>Calculation</th>
<th>% Alice Present</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grocery Store</td>
<td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>25</mn><mi mathvariant="normal">%</mi><mo>∗</mo><mn>30</mn><mi mathvariant="normal">%</mi><mo>+</mo><mn>75</mn><mi mathvariant="normal">%</mi><mo>∗</mo><mn>10</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">25\% * 30\% + 75\% * 10\%</annotation></semantics></math></span></span></td>
<td>15%</td>
</tr>
<tr>
<td>Planetarium</td>
<td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>25</mn><mi mathvariant="normal">%</mi><mo>∗</mo><mn>70</mn><mi mathvariant="normal">%</mi><mo>+</mo><mn>75</mn><mi mathvariant="normal">%</mi><mo>∗</mo><mn>90</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">25\% * 70\% + 75\% * 90\%</annotation></semantics></math></span></span></td>
<td>85%</td>
</tr>
</tbody>
</table>
<p>The keen-eyed among you may have noticed that these operations look a lot like matrix multiplication.
Instead of a table, we may represent these possible transitions as a matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span></span>, and the Alice's current location as a vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>s</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{s}</annotation></semantics></math></span></span>.</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.3</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.7</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.9</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">T = \begin{bmatrix}
  0.3 &amp; 0.1 \\
  0.7 &amp; 0.9
\end{bmatrix}</annotation></semantics></math></span></span></span>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>s</mi><mo>⃗</mo></mover><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">.</mi><mn>25</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">.</mi><mn>75</mn></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\vec{s} = \begin{bmatrix}
  .25 \\
  .75 \\
\end{bmatrix}</annotation></semantics></math></span></span></span></p><blockquote>
<p><strong>Note:</strong> The location of each element remains the same as the table, even if we aren't explicitly labeling the rows and columns.</p>
</blockquote>
<p>Finding the next state matrix becomes as easy as multiplying the current location vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>s</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{s}</annotation></semantics></math></span></span> by <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span></span>. To find further hours in the future, we do it more than once. For example, to estimate three hours in the future: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mi>T</mi><mi>T</mi><mover accent="true"><mi>s</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">TTT\vec{s}</annotation></semantics></math></span></span>. We can condense this with an exponent: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mn>3</mn></msup><mover accent="true"><mi>s</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">T^3\vec{s}</annotation></semantics></math></span></span> or generalize it to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> hours with: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mi>n</mi></msup><mover accent="true"><mi>s</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">T^n\vec{s}</annotation></semantics></math></span></span>.</p>
<h2>Application to Text-Completion</h2>
<p>The principles above can be applied to a variety of probabilistic situations. Most relavant to this particular webpage, is text completion.
We want to estimate the most likely next word to the user. Given the last word, what are the most likely next words? First, we need a dictionary.</p>
<h3>The Dictionary</h3>
<p>It is trivial to build a dictionary from sample text. For the purposes of the explanation, we are going to start with an arbitrary dictionary.</p>
<table>
<thead>
<tr>
<th>Index</th>
<th>Word</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>orange</td>
</tr>
<tr>
<td>1</td>
<td>fruit</td>
</tr>
<tr>
<td>2</td>
<td>passion</td>
</tr>
<tr>
<td>3</td>
<td>cheese</td>
</tr>
<tr>
<td>4</td>
<td>not</td>
</tr>
<tr>
<td>5</td>
<td>is</td>
</tr>
</tbody>
</table>
<h3>Building the Transition Matrix</h3>
<p>To build our transition matrix, we need to count all the transitions that occur between possible words in our dictionary.
In the interest of performance, my implementation converts the dictionary into a <code>HashMap&lt;String, usize&gt;</code>.
Next, I go through the training text and match each word to it's index in the dictionary, effectively transforming the <code>String</code> into a <code>Vec&lt;usize&gt;</code>.
For example, the phrase, "passion fruit is not orange, cheese is orange," becomes, <code>[ 2, 1, 5, 4, 0, 3, 5, 0 ]</code>.
Next, the implementation iterates through each element in this vector, counting each transition. The counts are stored in another <code>HashMap</code> in the interest of performance, but is eventually converted into a matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span></span>. Each row is the output word's index, and the column is the input word's index.
For example, the transition <code>"fruit" (index 1) -&gt; "is" (index 5)</code> occurs exactly once, so we record <code>1</code> in column 1, row 5.</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>C</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">C = \begin{bmatrix} 
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\
    0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
    0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0
\end{bmatrix}</annotation></semantics></math></span></span></span></p><p>Not a very interesting matrix, is it?</p>
<p>Each element needs to be converted into a probability. Take the sum of each column:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><annotation encoding="application/x-tex">\begin{bmatrix} 
    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 2
\end{bmatrix}</annotation></semantics></math></span></span></span></p><p>Create a diagonal matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span></span> composed of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mtext>column&nbsp;sum</mtext></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{\text{column sum}}</annotation></semantics></math></span></span></p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>C</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.5</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.5</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">C = \begin{bmatrix} 
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0.5 \\
    0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.5 \\
    0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0
\end{bmatrix}</annotation></semantics></math></span></span></span></p><p>To finalize our Markov (a.k.a. transition) matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span>, we simply perform:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>M</mi><mo>=</mo><mi>D</mi><mi>C</mi></mrow><annotation encoding="application/x-tex">M = DC</annotation></semantics></math></span></span></span></p><h3>Using the transition matrix</h3>
<p>There are two possible situations: the user is in the process of typing, or they have finished their last word.
The latter is the easiest to implement.
Scan the user's text, and isolate the last word. Perform a lookup on the word list to identify it's index. Create a new vector containing <code>0</code>s except for that index, which should contain a <code>1</code>.
For example, if the last word was 'is',</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>s</mi><mo>⃗</mo></mover><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\vec{s} = \begin{bmatrix}
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}</annotation></semantics></math></span></span></span></p><p>Run it through our transition matrix:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>M</mi><mover accent="true"><mi>s</mi><mo>⃗</mo></mover><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.5</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0.5</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">M\vec{s} = \begin{bmatrix} 
  0.5 &amp; 0 &amp; 0 &amp; 0 &amp; 0.5 &amp; 0
\end{bmatrix}</annotation></semantics></math></span></span></span></p><p>Meaning the most probable next choices are at indices <code>0</code> and <code>4</code>, which correspond to "orange" and "not" respectively.
This is great for autocomplete. We can simply list the most probable options to the user.</p>
<h3>Text-Generation and Steady State</h3>
<p>It would be pretty neat if we could use this method to automagically generate text, right?</p>
<h4>The Naive Solution</h4>
<p>Each iteration, choose the most likely word from the set. Maybe randomize it a bit: choose a random word from the top 5 options.
Unfortunately, there is an issue. All Markov chains are guaranteed to converge on a specific probabilistic state given enough iterations. In order to get text generation to work unpredictably and without converging, we need something a bit more complex.</p>
<h4>My Solution</h4>
<p>Create a square diagonal matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span></span> with a side length equal to the length of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>s</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{s}</annotation></semantics></math></span></span>. Fill the diagonal elements with random numbers between <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span></span>. Then choose the word whose index corresponds with the highest value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mover accent="true"><mi>s</mi><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">R\vec{s}</annotation></semantics></math></span></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU chip goal 'unrealistic' says ASML CEO (112 pts)]]></title>
            <link>https://www.electronicsweekly.com/news/business/eu-chip-goal-totally-unrealistic-2024-01/</link>
            <guid>39213180</guid>
            <pubDate>Thu, 01 Feb 2024 06:11:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.electronicsweekly.com/news/business/eu-chip-goal-totally-unrealistic-2024-01/">https://www.electronicsweekly.com/news/business/eu-chip-goal-totally-unrealistic-2024-01/</a>, See on <a href="https://news.ycombinator.com/item?id=39213180">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					                                
					
<p>Investments in fab by TSMC, Bosch, NXP and Infineon are “good for the European car industry but it’s not enough. It’s not enough at all,” says Wennink, “if you want to get to 20%, you just have to calculate how much you need to build here.”</p>
<p><img decoding="async" loading="lazy" src="https://static.electronicsweekly.com/wp-content/uploads/sites/16/2014/08/Broadwell_14nm_Wafer_White_p-150x150.jpg" alt="EU chip goal ‘totally unrealistic’ says ASML CEO" width="150" height="150" srcset="https://static.electronicsweekly.com/wp-content/uploads/sites/16/2014/08/Broadwell_14nm_Wafer_White_p-150x150.jpg 150w, https://static.electronicsweekly.com/wp-content/uploads/sites/16/2014/08/Broadwell_14nm_Wafer_White_p-70x70.jpg 70w" sizes="(max-width: 150px) 100vw, 150px">In 2022, Kurt Sievers, CEO of NXP stated: “We have calculated that we would need €500 billion investment in Europe to reach the 20 percent market share goal.”</p>
<p>According to SEMI, China will have 8.6 million 8 inch equivalent wpm capacity this year, Taiwan will have 5.7 million wpm, Korea will have 5.1 million wpm, Japan will have 4.7 million wpm, the USA will have 3.1 million wpm, Europe will have 2.7 million wpm, and S.E.Asia will&nbsp;have 1.7 million wpm.</p>
<p>So Europe has roughly 8% of the industry measured by capacity and has roughly 8% of the market measured by value.</p>
<p>To up that to 20% in a world where capacity is currently growing at about 6% a year would need about a dozen new fabs to be built and in full volume production by 2030.</p>
                                    
										
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Marlon Brando Was a Secret Tech Geek and Photoshop Ace (2015) (146 pts)]]></title>
            <link>https://www.yahoo.com/entertainment/marlon-brando-was-a-secret-tech-geek-and-photoshop-124061824672.html</link>
            <guid>39213150</guid>
            <pubDate>Thu, 01 Feb 2024 06:04:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.yahoo.com/entertainment/marlon-brando-was-a-secret-tech-geek-and-photoshop-124061824672.html">https://www.yahoo.com/entertainment/marlon-brando-was-a-secret-tech-geek-and-photoshop-124061824672.html</a>, See on <a href="https://news.ycombinator.com/item?id=39213150">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Marlon Brando: Actor, activist… special effects tech geek?<br></p><p>Though the late star was considered by many to be the greatest actor of the 20th century, Brando always had a love-hate relationship with his craft, often remarking that it was a <a href="http://www.vanityfair.com/news/2005/03/brando200503" rel="nofollow noopener" target="_blank" data-ylk="slk:silly profession for a grown adult;elm:context_link;itc:0;sec:content-canvas">silly profession for a grown adult</a>. He was so frustrated by acting that, in the late 1980s, he embraced a new technology that promised to keep him on the big screen while simultaneously lightening his workload.</p><p>In the new documentary <i>Listen to Me Marlon</i>, director Stevan Riley stitches together intimate home footage that the actor took over the years and combines it with rare interviews to portray Brando as a complicated and volatile soul. But the actor still had a spark for life before his death at age 80 in 2004, and he was fascinated with visual effects, as you can see in this exclusive clip above from the documentary, which you can watch above. The footage was originally intended for an experimental film called <i>Software</i>, which Brando tried to make in the 1990s — more on that later — and features Brando, then in his seventies, delivering a monologue from <i>Macbeth</i>, his face rendered in blue pixels. <br></p><p>The technology in the clip, known as “digital equivalence,” involves creating a photo-realistic digital double of an actor that can look, move, and sound just like the original. It had long been a holy grail of the computer-graphics industry, and when Brando found out about its potential, he partnered with Scott Billups, a veteran cinematographer and graphics pioneer, in the pursuit of perfecting the technology.</p><p>“He dug technology — he was always had the latest system,” Billups, who contributed the footage to the new doc, told Yahoo Movies. “He was always upgraded. He had some new Macs before I did — and I was a Mac developer.”</p><p>The two met in the mid 1970s, but didn’t begin working together in earnest until the late 1980s, when Billups was conducting digital experiments with an actor who lived near Brando’s Hollywood estate (Billups declined to identify the thespian by name). While that neighbor was unnerved by the idea of creating a digital double, Brando eagerly volunteered to be the next test-subject.</p><p>“The friend was freaked, and thought it would kill acting and kill us all and put us out of business, but the test models worked, and you knew who was [on screen]. So Marlon was like, ‘I’m next, I’m next,’” Billups said. They worked closely with Stephen and Lloyd Addleman, brothers who <a href="https://books.google.com/books?id=xlP1uoxU1nEC&amp;pg=PA14&amp;lpg=PA14&amp;dq=Stephen+and+Lloyd+Addleman+cyberware&amp;source=bl&amp;ots=gEmbqlYhVv&amp;sig=f10haQ9tsaT8n3H14OQnqaXdfUY&amp;hl=en&amp;sa=X&amp;ved=0CDMQ6AEwBGoVChMI0YHO2tnYxgIViowNCh3jmgSw#v=onepage&amp;q=Stephen%20and%20Lloyd%20Addleman%20cyberware&amp;f=false" rel="nofollow noopener" target="_blank" data-ylk="slk:invented an early version of the 3D scanner hardware;elm:context_link;itc:0;sec:content-canvas">invented an early version of the 3D scanner hardware</a>, which Billups likened to an advanced supermarket grocery scanner and a spinning chair.</p><p>“The concept was for [the actor] to sit in this chair and get really still, and the scanner would go around you in 360 degrees [in] just under a minute,” Billups explained. “Sometimes the scanner would freeze up, so we’d spin the chair. You put somebody like Marlon in a chair and try to get an even spin on him — well, he’s really not the kind of guy who likes to sit still for long.”</p><figure><div><p><img alt="image" src="https://s.yimg.com/ny/api/res/1.2/4o8McHDwlF0pyFFynGH2yA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTcyMA--/http://40.media.tumblr.com/214f05b1313f97c892af69e1eacfa8bb/tumblr_inline_nrna90Gdb21t2ear7_540.jpg" data-src="https://s.yimg.com/ny/api/res/1.2/4o8McHDwlF0pyFFynGH2yA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTcyMA--/http://40.media.tumblr.com/214f05b1313f97c892af69e1eacfa8bb/tumblr_inline_nrna90Gdb21t2ear7_540.jpg"></p></div></figure><p><i>Brando, David Addleson and Billups in the late ‘80s (Stephen Addleson) </i></p><p>Billups and Brando wound up doing four different tests of the technology throughout the late ‘80s and early ‘90s; one such test focused on replacing the actor in a film he probably shouldn’t have made anyway. “He was doing <i>Island of Doctor Moreau</i>,” Billups remembered, “and he said he could do half of that script digitally.”</p><p>That 1996 sci-fi movie, which co-starred Val Kilmer, was a notorious bomb, but the digital equivalence tests did lead to the duo working on an even more off-beat project: An adaptation of Rudy Rucker’s <a href="http://www.amazon.com/Software-Rudy-Rucker/dp/0380701774" rel="nofollow noopener" target="_blank" data-ylk="slk:Philip K. Dick Award-winning cyberpunk novel Software;elm:context_link;itc:0;sec:content-canvas">Philip K. Dick Award-winning cyberpunk novel <i>Software</i></a>, on which Billups and Brando began pre-production in 1997.</p><p>They set up shop at a company called House of Moves in Santa Monica, building a motion capture studio where they hosted Brando’s A-list friends, including Faye Dunaway. (Dennis Hopper was also attached to the film early on.) “We were scanning them and doing mo-cap — we were going to do this whole movie with digital equivalents,” Billups said. “You could do a movie with a week’s worth of work instead of sitting on set bored out of their minds for a couple months.”</p><p>Billups’ version of <i>Software</i> ultimately fell apart due to disagreements with producers and financiers; it was awfully expensive to attempt to make a film with such an ambitious and risky technology.</p><p>In the photo above, you can see Brando sitting in the lab. Below, we have scans of some of the early tests that Billups and Brando did together; the actor was required to speak — with exaggerated expression —every vowel and syllable, so that they could later generate a fully digital character that made the same exact facial movements. The result is found in the video at the top of this story.</p><figure><div><p><img alt="image" src="https://s.yimg.com/ny/api/res/1.2/deXKktli0lzDktsXE.kCGw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTkyMQ--/http://40.media.tumblr.com/1a54a4f0aa630d8c7b90b9455b5cfe98/tumblr_inline_nrna90lH0f1t2ear7_540.jpg" data-src="https://s.yimg.com/ny/api/res/1.2/deXKktli0lzDktsXE.kCGw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTkyMQ--/http://40.media.tumblr.com/1a54a4f0aa630d8c7b90b9455b5cfe98/tumblr_inline_nrna90lH0f1t2ear7_540.jpg"></p></div></figure><p><i>Some early scans of Brando’s face</i></p><p>The collapse of <i>Software</i> didn’t sink the actor’s intense interest in technology; as the new century dawned, he pursued not just high-grade experimental electronics, but consumer-level tech, as well.<br></p><p>“Marlon was a closet geek. Well, not even in the closet — he was big into it,” Billups explained. “He was an ace with Photoshop. He would take people’s pictures and put them in places they never were. He’d say, ‘Remember when we were in so-and-so?’ and you’d see people trying to remember that situation.”</p><p>The trick to successfully confusing his friends, Billups explained, was being subtle about it. “He would maneuver it so wasn’t totally unbelievable; he would create an alternate reality,” Billups remembered. “He’d say, ‘I’ve got a picture of it right here, you’re looking at a picture of you at the Statue of Liberty.’ He would do this to all kinds of people, it was hilarious.”</p><p>Brando invested heavily in his hardware, always going after the latest computer systems — often owning them before Billups, who still makes his living in the visual effects industry. The actor was even an early user of CoSa After Effects, the post-production program used for green screen and other effects work, which was eventually sold to Adobe.</p><p>Billups remembers Brando first talking about potentially using motion capture and digital equivalence to perform his role as Jor-El in the original <i>Superman</i> movie, which came out in 1978. It’s taken almost 40 years for effects artists and engineers to realize the technology he was pursuing; this summer, audiences will see aged-down versions of Arnold Schwarzenegger and Michael Douglas in <i>Terminator Genisys</i> and <i>Ant-Man</i>, respectively. The tech isn’t perfect yet, but the facial scan used in those two blockbusters is a tremendous leap forward from where it was when Brando died in 2004.</p><p>In general, Brando would have loved how far technology has come, and it’s easy to imagine the actor being enamored with social media. Brando was a fan of the <i>Myst</i> PC game franchise, and was active in early forms of social media.</p><p>“He loved the anonymity of the web and ability to interact with people outside of his skin,” Billups recalled. “His online persona was that of Chinese exchange student and he handled it masterfully. Not in any skeezy way, but he could pull things out of people that might not have realized of themselves. He was quite masterful at this.”<br></p><p>His email address was just as curious: Brando went by the name BrainFlakes.</p><p>“He was the cleverest son of a bitch you’d ever met,” Billups added. “He was a really sweet guy.”</p><p><i>Listen to Me Marlon</i> will be released at New York’s Film Forum on July 29, LA’s Landmark Theater on July 31, and rollout nationwide after that. It will also air on Showtime in the fall.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The curious case of the disappearing Polish Ś (2015) (107 pts)]]></title>
            <link>https://medium.engineering/the-curious-case-of-disappearing-polish-s-fa398313d4df</link>
            <guid>39212833</guid>
            <pubDate>Thu, 01 Feb 2024 04:53:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.engineering/the-curious-case-of-disappearing-polish-s-fa398313d4df">https://medium.engineering/the-curious-case-of-disappearing-polish-s-fa398313d4df</a>, See on <a href="https://news.ycombinator.com/item?id=39212833">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><div><div><h2 id="3f03">One keyboard bug three decades in the making</h2><div><a href="https://mwichary.medium.com/?source=post_page-----fa398313d4df--------------------------------" rel="noopener follow"><div aria-hidden="false"><p><img alt="Marcin Wichary" src="https://miro.medium.com/v2/resize:fill:88:88/2*yemLwBKuCnBh79HADuLAGQ.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a><a href="https://medium.engineering/?source=post_page-----fa398313d4df--------------------------------" rel="noopener  ugc nofollow"><div aria-hidden="false"><p><img alt="Medium Engineering" src="https://miro.medium.com/v2/resize:fill:48:48/1*Q-Bif1DHR_y6Z7xR2m1yKg.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto"></p></div></a></div></div><p id="111f">A few weeks ago, someone reported this to us at Medium:</p><blockquote><p id="b704">“I just started an article in Polish. I can type in every letter, except Ś. When I press the key for Ś, the letter just doesn’t appear. It only happens on Medium.”</p></blockquote><p id="0e14">This was odd. We don’t really special-case any language in any way, and even if we did… out of 32 Polish characters, why would <a href="http://en.wikipedia.org/wiki/%C5%9A" rel="noopener ugc nofollow" target="_blank">this random one</a> be the only one causing problems?</p><p id="8b8e">Turns out, it wasn’t so random. This is a story of how four incidental ingredients spanning decades (if not centuries) came together to cause the most curious of bugs, and how we fixed it.</p></div><div><div><h2 id="8871">Ingredient 1 of 4:<br>Polish language</h2><p id="522f">Polish is the second most-used Slavic language, right after Russian, and before Ukrainian. Contrary to those two, however, and similar to Western European languages such as German or French, Polish uses the English/Latin alphabet with a few customizations.</p><p id="0578">This is the base English alphabet, largely the same as a classic Latin/Roman alphabet:</p></div><div><p id="3710">Original Polish words never contain Q, V or X, although we keep them for Latin and other borrowed words:</p></div><div><p id="775a">In exchange for those three, however, Polish adds nine additional diacritics using Latin characters as their base, all in relatively common use:</p></div></div><div><div><p id="203f">Starting in the early 20th century, typewriters needed to accommodate the extra 9 letters. If you compare an American typewriter with a Polish one:</p></div><div><p id="0ac7">…and look at the right side of the keyboard, you can see two of the diacritics — Ł and Ż — promoted to separate keys, and the rest sharing keys with other symbols. (Typists were encouraged to assemble uppercase versions of seven remaining letters by typing a Latin character, backspacing, and then overwriting an accent to “simulate” the proper letter. This was not uncommon during typewriter times.)</p><p id="ecf2">To find room for the extra letters, typewriters needed to dispense with some punctuation, most notably semicolons (comma + backspace + colon), and parentheses (replaced in common use by slashes).</p></div></div><div><div><h2 id="046c">Ingredient 2 of 4:<br>Communism</h2><p id="3e4f">For someone interested in the early personal computing in the 1980s, Communism in Poland meant a few things:</p><ul><li id="ead6">not a lot of disposable income,</li><li id="ba67">forbidden commercial importing of computers from the West (individual importing was still possible, assuming you had enough foreign currency and some means of acquiring it).</li></ul><p id="c94b">I grew up in Poland. My first computer — the glorious Atari 800XL—was an original 1979 technology, repackaged in 1983. We got it in 1986. Used.</p><p id="d229">This wasn’t uncommon. Technology was delayed on that side of the Iron Curtain; most computers were imported from the West. Prohibited commercial importing meant that for the longest time there was no commercial entity that could prepare computers for use in Poland. Foreign computers arrived with original instructions, untranslated software, and American keyboards like this one:</p></div><div><p id="783f">While France, Germany, and other countries got their early PCs with customized keyboards whose layouts mirrored closely the typewriters that came before…</p></div><div><figure></figure><figure><figcaption>Early IBM PC keyboards with French AZERTY and German QWERTZ layouts</figcaption></figure></div><div><p id="14cb">…in Poland, we had to find another way of inputting the extra 9 diacritics unique to our language.</p><p id="7fcd">Our extra characters might look very much like Latin equivalents, and amount to only about 8% of letter distribution (you will <a href="http://en.wikipedia.org/wiki/Scrabble_letter_distributions#Polish" rel="noopener ugc nofollow" target="_blank">hate them playing Scrabble</a>), but they are important. You can’t just swap them around. Consider these two similar words:</p><figure></figure><p id="fff6">Perfectly interchangeable, right? Well, not quite:</p><figure></figure><p id="22ea">There are more examples like that. As it happens, in those early PC days, I was happy that my full name, Marcin Kazimierz Wichary, did not come with any diacritics… and used to make fun of some of my less “lucky” friends. I was a mean nerd.</p><p id="9bae">Surely, there’s something that can be done, though? Back to the keyboard:</p></div><div><p id="1818">We cannot modify it in any way since that’d require messing with hardware, but we can still try to find a clever solution. There are two modifier keys — Ctrl (where today’s Caps Lock is), and Alt. Ctrl was already used as a common shortcut key, even before Ctrl+C and Ctrl+V became typical for copy and paste. But Alt was relatively uncommon. And thus, a <em>de facto</em> standard was born, assigning each of our diacritics to its Latin counterpart:</p></div><div><p id="7516">People started calling the older layout <em>typist’s keyboard layout, </em>and the new invention <em>programmer’s layout, </em>either because early PC users were mostly programmers, or because it preserved all the punctuation symbols that were often used in programming.</p><p id="45dc">The new layout was an ergonomic nightmare — look at how many of those letters are very close to the Left Alt and need to be pressed using the same hand — but it was easy to understand and did not require any expensive hardware modifications or even cheap ones (for example stickers). It stuck. A few other nearby countries (Romania, then-Czechoslovakia) came up with similar schemes.</p><p id="faf7">The setup was so successful that even when, a decade later, proper typist’s keyboards started appearing, practically no one would switch to them, mirroring the ascendancy of the inferior QWERTY some 80 years before.</p></div><div><figure></figure><figure><figcaption>Contemporary typist’s layout vs. contemporary programmer’s layout (or, American keyboard)</figcaption></figure></div></div><div><h2 id="a94a">Ingredient 3 of 4:<br>Old habits dying hard</h2><p id="2add">Autosaving, common today, needed to wait for the right moment. Especially in the 1980s, and even 1990s, saving your document was lengthy (powering up that floppy drive and writing to the disk took some time), would slowly wear out whatever medium you were using, and sometimes occupy CPU so much it couldn’t be used for anything else.</p><p id="b182">Saving by hand was then what backing up is today: a habit you needed to learn for your own good. The unlucky ones figured it out the hard way, writing for hours on a computer that had a tendency to crash cruelly and often, only to realize <em>they never saved what you were typing.</em></p><p id="630d">I was one of them. And we all learned to press Command+S or Ctrl+S whenever we paused for breath. Sometimes it was after every chapter. Often, every paragraph. Later on, even a sentence. Ctrl+S became a keystroke buried in people’s muscle memory, a habit that was an intrinsic part of writing.</p><figure></figure></div><div><div><p id="65c1">Then that habit turned on them. If you write in any Web-based editor, the default thing that happens after pressing the save key combination, is a browser window — giving you a completely useless option to save the current website’s HTML code, often announcing itself <em>via</em> a slow, annoying animation.</p><p id="cac9">The same was true for Medium, a long time ago:</p><figure></figure><p id="b8e6">However, at some point we added a little bit of code to our editor to make the save dialog go away:</p></div><div><p id="cd7d">It translates to: if S happens to be pressed with Command (<em>metaKey,</em> used by Macintoshes) or Ctrl (used by Windows or Linux PCs), first: prompt our editor to save what it’s doing (even though we’d autosave it soon anyway), and second: prevent anything else that would normally happen… in this case the annoying browser save dialog.</p><p id="ab9b">Which is the right thing to do. (You can still get to the save dialog from the menu, if you want.)</p></div></div><div><p id="7ac5">Now, this should ring all sorts of alarms in your head if you remember the bug report mentioned at the beginning:</p><blockquote><p id="97bc">“I just started an article in Polish. I can type in every letter, except Ś. When I press the key for Ś, the letter just doesn’t appear. It only happens on Medium.”</p></blockquote><p id="a3e8">Something doesn’t compute, though. Medium is blocking Command+S and Ctrl+S, but you get to Ś by keying in… Alt+S.</p><p id="4791">For these two worlds to collide, we need just one more ingredient.</p></div><div><div><h2 id="ca46">Ingredient 4 of 4:<br>Microsoft Windows</h2><p id="a774">Both Windows 3.x and 95 had terrific keyboard support. The menu items and dialogs had controls that could be accessed easily by mouse… but also much quicker by pressing Alt and the underlined letter:</p><figure></figure><p id="8110">Most of Microsoft Windows UI could be turned into a sequence of keyboard shortcuts, which was incredibly powerful (and something Mac could still learn from). In Poland, however, Alt was already commonly used to enter the extra diacritics. That presented a conflict.</p><p id="2f52">Fortunately, by then, keyboards evolved to have extra modifier keys, including a secondary Alt key on the other side of the spacebar.</p></div><div><p id="9843">So, what if we kept the Left Alt key for Windows shortcuts, but used the Right Alt key for Polish diacritics? That would solve all the problems (although the short-lived victory of better ergonomics through two modifier keys would be nullified again… this time one-handed Right Alt+O, Right Alt+L, and Right Alt+N being RSI fodder).</p></div><div><p id="1263">For reasons having to do with the older keyboards without the Right Alt (and more modern notebook keyboards that might skip that key for space reasons), Right Alt in Windows was internally mapped as a rarely-used combination of <a href="http://blogs.msdn.com/b/oldnewthing/archive/2004/03/29/101121.aspx" rel="noopener ugc nofollow" target="_blank">Ctrl and Alt pressed together</a>.</p><p id="9337">Which meant that, from the system perspective, all of the above shortcuts looked like this:</p></div><div><p id="e455">And there’s something oddly uncomfortable with this picture. That Ctrl+Alt+S is starting to look very familiar now, doesn’t it? In fact, <em>incredibly </em>familiar to Ctrl+S that we once started blocking to improve saving.</p></div></div><div><h2 id="e9ca">The fix</h2><p id="ba7b">Putting all of this together, the reason for the bug becomes obvious:</p><ol><li id="948b">Through historical circumstance, people in Poland use Right Alt+S to type in Ś.</li><li id="c701">Because of an obscure technical decision, Right Alt+S is mapped internally to Ctrl+Alt+S.</li><li id="e29a">Through habit, people might occasionally want to press Ctrl+S to save a document.</li><li id="da41">Medium was overriding whatever default action happened when you pressed Ctrl+S, thinking it’s just a browser save dialog it was canceling… and never realizing it could be something else: <em>a letter Ś.</em></li></ol><p id="c739">And so, typing Ś became impossible.</p></div><div><div><p id="9108">Once I figured it out, the patch was trivial. Instead of blindly blocking Ctrl+S, we could block Ctrl+S <em>only if Alt key was not pressed.</em></p></div><div><p id="b501">We fixed this last week, and it wasn’t a lot of work. After seeing the Medium bug reports filed in, and having remembered once asking to change what happens when you press Ctrl+S, I figured it out without much trouble.</p><p id="d9a0">But I happen to be Polish, and I went through all of the above — learning to touch type on my mom’s typewriter, owning early PCs with American keyboards, rewiring my muscle memory for the Left Alt first, and the Right Alt second… and in the same muscle memory, putting Ctrl+S early on, and then replacing it with Command+S as I switched to Macs.</p><p id="cab3">Much has changed in Poland since the 1980s. In 1989, Communism fell. Computers are now imported legally by individuals and companies alike. Today’s Windows and Mac machines in Poland are bought new, fully support Polish, and the operating systems themselves are properly localized. If you open up system preferences, though, you will still see the programmer’s layout there, and it’s still the most commonly used of the two.</p><figure></figure></div></div><div><p id="746d">The few lines of code above might be, to me, one of the most curious in Medium’s codebase, the result of an arbitrary set of circumstances… today copiously commented so that people coming after me understand the one opaque conditional statement.</p><p id="63d7">As we see Medium being used in more and more countries, we will be dealing with those kind of thick, arbitrary internationalization and localization issues over and over again. With a big exception, though: no one will be there to give us a backstory, to unfold the opaque, to explain any of this to us the way I could explain the above.</p><p id="3a32">This bug is a harbinger of very scary, and very exciting times.</p></div><div><p id="e25a"><em>If you’ve read so far, maybe you want to help us out here at Medium? We’re looking for ambassadors in different countries — people knowing their language, caring about typography, wanting stories on Medium to look and behave most properly for where they are.</em></p><p id="5fb2"><em>Let us know at </em><a href="mailto:languages@medium.com" rel="noopener" target="_blank"><em>languages@medium.com</em></a><em>. Thank you!</em></p><p id="ec91"><em>Also, thanks to Michal Cierniak, Sommer Panage, Jamie Talbot, and Dan Pupius for reviewing this article.</em></p></div></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Lockbox: forward proxy for making third party API calls (120 pts)]]></title>
            <link>https://github.com/mkjt2/lockbox</link>
            <guid>39211857</guid>
            <pubDate>Thu, 01 Feb 2024 01:26:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mkjt2/lockbox">https://github.com/mkjt2/lockbox</a>, See on <a href="https://news.ycombinator.com/item?id=39211857">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:mkjt2/lockbox" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="EOO0ylnXEFjsZr9K3F_V5PBjpz5xD8MVMbkriUntE_HlXJ8wB5woh5jR8FOpnl7EkTLZAL1PeqZ7lxYgR4l75w" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="mkjt2/lockbox" data-current-org="" data-current-owner="mkjt2" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=sgPRTMgz8ku95KMqhkt%2Bc5eH7iEFcXSQxOL162YxiBpQ1kQrDjn3vBito38AHpDzRdv%2BfTPg5%2BaBLmRF0Pcdx1ZJClGV%2F%2BuaF2H7f6%2F7zqTci5qJgiHlNZ%2Fa4kplETiyAkg4d4c6G5ryHzqxAuzhzVz4loJTZl89EmFrvg%2Bs%2BVBmZrCIxGqvmrfiBMhE40Xbde1B9tvTWvm%2BhGWJfR2itHPAB2NtRP2ml%2FXGDGDOXtfz0fHZgo%2BC%2Bs61usHR1tBxFahTfOdkSGvGYK0%2F2N0xWZM0GHAt6dicoIASJxGCrCRWhsaveA4vUZtdHxuqAuX6%2Bw%2Bp5JKh2VqJoErsWSESzW0BD9bCSmMgBP6fFF38Kz5DflH94796roj%2FcpGZDqLbwKbg1kTfIMhW9IGrXNtrhgBxm5LdCB8loKrQtHF6PERztP%2BfLO%2FLvccQDo16WNaRpzpzBy7zJDRAU2%2B7m3SyaT7Kk9eXiWO%2BXXcMAHd%2FGZBPZFER92%2FMjIkXTN%2BZcO9e%2Fuzq6mwJ62stPw%3D%3D--SRlzcLraTcY3Xf9d--UDI9lK0l9MZklwNbxKg2hw%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=mkjt2%2Flockbox" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/mkjt2/lockbox&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="99513d7c9ca4e79466e8e86a7454ba13acb003d591c1d874fb5a07f69253f529" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making a PDF that's larger than Germany (653 pts)]]></title>
            <link>https://alexwlchan.net/2024/big-pdf/</link>
            <guid>39210507</guid>
            <pubDate>Wed, 31 Jan 2024 22:47:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alexwlchan.net/2024/big-pdf/">https://alexwlchan.net/2024/big-pdf/</a>, See on <a href="https://news.ycombinator.com/item?id=39210507">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" tabindex="-1"> <article>   <p>I was browsing social media this morning, and I saw a claim I’ve seen go past a few times now – that there’s a maximum size for a PDF document:</p>  <p>Some version of this has been floating around the Internet <a href="https://twitter.com/hassmanm/status/303086542">since 2007</a>, probably earlier. This tweet is pretty emblematic of posts about this claim: it’s stated as pure fact, with no supporting evidence or explanation. We’re meant to just accept that a single PDF can only cover about half the area of Germany, and we’re not given any reason why 381&nbsp;kilometres is the magic limit.</p> <p>I started wondering: has anybody made a PDF this big? How hard would it be? Can you make a PDF that’s even bigger?</p> <p>A few years ago I did some <a href="https://codegolf.stackexchange.com/a/48938/13285">silly noodling into PostScript</a>, the precursor to PDF, and it was a lot of fun. I’ve never actually dived into the internals of PDF, and this seems like a good opportunity.</p> <p>Let’s dig in.</p> <h2 id="where-does-the-claim-come-from">Where does the claim come from?</h2> <p>These posts are often accompanied by a “well, actually” where people in the replies explain this is a limitation of a particular PDF reader app, not a limitation of PDF itself. They usually link to something like <a href="https://en.wikipedia.org/wiki/PDF#:~:text=Page%20dimensions%20are%20not%20limited%20by%20the%20format%20itself">the Wikipedia article for PDF</a>, which explains:</p> <blockquote> <p>Page dimensions are not limited by the format itself. However, Adobe Acrobat imposes a limit of 15&nbsp;million by 15&nbsp;million inches, or 225 trillion in<sup>2</sup> (145,161 km<sup>2</sup>).<a href="https://en.wikipedia.org/wiki/PDF#cite_note-pdf-ref-1.7-2"><sup>[2]</sup></a></p> </blockquote> <p>If you follow the reference link, you find the <a href="https://web.archive.org/web/20081001170454/https://www.adobe.com/devnet/acrobat/pdfs/pdf_reference_1-7.pdf">specification for PDF&nbsp;1.7</a>, where an appendix item explains in more detail (emphasis mine):</p> <blockquote> <p>In PDF versions earlier than PDF&nbsp;1.6, the size of the default user space unit is fixed at 1/72&nbsp;inch. In Acrobat viewers earlier than version 4.0, the minimum allowed page size is 72 by 72&nbsp;units in default user space (1 by 1&nbsp;inch); the maximum is 3240 by 3240&nbsp;units (45 by 45&nbsp;inches). In Acrobat versions 5.0 and later, the minimum allowed page size is 3 by 3&nbsp;units (approximately 0.04 by 0.04&nbsp;inch); the maximum is 14,400 by 14,400&nbsp;units (200 by 200&nbsp;inches).</p> <p>Beginning with PDF&nbsp;1.6, the size of the default user space unit may be set with the UserUnit entry of the page dictionary. <strong>Acrobat 7.0 supports a maximum UserUnit value of 75,000, which gives a maximum page dimension of 15,000,000&nbsp;inches (14,400 * 75,000 * 1 ⁄ 72).</strong> The minimum UserUnit value is 1.0 (the default).</p> </blockquote> <p>15&nbsp;million inches is exactly 381&nbsp;kilometres, matching the number in the original tweet. And although this limit first appeared in PDF&nbsp;1.6, it’s “version 7” of Adobe Acrobat. This is probably where the original claim comes from.</p> <p>What if we make a PDF that exceeds these “maximum” values?</p> <h2 id="the-inner-structure-of-pdfs">The inner structure of PDFs</h2> <p>I’ve never dived into the internals of a PDF document – I’ve occasionally glimpsed some bits in a hex editor, but I’ve never really understood how they work. If I’m going to be futzing around for fun, this is a good opportunity to learn how to edit the PDF directly, rather than going through a library.</p> <p>I found <a href="https://help.callassoftware.com/a/798383-how-to-create-a-simple-pdf-file">a good article</a> which explains the internal structure of a PDF, and combined with asking ChatGPT a few questions, I was able to get enough to write some simple files by hand.</p> <p>I know that PDFs support a huge number of features, so this is probably a gross oversimplification, but this is the mental picture I created:</p> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 280 560" role="img" aria-labelledby="svg_pdf_layout" style="width: 300px;"> <defs> <marker id="arrowhead" markerWidth="7" markerHeight="4.9" refX="0" refY="2.45" orient="auto"> <polygon points="0 0, 7 2.45, 0 4.9"></polygon> </marker> </defs> <path d="M 230 470 L 270 470 L 270 350 L 250 350" marker-end="url(#arrowhead)"></path> <rect x="50" y="10" width="180" height="40"></rect> <text x="140" y="30">%PDF-1.6</text> <rect x="50" y="70" width="180" height="240"></rect> <text x="140" y="85" text-anchor="middle" dominant-baseline="middle" font-size="10pt">objects</text> <rect x="80" y="100" width="120" height="40"></rect> <text x="140" y="120">object 1</text> <rect x="80" y="150" width="120" height="40"></rect> <text x="140" y="170">object 2</text> <path d="M 140 195 L 140 245"></path> <rect x="80" y="250" width="120" height="40"></rect> <text x="140" y="270">object N</text> <path d="M 50 350 L 10 350 L 10 270 L 62 270" marker-end="url(#arrowhead)"></path> <path d="M 50 350 L 10 350 L 10 220 L 62 220" marker-end="url(#arrowhead)"></path> <path d="M 50 350 L 10 350 L 10 170 L 62 170" marker-end="url(#arrowhead)"></path> <path d="M 50 350 L 10 350 L 10 120 L 62 120" marker-end="url(#arrowhead)"></path> <rect x="50" y="330" width="180" height="40"></rect> <text x="140" y="350">xref</text> <rect x="50" y="390" width="180" height="40"></rect> <text x="140" y="410">trailer</text> <rect x="50" y="450" width="180" height="40"></rect> <text x="140" y="470">startxref</text> <rect x="50" y="510" width="180" height="40"></rect> <text x="140" y="530">%%EOF</text> <title id="svg_pdf_layout">tbc</title></svg> <p>The start and end of a PDF file are always the same: a version number (<code>%PDF-1.6</code>) and an end-of-file marker (<code>%%EOF</code>).</p> <p>After the version number comes a long list of objects. There are lots of types of objects, for all the various things you can find in a PDF, including the pages, the text, and the graphics.</p> <p>After that list comes the <code>xref</code> or cross-reference table, which is a lookup table for the objects. It points to all the objects in the file: it tells you that object 1 is 10&nbsp;bytes after the start, object 2 is after 20&nbsp;bytes, object 3 is after 30&nbsp;bytes, and so on. By looking at this table, a PDF reading app knows how many objects there are in the file, and where to find them.</p> <p>The <code>trailer</code> contains some metadata about the overall document, like the number of pages and whether it’s encrypted.</p> <p>Finally, the <code>startxref</code> value is a pointer to the start of the <code>xref</code> table. This is where a PDF reading app starts: it works from the end of the file until it finds the <code>startxref</code> value, then it can go and read the <code>xref</code> table and learn about all the objects.</p> <p>With this knowledge, I was able to write my first PDF by hand. If you save this code into a file named <code>myexample.pdf</code>, it should open and show a page with a red square in a PDF reading app:</p><pre><code><span>%PDF-1.6

% The first object.  The start of every object is marked by:
%
%     &lt;object number&gt; &lt;generation number&gt; obj
%
% (The generation number is used for versioning, and is usually 0.)
%
% This is object 1, so it starts as `1 0 obj`.  The second object will
% start with `2 0 obj`, then `3 0 obj`, and so on.  The end of each object
% is marked by `endobj`.
%
% This is a "stream" object that draws a shape.  First I specify the
% length of the stream (54&nbsp;bytes).  Then I select a colour as an
% RGB value (`1 0 0 RG` = red), then I set a line width (`5 w`) and
% finally I give it a series of coordinates for drawing the square:
%
%     (100, 100) ----&gt; (200, 100)
%                          |
%     [s = start]          |
%         ^                |
%         |                |
%         |                v
%     (100, 200) &lt;---- (200, 200)
%</span>
1 0 obj
&lt;&lt;
	/Length 54
&gt;&gt;
stream
1 0 0 RG
5 w
100 100 m
200 100 l
200 200 l
100 200 l
s
endstream
endobj

<span>% The second object.
%
% This is a "Page" object that defines a single page.  It contains a
% single object: object 1, the red square.  This is the line `1 0 R`.
%
% The "R" means "Reference", and `1 0 R` is saying "look at object number 1
% with generation number 0" -- and object 1 is the red square.
%
% It also points to a "Pages" object that contains the information about
% all the pages in the PDF -- this is the reference `3 0 R`.</span>
2 0 obj
&lt;&lt;
	/Type /Page
	/Parent 3 0 R
	/MediaBox [0 0 300 300]
	/Contents 1 0 R
&gt;&gt;
endobj

<span>% The third object.
%
% This is a "Pages" object that contains information about the different
% pages.  The `2 0 R` is reference to the "Page" object, defined above.</span>
3 0 obj
&lt;&lt;
	/Type /Pages
	/Kids [2 0 R ]
	/Count 1
&gt;&gt;
endobj

<span>% The fourth object.
%
% This is a "Catalog" object that provides the main structure of the PDF.
% It points to a "Pages" object that contains information about the
% different pages -- this is the reference `3 0 R`.</span>
4 0 obj
&lt;&lt;
	/Type /Catalog
	/Pages 3 0 R
&gt;&gt;
endobj

<span>% The xref table.  This is a lookup table for all the objects.
%
% I'm not entirely sure what the first entry is for, but it seems to be
% important.  The remaining entries correspond to the objects I created.</span>
xref
0 4
0000000000 65535 f
0000000851 00000 n
0000001396 00000 n
0000001655 00000 n
0000001934 00000 n

<span>% The trailer.  This contains some metadata about the PDF.  Here there
% are two entries, which tell us that:
%
%   - There are 4 entries in the `xref` table.
%   - The root of the document is object 4 (the "Catalog" object)
%</span>
trailer
&lt;&lt;
	/Size 4
	/Root 4 0 R
&gt;&gt;

<span>% The startxref marker tells us that we can find the xref table 2196&nbsp;bytes
% after the start of the file.</span>
startxref
2196

<span>% The end-of-file marker.
%%EOF</span></code></pre><p>I played with this file for a while, just doing simple things like adding extra shapes, changing how the shapes appeared, and putting different shapes on different pages. I tried for a while to get text working, but that was a bit beyond me.</p> <p>It quickly became apparent why nobody writes PDFs by hand – it got very fiddly to redo all the lookup tables! But I’m glad I did it; manipulating all the PDF objects and their references really helped me feel like I understand the basic model of PDFs. I opened some “real” PDFs created by other apps, and they have many more objects and types of object – but now I could at least follow some of what’s going on.</p> <p>With this newfound ability to edit PDFs by hand, how can I create monstrously big ones?</p> <h2 id="changing-the-page-size-mediabox-and-userunit">Changing the page size: /MediaBox and /UserUnit</h2> <p>Within a PDF, the size of each page is set on the individual “Page” objects – this allows different pages to be different sizes. We’ve already seen this once:</p><pre><code>&lt;&lt;
	/Type /Page
	/Parent 3 0 R
	<span>/MediaBox [0 0 300 300]</span>
	/Contents 1 0 R
&gt;&gt;</code></pre><p>Here, the <code>MediaBox</code> is setting the width and height of the page – in this case, a square of 300 × 300&nbsp;units. The default unit size is 1/72&nbsp;inch, so the page is 300 × 72 = 4.17&nbsp;inches. And indeed, if I open this PDF in Adobe Acrobat, that’s what it reports:</p> <picture> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_1x.avif 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_2x.avif 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/avif" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_1x.webp 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_2x.webp 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/webp" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_1x.png 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_2x.png 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/png" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_1x.avif 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_2x.avif 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/avif"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_1x.webp 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_2x.webp 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/webp"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_1x.png 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_2x.png 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/png"> <img src="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_1x.png" alt="Screenshot of Acrobat’s ‘Document Properties’ panel, showing the page size of 4.17 x 4.17 in." width="724"> </picture> <p>By changing the <code>MediaBox</code> value, we can make the page bigger. For example, if we change the value to <code>600 600</code>, Acrobat says it’s now <code>8.33 x 8.33 in</code>. Nice!</p> <p>We can increase it all the way to <code>14400 14400</code>, the max allowed by Acrobat, and then it says the page is now <code>200.00 x 200.00in</code>. (You <a href="https://alexwlchan.net/images/2024/acrobat-error.png">get a warning</a> if you try to push past that limit.)</p> <p>But 200&nbsp;inches is far short of 381&nbsp;kilometres – and that’s because we’re using the default unit of 1/72&nbsp;inch. We can increase the unit size by adding a <code>/UserUnit</code> value. For exaple, setting the value to 2 will double the page in both dimensions:</p><pre><code>&lt;&lt;
	/Type /Page
	/Parent 3 0 R
	<span>/MediaBox [0 0 14400 14400]</span>
	<span>/UserUnit 2</span>
	/Contents 1 0 R
&gt;&gt;</code></pre><p>And now Acrobat reports the size of the page as <code>400.00 x 400.00 in</code>.</p> <p>If we crank it all the way up to the maximum of <code>UserUnit 75000</code>, Acrobat now reports the size of our page as <code>15,000,000,000.00 x 15,000,000,000.00 in</code> – 381 km along both sides, matching the original claim. If you’re curious, you can <a href="https://alexwlchan.net/files/2024/biggest.pdf">download the PDF</a>.</p> <p>If you try to create a page with a larger size, either by increasing the <code>MediaBox</code> or <code>UserUnit</code> values, Acrobat just ignores it. It keeps saying that the size of a page is 15 billion inches, even if the page metadata says it’s higher. (And if you increase the <code>UserUnit</code> past <code>75000</code>, this happens silently – there’s no warning or error to suggest the size of the page is being capped.)</p> <p>This probably isn’t an issue – I don’t think the <code>UserUnit</code> value is widely used in practice. I found <a href="https://stackoverflow.com/a/59927201/1558022">one Stack Overflow answer</a> saying as such, and I couldn’t find any examples of it online. The builtin macOS Preview.app doesn’t even support it – it completely ignores the value, and treats all PDFs as if the unit size is 1/72&nbsp;inch.</p> <p>But unlike Acrobat, the Preview app doesn’t have an upper limit on what we can put in <code>MediaBox</code>. It’s perfectly happy for me to write a width which is a 1 followed by twelve 0s:</p> <picture> <source srcset="https://alexwlchan.net/images/2024/preview-megawide.dark_1x.avif 366w, https://alexwlchan.net/images/2024/preview-megawide.dark_2x.avif 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/avif" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide.dark_1x.webp 366w, https://alexwlchan.net/images/2024/preview-megawide.dark_2x.webp 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/webp" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide.dark_1x.png 366w, https://alexwlchan.net/images/2024/preview-megawide.dark_2x.png 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/png" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide_1x.avif 366w, https://alexwlchan.net/images/2024/preview-megawide_2x.avif 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/avif"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide_1x.webp 366w, https://alexwlchan.net/images/2024/preview-megawide_2x.webp 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/webp"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide_1x.png 366w, https://alexwlchan.net/images/2024/preview-megawide_2x.png 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/png"> <img src="https://alexwlchan.net/images/2024/preview-megawide_1x.png" alt="Screenshot of Preview’s Document inspector, showing the page size of 352777777777.78 x 10.59 cm." width="366"> </picture> <p>If you’re curious, that width is approximately the distance between the Earth and the Moon. I’d have to get my ruler to check, but I’m pretty sure that’s larger than Germany.</p> <p>I could keep going. And I did. Eventually I ended up with a PDF that Preview claimed is larger than the entire universe – approximately 37 trillion light years square. Admittedly it’s mostly empty space, but so is the universe. If you’d like to play with that PDF, you can <a href="https://alexwlchan.net/files/2024/universe.pdf">get it here</a>.</p> <p>Please don’t try to print it.</p> </article> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MobileDiffusion: Rapid text-to-image generation on-device (246 pts)]]></title>
            <link>https://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html</link>
            <guid>39210458</guid>
            <pubDate>Wed, 31 Jan 2024 22:42:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html">https://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html</a>, See on <a href="https://news.ycombinator.com/item?id=39210458">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-5966553114967673984">
<p><span>Posted by Yang Zhao, Senior Software Engineer, and Tingbo Hou, Senior Staff Software Engineer, Core ML</span>

</p><p>
Text-to-image <a href="https://arxiv.org/abs/2006.11239">diffusion models</a> have shown exceptional capabilities in generating high-quality images from text prompts. However, leading models feature billions of parameters and are consequently expensive to run, requiring powerful desktops or servers (e.g., <a href="https://stability.ai/news/stable-diffusion-public-release">Stable Diffusion</a>, <a href="https://openai.com/research/dall-e">DALL·E</a>, and <a href="https://imagen.research.google/">Imagen</a>). While recent advancements in inference solutions on <a href="https://blog.research.google/2023/06/speed-is-all-you-need-on-device.html">Android</a> via MediaPipe and <a href="https://github.com/apple/ml-stable-diffusion">iOS</a> via Core ML have been made in the past year, rapid (sub-second) text-to-image generation on mobile devices has remained out of reach.
</p> <p>
To that end, in “<a href="https://arxiv.org/abs/2311.16567">MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices</a>”, we introduce a novel approach with the potential for rapid text-to-image generation on-device. MobileDiffusion is an efficient latent diffusion model specifically designed for mobile devices. We also adopt <a href="https://arxiv.org/abs/2311.09257">DiffusionGAN</a> to achieve one-step sampling during inference, which fine-tunes a pre-trained diffusion model while leveraging a GAN to model the denoising step. We have tested MobileDiffusion on iOS and Android premium devices, and it can run in half a second to generate a 512x512 high-quality image. Its comparably small model size of just 520M parameters makes it uniquely suited for mobile deployment.
</p>




<p>Rapid text-to-image generation on-device.</p>





<h2>Background</h2>


<p>
The relative inefficiency of text-to-image diffusion models arises from two primary challenges. First, the inherent design of diffusion models requires <a href="https://blog.research.google/2023/06/on-device-diffusion-plugins-for.html">iterative denoising</a> to generate images, necessitating multiple evaluations of the model. Second, the complexity of the network architecture in text-to-image diffusion models involves a substantial number of parameters, regularly reaching into the billions and resulting in computationally expensive evaluations. As a result, despite the potential benefits of deploying generative models on mobile devices, such as enhancing user experience and addressing emerging privacy concerns, it remains relatively unexplored within the current literature.
</p>
<p>
The optimization of inference efficiency in text-to-image diffusion models has been an active research area. Previous studies predominantly concentrate on addressing the first challenge, seeking to reduce the number of function evaluations (NFEs). Leveraging advanced numerical solvers (e.g., <a href="https://arxiv.org/abs/2206.00927">DPM</a>) or distillation techniques (e.g., <a href="https://arxiv.org/abs/2202.00512">progressive distillation</a>, <a href="https://arxiv.org/abs/2303.01469">consistency distillation</a>), the number of necessary sampling steps have significantly reduced from several hundreds to single digits. Some recent techniques, like <a href="https://arxiv.org/abs/2311.09257">DiffusionGAN</a> and <a href="https://arxiv.org/abs/2311.17042#:~:text=We%20introduce%20Adversarial%20Diffusion%20Distillation,while%20maintaining%20high%20image%20quality.">Adversarial Diffusion Distillation</a>, even reduce to a single necessary step. 
</p>
<p>
However, on mobile devices, even a small number of evaluation steps can be slow due to the complexity of model architecture. Thus far, the architectural efficiency of text-to-image diffusion models has received comparatively less attention. A handful of earlier works briefly touches upon this matter, involving the removal of redundant neural network blocks (e.g., <a href="https://snap-research.github.io/SnapFusion/">SnapFusion</a>). However, these efforts lack a comprehensive analysis of each component within the model architecture, thereby falling short of providing a holistic guide for designing highly efficient architectures.
</p>





<h2>MobileDiffusion</h2>


<p>
Effectively overcoming the challenges imposed by the limited computational power of mobile devices requires an in-depth and holistic exploration of the model's architectural efficiency. In pursuit of this objective, our research undertakes a detailed examination of each constituent and computational operation within Stable Diffusion’s <a href="https://arxiv.org/abs/2112.10752">UNet architecture</a>. We present a comprehensive guide for crafting highly efficient text-to-image diffusion models culminating in the MobileDiffusion.
</p>
<p>
The design of MobileDiffusion follows that of <a href="https://arxiv.org/abs/2112.10752">latent diffusion models</a>. It contains three components: a text encoder, a diffusion UNet, and an image decoder. For the text encoder, we use <a href="https://arxiv.org/abs/2103.00020">CLIP-ViT/L14</a>, which is a small model (125M parameters) suitable for mobile. We then turn our focus to the diffusion UNet and image decoder. 
</p>




<h3>Diffusion UNet</h3>


<p>
As illustrated in the figure below, diffusion UNets commonly interleave transformer blocks and convolution blocks. We conduct a comprehensive investigation of these two fundamental building blocks. Throughout the study, we control the training pipeline (e.g., data, optimizer) to study the effects of different architectures.
</p>
<p>
In classic text-to-image diffusion models, a transformer block consists of a self-attention layer (SA) for modeling long-range dependencies among visual features, a cross-attention layer (CA) to capture interactions between text conditioning and visual features, and a feed-forward layer (FF) to post-process the output of attention layers. These transformer blocks hold a pivotal role in text-to-image diffusion models, serving as the primary components responsible for text comprehension. However, they also pose a significant efficiency challenge, given the computational expense of the attention operation, which is quadratic to the sequence length. We follow the idea of <a href="https://arxiv.org/abs/2301.11093">UViT</a> architecture, which places more transformer blocks at the bottleneck of the UNet. This design choice is motivated by the fact that the attention computation is less resource-intensive at the bottleneck due to its lower dimensionality. 
</p>





<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV/s915/image4.png"><img data-original-height="249" data-original-width="915" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV/s16000/image4.png"></a></td></tr><tr><td>Our UNet architecture incorporates more transformers in the middle, and skips self-attention (SA) layers at higher resolutions.</td></tr></tbody></table>




<p>
Convolution blocks, in particular <a href="https://arxiv.org/abs/1512.03385">ResNet</a> blocks, are deployed at each level of the UNet. While these blocks are instrumental for feature extraction and information flow, the associated computational costs, especially at high-resolution levels, can be substantial. One proven approach in this context is <a href="https://arxiv.org/abs/1704.04861">separable convolution</a>. We observed that replacing regular convolution layers with lightweight separable convolution layers in the deeper segments of the UNet yields similar performance.
</p>
<p>
In the figure below, we compare the UNets of several diffusion models. Our MobileDiffusion exhibits superior efficiency in terms of <a href="https://arxiv.org/pdf/2110.12894.pdf">FLOPs</a> (floating-point operations) and number of parameters. 
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s1200/image3.png"><img data-original-height="742" data-original-width="1200" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s16000/image3.png"></a></td></tr><tr><td>Comparison of some diffusion UNets.</td></tr></tbody></table>





<h3>Image decoder</h3>


<p>
In addition to the UNet, we also optimized the image decoder. We trained a <a href="https://arxiv.org/abs/2012.03715">variational autoencoder</a> (VAE) to encode an <a href="https://en.wikipedia.org/wiki/RGB_color_model">RGB</a> image to an 8-channel latent variable, with 8× smaller spatial size of the image. A latent variable can be decoded to an image and gets 8× larger in size.  To further enhance efficiency, we design a lightweight decoder architecture by pruning the original’s width and depth. The resulting lightweight decoder leads to a significant performance boost, with nearly 50% latency improvement and better quality. For more details, please refer to our <a href="https://arxiv.org/abs/2311.16567">paper</a>.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s1124/image6.png"><img data-original-height="789" data-original-width="1124" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s16000/image6.png"></a></td></tr><tr><td>VAE reconstruction. Our VAE decoders have better visual quality than SD (Stable Diffusion).</td></tr></tbody></table>



<br>

<table>
  <tbody><tr>
   <td><b>Decoder</b>
   </td>
   <td><b>&nbsp;&nbsp;#Params (M)&nbsp;&nbsp;</b>
   </td>
   <td><b>&nbsp;&nbsp;PSNR↑&nbsp;&nbsp;</b>
   </td>
   <td><b>&nbsp;&nbsp;SSIM↑&nbsp;&nbsp;</b>
   </td>
   <td><b>&nbsp;&nbsp;LPIPS↓&nbsp;&nbsp;</b>
   </td>
  </tr>
  <tr>
   <td><b>SD</b>
   </td>
   <td>49.5
   </td>
   <td>26.7
   </td>
   <td>0.76
   </td>
   <td>0.037
   </td>
  </tr>
  <tr>
   <td><b>Ours</b>
   </td>
   <td>39.3
   </td>
   <td>30.0
   </td>
   <td>0.83
   </td>
   <td>0.032
   </td>
  </tr>
  <tr>
   <td><b>Ours-Lite&nbsp;&nbsp;&nbsp;&nbsp;</b>
   </td>
   <td>9.8
   </td>
   <td>30.2
   </td>
   <td>0.84
   </td>
   <td>0.032
   </td>
  </tr>
</tbody></table>
<br>





<h3>One-step sampling</h3>


<p>
In addition to optimizing the model architecture, we adopt a <a href="https://arxiv.org/abs/2311.09257">DiffusionGAN hybrid</a> to achieve one-step sampling. Training DiffusionGAN hybrid models for text-to-image generation encounters several intricacies. Notably, the discriminator, a classifier distinguishing real data and generated data, must make judgments based on both texture and semantics. Moreover, the cost of training text-to-image models can be extremely high, particularly in the case of GAN-based models, where the discriminator introduces additional parameters. Purely GAN-based text-to-image models (e.g., <a href="https://arxiv.org/abs/2301.09515">StyleGAN-T</a>, <a href="https://arxiv.org/abs/2303.05511">GigaGAN</a>) confront similar complexities, resulting in highly intricate and expensive training.
</p>
<p>
To overcome these challenges, we use a pre-trained diffusion UNet to initialize the generator and discriminator. This design enables seamless initialization with the pre-trained diffusion model. We postulate that the internal features within the diffusion model contain rich information of the intricate interplay between textual and visual data. This initialization strategy significantly streamlines the training.
</p>
<p>
The figure below illustrates the training procedure. After initialization, a noisy image is sent to the generator for one-step diffusion. The result is evaluated against ground truth with a reconstruction loss, similar to diffusion model training. We then add noise to the output and send it to the discriminator, whose result is evaluated with a GAN loss, effectively adopting the GAN to model a denoising step. By using pre-trained weights to initialize the generator and the discriminator, the training becomes a fine-tuning process, which converges in less than 10K iterations.  
</p>



<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ-UILKw/s960/image7.jpg"><img data-original-height="576" data-original-width="960" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ-UILKw/s16000/image7.jpg"></a></td></tr><tr><td>Illustration of DiffusionGAN fine-tuning.</td></tr></tbody></table>




<h2>Results</h2>


<p>
Below we show example images generated by our MobileDiffusion with DiffusionGAN one-step sampling. With such a compact model (520M parameters in total), MobileDiffusion can generate high-quality diverse images for various domains.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s1728/image1.png"><img data-original-height="1296" data-original-width="1728" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s16000/image1.png"></a></td></tr><tr><td>Images generated by our MobileDiffusion</td></tr></tbody></table>



<p>
We measured the performance of our MobileDiffusion on both iOS and Android devices, using different runtime optimizers. The latency numbers are reported below. We see that MobileDiffusion is very efficient and can run within half a second to generate a 512x512 image. This lightning speed potentially enables many interesting use cases on mobile devices.
</p>



<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s1184/image8.png" imageanchor="1"><img data-original-height="742" data-original-width="1184" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s16000/image8.png"></a></td></tr><tr><td>Latency measurements (<b>s</b>) on mobile devices.</td></tr></tbody></table>




<h2>Conclusion</h2>


<p>
With superior efficiency in terms of latency and size, MobileDiffusion has the potential to be a very friendly option for mobile deployments given its capability to enable a rapid image generation experience while typing text prompts. And we will ensure any application of this technology will be in-line with Google’s <a href="https://ai.google/responsibility/responsible-ai-practices/">responsible AI practices</a>.
</p>




<h2>Acknowledgments</h2>


<p>
<em>We like to thank our collaborators and contributors that helped bring MobileDiffusion to on-device: Zhisheng Xiao, Yanwu Xu, Jiuqiang Tang, Haolin Jia, Lutz Justen, Daniel Fenner, Ronald Wotzlaw, Jianing Wei, Raman Sarokin, Juhyun Lee, Andrei Kulik, Chuo-Ling Chang, and Matthias Grundmann.</em>
</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Visual Analysis of Binary Files (203 pts)]]></title>
            <link>http://binvis.io/#/</link>
            <guid>39210436</guid>
            <pubDate>Wed, 31 Jan 2024 22:39:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://binvis.io/#/">http://binvis.io/#/</a>, See on <a href="https://news.ycombinator.com/item?id=39210436">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[I don't want anything your AI generates (216 pts)]]></title>
            <link>https://coryd.dev/posts/2024/i-dont-want-anything-your-ai-generates/</link>
            <guid>39210126</guid>
            <pubDate>Wed, 31 Jan 2024 22:11:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://coryd.dev/posts/2024/i-dont-want-anything-your-ai-generates/">https://coryd.dev/posts/2024/i-dont-want-anything-your-ai-generates/</a>, See on <a href="https://news.ycombinator.com/item?id=39210126">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>I really don't. AI output is fundamentally derivative and exploitative (of content, labor and the environment).</p><div><p>I really don't. AI output is fundamentally derivative and exploitative (of content, labor and the environment).</p><p>I can't trust the answers it provides or the text it generates. It's not a replacement for search, it simply makes search worse.</p><p>The images it generates are, at best, a polished regression to the mean. If you want custom art, pay an artist.</p><p>I want to talk to a person, not a chatbot. The chatbot wastes time while you wait for a person that can actually help.</p><p>I don't want music recommendations from something that can't appreciate or understand music. Human recommendations will always be better.</p><p>I don't want AI mediating social interactions that it cannot and does not understand (though it may appear to). If I'm weary of too much volume on any social platform or in any news feed, I'll cut back on what I'm following.</p><p>If you're having AI attend a meeting for you, it probably wasn't that important. If you're having AI write your email, it probably wasn't that important. If it's screening job candidates for you, you're missing quality candidates.</p><p>I don't like the idea of it being trained on anything I've written or created<sup><a href="#fn1" id="fnref1">[1]</a></sup>.</p><p>I'll concede that Copilot is better than traditional autocomplete, but that feels like a pretty low bar<sup><a href="#fn2" id="fnref2">[2]</a></sup>.</p><p>These tools will improve<sup><a href="#fn3" id="fnref3">[3]</a></sup>. The cost to the environment will increase, it will help companies deskill jobs and muddy public discourse. The "benefits" it provides to end users are, at best, dubious — though everyone responsible for creating it will most certainly enrich themselves.</p><p>This all portends a future I can't imagine wanting or even being interested in.</p><hr><section><ol><li id="fn1"><p>Although consuming that may well be to its detriment. Nonetheless, my <code>robots.txt</code> file reflects this stance. <a href="#fnref1">↩︎</a></p></li><li id="fn2"><p>My first experience with Copilot chat was it getting an answer wrong, apologizing when that was pointed out and then repeating the same or similar, but still incorrect, answer. <a href="#fnref2">↩︎</a></p></li><li id="fn3"><p>I don't think this is a good thing. <a href="#fnref3">↩︎</a></p></li></ol></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek Coder: Let the Code Write Itself (198 pts)]]></title>
            <link>https://deepseekcoder.github.io/</link>
            <guid>39209814</guid>
            <pubDate>Wed, 31 Jan 2024 21:43:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepseekcoder.github.io/">https://deepseekcoder.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=39209814">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>
            DeepSeek Coder comprises a series of code language models trained from scratch on both 87% code and 13% natural language in English and Chinese, with each model pre-trained on 2T tokens.
            We provide various sizes of the code model, ranging from 1B to 33B versions.
            Each model is pre-trained on repo-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, resulting in foundational models (DeepSeek-Coder-Base).
            We further fine-tune the base model with 2B tokens of instruction data to get instruction-tuned models, namedly DeepSeek-Coder-Instruct.
          </p>
          <!-- <p>
            For coding capabilities, DeepSeek-Coder-Base achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.
            And Compared to GPT35-turbo, DeepSeek-Coder-Instruct demonstrates superior performance in human evaluation while maintaining comparable performance in MBPP.
          </p> -->
          <ul>
            <li>
              Pretrained on <b>2 Trillion</b> tokens over more than 80 programming languages.
            </li>
            <li>
              Various model sizes (<b>1.3B</b>, <b>5.7B</b>, <b>6.7B</b> and <b>33B</b>) to support different requirements.
            </li>
            <li>
              A window size of <b>16K window </b>size, supporting <b>project-level</b> code completion and infilling.
            </li>
            <li>
              <b>State-of-the-Art</b> performance among open code models. 
            </li>
            <li>
              <b>Open source and free for research and commercial use</b>.
            </li>
          </ul>
          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Generating Fantasy Maps (2016) (166 pts)]]></title>
            <link>http://mewo2.com/notes/terrain/</link>
            <guid>39209687</guid>
            <pubDate>Wed, 31 Jan 2024 21:31:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://mewo2.com/notes/terrain/">http://mewo2.com/notes/terrain/</a>, See on <a href="https://news.ycombinator.com/item?id=39209687">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="container">
    
    





<p><img src="http://mewo2.com/deserts/map.png" alt="An example map from the generator" width="60%"></p>
<p>These are some notes on how I generate the maps for my Twitter bot
<a href="https://twitter.com/unchartedatlas">@unchartedatlas</a>, which is based on a generator I originally produced
during <a href="https://github.com/dariusk/NaNoGenMo-2015/issues/156">NaNoGenMo 2015</a>. There's JavaScript code for the generator
on Github <a href="https://github.com/mewo2/terrain">here</a>, and the original messy Python generator code can be
seen <a href="https://github.com/mewo2/deserts">here</a>.</p>
<p>You may also be interested in this <a href="http://mewo2.com/notes/naming-language/">companion piece</a>, which describes
the placename generation.</p>
<h3>Inspiration</h3>
<p>I wanted to make maps that look like something you'd find at the back of one of
the cheap paperback fantasy novels of my youth. I always had a fascination with
these imagined worlds, which were often much more interesting than whatever
luke-warm sub-Tolkien tale they were attached to.</p>
<p>At the same time, I wanted to play with terrain generation with a physical
basis. There are loads of articles on the internet which describe terrain
generation, and they almost all use some variation on a fractal noise approach,
either directly (by adding layers of noise functions), or indirectly (e.g.
through midpoint displacement). These methods produce lots of fine detail, but
the large-scale structure always looks a bit off. Features are attached in
random ways, with no thought to the processes which form landscapes. I wanted
to try something a little bit different.</p>
<p>There are a few different stages to the generator. First we build up a
height-map of the terrain, and do things like routing water flow over the
surface. Then we can render the 'physical' portion of the map. Finally we can
place cities and 'regions' on the map, and place their labels.</p>
<h3>Grids</h3>
<p>To represent the heightmap, first we need a grid of points. Although it can be
simpler to work on a regular square grid, I much prefer to work on an irregular
set of points for something like this. With a regular grid, it's very easy to
run into weird artifacts, and you often have to do a lot of postprocessing to
hide the effects of the grid. If you use an irregular grid, then there are a
few things which are more complicated, but the structure of the grid helps to
give the map a rough, organic feel, and you never have to worry about nasty
linear artifacts in the finished product.</p>
<p>The approach I use is the same as in <a href="http://www-cs-students.stanford.edu/~amitp/game-programming/polygon-map-generation/">this article</a>, which is one of the
better references out there on how to do non-fractal terrain generation. I
won't go into too much detail here because that article explains it very
clearly, with lots of diagrams.</p>
<p>I start by selecting points at random within the map. These points tend to be a
bit clumpy and uneven, so I use <a href="https://en.wikipedia.org/wiki/Lloyd%27s_algorithm">Lloyd relaxation</a> to improve the point
set. For speed, I only use one iteration of this process, but you can repeat it
as many times as you like. There are rapidly diminishing returns after a few
iterations though.</p>
<p>All of the calculations are actually carried out on the 'dual points' of the
original point set, which correspond to the corners of the Voronoi polygons.
This has the advantage that the number of neighbours per node is fixed at
three, which helps in some parts of the code.</p>
<p>Note: this shows 256 (2<sup>8</sup>) points, to make viewing easier, but the real generator uses 16,384 (2<sup>14</sup>) points. I have a programmer's superstitions about always using powers of 2, which are more pleasing to the spirit of the machine.</p>

<h3>Rough outlines</h3>
<p>One of the difficulties of creating landscapes in a realistic way is that real
landscapes aren't created all at once. Instead, they evolve from earlier
landscapes, which in turn evolved from even earlier landscapes, and so on back
for billions of years. There's no good way to simulate this process in a
reasonable amount of time, so we need to cheat slightly.</p>
<p>Rather than an infinite regress of older landscapes, I start with a simple
'proto-landscape', built with geometric primitives. This lets me control the
broad outlines of the terrain, while leaving the details for the more physical
processes to fill in later.</p>
<p>Some useful primitives which we can add together:</p>
<ul>
<li>Constant slope - if you want to pretend this is physically motivated, think of it as tectonic uplift on one side of the map</li>
<li>Cone shapes - these can be islands or mountains, or if inverted, lakes or seas</li>
<li>Rounded blobs - these make better hills, and can be scattered all around to make a noisy surface</li>
</ul>
<p>We also have a few operations which are handy:</p>
<ul>
<li>Normalize - rescale the heights to lie in the range 0-1</li>
<li>Round - normalize, then take the square root of the height value, to round off the tops of hills</li>
<li>Relax - replace each height value with the average of its neighbours, to smooth the surface</li>
<li>Set sea level - translate the heightmap up or down so that a particular quantile is at zero</li>
</ul>
<p>The particular sequence of primitives and operations used can be varied to produce different kinds of landscape, such as coastlines, islands and mountain ranges.</p>
<p>Note: the black line indicates the zero contour, which we treat as 'sea level'. Also, this map uses 4,096 (2<sup>12</sup>) points, for speed.</p>

<h3>Erosion</h3>
<p>The results of this process can be a little bit on the blobby side, which means
they rarely look good on their own. We want to scuff them up a bit, so they
look more like real landscapes. We do this by applying an erosion operation.</p>
<p>In most of the world, by far the largest influence on the shape of landforms is
fluvial (water-based) erosion. Water flows downhill, carrying sediment along
with it, carving out valleys and river basins. This is a massively complex
phenomenon, and modelling it correctly is a very active research area, but we
can get a long way by sketching a simple version of the process.</p>
<p>We need to start by tracing the routes that water would take over the grid. For
each grid point, we say that water flows to its lowest neighbour, and so on
down until we reach the edge of the map. This gives a map of water flow.</p>
<p>There's an obvious problem when we reach gridpoints which are lower than all of
their neighbours. Do we route the water back uphill? This will probably lead to
cycles in the water system, which are trouble. Instead, we want to fill in
these gaps (often called sinks or depressions), so that the water always runs
downhill all the way to the edge.</p>
<p>It's easy to see how to fill in a single gridpoint, but as the depression gets
bigger, and possibly links up with other depressions, the number of possible
cases multiplies enormously. Luckily, there's an algorithm for filling
depressions, called the Planchon-Darboux algorithm.</p>
<div>

<h4>Aside: the Planchon-Darboux algorithm</h4>

<p>The algorithm works by finding the lowest surface with the following two properties:</p>

<ul>
<li>The surface is everywhere at least as high as the input surface</li>
<li>Every non-edge point has a neighbour which is lower than it</li>
</ul>

<p>To calculate this, we start with an infinitely high surface everywhere
except on the edge, where we use the original heights. Then, on each iteration,
we find points which have a neighbour which is lower than them, and set their
height to their original height, or the height of their lowest neighbour (plus
a small amount), whichever is higher. We halt when we can go a full iteration
without changing any point.</p>

<p>There are various ways of speeding up this algorithm, mostly by tweaking the
order in which points are visited. For more details, and a proof of
correctness, you can read <a href="http://horizon.documentation.ird.fr/exl-doc/pleins_textes/pleins_textes_7/sous_copyright/010031925.pdf">the original paper</a>.</p>

</div>

<p>With the water routing calculated, we can work out how much water is flowing
through each point. I assume that rainfall is constant across the whole map,
and iterate through the points in descending order, passing the rainfall, plus
the accumulated water flux, from each point to its 'downhill point'. This gives
a map of water flux, which usually converges into a nice branching river
structure, with lots of small streams feeding a larger central channel.</p>
<p>To calculate erosion, I combine the water flux with the slope at each point, as
calculated based on the triangle of its neighbours. The exact formula I use is
the product of the slope with the square root of the water flux. This isn't
necessarily very physical, but it does give nice-looking results. I also add a
small term which is proportional to the slope squared. This prevents deep
gorges from forming, which might be physically realistic, but don't look good
in the graphical style I've chosen.</p>
<p>I find it's very important to cap the erosion rate, otherwise strange things
can happen. A little goes a very long way with this. Also, erosion always
lowers the surface, so it usually helps to drop the sea level afterwards to match.</p>
<p>A final tweak to the heightmap is to smooth out the coastlines slightly. The
erosion tends to produce quite rough terrain, which becomes tiny islands when
cut off by sea level. A few of these can look good, but too many just looks
messy. I repeatedly apply a filter where points which are below sea level, but
a majority of whose neighbours are above sea level, get pulled up, and vice
versa for points which are above sea level and have undersea neighbours. A
couple of repeats of this produces a much cleaner coastline.</p>


<h3>Rendering terrain</h3>
<p>Now comes the question of drawing the map (at least the physical portion). The
easy part is the coastline - we've been doing this already. It's just a matter
of drawing line segments where the heightmap crosses zero. There's not a lot
extra to do about this.</p>
<p>The next component is the rivers. We don't want to display the entire drainage
network, because that would cover the whole map. Instead, we only show the
drainage from points with above a certain threshold of water flux. By
connecting these points to their downstream neighbours, we can trace out the
river paths.</p>
<p>One problem with this approach is that the rivers tend to zigzag from grid
point to grid point, rather than following a smooth path. To solve this, I
relax the points in the middle of the path towards their upstream and
downstream neighbours (keeping the top and bottom fixed, so that intersections
work properly). This smooths things out beautifully.</p>
<p>The final part of this is the shading on the sides of hills, which helps
indicate the topography. It is a central principle of cartography that we tend
to interpret maps as though viewing the terrain from the bottom of the map,
looking towards the top. So we want to draw strokes which go up and right if
the terrain slopes upwards from left to right, and down and right if the
terrain slopes downwards. Similarly, the strokes on the 'near' side of hills
should be longer than those on the 'far' side.</p>
<p>For each grid point, I calculate the slope, and ignore the point if it is less
than a random threshold. For points which pass this test, I draw a short stroke
with slope proportional to the horizontal component of the heightmap slope,
with a small modifier for the vertical component. If the stroke would be too
steep, I split it into several shorter strokes, at the maximum slope, drawn at
random around the point.</p>


<h3>Cities, borders</h3>
<p>Now that we have the 'physical' portion of the map sorted, we can move to
looking at the 'political'. We want to place cities and towns on the map in
feasible-looking locations. At the same time, we want the cities to be spread
out enough that we can put labels on them without worrying too much about
overlap.</p>
<p>To place a city, I generate a score for each point, which is a combination of three things:</p>
<ul>
<li>Water flux - we want cities to be preferentially located on rivers, so high water flux gets a bonus</li>
<li>Distance from other cities - we want cities to be spread out, so penalize locations which are too close to an existing city</li>
<li>Distance from the edge of the map - the other two criteria alone tend to push cities to the map edge, which isn't ideal, so penalize locations too close to the edge</li>
</ul>
<p>Every time I add a city, I choose the point with the highest score, then
recalculate the scores for all points. I make a distinction between cities,
which have a 'region' associated with them, and towns, which don't. The cities
are placed first, but otherwise there's no distinction in the code.</p>
<p>The next step is to mark out the regions. We want the borders between regions
to seem fairly reasonable, following natural borders like rivers and mountain
ranges. The way I approach this is to expand regions outwards from each city,
so that each region consists of the points which are 'closest' to its city,
according to a particular distance measure. This distance measure is calculated
by adding up the cost of the route, based on these criteria:</p>
<ul>
<li>Horizontal distance</li>
<li>Slope - uphill is much cheaper than downhill, so regions expand until they hit the top of ridges, then stop</li>
<li>Water flux - crossing a river is expensive</li>
<li>Shorelines - there is a large penalty for going from land to water (or vice versa), and a smaller penalty for travelling by water</li>
</ul>
<p>Finally, the borders are drawn, using the same smoothing technique as the rivers.</p>


<h3>Placing labels</h3>
<p>At this point we can start naming things, using the process described in <a href="http://mewo2.com/notes/naming-language/">these
notes</a>. I generate names for cities, towns and regions, using a
consistent language for the whole map.</p>
<p>The very last part of the process is to place the labels, avoiding overlaps,
obscured cities, labels going off the edge of the map, etc. This sounds easy,
but it really really isn't. Ideally, some sort of intelligent layout algorithm
would place all the labels, rearranging as necessary. Instead, I have a few
hundred lines of spaghetti code which seems to get the right answer most of the
time, but which is packed full of magic numbers.</p>
<p>In rough outline, what happens is this: the labels are placed in the order 1.
cities, 2. towns, 3. regions. For cities and towns, there are four possible
slots for the label, above, below, and to each side of the marker. Each label
is placed, attempting to avoid the following overlaps (in rough order of
importance):</p>
<ul>
<li>City markers</li>
<li>Other city labels</li>
<li>The map edge</li>
<li>Borders</li>
<li>Coastlines</li>
<li>Rivers</li>
</ul>
<p>Obviously it's not usually possible to avoid all of these, so the least bad
solution is chosen.</p>
<p>For regions, there's a bit more freedom in where to place the label, but the
labels are also bigger, and the map is more cluttered at this point. The
scoring system rates positions based on proximity to the center of the region,
as well as being over land, and penalties for all the overlaps mentioned
before.</p>
<p>I wanted to make a nice interactive example for this, but trying to separate out the label placement code made me feel physically unwell. Sorry about that.</p>

<p>So that's the algorithm, at least in rough outline. The actual code running on
the bot has a few other little bits and pieces, which honestly don't do much,
but removing them is more trouble than it's worth when the code is working as
it stands. The JavaScript code behind this page is available <a href="https://github.com/mewo2/terrain">on
GitHub</a>, and if you're really really brave you can look at the original
<a href="https://github.com/mewo2/deserts">Python code</a> which was written while I was figuring all this out.</p>
<p>There's obviously lots that could be done to improve this. Erosion is just one
process, and the next obvious thing to add would be fluvial deposition, which
would allow for flood plains, river deltas, etc to form. If you wanted more
realistic mountains, then glacial processes would be worth looking at. Volcanic
stuff could also be fun.</p>
<p>On the graphical side, it could be fun to try to sketch more interesting
textures on the map, such as forests or fields. Or you could fill the oceans
with sea monsters and lost ships. If you're really brave, you could even look
at labelling more features, like mountain ranges and rivers.</p>
<p>As always, if you do any of these things, or make anything else interesting
with this code or these ideas, please get in touch and let me know.</p>
<p>If you would like to buy a print of one of these maps, please visit <a href="https://shop.mewo2.com/">my web shop</a>.</p>

    <hr><p>
    
    If you've enjoyed this piece, please consider <a href="https://www.patreon.com/mewo2">contributing on Patreon</a> so I can do more things like this.
    
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Comcast reluctantly agrees to stop its misleading "10G Network" claims (203 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2024/01/comcast-to-stop-calling-entire-network-10g-as-name-is-ruled-misleading/</link>
            <guid>39209469</guid>
            <pubDate>Wed, 31 Jan 2024 21:10:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2024/01/comcast-to-stop-calling-entire-network-10g-as-name-is-ruled-misleading/">https://arstechnica.com/tech-policy/2024/01/comcast-to-stop-calling-entire-network-10g-as-name-is-ruled-misleading/</a>, See on <a href="https://news.ycombinator.com/item?id=39209469">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      10G or not 10G    —
</h4>
            
            <h2 itemprop="description">Comcast said it will drop "Xfinity 10G Network" brand name after losing appeal. </h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/01/comcast-10g-800x528.jpg" alt="A Comcast router/modem gateway.">
      <figcaption><p>Comcast</p></figcaption>  </figure>

  




<!-- cache hit 181:single/related:5927e4533beb5ac868f3f9b9c0ce65dd --><!-- empty -->
<p>Comcast has reluctantly agreed to discontinue its "Xfinity 10G Network" brand name after losing an appeal of a ruling that found the marketing term was misleading. It will keep using the term 10G in other ways, however.</p>
<p>Verizon and T-Mobile both challenged Comcast's advertising of 10G, a term used by cable companies since it was unveiled in January 2019 by industry lobby group NCTA-The Internet &amp; Television Association. We <a href="https://arstechnica.com/information-technology/2019/01/envious-of-5g-hype-cable-cos-unveil-potentially-confusing-10g-trademark/">wrote in 2019</a> that the cable industry's 10G marketing was likely to confuse consumers and seemed to be a way of countering 5G hype generated by wireless companies.</p>
<p>10G doesn't refer to the 10th generation of a technology. It is a reference to potential 10Gbps broadband connections, which would be much faster than the actual speeds on standard cable networks today.</p>
<p>The challenges lodged against Comcast marketing were filed with the advertising industry's self-regulatory system run by BBB National Programs. BBB's National Advertising Division (NAD) <a href="https://arstechnica.com/tech-policy/2023/10/comcast-should-stop-advertising-slower-speeds-as-10g-industry-group-says/">ruled against Comcast</a> in October 2023, but Comcast appealed to the National Advertising Review Board (NARB).</p>
<p>The NARB <a href="https://bbbprograms.org/media-center/dd/comcast-10g-xfinity">announced its ruling</a> today, agreeing with the NAD that "Comcast should discontinue use of the term 10G, both when used in the name of the service itself ('Xfinity 10G Network') as well as when used to describe the Xfinity network. The use of 10G in a manner that is not false or misleading and is consistent with the panel decision is not precluded by the panel recommendations."</p>
<h2>“Comcast will discontinue brand name”</h2>
<p>Comcast agreed to make the change in an advertiser's statement that it provided to the NARB. "Although Comcast strongly disagrees with NARB's analysis and approach, Comcast will discontinue use of the brand name 'Xfinity 10G Network' and will not use the term '10G' in a manner that misleadingly describes the Xfinity network itself," Comcast said.</p>
<p>Comcast said it disagrees with "the recommendation to discontinue the brand name" because the company "makes available 10Gbps of Internet speed to 98 percent of its subscribers upon request." But those 10Gbps speeds aren't available in Comcast's typical service plans and require a fiber-to-the-home connection instead of a standard cable installation.</p>                                            
                                                        
<p>The Comcast "Gigabit Pro" fiber connection that provides 10Gbps speeds costs $299.95 a month plus a $19.95 modem lease fee. It also requires a $500 installation charge and a $500 activation charge.</p>
<p>Comcast said it may still use 10G in ways that are less likely to confuse consumers. "Consistent with the panel's recommendation... Comcast reserves the right to use the term '10G' or 'Xfinity 10G' in a manner that does not misleadingly describe the Xfinity network itself," the company said.</p>
<p>When contacted by Ars, a Comcast spokesperson said, "We disagree with the decision but are pleased that we have confirmed our continued use of 10G in advertising."</p>
<h2>Comcast claims “not supported”</h2>
<p>The NARB said the "recent availability of 10G speeds through [the Gigabit Pro] service tier does not support the superior speed claim (or a 10Gbps claim) for the Xfinity network as a whole." As the NARB noted, there is an "absence" of data showing how many Comcast customers actually use that service.</p>
<p>The NARB also said that 10G is misleading because of the implied comparison to 5G wireless networks.&nbsp;"The NARB panel concluded that 10G expressly communicates at a minimum that users of the Xfinity network will experience significantly faster speeds than are available on 5G networks," the announcement of the ruling said. "This express claim is not supported because the record does not contain any data comparing speeds experienced by Xfinity network users with speeds experienced by subscribers to 5G networks."</p>
<p>As the NAD has <a href="https://bbbprograms.org/media-center/dd/comcast-10g">previously stated</a>, 10G is more of an "aspirational" term rather than something that's offered over today's cable networks. Over the past five years, the NCTA has been using the term 10G to describe just about any improvement to cable networks, regardless of the actual speeds.</p>
<p>The NCTA coincidentally issued a <a href="https://www.ncta.com/whats-new/10g-five-years-on-the-future-is-even-closer">press release</a> yesterday hailing the fifth anniversary of its first 10G announcement. "Five years on, the future is even closer... Here in 2024, the promise of 10G is becoming more and more of a reality," the NCTA said.</p>
<p>The announcement listed some examples of multi-gigabit (but not 10-gigabit) cable speeds, some of which were only achieved in lab testing or demos. NCTA claimed that "10G can change lives" and that the "10G platform will facilitate the next great technological advancements in the coming decades, ensuring fast, reliable, and safe networks continue to power the American economy."</p>
<p>For all of you cable broadband users, just remember to ignore "10G" in cable-company marketing and check the actual speeds you're paying for.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FBI confirms it issued remote kill command to blow out Volt Typhoon's botnet (109 pts)]]></title>
            <link>https://www.theregister.com/2024/01/31/volt_typhoon_botnet/</link>
            <guid>39209352</guid>
            <pubDate>Wed, 31 Jan 2024 21:00:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2024/01/31/volt_typhoon_botnet/">https://www.theregister.com/2024/01/31/volt_typhoon_botnet/</a>, See on <a href="https://news.ycombinator.com/item?id=39209352">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>China's Volt Typhoon attackers used "hundreds" of outdated Cisco and NetGear routers infected with malware in an attempt to break into US critical infrastructure facilities, according to the Justice Department.</p>
<p>On Tuesday <a target="_blank" href="https://www.theregister.com/2024/01/30/fbi_china_volt/">news broke</a> that the Feds had blocked the malicious network that was set up on end-of-life, US-based small office/home office routers. Now more details have come out about how an FBI team infiltrated the attack and harvested the key data before remotely wiping the KV Botnet, according to four warrants (<a target="_blank" rel="nofollow" href="https://lnks.gd/l/eyJhbGciOiJIUzI1NiJ9.eyJidWxsZXRpbl9saW5rX2lkIjoxMDIsInVyaSI6ImJwMjpjbGljayIsInVybCI6Imh0dHBzOi8vd3d3Lmp1c3RpY2UuZ292L29wYS9tZWRpYS8xMzM2NDIxL2RsP2lubGluZT0mdXRtX21lZGl1bT1lbWFpbCZ1dG1fc291cmNlPWdvdmRlbGl2ZXJ5IiwiYnVsbGV0aW5faWQiOiIyMDI0MDEzMS44OTQzMDk5MSJ9.4LRHVZyUDYfMnRWC7s_-GUsAOhl4SewCG7GXGj4tAX8/s/3078670442/br/236285728656-l">5018</a>, <a target="_blank" rel="nofollow" href="https://lnks.gd/l/eyJhbGciOiJIUzI1NiJ9.eyJidWxsZXRpbl9saW5rX2lkIjoxMDMsInVyaSI6ImJwMjpjbGljayIsInVybCI6Imh0dHBzOi8vd3d3Lmp1c3RpY2UuZ292L29wYS9tZWRpYS8xMzM2NDE2L2RsP2lubGluZT0mdXRtX21lZGl1bT1lbWFpbCZ1dG1fc291cmNlPWdvdmRlbGl2ZXJ5IiwiYnVsbGV0aW5faWQiOiIyMDI0MDEzMS44OTQzMDk5MSJ9.8vmYc17z664gSNfykf8ncM2yRyRspFYF6SCIHiTZhb8/s/3078670442/br/236285728656-l">5530</a>, <a target="_blank" rel="nofollow" href="https://lnks.gd/l/eyJhbGciOiJIUzI1NiJ9.eyJidWxsZXRpbl9saW5rX2lkIjoxMDQsInVyaSI6ImJwMjpjbGljayIsInVybCI6Imh0dHBzOi8vd3d3Lmp1c3RpY2UuZ292L29wYS9tZWRpYS8xMzM2NDA2L2RsP2lubGluZT0mdXRtX21lZGl1bT1lbWFpbCZ1dG1fc291cmNlPWdvdmRlbGl2ZXJ5IiwiYnVsbGV0aW5faWQiOiIyMDI0MDEzMS44OTQzMDk5MSJ9.d-xmdG8esN2ZFRwqLXHNvwIo6ZpVS-2HQ5js3wwfJo8/s/3078670442/br/236285728656-l">5451</a> and <a target="_blank" rel="nofollow" href="https://lnks.gd/l/eyJhbGciOiJIUzI1NiJ9.eyJidWxsZXRpbl9saW5rX2lkIjoxMDUsInVyaSI6ImJwMjpjbGljayIsInVybCI6Imh0dHBzOi8vd3d3Lmp1c3RpY2UuZ292L29wYS9tZWRpYS8xMzM2NDExL2RsP2lubGluZT0mdXRtX21lZGl1bT1lbWFpbCZ1dG1fc291cmNlPWdvdmRlbGl2ZXJ5IiwiYnVsbGV0aW5faWQiOiIyMDI0MDEzMS44OTQzMDk5MSJ9.xD5SrU9r17bUh7t5QaI4TWGGTTTlIfn6D8a3QecwFJY/s/3078670442/br/236285728656-l">5432</a>) filed by the FBI in the Southern District Court of Texas last month and released today.</p>
<p>"China's hackers are targeting American civilian critical infrastructure, pre-positioning to cause real-world harm to American citizens and communities in the event of conflict," FBI Director Christopher Wray said in a <a target="_blank" rel="nofollow" href="https://www.justice.gov/usao-sdtx/pr/us-government-disrupts-botnet-peoples-republic-china-used-conceal-hacking-critical">statement</a>. "Volt Typhoon malware enabled China to hide as they targeted our communications, energy, transportation, and water sectors."</p>

    

<p>The Feds claim the Middle Kingdom keyboard warriors downloaded a virtual private network module to the vulnerable routers and set up an encrypted communication channel to control the botnet and hide their illegal activities. Specifically: Volt Typhoon used the US-based routers and IP addresses to <a target="_blank" href="https://www.theregister.com/2024/01/31/critical_infrastructure_hacking/">target</a> US critical infrastructure, we're told.</p>
<ul>

<li><a href="https://www.theregister.com/2024/01/30/fbi_china_volt/">US shorts China's Volt Typhoon crew targeting America's criticals</a></li>

<li><a href="https://www.theregister.com/2023/05/25/china_volt_typhoon_attacks/">Five Eyes and Microsoft accuse China of attacking US infrastructure again</a></li>

<li><a href="https://www.theregister.com/2024/01/31/critical_infrastructure_hacking/">We know nations are going after critical systems, but what happens when crims join in?</a></li>

<li><a href="https://www.theregister.com/2024/01/31/ivanti_patches_zero_days/">Ivanti releases patches for VPN zero-days, discloses two more high-severity vulns</a></li>
</ul>
<p>The warrants allowed law enforcement to remotely install software on the routers to search for, and then seize or copy, information about the illicit activity before wiping the malware from the compromised devices.</p>
<p>To do this — and to limit the cops' search to routers infected with the botnet — the FBI sent specific KV Botnet commands to compromised routers to collect "non-content information about those nodes," according to the warrants.</p>

        

<p>This includes the IP address, port numbers used by infected routers to communicate with other nodes, as well as IP addresses and ports used by each node's parent, and data on the command-and-control nodes.</p>
<p>"A router that is not infected by the KV Botnet malware would not receive or respond to this command," court documents claim.&nbsp;</p>

        

<p>The Feds, along with foreign agency partners in Five Eyes nations, <a target="_blank" href="https://www.theregister.com/2023/05/25/china_volt_typhoon_attacks/">first warned</a> about this threat in May 2023.</p>
<p>Also today, the US Cybersecurity Agency and FBI <a target="_blank" rel="nofollow" href="https://www.cisa.gov/news-events/alerts/2024/01/31/cisa-and-fbi-release-secure-design-alert-urging-manufacturers-eliminate-defects-soho-routers">issued an alert</a> urging manufacturers to eliminate defects in SOHO router web management interfaces. This, according to the agencies, includes automating update capabilities, locating the web management interface on LAN-side ports, and requiring a manual override to remove security settings. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Moved My Blog from IPFS to a Server (306 pts)]]></title>
            <link>https://neimanslab.org/2024-01-31/why-i-moved-my-blog-ipfs-to-server.html</link>
            <guid>39208673</guid>
            <pubDate>Wed, 31 Jan 2024 20:07:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neimanslab.org/2024-01-31/why-i-moved-my-blog-ipfs-to-server.html">https://neimanslab.org/2024-01-31/why-i-moved-my-blog-ipfs-to-server.html</a>, See on <a href="https://news.ycombinator.com/item?id=39208673">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p><span>Written by</span>
    
        Neiman
    

    
      <br>
      <span>on&nbsp;</span><time datetime="2024-01-31 00:00:33 +0100">January 31, 2024</time>
    
  </p>

  
  

  <p>It’s safe to say I was a pioneer of IPFS + ENS websites. When I set up my first ENS+IPFS website in March 2019 there were no more than 15 others. Between 2019 to 2022 I co-built an IPFS+ENS browser extension (Almonit), an IPFS+ENS search engine (Esteroids), and of course, my personal blog was available only in IPFS+ENS.</p>

<p>But today I moved my blog back to a server, and I’d like to discuss why.</p>

<p>What got me excited about peer-to-peer websites like IPFS is that, theoretically, the more visitors a website has, the more robust, censorship-resistant, and scalable it is. Again, theoretically.</p>

<p>Do you know how popular torrent files seem to live forever? I wanted the same but for websites. I imagined a website that is hard to ddos (Robust), difficult to block (Censorship-resistant), and the more readers it has, the faster it is to use it since some readers help to spread the content (Scalable).</p>

<p>I imagined a website with a big “Pin Me” button (pinning in IPFS is like seeding in BitTorrent). If a reader presses the button they wil help serve the website.</p>

<p>In practice this didn’t work out really, and for several reasons.</p>

<ol>
  <li>
    <p>IPFS users mostly don’t run their own nodes or software. Instead, they use gateways. It’s an educated guess I’m making based on what I see in the community, and based on the fact that it’s quite an inconvenience to run your own IPFS node. But even if you do run your own node, the fact you access a website doesn’t mean you pin it. Not at all.</p>

    <p>This is a huge difference from BitTorrent where the only way to get content is to run your own software, and when you download something you also share it, by default.</p>

    <p>Hence, most readers will not help share the website, but even for the ones who will there are still extra complications:</p>
  </li>
  <li>
    <p>Websites are dynamic objects. Their content is being updated all the time. If you just pin the content of the current version of a website, that’s not much help.</p>

    <p>What most IPFS websites do is use a name system that points to the latest version of its content. It’s usually either IPNS, the internal name system of IPFS, or ENS, Ethereum Name System. But IPFS doesn’t include yet an easy command to always pin the latest content of IPNS, and if someone uses ENS, it means that whoever pins it also needs to listen to Ethereum blockchain events, a huge extra challenge on its own to do without a centralized service.</p>
  </li>
  <li>
    <p>To make things worse, it’s actually quite hard to get IPFS content to be available in the browser in a reliable way!</p>

    <p>For example, I wanted my IPFS blog to be available to all major gateways, all IPFS nodes, Brave browser (which supports IPFS natively), and js-libp2p &amp; helia (the js libraries of IPFS). I didn’t find a reliable way to achieve that on my own.</p>

    <p><strong>Long rant:</strong> I pinned content from my own server and played forever with settings and definitions, but couldn’t get the content to be available everywhere. Worse was with Helia, where I just couldn’t manage to get my content accessible from Helia within the browser without connecting directly to my own node. But then what’s p2p network about it?</p>

    <p>I found out that there’s a service, <a href="https://cid.contact/">cid.contact</a>, called a “Content Routing” service. It’s written in the “about section” of cid.contact it’s related to Filecoin, but for some reason, it holds routing data for IPFS, which as far as I know is a different network. The address cid.contact is hard-coded into Helia’s code in the version I used at least, and it was clear that if a content is indexed by cid.contact, then it’s reachable almost everywhere, but if it’s not indexed - it’s not reachable always.</p>

    <p>I couldn’t figure out how to index my content in cid.contact. Honestly, I’m not sure I wanted to. Because what’s the point? it seems to just add a dependency on a centralized service. I could try to run my own indexer and define it in my website, but again, centralization. What’s the economic model for these indexers? Which actors do we expect to run them in the long term?</p>

    <p>The text in cid.contact says that at the current size of IPFS, it’s unreasonable to expect the DHT to handle routing efficiently on its own. This kind of makes sense, but what’s the alternative to that, that doesn’t break up the technology pros?</p>
  </li>
</ol>

<p>By now I got tired of the constant struggle for my IPFS blog to function well. At least for a short while, I want a simple, classic working solution. The blog you’re reading now is built with Jekyll and is hosted on my own 10$ server.</p>

<p>don’t get me wrong, I’m still an IPFS fanboy. It’s a great project managed very well. It just doesn’t fit a personal blog needs yet.</p>

<p>That said, It’s difficult to follow the constant development and innovation of IPFS or Filecoin without this becoming a day job. Did I miss some trivial solution or a recent innovation? If yes, let me know. There are no comments here yet, but I am available via old-style email (neiman@hackerspace.pl), or in Mastodon (@neiman@mastodon.social).</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fighting Infomania: Why 80% of Your Reading Is a Waste of Time (2016) (102 pts)]]></title>
            <link>https://www.nateliason.com/blog/infomania</link>
            <guid>39208629</guid>
            <pubDate>Wed, 31 Jan 2024 20:03:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nateliason.com/blog/infomania">https://www.nateliason.com/blog/infomania</a>, See on <a href="https://news.ycombinator.com/item?id=39208629">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I returned to college Junior Fall dissatisfied. I’d just finished a summer internship doing management consulting with a government contracting firm, and in the process, realized that working in a company wasn’t for me.</p><p>The original plan had been to leverage it into a better internship somewhere else, and eventually a full-time offer, but with how much I disliked the work I knew something had to change.</p><p>Then I hit a snag: I wanted to get into startups or lifestyle businesses, but how was I supposed to&nbsp;<em>get there</em>?</p><p>With most corporate work you can go the traditional route of school -&gt; internship -&gt; job, but skill-based jobs at smaller companies don’t have such a clear route. And this lack of a clear route is what stops many students from going after&nbsp;<a href="https://www.nateliason.com/fun-work/">fun work</a>.</p><p>Faced with not knowing the way forward, or where to begin with “starting a startup,” I began reading&nbsp;<em>everything</em>.</p><p>Over the next 6 months, I&nbsp;<a href="https://www.nateliason.com/can-easily-read-100-books-year/" target="_blank">read 30+ books</a>&nbsp;on entrepreneurship, startups, marketing, “growth hacking,” and everything tangentially related I could find. And that doesn’t include the countless blog posts, articles, reddit threads, and whatever else I could get my hands on.</p><p>A good plan, right? No, 80% of it was a waste of time,&nbsp;<strong>and most people make the same mistake with how they consume information every day</strong>.</p><h2>Just in Time vs Just in Case</h2><p>Toyota’s ability to kill it in the car market came from their (then) revolutionary manufacturing process:&nbsp;<a href="https://en.wikipedia.org/wiki/Just-in-time_manufacturing" target="_blank">just in time manufacturing</a>.</p><p>While Detroit and other car companies would pump out cars “just in case” they were purchased, Toyota would make them “just in time” for their orders. This saved them millions of dollars by minimizing unused inventory, and the cost savings allowed them to speed up their manufacturing process significantly with new technology.</p><p>What Toyota realized was that by creating cars just in time, they could move faster, respond to market changes quicker, and only make cars that they knew they needed.</p><p>You should approach tactical knowledge the same way.</p><p>Sidebar: There’s an important distinction here between&nbsp;tactical&nbsp;knowledge (e.g. how to do search engine optimization), and&nbsp;philosophical&nbsp;knowledge (e.g. understanding our tendency towards&nbsp;biased thinking). The first, tactical knowledge, is what we’re concerned with overdosing on.</p><p>The school model focuses on just in case knowledge. You take classes and study textbooks that cram knowledge into you hoping that one day you might find it useful. But that’s not how&nbsp;<a href="https://www.nateliason.com/self-education/" target="_blank">learning works in the real world</a>.</p><h2>Where We Make “Just In Case” Mistakes in the Real World</h2><p>When I was working on&nbsp;<a href="https://www.nateliason.com/life-after-sumome/" target="_blank">marketing at SumoMe</a>, I never followed any marketing blogs. I didn’t check my Feedly when I got to work, didn’t look on GrowthHackers or Inbound to see what was trending, and didn’t have any newsletters I was subscribed to.</p><p>Conventional wisdom says that you should follow what people in your industry are talking about tactics-wise, but it’s just noise. You should have the strategy and tactics that you’re working on, and then you should execute on them. Constantly listening to and checking what other people are doing doesn’t help you stay focused–it just makes you question yourself.</p><p>The exception to this is if you can find a site that publishes articles very infrequently but where they are very high quality (for example,&nbsp;<a href="http://backlinko.com/" target="_blank">Backlinko</a>&nbsp;for SEO).</p><h3>The Problem of Frequency</h3><p>As a general rule, the more frequently a site publishes about tactics (marketing, personal finance, weight loss, etc.), the&nbsp;<em>less</em>&nbsp;you should listen to it.</p><p>No topic is sufficiently complex that you need new information on executing on it every day. Getting in shape requires doing a few very simple things every day for months, not finding a new 13 minute 6 step workout every day so you can have a butt like today’s hot celebrity.</p><p>That’s part of the reason I’ve deliberately avoided having a narrow theme to my writing. You don’t need an entire site on&nbsp;<a href="https://www.nateliason.com/last-longer-in-bed/" target="_blank">lasting longer in bed</a>&nbsp;or&nbsp;<a href="https://www.nateliason.com/5-day-water-fast-health-benefits/" target="_blank">water fasting</a>, you just need one or a couple really good articles.</p><h3>The Problem of Recency</h3><p>Then, there’s the issue with new articles. We tend towards&nbsp;<em>neomania:</em>&nbsp;overly focusing on the new and shiny, when new and shiny things tend to be the quickest to go and the least likely to be valuable.</p><p>A safer bet is to see what content has stood the test of time, following the&nbsp;<em>Lindy Rule.&nbsp;</em>Anything that has been around for 50 years will probably be around for another 50 years, but something that’s been around a few days has no proof of staying power.</p><p>Will people be reading Aristotle in another 2,000 years? Probably. Will they be reading this week’s pop-business book in another 10 years? Probably not.</p><h2>Why We Do It</h2><p>There are three reasons why we feel compelled to constantly consume knowledge about things we’re working on, and I’ll explain the problems with each.</p><h3>1. Habit</h3><p>We carry that “just in case” habit with us from school, and feel like we’re not doing our job if we’re not front-loading knowledge.</p><p>But it should be clear the fault here now. Just because we were taught that way doesn’t mean it’s the best way, it’s simply the way things have always been done.</p><h3>2. History</h3><p>Prior to 20 years ago, you wanted to err on the side of just in case knowledge since getting new information was so difficult.</p><p>That’s not the case anymore. Anything you could possibly want to learn you could figure out the basics of in an afternoon with a WiFi connection. You don’t have to worry about front loading everything because you’ll hardly ever be in a situation where you can’t look up the answers.</p><h3>3. Fauxductivity</h3><p>Then there’s the fact that doing it makes us feel like we’re being productive when we’re really just slacking off, a phenomenon I’m going to call “Fauxductivity.”</p><p>Any time you’re doing something that feels productive&nbsp;but doesn’t directly impact your most important goal, you’re being fauxductive. That includes bingeing on just in case knowledge, as well as checking email,&nbsp;<a href="https://www.nateliason.com/news-waste-time/" target="_blank">reading the news</a>, trying productivity tools, organizing your desktop, etc.</p><h2>1 Rule to Fight Infomania</h2><p>Here’s your new rule for information, especially blogs and popular non-fiction books:</p><p><em><strong>If it doesn’t answer a specific question you’re currently asking, cover philosophical knowledge, or entertain you, then don’t read it.</strong></em></p><p>Unfollow the industry blogs, stop reading the newspaper, don’t go on site aggregators, quit it. It’s all fauxductivity and wasting your time with useless information that will only cause&nbsp;<a href="https://www.nateliason.com/decision-fatigue/" target="_blank">decision paralysis</a>, self-doubt, and decreased confidence in your existing decisions.</p><p>When you have a specific question (e.g. how do I grow my Instagram following) that’s when you start digging through the blogs and industry material. But only until you have enough information to answer your question and formulate a plan. Don’t fill up your RSS with every social media and Instagram blog out there hoping you stumble on something else tomorrow.</p><p>If you can do this, you’ll spend significantly less time reading things that you never use, and you’ll get more information and mileage out of what you do read.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Some blind hackers are bridging IRC to LMMs running locally (187 pts)]]></title>
            <link>https://2mb.codes/~cmb/ollama-bot/</link>
            <guid>39208451</guid>
            <pubDate>Wed, 31 Jan 2024 19:50:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://2mb.codes/~cmb/ollama-bot/">https://2mb.codes/~cmb/ollama-bot/</a>, See on <a href="https://news.ycombinator.com/item?id=39208451">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>This is a rudimentary IRC bot that communicates with a local instance
of <a href="https://ollama.ai/" rel="nofollow noopener">ollama</a>.</p>
<h3 id="installation"><a href="#installation" rel="nofollow noopener">#</a>Installation</h3>
<p>You'll need to have <a href="https://ollama.ai/" rel="nofollow noopener">ollama</a> installed in order to
actually use the bot.  Instructions for doing that are out of scope for
this document.  Then, build the bot from source:</p>
<pre><code>git clone 'https://git.2mb.codes/~cmb/ollama-bot
cd ollama-bot
go build
</code></pre>
<p>Put the ollama-bot binary on your <code>$PATH</code>.
We run this on a Void Linux host.  Runit services for both the bot and
ollama itself are included in the ollama-bot repository, along with some
instructions for getting it running under runit.  If you use something else,
you're on your own, but of course, patches are welcome.</p>
<h3 id="configuration"><a href="#configuration" rel="nofollow noopener">#</a>Configuration</h3>
<p>The program searches for its configuration file in one of several places,
in the following order, and uses the first file it finds:</p>
<pre><code>~/.config/ollama-bot/ollama-bot.toml
/usr/local/etc/ollama-bot/ollama-bot.toml
/etc/ollama-bot/ollama-bot.toml
</code></pre>
<p>The configuration file should contain one or more network definitions.
I.E., it is an array of tables using toml notation.  Look at the file
<code>config.toml.sample</code> for a sample configuration.</p>
<h3 id="usage"><a href="#usage" rel="nofollow noopener">#</a>Usage</h3>
<p>Once the bot is connected to one or more IRC networks, you can converse
with it, either by highlighting it in a channel, or by sending it a private
message.  For the most part, you just send it free-form text, and it will
respond with free-form text.  However, it also accepts various commands,
preceded by an exclamation mark <code>!</code>.  Send <code>!help</code> for a list of commands.
Use of the commands is best demonstrated by an example chat transcript.</p>
<h4 id="chat-transcript"><a href="#chat-transcript" rel="nofollow noopener">#</a>Chat Transcript</h4>
<pre><code>&lt;chris&gt; ai-bot: !help
&lt;ai-bot&gt; 
&lt;ai-bot&gt; This bot supports the following commands:
&lt;ai-bot&gt; !describe URL [model] -- Describe image at URL, optionally choosing model
&lt;ai-bot&gt; !switch-text-model -- Start a conversation using a different text model
&lt;ai-bot&gt; !list-text-models -- List models usable for text conversation
&lt;ai-bot&gt; !list-image-models -- List models usable for image description
&lt;ai-bot&gt; !help -- This command.
&lt;chris&gt; ai-bot: !list-image-models
&lt;ai-bot&gt; This bot supports the following image models: llava, llava:13b, bakllava
&lt;chris&gt; ai-bot: !list-text-models
&lt;ai-bot&gt; This bot supports the following text models: mistral:instruct, llama2, llama2-uncensored
&lt;chris&gt; ai-bot: !describe https://the-brannons.com/chris-and-bird.jpg llava:13b
&lt;ai-bot&gt;  The image features a bearded man with a bird perched on his shoulder. The bird is small, likely a parrot or a similar type of pet bird. The man appears to be in a relaxed setting, possibly sitting down while enjoying the company with his feathered companion.
&lt;chris&gt; ai-bot: What kind of beard does he have?
&lt;ai-bot&gt;  While I cannot precisely describe the man's beard style without more specific information, it is safe to say that he has some form of facial hair on his face. This could range from a full beard to a goatee or even just a five o'clock shadow.
# Use the default model, bakllava.  This resets conversation state.
&lt;chris&gt; ai-bot: !describe https://the-brannons.com/chris-and-bird.jpg
&lt;ai-bot&gt; 
&lt;ai-bot&gt; The image features a bearded man with a bird perched on his shoulder. The man has an odd facial expression, almost as if he is looking into the camera. The bird appears to be a parrot and is sitting close to the man's ear.
&lt;ai-bot&gt; 
&lt;ai-bot&gt; The scene takes place in a room furnished with a couch and a chair. The couch is positioned in the background, while the chair occupies a smaller portion of the frame on the left side.
&lt;chris&gt; ai-bot: Tell me about his beard?
&lt;ai-bot&gt; 
&lt;ai-bot&gt; The bearded man in the image has an odd facial expression, making his appearance quite distinctive. His beard seems to be well-groomed and likely represents a fashionable or stylish choice in his personal grooming.
&lt;chris&gt; ai-bot: What do you mean by odd facial expression?
&lt;ai-bot&gt; 
&lt;ai-bot&gt; By odd facial expression, I refer to the man's facial features looking unnatural, exaggerated, or distorted. This may include an awkward smile, squinting, or an unusual or contorted countenance. The phrase "what the heck" implies that the person's appearance seems out of the ordinary or unexpected, which adds a humorous and quirky touch to the image.
# Switch back to text chat, selecting a model and resetting conversation state:
&lt;chris&gt; ai-bot: !switch-to-text-model llama2-uncensored
&lt;ai-bot&gt; Text model switched.
&lt;chris&gt; ai-bot: Somebody on the Internet told me that I can believe everything I read on the Internet?  Were they full of shit?
&lt;ai-bot&gt; Unfortunately, no. This statement is not true. There are many websites and sources on the internet that provide incorrect or misleading information. It's important to be careful when using any online source for information and to verify it by checking multiple sources or contacting a reliable expert.
&lt;chris&gt; ai-bot: But Wikipedia is always reliable!
&lt;ai-bot&gt; Wikipedia is an excellent resource, but it's not infallible. Like any encyclopedia, it can be edited and updated by anyone with access to the website. While most of the information on Wikipedia is accurate, there are some inaccuracies and errors that may exist due to human error or deliberate vandalism. It's always a good idea to cross-reference information from different sources before relying solely on any one source.
&lt;chris&gt; ai-bot: Could you tell me about some known examples of errors in Wikipedia?  Otherwise, I'm just gonna assume you're full of it.
&lt;ai-bot&gt; Sure! One well-known example of an error in Wikipedia is the article on Abraham Lincoln. The entry stated that Lincoln was the first president to receive a patent for his invention - which was actually issued after he left office. Another example is the article on the Apollo 11 mission, which claimed that Neil Armstrong's quote "One small step" was misquoted from an interview with President John F. Kennedy. However, NASA confirmed that the quote used in Wikipedia w
&lt;ai-bot&gt; as
&lt;ai-bot&gt; accurate and had been recorded during the mission.
</code></pre>
<h3 id="why"><a href="#why" rel="nofollow noopener">#</a>Why?</h3>
<p>Because most of the people in the group behind this are hard-core free software
people to some degree or other.  We want to see access to AI liberated and
democratized, I.E., free as in freedom.  And almost as importantly, it sounded
like fun when we were bored on a Friday afternoon.</p>
<h3 id="legal"><a href="#legal" rel="nofollow noopener">#</a>Legal</h3>
<p>Licensed as GPL-2 to match the license of <code>irc-go</code>.</p>
<h3 id="thanks"><a href="#thanks" rel="nofollow noopener">#</a>Thanks</h3>
<p>The group behind ollama-bot consists of Kirk, Tyler, Mike, and Chris, plus the peanut gallery on IRC.</p>
<p>We use the <a href="https://git.zx2c4.com/irc-go" rel="nofollow noopener">irc-go</a> package from Jason Donenfeld.
This seems to have been an excellent choice.</p>
<p>This project would not be possible without <a href="https://ollama.ai/" rel="nofollow noopener">ollama</a>.</p>
<p>Code hosting is graciously provided by the fine people at <a href="https://2mb.solutions/" rel="nofollow noopener">2MB Solutions</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral CEO confirms 'leak' of new open source AI model nearing GPT4 performance (353 pts)]]></title>
            <link>https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/</link>
            <guid>39208213</guid>
            <pubDate>Wed, 31 Jan 2024 19:32:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/">https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/</a>, See on <a href="https://news.ycombinator.com/item?id=39208213">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
		<section>
			
			<p><time title="2024-01-31T18:44:56+00:00" datetime="2024-01-31T18:44:56+00:00">January 31, 2024 10:44 AM</time>
			</p>
			
		</section>
		<div>
					<p><img width="750" height="420" src="https://venturebeat.com/wp-content/uploads/2024/01/cfr0z3n_overhead_view_of_the_eiffel_tower_rising_from_a_landsca_d374695a-bca7-4db7-abcd-d30ef5fd0e04.png?fit=750%2C420&amp;strip=all" alt="Overhead view of Eiffel tower in a Paris made of circuit boards."></p><p><span>Credit: VentureBeat made with Midjourney V6</span></p>		</div><!-- .article-media-header -->
	</div><div id="primary" role="main">

			<article id="post-2932274">
				<div>
					<p>The past few days have been a wild ride for the growing open source AI community — even by its fast-moving and freewheeling standards.</p>



<p>Here’s the quick chronology: on or about January 28, a user with the handle “Miqu Dev” <a href="https://huggingface.co/miqudev/miqu-1-70b" target="_blank" rel="noreferrer noopener">posted a set of files on HuggingFace</a>, the leading open source AI model and code sharing platform, that together comprised a seemingly new open source large language model (LLM) labeled “miqu-1-70b.”</p>



<p>The HuggingFace entry, which is still up at the time of this article’s posting, noted that new LLM’s “Prompt format,” how users interact with it, was the same as <a href="https://mistral.ai/">Mistral</a>, the <a href="https://www.ft.com/content/cf939ea4-d96c-4908-896a-48a74381f251" target="_blank" rel="noreferrer noopener">well-funded open source Parisian AI company</a> behind <a href="https://venturebeat.com/ai/mistral-shocks-ai-community-as-latest-open-source-model-eclipses-gpt-3-5-performance/" target="_blank" rel="noreferrer noopener">Mixtral 8x7b</a>, viewed by many to be the top performing open source LLM presently available, a fine-tuned and retrained version of Meta’s Llama 2. </p>



<h2 id="h-posted-on-4chan">Posted on 4chan</h2>



<p>The same day, an anonymous user on 4chan (possibly “Miqu Dev”) posted a <a href="https://boards.4chan.org/g/thread/98696032#p98697258" target="_blank" rel="noreferrer noopener">link to the miqu-1-70b files on 4chan</a>, the notoriously longstanding haven of online memes and toxicity, where users began to notice it.</p>



<p>Some took to X, Elon Musk’s social network formerly known as Twitter, to share the discovery of the model and what appeared to be its exceptionally high performance at common LLM tasks (measured by tests known as benchmarks), approaching the previous leader, OpenAI’s GPT-4 on the <a href="https://eqbench.com/" target="_blank" rel="noreferrer noopener">EQ-Bench</a>. </p>



<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Whatever Miqu is, it has some sort of special sauce. It gets an 83.5 on EQ-Bench (evaluated locally), surpassing *every other LLM in the world except GPT-4*. EQ-Bench has a 0.97 correlation w/ MMLU, and a 0.94 correlation w/ Arena Elo. It *beats* Mistral Medium – at Q4_K_M. I… <a href="https://t.co/0gOOPjxjPD">pic.twitter.com/0gOOPjxjPD</a></p>— N8 Programs (@N8Programs) <a href="https://twitter.com/N8Programs/status/1752441060133892503?ref_src=twsrc%5Etfw">January 30, 2024</a></blockquote>
</div></figure>



<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Might be late but I am now 100% convinced that Miqu is the same model that's accessible as Mistral-Medium on Perplexity Labs. It was plausible that it knows standard puzzles, but there ain't no way in Hell a pranker has tuned it to identically phrase the responses in Russian too. <a href="https://t.co/zZMcpspXch">pic.twitter.com/zZMcpspXch</a></p>— Teortaxes▶️ (@teortaxesTex) <a href="https://twitter.com/teortaxesTex/status/1752427812466593975?ref_src=twsrc%5Etfw">January 30, 2024</a></blockquote>
</div></figure>



<h2 id="h-mistral-quantized">Mistral quantized?</h2>



<p>Machine learning (ML) researchers took notice on <a href="https://www.linkedin.com/posts/maxime-labonne_new-open-source-llm-competes-with-gpt-activity-7158414579671199744-MFfA/" target="_blank" rel="noreferrer noopener">LinkedIn</a>, as well. </p>



<div><p><em>“Does ‘miqu’ stand for MIstral QUantized? We don’t know for sure, but this quickly became one of, if not the best open-source LLM,”</em> wrote Maxime Labonne, an ML scientist at JP Morgan &amp; Chase, one of the world’s largest banking and financial companies.<em> “Thanks to @152334H, we also now have a good unquantized version of miqu here: <a href="https://lnkd.in/g8XzhGSM">https://lnkd.in/g8XzhGSM</a></em></p><p><em>The investigation continues. Meanwhile, we might see fine-tuned versions of miqu outperforming GPT-4 pretty soon.</em>“</p></div>



<p><a href="https://huggingface.co/docs/optimum/concept_guides/quantization" target="_blank" rel="noreferrer noopener">Quantization</a> in ML refers to a technique used to make it possible to run certain AI models on less powerful computers and chips by replacing specific long numeric sequences in a model’s architecture with shorter ones. </p>



<p>Users speculated “Miqu” might be a new Mistral model being covertly “leaked” by the company itself into the world — especially since Mistral is known for <a href="https://venturebeat.com/ai/mistral-ai-bucks-release-trend-by-dropping-torrent-link-to-new-open-source-llm/" target="_blank" rel="noreferrer noopener">dropping new models and updates without fanfare</a> through esoteric and technical means — or perhaps an employee or customer gone rouge.</p>



<h2 id="h-confirmation-from-the-top">Confirmation from the top</h2>



<p>Well, today it appears we finally have confirmation of the latter of those possibilities: Mistral co-founder and CEO Arthur Mensch took to X to clarify: <em>“An over-enthusiastic employee of one of our early access customers leaked a quantised (and watermarked) version of an old model we trained and distributed quite openly…</em></p>



<p><em>To quickly start working with a few selected customers, we retrained this model from Llama 2 the minute we got access to our entire cluster — the pretraining finished on the day of Mistral 7B release. We’ve made good progress since — stay tuned!<a href="https://twitter.com/arthurmensch/status/1752737462663684344/history"></a>“</em></p>



<figure><div>
<blockquote data-width="500" data-dnt="true"><div lang="en" dir="ltr"><p>An over-enthusiastic employee of one of our early access customers leaked a quantised (and watermarked) version of an old model we trained and distributed quite openly.</p><p>To quickly start working with a few selected customers, we retrained this model from Llama 2 the minute we got…</p></div>— Arthur Mensch (@arthurmensch) <a href="https://twitter.com/arthurmensch/status/1752737462663684344?ref_src=twsrc%5Etfw">January 31, 2024</a></blockquote>
</div></figure>



<p>Hilariously, Mensch also appears to have taken to the illicit HuggingFace post not to demand a takedown, but leaving a comment that the poster “might consider attribution.”</p>



<figure></figure>



<p>Still, with Mensch’s note to “stay tuned!” it appears that not only is Mistral training a version of this so-called “Miqu” model that approaches GPT-4 level performance, but it may, in fact, match or exceed it, if his comments are to be interpreted generously. </p>



<h2 id="h-a-pivotal-moment-in-open-source-ai-and-beyond">A pivotal moment in open source AI and beyond?</h2>



<p>That would be a watershed moment not just for open source generative AI but the entire field of AI and computer science: <a href="https://openai.com/research/gpt-4" target="_blank" rel="noreferrer noopener">since its release back in March 2023</a>, GPT-4 has remained the most powerful and highest performing LLM in the world by most benchmarks. Not even any of <a href="https://venturebeat.com/ai/google-gemini-is-not-even-as-good-as-gpt-3-5-turbo-researchers-find/" target="_blank" rel="noreferrer noopener">Google’s presently available, long-rumored Gemini models</a> have been able to eclipse it — yet (according to some measures, the current Gemini models are <a href="https://venturebeat.com/ai/google-gemini-is-not-even-as-good-as-gpt-3-5-turbo-researchers-find/" target="_blank" rel="noreferrer noopener">actually worse than the older OpenAI GPT-3.5 mode</a>l). </p>



<p>The release of an open source GPT-4 class model, which would presumably be functionally free to use, would likely place enormous competitive pressure on OpenAI and its subscription tiers, especially as more enterprises look to open source models, or a mixture of open source and closed source, to power their applications, <a href="https://venturebeat.com/ai/how-enterprises-are-using-open-source-llms-16-examples/">as VentureBeat’s founder and CEO Matt Marshall recently reported</a>. OpenAI may retain the edge with its faster GPT-4 Turbo and GPT-4V (vision), but the writing on the wall is pretty clear: the open source AI community is catching up fast. Will OpenAI have enough of a head start, and a metaphorical “moat” with its <a href="https://venturebeat.com/ai/openai-launches-gpt-store-but-revenue-sharing-is-still-to-come/">GPT Store</a> and other features, to remain in the top spot for LLMs? </p>
<p><strong>VentureBeat's mission</strong> is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. <a href="https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=bottomBoilerplate" data-type="URL" data-id="/newsletters/">Discover our Briefings.</a></p><!-- Boilerplate CSS for "after" -->				</div><!-- .article-content -->

									
				
			</article><!-- #post-2932274 .article-wrapper -->


		</div></div>]]></description>
        </item>
    </channel>
</rss>