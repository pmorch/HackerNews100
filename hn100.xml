<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 30 Nov 2023 16:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[CSAR: European Parliament rejects mass scanning of private messages (233 pts)]]></title>
            <link>https://edri.org/our-work/csar-european-parliament-rejects-mass-scanning-of-private-messages/</link>
            <guid>38472198</guid>
            <pubDate>Thu, 30 Nov 2023 11:25:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://edri.org/our-work/csar-european-parliament-rejects-mass-scanning-of-private-messages/">https://edri.org/our-work/csar-european-parliament-rejects-mass-scanning-of-private-messages/</a>, See on <a href="https://news.ycombinator.com/item?id=38472198">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content" role="main">

		

<div>

            <!-- if post type is event, print postmeta + timezone here-->
	   
							
			
							<p>
					On 22 November, the European Parliament officially adopted its position on the draft ‘Regulation laying down rules to prevent and combat child sexual abuse’ (CSAR). With strong support for this position from all seven European political groups, this marks a positive development for human rights in one of the most controversial draft European Union (EU) laws in recent memory.				</p>
			
<!-- if post type is event, print location, organiser, registration link here -->
			
				<p>
					By <strong>EDRi</strong> · November 30, 2023				</p>
			

			
					</div>


<section>

	
			<div>
	<p>On 22 November, <a href="https://www.europarl.europa.eu/news/en/press-room/20231117IPR12219/child-sexual-abuse-online-meps-ready-to-start-negotiations" target="_blank" rel="noopener">the European Parliament officially adopted its position on the draft ‘Regulation laying down rules to prevent and combat child sexual abuse’</a> (CSAR). With strong support for this position from all seven European political groups, this marks a <b>positive</b><b> </b><b>development </b><b>for human rights </b><b>in one of the most controversial draft E</b><b>uropean</b><b> </b><b>Union (EU)</b><b> laws in recent memory</b>.</p>
<p>EDRi has long advocated against the <b>CSAR’s </b><b>mass scanning and encryption-breaking measures</b> proposed in 2022 by the EU executive’s unit for home affairs. We are reassured, therefore, that the Parliament has listened to the evidence and the rule of law. At the same time, <b>we are still </b><b>far from the end of the</b><b> legislative process.</b> This means that we must stay alert to how the other two law-making institutions – the Council of EU Member States and the European Commission – respond. Will they agree with the Parliament that new EU laws need to respect fundamental rights? Or will they double down on ‘Chat Control’?</p>
<h3>The position of the European Parliament</h3>
<p>As we explained when <a href="https://edri.org/our-work/eu-parliament-committee-rejects-mass-scanning-of-private-and-encrypted-communications/">this position was </a><a href="https://edri.org/our-work/eu-parliament-committee-rejects-mass-scanning-of-private-and-encrypted-communications/" target="_blank" rel="noopener">provisionally agreed</a><a href="https://edri.org/our-work/eu-parliament-committee-rejects-mass-scanning-of-private-and-encrypted-communications/"> by the </a><a href="https://edri.org/our-work/eu-parliament-committee-rejects-mass-scanning-of-private-and-encrypted-communications/">Parliament’s</a><a href="https://edri.org/our-work/eu-parliament-committee-rejects-mass-scanning-of-private-and-encrypted-communications/"> Civil Liberties </a><a href="https://edri.org/our-work/eu-parliament-committee-rejects-mass-scanning-of-private-and-encrypted-communications/">C</a><a href="https://edri.org/our-work/eu-parliament-committee-rejects-mass-scanning-of-private-and-encrypted-communications/">ommittee </a><a href="https://edri.org/our-work/eu-parliament-committee-rejects-mass-scanning-of-private-and-encrypted-communications/">on 14</a><a href="https://edri.org/our-work/eu-parliament-committee-rejects-mass-scanning-of-private-and-encrypted-communications/"> November</a>, it is a clear political statement that <b>even </b><b>the most </b><b>important </b><b>societal </b><b>aims do not justify measures at any cost.</b> EU fundamental rights law requires that limitations on people’s rights are necessary for the aim they seek to achieve – including being objectively effective and the least intrusive possible – and proportionate. That means that their broader impact must be reasonable.</p>
<p>On this basis, <b>the Parliament firmly rejected </b><b>rules which would force companies to scan huge volumes of people’s private messages </b>– instead now requiring there to be reasonable suspicion. <a href="https://www.bitsoffreedom.nl/wp-content/uploads/2023/05/20230426-opinion-legal-services-on-csar-proposal.pdf" target="_blank" rel="noopener">Lawyers for the Council of EU Member States </a><a href="https://www.bitsoffreedom.nl/wp-content/uploads/2023/05/20230426-opinion-legal-services-on-csar-proposal.pdf">had </a><a href="https://www.bitsoffreedom.nl/wp-content/uploads/2023/05/20230426-opinion-legal-services-on-csar-proposal.pdf">previously</a><a href="https://www.bitsoffreedom.nl/wp-content/uploads/2023/05/20230426-opinion-legal-services-on-csar-proposal.pdf"> made </a>a<a href="https://www.bitsoffreedom.nl/wp-content/uploads/2023/05/20230426-opinion-legal-services-on-csar-proposal.pdf">n</a><a href="https://www.bitsoffreedom.nl/wp-content/uploads/2023/05/20230426-opinion-legal-services-on-csar-proposal.pdf"> unprecedented warning</a> that the original proposal would violate the essence of the right to privacy. In EU-speak, this is a damning assessment, because case law from the Court of Justice of the EU has always upheld that while rights can be limited for justifiable reasons, <b>the “essential core” of any human right must never be violated.</b> The Parliament has clearly listened to this warning.</p>
<h3>EDRi’s work and coalition</h3>
<p>Since before the home affairs unit (DG HOME) first put forward this law, <b>EDRi has been </b><b>on the front lines </b>urging the EU to ensure that measures to tackle the serious crime of child sexual abuse are in line with human rights rules. Yet our ‘<a href="https://edri.org/our-work/chat-control-10-principles-to-defend-children-in-the-digital-age/">10 principles to </a><a href="https://edri.org/our-work/chat-control-10-principles-to-defend-children-in-the-digital-age/" target="_blank" rel="noopener">defend children in the digital age</a>’ were ignored in the original legislative proposal (so too were <a href="https://edri.org/our-work/leaked-opinion-of-the-commission-sets-off-alarm-bells-for-mass-surveillance-of-private-communications/">concerns from the Commission’s own review board</a>).</p>
<p>Thanks to the <b>EDRi-led <a href="https://stopscanningme.eu/" target="_blank" rel="noopener">Stop Scanning Me</a> campaign</b>, thousands of people across Europe have since sounded the alarm about the draft measures. <a href="https://docs.google.com/document/d/13Aeex72MtFBjKhExRTooVMWN9TC-pbH-5LEaAbMF91Y/edit" target="_blank" rel="noopener">Scientists and researchers across the world</a> have been unambiguous that as proposed, the measures would undermine encryption, putting everyone’s digital information at risk of harm. And <a href="https://edri.org/our-work/most-criticised-eu-law-of-all-time/" target="_blank" rel="noopener">other stakeholders</a> such as journalists, youth activists, lawyers and survivors’ associations have warned how they could be put at risk by the proposal.</p>
<p><img decoding="async" loading="lazy" src="https://edri.org/wp-content/uploads/2023/10/6.jpeg" alt="" width="2048" height="1153" srcset="https://edri.org/wp-content/uploads/2023/10/6.jpeg 2048w, https://edri.org/wp-content/uploads/2023/10/6-300x169.jpeg 300w, https://edri.org/wp-content/uploads/2023/10/6-1024x577.jpeg 1024w, https://edri.org/wp-content/uploads/2023/10/6-768x432.jpeg 768w, https://edri.org/wp-content/uploads/2023/10/6-1536x865.jpeg 1536w, https://edri.org/wp-content/uploads/2023/10/6-180x101.jpeg 180w" sizes="(max-width: 2048px) 100vw, 2048px"></p>
<h3>What’s next?</h3>
<p>As their position has been officially adopted, <b>the European Parliament is ready to enter “trilogues”</b>. This is closed-door negotiations between lead Parliamentarians and the Council of EU Member State governments. However, in this case, the <b>Council does not </b><b>currently </b><b>have </b><b>a</b><b> negotiating mandate with which to enter the trilogues</b>. In fact, <a href="https://netzpolitik.org/2023/internes-protokoll-immer-mehr-eu-staaten-gegen-unverhaeltnismaessige-chatkontrolle/#2023-10-18_AA_AStV_CSAR_Weisung" target="_blank" rel="noopener">EU Member State governments have been divided on the issue</a>, with <b>some countries refusing to listen to technological </b><b>and</b><b> legal </b><b>reality</b><b>.</b> Fortunately, many others have stood up against their colleagues, rightly warning that the <b>EU cannot give a </b><i><b>carte blanche </b></i><b>to the destruction of digital security</b>, privacy and anonymity.</p>
<p>Back in July, <a href="https://edri.org/our-work/council-poised-to-endorse-mass-surveillance-as-official-position-for-csa-regulation/" target="_blank" rel="noopener">EDRi urged Member State governments not to agree</a> to a position which would usher in the mass surveillance of everyone’s digital private lives. People subsequently took to the streets in Germany, Sweden and several other locations to urge their governments not to accept ‘Chat Control’. These efforts paid off, with the <b>governments of Germany, Austria, Poland, Estonia and Slovenia <a href="https://netzpolitik.org/2023/internes-protokoll-immer-mehr-eu-staaten-gegen-unverhaeltnismaessige-chatkontrolle/#2023-10-18_AA_AStV_CSAR_Weisung">reportedly</a> taking a firm stance against the misguided proposal, and France </b><b>subsequently raising major concerns.</b></p>



<p><b>W</b><b>ithout a Council position, the legislative process for the CSAR is currently in limbo</b>. The Spanish Presidency of the Council is reportedly attempting to push through a position before the end of their mandate (end December 2023). But at the time of writing, they do not have a text on the table – let alone political agreement from a sufficient number of Member States.</p>
<p>Even if the Council is able to agree their position soon, <b>it is highly unlikely that they would be able to pass </b><b>the </b><b>law </b><b>during this political mandate</b><b>.</b> That’s because we are approaching a once-in-five-years event: European elections. In June 2024, a new Parliament will be elected, and subsequently, a new set of European Commissioners will be appointed. According to <a href="https://www.euractiv.com/section/eu-institutions/news/leak-eu-parliament-sets-deadlines-for-closing-files-under-this-mandate/">leaked documents</a>, this means that <b>any </b><b>negotiations between the legislative institutions must wrap up by early February 2024.</b></p>
<p>Whilst trilogue negotiations on even simple laws can take months, <b>it would be unprecedented for such a complex and sensitive file </b><b>as CSAR</b><b> – with so </b><b>much </b><b>at stake – to be pushed through in </b><b>such a</b><b> short time.</b> This current limbo also means that the political figurehead for the CSAR, controversial Swedish Commissioner Ylva Johansson, is unlikely to remain in post for this proposal’s life-cycle.</p>
<p><a href="https://edri.org/our-work/european-commission-must-uphold-privacy-security-and-free-expression-by-withdrawing-new-law/">EDRi has consistently advocated</a> that laws to tackle CSA online must be in line with fundamental rights law and with objective evidence of effectiveness. With this latest step, the European Parliament has firmly rebutted DG HOME’s attempt to pass a law which bitterly fails on both of these counts. Whilst the next steps for this law are not clear, this is nevertheless a huge milestone for the protection of digital human rights, and <b>we are counting on the Council not to go backwards.</b></p>
</div>

			
			

			
</section>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Q-Transformer (103 pts)]]></title>
            <link>https://qtransformer.github.io/</link>
            <guid>38471942</guid>
            <pubDate>Thu, 30 Nov 2023 10:42:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qtransformer.github.io/">https://qtransformer.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=38471942">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
        <p>
            <h2>
                <strong><span size="+6">Q-Transformer: </span></strong> <br> Scalable Offline Reinforcement Learning via Autoregressive Q-Functions <br> 
            </h2>
        </p>
        <div>
                <ul>
                <br>
                <li>Yevgen Chebotar*</li> <li>Quan Vuong*</li> <li>Alex Irpan</li> <li>Karol Hausman</li> <li>Fei Xia</li> <li>Yao Lu</li>  <li>Aviral Kumar</li> <br>
                 <li>Tianhe Yu</li> <li>Alexander Herzog</li> <li>Karl Pertsch</li> <li>Keerthana Gopalakrishnan</li> <li>Julian Ibarz</li> <li>Ofir Nachum</li>  <br>
                  <li>Sumedh Sontakke</li> <li>Grecia Salazar</li> <li>Huong T Tran</li> <li>Jodilyn Peralta</li> <li>Clayton Tan</li> <li>Deeksha Manjunath</li> <br>
                  <li>Jaspiar Singht</li> <li>Brianna Zitkovich</li> <li>Tomas Jackson</li>  <li>Kanishka Rao</li> <li>Chelsea Finn</li> <li>Sergey Levine</li>
                <br>
		   <i>*equal contribution</i>
		<br>
                    <a href="http://deepmind.com/">
                    <img src="https://qtransformer.github.io/img/deepmind.png" height="50px"> </a>
                    
                </ul>
            </div>


        

        <div>
		<p>
	    		<img src="https://qtransformer.github.io/img/qt_animation.gif">
		</p>
                <h3>
                    Abstract
                </h3>
                <p>
                    In this work, we present a scalable reinforcement learning method for training multi-task policies from large offline datasets that can leverage both human demonstrations and autonomously collected data. Our method uses a Transformer to provide a scalable representation for Q-functions trained via offline temporal difference backups. We therefore refer to the method as Q-Transformer. By discretizing each action dimension and representing the Q-value of each action dimension as separate tokens, we can apply effective high-capacity sequence modeling techniques for Q-learning. We present several design decisions that enable good performance with offline RL training, and show that Q-Transformer outperforms prior offline RL algorithms and imitation learning techniques on a large diverse real-world robotic manipulation task suite.
                </p>
            </div>

	<!--
	<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/UuKAp9a6wMs" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>
	-->

<div>
	<h3>
	    Approach
	</h3>

        <p>
	    <img src="https://qtransformer.github.io/img/qt_title_overview.png">
	</p>
	<p>
		<img src="https://qtransformer.github.io/img/qt_robot_frames.png">
        </p>
	<p>
	We first describe how to enable using Transformers for Q-learning by applying discretization and autoregression of the action space.
	The classical way for learning a Q-function using TD-learning is based on the Bellman update rule:
	</p><p>
	        <img src="https://qtransformer.github.io/img/bellman_original.png">
	</p>
	<p>
	We change the Bellman update to be performed for each action dimension by transforming the original MDP of the problem into an MDP where each
	action dimension is treated as a separate step for Q-learning. In particular, given the action dimensionality <i>d<sub>A</sub></i>, the new Bellman update rule is:
	</p>
	<p>
	        <img src="https://qtransformer.github.io/img/bellman_qt.png">
	</p>
	<p>
	This means that for each intermediate action dimension we maximize over the next action dimension given the same state, 
	and for the final action dimension we use the first action dimension from the next state. This decomposition makes sure that the maximization 
	within the Bellman update remains tractable while ensuring that we still solve the original MDP problem.
	</p>
		
	<p>
	        <img src="https://qtransformer.github.io/img/qt_update_fig.png">
	</p>
	<p>
	In order to account for the distribution shift during offline learning, we introduce a simple regularization technique 
	that minimizes unseen actions (in the discretized case unseen action bins) to the lowest value. To accelerate learning, we also employ
	Monte-Carlo (MC) returns that use the original return-to-go from a given episode and n-step returns that can skip per-dimension maximization.
	</p>	    
 </div>
	    
<div id="videos">
	<h3>
	    Results and Videos
	</h3>
	<p>
        In our experiments, we start by evaluating Q-Transformer on a suite of real world tasks introduced in the RT-1 paper while limiting the data per task to only contain 100 human demonstrations. In addition to demonstrations, we also add autonomously collected failed episodes, resulting in a dataset of 38,000 positive examples from demos and 20,000 negative autonomously collected examples.
        </p><p>
	    <img src="https://qtransformer.github.io/img/real_results_table.png">
	</p>
	<p>
	    <video id="qt-robot-videos" autoplay="" loop="" muted="" playsinline="" controls="">
       		 <source src="https://qtransformer.github.io/videos/qt_robot_videos.mp4" type="video/mp4">
        	Your browser does not support the video tag.
   	    </video>
	</p><p>

		    
    Compared to such baselines as <a href="https://arxiv.org/abs/2212.06817">RT-1</a>,
    <a href="https://arxiv.org/abs/2110.06169">IQL</a> and
    <a href="https://arxiv.org/abs/2106.01345">Decision Transformer (DT)</a>, Q-Transformer can effectively utilize autonomous episodes to significantly improve on such skills as picking from and placing objects into drawers, moving objects near targets and closing and opening drawers.
	improve on such skills as picking from and placing objects into drawers, moving objects near targets and closing and opening drawers.
	</p>

	<p>
	We also benchmark our method in a challenging simulated picking task, where only ~8% of the data are positive examples, and the rest are
	noisy negative examples. Q-learning methods, such as 
    <a href="https://arxiv.org/abs/1806.10293">QT-Opt</a>
    , 
    <a href="https://arxiv.org/abs/2110.06169">IQL</a>
    ,
    <a href="https://arxiv.org/abs/2111.05424">AW-Opt</a>
     and our Q-Transformer are generally performing better on this task
	as they are able to utilize negative examples to learn policies through dynamic programming.
	</p>
	<p>
	    <img src="https://qtransformer.github.io/img/sim_exps.png">
	</p>
	
	<p>
        Ablating our decision choices on this picking task, we notice that both the conservative regularizer and MC returns are important for retaining the performance. Switching to a Softmax regularizer, which is similar to a 
        <a href="https://arxiv.org/abs/2006.04779">CQL</a>
         regularizer for discrete actions, performs significantly worse as it bounds the policy too much to the distribution of the data, showing that our choice of the regularizer works better for such tasks.
	</p>
	  <p>
	    <img src="https://qtransformer.github.io/img/qt_ablations.png">
	</p>
		  
	<p> 
	 We also ablate n-step returns and notice that although introducing bias they can help us achieving the same high performance in much fewer number of gradient steps,
	making them an efficient choice in many problems.
	</p>
	 <p>
	    <img src="https://qtransformer.github.io/img/nstep_table.png">
	</p>
		  
	<p>
        We also try to run our Q-Transformer on a much larger dataset, scaling up the number of positive examples to 115,000 and the number of negative examples to 185,000 resulting in 300,000 episodes. Q-Transformer is still able to learn from this large dataset and even provide some improvement over the RT-1 BC baseline.
	</p>
		
	 <p>
	    <img src="https://qtransformer.github.io/img/large_offline_results.png">
	</p>

	<p>
	Finally, we use the Q-function trained by Q-Transformer as an affordance model in combination with a language planner,
	similar to the 
    <a href="https://arxiv.org/abs/2204.01691">SayCan</a>
    work.
	</p>
	 <p>
	    <img src="https://qtransformer.github.io/img/affordance_perf.png">
	</p>
	<p>
        Q-Transformer affordance estimation works better than the previously used Q-functions trained with 
        QT-Opt
        , especially when combined with relabeling non-sampled tasks as negatives for the current task during training. As Q-Transformer does not require sim-to-real training that was used for the 
        QT-Opt
        training, it makes it easier to use it in the absence of suitable simulations.
	</p>

	<p>
        To test the full planning + execution system, we use Q-Transformer for both affordance estimation and the actual policy execution, where it shows to outperform the previous combination of 
        QT-Opt
         and 
        RT-1.
	</p>
	 <p>
	    <img src="https://qtransformer.github.io/img/saycan_perf.png">
	</p>
	
	<p>
	    <video id="qt-saycan-videos" autoplay="" loop="" muted="" playsinline="" controls="">
       		 <source src="https://qtransformer.github.io/videos/qt_saycan.mp4" type="video/mp4">
        	Your browser does not support the video tag.
   	    </video>
	</p><p>
    As can be seen in the examples of task affordance values for a given image, Q-Transformer can provide high-quality affordance values that can be used in downstream plan-and-execute frameworks.
</p></div>

<!--
<div class="row">
    <div class="col-md-8 col-md-offset-2">
	<h3>
	    Video
	</h3>
	<video id="v0" width="100%" playsinline muted loop controls>
	       <source src="img/Q-Transformer_Video.mov" type="video/mp4">
        </video>		
    </div>
</div>
-->
         <div>
                <h3>
                    Citation 
                </h3>
                
            </div>
        <div>
            <p>
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
        </div>
<!--
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
The authors would like to acknowledge TODO and the greater teams at Google DeepMind for their feedback and contributions.
                    <br><br>
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
	    -->


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ripgrep is faster than grep, ag, Git grep, ucg, pt, sift (2016) (218 pts)]]></title>
            <link>https://blog.burntsushi.net/ripgrep/</link>
            <guid>38471822</guid>
            <pubDate>Thu, 30 Nov 2023 10:21:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.burntsushi.net/ripgrep/">https://blog.burntsushi.net/ripgrep/</a>, See on <a href="https://news.ycombinator.com/item?id=38471822">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
      

      <article>
      <p>In this article I will introduce a new command line search tool,
<a href="https://github.com/BurntSushi/ripgrep"><code>ripgrep</code></a>,
that combines the usability of
<a href="https://github.com/ggreer/the_silver_searcher">The Silver Searcher</a>
(an <a href="http://beyondgrep.com/"><code>ack</code></a> clone) with the
raw performance of GNU grep. <code>ripgrep</code> is fast, cross platform (with binaries
available for Linux, Mac and Windows) and written in
<a href="https://www.rust-lang.org/">Rust</a>.</p>
<p><code>ripgrep</code> is available on
<a href="https://github.com/BurntSushi/ripgrep">Github</a>.</p>
<p>We will attempt to do the impossible: a fair benchmark comparison between
several popular code search tools. Specifically, we will dive into a series of
25 benchmarks that substantiate the following claims:</p>
<ul>
<li>For both searching single files <em>and</em> huge directories of files, no other
tool obviously stands above <code>ripgrep</code> in either performance or correctness.</li>
<li><code>ripgrep</code> is the only tool with proper Unicode support that doesn’t make
you pay dearly for it.</li>
<li>Tools that search many files at once are generally <em>slower</em> if they use
memory maps, not faster.</li>
</ul>
<p>As someone who has worked on text search in Rust in their free time for the
last 2.5 years, and as the author of both <code>ripgrep</code> and
<a href="https://github.com/rust-lang-nursery/regex">the underlying regular expression engine</a>,
I will use this opportunity to provide detailed insights into the performance
of each code search tool. No benchmark will go unscrutinized!</p>
<p><strong>Target audience</strong>: Some familiarity with Unicode, programming and some
experience with working on the command line.</p>
<p><strong>NOTE</strong>: I’m hearing reports from some people that <code>rg</code> isn’t as fast as I’ve
claimed on their data. I’d love to help explain what’s going on, but to do
that, I’ll need to be able to reproduce your results. If you
<a href="https://github.com/BurntSushi/ripgrep/issues">file an issue</a>
with something I can reproduce, I’d be happy to try and explain it.</p>
<h2 id="screenshot-of-search-results">Screenshot of search results</h2>
<p><a href="https://burntsushi.net/stuff/ripgrep1.png"><img src="https://burntsushi.net/stuff/ripgrep1.png" alt="A screenshot of a sample search with ripgrep"></a></p>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#introducing-ripgrep">Introducing ripgrep</a>
<ul>
<li><a href="#pitch">Pitch</a></li>
<li><a href="#anti-pitch">Anti-pitch</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#whirlwind-tour">Whirlwind tour</a></li>
<li><a href="#regex-syntax">Regex syntax</a></li>
</ul>
</li>
<li><a href="#anatomy-of-a-grep">Anatomy of a grep</a>
<ul>
<li><a href="#background">Background</a></li>
<li><a href="#gathering-files-to-search">Gathering files to search</a></li>
<li><a href="#searching">Searching</a>
<ul>
<li><a href="#regex-engine">Regex engine</a></li>
<li><a href="#literal-optimizations">Literal optimizations</a></li>
<li><a href="#mechanics">Mechanics</a></li>
</ul>
</li>
<li><a href="#printing">Printing</a></li>
</ul>
</li>
<li><a href="#methodology">Methodology</a>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#benchmark-runner">Benchmark runner</a></li>
<li><a href="#environment">Environment</a></li>
</ul>
</li>
<li><a href="#code-search-benchmarks">Code search benchmarks</a>
<ul>
<li><a href="#linux_literal_default"><code>linux_literal_default</code></a></li>
<li><a href="#linux_literal"><code>linux_literal</code></a></li>
<li><a href="#linux_literal_casei"><code>linux_literal_casei</code></a></li>
<li><a href="#linux_word"><code>linux_word</code></a></li>
<li><a href="#linux_unicode_word"><code>linux_unicode_word</code></a></li>
<li><a href="#linux_re_literal_suffix"><code>linux_re_literal_suffix</code></a></li>
<li><a href="#linux_alternates"><code>linux_alternates</code></a></li>
<li><a href="#linux_alternates_casei"><code>linux_alternates_casei</code></a></li>
<li><a href="#linux_unicode_greek"><code>linux_unicode_greek</code></a></li>
<li><a href="#linux_unicode_greek_casei"><code>linux_unicode_greek_casei</code></a></li>
<li><a href="#linux_no_literal"><code>linux_no_literal</code></a></li>
</ul>
</li>
<li><a href="#single-file-benchmarks">Single file benchmarks</a>
<ul>
<li><a href="#subtitles_literal"><code>subtitles_literal</code></a></li>
<li><a href="#subtitles_literal_casei"><code>subtitles_literal_casei</code></a></li>
<li><a href="#subtitles_alternate"><code>subtitles_alternate</code></a></li>
<li><a href="#subtitles_alternate_casei"><code>subtitles_alternate_casei</code></a></li>
<li><a href="#subtitles_surrounding_words"><code>subtitles_surrounding_words</code></a></li>
<li><a href="#subtitles_no_literal"><code>subtitles_no_literal</code></a></li>
</ul>
</li>
<li><a href="#bonus-benchmarks">Bonus benchmarks</a>
<ul>
<li><a href="#everything"><code>everything</code></a></li>
<li><a href="#nothing"><code>nothing</code></a></li>
<li><a href="#context"><code>context</code></a></li>
<li><a href="#huge"><code>huge</code></a></li>
</ul>
</li>
<li><a href="#conclusions">Conclusions</a></li>
</ul>
<h2 id="introducing-ripgrep">Introducing ripgrep</h2>
<h3 id="pitch">Pitch</h3>
<p>Why should you use <code>ripgrep</code> over any other search tool? Well…</p>
<ul>
<li>It can replace many use cases served by other search tools
because it contains most of their features and is generally faster. (See
<a href="https://github.com/BurntSushi/ripgrep/blob/master/FAQ.md#posix4ever">the FAQ</a>
for more details on whether ripgrep can truly replace grep.)</li>
<li>Like other tools specialized to code search, ripgrep defaults to recursive
directory search and won’t search files ignored by your <code>.gitignore</code> files.
It also ignores hidden and binary files by default. ripgrep also implements
full support for <code>.gitignore</code>, whereas there are many bugs related to that
functionality in other code search tools claiming to provide the same
functionality.</li>
<li>ripgrep can search specific types of files. For example, <code>rg -tpy foo</code>
limits your search to Python files and <code>rg -Tjs foo</code> excludes Javascript
files from your search. ripgrep can be taught about new file types with
custom matching rules.</li>
<li>ripgrep supports many features found in <code>grep</code>, such as showing the context
of search results, searching multiple patterns, highlighting matches with
color and full Unicode support. Unlike GNU grep, ripgrep stays fast while
supporting Unicode (which is always on).</li>
<li>ripgrep has optional support for switching its regex engine to use PCRE2.
Among other things, this makes it possible to use look-around and
backreferences in your patterns, which are not supported in ripgrep’s default
regex engine. PCRE2 support is enabled with <code>-P</code>.</li>
<li>ripgrep supports searching files in text encodings other than UTF-8, such
as UTF-16, latin-1, GBK, EUC-JP, Shift_JIS and more. (Some support for
automatically detecting UTF-16 is provided. Other text encodings must be
specifically specified with the <code>-E/--encoding</code> flag.)</li>
<li>ripgrep supports searching files compressed in a common format (gzip, xz,
lzma, bzip2 or lz4) with the <code>-z/--search-zip</code> flag.</li>
<li>ripgrep supports arbitrary input preprocessing filters which could be PDF
text extraction, less supported decompression, decrypting, automatic encoding
detection and so on.</li>
</ul>
<p>In other words, use ripgrep if you like speed, filtering by default, fewer
bugs and Unicode support.</p>
<h3 id="anti-pitch">Anti-pitch</h3>
<p>I’d like to try to convince you why you <em>shouldn’t</em> use <code>ripgrep</code>. Often, this
is far more revealing than reasons why I think you <em>should</em> use <code>ripgrep</code>.</p>
<p>Despite initially not wanting to add every feature under the sun to ripgrep,
over time, ripgrep has grown support for most features found in other
file searching tools. This includes searching for results spanning across
multiple lines, and opt-in support for PCRE2, which provides look-around and
backreference support.</p>
<p>At this point, the primary reasons not to use ripgrep probably consist of one
or more of the following:</p>
<ul>
<li>You need a portable and ubiquitous tool. While ripgrep works on Windows,
macOS and Linux, it is not ubiquitous and it does not conform to any
standard such as POSIX. The best tool for this job is good old grep.</li>
<li>There still exists some other feature (or bug) not listed in this README that
you rely on that’s in another tool that isn’t in ripgrep.</li>
<li>There is a performance edge case where ripgrep doesn’t do well where another
tool does do well. (Please file a bug report!)</li>
<li>ripgrep isn’t possible to install on your machine or isn’t available for your
platform. (Please file a bug report!)</li>
</ul>
<h3 id="installation">Installation</h3>
<p>The binary name for <code>ripgrep</code> is <code>rg</code>.</p>
<p><a href="https://github.com/BurntSushi/ripgrep/releases">Binaries for <code>ripgrep</code> are available for Windows, Mac and
Linux.</a> Linux binaries are
static executables. Windows binaries are available either as built with MinGW
(GNU) or with Microsoft Visual C++ (MSVC). When possible, prefer MSVC over GNU,
but you’ll need to have the
<a href="https://www.microsoft.com/en-us/download/details.aspx?id=48145">Microsoft VC++ 2015 redistributable</a>
installed.</p>
<p>If you’re a <strong>Homebrew</strong> user, then you can install it like so:</p>






<p>If you’re an <strong>Archlinux</strong> user, then you can install <code>ripgrep</code> from the
official repos:</p>






<p>If you’re a <strong>Rust programmer</strong>, <code>ripgrep</code> can be installed with <code>cargo</code>:</p>






<p>If you’d like to build <code>ripgrep</code> from source, that is also easy to do.
<code>ripgrep</code> is written in Rust, so you’ll need to grab a
<a href="https://www.rust-lang.org/">Rust installation</a> in order to compile it.
<code>ripgrep</code> compiles with Rust 1.9 (stable) or newer. To build:</p>



<div><pre tabindex="0"><code data-lang="sh"><span><span>$ git clone git://github.com/BurntSushi/ripgrep
</span></span><span><span>$ <span>cd</span> ripgrep
</span></span><span><span>$ cargo build --release
</span></span><span><span>$ ./target/release/rg --version
</span></span><span><span>0.1.2</span></span></code></pre></div>


<p>If you have a Rust nightly compiler, then you can enable optional SIMD
acceleration like so, which is used in all benchmarks reported in this article.</p>



<div><pre tabindex="0"><code data-lang="sh"><span><span><span>RUSTFLAGS</span><span>=</span><span>"-C target-cpu=native"</span> cargo build --release --features simd-accel</span></span></code></pre></div>


<h3 id="whirlwind-tour">Whirlwind tour</h3>
<p>The command line usage of <code>ripgrep</code> doesn’t differ much from other tools that
perform a similar function, so you probably already know how to use <code>ripgrep</code>.
The full details can be found in <code>rg --help</code>, but let’s go on a whirlwind tour.</p>
<p><code>ripgrep</code> detects when its printing to a terminal, and will automatically
colorize your output and show line numbers, just like The Silver Searcher.
Coloring works on Windows too! Colors can be controlled more granularly with
the <code>--color</code> flag.</p>
<p>One last thing before we get started: generally speaking, <code>ripgrep</code> assumes the
input is reading is UTF-8. However, if ripgrep notices a file is encoded as
UTF-16, then it will know how to search it. For other encodings, you’ll need to
explicitly specify them with the <code>-E/--encoding</code> flag.</p>
<p>To recursively search the current directory, while respecting all <code>.gitignore</code>
files, ignore hidden files and directories and skip binary files:</p>






<p>The above command also respects all <code>.rgignore</code> files, including in parent
directories. <code>.rgignore</code> files can be used when <code>.gitignore</code> files are
insufficient. In all cases, <code>.rgignore</code> patterns take precedence over
<code>.gitignore</code>.</p>
<p>To ignore all ignore files, use <code>-u</code>. To additionally search hidden files
and directories, use <code>-uu</code>. To additionally search binary files, use <code>-uuu</code>.
(In other words, “search everything, dammit!”) In particular, <code>rg -uuu</code> is
similar to <code>grep -a -r</code>.</p>



<div><pre tabindex="0"><code data-lang="sh"><span><span>$ rg -uu foobar  <span># similar to `grep -r`</span>
</span></span><span><span>$ rg -uuu foobar  <span># similar to `grep -a -r`</span></span></span></code></pre></div>


<p>(Tip: If your ignore files aren’t being adhered to like you expect, run your
search with the <code>--debug</code> flag.)</p>
<p>Make the search case insensitive with <code>-i</code>, invert the search with <code>-v</code> or
show the 2 lines before and after every search result with <code>-C2</code>.</p>
<p>Force all matches to be surrounded by word boundaries with <code>-w</code>.</p>
<p>Search and replace (find first and last names and swap them):</p>



<div><pre tabindex="0"><code data-lang="sh"><span><span>$ rg <span>'([A-Z][a-z]+)\s+([A-Z][a-z]+)'</span> --replace <span>'$2, $1'</span></span></span></code></pre></div>


<p>Named groups are supported:</p>



<div><pre tabindex="0"><code data-lang="sh"><span><span>$ rg <span>'(?P&lt;first&gt;[A-Z][a-z]+)\s+(?P&lt;last&gt;[A-Z][a-z]+)'</span> --replace <span>'$last, $first'</span></span></span></code></pre></div>


<p>Up the ante with full Unicode support, by matching any uppercase Unicode letter
followed by any sequence of lowercase Unicode letters (good luck doing this
with other search tools!):</p>



<div><pre tabindex="0"><code data-lang="sh"><span><span>$ rg <span>'(\p{Lu}\p{Ll}+)\s+(\p{Lu}\p{Ll}+)'</span> --replace <span>'$2, $1'</span></span></span></code></pre></div>


<p>Search only files matching a particular glob:</p>






<!-- raw HTML omitted -->
<p>Or exclude files matching a particular glob:</p>






<!-- raw HTML omitted -->
<p>Search only HTML and CSS files:</p>






<p>Search everything except for Javascript files:</p>






<p>To see a list of types supported, run <code>rg --type-list</code>. To add a new type, use
<code>--type-add</code>, which must be accompanied by a pattern for searching (<code>rg</code> won’t
persist your type settings):</p>



<div><pre tabindex="0"><code data-lang="sh"><span><span>$ rg --type-add <span>'foo:*.{foo,foobar}'</span> -tfoo bar</span></span></code></pre></div>


<p>The type <code>foo</code> will now match any file ending with the <code>.foo</code> or <code>.foobar</code>
extensions.</p>
<h3 id="regex-syntax">Regex syntax</h3>
<p>The syntax supported is
<a href="https://docs.rs/regex/1.*/regex/#syntax">documented as part of Rust’s regex library</a>.</p>
<h2 id="anatomy-of-a-grep">Anatomy of a grep</h2>
<p>Before we dive into benchmarks, I thought it might be useful to provide a high
level overview of how a grep-like search tool works, with a special focus on
<code>ripgrep</code> in particular. The goal of this section is to provide you with a bit
of context that will help make understanding the analysis for each benchmark
easier.</p>
<h3 id="background">Background</h3>
<p>Modulo parsing command line arguments, the first “real” step in any search tool
is figuring out what to search. Tools like <code>grep</code> don’t try to do anything
smart: they simply search the files given to it on the command line. An
exception to this is the <code>-r</code> flag, which will cause <code>grep</code> to recursively
search all files in the current directory. Various command line flags can be
passed to control which files are or aren’t searched.</p>
<p><a href="http://beyondgrep.com/"><code>ack</code></a> came along and turned this type of default
behavior on its head. Instead of trying to search everything by default, <code>ack</code>
tries to be smarter about what to search. For example, it will recursively
search your current directory <em>by default</em>, and it will automatically skip over
any source control specific files and directories (like <code>.git</code>). This method
of searching undoubtedly has its own pros and cons, because it tends to make
the tool “smarter,” which is another way of saying “opaque.” That is, when
you really do need the tool to search everything, it can sometimes be tricky
to know how to speak the right incantation for it to do so. With that said,
being smart by default is incredibly convenient, especially when “smart” means
“figure out what to search based on your source control configuration.” There’s
no shell alias that can do that with <code>grep</code>.</p>
<p>All of the other search tools in this benchmark share a common ancestor with
either <code>grep</code> or <code>ack</code>. <code>sift</code> is descended from <code>grep</code>, while <code>ag</code>, <code>ucg</code>, and
<code>pt</code> are descended from <code>ack</code>. <code>ripgrep</code> is a bit of a hybrid because it was
specifically built to be good at searching huge files just like <code>grep</code>, but at
the same time, provide the “smart” kind of default searching like <code>ack</code>.
Finally, <code>git grep</code> deserves a bit of a special mention. <code>git grep</code> is very
similar to plain <code>grep</code> in the kinds of options it supports, but its default
mode of searching is clearly descended from <code>ack</code>: it will only search files
checked into source control.</p>
<p>Of course, both types of search tools have <em>a lot</em> in common, but there are a
few broad distinctions worth making if you allow yourself to squint your eyes a
bit:</p>
<ul>
<li><code>grep</code>-like tools need to be really good at searching large files, so the
performance of the underlying regex library is paramount.</li>
<li><code>ack</code>-like tools need to be really good at recursive directory traversal
while also applying ignore rules from files like <code>.gitignore</code> quickly.
<code>ack</code>-like tools are built to run many searches in parallel, so the raw
performance of the underlying regex library can be papered over somewhat
while still being faster than single-threaded “search everything” tools like
<code>grep</code>. If the “smarts” of <code>ack</code> also mean skipping over that 2GB artifact
in your directory tree, then the performance difference becomes even bigger.</li>
<li><code>ripgrep</code> tries hard to combine the best of both worlds. Not only is its
underlying regex engine very fast, but it parallelizes searches and tries to
be smart about what it searches too.</li>
</ul>
<h3 id="gathering-files-to-search">Gathering files to search</h3>
<p>For an <code>ack</code>-like tool, it is important to figure out which files to search in
the current directory. This means using a very fast recursive directory
iterator, filtering file paths quickly and distributing those file paths to a
pool of workers that actually execute the search.</p>
<p>Directory traversal can be tricky because some recursive directory
iterators make more stat calls than are strictly necessary, which
can have a large impact on performance. It can be terribly difficult
to track down these types of performance problems because they
tend to be buried in a standard library somewhere. <a href="http://benhoyt.com/writings/scandir/">Python only
recently fixed this</a>, for
example. Rest assured that <a href="https://docs.rs/walkdir"><code>ripgrep</code> uses a recursive directory
iterator</a> that makes the minimum number
of system calls possible.</p>
<p>Filtering file paths requires not only respecting rules given at the command
line (e.g., <code>grep</code>’s <code>--include</code> or <code>--exclude</code>) flags, but also requires
reading files like <code>.gitignore</code> and applying their rules correctly to all file
paths. Even the mere act of looking for a <code>.gitignore</code> file in every directory
can have measurable overhead! Otherwise, the key performance challenge with
this functionality is making sure you don’t try to match every ignore rule
individually against every file path. Large repositories like the Linux kernel
source tree have over a hundred <code>.gitignore</code> files with thousands of rules
combined.</p>
<p>Finally, distributing work to other threads for searching requires some kind of
synchronization. One solution is a mutex protected ring buffer that acts as
a sort of queue, but there are lock-free solutions that might be faster.
Rust’s ecosystem is so great that I was able to reuse a lock-free <a href="https://github.com/kinghajj/deque">Chase-Lev
work-stealing queue</a> for distributing work
to a pool of searchers. Every <em>other</em> tool that parallelizes work in this
benchmark uses a variant of a mutex protected queue. (<code>sift</code> and <code>pt</code> might not
fit this criteria, since they use Go channels, and I haven’t followed any
implementation improvements to that code for a few years.)</p>
<h3 id="searching">Searching</h3>
<p>Searching is the heart of any of these tools, and we could dig ourselves into a
hole on just this section alone and not come out alive for at least 2.5 years.
(Welcome to “How Long I’ve Been Working On Text Search In Rust.”) Instead, we
will lightly touch on the big points.</p>
<h4 id="regex-engine">Regex engine</h4>
<p>First up is the regex engine. Every search tool supports some kind of syntax
for regular expressions. Some examples:</p>
<ul>
<li><code>foo|bar</code> matches any literal string <code>foo</code> or <code>bar</code></li>
<li><code>[a-z]{2}_[a-z]+</code> matches two lowercase latin letters, followed by an
underscore, followed by one or more lowercase latin letters.</li>
<li><code>\bfoo\b</code> matches the literal <code>foo</code> only when it is surrounded by word
boundaries. For example, the <code>foo</code> in <code>foobar</code> won’t match but it will in
<code>I love foo.</code>.</li>
<li><code>(\w+) \1</code> matches any sequence of word characters followed by a space and
followed by exactly the word characters that were matched previously. The
<code>\1</code> in this example is called a “back-reference.” For example, this pattern
will match <code>foo foo</code> but not <code>foo bar</code>.</li>
</ul>
<p>Regular expression engines themselves tend to be divided into two categories
predominantly based on the features they expose. Regex engines that provide
support for all of the above tend to use an approach called <em>backtracking</em>,
which is typically quite fast, but can be very slow on some inputs. “Very
slow” in this case means that it might take exponential time to complete a
search. For example, try running this Python code:</p>



<div><pre tabindex="0"><code data-lang="python"><span><span><span>&gt;&gt;&gt;</span> <span>import</span> <span>re</span>
</span></span><span><span><span>&gt;&gt;&gt;</span> <span>re</span><span>.</span><span>search</span><span>(</span><span>'(a*)*c'</span><span>,</span> <span>'a'</span> <span>*</span> <span>30</span><span>)</span></span></span></code></pre></div>


<p>Even though both the regex and the search string are tiny, it will take a very
long time to terminate, and this is because the underlying regex engine uses
backtracking, and can therefore take exponential time to answer some queries.</p>
<p>The other type of regex engine generally supports fewer features and is based
on finite automata. For example, these kinds of regex engines typically don’t
support back-references. Instead, these regex engines will often provide a
guarantee that <em>all searches</em>, regardless of the regex or the input, will
complete in linear time with respect to the search text.</p>
<p>It’s worth pointing out that neither type of engine has a monopoly on average
case performance. There are examples of regex engines of both types that are
blazing fast. With that said, here’s a breakdown of some search tools and the
type of regex engine they use:</p>
<ul>
<li>GNU grep and <code>git grep</code> each use their own hand-rolled finite automata based
engine.</li>
<li><code>ripgrep</code> uses
<a href="https://github.com/rust-lang-nursery/regex">Rust’s regex library</a>,
which uses finite automata.</li>
<li>The Silver Searcher and Universal Code Grep use <a href="http://www.pcre.org/">PCRE</a>,
which uses backtracking.</li>
<li>Both The Platinum Searcher and sift use
<a href="https://golang.org/pkg/regexp/">Go’s regex library</a>,
which uses finite automata.</li>
</ul>
<p>Both Rust’s regex library and Go’s regex library share Google’s
<a href="https://github.com/google/re2">RE2</a>
as a common ancestor.</p>
<p>Finally, both tools that use PCRE (The Silver Searcher and Universal Code Grep)
are susceptible to worst case backtracking behavior. For example:</p>



<div><pre tabindex="0"><code data-lang="sh"><span><span>$ cat wat
</span></span><span><span>c
</span></span><span><span>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
</span></span><span><span>c
</span></span><span><span>$ ucg <span>'(a*)*c'</span> wat
</span></span><span><span>terminate called after throwing an instance of <span>'FileScannerException'</span>
</span></span><span><span>  what<span>()</span>:  PCRE2 match error: match limit exceeded
</span></span><span><span>Aborted <span>(</span>core dumped<span>)</span></span></span></code></pre></div>


<p>The Silver Searcher fails similarly. It reports the first line as a match and
neglects the match in the third line. The rest of the search tools benchmarked
in this article handle this case without a problem.</p>
<h4 id="literal-optimizations">Literal optimizations</h4>
<p>Picking a fast regex engine is important, because every search tool will need
to rely on it sooner or later. Nevertheless, even the performance of the
fastest regex engine can be dwarfed by the time it takes to search for a simple
literal string.
<a href="https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm">Boyer-Moore</a>
is the classical algorithm that is used to find a substring, and even today, it
is hard to beat for general purpose searching. One of its defining qualities is
its ability to skip some characters in the search text by pre-computing a small
skip table at the beginning of the search.</p>
<p>On modern CPUs, the key to making a Boyer-Moore implementation fast is not
necessarily the number of characters it can skip, but how fast it can identify
a candidate for a match. For example, most Boyer-Moore implementations look for
the <em>last</em> byte in a literal. Each occurrence of that byte is considered a
candidate for a match by Boyer-Moore. It is only at this point that Boyer-Moore
can use its precomputed table to skip characters, which means you still need a
fast way of identifying the candidate in the first place. Thankfully,
specialized routines found in the C standard library, like
<a href="http://man7.org/linux/man-pages/man3/memchr.3.html"><code>memchr</code></a>,
exist for precisely this purpose. Often, <code>memchr</code> implementations are compiled
down to SIMD instructions that examine <em>sixteen</em> bytes in a single loop
iteration. This makes it very fast. On my system, <code>memchr</code> often gets
throughputs at around several gigabytes a second. (In my own experiments,
Boyer-Moore with <code>memchr</code> can be just as fast as an explicit SIMD
implementation using the
<a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=PCMPESTR&amp;expand=786">PCMPESTRI</a>
instruction, but this is something I’d like to revisit.)</p>
<p>For a search tool to compete in most benchmarks, either it or its regex engine
needs to use some kind of literal optimizations. For example, Rust’s regex
library goes to great lengths to extract both prefix and suffix literals from
every pattern. The following patterns all have literals extracted from them:</p>
<ul>
<li><code>foo|bar</code> detects <code>foo</code> and <code>bar</code></li>
<li><code>(a|b)c</code> detects <code>ac</code> and <code>bc</code></li>
<li><code>[ab]foo[yz]</code> detects <code>afooy</code>, <code>afooz</code>, <code>bfooy</code> and <code>bfooz</code></li>
<li><code>(foo)?bar</code> detects <code>foobar</code> and <code>bar</code></li>
<li><code>(foo)*bar</code> detects <code>foo</code> and <code>bar</code></li>
<li><code>(foo){3,6}</code> detects <code>foofoofoo</code></li>
</ul>
<p>If any of these patterns appear at the <em>beginning</em> of a regex, Rust’s regex
library will notice them and use them to find candidate matches very quickly
(even when there is more than one literal detected). While Rust’s core regex
engine is fast, it is still faster to look for literals first, and only drop
down into the core regex engine when it’s time to verify a match.</p>
<p>The best case happens when an entire regex can be broken down into a single
literal or an alternation of literals. In that case, the core regex engine
won’t be used at all!</p>
<p>A search tool in particular has an additional trick up its sleeve. Namely,
since most search tools do line-by-line searching (The Silver Searcher is a
notable exception, which does multiline searching by default), they can extract
<em>non-prefix</em> or “inner” literals from a regex pattern, and search for those to
identify candidate <em>lines</em> that match. For example, the regex <code>\w+foo\d+</code> could
have <code>foo</code> extracted. Namely, when a candidate line is found, <code>ripgrep</code> will
find the beginning and end of only that line, and then run the full regex
engine on the entire line. This lets <code>ripgrep</code> very quickly skip through files
by staying out of the regex engine. Most of the search tools we benchmark here
don’t perform this optimization, which can leave a lot of performance on the
table, especially if your core regex engine isn’t that fast.</p>
<p>Handling the case of multiple literals (e.g., <code>foo|bar</code>) is just as important.
GNU grep uses a little known algorithm similar to
<a href="https://en.wikipedia.org/wiki/Commentz-Walter_algorithm">Commentz-Walter</a>
for searching multiple patterns. In short, Commentz-Walter is what you get when
you merge
<a href="https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm">Aho-Corasick</a>
with Boyer-Moore: a skip table with a <em>reverse</em> automaton.
Rust’s regex library, on the other hand, will either use plain Aho-Corasick,
or, when enabled, a special
<a href="https://github.com/rust-lang-nursery/regex/blob/3de8c44f5357d5b582a80b7282480e38e8b7d50d/src/simd_accel/teddy128.rs">SIMD algorithm called
Teddy</a>,
which was invented by Geoffrey Langdale as part of the
<a href="https://github.com/01org/hyperscan">Hyperscan regex library</a>
developed by Intel. This SIMD algorithm will prove to be at least one of the
key optimizations that propels <code>ripgrep</code> past GNU grep.</p>
<p>The great thing about this is that <code>ripgrep</code> doesn’t have to do much of this
literal optimization work itself. Most of it is done inside Rust’s regex
library, so every consumer of that library gets all these performance
optimizations automatically!</p>
<h4 id="mechanics">Mechanics</h4>
<p>Repeat after me: Thou Shalt Not Search Line By Line.</p>
<p>The naive approach to implementing a search tool is to read a file line by line
and apply the search pattern to each line individually. This approach is
problematic primarily because, in the common case, finding a match is <em>rare</em>.
Therefore, you wind up doing a ton of work parsing out each line all for
naught, because most files simply aren’t going to match at all in a large
repository of code.</p>
<p>Not only is finding every line extra work that you don’t need to do, but you’re
also paying a huge price in overhead. Whether you’re searching for a literal or
a regex, you’ll need to start and stop that search for every single line in a
file. The overhead of each search will be your undoing.</p>
<p>Instead, all search tools find a way to search a big buffer of bytes all at
once. Whether that’s memory mapping a file, reading an entire file into memory
at once or incrementally searching a file using a constant sized intermediate
buffer, they all find a way to do it to some extent. There are some exceptions
though. For example, tools that use memory maps or read entire files into
memory either can’t support <code>stdin</code> (like Universal Code Grep), or revert to
line-by-line searching (like The Silver Searcher). Tools that support
incremental searching (<code>ripgrep</code>, GNU grep and <code>git grep</code>) can use its
incremental approach on any file or stream with no problems.</p>
<p>There’s a reason why not every tool implements incremental search: it’s <em>hard</em>.
For example, you need to consider all of the following in a fully featured
search tool:</p>
<ul>
<li>Line counting, when requested.</li>
<li>If a read from a file ends in the middle of a line, you need to do the
bookkeeping required to make sure the incomplete line isn’t searched until
more data is read from the file.</li>
<li>If a line is too long to fit into your buffer, you need to decide to either
give up or grow your buffer to fit it.</li>
<li>Your searcher needs to know how to invert the match.</li>
<li>Worst of all: your searcher needs to be able to show the context of a match,
e.g., the lines before and after a matching line. For example, consider the
case of a match that appears at the beginning of your buffer. How do you show
the previous lines if they aren’t in your buffer? You guessed it: you need to
carry over at least as many lines that are required to satisfy a context
request from buffer to buffer.</li>
</ul>
<p>It’s a steep price to pay in terms of code complexity, but by golly, is it
worth it. You’ll need to read on to the benchmarks to discover when it is
faster than memory maps!</p>
<h3 id="printing">Printing</h3>
<p>It might seem like printing is such a trivial step, but it must be done with at
least some care. For example, you can’t just print matches from each search
thread as you find them, because you really don’t want to interleave the search
results of one file with the search results of another file. A naive approach
to this is to serialize the printer so that only one thread can print to it at
a time. This is problematic though, because if a search thread acquires a lock
to the printer before starting the search (and not releasing it until it has
finished searching one file), you’ll end up also serializing every search as
well, effectively defeating your entire approach to parallelism.</p>
<p>All code search tools in this benchmark that parallelize search therefore write
results to some kind of intermediate buffer <em>in memory</em>. This enables all of
the search threads to actually perform a search in parallel. The printing still
needs to be serialized, but we’ve reduced that down to simply dumping the
contents of the intermediate buffer to <code>stdout</code>. Using an in memory buffer
might set off alarm bells: what if you search a 2GB file and every line
matches? Doesn’t that lead to excessive memory usage? The answer is: “Why,
yes, indeed it does!” The key insight is that the common case is returning far
fewer matches than there are total lines searched. Nevertheless, there are
ways to mitigate excessive memory usage. For example, if <code>ripgrep</code> is used to
search <code>stdin</code> or a single file, then it will write search results directly
to <code>stdout</code> and forgo the intermediate buffer because it just doesn’t need
it. (<code>ripgrep</code> should also do this when asked to <em>not</em> do any parallelism,
but I haven’t gotten to it yet.) In other words, pick two: space, time or
correctness.</p>
<p>Note that the details aren’t quite the same in every tool. Namely, while The
Silver Searcher and Universal Code Grep write matches as structured data to
memory (i.e., an array of <code>match</code> structs or something similar), both <code>git grep</code> and <code>ripgrep</code> write the actual output to a dynamically growable string
buffer in memory. While either approach does seem to be fast enough, <code>git grep</code>
and <code>ripgrep</code> have to do things this way because they support incremental
search where as The Silver Searcher always memory maps the entire file and
Universal Code Grep always reads the entire contents of the file into memory.
The latter approach can refer back to the file’s contents in memory when doing
the actual printing, where as neither <code>git grep</code> nor <code>ripgrep</code> can do that.</p>
<h2 id="methodology">Methodology</h2>
<h3 id="overview">Overview</h3>
<p>Coming up with a good and fair benchmark is <em>hard</em>, and I have assuredly made
some mistakes in doing so. In particular, there are so many variables to
control for that testing every possible permutation isn’t feasible. This means
that the benchmarks I’m presenting here are <em>curated</em>, and, given that I am the
author of one of the tools in the benchmark, they are therefore also <em>biased</em>.
Nevertheless, even if I fail in my effort to provide a fair benchmark suite, I
do hope that some of you may find my analysis interesting, which will try to
explain the results in each benchmark. The analysis is in turn heavily biased
toward explaining my own work, since that is the implementation I’m most
familiar with. I have, however, read at least part of the source code of every
tool I benchmark, including their underlying regex engines.</p>
<p>In other words, I’m pretty confident that I’ve gotten the <em>details</em> correct,
but I could have missed something in the bigger picture. Because of that, let’s
go over some important insights that guided construction of this benchmark.</p>
<ul>
<li>Focus on the problem that an <em>end user</em> is trying to solve. For example, we
split the entire benchmark in two: one for searching a large directory of
files and one for searching a single large file. The former might correspond
to an end user searching their code while the latter might correspond to an
end user searching logs. As we will see, these two use cases have markedly
different performance characteristics. A tool that is good at one isn’t
necessarily good at the other. (The premise of <code>ripgrep</code> is that it is
possible to be good at both!)</li>
<li>Apply <em>end user</em> problems more granularly as well. For example, most
searches result in few hits relative to the corpus searched, so prefer
benchmarks that report few matches. Another example: I hypothesize, based on
my own experience, that most searches use patterns that are simple literals,
alternations or very light regexes, so bias the benchmarks towards those
types of patterns.</li>
<li>Almost every search tool has slightly different default behavior, and these
behavioral changes can have an impact on performance. There is some value in
looking at “out-of-the-box” performance, and we therefore do look at a
benchmark for that, but stopping there is a bit unsatisfying. If our goal is
to do a <em>fair</em> comparison, then we need to at least try to convince each tool
to do roughly the same work, <strong>from the perspective of an end user</strong>. A good
example of this is reporting line numbers. Some tools don’t provide a way of
disabling line counting, so when doing comparisons between tools that do, we
need to explicitly enable line numbers. This is important, because counting
lines can be quite costly! A good <em>non-example</em> of this is if one tool uses
memory maps and another uses an intermediate buffer. This is an
implementation choice, and not one that alters what the user actually sees,
therefore comparing those two implementation choices in a benchmark is
completely fair (assuming an analysis that points it out).</li>
</ul>
<p>With that out of the way, let’s get into the nitty gritty. First and foremost,
what tools are we benchmarking?</p>
<ul>
<li><a href="https://github.com/BurntSushi/ripgrep"><code>ripgrep</code> (rg)</a> (v0.1.2) - You’ve
heard enough about this one already.</li>
<li><a href="https://www.gnu.org/software/grep/">GNU grep</a> (v2.25) - Ol’ reliable.</li>
<li><a href="https://www.kernel.org/pub/software/scm/git/docs/git-grep.html">git grep</a>
(v2.7.4) -
Like <code>grep</code>, but built into <code>git</code>. Only works well in <code>git</code> repositories.</li>
<li><a href="https://github.com/ggreer/the_silver_searcher">The Silver Searcher (ag)</a>
(commit <code>cda635</code>, using PCRE 8.38) - Like <code>ack</code>, but written in C and much
faster. Reads your <code>.gitignore</code> files just like <code>ripgrep</code>.</li>
<li><a href="https://github.com/gvansickle/ucg">Universal Code Grep (ucg)</a> (commit
<code>487bfb</code>, using PCRE 10.21 <strong>with the JIT enabled</strong>) - Also like <code>ack</code> but
written in C++, and only searches files from a whitelist, and doesn’t support
reading <code>.gitignore</code>.</li>
<li><a href="https://github.com/monochromegane/the_platinum_searcher">The Platinum Searcher
(pt)</a> (commit
<code>509368</code>) - Written in Go and does support <code>.gitignore</code> files.</li>
<li><a href="https://github.com/svent/sift">sift</a> (commit <code>2d175c</code>) - Written in Go and
supports <code>.gitignore</code> files with an optional flag, but generally prefers
searching everything (unlike every other tool in this list except for
<code>grep</code>).</li>
</ul>
<p>Notably absent from this list is <code>ack</code>. I chose not to benchmark it because,
at the time of writing, <code>ack</code> was much slower than the other tools in this
list. However, <a href="https://beyondgrep.com/ack3/">ack 3 is now in beta</a> and
includes some performance improvements, sometimes decreasing search times by
half.</p>
<h3 id="benchmark-runner">Benchmark runner</h3>
<p>The benchmark runner is a Python program (requires at least Python 3.5) that
you can use to not only run the benchmarks themselves, but download the corpora
used in the benchmarks as well. The script is called <code>benchsuite</code> and
<a href="https://github.com/BurntSushi/ripgrep/blob/master/benchsuite/benchsuite">is in the <code>ripgrep</code>
repository</a>.
You can use it like so:</p>



<div><pre tabindex="0"><code data-lang="sh"><span><span>$ git clone git://github.com/BurntSushi/ripgrep
</span></span><span><span>$ <span>cd</span> ripgrep/benchsuite
</span></span><span><span><span># WARNING! This downloads several GB of data, and builds the Linux kernel.</span>
</span></span><span><span><span># This took about 15 minutes on a high speed connection.</span>
</span></span><span><span><span># Tip: try `--download subtitles-ru` to grab the smallest corpus, but you'll</span>
</span></span><span><span><span># be limited to running benchmarks for only that corpus.</span>
</span></span><span><span>$ ./benchsuite --dir /path/to/data/dir --download all
</span></span><span><span><span># List benchmarks available.</span>
</span></span><span><span>$ ./benchsuite --dir /path/to/data/dir --list
</span></span><span><span><span># Run a benchmark.</span>
</span></span><span><span><span># Omit the benchmark name to run all benchmarks. The full suite can take around</span>
</span></span><span><span><span># 30 minutes to complete on default settings and 120 minutes to complete with</span>
</span></span><span><span><span># --warmup-iter 3 --bench-iter 10.</span>
</span></span><span><span>$ ./benchsuite --dir /path/to/data/dir <span>'^subtitles_ru_literal$'</span></span></span></code></pre></div>


<p>If you don’t have all of the code search tools used in the benchmarks, then
pass <code>--allow-missing</code> to give <code>benchsuite</code> permission to skip running them. To
save the raw data (the timing for every command run), pass <code>--raw /path/to/raw.csv</code>.</p>
<p>The benchmark runner tries to do a few basic things for us to help reduce the
chance that we get misleading data:</p>
<ul>
<li>Every benchmarked command is run three times before being measured as a
“warm up.” Specifically, this is to ensure that the corpora being searched
is already in the operating system’s page cache. If we didn’t do this, we
might end up benchmarking disk I/O, which is not only uninteresting for our
purposes, but is probably not a common end user scenario. It’s more likely
that you’ll be executing lots of searches against the same corpus (at least,
I know I do).</li>
<li>Every benchmarked command is run ten times, with a timing recorded for each
run. The final “result” of that command is its distribution (mean +/-
standard deviation). If I were a statistician, I could probably prove that
ten samples is insufficient. Nevertheless, getting more samples takes more
time, and for the most part, the variance is very low.</li>
</ul>
<p>Each individual benchmark definition is responsible for making sure each
command is trying to do similar work as other commands we’re comparing it to.
For example, we need to be careful to enable and disable Unicode support in GNU
grep where appropriate, because full Unicode handling can make GNU grep run
very slowly. Within each benchmark, there are often multiple variables of
interest. To account for this, I’ve added labels like <code>(ASCII)</code> or
<code>(whitelist)</code> where appropriate. We’ll dig into those labels in more detail
later.</p>
<p>Please also feel encouraged to add your own benchmarks if you’d like to play
around. The benchmarks are in the top-half of the file, and it should be fairly
straight-forward to copy &amp; paste another benchmark and modify it. Simply
defining a new benchmark will make it available. The second half of the script
is the runner itself and probably shouldn’t need to be modified.</p>
<h3 id="environment">Environment</h3>
<p>The actual environment used to run the benchmarks presented in this article was
a <code>c3.2xlarge</code> instance on Amazon EC2. It ran Ubuntu 16.04, had a Xeon E5-2680
2.8 GHz CPU, 16 GB of memory and an 80 GB SSD (on which the corpora was
stored). This was enough memory to fit all of the corpora in memory. The box
was specifically provisioned for the purpose of running benchmarks, so it was
not doing anything else.</p>
<p>The full log of system setup and commands I used to install each of the search
tools and run benchmarks can be found
<a href="https://github.com/BurntSushi/ripgrep/blob/master/benchsuite/runs/2016-09-20-ubuntu1604-ec2/README.SETUP">here</a>.
I also captured the
<a href="https://github.com/BurntSushi/ripgrep/blob/master/benchsuite/runs/2016-09-20-ubuntu1604-ec2/summary">output of the bench runner (SPOILER ALERT)</a>
and the
<a href="https://github.com/BurntSushi/ripgrep/blob/master/benchsuite/runs/2016-09-20-ubuntu1604-ec2/raw.csv">raw output</a>,
which includes the timings, full set of command line arguments and any
environment variables set for every command run in every benchmark.</p>
<h2 id="code-search-benchmarks">Code search benchmarks</h2>
<p>This is the first half of our benchmarks, and corresponds to an <em>end user</em>
trying to search a large repository of code for a particular pattern.</p>
<p>The corpus used for this benchmark is a <em>built</em> checkout of the Linux kernel,
specifically commit <code>d0acc7</code>. We actually build the Linux kernel because the
process of building the kernel leaves a lot of garbage in the repository that
you probably don’t want to search. This can influence not only the relevance of
the results returned by a search tool, but the performance as well.</p>
<p>All benchmarks run in this section were run in the root of the repository.
Remember, you can see the full
<a href="https://github.com/BurntSushi/ripgrep/blob/master/benchsuite/runs/2016-09-20-ubuntu1604-ec2/raw.csv">raw results of each command</a>
if you like. The benchmark names correspond to the headings below.</p>
<p>Note that since these benchmarks were run on an EC2 instance, which uses a VM,
which in turn can penalize search tools that use memory maps, I’ve also
<a href="https://github.com/BurntSushi/ripgrep/blob/master/benchsuite/runs/2016-09-22-archlinux-cheetah/summary">recorded benchmarks on my local
machine</a>.
My local machine is an Intel i7-6900K 3.2 GHz, 16 CPUs, 64 GB memory and an
SSD. You’ll notice that <code>ag</code> does a lot better (but still worse than <code>rg</code>)
on my machine. Lest you think I’ve chosen results from the EC2 machine because
they paint <code>rg</code> more favorably, rest assured that I haven’t. Namely, <code>rg</code> wins
<em>every single benchmark</em> on my local machine except for <em>one</em>, where as <code>rg</code> is
beat out just slightly by a few tools on some benchmarks on the EC2 machine.</p>
<p>Without further ado, let’s start looking at benchmarks.</p>
<h3 id="linux_literal_default"><code>linux_literal_default</code></h3>
<p><strong>Description</strong>: This benchmark compares the time it takes to execute a simple
literal search using each tool’s default settings. This is an intentionally
unfair benchmark meant to highlight the differences between tools and their
“out-of-the-box” settings.</p>
<p><strong>Pattern</strong>: <code>PM_RESUME</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg         0.349 +/- 0.104 (lines: 16)
</span></span><span><span>ag         1.589 +/- 0.009 (lines: 16)
</span></span><span><span>ucg        0.218 +/- 0.007 (lines: 16)*+
</span></span><span><span>pt         0.462 +/- 0.012 (lines: 16)
</span></span><span><span>sift       0.352 +/- 0.018 (lines: 16)
</span></span><span><span>git grep   0.342 +/- 0.005 (lines: 16)</span></span></code></pre></div>


<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
<li><code>rg == ripgrep</code>, <code>ag == The Silver Searcher</code>, <code>ucg == Universal Code Grep</code>,
<code>pt == The Platinum Searcher</code></li>
</ul>
<p><strong>Analysis</strong>: We’ll first start by actually describing what each tool is doing:</p>
<ul>
<li><code>rg</code> respects the Linux repo’s <code>.gitignore</code> files (of which there are
<code>178</code>(!) of them), and skips hidden and binary files. <code>rg</code> does not count
lines.</li>
<li><code>ag</code> has the same default behavior as <code>rg</code>, except it counts lines.</li>
<li><code>ucg</code> also counts lines, but does not attempt to read <code>.gitignore</code> files.
Instead, it only searches files from an (extensible) whitelist according
to a set of glob rules. For example, both <code>rg</code> and <code>ag</code> will search
<code>fs/jffs2/README.Locking</code> while <code>ucg</code> won’t, because it doesn’t recognize
the <code>Locking</code> extension. (A search tool probably <em>should</em> search that file,
although it does not impact the results of this specific benchmark.)</li>
<li><code>pt</code> has the same default behavior as <code>ag</code>.</li>
<li><code>sift</code> searches everything, including binary files and hidden files. It
<em>should</em> be equivalent to <code>grep -r</code>, for example. It also does not count
lines.</li>
<li><code>git grep</code> should have the same behavior at <code>rg</code>, and similarly does not
count lines. Note though that <code>git grep</code> has a special advantage: it does not
need to traverse the directory hierarchy. It can discover the set of files to
search straight from its git index.</li>
</ul>
<p>The high-order bit to extract from this benchmark is that a naive comparison
between search tools is completely unfair from the perspective of performance,
but is really important if you care about the <em>relevance</em> of results returned
to you. <code>sift</code>, like <code>grep -r</code>, will throw everything it can back at you, which
is totally at odds with the philosophy behind every other tool in this
benchmark: only return results that are <em>probably</em> relevant. Things inside your
<code>.git</code> probably aren’t, for example. (This isn’t to say that <code>sift</code>’s
philosophy is wrong. The tool is clearly intended to be configured by an end
user to their own tastes, which has its own pros and cons.)</p>
<p>With respect to performance, there are two key variables to pay attention to.
They will appear again and again throughout our benchmark:</p>
<ul>
<li>Counting lines <em>can be</em> quite expensive. A naive solution—a loop over every
byte and comparing it to a <code>\n</code>—will be quite slow for example.
<a href="https://github.com/gvansickle/ucg/blob/8bbebc002bbf112d147928f89677cba703d007bb/src/FileScanner_sse4_2.cpp#L190">Universal Code Grep counts lines using SIMD</a>
and
<a href="https://github.com/BurntSushi/ripgrep/blob/919c5c72994edb378706594f6268542983eeee6d/src/search_stream.rs#L549"><code>ripgrep</code> counts lines using packed comparisons (16 bytes at a time)</a>.
However, in the Linux code search benchmarks, because the size of each
individual file is very small and the number of matches is tiny compared
to the corpus size, the time spent counting lines tends to not be so
significant. Especially since every tool in this benchmark parallelizes
search to some degree. When we get to the single-file benchmarks, this
variable will become much more pertinent.</li>
<li>Respecting <code>.gitignore</code> files incurs some amount of overhead. Even though
respecting <code>.gitignore</code> reduces the number of files searched, it can be
slower overall to actually read the patterns, compile them and match them
against every path than to just search every file. This is precisely how
<code>ucg</code> soundly beats <code>ripgrep</code> in this benchmark. (We will control for this
variable in future benchmarks.) In other words, respecting <code>.gitignore</code> is a
feature that improves <em>relevance</em> first and foremost. It is strictly a bonus
if it also happens to improve performance.</li>
</ul>
<p>The specific reasons why supporting <code>.gitignore</code> leads to a slower overall
search are:</p>
<ul>
<li>Every directory descended requires looking for a corresponding <code>.gitignore</code>.
Multiply the number of calls if you support additional ignore files, like
both The Silver Searcher and <code>ripgrep</code> do. The Linux kernel repository has
<code>4,640</code> directories. <code>178</code> of them have <code>.gitignore</code> files.</li>
<li>Each <code>.gitignore</code> file needs to be compiled into something that can match
file paths. Both The Silver Searcher and <code>ripgrep</code> use tricks to make this
faster. For example, simple patterns like <code>/vmlinux</code> or <code>*.o</code> can be matched
using simple literal comparisons or by looking at the file extension of a
candidate path and comparing it directly. For more complex patterns like
<code>*.c.[012]*.*</code>, a full glob matcher needs to be used. The Silver Searcher
uses <code>fnmatch</code> while <code>ripgrep</code> translates all such globs into a single
regular expression which can be matched against a single path all at once.
Doing all this work takes time.</li>
<li>Unlike <code>ag</code>, <code>rg</code> will try to support the full semantics of a <code>.gitignore</code>
file. This means finding <em>every</em> ignore pattern that matches a file path and
giving precedent to the most recently defined pattern. <code>ag</code> will bail on the
first match it sees.</li>
<li>Actually matching a path has non-trivial overhead that must be paid for
<em>every</em> path searched. The compilation phase described above is complex
precisely for making this part faster. We try to stay out of the regex
machinery as best we can, but we can’t avoid it completely.</li>
</ul>
<p>In contrast, a whitelist like the one used by <code>ucg</code> is comparatively easy to
make fast. The set of globs is known upfront, so no additional checks need to
be made while traversing the file tree. Moreover, the globs tend to be of the
<code>*.ext</code> variety, which fall into the bucket of globs that can be matched
efficiently just by looking at the extension of a file path.</p>
<p>The downside of a whitelist is obvious: you might end up missing search results
simply because <code>ucg</code> didn’t know about a particular file extension. You could
always teach <code>ucg</code> about the file extension, but you’re still blind to “unknown
unknowns” (i.e., files that you probably want to search but didn’t know upfront
that you needed to).</p>
<h3 id="linux_literal"><code>linux_literal</code></h3>
<p><strong>Description</strong>: This benchmark runs the same query as in the
<a href="#linux-literal-default"><code>linux_literal_default</code></a>
benchmark, but we try to do a fair comparison. In particular, we run <code>ripgrep</code>
in two modes: one where it respects <code>.gitignore</code> files (corresponding to
the <code>(ignore)</code> label) and one where it uses a whitelist and doesn’t respect
<code>.gitignore</code> (corresponding to the <code>(whitelist)</code> label). The former mode is
comparable to <code>ag</code>, <code>pt</code>, <code>sift</code> and <code>git grep</code>, while the latter mode is
comparable to <code>ucg</code>. We also run <code>rg</code> a third time by explicitly telling it
to use memory maps for search, which matches the implementation strategy used
by <code>ag</code>. <code>sift</code> is run such that it respects <code>.gitignore</code> files and excludes
binary, hidden and PDF files. All commands executed here count lines, because
some commands (<code>ag</code> and <code>ucg</code>) don’t support disabling line counting.</p>
<p><strong>Pattern</strong>: <code>PM_RESUME</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg (ignore)          0.334 +/- 0.053 (lines: 16)
</span></span><span><span>rg (ignore) (mmap)   1.611 +/- 0.009 (lines: 16)
</span></span><span><span>ag (ignore) (mmap)   1.588 +/- 0.011 (lines: 16)
</span></span><span><span>pt (ignore)          0.456 +/- 0.025 (lines: 16)
</span></span><span><span>sift (ignore)        0.630 +/- 0.004 (lines: 16)
</span></span><span><span>git grep (ignore)    0.345 +/- 0.007 (lines: 16)
</span></span><span><span>rg (whitelist)       0.228 +/- 0.042 (lines: 16)+
</span></span><span><span>ucg (whitelist)      0.218 +/- 0.007 (lines: 16)*</span></span></code></pre></div>


<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
</ul>
<p><strong>Analysis</strong>: We have a ton of ground to cover on this one.</p>
<p>First and foremost, the <code>(ignore)</code> vs. <code>(whitelist)</code> variables have a clear
impact on the performance of <code>rg</code>. We won’t rehash all the details from the
analysis in
<a href="#linux-literal-default"><code>linux_literal_default</code></a>,
but switching <code>rg</code> into its whitelist mode brings it into a dead heat with
<code>ucg</code>.</p>
<p>Secondly, <code>ucg</code> is just as fast as <code>ripgrep</code> and <code>git grep (ignore)</code> is just as
fast as <code>rg (ignore)</code>, even though I’ve said that
<code>ripgrep</code> is the fastest. It turns out that <code>ucg</code>, <code>git grep</code> and <code>rg</code> are
pretty evenly matched when searching for plain literals in large repositories.
We will see a stronger separation in later benchmarks. Still, what makes <code>ucg</code>
fast?</p>
<ul>
<li><code>ucg</code> reads the entire file into memory before searching it, which means it
avoids the memory map problem described below. On a code repository, this
approach works well, but it comes with a steep price in the single-file
benchmarks.</li>
<li>It has a fast explicitly SIMD based line counting algorithm. <code>ripgrep</code> has
something similar, but relies on the compiler for autovectorization.</li>
<li><code>ucg</code> uses PCRE2’s JIT, which is <em>insanely</em> fast. In my own very rough
benchmarks, PCRE2’s JIT is one of the few general purpose regex engines that
is competitive with Rust’s regex engine (on regexes that don’t expose PCRE’s
exponential behavior due to backtracking, since Rust’s regex engine doesn’t
suffer from that weakness).</li>
<li><code>ucg</code> parallelizes directory traversal, which is something that <code>ripgrep</code>
doesn’t do. <code>ucg</code> has it a bit easier here because it doesn’t support
<code>.gitignore</code> files. Parallelizing directory traversal while maintaining state
for <code>.gitignore</code> files in a way that scales isn’t a problem I’ve figured out
how to cleanly solve yet.</li>
</ul>
<p>What about <code>git grep</code>? A key performance advantage of <code>git grep</code> is that it
doesn’t need to walk the directory tree, which can save it quite a bit of time.
Its regex engine is also quite fast, and works similarly to GNU grep’s, RE2 and
Rust’s regex engine (i.e., it uses a DFA).</p>
<p>Both <code>sift</code> and <code>pt</code> perform almost as well as <code>ripgrep</code>. In fact, both <code>sift</code>
and <code>pt</code> do implement a parallel recursive directory traversal while
still respecting <code>.gitignore</code> files, which is likely one reason for their
speed. As we will see in future benchmarks, their speed here is misleading.
Namely, they are fast because they stay outside of Go’s regexp engine since the
pattern is a literal. (There will be more discussion on this point later.)</p>
<p>Finally, what’s going on with The Silver Searcher? Is it really that much
slower than everything else? The key here is that its use of memory maps is
making it <em>slower</em>, not faster (in direct contradiction to the claims in its
README).</p>
<p>OK, let’s pause and pop up a level to talk about what this actually means.
First, we need to consider how these search tools fundamentally work. Generally
speaking, a search tool like this has two ways of actually searching files on
disk:</p>
<ol>
<li>It can memory map the file and search the entire file all at once <em>as if</em> it
were a single contiguous region of bytes in memory. The operating system
does the work behind the scenes to make a file look like a contiguous
region of memory. This particular approach is <em>really</em> convenient when
comparing it to the alternative described next.</li>
<li>… or it can allocate an intermediate buffer, read a fixed size block of
bytes from the file into it, search the buffer and then repeat the process.
This particular approach is absolutely ghoulish to implement, because you
need to account for the fact that a buffer may end in the middle of the
line. You also need to account for the fact that a single line may exceed
the size of your buffer. Finally, if you’re going to support showing the
lines around a match (its “context”) as both <code>grep</code> and <code>ripgrep</code> do, then
you need to do additional bookkeeping to make sure any lines from a previous
buffer are printed even if a match occurs at the beginning of the next block
read from the file.</li>
</ol>
<p>Naively, it seems like (1) would be <em>obviously</em> faster. Surely, all of the
bookkeeping and copying in (2) would make it much slower! In fact, this is not
at all true. (1) may not require much bookkeeping from the perspective of the
programmer, but there is a lot of <a href="http://lkml.iu.edu/hypermail/linux/kernel/0004.0/0728.html">bookkeeping going on inside the
Linux kernel to maintain the memory
map</a>. (That link
goes to a mailing list post that is quite old, but it still appears relevant
today.)</p>
<p>When I first started writing <code>ripgrep</code>, I used the memory map approach. It took
me a long time to be convinced enough to start down the second path with an
intermediate buffer (because neither a CPU profile nor the output of <code>strace</code>
ever showed any convincing evidence that memory maps were to blame), but as
soon as I had a prototype of (2) working, it was clear that it was much faster
than the memory map approach.</p>
<p>With all that said, memory maps aren’t all bad. They just happen to be bad for
the particular use case of “rapidly open, scan and close memory maps for
thousands of small files.” For a different use case, like, say, “open this
large file and search it once,” memory maps turn out to be a boon. We’ll see
that in action in our single-file benchmarks later.</p>
<p>The key datapoint that supports this conclusion is the comparison between
<code>rg (ignore)</code> and <code>rg (ignore) (mmap)</code>. In particular, this controls for
everything <em>except</em> for the search strategy and fairly conclusively points
right at memory maps as the problem.</p>
<p>With all that said, the performance of memory maps is very dependent on your
environment, and the absolute difference between <code>rg (ignore)</code> and <code>ag (ignore) (mmap)</code> can be misleading. In particular, since these benchmarks were run on an
EC2 <code>c3.2xlarge</code>, we were probably inside a virtual machine, which could
feasibly impact memory map performance. To test this, I ran the same benchmark
on my machine under my desk (Intel i7-6900K 3.2 GHz, 16 CPUs, 64 GB memory,
SSD) and got these results:</p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg (ignore)          0.156 +/- 0.006 (lines: 16)
</span></span><span><span>rg (ignore) (mmap)   0.397 +/- 0.013 (lines: 16)
</span></span><span><span>ag (ignore) (mmap)   0.444 +/- 0.016 (lines: 16)
</span></span><span><span>pt (ignore)          0.159 +/- 0.008 (lines: 16)
</span></span><span><span>sift (ignore)        0.344 +/- 0.002 (lines: 16)
</span></span><span><span>git grep (ignore)    0.195 +/- 0.023 (lines: 16)
</span></span><span><span>rg (whitelist)       0.108 +/- 0.005 (lines: 16)*+
</span></span><span><span>ucg (whitelist)      0.165 +/- 0.005 (lines: 16)</span></span></code></pre></div>


<!-- raw HTML omitted -->
<p><code>rg (ignore)</code> still soundly beats <code>ag</code>, and our memory map conclusions above
are still supported by this data, but the difference between <code>rg (ignore)</code> and
<code>ag (ignore) (mmap)</code> has narrowed quite a bit!</p>
<h3 id="linux_literal_casei"><code>linux_literal_casei</code></h3>
<p><strong>Description</strong>: This benchmark is like
<a href="#linux-literal"><code>linux_literal</code></a>,
except it asks the search tool to perform a case insensitive search.</p>
<p><strong>Pattern</strong>: <code>PM_RESUME</code> (with the <code>-i</code> flag set)</p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg (ignore)          0.345 +/- 0.073 (lines: 370)
</span></span><span><span>rg (ignore) (mmap)   1.612 +/- 0.011 (lines: 370)
</span></span><span><span>ag (ignore) (mmap)   1.609 +/- 0.015 (lines: 370)
</span></span><span><span>pt (ignore)          17.204 +/- 0.126 (lines: 370)
</span></span><span><span>sift (ignore)        0.805 +/- 0.005 (lines: 370)
</span></span><span><span>git grep (ignore)    0.343 +/- 0.007 (lines: 370)
</span></span><span><span>rg (whitelist)       0.222 +/- 0.021 (lines: 370)+
</span></span><span><span>ucg (whitelist)      0.217 +/- 0.006 (lines: 370)*</span></span></code></pre></div>


<!-- raw HTML omitted -->
<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
</ul>
<p><strong>Analysis</strong>: The biggest change from the previous benchmark is that <code>pt</code> got
an order of magnitude slower than the next slowest tool.</p>
<p>So why did <code>pt</code> get so slow? In particular, both <code>sift</code> and <code>pt</code> use Go’s
<code>regexp</code> package for searching, so why did one perish while the other only got
slightly slower? It turns out that when <code>pt</code> sees the <code>-i</code> flag indicating case
insensitive search, it will force itself to use Go’s <code>regexp</code> engine with the
<code>i</code> flag set. So for example, given a CLI invocation of <code>pt -i foo</code>, it will
translate that to a Go regexp of <code>(?i)foo</code>, which will handle the case
insensitive search.</p>
<p>On the other hand, <code>sift</code> will notice the <code>-i</code> flag and take a different route.
<code>sift</code> will lowercase both the pattern and every block of bytes it searches.
This filter over all the bytes searched is likely the cause of <code>sift</code>’s
performance drop from the previous
<a href="#linux-literal"><code>linux_literal</code></a>
benchmark. (It’s worth pointing out that this optimization is actually
incorrect, because it only accounts for ASCII case insensitivity, and not full
Unicode case insensitivity, which <code>pt</code> gets by virture of Go’s regexp engine.)</p>
<p>But still, is Go’s regexp engine really that slow? Unfortunately, yes, it is.
While Go’s regexp engine takes worst case linear time on all searches (and is
therefore exponentially faster than even PCRE2 for some set of regexes and
corpora), its actual implementation hasn’t quite matured yet. Indeed, every
<em>fast</em> regex engine based on finite automata that I’m aware of implements
some kind of DFA engine. For example, GNU grep, Google’s RE2 and Rust’s regex
library all do this. Go’s does not (but there is work in progress to make this
happen, so perhaps <code>pt</code> will get faster on this benchmark without having to do
anything at all!).</p>
<p>There is one other thing worth noting here before moving on. Namely, that <code>rg</code>,
<code>ag</code>, <code>git grep</code> and <code>ucg</code> didn’t noticeably change much from the previous
benchmark. Shouldn’t a case insensitive search incur some kind of overhead? The
answer is complicated and actually requires more knowledge of the underlying
regex engines than I have. Thankfully, I can at least answer it for Rust’s
regex engine.</p>
<p>The key insight is that a case insensitive search for <code>PM_RESUME</code> is precisely
the same as a case sensitive search of the alternation of all possible case
agnostic versions of <code>PM_RESUME</code>. So for example, it might start like:
<code>PM_RESUME|pM_RESUME|Pm_RESUME|PM_rESUME|...</code> and so on. Of course, the
full alternation, even for a small literal like this, would be <em>quite</em> large.
The key is that we can extract a small prefix and enumerate all of <em>its</em>
combinations quite easily. In this case, Rust’s regex engine figures out this
alternation (which you can see by passing <code>--debug</code> to <code>rg</code> and examining
<code>stderr</code>):</p>



<div><pre tabindex="0"><code data-lang="text"><span><span>PM_RE
</span></span><span><span>PM_Re
</span></span><span><span>PM_rE
</span></span><span><span>PM_re
</span></span><span><span>Pm_RE
</span></span><span><span>Pm_Re
</span></span><span><span>Pm_rE
</span></span><span><span>Pm_re
</span></span><span><span>pM_RE
</span></span><span><span>pM_Re
</span></span><span><span>pM_rE
</span></span><span><span>pM_re
</span></span><span><span>pm_RE
</span></span><span><span>pm_Re
</span></span><span><span>pm_rE
</span></span><span><span>pm_re</span></span></code></pre></div>


<p>(Rest assured that Unicode support is baked into this process. For example, a
case insensitive search for <code>S</code> would yield the following literals: <code>S</code>, <code>s</code>
and <code>ſ</code>.)</p>
<p>Now that we have this alternation of literals, what do we do with them? The
classical answer is to compile them into a DFA
(perhaps <a href="https://github.com/BurntSushi/aho-corasick">Aho-Corasick</a>),
and use it as a way to quickly skip through the search text. A match of any of
the literals would then cause the regex engine to activate and try to verify
the match. This way, we aren’t actually running the entire search text through
the regex engine, which could be quite a bit slower.</p>
<p>But, Rust’s regex engine doesn’t actually use Aho-Corasick for this. When SIMD
acceleration is enabled (and you can be sure it is for these benchmarks, and
for the binaries I distribute), a special multiple pattern search algorithm
called Teddy is used. The algorithm is unpublished, but was invented by
Geoffrey Langdale as part of <a href="https://github.com/01org/hyperscan">Intel’s Hyperscan regex
library</a>. The algorithm works roughly by
using packed comparisons of 16 bytes at a time to find candidate locations
where a literal might match.
<a href="https://github.com/rust-lang-nursery/regex/blob/master/src/simd_accel/teddy128.rs">I adapted the algorithm from the Hyperscan project to
Rust</a>,
and included an extensive write up in the comments if you’re interested.</p>
<p>While Teddy doesn’t buy us much over other tools in this particular benchmark,
we will see much larger wins in later benchmarks.</p>
<h3 id="linux_word"><code>linux_word</code></h3>
<p><strong>Description</strong>: This benchmarks the <code>PM_RESUME</code> literal again, but adds the
<code>-w</code> flag to each tool. The <code>-w</code> flag has the following behavior: all matches
reported must be considered “words.” That is, a “word” is something that starts
and ends at a word boundary, where a word boundary is defined as a position in
the search text that is adjacent to both a word character and a non-word
character.</p>
<p><strong>Pattern</strong>: <code>PM_RESUME</code> (with the <code>-w</code> flag set)</p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg (ignore)         0.362 +/- 0.080 (lines: 6)
</span></span><span><span>ag (ignore)         1.603 +/- 0.009 (lines: 6)
</span></span><span><span>pt (ignore)         14.417 +/- 0.144 (lines: 6)
</span></span><span><span>sift (ignore)       7.840 +/- 0.123 (lines: 6)
</span></span><span><span>git grep (ignore)   0.341 +/- 0.005 (lines: 6)
</span></span><span><span>rg (whitelist)      0.220 +/- 0.026 (lines: 6)*+
</span></span><span><span>ucg (whitelist)     0.221 +/- 0.007 (lines: 6)</span></span></code></pre></div>


<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
</ul>
<p><strong>Analysis</strong>: Not much has changed between this benchmark and the previous
<a href="#linux-literal"><code>linux_literal</code></a>
or
<a href="#linux-literal-casei"><code>linux_literal_casei</code></a>
benchmarks. The most important thing to note is that most search tools handle
the <code>-w</code> flag just fine without any noticeable drop in performance. There are
two additional things I’d like to note.</p>
<p><code>rg</code> is searching with Unicode aware word boundaries where as the rest of the
tools are using ASCII only word boundaries. (<code>git grep</code> can be made to use
Unicode word boundaries by adjusting your system’s locale settings. In this
benchmark, we force it to use ASCII word boundaries.)</p>
<p><code>sift</code> and <code>pt</code> are the only tools that gets noticeably slower in this
benchmark compared to previous benchmarks. The reason is the same as the reason
why <code>pt</code> got noticeably slower in the
<a href="#linux-literal-casei"><code>linux_literal_casei</code></a>
benchmark: both <code>pt</code> and <code>sift</code> are now also bottlenecked on Go’s regexp
library. <code>pt</code> and <code>sift</code> could do a little better here by staying out of
Go’s regexp library and searching for the <code>PM_RESUME</code> literal, and then only
confirming whether the match corresponds to a word boundary after it found a
hit for <code>PM_RESUME</code>. This still might use Go’s regexp library, but in a much
more limited form.</p>
<h3 id="linux_unicode_word"><code>linux_unicode_word</code></h3>
<p><strong>Description</strong>: This benchmarks a simple query for all prefixed forms of the
“amp-hour” (Ah) unit of measurement. For example, it should show things like
<code>mAh</code> (for milliamp-hour) and <code>µAh</code> (for microamp-hour). It is particularly
interesting because the second form starts with <code>µ</code>, which is part of a Unicode
aware <code>\w</code> character class, but not an ASCII-only <code>\w</code> character class. We
again continue to control for the overhead of respecting <code>.gitignore</code> files.</p>
<p><strong>Pattern</strong>: <code>\wAh</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg (ignore)                 0.355 +/- 0.073 (lines: 186)
</span></span><span><span>rg (ignore) (ASCII)         0.329 +/- 0.060 (lines: 174)
</span></span><span><span>ag (ignore) (ASCII)         1.774 +/- 0.011 (lines: 174)
</span></span><span><span>pt (ignore) (ASCII)         14.180 +/- 0.180 (lines: 174)
</span></span><span><span>sift (ignore) (ASCII)       11.087 +/- 0.108 (lines: 174)
</span></span><span><span>git grep (ignore)           13.045 +/- 0.008 (lines: 186)
</span></span><span><span>git grep (ignore) (ASCII)   2.991 +/- 0.004 (lines: 174)
</span></span><span><span>rg (whitelist)              0.235 +/- 0.031 (lines: 180)
</span></span><span><span>rg (whitelist) (ASCII)      0.225 +/- 0.023 (lines: 168)*+
</span></span><span><span>ucg (ASCII)                 0.229 +/- 0.007 (lines: 168)</span></span></code></pre></div>


<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
</ul>
<!-- raw HTML omitted -->
<p><strong>Analysis</strong>: In this benchmark, we’ve introduced a new variable: whether or
not to enable Unicode support in each tool. Searches that are Unicode aware
report slightly more matches that are missed by the other ASCII only searches.</p>
<p>Of all the tools here, the only ones that support Unicode toggling are <code>rg</code>
and <code>git grep</code>. <code>rg</code>’s Unicode support can be toggled by setting a flag in
the pattern itself (e.g., <code>\w</code> is Unicode aware while <code>(?-u)\w</code> is not), and
<code>git grep</code>’s Unicode suport can be toggled by setting the <code>LC_ALL</code> environment
variable (where <code>en_US.UTF-8</code> is one way to enable Unicode support and <code>C</code>
forces it to be ASCII). More generally, <code>git grep</code>’s Unicode support is
supposed to line up with your system’s locale settings—setting <code>LC_ALL</code> is a
bit of a hack.</p>
<p>It gets a little worse than that actually. Not only are <code>rg</code> and <code>git grep</code> the
only ones to support toggling Unicode, but they are the only ones to support
Unicode <em>at all</em>. <code>ag</code>, <code>pt</code>, <code>sift</code> and <code>ucg</code> will all force you to use the
ASCII only <code>\w</code> character class. (For <code>pt</code> and <code>sift</code> in particular, Go’s
<code>regexp</code> library doesn’t have the ability to treat <code>\w</code> as Unicode aware. For
<code>ag</code> and <code>ucg</code>, which use PCRE, <code>\w</code> could be made Unicode aware with a flag
sent to PCRE. Neither tool exposes that functionality though.)</p>
<p>The key result to note here is that while <code>git grep</code> suffers a major
performance hit for enabling Unicode support, <code>ripgrep</code> hums along just fine
with no noticeable loss in performance, even though both <code>rg (ignore)</code> and <code>git grep (ignore)</code> report the same set of results.</p>
<p>As in the previous benchmark, both <code>pt</code> and <code>sift</code> could do better here by
searching for the <code>Ah</code> literal, and only using Go’s regexp library to verify a
match.)</p>
<p>Looking at the benchmark results, I can think of two important questions to
ask:</p>
<ol>
<li>Why is <code>git grep (ignore) (ASCII)</code> so much slower than
<code>rg (ignore) (ASCII)</code>? And while the two aren’t directly comparable,
it’s also a lot slower than <code>ucg (ASCII)</code>.</li>
<li>How is <code>rg (ignore)</code> (which is Unicode aware) just as fast as
<code>rg (ignore) (ASCII)</code>?</li>
</ol>
<p>I actually don’t have a great answer for (1). In the case of <code>rg</code> at least,
it will extract the <code>Ah</code> literal suffix from the regex and use that to find
candidate matches before running the <code>\w</code> prefix. While GNU grep has
sophisticated literal extraction as well, it looks like <code>git grep</code> doesn’t go
to similar lengths to extract literals. (I’m arriving at this conclusion after
skimming the source of <code>git grep</code>, so I could be wrong.)</p>
<p>In the case of <code>ucg</code>, it’s likely that PCRE2 is doing a similar literal
optimization that <code>rg</code> is doing.</p>
<p>(2) is fortunately much easier to answer. The trick is not inside of <code>rg</code>, but
inside its regex library. Namely, the regex engine <em>builds UTF-8 decoding into
its finite state machine</em>. (This is a trick that is originally attributed to
Ken Thompson, but was more carefully
<a href="https://swtch.com/~rsc/regexp/regexp3.html">described by Russ Cox</a>.
To read more about how this is achieved in Rust’s regex engine, please see the
<a href="https://docs.rs/utf8-ranges"><code>utf8-ranges</code></a>
library.) The reason why this is fast is because there is no extra decoding
step required. The regex can be matched directly against UTF-8 encoded byte
strings one byte at a time. Invalid UTF-8 doesn’t pose any problems: the finite
automaton simply won’t match it because it doesn’t recognize it.</p>
<p>In contrast, <code>git grep</code> (and GNU grep) have a completely separate path in their
core matching code for handling Unicode aware features like this. To be fair,
<code>git grep</code> can handle text encodings other than UTF-8, where as <code>rg</code> is limited
to UTF-8 (or otherwise “ASCII compatible” text encodings) at the moment.</p>
<h3 id="linux_re_literal_suffix"><code>linux_re_literal_suffix</code></h3>
<p><strong>Description</strong>: This benchmarks a simple regex pattern that ends with a
literal. We continue to control for the overhead of respecting <code>.gitignore</code>
files.</p>
<p><strong>Pattern</strong>: <code>[A-Z]+_RESUME</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg (ignore)         0.318 +/- 0.034 (lines: 1652)
</span></span><span><span>ag (ignore)         1.899 +/- 0.008 (lines: 1652)
</span></span><span><span>pt (ignore)         13.713 +/- 0.241 (lines: 1652)
</span></span><span><span>sift (ignore)       10.172 +/- 0.186 (lines: 1652)
</span></span><span><span>git grep (ignore)   1.108 +/- 0.004 (lines: 1652)
</span></span><span><span>rg (whitelist)      0.221 +/- 0.022 (lines: 1630)*+
</span></span><span><span>ucg (whitelist)     0.301 +/- 0.001 (lines: 1630)</span></span></code></pre></div>


<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
</ul>
<p><strong>Analysis</strong>: This benchmark doesn’t reveal anything particularly new that we
haven’t already learned from previous benchmarks. In particular, both <code>rg</code> and
<code>ucg</code> continue to be competitive, <code>pt</code> and <code>sift</code> are getting bottlenecked by
Go’s regexp library and <code>git grep</code> has a slow down similar to the one observed
in
<a href="#linux-unicode-word"><code>linux_unicode_word</code></a>.
(My hypothesis for that slow down continues to be that <code>git grep</code> is missing
the literal optimization.) Finally, <code>ag</code> continues to be held back by its use
of memory maps.</p>
<p><code>rg</code>, and almost assuredly <code>ucg</code> (by virtue of PCRE2), are picking on the
<code>_RESUME</code> literal suffix and searching for that instead of running the regex
over the entire search text. This explains why both tools are able to maintain
their speed even as the pattern gets slightly more complex. <code>rg</code> does seem to
slightly edge out <code>ucg</code> here, which might be attributable to differences in how
each underlying regex library does literal search.</p>
<h3 id="linux_alternates"><code>linux_alternates</code></h3>
<p><strong>Description</strong>: This benchmarks an alternation of four literals. The literals
were specifically chosen to start with four distinct bytes to make it harder to
optimize.</p>
<p><strong>Pattern</strong>: <code>ERR_SYS|PME_TURN_OFF|LINK_REQ_RST|CFG_BME_EVT</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg (ignore)         0.351 +/- 0.074 (lines: 68)
</span></span><span><span>ag (ignore)         1.747 +/- 0.005 (lines: 68)
</span></span><span><span>git grep (ignore)   0.501 +/- 0.003 (lines: 68)
</span></span><span><span>rg (whitelist)      0.216 +/- 0.031 (lines: 68)+
</span></span><span><span>ucg (whitelist)     0.214 +/- 0.008 (lines: 68)*</span></span></code></pre></div>


<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
<li>We drop <code>pt</code> and <code>sift</code> from this benchmark and the next one for expediency.
In this benchmark and in a few previous benchmarks, they have been hovering
around an order of magnitude slower than the next slowest tool. Neither get
any better as the complexity of our patterns increase.</li>
</ul>
<!-- raw HTML omitted -->
<p><strong>Analysis</strong>: Yet again, both <code>rg</code> and <code>ucg</code> maintain high speed even as the
pattern grows beyond a simple literal. In this case, there isn’t any <em>one</em>
particular literal that we can search to find match candidates quickly, but a
good regular expression engine can still find ways to speed this up.</p>
<p>For <code>rg</code> in particular, it sees the four literals and diverts to the Teddy
multiple pattern SIMD algorithm (as described in the
<a href="#linux-literal-casei"><code>linux_literal_casei</code></a>
benchmark). In fact, for this particular pattern, Rust’s core regex engine
is never used at all. Namely, it notices that a literal match of any of the
alternates corresponds to an overall match of the pattern, so it can completely
skip the verification step. This makes searching alternates of literals <em>very</em>
fast.</p>
<h3 id="linux_alternates_casei"><code>linux_alternates_casei</code></h3>
<p><strong>Description</strong>: This benchmark is precisely the same as the
<a href="#linux-alternates"><code>linux_alternates</code></a>
benchmark, except we make the search case insensitive by adding the <code>-i</code> flag.
Note that <code>git grep</code> is run under ASCII mode, in order to give it every chance
to be fast.</p>
<p><strong>Pattern</strong>: <code>ERR_SYS|PME_TURN_OFF|LINK_REQ_RST|CFG_BME_EVT</code> (with the <code>-i</code>
flag set)</p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg (ignore)         0.391 +/- 0.078 (lines: 160)
</span></span><span><span>ag (ignore)         1.968 +/- 0.009 (lines: 160)
</span></span><span><span>git grep (ignore)   2.018 +/- 0.006 (lines: 160)
</span></span><span><span>rg (whitelist)      0.222 +/- 0.001 (lines: 160)*+
</span></span><span><span>ucg (whitelist)     0.522 +/- 0.002 (lines: 160)</span></span></code></pre></div>


<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
</ul>
<!-- raw HTML omitted -->
<p><strong>Analysis</strong>: The case insensitive flag causes quite a bit of separation,
relative to the previous
<a href="#linux-alternates"><code>linux_alterates</code></a>
benchmark. For one, <code>git grep</code> gets over 4 times slower. Even <code>ucg</code> gets twice
as slow. Yet, <code>rg</code> continues to maintain its speed!</p>
<p>The secret continues to be the Teddy algorithm, just as in the
<a href="#linux-alternates"><code>linux_alternates</code></a>
benchmark. The trick lies in how we transform an alternation of <em>case
insensitive</em> literals into a larger alternation that the Teddy algorithm
can actually use. In fact, it works exactly how it was described in the
<a href="#linux-literal-casei"><code>linux_literal_casei</code></a>
benchmark: we enumerate all possible alternations of each literal that are
required for case insensitive match. Since that can be quite a large number,
we limit ourselves to a small number of prefixes from that set that we can
enumerate. In this case, we use the following prefixes (which can be seen by
running <code>rg</code> with the <code>--debug</code> flag):</p>



<div><pre tabindex="0"><code data-lang="text"><span><span>CFG_
</span></span><span><span>CFg_
</span></span><span><span>CfG_
</span></span><span><span>Cfg_
</span></span><span><span>ERR_
</span></span><span><span>ERr_
</span></span><span><span>ErR_
</span></span><span><span>Err_
</span></span><span><span>LIN
</span></span><span><span>LIn
</span></span><span><span>LiN
</span></span><span><span>Lin
</span></span><span><span>PME_
</span></span><span><span>PMe_
</span></span><span><span>PmE_
</span></span><span><span>Pme_
</span></span><span><span>cFG_
</span></span><span><span>cFg_
</span></span><span><span>cfG_
</span></span><span><span>cfg_
</span></span><span><span>eRR_
</span></span><span><span>eRr_
</span></span><span><span>erR_
</span></span><span><span>err_
</span></span><span><span>lIN
</span></span><span><span>lIn
</span></span><span><span>liN
</span></span><span><span>lin
</span></span><span><span>pME_
</span></span><span><span>pMe_
</span></span><span><span>pmE_
</span></span><span><span>pme_</span></span></code></pre></div>


<p>We feed these literals to the Teddy algorithm, which will quickly identify
<em>candidate</em> matches in the search text. When a candidate match is found, we
need to verify it since a match of a prefix doesn’t necessarily mean the entire
pattern matches. It is only at that point that we actually invoke the full
regex engine.</p>
<h3 id="linux_unicode_greek"><code>linux_unicode_greek</code></h3>
<p><strong>Description</strong>: This benchmarks usage of a particular Unicode feature that
permits one to match a certain class of codepoints defined in Unicode. Both
Rust’s regex engine and Go’s regex engine support this natively, but none of
the other tools do.</p>
<p><strong>Pattern</strong>: <code>\p{Greek}</code> (matches any Greek symbol)</p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg     0.414 +/- 0.021 (lines: 23)*+
</span></span><span><span>pt     12.745 +/- 0.166 (lines: 23)
</span></span><span><span>sift   7.767 +/- 0.264 (lines: 23)</span></span></code></pre></div>


<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
</ul>
<p><strong>Analysis</strong>: This one is pretty simple. <code>rg</code> compiles <code>\p{Greek}</code> into a
deterministic finite state machine while Go (used in <code>pt</code> and <code>sift</code>) will also
use a finite state machine, but it is a <em>nondeterministic</em> simulation. The core
difference between the two approaches is that the former is only ever in one
state at any point in time, while the latter must constantly keep track of all
the different states it is in.</p>
<h3 id="linux_unicode_greek_casei"><code>linux_unicode_greek_casei</code></h3>
<p><strong>Description</strong>: This benchmark is just like the
<a href="#linux-unicode-greek"><code>linux_unicode_greek</code></a>
benchmark, except it makes the search case insensitive. This particular query
is a bit idiosyncratic, but it does demonstrate just how well supported Unicode
is in <code>rg</code>.</p>
<p><strong>Pattern</strong>: <code>\p{Greek}</code> (with the <code>-i</code> flag set, matches any Greek symbol)</p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg     0.425 +/- 0.027 (lines: 103)
</span></span><span><span>pt     12.612 +/- 0.217 (lines: 23)
</span></span><span><span>sift   0.002 +/- 0.000 (lines: 0)*+</span></span></code></pre></div>


<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
</ul>
<!-- raw HTML omitted -->
<p><strong>Analysis</strong>: <code>sift</code> doesn’t actually beat <code>rg</code> here: it just gets so confused
by the search request that it gives up and reports no matches. <code>pt</code> seems to
execute the search, but doesn’t handle Unicode case insensitivity correctly.
Meanwhile, <code>rg</code> handles the request just fine, <em>and it’s still fast</em>.</p>
<p>In this particular case, the entire <code>Greek</code> category, along with all of its
case-insensitive variants, are compiled into a single fast deterministic finite
state machine.</p>
<p>One interesting thing to note about this search is that if you run it, you’ll
see a lot more results containing the character <code>µ</code>, which looks essentially
identical to the character <code>μ</code> that also shows up in a case sensitive search.
As you might have guessed, even though these two characters look the same, they
are in fact distinct Unicode codepoints:</p>
<ul>
<li><code>µ</code> is <code>MICRO SIGN</code> with codepoint <code>U+000000B5</code>.</li>
<li><code>μ</code> is <code>GREEK SMALL LETTER MU</code> with codepoint <code>U+000003BC</code>.</li>
</ul>
<p>The latter codepoint is considered part of the <code>\p{Greek}</code> group while the
former codepoint is not (the former codepoint appears to be the correct sigil
to use in the case of the Linux kernel). However, the
<a href="http://www.unicode.org/Public/UNIDATA/CaseFolding.txt">Unicode simple case folding
tables</a>
map <code>MICRO SIGN</code> to <code>GREEK SMALL LETTER MU</code>, which causes <code>rg</code> to pick up on
lines containing <code>MICRO SIGN</code> even though it strictly isn’t part of the <code>Greek</code>
group.</p>
<h3 id="linux_no_literal"><code>linux_no_literal</code></h3>
<p><strong>Description</strong>: This is the last benchmark on the Linux kernel source code and
is a bit idiosyncratic like
<a href="#linux-unicode-greek-casei"><code>linux_unicode_greek_casei</code></a>.
In particular, it looks for lines containing 5 consecutive repetitions of
5 word characters, each separated by one or more space characters. The key
distinction of this pattern from every other pattern in this benchmark is that
it does not contain any literals. Given the presence of <code>\w</code> and <code>\s</code>, which
have valid Unicode and ASCII interpretations, we attempt to control for the
presence of Unicode support.</p>
<p><strong>Pattern</strong>: <code>\w{5}\s+\w{5}\s+\w{5}\s+\w{5}\s+\w{5}</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg (ignore)                 0.577 +/- 0.003 (lines: 490)
</span></span><span><span>rg (ignore) (ASCII)         0.416 +/- 0.025 (lines: 490)
</span></span><span><span>ag (ignore) (ASCII)         2.339 +/- 0.010 (lines: 766)
</span></span><span><span>pt (ignore) (ASCII)         22.066 +/- 0.057 (lines: 490)
</span></span><span><span>sift (ignore) (ASCII)       25.563 +/- 0.108 (lines: 490)
</span></span><span><span>git grep (ignore)           26.382 +/- 0.044 (lines: 490)
</span></span><span><span>git grep (ignore) (ASCII)   4.153 +/- 0.010 (lines: 490)
</span></span><span><span>rg (whitelist)              0.503 +/- 0.011 (lines: 419)
</span></span><span><span>rg (whitelist) (ASCII)      0.343 +/- 0.038 (lines: 419)*+
</span></span><span><span>ucg (whitelist) (ASCII)     1.130 +/- 0.003 (lines: 416)</span></span></code></pre></div>


<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
<li><code>ag</code> reports many more matches than other tools because it does multiline
search where the <code>\s</code> can match a <code>\n</code>.</li>
</ul>
<!-- raw HTML omitted -->
<p><strong>Analysis</strong>: Since this particular pattern doesn’t have any literals in it,
it’s entirely up to the underlying regex engine to answer this query. It can’t
be smart and skip through the input—it has to pass it completely through the
regex engine. Since non-literal patterns are pretty rare in my experience, this
benchmark exists primarily as an engineered way to test how well the underlying
regex engines perform.</p>
<p><code>rg</code>, regardless of whether it respects <code>.gitignore</code> files or whether it
handles Unicode correctly, does quite well here compared to other tools. <code>git grep</code> in particular pays a 5x penalty for Unicode support. <code>rg</code> on the other
hand pays about a 0.3x penalty for Unicode support. Interestingly, even though
<code>ucg</code> doesn’t enable Unicode support, not even PCRE2’s JIT can compete with
<code>rg</code>!</p>
<p>What makes <code>rg</code> so fast here? And what actually causes the 0.3x penalty?</p>
<p><code>rg</code> continues to be fast on this benchmark primarily for the same reason why
it’s fast with other Unicode-centric benchmarks: it compiles the UTF-8 decoding
right into its deterministic finite state machine. This means there is no extra
step to decode the search text into Unicode codepoints first. We can match
directly on the raw bytes.</p>
<p>To a first approximation, the performance penalty comes from compiling the DFA
to match the pattern. In particular, the DFA to match the Unicode variant is
much much larger than the DFA to match the ASCII variant. To give you a rough
idea of the size difference:</p>
<ul>
<li>The ASCII DFA has about <strong>250</strong> distinct NFA states.</li>
<li>The Unicode DFA has about <strong>77,000</strong> distinct NFA states.</li>
</ul>
<p>(These numbers are produced directly from the compiler in Rust’s regex library,
and don’t necessarily reflect a minimal automaton.)</p>
<p>A DFA produced from these patterns doesn’t necessarily have the same number of
states, since each DFA state typically corresponds to multiple NFA states.
(Check out the
<a href="https://en.wikipedia.org/wiki/Powerset_construction">Powerset construction</a>
Wikipedia article. Although it doesn’t correspond to the same implementation
strategy used in Rust’s regex engine, it should give good intuition.)</p>
<p>However, the first approximation is a bit misleading. While Rust’s regex engine
does have a preprocessing compilation phase, it does not actually include
converting an NFA into a DFA. Indeed, that would be far too slow and could
take exponential time! Instead, Rust’s regex engine builds the DFA <em>on the fly</em>
or “lazily,” as it searches the text. In the case of the ASCII pattern, this
search barely spends any time constructing the DFA states since there are so
few of them. However, in the Unicode case, since there are so many NFA states,
it winds up spending a lot of time compiling new DFA states. (I’ve confirmed
this by inspecting a profile generated by
<a href="https://perf.wiki.kernel.org/index.php/Main_Page"><code>perf</code></a>.)
Digging a bit deeper, the actual story here might be subtler. For example, the
Unicode pattern might wind up with the same number of DFA states as the ASCII
pattern, primarily because the input its searching is the same and is primarily
ASCII. The slow down then must come from the fact that each individual DFA
state takes longer to build. This is likely correct since a single Unicode <code>\w</code>
is over two orders of magnitude larger than a single ASCII <code>\w</code>. Therefore,
each DFA state probably has a lot more NFA states in it for the Unicode pattern
as opposed to the ASCII pattern. It’s not clear whether we can do any better
here (other than trying to minimize the Unicode <code>\w</code>, which would be totally
feasible), since we don’t actually know the composition of the search text
ahead of time.</p>
<p>One idea for improvement is to have multiple types of DFAs. For example,
you might imagine trying to match with an ASCII only DFA. If the DFA sees a
non-ASCII byte, then it could cause a transition into a Unicode-aware DFA.
However, the penalty here is so small that it’s hard to justify this kind of
implementation complexity!</p>
<h2 id="single-file-benchmarks">Single file benchmarks</h2>
<p>In the second half of our benchmarks, we will shift gears and look more closely
at the performance of search tools on a single large file. Each benchmark will
be run on two samples of the
<a href="http://opus.lingfil.uu.se/OpenSubtitles2016.php">OpenSubtitles2016</a> dataset.
One sample will be English and therefore predominantly ASCII, and another
sample will be in Russian and therefore predominantly Cyrillic. The patterns
for the Russian sample were translated from English using Google Translate.
(Sadly, I can’t read Russian, but I have tried each search by hand and
confirmed that a sample of the results I was looking at were relevant by piping
them back through Google Translate.) The English sample is around 1GB and
the Russian sample is around 1.6GB, so the benchmark timings aren’t directly
comparable.</p>
<p>In this benchmark, the performance of the underlying regex engine and various
literal optimizations matter a lot more. The two key variables we’ll need to
control for are line counting and Unicode support. Normally, we’d just not
request line counting from any of the tools, but neither of The Silver Searcher
or Universal Code Grep support disabling line numbers. Additionally, Unicode
support is tricky to control for in some examples because <code>ripgrep</code> does not
support ASCII only case insensitive semantics when searching with a non-ASCII
string. It’s Unicode all the way and there’s no way to turn it off. As we’ll
see, at least for <code>ripgrep</code>, it’s still faster than its ASCII alternatives even
when providing case insensitive Unicode support.</p>
<p>As with the Linux benchmark, you can see precisely which command was run and
its recorded time in
<a href="https://github.com/BurntSushi/ripgrep/blob/master/benchsuite/runs/2016-09-20-ubuntu1604-ec2/raw.csv">the raw data</a>.</p>
<p><code>ripgrep</code> utterly dominates this round, both in performance <em>and</em> correctness.</p>
<h3 id="subtitles_literal"><code>subtitles_literal</code></h3>
<p><strong>Description</strong>: This benchmarks the simplest case for any search tool: find
all occurrences of a literal string. Tools annotated with <code>(lines)</code> were passed
the <code>-n</code> flag (or equivalent) so that the output reports line numbers.</p>
<p><strong>English pattern</strong>: <code>Sherlock Holmes</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg             0.268 +/- 0.000 (lines: 629)*+
</span></span><span><span>rg (no mmap)   0.336 +/- 0.001 (lines: 629)
</span></span><span><span>pt             3.433 +/- 0.002 (lines: 629)
</span></span><span><span>sift           0.326 +/- 0.002 (lines: 629)
</span></span><span><span>grep           0.516 +/- 0.001 (lines: 629)
</span></span><span><span>rg (lines)     0.595 +/- 0.001 (lines: 629)
</span></span><span><span>ag (lines)     2.730 +/- 0.003 (lines: 629)
</span></span><span><span>ucg (lines)    0.745 +/- 0.001 (lines: 629)
</span></span><span><span>pt (lines)     3.434 +/- 0.005 (lines: 629)
</span></span><span><span>sift (lines)   0.756 +/- 0.002 (lines: 629)
</span></span><span><span>grep (lines)   0.969 +/- 0.001 (lines: 629)</span></span></code></pre></div>


<!-- raw HTML omitted -->
<p><strong>Russian pattern</strong>: <code>Шерлок Холмс</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg             0.325 +/- 0.001 (lines: 583)*+
</span></span><span><span>rg (no mmap)   0.452 +/- 0.002 (lines: 583)
</span></span><span><span>pt             12.917 +/- 0.009 (lines: 583)
</span></span><span><span>sift           16.418 +/- 0.008 (lines: 583)
</span></span><span><span>grep           0.780 +/- 0.001 (lines: 583)
</span></span><span><span>rg (lines)     0.926 +/- 0.001 (lines: 583)
</span></span><span><span>ag (lines)     4.481 +/- 0.003 (lines: 583)
</span></span><span><span>ucg (lines)    1.889 +/- 0.004 (lines: 583)
</span></span><span><span>pt (lines)     12.935 +/- 0.011 (lines: 583)
</span></span><span><span>sift (lines)   17.177 +/- 0.010 (lines: 583)
</span></span><span><span>grep (lines)   1.300 +/- 0.003 (lines: 583)</span></span></code></pre></div>


<!-- raw HTML omitted -->
<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
<li>This is the only benchmark that contains <code>pt</code> and <code>sift</code>, since they become
too slow in all future benchmarks.</li>
</ul>
<p><strong>Analysis</strong>: Whether it’s part of the underlying regex engine or part of the
search tool itself, every search tool in this benchmark does some kind of
literal optimization. <code>ag</code> will inspect the pattern, and if it doesn’t
contain any special regex characters, then it will use a Boyer-Moore variant
to perform the search instead of PCRE. GNU grep does something similar,
although it has clearly been the
<a href="http://ridiculousfish.com/blog/posts/old-age-and-treachery.html">subject of much
optimization</a>.</p>
<p>If that’s true, how does <code>rg</code> beat GNU grep by almost a factor of 2? Well,
first and foremost, we note that both <code>sift</code> and <code>ucg</code> beat GNU grep as well.
I won’t be able to go into detail on <code>ucg</code>’s speed since PCRE2’s JIT isn’t
something I understand very well, but I can at least tell you that the reasons
why <code>rg</code> and <code>sift</code> are faster than GNU grep are actually distinct:</p>
<ul>
<li><code>sift</code> uses Go’s regexp library, which will do at least one small literal
optimization: if every match of a regex starts with the same byte, the regex
engine will scan for that byte before starting a match. If you follow the
code that does the scan for the byte all the way back to its source for
<code>x86_64</code> systems, then you’ll find that it is using
<a href="https://github.com/golang/go/blob/b851ded09a300033849b60ab47a468087ce557a1/src/runtime/asm_amd64.s#L1394-L1413">AVX2 instructions and <code>ymm</code>
registers</a>,
which permit scanning 32 bytes in each iteration. In contrast, GNU grep uses
<code>libc</code>’s <code>memchr</code>, which
<a href="https://sourceware.org/git/?p=glibc.git;a=blob;f=sysdeps/x86_64/memrchr.S;h=840de30cd71ba96b3ae43540e6ac255c28906cc5;hb=HEAD">doesn’t use AVX2</a>.
However, that C code will be autovectorized to use <code>xmm</code> registers and SIMD
instructions, which are half the size of <code>ymm</code> registers. In other words, by
virture of being written in Go, <code>sift</code> is making more efficient use of the
CPU.</li>
<li><code>rg</code> also uses <code>memchr</code> from <code>libc</code>. The <code>rg</code> binary that was used in this
benchmark was statically linked with
<a href="http://www.musl-libc.org/"><code>musl</code></a>,
which provides its own
<a href="https://github.com/ifduyue/musl/blob/master/src/string/memchr.c">implementation of
<code>memchr</code></a>.
Despite it being quite a bit terser than GNU’s libc implementation used in
GNU grep, it appears to be doing roughly the same work. If that’s the case,
how is <code>rg</code> faster? The answer lies not in <code>memchr</code> nor in the variant of
Boyer-Moore nor in the number characters Boyer-Moore can skip. The answer
instead lies in <em>which byte is given to <code>memchr</code>.</em> <code>rg</code> will actually try to
guess the “rarest” byte in a literal, and use <code>memchr</code> on that. (A standard
Boyer-Moore implementation will use <code>memchr</code> always on the last byte.) In
this particular case, running <code>memchr</code> on either <code>S</code> or <code>H</code> is probably
quite a bit better than running it on <code>s</code> because <code>S</code> and <code>H</code> are far less
common than <code>s</code>. That is, <code>rg</code> tries harder than GNU grep to spend more time
skipping bytes in a fast SIMD optimized loop. <code>rg</code> can get this wrong, but
it seems strictly better to at least guess and probably get it right in the
common case than to submit to an artifact of common Boyer-Moore
implementations.</li>
</ul>
<p>Now that the secrets of literal search have been revealed, we can better
analyze the Russian benchmark. The answer once again lies in <em>which byte is
used</em> for quick scanning. Both <code>sift</code> and <code>pt</code> use the same AVX2 routine in
Go’s runtime, so why did they get so much slower than every other tool in the
Russian benchmark? The answer becomes more clear when we look at the actual
UTF-8 bytes of the pattern <code>Шерлок Холмс</code>:</p>



<div><pre tabindex="0"><code data-lang="text"><span><span>\xd0\xa8\xd0\xb5\xd1\x80\xd0\xbb\xd0\xbe\xd0\xba \xd0\xa5\xd0\xbe\xd0\xbb\xd0\xbc\xd1\x81</span></span></code></pre></div>


<p>There are two key observations to take away from this:</p>
<ol>
<li>Every character in the pattern <code>Шерлок Холмс</code> is encoded with two UTF-8 code
units, which corresponds to two bytes.</li>
<li>Every character starts with either the byte <code>\xD0</code> or <code>\xD1</code>.</li>
</ol>
<p>If we looked at the UTF-8 bytes of the Russian subtitles we’re searching, we’d
end up seeing exactly the same pattern. This pattern occurs because the
contents of the file are mostly Cyrllic, which are all mostly part of a couple
small ranges in Unicode. This means that the <code>\xD0</code> and <code>\xD1</code> bytes occur <em>a
lot</em>.</p>
<p>If you recall from above, Go’s regex engine will scan for occurrences of the
first byte. But if that first byte happens as frequently as it does here, the
overall search will wind up going slower because there is overhead
associated with doing that scan. This is <em>precisely</em> the trade off one is
exposed to whenever <code>memchr</code> is used.</p>
<p>As you might have guessed, <code>rg</code> works around this issue by trying to guess the
rarest byte. <code>rg</code> specifically draws from a pre-computed frequency table of all
256 bytes. Bytes like <code>\xD0</code> and <code>\xD1</code> are considered to be among the most
frequent while bytes like <code>\xA8</code> and <code>\x81</code> are considered more rare.
Therefore, <code>rg</code> will prefer bytes other than <code>\xD0</code> and <code>\xD1</code> for use with
<code>memchr</code>.</p>
<p>GNU grep continues to do well on this benchmark mostly because of blind luck:
Boyer-Moore uses the last byte, which will correspond to <code>\x81</code>, which is much
rarer than <code>\xD0</code> or <code>\xD1</code>.</p>
<p>Switching gears, we should briefly discuss memory maps. In this benchmark, <code>rg</code>
beats out <code>rg (no mmap)</code> by about 25%. The only difference between the two is
that the former memory maps the file into memory while the latter incrementally
reads bytes from the file into an intermediate buffer, and searches it. In this
case, the overhead of the memory map is very small because we only need to
create one of them. This is the <em>opposite</em> result from our Linux benchmark
above, where memory maps proved to be worse than searching with an intermediate
buffer since we needed to create a new memory map for every file we searched,
which ends up incurring quite a bit of overhead. <code>rg</code> takes an empirical
approach here and enables memory map searching when it knows it only needs to
search a few files, and otherwise searches using an intermediate buffer.</p>
<p>One last note: I’ve neglected to talk about <code>(lines)</code> because there’s really
not much to say here: counting lines takes work, and if you don’t need to
report line numbers, you can avoid doing that work. <code>ucg</code> has a rather cool
SIMD algorithm to count lines and <code>rg</code> also has a packed counting algorithm
that works similarly to the <code>memchr</code> implementations we talked about.</p>
<p>If it were up to me, I’d probably remove benchmarks with line numbers
altogether, since most tools tend to reliably pay just a little bit extra for
them. However, neither <code>ag</code> nor <code>ucg</code> allow turning them off, so we need to
turn them on in other tools in order to make a fair comparison.</p>
<h3 id="subtitles_literal_casei"><code>subtitles_literal_casei</code></h3>
<p><strong>Description</strong>: This benchmark is just like
<a href="#subtitles-literal"><code>subtitles_literal</code></a>, except it does case insensitive
search. Tools annotated with <code>(lines)</code> show line numbers in their output, and
tools annotated with <code>(ASCII)</code> are doing an ASCII-only search. Correspondingly,
tools <em>not</em> labeled with <code>(ASCII)</code> are doing a proper Unicode search.</p>
<p><strong>English pattern</strong>: <code>Sherlock Holmes</code> (with the <code>-i</code> flag set)</p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg                    0.366 +/- 0.001 (lines: 642)*+
</span></span><span><span>grep                  4.084 +/- 0.005 (lines: 642)
</span></span><span><span>grep (ASCII)          0.614 +/- 0.001 (lines: 642)
</span></span><span><span>rg (lines)            0.696 +/- 0.002 (lines: 642)
</span></span><span><span>ag (lines) (ASCII)    2.775 +/- 0.004 (lines: 642)
</span></span><span><span>ucg (lines) (ASCII)   0.841 +/- 0.002 (lines: 642)</span></span></code></pre></div>


<!-- raw HTML omitted -->
<p><strong>Russian pattern</strong>: <code>Шерлок Холмс</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg                    1.131 +/- 0.001 (lines: 604)
</span></span><span><span>grep                  8.187 +/- 0.006 (lines: 604)
</span></span><span><span>grep (ASCII)          0.785 +/- 0.001 (lines: 583)
</span></span><span><span>rg (lines)            1.733 +/- 0.002 (lines: 604)
</span></span><span><span>ag (lines) (ASCII)    0.729 +/- 0.001 (lines: 0)*+
</span></span><span><span>ucg (lines) (ASCII)   1.896 +/- 0.005 (lines: 583)</span></span></code></pre></div>


<!-- raw HTML omitted -->
<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
<li>There is no <code>rg (ASCII)</code> because <code>rg</code> can’t do ASCII-only case insensitive
search.</li>
</ul>
<p><strong>Analysis</strong>: This is a fun benchmark, because we start to see just how awesome
<code>rg</code>’s support for Unicode is. Namely, that it not only gets it correct, but
it’s also <em>fast</em>. It’s fast enough that it beats the competition even when the
competition is using ASCII-only rules.</p>
<p>Right off the bat, GNU grep pays dearly for doing a case insensitive search
with Unicode support. The problem it faces is that it can no longer do a
straight-forward Boyer-Moore search, so it either needs to fall back to some
alternative literal search or its full regex engine. Even though GNU grep is
much faster at ASCII-only case sensitive search than its Unicode aware variant,
<code>rg</code>’s Unicode case insensitive search still handedly beats GNU grep’s
ASCII-only case insensitive search.</p>
<p>The reason why <code>rg</code> is so fast on this benchmark is the same reason why it’s
fast in the
<a href="#linux-literal-casei"><code>linux_literal_casei</code></a>
benchmark: it turns the pattern <code>Sherlock Holmes</code> into an alternation of all
possible literals according to Unicode’s simple case folding rules. It then
takes a small prefix from each alternate so that our set of literals looks like
this:</p>



<div><pre tabindex="0"><code data-lang="text"><span><span>SHER
</span></span><span><span>SHEr
</span></span><span><span>SHeR
</span></span><span><span>SHer
</span></span><span><span>ShER
</span></span><span><span>ShEr
</span></span><span><span>SheR
</span></span><span><span>Sher
</span></span><span><span>sHER
</span></span><span><span>sHEr
</span></span><span><span>sHeR
</span></span><span><span>sHer
</span></span><span><span>shER
</span></span><span><span>shEr
</span></span><span><span>sheR
</span></span><span><span>sher
</span></span><span><span>ſHER
</span></span><span><span>ſHEr
</span></span><span><span>ſHeR
</span></span><span><span>ſHer
</span></span><span><span>ſhER
</span></span><span><span>ſhEr
</span></span><span><span>ſheR
</span></span><span><span>ſher</span></span></code></pre></div>


<p>(Notice that we get Unicode right by including <code>ſ</code> as a case variant of <code>S</code>.)</p>
<p>It then feeds these literals to the Teddy SIMD multiple pattern algorithm. The
algorithm is unpublished, but was invented by Geoffrey Langdale as part of
<a href="https://github.com/01org/hyperscan">Intel’s Hyperscan regex library</a>.
The algorithm works roughly by using packed comparisons of 16 bytes at a time
to find candidate locations where a literal might match.
<a href="https://github.com/rust-lang-nursery/regex/blob/master/src/simd_accel/teddy128.rs">I adapted the algorithm from the Hyperscan project to
Rust</a>,
and included an extensive write up in the comments if you’re interested.</p>
<p>While essentially the same analysis applies to the Russian benchmark, there are
a few interesting things to note. Namely, while the results show <code>grep (ASCII)</code>
as being very fast, it seems clear that it’s completely ignoring the <code>-i</code> flag
in this case since the pattern is not an ASCII string. Notably, its timing is
essentially identical to its timing on the previous
<a href="#subtitles-literal"><code>subtitles_literal</code></a>
benchmark. The other interesting thing to note is that <code>ag</code> reports <code>0</code>
matches. This isn’t entirely unreasonable, if it somehow knows that it can’t
satisfy the request (case insensitive search of a non-ASCII string when Unicode
support isn’t enabled). If I had to guess, I’d say PCRE is returning an error
(possibly from <code>pcre_exec</code>) and it isn’t being forwarded to the end user, but
that’s just a shot in the dark.</p>
<h3 id="subtitles_alternate"><code>subtitles_alternate</code></h3>
<p><strong>Description</strong>: This benchmarks an alternation of literals, where there are
several distinct leading bytes from each literal. We control for line counting.</p>
<p><strong>English pattern</strong>: <code>Sherlock Holmes|John Watson|Irene Adler|Inspector Lestrade|Professor Moriarty</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg             0.294 +/- 0.001 (lines: 848)*+
</span></span><span><span>grep           2.955 +/- 0.003 (lines: 848)
</span></span><span><span>rg (lines)     0.619 +/- 0.001 (lines: 848)
</span></span><span><span>ag (lines)     3.757 +/- 0.001 (lines: 848)
</span></span><span><span>ucg (lines)    1.479 +/- 0.002 (lines: 848)
</span></span><span><span>grep (lines)   3.412 +/- 0.004 (lines: 848)</span></span></code></pre></div>


<!-- raw HTML omitted -->
<p><strong>Russian pattern</strong>: <code>Шерлок Холмс|Джон Уотсон|Ирен Адлер|инспектор Лестрейд|профессор Мориарти</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg             1.300 +/- 0.002 (lines: 691)*+
</span></span><span><span>grep           7.994 +/- 0.017 (lines: 691)
</span></span><span><span>rg (lines)     1.902 +/- 0.002 (lines: 691)
</span></span><span><span>ag (lines)     5.892 +/- 0.003 (lines: 691)
</span></span><span><span>ucg (lines)    2.864 +/- 0.006 (lines: 691)
</span></span><span><span>grep (lines)   8.511 +/- 0.005 (lines: 691)</span></span></code></pre></div>


<!-- raw HTML omitted -->
<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
</ul>
<p><strong>Analysis</strong>: <code>rg</code> does really well here, on both the English and Russian
patterns, primarily thanks to Teddy as described in the analysis for
<a href="#subtitles-literal-casei"><code>subtitles_literal_casei</code></a>. On the English pattern,
<code>rg</code> is around an <em>order of magnitude</em> faster than GNU grep.</p>
<p>The performance cost of counting lines is on full display here. For <code>rg</code> at
least, it makes returning search results take twice as long.</p>
<p>Note that the benchmark description mentions picking literals with distinct
leading bytes. This is to avoid measuring an optimization where the regex
engine detects the leading byte and runs <code>memchr</code> on it. Of course, this
optimization is important (and <code>rg</code> will of course do it), but it’s far more
interesting to benchmark what happens in a slightly trickier case.</p>
<h3 id="subtitles_alternate_casei"><code>subtitles_alternate_casei</code></h3>
<p><strong>Description</strong>: This benchmark is just like
<a href="#subtitles-alternate"><code>subtitles_alternate</code></a>,
except it searches case insensitively. In this benchmark, instead of
controlling for line counting (all commands count lines), we control for
Unicode support.</p>
<p><strong>English pattern</strong>: <code>Sherlock Holmes|John Watson|Irene Adler|Inspector Lestrade|Professor Moriarty</code>
(with the <code>-i</code> flag set)</p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg             2.724 +/- 0.002 (lines: 862)*+
</span></span><span><span>grep           5.125 +/- 0.006 (lines: 862)
</span></span><span><span>ag (ASCII)     5.170 +/- 0.004 (lines: 862)
</span></span><span><span>ucg (ASCII)    3.453 +/- 0.005 (lines: 862)
</span></span><span><span>grep (ASCII)   4.537 +/- 0.025 (lines: 862)</span></span></code></pre></div>


<!-- raw HTML omitted -->
<p><strong>Russian pattern</strong>: <code>Шерлок Холмс|Джон Уотсон|Ирен Адлер|инспектор Лестрейд|профессор Мориарти</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg             4.834 +/- 0.004 (lines: 735)
</span></span><span><span>grep           8.729 +/- 0.004 (lines: 735)
</span></span><span><span>ag (ASCII)     5.891 +/- 0.001 (lines: 691)
</span></span><span><span>ucg (ASCII)    2.868 +/- 0.005 (lines: 691)*+
</span></span><span><span>grep (ASCII)   8.572 +/- 0.009 (lines: 691)</span></span></code></pre></div>


<!-- raw HTML omitted -->
<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
</ul>
<p><strong>Analysis</strong>: While <code>rg</code> gets an <em>order of magnitude</em> slower on this benchmark
compared to
<a href="#subtitles-alternate"><code>subtitles_alternate</code></a>,
it still comfortably beats out the rest of the search tools, even when other
tools don’t support Unicode. A key thing this benchmark demonstrates are the
limits of the Teddy algorithm. In fact, <code>rg</code> opts to not use Teddy in this
benchmark because it predicts it won’t perform well.</p>
<p>Why doesn’t Teddy perform well here? Well, the answer is in how we generate
literals for this pattern. Namely, <code>rg</code> will try to generate all possible
literals that satisfy Unicode simple case folding rules, and then will take a
short prefix of that set to cut the number of literals down to reasonable size.
In this particular case, we wind up with 48 literals:</p>



<div><pre tabindex="0"><code data-lang="text"><span><span>INS
</span></span><span><span>INs
</span></span><span><span>INſ
</span></span><span><span>IRE
</span></span><span><span>IRe
</span></span><span><span>InS
</span></span><span><span>Ins
</span></span><span><span>Inſ
</span></span><span><span>IrE
</span></span><span><span>Ire
</span></span><span><span>JOH
</span></span><span><span>JOh
</span></span><span><span>JoH
</span></span><span><span>Joh
</span></span><span><span>PRO
</span></span><span><span>PRo
</span></span><span><span>PrO
</span></span><span><span>Pro
</span></span><span><span>SHE
</span></span><span><span>SHe
</span></span><span><span>ShE
</span></span><span><span>She
</span></span><span><span>iNS
</span></span><span><span>iNs
</span></span><span><span>iNſ
</span></span><span><span>iRE
</span></span><span><span>iRe
</span></span><span><span>inS
</span></span><span><span>ins
</span></span><span><span>inſ
</span></span><span><span>irE
</span></span><span><span>ire
</span></span><span><span>jOH
</span></span><span><span>jOh
</span></span><span><span>joH
</span></span><span><span>joh
</span></span><span><span>pRO
</span></span><span><span>pRo
</span></span><span><span>prO
</span></span><span><span>pro
</span></span><span><span>sHE
</span></span><span><span>sHe
</span></span><span><span>shE
</span></span><span><span>she
</span></span><span><span>ſHE
</span></span><span><span>ſHe
</span></span><span><span>ſhE
</span></span><span><span>ſhe</span></span></code></pre></div>


<p>If we passed all of those to Teddy, it would become overwhelmed. In particular,
Teddy works by finding candidates for matches very quickly. When there are
roughly the same number of candidates as there are matches, Teddy performs
exceedingly well. But, if we give it more literals, then it’s more likely to
find candidates that don’t match, and will therefore have to spend a lot more
time verifying the match, which can be costly.</p>
<p>(A more subtle aspect of the Teddy implementation is that a larger number
of literals increases the cost of every verification, even if the number of
candidates produced doesn’t increase. As I’ve mentioned before, if you want the
full scoop on Teddy, see its
<a href="https://github.com/rust-lang-nursery/regex/blob/3de8c44f5357d5b582a80b7282480e38e8b7d50d/src/simd_accel/teddy128.rs">well commented
implementation</a>.
Going into more detail on Teddy would require a whole blog post on its own!)</p>
<p>When <code>rg</code> sees that there are a large number of literals, it could do one of
two things:</p>
<ol>
<li>Try to cut down the set even more. For example, in this case, we could strip
the last character from each prefix off and end up with a much smaller set.
Unfortunately, even though we have fewer literals, we wind up with a still
not-so-small set of two-character literals, which will also tend to produce
a lot more false positive candidates just because of their length.</li>
<li>Move to a different multiple pattern algorithm, such as
<a href="https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm">Aho-Corasick</a>.</li>
</ol>
<p>I have tried to implement (1) in the past, but I’ve always wound up in a game
of whack-a-mole. I might make one common case faster, but another common case a
lot slower. In those types of cases, it’s usually better to try and achieve
good average case performance. Luckily for us, Aho-Corasick does <em>exactly</em>
that.</p>
<p>We do still have a few tricks up our sleeve though. For example, many
Aho-Corasick implementations are built as-if they were
<a href="https://en.wikipedia.org/wiki/Trie">tries</a>
with back-pointers for their failure transitions.
We can actually do better than that. We can compile all of its failure
transitions into a DFA with a transition table contiguous in memory. This means
that every byte of input corresponds to a single lookup in the transition table
to find the next state. We never have to waste time chasing pointers or walking
more than one failure transition for any byte in the search text.</p>
<p>Of course, this transition table based approach is memory intensive, since you
need space for <code>number_of_literals * number_of_states</code>, where
<code>number_of_states</code> is roughly capped at the total number of bytes in all of the
literals. While 48 literals of length 3 is too much for Teddy to handle, it’s
barely a blip when it comes to Aho-Corasick, even with its memory expensive
transition table based approach. (N.B. In the literature, this particular
implementation of Aho-Corasick is often called “Advanced” Aho-Corasick.)</p>
<h3 id="subtitles_surrounding_words"><code>subtitles_surrounding_words</code></h3>
<p><strong>Description</strong>: This benchmarks a pattern that searches for words surrounding
the literal string <code>Holmes</code>. This pattern was specifically constructed to
defeat both prefix and suffix literal optimizations.</p>
<p><strong>English pattern</strong>: <code>\w+\s+Holmes\s+\w+</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg             0.605 +/- 0.000 (lines: 317)
</span></span><span><span>grep           1.286 +/- 0.002 (lines: 317)
</span></span><span><span>rg (ASCII)     0.602 +/- 0.000 (lines: 317)*+
</span></span><span><span>ag (ASCII)     11.663 +/- 0.008 (lines: 323)
</span></span><span><span>ucg (ASCII)    4.690 +/- 0.002 (lines: 317)
</span></span><span><span>grep (ASCII)   1.276 +/- 0.002 (lines: 317)</span></span></code></pre></div>


<!-- raw HTML omitted -->
<p><strong>Russian pattern</strong>: <code>\w+\s+Холмс\s+\w+</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg             0.957 +/- 0.001 (lines: 278)*+
</span></span><span><span>grep           1.660 +/- 0.002 (lines: 278)
</span></span><span><span>ag (ASCII)     2.411 +/- 0.001 (lines: 0)
</span></span><span><span>ucg (ASCII)    2.980 +/- 0.002 (lines: 0)
</span></span><span><span>grep (ASCII)   1.596 +/- 0.003 (lines: 0)</span></span></code></pre></div>


<!-- raw HTML omitted -->
<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
</ul>
<p><strong>Analysis</strong>: In order to compete on this benchmark, a search tool will need to
implement a so-called “inner literal” optimization. You can probably guess what
that means: it is an optimization that looks for literal strings that appear
<em>anywhere</em> in the pattern, and if a literal is found that must appear in every
match, then a search tool can quickly scan for that literal instead of applying
the full regex to the search text.</p>
<p>The key thing that permits this optimization to work is the fact that most
search tools report results <em>per line</em>. For example, in this case, if a line
contains the literal <code>Holmes</code>, then the search tool can find the beginning and
ending of that line and run the full pattern on <em>just that line</em>. If the
literal is relatively rare, this keeps us out of the regex engine for most of
the search. And of course, if the literal doesn’t appear at all in the corpus,
then we will have never touched the regex engine at all.</p>
<p>To achieve the full optimization, you probably need to parse your pattern
into its abstract syntax (abbreviated “AST” for abstract syntax tree) to
extract the literal. It is worth pointing out however that one can probably get
a lot of mileage with simpler heuristics, but a real pattern parser is the only
way to do this optimization robustly. The problem here is that for most regex
engines, parsing the pattern is an unexposed implementation detail, so it can
be hard for search tools to extract literals in a robust way without writing
their own parser, and a modern regex parser is no easy task! Thankfully, Rust’s
regex library exposes an additional library,
<a href="https://doc.rust-lang.org/regex/regex_syntax/index.html"><code>regex-syntax</code></a>,
which provides a full parser. <code>rg</code> implements this optimization relatively
easily with the help of <code>regex-syntax</code>, while GNU grep implements this
optimization because the search tool and the underlying regex engine are
coupled together.</p>
<p>Why does the search tool need to perform this optimization? Why can’t the
underlying regex engine do it? I personally have thought long and hard about
this particular problem and haven’t been able to come up with an elegant
solution. The core problem is that once you find an occurrence of the literal,
you <em>don’t know where to start searching the full regex</em>. In a general purpose
regex engine, a pattern could match an arbitrarily long string. For example,
<code>\w+\s+Holmes\s+\w+</code> mightly only match at the very end of a gigabyte sized
document. There are ways to work around this. For example, you could split the
regex into three pieces: <code>\w+\s+</code>, <code>Holmes</code> and <code>\s+\w+</code>. On every occurrence
of the <code>Holmes</code> literal, you could search for the beginning of the match by
executing <code>\w+\s+</code> in reverse starting just before the literal, and executing
<code>\s+\w+</code> forwards starting just after the literal. The key problem with this
approach is that it exposes you to quadratic behavior in the worst case (since
<code>\w+\s+</code> or <code>\s+\w+</code> could cause you to re-scan text you’ve already seen).
While I believe there is a general purpose way to solve this and still
guarantee linear time searching, a good solution hasn’t revealed itself yet.</p>
<p>Based on the data in this benchmark, only <code>rg</code> and GNU grep perform this
optimization. Neither <code>ag</code> nor <code>ucg</code> attempt to extract any inner literals from
the pattern, and it looks like PCRE doesn’t try to do anything too clever.
(Of course, Rust’s regex library doesn’t either, this optimization is done in
<code>rg</code> proper.)</p>
<p>As for the Russian pattern, we see that only tools with proper Unicode support
can execute the query successfully. The reason is because <code>\w</code> is ASCII only in
<code>ucg</code> and <code>ag</code>, so it can’t match the vast majority of word characters (which
are Cyrllic) in our sample. Otherwise, both <code>rg</code> and GNU grep remain fast,
primarily because of the inner literal optimization.</p>
<h3 id="subtitles_no_literal"><code>subtitles_no_literal</code></h3>
<p><strong>Description</strong>: This benchmark purposefully has no literals in it, which makes
it a bit idiosyncratic, since most searches done by end users probably have at
least some literal in them. However, it is a useful benchmark to gauge the
general performance of the underlying regex engine.</p>
<p><strong>English pattern</strong>: <code>\w{5}\s+\w{5}\s+\w{5}\s+\w{5}\s+\w{5}\s+\w{5}\s+\w{5}</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg             2.777 +/- 0.003 (lines: 13)
</span></span><span><span>rg (ASCII)     2.541 +/- 0.005 (lines: 13)*+
</span></span><span><span>ag (ASCII)     10.076 +/- 0.005 (lines: 48)
</span></span><span><span>ucg (ASCII)    7.771 +/- 0.004 (lines: 13)
</span></span><span><span>grep (ASCII)   4.411 +/- 0.004 (lines: 13)</span></span></code></pre></div>


<!-- raw HTML omitted -->
<p><strong>Russian pattern</strong>: <code>\w{5}\s+\w{5}\s+\w{5}\s+\w{5}\s+\w{5}\s+\w{5}\s+\w{5}</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg             4.905 +/- 0.003 (lines: 41)
</span></span><span><span>rg (ASCII)     3.973 +/- 0.002 (lines: 0)
</span></span><span><span>ag (ASCII)     2.395 +/- 0.004 (lines: 0)*+
</span></span><span><span>ucg (ASCII)    3.006 +/- 0.005 (lines: 0)
</span></span><span><span>grep (ASCII)   2.483 +/- 0.005 (lines: 0)</span></span></code></pre></div>


<!-- raw HTML omitted -->
<ul>
<li><code>*</code> - Best mean time.</li>
<li><code>+</code> - Best sample time.</li>
<li><code>ag</code> gets more matches on the English pattern since it does multiline search.
Namely, the <code>\s</code> can match a <code>\n</code>.</li>
<li><code>grep</code> with Unicode support was dropped from this benchmark because it takes
over 90 seconds on the English pattern and over 4 <strong>minutes</strong> on the Russian
pattern. In both cases, GNU grep and <code>rg</code> report the same results.</li>
</ul>
<p><strong>Analysis</strong>: Once again, no other search tool performs as well as <code>rg</code>. For
the English pattern, both <code>rg</code> and <code>rg (ASCII)</code> have very similar performance,
despite <code>rg</code> supporting Unicode.</p>
<p>What specifically makes <code>rg</code> faster than GNU grep in this case? Both search
tools ultimately use a DFA to execute this pattern, so their performance should
be roughly the same. I don’t actually have a particularly good answer for this.
Both GNU grep and Rust’s regex library unroll the DFA’s inner loop, and both
implementations compute states on the fly. I can make a guess though.</p>
<p>Rust’s regex library avoids a single pointer dereference when following a
transition. How it achieves this is complicated, but it’s done by representing
states as indices into the transition table rather than simple incremental ids.
This permits the generated code to use simple addition to address the location
of the next transition, which can be done with addressing modes in a single
instruction. (Specifically, this optimization means we don’t need to do any
multiplication to find the state transition.) A single pointer dereference
might not seem like much, but when it’s done for every state transition over a
large corpus such as this, it can have an impact.</p>
<p>When it comes to the Russian pattern, such details are far less important
because GNU grep takes <em>minutes</em> to run. This suggests that it isn’t building
UTF-8 decoding into its DFA, and is instead doing something like decoding a
character at a time, which can have a lot of overhead associated with it. I
admit that I don’t quite grok this aspect of GNU grep though, so I could have
its cost model wrong. Now, in all fairness, GNU grep’s locale and encoding
support far exceeds what <code>rg</code> supports. However, in today’s world, UTF-8 is
quite prevalent, so supporting that alone is often enough. More to the point,
given how common UTF-8 is, it’s important to remain fast while supporting
Unicode, which GNU grep isn’t able to do.</p>
<p>Unfortunately, the other tools don’t support Unicode, so they can’t be
meaningfully benchmarked on the Russian pattern.</p>
<h2 id="bonus-benchmarks">Bonus benchmarks</h2>
<p>In this section, we’ll take a look at a few crazier benchmarks that aren’t
actually part of the suite I’ve published. Indeed, the performance differences
between tools are often so large that a fastidious analysis isn’t really
necessary. More to the point, these usage patterns aren’t necessarily
representative of common usage (not that these usages aren’t important, they’re
just niche), so the performance differences are less important. Nevertheless,
it is fun to see how well <code>rg</code> and the other tools hold up under these
requests.</p>
<h3 id="everything"><code>everything</code></h3>
<p><strong>Description</strong>: In this benchmark, we compare how long it takes for each tool
to report every line as a match. This benchmark was run in the root of the
Linux repository.</p>
<p><strong>Pattern</strong>: <code>.*</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg                 1.081 (lines: 22065361)
</span></span><span><span>ag                 1.660 (lines: 55939)
</span></span><span><span>git grep           3.448 (lines: 22066395)
</span></span><span><span>sift             110.018 (lines: 22190112)
</span></span><span><span>pt                 0.245 (lines: 3027)
</span></span><span><span>rg (whitelist)     0.987 (lines: 20936584)
</span></span><span><span>ucg (whitelist)    5.558 (lines: 23163359)</span></span></code></pre></div>


<p><strong>Analysis</strong>: This benchmark is somewhat silly since it’s something you
probably never want a search tool to do. Nevertheless, it is useful to know
that <code>rg</code> scales quite well to a huge number of matches.</p>
<p>One of the key tricks that a good regex engine will do in this case is stop
searching text as soon as it knows it has a match if all the caller cares about
is “is there a match or not?” In this case, we will find a match at the
beginning of every line, immediately quit, find the line boundaries and then
repeat the process. There is no particular special cased optimization for <code>.*</code>
in either <code>rg</code> or Rust’s regex library (although there could be).</p>
<p>Interestingly, neither <code>ag</code> nor <code>pt</code> actually report every line. They appear to
have some kind of match limit. Which isn’t altogether unreasonable. This is a
search tool after all, and some might consider that returning every result
isn’t useful.</p>
<h3 id="nothing"><code>nothing</code></h3>
<p><strong>Description</strong>: This is just like the <a href="#everything"><code>everything</code></a> benchmark,
except it inverts the results. The correct result is for a search tool to
report no lines as matching. This benchmark also searches the Linux kernel
source code, from the root of repository.</p>
<p><strong>Pattern</strong>: <code>.*</code> (with the <code>-v</code> or <code>--invert-match</code> flag set)</p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg                0.302 (lines: 0)
</span></span><span><span>ag                takes minutes
</span></span><span><span>git grep          0.905 (lines: 0)
</span></span><span><span>sift             12.804 (lines: 0)
</span></span><span><span>pt                -----
</span></span><span><span>rg (whitelist)    0.251 (lines: 0)
</span></span><span><span>ucg (whitelist)   -----</span></span></code></pre></div>


<p><strong>Analysis</strong>: While this benchmark is even more ridiculous than the previous
one (“give me nothing of everything”), it does expose a few warts and omissions
in other tools. Namely, <code>ag</code> seems to slow way down when reporting inverted
matches. Neither <code>pt</code> nor <code>ucg</code> support inverted searching at all. <code>sift</code>
redeems itself from the previous benchmark (perhaps it has a lot of overhead
associated with printing matches that it doesn’t hit here). Neither <code>rg</code> nor
<code>git grep</code> have any problems satisfying the request.</p>
<h3 id="context"><code>context</code></h3>
<p><strong>Description</strong>: This benchmarks how well a search tool can show the context
around each match. Specifically, in this case, we ask for the two lines
preceding and succeeding every match. We run this benchmark on the English
subtitle corpus. Note that all tools are asked to count lines.</p>
<p><strong>Pattern</strong>: <code>Sherlock Holmes</code> (with <code>--context 2</code>)</p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg        0.612 (lines: 3533)
</span></span><span><span>ag        3.530 (lines: 3533)
</span></span><span><span>grep      1.075 (lines: 3533)
</span></span><span><span>sift      0.717 (lines: 3533)
</span></span><span><span>pt       17.331 (lines: 2981)
</span></span><span><span>ucg       -----</span></span></code></pre></div>


<p><strong>Analysis</strong>: <code>rg</code> continues to do well here, but beats <code>sift</code> by only a hair.
In general, computing the context shouldn’t be that expensive since it is done
rarely (only for each match). Nevertheless, both <code>ag</code> and <code>pt</code> seem to take a
pretty big hit for it. <code>pt</code> also seems to have a bug. (Which is understandable,
getting contexts right is tricky.) Finally, <code>ucg</code> doesn’t support this feature,
so we can’t benchmark it.</p>
<h3 id="huge"><code>huge</code></h3>
<p><strong>Description</strong>: This benchmark runs a simple literal search on a file that is
<code>9.3GB</code>. In fact, this is the original English subtitle corpus in its entirety.
(In the benchmark suite, we take a 1GB sample.)</p>
<p><strong>Pattern</strong>: <code>Sherlock Holmes</code></p>



<div><pre tabindex="0"><code data-lang="text"><span><span>rg                1.786 (lines: 5107)
</span></span><span><span>grep              5.119 (lines: 5107)
</span></span><span><span>sift              3.047 (lines: 5107)
</span></span><span><span>pt               14.966 (lines: 5107)
</span></span><span><span>rg (lines)        4.467 (lines: 5107)
</span></span><span><span>ag (lines)       19.132 (lines: 5107)
</span></span><span><span>grep (lines)      9.213 (lines: 5107)
</span></span><span><span>sift (lines)      6.303 (lines: 5107)
</span></span><span><span>pt (lines)       15.485 (lines: 5107)
</span></span><span><span>ucg (lines)       4.843 (lines: 1543)</span></span></code></pre></div>


<p><strong>Analysis</strong>: At first glance, it appears <code>ucg</code> competes with <code>rg</code> when
counting lines (being only slightly slower), but in fact, <code>ucg</code> reports the
wrong number of results! My suspicion is that <code>ucg</code> gets into trouble when
trying to search files over 2GB.</p>
<p>The other intesting bit here is how slow <code>pt</code> is, even when not counting lines,
despite the fact that <code>sift</code> is fast. They both use Go’s regexp engine and
should be able to be fast in the case of a simple literal. It’s not clear what
<code>pt</code>’s slow down here is. One hypothesis is that even though I’m asking it to
not count lines, it’s still counting them but simply not showing them.</p>
<h2 id="conclusions">Conclusions</h2>
<p>I started this blog post by claiming that I could support the following claims
with evidence:</p>
<ul>
<li>For both searching single files <em>and</em> huge directories of files, no other
tool obviously stands above <code>ripgrep</code> in either performance or correctness.</li>
<li><code>ripgrep</code> is the only tool with proper Unicode support that doesn’t make
you pay dearly for it.</li>
<li>Tools that search many files at once are generally <em>slower</em> if they use
memory maps, not faster.</li>
</ul>
<p>I attempted to substantiate the first claim by picking a popular repository
(Linux kernel) and a variety of patterns that an end user might search for.
While <code>rg</code> doesn’t quite come out on top on every benchmark, no other tool can
claim superiority. In particular, <code>git grep</code> edges out <code>rg</code> on occasion by a
few milliseconds, but <code>rg</code> in turn will beat <code>git grep</code> handedly
(sometimes by an order of magnitude, as in the case of
<a href="#linux-unicode-word"><code>linux_unicode_word</code></a>) as the patterns grow more complex,
especially when the search tool is asked to support Unicode. <code>rg</code> manages to
compete with <code>git grep</code> and beat other tools like The Silver Searcher by:</p>
<ul>
<li>Implementing fast directory traversal with a minimal number of stat calls.</li>
<li>Applying <code>.gitignore</code> filters with a
<a href="https://doc.rust-lang.org/regex/regex/struct.RegexSet.html"><code>RegexSet</code></a>,
which enables matching multiple globs against a single path all at once.</li>
<li>Distributing work quickly to multiple threads with a Chase-Lev work stealing
queue.</li>
<li>Explicitly <em>not</em> using memory maps.</li>
<li>Using an overall very fast regex engine.</li>
</ul>
<p>I also attempted to substantiate the first claim by showing benchmarks of <code>rg</code>
against other tools on a single file. In this benchmark, <code>rg</code> comes out on top
in every single one, often by a large margin. Some of those results are a
result of the following optimizations:</p>
<ul>
<li>Attempting to pick a “rare” byte to use <code>memchr</code> with for fast skipping.</li>
<li>Using a special SIMD algorithm called Teddy for fast multiple pattern search.</li>
<li>When Teddy isn’t usable, fallback to an “advanced” form of Aho-Corasick that
never moves through more than one transition on each byte of input.</li>
<li>Building UTF-8 decoding into a finite state machine.</li>
</ul>
<p>For the second claim, I provided benchmarks that attempt to use Unicode
features such as conforming to Unicode’s simple case folding rules and Unicode
aware character classes such as <code>\w</code>. The only tools capable of handling
Unicode are <code>rg</code>, GNU grep and <code>git grep</code>. The latter two tend to get much
slower when supporting the full gamut of Unicode while <code>rg</code> mostly maintains
its performance.</p>
<p>For the third claim, I showed multiple benchmarks of <code>rg</code> controlling for
memory maps. Namely, we measured how fast <code>rg</code> was both with and without memory
maps, and showed that memory maps perform worse when searching many small files
in parallel, but perform better on searching single large files. (At least, on
<code>Linux x86_64</code>.) We also learned that memory maps probably pay an additional
penalty inside a VM.</p>
<p>My hope is that this article not only convinced you that <code>rg</code> is quite fast,
but more importantly, that you found my analysis of each benchmark educational.
String searching is an old problem in computer science, but there is still
plenty of work left to do to advance the state of the art.</p>
      </article>

      

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I worked in Amazon HR and was disgusted at what I was seeing with PIP plans (398 pts)]]></title>
            <link>https://www.businessinsider.com/amazon-hr-performance-improvement-plans-pip-pivot-had-to-quit-2023-11</link>
            <guid>38471744</guid>
            <pubDate>Thu, 30 Nov 2023 10:09:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/amazon-hr-performance-improvement-plans-pip-pivot-had-to-quit-2023-11">https://www.businessinsider.com/amazon-hr-performance-improvement-plans-pip-pivot-had-to-quit-2023-11</a>, See on <a href="https://news.ycombinator.com/item?id=38471744">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-type="content-lock" data-load-strategy="exclude">
                                  <ul><li>A former HR staffer at Amazon put employees on a performance-improvement plan known as Pivot.</li><li>Then, the HR staffer, who says they developed PTSD from the work, was put on their own PIP.</li><li>An Amazon spokesperson said the account contained inaccuracies about the company's process.</li></ul><!-- Excluded mobile ad on desktop --><div id="formContainer" data-component-type="inline-newsletter-module" data-event-label="insider_today" data-newsletter-id="1" data-list="Insider Today" data-acq-source="careersinlinesignup">
                        
                        
                          <section>
                              
                        
                            <p><svg version="1.1" xmlns="http://www.w3.org/2000/svg" role="img" width="50" height="50" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50;" xml:space="preserve">
                          <title>Loading</title>
                          <desc>Something is loading.</desc>
                          <path fill="#111" d="M43.935,25.145c0-10.318-8.364-18.683-18.683-18.683c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615c8.072,0,14.615,6.543,14.615,14.615H43.935z">
                            <animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform>
                          </path>
                        </svg></p>
                            
                        
                            
                        
                            <div>
                              <p>Thanks for signing up!</p>
                              
                              <p>
                              Access your favorite topics in a personalized feed while you're on the go.
                                    </p>
                            </div>
                        
                            
                            
                          </section>
                        
                            <div>
                                <p><img src="https://www.businessinsider.com/public/assets/rebrand/newsletter-bull.png" data-old-src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1 1'%3E%3C/svg%3E" data-src="/public/assets/rebrand/newsletter-bull.png">
                              
                              
                              
                              </p>    </div>
                        
                          
                        </div><p><em>This as-told-to essay is based on a conversation with a former Amazon human-resources worker who was put into the company's performance-management program known as Pivot. This person spoke on condition of anonymity to avoid jeopardizing their career. Business Insider has verified their identity and employment at the company. The conversation has been edited for length and clarity.</em></p><p>I worked at <a target="_blank" href="https://www.businessinsider.com/amazon" data-analytics-product-module="body_link" rel="">Amazon</a> in HR for several years. Not only did I administer Pivots, but it was eventually brought to my attention that I was going to be going through one.</p><p>They made a mistake by doing that with me. There wasn't a lot of information to justify a poor performance.</p><p>The Pivot goal was a straight, <a target="_blank" href="https://www.businessinsider.com/amazon-focus-coaching-plans-unregretted-attrition-2021-5" data-analytics-product-module="body_link" rel=""><u>across-the-board 6% number</u></a>. And as an HR person, that is a hefty figure.</p><!-- Excluded mobile ad on desktop --><p>And it was driven hard by the HR VPs to show the metrics — daily, weekly — to make sure we knew who was in the pipeline. Not to improve, but who was in the pipeline to get out. There wasn't a lot of interest in improving people.</p><p>You might be cutting some prime choice with the fat. And they were OK with that. <a target="_blank" href="https://www.businessinsider.com/amazon-employees-annual-reviews-rating-2021-4" data-analytics-product-module="body_link" rel=""><u>They wanted that number</u></a>. The managers who had to implement it and tell their people they were on Pivot — I would say a majority of them hated it. Because, one, they didn't have the skills to be able to manage performance that soon out of the gate. A lot of our managers were brand new.</p><p>The first thing you had to do was work with a Pivot consultant. So, that was somebody in HR besides the manager's business partner. And you'd talk about if it was the right time or if it was the wrong time to Pivot someone.</p><p>I would say 80% of my time ended up being focused one way or another on Pivot. Either the Pivot appeal or the Pivot work that workers' managers had to do. And look, I'm not going to say you're going to ever find this somewhere, locked down in words. But the idea is, if you're putting somebody in Pivot, you make that so damn hard that they don't get out.</p><!-- Excluded mobile ad on desktop --><p>Almost always, unless there was some really unique set of situations where it came out during the appeal, the success rate of that was virtually none.</p><p>When I wasn't working on Pivots, working in HR was great. We were supposed to be doing coaching and focusing on strengths and moving people through the organization in a positive way.</p><p>Later, when Pivot came back, we had to stack rank all of our employees. The way we broke it down, we called it top tier, which was, you know, maybe 15-20% by the time it worked out. And then you had the middle. And then you have the bottom tier. The bottom tier was about 20-25%, maybe even up to 30%. The guidelines that they expressed publicly may be different because we always worked to make sure we had more than that because some went bad — or went off the rails, and we couldn't exit them for whatever reason.</p><p>We were way over how many people were actually underperforming or detrimental to the business. Maybe around 1% or 1.5% to 2% were actually not performing well.</p><!-- Excluded mobile ad on desktop --><h2><strong>I have PTSD</strong></h2><p>I was disgusted at what I was seeing with the Pivot process. This process alone has given me post-traumatic stress disorder. It impacted me so much as a person that I had to get out of there.</p><p>When it was justified, it was easier to push someone out. If it's deserved, there's no problem. But when it wasn't deserved, you had people crying and begging, and they couldn't understand.</p><p>You had visa-sponsored employees who, once we Pivoted them and moved them out, no longer were authorized to work in the United States. So, they had to make immediate plans to get out of the country. And it's a long process to get sponsored by another group.</p><p>In the years I was there, I never, ever, ever had any performance issue given to me — not even anything close to being serious. I had no worries because I asked for feedback all the time. I'm like, "What can I do? How can I do better?" I didn't ever want to be blindsided by Pivot myself. And what a lot of people did —&nbsp;if they got the indication that they were going down that track —&nbsp;was they would transfer jobs right away. Some people were successful. A lot of people weren't.</p><!-- Excluded mobile ad on desktop --><p>Normally, with a performance-improvement program, as an HR person, you're following progressive discipline. Are you seeing notes that this person is having trouble? Are you seeing coaching conversations that are taking place? So for it to actually just — boom —&nbsp;be there is really problematic.</p><h2><strong>It was my turn</strong></h2><p>During my performance evaluation, when it was clear I was on a PIP, my manager shared criticisms that I'd never heard before. I said, "I've never had any of these comments come to me ever." Essentially, it was a lot of made-up stuff. I mean, you could put some truth to it. I'd been late on a few assignments. But everybody's got some element of things they can improve on in their work. My manager just chose to bring those out.</p><p>Amazon broke down people into three categories. You were either top-tier, middle of the pack, or at least effective.</p><p>Normally, they won't tell you what they rated you, and I'm like, "Come on. I know this stuff just as much as you do. I know the wording. You didn't put me in the medium category. Would you just admit that you put me in the least-effective category?" And I got my manager to admit that.</p><!-- Excluded mobile ad on desktop --><p>I wasn't put on Pivot. My manager wanted to work with me a little bit to see if I was going to commit to the job. So they sat me down and said I could go on Pivot and leave right away, or they would work with me. Well, obviously not having any job opportunities, I said, "Look, I'm in it. Let's try to get better and go from there." So my manager took away all my direct reports, and shoved me into a small box, and said you could do this and try to work yourself out of it.</p><p>Right after that, I started putting in the full push to get another job. And so I started interviewing. I actually had a headhunter that reached out to me. Originally, I told her no, but then some of this stuff happened. And I'm like, "OK, let's revisit it." I got to the point where they offered me a job, and I was going to quit. But I had a huge stock investment coming up. So there was no way I was going to rock the boat in any way, shape, or form just trying to get to this date.</p><p>If you walked away during the Pivot or anytime before you had your investment before it was there for you, you would lose it all. And I'm not talking a little bit of money. I'm talking: I had a couple hundred thousand dollars coming to me.</p><p>I played along, and I'm good at playing along when I have to be. So then the money is in my account. That next day, I called my manager and I told them I was resigning. They blew a gasket —&nbsp;absolutely blew a gasket because I had told them that I was in it for the long run. I said, "Look, you gave me no choice. You put this threat against me. I'm not just gonna sit there and wait for it to be dependent on you. You get to make the call whether I make it or not." My manager was super mad and asked me when I was leaving. I said two weeks. They were incredulous that I wasn't giving them more respect.</p><!-- Excluded mobile ad on desktop --><p>The biggest thing —&nbsp;and I'm gonna say this goes for many, many people that were put on Pivot —&nbsp;is there were no warning signs. There was no trail of communication saying, "You are underperforming." I mean, even if it's something as simple as, "Hey, can you do better on this next time?" I know, certainly, I got zero negative feedback. I got the feedback that I was rocking it. And then all of a sudden, to be in this place, it's like, "Huh."</p><p>I still wonder about what happened to all the people that went through that process. How did it impact their life? I think it leads to a lot of mental-health issues.</p><blockquote><section><em>Margaret Callahan, an Amazon spokesperson, told BI via email:</em></section><section><em>"Like most companies, we have a performance management process that helps our managers identify who on their teams are performing well and who may need more support. For the small number of employees who are underperforming, we use performance management programs to help them improve, and many employees do just that. Sometimes the programs result in employees leaving the company. Business Insider declined to share the information needed to verify this individual's account, but it contains a number of inaccuracies about our performance management process. An unverified, anonymous anecdote in a Business Insider 'As told to' essay does not represent the experience of the vast majority of our employees."</em></section></blockquote><p><em>Do you have something to share about what you're seeing in your workplace? Insider would like to hear from you. Email our workplace team from a nonwork device at <u>thegrind@businessinsider.com</u> with your story or to ask for one of our reporter's Signal numbers. Or </em><a target="_blank" href="https://www.businessinsider.com/insider-guide-to-securely-sharing-whistleblower-information-about-powerful-institutions-2021-10" data-analytics-product-module="body_link" rel=""><em><u>check out Business Insider's source guide</u></em></a><em> for tips on sharing information securely.</em></p>
                      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[XCurl (280 pts)]]></title>
            <link>https://daniel.haxx.se/blog/2023/11/30/xcurl/</link>
            <guid>38471004</guid>
            <pubDate>Thu, 30 Nov 2023 08:10:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daniel.haxx.se/blog/2023/11/30/xcurl/">https://daniel.haxx.se/blog/2023/11/30/xcurl/</a>, See on <a href="https://news.ycombinator.com/item?id=38471004">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>It is often said that <em>Imitation is the Sincerest Form of Flattery</em>.</p>



<p>Also, remember <a href="https://daniel.haxx.se/blog/2019/06/19/google-to-reimplement-curl-in-libcrurl/" data-type="post" data-id="12576">libcrurl</a>? That was the name of the thing Google once planned to do:  reimplement the libcurl API on top of their Chrome networking library. Flattery or not, it never went anywhere.</p>



<p>The other day I received an email asking me about details regarding something called <strong>xCurl</strong>. Not having a clue what that was, a quick search soon had me enlightened.</p>



<p>xCurl is, using <a href="https://learn.microsoft.com/en-us/gaming/gdk/_content/gc/networking/overviews/web-requests/intro-xcurl">their own words</a>, <em>a Microsoft Game Development Kit (GDK) compliant implementation of the libCurl API</em>.</p>



<h2>A Frankencurl</h2>



<p>The article I link to above describes how xCurl differs from libcurl: </p>



<blockquote>
<p>xCurl differs from libCurl in that xCurl is implemented on top of WinHttp and automatically follows all the extra Microsoft Game Development Kit (GDK) requirements and best practices. While libCurl itself doesn’t meet the security requirements for the Microsoft Game Development Kit (GDK) console platform, use xCurl to maintain your same libCurl HTTP implementation across all platforms while changing only one header include and library linkage.</p>
</blockquote>



<p>I don’t know anything about WinHttp, but since that is an HTTP API I can only presume that making libcurl use that instead of plain old sockets has to mean a pretty large surgery and code change. I also have no idea what the mentioned “security requirements” might be. I’m not very familiar with Windows internals nor with their game development setups.</p>



<p>The article then goes on to describe with some detail exactly which libcurl options that work, and which don’t and what libcurl build options that were used when xCurl was built. No DoH, no proxy support, no cookies etc.</p>



<p>The provided functionality is certainly a very stripped down and limited version of the libcurl API. A fun detail is that the quite bluntly just link to the <a href="https://curl.se/libcurl/c/">libcurl API documentation</a> to describe how xCurl works. It is easy and convenient of course, and it will certainly make xCurl “forced” to stick to the libcurl behavior</p>



<p>With large invasive changes of this kind we can certainly hope that the team making it has invested time and spent serious effort on additional testing, both before release and ongoing.</p>



<h2>Source code?</h2>



<p>I have not been able to figure out how to download xCurl in any form, and since I can’t find the source code I cannot really get a grip of exactly how much and how invasive Microsoft has patched this. They have not been in touch or communicated about this work of theirs to anyone in the curl project.</p>



<p>Therefore, I also cannot say which libcurl version this is based on – as there is no telling of that on the page describing xCurl.</p>



<p>The email that triggered me to crawl down this rabbit hole included a copyright file that seems to originate from an xCurl package, and that includes the curl license. The curl license file has the specific detail that it shows the copyright year range at the top and this file said </p>



<pre>Copyright (c) 1996 - 2020, Daniel Stenberg, daniel@haxx.se, and many contributors, see the THANKS file.</pre>



<p>It <em>might</em> indicate that they use a libcurl from a few years back. Only <em>might</em>, because it is quite common among users of libcurl to “forget” (sometimes I’m sure on purpose) to update this copyright range even when they otherwise upgrade the source code. This makes the year range a rather weak evidence of the actual age of the libcurl code this is based on.</p>



<h2>Updates</h2>



<p>curl (including libcurl) ships a new version <em>at least</em> once every eight weeks. We merge bugfixes at a rate of around three bugfixes <em>per day</em>. Keeping a heavily modified libcurl version in sync with the latest curl releases is hard work.</p>



<p>Of course, since they deliberately limit the scope of the functionality of their clone, lots of upstream changes in curl will not affect xCurl users.</p>



<h2>License</h2>



<p>curl is licensed under… <a href="https://curl.se/docs/copyright.html">the curl license</a>! It is an MIT license that I was unclever enough to slightly modify many years ago. The changes are enough for organizations such as SPDX to consider it a separate one: <a href="https://spdx.org/licenses/curl.html">curl</a>. I normally still say that curl is MIT licensed because the changes are minuscule and do not change the spirit of the license.</p>



<p>The curl license of course allows Microsoft or anyone else do to this kind of stunt and they don’t even have to provide the source code for their changes or the final product and they don’t have to ask or tell anyone:</p>



<p><em>Permission to use, copy, modify, and distribute this software for any purpose with or without fee is hereby granted, provided that the above copyright notice and this permission notice appear in all copies.</em></p>



<p>I once picked this license for curl exactly because it allows this. Sure it might sometimes then make people do things in secret that they never contribute back, and we miss out on possible improvements because of that, but I think the more important property is that no company feels scared or restricted as to when and where they can use this code. A license designed for maximum adoption.</p>



<p>I have always had the belief that it is our relentless update scheme and never-ending flood of bugfixes that is what will keep users wanting to use the real thing and avoid maintaining long-running external patches. There will of course always be exceptions to that.</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brickception (288 pts)]]></title>
            <link>https://brickception.xyz/</link>
            <guid>38470764</guid>
            <pubDate>Thu, 30 Nov 2023 07:35:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brickception.xyz/">https://brickception.xyz/</a>, See on <a href="https://news.ycombinator.com/item?id=38470764">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <section>
          <p>Brickception is a fun take on the classic breakout game with popup windows!</p>
          <p>
            <mark>Enable popup windows to play this game.</mark>
            Two popup windows are launched - the main game window and the paddle window. (Sorry, you cannot play this
            game where popups are not supported, e.g., mobile devices)
          </p>
          <p><mark>Paddle window has a nested game!</mark> You need to win in both the windows.</p>
          <p>
            <mark>Moving the paddle window moves the paddle.</mark>
            The ball of the main window bounces off the top of the paddle window.
          </p>
        </section>
        
        <section>
          <p>
            Developed by <a href="https://twitter.com/preetster" target="_blank" rel="noopener">Preet Shihn</a>
            <br>
            Original concept by <a href="https://twitter.com/Reputeless" target="_blank" rel="noopener">Ryo Suzuki</a>
            <br>
            Source code <a href="https://github.com/pshihn/brickception" target="_blank" rel="noopener">available on
              GitHub</a>
          </p>
        </section>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Adobe's buy of Figma is 'likely' bad for developers, rules UK regulator (192 pts)]]></title>
            <link>https://www.theregister.com/2023/11/29/adobes_buy_of_figma_is/</link>
            <guid>38470046</guid>
            <pubDate>Thu, 30 Nov 2023 05:33:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/11/29/adobes_buy_of_figma_is/">https://www.theregister.com/2023/11/29/adobes_buy_of_figma_is/</a>, See on <a href="https://news.ycombinator.com/item?id=38470046">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Adobe’s $20 billion buy of web-first design collaboration start-up Figma will harm software developers if it goes ahead as proposed, according to a provisional ruling on the merger by Britain’s competition regulator.</p>
<p>The Competition and Markets Authority launched a <a target="_blank" href="https://www.theregister.com/2023/07/13/adobe_figma_cma_investigation/">deeper investigation of the tie-up in July</a> when it classified Figma as an “emerging threat to Adobe”.</p>
<p>Now in the latest twist, the regulator says it found the merger would eliminate one of two major players in three software sub-markets: product design; image editing; and illustration.</p>

    

<p>Figma’s tools are used by well-known businesses that are key to the success of the digital economy, the CMA reckons, including Airbnb, Patagonia and Vodafone. Approving the acquisition “would remove the constraint Adobe exerts on Figma through its product design software, AdobeXD.”</p>

        


        

<p>The CMA adds in its <a target="_blank" rel="nofollow" href="https://www.gov.uk/government/news/adobe-figma-deal-could-harm-uk-digital-design-sector?utm_medium=email&amp;utm_campaign=govuk-notifications-topic&amp;utm_source=a9bd367c-1ec0-4a7f-94d8-0d328af5b1ac&amp;utm_content=immediately">report</a>: “The inquiry group also provisionally concluded that Adobe abandoned development of new product design software which could have competed even more closely with Figma and, given the timing of the decision, did this as a consequence of the merger.</p>
<p>“This supports the CMA’s concern that this proposed deal would likely reduce innovation and the development of competitive new products.”</p>

        

<p>Some software developers are worried that Adobe would up the price of Figma’s subsciption post merger, something Figma denied would happen.</p>
<p>Figma offers a browser-based app that manages file organization by listing projects and related files in a dedicated format. It requires no installation, no patching and no updates. The company has an estimated four million users.</p>
<ul>

<li><a href="https://www.theregister.com/2023/05/23/meta_giphy_sale/">Meta forced to sell Giphy, takes 87% loss in Shutterstock deal</a></li>

<li><a href="https://www.theregister.com/2023/11/11/google_proxy_plan_cma/">Google dragged to UK watchdog over Chrome's upcoming IP address cloaking</a></li>

<li><a href="https://www.theregister.com/2023/11/04/amazon_meta_cma/">Brits make Amazon, Meta stop using third-party data to undercut rivals</a></li>

<li><a href="https://www.theregister.com/2023/10/20/cma_cloud_monopoly_remedies/">Imagine a world without egress fees or cloud software license disparities</a></li>

<li><a href="https://www.theregister.com/2023/10/13/cma_criticizes_microsoft_as_it/">Brit watchdog slams Microsoft as it clears $69B Activision Blizzard buy</a></li>

<li><a href="https://www.theregister.com/2023/10/12/cma_vodafone_three/">Brit competition regulator will make or break Vodafone and Three union</a></li>
</ul>
<p>As for image editing and illustration software, the “threat posed” by Figma has fueled product development of Adobe’s Photoshop and Illustrator applications, including web versions, and this dynamic would be altered by the merger.</p>
<p>“This competition would be lost as a result of the transaction, harming designers and creative agencies who might have used these new tools or relied on future updates,” the CMA’s report adds.</p>
<p>The nature of the ruling is provisions., and the CMA will now consult of them and consider potential remedies “which could include blocking the deal outright”.</p>

        

<p>This is not the type of talk that Adobe execs will want to hear. The company’s top brass has talked endlessly about the benefits of a merger, and had planned to conclude the deal by the end of last month until the <a target="_blank" href="https://www.theregister.com/2023/11/20/ec_rules_adobes_20bn_buy/">European Commission notified Adobe</a> that it too had multiple concerns that must be addressed.</p>
<p>The EC has a new February deadline to conclude its own probe. And of course the Dept of Justice’s anti-trust team is also <a target="_blank" href="https://www.theregister.com/2022/11/18/adobe_figma_antitrust/">taking a keen interest</a> in the $20 billion sale, which would be the most expensive for a private company in enterprise software history.</p>
<p>Alex Haffner, competition partner at UK law firm Fladgate, told us the EC and CMA are highlighting "broadly the same concerns".</p>
<p>"The challenge will now be for the merging parties to persuade the competition regulators that they have got the analysis wrong in their provisional assessments or, more likely, to come up with a package of remedies which can satisfy their stated concerns."</p>
<p>A spokespeson at Adobe sent us a statement: "We are disappointed in the CMA’s findings and disagree with the CMA’s perspective on this transaction. Adobe and Figma will deliver significant value to customers. We are reviewing the provisional findings and will reengage with the CMA on the facts and merits of the case." ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. GDP Grew at a 5.2% Rate in the Third Quarter, Even Stronger Than Indicated (113 pts)]]></title>
            <link>https://www.cnbc.com/2023/11/29/us-gdp-grew-at-a-5point2percent-rate-in-the-third-quarter-even-stronger-than-first-indicated.html</link>
            <guid>38468617</guid>
            <pubDate>Thu, 30 Nov 2023 02:29:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2023/11/29/us-gdp-grew-at-a-5point2percent-rate-in-the-third-quarter-even-stronger-than-first-indicated.html">https://www.cnbc.com/2023/11/29/us-gdp-grew-at-a-5point2percent-rate-in-the-third-quarter-even-stronger-than-first-indicated.html</a>, See on <a href="https://news.ycombinator.com/item?id=38468617">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-6" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-6-2"><div id="Placeholder-ArticleBody-Video-107340557" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000324079" aria-labelledby="Placeholder-ArticleBody-Video-107340557"><p><img src="https://image.cnbcfm.com/api/v1/image/107340558-17012659861701265982-32234114465-1080pnbcnews.jpg?v=1701265985&amp;w=750&amp;h=422&amp;vtcrop=y" alt="U.S. GDP grew at a 5.2% rate in the third quarter, even stronger than first indicated"><span></span><span></span></p></div><div><p>The U.S. economy grew at an even stronger pace then previously indicated in the third quarter, the product of better-than-expected business investment and stronger government spending, the <a href="https://www.bea.gov/sites/default/files/2023-11/gdp3q23_2nd.pdf" target="_blank">Commerce Department reported</a> Wednesday.</p><p>Gross domestic product, a measure of all goods and services produced during the three-month period, accelerated at a 5.2% annualized pace, the department's second estimate showed. The acceleration topped the initial 4.9% reading and was better than the 5% forecast from economists polled by Dow Jones.</p><p>Primarily, the upward revision came from increases in nonresidential fixed investment, which includes structures, equipment and intellectual property. The category showed a rise of 1.3%, which still marked a sharp downward shift from previous quarters.</p><p>Government spending also helped boost the Q3 estimate, rising 5.5% for the July-through-September period.</p><p>However, consumer spending saw a downward revision, now rising just 3.6%, compared with 4% in the initial estimate.</p><p>There was some mixed news on the inflation front. The personal consumption expenditures price index, a gauge the Federal Reserve follows closely, increased 2.8% for the period, a 0.1 percentage point downward revision. However, the chain-weighted price index increased 3.6%, a 0.1 percentage point upward move.</p><p>Corporate profits accelerated 4.3% during the period, up sharply from the 0.8% gain in the second quarter.</p><p><em><strong>Don't miss these stories from CNBC PRO:</strong></em></p><ul><li><a href="https://www.cnbc.com/2023/11/21/the-sp-500-is-starting-to-form-a-cup-and-handle-pattern-how-to-watch-for-the-potential-breakout-ahead.html"><em>The S&amp;P 500 is starting to form a 'cup and handle' pattern. How to watch for the potential breakout ahead</em></a></li><li><a href="https://www.cnbc.com/2023/11/21/bank-of-america-sees-the-sp-500-rising-to-5000-next-year-anticipates-a-stock-pickers-paradise.html"><em>Bank of America sees the S&amp;P 500 rising to 5,000 next year, anticipates a 'stock picker's paradise'</em></a></li><li><a href="https://www.cnbc.com/2023/11/23/morgan-stanley-is-bullish-on-this-emerging-ai-trend-names-6-stocks.html"><em>Morgan Stanley is bullish on this emerging AI trend — and names 6 stocks to play it</em></a></li><li><a href="https://www.cnbc.com/2023/11/26/warren-buffett-berkshire-stock-portfolio-market-outlook.html"><em>These are Wall Street's favorite Warren Buffett stocks</em></a></li></ul></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Henry Kissinger Has Died (551 pts)]]></title>
            <link>https://www.bbc.com/news/world-us-canada-67574495</link>
            <guid>38468326</guid>
            <pubDate>Thu, 30 Nov 2023 01:57:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/world-us-canada-67574495">https://www.bbc.com/news/world-us-canada-67574495</a>, See on <a href="https://news.ycombinator.com/item?id=38468326">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="main-content" data-testid="main-content"><article><header></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/A8F0/production/_131884234_gettyimages-1164968135.jpg.webp 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/A8F0/production/_131884234_gettyimages-1164968135.jpg.webp 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/A8F0/production/_131884234_gettyimages-1164968135.jpg.webp 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/A8F0/production/_131884234_gettyimages-1164968135.jpg.webp 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/A8F0/production/_131884234_gettyimages-1164968135.jpg.webp 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/A8F0/production/_131884234_gettyimages-1164968135.jpg.webp 976w" type="image/webp"><img alt="Henry Kissinger" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/A8F0/production/_131884234_gettyimages-1164968135.jpg 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/A8F0/production/_131884234_gettyimages-1164968135.jpg 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/A8F0/production/_131884234_gettyimages-1164968135.jpg 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/A8F0/production/_131884234_gettyimages-1164968135.jpg 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/A8F0/production/_131884234_gettyimages-1164968135.jpg 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/A8F0/production/_131884234_gettyimages-1164968135.jpg 976w" src="https://ichef.bbci.co.uk/news/976/cpsprodpb/A8F0/production/_131884234_gettyimages-1164968135.jpg" width="976" height="549" loading="eager"></picture></span><span role="text"><span>Image source, </span>Getty Images</span></p><figcaption><span>Image caption, </span><p>Henry Kissinger at the State Department's 230th anniversary celebrations in 2019.</p></figcaption></figure></div><div data-component="text-block"><p><b>Former US Secretary of State Henry Kissinger has died at the age of 100.</b></p></div><div data-component="text-block"><p>He served as America's top diplomat and national security adviser during the Nixon and Ford administrations.</p></div><div data-component="text-block"><p>In a statement, Kissinger Associates, a political consulting firm he founded, said the German-born former diplomat died at his home in Connecticut. </p></div><div data-component="text-block"><p>During his decades-long career, Mr Kissinger played a pivotal, and sometimes controversial, role in US foreign policy. </p></div><div data-component="text-block"><p>The statement from Kissinger Associates did not give a cause of death.  </p></div><div data-component="text-block"><p>Born in Germany in 1923, Kissinger first came to the US in 1938 when his family fled Nazi Germany. </p></div><div data-component="text-block"><p>He became a US citizen in 1943 and went on to serve three years in the US Army and later in the Counter Intelligence Corps. </p></div><div data-component="text-block"><p>After earning bachelor's, master's and PhD degrees, he taught international relations at Harvard. </p></div><div data-component="text-block"><p>In 1969, then-President Richard Nixon appointed him National Security Adviser, a position which gave him enormous influence over US foreign policy. </p></div><div data-component="text-block"><p>As secretary of state during the Nixon administration - and later under President Gerald Ford - Mr Kissinger led diplomatic efforts towards China, helped negotiate an end to the 1973 Yom Kippur War between Israel and its neighbours and was instrumental in the Paris Peace Accords that ended the Vietnam War. </p></div><div data-component="text-block"><p>Over the years, however, Kissinger was also subject to scathing criticism from those who accused him of putting rivalry with the Soviet Union over human rights and supporting repressive regimes across the world, including Augusto Pinochet's regime in Chile. </p></div><div data-component="text-block"><p>In 1973, he was awarded a Nobel Peace Prize alongside North Vietnam's Le Duc Tho, who refused to accept. </p></div><div data-component="text-block"><p>The controversial award led to two members of the Nobel committee resigning. </p></div><div data-component="text-block"><p>While Kissinger left government service in 1977, he continued to be a prolific commentator on public affairs. His counsel was often sought by US presidents and lawmakers. </p></div><div data-component="text-block"><p>He also served on the boards of various companies and was a fixture of foreign policy and security forums, as well as penning 21 books. </p></div><div data-component="text-block"><p>Kissinger turned 100 years old in May and continued to be active even late in life, including a surprise visit to Beijing to meet Chinese President Xi Jinping in July. </p></div><div data-component="text-block"><p>He is survived by his wife of nearly 50 years, as well as by two children from a previous marriage and five grandchildren.</p></div><div data-component="text-block"><p>Winston Lord, a former US ambassador to China and Kissinger's former special assistant at the White House National Security Council, said in a statement that "the world has lost a tireless advocate for peace". </p></div><div data-component="text-block"><p>"America has lost a towering champion for national interest," Mr Lord was quoted as saying by Reuters. </p></div><div data-component="text-block"><p>"During more than seven decades, he transformed America's role in the world, held the nation together during a constitutional crisis, crafted visionary volumes, counseled world leaders, and enriched the national and international discourse," Mr Lord added. </p></div></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sam Altman returns as CEO, OpenAI has a new initial board (670 pts)]]></title>
            <link>https://openai.com/blog/sam-altman-returns-as-ceo-openai-has-a-new-initial-board</link>
            <guid>38467850</guid>
            <pubDate>Thu, 30 Nov 2023 01:08:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/blog/sam-altman-returns-as-ceo-openai-has-a-new-initial-board">https://openai.com/blog/sam-altman-returns-as-ceo-openai-has-a-new-initial-board</a>, See on <a href="https://news.ycombinator.com/item?id=38467850">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I am returning to OpenAI as CEO. Mira will return to her role as CTO. The new initial board will consist of Bret Taylor (Chair), Larry Summers, and Adam D’Angelo.</p><p>I have never been more excited about the future. I am extremely grateful for everyone’s hard work in an unclear and unprecedented situation, and I believe our resilience and spirit set us apart in the industry. I feel so, so good about our probability of success for achieving our mission.</p><p>Before getting to what comes next, I’d like to share some thanks.</p><p>I love and respect Ilya, I think he's a guiding light of the field and a gem of a human being. I harbor zero ill will towards him. While Ilya will no longer serve on the board, we hope to continue our working relationship and are discussing how he can continue his work at OpenAI.</p><p>I am grateful to Adam, Tasha, and Helen for working with us to come to this solution that best serves the mission. I’m excited to continue to work with Adam and am sincerely thankful to Helen and Tasha for investing a huge amount of effort in this process.</p><p>Thank you also to Emmett who had a key and constructive role in helping us reach this outcome. Emmett’s dedication to AI safety and balancing stakeholders’ interests was clear.</p><p>Mira did an amazing job throughout all of this, serving the mission, the team, and the company selflessly throughout. She is an incredible leader and OpenAI would not be OpenAI without her. Thank you.</p><p>Greg and I are partners in running this company. We have never quite figured out how to communicate that on the org chart, but we will. In the meantime, I just wanted to make it clear. Thank you for everything you have done since the very beginning, and for how you handled things from the moment this started and over the last week.</p><p>The leadership team–Mira, Brad, Jason, Che, Hannah, Diane, Anna, Bob, Srinivas, Matt, Lilian, Miles, Jan, Wojciech, John, Jonathan, Pat, and many more–is clearly ready to run the company without me. They say one way to evaluate a CEO is how you pick and train your potential successors; on that metric I am doing far better than I realized. It’s clear to me that the company is in great hands, and I hope this is abundantly clear to everyone. Thank you all.</p><p>Jakub, Szymon, and Aleksander are exceptional talents and I’m so happy they have rejoined to move us and our research forward. Thank you.</p><p>To all of you, our team: I am sure books are going to be written about this time period, and I hope the first thing they say is how amazing the entire team has been. Now that we’re through all of this, we didn’t lose a single employee. You stood firm for each other, this company, and our mission. One of the most important things for the team that builds AGI safely is the ability to handle stressful and uncertain situations, and maintain good judgment throughout. Top marks. Thank you all.</p><p>Satya, Kevin, Amy, and Brad have been incredible partners throughout this, with exactly the right priorities all the way through. They’ve had our backs and were ready to welcome all of us if we couldn’t achieve our primary goal. We clearly made the right choice to partner with Microsoft and I’m excited that our new board will include them as a non-voting observer. Thank you.</p><p>To our partners and users, thank you for sticking with us. We really felt the outpouring of support and love, and it helped all of us get through this. The fact that we did not lose a single customer will drive us to work even harder for you, and we are all excited to get back to work.</p><p>Will Hurd, Brian Chesky, Bret Taylor and Larry Summers put their lives on hold and did an incredible amount to support the mission. I don’t know how they did it so well, but they really did. Thank you.</p><p>Ollie also put his life on hold this entire time to just do everything he could to help out, in addition to providing his usual unconditional love and support. Thank you and I love you.</p><p><strong>So what’s next?</strong></p><p>We have three immediate priorities.</p><p>Advancing our research plan and further investing in our full-stack safety efforts, which have always been critical to our work. Our research roadmap is clear; this was a wonderfully focusing time. I share the excitement you all feel; we will turn this crisis into an opportunity! I’ll work with Mira on this.</p><p>Continuing to improve and deploy our products and serve our customers. It’s important that people get to experience the benefits and promise of AI, and have the opportunity to shape it. We continue to believe that great products are the best way to do this. I’ll work with Brad, Jason and Anna to ensure our unwavering commitment to users, customers, partners and governments around the world is clear.</p><p>Bret, Larry, and Adam will be working very hard on the extremely important task of building out a board of diverse perspectives, improving our governance structure and overseeing an independent review of recent events. I look forward to working closely with them on these crucial steps so everyone can be confident in the stability of OpenAI.&nbsp;</p><p>I am so looking forward to finishing the job of building beneficial AGI with you all—best team in the world, best mission in the world.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cybertruck Launch (134 pts)]]></title>
            <link>https://www.tesla.com/cybertruck</link>
            <guid>38465944</guid>
            <pubDate>Wed, 29 Nov 2023 22:03:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tesla.com/cybertruck">https://www.tesla.com/cybertruck</a>, See on <a href="https://news.ycombinator.com/item?id=38465944">Hacker News</a></p>
Couldn't get https://www.tesla.com/cybertruck: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Loongson releases next-generation CPU (104 pts)]]></title>
            <link>https://www.ecns.cn/news/sci-tech/2023-11-29/detail-ihcvixpi0428703.shtml</link>
            <guid>38465895</guid>
            <pubDate>Wed, 29 Nov 2023 21:58:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ecns.cn/news/sci-tech/2023-11-29/detail-ihcvixpi0428703.shtml">https://www.ecns.cn/news/sci-tech/2023-11-29/detail-ihcvixpi0428703.shtml</a>, See on <a href="https://news.ycombinator.com/item?id=38465895">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="yanse">
                                            <!--新视频播放器1显示start-->
                         
                                                <!--新视频播放器1显示end-->
                                                                                                                            <!--正文分页开始-->
                                        <p>Chinese company Loongson released its next-generation computer central processing unit (CPU) 3A6000 in Beijing on Tuesday. The CPU is self-designed with key indexes comparable to those of global mainstream products, which observers said marked China's new breakthrough in the design of domestic CPUs.</p>
<p>The Loongson 3A6000 processor uses a fourth-generation LoongArch, an instruction set architecture (ISA) processor. It is manufactured using 12-nanometer technology and features a quad-core design, with an operating frequency of 2.5GHz. It adopts a domestic instruction system and structure, and it belongs to a new generation of general-purpose processors that are self-developed and technologically controllable.</p>
<p>The Loongson 3A6000 is comparable to the Intel Core i3-10100 and AMD Ryzen 3 3100 CPUs in certain key functions, according to tech website Wccf tech.</p>
<p>"The release shows that domestic CPUs have reached a new height in self-sufficiency and performance. It also demonstrates that China is capable of producing first-class products based on a proprietary CPU structure," Shi Huikang, an official of the Ministry of Industry and Information Technology, told media outlets.</p>
<p>Multiple Chinese computer brands, including Lenovo, Tongfang, Ipason, Founder and Hair, have launched new computer series based on the Loongson 3A6000 processor.</p>
<p>Loongson announced on Tuesday broader cooperation with 10 companies by authorizing the licensing of its Loongson CPU core intellectual property and LoongArch, which aims to build a LoongArch ecosystem with its partners, according to media reports.</p>

<p>Amid tech decoupling sought by certain Western countries, there are growing demands for China's computer industry to seek new solutions. After years of development, Loongson's CPUs can fit the industry's needs of supply chain security and performance, Ma Jihua, a telecom analyst at Beijing Daojing Consultant Co, told the Global Times on Tuesday.</p>

<p>Loongson's development in the chip industry depends on two major factors - whether the company can have access to technologies and manufacturing equipment, and whether it can find enough partners in its ecosystem.</p>
<p>If the computer industry buys enough of Loongson's products, it can cut the cost of the chip and generate demand for future upgrades, Ma said.</p>
<p>China has made tremendous efforts to achieving tech self-sufficiency in the chip sector amid Western blockades. The release of the CPU3A6000 shows the speed at which Chinese companies have been catching up with foreign peers.</p>
<p>The release of the Huawei Mate 60 series on August 30 showed that the high-end smartphone series adopted Kirin 9000S chips, a major breakthrough of the company's development in smartphones.</p>
<p>On November 15, Chinese GPU chip company Moore Threads completed a new round of financing, reportedly in the range of 2-2.5 billion yuan.</p>
<p>The company has released several products, including consumer graphics cards MTT S80 and MTT S70, which are mainly used in the entertainment and creative fields.</p>                                                                                    <!--正文分页结束-->
                                            <p>
                                                <a href="https://www.facebook.com/echinanews" target="_blank"><img src="https://www.ecns.cn/part/2015/07/2015-07-23/U435P886T30D169F196DT20150723092203.jpg"></a>
                                                <a href="https://twitter.com/Echinanews" target="_blank"><img src="https://www.ecns.cn/part/2015/07/2015-07-23/U435P886T30D169F203DT20150723092203.jpg"></a>
                                            </p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon Unveils Graviton4: A 96-Core ARM CPU with 536.7 GBps Memory Bandwidth (157 pts)]]></title>
            <link>https://www.anandtech.com/show/21172/amazon-unveils-graviton4-a-96core-cpu-with-5367-gbs-memory-bandwidth</link>
            <guid>38465736</guid>
            <pubDate>Wed, 29 Nov 2023 21:44:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anandtech.com/show/21172/amazon-unveils-graviton4-a-96core-cpu-with-5367-gbs-memory-bandwidth">https://www.anandtech.com/show/21172/amazon-unveils-graviton4-a-96core-cpu-with-5367-gbs-memory-bandwidth</a>, See on <a href="https://news.ycombinator.com/item?id=38465736">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>

                

<div>
    <nav>
        <ul>
            <li><a href="https://www.anandtech.com/">Home</a><span>&gt;</span></li>
                <li><a href="https://www.anandtech.com/tag/cpus">CPUs</a></li>
        </ul>
        
    </nav>

    
    
</div>






<div>

                <p><img src="https://images.anandtech.com/doci/21172/amazon-aws-trainium-graviton-678_678x452.jpg" alt="">
                </p>

            


        <div>
            <p>Nowadays many cloud service providers design their own silicon, but Amazon Web Services (AWS) started to do this ahead of its rivals and by now its Annapurna Labs develops processors that can well compete with those from AMD and Intel. This week AWS introduced its Graviton4, a 96-core ARM-based system-on-chip (SoC) that promises to challenge renowned CPU designers and offer unprecedented performance to AWS clients.</p>

<p>"By focusing our chip designs on real workloads that matter to customers, we are able to deliver the most advanced cloud infrastructure to them," said David Brown, vice president of Compute and Networking at AWS. "Graviton4 marks the fourth generation we have delivered in just five years, and is the most powerful and energy efficient chip we have ever built for a broad range of workloads."</p>

<p>The AWS Graviton4 processor packs 96 cores that offer on average 30% higher compute performance compared to Graviton3 and is 40% faster in database applications as well as 45% faster in Java applications, according to Amazon. Given that Amazon did not reveal many details about its Graviton4, it is hard to attribute performance increases to any particular characteristics of the CPU.</p>

<p>Yet, <a href="https://www.nextplatform.com/2023/11/28/aws-adopts-arm-v2-cores-for-expansive-graviton4-server-cpu/"><em>NextPlatform</em> believes that the processor uses Arm Neoverse V2 cores</a>, which are more capable than V1 cores used in previous-generation AWS processors when it comes to instruction per clock (IPC). Furthermore, the new CPU is expected to be fabricated using one of TSMC's N4 process technologies (4nm-class), which offers a higher clock-speed potential than TSMC's N5 nodes.</p>

<p>"AWS Graviton4 instances are the fastest EC2 instances we have ever tested, and they are delivering outstanding performance across our most competitive and latency sensitive workloads," said Roman Visintine, lead cloud engineer at Epic. "We look forward to using Graviton4 to improve player experience and expand what is possible within Fortnite.”</p>

<p>In addition, the new processor features a revamped memory subsystem with a 536.7 GB/s peak bandwidth, which is 75% higher compared to the previous-generation AWS CPU. Higher memory bandwidth improves performance of CPUs in memory intensive applications, such as databases.</p>

<p>Meanwhile, such a major memory bandwidth improvement indicates that the new processor employs a memory subsystem with a higher number of channels compared to Graviton3, though AWS has not formally confirmed this.</p>

<p>Graviton4 will be featured in memory-optimized Amazon EC2 R8g instances, which is particularly useful to boost performance in high-end databases and analytics. Furthermore, these R8g instances provide up to three times more vCPUs and memory than Graviton 3-based R7g instances, enabling higher throughput for data processing, better scalability, faster results, and reduced costs. To ensure security of AWS EC2 instances, Amazon equipped all high-speed physical hardware interfaces of Graviton4 CPUs.</p>

<p>It should be noted that Amazon offers up to 64 vCPU cores with its R7g instances (i.e., one full Graviton3 CPU). Therefore, to offer 192 vCPU cores, Amazon will need to either install two 96-core CPUs into one server (which marks one of the world's first Armv9-based 2-way server design), enable three-way SMT on a 96-core CPU, or implement a very low-latency high-bandwidth interconnection between servers running 96-core cPUs.</p>

<p>Graviton4 R8g is currently in preview, these instances will be available widely in the coming months.</p>

<p>Sources: <a href="https://press.aboutamazon.com/2023/11/aws-unveils-next-generation-aws-designed-chips">AWS</a>, <a href="https://www.nextplatform.com/2023/11/28/aws-adopts-arm-v2-cores-for-expansive-graviton4-server-cpu/">NextPlatform</a></p>

        </div>
        
            
            
            
            
            


</div>

    



            </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Unix-Haters Handbook (1994) [pdf] (137 pts)]]></title>
            <link>https://web.mit.edu/~simsong/www/ugh.pdf</link>
            <guid>38464715</guid>
            <pubDate>Wed, 29 Nov 2023 20:20:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://web.mit.edu/~simsong/www/ugh.pdf">https://web.mit.edu/~simsong/www/ugh.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=38464715">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Llamafile lets you distribute and run LLMs with a single file (895 pts)]]></title>
            <link>https://github.com/Mozilla-Ocho/llamafile</link>
            <guid>38464057</guid>
            <pubDate>Wed, 29 Nov 2023 19:29:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Mozilla-Ocho/llamafile">https://github.com/Mozilla-Ocho/llamafile</a>, See on <a href="https://news.ycombinator.com/item?id=38464057">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">llamafile</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Mozilla-Ocho/llamafile/blob/main/llamafile/llamafile.png"><img src="https://github.com/Mozilla-Ocho/llamafile/raw/main/llamafile/llamafile.png" alt="llamafile-250"></a></p>
<p dir="auto"><strong>llamafile lets you distribute and run LLMs with a single file (<a href="https://hacks.mozilla.org/2023/11/introducing-llamafile/" rel="nofollow">blog post</a>)</strong></p>
<p dir="auto">Our goal is to make the "build once anywhere, run anywhere" dream come
true for AI developers. We're doing that by combining <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>
with <a href="https://github.com/jart/cosmopolitan">Cosmopolitan Libc</a> into one
framework that lets you build apps for LLMs as a single-file artifact
that runs locally on most PCs and servers.</p>
<p dir="auto">First, your llamafiles can run on multiple CPU microarchitectures. We
added runtime dispatching to llama.cpp that lets new Intel systems use
modern CPU features without trading away support for older computers.</p>
<p dir="auto">Secondly, your llamafiles can run on multiple CPU architectures. We do
that by concatenating AMD64 and ARM64 builds with a shell script that
launches the appropriate one. Our file format is compatible with WIN32
and most UNIX shells. It's also able to be easily converted (by either
you or your users) to the platform-native format, whenever required.</p>
<p dir="auto">Thirdly, your llamafiles can run on six OSes (macOS, Windows, Linux,
FreeBSD, OpenBSD, and NetBSD). You'll only need to build your code once,
using a Linux-style toolchain. The GCC-based compiler we provide is
itself an Actually Portable Executable, so you can build your software
for all six OSes from the comfort of whichever one you prefer most for
development.</p>
<p dir="auto">Lastly, the weights for your LLM can be embedded within your llamafile.
We added support for PKZIP to the GGML library. This lets uncompressed
weights be mapped directly into memory, similar to a self-extracting
archive. It enables quantized weights distributed online to be prefixed
with a compatible version of the llama.cpp software, thereby ensuring
its originally observed behaviors can be reproduced indefinitely.</p>
<h2 tabindex="-1" dir="auto">Binary Instructions</h2>
<p dir="auto">We provide example binaries that embed several different models. You can
download these from Hugging Face via the links below. "Command-line
binaries" run from the command line, just as if you were invoking
llama.cpp's "main" function manually. "Server binaries" launch a local
web server (at 127.0.0.1:8080) that provides a web-based chatbot.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Command-line binary</th>
<th>Server binary</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mistral-7B-Instruct</td>
<td><a href="https://huggingface.co/jartine/mistral-7b.llamafile/resolve/main/mistral-7b-instruct-v0.1-Q4_K_M-main.llamafile?download=true" rel="nofollow">mistral-7b-instruct-v0.1-Q4_K_M-main.llamafile (4.4 GB)</a></td>
<td><a href="https://huggingface.co/jartine/mistral-7b.llamafile/resolve/main/mistral-7b-instruct-v0.1-Q4_K_M-server.llamafile?download=true" rel="nofollow">mistral-7b-instruct-v0.1-Q4_K_M-server.llamafile (4.4 GB)</a></td>
</tr>
<tr>
<td>LLaVA 1.5</td>
<td>(Not provided because this model's features are best utilized via the web UI)</td>
<td><strong><a href="https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-q4-server.llamafile?download=true" rel="nofollow">llava-v1.5-7b-q4-server.llamafile (4.3 GB)</a></strong></td>
</tr>
<tr>
<td>WizardCoder-Python-13B</td>
<td><a href="https://huggingface.co/jartine/wizardcoder-13b-python/resolve/main/wizardcoder-python-13b-main.llamafile?download=true" rel="nofollow">wizardcoder-python-13b-main.llamafile (7.9 GB)</a></td>
<td><a href="https://huggingface.co/jartine/wizardcoder-13b-python/resolve/main/wizardcoder-python-13b-server.llamafile?download=true" rel="nofollow">wizardcoder-python-13b-server.llamafile (7.9GB)</a></td>
</tr>
</tbody>
</table>
<p dir="auto">You can also also download <em>just</em> the llamafile software (without any
weights included) from our releases page, or directly in your terminal
or command prompt. This is mandatory currently on Windows.</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -L https://github.com/Mozilla-Ocho/llamafile/releases/download/0.1/llamafile-server-0.1 >llamafile
chmod +x llamafile
./llamafile --help
./llamafile -m ~/weights/foo.gguf"><pre>curl -L https://github.com/Mozilla-Ocho/llamafile/releases/download/0.1/llamafile-server-0.1 <span>&gt;</span>llamafile
chmod +x llamafile
./llamafile --help
./llamafile -m <span>~</span>/weights/foo.gguf</pre></div>
<h3 tabindex="-1" dir="auto">Gotchas</h3>
<p dir="auto">On macOS with Apple Silicon you need to have Xcode installed for
llamafile to be able to bootstrap itself.</p>
<p dir="auto">On Windows, you may need to rename <code>llamafile</code> to <code>llamafile.exe</code> in
order for it to run. Windows also has a maximum file size limit of 4GB
for executables. The LLaVA server executable above is just 30MB shy of
that limit, so it'll work on Windows, but with larger models like
WizardCoder 13B, you need to store the weights in a separate file.</p>
<p dir="auto">If you use zsh and have trouble running llamafile, try saying <code>sh -c ./llamafile</code>. This is due to a bug that was fixed in zsh 5.9+. The same
is the case for Python <code>subprocess</code>, old versions of Fish, etc.</p>
<p dir="auto">On Linux <code>binfmt_misc</code> has been known to cause problems. You can fix
that by installing the actually portable executable interpreter.</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo wget -O /usr/bin/ape https://cosmo.zip/pub/cosmos/bin/ape-$(uname -m).elf
sudo sh -c &quot;echo ':APE:M::MZqFpD::/usr/bin/ape:' >/proc/sys/fs/binfmt_misc/register&quot;
sudo sh -c &quot;echo ':APE-jart:M::jartsr::/usr/bin/ape:' >/proc/sys/fs/binfmt_misc/register&quot;"><pre>sudo wget -O /usr/bin/ape https://cosmo.zip/pub/cosmos/bin/ape-<span><span>$(</span>uname -m<span>)</span></span>.elf
sudo sh -c <span><span>"</span>echo ':APE:M::MZqFpD::/usr/bin/ape:' &gt;/proc/sys/fs/binfmt_misc/register<span>"</span></span>
sudo sh -c <span><span>"</span>echo ':APE-jart:M::jartsr::/usr/bin/ape:' &gt;/proc/sys/fs/binfmt_misc/register<span>"</span></span></pre></div>
<h3 tabindex="-1" dir="auto">GPU Support</h3>
<p dir="auto">On Apple Silicon, everything should just work if Xcode is installed.</p>
<p dir="auto">On Linux, Nvidia cuBLAS GPU support will be compiled on the fly if (1)
you have the <code>cc</code> compiler installed, (2) you pass the <code>--n-gpu-layers 35</code> flag (or whatever value is appropriate) to enable GPU, and (3) the
CUDA developer toolkit is installed on your machine and the <code>nvcc</code>
compiler is on your path.</p>
<p dir="auto">On Windows, that usually means you need to open up the MSVC x64 native
command prompt and run llamafile there, for the first invocation, so it
can build a DLL with native GPU support. After that, <code>$CUDA_PATH/bin</code>
still usually needs to be on the <code>$PATH</code> so the GGML DLL can find its
other CUDA dependencies.</p>
<p dir="auto">In the event that GPU support couldn't be compiled and dynamically
linked on the fly for any reason, llamafile will fall back to CPU
inference.</p>
<h2 tabindex="-1" dir="auto">Source Instructions</h2>
<p dir="auto">Here's how to build llamafile from source. First, you need the cosmocc
toolchain, which is a fat portable binary version of GCC. Here's how you
can download the latest release and add it to your path.</p>
<div dir="auto" data-snippet-clipboard-copy-content="mkdir -p cosmocc
cd cosmocc
curl -L https://github.com/jart/cosmopolitan/releases/download/3.1.1/cosmocc-3.1.1.zip >cosmocc.zip
unzip cosmocc.zip
cd ..
export PATH=&quot;$PWD/cosmocc/bin:$PATH&quot;"><pre>mkdir -p cosmocc
<span>cd</span> cosmocc
curl -L https://github.com/jart/cosmopolitan/releases/download/3.1.1/cosmocc-3.1.1.zip <span>&gt;</span>cosmocc.zip
unzip cosmocc.zip
<span>cd</span> ..
<span>export</span> PATH=<span><span>"</span><span>$PWD</span>/cosmocc/bin:<span>$PATH</span><span>"</span></span></pre></div>
<p dir="auto">You can now build the llamafile repository by running make:</p>

<p dir="auto">Here's an example of how to generate code for a libc function using the
llama.cpp command line interface, utilizing <a href="https://huggingface.co/TheBloke/WizardCoder-Python-13B-V1.0-GGUF/tree/main" rel="nofollow">WizardCoder-Python-13B</a>
(license: <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" rel="nofollow">LLaMA 2</a>)
weights.</p>
<div dir="auto" data-snippet-clipboard-copy-content="make -j8 o//llama.cpp/main/main
o//llama.cpp/main/main \
  -m ~/weights/wizardcoder-python-13b-v1.0.Q8_0.gguf \
  --temp 0 \
  -r $'```\n' \
  -p $'```c\nvoid *memcpy_sse2(char *dst, const char *src, size_t size) {\n'"><pre>make -j8 o//llama.cpp/main/main
o//llama.cpp/main/main \
  -m <span>~</span>/weights/wizardcoder-python-13b-v1.0.Q8_0.gguf \
  --temp 0 \
  -r <span><span>$'</span>```<span>\n</span><span>'</span></span> \
  -p <span><span>$'</span>```c<span>\n</span>void *memcpy_sse2(char *dst, const char *src, size_t size) {<span>\n</span><span>'</span></span></pre></div>
<p dir="auto">Here's a similar example that instead utilizes <a href="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/tree/main" rel="nofollow">Mistral-7B-Instruct</a>
(license: <a href="https://choosealicense.com/licenses/apache-2.0/" rel="nofollow">Apache 2.0</a>) weights.</p>
<div dir="auto" data-snippet-clipboard-copy-content="make -j8 o//llama.cpp/main/main
o//llama.cpp/main/main \
  -m ~/weights/mistral-7b-instruct-v0.1.Q4_K_M.gguf \
  --temp 0.7 \
  -r $'\n' \
  -p $'### Instruction: Write a story about llamas\n### Response:\n'"><pre>make -j8 o//llama.cpp/main/main
o//llama.cpp/main/main \
  -m <span>~</span>/weights/mistral-7b-instruct-v0.1.Q4_K_M.gguf \
  --temp 0.7 \
  -r <span><span>$'</span><span>\n</span><span>'</span></span> \
  -p <span><span>$'</span>### Instruction: Write a story about llamas<span>\n</span>### Response:<span>\n</span><span>'</span></span></pre></div>
<p dir="auto">Here's an example of how to run llama.cpp's built-in HTTP server in such
a way that the weights are embedded inside the executable. This example
uses <a href="https://huggingface.co/jartine/llava-v1.5-7B-GGUF/tree/main" rel="nofollow">LLaVA v1.5-7B</a> (license: <a href="https://github.com/facebookresearch/llama/blob/main/LICENSE">LLaMA</a>,
<a href="https://openai.com/policies/terms-of-use" rel="nofollow">OpenAI</a>),
a multimodal LLM that works with llama.cpp's recently-added support for
image inputs.</p>
<div dir="auto" data-snippet-clipboard-copy-content="make -j8

o//llamafile/zipalign -j0 \
  o//llama.cpp/server/server \
  ~/weights/llava-v1.5-7b-Q8_0.gguf \
  ~/weights/llava-v1.5-7b-mmproj-Q8_0.gguf

o//llama.cpp/server/server \
  -m llava-v1.5-7b-Q8_0.gguf \
  --mmproj llava-v1.5-7b-mmproj-Q8_0.gguf \
  --host 0.0.0.0"><pre>make -j8

o//llamafile/zipalign -j0 \
  o//llama.cpp/server/server \
  <span>~</span>/weights/llava-v1.5-7b-Q8_0.gguf \
  <span>~</span>/weights/llava-v1.5-7b-mmproj-Q8_0.gguf

o//llama.cpp/server/server \
  -m llava-v1.5-7b-Q8_0.gguf \
  --mmproj llava-v1.5-7b-mmproj-Q8_0.gguf \
  --host 0.0.0.0</pre></div>
<p dir="auto">The above command will launch a browser tab on your personal computer to
display a web interface. It lets you chat with your LLM and upload
images to it.</p>
<p dir="auto">If you want to be able to just say:</p>

<p dir="auto">...and have it run the web server without having to specify arguments (for
the paths you already know are in there), then you can add a special
<code>.args</code> to the zip archive, which specifies the default arguments. In
this case, we're going to try our luck with the normal <code>zip</code> command,
which requires we temporarily rename the file. First, let's create the
arguments file:</p>
<div data-snippet-clipboard-copy-content="cat <<EOF >.args
-m
llava-v1.5-7b-Q8_0.gguf
--mmproj
llava-v1.5-7b-mmproj-Q8_0.gguf
--host
0.0.0.0
...
EOF"><pre><code>cat &lt;&lt;EOF &gt;.args
-m
llava-v1.5-7b-Q8_0.gguf
--mmproj
llava-v1.5-7b-mmproj-Q8_0.gguf
--host
0.0.0.0
...
EOF
</code></pre></div>
<p dir="auto">As we can see above, there's one argument per line. The <code>...</code> argument
optionally specifies where any additional CLI arguments passed by the
user are to be inserted. Next, we'll add the argument file to the
executable:</p>
<div data-snippet-clipboard-copy-content="mv o//llama.cpp/server/server server.com
zip server.com .args
mv server.com server
./server"><pre><code>mv o//llama.cpp/server/server server.com
zip server.com .args
mv server.com server
./server
</code></pre></div>
<p dir="auto">Congratulations. You've just made your own LLM executable that's easy to
share with your friends.</p>
<p dir="auto">(Note that the examples provided above are not endorsements or
recommendations of specific models, licenses, or data sets on the part
of Mozilla.)</p>
<h2 tabindex="-1" dir="auto">zipalign documentation</h2>
<div data-snippet-clipboard-copy-content="SYNOPSIS

  o//llamafile/zipalign ZIP FILE...

DESCRIPTION

  Adds aligned uncompressed files to PKZIP archive

  This tool is designed to concatenate gigabytes of LLM weights to an
  executable. This command goes 10x faster than `zip -j0`. Unlike zip
  you are not required to use the .com file extension for it to work.
  But most importantly, this tool has a flag that lets you insert zip
  files that are aligned on a specific boundary. The result is things
  like GPUs that have specific memory alignment requirements will now
  be able to perform math directly on the zip file's mmap()'d weights

FLAGS

  -h        help
  -N        nondeterministic mode
  -a INT    alignment (default 65536)
  -j        strip directory components
  -0        store uncompressed (currently default)"><pre><code>SYNOPSIS

  o//llamafile/zipalign ZIP FILE...

DESCRIPTION

  Adds aligned uncompressed files to PKZIP archive

  This tool is designed to concatenate gigabytes of LLM weights to an
  executable. This command goes 10x faster than `zip -j0`. Unlike zip
  you are not required to use the .com file extension for it to work.
  But most importantly, this tool has a flag that lets you insert zip
  files that are aligned on a specific boundary. The result is things
  like GPUs that have specific memory alignment requirements will now
  be able to perform math directly on the zip file's mmap()'d weights

FLAGS

  -h        help
  -N        nondeterministic mode
  -a INT    alignment (default 65536)
  -j        strip directory components
  -0        store uncompressed (currently default)
</code></pre></div>
<h2 tabindex="-1" dir="auto">Technical Details</h2>
<p dir="auto">Here is a succinct overview of the tricks we used to create the fattest
executable format ever. The long story short is llamafile is a shell
script that launches itself and runs inference on embedded weights in
milliseconds without needing to be copied or installed. What makes that
possible is mmap(). Both the llama.cpp executable and the weights are
concatenated onto the shell script. A tiny loader program is then
extracted by the shell script, which maps the executable into memory.
The llama.cpp executable then opens the shell script again as a file,
and calls mmap() again to pull the weights into memory and make them
directly accessible to both the CPU and GPU.</p>
<h3 tabindex="-1" dir="auto">ZIP Weights Embedding</h3>
<p dir="auto">The trick to embedding weights inside llama.cpp executables is to ensure
the local file is aligned on a page size boundary. That way, assuming
the zip file is uncompressed, once it's mmap()'d into memory we can pass
pointers directly to GPUs like Apple Metal, which require that data be
page size aligned. Since no existing ZIP archiving tool has an alignment
flag, we had to write about <a href="https://github.com/Mozilla-Ocho/llamafile/blob/main/llamafile/zipalign.c">400 lines of code</a> to
insert the ZIP files ourselves. However, once there, every existing ZIP
program should be able to read them, provided they support ZIP64. This
makes the weights much more easily accessible than they otherwise would
have been, had we invented our own file format for concatenated files.</p>
<h3 tabindex="-1" dir="auto">Microarchitectural Portability</h3>
<p dir="auto">On Intel and AMD microprocessors, llama.cpp spends most of its time in
the matmul quants, which are usually written thrice for SSSE3, AVX, and
AVX2. llamafile pulls each of these functions out into a separate file
that can be <code>#include</code>ed multiple times, with varying
<code>__attribute__((__target__("arch")))</code> function attributes. Then, a
wrapper function is added which uses Cosmopolitan's <code>X86_HAVE(FOO)</code>
feature to runtime dispatch to the appropriate implementation.</p>
<h3 tabindex="-1" dir="auto">Architecture Portability</h3>
<p dir="auto">llamafile solves architecture portability by building llama.cpp twice:
once for AMD64 and again for ARM64. It then wraps them with a shell
script which has an MZ prefix. On Windows, it'll run as a native binary.
On Linux, it'll extract a small 8kb executable called <a href="https://github.com/jart/cosmopolitan/blob/master/ape/loader.c">APE
Loader</a>
to <code>${TMPDIR:-${HOME:-.}}/.ape</code> that'll map the binary portions of the
shell script into memory. It's possible to avoid this process by running
the
<a href="https://github.com/jart/cosmopolitan/blob/master/tool/build/assimilate.c"><code>assimilate</code></a>
program that comes included with the <code>cosmocc</code> compiler. What the
<code>assimilate</code> program does is turn the shell script executable into
the host platform's native executable format. This guarantees a fallback
path exists for traditonal release processes when it's needed.</p>
<h3 tabindex="-1" dir="auto">GPU Support</h3>
<p dir="auto">Cosmopolitan Libc uses static linking, since that's the only way to get
the same executable to run on six OSes. This presents a challenge for
llama.cpp, because it's not possible to statically link GPU support. The
way we solve that is by checking if a compiler is installed on the host
system. For Apple, that would be Xcode, and for other platforms, that
would be <code>nvcc</code>. llama.cpp has a single file implementation of each GPU
module, named <code>ggml-metal.m</code> (Objective C) and <code>ggml-cuda.cu</code> (Nvidia
C). llamafile embeds those source files within the zip archive and asks
the platform compiler to build them at runtime, targeting the native GPU
microarchitecture. If it works, then it's linked with platform C library
dlopen() implementation. See <a href="https://github.com/Mozilla-Ocho/llamafile/blob/main/llamafile/cuda.c">llamafile/cuda.c</a> and
<a href="https://github.com/Mozilla-Ocho/llamafile/blob/main/llamafile/metal.c">llamafile/metal.c</a>.</p>
<p dir="auto">In order to use the platform-specific dlopen() function, we need to ask
the platform-specific compiler to build a small executable that exposes
these interfaces. On ELF platforms, Cosmopolitan Libc maps this helper
executable into memory along with the platform's ELF interpreter. The
platform C library then takes care of linking all the GPU libraries, and
then runs the helper program which longjmp()'s back into Cosmopolitan.
The executable program is now in a weird hybrid state where two separate
C libraries exist which have different ABIs. For example, thread local
storage works differently on each operating system, and programs will
crash if the TLS register doesn't point to the appropriate memory. The
way Cosmopolitan Libc solves that is by JITing a trampoline around each
dlsym() import, which blocks signals using <code>sigprocmask()</code> and changes
the TLS register using <code>arch_prctl()</code>. Under normal circumstances,
aspecting each function call with four additional system calls would be
prohibitively expensive, but for llama.cpp that cost is infinitesimal
compared to the amount of compute used for LLM inference. Our technique
has no noticeable slowdown. The major tradeoff is that, right now, you
can't pass callback pointers to the dlopen()'d module. Only one such
function needed to be removed from the llama.cpp codebase, which was an
API intended for customizing logging. In the future, Cosmoplitan will
just trampoline signal handlers and code morph the TLS instructions to
avoid these tradeoffs entirely. See
<a href="https://github.com/jart/cosmopolitan/blob/master/libc/dlopen/dlopen.c">cosmopolitan/dlopen.c</a>
for further details.</p>
<h2 tabindex="-1" dir="auto">Licensing</h2>
<p dir="auto">While the llamafile project is Apache 2.0-licensed, our changes
to llama.cpp are licensed under MIT (just like the llama.cpp project
itself) so as to remain compatible and upstreamable in the future,
should that be desired.</p>
<p dir="auto">The llamafile logo on this page was generated with the assistance of DALL·E 3.</p>
<h2 tabindex="-1" dir="auto">Known Issues</h2>
<ul dir="auto">
<li>The 64-bit version of Windows has a 4GB file size limit. While
llamafile will work fine on 64-bit Windows with the weights as a
separate file, you'll get an error if you load them into the
executable itself and try to run it.</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Write shaders for the (sim) Vegas sphere (342 pts)]]></title>
            <link>https://whenistheweekend.com/theSphere.html</link>
            <guid>38463832</guid>
            <pubDate>Wed, 29 Nov 2023 19:14:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://whenistheweekend.com/theSphere.html">https://whenistheweekend.com/theSphere.html</a>, See on <a href="https://news.ycombinator.com/item?id=38463832">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Who makes the most reliable new cars? (451 pts)]]></title>
            <link>https://www.consumerreports.org/cars/car-reliability-owner-satisfaction/who-makes-the-most-reliable-cars-a7824554938/</link>
            <guid>38463355</guid>
            <pubDate>Wed, 29 Nov 2023 18:42:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.consumerreports.org/cars/car-reliability-owner-satisfaction/who-makes-the-most-reliable-cars-a7824554938/">https://www.consumerreports.org/cars/car-reliability-owner-satisfaction/who-makes-the-most-reliable-cars-a7824554938/</a>, See on <a href="https://news.ycombinator.com/item?id=38463355">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
                    

<div>
                      <figure>
            <picture>
                    <source srcset="https://article.images.consumerreports.org/image/upload/w_510,f_auto,q_auto,ar_16:9,c_lfill,dpr_2.0/v1699554174/prod/content/dam/CRO-Images-2023/11November/Cars/CR-Cars-Who-Makes-the-Most-Reliable-Cars-1123" media="(max-width: 767px) and (-webkit-min-device-pixel-ratio: 2),
            (max-width: 767px) and (min-resolution: 192dpi)">
        <source srcset="https://article.images.consumerreports.org/image/upload/w_510,f_auto,q_auto,ar_16:9,c_lfill/v1699554174/prod/content/dam/CRO-Images-2023/11November/Cars/CR-Cars-Who-Makes-the-Most-Reliable-Cars-1123" media="(max-width: 767px)">
                            <source srcset="https://article.images.consumerreports.org/image/upload/w_770,f_auto,q_auto,ar_16:9,c_lfill,dpr_2.0/v1699554174/prod/content/dam/CRO-Images-2023/11November/Cars/CR-Cars-Who-Makes-the-Most-Reliable-Cars-1123" media="(max-width: 992px) and (-webkit-min-device-pixel-ratio: 2),
            (max-width: 992px) and (min-resolution: 192dpi)">
        <source srcset="https://article.images.consumerreports.org/image/upload/w_770,f_auto,q_auto,ar_16:9,c_lfill/v1699554174/prod/content/dam/CRO-Images-2023/11November/Cars/CR-Cars-Who-Makes-the-Most-Reliable-Cars-1123" media="(max-width: 992px)">
                            <source srcset="https://article.images.consumerreports.org/image/upload/w_945,f_auto,q_auto,ar_16:9,c_lfill,dpr_2.0/v1699554174/prod/content/dam/CRO-Images-2023/11November/Cars/CR-Cars-Who-Makes-the-Most-Reliable-Cars-1123" media="(min-width: 1200px) and (-webkit-min-device-pixel-ratio: 2),
            (min-width: 1200px) and (min-resolution: 192dpi)">
        <source srcset="https://article.images.consumerreports.org/image/upload/w_945,f_auto,q_auto,ar_16:9,c_lfill/v1699554174/prod/content/dam/CRO-Images-2023/11November/Cars/CR-Cars-Who-Makes-the-Most-Reliable-Cars-1123" media="(min-width: 1200px)">
            
    <img src="https://article.images.consumerreports.org/image/upload/w_945,f_auto,q_auto,ar_16:9,c_lfill/v1699554174/prod/content/dam/CRO-Images-2023/11November/Cars/CR-Cars-Who-Makes-the-Most-Reliable-Cars-1123" alt="detail of front of Subaru with green up arrow to right of it" fetchpriority="high" width="945" height="531">
</picture>
    
                <span>
            Graphic: Consumer Reports, Subaru
        </span>
    </figure>

                </div>

<div>
            <p><time datetime="2023-11-29T09:51">
                    November 29, 2023
            </time>
</p>
                        <div>
                    <p><span itemprop="author">By Jon Linkov. Data Visualizations by Andy Bergmann.</span>
                    </p>
                </div>
                        
        </div>

    



                    <div id="intro">
                <p><a href="https://www.consumerreports.org/cars/lexus/">Lexus</a>, <a href="https://www.consumerreports.org/cars/toyota/">Toyota</a>, and <a href="https://www.consumerreports.org/cars/mini/">Mini</a> are the three most reliable brands in this year’s annual auto reliability brand rankings, with the two Japanese brands swapping spaces from last year.&nbsp;The automaker <a href="https://www.consumerreports.org/cars/honda/">Honda</a> follows, with its luxury <a href="https://www.consumerreports.org/cars/acura/">Acura</a> brand coming in fourth and the mainstream Honda brand rounding out the top five. Scroll down to see a full comparison of how brands compare using our interactive tool. </p>
<p><strong>In This Article</strong>: <a href="#how-we-score-reliability">How We Score Reliability</a> • <a href="#how-the-regions-compare-for-reliability">How the Regions Compare</a> • <a href="#hybrids-soar-and-e-vs-still-struggle">Hybrids Up, EVs Down</a> </p>
            </div>
    
    
    <div id="how-we-score-reliability">
                    <p>Every year CR asks its members about problems they’ve had with their vehicles in the previous 12 months. This year we gathered data on over 330,000 vehicles, from the 2000 to 2023 model years, with a few early-introduced 2024 model years.&nbsp;</p>
<p>We study 20 trouble areas, from nuisances—such as squeaky brakes and broken interior trim—to major bummers, such as potentially expensive out-of-warranty engine, transmission, EV battery, and EV charging problems. We use that information to give reliability ratings for every major mainstream model.</p>
<p>We weigh the severity of each type of problem to create a predicted reliability score for each vehicle, from 1 to 100. We use that information to give reliability ratings for every major mainstream vehicle. (The reliability rating is then combined with data collected from our track testing, as well as our owner satisfaction survey results and safety data, to calculate each test vehicle’s Overall Score.)</p>
<p>This year we have addressed the rapidly growing number of electrified offerings that automakers are producing: hybrids, plug-in hybrids (PHEV), and electric vehicles (EV). As a result, we added three new trouble areas: electric motor, EV/hybrid battery, and EV charging:</p>
<p>• <strong>Internal Combustion Engine (ICE)</strong> vehicles have 17 potential trouble areas.<br>• <strong>EVs</strong> can have up to 12 trouble areas. Traditional ICE problems are not included, such as those with the engine and transmission.<br>• <strong>Hybrids</strong> have 19 potential trouble areas: 17 from ICE vehicles, as well as electric motor and EV battery.<br>• <strong>Plug-in electric vehicles (PHEVs)</strong> can experience all 20 trouble areas: 17 from ICE vehicles, as well as electric motor, EV battery, and EV charging.</p>
<p><strong>See the <a rel="noreferrer noopener" href="https://www.consumerreports.org/car-reliability-owner-satisfaction/10-most-reliable-cars-a6569295379/" target="_blank">Most Reliable Cars</a> • <a rel="noreferrer noopener" href="https://www.consumerreports.org/cars/car-reliability-owner-satisfaction/10-least-reliable-cars-a2967595976/" target="_blank">Least Reliable Cars</a> • <a href="https://www.consumerreports.org/cars/car-reliability-owner-satisfaction/guide-to-car-reliability-owner-satisfaction-a9213219653/">CR’s Guide to Car Reliability</a></strong></p>
                </div>
    <div id="how-the-regions-compare-for-reliability">
                    <p>Asian automakers are still leading reliability by a wide margin with an overall reliability average score of 63 for the region, on a scale of 1 to 100. Seven of the 10 most reliable brands are from Asian automakers.&nbsp;</p>
<p>European automakers are in second place at 46, with three brands filling out the rest of the top 10 most reliable brands.&nbsp;</p>
<p>Domestic brands trail both with an average score of 39. Each domestic automaker had at least one model that had an average or better reliability ranking, and Buick’s entire lineup scored average or better.</p>
<p>Both the <a href="https://www.consumerreports.org/cars/ford/edge/">Ford Edge</a> and <a href="https://www.consumerreports.org/cars/ford/maverick/">Maverick</a> scored above-average this year, while the <a href="https://www.consumerreports.org/cars/buick/encore-gx">Buick Encore GX</a>, <a href="https://www.consumerreports.org/cars/cadillac/xt5">Cadillac XT5</a>, <a href="https://www.consumerreports.org/cars/chevrolet/equinox">Chevrolet Equinox</a>, <a href="https://www.consumerreports.org/cars/gmc/terrain">GMC Terrain</a>, <a href="https://www.consumerreports.org/cars/lincoln/corsair">Lincoln Corsair</a>, and <a href="https://www.consumerreports.org/cars/tesla/model-3">Tesla Model 3</a> had high scores.</p>
<p>Cars, including sedans, hatchbacks, and wagons, remain the most reliable vehicle type, with an average reliability rating of 57 (on a scale of 0 to 100), followed by SUVs (50) and minivans (45). “Sedans have fallen out of favor with consumers, but as a class they are very reliable,” says Jake Fisher, senior director of <a href="https://www.consumerreports.org/cars/cars-driving/how-consumer-reports-tests-cars-auto-test-center-a3516544374/">auto testing at Consumer Reports</a>. “They often have less of the latest technology and features that can cause problems before the bugs are worked out.” Pickup trucks come in last, with an average reliability rating of 41.</p>
                </div>
    <div id="hybrids-soar-and-e-vs-still-struggle">
                    <p>There are interesting insights with the reliability of electrified models this year. Overall, hybrids have 26 percent fewer problems than cars powered by internal combustion engines (ICE). Some standouts include the <a href="https://www.consumerreports.org/cars/lexus/ux">Lexus UX</a> and <a href="https://www.consumerreports.org/cars/lexus/nx-hybrid">NX Hybrid</a> and the <a href="https://www.consumerreports.org/cars/toyota/camry-hybrid">Toyota Camry Hybrid</a>, <a href="https://www.consumerreports.org/cars/toyota/highlander-hybrid">Highlander Hybrid</a>, and <a href="https://www.consumerreports.org/cars/toyota/rav4-hybrid">RAV4 Hybrid</a>.</p>
<p>Plug-in hybrid electric vehicles (PHEVs) are more of a mixed bag. As a category, they have 146 percent more problems than ICE vehicles. Several PHEVs are even less reliable than their conventional counterparts, such as the below-average <a href="https://www.consumerreports.org/cars/audi/q5">Audi Q5</a> and <a href="https://www.consumerreports.org/cars/chrysler/pacifica-hybrid">Chrysler Pacifica</a>. The latter has the lowest score in our survey, at 14.</p>
<p>Still, there are PHEVs that buck that trend, including standouts such as the <a href="https://www.consumerreports.org/cars/toyota/rav4-prime">Toyota RAV4 Prime</a> and <a href="https://www.consumerreports.org/cars/kia/sportage-hybrid">Kia Sportage</a>, which score well above average. The <a href="https://www.consumerreports.org/cars/bmw/x5">BMW X5</a>, <a href="https://www.consumerreports.org/cars/hyundai/tucson-hybrid">Hyundai Tucson</a>, and <a href="https://www.consumerreports.org/cars/ford/escape-hybrid">Ford Escape</a> PHEVs earn average reliability scores.</p>
                </div>
    
    <div>
                <p>Electric cars and electric SUVs don’t fare much better, with average reliability scores of 44 and 43, respectively. At the bottom of our vehicle-type rankings are electric pickup trucks, with an average score of 30.</p>
<p>As more EVs hit the marketplace and automakers build each model in greater numbers, we are seeing that some of them have problems with the EV drive system motors, EV charging systems, and EV batteries (which are different from the low-power 12-volt batteries that power accessories). Owners of the <a href="https://www.consumerreports.org/cars/ford/f-150-lightning">Ford F-150 Lightning</a> and <a href="https://www.consumerreports.org/cars/ford/mustang-mach-e">Mustang Mach-E</a>, <a href="https://www.consumerreports.org/cars/genesis/gv60">Genesis GV60</a>, <a href="https://www.consumerreports.org/cars/hyundai/ioniq-5">Hyundai Ioniq 5</a>, <a href="https://www.consumerreports.org/cars/kia/niro-electric">Kia Niro EV</a> and <a href="https://www.consumerreports.org/cars/kia/ev6">EV6</a>, <a href="https://www.consumerreports.org/cars/subaru/solterra">Subaru Solterra</a>, <a href="https://www.consumerreports.org/cars/toyota/bz4x">Toyota bZ4X</a>, and <a href="https://www.consumerreports.org/cars/volkswagen/id4">Volkswagen ID.4</a> all reported some of these issues.</p>
<p>Among the top five brands, Lexus has just one model, the <a href="https://www.consumerreports.org/cars/lexus/nx/">NX</a>, that scored average predicted reliability. All of its other models scored above-average or better reliability this year, led by the <a href="https://www.consumerreports.org/cars/lexus/ux/">UX</a> hybrid SUV.</p>
<p>A majority of Toyota models have above- or well-above-average reliability. The brand is topped by the <a href="https://www.consumerreports.org/cars/toyota/4runner">4Runner</a> SUV, which is among the most reliable models in the survey. But the brand’s full-sized pickup truck, the <a href="https://www.consumerreports.org/cars/toyota/tundra/">Tundra</a>, remains among the less reliable vehicles in the survey.</p>
<p>Acura’s RDX and TLX both have above-average reliability, while the brand’s two recently redesigned models, the <a href="https://www.consumerreports.org/cars/acura/mdx/">MDX</a> SUV and Integra sedan, come in at average.&nbsp;</p>
<p>Consumer Reports’ brand-level rankings are based on the average predicted reliability score for vehicles in the brand’s model lineup.</p>
<p>Here we present the rankings of brands based on their average reliability scores. We identify the most and least reliable model scores within each brand and list whether the brand’s rank went up or down based on their relative position from our last survey. This means we consider only those brands in both surveys that had sufficient data for two or more models.</p>
<p><strong>For more details, go to <a href="https://www.consumerreports.org/cars-car-reliability-guide/">CR’s Guide to Car Reliability</a>.</strong></p>
            </div>
    
    <div>
            <p><strong>Editor’s Note:</strong> This article also appeared in the January 2024 issue of Consumer Reports magazine.</p>
        </div>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Okta hackers stole data on all customer support users in major breach (117 pts)]]></title>
            <link>https://sec.okta.com/harfiles/</link>
            <guid>38462681</guid>
            <pubDate>Wed, 29 Nov 2023 17:56:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sec.okta.com/harfiles/">https://sec.okta.com/harfiles/</a>, See on <a href="https://news.ycombinator.com/item?id=38462681">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Related Posts:<strong>&nbsp;</strong><a href="https://sec.okta.com/articles/2023/11/unauthorized-access-oktas-support-case-management-system-root-cause">Root Cause Analysis [RCA]</a>&nbsp;- Nov 3, 2023 /&nbsp;<a href="https://sec.okta.com/articles/2023/10/tracking-unauthorized-access-oktas-support-system">Security Incident</a> - Oct 20, 2023</p><p><span><span><span><span><span><span>In the wake of the security incident Okta </span></span></span></span></span></span><a href="https://sec.okta.com/articles/2023/10/tracking-unauthorized-access-oktas-support-system"><span><span><span><span><span><span><span><span>disclosed</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span> in October 2023 affecting our customer support management system (also known as the Okta Help Center), Okta Security has continued to review our initial analysis </span></span></span></span><a href="https://sec.okta.com/articles/2023/11/unauthorized-access-oktas-support-case-management-system-root-cause"><span><span><span><span><span><span>shared</span></span></span></span></span></span></a></span></span><a href="https://sec.okta.com/harfiles"><span><span><span><span><span><span><span><span> </span></span></span></span></span></span></span></span></a><span><span><span><span><span><span>on November 3, re-examining the actions that the threat actor performed. This included manually recreating reports the threat actor ran in the system and the files the threat actor downloaded.&nbsp;</span></span></span></span></span></span></p><p><span><span><span><span><span><span>Today we are sharing new information that potentially impacts the security of our customers.&nbsp;</span></span></span></span></span></span></p><p><span><span><span><span><span><span>We have determined that the threat actor ran and downloaded a report that contained the names and email addresses of all Okta customer support system users. All Okta Workforce Identity Cloud (WIC) and Customer Identity Solution (CIS) customers are impacted except customers in our FedRamp High and DoD IL4 environments (these environments use a separate support system NOT accessed by the threat actor). The Auth0/CIC support case management system was also not impacted by this incident.&nbsp;</span></span></span></span></span></span></p><p><span><span><span><span><span><span>The threat actor ran a report on September 28, 2023 at 15:06 UTC that contained the following fields for each user in Okta’s customer support system:&nbsp;</span></span></span></span></span></span></p><div><table><colgroup><col><col><col><col><col></colgroup><tbody><tr><td><p><span><span><span><span><span><span><span><span>Created Date</span></span></span></span></span></span></span></span></p></td><td><p><span><span><span><span><span><span><span><span>Last Login</span></span></span></span></span></span></span></span></p></td><td><p><span><span><span><span><span><span><span><span>Full Name</span></span></span></span></span></span></span></span></p></td><td><p><span><span><span><span><span><span><span><span>Username</span></span></span></span></span></span></span></span></p></td><td><p><span><span><span><span><span><span><span><span>Email</span></span></span></span></span></span></span></span></p></td></tr><tr><td><p><span><span><span><span><span><span><span><span>Company Name</span></span></span></span></span></span></span></span></p></td><td><p><span><span><span><span><span><span><span><span>User Type</span></span></span></span></span></span></span></span></p></td><td><p><span><span><span><span><span><span><span><span>Address</span></span></span></span></span></span></span></span></p></td><td><p><span><span><span><span><span><span><span><span>[Date of] Last Password Change or Reset</span></span></span></span></span></span></span></span></p></td><td><p><span><span><span><span><span><span><span><span>Role: Name</span></span></span></span></span></span></span></span></p></td></tr><tr><td><p><span><span><span><span><span><span><span><span>Role: Description</span></span></span></span></span></span></span></span></p></td><td><p><span><span><span><span><span><span><span><span>Phone</span></span></span></span></span></span></span></span></p></td><td><p><span><span><span><span><span><span><span><span>Mobile</span></span></span></span></span></span></span></span></p></td><td><p><span><span><span><span><span><span><span><span>Time Zone</span></span></span></span></span></span></span></span></p></td><td><p><span><span><span><span><span><span><span><span>SAML Federation ID</span></span></span></span></span></span></span></span></p></td></tr></tbody></table></div><p><span><span><span><span><span><span>The majority of the fields in the report are blank and the report does not include user credentials or sensitive personal data. For 99.6% of users in the report, the only contact information recorded is full name and email address.&nbsp;</span></span></span></span></span></span></p><p><span><span><span><span><span><span>While we do not have direct knowledge or evidence that this information is being actively exploited, there is a possibility that the threat actor may use this information to target Okta customers via phishing or social engineering attacks. Okta customers sign-in to Okta’s customer support system with the same accounts they use in their own Okta org. Many users of the customer support system are Okta administrators. It is critical that these users have multi-factor authentication (MFA) enrolled to protect not only the customer support system, but also to secure access to their Okta admin console(s).</span></span></span></span></span></span></p><p><span><span><span><span><span><span>Given that names and email addresses were downloaded, we assess that there is an increased risk of phishing and social engineering attacks directed at these users. While 94% of Okta customers already require MFA for their administrators, we recommend ALL Okta customers employ MFA and consider the use of phishing resistant authenticators to further enhance their security. Please refer to product documentation to enable MFA for the admin console (</span></span></span></span></span></span><a href="https://help.okta.com/en-us/content/topics/security/mfa/mfa-enable-admins.htm"><span><span><span><span><span><span><span><span>Classic</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span> or </span></span></span></span></span></span><a href="https://help.okta.com/oie/en-us/content/topics/security/mfa/mfa-enable-admins.htm"><span><span><span><span><span><span><span><span>OIE</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span>).</span></span></span></span></span></span></p><p><span><span><span><span><span><span>How we discovered this</span></span></span></span></span></span></p><p><span><span><span><span><span><span>Following the publication of the RCA on November 3, Okta Security reviewed our initial analysis of the actions that the threat actor performed, including manually recreating the reports that the threat actor ran within the customer support system. We identified that the file size of one particular report downloaded by the threat actor was larger than the file generated during our initial investigation. After additional analysis, we concluded that the report contained a list of all customer support system users. The discrepancy in our initial analysis stems from the threat actor running an unfiltered view of the report. Our November review identified that if the filters were removed from the templated report, the downloaded file was considerably larger - and more closely matched the size of the file download logged in our security telemetry.</span></span></span></span></span></span></p><p><span><span><span><span><span><span>We also identified additional reports and support cases that the threat actor accessed, which contain contact information of all Okta certified users and some Okta Customer Identity Cloud (CIC) customer contacts, and other information. Some Okta employee information was also included in these reports. This contact information does not include user credentials or sensitive personal data.</span></span></span></span></span></span></p><p><span><span><span><span><span><span>We are working with a third-party digital forensics firm to validate our findings and we will be sharing the report with customers upon completion.&nbsp;</span></span></span></span></span></span></p><p><span><span><span><span><span><span>Implementing recommended best practices</span></span></span></span></span></span></p><p><span><span><span><span><span><span>We recommend all customers immediately take the following actions to defend against potential attacks that target their Okta administrators.&nbsp;&nbsp;</span></span></span></span></span></span></p><ul><li aria-level="1"><span><span><span><span><span><span>Multi-Factor Authentication (MFA)</span></span></span></span></span></span><span><span><span><span><span><span>: We strongly recommend all Okta customers secure admin access using MFA at a minimum. We also strongly encourage customers to enroll administrative users in phishing resistant authenticators (such as Okta Verify FastPass, FIDO2 WebAuthn, or PIV/CAC Smart Cards) and to enforce phishing resistance for access to all administrative applications. Please refer to product documentation to enable MFA for the admin console (</span></span></span></span></span></span><a href="https://help.okta.com/en-us/content/topics/security/mfa/mfa-enable-admins.htm"><span><span><span><span><span><span><span><span>Classic</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span> or </span></span></span></span></span></span><a href="https://help.okta.com/oie/en-us/content/topics/security/mfa/mfa-enable-admins.htm"><span><span><span><span><span><span><span><span>OIE</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span>).</span></span></span></span></span></span></li><li aria-level="1"><span><span><span><span><span><span>Admin Session Binding</span></span></span></span></span></span><span><span><span><span><span><span>: As communicated in the </span></span></span></span><a href="https://sec.okta.com/articles/2023/11/unauthorized-access-oktas-support-case-management-system-root-cause"><span><span><span><span><span><span>Security Incident RCA</span></span></span></span></span></span></a><span><span><span><span>, customers can now enable an Early Access feature in Okta that requires admins to </span></span></span></span></span></span><span><span><span><span><span><span><span>reauthenticate if their session is reused from an IP address with a different ASN (Autonomous System Number).</span></span></span></span></span></span></span><span><span><span><span><span><span> Okta strongly recommends customers enable this feature to further secure admin sessions.</span></span></span></span></span></span></li><li aria-level="1"><span><span><span><span><span><span>Admin Session Timeout</span></span></span></span></span></span><span><span><span><span><span><span>: To align with </span></span></span></span></span></span><a href="https://pages.nist.gov/800-63-4/sp800-63b.html"><span><span><span><span><span><span><span><span>NIST AAL3</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span> </span></span></span></span></span></span><span><span><span><span><span><span>guidelines and increase the security posture of every customer, Okta is introducing Admin Console timeouts that will be set to a default of 12-hour session duration and a 15-minute idle time. Customers will have the option to edit these settings</span></span></span></span></span></span><span><span><span><span><span><span>. </span></span></span></span></span></span><span><span><span><span><span><span>This will be available as an Early Access feature starting November 29th for preview orgs and December 4th for production orgs. The feature will be available for all production orgs by January 8th, 2024. An email was sent to all Super Admins regarding this change on November 27th, and a copy of that communication can be found in the Knowledge Base article: </span></span></span></span></span></span><a href="https://support.okta.com/help/s/article/admin-session-lifetime-idle-timeout-security-enhancements?language=en_US"><span><span><span><span><span><span><span><span>Admin Session Lifetime/Idle Timeout Security Enhancements</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span>.</span></span></span></span></span></span></li><li aria-level="1"><span><span><span><span><span><span>Phishing Awareness</span></span></span></span></span></span><span><span><span><span><span><span>: In addition, Okta customers should be vigilant of phishing attempts that target their employees and especially wary of social engineering attempts that target their IT Help Desks and related service providers. We recommend Okta customers implement our industry-leading, phishing-resistant methods for enrollment, authentication, and recovery. Please see </span></span></span></span></span></span><a href="https://help.okta.com/oie/en-us/content/topics/architecture/pr/pr-overview.htm"><span><span><span><span><span><span><span><span>Okta Solutions for Phishing Resistance</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span> for more information on protecting your organization from phishing. We also strongly recommend that customers review their IT Help Desk verification processes and ensure that appropriate checks, such as visual verification, are performed before performing high risk actions such as password or factor resets on privileged accounts.</span></span></span></span></span></span></li></ul></div><div><p>David Bradbury is Chief Security Officer at Okta. As CSO, he leads overall security execution for the organization and his team is responsible for navigating the evolving threat landscape to best protect employees and customers. In addition, he is instrumental in helping Okta’s customers continue to adopt and accelerate Zero Trust security strategies.&nbsp;</p>

<p>Prior to joining Okta, Bradbury was Senior Vice President and Chief Security Officer at Symantec where he led and had global oversight of all cyber security and physical security programs.&nbsp;</p>

<p>Bradbury has built an international reputation for leading and delivering cybersecurity at scale. He has worked across his native Australia, as well as in the United Kingdom and the United States, leading highly-regarded security teams at some of the world’s largest banks, including ABN AMRO, Barclays, Morgan Stanley and the Commonwealth Bank of Australia. He holds a B.S. in Computer Science from the University of Sydney.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Visualizing Pokémon Red and Blue Connections (2020) (151 pts)]]></title>
            <link>http://peterhajas.com/blog/pokemon_rb_connections.html</link>
            <guid>38462318</guid>
            <pubDate>Wed, 29 Nov 2023 17:32:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://peterhajas.com/blog/pokemon_rb_connections.html">http://peterhajas.com/blog/pokemon_rb_connections.html</a>, See on <a href="https://news.ycombinator.com/item?id=38462318">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

<h3>April 14, 2020  • &nbsp;😺🗺</h3>

<p>I love Pokémon. My fascination with the series began with the original games released in the US, Pokémon Red and Blue (I had the Blue version).</p>

<p>In the past few years, people have been disassembling Pokémon games. You can check these out for Pokémon <a href="https://github.com/pret/pokered">Red and Blue</a>, <a href="https://github.com/pret/pokecrystal">Crystal</a>, <a href="https://github.com/pret/pokeemerald">Emerald</a>, and others. It's really cool to be able to compile and build a game that was such a huge part of my youth.</p>

<p>I thought it would be fun to play with this source code, viewing these games through a new lens. A few months ago, I discovered <a href="https://www.graphviz.org/">Graphviz</a>, a software package for rendering graphs written in the <a href="https://en.wikipedia.org/wiki/DOT_(graph_description_language)">Dot language</a>. Dot is a very simple language, and it's easy to filter data into its format. Graphviz includes some command line tools that can render dot files to nice human-readable output. Let's see how we can use Graphviz to visualize Pokémon Red and Blue.</p>

<p>Inside of <code>pokered</code>, there's a <code>data</code> directory with a <code>mapHeaders</code> subdirectory inside. <code>mapHeaders</code> includes metadata about every overworld map in the game. This includes the connections between maps. For example, here is the metadata for Route 10:</p>

<pre><code>$ cat Route10.asm 
Route10_h:
    db OVERWORLD ; tileset
    db ROUTE_10_HEIGHT, ROUTE_10_WIDTH ; dimensions (y, x)
    dw Route10_Blocks ; blocks
    dw Route10_TextPointers ; texts
    dw Route10_Script ; scripts
    db SOUTH | WEST ; connections
    SOUTH_MAP_CONNECTION ROUTE_10, LAVENDER_TOWN, 0, 0, LavenderTown_Blocks
    WEST_MAP_CONNECTION ROUTE_10, ROUTE_9, 0, 0, Route9_Blocks
    dw Route10_Object ; objects
</code></pre>

<p>So south of Route 10 is Lavender Town, and west is Route 9. We can use this connection data and some simple uses of <code>grep</code> and <code>awk</code> to generate Dot code representing these connections. The following commands are all run from <code>/data/mapHeaders</code> in the <code>pokered</code> repository. First, we use <code>grep</code> to see the connections:</p>

<pre><code>$ grep -R "MAP_CONNECTION" ./
.//PewterCity.asm:  SOUTH_MAP_CONNECTION PEWTER_CITY, ROUTE_2, 5, 0, Route2_Blocks
.//PewterCity.asm:  EAST_MAP_CONNECTION PEWTER_CITY, ROUTE_3, 4, 0, Route3_Blocks
...
</code></pre>

<p>Next, let's pipe that to <code>awk</code> to print the endpoints of that connection:</p>

<pre><code>$ grep -R "MAP_CONNECTION" ./ | awk -F" " '{ print $3 $4; }'
PEWTER_CITY,ROUTE_2,
PEWTER_CITY,ROUTE_3,
...
</code></pre>

<p>We can use a second <code>awk</code> invocation to print these as Dot edges:</p>

<pre><code>$ grep -R "MAP_CONNECTION" ./ | awk -F" " '{ print $3 $4; }' | awk -F"," '{ print $1" -- "$2 }'
PEWTER_CITY -- ROUTE_2
PEWTER_CITY -- ROUTE_3
...
</code></pre>

<p>Undirected Dot edges are represented with the two nodes and a <code>--</code> between them.</p>

<p>We can represent a strict (one connection between nodes) graph (non-directed, as these are bidirectional connections) by wrapping all these connections in a <code>strict graph {}</code>:</p>

<pre><code>strict graph {
    PEWTER_CITY -- ROUTE_2
    PEWTER_CITY -- ROUTE_3
    ...
</code></pre>

<p>We can add two other options (<code>overlap=false</code> to avoid edge overlap and <code>splines=true</code> to use splines for edges) to get a better looking graph. Here's my <code>pokemon_rb_towns_and_routes.dot</code> generated from the above steps:</p>

<pre><code>strict graph {
    overlap=false;
    splines=true;
    PEWTER_CITY -- ROUTE_2
    PEWTER_CITY -- ROUTE_3
    CELADON_CITY -- ROUTE_16
    CELADON_CITY -- ROUTE_7
    ROUTE_9 -- CERULEAN_CITY
    ROUTE_9 -- ROUTE_10
    ROUTE_8 -- SAFFRON_CITY
    ROUTE_8 -- LAVENDER_TOWN
    ROUTE_21 -- PALLET_TOWN
    ROUTE_21 -- CINNABAR_ISLAND
    ROUTE_20 -- CINNABAR_ISLAND
    ROUTE_20 -- ROUTE_19
    ROUTE_22 -- ROUTE_23
    ROUTE_22 -- VIRIDIAN_CITY
    PALLET_TOWN -- ROUTE_1
    PALLET_TOWN -- ROUTE_21
    ROUTE_23 -- INDIGO_PLATEAU
    ROUTE_23 -- ROUTE_22
    VERMILION_CITY -- ROUTE_6
    VERMILION_CITY -- ROUTE_11
    ROUTE_24 -- CERULEAN_CITY
    ROUTE_24 -- ROUTE_25
    ROUTE_18 -- ROUTE_17
    ROUTE_18 -- FUCHSIA_CITY
    ROUTE_19 -- FUCHSIA_CITY
    ROUTE_19 -- ROUTE_20
    ROUTE_25 -- ROUTE_24
    LAVENDER_TOWN -- ROUTE_10
    LAVENDER_TOWN -- ROUTE_12
    LAVENDER_TOWN -- ROUTE_8
    ROUTE_14 -- ROUTE_15
    ROUTE_14 -- ROUTE_13
    ROUTE_15 -- FUCHSIA_CITY
    ROUTE_15 -- ROUTE_14
    ROUTE_17 -- ROUTE_16
    ROUTE_17 -- ROUTE_18
    ROUTE_16 -- ROUTE_17
    ROUTE_16 -- CELADON_CITY
    ROUTE_12 -- LAVENDER_TOWN
    ROUTE_12 -- ROUTE_13
    ROUTE_12 -- ROUTE_11
    ROUTE_13 -- ROUTE_12
    ROUTE_13 -- ROUTE_14
    ROUTE_11 -- VERMILION_CITY
    ROUTE_11 -- ROUTE_12
    CERULEAN_CITY -- ROUTE_24
    CERULEAN_CITY -- ROUTE_5
    CERULEAN_CITY -- ROUTE_4
    CERULEAN_CITY -- ROUTE_9
    ROUTE_10 -- LAVENDER_TOWN
    ROUTE_10 -- ROUTE_9
    ROUTE_5 -- CERULEAN_CITY
    ROUTE_5 -- SAFFRON_CITY
    FUCHSIA_CITY -- ROUTE_19
    FUCHSIA_CITY -- ROUTE_18
    FUCHSIA_CITY -- ROUTE_15
    SAFFRON_CITY -- ROUTE_5
    SAFFRON_CITY -- ROUTE_6
    SAFFRON_CITY -- ROUTE_7
    SAFFRON_CITY -- ROUTE_8
    ROUTE_4 -- ROUTE_3
    ROUTE_4 -- CERULEAN_CITY
    ROUTE_6 -- SAFFRON_CITY
    ROUTE_6 -- VERMILION_CITY
    VIRIDIAN_CITY -- ROUTE_2
    VIRIDIAN_CITY -- ROUTE_1
    VIRIDIAN_CITY -- ROUTE_22
    INDIGO_PLATEAU -- ROUTE_23
    ROUTE_7 -- CELADON_CITY
    ROUTE_7 -- SAFFRON_CITY
    ROUTE_3 -- ROUTE_4
    ROUTE_3 -- PEWTER_CITY
    ROUTE_2 -- PEWTER_CITY
    ROUTE_2 -- VIRIDIAN_CITY
    CINNABAR_ISLAND -- ROUTE_21
    CINNABAR_ISLAND -- ROUTE_20
    ROUTE_1 -- VIRIDIAN_CITY
    ROUTE_1 -- PALLET_TOWN
}
</code></pre>

<p>We can use a simple invocation of <code>neato</code> to produce a PDF file with:</p>

<pre><code>neato -Tpdf pokemon_rb_towns_and_routes.dot &gt; pokemon_rb_towns_and_routes.pdf
</code></pre>

<p>Check it out:</p>

<p><img src="http://peterhajas.com/media/pokemon_rb_towns_and_routes_preview.jpeg" alt="A graph visualizing all towns and routes in Pokémon Red and Blue">
(<a href="http://peterhajas.com/media/pokemon_rb_towns_and_routes.pdf">PDF file here</a>)</p>

<p>OK, so towns and routes are cool. Can we augment this file to include buildings, tunnels, and rooms? There are <code>warp</code> and <code>warp_to</code> markers in the files in <code>/data/mapObjects</code>. For example, let's look at <code>SaffronCity.asm</code>:</p>

<pre><code>$ cat data/mapObjects/SaffronCity.asm
SaffronCity_Object:
    db $f ; border block

    db 8 ; warps
    warp 7, 5, 0, COPYCATS_HOUSE_1F
    warp 26, 3, 0, FIGHTING_DOJO
    warp 34, 3, 0, SAFFRON_GYM
    warp 13, 11, 0, SAFFRON_PIDGEY_HOUSE
    warp 25, 11, 0, SAFFRON_MART
    warp 18, 21, 0, SILPH_CO_1F
    warp 9, 29, 0, SAFFRON_POKECENTER
    warp 29, 29, 0, MR_PSYCHICS_HOUSE

    ...

    ; warp-to
    warp_to 7, 5, SAFFRON_CITY_WIDTH ; COPYCATS_HOUSE_1F
    warp_to 26, 3, SAFFRON_CITY_WIDTH ; FIGHTING_DOJO
    warp_to 34, 3, SAFFRON_CITY_WIDTH ; SAFFRON_GYM
    warp_to 13, 11, SAFFRON_CITY_WIDTH ; SAFFRON_PIDGEY_HOUSE
    warp_to 25, 11, SAFFRON_CITY_WIDTH ; SAFFRON_MART
    warp_to 18, 21, SAFFRON_CITY_WIDTH ; SILPH_CO_1F
    warp_to 9, 29, SAFFRON_CITY_WIDTH ; SAFFRON_POKECENTER
    warp_to 29, 29, SAFFRON_CITY_WIDTH ; MR_PSYCHICS_HOUSE

    ...
</code></pre>

<p>(These <code>_WIDTH</code> suffixes seem to indicate the coordinates are inside of the width of the map. We'll clean them up later.)</p>

<p>So, if we parse out the <code>warp_to</code> statements, we should be able to get a more complete view of the game's locations and how they connect. Let's start with a simple <code>grep</code> to find all the <code>warp_to</code> statements (run from <code>/data/mapObjects</code>):</p>

<pre><code>grep -R "warp_to " ./
.//RocketHideoutB4F.asm:    warp_to 19, 10, ROCKET_HIDEOUT_B4F_WIDTH ; ROCKET_HIDEOUT_B3F
.//RocketHideoutB4F.asm:    warp_to 24, 15, ROCKET_HIDEOUT_B4F_WIDTH ; ROCKET_HIDEOUT_ELEVATOR
.//RocketHideoutB4F.asm:    warp_to 25, 15, ROCKET_HIDEOUT_B4F_WIDTH ; ROCKET_HIDEOUT_ELEVATOR
...
</code></pre>

<p>Next, pipe to <code>awk</code> to find the endpoints of the warp:</p>

<pre><code>$ grep -R "warp_to " ./ | awk -F"," '{ print $3; }'
ROCKET_HIDEOUT_B4F_WIDTH ; ROCKET_HIDEOUT_B3F
ROCKET_HIDEOUT_B4F_WIDTH ; ROCKET_HIDEOUT_ELEVATOR
ROCKET_HIDEOUT_B4F_WIDTH ; ROCKET_HIDEOUT_ELEVATOR
CELADON_MART_3F_WIDTH ; CELADON_MART_4F
CELADON_MART_3F_WIDTH ; CELADON_MART_2F
CELADON_MART_3F_WIDTH ; CELADON_MART_ELEVATOR
BRUNOS_ROOM_WIDTH ; LORELEIS_ROOM
BRUNOS_ROOM_WIDTH ; LORELEIS_ROOM
BRUNOS_ROOM_WIDTH ; AGATHAS_ROOM
BRUNOS_ROOM_WIDTH ; AGATHAS_ROOM
BIKE_SHOP_WIDTH
...
</code></pre>

<p>This is close, but includes some warps that appear to point to themselves (like <code>BIKE_SHOP_WIDTH</code> above). No problem - we can only print lines with <code>;</code> in them using <code>grep</code>:</p>

<pre><code>$ grep -R "warp_to " ./ | awk -F"," '{ print $3; }' | grep ";"
ROCKET_HIDEOUT_B4F_WIDTH ; ROCKET_HIDEOUT_B3F
ROCKET_HIDEOUT_B4F_WIDTH ; ROCKET_HIDEOUT_ELEVATOR
ROCKET_HIDEOUT_B4F_WIDTH ; ROCKET_HIDEOUT_ELEVATOR
CELADON_MART_3F_WIDTH ; CELADON_MART_4F
CELADON_MART_3F_WIDTH ; CELADON_MART_2F
CELADON_MART_3F_WIDTH ; CELADON_MART_ELEVATOR
...
</code></pre>

<p>OK, almost done. Next, let's strip out the <code>_WIDTH</code> text and put in edge connections:</p>

<pre><code>$ grep -R "warp_to " ./ | awk -F"," '{ print $3; }' | grep ";" | sed -e "s/_WIDTH//" | sed -e "s/;/--/"
 ROCKET_HIDEOUT_B4F -- ROCKET_HIDEOUT_B3F
 ROCKET_HIDEOUT_B4F -- ROCKET_HIDEOUT_ELEVATOR
 ROCKET_HIDEOUT_B4F -- ROCKET_HIDEOUT_ELEVATOR
 CELADON_MART_3F -- CELADON_MART_4F
 CELADON_MART_3F -- CELADON_MART_2F
 CELADON_MART_3F -- CELADON_MART_ELEVATOR
...
</code></pre>

<p>(note the leading space here - not a big deal for the Graphviz tools)</p>

<p>Now, we'll put this all into a <code>pokemon_rb_all.dot</code> file (along with the connections from <code>pokemon_rb_towns_and_routes.dot</code>) to make a graph of all of the locations in Pokémon Red and Blue. For this invocation, I also used <code>neato</code>:</p>

<pre><code>neato -Tpdf pokemon_rb_all.dot &gt; pokemon_rb_all.pdf
</code></pre>

<p>This graph is so cool! Check it out:</p>

<p><img src="http://peterhajas.com/media/pokemon_rb_all_preview.jpeg" alt="A graph visualizing all locations in Pokémon Red and Blue">
(<a href="http://peterhajas.com/media/pokemon_rb_all.pdf">PDF file here</a>, <a href="http://peterhajas.com/media/pokemon_rb_all.dot">dot file here</a>)</p>

<p>There are so many sections of this graph with interesting details, like Victory Road leading into the Indigo Plateau and Elite Four:</p>

<p><img src="http://peterhajas.com/media/pokemon_rb_all_e4.jpeg" alt="A cropped version of the &quot;all locations&quot; graph showing just Victory Road, the Indigo Plateau, and the Elite Four sections of the graph"></p>

<p>Or the maze-like Silph Company building:</p>

<p><img src="http://peterhajas.com/media/pokemon_rb_all_silph.jpeg" alt="A cropped version of the &quot;all locations&quot; graph showing just the Silph Company building floors"></p>

<p>I think it's really cool how easy it is to use simple tools to see these games from a new angle. I hope to look at other aspects of these games sometime in the future.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What should I do if I suspect one of the journal reviews I got is AI-generated? (137 pts)]]></title>
            <link>https://academia.stackexchange.com/questions/204370/what-should-i-do-if-i-suspect-one-of-the-journal-reviews-i-got-is-al-generated</link>
            <guid>38462269</guid>
            <pubDate>Wed, 29 Nov 2023 17:29:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://academia.stackexchange.com/questions/204370/what-should-i-do-if-i-suspect-one-of-the-journal-reviews-i-got-is-al-generated">https://academia.stackexchange.com/questions/204370/what-should-i-do-if-i-suspect-one-of-the-journal-reviews-i-got-is-al-generated</a>, See on <a href="https://news.ycombinator.com/item?id=38462269">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mainbar" role="main" aria-label="question and answers">

                
<div data-questionid="204370" data-position-on-page="0" data-score="28" id="question">
    
    <div itemprop="text">
                
<p>I've submitted a paper to one of the well-reputable journals and received two reviews asking for a revision. One of the reviews is on-topic and helpful. The other, however, I suspect was generated by an Al <em>based on the abstract only</em>. I don't want to debate the decision (the second reviewer is fair), but I wonder if I should mention my suspicions to the journal editor.</p>
<p>Should I? I doubt I can prove Al's use. All I have is a strong suspicion. Plus, I don't want to hurt my article. Yet, using Al for generating (not merely styling) reviews seems like such a bad practice that maybe it's worth letting the editor know.</p>
<p>My suspicions are based on several things:</p>
<ul>
<li>First, the style is a bit off. The review consists only of very long questions like: "In terms of the paper's stated goal of contributing to the literature, how well does it provide [contribution mentioned in abstract] for [use-case mentioned in abstract]?". There are no suggestions, no feedback, and no statements. It's only broad questions suspiciously rephrasing each line of the abstract (and heavily re-using entire phrases from it).</li>
<li>Second, the list of suggested articles is strange and includes existing papers (only names) from irrelevant fields.</li>
<li>Third, out of curiosity, I've run the text via several online "detect AL" tools, and all of them conclude it's Al. It's not valid proof, just a fun point.</li>
</ul>
    </div>

        

    <div>
                <div>
    <p>
        asked <span title="2023-11-29 15:45:38Z">6 hours ago</span>
    </p>
    <div>
        <a href="https://academia.stackexchange.com/users/179907/pintor"><p><img src="https://www.gravatar.com/avatar/9be6b51a18fb1904a5b715bb2773ff97?s=64&amp;d=identicon&amp;r=PG&amp;f=y&amp;so-version=2" alt="pintor's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>

    <div>
        <p><span> New contributor</span>
        </p>
        <div>
            
            <p><a href="https://academia.stackexchange.com/users/179907/pintor">pintor</a> is a new contributor to this site. Take care in asking for clarification, commenting, and answering.
Check out our <a href="https://academia.stackexchange.com/conduct">Code of Conduct</a>.        </p></div>
    </div>

            </div>
    
</div>



                
                
                <div id="answers">
                    


                                    
<div id="answer-204372" data-answerid="204372" data-parentid="204370" data-score="30" data-position-on-page="1" data-highest-scored="1" data-question-has-accepted-highest-score="0" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>I think you should contact the editor with basically the same content as your post here: explain why you think the review could be AI generated, including the limitations of each point of evidence. I think most editors see their job as letting reviewers guide their decision rather than reviewing the reviews, so I would start from the assumption that the editor would not have considered checking the reviews for suspicion of gen AI.</p>
<p>Explain why you think this is concerning, which I think at minimum would include the integrity of the peer review process (you and your colleagues are expecting papers to be reviewed by humans, not AI) and the protection of your intellectual property (having an AI review your paper involves inserting your work into a 3rd party tool which may incorporate it into the training data and is a definite breach of the confidentiality that reviewers are sworn to). I would check the journal's website to see if they have an explicit statement about use of AI in peer review; if so, you can refer to that as well.</p>
<p>Finally, make clear that you're not protesting the decision for a revision and plan to work on revising and resubmitting your paper. I'd include a note thanking the editor and the reviewers for their time.</p>
<p>From there, let the editor decide what should be done next; it's their call, your only duty here is to raise their awareness. It's possible they'll want to bring in another reviewer. It's also possible they'll decide it's okay. At that point you'll have to decide whether you're satisfied with their effort and whether that affects your opinion of the journal as an appropriate place to submit your work.</p>
    </div>
    <div>
    <p>
        answered <span title="2023-11-29 15:59:56Z">6 hours ago</span>
    </p>
    <div>
        <a href="https://academia.stackexchange.com/users/63475/bryan-krause"><p><img src="https://i.stack.imgur.com/43OB6.png?s=64&amp;g=1" alt="Bryan Krause's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://academia.stackexchange.com/users/63475/bryan-krause">Bryan Krause</a><span title="Moderator">♦</span><span itemprop="name">Bryan Krause</span></p><p><span title="reputation score 103,146" dir="ltr">103k</span><span>24 gold badges</span><span>297 silver badges</span><span>376 bronze badges</span>
        </p>
    </div>
</div>
    
</div>




            <p><span itemprop="commentCount">3</span></p>
    </div>


                                    
<div id="answer-204371" data-answerid="204371" data-parentid="204370" data-score="2" data-position-on-page="2" data-highest-scored="0" data-question-has-accepted-highest-score="0" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p><strong>Edit</strong> in response to discussions here.</p>
<p>I think my answer is wrong. @BriahKrause and @Buffy and @leonos are right. The OP should contact the editor immediately.</p>
<p>Leaving this post up so the back and forth is visible.</p>
<hr>
<p>Interesting puzzle.</p>
<p>Since the (possibly AI prepared) review is generally positive, treat it as if it were legitimate. Since the reviewer asks for no changes you need not argue about whether they are appropriate. If any of the suggested references are in fact relevant, include them.</p>
<p>After the paper is accepted you could contact the editor to voice your suspicions, with whatever supporting evidence you have.</p>
<p><strong>Edit</strong> after reading @BrianKrause 's answer.</p>
<p>Clearly you will revise your manuscript in response to the good suggestions from the unquestionably legitimate review. If you think the work would benefit from a second good review then you should tell the editor now about your suspicions.</p>
    </div>
    <div>
    <p>
        answered <span title="2023-11-29 15:59:48Z">6 hours ago</span>
    </p>
    <div>
        <a href="https://academia.stackexchange.com/users/7018/ethan-bolker"><p><img src="https://www.gravatar.com/avatar/5761bb874d0538564e37a7f131de3b87?s=64&amp;d=identicon&amp;r=PG" alt="Ethan Bolker's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://academia.stackexchange.com/users/7018/ethan-bolker">Ethan Bolker</a><span itemprop="name">Ethan Bolker</span></p><p><span title="reputation score 32,624" dir="ltr">32.6k</span><span>1 gold badge</span><span>76 silver badges</span><span>119 bronze badges</span>
        </p>
    </div>
</div>
    
</div>




            <p><span itemprop="commentCount">5</span></p>
    </div>

                                    
<div id="answer-204380" data-answerid="204380" data-parentid="204370" data-score="0" data-position-on-page="3" data-highest-scored="0" data-question-has-accepted-highest-score="0" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<blockquote>
<p>I suspect one of the journal reviews I got is Al-generated
I wonder if I should mention my suspicions to the journal editor.</p>
</blockquote>
<p>I don't think that's a relevant criteria.
Whether the review is useful is a more important criteria.</p>
<p>You could focus on the lack of constructive feedback or how the comments seemed to closely align with your abstract without offering substantial insights or suggestions for improvement.</p>
<p>Expressing gratitude for the helpful feedback from one reviewer while politely pointing out the absence of actionable feedback from the other might prompt the editor to consider the nature of the review.</p>
    </div>
    <div>
                <div>
    <p>
        answered <span title="2023-11-29 18:17:06Z">3 hours ago</span>
    </p>
    <div>
        <a href="https://academia.stackexchange.com/users/179920/julien-reszka"><p><img src="https://i.stack.imgur.com/6CL6v.jpg?s=64&amp;g=1" alt="Julien Reszka's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>

    <div>
        <p><span> New contributor</span>
        </p>
        <div>
            
            <p><a href="https://academia.stackexchange.com/users/179920/julien-reszka">Julien Reszka</a> is a new contributor to this site. Take care in asking for clarification, commenting, and answering.
Check out our <a href="https://academia.stackexchange.com/conduct">Code of Conduct</a>.        </p></div>
    </div>

            </div>
    
</div>




            <p><span itemprop="commentCount">2</span></p>
    </div>

                                <h2>
                                    You must <a href="https://academia.stackexchange.com/users/login?ssrc=question_page&amp;returnurl=https%3a%2f%2facademia.stackexchange.com%2fquestions%2f204370">log in</a> to answer this question.
                                </h2>



                            <h2 data-loc="1">
                                <div><p>
Not the answer you're looking for? Browse other questions tagged </p><p>.                                </p></div>
                            </h2>
                </div>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Person-in-WiFi: Fine-Grained Person Perception Using WiFi [pdf] (141 pts)]]></title>
            <link>https://www.ri.cmu.edu/app/uploads/2019/09/Person_in_WiFi_ICCV2019.pdf</link>
            <guid>38461344</guid>
            <pubDate>Wed, 29 Nov 2023 16:19:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ri.cmu.edu/app/uploads/2019/09/Person_in_WiFi_ICCV2019.pdf">https://www.ri.cmu.edu/app/uploads/2019/09/Person_in_WiFi_ICCV2019.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=38461344">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Graph Networks for Materials Exploration (219 pts)]]></title>
            <link>https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/</link>
            <guid>38461323</guid>
            <pubDate>Wed, 29 Nov 2023 16:17:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/">https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/</a>, See on <a href="https://news.ycombinator.com/item?id=38461323">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      
  <article>
    
  
    
  
  
  
    

    
    
      
        <div>
          
            
            
              
              

<div>
    <div>
      <p>Research</p>
      

      
        <dl>
          
            <dt>Published</dt>
            <dd>
              <time datetime="2023-11-29">
                29 November 2023
              </time>
            </dd>
          
          
            <dt>Authors</dt>
            
          
        </dl>
      

      
    </div>

    
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="1072" height="603" srcset="https://lh3.googleusercontent.com/WpdZQYgBnHWIjBs-21n8EQrrdsvHnrUo_IhkDIz8UIIJkB-Fv09q1r0u97QKIv7Uja1pmujl1aLkZqqlf62UsPRP53t478q_-_GShn7DfZlBd9wT=w1072-h603-n-nu-rw 1x, https://lh3.googleusercontent.com/WpdZQYgBnHWIjBs-21n8EQrrdsvHnrUo_IhkDIz8UIIJkB-Fv09q1r0u97QKIv7Uja1pmujl1aLkZqqlf62UsPRP53t478q_-_GShn7DfZlBd9wT=w2144-h1206-n-nu-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="928" height="522" srcset="https://lh3.googleusercontent.com/WpdZQYgBnHWIjBs-21n8EQrrdsvHnrUo_IhkDIz8UIIJkB-Fv09q1r0u97QKIv7Uja1pmujl1aLkZqqlf62UsPRP53t478q_-_GShn7DfZlBd9wT=w928-h522-n-nu-rw 1x, https://lh3.googleusercontent.com/WpdZQYgBnHWIjBs-21n8EQrrdsvHnrUo_IhkDIz8UIIJkB-Fv09q1r0u97QKIv7Uja1pmujl1aLkZqqlf62UsPRP53t478q_-_GShn7DfZlBd9wT=w1856-h1044-n-nu-rw 2x"><source type="image/webp" width="528" height="297" srcset="https://lh3.googleusercontent.com/WpdZQYgBnHWIjBs-21n8EQrrdsvHnrUo_IhkDIz8UIIJkB-Fv09q1r0u97QKIv7Uja1pmujl1aLkZqqlf62UsPRP53t478q_-_GShn7DfZlBd9wT=w528-h297-n-nu-rw 1x, https://lh3.googleusercontent.com/WpdZQYgBnHWIjBs-21n8EQrrdsvHnrUo_IhkDIz8UIIJkB-Fv09q1r0u97QKIv7Uja1pmujl1aLkZqqlf62UsPRP53t478q_-_GShn7DfZlBd9wT=w1056-h594-n-nu-rw 2x">
      <img alt="" height="603" src="https://lh3.googleusercontent.com/WpdZQYgBnHWIjBs-21n8EQrrdsvHnrUo_IhkDIz8UIIJkB-Fv09q1r0u97QKIv7Uja1pmujl1aLkZqqlf62UsPRP53t478q_-_GShn7DfZlBd9wT=w1072-h603-n-nu" width="1072">
    </picture>
    
  
  </div>
            
          
            
            
              
              <div>
  <p data-block-key="ulgua">AI tool GNoME finds 2.2 million new crystals, including 380,000 stable materials that could power future technologies</p><p data-block-key="3t80m">Modern technologies from computer chips and batteries to solar panels rely on inorganic crystals. To enable new technologies, crystals must be stable otherwise they can decompose, and behind each new, stable crystal can be months of painstaking experimentation.</p><p data-block-key="ofmi">Today, in a <a href="https://www.nature.com/articles/s41586-023-06735-9" rel="noopener" target="_blank">paper published in <i>Nature</i></a>, we share the discovery of 2.2 million new crystals – equivalent to nearly 800 years’ worth of knowledge. We introduce Graph Networks for Materials Exploration (GNoME), our new deep learning tool that dramatically increases the speed and efficiency of discovery by predicting the stability of new materials.</p><p data-block-key="ak633">With GNoME, we’ve multiplied the number of technologically viable materials known to humanity. Of its 2.2 million predictions, 380,000 are the most stable, making them promising candidates for experimental synthesis. Among these candidates are materials that have the potential to develop future transformative technologies ranging from superconductors, powering supercomputers, and next-generation batteries to boost the efficiency of electric vehicles.</p><p data-block-key="4too3">GNoME shows the potential of using AI to discover and develop new materials at scale. External researchers in labs around the world have independently created 736 of these new structures experimentally in concurrent work. In partnership with Google DeepMind, a team of researchers at the Lawrence Berkeley National Laboratory has also published <a href="https://www.nature.com/articles/s41586-023-06734-w" rel="noopener" target="_blank">a second paper in<i> Nature</i></a> that shows how our AI predictions can be leveraged for autonomous material synthesis.</p><p data-block-key="9fvee">We’ve made <a href="https://www.nature.com/articles/s41586-023-06735-9" rel="noopener" target="_blank">GNoME’s predictions available</a> to the research community. We will be contributing 380,000 materials that we predict to be stable to the Materials Project, which is now processing the compounds and adding them into <a href="https://next-gen.materialsproject.org/" rel="noopener" target="_blank">its online database</a>. We hope these resources will drive forward research into inorganic crystals, and unlock the promise of machine learning tools as guides for experimentation</p>
</div>
            
          
            
            
              
              <p>
  <h2 data-block-key="ulgua">Accelerating materials discovery with AI</h2>
</p>
            
          
            
            
              
              


<figure>
  
  
    <figcaption>
      <p data-block-key="6hdcv">About 20,000 of the crystals experimentally identified in the ICSD database are computationally stable. Computational approaches drawing from the Materials Project, Open Quantum Materials Database and WBM database boosted this number to 48,000 stable crystals. GNoME expands the number of stable materials known to humanity to 421,000.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <p data-block-key="ulgua">In the past, scientists searched for novel crystal structures by tweaking known crystals or experimenting with new combinations of elements - an expensive, trial-and-error process that could take months to deliver even limited results. Over the last decade, computational approaches led by <a href="https://materialsproject.org/" rel="noopener" target="_blank">the Materials Project</a> and other groups have helped discover 28,000 new materials. But up until now, new AI-guided approaches hit a fundamental limit in their ability to accurately predict materials that could be experimentally viable. GNoME’s discovery of 2.2 million materials would be equivalent to about 800 years’ worth of knowledge and demonstrates an unprecedented scale and level of accuracy in predictions.</p><p data-block-key="c4h6v">For example, 52,000 new layered compounds similar to graphene that have the potential to revolutionize electronics with the development of superconductors. Previously, about <a href="https://pubmed.ncbi.nlm.nih.gov/28191965/" rel="noopener" target="_blank">1,000 such materials had been identified</a>. We also found 528 potential lithium ion conductors, 25 times more than a <a href="https://pubs.rsc.org/en/content/articlelanding/2017/ee/c6ee02697d" rel="noopener" target="_blank">previous study</a>, which could be used to improve the performance of rechargeable batteries.</p><p data-block-key="bh153">We are releasing the predicted structures for 380,000 materials that have the highest chance of successfully being made in the lab and being used in viable applications. For a material to be considered stable, it must not decompose into similar compositions with lower energy. For example, carbon in a graphene-like structure is stable compared to carbon in diamonds. Mathematically, these materials lie on the convex hull. This project discovered 2.2 million new crystals that are stable by current scientific standards and lie below the convex hull of previous discoveries. Of these, 380,000 are considered the most stable, and lie on the “final” convex hull – the new standard we have set for materials stability.<br></p><h2 data-block-key="a60kf">GNoME: Harnessing graph networks for materials exploration</h2>
</div>
            
          
            
            
              
              


<figure>
  
  
    <figcaption>
      <p data-block-key="6hdcv">GNoME uses two pipelines to discover low-energy (stable) materials. The structural pipeline creates candidates with structures similar to known crystals, while the compositional pipeline follows a more randomized approach based on chemical formulas. The outputs of both pipelines are evaluated using established Density Functional Theory calculations and those results are added to the GNoME database, informing the next round of active learning.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <p data-block-key="ulgua">GNoME is a state-of-the-art graph neural network (GNN) model. The input data for GNNs take the form of a graph that can be likened to connections between atoms, which makes GNNs particularly suited to discovering new crystalline materials.</p><p data-block-key="dn385">GNoME was originally trained with data on crystal structures and their stability, openly available through <a href="https://next-gen.materialsproject.org/" rel="noopener" target="_blank">the Materials Project</a>. We used GNoME to generate novel candidate crystals, and also to predict their stability. To assess our model’s predictive power during progressive training cycles, we repeatedly checked its performance using established computational techniques known as Density Functional Theory (DFT), used in physics, chemistry and materials science to understand structures of atoms, which is important to assess the stability of crystals.</p><p data-block-key="1v2m6">We used a training process called ‘active learning’ that dramatically boosted GNoME’s performance. GNoME would generate predictions for the structures of novel, stable crystals, which were then tested using DFT. The resulting high-quality training data was then fed back into our model training.</p><p data-block-key="3ja0j">Our research boosted the discovery rate of materials stability prediction from around 50%, to 80% - based on an external benchmark set by previous state-of-the-art models. We also managed to scale up the efficiency of our model by improving the discovery rate from under 10% to over 80% - such efficiency increases could have significant impact on how much compute is required per discovery.</p><h2 data-block-key="9q825">AI ‘recipes’ for new materials</h2><p data-block-key="ei4a6">The GNoME project aims to drive down the cost of discovering new materials. External researchers have independently created 736 of GNoME’s new materials in the lab, demonstrating that our model’s predictions of stable crystals accurately reflect reality. We’ve released our database of newly discovered crystals to the research community. By giving scientists the full catalog of the promising ‘recipes’ for new candidate materials, we hope this helps them to test and potentially make the best ones.</p>
</div>
            
          
            
            
              
              


<figure>
  
  
    <figcaption>
      <p data-block-key="sfacz">Upon completion of our latest discovery efforts, we searched the scientific literature and found 736 of our computational discoveries were independently realized by external teams across the globe. Above are six examples ranging from a first-of-its-kind Alkaline-Earth Diamond-Like optical material (Li4MgGe2S7) to a potential superconductor (Mo5GeB2).</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <p data-block-key="ulgua">Rapidly developing new technologies based on these crystals will depend on the ability to manufacture them. In a paper led by our collaborators at Berkeley Lab, researchers showed a robotic lab could rapidly make new materials with automated synthesis techniques. Using materials from the Materials Project and insights on stability from GNoME, the autonomous lab created new recipes for crystal structures and successfully synthesized more than 41 new materials, opening up new possibilities for AI-driven materials synthesis.</p>
            
          
            
            
              
              


<figure>
  
  
    <figcaption>
      <p data-block-key="bu4rv">A-Lab, a facility at Berkeley Lab where artificial intelligence guides robots in making new materials. Photo credit: Marilyn Sargent/Berkeley Lab</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="2wk2x">New materials for new technologies</h2><p data-block-key="5lhj1">To build a more sustainable future, we need new materials. GNoME has discovered 380,000 stable crystals that hold the potential to develop greener technologies – from better batteries for electric cars, to superconductors for more efficient computing.</p><p data-block-key="80bof">Our research – and that of collaborators at the Berkeley Lab, Google Research, and teams around the world — shows the potential to use AI to guide materials discovery, experimentation, and synthesis. We hope that GNoME together with other AI tools can help revolutionize materials discovery today and shape the future of the field.</p>
</div>
            
          
            
            
              
              


            
          
            
            
              
              
            
          
            
            
              
              



  
    
  

            
          
        </div>
      
    

    
  
  

  

  </article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jaq – A jq clone focused on correctness, speed, and simplicity (390 pts)]]></title>
            <link>https://github.com/01mf02/jaq</link>
            <guid>38461249</guid>
            <pubDate>Wed, 29 Nov 2023 16:13:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/01mf02/jaq">https://github.com/01mf02/jaq</a>, See on <a href="https://news.ycombinator.com/item?id=38461249">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">jaq</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/01mf02/jaq/workflows/Rust/badge.svg"><img src="https://github.com/01mf02/jaq/workflows/Rust/badge.svg" alt="Build status"></a>
<a href="https://crates.io/crates/jaq-interpret" rel="nofollow"><img src="https://camo.githubusercontent.com/3c79938a5952cfb706aa084332dcb2870ba5c86dbe328e6b03a816ba928ea806/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f6a61712d696e746572707265742e737667" alt="Crates.io" data-canonical-src="https://img.shields.io/crates/v/jaq-interpret.svg"></a>
<a href="https://docs.rs/jaq-interpret" rel="nofollow"><img src="https://camo.githubusercontent.com/7c9ab2bea037ce46569ecf5370070f9dad63ff1c40cf5cef41ba8f77ad6bd436/68747470733a2f2f646f63732e72732f6a61712d696e746572707265742f62616467652e737667" alt="Documentation" data-canonical-src="https://docs.rs/jaq-interpret/badge.svg"></a>
<a href="https://www.rust-lang.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/9595c6fea652a42b90d506e8670efde9cdb1ca0e086b154d202abd3588162bc8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f727573742d312e36342b2d6f72616e67652e737667" alt="Rust 1.64+" data-canonical-src="https://img.shields.io/badge/rust-1.64+-orange.svg"></a></p>
<p dir="auto">jaq (pronounced like <em>Jacques</em><sup><a href="#user-content-fn-jacques-ae9996a28418554354c9e349ca982c3b" id="user-content-fnref-jacques-ae9996a28418554354c9e349ca982c3b" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>) is a clone of the JSON data processing tool <a href="https://jqlang.github.io/jq/" rel="nofollow">jq</a>.
jaq aims to support a large subset of jq's syntax and operations.</p>
<p dir="auto">jaq focuses on three goals:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Correctness</strong>:
jaq aims to provide a more correct and predictable implementation of jq,
while preserving compatibility with jq in most cases.</p>
<details><summary>Examples of surprising jq behaviour</summary>
<ul dir="auto">
<li><code>nan &gt; nan</code> is false, while <code>nan &lt; nan</code> is true.</li>
<li><code>[[]] | implode</code> crashes jq, and this was not fixed at the time of writing despite
<a href="https://github.com/jqlang/jq/issues/1160" data-hovercard-type="issue" data-hovercard-url="/jqlang/jq/issues/1160/hovercard">being known since five years</a>.</li>
<li>The <a href="https://jqlang.github.io/jq/manual/v1.6/" rel="nofollow">jq manual</a> claims that <code>limit(n; exp)</code> "extracts up to <code>n</code> outputs from <code>exp</code>".
This holds for values of <code>n &gt; 1</code>, e.g. <code>jq -n '[limit(2; 1, 2, 3)]'</code> yields
<code>[1, 2]</code>, but when <code>n == 0</code>, <code>jq -n '[limit(0; 1, 2, 3)]'</code> yields <code>[1]</code> instead of <code>[]</code>.
And perhaps even worse, when <code>n &lt; 0</code>, then <code>limit</code> yields <em>all</em> outputs from <code>exp</code>,
which is not documented.</li>
</ul>
</details>
</li>
<li>
<p dir="auto"><strong>Performance</strong>:
I created jaq originally because I was bothered by
<a href="https://github.com/jqlang/jq/issues/1411" data-hovercard-type="issue" data-hovercard-url="/jqlang/jq/issues/1411/hovercard">jq's long start-up time</a>,
which amounts to about 50ms on my machine.
This can particularly show when processing a large number of small files.
jaq starts up about 30 times faster than jq 1.6 and
<a href="#performance">outperforms jq also on many other benchmarks</a>.</p>
</li>
<li>
<p dir="auto"><strong>Simplicity</strong>:
jaq aims to have a simple and small implementation, in order to
reduce the potential for bugs and to
facilitate contributions.</p>
</li>
</ul>
<p dir="auto">I drew inspiration from another Rust program, namely <a href="https://github.com/yamafaktory/jql">jql</a>.
However, unlike jql, jaq aims to closely imitate jq's syntax and semantics.
This should allow users proficient in jq to easily use jaq.</p>
<h2 tabindex="-1" dir="auto">Installation</h2>
<h2 tabindex="-1" dir="auto">From Source</h2>
<p dir="auto">To compile jaq, you need a Rust toolchain.
See <a href="https://rustup.rs/" rel="nofollow">https://rustup.rs/</a> for instructions.
(Note that Rust compilers shipped with Linux distributions
may be too outdated to compile jaq.)</p>
<p dir="auto">Any of the following commands install jaq:</p>
<div data-snippet-clipboard-copy-content="$ cargo install --locked jaq
$ cargo install --locked --git https://github.com/01mf02/jaq # latest development version"><pre><code>$ cargo install --locked jaq
$ cargo install --locked --git https://github.com/01mf02/jaq # latest development version
</code></pre></div>
<p dir="auto">On my system, both commands place the executable at <code>~/.cargo/bin/jaq</code>.</p>
<p dir="auto">If you have cloned this repository, you can also build jaq by executing one of the commands in the cloned repository:</p>
<div data-snippet-clipboard-copy-content="$ cargo build --release # places binary into target/release/jaq
$ cargo install --locked --path jaq # installs binary"><pre><code>$ cargo build --release # places binary into target/release/jaq
$ cargo install --locked --path jaq # installs binary
</code></pre></div>
<p dir="auto">jaq should work on any system supported by Rust.
If it does not, please file an issue.</p>
<h2 tabindex="-1" dir="auto">Binaries</h2>
<p dir="auto">You may also install jaq using <a href="https://formulae.brew.sh/formula/jaq" rel="nofollow">homebrew</a> on macOS or Linux:</p>
<div data-snippet-clipboard-copy-content="$ brew install jaq
$ brew install --HEAD jaq # latest development version"><pre><code>$ brew install jaq
$ brew install --HEAD jaq # latest development version
</code></pre></div>
<h2 tabindex="-1" dir="auto">Examples</h2>
<p dir="auto">The following examples should give an impression of what jaq can currently do.
You should obtain the same outputs by replacing jaq with jq.
If not, your filing an issue would be appreciated. :)
The syntax is documented in the <a href="https://jqlang.github.io/jq/manual/v1.6/" rel="nofollow">jq manual</a>.</p>
<p dir="auto">Access a field:</p>
<div data-snippet-clipboard-copy-content="$ echo '{&quot;a&quot;: 1, &quot;b&quot;: 2}' | jaq '.a'
1"><pre><code>$ echo '{"a": 1, "b": 2}' | jaq '.a'
1
</code></pre></div>
<p dir="auto">Add values:</p>
<div data-snippet-clipboard-copy-content="$ echo '{&quot;a&quot;: 1, &quot;b&quot;: 2}' | jaq 'add'
3"><pre><code>$ echo '{"a": 1, "b": 2}' | jaq 'add'
3
</code></pre></div>
<p dir="auto">Construct an array from an object in two ways and show that they are equal:</p>
<div data-snippet-clipboard-copy-content="$ echo '{&quot;a&quot;: 1, &quot;b&quot;: 2}' | jaq '[.a, .b] == [.[]]'
true"><pre><code>$ echo '{"a": 1, "b": 2}' | jaq '[.a, .b] == [.[]]'
true
</code></pre></div>
<p dir="auto">Apply a filter to all elements of an array and filter the results:</p>
<div data-snippet-clipboard-copy-content="$ echo '[0, 1, 2, 3]' | jaq 'map(.*2) | [.[] | select(. < 5)]'
[0, 2, 4]"><pre><code>$ echo '[0, 1, 2, 3]' | jaq 'map(.*2) | [.[] | select(. &lt; 5)]'
[0, 2, 4]
</code></pre></div>
<p dir="auto">Read (slurp) input values into an array and get the average of its elements:</p>
<div data-snippet-clipboard-copy-content="$ echo '1 2 3 4' | jaq -s 'add / length'
2.5"><pre><code>$ echo '1 2 3 4' | jaq -s 'add / length'
2.5
</code></pre></div>
<p dir="auto">Repeatedly apply a filter to itself and output the intermediate results:</p>
<div data-snippet-clipboard-copy-content="$ echo '0' | jaq '[recurse(.+1; . < 3)]'
[0, 1, 2]"><pre><code>$ echo '0' | jaq '[recurse(.+1; . &lt; 3)]'
[0, 1, 2]
</code></pre></div>
<p dir="auto">Lazily fold over inputs and output intermediate results:</p>
<div data-snippet-clipboard-copy-content="$ seq 1000 | jaq -n 'foreach inputs as $x (0; . + $x)'
1 3 6 10 15 [...]"><pre><code>$ seq 1000 | jaq -n 'foreach inputs as $x (0; . + $x)'
1 3 6 10 15 [...]
</code></pre></div>
<h2 tabindex="-1" dir="auto">Performance</h2>
<p dir="auto">The following evaluation consists of several benchmarks that
allow comparing the performance of jaq, jq, and <a href="https://github.com/itchyny/gojq">gojq</a>.
The <code>empty</code> benchmark runs <code>n</code> times the filter <code>empty</code> with null input,
serving to measure the startup time.
The <code>bf-fib</code> benchmark runs a Brainfuck interpreter written in jq,
interpreting a Brainfuck script that produces <code>n</code> Fibonacci numbers.
The other benchmarks evaluate various filters with <code>n</code> as input;
see <a href="https://github.com/01mf02/jaq/blob/main/bench.sh"><code>bench.sh</code></a> for details.</p>
<p dir="auto">I generated the benchmark data with
<code>bench.sh target/release/jaq jq-1.7 gojq-0.12.13 jq-1.6 | tee bench.json</code>
on a Linux system with an AMD Ryzen 5 5500U.<sup><a href="#user-content-fn-binaries-ae9996a28418554354c9e349ca982c3b" id="user-content-fnref-binaries-ae9996a28418554354c9e349ca982c3b" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>
I then processed the results with a "one-liner" (stretching the term and the line a bit):</p>
<div data-snippet-clipboard-copy-content="jq -rs '.[] | &quot;|`\(.name)`|\(.n)|&quot; + ([.time[] | min | (.*1000|round)? // &quot;N/A&quot;] | min as $total_min | map(if . == $total_min then &quot;**\(.)**&quot; else &quot;\(.)&quot; end) | join(&quot;|&quot;))' bench.json"><pre><code>jq -rs '.[] | "|`\(.name)`|\(.n)|" + ([.time[] | min | (.*1000|round)? // "N/A"] | min as $total_min | map(if . == $total_min then "**\(.)**" else "\(.)" end) | join("|"))' bench.json
</code></pre></div>
<p dir="auto">(Of course, you can also use jaq here instead of jq.)
Finally, I concatenated the table header with the output and piped it through <code>pandoc -t gfm</code>.</p>
<p dir="auto">Table: Evaluation results in milliseconds ("N/A" if more than 10 seconds).</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>n</th>
<th>jaq-1.2</th>
<th>jq-1.7</th>
<th>gojq-0.12.13</th>
<th>jq-1.6</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>empty</code></td>
<td>512</td>
<td><strong>650</strong></td>
<td>790</td>
<td>740</td>
<td>8340</td>
</tr>
<tr>
<td><code>bf-fib</code></td>
<td>13</td>
<td><strong>410</strong></td>
<td>1280</td>
<td>820</td>
<td>1420</td>
</tr>
<tr>
<td><code>reverse</code></td>
<td>1048576</td>
<td><strong>60</strong></td>
<td>680</td>
<td>310</td>
<td>630</td>
</tr>
<tr>
<td><code>sort</code></td>
<td>1048576</td>
<td><strong>140</strong></td>
<td>530</td>
<td>600</td>
<td>670</td>
</tr>
<tr>
<td><code>group-by</code></td>
<td>1048576</td>
<td><strong>420</strong></td>
<td>1850</td>
<td>1680</td>
<td>2830</td>
</tr>
<tr>
<td><code>min-max</code></td>
<td>1048576</td>
<td><strong>220</strong></td>
<td>320</td>
<td>290</td>
<td>310</td>
</tr>
<tr>
<td><code>add</code></td>
<td>1048576</td>
<td><strong>480</strong></td>
<td>650</td>
<td>1540</td>
<td>750</td>
</tr>
<tr>
<td><code>kv</code></td>
<td>131072</td>
<td>160</td>
<td><strong>150</strong></td>
<td>250</td>
<td>200</td>
</tr>
<tr>
<td><code>kv-update</code></td>
<td>131072</td>
<td><strong>190</strong></td>
<td>530</td>
<td>570</td>
<td>N/A</td>
</tr>
<tr>
<td><code>kv-entries</code></td>
<td>131072</td>
<td><strong>580</strong></td>
<td>1170</td>
<td>820</td>
<td>1110</td>
</tr>
<tr>
<td><code>ex-implode</code></td>
<td>1048576</td>
<td><strong>460</strong></td>
<td>1110</td>
<td>740</td>
<td>1080</td>
</tr>
<tr>
<td><code>reduce</code></td>
<td>1048576</td>
<td><strong>740</strong></td>
<td>880</td>
<td>N/A</td>
<td>850</td>
</tr>
<tr>
<td><code>try-catch</code></td>
<td>1048576</td>
<td><strong>180</strong></td>
<td>330</td>
<td>480</td>
<td>650</td>
</tr>
<tr>
<td><code>tree-flatten</code></td>
<td>17</td>
<td>650</td>
<td>360</td>
<td><strong>0</strong></td>
<td>480</td>
</tr>
<tr>
<td><code>tree-update</code></td>
<td>17</td>
<td><strong>450</strong></td>
<td>980</td>
<td>1850</td>
<td>1180</td>
</tr>
<tr>
<td><code>tree-paths</code></td>
<td>17</td>
<td>450</td>
<td><strong>380</strong></td>
<td>920</td>
<td>470</td>
</tr>
<tr>
<td><code>to-fromjson</code></td>
<td>65536</td>
<td><strong>40</strong></td>
<td>370</td>
<td>100</td>
<td>380</td>
</tr>
<tr>
<td><code>ack</code></td>
<td>7</td>
<td><strong>570</strong></td>
<td>680</td>
<td>1090</td>
<td>610</td>
</tr>
<tr>
<td><code>range-prop</code></td>
<td>128</td>
<td><strong>260</strong></td>
<td>310</td>
<td>320</td>
<td>580</td>
</tr>
</tbody>
</table>
<p dir="auto">The results show that
jaq-1.2 is fastest on 16 benchmarks, whereas
jq-1.7 is fastest on 2 benchmarks and
gojq-0.12.13 is fastest on 1 benchmark.
gojq is much faster on <code>tree-flatten</code> because it implements the filter <code>flatten</code> natively instead of by definition.</p>
<h2 tabindex="-1" dir="auto">Features</h2>
<p dir="auto">Here is an overview that summarises:</p>
<ul>
<li> features already implemented, and</li>
<li> features not yet implemented.</li>
</ul>
<p dir="auto"><a href="#contributing">Contributions to extend jaq are highly welcome.</a></p>
<h2 tabindex="-1" dir="auto">Basics</h2>
<ul>
<li> Identity (<code>.</code>)</li>
<li> Recursion (<code>..</code>)</li>
<li> Basic data types (null, boolean, number, string, array, object)</li>
<li> if-then-else (<code>if .a &lt; .b then .a else .b end</code>)</li>
<li> Folding (<code>reduce .[] as $x (0; . + $x)</code>, <code>foreach .[] as $x (0; . + $x; . + .)</code>)</li>
<li> Error handling (<code>try ... catch ...</code>) (see the <a href="#error-handling">differences from jq</a>)</li>
<li> String interpolation (<code>"The successor of \(.) is \(.+1)."</code>)</li>
<li> Format strings (<code>@json</code>, <code>@text</code>, <code>@csv</code>, <code>@tsv</code>, <code>@html</code>, <code>@sh</code>, <code>@base64</code>, <code>@base64d</code>)</li>
</ul>
<h2 tabindex="-1" dir="auto">Paths</h2>
<ul>
<li> Indexing of arrays/objects (<code>.[0]</code>, <code>.a</code>, <code>.["a"]</code>)</li>
<li> Iterating over arrays/objects (<code>.[]</code>)</li>
<li> Optional indexing/iteration (<code>.a?</code>, <code>.[]?</code>)</li>
<li> Array slices (<code>.[3:7]</code>, <code>.[0:-1]</code>)</li>
<li> String slices</li>
</ul>
<h2 tabindex="-1" dir="auto">Operators</h2>
<ul>
<li> Composition (<code>|</code>)</li>
<li> Binding (<code>. as $x | $x</code>)</li>
<li> Concatenation (<code>,</code>)</li>
<li> Plain assignment (<code>=</code>)</li>
<li> Update assignment (<code>|=</code>, <code>+=</code>, <code>-=</code>)</li>
<li> Alternation (<code>//</code>)</li>
<li> Logic (<code>or</code>, <code>and</code>)</li>
<li> Equality and comparison (<code>.a == .b</code>, <code>.a &lt; .b</code>)</li>
<li> Arithmetic (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>%</code>)</li>
<li> Negation (<code>-</code>)</li>
<li> Error suppression (<code>?</code>)</li>
</ul>
<h2 tabindex="-1" dir="auto">Definitions</h2>
<ul>
<li> Basic definitions (<code>def map(f): [.[] | f];</code>)</li>
<li> Recursive definitions (<code>def r: r; r</code>)</li>
</ul>
<h2 tabindex="-1" dir="auto">Core filters</h2>
<ul>
<li> Empty (<code>empty</code>)</li>
<li> Errors (<code>error</code>)</li>
<li> Input (<code>inputs</code>)</li>
<li> Length (<code>length</code>, <code>utf8bytelength</code>)</li>
<li> Rounding (<code>floor</code>, <code>round</code>, <code>ceil</code>)</li>
<li> String &lt;-&gt; JSON (<code>fromjson</code>, <code>tojson</code>)</li>
<li> String &lt;-&gt; integers (<code>explode</code>, <code>implode</code>)</li>
<li> String normalisation (<code>ascii_downcase</code>, <code>ascii_upcase</code>)</li>
<li> String prefix/postfix (<code>startswith</code>, <code>endswith</code>, <code>ltrimstr</code>, <code>rtrimstr</code>)</li>
<li> String splitting (<code>split("foo")</code>)</li>
<li> Array filters (<code>reverse</code>, <code>sort</code>, <code>sort_by(-.)</code>, <code>group_by</code>, <code>min_by</code>, <code>max_by</code>)</li>
<li> Stream consumers (<code>first</code>, <code>last</code>, <code>range</code>, <code>fold</code>)</li>
<li> Stream generators (<code>range</code>, <code>recurse</code>)</li>
<li> Time (<code>now</code>, <code>fromdateiso8601</code>, <code>todateiso8601</code>)</li>
<li> More numeric filters (<code>sqrt</code>, <code>sin</code>, <code>log</code>, <code>pow</code>, ...) (<a href="#numeric-filters">list of numeric filters</a>)</li>
<li> More time filters (<code>strptime</code>, <code>strftime</code>, <code>strflocaltime</code>, <code>mktime</code>, <code>gmtime</code>, and <code>localtime</code>)</li>
</ul>
<h2 tabindex="-1" dir="auto">Standard filters</h2>
<p dir="auto">These filters are defined via more basic filters.
Their definitions are at <a href="https://github.com/01mf02/jaq/blob/main/jaq-std/src/std.jq"><code>std.jq</code></a>.</p>
<ul>
<li> Undefined (<code>null</code>)</li>
<li> Booleans (<code>true</code>, <code>false</code>, <code>not</code>)</li>
<li> Special numbers (<code>nan</code>, <code>infinite</code>, <code>isnan</code>, <code>isinfinite</code>, <code>isfinite</code>, <code>isnormal</code>)</li>
<li> Type (<code>type</code>)</li>
<li> Filtering (<code>select(. &gt;= 0)</code>)</li>
<li> Selection (<code>values</code>, <code>nulls</code>, <code>booleans</code>, <code>numbers</code>, <code>strings</code>, <code>arrays</code>, <code>objects</code>, <code>iterables</code>, <code>scalars</code>)</li>
<li> Conversion (<code>tostring</code>, <code>tonumber</code>)</li>
<li> Iterable filters (<code>map(.+1)</code>, <code>map_values(.+1)</code>, <code>add</code>, <code>join("a")</code>)</li>
<li> Array filters (<code>transpose</code>, <code>first</code>, <code>last</code>, <code>nth(10)</code>, <code>flatten</code>, <code>min</code>, <code>max</code>)</li>
<li> Object-array conversion (<code>to_entries</code>, <code>from_entries</code>, <code>with_entries</code>)</li>
<li> Universal/existential (<code>all</code>, <code>any</code>)</li>
<li> Recursion (<code>walk</code>)</li>
<li> I/O (<code>input</code>)</li>
<li> Regular expressions (<code>test</code>, <code>scan</code>, <code>match</code>, <code>capture</code>, <code>splits</code>, <code>sub</code>, <code>gsub</code>)</li>
<li> Time (<code>fromdate</code>, <code>todate</code>)</li>
</ul>
<h2 tabindex="-1" dir="auto">Numeric filters</h2>
<p dir="auto">jaq imports many filters from <a href="https://crates.io/crates/libm" rel="nofollow">libm</a>
and follows their type signature.</p>
<details><summary>Full list of numeric filters defined in jaq</summary>
<p dir="auto">Zero-argument filters:</p>
<ul>
<li> <code>acos</code></li>
<li> <code>acosh</code></li>
<li> <code>asin</code></li>
<li> <code>asinh</code></li>
<li> <code>atan</code></li>
<li> <code>atanh</code></li>
<li> <code>cbrt</code></li>
<li> <code>cos</code></li>
<li> <code>cosh</code></li>
<li> <code>erf</code></li>
<li> <code>erfc</code></li>
<li> <code>exp</code></li>
<li> <code>exp10</code></li>
<li> <code>exp2</code></li>
<li> <code>expm1</code></li>
<li> <code>fabs</code></li>
<li> <code>frexp</code>, which returns pairs of (float, integer).</li>
<li> <code>ilogb</code>, which returns integers.</li>
<li> <code>j0</code></li>
<li> <code>j1</code></li>
<li> <code>lgamma</code></li>
<li> <code>log</code></li>
<li> <code>log10</code></li>
<li> <code>log1p</code></li>
<li> <code>log2</code></li>
<li> <code>logb</code></li>
<li> <code>modf</code>, which returns pairs of (float, float).</li>
<li> <code>nearbyint</code></li>
<li> <code>pow10</code></li>
<li> <code>rint</code></li>
<li> <code>significand</code></li>
<li> <code>sin</code></li>
<li> <code>sinh</code></li>
<li> <code>sqrt</code></li>
<li> <code>tan</code></li>
<li> <code>tanh</code></li>
<li> <code>tgamma</code></li>
<li> <code>trunc</code></li>
<li> <code>y0</code></li>
<li> <code>y1</code></li>
</ul>
<p dir="auto">Two-argument filters that ignore <code>.</code>:</p>
<ul>
<li> <code>atan2</code></li>
<li> <code>copysign</code></li>
<li> <code>drem</code></li>
<li> <code>fdim</code></li>
<li> <code>fmax</code></li>
<li> <code>fmin</code></li>
<li> <code>fmod</code></li>
<li> <code>hypot</code></li>
<li> <code>jn</code>, which takes an integer as first argument.</li>
<li> <code>ldexp</code>, which takes an integer as second argument.</li>
<li> <code>nextafter</code></li>
<li> <code>nexttoward</code></li>
<li> <code>pow</code></li>
<li> <code>remainder</code></li>
<li> <code>scalb</code></li>
<li> <code>scalbln</code>, which takes as integer as second argument.</li>
<li> <code>yn</code>, which takes an integer as first argument.</li>
</ul>
<p dir="auto">Three-argument filters that ignore <code>.</code>:</p>
<ul>
<li> <code>fma</code></li>
</ul>
</details>
<h2 tabindex="-1" dir="auto">Advanced features</h2>
<p dir="auto">jaq currently does <em>not</em> aim to support several features of jq, such as:</p>
<ul dir="auto">
<li>Modules</li>
<li>SQL-style operators</li>
<li>Streaming</li>
</ul>
<h2 tabindex="-1" dir="auto">Differences between jq and jaq</h2>
<h2 tabindex="-1" dir="auto">Numbers</h2>
<p dir="auto">jq uses 64-bit floating-point numbers (floats) for any number.
By contrast, jaq interprets
numbers such as 0   or -42 as machine-sized integers and
numbers such as 0.0 or 3e8 as 64-bit floats.
Many operations in jaq, such as array indexing,
check whether the passed numbers are indeed integer.
The motivation behind this is to avoid
rounding errors that may silently lead to wrong results.
For example:</p>
<div data-snippet-clipboard-copy-content="$ jq  -n '[0, 1, 2] | .[1.0000000000000001]'
1
$ jaq -n '[0, 1, 2] | .[1.0000000000000001]'
Error: cannot use 1.0 as integer
$ jaq -n '[0, 1, 2] | .[1]'
1"><pre><code>$ jq  -n '[0, 1, 2] | .[1.0000000000000001]'
1
$ jaq -n '[0, 1, 2] | .[1.0000000000000001]'
Error: cannot use 1.0 as integer
$ jaq -n '[0, 1, 2] | .[1]'
1
</code></pre></div>
<p dir="auto">The rules of jaq are:</p>
<ul dir="auto">
<li>The sum, difference, product, and remainder of two integers is integer.</li>
<li>Any other operation between two numbers yields a float.</li>
</ul>
<p dir="auto">Examples:</p>
<div data-snippet-clipboard-copy-content="$ jaq -n '1 + 2'
3
$ jaq -n '10 / 2'
5.0
$ jaq -n '1.0 + 2'
3.0"><pre><code>$ jaq -n '1 + 2'
3
$ jaq -n '10 / 2'
5.0
$ jaq -n '1.0 + 2'
3.0
</code></pre></div>
<p dir="auto">You can convert an integer to a floating-point number e.g.
by adding 0.0, by multiplying with 1.0, or by dividing with 1.
You can convert a floating-point number to an integer by
<code>round</code>, <code>floor</code>, or <code>ceil</code>:</p>
<div data-snippet-clipboard-copy-content="$ jaq -n '1.2 | [floor, round, ceil]'
[1, 1, 2]"><pre><code>$ jaq -n '1.2 | [floor, round, ceil]'
[1, 1, 2]
</code></pre></div>
<h3 tabindex="-1" dir="auto">NaN and infinity</h3>
<p dir="auto">In jq, division by 0 has some surprising properties; for example,
<code>0 / 0</code> yields <code>nan</code>, whereas
<code>0 as $n | $n / 0</code> yields an error.
In jaq, <code>n / 0</code> yields <code>nan</code> if <code>n == 0</code>, <code>infinite</code> if <code>n &gt; 0</code>, and <code>-infinite</code> if <code>n &lt; 0</code>.
jaq's behaviour is closer to the IEEE standard for floating-point arithmetic (IEEE 754).</p>
<p dir="auto">jaq implements a total ordering on floating-point numbers to allow sorting values.
Therefore, it unfortunately has to enforce that <code>nan == nan</code>.
(jq gets around this by enforcing <code>nan &lt; nan</code>, which breaks basic laws about total orders.)</p>
<p dir="auto">Like jq, jaq prints <code>nan</code> and <code>infinite</code> as <code>null</code> in JSON,
because JSON does not support encoding these values as numbers.</p>
<h3 tabindex="-1" dir="auto">Preservation of fractional numbers</h3>
<p dir="auto">jaq preserves fractional numbers coming from JSON data perfectly
(as long as they are not used in some arithmetic operation),
whereas jq 1.6 may silently convert to 64-bit floating-point numbers:</p>
<div data-snippet-clipboard-copy-content="$ echo '1e500' | jq '.'
1.7976931348623157e+308
$ echo '1e500' | jaq '.'
1e500"><pre><code>$ echo '1e500' | jq '.'
1.7976931348623157e+308
$ echo '1e500' | jaq '.'
1e500
</code></pre></div>
<p dir="auto">Therefore, unlike jq 1.6, jaq satisfies the following paragraph in the <a href="https://jqlang.github.io/jq/manual/v1.6/" rel="nofollow">jq manual</a>:</p>
<blockquote>
<p dir="auto">An important point about the identity filter is that
it guarantees to preserve the literal decimal representation of values.
This is particularly important when dealing with numbers which can't be
losslessly converted to an IEEE754 double precision representation.</p>
</blockquote>
<p dir="auto">Please note that newer versions of jq, e.g. 1.7,
seem to preserve the literal decimal representation as well.</p>
<h2 tabindex="-1" dir="auto">Assignments</h2>
<p dir="auto">Like jq, jaq allows for assignments of the form <code>p |= f</code>.
However, jaq interprets these assignments differently.
Fortunately, in most cases, the result is the same.</p>
<p dir="auto">In jq, an assignment <code>p |= f</code> first constructs paths to all values that match <code>p</code>.
<em>Only then</em>, it applies the filter <code>f</code> to these values.</p>
<p dir="auto">In jaq, an assignment <code>p |= f</code> applies <code>f</code> <em>immediately</em> to any value matching <code>p</code>.
Unlike in jq, assignment does not explicitly construct paths.</p>
<p dir="auto">jaq's implementation of assignment likely yields higher performance,
because it does not construct paths.
Furthermore, this also prevents several bugs in jq "by design".
For example, given the filter <code>[0, 1, 2, 3] | .[] |= empty</code>,
jq  yields <code>[1, 3]</code>, whereas
jaq yields <code>[]</code>.
What happens here?</p>
<p dir="auto">jq first constructs the paths corresponding to <code>.[]</code>, which are <code>.0, .1, .2, .3</code>.
Then, it removes the element at each of these paths.
However, each of these removals <em>changes</em> the value that the remaining paths refer to.
That is, after removing <code>.0</code> (value 0), <code>.1</code> does not refer to value 1, but value 2!
That is also why value 1 (and in consequence also value 3) is not removed.</p>
<p dir="auto">There is more weirdness ahead in jq;
for example, <code>0 | 0 |= .+1</code> yields <code>1</code> in jq,
although <code>0</code> is not a valid path expression.
However, <code>1 | 0 |= .+1</code> yields an error.
In jaq, any such assignment yields an error.</p>
<p dir="auto">jaq attempts to use multiple outputs of the right-hand side, whereas
jq uses only the first.
For example, <code>0 | (., .) |= (., .+1)</code> yields <code>0 1 1 2</code> in jaq,
whereas it yields only <code>0</code> in jq.
However, <code>{a: 1} | .a |= (2, 3)</code> yields <code>{"a": 2}</code> in both jaq and jq,
because an object can only associate a single value with any given key,
so we cannot use multiple outputs in a meaningful way here.</p>
<p dir="auto">Because jaq does not construct paths,
it does not allow some filters on the left-hand side of assignments,
for example <code>first</code>, <code>last</code>, <code>limit</code>:
For example, <code>[1, 2, 3] | first(.[]) |= .-1</code>
yields <code>[0, 2, 3]</code> in jq, but is invalid in jaq.
Similarly, <code>[1, 2, 3] | limit(2; .[]) |= .-1</code>
yields <code>[0, 1, 3]</code> in jq, but is invalid in jaq.
(Inconsequentially, jq also does not allow for <code>last</code>.)</p>
<h2 tabindex="-1" dir="auto">Definitions</h2>
<p dir="auto">Like jq, jaq allows for the definition of filters, such as:</p>

<p dir="auto">Arguments can also be passed <em>by value</em>, such as:</p>
<div data-snippet-clipboard-copy-content="def cartesian($f; $g): [$f, $g];"><pre><code>def cartesian($f; $g): [$f, $g];
</code></pre></div>
<p dir="auto">Filter definitions can be nested and recursive, i.e. refer to themselves.
That is, a filter such as <code>recurse</code> can be defined in jaq:</p>
<div data-snippet-clipboard-copy-content="def recurse(f): def r: ., (f | r); r;"><pre><code>def recurse(f): def r: ., (f | r); r;
</code></pre></div>
<p dir="auto">Since jaq 1.2, jaq optimises tail calls, like jq.
Since jaq 1.1, recursive filters can also have non-variable arguments, like in jq.
For example:</p>

<p dir="auto">Recursive filters with non-variable arguments can yield surprising effects;
for example, a call <code>f(0)</code> builds up calls of the shape <code>f(1+(..(1+0)...))</code>,
which leads to exponential execution times.</p>
<p dir="auto">Recursive filters with non-variable arguments can
very frequently be alternatively implemented by either:</p>
<ul dir="auto">
<li>A nested filter: for example, instead of
<code>def walk(f): (.[]? |= walk(f)) | f;</code>, you can use
<code>def walk(f): def rec: (.[]? |= rec) | f; rec;</code>.</li>
<li>A filter with variable arguments: for example, instead of
<code>def f(a): a, f(1+a);</code>, you can equally well write
<code>def f($a): $a, f(1+$a);</code>.</li>
<li>A filter with <code>recurse</code>: for example, you may write
<code>def f(a): a | recurse(1+.);</code>.
If you expect your filter to recurse deeply,
it is advised to implement it using <code>recurse</code>,
because jaq has an optimised implementation of <code>recurse</code>.</li>
</ul>
<p dir="auto">All of these options are supported by jaq.</p>
<h2 tabindex="-1" dir="auto">Arguments</h2>
<p dir="auto">Like jq, jaq allows to define arguments via the command line,
in particular by the options <code>--arg</code>, <code>--rawfile</code>, <code>--slurpfile</code>.
This binds variables to values, and
for every variable <code>$x</code> bound to <code>v</code> this way,
<code>$ARGS.named</code> contains an entry with key <code>x</code> and value <code>v</code>.
For example:</p>
<div data-snippet-clipboard-copy-content="$ jaq -n --arg x 1 --arg y 2 '$x, $y, $ARGS.named'
&quot;1&quot;
&quot;2&quot;
{
  &quot;x&quot;: &quot;1&quot;,
  &quot;y&quot;: &quot;2&quot;
}"><pre><code>$ jaq -n --arg x 1 --arg y 2 '$x, $y, $ARGS.named'
"1"
"2"
{
  "x": "1",
  "y": "2"
}
</code></pre></div>
<h2 tabindex="-1" dir="auto">Folding</h2>
<p dir="auto">jq and jaq provide filters
<code>reduce xs as $x (init; f)</code> and
<code>foreach xs as $x (init; f)</code>.</p>
<p dir="auto">In jaq, the output of these filters is defined very simply:
Assuming that <code>xs</code> evaluates to <code>x0</code>, <code>x1</code>, ..., <code>xn</code>,
<code>reduce xs as $x (init; f)</code> evaluates to</p>
<div data-snippet-clipboard-copy-content="init
| x0 as $x | f
| ...
| xn as $x | f"><pre><code>init
| x0 as $x | f
| ...
| xn as $x | f
</code></pre></div>
<p dir="auto">and <code>foreach xs as $x (init; f)</code> evaluates to</p>
<div data-snippet-clipboard-copy-content="init
| x0 as $x | f | (.,
| ...
| xn as $x | f | (.,
empty)...)"><pre lang="text"><code>init
| x0 as $x | f | (.,
| ...
| xn as $x | f | (.,
empty)...)
</code></pre></div>
<p dir="auto">Additionally, jaq provides the filter <code>for xs as $x (init; f)</code> that evaluates to</p>
<div data-snippet-clipboard-copy-content="init
| ., (x0 as $x | f
| ...
| ., (xn as $x | f
)...)"><pre lang="text"><code>init
| ., (x0 as $x | f
| ...
| ., (xn as $x | f
)...)
</code></pre></div>
<p dir="auto">The difference between <code>foreach</code> and <code>for</code> is that
<code>for</code> yields the output of <code>init</code>, whereas <code>foreach</code> omits it.
For example,
<code>foreach (1, 2, 3) as $x (0; .+$x)</code> yields <code>1, 3, 6</code>, whereas
<code>for (1, 2, 3) as $x (0; .+$x)</code> yields <code>0, 1, 3, 6</code>.</p>
<p dir="auto">The interpretation of <code>reduce</code>/<code>foreach</code> in jaq has the following advantages over jq:</p>
<ul dir="auto">
<li>It deals very naturally with filters that yield multiple outputs.
In contrast, jq discriminates outputs of <code>f</code>,
because it recurses only on the last of them,
although it outputs all of them.
<details><summary>Example</summary>
`foreach (5, 10) as $x (1; .+$x, -.)` yields
`6, -1, 9, 1` in jq, whereas it yields
`6, 16, -6, -1, 9, 1` in jaq.
We can see that both jq and jaq yield the values `6` and `-1`
resulting from the first iteration (where `$x` is 5), namely
`1 | 5 as $x | (.+$x, -.)`.
However, jq performs the second iteration (where `$x` is 10)
*only on the last value* returned from the first iteration, namely `-1`,
yielding the values `9` and `1` resulting from
`-1 | 10 as $x | (.+$x, -.)`.
jaq yields these values too, but it also performs the second iteration
on all other values returned from the first iteration, namely `6`,
yielding the values `16` and `-6` that result from
` 6 | 10 as $x | (.+$x, -.)`.
</details>
</li>
<li>It makes the implementation of <code>reduce</code> and <code>foreach</code>
special cases of the same code, reducing the potential for bugs.</li>
</ul>
<p dir="auto">Compared to <code>foreach ...</code>, the filter <code>for ...</code>
(where <code>...</code> refers to <code>xs as $x (init; f)</code>)
has a stronger relationship with <code>reduce</code>.
In particular,
the values yielded by <code>reduce ...</code> are a subset of
the values yielded by <code>for ...</code>.
This does not hold if you replace <code>for</code> by <code>foreach</code>.</p>
<details>
As an example, if we set `...` to `empty as $x (0; .+$x)`, then
`foreach ...` yields no value, whereas
`for ...` and `reduce ...` yield `0`.
</details>
<p dir="auto">Furthermore, jq provides the filter
<code>foreach xs as $x (init; f; proj)</code> (<code>foreach/3</code>) and interprets
<code>foreach xs as $x (init; f)</code> (<code>foreach/2</code>) as
<code>foreach xs as $x (init; f; .)</code>, whereas
jaq does <em>not</em> provide <code>foreach/3</code> because
it requires completely separate logic from <code>foreach/2</code> and <code>reduce</code>
in both the parser and the interpreter.</p>
<h2 tabindex="-1" dir="auto">Error handling</h2>
<p dir="auto">In jq, the <code>try f catch g</code> expression breaks out of the <code>f</code> stream as
soon as an error occurs, ceding control to <code>g</code> after that. This is
mentioned in its manual as a possible mechanism for breaking out of
loops
(<a href="https://jqlang.github.io/jq/manual/#breaking-out-of-control-structures" rel="nofollow">here</a>). jaq
however doesn't interrupt the <code>f</code> stream, but instead sends <em>each</em>
error value emitted to the <code>g</code> filter; the result is a stream of
values emitted from <code>f</code> with values emitted from <code>g</code> interspersed
where errors occurred.</p>
<p dir="auto">Consider the following example: this expression is <code>true</code> in jq,
because the first <code>error(2)</code> interrupts the stream:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[try (1, error(2), 3, error(4)) catch .] == [1, 2]"><pre>[<span>try</span> (<span>1</span>, <span>error</span>(<span>2</span>), <span>3</span>, <span>error</span>(<span>4</span>)) <span>catch</span> <span>.</span>] <span>==</span> [<span>1</span>, <span>2</span>]</pre></div>
<p dir="auto">In jaq however, this holds:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[try (1, error(2), 3, error(4)) catch .] == [1, 2, 3, 4]"><pre>[<span>try</span> (<span>1</span>, <span>error</span>(<span>2</span>), <span>3</span>, <span>error</span>(<span>4</span>)) <span>catch</span> <span>.</span>] <span>==</span> [<span>1</span>, <span>2</span>, <span>3</span>, <span>4</span>]</pre></div>
<h2 tabindex="-1" dir="auto">Miscellaneous</h2>
<ul dir="auto">
<li>Slurping: When files are slurped in (via the <code>-s</code> / <code>--slurp</code> option),
jq combines the inputs of all files into one single array, whereas
jaq yields an array for every file.
The behaviour of jq can be approximated in jaq;
for example, to achieve the output of
<code>jq -s . a b</code>, you may use
<code>jaq -s . &lt;(cat a b)</code>.</li>
<li>Cartesian products:
In jq, <code>[(1,2) * (3,4)]</code> yields <code>[3, 6, 4, 8]</code>, whereas
<code>[{a: (1,2), b: (3,4)} | .a * .b]</code> yields <code>[3, 4, 6, 8]</code>.
jaq yields <code>[3, 4, 6, 8]</code> in both cases.</li>
<li>List updating:
In jq, <code>[0, 1] | .[3] = 3</code> yields <code>[0, 1, null, 3]</code>; that is,
jq fills up the list with <code>null</code>s if we update beyond its size.
In contrast, jaq fails with an out-of-bounds error in such a case.</li>
<li>Input reading:
When there is no more input value left,
in jq, <code>input</code> yields an error, whereas in jaq, it yields no output value.</li>
<li>Joining:
When given an array <code>[x0, x1, ..., xn]</code>,
in jq, <code>join(x)</code> converts all elements of the input array to strings and intersperses them with <code>x</code>, whereas
in jaq, <code>join(x)</code> simply calculates <code>x0 + x + x1 + x + ... + xn</code>.
When all elements of the input array and <code>x</code> are strings, jq and jaq yield the same output.</li>
</ul>
<h2 tabindex="-1" dir="auto">Contributing</h2>
<p dir="auto">Contributions to jaq are welcome.
Please make sure that after your change, <code>cargo test</code> runs successfully.</p>
<h2 tabindex="-1" dir="auto">Acknowledgements</h2>
<p dir="auto">jaq has profited tremendously from:</p>
<ul dir="auto">
<li><a href="https://docs.rs/serde_json/" rel="nofollow">serde_json</a> to read and <a href="https://docs.rs/colored_json/" rel="nofollow">colored_json</a> to output JSON,</li>
<li><a href="https://docs.rs/chumsky/" rel="nofollow">chumsky</a> to parse and <a href="https://docs.rs/ariadne/" rel="nofollow">ariadne</a> to pretty-print parse errors,</li>
<li><a href="https://docs.rs/mimalloc/" rel="nofollow">mimalloc</a> to boost the performance of memory allocation, and</li>
<li>the Rust standard library, in particular its awesome <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html" rel="nofollow">Iterator</a>,
which builds the rock-solid base of jaq's filter execution</li>
</ul>
<section data-footnotes="">
<ol dir="auto">
<li id="user-content-fn-jacques-ae9996a28418554354c9e349ca982c3b">
<p dir="auto">I wanted to create a tool that should be discreet and obliging, like a good waiter.
And when I think of a typical name for a (French) waiter, to my mind comes "Jacques".
Later, I found out about the old French word <em>jacquet</em>, meaning "squirrel",
which makes for a nice <em>ex post</em> inspiration for the name. <a href="#user-content-fnref-jacques-ae9996a28418554354c9e349ca982c3b" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-binaries-ae9996a28418554354c9e349ca982c3b">
<p dir="auto">The binaries for jq-1.7 and gojq-0.12.13 were retrieved from their GitHub release pages,
the binary for jq-1.6 was installed from the standard Ubuntu repository. <a href="#user-content-fnref-binaries-ae9996a28418554354c9e349ca982c3b" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
</ol>
</section>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deno Cron (253 pts)]]></title>
            <link>https://deno.com/blog/cron</link>
            <guid>38461121</guid>
            <pubDate>Wed, 29 Nov 2023 16:03:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deno.com/blog/cron">https://deno.com/blog/cron</a>, See on <a href="https://news.ycombinator.com/item?id=38461121">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Building for the web is increasingly complex. Writing modern software includes
leveraging cloud infrastructure, dissecting boilerplate code, and managing
intricate configurations — when developers only want to focus on writing
business logic.</p>
<p>Deno aims to radically simplify web development by removing config and
unnecessary boilerplate. We’ve built <a href="https://deno.com/kv" rel="noopener noreferrer">Deno KV</a>, a serverless database, and
<a href="https://deno.com/blog/queues" rel="noopener noreferrer">Deno Queues</a>, a way to offload tasks or schedule future work,
right into the runtime, so adding them to your application only requires a few
lines of code.</p>
<p>Today, we’re thrilled to take another step in simplifying web development by
<strong>introducing Deno Cron, an easy way to create scheduled jobs</strong>:</p>
<div><pre><span>Deno</span><span>.</span><span>cron</span><span>(</span><span>"Sample cron job"</span><span>,</span> <span>"*/10 * * * *"</span><span>,</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>"This will run every 10 minutes"</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span></pre></div><p>In this post, we’ll go over:</p>
<ul>
<li><a href="#using-deno-cron">Using <code>Deno.cron</code></a></li>
<li><a href="#deno-cron-on-deno-deploy">Deno Cron on Deno Deploy</a></li>
<li><a href="#how-does-it-work-on-deno-deploy">How it works on Deno Deploy</a></li>
<li><a href="#whats-next">What’s next?</a></li>
</ul>
<figure>

<iframe width="100%" src="https://www.youtube.com/embed/DFVzxJtDYSs?si=4VqtSxs0Lmf6jl1f" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>

<figcaption>Learn some tips and tricks about Deno.cron() in the above YouTube video.
</figcaption>

</figure>

<h2 id="using-deno-cron">Using Deno Cron</h2><p><a href="https://docs.deno.com/kv/manual/cron" rel="noopener noreferrer"><code>Deno.cron()</code></a> (available behind the
<code>--unstable</code> flag as of <a href="https://deno.com/blog/v1.38" rel="noopener noreferrer">1.38</a>), is a function that takes three
parameters:</p>
<ul>
<li><code>name</code> , the name of your scheduled job</li>
<li><code>schedule</code> , which uses
<a href="https://www.ibm.com/docs/en/db2/11.5?topic=task-unix-cron-format" rel="noopener noreferrer">the Unix cron format</a>
and where the time is in the UTC timezone</li>
<li><code>handler</code>, a function that is executed on the schedule provided</li>
</ul>
<p>Unlike cron on UNIX/Linux, <strong>Deno Cron executions do not overlap</strong>. This means
that if you schedule something to run every 10 minutes, but the task takes 30
minutes to complete, Deno Cron automatically skips the next scheduled run until
the task is complete. Overlapping cron jobs can lead to unintended issues and
requires extra tedious logic to avoid, but Deno Cron side steps that completely.</p>
<p>We’re also working to
<a href="https://github.com/denoland/deno/pull/21340" rel="noopener noreferrer">support a JavaScript friendly API for specifying the cron schedule</a>.</p>
<h2 id="deno-cron-on-deno-deploy">Deno Cron on Deno Deploy</h2><p>On <a href="https://deno.com/deploy" rel="noopener noreferrer">Deno Deploy</a>, our multi-tenant distributed serverless JavaScript
platform, <code>Deno.cron()</code> is automatically detected and managed so you don’t need
to worry about anything.</p>
<p><strong>You can run cron jobs without a web server</strong> or even consistent incoming
requests to keep your isolate alive. That’s because whenever your project is
deployed, Deno Deploy automatically detects your cron jobs and evaluates them.
When its time for your <code>handler</code> to run, Deno Deploy automatically spins up an
isolate on-demand to run them.</p>
<figure>

<figcaption>
This code works on Deno Deploy. <a target="_blank" href="https://dash.deno.com/playground/cron-without-a-server">Check out the playground</a>.
</figcaption>
</figure>

<p>We’ve also added a <strong>new <code>Cron</code> tab in the Deno Deploy dashboard,</strong> which shows
all active cron jobs in your project:</p>
<figure>

<p><img src="https://deno.com/blog/cron/cron-tab-on-deploy.png" alt="The new Cron tab in your project settings will show your scheduled jobs detected by Deno Deploy." title=""></p>
<figcaption>
A new cron tab in your Project that shows your active cron jobs.
</figcaption>

</figure>

<figure>

<p><img src="https://deno.com/blog/cron/cron-logs-on-deploy.png" alt="Your cron jobs will also appear in your logs." title=""></p>
<figcaption>
Your cron jobs will appear in your logs.
</figcaption>
</figure>

<p>To modify or stop an existing cron, change your code and create a new
deployment. For example, if you remove a &nbsp;<code>Deno.cron</code>&nbsp;from your code&nbsp;and deploy
it, those jobs will no longer be scheduled to run.</p>
<p>Your Deno Cron handlers can perform all sorts of actions, such as updating state
in Deno KV, pinging a website, sending an email, initiating a database backup,
calling an API at regular intervals, and more.</p>
<h3 id="how-does-it-work-on-deno-deploy">How does it work on Deno Deploy?</h3><p>How exactly does Deno Deploy know there’s a <code>cron</code> in your code, even when
there’s no web server handling requests?</p>
<p>When a new production deployment of your project is created, an ephemeral V8
isolate is used to evaluate your project’s top-level scope and to discover any
<code>Deno.cron</code> definitions. A global cron scheduler is then updated with your
project’s latest cron definitions, which includes updates to your existing
crons, new crons, and deleted crons.</p>
<p>The global cron scheduler is a reliable service that’s responsible for
scheduling and dispatching cron jobs based on the specified schedule. During
dispatch, an on-demand v8 isolate is spun up to execute the job using the same
production deployment.</p>
<figure>

<figcaption>
These 24 lines of code creates a cron job to fetch weather data from a public API and adds it to Deno KV every hour. <a href="https://dash.deno.com/playground/deno-cron-weather" target="_blank">The web server endpoint prints out all of the timestamps and weather data.</a>
</figcaption>
</figure>

<p>Using Deno Cron on Deno Deploy makes it simple to create cron jobs and host them
in the cloud without any configuration in a matter of minutes.</p>
<h3 id="other-resources">Other resources</h3><ul>
<li><a href="https://docs.deno.com/kv/manual/cron" rel="noopener noreferrer">Cron docs</a></li>
<li><a href="https://docs.deno.com/kv/manual/cron#usage-on-deno-deploy" rel="noopener noreferrer">Cron on Deno Deploy docs</a></li>
<li><a href="https://deno.land/api?s=Deno.cron&amp;unstable=" rel="noopener noreferrer">API reference</a></li>
</ul>
<h2 id="whats-next">What’s next?</h2><p>Building a scalable business requires the ability to schedule jobs reliably and
easily. Deno Cron is a simple way to schedule jobs without unnecessary
configuration.</p>
<p>With Deno Cron, <a href="https://deno.com/kv" rel="noopener noreferrer">Deno KV</a>,
<a href="https://deno.com/blog/queues" rel="noopener noreferrer">Deno Queues</a>,
<a href="https://deno.com/blog/npm-on-deno-deploy" rel="noopener noreferrer">npm</a>, and
<a href="https://deno.land/manual/runtime/web_platform_apis" rel="noopener noreferrer">web standards APIs</a>,
building and launching a production-ready server on Deno is simpler and more
productive. But we are not there yet. We have a few more cloud primitives that
we hope to add to the runtime —&nbsp;stay tuned.</p>
<p><em><a href="https://news.ycombinator.com/item?id=38461121" rel="noopener noreferrer">Join the discussion on Hacker News.</a></em></p>
<blockquote>
<p>🚨️ We’re actively seeking feedback for Deno Cron 🚨️</p>
<p>If you’re using it (or plan to use it) in a commercial capacity and want to
connect directly to one of our engineers for technical support, please let us
know <a href="mailto:deploy@deno.com" rel="noopener noreferrer">via email</a> or
<a href="https://discord.gg/deno" rel="noopener noreferrer">Discord</a>.</p>
</blockquote>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Resume Matcher – An open source, free tool to improve your resume (138 pts)]]></title>
            <link>https://github.com/srbhr/Resume-Matcher</link>
            <guid>38461101</guid>
            <pubDate>Wed, 29 Nov 2023 16:01:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/srbhr/Resume-Matcher">https://github.com/srbhr/Resume-Matcher</a>, See on <a href="https://news.ycombinator.com/item?id=38461101">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a href="https://www.resumematcher.fyi/" rel="nofollow"><img src="https://github.com/srbhr/Resume-Matcher/raw/main/Assets/img/header_image.png" alt="Resume Matcher"></a></p>

<br>

<div dir="auto">
<p dir="auto"><strong>Don't let your resume be a roadblock from getting your next job. Use Resume Matcher!</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srbhr/Resume-Matcher/blob/main/Assets/img/Resume_Matcher_Gif.gif"><img src="https://github.com/srbhr/Resume-Matcher/raw/main/Assets/img/Resume_Matcher_Gif.gif" alt="Resume_Matcher_streamlit_demo" data-animated-image=""></a></p>
<h2 tabindex="-1" dir="auto">How does it work?</h2>
</div>
<p dir="auto">The Resume Matcher takes your resume and job descriptions as input, parses them using Python, and mimics the functionalities of an ATS, providing you with insights and suggestions to make your resume ATS-friendly.</p>
<p dir="auto">The process is as follows:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Parsing</strong>: The system uses Python to parse both your resume and the provided job description, just like an ATS would.</p>
</li>
<li>
<p dir="auto"><strong>Keyword Extraction</strong>: The tool uses advanced machine learning algorithms to extract the most relevant keywords from the job description. These keywords represent the skills, qualifications, and experiences the employer seeks.</p>
</li>
<li>
<p dir="auto"><strong>Key Terms Extraction</strong>: Beyond keyword extraction, the tool uses textacy to identify the main key terms or themes in the job description. This step helps in understanding the broader context of what the resume is about.</p>
</li>
<li>
<p dir="auto"><strong>Vector Similarity Using Qdrant</strong>: The tool uses <a href="https://github.com/qdrant/qdrant">Qdrant</a>, a highly efficient vector similarity search tool, to measure how closely your resume matches the job description. The more similar they are, the higher the likelihood that your resume will pass the ATS screening.</p>
</li>
</ol>

<p dir="auto">
<h2 tabindex="-1" dir="auto">How to install</h2>
</p>
<p dir="auto">Follow these steps to set up the environment and run the application.</p>
<ol dir="auto">
<li>
<p dir="auto">Fork the repository <a href="https://github.com/srbhr/Resume-Matcher/fork">here</a>.</p>
</li>
<li>
<p dir="auto">Clone the forked repository.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/<YOUR-USERNAME>/Resume-Matcher.git
cd Resume-Matcher"><pre>git clone https://github.com/<span>&lt;</span>YOUR-USERNAME<span>&gt;</span>/Resume-Matcher.git
<span>cd</span> Resume-Matcher</pre></div>
</li>
<li>
<p dir="auto">Create a Python Virtual Environment:</p>
<ul dir="auto">
<li>
<p dir="auto">Using <a href="https://learnpython.com/blog/how-to-use-virtualenv-python/" rel="nofollow">virtualenv</a>:</p>
<p dir="auto"><em>Note</em>: Check how to install virtualenv on your system here <a href="https://learnpython.com/blog/how-to-use-virtualenv-python/" rel="nofollow">link</a>.</p>

</li>
</ul>
<p dir="auto"><strong>OR</strong></p>
<ul dir="auto">
<li>
<p dir="auto">Create a Python Virtual Environment:</p>

</li>
</ul>
</li>
<li>
<p dir="auto">Activate the Virtual Environment.</p>
<ul dir="auto">
<li>
<p dir="auto">On Windows.</p>

</li>
<li>
<p dir="auto">On macOS and Linux.</p>

</li>
</ul>
<p dir="auto"><strong>OPTIONAL (For pyenv users)</strong></p>
<p dir="auto">Run the application with pyenv (Refer this <a href="https://realpython.com/intro-to-pyenv/#installing-pyenv" rel="nofollow">article</a>)</p>
<ul dir="auto">
<li>
<p dir="auto">Build dependencies (on ubuntu)</p>
<div data-snippet-clipboard-copy-content="sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python openssl"><pre><code>sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python openssl
</code></pre></div>
<div data-snippet-clipboard-copy-content="
sudo apt-get install build-essential zlib1g-dev libffi-dev libssl-dev libbz2-dev libreadline-dev libsqlite3-dev liblzma-dev libncurses-dev

sudo apt-get install python-tk python3-tk tk-dev

sudo apt-get install build-essential zlib1g-dev libffi-dev libssl-dev libbz2-dev libreadline-dev libsqlite3-dev liblzma-dev
"><pre><code>
sudo apt-get install build-essential zlib1g-dev libffi-dev libssl-dev libbz2-dev libreadline-dev libsqlite3-dev liblzma-dev libncurses-dev

sudo apt-get install python-tk python3-tk tk-dev

sudo apt-get install build-essential zlib1g-dev libffi-dev libssl-dev libbz2-dev libreadline-dev libsqlite3-dev liblzma-dev

</code></pre></div>
</li>
<li>
<p dir="auto">pyenv installer</p>
<div data-snippet-clipboard-copy-content="   curl https://pyenv.run | bash"><pre><code>   curl https://pyenv.run | bash
</code></pre></div>
</li>
<li>
<p dir="auto">Install desired python version</p>

</li>
<li>
<p dir="auto">pyenv with virtual enviroment</p>
<div data-snippet-clipboard-copy-content="   pyenv virtualenv 3.11.0 venv"><pre><code>   pyenv virtualenv 3.11.0 venv
</code></pre></div>
</li>
<li>
<p dir="auto">Activate virtualenv with pyenv</p>

</li>
</ul>
</li>
<li>
<p dir="auto">Install Dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
</li>
<li>
<p dir="auto">Prepare Data:</p>
<ul dir="auto">
<li>Resumes: Place your resumes in PDF format in the <code>Data/Resumes</code> folder. Remove any existing contents in this folder.</li>
<li>Job Descriptions: Place your job descriptions in PDF format in the <code>Data/JobDescription</code> folder. Remove any existing contents in this folder.</li>
</ul>
</li>
<li>
<p dir="auto">Parse Resumes to JSON:</p>

</li>
<li>
<p dir="auto">Run the Application:</p>
<div dir="auto" data-snippet-clipboard-copy-content="streamlit run streamlit_app.py"><pre><span>streamlit</span> <span>run</span> <span>streamlit_app</span>.<span>py</span></pre></div>
</li>
</ol>
<p dir="auto"><strong>Note</strong>: For local versions, you do not need to run "streamlit_second.py" as it is specifically for deploying to Streamlit servers.</p>
<p dir="auto"><strong>Additional Note</strong>: The Vector Similarity part is precomputed to optimize performance due to the resource-intensive nature of sentence encoders that require significant GPU and RAM resources. If you are interested in leveraging this feature in a Google Colab environment for free, refer to the upcoming blog (link to be provided) for further guidance.</p>
<br>
<h3 tabindex="-1" dir="auto">Docker</h3>
<ol dir="auto">
<li>
<p dir="auto">Build the image and start application</p>

</li>
<li>
<p dir="auto">Open <code>localhost:80</code> on your browser</p>
</li>
</ol>
<br>
<h3 tabindex="-1" dir="auto">Running the Web Application</h3>
<p dir="auto">The full stack Next.js (React and FastAPI) web application allows users to interact with the Resume Matcher tool interactively via a web browser.</p>
<div dir="auto"><p dir="auto">Warning</p><p dir="auto">The results returned from through the web app are currently entirely mocked / faked. This means that the results returned are not real and are just for demonstration purposes. This will be implemented with real data results in a future release.</p>
</div>
<p dir="auto">To run the full stack web application (frontend client and backend api servers), follow the instructions over on the <a href="https://github.com/srbhr/Resume-Matcher/blob/main/webapp/README.md">webapp README</a> file.</p>
<br>
<h3 tabindex="-1" dir="auto">Google Colab</h3>
<ol dir="auto">
<li>Create an account in ngrok and get you token</li>
<li><a target="_blank" rel="noopener noreferrer" href="https://github.com/srbhr/Resume-Matcher/blob/main/img_1.png"><img src="https://github.com/srbhr/Resume-Matcher/raw/main/img_1.png" alt="img_1.png"></a></li>
<li>Go to archive/resume_matcher_colab.ipynb and run the notebook.</li>
<li>Enter your ngrok token and run the notebook.</li>
<li>Copy the url and open it in your browser.</li>
<li><a target="_blank" rel="noopener noreferrer" href="https://github.com/srbhr/Resume-Matcher/blob/main/img_2.png"><img src="https://github.com/srbhr/Resume-Matcher/raw/main/img_2.png" alt="img_2.png"></a></li>
</ol>
<h3 tabindex="-1" dir="auto">Cohere and Qdrant</h3>
<ol dir="auto">
<li>Visit <a href="https://dashboard.cohere.ai/welcome/register" rel="nofollow">Cohere website registration</a> and create an account.</li>
<li>Go to API keys and copy your cohere api key.</li>
<li>Visit <a href="https://cloud.qdrant.io/" rel="nofollow">Qdrant website</a> and create an account.</li>
<li>Get your api key and cluster url.</li>
<li>Go to open dashboard in qdrant and enter your api key <strong>for only the first time</strong></li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/srbhr/Resume-Matcher/blob/main/Assets/img/quadrant_cloud.png"><img src="https://github.com/srbhr/Resume-Matcher/raw/main/Assets/img/quadrant_cloud.png" height="60%" width="60%"></a></p>
6.  Now create a yaml file named config.yml in Scripts/Similarity/ folder.
7.  The format for the conifg file should be as below:
    ```yaml
    cohere:
      api_key: cohere_key
    qdrant:
      api_key: qdrant_api_key
      url: qdrant_cluster_url
    ```
8.  Please replace your values without any quotes.
<p dir="auto"><em>Note: Please make sure that Qdrant_client's version is higher than v1.1</em></p>

<p dir="auto">
<h2 tabindex="-1" dir="auto">Join Us, Contribute!</h2>
</p>
<p dir="auto">Pull Requests &amp; Issues are not just welcomed, they're celebrated! Let's create together.</p>
<p dir="auto">🎉 Join our lively <a href="https://dsc.gg/resume-matcher" rel="nofollow">Discord</a> community and discuss away!</p>
<p dir="auto">💡 Spot a problem? Create an issue!</p>
<p dir="auto">👩‍💻 Dive in and help resolve existing <a href="https://github.com/srbhr/Resume-Matcher/issues">issues</a>.</p>
<p dir="auto">🔔 Share your thoughts in our <a href="https://github.com/srbhr/Resume-Matcher/discussions">Discussions &amp; Announcements</a>.</p>
<p dir="auto">🚀 Explore and improve our <a href="https://github.com/srbhr/website-for-resume-matcher">Landing Page</a>. PRs always welcome!</p>
<p dir="auto">📚 Contribute to the <a href="https://github.com/srbhr/Resume-Matcher-Docs">Resume Matcher Docs</a> and help people get started with using the software.</p>
<h4 tabindex="-1" dir="auto">Tech Stack</h4>
<p dir="auto">Current:</p>
<ul dir="auto">
<li>Python webapp in Streamlit.</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/86b23a17e4cf4ae76e5355226fb78bb315293dc4c859e823291df1987a8c7120/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d3337373641423f7374796c653d666c61742d737175617265266c6f676f3d707974686f6e26636f6c6f723d626c7565266c6f676f436f6c6f723d677265656e"><img src="https://camo.githubusercontent.com/86b23a17e4cf4ae76e5355226fb78bb315293dc4c859e823291df1987a8c7120/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d3337373641423f7374796c653d666c61742d737175617265266c6f676f3d707974686f6e26636f6c6f723d626c7565266c6f676f436f6c6f723d677265656e" alt="Python" data-canonical-src="https://img.shields.io/badge/Python-3776AB?style=flat-square&amp;logo=python&amp;color=blue&amp;logoColor=green"></a></p>
<p dir="auto">In Development:</p>
<ul dir="auto">
<li>Check the <a href="https://github.com/srbhr/Resume-Matcher/blob/main/webapp">webapp</a> folder for a Next JS app in development. (In Development)</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ad74671cadbd99edc553fb2ea16b85b597fc1272c1643e9e06300d471f5ccca1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d4646443433423f7374796c653d666c61742d737175617265266c6f676f3d707974686f6e266c6f676f436f6c6f723d626c7565"><img src="https://camo.githubusercontent.com/ad74671cadbd99edc553fb2ea16b85b597fc1272c1643e9e06300d471f5ccca1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d4646443433423f7374796c653d666c61742d737175617265266c6f676f3d707974686f6e266c6f676f436f6c6f723d626c7565" alt="Python" data-canonical-src="https://img.shields.io/badge/Python-FFD43B?style=flat-square&amp;logo=python&amp;logoColor=blue"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b9fc4c7606862c1ac532bbe5349997bbfe79d7429d7ff4ddfc666442e46b956e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5461696c77696e645f4353532d3338423241433f7374796c653d666c61742d737175617265266c6f676f3d7461696c77696e642d637373266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/b9fc4c7606862c1ac532bbe5349997bbfe79d7429d7ff4ddfc666442e46b956e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5461696c77696e645f4353532d3338423241433f7374796c653d666c61742d737175617265266c6f676f3d7461696c77696e642d637373266c6f676f436f6c6f723d7768697465" alt="Tailwind CSS" data-canonical-src="https://img.shields.io/badge/Tailwind_CSS-38B2AC?style=flat-square&amp;logo=tailwind-css&amp;logoColor=white"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1bd05d7da561a69c3850a6d63722508ecd814df1c38e866dc65c9d08828093bc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4e6578742d626c61636b3f7374796c653d666c61742d737175617265266c6f676f3d6e6578742e6a73266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/1bd05d7da561a69c3850a6d63722508ecd814df1c38e866dc65c9d08828093bc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4e6578742d626c61636b3f7374796c653d666c61742d737175617265266c6f676f3d6e6578742e6a73266c6f676f436f6c6f723d7768697465" alt="Next JS" data-canonical-src="https://img.shields.io/badge/Next-black?style=flat-square&amp;logo=next.js&amp;logoColor=white"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a0a130c16e964323ed69a2baadce6a9c38e451fa78dd816351a76112712c0b52/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f466173744150492d3030353537313f7374796c653d666c61742d737175617265266c6f676f3d66617374617069"><img src="https://camo.githubusercontent.com/a0a130c16e964323ed69a2baadce6a9c38e451fa78dd816351a76112712c0b52/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f466173744150492d3030353537313f7374796c653d666c61742d737175617265266c6f676f3d66617374617069" alt="FastAPI" data-canonical-src="https://img.shields.io/badge/FastAPI-005571?style=flat-square&amp;logo=fastapi"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bd28dbe28fea848509e8d45abd23916130462dc9236c3001967e61e76eab443c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547970655363726970742d3030374143433f7374796c653d666c61742d737175617265266c6f676f3d74797065736372697074266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/bd28dbe28fea848509e8d45abd23916130462dc9236c3001967e61e76eab443c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547970655363726970742d3030374143433f7374796c653d666c61742d737175617265266c6f676f3d74797065736372697074266c6f676f436f6c6f723d7768697465" alt="TypeScript" data-canonical-src="https://img.shields.io/badge/TypeScript-007ACC?style=flat-square&amp;logo=typescript&amp;logoColor=white"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/78dc5835c254ff7423aabdd3a0fb6592c334072417a09e6556f446029395bae8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f48544d4c352d4533344632363f7374796c653d666c61742d737175617265266c6f676f3d68746d6c35266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/78dc5835c254ff7423aabdd3a0fb6592c334072417a09e6556f446029395bae8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f48544d4c352d4533344632363f7374796c653d666c61742d737175617265266c6f676f3d68746d6c35266c6f676f436f6c6f723d7768697465" alt="HTML5" data-canonical-src="https://img.shields.io/badge/HTML5-E34F26?style=flat-square&amp;logo=html5&amp;logoColor=white"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c9bb78d3bce7cdaaaaecc956736c1f2cf629065a8d02e5fbd6825efa409718d2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f435353332d3135373242363f7374796c653d666c61742d737175617265266c6f676f3d63737333266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/c9bb78d3bce7cdaaaaecc956736c1f2cf629065a8d02e5fbd6825efa409718d2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f435353332d3135373242363f7374796c653d666c61742d737175617265266c6f676f3d63737333266c6f676f436f6c6f723d7768697465" alt="CSS3" data-canonical-src="https://img.shields.io/badge/CSS3-1572B6?style=flat-square&amp;logo=css3&amp;logoColor=white"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/aac4f0dcb884c59aa8a2e77176b44e2f202f1cbf2d3b339dbe01c577df784b1c/68747470733a2f2f637573746f6d2d69636f6e2d6261646765732e64656d6f6c61622e636f6d2f62616467652f416e645f4d6f72652d77686974653f7374796c653d666c61742d737175617265266c6f676f3d706c7573266c6f676f436f6c6f723d626c61636b"><img src="https://camo.githubusercontent.com/aac4f0dcb884c59aa8a2e77176b44e2f202f1cbf2d3b339dbe01c577df784b1c/68747470733a2f2f637573746f6d2d69636f6e2d6261646765732e64656d6f6c61622e636f6d2f62616467652f416e645f4d6f72652d77686974653f7374796c653d666c61742d737175617265266c6f676f3d706c7573266c6f676f436f6c6f723d626c61636b" alt="&amp; More" data-canonical-src="https://custom-icon-badges.demolab.com/badge/And_More-white?style=flat-square&amp;logo=plus&amp;logoColor=black"></a></p>
<br>
<div dir="auto">
<h2 tabindex="-1" dir="auto">Please support the development by donating.</h2>
<p dir="auto"><a href="https://buymeacoffee.com/srbhr" rel="nofollow"><img src="https://camo.githubusercontent.com/05fbf7164544b280f6c2aed38dbecfbbc4c412fbffdc0668ee3cd0ee9c78194a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4275792532304d6525323061253230436f666665652d6666646430303f7374796c653d666f722d7468652d6261646765266c6f676f3d6275792d6d652d612d636f66666565266c6f676f436f6c6f723d626c61636b" alt="BuyMeACoffee" data-canonical-src="https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&amp;logo=buy-me-a-coffee&amp;logoColor=black"></a>
<a href="https://github.com/sponsors/srbhr"><img src="https://camo.githubusercontent.com/37a7cffb8b4d73c06dbfea643b9e783b144ed5383815c08bcffc9d40a1bee61c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73706f6e736f722d3330333633443f7374796c653d666f722d7468652d6261646765266c6f676f3d4769744875622d53706f6e736f7273266c6f676f436f6c6f723d237768697465" alt="Sponsor on GitHub" data-canonical-src="https://img.shields.io/badge/sponsor-30363D?style=for-the-badge&amp;logo=GitHub-Sponsors&amp;logoColor=#white"></a></p>
</div>
<hr>
<h3 tabindex="-1" dir="auto">Heads Up! 📝</h3>
<p dir="auto">Your support means the world to us 💙. We're nurturing this project with an open-source community spirit, and we have an ambitious roadmap ahead! Here are some ways you could contribute and make a significant impact:</p>
<p dir="auto">✨ Transform our Streamlit dashboard into something more robust.</p>
<p dir="auto">💡 Improve our parsing algorithm, making data more accessible.</p>
<p dir="auto">🖋 Share your insights and experiences in a blog post to help others.</p>
<p dir="auto">Take the leap, contribute, and let's grow together! 🚀</p>
<hr>
<h3 tabindex="-1" dir="auto">Our Contributors ✨</h3>
<a href="https://github.com/srbhr/Resume-Matcher/graphs/contributors">
  <img src="https://camo.githubusercontent.com/320796fb62686d865e603e3bfc1856b8a1e17eecaf0b1e503dad77524231a537/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d73726268722f526573756d652d4d617463686572" data-canonical-src="https://contrib.rocks/image?repo=srbhr/Resume-Matcher">
</a>
</article>
          </div></div>]]></description>
        </item>
    </channel>
</rss>