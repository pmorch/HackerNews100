<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 16 Mar 2024 00:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Boeing whistleblower before death: "If anything happens, it's not suicide" (188 pts)]]></title>
            <link>https://futurism.com/the-byte/boeing-whistleblower-warning-not-suicide</link>
            <guid>39718672</guid>
            <pubDate>Fri, 15 Mar 2024 17:55:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://futurism.com/the-byte/boeing-whistleblower-warning-not-suicide">https://futurism.com/the-byte/boeing-whistleblower-warning-not-suicide</a>, See on <a href="https://news.ycombinator.com/item?id=39718672">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="incArticle"><h2>"I know that he did not commit suicide."</h2><h2>Curious Causes</h2><p>As Boeing continues <a href="https://futurism.com/the-byte/jet-tire-falls-off">to be in the news</a> for its <a href="https://futurism.com/the-byte/plane-wing-breaking-footage">repeatedly</a> <a href="https://futurism.com/the-byte/boeing-plane-fire">malfunctioning</a> <a href="https://futurism.com/the-byte/boeing-trouble-crack-cockpit">planes</a>, the fallout from one ex-employee's death continues ‚Äî and new reports complicate the coroner's initial suicide ruling.</p><p>In an <a href="https://abcnews4.com/news/local/if-anything-happens-its-not-suicide-boeing-whistleblowers-prediction-before-death-south-carolina-abc-news-4-2024">interview with Charleston, South Carolina's&nbsp;<em>ABC4 News</em></a>, a friend of Boeing whistleblower John Barnett, whose body was found dead in a car parked in a hotel lot amid his testimony against his former employer last weekend, said that he warned her that something might happen to him.</p><p>"I said, 'Aren't you scared?'" the woman, who gave only her first name Jennifer, told the local broadcaster. "And he said, 'No, I ain't scared, but if anything happens to me, it's not suicide.'"</p><p>Jennifer said that Barnett's words were spoken ahead of his deposition against Boeing. At the time, he'd mentioned that the company had retaliated against him for raising safety concerns before ‚Äî which was, indeed, the subject of his Occupational Safety and Health Administration (OSHA) <a href="https://www.corporatecrimereporter.com/news/200/john-barnett-on-why-he-wont-fly-on-a-boeing-787-dreamliner/">complaint</a> that led to his now-unfinished deposition.</p><p>The woman, who lives in the whistleblower's home state of Louisiana where he'd moved in recent years to take care of his aging mother, said that that cryptic warning has come back to haunt her since Barnett's death, which a coroner in Charleston says was self-inflicted.</p><p>"I know that he did not commit suicide," Jennifer told <em>ABC4</em>. "There's no way. He loved life too much. He loved his family too much. He loved his brothers too much to put them through what they're going through right now."</p><h2>Hearsay</h2><p>She's not alone in that sentiment either, it seems.</p><p>In a statement to&nbsp;<em>Futurism</em>, Barnett's attorneys said that they also "<a href="https://futurism.com/boeing-whistleblowers-lawyers-statement">didn't see any indication</a>" that the whistleblower may have been planning to take his own life, and that he'd seemed in "good spirits" as his deposition was coming to a close.</p><p>Not everyone close to the longtime Boeing quality control manager agrees with that sentiment, however.</p><p>In an <a href="https://www.seattletimes.com/business/boeing-787-whistleblower-found-dead-in-apparent-suicide/">interview with the&nbsp;<em>Seattles Times</em></a>, Barnett's niece, Katelyn Gillespie, said that her "fun uncle" had become "stressed and depressed" in recent months as his ex-employer was in the news amid the same kinds of safety concerns he'd raised and ultimately resigned over.</p><p>"He battled a lot due to the Boeing stuff," the whistleblower's niece said. "It took a major toll on him."</p><p>Despite the Charleston County coroner's preliminary autopsy report on the cause of death being a self-inflicted gunshot wound and that local police found, <a href="https://www.live5news.com/2024/03/12/brave-honest-man-boeing-whistleblowers-attorneys-release-statement-his-death/">per the city's <em>Live 5 News</em></a>, "some sort of note," authorities said they are still actively investigating the case while awaiting an official coroner's ruling.</p><p>As in other <a href="https://www.theguardian.com/politics/2013/jul/16/david-kelly-death-10-years-on">suspicious</a> <a href="https://www.cbsnews.com/sacramento/news/dhs-whistleblower-philip-haney-death-ruled-suicide/">whistleblower</a> <a href="https://foreignpolicy.com/2022/05/17/robert-mcfarlane-death-national-security-advisor-iran-contra/">suicides</a>, it's almost impossible for anyone to know exactly what happened when Barnett died ‚Äî but with this new claim from someone close to Barnett, things just got a lot more complicated.</p><p><strong>More on Boeing:</strong> <a href="https://futurism.com/the-byte/pilot-boeing-gauges-nosedive"><em>Pilot Lost Control of Boeing Jet Because Gauges ‚ÄúWent Blank," Causing Nosedive</em></a></p><br></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ollama now supports AMD graphics cards (427 pts)]]></title>
            <link>https://ollama.com/blog/amd-preview</link>
            <guid>39718558</guid>
            <pubDate>Fri, 15 Mar 2024 17:47:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ollama.com/blog/amd-preview">https://ollama.com/blog/amd-preview</a>, See on <a href="https://news.ycombinator.com/item?id=39718558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <h2>March 14, 2024</h2>
      <section>
        <p><a href="https://ollama.com/download"><img src="https://ollama.com/public/blog/amd-preview.png" alt="Ollama AMD"></a></p>

<p>Ollama now supports AMD graphics cards in preview on Windows and Linux. All the features of Ollama can now be accelerated by AMD graphics cards on Ollama for <a href="https://ollama.com/download/linux">Linux</a> and <a href="https://ollama.com/download/windows">Windows</a>.</p>

<video autoplay="" controls="">
  <source src="https://github.com/ollama/ollama/assets/3325447/671a8031-1915-448e-b033-16b367b359d9" type="video/mp4">
</video>

<h2>Supported graphics cards</h2>

<table>
<thead>
<tr>
<th>Family</th>
<th>Supported cards and accelerators</th>
</tr>
</thead>

<tbody>
<tr>
<td>AMD Radeon RX</td>
<td><code>7900 XTX</code> <code>7900 XT</code> <code>7900 GRE</code> <code>7800 XT</code> <code>7700 XT</code> <code>7600 XT</code> <code>7600</code> <br><code>6950 XT</code> <code>6900 XTX</code> <code>6900XT</code> <code>6800 XT</code> <code>6800</code><br><code>Vega 64</code> <code>Vega 56</code></td>
</tr>

<tr>
<td>AMD Radeon PRO</td>
<td><code>W7900</code> <code>W7800</code> <code>W7700</code> <code>W7600</code> <code>W7500</code> <br><code>W6900X</code> <code>W6800X Duo</code> <code>W6800X</code> <code>W6800</code><br><code>V620</code> <code>V420</code> <code>V340</code> <code>V320</code><br><code>Vega II Duo</code> <code>Vega II</code> <code>VII</code> <code>SSG</code></td>
</tr>

<tr>
<td>AMD Instinct</td>
<td><code>MI300X</code> <code>MI300A</code> <code>MI300</code><br><code>MI250X</code> <code>MI250</code> <code>MI210</code> <code>MI200</code><br><code>MI100</code> <code>MI60</code> <code>MI50</code></td>
</tr>
</tbody>
</table>
<p>Support for more AMD graphics cards is coming soon.</p>

<h2>Get started</h2>

<p>To get started with Ollama with support for AMD graphics cards, download Ollama for <a href="https://ollama.com/download/linux">Linux</a> or <a href="https://ollama.com/download/windows">Windows</a>.</p>

      </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Compressing Chess Moves for Fun and Profit (127 pts)]]></title>
            <link>https://mbuffett.com/posts/compressing-chess-moves/</link>
            <guid>39717615</guid>
            <pubDate>Fri, 15 Mar 2024 16:35:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mbuffett.com/posts/compressing-chess-moves/">https://mbuffett.com/posts/compressing-chess-moves/</a>, See on <a href="https://news.ycombinator.com/item?id=39717615">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>Chess notation has come a long way since <a href="https://en.wikipedia.org/wiki/Descriptive_notation">descriptive
notation</a>, now we have nice
and decipherable Standard Algebraic Notation, like <code>Qxf7</code> (queen takes on
f7) or <code>Nf3</code> (knight takes on f3).</p><p>This is a great text format, but a massive waste of
space if you‚Äôre trying to store a lot of these. <code>Qxf7</code> takes 4 bytes, or 32 bits. Let‚Äôs do some rough back-of-the-envelope math of how much information is actually being transmitted though.
This move affects one of 6 pieces (3 bits), this move is also a capture (1
bit), and it specifies a destination squuare (64 possibilities == 6 bits). Add those up, you get 10 bits. Far from the 32 bits that the textual representation needs.</p><p>Why do I care? I run a site that stores a <em>ton</em> of
chess lines, something like 100 million in total. Assume an average of 6 moves for each line, that‚Äôs 600 million moves. The database is growing large
enough that querying it is IO-constrained. I want to speed up the reads from this database when I‚Äôm fetching thousands of lines.</p><h2 id="a-first-pass">A first pass</h2><p>First some general numbers:</p><ul><li>Encoding a file or rank (a-h or 1-8) takes 3 bits (8 possibilities)</li><li>Encoding a piece (k, q, r, b, n, p) takes 3 bits (6 possibilities)</li><li>Encoding a square takes 6 bits (64 possibilities)</li></ul><p>So let‚Äôs see the pieces we‚Äôre working with for encoding a SAN.</p><p>So let‚Äôs go with the most naive approach.</p><ul><li>Which piece was it? 3 bits</li><li>Is it a capture? 1 bit</li><li>Do we have to disabiguate it (ie <code>Ngf3</code>)? Maximum of 2 bits + 6 bits (this is explained more further down)</li><li>Where did it go? 6 bits</li><li>Is it a promotion, and to which piece? 7 bits</li><li>Is it a check? 1 bit</li><li>Is it a checkmate? 1 bit</li><li>Is it a castle? Short or long? 2 bits</li></ul><p>This gives us a total of <code>3+1+2+6+6+7+1+1+2 = 29</code> bits, or about 3.5 bytes per move. That‚Äôs not great though. A lot of moves actually take up less bits than that in text format.</p><h2 id="getting-smarter">Getting smarter</h2><h3 id="the-first-few-bits">The first few bits</h3><p>In the first pass, we just encoded the piece that moved using 3 bits, but that
leaves 2 unused permutations available, since there are only 6 pieces in chess.
Luckily, there are two very common moves that fit neatly into this hole, those
being short castles (<code>O-O</code>) and long castles (<code>O-O-O</code>).</p><p>So we can encode the first 3 bits like so:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>match</span> first_bytes {
</span></span><span><span>    FirstBytes::Pawn <span>=&gt;</span> bits.extend(<span>&amp;</span>[<span>false</span>, <span>false</span>, <span>false</span>]),
</span></span><span><span>    FirstBytes::Knight <span>=&gt;</span> bits.extend(<span>&amp;</span>[<span>false</span>, <span>false</span>, <span>true</span>]),
</span></span><span><span>    FirstBytes::Bishop <span>=&gt;</span> bits.extend(<span>&amp;</span>[<span>false</span>, <span>true</span>, <span>false</span>]),
</span></span><span><span>    FirstBytes::Rook <span>=&gt;</span> bits.extend(<span>&amp;</span>[<span>false</span>, <span>true</span>, <span>true</span>]),
</span></span><span><span>    FirstBytes::Queen <span>=&gt;</span> bits.extend(<span>&amp;</span>[<span>true</span>, <span>false</span>, <span>false</span>]),
</span></span><span><span>    FirstBytes::King <span>=&gt;</span> bits.extend(<span>&amp;</span>[<span>true</span>, <span>false</span>, <span>true</span>]),
</span></span><span><span>    FirstBytes::ShortCastling <span>=&gt;</span> bits.extend(<span>&amp;</span>[<span>true</span>, <span>true</span>, <span>false</span>]),
</span></span><span><span>    FirstBytes::LongCastling <span>=&gt;</span> bits.extend(<span>&amp;</span>[<span>true</span>, <span>true</span>, <span>true</span>]),
</span></span><span><span>}
</span></span></code></pre></div><h3 id="the-destination-square">The destination square</h3><p>In all cases except castling, you need to know the square that the piece is moving to, to reproduce the SAN. So we‚Äôll skip this entirely when castling, but otherwise always include 6 bits for the destination square.</p><h4 id="just-kidding-pawn-moves">Just kidding, pawn moves!</h4><p>There‚Äôs another exception to the destination square rule that might not seem so
obvious. Let‚Äôs take the move <code>exf6</code>. We know it‚Äôs a pawn move from the e-file,
so we don‚Äôt really need to encode the file it‚Äôs capturing using 6 bits. After
all, you‚Äôll never see <code>exa6</code>. So in these cases instead of 6 bits for the
destination square, we only need 4 (one for the direction, and one for the
rank).</p><p>But we can get even more clever here. Take <code>hxg6</code> for example. You know as soon as you see <code>hx</code>, that the file is going to be the g-file. So we don‚Äôt even need the extra bit to encode direction, we can just encode the file once, and the rank once.</p><p>So here‚Äôs the setup:</p><ul><li>Pawn capture from b-g files: 3 bits for the file you‚Äôre capturing from, 1 bit for the direction of the capture, and 3 bits for the rank<ul><li>Total: 10 bits for movement</li></ul></li><li>Pawn capture from a and h files: 3 bits for the file you‚Äôre capturing from, 3 bits for rank<ul><li>Total: 6 bits for movement</li></ul></li></ul><h3 id="special-moves">‚ÄúSpecial‚Äù moves</h3><p>It‚Äôs a bit of a waste to encode for each move, whether it‚Äôs a promotion, check,
checkmate, capture, etc. After all, the vast majority of chess moves in a game are not any of these.</p><p>So we‚Äôll devote one bit to determining whether a move is ‚Äúspecial‚Äù. It means we
have to use an extra bit for moves that are promotions/checks/captures, but it
also means that we save a whole lot of bits for ‚Äúregular‚Äù moves.</p><h3 id="promotions">Promotions</h3><p>Promotions are nice, because even though there are 6 chess pieces, there are
only 4 valid pieces that you can promote to (after all, you can‚Äôt promote to a
King or Pawn). So we only need 2 bits instead of 3, to encode the promotion
piece.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>if</span> <span>let</span> Some(promotion) <span>=</span> promotion {
</span></span><span><span>    bits.push(<span>true</span>);
</span></span><span><span>    bits.extend(<span>match</span> promotion {
</span></span><span><span>        PromotionPiece::Queen <span>=&gt;</span> <span>&amp;</span>[<span>false</span>, <span>false</span>],
</span></span><span><span>        PromotionPiece::Rook <span>=&gt;</span> <span>&amp;</span>[<span>false</span>, <span>true</span>],
</span></span><span><span>        PromotionPiece::Bishop <span>=&gt;</span> <span>&amp;</span>[<span>true</span>, <span>false</span>],
</span></span><span><span>        PromotionPiece::Knight <span>=&gt;</span> <span>&amp;</span>[<span>true</span>, <span>true</span>],
</span></span><span><span>    });
</span></span><span><span>} <span>else</span> {
</span></span><span><span>    bits.push(<span>false</span>);
</span></span><span><span>}
</span></span></code></pre></div><h3 id="disambiguation">Disambiguation</h3><p>Disambiguation is a bit thorny. You have to encode a surprising amount of information, to be able to decode the SAN exactly as you received it.</p><p>Take <code>Ngf3</code> for example. Besides the usual stuff you also need to encode the
disambiguation (file=g). There are 3 different disambiguation possibilities (rank, file, or whole square), so
we need 2 bits. One goes to waste but disambiguated moves aren‚Äôt that common
and I can‚Äôt think of a nice way to take advantage of that last permutation.</p><p>So we always use 2 bits, then either 3 bits for the rank, 3 bits for the file, or 6 bits for a whole square (very rare).</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>if</span> <span>let</span> Some(disambiguate) <span>=</span> disambiguate {
</span></span><span><span>    bits.push(<span>true</span>);
</span></span><span><span>    <span>match</span> disambiguate {
</span></span><span><span>        Disambiguation::File(file) <span>=&gt;</span> {
</span></span><span><span>            bits.extend(<span>&amp;</span>[<span>false</span>, <span>true</span>]);
</span></span><span><span>            bits.extend(square_component_to_bits(file));
</span></span><span><span>        }
</span></span><span><span>        Disambiguation::Rank(rank) <span>=&gt;</span> {
</span></span><span><span>            bits.extend(<span>&amp;</span>[<span>true</span>, <span>false</span>]);
</span></span><span><span>            bits.extend(square_component_to_bits(rank));
</span></span><span><span>        }
</span></span><span><span>        Disambiguation::Square(square) <span>=&gt;</span> {
</span></span><span><span>            bits.extend(<span>&amp;</span>[<span>true</span>, <span>true</span>]);
</span></span><span><span>            bits.extend(square_component_to_bits(square.<span>0</span>));
</span></span><span><span>            bits.extend(square_component_to_bits(square.<span>1</span>));
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>} <span>else</span> {
</span></span><span><span>    bits.push(<span>false</span>);
</span></span><span><span>}
</span></span></code></pre></div><h2 id="how-does-this-do">How does this do?</h2><table><thead><tr><th>Move</th><th>Original bits</th><th>Encoded bits</th><th>Savings</th></tr></thead><tbody><tr><td>e4</td><td>16</td><td>10</td><td>37.5%</td></tr><tr><td>exd5</td><td>32</td><td>12</td><td>62.5%</td></tr><tr><td>Nf3+</td><td>24</td><td>10</td><td>58.33%</td></tr><tr><td>Qxa5+</td><td>40</td><td>16</td><td>60%</td></tr><tr><td>cxd8=Q#</td><td>56</td><td>16</td><td>71.43%</td></tr></tbody></table><p>Not bad! We‚Äôre saving anywhere from 37.5% to 71.43% of the bits.</p><h2 id="pgns">PGNs</h2><p>You may be thinking something like this: ‚ÄúIsn‚Äôt is sorta cheating to measure these in bits? Since you can only address one byte at a time, needing 10 bits for a move is virtually the same as 16 bits‚Äù</p><p>Well yes, that‚Äôs true and means that we get less savings when storing individual SANs. But they don‚Äôt account for the majority of what I‚Äôm storing, which are PGNs.</p><p>PGNs are a way to store lines or games, and they look something like this:</p><pre tabindex="0"><code>1.e4 d5 2.exd5 Nf6 3.d4 Bg4 4.Be2 Bxe2 5.Nxe2 Qxd5 6.O-O Nc6 7.c3 O-O-O 8.Qb3 Qh5 9.Nf4 Qh4 10.Qxf7 Ng4 11.h3 Nf6 12.Be3 e6 13.Qxe6+ Kb8 14.Nd2 Bd6 15.g3 Qh6 16.Qf5 Ne7 17.Qa5 b6 18.Qa6 Bxf4 19.Bxf4 Qxh3 20.Nf3 Ned5 21.Be5 Qh5 22.Kg2 Qg6 23.Rae1 Nh5 24.Nh4 Nhf4+ 25.Bxf4 Nxf4+ 26.Kh2 b5 27.Qxg6 hxg6 28.gxf4 Rxh4+ 29.Kg3 Rdh8 30.Re5 Rh3+ 31.Kg4 R8h4+ 32.Kg5 Rh5+ 33.Kxg6 Rh6+ 34.Kf7 Rh7 35.Rxb5+ Kc8 36.Rg5 g6+ 37.Kxg6 R3h6+ 38.Kf5 Rf7+ 39.Kg4 Rhf6 40.f5 Kd7 41.Re1 Kd6 42.Re8 Kd7 43.Ra8 Rh6 44.Rxa7 Rfh7 45.Kf4 Rh4+ 46.Rg4 Rh2 47.f3 Rxb2 48.f6 Rf7 49.Rg7 Ke6 50.Rxf7 Kxf7 51.Rxc7+ Kxf6 52.a4 Ra2 53.Rc4 Ra3 54.d5 Ke7 55.Ke5 Kd7 56.f4 Ra2 57.Kd4 Rd2+ 58.Kc5 Ra2 59.Kb5 Kd6 60.Rd4 Rb2+ 61.Ka6 Ra2 62.a5 Kc5 63.d6 Ra3 64.d7 Kc6 65.d8=Q Rxa5+ 66.Kxa5 Kc5 67.Qc7#
</code></pre><p>This PGN is 759 bytes. There‚Äôs a ton of wasted space though. One byte between each move (the space). Then <strong>at least 3 bytes</strong> between full moves (<code> 2.</code>). This is sort of crazy, and if we combine our SAN encoding, we can compress this to be way smaller.</p><p>If we encode the whole PGN using our SAN encoding, with no space between moves because we know once we‚Äôve reached the end of a move, we can compress this specific 759-byte PGN down to <strong>195 bytes</strong>, for a savings of <strong>74%</strong>.</p><h2 id="impact">Impact</h2><p>This hasn‚Äôt been deployed yet, I‚Äôm working on a ton of other performance
improvements. But I anticipate this along with EPD compression (which I may
write another at article on) will reduce the size of the database by about 70%.
We‚Äôre almost entierly read-constrained, which should mean a 3x speedup for the
most expensive queries we run.</p><h2 id="speed">Speed</h2><p>Another consideration here is the speed of doing this encoding/decoding; will
it just cancel out the gains from having a much smaller database? Turns out,
computers are really fast at this stuff, and conversion to and from this
encoding is a rounding error.</p><p>I‚Äôm using Rust + the <code>bitvec</code> library. Encoding and then decoding 1000 moves
takes about 600,000ns, or 0.6ms. I haven‚Äôt taken a performance pass at all
either, and there‚Äôs a few places I know are very inefficient. I‚Äôm guessing I‚Äôm
not even within 10x of optimal, but it should be good enough.</p></div></article><div><p>Thanks for reading! If you have any questions, comments, or just want to say hi,
please email me at <a href="mailto:me@mbuffett.com">me@mbuffett.com</a>. I'm not
very active <a href="https://twitter.com/MarcusBuffett">on twitter</a>,
but you can choose to follow me in case that changes.</p><p>If you're into chess, I've made a <a href="https://chessbook.com/">repertoire builder</a>. It uses statistics from
hundreds of millions of games at your level to find the gaps in your
repertoire, and uses spaced repetition to quiz you on them.</p><p>Samar Haroon, my girlfriend, has started a podcast where she talks about the
South Asian community, from the perspective of a psychotherapist.
<a href="https://open.spotify.com/show/7teSzaHt5I3r9s5PPLZFrF?si=J1-h-uFCTLyXGPbZnYSIGQ" target="_blank">Go check it out!</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FTC and DOJ want to free McDonald's ice cream machines from DMCA repair rules (241 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2024/03/ftc-and-doj-want-to-free-mcdonalds-ice-cream-machines-from-dmca-repair-rules/</link>
            <guid>39717558</guid>
            <pubDate>Fri, 15 Mar 2024 16:30:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2024/03/ftc-and-doj-want-to-free-mcdonalds-ice-cream-machines-from-dmca-repair-rules/">https://arstechnica.com/tech-policy/2024/03/ftc-and-doj-want-to-free-mcdonalds-ice-cream-machines-from-dmca-repair-rules/</a>, See on <a href="https://news.ycombinator.com/item?id=39717558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      I scream, you scream, we all scream for 1201(c)3 exemptions    ‚Äî
</h4>
            
            <h2 itemprop="description">McFlurries are a notable part of petition for commercial and industrial repairs.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/03/ice_cream_fix-800x536.jpg" alt="Taylor ice cream machine, with churning spindle removed by hand.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/03/ice_cream_fix-scaled.jpg" data-height="1714" data-width="2560">Enlarge</a> <span>/</span> Taylor's C709 Soft Serve Freezer isn't so much mechanically complicated as it is a software and diagnostic trap for anyone without authorized access.</p></figcaption>  </figure>

  




<!-- cache hit 149:single/related:70bb8fbbd06d2f1057c24bd5f3501ac0 --><!-- empty -->
<p>Many devices have been made difficult or financially nonviable to repair, whether by design or because of a lack of parts, manuals, or specialty tools. Machines that make ice cream, however, seem to have a special place in the hearts of lawmakers. Those machines are often broken and locked down for only the most profitable repairs.</p>
<p>The Federal Trade Commission and the antitrust division of the Department of Justice have <a href="https://www.ftc.gov/system/files/ftc_gov/pdf/ATR-FTC-JointComment.pdf">asked the US Copyright Office</a> (PDF) to exempt "commercial soft serve machines" from the anti-circumvention rules of <a href="https://www.copyright.gov/1201/2018/">Section 1201</a> of the Digital Millennium Copyright Act (DMCA). The governing bodies also submitted proprietary diagnostic kits, programmable logic controllers, and enterprise IT devices for DMCA exemptions.</p>
<p>"In each case, an exemption would give users more choices for third-party and self-repair and would likely lead to cost savings and a better return on investment in commercial and industrial equipment," the joint comment states. Those markets would also see greater competition in the repair market, and companies would be prevented from using DMCA laws to enforce monopolies on repair, according to the comment.</p>
<p>The joint comment builds upon a petition filed by repair vendor and advocate iFixit and interest group Public Knowledge, which advocated for broad reforms while keeping a relatable, ingestible example at its center. McDonald's soft serve ice cream machines, which are <a href="https://mcbroken.com/">famously frequently broken</a>, are supplied by industrial vendor Taylor. <a href="https://publicknowledge.org/public-knowledge-petitions-copyright-office-for-dmca-exemption-for-ice-cream-machines/">Taylor's C709 Soft Serve Freezer</a> requires lengthy, finicky warm-up and cleaning cycles, produces obtuse error codes, and, perhaps not coincidentally, costs $350 per 15 minutes of service for a Taylor technician to fix. iFixit <a href="https://www.ifixit.com/News/80215/whats-inside-that-mcdonalds-ice-cream-machine-broken-copyright-law">tore down such a machine</a>, confirming the lengthy process between plugging in and soft serving.
</p><p>After one company built a Raspberry Pi-powered device, the <a href="https://www.kytch.com/landing">Kytch</a>, that could provide better diagnostics and insights, Taylor moved to ban franchisees from installing the device, then offered up its own competing product. Kytch has <a href="https://www.wired.com/story/kytch-ice-cream-machine-hackers-sue-mcdonalds-900-million/">sued Taylor for $900 million</a>&nbsp;in a case that is still pending.</p>                                            
                                                        
<p>Beyond ice cream, the petitions to the Copyright Office would provide more broad exemptions for industrial and commercial repairs that require some kind of workaround, decryption, or other software tinkering. Going past technological protection measures (TPMs) was made illegal by the 1998 DMCA, which was put in place largely because of the concerns of media firms facing what they considered rampant piracy.</p>
<p>Every three years, the Copyright Office allows for petitions to exempt certain exceptions to DMCA violations (and renew prior exemptions). Repair advocates have won exemptions for farm equipment repair, <a href="https://arstechnica.com/tech-policy/2021/10/us-copyright-office-oks-right-to-repair-for-video-game-console-optical-drives/">video game consoles</a>, cars, and certain medical gear. The exemption is often granted for device fixing if a repair person can work past its locks, but not for the distribution of tools that would make such a repair far easier. The esoteric nature of such "release valve" offerings has led groups like the EFF to <a href="https://arstechnica.com/tech-policy/2016/07/eff-sues-us-government-saying-copyright-rules-on-drm-are-unconstitutional/">push for the DMCA's abolishment</a>.</p>
<p>DMCA exemptions occur on a parallel track to <a href="https://arstechnica.com/tech-policy/2024/03/oregon-oks-right-to-repair-bill-that-bans-the-blocking-of-aftermarket-parts/">state right-to-repair bills</a> and broader federal action. President Biden issued <a href="https://arstechnica.com/tech-policy/2021/07/bidens-right-to-repair-order-could-stop-companies-from-blocking-diy-fixes/">an executive order</a> that included a push for repair reforms. The FTC has issued studies that call out <a href="https://www.ftc.gov/system/files/documents/reports/nixing-fix-ftc-report-congress-repair-restrictions/nixing_the_fix_report_final_5521_630pm-508_002.pdf">unnecessary repair restrictions</a> and has taken <a href="https://www.ftc.gov/news-events/news/press-releases/2022/10/ftc-approves-final-orders-right-repair-cases-against-harley-davidson-mwe-investments-weber">action</a> against firms like Harley-Davidson, Westinghouse, and grill maker Weber for tying warranties to an authorized repair service.</p>
<p><i>Disclosure: Kevin Purdy previously worked for iFixit. He has no financial ties to the company.</i></p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Open-source, browser-local data exploration using DuckDB-WASM and PRQL (123 pts)]]></title>
            <link>https://github.com/pretzelai/pretzelai</link>
            <guid>39717268</guid>
            <pubDate>Fri, 15 Mar 2024 16:02:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/pretzelai/pretzelai">https://github.com/pretzelai/pretzelai</a>, See on <a href="https://news.ycombinator.com/item?id=39717268">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">ü•® Pretzel</h2><a id="user-content--pretzel" aria-label="Permalink: ü•® Pretzel" href="#-pretzel"></a></p>
<p dir="auto"><a href="https://github.com/pretzelai/pretzelai/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/c74c83918e248d250e94ac5b8c93771ebb0d3127155fc17220595489455e643d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f707265747a656c61692f707265747a656c6169" alt="License" data-canonical-src="https://img.shields.io/github/license/pretzelai/pretzelai"></a>
<a href="https://github.com/pretzelai/pretzelai"><img src="https://camo.githubusercontent.com/93e4c3cd9a6b630a89aaa2c6a9309d696c0ca515a050ac7ea5245a5b1122af91/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f707265747a656c61692f707265747a656c61693f7374796c653d736f6369616c" alt="GitHub Stars" data-canonical-src="https://img.shields.io/github/stars/pretzelai/pretzelai?style=social"></a></p>
<p dir="auto">Live deployed build: <a href="https://pretzelai.github.io/" rel="nofollow">https://pretzelai.github.io</a></p>
<p dir="auto">Pretzel is an open-source, offline browser-based tool for fast and intuitive data exploration and visualization. It can handle large data files, runs locally in your browser, and requires no backend setup. Pretzel makes it easy to manipulate data via visual chained data transform blocks. It's also reactive - chaging an tranform block in the chain automatically updates all transform blocks and charts that follow.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/121360087/313066071-e7f20a16-b19c-4a29-b468-88d42eaa9b43.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1NDM5MDYsIm5iZiI6MTcxMDU0MzYwNiwicGF0aCI6Ii8xMjEzNjAwODcvMzEzMDY2MDcxLWU3ZjIwYTE2LWIxOWMtNGEyOS1iNDY4LTg4ZDQyZWFhOWI0My5naWY_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxNVQyMzAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0xNGIzOWM4ZDRmMTRhMDEyMWIzMmVjMGFlOGQ0ZDNkMjRhYzIyNjNhMjBlOTdhZGJlYjhmMTI5ZTM3ZjYxNjE4JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.gcFiE42OSFIMDrDV7Z6IdBzOBgxHjnEOVfO4V2X5aSU"><img src="https://private-user-images.githubusercontent.com/121360087/313066071-e7f20a16-b19c-4a29-b468-88d42eaa9b43.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1NDM5MDYsIm5iZiI6MTcxMDU0MzYwNiwicGF0aCI6Ii8xMjEzNjAwODcvMzEzMDY2MDcxLWU3ZjIwYTE2LWIxOWMtNGEyOS1iNDY4LTg4ZDQyZWFhOWI0My5naWY_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxNVQyMzAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0xNGIzOWM4ZDRmMTRhMDEyMWIzMmVjMGFlOGQ0ZDNkMjRhYzIyNjNhMjBlOTdhZGJlYjhmMTI5ZTM3ZjYxNjE4JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.gcFiE42OSFIMDrDV7Z6IdBzOBgxHjnEOVfO4V2X5aSU" alt="demo.gif" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>üöÄ Blazing-fast performance with WebAssembly-based <a href="https://duckdb.org/" rel="nofollow">DuckDB</a> and <a href="https://prql-lang.org/" rel="nofollow">PRQL</a></li>
<li>üîç Intuitive data exploration with a visual, top-down pipeline of data transformations and visualizations</li>
<li>üß† AI-powered transformation block to help with fast data manipulation</li>
<li>üîí Privacy-first design: run Pretzel AI locally or host it yourself for full control over your data</li>
<li>üìä Upcoming features: Local LLM support, API calls, in-browser Python support with <a href="https://github.com/pyodide/pyodide">Pyodide</a>, save and share workflows securely and canvas based table rendering</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#demo-video">Demo video</a></li>
<li><a href="#getting-started">Getting started</a>
<ul dir="auto">
<li><a href="#website-easiest">Website (Easiest)</a></li>
<li><a href="#offline-standalone-app">Offline standalone app</a></li>
<li><a href="#developers">Developers</a>
<ul dir="auto">
<li><a href="#run-locally">Run locally</a></li>
<li><a href="#host-pretzel">Host Pretzel</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#optional-configuration">Optional Configuration</a></li>
<li><a href="#implemented-transformation-blocks">Implemented Transformation Blocks</a></li>
<li><a href="#known-bugs">Known Bugs</a></li>
<li><a href="#contact">Contact</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo video</h2><a id="user-content-demo-video" aria-label="Permalink: Demo video" href="#demo-video"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description Pretzel.mp4">Pretzel.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/161899563/313245661-cb5b0f00-4add-40e8-b0c8-f59a0186e3ff.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1NDM5MDYsIm5iZiI6MTcxMDU0MzYwNiwicGF0aCI6Ii8xNjE4OTk1NjMvMzEzMjQ1NjYxLWNiNWIwZjAwLTRhZGQtNDBlOC1iMGM4LWY1OWEwMTg2ZTNmZi5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxNVQyMzAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zMjU1N2M2YzNmMTg5YmE1NDljZjRiZTcwNTE2NzI0YmY4YWI0YmYzODBlMjc4NTNlOTg4ZjQ3OWNkMjcxMTU1JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.p9Cwumfo8cZKz_5umvOXFKLi_sVhkhB5q0S65SARwZs" data-canonical-src="https://private-user-images.githubusercontent.com/161899563/313245661-cb5b0f00-4add-40e8-b0c8-f59a0186e3ff.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1NDM5MDYsIm5iZiI6MTcxMDU0MzYwNiwicGF0aCI6Ii8xNjE4OTk1NjMvMzEzMjQ1NjYxLWNiNWIwZjAwLTRhZGQtNDBlOC1iMGM4LWY1OWEwMTg2ZTNmZi5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxNVQyMzAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0zMjU1N2M2YzNmMTg5YmE1NDljZjRiZTcwNTE2NzI0YmY4YWI0YmYzODBlMjc4NTNlOTg4ZjQ3OWNkMjcxMTU1JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.p9Cwumfo8cZKz_5umvOXFKLi_sVhkhB5q0S65SARwZs" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Website (Easiest)</h3><a id="user-content-website-easiest" aria-label="Permalink: Website (Easiest)" href="#website-easiest"></a></p>
<p dir="auto">The easiest way to use Pretzel is to visit <a href="https://pretzelai.github.io/" rel="nofollow">https://pretzelai.github.io</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Offline standalone app</h3><a id="user-content-offline-standalone-app" aria-label="Permalink: Offline standalone app" href="#offline-standalone-app"></a></p>
<p dir="auto">Since Pretzel doesn't have a backend you can easily install it as a Chrome app and it will work even without internet (for those long flights!)</p>
<ol dir="auto">
<li>
<p dir="auto">Visit <a href="https://pretzelai.github.io/" rel="nofollow">https://pretzelai.github.io</a> in Chrome</p>
</li>
<li>
<p dir="auto">Click the install app icon</p>
</li>
</ol>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/121360087/313066187-c6276699-5109-4e59-8bf5-2858c51cb4c3.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1NDM5MDYsIm5iZiI6MTcxMDU0MzYwNiwicGF0aCI6Ii8xMjEzNjAwODcvMzEzMDY2MTg3LWM2Mjc2Njk5LTUxMDktNGU1OS04YmY1LTI4NThjNTFjYjRjMy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxNVQyMzAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00OWNhNTYyZWM4NDNkYmFkYzc2ZDRiMmIyM2Y4NjRhZTFhYWVhODgwODM0MTNlNGQ3NzBiMjI3ZTc1Y2IzYzU4JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.GbmRczgdy_3OGVxJiejCrCV8iI2cODu7sqC7w45rRFA"><img width="521" alt="pretzel_chrome_install" src="https://private-user-images.githubusercontent.com/121360087/313066187-c6276699-5109-4e59-8bf5-2858c51cb4c3.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1NDM5MDYsIm5iZiI6MTcxMDU0MzYwNiwicGF0aCI6Ii8xMjEzNjAwODcvMzEzMDY2MTg3LWM2Mjc2Njk5LTUxMDktNGU1OS04YmY1LTI4NThjNTFjYjRjMy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxNVQyMzAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00OWNhNTYyZWM4NDNkYmFkYzc2ZDRiMmIyM2Y4NjRhZTFhYWVhODgwODM0MTNlNGQ3NzBiMjI3ZTc1Y2IzYzU4JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.GbmRczgdy_3OGVxJiejCrCV8iI2cODu7sqC7w45rRFA"></a>
<ol start="3" dir="auto">
<li>Now you can launch Pretzel as a standalone app. It will also work offline, it may error if you try to use some internet feature (like the AI Block), just close it and open it again to fix it</li>
</ol>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/121360087/313066562-cc13e552-d93a-4990-be22-1f6b5d906b15.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1NDM5MDYsIm5iZiI6MTcxMDU0MzYwNiwicGF0aCI6Ii8xMjEzNjAwODcvMzEzMDY2NTYyLWNjMTNlNTUyLWQ5M2EtNDk5MC1iZTIyLTFmNmI1ZDkwNmIxNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxNVQyMzAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wYzQzMzg0MzRjY2MyYTFiMjVlN2EwMzg3NTNhYTRiYjIwMGM1YTMxYzY4ZmM5MTY4OGFmMDYwYWFkNmVhZTNlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.6kReT3qm1l4_zW6J34SEr9DyUpEEYNDXSKkbJaA-Smo"><img width="268" alt="pretzel_app_icon" src="https://private-user-images.githubusercontent.com/121360087/313066562-cc13e552-d93a-4990-be22-1f6b5d906b15.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1NDM5MDYsIm5iZiI6MTcxMDU0MzYwNiwicGF0aCI6Ii8xMjEzNjAwODcvMzEzMDY2NTYyLWNjMTNlNTUyLWQ5M2EtNDk5MC1iZTIyLTFmNmI1ZDkwNmIxNS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMzE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDMxNVQyMzAwMDZaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wYzQzMzg0MzRjY2MyYTFiMjVlN2EwMzg3NTNhYTRiYjIwMGM1YTMxYzY4ZmM5MTY4OGFmMDYwYWFkNmVhZTNlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.6kReT3qm1l4_zW6J34SEr9DyUpEEYNDXSKkbJaA-Smo"></a>
<p dir="auto"><h3 tabindex="-1" dir="auto">Developers</h3><a id="user-content-developers" aria-label="Permalink: Developers" href="#developers"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Run locally</h4><a id="user-content-run-locally" aria-label="Permalink: Run locally" href="#run-locally"></a></p>
<p dir="auto">To run Pretzel locally, follow these steps:</p>
<ol dir="auto">
<li>
<p dir="auto">Clone the repository:</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/pretzelai/pretzelai.git"><pre><code>git clone https://github.com/pretzelai/pretzelai.git
</code></pre></div>
</li>
<li>
<p dir="auto">Install dependencies:</p>

</li>
<li>
<p dir="auto">Start the development server:</p>

</li>
<li>
<p dir="auto">Open your browser and navigate to <code>http://localhost:3000</code></p>
</li>
</ol>
<p dir="auto"><h4 tabindex="-1" dir="auto">Host Pretzel</h4><a id="user-content-host-pretzel" aria-label="Permalink: Host Pretzel" href="#host-pretzel"></a></p>
<p dir="auto">To host Pretzel, follow these steps (it's just a static website!):</p>
<ol dir="auto">
<li>Build the app</li>
</ol>

<ol start="2" dir="auto">
<li>Upload the contents of the <code>build</code> folder to your hosting. This is what you can find live at <a href="https://pretzelai.github.io/" rel="nofollow">https://pretzelai.github.io</a></li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Optional configuration</h2><a id="user-content-optional-configuration" aria-label="Permalink: Optional configuration" href="#optional-configuration"></a></p>
<ul dir="auto">
<li>Bug report box: Update <code>/src/lib/config.ts</code> with your PostHog config to let users report bugs directly on the website</li>
<li>AI Endpoint: Deploy a cloud function to provide an AI endpoint for users without an OpenAI API key. Check the <code>cloud</code> folder for instructions.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Implemented transformation blocks</h2><a id="user-content-implemented-transformation-blocks" aria-label="Permalink: Implemented transformation blocks" href="#implemented-transformation-blocks"></a></p>
<ul dir="auto">
<li><strong>Upload:</strong> accepts CSV / Excel (XLSX) files</li>
<li><strong>Filter</strong>: string/number/date filtering including nested filters</li>
<li><strong>Ask AI</strong>: connects to OpenAI to transform user command to SQL</li>
<li><strong>Pivot</strong>: to create a pivot table (you can also go group-by using this - only use the <code>Rows</code> and <code>Values</code> fields)</li>
<li><strong>Sort</strong>: sorts ascending or descending on multiple columns</li>
<li><strong>Chart</strong>: supports line (including multi-line) charts, bar charts (grouped and stacked) and scatter plot</li>
<li><strong>Create column</strong>: make a new column with basic math or use <a href="https://prql-lang.org/book/reference/declarations/functions.html" rel="nofollow">PRQL functions</a></li>
<li><strong>Remove columns</strong>: easily add/remove columns with visual toggles</li>
<li><strong>Table</strong>: add a table in the middle of your workflow to visualize data in a intermediate step</li>
<li><strong>Download</strong>: export your transformed data in CSV</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Known Bugs</h2><a id="user-content-known-bugs" aria-label="Permalink: Known Bugs" href="#known-bugs"></a></p>
<ul dir="auto">
<li>Dates are sometimes parsed incorrectly - existing GH issue <a href="https://github.com/pretzelai/pretzelai/issues/23" data-hovercard-type="issue" data-hovercard-url="/pretzelai/pretzelai/issues/23/hovercard">here</a></li>
<li>Table panel is slow for large datasets. We're planning on moving to a canvas based table</li>
<li>[Rare] Charts axes can sometimes not be ordered correctly</li>
</ul>
<p dir="auto">Please report any bugs you find in <a href="https://github.com/pretzelai/pretzelai">GitHub issues</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contact</h2><a id="user-content-contact" aria-label="Permalink: Contact" href="#contact"></a></p>
<p dir="auto">You can email us at founders [at] withpretzel [dot] com.</p>
<p dir="auto">We also read all the feedback and bugs you report at the top left of <a href="https://pretzelai.github.io/" rel="nofollow">https://pretzelai.github.io</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reversing for dummies ‚Äì x86 assembly and C code (Beginner/ADHD friendly) (112 pts)]]></title>
            <link>https://0x44.cc/reversing/2021/07/21/reversing-x86-and-c-code-for-beginners.html</link>
            <guid>39716494</guid>
            <pubDate>Fri, 15 Mar 2024 14:58:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://0x44.cc/reversing/2021/07/21/reversing-x86-and-c-code-for-beginners.html">https://0x44.cc/reversing/2021/07/21/reversing-x86-and-c-code-for-beginners.html</a>, See on <a href="https://news.ycombinator.com/item?id=39716494">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <h3 id="context">Context</h3>

          <p>
            Before I got into reverse engineering, executables always seemed
            like black magic to me. I always wondered how stuff worked under the
            hood, and how binary code is represented inside .exe files, and how
            hard it is to modify this ‚Äòcompiled code‚Äô without access to the
            original source code.
          </p>

          <p>
            But one of the main intimidating hurdles always seemed to be the
            assembly language, it‚Äôs the thing that scares most people away from
            trying to learn about this field.
          </p>

          <p>
            That‚Äôs the main reason why I thought of writing this
            straight-to-the-point article that only contains the essential stuff
            that you encounter the most when reversing, albeit missing crucial
            details for the sake of brevity, and assumes the reader has a reflex
            of finding answers online, looking up definitions, and more
            importantly, coming up with examples/ideas/projects to practice on.
          </p>

          <p>
            The goal is to hopefully guide an aspiring reverse engineer and
            arouse motivation towards learning more about this seemingly elusive
            passion.
          </p>

          <p>
            <strong><em>Note</em></strong>: This article assumes the reader has elementary knowledge
            regarding the
            <a href="https://en.wikipedia.org/wiki/Hexadecimal" rel="noopener noreferrer" target="_blank">hexadecimal numeral system</a>, as well as the
            <a href="https://en.wikipedia.org/wiki/C_(programming_language)" rel="noopener noreferrer" target="_blank">C programming language</a>, and is based on a 32-bit Windows executable case study - results
            might differ across different OSes/architectures.
          </p>

          <h3 id="introduction">Introduction</h3>

          <h4 id="compilation">Compilation</h4>

          <p>
            After writing code using a
            <a href="https://en.wikipedia.org/wiki/Compiled_language" rel="noopener noreferrer" target="_blank">compiled language</a>, a compilation takes place <del>(duh)</del>, in order to generate
            the output binary file (an example of such is an .exe file).
          </p>

          <p>
            <img src="https://i.0x44.cc/b/compilation-c-to-exe-file.png">
          </p>

          <p>
            Compilers are sophisticated programs which do this task. They make
            sure the syntax of your <del>ugly</del> code is correct, before
            compiling and optimizing the resulting machine code by minimizing
            its size and improving its performance, whenever applicable.
          </p>

          <h4 id="binary-code">Binary code</h4>

          <p>
            As we were saying, the resulting output file contains binary code,
            which can only be ‚Äòunderstood‚Äô by a CPU, it‚Äôs essentially a
            succession of varying-length instructions to be executed in order -
            here‚Äôs what some of them look like:
          </p>

          <table>
            <thead>
              <tr>
                <th>CPU-readable instruction data (in hex)</th>
                <th>Human-readable interpretation</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>55</td>
                <td>push ebp</td>
              </tr>
              <tr>
                <td>8B EC</td>
                <td>mov ebp, esp</td>
              </tr>
              <tr>
                <td>83 EC 08</td>
                <td>sub esp, 8</td>
              </tr>
              <tr>
                <td>33 C5</td>
                <td>xor eax, ebp</td>
              </tr>
              <tr>
                <td>83 7D 0C 01</td>
                <td>cmp dword ptr [ebp+0Ch], 1</td>
              </tr>
            </tbody>
          </table>

          <p>
            These instructions are predominantly arithmetical, and they
            manipulate CPU registers/flags as well as volatile memory, as
            they‚Äôre executed.
          </p>

          <h4 id="cpu-registers">CPU registers</h4>

          <p>
            <a href="https://en.wikipedia.org/wiki/Processor_register" rel="noopener noreferrer" target="_blank">A CPU register</a>
            is almost like a temporary integer variable - there‚Äôs a small fixed
            number of them, and they exist because they‚Äôre quick to access,
            unlike memory-based variables, and they help the CPU keep track of
            its data (results, operands, counts, etc.) during execution.
          </p>

          <p>
            It‚Äôs important to note the presence of a special register called the
            <a href="https://en.wikipedia.org/wiki/FLAGS_register" rel="noopener noreferrer" target="_blank"><code>FLAGS</code>
              register</a>
            (<code>EFLAGS</code> on
            32-bit), which houses a bunch of flags (boolean indicators), which
            hold information about the state of the CPU, which include details
            about the last arithmetic operation (zero:
            <code>ZF</code>,
            overflow:
            <code>OF</code>,
            parity:
            <code>PF</code>, sign:
            <code>SF</code>, etc.).
          </p>

          <p>
            <img src="https://i.0x44.cc/b/x32dbg-cpu-registers.png">
            <small>CPU registers visualized while debugging a 32-bit process on
              x64dbg, a debugging tool.</small>
          </p>

          <p>
            Some of these registers can also be spotted on the assembly excerpt
            mentioned <a href="#binary-code">previously</a>, namely:
            <code>EAX</code>,
            <code>ESP</code> (stack
            pointer) and
            <code>EBP</code> (base
            pointer).
          </p>

          <h4 id="memory-access">Memory access</h4>

          <p>
            As the CPU executes stuff, it needs to access and interact with
            memory, that‚Äôs when the role of the <em>stack</em> and the
            <em>heap</em> comes.
          </p>

          <p>
            These are (without getting into too much detail) the 2 main ways of
            ‚Äòkeeping track of variable data‚Äô during the execution of a program:
          </p>

          <h5 id="-stack">ü•û <em>Stack</em></h5>
          <p>
            The simpler and faster of the two - it‚Äôs a linear contiguous LIFO
            (last in = first out) data structure with a push/pop mechanism, it
            serves to remember function-scoped variables, arguments, and keeps
            track of calls (ever heard of a
            <a href="https://en.wikipedia.org/wiki/Stack_trace" rel="noopener noreferrer" target="_blank">stack trace</a>?)
          </p>

          <h5 id="-heap">‚õ∞ <em>Heap</em></h5>
          <p>
            The heap, however, is pretty unordered, and is for more complicated
            data structures, it‚Äôs typically used for dynamic allocations, where
            the size of the buffer isn‚Äôt initially known, and/or if it‚Äôs too
            big, and/or needs to be modified later.
          </p>

          <h3 id="assembly-instructions">Assembly instructions</h3>

          <p>
            As I‚Äôve mentioned earlier, assembly instructions have a varying
            ‚Äòbyte-size‚Äô, and a varying number of arguments.
          </p>

          <p>
            Arguments can also be either immediate (‚Äòhardcoded‚Äô), or they can be
            registers, depending on the instruction:
          </p>

          <div>
              <pre><code>55         push    ebp     ; size: 1 byte,  argument: register
6A 01      push    1       ; size: 2 bytes, argument: immediate
</code></pre>
            </div>

          <p>
            Let‚Äôs quickly run through a very small set of some of the common
            ones we‚Äôll get to see - feel free to do your own research for more
            detail:
          </p>

          <h4 id="stack-operations">Stack operations</h4>
          <ul>
            <li>
              <strong>push
                <code>value</code></strong>
              <em>; pushes a value into the stack (decrements
                <code>ESP</code> by
                4, the size of one stack ‚Äòunit‚Äô).</em>
            </li>
            <li>
              <strong>pop
                <code>register</code></strong>
              <em>; pops a value to a register (increments
                <code>ESP</code> by
                4).</em>
            </li>
          </ul>

          <h4 id="data-transfer">Data transfer</h4>
          <ul>
            <li>
              <strong>mov
                <code>destination</code>,
                <code>source</code></strong>
              ; <em><del>moves</del> copies a value from/to a register.</em>
            </li>
            <li>
              <strong>mov
                <code>destination</code>, [<code>expression</code>]</strong>
              ;
              <em>copies a value from a memory address resolved from a ‚Äòregister
                expression‚Äô (single register or arithmetic expression involving
                one or more registers) into a register.</em>
            </li>
          </ul>

          <h4 id="flow-control">Flow control</h4>
          <ul>
            <li>
              <strong>jmp
                <code>destination</code></strong>
              ;
              <em>jumps into a code location (sets
                <code>EIP</code>
                (instruction pointer)).</em>
            </li>
            <li>
              <strong>jz/je
                <code>destination</code></strong>
              ;
              <em>jumps into a code location if
                <code>ZF</code>
                (the zero flag) is set.</em>
            </li>
            <li>
              <strong>jnz/jne
                <code>destination</code></strong>
              ;
              <em>jumps into a code location if
                <code>ZF</code> is
                not set.</em>
            </li>
          </ul>

          <h4 id="operations">Operations</h4>
          <ul>
            <li>
              <strong>cmp
                <code>operand1</code>,
                <code>operand2</code></strong>
              ;
              <em>compares the 2 operands and sets
                <code>ZF</code> if
                they‚Äôre equal.</em>
            </li>
            <li>
              <strong>add
                <code>operand1</code>,
                <code>operand2</code></strong>
              ; <em>operand1 += operand2;</em>
            </li>
            <li>
              <strong>sub
                <code>operand1</code>,
                <code>operand2</code></strong>
              ; <em>operand1 -= operand2;</em>
            </li>
          </ul>

          <h4 id="function-transitions">Function transitions</h4>
          <ul>
            <li>
              <strong>call
                <code>function</code></strong>
              ;
              <em>calls a function (pushes current
                <code>EIP</code>,
                then jumps to the function).</em>
            </li>
            <li>
              <strong>retn</strong> ;
              <em>returns to caller function (pops back the previous
                <code>EIP</code>).</em>
            </li>
          </ul>

          <p>
            <strong><em>Note</em></strong>: You might notice the words ‚Äòequal‚Äô and ‚Äòzero‚Äô being used
            interchangeably in x86 terminology - that‚Äôs because comparison
            instructions internally perform a subtraction, which means if the 2
            operands are equal,
            <code>ZF</code> is set.
          </p>

          <h3 id="assembly-patterns">Assembly patterns</h3>

          <p>
            Now that we have a rough idea of the main elements used during the
            execution of a program, let‚Äôs get familiarized with the patterns of
            instructions that you can encounter reverse engineering your average
            everyday 32-bit
            <a href="https://en.wikipedia.org/wiki/Portable_Executable" rel="noopener noreferrer" target="_blank">PE</a>
            binary.
          </p>

          <h4 id="function-prologue">Function prologue</h4>

          <p>
            A
            <a href="https://en.wikipedia.org/wiki/Function_prologue" rel="noopener noreferrer" target="_blank">function prologue</a>
            is some initial code embedded in the beginning of most functions, it
            serves to set up a new stack frame for said function.
          </p>

          <p>It typically looks like this (X being a number):</p>

          <div>
              <pre><code>55          push    ebp        ; preserve caller function's base pointer in stack
8B EC       mov     ebp, esp   ; caller function's stack pointer becomes base pointer (new stack frame)
83 EC XX    sub     esp, X     ; adjust the stack pointer by X bytes to reserve space for local variables
</code></pre>
            </div>

          <h4 id="function-epilogue">Function epilogue</h4>

          <p>
            The
            <a href="https://en.wikipedia.org/wiki/Function_epilogue" rel="noopener noreferrer" target="_blank">epilogue</a>
            is simply the opposite of the prologue - it undoes its steps to
            restore the stack frame of the caller function, before it returns to
            it:
          </p>

          <div>
              <pre><code>8B E5    mov    esp, ebp    ; restore caller function's stack pointer (current base pointer) 
5D       pop    ebp         ; restore base pointer from the stack
C3       retn               ; return to caller function
</code></pre>
            </div>

          <p>
            Now at this point, you might be wondering - how do functions talk to
            each other? How exactly do you send/access arguments when calling a
            function, and how do you receive the return value? That‚Äôs precisely
            why we have calling conventions.
          </p>

          <h4 id="calling-conventions-__cdecl">Calling conventions: __cdecl</h4>

          <p>
            A
            <a href="https://en.wikipedia.org/wiki/Calling_convention" rel="noopener noreferrer" target="_blank">calling convention</a>
            is basically a protocol used to communicate with functions, there‚Äôs
            a few variations of them, but they share the same principle.
          </p>

          <p>
            We will be looking at the
            <a href="https://en.wikipedia.org/wiki/X86_calling_conventions#cdecl" rel="noopener noreferrer" target="_blank">__cdecl (C declaration) convention</a>, which is the standard one when compiling C code.
          </p>

          <p>
            In __cdecl (32-bit), function arguments are passed on the stack
            (pushed in reverse order), while the return value is returned in the
            <code>EAX</code>
            register (assuming it‚Äôs not a float).
          </p>

          <p>
            This means that a
            <code>func(1, 2, 3);</code>
            call will generate the following:
          </p>

          <div>
              <pre><code>6A 03             push    3
6A 02             push    2
6A 01             push    1
E8 XX XX XX XX    call    func
</code></pre>
            </div>

          <h4 id="putting-everything-together">Putting everything together</h4>

          <p>
            Assuming
            <code>func()</code>
            simply does an addition on the arguments and returns the result, it
            would probably look like this:
          </p>

          <div>
              <pre><code>int __cdecl func(int, int, int):

           prologue:
55           push    ebp               ; save base pointer
8B EC        mov     ebp, esp          ; new stack frame

           body:
8B 45 08     mov     eax, [ebp+8]      ; load first argument to EAX (return value)
03 45 0C     add     eax, [ebp+0Ch]    ; add 2nd argument
03 45 10     add     eax, [ebp+10h]    ; add 3rd argument

           epilogue:
5D           pop     ebp               ; restore base pointer
C3           retn                      ; return to caller
</code></pre>
            </div>

          <p>
            Now if you‚Äôve been paying attention and you‚Äôre still confused, you
            might be asking yourself one of these 2 questions:
          </p>

          <p>
            1) Why do we have to adjust
            <code>EBP</code> by 8
            to get to the first argument?
          </p>

          <ul>
            <li>
              If you
              <a href="#assembly-instructions">check the definition</a> of the
              <code>call</code>
              instruction we mentioned earlier, you‚Äôll realize that, internally,
              it actually pushes
              <code>EIP</code> to
              the stack. And if you also check the definition for
              <code>push</code>,
              you‚Äôll realize that it decrements
              <code>ESP</code>
              (which is copied to
              <code>EBP</code>
              after the prologue) by 4 bytes. In addition, the prologue‚Äôs first
              instruction is also a
              <code>push</code>, so
              we end up with 2 decrements of 4, hence the need to add 8.
            </li>
          </ul>

          <p>
            2) What happened to the prologue and epilogue, why are they
            seemingly ‚Äòtruncated‚Äô?
          </p>

          <ul>
            <li>
              It‚Äôs simply because we haven‚Äôt had a use for the stack during the
              execution of our function - if you‚Äôve noticed, we haven‚Äôt modified
              <code>ESP</code> at
              all, which means we also don‚Äôt need to restore it.
            </li>
          </ul>

          <h4 id="if-conditions">If conditions</h4>

          <p>
            To demo the flow control assembly instructions, I‚Äôd like to add one
            more example to show how an if condition was compiled to assembly.
          </p>

          <p>Assume we have the following function:</p>

          <div>
              <pre><code><span>void</span> <span>print_equal</span><span>(</span><span>int</span> <span>a</span><span>,</span> <span>int</span> <span>b</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>a</span> <span>==</span> <span>b</span><span>)</span> <span>{</span>
        <span>printf</span><span>(</span><span>"equal"</span><span>);</span>
    <span>}</span>
    <span>else</span> <span>{</span>
        <span>printf</span><span>(</span><span>"nah"</span><span>);</span>
    <span>}</span>
<span>}</span>
</code></pre>
            </div>

          <p>
            After compiling it, here‚Äôs the disassembly that I got with the help
            of
            <a href="https://hex-rays.com/ida-pro/" rel="noopener noreferrer" target="_blank">IDA</a>:
          </p>

          <div>
              <pre><code>void __cdecl print_equal(int, int):

     10000000   55                push   ebp
     10000001   8B EC             mov    ebp, esp
     10000003   8B 45 08          mov    eax, [ebp+8]       ; load 1st argument
     10000006   3B 45 0C          cmp    eax, [ebp+0Ch]     ; compare it with 2nd
  ‚îå‚îÖ 10000009   75 0F             jnz    short loc_1000001A ; jump if not equal
  ‚îä  1000000B   68 94 67 00 10    push   offset aEqual  ; "equal"
  ‚îä  10000010   E8 DB F8 FF FF    call   _printf
  ‚îä  10000015   83 C4 04          add    esp, 4
‚îå‚îÄ‚îä‚îÄ 10000018   EB 0D             jmp    short loc_10000027
‚îÇ ‚îä
‚îÇ ‚îî loc_1000001A:
‚îÇ    1000001A   68 9C 67 00 10    push   offset aNah    ; "nah"
‚îÇ    1000001F   E8 CC F8 FF FF    call   _printf
‚îÇ    10000024   83 C4 04          add    esp, 4
‚îÇ
‚îî‚îÄ‚îÄ loc_10000027:
     10000027   5D                pop    ebp
     10000028   C3                retn
</code></pre>
            </div>

          <p>
            Give yourself a minute and try to make sense of this disassembly
            output (for simplicity‚Äôs sake, I‚Äôve changed the real addresses and
            made the function start from
            <code>10000000</code>
            instead).
          </p>

          <p>
            In case you‚Äôre wondering about the
            <code>add esp, 4</code>
            part, it‚Äôs simply there to adjust
            <code>ESP</code> back
            to its initial value (same effect as a
            <code>pop</code>,
            except without modifying any register), since we had to
            <code>push</code> the
            printf string argument.
          </p>

          <h3 id="basic-data-structures">Basic data structures</h3>

          <p>
            Now let‚Äôs move on and talk about how data is stored (integers and
            strings especially).
          </p>

          <h4 id="endianness">Endianness</h4>

          <p>
            <a href="https://en.wikipedia.org/wiki/Endianness" rel="noopener noreferrer" target="_blank">Endianness</a>
            is the order of the sequence of bytes representing a value in
            computer memory.
          </p>

          <p>There‚Äôs 2 types - big-endian and little-endian:</p>

          

          <p>
            For reference, x86 family processors (the ones on pretty much any
            computer you can find) always use little-endian.
          </p>

          <p>
            To give you a live example of this concept, I‚Äôve compiled a Visual
            Studio C++ console app, where I declared an
            <code>int</code>
            variable with the value
            <code>1337</code>
            assigned to it, then I printed the variable‚Äôs address using
            <code>printf()</code>,
            on the main function.
          </p>

          <p>
            Then I ran the program attached to the debugger in order to check
            the printed variable‚Äôs address on the memory hex view, and here‚Äôs
            the result I obtained:
          </p>

          <p>
            <img src="https://i.0x44.cc/b/vs-debug-memory-view.png" alt="">
          </p>

          <p>
            To elaborate more on this -
            <code>int</code>
            variables are 4 bytes long (32 bits) (in case you didn‚Äôt know), so
            this means that if the variable starts from the address
            <code>D2FCB8</code> it
            would end right before
            <code>D2FCBC</code>
            (+4).
          </p>

          <p>
            To go from human readable value to memory bytes, follow these steps:
          </p>

          <p>
            decimal:
            <code>1337</code> -&gt;
            hex:
            <code>539</code> -&gt;
            bytes:
            <code>00 00 05 39</code>
            -&gt; little-endian:
            <code>39 05 00 00</code>
          </p>

          <h4 id="signed-integers">Signed integers</h4>

          <p>
            This part is interesting yet relatively simple. What you should know
            here is that integer signing (positive/negative) is typically done
            on computers with the help of a concept called
            <a href="https://en.wikipedia.org/wiki/Signed_number_representations#Two's_complement" rel="noopener noreferrer" target="_blank">two‚Äôs complement</a>.
          </p>

          <p>
            The gist of it is that the lowest/first half of an integer is
            reserved for positive numbers, while the highest/last half is for
            negative numbers, here‚Äôs what this looks like in hex, for a 32-bit
            signed int (highlighted = hex, in parenthesis = decimal):
          </p>

          <p>
            Positives (1/2):
            <code>00000000</code>
            (0) -&gt;
            <code>7FFFFFFF</code>
            (2,147,483,647 or
            <code>INT_MAX</code>)
          </p>

          <p>
            Negatives (2/2):
            <code>80000000</code>
            (-2,147,483,648 or
            <code>INT_MIN</code>)
            -&gt;
            <code>FFFFFFFF</code>
            (-1)
          </p>

          <p>
            If you‚Äôve noticed, we‚Äôre always <em>ascending</em> in value. Whether
            we go up in hex or decimal. And that‚Äôs the crucial point of this
            concept - arithmetical operation do not have to do anything special
            to handle signing, they can simply treat all values as
            unsigned/positive, and the result would still be interpreted
            correctly (as long as we don‚Äôt go beyond
            <code>INT_MAX</code> or
            <code>INT_MIN</code>),
            and that‚Äôs because integers will also <em>‚Äòrollover‚Äô</em> on
            overflow/underflow by design, kinda like an analog odometer.
          </p>

          <p>
            <img src="https://i.0x44.cc/b/odometer-rollover.jpg">
          </p>

          <p>
            <strong><em>Protip</em></strong>: The Windows calculator is a very helpful tool - you can set it to
            programmer mode and set the size to DWORD (4 bytes), then enter
            negative decimal values and visualize them in hex and binary, and
            have fun performing operations on them.
          </p>

          <p>
            <img src="https://i.0x44.cc/b/calcexe-int-signing.png" alt="">
          </p>

          <h4 id="strings">Strings</h4>

          <p>
            In C, strings are stored as
            <code>char</code>
            arrays, therefore, there‚Äôs nothing special to note here, except for
            something called null termination.
          </p>

          <p>
            If you ever wondered how
            <code>strlen()</code>
            is able to know the size of a string, it‚Äôs very simple - strings
            have a character that indicates their end, and that‚Äôs the null
            byte/character -
            <code>00</code> or
            <code>'\0'</code>.
          </p>

          <p>
            If you declare a string constant in C code, and hover over it in
            Visual Studio, for instance, it will tell you the size of the
            generated array, and as you can see, for this reason, it‚Äôs one
            element more than the ‚Äòvisible‚Äô string size.
          </p>

          <p>
            <img src="https://i.0x44.cc/b/vs-null-termination.png" alt="">
          </p>

          <p>
            <strong><em>Note</em></strong>: The endianness concept is not applicable on arrays, only on
            single variables. Therefore, the order of characters in memory would
            be normal here - low to high.
          </p>

          <h3 id="making-sense-of-call-and-jmp-instructions">
            Making sense of
            <code>call</code> and
            <code>jmp</code>
            instructions
          </h3>

          <p>
            Now that you know all of this, you‚Äôre likely able to start making
            sense of some machine code, and emulate a CPU with your brain, to
            some extent, so to speak.
          </p>

          <p>
            Let‚Äôs take the
            <a href="#if-conditions"><code>print_equal()</code>
              example</a>, but let‚Äôs only focus on the
            <code>printf()</code>
            <code>call</code>
            instructions this time.
          </p>

          <div>
              <pre><code>void print_equal(int, int):
...
     10000010   E8 DB F8 FF FF    call   _printf
...
     1000001F   E8 CC F8 FF FF    call   _printf
</code></pre>
            </div>

          <p>
            You might be wondering to yourself - wait a second, if these are the
            same instructions, then why are their bytes different?
          </p>

          <p>
            That‚Äôs because,
            <code>call</code> (and
            <code>jmp</code>)
            instructions (usually) take an <em>offset</em> (relative address) as
            an argument, not an absolute address.
          </p>

          <p>
            An offset is basically the difference between the current location,
            and the destination, which also means that it can be either negative
            or positive.
          </p>

          <p>
            As you can see, the
            <a href="https://en.wikipedia.org/wiki/Opcode" rel="noopener noreferrer" target="_blank">opcode</a>
            of a
            <code>call</code>
            instruction that takes a 32-bit offset, is
            <code>E8</code>, and is
            followed by said offset - which makes the full instruction:
            <code>E8 XX XX XX XX</code>.
          </p>

          <p>
            Pull out your calculator,
            <del>why‚Äôd you close it so early?!</del> and calculate the
            difference between the offset of both instructions (don‚Äôt forget the
            endianness).
          </p>

          <p>
            You‚Äôll notice that (the absolute value of) this difference is the
            same as the one between the instruction addresses (<code>1000001F</code>
            -
            <code>10000010</code> =
            <code>F</code>):
          </p>

          <p>
            <img src="https://i.0x44.cc/b/calcexe-call-inst-diff.png" alt="">
          </p>

          <p>
            Another small detail that we should add, is the fact that the CPU
            only executes an instruction after fully ‚Äòreading‚Äô it, which means
            that by the time the CPU starts ‚Äòexecuting‚Äô,
            <code>EIP</code> (the
            instruction pointer) is already pointing at the
            <em>next</em> instruction to be executed.
          </p>

          <p>
            That‚Äôs why these offsets are actually accounting for this behaviour,
            which means that in order to get the <em>real</em> address of the
            target function, we have to also <em>add</em> the size of the
            <code>call</code>
            instruction: 5.
          </p>

          <p>
            Now let‚Äôs apply all these steps in order to resolve
            <code>printf()</code>‚Äôs
            address from the first instruction on the example:
          </p>

          <div>
              <pre><code>10000010   E8 DB F8 FF FF    call   _printf
</code></pre>
            </div>

          <p>
            1) Extract the offset from the instruction:
            <code>E8 (DB F8 FF FF)</code>
            -&gt;
            <code>FFFFF8DB</code>
            (-1829)
          </p>

          <p>
            2) Add it to the instruction address:
            <code>10000010</code> +
            <code>FFFFF8DB</code> =
            <code>0FFFF8EB</code>
          </p>

          <p>
            3) And finally, add the instruction size:
            <code>0FFFF8EB</code> +
            5 =
            <code>0FFFF8F0</code>
            (<code>&amp;printf</code>)
          </p>

          <p>
            The exact same principle applies to the
            <code>jmp</code>
            instruction:
          </p>

          <div>
              <pre><code>...
‚îå‚îÄ‚îÄ‚îÄ 10000018   EB 0D             jmp    short loc_10000027
...
‚îî‚îÄ‚îÄ loc_10000027:
     10000027   5D                pop    ebp
...
</code></pre>
            </div>
          <p>
            The only difference in this example is that
            <code>EB XX</code> is a
            short version
            <code>jmp</code>
            instruction - which means it only takes an 8-bit (1 byte) offset.
          </p>

          <p>
            Therefore:
            <code>10000018</code> +
            <code>0D</code> + 2 =
            <code>10000027</code>
          </p>

          <h3 id="conclusion">Conclusion</h3>

          <p>
            That‚Äôs it! You should now have enough information (and hopefully,
            motivation) to start your journey reverse engineering executables.
          </p>

          <p>
            Start by writing dummy C code, compiling it, and debugging it while
            single-stepping through the disassembly instructions (Visual Studio
            allows you to do this, by the way).
          </p>

          <p>
            <a href="https://godbolt.org/" rel="noopener noreferrer" target="_blank">Compiler Explorer</a>
            is also an extremely helpful website which compiles C code to
            assembly for you in real time using multiple compilers (select the
            <code>x86 msvc</code>
            compiler for Windows 32-bit).
          </p>

          <p>
            After that, you can try your luck with closed-source native
            binaries, by the help of disassemblers such as
            <a href="https://ghidra-sre.org/" rel="noopener noreferrer" target="_blank">Ghidra</a>
            and
            <a href="https://hex-rays.com/ida-free" rel="noopener noreferrer" target="_blank">IDA</a>, and debuggers such as
            <a href="https://x64dbg.com/" rel="noopener noreferrer" target="_blank">x64dbg</a>.
          </p>

          <p>
            <strong><em>Note</em></strong>: If you‚Äôve noticed inaccurate information, or room for improvement
            regarding this article, and would like to improve it, feel free to
            <a href="https://github.com/thedroidgeek/0x44.cc/edit/master/_posts/2021-07-21-reversing-x86-and-c-code-for-beginners.md" rel="noopener noreferrer" target="_blank">submit a pull request</a>
            on GitHub.
          </p>

          <p>Thanks for reading!</p>

          <p>
            <a href="https://github.com/thedroidgeek/0x44.cc/commits/master/_posts/2021-07-21-reversing-x86-and-c-code-for-beginners.md" rel="noopener noreferrer" target="_blank">(edited)</a>
          </p>

          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing Whistleblower: "If Anything Happens to Me, It's Not Suicide" (132 pts)]]></title>
            <link>https://twitter.com/WallStreetSilv/status/1768517997285482626</link>
            <guid>39715161</guid>
            <pubDate>Fri, 15 Mar 2024 13:20:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/WallStreetSilv/status/1768517997285482626">https://twitter.com/WallStreetSilv/status/1768517997285482626</a>, See on <a href="https://news.ycombinator.com/item?id=39715161">Hacker News</a></p>
Couldn't get https://twitter.com/WallStreetSilv/status/1768517997285482626: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[IAM Is the Worst (210 pts)]]></title>
            <link>https://matduggan.com/iam-is-the-worst/</link>
            <guid>39714155</guid>
            <pubDate>Fri, 15 Mar 2024 10:55:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matduggan.com/iam-is-the-worst/">https://matduggan.com/iam-is-the-worst/</a>, See on <a href="https://news.ycombinator.com/item?id=39714155">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
<p>Imagine your job was to clean a giant office building. You go from floor to floor, opening doors, collecting trash, getting a vacuum out of the cleaning closet and putting it back. It's a normal job and part of that job is someone gives you a key. The key opens every door everywhere. Everyone understands the key is powerful, but they also understand you need to do your job. </p><p>Then your management hears about someone stealing janitor keys. So they take away your universal key and they say "you need to tell Suzie, our security engineer, which keys you need at which time". But the keys don't just unlock one door, some unlock a lot of doors and some desk drawers, some open the vault (imagine this is the Die Hard building), some don't open any doors but instead turn on the coffee machine. Obviously the keys have titles, but the titles mean nothing. Do you need the "executive_floor/admin" key or the "executive_floor/viewer" key? </p><p>But you are a good employee and understand that security is a part of the job. So you dutifully request the keys you think you need, try to do your job, open a new ticket when the key doesn't open a door you want, try it again, it still doesn't open the door you want so then there's another key. Soon your keyring is massive, just a clanging sound as you walk down the hallway. It mostly works, but a lot of the keys open stuff you don't need, which makes you think maybe this entire thing was pointless. </p><p>The company is growing and we need new janitors, but they don't want to give all the new janitors your key ring. So they roll out a new system which says "now the keys can only open doors that we have written down that this key can open, even if it says "executive_floor/admin". The problem is people move offices all the time, so even if the list of what doors that key opened was true when it was issued, it's not true tomorrow. The Security team and HR share a list, but the list sometimes drifts or maybe someone moves offices without telling the right people. </p><p>Soon nobody is really 100% sure what you can or cannot open, including you. Sure someone can audit it and figure it out, but the risk of removing access means you cannot do your job and the office doesn't get cleaned. So practically speaking the longer someone works as a janitor the more doors they can open until eventually they have the same level of access as your original master key even if that wasn't the intent. </p><p>That's IAM (Identity and access management) in cloud providers today. </p><h3 id="stare-into-madness">Stare Into Madness</h3><figure><img src="https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/PolicyEvaluationHorizontal111621.png" alt="" loading="lazy"><figcaption><span>AWS IAM Approval Flow</span></figcaption></figure><figure><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image1_copy_3.max-2000x2000.jpg" alt="" loading="lazy"><figcaption><span>GCP IAM Approval Flow</span></figcaption></figure><figure><img src="https://images.fastcompany.com/upload/Simple.jpg" alt="It's Not Natural, It's Just Simple: Food Branding Co-Opts Another Mean" loading="lazy"></figure><p>Honestly I don't even know why I'm complaining. Of course it's entirely reasonable to expect anyone working in a cloud environment to understand the dozen+ ways that they may or may not have access to a particular resource. Maybe they have permissions at a folder level, or an org level, but that permission is gated by specific resources. </p><p>Maybe they don't even have access but the tool they're interacting with the resource with has permission to do it, so they can do it but only as long as they are SSH'd into host01, not if they try to do it through some cloud shell. Possibly they had access to it before, but now they don't since they moved teams. Perhaps the members of this team were previously part of some existing group but now new employees aren't added to that group so some parts of the team can access X but others cannot. Or they actually have the correct permissions to the resource but the resource is located in another account and they don't have the right permission to traverse the networking link between the two VPCs.</p><p>Meanwhile someone is staring at these flowcharts trying to figure out what in hell is even happening here. As someone who has had to do this multiple times in my life, let me tell you the real-world workflow that ends up happening. </p><ul><li>Developer wants to launch a new service using new cloud products. They put in a ticket for me to give them access to the correct "roles" to do this. </li><li>I need to look at two elements of it, both what are the permissions the person needs in order to see if the thing is working and then the permissions the service needs in order to complete the task it is trying to complete. </li><li>So I go through my giant list of roles and try to cobble together something that I think based on the names will do what I want. Do you feel like a <code>roles/datastore.viewer</code> or more of a <code>roles/datastore.keyVisualizerViewer</code>? To run backups is <code>roles/datastore.backupsAdmin</code> sufficient or do I need to add <code>roles/datastore.backupSchedulesAdmin</code> in there as well?</li><li>They try it and it doesn't work. Reopen the ticket with "I still get authorizationerror:foo". I switch that role with a different role, try it again. Run it through the simulator, it seems to work, but they report a new different error because actually in order to use service A you need to also have a role in service B. Go into bathroom, scream into the void and return to your terminal.</li><li>We end up cobbling together a custom role that includes all the permissions that this application needs and the remaining 90% of permissions are something it will never ever use but will just sit there as a possible security hole. </li><li>Because /* permissions are the work of Satan, I need to scope it to specific instances of that resource and just hope nobody ever adds a SQS queue without....checking the permissions I guess. In theory we should catch it in the non-prod environments but there's always the chance that someone messes up something at a higher level of permissions that does something in non-prod and doesn't exist in prod so we'll just kinda cross our fingers there. </li></ul><h3 id="gcp-makes-it-worse">GCP Makes It Worse</h3><p>So that's effectively the AWS story, which is terrible but at least it's possible to cobble together something that works and you can audit. Google looked at this and said "what if we could express how much we hate Infrastructure teams as a service?" Expensive coffee robots were engaged, colorful furniture was sat on and the brightest minds of our generation came up with a system so punishing you'd think you did something to offend them personally. </p><p>Google looked at AWS and said "this is a tire fire" as corporations put non-prod and prod environments in the same accounts and then tried to divide them by conditionals. So they came up with a folder structure:</p><figure><img src="https://infosec.rodeo/assets/img/blog/gcp_resource_hierarchy.png" alt="GCP Resource Hierarchy" loading="lazy"></figure><p>The problem is that this design encourages unsafe practices by promoting "groups should be set at the folder level with one of the default basic roles". It makes sense logically at first that you are a viewer, editor or owner. But as GCP adds more services this model breaks down quickly because each one of these encompasses thousands upon thousands of permissions. So additional IAM predefined roles were layered on. </p><p>People were encouraged to move away from the basic roles and towards the predefined roles. There are ServiceAgent roles that were designated for service accounts, aka the permissions you actual application has and then everything else. Then there are 1687 other roles for you to pick from to assign to your groups of users. </p><figure><img src="https://matduggan.com/content/images/2024/03/image-2.png" alt="" loading="lazy" width="860" height="334" srcset="https://matduggan.com/content/images/size/w600/2024/03/image-2.png 600w, https://matduggan.com/content/images/2024/03/image-2.png 860w" sizes="(min-width: 720px) 720px"></figure><p>The problem is none of this is actually best practice. Even when assigning users "small roles", we're still not following the principal of least privilege. Also the roles don't remain static. As new services come online permissions are added to roles. </p><figure><img src="https://matduggan.com/content/images/2024/03/image-4.png" alt="" loading="lazy" width="2000" height="1254" srcset="https://matduggan.com/content/images/size/w600/2024/03/image-4.png 600w, https://matduggan.com/content/images/size/w1000/2024/03/image-4.png 1000w, https://matduggan.com/content/images/size/w1600/2024/03/image-4.png 1600w, https://matduggan.com/content/images/size/w2400/2024/03/image-4.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>The above is an automated process that pulls down all the roles from the gcloud CLI tool and updates them for latest. It is a constant state of flux with roles with daily changes. It gets even more complicated though. </p><p>You also need to check the launch stage of a role. </p><blockquote>Custom roles include a launch stage as part of the role's metadata. The most common launch stages for custom roles are ALPHA, BETA, and GA. These launch stages are informational; they help you keep track of whether each role is ready for widespread use. Another common launch stage is DISABLED. This launch stage lets you disable a custom role.</blockquote><blockquote>We recommend that you use launch stages to convey the following information about the role:</blockquote><blockquote>EAP or ALPHA: The role is still being developed or tested, or it includes permissions for Google Cloud services or features that are not yet public. It is not ready for widespread use.<br>BETA: The role has been tested on a limited basis, or it includes permissions for Google Cloud services or features that are not generally available.<br>GA: The role has been widely tested, and all of its permissions are for Google Cloud services or features that are generally available.<br>DEPRECATED: The role is no longer in use.</blockquote><h3 id="who-cares">Who Cares?</h3><p>Why would anyone care if Google is constantly changing roles? Well it matters because with GCP to make a custom role, you cannot combine predefined roles. Instead you need to go down to the permission level to list out all of the things those roles can do, then feed that list of permissions into the definition of your custom role and push that up to GCP. </p><p>In order to follow best practices this is what you have to do. Otherwise you will always be left with users that have a ton of unused permissions along with the fear of a security breach allowing someone to execute commands in your GCP account through an applications service account that cause way more damage than the actual application justifies. </p><p>So you get to build automated tooling which either queries the predefined roles for change over time and roll those into your custom roles so that you can assign a user or group one specific role that lets them do everything they need. Or you can assign these same folks multiple of the 1600+ predefined roles, accept that they have permissions they don't need and also just internalize that day to day you don't know how much the scope of those permissions have changed. </p><h3 id="the-obvious-solution">The Obvious Solution</h3><p>Why am I ranting about this? Because the solution is so blindly obvious I don't understand why we're not already doing it. It's a solution I've had to build, myself, multiple times and at this point am furious that this keeps being my responsibility as I funnel hundreds of thousands of dollars to cloud providers. </p><p>What is this obvious solution? You, an application developer, need to launch a new service. I give you a service account that lets you do almost everything inside of that account along with a viewer account for your user that lets you go into the web console and see everything. You churn away happily, writing code that uses all those new great services. Meanwhile, we're tracking all the permissions your application and you are using. </p><p>At some time interval, 30 or 90 or whatever days, my tool looks at the permissions your application has used over the last 90 days and says "remove the global permissions and scope it to these". I don't need to ask you what you need, because I can see it. In the same vein I do the same thing with your user or group permissions. You don't need viewer everywhere because I can see what you've looked at. </p><p>Both GCP and AWS support this and have all this functionality baked in. GCP has the <a href="https://cloud.google.com/policy-intelligence/docs/role-recommendations-overview" rel="noreferrer">role recommendations</a> which tracks exactly what I'm talking about and recommends lowering the role. <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_access-advisor.html" rel="noreferrer">AWS tracks the exact same information</a> and can be used to do the exact same thing. </p><p><strong>What if the user needs different permissions in a hurry?</strong></p><p>This is not actually that hard to account for and <em>again</em> is something I and countless others have been forced to make over and over. You can issue expiring permissions in both situations where a user can request a role be temporarily granted to them and then it disappears in 4 hours. I've seen every version of these, from Slack bots to websites, but they're all the same thing. If user is in X group they're allowed to request Y temporary permissions. OR if the user is on-call as determined with an API call to the on-call provider they get more powers. Either design works fine. </p><p><strong>That seems like a giant security hole</strong></p><p>Compared to what? Team A guessing what Team B needs even though they don't ever do the work that Team B does? Some security team receiving a request for permissions and trying to figure out if the request "makes sense" or not? At least this approach is based on actual data and not throwing darts at a list of IAM roles and seeing what "feels right". </p><h3 id="conclusion">Conclusion</h3><p>IAM started out as an easy idea that as more and more services were launched, started to become nightmarish to organize. It's too hard to do the right thing now and it's even harder to do the right thing in GCP compared to AWS. The solution is not complicated. We have all the tools, all the data, we understand how they fit together. We just need one of the providers to be brave enough to say "obviously we messed up and this legacy system you all built your access control on is bad and broken". It'll be horrible, we'll all grumble and moan but in the end it'll be a better world for us all. </p><p>Feedback: <a href="https://c.im/@matdevdug">https://c.im/@matdevdug</a></p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Matrix Multiplication with Half the Multiplications (211 pts)]]></title>
            <link>https://github.com/trevorpogue/algebraic-nnhw</link>
            <guid>39714053</guid>
            <pubDate>Fri, 15 Mar 2024 10:36:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/trevorpogue/algebraic-nnhw">https://github.com/trevorpogue/algebraic-nnhw</a>, See on <a href="https://news.ycombinator.com/item?id=39714053">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">This repository contains the source code for ML hardware architectures that require nearly half the number of multiplier units to achieve the same performance, by executing alternative inner-product algorithms that trade nearly half the multiplications for cheap low-bitwidth additions, while still producing identical output as the conventional inner product. This increases the theoretical throughput and compute efficiency limits of ML accelerators. See the following journal publication for the full details:</p>
<p dir="auto">T. E. Pogue and N. Nicolici, "Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators," in IEEE Transactions on Computers, vol. 73, no. 2, pp. 495-509, Feb. 2024, doi: 10.1109/TC.2023.3334140.</p>

<p dir="auto">Article URL: <a href="https://ieeexplore.ieee.org/document/10323219" rel="nofollow">https://ieeexplore.ieee.org/document/10323219</a></p>
<p dir="auto">Open-access version: <a href="https://arxiv.org/abs/2311.12224" rel="nofollow">https://arxiv.org/abs/2311.12224</a></p>
<p dir="auto">Abstract: We introduce a new algorithm called the Free-pipeline Fast Inner Product (FFIP) and its hardware architecture that improve an under-explored fast inner-product algorithm (FIP) proposed by Winograd in 1968. Unlike the unrelated Winograd minimal filtering algorithms for convolutional layers, FIP is applicable to all machine learning (ML) model layers that can mainly decompose to matrix multiplication, including fully-connected, convolutional, recurrent, and attention/transformer layers. We implement FIP for the first time in an ML accelerator then present our FFIP algorithm and generalized architecture which inherently improve FIP's clock frequency and, as a consequence, throughput for a similar hardware cost. Finally, we contribute ML-specific optimizations for the FIP and FFIP algorithms and architectures. We show that FFIP can be seamlessly incorporated into traditional fixed-point systolic array ML accelerators to achieve the same throughput with half the number of multiply-accumulate (MAC) units, or it can double the maximum systolic array size that can fit onto devices with a fixed hardware budget. Our FFIP implementation for non-sparse ML models with 8 to 16-bit fixed-point inputs achieves higher throughput and compute efficiency than the best-in-class prior solutions on the same type of compute platform.</p>
<p dir="auto">The following diagram shows an overview of the ML accelerator system implemented in this source code:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/12535207/285502293-11a7d485-04a3-4e9d-b9fb-91c35c80086f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8yODU1MDIyOTMtMTFhN2Q0ODUtMDRhMy00ZTlkLWI5ZmItOTFjMzVjODAwODZmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjZTJkM2MyZDdkODdkNDZlODg3NTJkZDcyZmYzNjhhMzJiYmM1NzRlNDk3MjM3YTM3MzMwMjNkODllOTdiZGUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.9a6yVkzLncS7pXRGFoxzDOrjm01sfCz7jQEFtOGf5oE"><img src="https://private-user-images.githubusercontent.com/12535207/285502293-11a7d485-04a3-4e9d-b9fb-91c35c80086f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8yODU1MDIyOTMtMTFhN2Q0ODUtMDRhMy00ZTlkLWI5ZmItOTFjMzVjODAwODZmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjZTJkM2MyZDdkODdkNDZlODg3NTJkZDcyZmYzNjhhMzJiYmM1NzRlNDk3MjM3YTM3MzMwMjNkODllOTdiZGUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.9a6yVkzLncS7pXRGFoxzDOrjm01sfCz7jQEFtOGf5oE" width="450"></a></p>
<p dir="auto">The FIP and FFIP systolic array/MXU processing elements (PE)s shown below in (b) and (c) implement the FIP and FFIP inner-product algorithms and each individually provide the same effective computational power as the two baseline PEs shown in (a) combined which implement the baseline inner product as in previous systolic-array ML accelerators:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/12535207/300184475-d9b956a2-25fa-4173-8ba9-8fd27d02f0c1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8zMDAxODQ0NzUtZDliOTU2YTItMjVmYS00MTczLThiYTktOGZkMjdkMDJmMGMxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRkMWY4NjIyODBkNDAwNjExNzZkZTJiMmJiZDk0YTRlY2I1Zjk0MDE1YjZlM2UyMGNiMzIyYzA1NjQyZGY0OTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.433P7aTz05yurSzH_kAJDMyMGRZZAi1IUDxKuesO_zU"><img src="https://private-user-images.githubusercontent.com/12535207/300184475-d9b956a2-25fa-4173-8ba9-8fd27d02f0c1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8zMDAxODQ0NzUtZDliOTU2YTItMjVmYS00MTczLThiYTktOGZkMjdkMDJmMGMxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRkMWY4NjIyODBkNDAwNjExNzZkZTJiMmJiZDk0YTRlY2I1Zjk0MDE1YjZlM2UyMGNiMzIyYzA1NjQyZGY0OTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.433P7aTz05yurSzH_kAJDMyMGRZZAi1IUDxKuesO_zU" width="450"></a></p>
<p dir="auto">The following is a diagram of the MXU/systolic array and shows how the PEs are connected:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/12535207/300986120-baf3e2f7-1767-49ec-811e-7cb44fac8d92.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8zMDA5ODYxMjAtYmFmM2UyZjctMTc2Ny00OWVjLTgxMWUtN2NiNDRmYWM4ZDkyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTBlYWRkZWQ0MjM3YWJkNWRkZjYyOGNmZTc5ZWQ1M2RkNjEyYmFiOGZjOWFkYTg4ZGZhNjlhNmUwOThkOTViNzEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QnLh8a3ntuJUB_ICABvi0rG-Czg0Yt2kVquJBTFXLZ0"><img src="https://private-user-images.githubusercontent.com/12535207/300986120-baf3e2f7-1767-49ec-811e-7cb44fac8d92.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8zMDA5ODYxMjAtYmFmM2UyZjctMTc2Ny00OWVjLTgxMWUtN2NiNDRmYWM4ZDkyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTBlYWRkZWQ0MjM3YWJkNWRkZjYyOGNmZTc5ZWQ1M2RkNjEyYmFiOGZjOWFkYTg4ZGZhNjlhNmUwOThkOTViNzEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QnLh8a3ntuJUB_ICABvi0rG-Czg0Yt2kVquJBTFXLZ0" width="450"></a></p>
<p dir="auto">The source code organization is as follows:</p>
<ul dir="auto">
<li>compiler
<ul dir="auto">
<li>A compiler for parsing Python model descriptions into accelerator instructions that allow it to accelerate the model. This part also includes code for interfacing with a PCIe driver for initiating model execution on the accelerator, reading back results and performance counters, and testing the correctness of the results.</li>
</ul>
</li>
<li>rtl
<ul dir="auto">
<li>Synthesizable SystemVerilog RTL.</li>
</ul>
</li>
<li>sim
<ul dir="auto">
<li>Scripts for setting up simulation environments for testing.</li>
</ul>
</li>
<li>tests
<ul dir="auto">
<li>UVM-based testbench source code for verifying the accelerator in simulation using Cocotb.</li>
</ul>
</li>
<li>utils
<ul dir="auto">
<li>Additional Python packages and scripts used in this project that the author created for general development utilities and aids.</li>
</ul>
</li>
</ul>
<p dir="auto">The files rtl/top/define.svh and rtl/top/pkg.sv contain a number of configurable parameters such as FIP_METHOD in define.svh which defines the systolic array type (baseline, FIP, or FFIP), SZI and SZJ which define the systolic array height/width, and LAYERIO_WIDTH/WEIGHT_WIDTH which define the input bitwidths.</p>
<p dir="auto">The directory rtl/arith includes mxu.sv and mac_array.sv which contain the RTL for the baseline, FIP, and, FFIP systolic array architectures (depending on the value of the parameter FIP_METHOD).</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking (244 pts)]]></title>
            <link>https://arxiv.org/abs/2403.09629</link>
            <guid>39713634</guid>
            <pubDate>Fri, 15 Mar 2024 09:24:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.09629">https://arxiv.org/abs/2403.09629</a>, See on <a href="https://news.ycombinator.com/item?id=39713634">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2403.09629">Download PDF</a>
    <a href="https://arxiv.org/html/2403.09629v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM's ability to directly answer difficult questions. In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and observe a perplexity improvement of difficult tokens in natural text. Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Eric Zelikman [<a href="https://arxiv.org/show-email/094380da/2403.09629">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 14 Mar 2024 17:58:16 UTC (510 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sunlight, a Certificate Transparency log implementation (162 pts)]]></title>
            <link>https://letsencrypt.org/2024/03/14/introducing-sunlight.html</link>
            <guid>39713370</guid>
            <pubDate>Fri, 15 Mar 2024 08:37:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://letsencrypt.org/2024/03/14/introducing-sunlight.html">https://letsencrypt.org/2024/03/14/introducing-sunlight.html</a>, See on <a href="https://news.ycombinator.com/item?id=39713370">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<article>
		<p><img alt="Logo for Sunlight" src="https://letsencrypt.org/images/blog/sunlight_logo_main.png">
</p>
<p>Let‚Äôs Encrypt is proud to introduce Sunlight, a new implementation of a Certificate Transparency log that we built from the ground up with modern Web PKI opportunities and constraints in mind. In partnership with <a href="https://filippo.io/">Filippo Valsorda</a>, who led the design and implementation, we incorporated feedback from the broader transparency logging community, including the Chrome and TrustFabric teams at Google, the Sigsum project, and other CT log and monitor operators. Their insights have been instrumental in shaping the project‚Äôs direction.</p>
<p>CT plays an important role in the Web PKI, enhancing the ability to monitor and research certificate issuance. The operation of a CT log, however, faces growing challenges with the increasing volume of certificates. For instance, Let‚Äôs Encrypt issues over four million certificates daily, each of which must be logged in two separate CT logs. Our well-established ‚ÄúOak‚Äù log currently holds over 700 million entries, reflecting the significant scale of these challenges.</p>
<p>In this post, we‚Äôll explore the motivation behind Sunlight and how its design aims to improve the robustness and diversity of the CT ecosystem, while also improving the reliability and performance of Let‚Äôs Encrypt‚Äôs logs.</p>
<h2 id="bottlenecks-from-the-database">Bottlenecks from the Database</h2>
<p>Let‚Äôs Encrypt has been <a href="https://letsencrypt.org/docs/ct-logs/">running public CT logs</a> since 2019, and we‚Äôve gotten a lot of operational experience with running them, but it hasn‚Äôt been trouble-free. The biggest challenge in the architecture we‚Äôve deployed for our ‚ÄúOak‚Äù log is that the data is stored in a relational database. We‚Äôve <a href="https://letsencrypt.org/2022/05/19/nurturing-ct-log-growth">scaled that up</a> by splitting each year‚Äôs worth of data into a ‚Äúshard‚Äù with its own database, and then later shrinking the shards to cover six months instead of a full year.</p>
<p>The approach of splitting into more and more databases is not something we want to continue doing forever, as the operational burden and costs increase. The current storage size of a CT log shard is between 5 and 10 terabytes. That‚Äôs big enough to be concerning for a single database: We previously had a test log fail when we ran into a <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.KnownIssuesAndLimitations.html#MySQL.Concepts.Limits.FileSize">16TiB limit</a> in MySQL.</p>
<p>Scaling read capacity up requires large database instances with fast disks and lots of RAM, which are not cheap. We‚Äôve had numerous instances of CT logs becoming overloaded by clients attempting to read all the data in the log, overloading the database in the process. When rate limits are imposed to prevent overloading, clients are forced to slowly crawl the API, diminishing CT‚Äôs efficiency as a fast mechanism for detecting mis-issued certificates.</p>
<h2 id="serving-tiles">Serving Tiles</h2>
<p>Initially, Let‚Äôs Encrypt only planned on building a new CT log implementation. However, our discussions with Filippo made us realize that other transparency systems had improved on the original Certificate Transparency design, and we could make our logs even more robust and scalable by changing the read path APIs. In particular, the <a href="https://golang.org/design/25530-sumdb">Go Checksum Database</a> is inspired by Certificate Transparency, but uses a more efficient format for publishing its data as a series of easily stored and cached tiles.</p>
<p>Certificate Transparency logs are a binary tree, with every node containing a hash of its two children. The ‚Äúleaf‚Äù level contains the actual entries of the log: the certificates, appended to the right side of the tree. The top of the tree is digitally signed. This forms a cryptographically verifiable structure called a Merkle Tree, which can be used to check if a certificate is in the tree, and that the tree is append-only.</p>
<p>Sunlight tiles are files containing 256 elements each, either hashes at a certain tree ‚Äúheight‚Äù or certificates (or pre-certificates) at the leaf level. Russ Cox has a great explanation <a href="https://research.swtch.com/tlog#tiling_a_log">of how tiles work on his blog</a>, or you can read <a href="https://c2sp.org/sunlight#merkle-tree">the relevant section of the Sunlight specification</a>. Even Trillian, the current implementation of CT we run, <a href="https://github.com/google/trillian/blob/master/docs/storage/storage.md">uses a subtree system</a> similar to these tiles as its internal storage.</p>
<p>Unlike the dynamic endpoints in previous CT APIs, serving a tree as tiles doesn‚Äôt require any dynamic computation or request processing, so we can eliminate the need for API servers. Because the tiles are static, they‚Äôre efficiently cached, in contrast with CT APIs like get-proof-by-hash which have a different response for every certificate, so there‚Äôs no shared cache. The leaf tiles can also be stored compressed, saving even more storage!</p>
<p>The idea of exposing the log as a series of static tiles is motivated by our desire to scale out the read path horizontally and relatively inexpensively. We can directly expose tiles in cloud object storage like S3, use a caching CDN, or use a webserver and a filesystem.</p>
<p>Object or file storage is readily available, can scale up easily, and costs significantly less than databases from cloud providers. It seemed like the obvious path forward. In fact, we already have an S3-backed cache in front of our existing CT logs, which means we are currently storing our data twice.</p>
<h2 id="running-more-logs">Running More Logs</h2>
<p>The tiles API improves the read path, but we also wanted to simplify our architecture on the write path. With Trillian, we run a collection of nodes along with etcd for leader election to choose which will handle writing. This is somewhat complex, and we believe the CT ecosystem allows a different tradeoff.</p>
<p>The key realization is that Certificate Transparency is already a distributed system, with clients submitting certificates to multiple logs, and gracefully failing over from any unavailable ones to the others. Each individual log‚Äôs write path doesn‚Äôt require a highly available leader election system. A simple single-node writer can meet the 99% Service Level Objective of a CT log.</p>
<p>The single-node Sunlight architecture lets us run multiple independent logs with the same amount of computing power. This increases the system‚Äôs overall robustness, even if each individual log has lower potential uptime. No more leader election needed. We use a simple compare-and-swap mechanism to store checkpoints and prevent accidentally running two instances at once, which could result in a forked tree, but that has much less overhead than leader election.</p>
<h2 id="no-more-merge-delay">No More Merge Delay</h2>
<p>One of the goals of CT was to have limited latency for submission to the logs. A design feature called Merge Delay was added to support that. When submitting a certificate to a log, the log can return a Signed Certificate Timestamp (SCT) immediately, with a promise to include it in the log within the log‚Äôs Maximum Merge Delay, conventionally 24 hours. While this seems like a good tradeoff to not slow down issuance, there have been multiple incidents and near-misses where a log stops operating with unmerged certificates, missing its maximum merge delay, and breaking that promise.</p>
<p>Sunlight takes a different approach, holding submissions while it batches and integrates certificates in the log, eliminating the merge delay. While this leads to a small latency increase, we think it‚Äôs worthwhile to avoid one of the more common CT log failure cases.</p>
<p>It also lets us embed the final leaf index in an extension of our SCTs, bringing CT a step closer to direct client verification of Merkle tree proofs. The extension also makes it possible for clients to fetch the proof of log inclusion from the new static tile-based APIs, without requiring server-side lookup tables or databases.</p>
<h2 id="a-sunny-future">A Sunny Future</h2>
<p>Today‚Äôs announcement of Sunlight is just the beginning. We‚Äôve released <a href="https://github.com/FiloSottile/sunlight">software</a> and a <a href="https://c2sp.org/sunlight">specification</a> for Sunlight, and have Sunlight CT logs running. Head to <a href="https://sunlight.dev/">sunlight.dev</a> to find resources to get started. We encourage CAs to start test submitting to <a href="https://letsencrypt.org/docs/ct-logs/#Sunlight">Let‚Äôs Encrypt‚Äôs new Sunlight CT logs</a>, for CT Monitors and Auditors to add support for consuming Sunlight logs, and for the CT programs to consider trusting logs running on this new architecture. We hope Sunlight logs will be made usable for SCTs by the CT programs run by the browsers in the future, allowing CAs to rely on them to meet the browser CT logging requirements.</p>
<p>We‚Äôve gotten positive feedback so far, with comments such as ‚ÄúGoogle‚Äôs TrustFabric team, maintainers of Trillian, are supportive of this direction and the Sunlight spec. We have been working towards the same goal of cacheable tile-based logs for other ecosystems with <a href="https://github.com/transparency-dev/serverless-log">serverless tooling</a>, and will be folding this into Trillian and ctfe, along with adding support for the Sunlight API.‚Äù</p>
<p>If you have feedback on the design, please join in the conversation on the <a href="https://groups.google.com/a/chromium.org/g/ct-policy">ct-policy mailing list</a>, or in the <a href="https://transparency-dev.slack.com/archives/C06PCS2P75Y">#sunlight</a> channel on the transparency-dev Slack (<a href="https://join.slack.com/t/transparency-dev/shared_invite/zt-27pkqo21d-okUFhur7YZ0rFoJVIOPznQ">invitation</a> to join).</p>
<p>We‚Äôd like to thank Chrome for supporting the development of Sunlight, and Amazon Web Services for their ongoing support for our CT log operation. If your organization monitors or values CT, please consider a financial gift of support. Learn more at <a href="https://www.abetterinternet.org/sponsor/">https://www.abetterinternet.org/sponsor/</a> or contact us at: <a href="mailto:sponsor@abetterinternet.org">sponsor@abetterinternet.org</a>.</p>

	</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Berlin's techno scene added to Unesco intangible cultural heritage list (358 pts)]]></title>
            <link>https://www.theguardian.com/world/2024/mar/15/berlins-techno-scene-added-to-unesco-intangible-cultural-heritage-list</link>
            <guid>39713323</guid>
            <pubDate>Fri, 15 Mar 2024 08:29:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/2024/mar/15/berlins-techno-scene-added-to-unesco-intangible-cultural-heritage-list">https://www.theguardian.com/world/2024/mar/15/berlins-techno-scene-added-to-unesco-intangible-cultural-heritage-list</a>, See on <a href="https://news.ycombinator.com/item?id=39713323">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Germany‚Äôs culture ministry and Unesco commission have added Berlin‚Äôs techno scene to the country‚Äôs list of intangible cultural heritage, in recognition of the scene‚Äôs contribution to the cultural identity of the city.</p><p>Berlin‚Äôs Clubcommission, a network for Berlin‚Äôs techno clubs and musicians, <a href="https://twitter.com/clubcommission/status/1767979957173580132" data-link-name="in body link">described</a> the move as ‚Äúanother milestone for Berlin techno producers, artists, club operators and event organisers‚Äù.</p><p>Lutz Leichsenring, an executive member of Clubcommission‚Äôs board, <a href="https://www.dw.com/en/berlin-techno-added-to-unesco-cultural-heritage-list/a-68515354" data-link-name="in body link">told the German broadcaster DW</a>: ‚ÄúThe decision will help us ensure that club culture is recognised as a valuable sector worthy of protection and support.‚Äù</p><p>For more than a decade there has been a campaign to have techno culture and music added to Germany‚Äôs list, spearheaded by Rave the Planet, a non-profit supporting electronic music culture.</p><p>‚ÄúCongratulations to all the cultural creators who have shaped and contributed to Berlin‚Äôs techno culture,‚Äù the group said in a statement on social media. ‚ÄúThis is a major milestone for the entire culture, and our joy is beyond words.‚Äù</p><p>Rave the Planet submitted the application for techno to be included in the list in November 2022.</p><p>Intangible cultural heritage status is more commonly granted to more traditional cultural activities, such as Malawian Mwinoghe dancing or Slovakian bagpipe culture. The recent recognition on Unesco‚Äôs list of intangible cultural heritage of Jamaican reggae and India‚Äôs huge Kumbh Mela festival, however, prompted techno community leaders in Berlin to campaign for their scene to be included in Germany‚Äôs register, which is separate to the Unesco list.</p><p>Techno is a fundamental part of the city, according to Peter Kirn, a Berlin-based DJ and music producer. In 2021 he told the Observer: ‚ÄúIn other cities, people wouldn‚Äôt accept music that‚Äôs really hard or weird and full of synthesisers and really brutal, distorted drum machines. You can‚Äôt play that at peak hour in a club, let alone over lunch. And here it‚Äôs totally acceptable to play that over lunch.</p><p>‚ÄúTechno has become a refuge for people who are marginalised, and there‚Äôs a natural attraction to Berlin as a place which is more permissive when you come from places that are less permissive.‚Äù</p><p>The techno scene is one of six new entries on the intangible cultural heritage list in Germany; others include fruit wine and mountaineering. A parade in Bavaria known as the <em>Kirchseeoner Perchtenlauf</em>, where attenders dress as furry monsters, was also added to the list.</p></div><div><p><span data-dcr-style="bullet"></span> The headline and text of this article were amended on 15 March 2024. The techno scene has been added to the national intangible cultural heritage list compiled by the German commission for Unesco, not the global intangible cultural heritage list compiled by Unesco as an earlier version indicated.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[John Barnett before death "if anything happens, it's not suicide", claims friend (540 pts)]]></title>
            <link>https://abcnews4.com/news/local/if-anything-happens-its-not-suicide-boeing-whistleblowers-prediction-before-death-south-carolina-abc-news-4-2024</link>
            <guid>39712618</guid>
            <pubDate>Fri, 15 Mar 2024 06:32:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abcnews4.com/news/local/if-anything-happens-its-not-suicide-boeing-whistleblowers-prediction-before-death-south-carolina-abc-news-4-2024">https://abcnews4.com/news/local/if-anything-happens-its-not-suicide-boeing-whistleblowers-prediction-before-death-south-carolina-abc-news-4-2024</a>, See on <a href="https://news.ycombinator.com/item?id=39712618">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>John Barnett's family friend Jennifer doesn't think the Boeing whistleblower committed suicide in Charleston. In fact, she says he predicted what may happen to him days before he left for his deposition. March 14, 2024. (Provided-FILE, WCIV)</p><div id="js-Story-Content-0"><p><span>CHARLESTON COUNTY, S.C. (WCIV)  ‚Äî </span>A close family friend of John Barnett said he predicted he might wind up dead and that a story could surface that he killed himself. </p><p>But at the time, he told her not to believe it. </p><p>"I know that he did not commit suicide," said Jennifer, a friend of Barnett's. "There's no way." </p><p>Jennifer said they talked about this exact scenario playing out. However, now, his words seem like a premonition he told her directly not to believe. </p><p>"I know John because his mom and my mom are best friends," Jennifer said. "Over the years, get-togethers, birthdays, celebrations and whatnot. We've all got together and talked." </p><p><strong><em>READ MORE:<a href="https://abcnews4.com/news/local/mystery-lingers-around-boeing-whistleblower-death-at-charleston-hotel-charleston-county-john-barnett-boeing-news-lawsuit-abc-news-4-wciv-2024" target="_blank" title="https://abcnews4.com/news/local/mystery-lingers-around-boeing-whistleblower-death-at-charleston-hotel-charleston-county-john-barnett-boeing-news-lawsuit-abc-news-4-wciv-2024"> "Mystery lingers around Boeing whistleblower's death at Charleston hotel."</a></em></strong></p><p>When Jennifer needed help one day, Barnett came by to see her. They talked about his upcoming deposition in Charleston. Jennifer knew Barnett filed an extremely damaging complaint against Boeing. He said the aerospace giant retaliated against him when he blew the whistle on unsafe practices. <br></p><p>For more than 30 years, he was a quality manager. He'd recently retired and moved back to Louisiana to look after his mom. </p><p>"He wasn't concerned about safety because I asked him," Jennifer said. "I said, 'Aren't you scared?' And he said, 'No, I ain't scared, but if anything happens to me, it's not suicide.'" </p><p>Jennifer added: "I know that he did not commit suicide. There's no way. He loved life too much. He loved his family too much. He loved his brothers too much to put them through what they're going through right now." </p><p>Jennifer said she thinks somebody "didn't like what he had to say" and wanted to "shut him up" without it coming back to anyone. </p><p><strong><em>READ MORE: <a href="https://abcnews4.com/news/local/john-was-a-brave-boeing-whistleblowers-lawyer-responds-to-news-of-his-death-john-barnett-boeing-news-abc-news-wciv-news-4-2024" target="_blank" title="https://abcnews4.com/news/local/john-was-a-brave-boeing-whistleblowers-lawyer-responds-to-news-of-his-death-john-barnett-boeing-news-abc-news-wciv-news-4-2024">"'John was brave': Boeing whistleblower's lawyer responds to news of his death."</a></em></strong></p><p>"That's why they made it look like a suicide," Jennifer said. </p><p>The last time Jennifer saw Barnett was at her father's funeral in late February. He was one of the pallbearers. Sometimes family and friends referred to him by his middle name ‚Äì Mitch. </p><p>"I think everybody is in disbelief and can't believe it," Jennifer said. "I don't care what they say, I know that Mitch didn't do that." </p><p>Just because Barnett is dead doesn't mean the case won't move forward. </p><p>His attorney said they're still prepared to go to trial in June. </p><p>News 4 reached out to Boeing following Barnett's death. They provided the following statement: </p><p>"We are saddened by Mr. Barnett‚Äôs passing, and our thoughts are with his family and friends.‚Äù<br></p><p><strong><em>READ MORE: <a href="https://abcnews4.com/news/local/boeing-whistleblower-dies-in-charleston-charleston-county-coroners-office-confirms-south-carolina-boeing-news-abc-news-4" target="_blank" title="https://abcnews4.com/news/local/boeing-whistleblower-dies-in-charleston-charleston-county-coroners-office-confirms-south-carolina-boeing-news-abc-news-4">"Boeing whistleblower dies in Charleston, Charleston County Coroner's Office confirms."</a></em></strong></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Harvard concluded that a dishonesty expert committed misconduct (236 pts)]]></title>
            <link>https://www.chronicle.com/article/heres-the-unsealed-report-showing-how-harvard-concluded-that-a-dishonesty-expert-committed-misconduct</link>
            <guid>39712173</guid>
            <pubDate>Fri, 15 Mar 2024 04:53:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chronicle.com/article/heres-the-unsealed-report-showing-how-harvard-concluded-that-a-dishonesty-expert-committed-misconduct">https://www.chronicle.com/article/heres-the-unsealed-report-showing-how-harvard-concluded-that-a-dishonesty-expert-committed-misconduct</a>, See on <a href="https://news.ycombinator.com/item?id=39712173">Hacker News</a></p>
Couldn't get https://www.chronicle.com/article/heres-the-unsealed-report-showing-how-harvard-concluded-that-a-dishonesty-expert-committed-misconduct: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[RSS was released 25 years ago today (110 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/RSS</link>
            <guid>39712025</guid>
            <pubDate>Fri, 15 Mar 2024 04:16:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/RSS">https://en.wikipedia.org/wiki/RSS</a>, See on <a href="https://news.ycombinator.com/item?id=39712025">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en" dir="ltr" id="mw-content-text">




<table><caption>RSS</caption><tbody><tr><td colspan="2"><span typeof="mw:File"><a href="https://en.wikipedia.org/wiki/File:Feed-icon.svg" title="Feed Computer icon."><img alt="Feed Computer icon." src="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/128px-Feed-icon.svg.png" decoding="async" width="128" height="128" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/192px-Feed-icon.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/256px-Feed-icon.svg.png 2x" data-file-width="128" data-file-height="128"></a></span></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Filename_extension" title="Filename extension">Filename extension</a></th><td><p><kbd>.rss, .xml</kbd></p></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Media_type" title="Media type">Internet media&nbsp;type</a></th><td><code>application/rss+xml</code>&nbsp;(registration not finished)<sup id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup></td></tr><tr><th scope="row">Developed&nbsp;by</th><td><a href="https://en.wikipedia.org/wiki/RSS_Advisory_Board" title="RSS Advisory Board">RSS Advisory Board</a></td></tr><tr><th scope="row">Initial release</th><td>RSS 0.90 (Netscape), March&nbsp;15, 1999<span>; 25 years ago</span></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Software_release_life_cycle" title="Software release life cycle">Latest release</a></th><td><p>RSS 2.0 (version 2.0.11)<br>March&nbsp;30, 2009<span>; 14 years ago</span> </p></td></tr><tr><th scope="row">Type of format</th><td><a href="https://en.wikipedia.org/wiki/Web_syndication" title="Web syndication">Web syndication</a></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Digital_container_format" title="Digital container format">Container&nbsp;for</a></th><td>Updates of a website and its related metadata (<a href="https://en.wikipedia.org/wiki/Web_feed" title="Web feed">web feed</a>)</td></tr><tr><th scope="row">Extended&nbsp;from</th><td><a href="https://en.wikipedia.org/wiki/XML" title="XML">XML</a></td></tr><tr><th scope="row"><span><a href="https://en.wikipedia.org/wiki/Open_file_format" title="Open file format">Open format</a>?</span></th><td>Yes</td></tr><tr><th scope="row">Website</th><td><span><a rel="nofollow" href="http://rssboard.org/rss-specification">rssboard<wbr>.org<wbr>/rss-specification</a></span></td></tr></tbody></table>
<p><b>RSS</b> (<b><a href="https://en.wikipedia.org/wiki/Resource_Description_Framework" title="Resource Description Framework">RDF</a> Site Summary</b> or <b>Really Simple Syndication</b>)<sup id="cite_ref-powers-2003-1_2-0"><a href="#cite_note-powers-2003-1-2">[2]</a></sup> is a <a href="https://en.wikipedia.org/wiki/Web_feed" title="Web feed">web feed</a><sup id="cite_ref-Netsc99_3-0"><a href="#cite_note-Netsc99-3">[3]</a></sup> that allows users and applications to access updates to websites in a <a href="https://en.wikipedia.org/wiki/Standardization" title="Standardization">standardized</a>, computer-readable format. Subscribing to RSS feeds can allow a user to keep track of many different websites in a single <a href="https://en.wikipedia.org/wiki/News_aggregator" title="News aggregator">news aggregator</a>, which constantly monitor sites for new content, removing the need for the user to manually check them. News aggregators (or "RSS readers") can be built into a <a href="https://en.wikipedia.org/wiki/Web_application" title="Web application">browser</a>, installed on a <a href="https://en.wikipedia.org/wiki/Application_software" title="Application software">desktop computer</a>, or installed on a <a href="https://en.wikipedia.org/wiki/Mobile_app" title="Mobile app">mobile device</a>.
</p><p>Websites usually use RSS feeds to publish frequently updated information, such as <a href="https://en.wikipedia.org/wiki/Blog" title="Blog">blog</a> entries, news headlines, episodes of audio and video series, or for distributing <a href="https://en.wikipedia.org/wiki/Podcast" title="Podcast">podcasts</a>. An RSS document (called "feed", "web feed",<sup id="cite_ref-GuardWF_4-0"><a href="#cite_note-GuardWF-4">[4]</a></sup> or "channel") includes full or summarized text, and <a href="https://en.wikipedia.org/wiki/Metadata" title="Metadata">metadata</a>, like publishing date and author's name. RSS formats are specified using a generic <a href="https://en.wikipedia.org/wiki/XML" title="XML">XML</a> file.
</p><p>Although RSS formats have evolved from as early as March 1999,<sup id="cite_ref-Qstart_5-0"><a href="#cite_note-Qstart-5">[5]</a></sup> it was between 2005 and 2006 when RSS gained widespread use, and the ("<span typeof="mw:File"><a href="https://en.wikipedia.org/wiki/File:Feed-icon.svg"><img src="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/16px-Feed-icon.svg.png" decoding="async" width="16" height="16" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/24px-Feed-icon.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/32px-Feed-icon.svg.png 2x" data-file-width="128" data-file-height="128"></a></span>") icon was decided upon by several major web browsers. RSS feed data is presented to users using software called a news aggregator and the passing of content is called <a href="https://en.wikipedia.org/wiki/Web_syndication" title="Web syndication">web syndication</a>. Users subscribe to feeds either by entering a feed's <a href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier" title="Uniform Resource Identifier">URI</a> into the reader or by clicking on the browser's <a href="https://en.wikipedia.org/wiki/Web_feed#Feed_icon" title="Web feed">feed icon</a>. The RSS reader checks the user's feeds regularly for new information and can automatically download it, if that function is enabled.
</p>
<meta property="mw:PageProp/toc">
<h2><span id="History">History</span></h2>
<table role="presentation"><tbody><tr><td><p><span typeof="mw:File"><span><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/Ambox_current_red.svg/42px-Ambox_current_red.svg.png" decoding="async" width="42" height="34" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/Ambox_current_red.svg/63px-Ambox_current_red.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/Ambox_current_red.svg/84px-Ambox_current_red.svg.png 2x" data-file-width="360" data-file-height="290"></span></span></p></td><td><p>This section needs to be <b>updated</b>.<span> Please help update this article to reflect recent events or newly available information.</span>  <span><i>(<span>October 2013</span>)</i></span></p></td></tr></tbody></table>

<p>The RSS formats were preceded by several attempts at <a href="https://en.wikipedia.org/wiki/Web_syndication" title="Web syndication">web syndication</a> that did not achieve widespread popularity. The basic idea of restructuring information about websites goes back to as early as 1995, when <a href="https://en.wikipedia.org/wiki/Ramanathan_V._Guha" title="Ramanathan V. Guha">Ramanathan V. Guha</a> and others in <a href="https://en.wikipedia.org/wiki/Apple_Inc." title="Apple Inc.">Apple</a>'s <a href="https://en.wikipedia.org/wiki/Apple_Advanced_Technology_Group" title="Apple Advanced Technology Group">Advanced Technology Group</a> developed the <a href="https://en.wikipedia.org/wiki/Meta_Content_Framework" title="Meta Content Framework">Meta Content Framework</a>.<sup id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup>
</p><p><a href="https://en.wikipedia.org/wiki/Resource_Description_Framework" title="Resource Description Framework">RDF</a> Site Summary, the first version of RSS, was created by <a href="https://en.wikipedia.org/w/index.php?title=Dan_Libby&amp;action=edit&amp;redlink=1" title="Dan Libby (page does not exist)">Dan Libby</a> and Ramanathan V. Guha at <a href="https://en.wikipedia.org/wiki/Netscape" title="Netscape">Netscape</a>. It was released in March 1999 for use on the My.Netscape.Com portal.<sup id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup> This version became known as RSS 0.9.<sup id="cite_ref-Qstart_5-1"><a href="#cite_note-Qstart-5">[5]</a></sup> In July 1999, Dan Libby of Netscape produced a new version, RSS 0.91,<sup id="cite_ref-Netsc99_3-1"><a href="#cite_note-Netsc99-3">[3]</a></sup> which simplified the format by removing RDF elements and incorporating elements from <a href="https://en.wikipedia.org/wiki/Dave_Winer" title="Dave Winer">Dave Winer</a>'s news syndication format.<sup id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup> Libby also renamed the format from RDF to RSS <b>Rich Site Summary</b> and outlined further development of the format in a "futures document".<sup id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup>
</p><p>This would be Netscape's last participation in RSS development for eight years. As RSS was being embraced by web publishers who wanted their feeds to be used on My.Netscape.Com and other early RSS portals, Netscape dropped RSS support from My.Netscape.Com in April 2001 during new owner <a href="https://en.wikipedia.org/wiki/AOL" title="AOL">AOL</a>'s restructuring of the company, also removing documentation and tools that supported the format.<sup id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup>
</p><p>Two parties emerged to fill the void, with neither Netscape's help nor approval: The <a href="https://en.wikipedia.org/wiki/RSS-DEV_Working_Group" title="RSS-DEV Working Group">RSS-DEV Working Group</a> and Dave Winer, whose <a href="https://en.wikipedia.org/wiki/UserLand_Software" title="UserLand Software">UserLand Software</a> had published some of the first publishing tools outside Netscape that could read and write RSS.
</p><p>Winer published a modified version of the RSS 0.91 specification on the UserLand website, covering how it was being used in his company's products, and claimed copyright to the document.<sup id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup> A few months later, UserLand filed a U.S. trademark registration for RSS, but failed to respond to a <a href="https://en.wikipedia.org/wiki/United_States_Patent_and_Trademark_Office" title="United States Patent and Trademark Office">USPTO</a> trademark examiner's request and the request was rejected in December 2001.<sup id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup>
</p><p>The RSS-DEV Working Group, a project whose members included <a href="https://en.wikipedia.org/wiki/Aaron_Swartz" title="Aaron Swartz">Aaron Swartz</a>,<sup id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup> Guha and representatives of <a href="https://en.wikipedia.org/wiki/O%27Reilly_Media" title="O'Reilly Media">O'Reilly Media</a> and <a href="https://en.wikipedia.org/wiki/Moreover_Technologies" title="Moreover Technologies">Moreover</a>, produced RSS 1.0 in December 2000.<sup id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup> This new version, which reclaimed the name RDF Site Summary from RSS 0.9, reintroduced support for RDF and added <a href="https://en.wikipedia.org/wiki/XML_namespace" title="XML namespace">XML namespaces</a> support, adopting elements from standard metadata vocabularies such as <a href="https://en.wikipedia.org/wiki/Dublin_Core" title="Dublin Core">Dublin Core</a>.
</p><p>In December 2000, Winer released RSS 0.92<sup id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup>
a minor set of changes aside from the introduction of the enclosure element, which permitted audio files to be carried in RSS feeds and helped spark <a href="https://en.wikipedia.org/wiki/Podcast" title="Podcast">podcasting</a>. He also released drafts of RSS 0.93 and RSS 0.94 that were subsequently withdrawn.<sup id="cite_ref-17"><a href="#cite_note-17">[17]</a></sup>
</p><p>In September 2002, Winer released a major new version of the format, RSS 2.0, that redubbed its initials Really Simple Syndication. RSS 2.0 removed the <i>type</i> attribute added in the RSS 0.94 draft and added support for namespaces. To preserve backward compatibility with RSS 0.92, namespace support applies only to other content included within an RSS 2.0 feed, not the RSS 2.0 elements themselves.<sup id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup> (Although other standards such as <a href="https://en.wikipedia.org/wiki/Atom_(standard)" title="Atom (standard)">Atom</a> attempt to correct this limitation, RSS feeds are not aggregated with other content often enough to shift the popularity from RSS to other formats having full namespace support.)
</p><p>Because neither Winer nor the RSS-DEV Working Group had Netscape's involvement, they could not make an official claim on the RSS name or format. This has fueled ongoing controversy<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citing_sources" title="Wikipedia:Citing sources"><span title="Statement needs to be more specific about the content to which it refers. (September 2016)">specify</span></a></i>]</sup> in the syndication development community as to which entity was the proper publisher of RSS.
</p><p>One product of that contentious debate was the creation of an alternative syndication format, Atom, that began in June 2003.<sup id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup> The Atom syndication format, whose creation was in part motivated by a desire to get a clean start free of the issues surrounding RSS, has been adopted as <a href="https://en.wikipedia.org/wiki/IETF" title="IETF">IETF</a> Proposed Standard <a href="https://en.wikipedia.org/wiki/RFC_(identifier)" title="RFC (identifier)">RFC</a>&nbsp;<a rel="nofollow" href="https://datatracker.ietf.org/doc/html/rfc4287">4287</a>.
</p><p>In July 2003, Winer and UserLand Software assigned the copyright of the RSS 2.0 specification to Harvard's <a href="https://en.wikipedia.org/wiki/Berkman_Klein_Center_for_Internet_%26_Society" title="Berkman Klein Center for Internet &amp; Society">Berkman Klein Center for Internet &amp; Society</a>, where he had just begun a term as a visiting fellow.<sup id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup> At the same time, Winer launched the <a href="https://en.wikipedia.org/wiki/RSS_Advisory_Board" title="RSS Advisory Board">RSS Advisory Board</a> with <a href="https://en.wikipedia.org/wiki/NetNewsWire" title="NetNewsWire">Brent Simmons</a> and <a href="https://en.wikipedia.org/wiki/Jon_Udell" title="Jon Udell">Jon Udell</a>, a group whose purpose was to maintain and publish the specification and answer questions about the format.<sup id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup>
</p><p>In September 2004, Stephen Horlander created the now ubiquitous <a href="https://en.wikipedia.org/wiki/Web_feed#Feed_icon" title="Web feed">RSS icon</a> (<span typeof="mw:File"><a href="https://en.wikipedia.org/wiki/File:Feed-icon.svg"><img src="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/16px-Feed-icon.svg.png" decoding="async" width="16" height="16" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/24px-Feed-icon.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/32px-Feed-icon.svg.png 2x" data-file-width="128" data-file-height="128"></a></span>) for use in the <a href="https://en.wikipedia.org/wiki/Mozilla" title="Mozilla">Mozilla</a> <a href="https://en.wikipedia.org/wiki/Firefox" title="Firefox">Firefox</a> <a href="https://en.wikipedia.org/wiki/Web_Browser" title="Web Browser">browser</a>.<sup id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup>
</p><p>In December 2005, the Microsoft Internet Explorer team and
<a href="https://en.wikipedia.org/wiki/Microsoft_Outlook" title="Microsoft Outlook">Microsoft Outlook</a> team<sup id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup> announced on their blogs that they were adopting Firefox's RSS icon. In February 2006, <a href="https://en.wikipedia.org/wiki/Opera_Software" title="Opera Software">Opera Software</a> followed suit.<sup id="cite_ref-24"><a href="#cite_note-24">[24]</a></sup> This effectively made the orange square with white radio waves the industry standard for RSS and Atom feeds, replacing the large variety of icons and text that had been used previously to identify syndication data.
</p><p>In January 2006, <a href="https://en.wikipedia.org/wiki/Rogers_Cadenhead" title="Rogers Cadenhead">Rogers Cadenhead</a> relaunched the RSS Advisory Board without Dave Winer's participation, with a stated desire to continue the development of the RSS format and resolve ambiguities. In June 2007, the board revised their version of the specification to confirm that namespaces may extend core elements with namespace attributes, as Microsoft has done in Internet Explorer 7. According to their view, a difference of interpretation left publishers unsure of whether this was permitted or forbidden.
</p>
<h2><span id="Example">Example</span></h2>
<p>RSS is <a href="https://en.wikipedia.org/wiki/XML" title="XML">XML</a>-formatted plain text. The RSS format itself is relatively easy to read both by automated processes and by humans alike. An example feed could have contents such as the following:
</p>
<div dir="ltr"><pre><span></span><span>&lt;?xml version="1.0" encoding="UTF-8"&nbsp;?&gt;</span>
<span>&lt;rss</span><span> </span><span>version=</span><span>"2.0"</span><span>&gt;</span>
<span>&lt;channel&gt;</span>
<span> </span><span>&lt;title&gt;</span>RSS<span> </span>Title<span>&lt;/title&gt;</span>
<span> </span><span>&lt;description&gt;</span>This<span> </span>is<span> </span>an<span> </span>example<span> </span>of<span> </span>an<span> </span>RSS<span> </span>feed<span>&lt;/description&gt;</span>
<span> </span><span>&lt;link&gt;</span>http://www.example.com/main.html<span>&lt;/link&gt;</span>
<span> </span><span>&lt;copyright&gt;</span>2020<span> </span>Example.com<span> </span>All<span> </span>rights<span> </span>reserved<span>&lt;/copyright&gt;</span>
<span> </span><span>&lt;lastBuildDate&gt;</span>Mon,<span> </span>6<span> </span>Sep<span> </span>2010<span> </span>00:01:00<span> </span>+0000<span>&lt;/lastBuildDate&gt;</span>
<span> </span><span>&lt;pubDate&gt;</span>Sun,<span> </span>6<span> </span>Sep<span> </span>2009<span> </span>16:20:00<span> </span>+0000<span>&lt;/pubDate&gt;</span>
<span> </span><span>&lt;ttl&gt;</span>1800<span>&lt;/ttl&gt;</span>

<span> </span><span>&lt;item&gt;</span>
<span>  </span><span>&lt;title&gt;</span>Example<span> </span>entry<span>&lt;/title&gt;</span>
<span>  </span><span>&lt;description&gt;</span>Here<span> </span>is<span> </span>some<span> </span>text<span> </span>containing<span> </span>an<span> </span>interesting<span> </span>description.<span>&lt;/description&gt;</span>
<span>  </span><span>&lt;link&gt;</span>http://www.example.com/blog/post/1<span>&lt;/link&gt;</span>
<span>  </span><span>&lt;guid</span><span> </span><span>isPermaLink=</span><span>"false"</span><span>&gt;</span>7bd204c6-1655-4c27-aeee-53f933c5395f<span>&lt;/guid&gt;</span>
<span>  </span><span>&lt;pubDate&gt;</span>Sun,<span> </span>6<span> </span>Sep<span> </span>2009<span> </span>16:20:00<span> </span>+0000<span>&lt;/pubDate&gt;</span>
<span> </span><span>&lt;/item&gt;</span>

<span>&lt;/channel&gt;</span>
<span>&lt;/rss&gt;</span>
</pre></div>
<h3><span id="Aggregators">Aggregators</span></h3>

<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Tiny_Tiny_RSS_English_Interface.png"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Tiny_Tiny_RSS_English_Interface.png/330px-Tiny_Tiny_RSS_English_Interface.png" decoding="async" width="330" height="186" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Tiny_Tiny_RSS_English_Interface.png/495px-Tiny_Tiny_RSS_English_Interface.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Tiny_Tiny_RSS_English_Interface.png/660px-Tiny_Tiny_RSS_English_Interface.png 2x" data-file-width="1366" data-file-height="768"></a><figcaption>User interface of an RSS feed reader on a desktop computer</figcaption></figure>
<p>When retrieved, RSS reading software could use the XML structure to present a neat display to the end users. There are various news aggregator software for desktop and mobile devices, but RSS can also be built-in inside <a href="https://en.wikipedia.org/wiki/Web_browser" title="Web browser">web browsers</a> or <a href="https://en.wikipedia.org/wiki/Email_client" title="Email client">email clients</a> like <a href="https://en.wikipedia.org/wiki/Mozilla_Thunderbird" title="Mozilla Thunderbird">Mozilla Thunderbird</a>.
</p>
<h2><span id="Variants">Variants</span></h2>
<p>There are several different versions of RSS, falling into two major branches (RDF and 2.*).
</p><p>The RDF (or RSS 1.*) branch includes the following versions:
</p>
<ul><li>RSS 0.90 was the original Netscape RSS version. This RSS was called <i>RDF Site Summary</i>, but was based on an early working draft of the RDF standard, and was not compatible with the final RDF Recommendation.</li>
<li>RSS 1.0 is an open format by the RSS-DEV Working Group, again standing for <i>RDF Site Summary</i>. RSS 1.0 is an RDF format like RSS 0.90, but not fully compatible with it, since 1.0 is based on the final RDF 1.0 Recommendation.</li>
<li>RSS 1.1 is also an open format and is intended to update and replace RSS 1.0. The specification is an independent draft not supported or endorsed in any way by the RSS-Dev Working Group or any other organization.</li></ul>
<p>The RSS 2.* branch (initially UserLand, now Harvard) includes the following versions:
</p>
<ul><li>RSS 0.91 is the simplified RSS version released by Netscape, and also the version number of the simplified version originally championed by Dave Winer from Userland Software. The Netscape version was now called <i>Rich Site Summary</i>; this was no longer an RDF format, but was relatively easy to use.</li>
<li>RSS 0.92 through 0.94 are expansions of the RSS 0.91 format, which are mostly compatible with each other and with Winer's version of RSS 0.91, but are not compatible with RSS 0.90.</li>
<li>RSS 2.0.1 has the internal version number 2.0. RSS 2.0.1 was proclaimed to be "frozen", but still updated shortly after release without changing the version number.  RSS now stood for <i>Really Simple Syndication</i>.  The major change in this version is an explicit extension mechanism using XML namespaces.<sup id="cite_ref-W3C_REC_XML_Namespace_25-0"><a href="#cite_note-W3C_REC_XML_Namespace-25">[25]</a></sup></li></ul>
<p>Later versions in each branch are <a href="https://en.wikipedia.org/wiki/Backward_compatibility" title="Backward compatibility">backward-compatible</a> with earlier versions (aside from non-conformant RDF syntax in 0.90), and both versions include properly documented extension mechanisms using XML Namespaces, either directly (in the 2.* branch) or through RDF (in the 1.* branch).  Most syndication software supports both branches. "The Myth of RSS Compatibility", an article written in 2004 by RSS critic and <a href="https://en.wikipedia.org/wiki/Atom_(standard)" title="Atom (standard)">Atom</a> advocate <a href="https://en.wikipedia.org/wiki/Mark_Pilgrim" title="Mark Pilgrim">Mark Pilgrim</a>, discusses RSS version compatibility issues in more detail.
</p><p>The extension mechanisms make it possible for each branch to copy innovations in the other. For example, the RSS 2.* branch was the first to support <a href="https://en.wikipedia.org/wiki/RSS_enclosure" title="RSS enclosure">enclosures</a>, making it the current leading choice for podcasting, and as of 2005 is the format supported for that use by <a href="https://en.wikipedia.org/wiki/ITunes" title="ITunes">iTunes</a> and other podcasting software; however, an enclosure extension is now available for the RSS 1.* branch, mod_enclosure.  Likewise, the RSS 2.* core specification does not support providing full-text in addition to a synopsis, but the RSS 1.* markup can be (and often is) used as an extension.  There are also several common outside extension packages available, e.g. one  from <a href="https://en.wikipedia.org/wiki/Microsoft" title="Microsoft">Microsoft</a> for use in <a href="https://en.wikipedia.org/wiki/Internet_Explorer" title="Internet Explorer">Internet Explorer</a> 7.
</p><p>The most serious compatibility problem is with HTML markup. Userland's RSS reader‚Äîgenerally considered as the reference implementation‚Äîdid not originally filter out <a href="https://en.wikipedia.org/wiki/HTML" title="HTML">HTML</a> markup from feeds. As a result, publishers began placing HTML markup into the titles and descriptions of items in their RSS feeds. This behavior has become expected of readers, to the point of becoming a <a href="https://en.wikipedia.org/wiki/De_facto" title="De facto">de facto</a> standard.<sup id="cite_ref-26"><a href="#cite_note-26">[26]</a></sup> Though there is still some inconsistency in how software handles this markup, particularly in titles. The RSS 2.0 specification was later updated to include examples of entity-encoded HTML; however, all prior plain text usages remain valid.
</p><p>As of January&nbsp;2007, tracking data from www.syndic8.com indicates that the three main versions of RSS in current use are 0.91, 1.0, and 2.0, constituting 13%, 17%, and 67% of worldwide RSS usage, respectively.<sup id="cite_ref-27"><a href="#cite_note-27">[27]</a></sup> These figures, however, do not include usage of the rival web feed format Atom. As of August&nbsp;2008, the syndic8.com website is indexing 546,069 total feeds, of which 86,496 (16%) were some dialect of Atom and 438,102 were some dialect of RSS.<sup id="cite_ref-28"><a href="#cite_note-28">[28]</a></sup>
</p>
<h2><span id="Modules">Modules</span></h2>
<p>The primary objective of all RSS modules is to extend the basic XML schema established for more robust syndication of content. This inherently allows for more diverse, yet standardized, transactions without modifying the core RSS specification.
</p><p>To accomplish this extension, a tightly controlled vocabulary (in the RSS world, "module"; in the XML world, "schema") is declared through an XML namespace to give names to concepts and relationships between those concepts.
</p><p>Some RSS 2.0 modules with established namespaces are:
</p>
<ul><li><a href="https://en.wikipedia.org/wiki/Media_RSS" title="Media RSS">Media RSS</a> (MRSS) 2.0 Module</li>
<li><a rel="nofollow" href="http://www.opensearch.org/Specifications/OpenSearch/1.1">OpenSearch RSS 2.0 Module</a></li></ul>
<h2><span id="Interoperability">Interoperability</span></h2>
<p>Although the number of items in an RSS channel is theoretically unlimited, some <a href="https://en.wikipedia.org/wiki/News_aggregators" title="News aggregators">news aggregators</a> do not support RSS files larger than 150KB. For example, applications that rely on the Common Feed List of <a href="https://en.wikipedia.org/wiki/Microsoft_Windows" title="Microsoft Windows">Windows</a> might handle such files as if they were corrupt, and not open them. <a href="https://en.wikipedia.org/wiki/Interoperability" title="Interoperability">Interoperability</a> can be maximized by keeping the file size under this limit.
</p><p><a href="https://en.wikipedia.org/wiki/Podcast" title="Podcast">Podcasts</a> are distributed using RSS. To listen to a podcast, a user adds the RSS feed to their podcast client, and the client can then list available episodes and download or stream them for listening or viewing. To be included in a podcast directory the feed must for each episode provide a title, description, artwork, category, language, and explicit rating. There are some services that specifically indexes and is a <a href="https://en.wikipedia.org/wiki/Search_engine" title="Search engine">search engine</a> for podcasts.<sup id="cite_ref-29"><a href="#cite_note-29">[29]</a></sup>
</p><p>Some <a href="https://en.wikipedia.org/wiki/BitTorrent" title="BitTorrent">BitTorrent</a> clients support RSS. RSS feeds which provide links to .torrent files allow users to <a href="https://en.wikipedia.org/wiki/Broadcatching" title="Broadcatching">subscribe and automatically download</a> content as soon as it is published.
</p>
<h3></h3>

<p>Some services deliver RSS to an email inbox, sending updates from user's personal selection and schedules. Examples of such services include <a href="https://en.wikipedia.org/wiki/IFTTT" title="IFTTT">IFTTT</a>, <a href="https://en.wikipedia.org/wiki/Zapier" title="Zapier">Zapier</a> and others.<sup id="cite_ref-30"><a href="#cite_note-30">[30]</a></sup> Conversely, some services deliver email to RSS readers.<sup id="cite_ref-31"><a href="#cite_note-31">[31]</a></sup> Further services like e. g. <a href="https://en.wikipedia.org/wiki/Gmane" title="Gmane">Gmane</a> allow to subscribe to feeds via <a href="https://en.wikipedia.org/wiki/NNTP" title="NNTP">NNTP</a>.
</p><p>It may be noted that <a href="https://en.wikipedia.org/wiki/Email_client" title="Email client">email clients</a> such as <a href="https://en.wikipedia.org/wiki/Mozilla_Thunderbird" title="Mozilla Thunderbird">Thunderbird</a> supports RSS natively.<sup id="cite_ref-32"><a href="#cite_note-32">[32]</a></sup>
</p>
<h2></h2>
<p>Both RSS and <a href="https://en.wikipedia.org/wiki/Atom_(web_standard)" title="Atom (web standard)">Atom</a> are widely supported and are compatible with all major consumer feed readers. RSS gained wider use because of early feed reader support. Technically, Atom has several advantages: less restrictive licensing, <a href="https://en.wikipedia.org/wiki/Internet_Assigned_Numbers_Authority" title="Internet Assigned Numbers Authority">IANA</a>-registered <a href="https://en.wikipedia.org/wiki/MIME_type" title="MIME type">MIME type</a>, XML namespace, <a href="https://en.wikipedia.org/wiki/URI" title="URI">URI</a> support, <a href="https://en.wikipedia.org/wiki/RELAX_NG" title="RELAX NG">RELAX NG</a> support.<sup id="cite_ref-33"><a href="#cite_note-33">[33]</a></sup>
</p><p>The following table shows RSS elements alongside Atom elements where they are equivalent.
</p><p>Note: the <a href="https://en.wikipedia.org/wiki/Asterisk" title="Asterisk">asterisk</a> character (*) indicates that an element must be provided (Atom elements "author" and "link" are only required under certain conditions).
</p>
<table>

<tbody><tr>
<th scope="col">RSS 2.0
</th>
<th scope="col">Atom 1.0
</th></tr>
<tr>
<td><code>author</code>
</td>
<td><code>author</code>*
</td></tr>
<tr>
<td><code>category</code>
</td>
<td><code>category</code>
</td></tr>
<tr>
<td><code>channel</code>
</td>
<td><code>feed</code>
</td></tr>
<tr>
<td><code>copyright</code>
</td>
<td><code>rights</code>
</td></tr>
<tr>
<td>‚Äî
</td>
<td><code>subtitle</code>
</td></tr>
<tr>
<td><code>description</code>*
</td>
<td><code>summary</code> and/or <code>content</code>
</td></tr>
<tr>
<td><code>generator</code>
</td>
<td><code>generator</code>
</td></tr>
<tr>
<td><code>guid</code>
</td>
<td><code>id</code>*
</td></tr>
<tr>
<td><code>image</code>
</td>
<td><code>logo</code>
</td></tr>
<tr>
<td><code>item</code>
</td>
<td><code>entry</code>
</td></tr>
<tr>
<td><code>lastBuildDate</code> (in <code>channel</code>)
</td>
<td><code>updated</code>*
</td></tr>
<tr>
<td><code>link</code>*
</td>
<td><code>link</code>*
</td></tr>
<tr>
<td><code>managingEditor</code>
</td>
<td><code>author</code> or <code>contributor</code>
</td></tr>
<tr>
<td><code>pubDate</code>
</td>
<td><code>published</code> (subelement of <code>entry</code>)
</td></tr>
<tr>
<td><code>title</code>*
</td>
<td><code>title</code>*
</td></tr>
<tr>
<td><code><a href="https://en.wikipedia.org/wiki/Time_to_live" title="Time to live">ttl</a></code>
</td>
<td>‚Äî
</td></tr></tbody></table>
<h2><span id="Current_usage">Current usage</span></h2>
<p>Several major sites such as <a href="https://en.wikipedia.org/wiki/Facebook" title="Facebook">Facebook</a> and <a href="https://en.wikipedia.org/wiki/Twitter" title="Twitter">Twitter</a> previously offered RSS feeds but have reduced or removed support. Additionally, widely used readers such as <a href="https://en.wikipedia.org/wiki/Shiira" title="Shiira">Shiira</a>, FeedDemon, and particularly <a href="https://en.wikipedia.org/wiki/Google_Reader" title="Google Reader">Google Reader</a>, have all been discontinued as of 2013, citing declining popularity in RSS.<sup id="cite_ref-ClosureAnnouncement_34-0"><a href="#cite_note-ClosureAnnouncement-34">[34]</a></sup> RSS support was removed in <a href="https://en.wikipedia.org/wiki/OS_X_Mountain_Lion" title="OS X Mountain Lion">OS X Mountain Lion</a>'s versions of <a href="https://en.wikipedia.org/wiki/Apple_Mail" title="Apple Mail">Mail</a> and <a href="https://en.wikipedia.org/wiki/Safari_(web_browser)" title="Safari (web browser)">Safari</a>, although the features were partially restored in Safari 8.<sup id="cite_ref-36"><a href="#cite_note-36">[36]</a></sup> Mozilla removed RSS support from <a href="https://en.wikipedia.org/wiki/Mozilla_Firefox" title="Mozilla Firefox">Mozilla Firefox</a> version 64.0, joining <a href="https://en.wikipedia.org/wiki/Google_Chrome" title="Google Chrome">Google Chrome</a> and <a href="https://en.wikipedia.org/wiki/Microsoft_Edge" title="Microsoft Edge">Microsoft Edge</a> which do not include RSS support, thus leaving <a href="https://en.wikipedia.org/wiki/Internet_Explorer" title="Internet Explorer">Internet Explorer</a> as the last major browser to include RSS support by default.<sup id="cite_ref-37"><a href="#cite_note-37">[37]</a></sup><sup id="cite_ref-38"><a href="#cite_note-38">[38]</a></sup>
</p><p>Since the late 2010s there has been an uptick in RSS interest again. In 2018, <i><a href="https://en.wikipedia.org/wiki/Wired_(magazine)" title="Wired (magazine)">Wired</a></i> published an article named "It's Time for an RSS Revival", citing that RSS gives more control over content compared to algorithms and trackers from social media sites. At that time, <a href="https://en.wikipedia.org/wiki/Feedly" title="Feedly">Feedly</a> was the most popular RSS reader.<sup id="cite_ref-39"><a href="#cite_note-39">[39]</a></sup> Chrome on <a href="https://en.wikipedia.org/wiki/Android_(operating_system)" title="Android (operating system)">Android</a> has added the ability to follow RSS feeds as of 2021.<sup id="cite_ref-40"><a href="#cite_note-40">[40]</a></sup>
</p>
<h2><span id="See_also">See also</span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/JSON_Feed" title="JSON Feed">JSON Feed</a></li>
<li><a href="https://en.wikipedia.org/wiki/Aaron_Swartz" title="Aaron Swartz">Aaron Swartz</a></li>
<li><a href="https://en.wikipedia.org/wiki/Comparison_of_feed_aggregators" title="Comparison of feed aggregators">Comparison of feed aggregators</a></li>
<li><a href="https://en.wikipedia.org/wiki/Data_portability" title="Data portability">Data portability</a></li>
<li><a href="https://en.wikipedia.org/wiki/FeedSync" title="FeedSync">FeedSync</a> previously Simple Sharing Extensions</li>
<li><a href="https://en.wikipedia.org/wiki/HAtom" title="HAtom">hAtom</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mashup_(web_application_hybrid)" title="Mashup (web application hybrid)">Mashup (web application hybrid)</a></li>
<li><a href="https://en.wikipedia.org/wiki/WebSub" title="WebSub">WebSub</a></li></ul>
<h2><span id="Notes">Notes</span></h2>
<div>
<ul><li><cite id="CITEREFPowers2003"><a href="https://en.wikipedia.org/wiki/Shelley_Powers" title="Shelley Powers">Powers, Shelley</a> (2003). <i>Practical RDF</i>. <a href="https://en.wikipedia.org/wiki/O%27Reilly_Media" title="O'Reilly Media">O'Reilly</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Practical+RDF&amp;rft.pub=O%27Reilly&amp;rft.date=2003&amp;rft.aulast=Powers&amp;rft.aufirst=Shelley&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></li></ul>
</div>
<h2><span id="References">References</span></h2>
<div>
<ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span><cite><a rel="nofollow" href="https://tools.ietf.org/id/draft-nottingham-rss-media-type-00">"The application/rss+xml Media Type"</a>. Network Working Group. May 22, 2006. <a rel="nofollow" href="https://web.archive.org/web/20220614140253/https://tools.ietf.org/id/draft-nottingham-rss-media-type-00.txt">Archived</a> from the original on June 14, 2022<span>. Retrieved <span>August 16,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+application%2Frss%2Bxml+Media+Type&amp;rft.pub=Network+Working+Group&amp;rft.date=2006-05-22&amp;rft_id=https%3A%2F%2Ftools.ietf.org%2Fid%2Fdraft-nottingham-rss-media-type-00&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-powers-2003-1-2"><span><b><a href="#cite_ref-powers-2003-1_2-0">^</a></b></span> <span><a href="#CITEREFPowers2003">Powers 2003</a>, p.&nbsp;10: "Another very common use of RDF/XML is in a version of RSS called RSS 1.0 or RDF/RSS. The meaning of the RSS abbreviation has changed over the years, but the basic premise behind it is to provide an XML-formatted feed consisting of an abstract of content and a link to a document containing the full content. When Netscape originally created the first implementation of an RSS specification, RSS stood for RDF Site Summary, and the plan was to use RDF/XML. When the company released, instead, a non-RDF XML version of the specification, RSS stood for Rich Site Summary. Recently, there has been increased activity with RSS, and two paths are emerging: one considers RSS to stand for Really Simple Syndication, a simple XML solution (promoted as RSS 2.0 by Dave Winer at Userland), and one returns RSS to its original roots of RDF Site Summary (RSS 1.0 by the RSS 1.0 Development group)."</span>
</li>
<li id="cite_note-Netsc99-3"><span>^ <a href="#cite_ref-Netsc99_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Netsc99_3-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFLibby,_Dan1999">Libby, Dan (July 10, 1999). <a rel="nofollow" href="https://web.archive.org/web/20001204093600/http://my.netscape.com/publish/formats/rss-spec-0.91.html">"RSS 0.91 Spec, revision 3"</a>. <a href="https://en.wikipedia.org/wiki/Netscape" title="Netscape">Netscape ttem</a>. Archived from <a rel="nofollow" href="http://my.netscape.com/publish/formats/rss-spec-0.91.html">the original</a> on December 4, 2000<span>. Retrieved <span>February 14,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+0.91+Spec%2C+revision+3&amp;rft.pub=Netscape+ttem&amp;rft.date=1999-07-10&amp;rft.au=Libby%2C+Dan&amp;rft_id=http%3A%2F%2Fmy.netscape.com%2Fpublish%2Fformats%2Frss-spec-0.91.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-GuardWF-4"><span><b><a href="#cite_ref-GuardWF_4-0">^</a></b></span> <span>"Web feeds | RSS | The Guardian | guardian.co.uk",
  <i>The Guardian</i>, London, 2008, webpage:
  <a rel="nofollow" href="https://www.theguardian.com/help/feeds">GuardianUK-webfeeds</a>. <a rel="nofollow" href="https://web.archive.org/web/20171215111443/https://www.theguardian.com/help/feeds">Archived</a> December 15, 2017, at the <a href="https://en.wikipedia.org/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a>.</span>
</li>
<li id="cite_note-Qstart-5"><span>^ <a href="#cite_ref-Qstart_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Qstart_5-1"><sup><i><b>b</b></i></sup></a></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20001208063100/http://my.netscape.com/publish/help/quickstart.html">"My Netscape Network: Quick Start"</a>. <a href="https://en.wikipedia.org/wiki/Netscape" title="Netscape">Netscape Communications</a>. Archived from <a rel="nofollow" href="http://my.netscape.com/publish/help/quickstart.html">the original</a> on December 8, 2000<span>. Retrieved <span>October 31,</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=My+Netscape+Network%3A+Quick+Start&amp;rft.pub=Netscape+Communications&amp;rft_id=http%3A%2F%2Fmy.netscape.com%2Fpublish%2Fhelp%2Fquickstart.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>

<li id="cite_note-7"><span><b><a href="#cite_ref-7">^</a></b></span> <span><cite id="CITEREFLash,_Alex1997">Lash, Alex (October 3, 1997). <a rel="nofollow" href="https://web.archive.org/web/20110809151456/http://news.cnet.com/2100-1001-203893.html">"W3C takes first step toward RDF spec"</a>. Archived from <a rel="nofollow" href="http://news.cnet.com/2100-1001-203893.html">the original</a> on August 9, 2011<span>. Retrieved <span>February 16,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=W3C+takes+first+step+toward+RDF+spec&amp;rft.date=1997-10-03&amp;rft.au=Lash%2C+Alex&amp;rft_id=http%3A%2F%2Fnews.cnet.com%2F2100-1001-203893.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-8"><span><b><a href="#cite_ref-8">^</a></b></span> <span><cite id="CITEREFHines1999">Hines, Matt (March 15, 1999). "Netscape Broadens Portal Content Strategy". <i>Newsbytes</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Newsbytes&amp;rft.atitle=Netscape+Broadens+Portal+Content+Strategy&amp;rft.date=1999-03-15&amp;rft.aulast=Hines&amp;rft.aufirst=Matt&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-9"><span><b><a href="#cite_ref-9">^</a></b></span> <span><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+History&amp;rft.date=2007-06-07&amp;rft.au=RSS+Advisory+Board&amp;rft_id=http%3A%2F%2Fwww.rssboard.org%2Frss-history&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-10"><span><b><a href="#cite_ref-10">^</a></b></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20001204123600/http://my.netscape.com/publish/help/futures.html">"MNN Future Directions"</a>. <a href="https://en.wikipedia.org/wiki/Netscape" title="Netscape">Netscape Communications</a>. Archived from <a rel="nofollow" href="http://my.netscape.com/publish/help/futures.html">the original</a> on December 4, 2000<span>. Retrieved <span>October 31,</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=MNN+Future+Directions&amp;rft.pub=Netscape+Communications&amp;rft_id=http%3A%2F%2Fmy.netscape.com%2Fpublish%2Fhelp%2Ffutures.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-11"><span><b><a href="#cite_ref-11">^</a></b></span> <span><cite id="CITEREFAndrew_King2003">Andrew King (April 13, 2003). <a rel="nofollow" href="https://web.archive.org/web/20070119031128/http://www.webreference.com/authoring/languages/xml/rss/1/">"The Evolution of RSS"</a>. Archived from <a rel="nofollow" href="http://www.webreference.com/authoring/languages/xml/rss/1/">the original</a> on January 19, 2007<span>. Retrieved <span>January 17,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+Evolution+of+RSS&amp;rft.date=2003-04-13&amp;rft.au=Andrew+King&amp;rft_id=http%3A%2F%2Fwww.webreference.com%2Fauthoring%2Flanguages%2Fxml%2Frss%2F1%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-12"><span><b><a href="#cite_ref-12">^</a></b></span> <span><cite id="CITEREFWiner,_Dave2000">Winer, Dave (June 4, 2000). <a rel="nofollow" href="https://web.archive.org/web/20061110001520/http://backend.userland.com/rss091#copyrightAndDisclaimer">"RSS 0.91: Copyright and Disclaimer"</a>. UserLand Software. Archived from <a rel="nofollow" href="http://backend.userland.com/rss091#copyrightAndDisclaimer">the original</a> on November 10, 2006<span>. Retrieved <span>October 31,</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+0.91%3A+Copyright+and+Disclaimer&amp;rft.pub=UserLand+Software&amp;rft.date=2000-06-04&amp;rft.au=Winer%2C+Dave&amp;rft_id=http%3A%2F%2Fbackend.userland.com%2Frss091%23copyrightAndDisclaimer&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-13"><span><b><a href="#cite_ref-13">^</a></b></span> <span><cite id="CITEREFU.S._Patent_&amp;_Trademark_Office">U.S. Patent &amp; Trademark Office. <a rel="nofollow" href="http://tarr.uspto.gov/servlet/tarr?regser=serial&amp;entry=78025336">"<span></span>'RSS' Trademark Latest Status Info"</a>. <a rel="nofollow" href="https://web.archive.org/web/20070816233807/http://tarr.uspto.gov/servlet/tarr?regser=serial&amp;entry=78025336">Archived</a> from the original on August 16, 2007<span>. Retrieved <span>September 4,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=%27RSS%27+Trademark+Latest+Status+Info&amp;rft.au=U.S.+Patent+%26+Trademark+Office&amp;rft_id=http%3A%2F%2Ftarr.uspto.gov%2Fservlet%2Ftarr%3Fregser%3Dserial%26entry%3D78025336&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-14"><span><b><a href="#cite_ref-14">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.harvardmagazine.com/2013/01/rss-creator-aaron-swartz-dead-at-26">"RSS Creator Aaron Swartz Dead at 26"</a>. <i>Harvard Magazine</i>. January 14, 2013. <a rel="nofollow" href="https://web.archive.org/web/20210629135531/https://www.harvardmagazine.com/2013/01/rss-creator-aaron-swartz-dead-at-26">Archived</a> from the original on June 29, 2021<span>. Retrieved <span>June 29,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Harvard+Magazine&amp;rft.atitle=RSS+Creator+Aaron+Swartz+Dead+at+26&amp;rft.date=2013-01-14&amp;rft_id=https%3A%2F%2Fwww.harvardmagazine.com%2F2013%2F01%2Frss-creator-aaron-swartz-dead-at-26&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-15"><span><b><a href="#cite_ref-15">^</a></b></span> <span><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RDF+Site+Summary+%28RSS%29+1.0&amp;rft.date=2000-12-09&amp;rft.au=RSS-DEV+Working+Group&amp;rft_id=http%3A%2F%2Fweb.resource.org%2Frss%2F1.0%2Fspec&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-16"><span><b><a href="#cite_ref-16">^</a></b></span> <span><cite id="CITEREFWiner,_Dave2000">Winer, Dave (December 25, 2000). <a rel="nofollow" href="https://web.archive.org/web/20110131184230/http://backend.userland.com/rss092">"RSS 0.92 Specification"</a>. UserLand Software. Archived from <a rel="nofollow" href="http://backend.userland.com/rss092">the original</a> on January 31, 2011<span>. Retrieved <span>October 31,</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+0.92+Specification&amp;rft.pub=UserLand+Software&amp;rft.date=2000-12-25&amp;rft.au=Winer%2C+Dave&amp;rft_id=http%3A%2F%2Fbackend.userland.com%2Frss092&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-17"><span><b><a href="#cite_ref-17">^</a></b></span> <span><cite id="CITEREFWiner,_Dave2001">Winer, Dave (April 20, 2001). <a rel="nofollow" href="https://web.archive.org/web/20061102171227/http://backend.userland.com/rss093">"RSS 0.93 Specification"</a>. UserLand Software. Archived from <a rel="nofollow" href="http://backend.userland.com/rss093">the original</a> on November 2, 2006<span>. Retrieved <span>October 31,</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+0.93+Specification&amp;rft.pub=UserLand+Software&amp;rft.date=2001-04-20&amp;rft.au=Winer%2C+Dave&amp;rft_id=http%3A%2F%2Fbackend.userland.com%2Frss093&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-18"><span><b><a href="#cite_ref-18">^</a></b></span> <span><cite id="CITEREFHarvard_Law2007">Harvard Law (April 14, 2007). <a rel="nofollow" href="http://cyber.law.harvard.edu/rss/toplevelNamespace.html">"Top-level namespaces"</a>. <a rel="nofollow" href="https://web.archive.org/web/20110605164517/http://cyber.law.harvard.edu/rss/toplevelNamespace.html">Archived</a> from the original on June 5, 2011<span>. Retrieved <span>August 3,</span> 2009</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Top-level+namespaces&amp;rft.date=2007-04-14&amp;rft.au=Harvard+Law&amp;rft_id=http%3A%2F%2Fcyber.law.harvard.edu%2Frss%2FtoplevelNamespace.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-19"><span><b><a href="#cite_ref-19">^</a></b></span> <span><cite id="CITEREFFesta2003">Festa, Paul (August 4, 2003). <a rel="nofollow" href="http://news.cnet.com/Battle-of-the-blog/2009-1032_3-5059006.html">"Dispute exposes bitter power struggle behind Web logs"</a>. news.cnet.com. <a rel="nofollow" href="https://web.archive.org/web/20090806234534/http://news.cnet.com/Battle-of-the-blog/2009-1032_3-5059006.html">Archived</a> from the original on August 6, 2009<span>. Retrieved <span>August 6,</span> 2008</span>. <q>The conflict centers on something called Really Simple Syndication (RSS), a technology widely used to syndicate blogs and other Web content. The dispute pits Harvard Law School fellow Dave Winer, the blogging pioneer who is the key gatekeeper of RSS, against advocates of a different format.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Dispute+exposes+bitter+power+struggle+behind+Web+logs&amp;rft.pub=news.cnet.com&amp;rft.date=2003-08-04&amp;rft.aulast=Festa&amp;rft.aufirst=Paul&amp;rft_id=http%3A%2F%2Fnews.cnet.com%2FBattle-of-the-blog%2F2009-1032_3-5059006.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-20"><span><b><a href="#cite_ref-20">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.rssboard.org/advisory-board-notes">"Advisory Board Notes"</a>. <a href="https://en.wikipedia.org/wiki/RSS_Advisory_Board" title="RSS Advisory Board">RSS Advisory Board</a>. July 18, 2003. <a rel="nofollow" href="https://web.archive.org/web/20070927051743/http://www.rssboard.org/advisory-board-notes">Archived</a> from the original on September 27, 2007<span>. Retrieved <span>September 4,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Advisory+Board+Notes&amp;rft.pub=RSS+Advisory+Board&amp;rft.date=2003-07-18&amp;rft_id=http%3A%2F%2Fwww.rssboard.org%2Fadvisory-board-notes&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-21"><span><b><a href="#cite_ref-21">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.scripting.com/2003/07/18.html#rss20News">"RSS 2.0 News"</a>. <i>Scripting News</i>. <a href="https://en.wikipedia.org/wiki/Dave_Winer" title="Dave Winer">Dave Winer</a>. July 18, 2003. <a rel="nofollow" href="https://web.archive.org/web/20070822014007/http://www.scripting.com/2003/07/18.html#rss20News">Archived</a> from the original on August 22, 2007<span>. Retrieved <span>September 4,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Scripting+News&amp;rft.atitle=RSS+2.0+News&amp;rft.date=2003-07-18&amp;rft_id=http%3A%2F%2Fwww.scripting.com%2F2003%2F07%2F18.html%23rss20News&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-22"><span><b><a href="#cite_ref-22">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.squarefree.com/burningedge/2004/09/26/2004-09-26-branch-builds/">"2004-09-26 Branch builds"</a>. <i>The Burning Edge</i>. September 26, 2004. <a rel="nofollow" href="https://web.archive.org/web/20141009071447/http://www.squarefree.com/burningedge/2004/09/26/2004-09-26-branch-builds/">Archived</a> from the original on October 9, 2014<span>. Retrieved <span>October 6,</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Burning+Edge&amp;rft.atitle=2004-09-26+Branch+builds&amp;rft.date=2004-09-26&amp;rft_id=http%3A%2F%2Fwww.squarefree.com%2Fburningedge%2F2004%2F09%2F26%2F2004-09-26-branch-builds%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-23"><span><b><a href="#cite_ref-23">^</a></b></span> <span>"<a rel="nofollow" href="https://web.archive.org/web/20051217102644/http://blogs.msdn.com/michael_affronti/archive/2005/12/15/504316.aspx">RSS icon goodness</a>", blog post by Michael A. Affronti of Microsoft (Outlook Program Manager), December 15, 2005</span>
</li>
<li id="cite_note-24"><span><b><a href="#cite_ref-24">^</a></b></span> <span><cite id="CITEREFtrond2006">trond (February 16, 2006). <a rel="nofollow" href="https://web.archive.org/web/20100417170259/http://my.opera.com/desktopteam/blog/show.dml/146296">"Making love to the new feed icon"</a>. Opera Desktop Team. Archived from <a rel="nofollow" href="http://my.opera.com/desktopteam/blog/show.dml/146296">the original</a> on April 17, 2010<span>. Retrieved <span>July 4,</span> 2010</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Making+love+to+the+new+feed+icon&amp;rft.pub=Opera+Desktop+Team&amp;rft.date=2006-02-16&amp;rft.au=trond&amp;rft_id=http%3A%2F%2Fmy.opera.com%2Fdesktopteam%2Fblog%2Fshow.dml%2F146296&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-W3C_REC_XML_Namespace-25"><span><b><a href="#cite_ref-W3C_REC_XML_Namespace_25-0">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.w3.org/TR/REC-xml-names/">"Namespaces in XML 1.0"</a> (2nd&nbsp;ed.). W3C. August 16, 2006. <a rel="nofollow" href="https://web.archive.org/web/20110316043909/http://www.w3.org/TR/REC-xml-names/">Archived</a> from the original on March 16, 2011<span>. Retrieved <span>May 22,</span> 2008</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Namespaces+in+XML+1.0&amp;rft.edition=2nd&amp;rft.pub=W3C&amp;rft.date=2006-08-16&amp;rft_id=http%3A%2F%2Fwww.w3.org%2FTR%2FREC-xml-names%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-26"><span><b><a href="#cite_ref-26">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.w3.org/2001/10/glance/doc/howto.html">"W3C RSS 1.0 News Feed Creation How-To"</a>. <i>www.w3.org</i>. <a rel="nofollow" href="https://web.archive.org/web/20220614140126/https://www.w3.org/2001/10/glance/doc/howto.html">Archived</a> from the original on June 14, 2022<span>. Retrieved <span>February 5,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.w3.org&amp;rft.atitle=W3C+RSS+1.0+News+Feed+Creation+How-To&amp;rft_id=https%3A%2F%2Fwww.w3.org%2F2001%2F10%2Fglance%2Fdoc%2Fhowto.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-27"><span><b><a href="#cite_ref-27">^</a></b></span> <span><cite id="CITEREFHolzner">Holzner, Steven. <a rel="nofollow" href="http://www.peachpit.com/articles/article.aspx?p=674690">"Peachpit article"</a>. Peachpit article. <a rel="nofollow" href="https://web.archive.org/web/20111109173320/http://www.peachpit.com/articles/article.aspx?p=674690">Archived</a> from the original on November 9, 2011<span>. Retrieved <span>December 11,</span> 2010</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Peachpit+article&amp;rft.pub=Peachpit+article&amp;rft.aulast=Holzner&amp;rft.aufirst=Steven&amp;rft_id=http%3A%2F%2Fwww.peachpit.com%2Farticles%2Farticle.aspx%3Fp%3D674690&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-28"><span><b><a href="#cite_ref-28">^</a></b></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20020803040757/http://www.syndic8.com/stats.php?Section=feeds#tabtable">"Syndic8 stats table"</a>. Syndic8.com. Archived from <a rel="nofollow" href="http://www.syndic8.com/stats.php?Section=feeds#tabtable">the original</a> on August 3, 2002<span>. Retrieved <span>August 12,</span> 2011</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Syndic8+stats+table&amp;rft.pub=Syndic8.com&amp;rft_id=http%3A%2F%2Fwww.syndic8.com%2Fstats.php%3FSection%3Dfeeds%23tabtable&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-29"><span><b><a href="#cite_ref-29">^</a></b></span> <span><cite><a rel="nofollow" href="https://lifehacker.com/the-best-podcast-search-engine-1818560337">"The Best Podcast Search Engine"</a>. <i>Lifehacker</i>. September 20, 2017. <a rel="nofollow" href="https://web.archive.org/web/20201129195032/https://lifehacker.com/the-best-podcast-search-engine-1818560337">Archived</a> from the original on November 29, 2020<span>. Retrieved <span>February 5,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Lifehacker&amp;rft.atitle=The+Best+Podcast+Search+Engine&amp;rft.date=2017-09-20&amp;rft_id=https%3A%2F%2Flifehacker.com%2Fthe-best-podcast-search-engine-1818560337&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-30"><span><b><a href="#cite_ref-30">^</a></b></span> <span><cite><a rel="nofollow" href="https://blogtrottr.com/">"Free realtime RSS and Atom feed to email service. Get your favourite blogs, feeds, and news delivered to your inbox"</a>. <a rel="nofollow" href="https://web.archive.org/web/20170128081150/http://blogtrottr.com/">Archived</a> from the original on January 28, 2017<span>. Retrieved <span>January 26,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Free+realtime+RSS+and+Atom+feed+to+email+service.+Get+your+favourite+blogs%2C+feeds%2C+and+news+delivered+to+your+inbox.&amp;rft_id=https%3A%2F%2Fblogtrottr.com%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-31"><span><b><a href="#cite_ref-31">^</a></b></span> <span><cite><a rel="nofollow" href="https://rss.com/">"RSS Feed Reader, your tool for saving time and money at RSS.com"</a>. <a rel="nofollow" href="https://web.archive.org/web/20170125224151/https://www.rss.com/">Archived</a> from the original on January 25, 2017<span>. Retrieved <span>January 26,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+Feed+Reader%2C+your+tool+for+saving+time+and+money+at+RSS.com&amp;rft_id=https%3A%2F%2Frss.com%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-32"><span><b><a href="#cite_ref-32">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.uslsoftware.com/how-to-use-thunderbird-to-get-rss-feeds/">"How to use Thunderbird to get RSS feeds! Here's How it Works"</a>. October 17, 2018. <a rel="nofollow" href="https://web.archive.org/web/20210413005112/https://www.uslsoftware.com/how-to-use-thunderbird-to-get-rss-feeds/">Archived</a> from the original on April 13, 2021<span>. Retrieved <span>February 5,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=How+to+use+Thunderbird+to+get+RSS+feeds%21+Here%27s+How+it+Works&amp;rft.date=2018-10-17&amp;rft_id=https%3A%2F%2Fwww.uslsoftware.com%2Fhow-to-use-thunderbird-to-get-rss-feeds%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-33"><span><b><a href="#cite_ref-33">^</a></b></span> <span><cite id="CITEREFLeslie_Sikos2011">Leslie Sikos (2011). <a rel="nofollow" href="https://web.archive.org/web/20150402152305/http://www.masteringhtml5css3.com/"><i>Web standards ‚Äì Mastering HTML5, CSS3, and XML</i></a>. <a href="https://en.wikipedia.org/wiki/Apress" title="Apress">Apress</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-1-4302-4041-9" title="Special:BookSources/978-1-4302-4041-9"><bdi>978-1-4302-4041-9</bdi></a>. Archived from <a rel="nofollow" href="http://www.masteringhtml5css3.com/">the original</a> on April 2, 2015<span>. Retrieved <span>June 14,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Web+standards+%E2%80%93+Mastering+HTML5%2C+CSS3%2C+and+XML&amp;rft.pub=Apress&amp;rft.date=2011&amp;rft.isbn=978-1-4302-4041-9&amp;rft.au=Leslie+Sikos&amp;rft_id=http%3A%2F%2Fwww.masteringhtml5css3.com&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-ClosureAnnouncement-34"><span><b><a href="#cite_ref-ClosureAnnouncement_34-0">^</a></b></span> <span><cite id="CITEREFH√∂lzle">H√∂lzle, Urs. <a rel="nofollow" href="http://googleblog.blogspot.com/2013/03/a-second-spring-of-cleaning.html">"A second spring of cleaning"</a>. googleblog.blogspot.com. <a rel="nofollow" href="https://web.archive.org/web/20130314045128/http://googleblog.blogspot.com/2013/03/a-second-spring-of-cleaning.html">Archived</a> from the original on March 14, 2013<span>. Retrieved <span>March 14,</span> 2013</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=A+second+spring+of+cleaning&amp;rft.pub=googleblog.blogspot.com&amp;rft.aulast=H%C3%B6lzle&amp;rft.aufirst=Urs&amp;rft_id=http%3A%2F%2Fgoogleblog.blogspot.com%2F2013%2F03%2Fa-second-spring-of-cleaning.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>

<li id="cite_note-36"><span><b><a href="#cite_ref-36">^</a></b></span> <span><cite><a rel="nofollow" href="http://osxdaily.com/2014/11/03/subscribe-rss-feeds-safari-os-x/">"Subscribe to RSS Feeds in Safari for OS X Yosemite"</a>. OSX Daily. November 3, 2014. <a rel="nofollow" href="https://web.archive.org/web/20150121185232/http://osxdaily.com/2014/11/03/subscribe-rss-feeds-safari-os-x/">Archived</a> from the original on January 21, 2015<span>. Retrieved <span>January 24,</span> 2015</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Subscribe+to+RSS+Feeds+in+Safari+for+OS+X+Yosemite&amp;rft.pub=OSX+Daily&amp;rft.date=2014-11-03&amp;rft_id=http%3A%2F%2Fosxdaily.com%2F2014%2F11%2F03%2Fsubscribe-rss-feeds-safari-os-x%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-37"><span><b><a href="#cite_ref-37">^</a></b></span> <span><cite id="CITEREFCimpanu2018">Cimpanu, Catalin (July 26, 2018). <a rel="nofollow" href="https://www.bleepingcomputer.com/news/software/mozilla-to-remove-support-for-built-in-feed-reader-from-firefox/">"Mozilla to Remove Support for Built-In Feed Reader From Firefox"</a>. <i>BleepingComputer</i>. <a rel="nofollow" href="https://web.archive.org/web/20180726144716/https://www.bleepingcomputer.com/news/software/mozilla-to-remove-support-for-built-in-feed-reader-from-firefox/">Archived</a> from the original on July 26, 2018<span>. Retrieved <span>July 26,</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=BleepingComputer&amp;rft.atitle=Mozilla+to+Remove+Support+for+Built-In+Feed+Reader+From+Firefox&amp;rft.date=2018-07-26&amp;rft.aulast=Cimpanu&amp;rft.aufirst=Catalin&amp;rft_id=https%3A%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fsoftware%2Fmozilla-to-remove-support-for-built-in-feed-reader-from-firefox%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-38"><span><b><a href="#cite_ref-38">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.mozilla.org/en-US/firefox/64.0/releasenotes/">"Firefox 64.0, See All New Features, Updates and Fixes"</a>. <i>Mozilla</i>. December 11, 2018. <a rel="nofollow" href="https://web.archive.org/web/20181211143259/https://www.mozilla.org/en-US/firefox/64.0/releasenotes/">Archived</a> from the original on December 11, 2018<span>. Retrieved <span>December 12,</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Mozilla&amp;rft.atitle=Firefox+64.0%2C+See+All+New+Features%2C+Updates+and+Fixes&amp;rft.date=2018-12-11&amp;rft_id=https%3A%2F%2Fwww.mozilla.org%2Fen-US%2Ffirefox%2F64.0%2Freleasenotes%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-39"><span><b><a href="#cite_ref-39">^</a></b></span> <span><cite id="CITEREFBarrett2018">Barrett, Brian (March 30, 2018). <a rel="nofollow" href="https://www.wired.com/story/rss-readers-feedly-inoreader-old-reader/">"It's Time for an RSS Revival"</a>. <i>Wired</i>. <a rel="nofollow" href="https://web.archive.org/web/20210812114050/https://www.wired.com/story/rss-readers-feedly-inoreader-old-reader/">Archived</a> from the original on August 12, 2021<span>. Retrieved <span>July 26,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wired&amp;rft.atitle=It%27s+Time+for+an+RSS+Revival&amp;rft.date=2018-03-30&amp;rft.aulast=Barrett&amp;rft.aufirst=Brian&amp;rft_id=https%3A%2F%2Fwww.wired.com%2Fstory%2Frss-readers-feedly-inoreader-old-reader%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-40"><span><b><a href="#cite_ref-40">^</a></b></span> <span><cite id="CITEREFCampbell2021">Campbell, Ian Carlos (October 8, 2021). <a rel="nofollow" href="https://www.theverge.com/2021/10/8/22716813/google-chrome-follow-button-rss-reader">"Google Reader is still defunct, but now you can 'follow' RSS feeds in Chrome on Android"</a>. <i>The Verge</i>. <a rel="nofollow" href="https://web.archive.org/web/20220605165318/https://www.theverge.com/2021/10/8/22716813/google-chrome-follow-button-rss-reader">Archived</a> from the original on June 5, 2022<span>. Retrieved <span>June 19,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Verge&amp;rft.atitle=Google+Reader+is+still+defunct%2C+but+now+you+can+%27follow%27+RSS+feeds+in+Chrome+on+Android&amp;rft.date=2021-10-08&amp;rft.aulast=Campbell&amp;rft.aufirst=Ian+Carlos&amp;rft_id=https%3A%2F%2Fwww.theverge.com%2F2021%2F10%2F8%2F22716813%2Fgoogle-chrome-follow-button-rss-reader&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
</ol></div>
<h2><span id="External_links">External links</span></h2>

<ul><li><a rel="nofollow" href="http://www.rssboard.org/rss-0-9-0">RSS 0.90 Specification</a></li>
<li><a rel="nofollow" href="http://www.rssboard.org/rss-0-9-1-netscape">RSS 0.91 Specification</a></li>
<li><a rel="nofollow" href="http://web.resource.org/rss/1.0/">RSS 1.0 Specifications</a></li>
<li><a rel="nofollow" href="http://www.rssboard.org/rss-specification">RSS 2.0 Specification</a></li>
<li><a rel="nofollow" href="https://web.archive.org/web/20110718034619/http://diveintomark.org/archives/2002/09/06/history_of_the_rss_fork">History of the RSS Fork</a> (Mark Pilgrim)</li>
<li><a rel="nofollow" href="https://www.xul.fr/en-xml-rss.html">Building an RSS feed</a> Tutorial with example</li></ul>





<!-- 
NewPP limit report
Parsed by mw‚Äêweb.eqiad.main‚Äê7d644d6d99‚Äêqsftd
Cached time: 20240315110540
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‚Äêrevision‚Äêsha1, show‚Äêtoc]
CPU time usage: 0.817 seconds
Real time usage: 1.060 seconds
Preprocessor visited node count: 4966/1000000
Post‚Äêexpand include size: 166970/2097152 bytes
Template argument size: 5945/2097152 bytes
Highest expansion depth: 24/100
Expensive parser function count: 17/500
Unstrip recursion depth: 1/20
Unstrip post‚Äêexpand size: 148753/5000000 bytes
Lua time usage: 0.512/10.000 seconds
Lua memory usage: 8675186/52428800 bytes
Number of Wikibase entities loaded: 1/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  915.847      1 -total
 27.41%  251.036      1 Template:Reflist
 16.24%  148.715     34 Template:Cite_web
 10.77%   98.630      9 Template:Navbox
  8.56%   78.396      1 Template:Web_syndication
  8.45%   77.375      1 Template:Infobox_file_format
  8.43%   77.162      1 Template:Short_description
  8.05%   73.749      1 Template:Infobox
  7.57%   69.316     13 Template:Main_other
  6.33%   58.011      2 Template:Cite_book
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:93489-0!canonical and timestamp 20240315110540 and revision id 1210081177. Rendering was triggered because: page-view
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Postgres is eating the database world (242 pts)]]></title>
            <link>https://medium.com/@fengruohang/postgres-is-eating-the-database-world-157c204dcfc4</link>
            <guid>39711863</guid>
            <pubDate>Fri, 15 Mar 2024 03:43:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/@fengruohang/postgres-is-eating-the-database-world-157c204dcfc4">https://medium.com/@fengruohang/postgres-is-eating-the-database-world-157c204dcfc4</a>, See on <a href="https://news.ycombinator.com/item?id=39711863">Hacker News</a></p>
Couldn't get https://medium.com/@fengruohang/postgres-is-eating-the-database-world-157c204dcfc4: Error: Request failed with status code 429]]></description>
        </item>
        <item>
            <title><![CDATA[Vision Pro: What we got wrong at Oculus that Apple got right (440 pts)]]></title>
            <link>https://hugo.blog/2024/03/11/vision-pro/</link>
            <guid>39711725</guid>
            <pubDate>Fri, 15 Mar 2024 03:15:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hugo.blog/2024/03/11/vision-pro/">https://hugo.blog/2024/03/11/vision-pro/</a>, See on <a href="https://news.ycombinator.com/item?id=39711725">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><em>by <a href="https://twitter.com/hbarra" target="_blank" rel="noreferrer noopener">Hugo Barra</a></em> (former Head of Oculus at Meta)</p>



<p>Friends and colleagues have been asking me to share my perspective on the Apple Vision Pro as a product. Inspired by my dear friend <a href="https://ma.tt/category/birthday/" target="_blank" rel="noreferrer noopener">Matt Mullenweg‚Äôs 40th post</a>, I decided to put pen to paper.</p>



<p>This started as blog post and became an essay before too long, so I‚Äôve structured my writing in multiple sections each with a clear lead to make it a bit easier to digest ‚Äî peppered with my own ‚Äòtakes‚Äô. I‚Äôve tried to stick to original thoughts for the most part and link to what others have said where applicable.</p>



<p>Some of the topics I touch on:</p>



<ul>
<li>Why I believe Vision Pro may be an over-engineered ‚Äúdevkit‚Äù</li>



<li>The genius &amp; audacity behind some of Apple‚Äôs hardware decisions</li>



<li><em>Gaze &amp; pinch</em> is an incredible UI superpower and major industry ah-ha moment</li>



<li>Why the Vision Pro software/content story is so dull and unimaginative</li>



<li>Why most people won‚Äôt use Vision Pro for watching TV/movies</li>



<li>Apple‚Äôs bet in immersive video is a total game-changer for <em>Live Sports</em></li>



<li>Why I returned my Vision Pro‚Ä¶ and my Top 10 wishlist to reconsider</li>



<li>Apple‚Äôs VR debut is the best thing that ever happened to Oculus/Meta</li>



<li>My unsolicited product advice to Meta for <em>Quest Pro 2</em> and beyond</li>
</ul>



<h2><strong>The Apple Vision Pro is the Northstar the VR industry needed, whether we admit it or not</strong></h2>



<p>I‚Äôve been a VR enthusiast for most of my adult life, from working as an intern at <a href="https://www.roadtovr.com/end-of-an-era-disneyquest-first-vr-attraction-set-to-close/" target="_blank" rel="noreferrer noopener">Disney Quest VR</a> in the 1990s, to being an early backer of the <a href="https://www.kickstarter.com/projects/1523379957/oculus-rift-step-into-the-game">Oculus Rift DK1 on Kickstarter</a> in 2013, to leading the Oculus VR/AR team at Meta from 2017 to 2020 (and getting to work alongside VR legends like John Carmack, Brendan Iribe and Jason Rubin), and always testing every VR product or experience I can get my hands on.</p>



<p>Back in my Oculus days, I used to semi-seriously joke with our team (and usually got a lot of heat for it!) that the best thing that could ever happen to us was having Apple enter the VR industry and become a direct competitor to Oculus. I‚Äôve always believed that strong competition pushes a team to do their best work in any industry. This became clear to me especially after living for nearly 10 years at the center of the iOS/Android battle of ecosystems where each side made the other infinitely better by constantly raising the bar on UX, features, performance, developer APIs etc, and seeing each side respond by not only fast following but usually also improving on what the other had released. (And this definitely went both ways: iOS copied Android as much as Android copied iOS).</p>



<p>But in the case of VR at Oculus, we also never really felt like the world had a Northstar that could truly capture human hearts and minds, and without that it would be impossible to transition VR from being a niche gamer tech to the incredible spatial computing paradigm that we always thought it potentially represented (which I still very much believe in). Apple could <em>really </em>help us if they cared about VR.</p>



<p>The Vision Pro launch has more or less done exactly what I had always hoped for, which is to build a huge wave of awareness and curiosity that elevates the spatial computing ecosystem and could ultimately lead to mass-market consumer demand and a lot more developer interest that VR has ever had. Now it‚Äôs up to the industry to create enough user value and demonstrate whether this is in fact the future of computing.</p>



<h2><strong>The Vision Pro‚Äôs <em>instant magic</em> comes down to just: (1) an unprecedented new level of presence in VR, and (2) a new UI superpower using <strong>gaze &amp; pinch</strong></strong></h2>



<p>Using Vision Pro is an instantly magical and intuitive experience ‚Äî whether or not you‚Äôve used other VR headsets ‚Äî purely because of Apple‚Äôs unrelenting focus on delivering two specific capabilities that speak to our humanity:</p>



<p><strong>1) Feeling present and connected to your physical world</strong>: thanks to a high-fidelity passthrough (‚Äúmixed reality‚Äù) experience with very low latency, excellent distortion correction (<em>much</em> better than Quest 3), and sufficiently high resolution that allows you to even see your phone/computer screen through the passthrough cameras (i.e. without taking your headset off).</p>



<p>Even though there are major gaps left to be filled in future versions of the Vision Pro hardware (which I‚Äôll get into later), this level of connection with the real world ‚Äî or ‚Äúpresence‚Äù as VR folks like to call it ‚Äî is something that no other VR headset has ever come even close to delivering and so far was only remotely possible with AR headsets (ex: HoloLens and Magic Leap) which feature physically transparent displays but have their own significant limitations in many other areas. Apple‚Äôs implementation of Optic ID as an overlay on top of live passthrough is a beautiful design decision that only enhances this sense of presence.</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong>The Vision Pro high-fidelity passthrough experience parallels <strong>Apple‚Äôs introduction of the iPhone‚Äôs original <em>retina display</em></strong>, which set a new experience bar and gold standard in mobile display fidelity. While much remains to be improved in the Vision Pro passthrough experience, Apple is unquestionably setting a new standard for all future headsets (by any vendor) that VR passthrough must be good enough to closely resemble reality.</mark></p>
</blockquote>



<p><strong>2) Having a new UI superpower with gaze &amp; pinch</strong>, thanks to a very precise eye tracking system (with 2 dedicated cameras per eye) embedded into the lenses, coupled with a wide-field-of-view hand tracking system that can ‚Äúsee‚Äù a finger pinch even with your hands are down or resting on your lap. Because it works so effortlessly for the user, it really feels like having a new ‚Äúlaser vision‚Äù superpower.</p>



<p>The hardware needed to track eyes and hands in VR has been around for over a decade, and it‚Äôs Apple unique ability to bring everything together in a magical way that makes this UI superpower the most important achievement of the entire Vision Pro product, without a shadow of doubt.</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong>The Vision Pro‚Äôs new ‚Äúgaze + pinch‚Äù input modality is <strong>the VR equivalent of the iPhone‚Äôs capacitive multi-touch gestures</strong>. Introduced by Apple with the first iPhone launch nearly 17 years ago, multi-touch instantly became a new standard that changed computing forever. ‚ÄúGaze + pinch‚Äù is so groundbreaking that it‚Äôs an instant defacto standard for VR interaction that future VR headsets be forced to adopt sooner or later. It‚Äôs also going to be a huge developer unlock that leads to gaze-based interaction ideas that will blow our minds. </mark></p>
</blockquote>



<h2>Hardware</h2>



<h2><strong>Vision Pro is a meticulously over-engineered ‚Äúdevkit‚Äù that is <em>far too heavy</em> <em>to have product-market fit</em> but good enough to seed curiosity into the world</strong></h2>



<p>The Oculus VR story began with the 2013 launch of <em><a href="https://en.wikipedia.org/wiki/Oculus_Rift#Development_Kit_1" target="_blank" rel="noreferrer noopener">Oculus Rift DK1</a></em> (short for ‚Äúdevkit v1‚Äù or ‚Äúdevelopment kit v1‚Äù). This was a headset launched by the original Oculus startup team ‚Äî years before it was acquired by Facebook ‚Äî with the explicit goal of seeding developer interest well before a commercial release. Given that VR was a non-existing market then, releasing a devkit was the correct and necessary strategy <em>there and then</em> for a startup to start building a content library as well as momentum among enthusiasts ahead of launching a consumer product. The team released a <a href="https://en.wikipedia.org/wiki/Oculus_Rift#Development_Kit_2" target="_blank" rel="noreferrer noopener"><em>DK2</em></a> about a year later in 2014, and finally launched the first Oculus Rift consumer headset in 2015.</p>







<figure><img data-attachment-id="208" data-permalink="https://hugo.blog/2024/03/11/vision-pro/palmer-luckey-oculus-dk1/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png" data-orig-size="1500,844" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="palmer-luckey-oculus-dk1" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=1024" width="1024" height="576" src="https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=768 768w, https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png 1500w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Oculus co-founder Palmer Luckey wearing the original Oculus Rift DK1 released in 2013 </figcaption></figure>



<p>When I joined Facebook to lead the Oculus team in 2017 after the acquisition, one of the many battles I found myself in the middle of almost immediately was the ‚Äúdevkit war‚Äù. The Oculus team‚Äôs DK1 and DK2 legacy was so strong that it was not uncommon to hear arguments in product meetings pushing for us to launch VR headsets still in prototype stage as ‚Äúdevkits‚Äù to end users. Since Oculus was no longer a startup ‚Äî and had the resources to both extensively test prototypes without launching them as products <em>and </em>run extensive pre-launch developer programs ‚Äî it no longer made sense for Oculus devkits to exist. This stance often didn‚Äôt make me very popular amongst some of my Oculus OG colleagues.</p>



<p>Fast forward to 2024. After the Vision Pro launch, the VR hardware enthusiast community (including Oculus OG folks I‚Äôm still in touch with) quickly arrived at the conclusion that Apple really played it safe in the design of this first VR product by over-engineering it. For starters, Vision Pro ships with more sensors than what‚Äôs likely necessary to deliver Apple‚Äôs intended experience. This is typical in a first-generation product that‚Äôs been under development for so many years. It makes Vision Pro start to feel like a devkit.</p>







<figure><img data-attachment-id="148" data-permalink="https://hugo.blog/2024/03/11/vision-pro/visionpro_sensors/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png" data-orig-size="804,486" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="visionpro_sensors" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=804" width="804" height="486" src="https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=804" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png 804w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=768 768w" sizes="(max-width: 804px) 100vw, 804px"><figcaption>A sensor party: 6 tracking cameras, 2 passthrough cameras, 2 depth sensors<br>(plus 4 eye-tracking cameras not shown)</figcaption></figure>



<p>Here‚Äôs a quick comparison with existing VR headsets:</p>







<figure><table><thead><tr><th>Sensor</th><th>Vision Pro</th><th>Meta <br>Quest 3</th><th>Meta <br>Quest Pro</th></tr></thead><tbody><tr><td>Environment passthrough cameras</td><td>2</td><td>2</td><td>1</td></tr><tr><td>World tracking cameras</td><td>6</td><td>4</td><td>6</td></tr><tr><td>Depth sensors</td><td>2</td><td>1</td><td>‚Äì</td></tr><tr><td>Eye tracking cameras</td><td>4</td><td>‚Äì</td><td>2</td></tr><tr><td><strong>Total</strong></td><td><strong>14</strong></td><td><strong>7</strong></td><td><strong>9</strong></td></tr></tbody></table><figcaption>Side-by-side comparison with the sensor stacks of other VR headsets</figcaption></figure>



<p>This over-spec‚Äôing is unsurprising and characteristic of a v1 product where its creator wants to ensure it survives the hardest tests early users will no doubt want to put the product through. It‚Äôs also a way for Apple to see how far developers will push the product‚Äôs capabilities, as Apple is no doubt relying on that community to produce the majority of software/content magic for this new type of computer, as they‚Äôve previously done with every other device class.</p>



<p>Apple‚Äôs decision to over-spec the Vision Pro does, however, lead to the inevitable consequence of a headset weighing above 600g ‚Äî heavier than most other VR headsets in the market to date ‚Äî that <strong>makes it difficult for most people to wear it for more than 30-45 minutes at a time without suffering a lot of discomfort</strong>. Most of the discomfort comes in the form of pressure against the user‚Äôs face and the back of the person‚Äôs head.</p>







<blockquote>
<p><mark><strong>MY TAKE:</strong> Because of its heavy weight, Vision Pro has inevitably landed in the world as a high-quality ‚Äúdevkit‚Äù designed to capture everyone‚Äôs curiosity, hearts &amp; minds with its magic (especially through the voice of enthusiastic tech influencers) while being realistically focused on developers as its primary audience. In other words, the Vision Pro is a devkit that helps prepare the world to receive a more mainstream Apple VR headset that could have product-market fit in 1 or 2 generations.</mark></p>
</blockquote>



<p>All things considered, I do believe Apple‚Äôs calculus was correct in prioritizing launching a first-generation product with fewer experience and design compromises at the expense of user comfort. And while many people have argued Apple could have avoided this major comfort issue by redistributing weight or using lighter materials, those attempts would have come at the expense of beauty and design. (I‚Äôll come back to the weight issue shortly.)</p>



<p>With this in mind, it‚Äôs easy to understand two particularly important decisions Apple made for the Vision Pro launch:</p>



<ul>
<li><strong>Designing an incredible in-store Vision Pro demo experience</strong>, with the primary goal of getting as many people as possible to experience the magic of VR through Apple‚Äôs lenses ‚Äî most of whom have no intention to even consider a $4,000 purchase. The demo is only secondarily focused on actually selling Vision Pro headsets.</li>



<li><strong>Launching an iconic woven strap that photographs beautifully</strong> even though this strap simply isn‚Äôt comfortable enough for the vast majority of head shapes. It‚Äôs easy to conclude that this decision paid off because nearly every bit of media coverage (including and especially third-party reviews on YouTube) uses the woven strap despite the fact that it‚Äôs less comfortable than the dual loop strap that‚Äôs ‚Äúhidden in the box‚Äù.</li>
</ul>



<h2><strong>The existence of Vision Pro in 2024 is entirely a function of Apple managing to ship a <em>first-of-its-kind</em> ultra <strong>high-resolution display</strong></strong></h2>



<p>One of our biggest product positioning struggles within the Oculus VR team from the very beginning ‚Äî especially when trying to convince reviewers ‚Äî was always related to having <em>underwhelming displays</em>. Every single Oculus headset that ever shipped (including the latest Quest 3) has suffered from resolution/pixelation issues varying from ‚Äúterrible‚Äù to ‚Äúpretty bad‚Äù. It‚Äôs like we‚Äôre living in the VR-equivalent world of VGA computer monitors.</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong>In order For Apple to make a huge splash entering the VR market ‚Äî a category that‚Äôs been around the consumer world for nearly 10 years ‚Äî they needed to launch a product that was unambiguously better than anything that had ever existed. The obvious way to do that was to <strong>attack the Achilles heel of all existing headsets and reinvent the VR display</strong>, and that‚Äôs exactly what Apple did with the Vision Pro.</mark></p>
</blockquote>



<p>Vision Pro is the first VR headset that offers good enough resolution and visual acuity with little semblance of a <a href="https://pimax.com/what-is-the-screen-door-effect-in-vr/" target="_blank" rel="noreferrer noopener"><em>screen door effect</em></a> or pixelation artifacts. This level of presence and fidelity could only be made possible with an ultra high-res display, and it‚Äôs 100% clear that achieving an first-of-its-kind level of display quality was the internal launch bar for Vision Pro at Apple.</p>



<p>Apple‚Äôs relentless and uncompromising hardware insanity is largely what made it possible for such a high-res display to exist in a VR headset, and it‚Äôs clear that this product couldn‚Äôt possibly have launched much sooner than 2024 for one simple limiting factor ‚Äî the maturity of micro-OLED displays plus the existence of power-efficient chipsets that can deliver the heavy compute required to drive this kind of display (i.e. the M2).</p>



<p>Micro-OLED displays differ from any other previous consumer display technology because they are manufactured on top of a silicon substrate (similar to how semiconductor chips are made). To put the insanity of micro-OLED displays in perspective, the <strong>Vision Pro panel has a 7.4x higher pixel density than the latest iPhone and nearly 3x the Quest 3</strong>:</p>







<figure><table><thead><tr><th>Feature</th><th>Vision Pro</th><th>Bigscreen Beyond</th><th>Quest 3</th><th>iPhone 15 Pro Max</th></tr></thead><tbody><tr><td>Display Type</td><td>Micro-OLED</td><td>Micro-OLED</td><td>LCD</td><td>OLED</td></tr><tr><td>Resolution <br>(pixels per eye)</td><td>3660 x 3200</td><td>2560 x 2560</td><td>2064 x 2208</td><td>2796 x 1290</td></tr><tr><td>Total Pixels</td><td>23 million</td><td>13 million</td><td>9 million</td><td>3.6 million</td></tr><tr><td>Pixels Per Inch (PPI)</td><td>3386</td><td>(unknown)</td><td>1218</td><td>460</td></tr><tr><td>Pixels Per Degree (PPD)</td><td>34</td><td>32</td><td>25</td><td>94 <br>(at 1 foot distance)</td></tr></tbody></table></figure>



<p><em>(See the appendix of this essay for a quick explanation of <strong>Pixels Per Degree </strong>or <strong>PPD</strong>)</em></p>



<p>The folks at iFixit created this stunning GIF using a scientific microscope to compare the pixel size of the Vision Pro display ‚Äî which measures 7.5 Œºm, the size of a human red blood cell ‚Äî with the pixel size of the latest iPad and iPhone displays:</p>







<figure><img data-attachment-id="54" data-permalink="https://hugo.blog/2024/03/11/vision-pro/avp-display/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif" data-orig-size="940,520" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="avp-display" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=940" width="940" height="520" src="https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=940" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif 940w, https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=768 768w" sizes="(max-width: 940px) 100vw, 940px"><figcaption>Source: <a href="https://www.ifixit.com/News/90409/vision-pro-teardown-part-2-whats-the-display-resolution" target="_blank" rel="noreferrer noopener">iFixit</a></figcaption></figure>



<p>The Apple Vision Pro‚Äôs micro-OLED display has created a lot of chatter in my hardware supply chain world, with lots of companies ‚Äî predominantly smartphone OEMs ‚Äî quickly racing to try and build a product that can deliver a similar experience to Vision Pro. Apple has secured a 1-year exclusive with <a href="https://www.sony-semicon.com/en/products/microdisplay/oled.html" target="_blank" rel="noreferrer noopener">Sony Semiconductor Solutions Group</a> and its second supplier <a href="https://www.seeya-tech.com/en/" target="_blank" rel="noreferrer noopener">SeeYA Technology</a>. There are also rumors Apple is dropping Sony as a display supplier and replacing it with <a href="https://www.boe.com/en/Enterprise/VR_AR">BOE</a> (whose <a href="https://www.boe.com/en/Enterprise/VR_AR">website</a> says a panel equivalent to Vision Pro is at ‚Äúsample‚Äù stage).</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong>I fully expect the <a href="https://www.prnewswire.com/news-releases/lg-and-meta-forge-collaboration-with-meta-to-accelerate-xr-business-302073794.html" target="_blank" rel="noreferrer noopener">recently announced Meta/LG partnership</a> to be all about creating a supply chain advantage for Meta so they can race a <em>Quest Pro 2 </em>product into market that can compete with Vision Pro with LG putting some skin in the game to lower the street price of the headset.</mark></p>
</blockquote>



<p>(P.S. For anyone who wants to dig into more details of the Vision Pro display and pass-through system, I highly recommend <a href="https://www.ifixit.com/News/90409/vision-pro-teardown-part-2-whats-the-display-resolution" target="_blank" rel="noreferrer noopener">this article from iFixit</a> and <a href="https://kguttag.com/about-karl-guttag/" target="_blank" rel="noreferrer noopener">this article from Karl Guttag</a>, who‚Äôs an amazingly talented expert in display devices).</p>



<h2><strong>Apple made the Vision Pro display <em>intentionally blurry</em> in order to hide pixelation artifacts and make graphics appear smoother</strong></h2>



<p>There is a very good reason Apple has not used the word <em>retina</em> anywhere in their marketing materials for Vision Pro. It‚Äôs the simple fact that Vision Pro‚Äôs display does not pass the retina test ‚Äî which is a <em>resolution high enough that the human eye can no longer discern individual pixels</em>. The Vision Pro display is nowhere near retina quality for a VR headset (see appendix for details) and yet <strong>our eyes cannot see individual pixels when looking at it</strong>. What gives?</p>



<p>During the first few days using Vision Pro, there was something that kept calling my attention but which I struggled to get my arms (or eyes) around. Everything my eyes saw in the headset felt a bit softer than I expected, and I initially attributed this to the seemingly refreshing absence of any <em><a href="https://pimax.com/what-is-the-screen-door-effect-in-vr/" target="_blank" rel="noreferrer noopener">screen door effect</a></em> ‚Äî a pixelation artifact that has essentially doomed all VR headsets created up until now.</p>



<p>Well, as it turns out, the incredible <a href="https://kguttag.com/about-karl-guttag/" target="_blank" rel="noreferrer noopener">Karl Guttag</a> ran a meticulous <a href="https://kguttag.com/2024/03/01/apple-vision-pros-optics-blurrier-lower-contrast-than-meta-quest-3/" target="_blank" rel="noreferrer noopener">photographic analysis of the Vision Pro display</a> and came to a curious and possibly disturbing conclusion: <strong>Apple <em>intentionally calibrated the Vision Pro display slightly out of focus</em> to make pixels a bit blurry and hide the screen door effect ‚Äúin plain sight‚Äù</strong>.</p>



<p>This image from Karl‚Äôs blog explains this well by comparing Vision Pro and Quest 3 displays side by side at a close enough distance where it‚Äôs possible to see individual pixels and clearly see the intentional blur that was added to the Vision Pro display:</p>







<figure><img data-attachment-id="179" data-permalink="https://hugo.blog/2024/03/11/vision-pro/avp-vs-mq3-close-up-crop-copy/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp" data-orig-size="471,519" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="avp-vs-mq3-close-up-crop-copy" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp?w=272" data-large-file="https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp?w=471" loading="lazy" width="471" height="519" src="https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp?w=471" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp 471w, https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp?w=136 136w, https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp?w=272 272w" sizes="(max-width: 471px) 100vw, 471px"><figcaption>Extreme close-up comparison between Vision Pro (AVP) and Quest 3 (MQ3) displays (Source: <a href="https://kguttag.com/2024/03/01/apple-vision-pros-optics-blurrier-lower-contrast-than-meta-quest-3/">KGOnTech</a>)</figcaption></figure>



<p>What Karl concluded is that even though Quest 3 has a much lower display resolution than Vision Pro (1,218 PPI vs. 3,386 PPI), Quest 3 appears objectively crisper especially when showing high-contrast graphics. In other words, Quest 3 is squeezing the highest possible resolution out of its display at the expense of a ‚Äúharsher look‚Äù while Apple is giving up some of the Vision Pro‚Äôs display resolution in order to achieve a ‚Äúsofter look‚Äù. Karl may disagree with my conclusion on this point:</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong> Intentionally making the Vision Pro optics blurry is a clever move by Apple because it results in way smoother graphics across the board by hiding the screen door effect (which in practice means that you won‚Äôt see pixelation artifacts). This is also where Apple‚Äôs ‚Äútaste‚Äù comes in, essentially resulting in the Vision Pro display being tuned to have a unique, softer, and more refined aesthetic than Quest 3 (or any other VR headsets). This is certainly a refreshing approach to designing VR hardware.</mark></p>
</blockquote>



<p>With this design decision, Apple is no doubt giving up a bit of the Vision Pro display‚Äôs high pixel resolution in order to achieve overall smoother graphics. You are definitely losing some text crispness in order to gain a higher perception of quality for images, video and 3D animations. This is a big benefit of starting with an ultra high-resolution micro-OLED display ‚Äî Apple had enough pixels to work with that they could afford to make this trade-off. <strong>This is the kind of thing that our hardcore VR engineers at Oculus would have fought against to the end of the world, and I doubt we could have ever shipped a ‚Äúblurred headset‚Äù, LOL!</strong></p>



<h2><strong>Sadly, the Vision Pro display suffers from <em>significant motion blur &amp; image quality issues</em> that render passthrough mode unusable for longer periods</strong></h2>



<p>While Apple‚Äôs decision to make individual pixels blurry on the Vision Pro display was extremely clever, the headset unfortunately suffers from a completely different type of blur that‚Äôs extremely problematic for the overall experience.</p>



<p>From the very first time I put on my Vision Pro, I noticed <strong>a lot of motion blur in passthrough</strong> mode even in excellent ambient lighting conditions and a still noticeable amount even when viewing immersive content. While my immediate instinct was to think that all VR headsets have that kind of motion blur and it‚Äôs just more noticeable on Vision Pro, a side-by-side comparison with Quest 3 quickly proved it‚Äôs significantly more serious on Vision Pro. This is particularly surprising considering that the passthrough cameras and display are both running at 90 hertz.</p>



<p>Since none of the initial Vision Pro reviews pointed out this issue, I ended up even calling Apple support to find out if this might be a known problem or possibly even a hardware defect. But then more in-depth reviews began pointing out the same problem (I highly recommend <a href="https://youtu.be/eOH33sWgds8?si=TTWCd8-UqX-D9-Eg" target="_blank" rel="noreferrer noopener">this review by Snazzy Labs</a>).</p>



<p>Motion blur in passthrough mode ended up being one of the many reasons why I decided to return my Vision Pro, because it‚Äôs just uncomfortable, leads to unnecessary eye strain, and really gets in the way of anyone using the headset for longer periods of time in passthrough mode.</p>



<p>There are other noticeable issues as well which affect passthrough mode, including <strong>very little dynamic range, incorrect white balance in most indoor use cases, and signs of edge distortion and chromatic aberration</strong>. Some of these might be addressed by software updates, but I expect most will not as they probably are limitations of the hardware stack.</p>



<h2><strong>The Vision Pro packs <em>a lot more computing power </em>than most people might realize ‚Äî the M2 + R1 combination puts it at the level of a MacBook Pro</strong></h2>



<p>Any standalone VR headset is basically a 2-in-1 system: a regular ‚Äúcomputing‚Äù computer and a spatial computer bundled together.</p>



<ul>
<li><strong>A <em>regular computer</em> in charge of running applications and performing general computation</strong>: this is everything that happens on your smartphone, tablet or notebook, including running the OS, executing applications across CPU/GPU loads, and doing computation work in the background.</li>



<li><strong>A <em>spatial computer</em> in charge of the environment</strong>: it keeps track of the whole environment, tracks your hands &amp; eyes, and ensures that everything ‚Äî your surroundings, the OS system UI, and your apps ‚Äî gets rendered in the right physical place in space and updated at 90 to 120 times per second while your head and body are moving around.</li>
</ul>



<p>These two ‚Äúcomputers‚Äù must operate together without missing a beat ‚Äî any latency above 20 milliseconds becomes quickly noticeable in VR and will often translate very quickly into user perception of unresponsiveness or jankiness, which can cause discomfort, eye strain or even dizziness for many people.</p>



<p>Enter the Vision Pro dual-chip design:</p>







<blockquote>
<p>‚Äú<em>A unique dual‚Äëchip design enables the spatial experiences on Apple Vision Pro. The powerful M2 chip simultaneously runs visionOS, executes advanced computer vision algorithms, and delivers stunning graphics, all with incredible efficiency. And the brand-new R1 chip is specifically dedicated to process input from the cameras, sensors, and microphones, streaming images to the displays within 12 milliseconds ‚Äî for a virtually lag-free, real-time view of the world.</em>‚Äú</p>
<cite>Apple Vision Pro website</cite></blockquote>



<p>The Vision Pro ships with the same M2 chip as the 2022 iPad Pro (or 2022 MacBook Air) alongside the new R1 chip which handles the massive amount of data coming from the 20+ tracking cameras and depth sensors (sensor fusion). What‚Äôs interesting to note is that Vision Pro actually does perform largely like an iPad Pro in benchmark tests that push CPU and GPU to their limits in both single-core and multi-core scenarios (see chart below).‚ÄÇ</p>







<figure><img data-attachment-id="83" data-permalink="https://hugo.blog/2024/03/11/vision-pro/vision-pro_geekbench/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png" data-orig-size="1542,1416" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="vision-pro_geekbench" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=1024" loading="lazy" width="1024" height="940" src="https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png 1542w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Source: <a href="https://www.pcmag.com/reviews/apple-vision-pro" target="_blank" rel="noreferrer noopener">PC Magazine</a></figcaption></figure>



<p>This is more impressive than it seems, and demonstrates that the R1 chip is doing a very significant amount of heavy lifting ‚Äî essentially the vast majority the spatial computing workload ‚Äî leaving a lot of compute room for the M2 chip to deliver the same level of performance as if it was just running inside an iPad Pro. </p>



<p>By all accounts so far, the R1 chip appears to be of a fairly similar package size as the M2 chip (though built using a specialized architecture), which puts the Vision Pro well ahead of any current generation iPad or MacBook Air, and likely more on par with a MacBook Pro from a silicon performance perspective. Definitely an impressive achievement by the Apple Silicon team.</p>



<p>This also begs the question‚Ä¶ what if you could completely offload the Vision Pro‚Äôs compute to another Apple device?</p>



<h2><strong>Apple‚Äôs decision to use a tethered pack <em>will</em> <em>enable future Vision headsets to be much lighter</em> by offloading compute to an iPhone, iPad or MacBook </strong></h2>



<p>One of the most controversial aspects of Vision Pro is the fact that it sports a tethered battery pack, differently from all other commercially available standalone VR headsets. Many people have heavily criticized Apple for this decision because of the inconvenience of the ‚Äúhanging‚Äù external battery.</p>



<p>I agree with Palmer Luckey (<a href="https://youtu.be/S-sD2FTjSaw?si=AQUTyJria4TKmSbc&amp;t=1437" target="_blank" rel="noreferrer noopener">from his recent interview with Peter Diamandis</a>) that this was a necessary short-term decision on Apple‚Äôs part given the reality of the hardware shipping inside the Vision Pro, but more importantly it was a very intentional long-term decision, which I‚Äôll explain.</p>



<p>As I mentioned earlier, the Vision is a meticulously over-engineered computer with a very significant collection of power-hungry components:</p>



<ul>
<li>2x laptop-class processors (the R1 chip is almost the same size as the M2, which is the same processor shipping on MacBooks)</li>



<li>2x very bright micro-OLED displays with high pixel density</li>



<li>1x auxiliary EyeSight display</li>



<li>12x cameras and other sensors</li>



<li>2x blower fans</li>



<li>2x speakers</li>
</ul>



<p>As Quin from Snazzy Labs carefully explains <a href="https://youtu.be/eOH33sWgds8?si=M_-SlaOYK5k-1SPP&amp;t=988" target="_blank" rel="noreferrer noopener">in his excellent review</a>, the Vision Pro likely draws as much as <strong>40 watts of power</strong>, which is more than most MacBook laptops. This also means it has a power supply with the potential of generating a lot of heat. So, in addition to transferring the battery weight out of the headset, the decision to move to a tethered pack also keeps a huge heat source safely away from your head.</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong>All that said, the long-term strategic reason for having an external battery pack is to set expectations with Vision Pro users that there will <em>always</em> be an external box connected to the headset. In future Vision headsets, Apple should be able to comfortably start moving a lot of electronics off the headset, possibly shaving off as much half of the weight over a few generations and <strong>target around 300g</strong>. This also opens an extremely interesting path for Apple in a few years to <strong>use an iPhone, iPad or MacBook as the tethered computer driving the headset</strong>, which would dramatically simplify the headset.</mark> </p>
</blockquote>



<p>Interestingly, there is a tethered VR headset in the market today that demonstrates this desirable end state. It‚Äôs the <a href="https://www.bigscreenvr.com/" target="_blank" rel="noreferrer noopener"><strong>Bigscreen Beyond</strong></a>, the world‚Äôs smallest PC VR headset (i.e. needs to be tethered to a computer) that is lighter than even most ski goggles at 127 grams. Bigscreen‚Äôs ability to build this product is in many ways a bit of cheating since the headset was stripped of all sensors (no external cameras or eye tracking), but its existence nonetheless plays an important role in letting us experience what the future holds and where Apple‚Äôs sights are focused.</p>







<figure><img data-attachment-id="152" data-permalink="https://hugo.blog/2024/03/11/vision-pro/carmack_bigscreen-beyond/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg" data-orig-size="1600,1183" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="carmack_bigscreen-beyond" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=1024" loading="lazy" width="1024" height="757" src="https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg 1600w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>John Carmack wearing the Bigscreen Beyond VR headset, <br>which weighs 127g (vs. the Vision Pro‚Äôs 600g)</figcaption></figure>



<h2>Software</h2>



<h2><strong>The Vision Pro software story is a <em>bold antithesis of VR</em> ‚Äî and the lack of exciting AR apps at launch paints the product into an empty corner</strong></h2>



<p><em>‚ÄúWelcome to the era of spatial computing‚Äù</em> is Apple‚Äôs leading slogan for Vision Pro and, and as expected by everyone in the VR industry, Apple is going all-in on AR (augmented reality) to deliver on this proposition. The company has gone out of their way to actively ignore everything that VR has been know for over the last decade.</p>



<p>At the center of Apple‚Äôs marketing for Vision Pro is <em>‚Äúkeeping users connected to their surroundings and other people‚Äù</em>. Reading in-between these lines, it‚Äôs not hard to see that Apple is taking an anti-VR stance that borderline accuses Meta‚Äôs approach to VR of promoting human isolation while positioning Vision Pro as the antithesis of that.</p>







<blockquote>
<p><mark><strong>MY TAKE:</strong> Apple‚Äôs anti-VR stance is a risky move because it negates most of the traditional immersive content that has made the VR medium popular until now, and at least for now is painting Vision Pro into an empty corner. This reminds me of Apple‚Äôs broad </mark><mark>stance on privacy ‚Äî built to be in complete opposition to Meta/Google ‚Äî which has put them in a tight spot by severely limiting their options and restricting innovation in the age of Gen AI. </mark></p>
</blockquote>



<p><strong>There are no fully immersive games in the Vision Pro app store</strong>, whereas easily &gt;90% of the Oculus Quest catalog is made of immersive VR games. Instead of leveraging the existing community of high-quality immersive VR content developers, Apple is focusing all of its energy exclusively on AR use cases that play to the company‚Äôs ecosystem strengths ‚Äî iOS apps and MacOS productivity ‚Äî which I‚Äôll dive into over the next few sections.</p>



<p>The launch roster of 3D AR apps &amp; games is a tremendous disappointment ‚Äî in both quality and quantity ‚Äî and mostly includes a few simple casual games, some of which are originally 2D games hastily converted into 3D art. The fact that <em>ARKit</em> has been available for so many years on iPad and iPhone (despite its limited success) should have made it possible for Apple to easily round up developers into building a sufficient number of exciting and impressive AR titles for Vision Pro. Instead, <strong>we‚Äôre seeing an initial lack of developer excitement for the category that should have been the <em>most defining and inspiring category </em>on Vision Pro.</strong></p>



<p>Ironically, Meta made almost exactly the same mistake launching Quest Pro in 2022. That headset shipped with close to zero AR apps despite their emphasis in the launch messaging on ‚Äúfull-color mixed reality‚Äù.</p>







<blockquote>
<p><mark><strong>MY TAKE</strong>: This may be the first device category where Apple‚Äôs ‚Äúbuild it and they will come‚Äù approach to creating developer traction may simply not work as previously. It will be many years (and possibly even more than a decade) before there are tens of millions of active Vision Pro users willing to pay for spatial AR apps. Apple will need to take a page out of the Oculus playbook and actively motivate developers financially to develop for Vision Pro.</mark></p>
</blockquote>



<h2><strong>Vision Pro‚Äôs positioning as <strong>a </strong><em><strong>productivity &amp; movie watching ‚Äúbig scree</strong>n‚Äù</em></strong> <strong>is dull &amp; unimaginative <strong>but Apple is unashamedly owning it</strong> </strong></h2>



<p>With a weak and limited launch roster of AR apps that doesn‚Äôt include a single flagship 3D app or game, Apple had to focus the entire positioning for Vision Pro at launch almost entirely on how it plugs into the existing Apple ecosystem of 2D apps.</p>



<p>In the usual Apple-style product marketing, the launch messaging for Vision Pro is very explicitly codified in the <a href="https://www.apple.com/apple-vision-pro/" target="_blank" rel="noreferrer noopener">product webpage</a> and every single marketing asset is consistent with it. How Apple has chosen to sequence their product messaging matters just as much as the messages themselves. <strong>Vision Pro is 60% about 2D productivity and 40% about watching media/movies on a big screen</strong>:</p>







<figure><table><tbody><tr><td><strong>Use case</strong></td><td><strong>Apple Slogans</strong></td></tr><tr><td>Productivity</td><td><em>‚ÄúFree your desktop. And your apps will follow.‚Äù</em><p><em>‚ÄúHow to work&nbsp;in all‚Äënew ways.‚Äù</em></p></td></tr><tr><td>Media</td><td><em>‚ÄúThe ultimate theater. Wherever you are. </em><p>‚Äú<em>An immersive way to experience entertainment.‚Äù</em></p></td></tr></tbody></table></figure>



<p>On a side note, <em>FaceTime with Persona avatars</em> and <em>spatial photos &amp; videos</em> are also pushed as core pillars in the Vision Pro product messaging, but they‚Äôre clearly just ancillary use cases to support marketing. Though too small to matter for now, they may (and for Apple‚Äôs sake, hopefully will) end up playing a much bigger role in the future.</p>







<blockquote>
<p><mark><strong>MY TAKE:</strong> The Vision Pro launch is a significant missed opportunity, with Apple ‚Äúwelcoming us into the era of spatial computing‚Äù with a software and services stack that is practically only focused on 2D use cases. Though the in-store demos paint an exciting future, the experience delivered by Apple at launch is dull and unimaginative at best.</mark></p>
</blockquote>



<p>Putting aside my criticism to Apple‚Äôs focus for the Vision Pro at launch, the next few sections will offer a deep dive into my thoughts and opinions on the software and experience enabling <em>Productivity</em> and <em>Media</em> use cases. </p>



<div>
<h2><strong>The Vision Pro desperately wants to be <em>the ‚Äúfuture of work</em>‚Äù and pick up where Meta Quest Pro completely dropped the ball, but‚Ä¶</strong></h2>



<p>One of our strongest thesis from the early Oculus days was always about VR playing a defining role in the ‚Äúfuture of work‚Äù, from running 2D apps in massive virtual displays to having native 3D apps that would make it a lot easier to work and collaborate with others on a project.</p>



<p>When Meta announced the Quest Pro in 2022, much of its marketing hype was in fact around the <a href="https://www.youtube.com/watch?v=eYjU9mV7-6g" target="_blank" rel="noreferrer noopener"><strong>Workrooms app</strong></a> (at the time led by my incredibly talented friend Mike LeBeau). The app allows you use your Mac from within VR, with a lot of attention to details needed to make it truly possible to work in VR for several hours, including support for up to 3 virtual monitors and the ability to see your physical keyboard in passthrough or replace it with a fully 3D rendered tracked twin.</p>



<figure><img data-attachment-id="196" data-permalink="https://hugo.blog/2024/03/11/vision-pro/quest-pro_workrooms-2/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png" data-orig-size="719,479" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="quest-pro_workrooms" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png?w=719" loading="lazy" width="719" height="479" src="https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png?w=719" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png 719w, https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png?w=300 300w" sizes="(max-width: 719px) 100vw, 719px"><figcaption>Image from the Quest Pro launch marketing</figcaption></figure>



<p>Quest Pro was designed with the goal of being a lot more comfortable than other VR headsets so that people could wear it for longer periods of time. While this was a well-intended attempt, the product had a major flaw which made it less than a ‚Äúminimum viable product‚Äù and simply did not justify a price tag well above $1,000. The display resolution ‚Äî at 22 PPD (pixels per degree) ‚Äî was too low and vastly insufficient to unlock ‚Äúworking in VR‚Äù because of poor text readability. This shortcoming (in addition to very poor quality passthrough) was so massive that it rendered the product practically irrelevant at launch ‚Äî I ended up returning my unit within 24 hours of first use.</p>



<p><strong><em>Can Vision Pro deliver where Quest Pro (and Quest 3) have failed?</em></strong></p>



<p>In order to really put Vision Pro to the test in real-life scenarios, I spent well over 100 hours trying to deploy as many of my own productivity workflows as I could, including about 1/3 of the work in this essay. I‚Äôll share my conclusions over the next couple of sections.</p>



<p>First off, before diving into the value proposition of Vision Pro as a work/productivity computer, I needed to clearly frame my ‚Äújobs to be done‚Äù as specifically as possible. When I‚Äôm in ‚Äúwork mode‚Äù ‚Äî whether doing actual professional work or just life management stuff ‚Äî I have three distinct workstations that I use back and forth (aside from my smartphone, which I won‚Äôt include here):</p>



<ul>
<li><strong>Office workstation</strong> <strong>| Mac Pro with 2x Apple XDR 6K displays:</strong> my highest productivity setup because it gives me access to everything I need in a single view and enables zero-hurdle multi-tasking; it‚Äôs basically my gold standard for any task or project no matter how complex with the highest speed &amp; quality</li>



<li><strong>Laptop | MacBook Pro 16-inch:</strong> medium-high productivity setup with a sufficiently large retina-quality display that still enables complex tasks with good enough multi-tasking though I do feel noticeably less productive; it requires a backpack to carry when going outside of home/office</li>



<li><strong>Tablet | iPad Pro 11-inch with keyboard:</strong> a low-medium productivity setup good for focused single-app work with extremely limited multi-tasking (ex. email, writing that doesn‚Äôt require research, some life planning) but still better than using my phone; one great advantage is that I can carry this ‚Äúmini computer‚Äù more easily than a laptop without really needing a backpack</li>
</ul>



<p>Here‚Äôs a table summarizing these workstations:</p>



<figure><table><tbody><tr><td><strong>Device</strong></td><td><strong>Number of Pixels</strong></td><td><strong>Number of simult. windows</strong></td><td><strong>Ideal for</strong></td><td><strong>Ergonomics</strong></td><td><strong>Portability</strong></td><td><strong>Cost</strong></td></tr><tr><td>Mac Pro + 2x XDR displays</td><td>XXLarge<br><em>(2√ó20 million)</em></td><td>8+</td><td>Any creative project with lots of multitasking</td><td>Ideal</td><td>‚Äì</td><td>&gt;$10,000</td></tr><tr><td>MacBook Pro 16-inch</td><td>Large<br><em>(7.7 million)</em></td><td>2-4</td><td>Most creative projects with limited multitasking</td><td>Good</td><td>Medium</td><td>$3,000</td></tr><tr><td>iPad Pro 11-inch + keyboard case</td><td>Medium<br><em>(4 million)</em></td><td>1</td><td>Full email, simple editing</td><td>Not great</td><td>High</td><td>$1,200</td></tr></tbody></table></figure>



<p><strong>I then asked myself: </strong>Could I see myself using Vision Pro as a productivity device instead of (or in conjunction with) any of my existing workstations?</p>



<p>These are the specific questions I set off to answer (from the lowest to highest bar):</p>



<ul>
<li>Can Vision Pro be a complete alternative to my Tablet Workstation so that I could carry it around instead of an iPad Pro?</li>



<li>Can Vision Pro enhance my Laptop Workstation enough that it feels like having a ‚Äúvirtual XDR display‚Äù or two?</li>



<li>Could Vision Pro ever be better than ALL of my workstations at least for some productivity tasks? <strong><em>‚áí This is what excites me the most!</em></strong></li>
</ul>
</div>



<div>
<h2><strong><span>Productivity Thesis #1</span>: Vision Pro as an iPad Pro replacement </strong></h2>



<blockquote>
<p><strong>Status:</strong> ‚ùå NOT READY <em>(but it‚Äôs promising!)</em></p>



<p><mark><strong>MY TAKE: </strong>The Vision Pro aspires to become your ‚Äúspatial iPad Pro‚Äù with really good potential for much better multi-tasking (than an iPad) and the ability to do focused work anywhere, but there‚Äôs simply too much usability friction and too many important apps missing for that to be a reality today (or likely in the next 1-2 years).</mark></p>
</blockquote>



<p>The Vision Pro is conveniently designed by Apple to immediately fit right into the existing Apple ecosystem as a (rather expensive) alternative to an iPad Pro. The headset has identical compute (same M2 chip) to an iPad Pro and conveniently supports iPad apps running natively. In fact, it‚Äôs easy to claim the Vision Pro should in principle be better than an iPad Pro because you can run multiple iPad apps side-by-side in full screen mode, which would overcome one of the biggest productivity limitations of iPads ‚Äî poor multitasking.</p>



<p>However, in reality this claim really doesn‚Äôt hold true at all (at least not yet) for a few important limitations of Vision Pro at launch:</p>



<ul>
<li><strong>Many iPad apps don‚Äôt work well </strong>(or at all) on Vision Pro despite <a href="https://developer.apple.com/help/app-store-connect/manage-your-apps-availability/manage-availability-of-iphone-and-ipad-apps-on-apple-vision-pro/" target="_blank" rel="noreferrer noopener">Apple automatically opting in developers</a>. There is substantial friction and instability navigating inside productivity apps given that they‚Äôre designed for a multi-touch UI (ex: some iPad gestures don‚Äôt exist in Vision Pro, and some touch targets are too small). A lot of apps will require some effort by their developers to work well enough.</li>



<li><strong>Most productivity apps are still missing from the App Store</strong> (likely for the reason above), which would leave large holes in most people‚Äôs workflows. For example, the most important missing apps for my own workflows include Chrome, Gmail, GDocs/Sheets/Slides, Asana.</li>



<li><strong>Text input is still quite buggy</strong> which adds more friction to any productivity workflow. Cursor placement, text selection and editing are super error prone. Dictation doesn‚Äôt stream results as you speak.</li>



<li><strong>You must carry a keyboard and a trackpad</strong> (mice are <span>not</span> supported) for the vast majority of your iPad-class productivity workflows on Vision Pro, which could be an added inconvenience (compared to carrying an iPad with a keyboard case or even a laptop). Editing documents, spreadsheets or presentations without those is virtually impossible.</li>



<li><strong>There is no reliable workspace persistency</strong> which adds even more friction ‚Äî you are forced to re-open apps, and then re-size and reposition windows almost every time. The capabilities we all want (which Apple should be able to ship soon if they want to) are (i) persistent workspaces, (ii) location-specific workspaces, and (iii) a spatial computing equivalent of Mission Control.</li>
</ul>



<p>All that said, these limitations can all be addressed by Apple and the potential of Vision Pro as an iPad Pro replacement really is there. Even though the iPad Pro has nearly twice the PPD (pixels per degree) as the Vision Pro, text readability of iPad apps on Vision Pro is good enough for you to run 3 or 4 side-by-side apps plus a number of ambient widgets.</p>



<p>I also really believe there‚Äôs a large enough white canvas for lots of Apple-style innovation and magic around letting users configure and manage their workspace with a combination of 2D panels and virtual 3D objects. The potential is really significant as long as Apple really empowers developers to innovate here (try <em>Nicholas Jitkoff‚Äôs <a href="https://www.widget.vision/" target="_blank" rel="noreferrer noopener">widget.vision</a></em> to see some great early examples ‚Äî the NY Times front page widget is my favorite). </p>



<p><strong>Call me crazy, but I personally could get quite excited by the idea of a ‚Äúspatial iPad Pro‚Äù</strong> if I was able to actually get all of my iPad apps on the Vision Pro, and if Apple addresses all of the issues causing friction in my workflows. The reason why is simply that of focus ‚Äì to be able to really ‚Äúdial down reality‚Äù and tune into the work wherever I am, without carrying my laptop with me but still having some degree of multitasking available.</p>



<h2><strong><span>Productivity Thesis #2</span>: Vision Pro as a MacBook virtual external monitor </strong></h2>



<blockquote>
<p><strong>Status:</strong> ‚úÖ ALMOST READY <em>(needs some bug fixing!)</em></p>



<p><mark><strong>MY TAKE: </strong>The Vision Pro is a few software bug fixes away from being a suitable virtual-equivalent to an external monitor similar to a 27-inch Apple Studio Display that makes it easy to work immersively in VR using all your existing MacOS apps and workflows on a huge screen (but don‚Äôt expect an Apple XDR 6K experience!).</mark></p>
</blockquote>



<p>One of Vision Pro‚Äôs best pieces of pure software/experience magic is the ability to seamlessly connect to a MacBook by simply looking at the computer while wearing the headset. This is a simple improvement to the traditional AirPlay UI that creates a profound sense of seamlessness which VR has always lacked.</p>



<p>Before diving into this thesis, I‚Äôll establish that the Vision Pro will <em>never</em> become a suitable alternative for my office workstation with dual Apple XDR 6K displays. At 32 inches per-monitor and a total of 40 million pixels (with pixel density of 218 PPI and angular resolution &gt;100 PPD) and without a weight around my head, it is simply not a bar I would hold a VR headset against today or at any point in the future.</p>



<p>The more interesting question to focus on is whether Vision Pro could even begin to look like a suitable replacement for one or more 27-inch Apple Studio Displays (or equivalent). The short answer today is that Vision Pro can indeed come close (or very close) to that, but there are some important limitations that Apple needs to address to make this a relatively frictionless use case:</p>



<ul>
<li><strong>Lack of dual (or triple) monitor support</strong> is a huge bummer despite the fact that there are decent reasons for it (largely related to requiring a lot of local Wi-Fi bandwidth). Even though Vision Pro has a relatively narrow field of view that still feels like looking through binoculars, if I could get two or three virtual monitors out of a MacBook Air for example, things would start to look more interesting.</li>



<li><strong>Inconsistent keyboard and trackpad behaviors</strong> makes it very hard to switch back and forth between iPad/Vision apps and the Mac virtual display. I constantly find myself looking for my cursor, seeing the virtual keyboard pop up onscreen when clearly I don‚Äôt need it (if I‚Äôm using a physical keyboard), not to mention that I cannot use my beloved Logitech MX mouse.</li>



<li><strong>There is no reliable workspace persistency</strong> which is exactly the same issue I discussed when talking about the iPad Pro use case in the previous section. This should be an easy fix.</li>



<li><strong>Eye tracking doesn‚Äôt work in MacOS</strong> which not only leads to inconsistent input modalities as I said above but also just feels like a huge missed opportunity to offer a magical capability that MacOS has never seen before. This is not a low-hanging bug fix, but I don‚Äôt see it as a huge technical leap for Apple if the MacOS team wants to address it.</li>



<li><strong>MacOS apps are ‚Äústuck‚Äù inside the virtual monitor</strong> instead of being allowed to move around the entire space. This is another missed opportunity to deliver a truly spatial/immersive experience with Vision Pro, though significantly more complicated for Apple to address and would require really careful engineering by both MacOS and visionOS teams.</li>
</ul>



<p>Many of the issues I highlighted above are straightforward software challenges that Apple is well equipped to address and would make a world of difference. I suspect it‚Äôs a matter of dealing with the usual internal politics/collaboration challenges getting the MacOS team to dedicate the necessary resources to address bugs and feature requests from the visionOS team.</p>



<p>The bottom line for me is that <strong>we can see a relatively near future where carrying a MacBook Air and a Vision Pro in your backpack could give you a reasonably good workstation</strong>, one that delivers enough benefits in the form of productivity gains that you might be willing to wear a headset for a few hours in a caf√©, on an airplane, or even on your couch at home. (This perspective is of course made in complete absence of value-for-money considerations).</p>



<p>Unsurprisingly, this is Apple‚Äôs strongest hand with the Vision Pro launch as it‚Äôs 100% controlled by them and uniquely leverages the existing Apple ecosystem. Yes, it‚Äôs a very uninspiring and unimaginative use case, but it might be powerful enough for Apple to move a lot of headsets.</p>



<h2><strong>Watching movies in Vision Pro is great at first but most people will stop doing it after the initial novelty excitement wears off</strong></h2>



<p>Watching TV/movies in virtual reality seemed like such an incredibly compelling idea that we (the Oculus team at Meta/Facebook) built an entire product around that idea ‚Äî <em>Oculus Go</em>. Launched in 2018, Oculus Go was the biggest product failure I‚Äôve ever been associated with for the simple reason that it had extremely low retention despite strong partnerships with Netflix and YouTube. <strong>Most users who bought Oculus Go completely abandoned the headset after a few weeks.</strong> The full story is much more nuanced (including the fact that the Oculus Go failure got us on the path to Oculus Quest very quickly), but it taught us an important lesson.</p>



<p>The lesson we learned is that watching traditional (rectilinear) TV or movies in VR feels incredibly compelling at first, but the novelty wears off for most people after a few weeks. The reasons are:</p>



<ul>
<li><strong>It‚Äôs just not physically comfortable</strong> compared to watching TV or movies on an TV, tablet, or laptop, primarily because of the pressure on your head and face, plus the fact that you can‚Äôt comfortably sit in any position or lie down while wearing with the headset</li>



<li><strong>There‚Äôs a lot of friction </strong>to start watching a video in a VR headset if you‚Äôre not already in VR ‚Äî frequently a lot more steps required (especially finding and putting on the headset) and a more cumbersome navigation UI compared to our other devices</li>



<li><strong>It‚Äôs socially isolating and lonely</strong> to watch videos in VR, which will be a deal breaker for many people (although definitely not all)</li>
</ul>



<p>Back in the Oculus Go days, we concluded rather quickly that media consumption in VR is simply not a core ‚Äúdaily driver‚Äù pillar but more an ancillary use case that adds some value to other core pillars (such as productivity or gaming).</p>



<p>Vision Pro does bring more to the table with a much better display than previous VR headsets which can create magical movie experiences on occasion. For instance, watching an animated Disney or Pixar movie in 3D is absolutely stunning. But the essential product-market fit challenge remains:  </p>



<blockquote>
<p><mark><strong>MY TAKE:</strong> VR is simply not a medium people will gravitate towards for watching 2D media on a regular basis. Adding to this all of the Vision Pro‚Äôs </mark><mark>comfort and friction issues, most people who get excited about watching media in the headset will eventually find themselves going back to their TV, tablet or laptop as their primary devices for video.</mark></p>
</blockquote>



<p>Watching 3D movies on Vision Pro is a fun entertainment experience, but these videos are ‚Äúboxed‚Äù and don‚Äôt feel anything like witnessing real life. With the Vision Pro, Apple launched its new <em>Apple Immersive</em> video format, which opens the door for a new class of entertainment.</p>



<h2><strong>Apple Immersive Video opens a new world of possibilities for media in VR ‚Äî but its <em>hyperrealism</em> may bring an unexpected <em>uncanny valley challenge</em></strong></h2>



<p>One of the big original bets we made at Facebook/Meta with the launch of <em>Oculus Go</em> in 2018 was that immersive 180-degree video would attract a massive amount of consumer interest and that this would somehow trigger a chain reaction in the world of entertainment. We were able to secure partnerships with a small number of media companies who had become specialized in capturing VR video early on, and we were off to the races.</p>



<p>Our initial excitement cooled off quickly. VR180 video quality on Oculus Go was decent but flat, washed out and far from amazing mostly due to low resolution. These videos didn‚Äôt create a true sense of presence, of feeling transported to another reality. And most of the content was of one-off nature, with no real franchises that would have people coming back for more (with the exception of sports, which failed initially for other reasons that I‚Äôll come back to later).</p>



<p>Within a year, the Oculus team pivoted to VR gaming and stopped investing in immersive video almost completely.</p>



<p>In 2020, Apple acquired <a href="https://www.youtube.com/@Nextvr" target="_blank" rel="noreferrer noopener"><strong>NextVR</strong></a>, one of the small but highly respected companies we had been working with as they were edging into bankruptcy (we passed on the acquisition at Meta/Oculus). NextVR had spent over a decade building and perfecting VR 180 camera technology and production pipelines for broadcast-quality video. <a href="https://www.youtube.com/@Nextvr" target="_blank" rel="noreferrer noopener">The NextVR YouTube channel</a> is still live and provides amazing examples of what became possible with their technology <em>(make sure to pan around using your mouse/finger while watching videos in their YT channel)</em>.</p>



<figure><img data-attachment-id="102" data-permalink="https://hugo.blog/2024/03/11/vision-pro/nextvr_camera/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png" data-orig-size="1200,800" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="nextvr_camera" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=1024" loading="lazy" width="1024" height="682" src="https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>The latest publicly displayed NextVR 180-3D camera in 2018 (Source: <a href="https://www.theverge.com/2018/1/8/16862048/nextvr-six-degrees-of-freedom-augmented-reality-events-ces-2018" target="_blank" rel="noreferrer noopener">The Verge</a>)</figcaption></figure>



<p>The NextVR acquisition is what led to the incredibly <strong>Apple Immersive </strong>video format, which enables capture of <em>3D video in 180 degrees in 8K resolution at 90 frames per second</em>, an absolute juggernaut format with 8 times the number of pixels of a regular 4K video. <strong>The best way to think of the new <em>Apple Immersive</em> video format is kind of like a new IMAX-3D</strong>, but the real magic is the fact that it‚Äôs projected inside an imaginary 180-degree sphere (horizontally <em>and</em> vertically) that takes over your entire field of view.</p>



<p>Vision Pro is the first VR headset that enables playback of 180-degree 3D video at what feels to the eyes like 4K quality. At launch, there are four Apple TV short films on Vision Pro shot in Apple Immersive video format. My absolute favorite of these films ‚Äî <em>Adventure</em> ‚Äî is a jaw-dropping cinematic piece that is likely to win its share of movie awards. Experiencing the Norwegian fjords with this level of immersion is completely breathtaking, so much so that it might be my favorite experience so far in Vision Pro. I have never felt transported to another place in this manner in any experience I‚Äôve ever had, anywhere, period.</p>



<figure><img data-attachment-id="112" data-permalink="https://hugo.blog/2024/03/11/vision-pro/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post-jpg-slideshow_large/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg" data-orig-size="1960,1104" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=1024" loading="lazy" width="1024" height="576" src="https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg 1960w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><strong>Adventure</strong> is one of the short films shot in Apple Immersive format released with the Vision Pro launch</figcaption></figure>



<p>My second favorite Apple Immersive video ‚Äî <em><a href="https://www.youtube.com/watch?v=d555q5vaYns" target="_blank" rel="noreferrer noopener">Alicia Keys: Rehearsal Room</a></em> ‚Äî is a super fun and intimate concert that really makes you feel what presence in VR could be like with another human. While it‚Äôll be fun for nearly everyone to see Alicia Keys in a close-up VR performance, it may not be nearly as happy and inspiring to see a human in close proximity in other situations.</p>



<blockquote>
<p><mark><strong>MY TAKE:</strong> The super high-fidelity <mark>Apple Immersive </mark>video format will run into an unexpected and significant ‚Äúuncanny valley‚Äù challenge as a consequence of its <em>hyperrealism</em>. Seeing someone right in such close proximity to you and in such high fidelity may feel cool to one person but will feel uncomfortable or overwhelming to others. Less so in a scene like this intimate music concert or sports game, but probably a lot more so in dramatic storytelling and other types of more realistic films.</mark></p>
</blockquote>



<p>Back in the Oculus days, we used to run experiments to try and really understand which lines could not be crossed in VR content to avoid people feeling overwhelmed or even unsafe. One of the findings in these experiments was that too much realism and fidelity could be one of the things that crosses a line. In other words, <em>hyperrealism</em> could quickly drag people into the <em>uncanny valley</em>, one of two places we always want to avoid in VR (the other place is motion sickness).</p>



<figure><img data-attachment-id="139" data-permalink="https://hugo.blog/2024/03/11/vision-pro/alicia/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg" data-orig-size="1280,720" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="alicia" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=1024" loading="lazy" width="1024" height="576" src="https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg 1280w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Many people will feel themselves crossing the uncanny valley while<br>watching <em><a href="https://www.youtube.com/watch?v=d555q5vaYns" target="_blank" rel="noreferrer noopener">Alicia Keys: Rehearsal Room</a></em> on Vision Pro</figcaption></figure>



<p>Navigating this creative challenge will take time and a lot of experimentation on Apple‚Äôs side, and they‚Äôre the one company in the world we can trust to have the level of sensitivity and artistry for this journey, not to mention the ability to hire the best of the best talent. Practically, it probably means we can expect to see beautiful experiential films in Apple Immersive format exploring topics such as beautiful landscapes, wildlife, travel and music, but are less likely to see deep human storytelling with people in close proximity to the camera (which is typical of nearly all traditional filmmaking).</p>



<p>Luckily for Apple, there is one category where hyperrealism is much less likely to be an issue especially for hardcore fans ‚Äî <strong><em>Live Sports</em></strong>.</p>



<h2><b><em>Live sports</em> will be Apple‚Äôs secret weapon to sell a huge number of Vision Pro headsets to hardcore fans ‚Äî but it‚Äôs going to be a long &amp; expensive journey</b></h2>



<p>One of the original Oculus Go 30-second TV commercials featured an <a href="https://www.youtube.com/watch?v=SUdJt_3i0Us" target="_blank" rel="noreferrer noopener">NBA courtside banter between Adam Levine and Jonah Hill</a> wearing the Oculus headset while watching a live game together in VR (each sitting in their own physical living room):</p>



<figure><img data-attachment-id="121" data-permalink="https://hugo.blog/2024/03/11/vision-pro/nextvr_nba/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png" data-orig-size="3476,1844" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="nextvr_nba" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=1024" loading="lazy" width="1024" height="543" src="https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=2048 2048w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Screenshot from an original <em>Oculus Go</em> TV commercial (<a href="https://www.youtube.com/watch?v=SUdJt_3i0Us" target="_blank" rel="noreferrer noopener">full video</a>)</figcaption></figure>



<p>This TV commercial did extremely well, drove a significant amount of Oculus Go sales (after all, that headset only cost $199) and definitely showed that we were on to something potentially quite powerful with hardcore sports fans. But as I explained in the previous section, we did not manage to bring it to reality in a way that would meet expectations.</p>



<p>In the end, our team at Oculus completely failed to realize the opportunity of redefining the sports audience experience through VR for a number of reasons, but primarily because we just didn‚Äôt have the patience to develop that market. We were unable to build the necessary industry support with sports leagues and broadcast right holders initially, so we stopped trying and the VR sports segment nearly died. There are small efforts on Quest today such as <a href="https://xtadiumvr.com/" target="_blank" rel="noreferrer noopener"><em>Xtadium</em></a> and <a href="https://www.oculus.com/vr/6525150070834263/" target="_blank" rel="noreferrer noopener"><em>Meta Horizons</em></a>, but the quality of the experience and the limited live content make it all too insignificant to matter. To date, nobody ever really tried hard enough to create this market.</p>



<p>Apple has the opportunity to completely change this, for a few reasons:</p>



<ul>
<li><strong>Apple Immersive on Vision Pro is a transformative experience</strong> in terms of video quality and its ability to deliver a real sense of presence. Watching a game in high-resolution VR has the <em>potential</em> to be legitimately better than a regular 4K TV broadcast by enabling hardcore fans to feel much closer to the action.</li>



<li><strong>Apple has VR broadcast expertise</strong> with its acquisition of NextVR, and could have been painstakingly building a robust production pipeline for live 8K video, which is a tall technical challenge requiring non-trivial investment and specialized talent.</li>



<li><strong>Apple is already active in the sports broadcast rights world</strong> through their existing MLS license and several other rumored conversations that may lead to Apple buying a lot more broadcast rights to continue to strengthen Apple TV (ex. English Premier League, Formula 1).</li>
</ul>



<p>The first place where Apple will likely explore using Apple Immersive and Vision Pro for a live broadcast is Major League Soccer in the US. Their recent announcement is a strong indicator this is likely coming in late 2024 or early 2025 (to continue building momentum for Vision Pro):</p>



<blockquote>
<p><em>Coming soon, all Apple Vision Pro users can experience the best of the 2023 MLS Cup Playoffs with <strong>the first-ever sports film captured in Apple Immersive Video</strong>. Viewers will feel every heart-pounding moment in 8K 3D with a 180-degree field of view and Spatial Audio that transports them to each match. </em></p>
<cite><a href="https://www.apple.com/newsroom/2024/02/2024-mls-season-kicks-off-today-exclusively-on-mls-season-pass-on-apple-tv/" target="_blank" rel="noreferrer noopener">Apple Press Release ‚Äì February 2024</a></cite></blockquote>



<figure><img data-attachment-id="110" data-permalink="https://hugo.blog/2024/03/11/vision-pro/nextvr_mls/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp" data-orig-size="1500,750" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="nextvr_mls" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=1024" loading="lazy" width="1024" height="512" src="https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp 1500w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Apple will likely use MLS as a testing ground for developing Apple Immersive live broadcasts</figcaption></figure>



<p>Beyond MLS (where Apple already has a long-term agreement and the ability to basically do anything), it will take a significant amount of time and money for Apple to strike the necessary agreements with the main sports leagues (NBA, NFL, MLB, Premier League etc) to enable this kind of immersive broadcast experience. That said, this is likely only a matter of time, as the opportunity to rethink audience sports is large enough that it would matter a lot even to a multi-trillion dollar company like Apple.</p>



<blockquote>
<p><mark><strong>MY TAKE: </strong>Just to put things in perspective, prices of tickets for watching live sports (in the actual venue) have been going steadily up and are now in the $100s even for average to bad seats, with premium tickets easily going into the $1000s (<a href="https://www.cbsnews.com/news/how-much-super-bowl-2024-tickets-prices/" target="_blank" rel="noreferrer noopener"><strong>the cheapest SuperBowl ticket in 2024 was around $2,000 at face value</strong></a>). The business case for a high-quality immersive ‚Äúcourtside‚Äù experience on Vision Pro is almost unquestionably very strong.</mark></p>
</blockquote>



<p>There are two major aspects Apple will have to nail in order to successfully monetize this opportunity, both of which will require a lot of design, engineering and experimentation:</p>



<ul>
<li><strong>Live sports are very social</strong>, which means Apple will have to invest heavily in delivering a co-watching experience that works equally well for people who are physically in the same room or virtually co-located, and which feels as natural as casually watching a game sitting on the couch with your family or at a bar with your friends.</li>



<li><strong>The experience bar will be very high</strong>, which means Apple will have to really customize every aspect of the experience to the nature of each sports to make it better than watching a game on a large 4K television ‚Äî including camera angles, special replays, birds-eye visualizations, analysis overlays, game stats etc.</li>
</ul>



<p>This is a massive canvas for innovation, and it will take several generations of Vision Pro to get there. I‚Äôm optimistic and, speaking from the position of having been part of a team that really tried to go after this opportunity, I really believe this is one of those things where ‚Äúit takes an Apple‚Äù to change the game (pun very much intended!).</p>



<figure><img data-attachment-id="137" data-permalink="https://hugo.blog/2024/03/11/vision-pro/xtadium_tennis/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png" data-orig-size="1024,591" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="xtadium_tennis" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=1024" loading="lazy" width="1024" height="591" src="https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png 1024w, https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Watching live tennis on Xtadium app on Quest: <br>multiple cameras to choose from <em>plus</em> simultaneous TV broadcast on giant virtual floating screen</figcaption></figure>



<figure><img data-attachment-id="142" data-permalink="https://hugo.blog/2024/03/11/vision-pro/visionpro_golf/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp" data-orig-size="1280,720" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="visionpro_golf" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=1024" loading="lazy" width="1024" height="576" src="https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp 1280w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>PGA app on Vision Pro: <br>birds-eye view of a 3D model of the course and ability to track shots of a recorded prior match</figcaption></figure>



<figure><img data-attachment-id="174" data-permalink="https://hugo.blog/2024/03/11/vision-pro/visionpro_f1/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png" data-orig-size="3448,1910" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="visionpro_f1" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=1024" loading="lazy" width="1024" height="567" src="https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=2048 2048w, https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Concept of a Formula 1 mixed reality broadcast by viz artist John LePore (Source: <a href="https://www.youtube.com/watch?v=y9FpgxNeWJk" target="_blank" rel="noreferrer noopener">YouTube</a>):<br>birds-eye track view, main broadcast on giant floating screen, multi-camera access, live telemetry</figcaption></figure>



<h2>Conclusions<strong> </strong></h2>



<h2><strong>Why I returned my Vision Pro, and my wish list for what Apple could do to fix &amp; improve the product</strong> </h2>



<p>As a ‚Äúproduct guy‚Äù, I usually force myself to behave like a real consumer making real trade-offs as much as I possibly can. I believe that always putting myself in the user‚Äôs shoes is an important part of what I do not just for my own products but also for products built by other people. I admit Vision Pro is the ultimate tech toy, but since I‚Äôm not an active developer I can‚Äôt justify the $4,049.78 price tag (512GB model + California sales tax) simply for keeping up with the VR market, so I returned my Vision Pro for a full refund inside the 14-day return window.</p>



<figure><img data-attachment-id="201" data-permalink="https://hugo.blog/2024/03/11/vision-pro/vision-pro_return/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png" data-orig-size="2018,1046" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="vision-pro_return" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=1024" loading="lazy" width="1024" height="530" src="https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=768 768w, https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png 2018w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>In Apple‚Äôs journey of <em>product-market fit</em> in VR, the Vision Pro has a long way to be able to deliver true retention. Apple‚Äôs high-risk decision to completely exclude immersive VR games from the Vision Pro app store ‚Äî <em>plus</em> their inexplicable failure to create exciting momentum by not having high-quality AR apps at launch ‚Äî don‚Äôt leave them with many options to deliver user value in the near term to non-developers.</p>



<p>The only low-hanging fruit is to make productivity really good, which despite being incredibly unimaginative and dull, should be one of Apple‚Äôs biggest focus in the next iterations of visionOS. I don‚Äôt discard the possibility of once again owning a 1st-gen Vision Pro in the future once Apple addresses all of the friction issues I uncovered and shared above. </p>



<p>During the 2 weeks of my Vision Pro experience, I accumulated a very long list of bug fixes and feature requests based on 2 weeks of Vision Pro usage. I‚Äôm going to share my Top 10 here:</p>



<ol>
<li><strong>Make productivity use cases frictionless </strong>first of all by closing the gap with developers to bring essential iPad apps to Vision Pro // fix text input &amp; editing and make it seamless // add support for 2 (and ideally 3) MacOS remote displays // add workspace window persistency // build ‚Äúspatial Mission Control‚Äù and enforce a minimal recommended focal distance</li>



<li><strong>Have developers build amazing AR games</strong> and do everything possible to set a really high quality bar &amp; reward their creativity // add SharePlay support with <em>Personas</em> and really push for multi-player support, enabling people to be <em>and</em> play together.</li>



<li><strong>Improve passthrough mode</strong> to the extent that the hardware sensor stack allows, ideally reducing motion blur, improving white balance, and making seeing your hands more seamless (when viewing immersive content)</li>



<li><strong>Create workspace spatial persistency</strong> and allow me to configure different rooms in my home or office in such a way that Vision Pro always remembers my room-specific configurations</li>



<li><strong>Make 3D widgets &amp; objects first-class citizens </strong>in visionOS and enable people to decorate their homes &amp; offices persistently</li>



<li><strong>Let people bring their iPhone into VR</strong> by simply looking at the device (like the MacOS virtual display feature) and then getting a floating panel that they can place anywhere in their space ‚Äî this will work wonders in reducing FOMO while in VR</li>



<li><strong>Add a <em>Guest mode</em> so anyone can give the Apple in-store demo</strong> and make it possible for Vision Pro users to ‚Äúspread the love‚Äù ‚Äî there‚Äôs nothing more magical than giving someone their first VR demo</li>



<li><strong>Add Persona support to SharePlay </strong>for watching video to so people can actually feel like they‚Äôre together ‚Äî VR has a bad reputation of loneliness &amp; isolation, so making VR social must be a priority even though few people will use social features at first. There aren‚Äôt enough people with Vision Pro for this to be a practical use case today, but it‚Äôs important for Apple to set the right tone.</li>



<li><strong>Launch tons of beautiful environments</strong> ideally with a steady frequency, taking a page out of the Apple TV screensaver playbook ‚Äî and include beautiful indoor environments as well (not just landscapes)</li>



<li><strong>(Lastly‚Ä¶) Find a way to let people play immersive VR games</strong> by implementing <a href="https://en.wikipedia.org/wiki/OpenXR" target="_blank" rel="noreferrer noopener">OpenXR</a> support, forming a partnership with <em>SteamVR</em> or simply opening up visionOS a bit to allow VR developers and enthusiasts to build compatibility themselves</li>
</ol>
</div>



<h2><strong>ONE MORE THING: (1) Why Meta‚Äôs Android moment is finally here, (2) My unsolicited product advice for <em>Quest Pro 2</em> and beyond</strong></h2>



<p>As I said at the beginning of this essay, while working at Meta/Oculus I used to semi-seriously joke that the best thing that could ever happen to us was having Apple enter the VR industry. One of the main reasons for me to say this was that I knew Apple would do the best job of any company making people really <em>want</em> VR through its unparalleled brand, design and marketing. Oculus co-founder Palmer Luckey puts it best:</p>







<blockquote>
<p><em>‚ÄúVR will become something everyone wants before it becomes something everyone can afford.‚Äù</em></p>
<cite><a href="https://twitter.com/palmerluckey/status/679907802962214912" target="_blank" rel="noreferrer noopener">Palmer Luckey, 2015 tweet</a></cite></blockquote>



<p>For Meta, the Vision Pro launch is the best marketing tool for Quest VR that the company could have dreamed of but could have never achieved on its own, for a few reasons:</p>



<ul>
<li><strong>It elevates VR to a level of mainstream consumer curiosity</strong> and breaks away from gamer and VR enthusiast niches; in media coverage alone the Vision Pro probably had 1,000x more reach than any Oculus/Quest launch in history</li>



<li><strong>It sets a new experience gold standard for VR</strong> especially by pushing the existing boundaries in display resolution and creating a new paradigm of ‚ÄúUI magic‚Äù with gaze &amp; pinch which may be an instant defacto standard</li>



<li><strong>It establishes a pricing envelope</strong> that enables Meta to break away from the $500 price point that Quest has been stuck in, and specifically allows them to ship a Quest Pro 2 headset priced at a $1,000 to $1,500 (but likely not higher) without being completely rejected by consumers</li>



<li><strong>It creates a formidable competitor</strong> for Meta teams to maniacally chase and will almost certainly force the company to move with a much greater sense of urgency internally (which would be a great outcome as friends on the inside are constantly complaining Meta Reality Labs moves too slowly)</li>
</ul>



<p><strong><em>What should Meta do in response to the Vision Pro launch?</em></strong></p>



<p>In order to really seize this moment and opportunity created by the Vision Pro launch, Meta needs to ensure it ships a VR headset by mid-2025 that both builds on the new experience gold standard created by the Vision Pro <em>and </em>is objectively a better product across as many dimensions as possible. It is imperative for Meta not to repeat the inexplicable debacle that was the <em>Quest Pro</em> launch in 2022.</p>



<p>I put together my own <strong>Top 10</strong> <strong>wish list for <em>Quest Pro 2</em></strong>:</p>



<ol>
<li><strong>Double down investment in micro-OLED</strong> as it‚Äôs likely the only way to achieve display resolution at or near Vision Pro; I suspect this may be exactly what the <a href="https://www.roadtovr.com/meta-lg-xr-partnership/" target="_blank" rel="noreferrer noopener">recently announced LG partnership</a> is about</li>



<li><strong>Build an ergonomic headset</strong> <strong>that can be worn for 2-4 hours</strong> without causing any major discomfort issues; ideally offering two battery options: (1) a head-strap with a built-in battery in the back of the head, and (2) a wired pack (like the Vision Pro) that moves the battery off the head and reduces the headset weight to below 500 grams while increasing energy capacity. </li>



<li><strong>Deliver better passthrough than Vision Pro</strong> by dramatically improving Quest 3‚Äôs latency and distortion correction and improving upon all of the Vision Pro passthrough issues ‚Äî ensure no perceivable motion blur, high dynamic range, accurate white balance</li>



<li><strong>Take Apple‚Äôs gaze+pinch UI to the next level</strong> by productizing all of the amazing research on hand tracking done at Meta (ex: Rob Wang‚Äôs super talented group) to enable fine-grained gestures such as scrolling and D-pad selection by detecting small finger movements solely via camera input (this is <em>not</em> the CTRL Labs stack‚Ä¶ that‚Äôs for the future)</li>



<li><strong>Partner with Microsoft to make Windows computers 1st class citizens</strong> in Quest Pro 2 and enable advanced desktop productivity use cases that go well beyond virtual monitors (ex: make it possible to take any window and place it in space)</li>



<li><strong>Launch Android 2D tablet apps natively on Quest</strong> to match the Vision Pro iPad compatibility library either by partnering with Google to license <em>Play Store</em> (which <a href="https://www.theinformation.com/articles/meta-rebuffed-google-proposal-for-a-vr-and-ar-tie-up">seems unlikely these days</a> though I still believe Ash Jhaveri and Hiroshi could work together to pull it off) or just build a curated tablet app store directly (which we had considered in the past at Oculus but passed on)</li>



<li><strong>Launch human-like avatars with Quest Pro 2</strong> by productizing Meta‚Äôs mind-blowing <a href="https://www.uploadvr.com/meta-codec-avatars-might-be-coming-to-quest/" target="_blank" rel="noreferrer noopener">Codec Avatars technology</a>, likely one of the VR research areas that has received the most R&amp;D dollars for the last 7+ years, used by <a href="https://youtu.be/MVYrJJNdrEg?si=EIZJ_Op4xRY9PD-h&amp;t=605" target="_blank" rel="noreferrer noopener">Lex Friedman in his interview with Mark Zuckerberg in late 2023</a></li>



<li><strong>Launch high-definition room scanning and unlock teleportation</strong> using technology that has existed within Oculus Research for several years now; it is time for Meta to make this future a reality where people can be remote but <em>feel</em> truly present by visiting each other‚Äôs home, office or favorite place</li>
</ol>



<h2>Appendix: Other Fun Things</h2>



<h2><strong>Despite all of its hardware insanity the Vision Pro display is still a far cry from a VR retina display (and may never get there)</strong></h2>



<p>As noted above the Vision Pro display delivers insane pixel density at over 3,000 PPI (compared to 500 PPI for the highest resolution smartphones), but because the panel is so close to our eyes, it still doesn‚Äôt come even close to the resolution it would need to have in order to qualify as a <em><a href="https://en.wikipedia.org/wiki/Retina_display">retina display</a></em>.</p>



<p>A retina display, by <a href="https://www.kybervision.com/Blog/files/AppleRetinaDisplay.html" target="_blank" rel="noreferrer noopener">Apple‚Äôs definition</a>, is a display with a high enough resolution that the human eye cannot resolve individual pixels. Because different devices are used at varying distances from the eye, there is no single PPI (pixels per inch) standard for retina across all device categories. Instead, it‚Äôs useful to look at PPD (pixels per degree), which is a measure of angular resolution independent of viewing distance, or specifically the number of horizontal pixels per degree of <em>viewing angle</em>. See <a href="https://simulavr.com/blog/ppd-optics/" target="_blank" rel="noreferrer noopener">this article from SimulaVR</a> for an excellent explanation, and here‚Äôs an image from that article:</p>







<figure><img data-attachment-id="74" data-permalink="https://hugo.blog/2024/03/11/vision-pro/ppd/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/ppd.png" data-orig-size="460,286" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ppd" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/ppd.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/ppd.png?w=460" loading="lazy" width="460" height="286" src="https://hugobarracom.files.wordpress.com/2024/02/ppd.png?w=460" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/ppd.png 460w, https://hugobarracom.files.wordpress.com/2024/02/ppd.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/ppd.png?w=300 300w" sizes="(max-width: 460px) 100vw, 460px"><figcaption>Source: <a href="https://simulavr.com/blog/ppd-optics/" target="_blank" rel="noreferrer noopener">SimularVR</a></figcaption></figure>



<p>A human eye with 20/20 vision has a resolution of 60 PPD. This means very specifically that we can resolve 60 pixels per each 1 degree of viewing angle (or 1 pixel per arc minute, which is 1/60th of a degree). The Vision Pro has an angular resolution of 34, which is 1/3 more than the Meta Quest 3 but still very far away from the 60 PPD we‚Äôd need for a retina-quality display.</p>



<p>Achieving angular resolution anywhere near 60 PPD in a VR headset is likely not possible with any technology in the mid-term horizon.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TextSnatcher: Copy text from images, for the Linux Desktop (337 pts)]]></title>
            <link>https://github.com/RajSolai/TextSnatcher</link>
            <guid>39711621</guid>
            <pubDate>Fri, 15 Mar 2024 02:57:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/RajSolai/TextSnatcher">https://github.com/RajSolai/TextSnatcher</a>, See on <a href="https://news.ycombinator.com/item?id=39711621">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a href="https://wiki.gnome.org/Projects/Vala" rel="nofollow"><img src="https://camo.githubusercontent.com/b7b96bad0bedef8e14b37aa18f8559eb09fae075833a1a640ae2d3af86a6d49f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d616465253230576974682d56616c612532302d413536444532" alt="Vala Programming language" data-canonical-src="https://img.shields.io/badge/Made%20With-Vala%20-A56DE2"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/RajSolai/TextSnatcher/actions/workflows/flatpak-build.yml/badge.svg"><img src="https://github.com/RajSolai/TextSnatcher/actions/workflows/flatpak-build.yml/badge.svg" alt="Flatpak Build workflow"></a></p>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/RajSolai/TextSnatcher/blob/master/data/icons/com.github.rajsolai.textsnatcher.svg"><img src="https://github.com/RajSolai/TextSnatcher/raw/master/data/icons/com.github.rajsolai.textsnatcher.svg" height="110px"></a></p><p dir="auto"><h2 tabindex="-1" dir="auto">TextSnatcher</h2><a id="user-content-textsnatcher" aria-label="Permalink: TextSnatcher" href="#textsnatcher"></a></p>
<p dir="auto">Copy Text from Images with ease, Perform OCR operations in seconds.</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-default.png"><img alt="TextSnatcher OCR App for Linux" src="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-default.png"></a><br>
<a href="https://www.producthunt.com/posts/textsnatcher?utm_source=badge-featured&amp;utm_medium=badge&amp;utm_souce=badge-textsnatcher" rel="nofollow"><img src="https://camo.githubusercontent.com/461f8889a0b1219ad9051d64231614176a85e2eb03b4b06a8770180fd8831e2b/68747470733a2f2f6170692e70726f6475637468756e742e636f6d2f776964676574732f656d6265642d696d6167652f76312f66656174757265642e7376673f706f73745f69643d333434343031267468656d653d6c69676874" alt="TextSnatcher - How to copy text from images, answer is TextSnatcher | Product Hunt" width="250" height="54" data-canonical-src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=344401&amp;theme=light"></a>
</p></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Download</h2><a id="user-content-download" aria-label="Permalink: Download" href="#download"></a></p>
<div dir="auto">
  <p><a href="https://flathub.org/apps/details/com.github.rajsolai.textsnatcher" rel="nofollow"><img width="240" alt="Download on Flathub" src="https://camo.githubusercontent.com/1a343de10fc46d1a887a0640899212590da30f4d415ce2caf85e4462bd95a3b8/68747470733a2f2f666c61746875622e6f72672f6173736574732f6261646765732f666c61746875622d62616467652d692d656e2e706e67" data-canonical-src="https://flathub.org/assets/badges/flathub-badge-i-en.png"></a></p><p dir="auto"><a href="https://appcenter.elementary.io/com.github.rajsolai.textsnatcher" rel="nofollow"><img src="https://camo.githubusercontent.com/02fb9c2fbb1ea0024a489c44798c28f84ba7149a8d144a20a8a5a28debd1e293/68747470733a2f2f61707063656e7465722e656c656d656e746172792e696f2f62616467652e737667" alt="Get it on AppCenter" data-canonical-src="https://appcenter.elementary.io/badge.svg"></a></p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Multiple Language Support.</li>
<li>Copy Text from images with a Drag.</li>
<li>Drag over any Image and Paste.</li>
<li>Fast and Easy to Use.</li>
<li>This application uses the Tesseract OCR 4.x for the character
recognition.</li>
<li>Read more about <a href="https://tesseract-ocr.github.io/tessdoc/Home.html" rel="nofollow">Tesseract</a> and Star ‚≠êÔ∏è <a href="https://github.com/tesseract-ocr/tesseract">Tesseract-Project</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screencasts</h2><a id="user-content-screencasts" aria-label="Permalink: Screencasts" href="#screencasts"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description textsnatcher-eng.mp4">textsnatcher-eng.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/54436424/152921719-228485ba-0d37-4b01-864e-63a2792248b5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA0ODYzMDUsIm5iZiI6MTcxMDQ4NjAwNSwicGF0aCI6Ii81NDQzNjQyNC8xNTI5MjE3MTktMjI4NDg1YmEtMGQzNy00YjAxLTg2NGUtNjNhMjc5MjI0OGI1Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDA3MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ4MmZjZDQ2ZmVlODU3MDY5NTg2ZDI2YTkzOGNjZjY1OGFmZmE4MTA3ZTAzNDAwNTg3N2Y4ZTY3N2MyZGQ3YzcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.j0LbbGRiW-lVKdyCCb2HQo0TYTj_CfpHqm5TSWM-tqo" data-canonical-src="https://private-user-images.githubusercontent.com/54436424/152921719-228485ba-0d37-4b01-864e-63a2792248b5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA0ODYzMDUsIm5iZiI6MTcxMDQ4NjAwNSwicGF0aCI6Ii81NDQzNjQyNC8xNTI5MjE3MTktMjI4NDg1YmEtMGQzNy00YjAxLTg2NGUtNjNhMjc5MjI0OGI1Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDA3MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ4MmZjZDQ2ZmVlODU3MDY5NTg2ZDI2YTkzOGNjZjY1OGFmZmE4MTA3ZTAzNDAwNTg3N2Y4ZTY3N2MyZGQ3YzcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.j0LbbGRiW-lVKdyCCb2HQo0TYTj_CfpHqm5TSWM-tqo" controls="controls" muted="muted">

  </video>
</details>

<details open="">
  <summary>
    
    <span aria-label="Video description textsnatcher-tamil.mp4">textsnatcher-tamil.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/54436424/152921736-c9567c9d-0afa-4c09-8706-6b2a1b6b635a.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA0ODYzMDUsIm5iZiI6MTcxMDQ4NjAwNSwicGF0aCI6Ii81NDQzNjQyNC8xNTI5MjE3MzYtYzk1NjdjOWQtMGFmYS00YzA5LTg3MDYtNmIyYTFiNmI2MzVhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDA3MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA4ZTFhZTUyN2RkOGU4YTI4OWMzMTUxZDc2MjkxYzFjZjU4NTQ5NmNlMzU2ODU3YjIyMDc0MGNjNzA4OGE4NjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.eeFrGifG7N1bcXi_wZRrMhFlRHV9v-gTz0_INWoCg1A" data-canonical-src="https://private-user-images.githubusercontent.com/54436424/152921736-c9567c9d-0afa-4c09-8706-6b2a1b6b635a.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA0ODYzMDUsIm5iZiI6MTcxMDQ4NjAwNSwicGF0aCI6Ii81NDQzNjQyNC8xNTI5MjE3MzYtYzk1NjdjOWQtMGFmYS00YzA5LTg3MDYtNmIyYTFiNmI2MzVhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDA3MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA4ZTFhZTUyN2RkOGU4YTI4OWMzMTUxZDc2MjkxYzFjZjU4NTQ5NmNlMzU2ODU3YjIyMDc0MGNjNzA4OGE4NjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.eeFrGifG7N1bcXi_wZRrMhFlRHV9v-gTz0_INWoCg1A" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Screenshots</h2><a id="user-content-screenshots" aria-label="Permalink: Screenshots" href="#screenshots"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-default.png"><img src="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-default.png" alt="TextSnatcher OCR App for Linux"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-dark.png"><img src="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-dark.png" alt="TextSnatcher OCR App for Linux"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support Me</h2><a id="user-content-support-me" aria-label="Permalink: Support Me" href="#support-me"></a></p>
<p dir="auto"><a href="https://www.buymeacoffee.com/rajsolai" rel="nofollow"><img src="https://camo.githubusercontent.com/cace41b0afc90c68d0207e2bd809ee121f9ff4f72ac032e8ced972aee7adbb23/68747470733a2f2f63646e2e6275796d6561636f666665652e636f6d2f627574746f6e732f76322f64656661756c742d79656c6c6f772e706e67" alt="Buy Me A Coffee" data-canonical-src="https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png"></a></p>
<p dir="auto"><a href="https://ko-fi.com/R6R7ABG0F" rel="nofollow"><img src="https://camo.githubusercontent.com/ce32b4940b9ebf361cfd346ba0582815846406854cd2f701c11a85cb21eaa939/68747470733a2f2f6b6f2d66692e636f6d2f696d672f676974687562627574746f6e5f736d2e737667" alt="ko-fi" data-canonical-src="https://ko-fi.com/img/githubbutton_sm.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Social Media Posts</h2><a id="user-content-social-media-posts" aria-label="Permalink: Social Media Posts" href="#social-media-posts"></a></p>
<p dir="auto"><a href="https://www.linkedin.com/posts/solai085_linux-commentbelow-apple-activity-6826408004519374848-wxsw" rel="nofollow">LinkedIn Post on Why I created TextSnatcher</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dependencies</h2><a id="user-content-dependencies" aria-label="Permalink: Dependencies" href="#dependencies"></a></p>
<p dir="auto">Ensure you have these dependencies installed</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Runtime Dependency</h3><a id="user-content-runtime-dependency" aria-label="Permalink: Runtime Dependency" href="#runtime-dependency"></a></p>
<ul dir="auto">
<li>scrot</li>
<li>tesseract-ocr</li>
<li>tesseract language data
<a href="https://archlinux.org/packages/community/x86_64/tesseract" rel="nofollow">arch repos</a>
<a href="https://packages.debian.org/search?keywords=tesseract-ocr" rel="nofollow">debian repos</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Buildtime Dependency</h3><a id="user-content-buildtime-dependency" aria-label="Permalink: Buildtime Dependency" href="#buildtime-dependency"></a></p>
<ul dir="auto">
<li>granite</li>
<li>gtk+-3.0</li>
<li>gobject-2.0</li>
<li>gdk-pixbuf-2.0</li>
<li>libhandy-1</li>
<li>libportal-0.5</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install, build and run</h2><a id="user-content-install-build-and-run" aria-label="Permalink: Install, build and run" href="#install-build-and-run"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# clone repository
git clone https://github.com/RajSolai/TextSnatcher.git TextSnatcher
# cd to dir
cd TextSnatcher
# run meson
meson build --prefix=/usr
# cd to build, build and test
cd build
sudo ninja install &amp;&amp; com.github.rajsolai.textsnatcher"><pre><span><span>#</span> clone repository</span>
git clone https://github.com/RajSolai/TextSnatcher.git TextSnatcher
<span><span>#</span> cd to dir</span>
<span>cd</span> TextSnatcher
<span><span>#</span> run meson</span>
meson build --prefix=/usr
<span><span>#</span> cd to build, build and test</span>
<span>cd</span> build
sudo ninja install <span>&amp;&amp;</span> com.github.rajsolai.textsnatcher</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inspirations</h2><a id="user-content-inspirations" aria-label="Permalink: Inspirations" href="#inspirations"></a></p>
<ul dir="auto">
<li>ReadMe: <a href="https://github.com/alainm23/planner">https://github.com/alainm23/planner</a></li>
<li>Application Structure: <a href="https://github.com/alcadica/develop">https://github.com/alcadica/develop</a></li>
<li>TextSniper (MacOS Application)</li>
</ul>
<p dir="auto">Made with ‚ù§Ô∏è for Linux</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>