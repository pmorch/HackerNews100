<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 10 Nov 2024 06:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[LLMs have reached a point of diminishing returns (111 pts)]]></title>
            <link>https://garymarcus.substack.com/p/confirmed-llms-have-indeed-reached</link>
            <guid>42097774</guid>
            <pubDate>Sun, 10 Nov 2024 00:25:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://garymarcus.substack.com/p/confirmed-llms-have-indeed-reached">https://garymarcus.substack.com/p/confirmed-llms-have-indeed-reached</a>, See on <a href="https://news.ycombinator.com/item?id=42097774">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg" width="1170" height="1227" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1227,&quot;width&quot;:1170,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:243099,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>For years I have been warning that “scaling” — eeking out improvements in AI by adding more data and more compute, without making fundamental architectural changes — would not continue forever. In my most notorious article, in March of 2022, I argued that “</span><a href="https://nautil.us/deep-learning-is-hitting-a-wall-238440/" rel="">deep learning is hitting a wall</a><span>”. Central to the argument was that pure scaling would not solve hallucinations or abstraction; I concluded that “there are serious holes in the scaling argument.” </span></p><p>And I got endless grief for it. Sam Altman implied (without saying my name, but riffing on the images in my then-recent article) I was a “mediocre deep learning skeptic”; Greg Brockman openly mocked the title. Yann LeCun wrote that deep learning wasn’t hitting a wall, and so on. Elon Musk himself made fun of me and the title earlier this year.</p><p><span>The thing is, </span><strong>in the long term, science isn’t majority rule</strong><span>. In the end, the truth generally outs. Alchemy had a good run, but it got replaced by chemistry. The truth is that scaling is running out, and that truth is, at last coming out.</span></p><p>A few days ago, the well-known venture capitalist Marc Andreesen started to spill the beans, saying on a podcast “we're increasing [graphics processing units] at the same rate, we're not getting the intelligent improvements at all out of it” –  which is basically VC-ese for “deep learning is hitting a wall.”</p><p><span>Just a few moments ago, Amir Efrati, editor of the industry trade journal </span><em>The Information</em><span> further confirmed that we have reached a period of diminishing returns, writing on X that “OpenAI's [upcoming] Orion model shows how GPT improvements are slowing down”.  </span></p><p><span>Just as I argued here in April 2024, LLMs </span><a href="https://open.substack.com/pub/garymarcus/p/evidence-that-llms-are-reaching-a?r=8tdk6&amp;utm_campaign=post&amp;utm_medium=web" rel="">have reached a point of diminishing returns</a><span>. </span></p><p>§</p><p>The economics are likely to be grim. Sky high valuation of companies like OpenAI and Microsoft are largely based on the notion that LLMs will, with continued scaling, become artificial general intelligence. As I have always warned, that’s just a fantasy. There is no principled solution to hallucinations in systems that traffic only in the statistics of language without explicit representation of facts and explicit tools to reason over those facts.</p><p>LLMs will not disappear, even if improvements diminish, but the economics will likely never make sense: additional training is expensive, the more scaling, the more costly. And, as I have been warning, everyone is landing in more or less the same place, which leaves nobody with a moat. LLMs such as they are, will become a commodity; price wars will keep revenue low. Given the cost of chips, profits will be elusive. When everyone realizes this, the financial bubble may burst quickly; even NVidia might take a hit, when people realize the extent to which its valuation was based on a false premise.</p><p>§</p><p>The sociology here has been perverse too, for a really long time. Many people (especially LeCun, but also a legion of tech influencers who followed his lead) have tried to deplatform me. </p><p>The media has done little to counter the mob psychology; they have mostly listened to people with money, with vested interests, not to scientists. Many of us, including Melanie Mitchell, Subbarao Kambahapati, Emily Bender, Ernie Davis, etc. have been emphasizing for ages that there are principled limits with LLMs. Media (with notable exceptions like Ezra Klein, who gave me a clear platform for skepticism in January 2023) has rarely listened, instead often glorifying the hype of people like Altman and Musk.</p><p>Worse, the US AI policy now, and likely in the next administration, has largely been driven by hype, and the assumption that returns for LLM scaling would not diminish.  And yet here we are at the end of 2024, and even Altman and Andreesen are perhaps starting to see it.</p><p>Meanwhile, precious little investment has been made in other approaches. If LLMs won’t get the US to trustworthy AI, and our adversaries invest in alternative approaches, we could easily be outfoxed. The US has been putting all its AI eggs in the LLM basket, and that may well prove to be an epic, massive mistake.</p><p>§</p><p>In April, when I first saw direct empirical evidence that this moment had come, I wrote (and stand by):</p><blockquote><p>If enthusiasm for GenAI dwindles and market valuations plummet, AI won’t disappear, and LLMs won’t disappear; they will still have their place as tools for statistical approximation.</p><p>But that place may be smaller; it is entirely possible that LLMs on their own will never live up to last year’s wild expectations.</p><p>Reliable, trustworthy AI is surely achievable, but we may need to go back to the drawing board to get there.</p></blockquote><p>I’m glad that the market is finally recognizing that what I’ve been saying is true. Hopefully now we can make real progress.</p><p><em><strong>Gary Marcus</strong><span> has been warning about the foundational limits to traditional neural network approaches since his 2001 book The Algebraic Mind (where he first described hallucinations), and amplified those warnings in Rebooting AI and his most recent book Taming Silicon Valley.</span></em><span> </span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grim Fandango (170 pts)]]></title>
            <link>https://www.filfre.net/2024/11/grim-fandango/</link>
            <guid>42097261</guid>
            <pubDate>Sat, 09 Nov 2024 22:17:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.filfre.net/2024/11/grim-fandango/">https://www.filfre.net/2024/11/grim-fandango/</a>, See on <a href="https://news.ycombinator.com/item?id=42097261">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
														<p><a href="https://www.filfre.net/2024/11/grim-fandango/10591938-grim-fandango-windows-front-cover/" rel="attachment wp-att-6154"><img fetchpriority="high" decoding="async" src="https://www.filfre.net/wp-content/uploads/2024/10/10591938-grim-fandango-windows-front-cover-251x300.jpg" alt="" width="377" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/10591938-grim-fandango-windows-front-cover-251x300.jpg 251w, https://www.filfre.net/wp-content/uploads/2024/10/10591938-grim-fandango-windows-front-cover.jpg 670w" sizes="(max-width: 377px) 100vw, 377px"></a></p>
<blockquote><p>My one big regret was <a href="https://www.filfre.net/2024/01/televising-the-revolution">the PlayStation version [of Broken Sword]</a>. No one thought it would sell, so we kept it like the PC version. In hindsight, I think if we had introduced direct control in this game, it would have been enormous.</p>
<p>— Charles Cecil of Revolution Software, speaking from the Department of Be Careful What You Wish For</p>
</blockquote>
<hr>

<p>One day in June of 1995, Tim Schafer came to work at LucasArts and realized that, for the first time in a long time, he didn’t have anything pressing to do. <a href="https://www.filfre.net/2021/07/full-throttle"><em>Full Throttle</em></a>, his biker movie of an adventure game, had been released several weeks before. Now, all of the initial crush of interviews and marketing logistics was behind him. A mountain had been climbed. So, as game designers do, he started to think about what his next Everest should be.</p>
<p>Schafer has told in some detail how he came up with the core ideas behind <em>Grim Fandango</em> over the course of that summer of 1995.</p>
<blockquote><p>The truth is, I had part of the Fandango idea before I did Full Throttle. I wanted to do a game that would feature those little papier-mâché folk-art skeletons from Mexico. I was looking at their simple shapes and how the bones were just painted on the outside, and I thought, “Texture maps! 3D! The bones will be on the outside! It’ll look cool!”</p>
<p>But then I was stuck. I had these skeletons walking around the Land of the Dead. So what? What did they do? Where were they going? What did they want? Who’s the main character? Who’s the villain? The mythology said that the dead walk the dark plane of the underworld known as Mictlān for four years, after which their souls arrive at the ninth plane, the land of eternal rest. Sounds pretty “questy” to me. There you have it: a game.</p>
<p>“Not cool enough,” said Peter Tscale, my lead artist. “A guy walking in a supernatural world? What’s he doing? Supernatural things? It just sounds boring to me.”</p>
<p>So, I revamped the story. Adventure games are all fantasies really, so I had to ask myself, “Who would people want to be in a game? What would people want to do?” And in the Land of the Dead, who would people rather be than Death himself? Being the Grim Reaper is just as cool as being a biker, I decided. And what does the Grim Reaper do? He picks up people who have died and carts them over from the other world. Just like a driver of a taxi or limo.</p>
<p>Okay, so that’s Manny Calavera, our main character. But who’s the bad guy? What’s the plot? I had just seen <a href="https://www.imdb.com/title/tt0071315">Chinatown</a>, and I really liked the whole water-supply/real-estate scam that Noah Cross had going there, so of course I tried to rip that off and have Manny be a real-estate salesman who got caught up in a real-estate scandal. Then he was just like the guys in <a href="https://www.imdb.com/title/tt0104348/">Glengarry Glen Ross</a>, always looking for the good leads. But why would Hector Lemans, my villain, want real estate? Why would anyone? They’re dead! They’re only souls. What do souls in the Land of the Dead want?</p>
<p>They want to get out! They want safe passage out, just like in <a href="https://www.imdb.com/title/tt0034583/">Casablanca</a>! The Land of the Dead is a transitory place, and everybody’s waiting around for their travel papers. So Manny is a travel agent, selling tickets on the big train out of town, and Hector’s stealing the tickets…</p></blockquote>
<div id="attachment_6156"><p><a href="https://www.filfre.net/2024/11/grim-fandango/glottis/" rel="attachment wp-att-6156"><img decoding="async" aria-describedby="caption-attachment-6156" src="https://www.filfre.net/wp-content/uploads/2024/10/glottis-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/glottis-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/glottis.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6156">The missing link between <em>Full Throttle</em> and <em>Grim Fandango</em> is Manny’s chauffeur and mechanic Glottis, a literal speed demon.</p></div>
<p>This, then, became the elevator pitch for <em>Grim Fandango</em>. Begin with the rich folklore surrounding Mexico’s Day of the Dead, a holiday celebrated each year just after Halloween, which combines European Christian myths about death and the afterlife with the older, indigenous ones that still haunt the Aztec ruins of Teopanzolco. Then combine it with classic film noir to wind up with Raymond Chandler in a Latino afterlife. It was nothing if not a strikingly original idea for an adventure game. But there was also one more, almost equally original part of it: to do it in 3D.</p>
<p>To hear Tim Schafer tell the story, the move away from LucasArts’s traditional pixel art and into the realm of points, polygons, and textures was motivated by his desire to deliver a more cinematic experience. By no means does this claim lack credibility; as you can gather by reading what he wrote above, Schafer was and is a passionate film buff, who tends to resort to talking in movie titles when other forms of communication fail him. The environments in previous LucasArts adventure games — even the self-consciously cinematic <em>Full Throttle</em> — could only be shown from the angle the pixel artists had chosen to drawn them from. In this sense, they were like a theatrical play, or a <em>really</em> old movie, from the time before Orson Welles emancipated his camera and let it begin to roam freely through his sets in <a href="https://www.imdb.com/title/tt0033467/"><em>Citizen Kane</em></a>. By using 3D, Schafer could become the Orson Welles of adventure games; he would be able to deliver dramatic angles and closeups as the player’s avatar moved about, would be able to put the player <em>in</em> his world rather than forever forcing her to look down on it from on-high. This is the story he still tells today, and there’s no reason to believe it isn’t true enough, as far as it goes.</p>
<p>Nevertheless, it’s only half of the full story. The other half is a messier, less idealistic tale of process and practical economics.</p>
<p>Reckoned in their cost of production per hour of play time delivered, adventure games stood apart from any other genre in their industry, and not in a good way. Building games entirely out of bespoke, single-use puzzles and assets was <em>expensive</em> in contrast to the more process-intensive genres. As time went on and gamers demanded ever bigger, prettier adventures, in higher resolutions with more colors, this became more and more of a problem. Already in 1995, when adventure games were still selling very well, the production costs that were seemingly inherent to the genre were a cause for concern. And the following year, when the genre <a href="https://www.filfre.net/2022/06/toonstruck-or-a-case-study-in-the-death-of-adventure-games">failed to produce</a> a single million-plus-selling breakout hit for the first time in half a decade, they began to look like an existential threat. At that point, LucasArts’s decision to address the issue proactively in <em>Grim Fandango </em>by switching from pixel art to 3D suddenly seemed a very wise move indeed. For a handful of Silicon Graphics workstations running 3D-modelling software could churn out images far more quickly than an army of pixel artists, at a fraction of the cost per image. If the graphics that resulted lacked some of the quirky, hand-drawn, cartoon-like personality that had marked LucasArts’s earlier adventure games, they made up for that by virtue of their flexibility: a scene could be shown from a different angle just by changing a few parameters instead of having to redraw it from scratch. This really did raise the prospect of making the more immersive games that Tim Schafer desired. But from a bean counter’s point of view, the best thing about it was the cost savings.</p>
<p>And there was one more advantage as well, one that began to seem ever more important as time went on and the market for adventure games running on personal computers continued to soften. Immersive 3D was more or less the default setting of the Sony PlayStation, which had <a href="https://www.filfre.net/2023/12/putting-the-j-in-the-rpg-part-2-playstation-for-the-win">come roaring out of Japan</a> in 1995 to seize the title of the most successful games console of the twentieth century just before the curtain fell on that epoch. In addition to its 3D hardware, the PlayStation sported a CD drive, memory cards for saving state, and a slightly older typical user than the likes of Nintendo and Sega. And yet, although a number of publishers ported their 2D computer-born adventure games to the PlayStation, they never felt entirely at home there, having been designed for a mouse rather than a game controller.<span><a role="button" tabindex="0" onclick="footnote_moveToReference_6147_1('footnote_plugin_reference_6147_1_1');" onkeypress="footnote_moveToReference_6147_1('footnote_plugin_reference_6147_1_1');"><sup id="footnote_plugin_tooltip_6147_1_1">[1]</sup></a><span id="footnote_plugin_tooltip_text_6147_1_1">A mouse was available as an accessory for the PlayStation, but it was never very popular.</span></span> A 3D adventure game with a controller-friendly interface might be a very different proposition. If it played its cards right, it would open the door to an installed base of customers five to ten times the size of the extant market for games on personal computers.</p>
<div id="attachment_6149"><p><a href="https://www.filfre.net/2024/11/grim-fandango/manny/" rel="attachment wp-att-6149"><img decoding="async" aria-describedby="caption-attachment-6149" src="https://www.filfre.net/wp-content/uploads/2024/10/manny-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/manny-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/manny.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6149">Working with 3D graphics in the late 1990s required some clever sleight of hand if they weren’t to end up looking terrible. <em>Grim Fandango’</em>s masterstroke was to make all of its characters — like the protagonist Manny Calavera, whom you see above — mere skeletons, whose faces are literally painted onto their skulls. (The characters are shown to speak by manipulating the <a href="https://www.filfre.net/2019/01/life-off-the-grid-part-1-making-ultima-underworld">texture maps</a> that represent their faces, not by manipulating the underlying 3D models themselves.) This approach gave the game a look reminiscent of another of its cinematic inspirations, Tim Burton’s <a href="https://www.imdb.com/title/tt0107688/"><em>The Nightmare Before Christmas</em></a>, whilst conveniently avoiding all of the complications of trying to render pliant flesh. A win-win, as they say. Or, as Tim Schafer said: “Instead of fighting the tech limitations of 3D, you have to embrace them and turn them into a style.”</p></div>
<p>But I’m afraid I’ve gotten slightly ahead of myself. This constellation of ideas, affordances, problems, and solutions was still in a nascent form in November of 1995, when LucasArts hired a young programmer fresh out of university by the name of Bret Mogilefsky. Mogilefsky was a known quantity already, having worked at LucasArts as a tester on and off while he was earning his high-school and university diplomas. Now, he was entrusted with the far more high-profile task of making SCUMM, LucasArts’s venerable adventure engine, safe for 3D.</p>
<p>After struggling for a few months, he concluded that this latest paradigm shift was just too extreme for an engine that had been <a href="https://www.filfre.net/2015/07/a-new-force-in-games-part-3-scumm">created on a Commodore 64</a> circa 1986 and ported and patched from there. He would have to tear SCUMM down so far in order to add 3D functionality that it would be easier and cleaner simply to make a new engine from scratch. He told his superiors this, and they gave him permission to do so — albeit suspecting all the while, Mogilefsky is convinced, that he would eventually realize that game engines are easier envisioned than implemented and come crawling back to SCUMM. By no means was he the first bright spark at LucasArts who thought he could reinvent the adventuring wheel.</p>
<p>But he did prove the first one to call his bosses’ bluff. The engine that he called GrimE (“<em>Grim</em> Engine,” but pronounced like the synonym for “dirt”) used a mixture of pre-rendered and real-time-rendered 3D. The sets in which Manny and his friends and enemies played out their dramas would be the former; the aforementioned actors themselves would be the latter. GrimE was a piebald beast in another sense as well: that of cheerfully appropriating whatever useful code Mogilefsky happened to find lying around the house at LucasArts, most notably from the first-person shooter <a href="https://www.filfre.net/2024/04/jedi-knight-plus-notes-on-an-expanded-universe"><em>Jedi Knight</em></a>.</p>
<p>Like SCUMM before it, GrimE provided relatively non-technical designers like Tim Schafer with a high-level scripting language that they could use themselves to code all of the mechanics of plot and puzzles. Mogilefsky adapted for this task Lua, a new, still fairly obscure programming language out of Brazil. It was an inspired choice. Elegant, learnable, and yet infinitely and easily extendible, Lua has gone on to become a staple language of modern game development, to be found today in such places as the wildly popular Roblox platform.</p>
<p>The most frustrating aspects of GrimE from a development perspective all clustered around the spots where its two approaches to 3D graphics rubbed against one another, producing a good deal of friction in the process. If, for example, Manny was to drink a glass of whiskey, the pre-rendered version of the glass that was part of the background set had to be artfully swapped with its real-time-rendered incarnation as soon as Manny began to interact with it. Getting such actions to look seamless absorbed vastly more time and energy than anyone had expected it to.</p>
<p>In fact, if the bean counters had been asked to pass judgment, they would have had a hard time labeling GrimE a success at all under their metrics. <em>Grim Fandango</em> was in active development for almost three full years, and may have ended up costing as much as $3 million. This was at least two and a half times as much as <em>Full Throttle</em> had cost, and placed it in the same ballpark as <a href="https://www.filfre.net/2024/04/the-curse-of-monkey-island"><em>The Curse of Monkey Island</em></a>, LucasArts’s last and most audiovisually lavish SCUMM adventure, which was released a year before <em>Grim Fandango</em>. Further, despite employing a distinctly console-like control scheme in lieu of pointing and clicking with the mouse, <em>Grim Fandango</em> would never make it to the PlayStation; GrimE ended up being just too demanding to be made to work on such limited hardware.<span><a role="button" tabindex="0" onclick="footnote_moveToReference_6147_1('footnote_plugin_reference_6147_1_2');" onkeypress="footnote_moveToReference_6147_1('footnote_plugin_reference_6147_1_2');"><sup id="footnote_plugin_tooltip_6147_1_2">[2]</sup></a><span id="footnote_plugin_tooltip_text_6147_1_2"><em>Escape from Monkey Island</em>, the only other game ever made using GrimE, was ported to the more capable PlayStation 2 in 2001.</span></span></p>
<p>All that aside, though, the new engine remained an impressive technical feat, and did succeed in realizing most of Tim Schafer’s aesthetic goals for it. Even the cost savings it apparently failed to deliver come with some mitigating factors. Making the first game with a new engine is always more expensive than making the ones that follow; there was no reason to conclude that GrimE couldn’t deliver real cost savings on LucasArts’s <em>next</em> adventure game. Then, too, for all that <em>Grim Fandango</em> wound up costing two and a half times as much as <em>Full Throttle</em>, it was also well over two and a half times as long as that game.</p>
<p>“Game production schedules are like flying jumbo jets,” says Tim Schafer. “It’s very intense at the takeoff and landing, but in the middle there’s this long lull.” The landing is the time of crunch, of course, and the crunch on <em>Grim Fandango</em> was protracted and brutal even by the industry’s usual standards, stretching out for months and months of sixteen- and eighteen-hour days. For by the beginning of 1998, the game was way behind schedule and way over budget, facing a marketplace that was growing more and more unkind to the adventure genre in general. This was not a combination to instill patience in the LucasArts executive suite. Schafer’s team did get the game done by the autumn of 1998, as they had been ordered to do in no uncertain terms, but only at a huge cost to their psychological and even physical health.</p>
<p>Bret Mogilefsky remembers coming to Schafer at one point to tell him that he just didn’t think he could go on like this, that he simply had to have a break. He was met with no sympathy whatsoever. To be fair, he probably shouldn’t have expected any. Crunch was considered par for the course in the industry during this era, and LucasArts was among the worst of its practitioners. Long hours spent toiling for ridiculously low wages — Mogilefsky was hired to be the key technical cog in this multi-million-dollar project for a salary of about $30,000 per year — were considered the price you paid for the privilege of working at The <em>Star Wars</em> Company.</p>
<p>Even setting aside the personal toll it took on the people who worked there, crunch did nothing positive for the games themselves. As we’ll see, <em>Grim Fandango</em> shows the scars of crunch most obviously in its dodgy puzzle design. Good puzzles result from a methodical, iterative process of testing and carefully considering the resulting feedback. <em>Grim Fandango</em> did not benefit from such a process, and this lack is all too plainly evident.</p>
<p>But before I continue making some of you very, very mad at me, let me take some time to note the strengths of <em>Grim Fandango</em>, which are every bit as real as its weaknesses. Indeed, if I squint just right, so that my eyes <em>only</em> take in its strengths, I have no problem understanding why it’s to be found on so many lists of “The Best Adventure Games Ever,” sometimes even at the very top.</p>
<p>There’s no denying the stuff that <em>Grim Fandango</em> does well. Its visual aesthetic, which I can best describe as 1930s Art Deco meets Mexican folk art meets 1940s gangster flick, is unforgettable. And it’s married to a script that positively crackles with wit and pathos. Our hero Manny is the rare adventure-game character who can be said to go through an actual character arc, who grows and evolves over the course of his story. The driving force behind the plot is his love for a woman named Meche. But his love isn’t the puppy love that Guybrush Threepwood has for Elaine in the <a href="https://www.filfre.net/2017/03/monkey-island-or-how-ron-gilbert-made-an-adventure-game-that-didnt-suck"><em>Monkey Island</em></a> games; the relationship is more nuanced, more adult, more <em>complicated</em>, and its ultimate resolution is all the more moving for that.</p>
<div id="attachment_6150"><p><a href="https://www.filfre.net/2024/11/grim-fandango/sprout/" rel="attachment wp-att-6150"><img decoding="async" aria-describedby="caption-attachment-6150" src="https://www.filfre.net/wp-content/uploads/2024/10/sprout-300x225.webp" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/sprout-300x225.webp 300w, https://www.filfre.net/wp-content/uploads/2024/10/sprout.webp 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6150">How do you create real stakes in a story where everyone is already dead? The Land of the Death’s equivalent of death is “sprouting,” in which a character is turned into a bunch of flowers and forced to live another life in that form. Why shouldn’t the dead fear life as much as the living fear death?</p></div>
<p>Tim Schafer did not grow up with the Latino traditions that are such an inextricable part of <em>Grim Fandango</em>. Yet the game never feels like the exercise in clueless or condescending cultural tourism it might easily have become. On the contrary, the setting feels full-bodied, lived-in, natural. The cause is greatly aided by a stellar cast of voice actors with just the right accents. The Hollywood veteran Tony Plana, who plays Manny, is particularly good, teasing out exactly the right blend of world-weary cynicism and tarnished romanticism. And Maria Canalas, who plays Meche, is equally perfect in her role. The non-verbal soundtrack by Peter McConnell is likewise superb, a mixture of mariachi music and cool jazz that shouldn’t work but does. Sometimes it soars to the forefront, but more often it tinkles away in the background, setting the mood. You’d only notice it if it was gone — but trust me, then you would <em>really</em> notice.</p>
<p>This is a <em>big</em> game as well as a striking and stylish one — in fact, by most reckonings the biggest adventure that LucasArts ever made. Each of its four acts, which neatly correspond to the four years that the average soul must spend wandering the underworld before going to his or her final rest, is almost big enough to be a self-contained game in its own right. Over the course of <em>Grim Fandango</em>, Manny goes from being a down-on-his-luck Grim Reaper cum travel agent to a nightlife impresario, from the captain of an ocean liner to a prisoner laboring in an underwater mine. The story does arguably peak too early; the second act, an extended homage to <em>Casablanca</em> with Manny in the role of Humphrey Bogart, is so beautifully realized that much of what follows is slightly diminished by the comparison. Be that as it may, though, it doesn’t mean any of what follows is bad.</p>
<div id="attachment_6155"><p><a href="https://www.filfre.net/2024/11/grim-fandango/rubacava/" rel="attachment wp-att-6155"><img decoding="async" aria-describedby="caption-attachment-6155" src="https://www.filfre.net/wp-content/uploads/2024/10/rubacava-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/rubacava-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/rubacava.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6155">The jump cut to Manny’s new life as a bar owner in the port city of Rubacava at the beginning of the second act is to my mind the most breathtaking moment of the game, the one where you first realize how expansive its scope and ambition really are.</p></div>
<p>All told, then, I have no real beef with anyone who chooses to label <em>Grim Fandango</em> an aesthetic masterpiece. If there was an annual award for style in adventure games, this game would have won it easily in 1998, just as Tim Schafer’s <em>Full Throttle</em> would have taken the prize for 1995. Sadly, though, it seems to me that the weaknesses of both games are also the same. In both of their cases, once I move beyond the aesthetics and the storytelling and turn to the gameplay, some of the air starts to leak out of the balloon.</p>
<p>The interactive aspects of <em>Grim Fandango</em> — you know, all that stuff that actually makes it a game — are dogged by two overarching sets of problems. The first is all too typical for the adventure genre: overly convoluted, often nonsensical puzzle design. Tim Schafer was always more intrinsically interested in the worlds, characters, and stories he dreamed up than he was in puzzles. This is fair enough on the face of it; he is very, very good at those things, after all. But it does mean that he needs a capable support network to ensure that his games play as well as they look and read. He had that support for 1993’s <a href="https://www.filfre.net/2019/06/day-of-the-tentacle"><em>Day of the Tentacle</em></a>, largely in the person of his co-designer Dave Grossman; the result was one of the best adventure games LucasArts ever made, a perfect combination of inspired fiction with an equally inspired puzzle framework. Unfortunately, he was left to make <em>Full Throttle</em> on his own, and it showed. Ditto <em>Grim Fandango</em>. For all that he loved movies, the auteur model was not a great fit for Tim Schafer the game designer.</p>
<p><em>Grim Fandango</em> seldom gives you a clear idea of what it is you’re even trying to accomplish. Compare this with <em>The Curse of Monkey Island</em>, the LucasArts adventure just before this one, a game which seemed at the time to herald a renaissance in the studio’s puzzle designs. There, you’re always provided with an explicit set of goals, usually in the form of a literal shopping list. Thus even when the mechanics of the puzzles themselves push the boundaries of real-world logic, you at least have a pretty good sense of where you should be focusing your efforts. Here, you’re mostly left to guess what Tim Schafer would like to have happen to Manny next. You stumble around trying to shake something loose, trying to figure out what you can do and then doing it just because you can. By no means is it lost on me that this sense of confusion arises to a large extent because <em>Grim Fandango</em> is such a character-driven story, one which eschews the mechanistic tic-tac-toe of other adventure-game plots. But recognizing this irony doesn’t make it any less frustrating when you’re wandering around with no clue what the story wants from you.</p>
<p>Compounding the frustrations of the puzzles are the frustrations of the interface. You don’t use the mouse at all; everything is done with the numeric keypad, or, if you’re lucky enough to have one, a console-style controller. (At the time <em>Grim Fandango</em> was released, virtually no one playing games on computers did.) <em>Grim Fandango’</em>s mode of navigation is most reminiscent of the <a href="https://www.filfre.net/2023/11/putting-the-j-in-the-rpg-part-1-dorakue">console-based JRPGs</a> of its era, such as the hugely popular <a href="https://www.filfre.net/2023/12/putting-the-j-in-the-rpg-part-3-playing-final-fantasy-vii-or-old-man-yells-at-jrpg"><em>Final Fantasy VII</em></a>, which sold over 10 million copies on the PlayStation during the late 1990s. Yet in practice it’s far more irritating, because you have to interact with the environment here on a much more granular level. LucasArts themselves referred to their method of steering Manny about as a “tank” interface, a descriptor which turns out to be all too descriptive. It really does feel like you’re driving a bulky, none too agile vehicle through an obstacle course of scenery.</p>
<div id="attachment_6151"><p><a href="https://www.filfre.net/2024/11/grim-fandango/angle/" rel="attachment wp-att-6151"><img decoding="async" aria-describedby="caption-attachment-6151" src="https://www.filfre.net/wp-content/uploads/2024/10/angle-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/angle-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/angle.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6151">Make no mistake: the 3D engine makes possible some truly striking views. But too often the designers prioritize visual aesthetics over playability.</p></div>
<p>In the final reckoning, then, an approach that is fine in a JRPG makes just about every aspect of an old-school, puzzle-solving adventure game — which is what <em>Grim Fandango</em> remains in form and spirit when you strip all of the details of its implementation away — more awkward and less fun. Instead of having hotspots in the environment that light up when you pass a mouse cursor over them, as you do in a SCUMM adventure, you have to watch Manny’s head carefully as you drive him around; when it turns to look in a certain direction, that means there’s something he can interact with there. Needless to say, it’s all too easy to miss a turn of his head, and thereby to miss something vital to your progress through the game.</p>
<div id="attachment_6152"><p><a href="https://www.filfre.net/2024/11/grim-fandango/inventory/" rel="attachment wp-att-6152"><img decoding="async" aria-describedby="caption-attachment-6152" src="https://www.filfre.net/wp-content/uploads/2024/10/inventory-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/inventory-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/inventory.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6152">The inventory system is also fairly excruciating. Instead of being able to bring up a screen showing all of the items Manny is carrying, you have to cycle through them one by one by punching a key or controller button over and over, listening to him drone out their descriptions over and over as you do so. This approach precludes using one inventory object on another one, cutting off a whole avenue of puzzle design.</p></div>
<p>Now, the apologists among you — and this game does have an inordinate number of them — might respond to these complaints of mine by making reference to the old cliché that, for every door that is closed in life (and presumably in games as well), another one is opened. And in theory, the new engine really does open a door to new types of puzzles that are more tactile and embodied, that make you feel more a part of the game’s world. To Tim Schafer’s credit, he does try to include these sorts of puzzles in quite a few places. To our detriment, though, they turn out to be the worst puzzles in the game, relying on finicky positioning and timing and giving no useful feedback when you get those things slightly wrong.</p>
<p>But even when <em>Grim Fandango</em> presents puzzles that could easily have been implemented in SCUMM, they’re made way more annoying than they ought to be by the engine and interface. When you’re reduced to that final adventurer’s gambit of just trying everything on everything, as you most assuredly will be from time to time here, the exercise takes many times longer than it would using SCUMM, what with having to laboriously drive Manny about from place to place.</p>
<p>Taken as a game rather than the movie it often seems more interested in being, <em>Grim Fandango</em> boils down to a lumpy stew of overthought and thoughtlessness. In the former category, there’s an unpleasant ideological quality to its approach, with its prioritization of some hazy ethic of 3D-powered “immersion” and its insistence that no visible interface elements whatsoever can appear onscreen, even when these choices actively damage the player’s experience. This is where Sid Meier can <a href="https://www.filfre.net/2017/03/whats-the-matter-with-covert-action">helpfully step in to remind us</a> that it is the player who is meant to be having the fun in a game, not the designer.</p>
<p>The thoughtlessness comes in the lack of consideration of what <em>kind</em> of game <em>Grim Fandango</em> is meant to be. Like all big-tent gaming genres, the adventure genre subsumes a lot of different styles of game with different priorities. Some adventures are primarily about exploration and puzzle solving. And that’s fine, although one does hope that those games execute their puzzles better than this one does. But <em>Grim Fandango</em> is not primarily about its puzzles; it wants to take you on a ride, to sweep you along on the wings of a compelling story. And boy, does it have a compelling story to share with you. For this reason, it would be best served by streamlined puzzles that don’t get too much in the way of your progress. The ones we have, however, are not only frustrating in themselves but murder on the story’s pacing, undermining what ought to be <em>Grim Fandango’</em>s greatest strengths. A game like this one that is best enjoyed with a walkthrough open on the desk beside it is, in this critic’s view at least, a broken game by definition.</p>
<p>As with so many near-miss games, the really frustrating thing about <em>Grim Fandango</em> is that the worst of its problems could so easily have been fixed with just a bit more testing, a bit more time, and a few more people who were empowered to push back against Tim Schafer’s more dogmatic tendencies. For the 2015 remastered version of the game, Schafer did grudgingly agree to include an alternative point-and-click interface that is more like that of a SCUMM adventure. The results verge on the transformational. By no means does the addition of a mouse cursor remedy all of the infelicities of the puzzle design, but it does make battering your way through them considerably less painful. If my less-than-systematic investigations on YouTube are anything to go by, this so-old-it’s-new-again interface has become by far the most common way to play the game today.</p>
<div id="attachment_6153"><p><a href="https://www.filfre.net/2024/11/grim-fandango/remaster/" rel="attachment wp-att-6153"><img decoding="async" aria-describedby="caption-attachment-6153" src="https://www.filfre.net/wp-content/uploads/2024/10/remaster-300x225.jpg" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/remaster-300x225.jpg 300w, https://www.filfre.net/wp-content/uploads/2024/10/remaster.jpg 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6153">The <em>Grim Fandango</em> remaster. Note the mouse cursor. The new interface is reportedly implemented entirely in in-engine Lua scripts rather than requiring any re-programming of the GrimE engine itself. This means that it would have been perfectly possible to include as an option in the original release.</p></div>
<p>In other places, the fixes could have been even simpler than revamping the interface. A shocking number of puzzles could have been converted from infuriating to delightful by nothing more than an extra line or two of dialog from Manny or one of the other characters. As it is, too many of the verbal nudges that do exist are too obscure by half and are given only once in passing, as part of conversations that can never be repeated. Hints for Part Four are to be found only in Part One; I defy even an elephant to remember them when the time comes to apply them. All told,<em> Grim Fandango</em> has the distinct odor of a game that no one other than those who were too close to it to see it clearly ever really tried to play before it was put in a box and shoved out the door. There was a time when seeking the feedback of outsiders was a standard part of LucasArts’s adventure-development loop. Alas, that era was long passed by the time of <em>Grim Fandango</em>.</p>
<p>Nonetheless, <em>Grim Fandango</em> was accorded a fairly rapturous reception in the gaming press when it was released in the last week of October in 1998, just in time for Halloween and the Mexican Day of the Dead which follows it on November 1. Its story, characters, and setting were justifiably praised, while the deficiencies of its interface and puzzle design were more often than not relegated to a paragraph or two near the end of the review. This is surprising, but not inexplicable. There was a certain sadness in the trade press — almost a collective guilt — about the diminished prospects of the adventure game in these latter years of the decade. Meanwhile LucasArts was still the beneficiary of a tremendous amount of goodwill, thanks to the many classics they had served up during those earlier, better years for the genre as a whole. <em>Grim Fandango</em> was held up as a sort of standard bearer for the embattled graphic adventure, the ideal mix of tradition and innovation to serve as proof that the genre was still relevant in a <a href="https://www.filfre.net/2023/04/the-next-generation-in-graphics-part-1-three-dimensions-in-software-or-quake-and-its-discontents">post-<em>Quake</em></a>, <a href="https://www.filfre.net/2024/07/starcraft-a-history-in-two-acts">post-<em>Starcraft</em></a> world.</p>
<p>For many years, the standard narrative had it that the unwashed masses of gamers utterly failed to respond to the magazines’ evangelism, that <em>Grim Fandango</em> became an abject failure in the marketplace. In more recent years, Tim Schafer has muddied those waters somewhat by claiming that the game actually sold close to half a million copies. I rather suspect that the truth is somewhere between these two extremes. Sales of a quarter of a million certainly don’t strike me as unreasonable once foreign markets are factored into the equation. Such a figure would have been enough to keep <em>Grim Fandango</em> from losing much if any money, but would have provided LucasArts with little motivation to make any more such boldly original adventure games. And indeed, LucasArts would release only one more adventure game of any stripe in their history. It would use the GrimE engine, but it would otherwise play it about as safe as it possibly could, by being yet another sequel to the venerable but beloved <em>Secret of Monkey Island</em>.</p>
<p>As I was at pains to note earlier, I do see what causes some people to rate <em>Grim Fandango</em> so highly, and I definitely don’t think any less of them for doing so. For my part, though, I’m something of a stickler on some points. To my mind, interactivity is the very quality that separates games from other forms of media, making it hard for me to pronounce a game “good” that botches it. I’ve learned to be deeply suspicious of games whose most committed fans want to talk about everything other than that which you the player actually <em>do</em> in them. The same applies when a game’s creators display the same tendency. Listening to the developers’ commentary tracks in the remastered edition of <em>Grim Fandango </em>(who would have imagined in 1998 that games would someday come with commentary tracks?), I was shocked by how little talk there was about the gameplay. It was all lighting and dialog beats and soundtrack stabs and Z-buffers instead — all of which is really, really important in its place, but none of which can yield a great game on its own. Tellingly, when the subject of puzzle design did come up, it always seemed to be in an off-hand, borderline dismissive way. “I don’t know how players are supposed to figure out this puzzle,” says Tim Schafer outright at one point. Such a statement from your lead designer is never a good sign.</p>
<p>But I won’t belabor the issue any further. Suffice to say that <em>Grim Fandango</em> is doomed to remain a promising might-have-been rather than a classic in my book. As a story and a world, it’s kind of amazing. It’s just a shame that the gameplay part of this game isn’t equally inspired.</p>
<hr>
<p><code> </code><br>
<strong>Did you enjoy this article? If so, please think about pitching in to help me make many more like it. You can pledge any amount you like.</strong></p>
<p><a href="https://www.patreon.com/DigitalAntiquarian" rel="attachment wp-att-5598"><img decoding="async" src="https://www.filfre.net/wp-content/uploads/2023/04/Patreon-300x133-1.png" alt="" width="300" height="133"></a></p>
<hr>

<p><strong>Sources:</strong> The book <em>Grim Fandango: Prima’s Official Strategy Guide</em> by Jo Ashburn. <em>Retro Gamer</em> 31 and 92; <em>Computer Gaming World</em> of November 1997, May 1998, and February 1999; <em>Ultimate PC</em> of August 1998. Plus the commentary track from the 2015 <em>Grim Fandango</em> remaster.</p>
<p>Online sources include&nbsp;<em>The International House of Mojo’</em>s <a href="https://mixnmojo.com/features/sitefeatures/LucasArts-Secret-History-13-Grim-Fandango/1">pages on the game</a>, the self-explanatory <a href="https://grimfandango.network/"><em>Grim Fandango Network</em></a>, <em>Gamespot’</em>s <a href="https://web.archive.org/web/20100201172300/http://www.gamespot.com/pc/adventure/grimfandango/review.html">vintage review of the game</a>, and Daniel Albu’s <a href="https://www.youtube.com/watch?v=_qwAzIYaGUI">YouTube conversation with Bret Mogilefsky</a>.</p>
<p>And a special thank-you to reader Matt Campbell, who shared with me the audio of a talk that Bret Mogilefsky gave at the 2005 Lua Workshop, during which he explained how he used that language in GrimE.</p>
<p><strong>Where to Get It:</strong> A modestly remastered version of <em>Grim Fandango</em> is <a href="https://www.gog.com/en/game/grim_fandango_remastered">available for digital purchase</a> at GOG.com.</p>
							
							
														
													</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You too can write a book (108 pts)]]></title>
            <link>https://parentheticallyspeaking.org/articles/write-a-book/</link>
            <guid>42096915</guid>
            <pubDate>Sat, 09 Nov 2024 21:10:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://parentheticallyspeaking.org/articles/write-a-book/">https://parentheticallyspeaking.org/articles/write-a-book/</a>, See on <a href="https://news.ycombinator.com/item?id=42096915">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Thanks to <a href="https://www.neeldhara.com/">Neeldhara Misra</a> for pushing me to write
this post, based on a thread on Twitter.</span></p><p>This article is primarily directed at academics. Its purpose is to
tell you two things:
</p><div><ul><li><p>You <span>can</span> write a book.</p></li><li><p>You probably <span>should</span> write a book.</p></li></ul></div><h3>1<tt>&nbsp;</tt><a name="(part._.You_.Can_.Write_a_.Book)"></a>You Can Write a Book<span><a href="#(part._.You_.Can_.Write_a_.Book)" title="Link to here">🔗</a><span> </span></span></h3><p>Let’s say you’re not using someone else’s textbook, or using it only
loosely. That means you’re going to spend a lot of time organizing
your thoughts. You will probably produce some kind of “lecture
notes”. The delta from there to a book is much smaller than you
imagine.</p><p>Here’s a pro-tip. Back in about 2003/4, I noticed that the quantity of
reading that students would do before class was at most about six
pages; once it got to about eight pages, they wouldn’t read at
all. (These numbers may be much lower now.) But this automatically
bounds how much you have to <span>write</span>!</p><p>In short: let’s say you’re writing up lecture notes. You’re writing
about four to at most six pages per class. Let’s say you have about 30
classes (often many more). You have automatically written about 200
pages without even especially thinking about it. Two hundred pages of
writing is often called a … book. It represents your “take”. So your
take now has a book!</p><h3>2<tt>&nbsp;</tt><a name="(part._.You_.Should_.Write_a_.Book)"></a>You Should Write a Book<span><a href="#(part._.You_.Should_.Write_a_.Book)" title="Link to here">🔗</a><span> </span></span></h3><p>What are the <span>incentives</span> to do this? There are many, but they
may not accrue immediately: they may take time. Think of it as a
long-term investment in yourself.</p><ul><li><p>First, simply: you believe strongly in your view of the world,
and you’re pursuing it with intensity. Right now nobody else is really
able to download your brain. Your book becomes how others can download
it.</p></li><li><p>People at other places might use your book, or at least put it on
reading lists. Even if only one student there reads and internalizes
that supplemental material, that student now carries your ideas with
them. Much more concretely, they could be a PhD applicant.</p><p>I have gotten so many great PhD applicants over the years thanks to my
books! In particular, when they come from a less-well-known university,
this guarantees for me that they have the preparation I need, and that
we have a shared mindset.</p></li><li><p>What’s out there may not be very good.  This is especially an
issue in programming languages, where some of the widely-used texts
are basically hot garbage: basically a broken, 1970s view of the
world. I once wrote a
<a href="https://cs.brown.edu/~sk/Publications/Papers/Published/sk-teach-pl-post-linnaean/">position piece</a> on it
entitled “Teaching Programming Languages in a Post-Linnaean Age”! So
worldview really matters.</p><p>So that’s another incentive. To drive out the bad with good. Do you
use the standard text that everyone else uses in your field? If you’ve
read this far, probably not. Why not? You know (or think you know!)
what should be taught in your field. Who uses a text? Someone less
certain. So it’s not their fault. We need to help them along.</p></li><li><p>Sometimes we have resources—<wbr>like software—<wbr>that many people
don’t know about, but that are especially well suited to education in
our field. For instance, the <span>#lang</span> feature of <a href="https://racket-lang.org/">Racket</a>
is one of the most powerful tools for teaching programming
languages. But most people don’t (yet) know that.</p></li></ul><p>I speak from experience. I have written several books, some solo and
some with co-authors. I did the very thing you are told to not do as a
tenure-track assistant professor: I wrote a quality
undergraduate-level book. I survived (it didn’t hurt my tenure in the
slightest), and I then benefited from it for a long time.</p><h3>3<tt>&nbsp;</tt><a name="(part._.Mechanics)"></a>Mechanics<span><a href="#(part._.Mechanics)" title="Link to here">🔗</a><span> </span></span></h3><p>The one <span>big</span> thing I haven’t said, which drives a lot of this,
is the publishing medium. And oh boy, do I have opinions on this!
I wrote up some of them when I published the first formal edition of
my programming languages book, in an essay entitled
<a href="https://cs.brown.edu/~sk/Memos/Books-as-Software/">Books as Software</a>. Let me summarize/expand.</p><ul><li><p>Don’t go to commercial publishers. They are either just not
ready for the modern world or will basically paywall your
work. Paywalling is the total antithesis of wanting to have impact and
influence and wanting to drive bad books out of the market.</p></li><li><p>Publish it free online. Especially those of us who are
immigrants from poorer countries know what it’s like to not be able to
afford high-quality material. The next you is sitting right now in
Bangalore stuck with a crappy course and crappy book. Be their light.</p></li><li><p>Some people like paper. Upload your file to a print-on-demand
service. Even with a markup, it’ll be a lot cheaper than a book from a
commercial publisher.</p></li><li><p>In a STEM subject, your tenure case is not going to hinge on a
contract from MIT Press. Getting that contract is actually relatively
easy. Impact is hard. Go for the hard part, as you do in your
research. Optimize for it.</p></li><li><p>Put out a new release once a year. Don’t fall for the temptation
of continuous releases. People using your book need to be able to
depend on a fixed version for the semester. They will have references
to pages, sections, etc. Don’t break their build.</p></li><li><p>Publish permalinks. Not everyone can upgrade their course every
year. Yes, it means your old mistakes are on permanent display, and
some people won’t use your latest and greatest. Live with it. Your ego
is not that fragile. You’ll get over it.</p></li><li><p>Make it easy for people to send you corrections. They will (just
as they did for your software). Sometimes you will even get very
insightful and creative ideas. Of course you’ll also get various
dreck. Just as with your software. Because books are software, as my
essay says.</p></li><li><p>Try to provide materials in both PDF and HTML. The reasons
should be obvious. It’s not always easy. I personally prefer to use
<a href="https://docs.racket-lang.org/scribble/index.html">Scribble</a> for this purpose. But I have also used LaTeX and
even Google Docs. The latter two are each terrible in their own way,
but the best tool is the one you use and that lets you get something
done reasonably quickly. There’s always time to revise. Don’t suffer
the paralysis of tool indecision and let that become the reason you
don’t write!</p></li><li><p>You won’t make much money this way. It’s okay, you’re probably
already paid pretty well. And the money you’re not getting is money
the author of the crappy textbook is also not getting! And you’ll get
paid in mindshare, which is infinitely more valuable.</p></li><li><p>You can also do what I did: I published the free PDF on my Web
site. On the print-on-demand site, offer a modestly-priced PDF. I let
readers know where they can get the free version. Therefore, the only
reason to buy the PDF is the equivalent of a tip-har. Most don’t, but
a few do. (You can also use one of the tip-jar services, though they
didn’t provide enough value for me.)</p></li></ul><p>Sure, my revenues have been modest. I view the checks as a little
surprise bonus. Added over time it could probably have bought me a new
bike frame (my unit of measure!), but mostly it’s mostly just
a nice dinner and ice cream. But I’m not doing it for the money. I’m
doing it to spread a worldview and to liberate a field from terrible
books. Both are much more worthwhile to me.</p><p>In short: if you’re even slightly tempted to write that textbook—<wbr>go
for it. I got you, fam.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Visprex – Open-source, in-browser data visualisation tool for CSV files (101 pts)]]></title>
            <link>https://docs.visprex.com/</link>
            <guid>42096837</guid>
            <pubDate>Sat, 09 Nov 2024 20:54:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.visprex.com/">https://docs.visprex.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42096837">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
          
        
      
      <main data-md-component="main">
        <div data-md-component="content">
              <article>
                
                  

  
  


<h2 id="_1"><img alt="Visprex" src="https://docs.visprex.com/assets/images/logo.webp#center"></h2>
<h2 id="what-is-visprex">What is Visprex?</h2>
<p>Visprex is a lightweight data visualisation tool that helps you speed up your statistical modelling and analytics workflows. The main high-level features include:</p>
<h3 id="quick">⏱️ Quick</h3>
<ul>
<li>You can visualise your data <strong>in seconds</strong> to quickly build an intuition on your dataset</li>
<li>No need for referring to specific sytax in your statistical analysis software</li>
</ul>
<h3 id="secure">🔒️ Secure</h3>
<ul>
<li>Your data is processed <strong>entirely on your browser</strong>, which means your data won't be sent anywhere</li>
<li>No tracking or analytics software is used for privacy</li>
</ul>
<h3 id="open-source">📖 Open Source</h3>
<ul>
<li>Source code is fully <strong>open source</strong> on GitHub: <a href="https://github.com/visprex/visprex">github.com/visprex/visprex</a></li>
<li>Visprex is also freely available on <a href="https://www.visprex.com/">visprex.com</a></li>
</ul>
<h2 id="who-is-visprex-for">Who is Visprex for?</h2>
<h3 id="students">Students</h3>
<p>Visprex is suitable for students who are starting out in their statistical modelling training.</p>
<p>There's no need for starting your computing environment on your machine or writing tedious visualisation scripts.</p>
<h3 id="data-scientists">Data Scientists</h3>
<p>Visprex is also for data analysts who would like to quickly inspect tabular data for analytical purposes, without worrying about privacy or PII as no data leaves your browser.</p>
<h2 id="quickstart">Quickstart</h2>
<p>You can start visualising your data with just a few clicks by first <a href="https://docs.visprex.com/features/datasets/">loading your dataset</a>.</p>












                
              </article>
            </div>
        
      </main>
      
        
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NYC Subway Station Layouts (166 pts)]]></title>
            <link>http://www.projectsubwaynyc.com/gallery</link>
            <guid>42096717</guid>
            <pubDate>Sat, 09 Nov 2024 20:35:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.projectsubwaynyc.com/gallery">http://www.projectsubwaynyc.com/gallery</a>, See on <a href="https://news.ycombinator.com/item?id=42096717">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="44" id="block-yui_3_17_2_1_1540856882851_7416"><p>  Inspired and want to support this project? Consider tipping me on the  <a data-preserve-html-node="true" href="http://www.projectsubwaynyc.com/contact/">Contact/Tip</a> page!
    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Jaws – a JavaScript to WASM ahead-of-time compiler (179 pts)]]></title>
            <link>https://github.com/drogus/jaws</link>
            <guid>42095879</guid>
            <pubDate>Sat, 09 Nov 2024 18:14:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/drogus/jaws">https://github.com/drogus/jaws</a>, See on <a href="https://news.ycombinator.com/item?id=42095879">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Jaws</h2><a id="user-content-jaws" aria-label="Permalink: Jaws" href="#jaws"></a></p>
<p dir="auto">Jaws is a JavaScript to WebAssembly compiler written in Rust. It is similar to <a href="https://github.com/CanadaHonk/porffor">porffor</a> in a way it also results in a standalone WASM binary that can be executed without an interpreter, but it takes a different implementation approach.</p>
<p dir="auto">It's an experimental tool and it's not ready for production. A lot of the language
features and builtin types are missing or incomplete. That said, my goal is to eventually support 100% of the language.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why Jaws?</h3><a id="user-content-why-jaws" aria-label="Permalink: Why Jaws?" href="#why-jaws"></a></p>
<p dir="auto">I started this project while working on a stress testing tool called <a href="https://github.com/drogus/crows">Crows</a> that runs WebAssembly scenarios. At the moment it only supports code compiled from Rust to WASM. As much as I love writing Rust, I also know it's not a widely popular language and besides, small tests are often easier to write in interpreted languages. The problem is, running scripting languages on top of WASM is not ideal at the moment. You have to either include an interpreter, which automatically makes the binary at least a few MBs in size and the memory usage even bigger, or use a variation of the language you're targetting (like TinyGo instead of Go, or AssemblyScript instead of TypeScript/JavaScript).</p>
<p dir="auto">I believe that with modern WASM proposals it is possible to implement 100% of JavaScript features without the need to use a compiled interpreter, as WASM runtimes are already interpreters.</p>
<p dir="auto">If you want to see it happen, please consider <a href="https://github.com/sponsors/drogus">sponsoring my work</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What works</h3><a id="user-content-what-works" aria-label="Permalink: What works" href="#what-works"></a></p>
<p dir="auto">As I eventually want to implment 100% of the language, I'm purposefully focused on implementing the semantics first, rather than go for 100% of builtins and grammar as I want to be 100% sure it's doable.</p>
<p dir="auto">I have a list of 4 things that I think are hard to implement and after I implement all of them I will focus on more grammar and builtins. These are:</p>
<ol dir="auto">
<li>Scopes/closures</li>
<li>try/catch</li>
<li>async/await</li>
<li>generators</li>
</ol>
<p dir="auto">The last two are kind of similar as by getting generators working, one essentially has tools to make async await work, but I still wanted to make the distinction. At the moment Jaws can compile code using closures with (mostly) proper scopes support, it allows try/catch and it implements (limited) <code>Promise</code> API and <code>async</code> (but not <code>await</code> yet). For example the following script will print <code>error: foo</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let value = &quot;foo&quot;;
async function foo() {
  throw value;
}

foo().then(
  function () {},
  function (v) {
    console.log(&quot;error&quot;, v);
  },
);"><pre><span>let</span> <span>value</span> <span>=</span> <span>"foo"</span><span>;</span>
<span>async</span> <span>function</span> <span>foo</span><span>(</span><span>)</span> <span>{</span>
  <span>throw</span> <span>value</span><span>;</span>
<span>}</span>

<span>foo</span><span>(</span><span>)</span><span>.</span><span>then</span><span>(</span>
  <span>function</span> <span>(</span><span>)</span> <span>{</span><span>}</span><span>,</span>
  <span>function</span> <span>(</span><span>v</span><span>)</span> <span>{</span>
    <span>console</span><span>.</span><span>log</span><span>(</span><span>"error"</span><span>,</span> <span>v</span><span>)</span><span>;</span>
  <span>}</span><span>,</span>
<span>)</span><span>;</span></pre></div>
<p dir="auto">A non exhaustive list of other stuff that should work:</p>
<ul dir="auto">
<li>declaring and assigning: <code>var</code>, <code>let</code>, <code>const</code></li>
<li><code>while</code></li>
<li>string lierals, adding string literals</li>
<li>numbers and basic operators (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>)</li>
<li>booleans and basic boolean operators</li>
<li>array literals</li>
<li>object literals</li>
<li><code>new</code> keyword</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Host requirements</h3><a id="user-content-host-requirements" aria-label="Permalink: Host requirements" href="#host-requirements"></a></p>
<p dir="auto">As Jaws is built with a few relatively recent WASM proposals, the generated binaries are not really portable between runtimes yet. I'm aiming to implement it with WASIp2 in mind, but the only runtime capable of running components and WASIp2, ie. Wasmtime, does not support some other things I use, like parts of the WASM GC proposal or exception handling.</p>
<p dir="auto">In order to make it easier to develop before the runtimes catch up with standardized proposals, I decided to use V8 (through Chromium or Node) with a Javascript polyfill for WASIp2 features that I need. There is a script <code>run.js</code> in the repo that allows to run binaries generated by Jaws. Eventually it should be possible to run them on any runtime implementing WASM GC, exception handling and WASIp2 API.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How to use it?</h3><a id="user-content-how-to-use-it" aria-label="Permalink: How to use it?" href="#how-to-use-it"></a></p>
<p dir="auto">Unless you want to contribute you probably shouldn't, but after cloning the repo
you can use an <code>execute.sh</code> script like:</p>
<div data-snippet-clipboard-copy-content="./execute.sh --cargo-run path/to/script.js"><pre><code>./execute.sh --cargo-run path/to/script.js
</code></pre></div>
<p dir="auto">It will generate a WAT file, compile it to a binary and then run using Node.js.</p>
<p dir="auto">It requires Rust's <code>cargo</code>, relatively new version of <code>wasm-tools</code> and Node.js v23.0.0 or newer. Passing <code>--cargo-run</code> will make the script use <code>cargo run</code> command to first compile and then run the project, otherwise it will try to run the release build (so you have to run <code>cargo build --release</code> prior to running <code>./execute.sh</code> without <code>--cargo-run</code> option)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What's next?</h3><a id="user-content-whats-next" aria-label="Permalink: What's next?" href="#whats-next"></a></p>
<p dir="auto">My plan is to finish implementing all of the "hard to implement" features first, so next in line are generators and <code>await</code> keyword support. Ideally I would use the <a href="https://github.com/WebAssembly/stack-switching">stack-switching</a> proposal for both await and generators, but alas it's only in Phase 2 and it has minimal runtime support (I could find some mentions in Chromium development groups, but I couldn't get it to work). In the absence of stack-switching I'm working on using CPS transforms in order to simulate continuations.</p>
<p dir="auto">After that's done, I will be slowly implementing all of the missing pieces, starting with grammar (for loops, switch etc) and then builtin types and APIs.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How does it work?</h3><a id="user-content-how-does-it-work" aria-label="Permalink: How does it work?" href="#how-does-it-work"></a></p>
<p dir="auto">The project is essentially translating JavaScript syntax into WASM instructions, leveraging instructions added by WASM GC, exception handling and tail call optimizations proposals. On top of the Rust code that is translating JavaScript code, there is about 3k lines of WAT code with all the plumbing needed to translate JavaScript semantics into WASM.</p>
<p dir="auto">To give an example let's consider scopes and closures. WASM has support for passing function references and for structs and arrays, but it doesn't have the scopes semantics that JavaScript has. Thus, we need to simulate how scopes work, by adding some extra WASM code. Imagine the following JavaScript code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let a = &quot;foo&quot;;

function bar() {
  console.log(a);
}

bar();"><pre><span>let</span> <span>a</span> <span>=</span> <span>"foo"</span><span>;</span>

<span>function</span> <span>bar</span><span>(</span><span>)</span> <span>{</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>a</span><span>)</span><span>;</span>
<span>}</span>

<span>bar</span><span>(</span><span>)</span><span>;</span></pre></div>
<p dir="auto">In JavaScript, because a function definition inherits the scope in which it's defined, the <code>bar()</code> function has access to the <code>a</code> variable. Thus, this script should print out the string <code>"foo"</code>. We could translate it to roughly the following pseudo code:</p>
<div data-snippet-clipboard-copy-content="// first we create a global scope, that has no parents
let scope = newScope(null);

// then we set the variable `a` on the scope
declareVariable(scope, &quot;a&quot;, &quot;foo&quot;);

// now we define the  bar function saving a reference to the function
let func = function(parentScope: Scope, arguments: JSArguments, this: Any) -> Any {
  // inside a function declaration we start a new scope, but keeping
  // a reference to the parentScope
  let scope = newScope(parentScope);

  // now we translate console.log call retreiving the variable from the scope
  // this will search for the `a` variable on the current scope and all of the
  // parent scopes
  console.log(retrieve(scope, &quot;a&quot;));
}
// when running a function we have to consider the scope
// in which it was defined
let fObject = createFunctionObject(func, scope);
// and now we also set `bar` on the current scope
declareVariable(scope, &quot;bar&quot;, fObject)

// now we need to fetch the `bar` function from the scop
// and run it
let f = retrieve(scope, &quot;bar&quot;);
call(f);"><pre><code>// first we create a global scope, that has no parents
let scope = newScope(null);

// then we set the variable `a` on the scope
declareVariable(scope, "a", "foo");

// now we define the  bar function saving a reference to the function
let func = function(parentScope: Scope, arguments: JSArguments, this: Any) -&gt; Any {
  // inside a function declaration we start a new scope, but keeping
  // a reference to the parentScope
  let scope = newScope(parentScope);

  // now we translate console.log call retreiving the variable from the scope
  // this will search for the `a` variable on the current scope and all of the
  // parent scopes
  console.log(retrieve(scope, "a"));
}
// when running a function we have to consider the scope
// in which it was defined
let fObject = createFunctionObject(func, scope);
// and now we also set `bar` on the current scope
declareVariable(scope, "bar", fObject)

// now we need to fetch the `bar` function from the scop
// and run it
let f = retrieve(scope, "bar");
call(f);
</code></pre></div>
<p dir="auto">All of the helpers needed to make it work are hand written in WAT format. I have some ideas on how to make it more efficient, but before I can validate all the major features I didn't want to invest too much time into side quests. Writing WAT by hand is not that hard, too, especially when you consider WASM GC.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">License</h3><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">The code is licensed under Apache 2.0 license</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenCoder: Open-Source LLM for Coding (352 pts)]]></title>
            <link>https://arxiv.org/abs/2411.04905</link>
            <guid>42095580</guid>
            <pubDate>Sat, 09 Nov 2024 17:27:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2411.04905">https://arxiv.org/abs/2411.04905</a>, See on <a href="https://news.ycombinator.com/item?id=42095580">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+S" rel="nofollow">Siming Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng,+T" rel="nofollow">Tianhao Cheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J+K" rel="nofollow">Jason Klein Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hao,+J" rel="nofollow">Jiaran Hao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+L" rel="nofollow">Liuyihan Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Y" rel="nofollow">Yang Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+J" rel="nofollow">J. Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J" rel="nofollow">J.H. Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+C" rel="nofollow">Chenchen Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chai,+L" rel="nofollow">Linzheng Chai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yuan,+R" rel="nofollow">Ruifeng Yuan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Z" rel="nofollow">Zhaoxiang Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fu,+J" rel="nofollow">Jie Fu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Q" rel="nofollow">Qian Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+G" rel="nofollow">Ge Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z" rel="nofollow">Zili Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qi,+Y" rel="nofollow">Yuan Qi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Y" rel="nofollow">Yinghui Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chu,+W" rel="nofollow">Wei Chu</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2411.04905">View PDF</a>
    <a href="https://arxiv.org/html/2411.04905v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent <a href="http://systems.while/" rel="external noopener nofollow">this http URL</a> open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an ``open cookbook'' for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Linzheng Chai [<a href="https://arxiv.org/show-email/c945dd3e/2411.04905" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 7 Nov 2024 17:47:25 UTC (25,625 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When machine learning tells the wrong story (104 pts)]]></title>
            <link>https://jackcook.com/2024/11/09/bigger-fish.html</link>
            <guid>42095302</guid>
            <pubDate>Sat, 09 Nov 2024 16:38:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jackcook.com/2024/11/09/bigger-fish.html">https://jackcook.com/2024/11/09/bigger-fish.html</a>, See on <a href="https://news.ycombinator.com/item?id=42095302">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="post">
  
  <p>
    Oxford, UK — November 09, 2024
  </p>
  <p>In June 2022, three short weeks after my college graduation, I presented at ISCA, my first serious research conference.
Onstage with my co-author <a href="https://www.drean.io/">Jules Drean</a>, we gave a 15-minute talk about our hardware security research paper, <a href="https://jackcook.github.io/bigger-fish/" target="\_blank">There’s Always a Bigger Fish: A Clarifying Analysis of a Machine-Learning-Assisted Side-Channel Attack</a>, that had taken the majority of my last two years at MIT to complete.
In hindsight, that talk was the culmination of one of my proudest accomplishments from my time in college.
The paper has since won awards and recognition, including first place in <a href="https://www.intel.com/content/www/us/en/security/security-practices/security-research/hardware-security-academic-award.html" target="\_blank">Intel’s 2024 Hardware Security Academic Award</a>,<sup><a href="#fn1" id="ref1">1</a></sup> and inclusion in the 2023 edition of <a href="https://ieeexplore.ieee.org/document/10167515" target="\_blank">IEEE Micro Top Picks</a>, which highlights 12 of the best papers in computer architecture each year.</p>

<p>Since our talk, every few months, I’ve gotten the urge to write a blogpost about the paper.
Among other cool things described in the paper, we…</p>

<ul>
  <li>Implemented a powerful machine-learning-assisted side-channel attack that can be pulled off in any modern web browser</li>
  <li>Demonstrated for the first time in the literature that <a href="https://en.wikipedia.org/wiki/Interrupt" target="\_blank">system interrupts</a>, a low-level mechanism that your operating system uses to interact with hardware devices, can leak information about user activity</li>
  <li>Learned a valuable lesson about the dangers of applying machine learning toward hardware security research</li>
</ul>

<p>I think some of these lessons are widely applicable, even outside of hardware security research.
But each time I’ve started writing, a few hundred words into the draft, I’ve stopped writing and put the post away.
For some reason, it always felt wrong.
Two years later, no blogpost exists.
If I could write about <a href="https://jackcook.com/2024/02/23/mamba.html" target="\_blank">other people’s research</a>, why couldn’t I write about my own?
I only recently figured out why.</p>

<p>As I’ll get into, one reason this is a hard post to write is because there’s a lot going on in this research paper.
I like writing for a general technical audience, and I need to explain a lot of background before I can get to the good stuff.
The paper also has two competing stories: one about how machine learning models can be used to attack web browsers, and another about how these same models are often misunderstood, leading them to be applied incorrectly.
But there’s also a third story embedded in this paper, about how this paper completely altered the trajectory of my life.
This is a post about machine learning, computer architecture, and more, but also about myself, how I got into research and academia, and how one great mentor can change everything.</p>

<hr>

<figure id="cpu-selection">
  <h3>Select your CPU</h3>
  <div>
    <p>
      This post contains demos and details that can be customized to your CPU. If you know your processor, you may search for it below:
    </p>
    <p>
      Otherwise, feel free to pick a sample CPU from the list below:
    </p>
    
  </div>
</figure>

<hr>

<p>It was September 2020, the start of my junior year at MIT, and I had just enrolled in <a href="https://csg.csail.mit.edu/6.S983/" target="\_blank">Secure Hardware Design</a>, a graduate seminar class that was being offered for the first time.
Nearly all of my past experience had been in software, and I saw the class as a great opportunity to branch out and learn new things.</p>

<p>I quickly found out I was in over my head.
Each week, we would read and discuss two recent hardware security research papers as a class.
Of the 10 or so students who came each week, I was one of only two undergrads, and half of the PhD students weren’t even enrolled in the class—they just wanted to discuss the papers for fun.
I felt that I had very little to contribute to the discussions compared to the wealth of experience everyone else brought to the table, but I was happy to listen nonetheless.</p>

<p>Alongside the paper discussions, each student was supposed to complete a final project.
I met with our professor, <a href="https://people.csail.mit.edu/mengjia/" target="\_blank">Mengjia Yan</a>, early on in the semester to discuss where I should start.
I told her about my prior experience with web development and machine learning, and she suggested I try to reimplement a recently published website fingerprinting attack, which relies on machine learning to exploit weaknesses in hardware.
Her gut told her that there was something wrong with the state-of-the-art research in that area, but she couldn’t put her finger on what it was.</p>

<h2 id="a-primer-on-side-channel-attacks">A primer on side-channel attacks</h2>

<p>In a perfect world, applications on your computer should always operate independently of each other.
For example, if you have Netflix and Spotify open at the same time, Spotify should never be able to know what movie you’re watching.
In practice, this is mostly kept true because of a mechanism known as <a href="https://en.wikipedia.org/wiki/Process_isolation" target="\_blank">process isolation</a>, through which your operating system keeps applications separate from each other by making them use separate resources.
For example, applications are given their own private memory space to store their data, and are restricted from accessing memory that belongs to another process.</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/Virtual_memory.svg">
  <figcaption>Credit: <a href="https://en.wikipedia.org/wiki/Virtual_memory#/media/File:Virtual_memory.svg" target="_blank">Ehamberg, 2009</a></figcaption>
</figure>

<p>However, process isolation is highly imperfect.
At the end of the day, you’re running Netflix and Spotify on the same computer, and they still share a lot of resources.
For example, they both use the same network card to fetch data from the Netflix and Spotify servers.
They use the same graphics card to display data on your screen.
They use the same processor to run their application code.
And so on and so forth.</p>

<p>Consider why this type of resource sharing, however limited, might compromise security.
Let’s say your roommate recently admitted to you that they’ve watched 20 movies in the last week.
They know they’re addicted and need to focus on their work, but they can’t stop watching movies, and they need your help.
To hold them accountable, it’s now up to you to figure out when they’re watching a movie, so you can go knock on their door and tell them to stop.
There are two important facts that will help you solve this problem:</p>

<ol>
  <li>Movies are very large files. Streaming one typically creates a lot of network activity.</li>
  <li>You both share the same Wi-Fi router.</li>
</ol>

<p>One solution might involve periodically downloading a large file and measuring how long each download takes.
If you notice that a download takes longer than usual, and this pattern holds for some period of time, you might begin to suspect that something is up.
Why is the download taking longer? Have you ever asked a relative to stop watching a movie because you needed to take an important call on the same Wi-Fi network, or turned off video on your Zoom call so that the audio would cut out less often? Your Wi-Fi router can only handle so much activity at once, and since you and your roommate are sharing the same router, your network activity is impacted by theirs.
You should go tell your roommate to stop watching their 21st movie of the week.</p>

<p>In this way, your Wi-Fi router creates a <em>side channel</em>, because it essentially reveals information about its users by accident.
This side channel is extremely crude, only able to transmit one bit of information at a time (1 = roommate is watching movie, 0 = roommate is not watching movie), but it still illustrates a very important concept: nearly every shared resource reveals information about its users by accident.
And when it comes to modern computers, there are lots of shared resources, which create lots of side channels.</p>

<p>Some notable examples of side channels that we learned about in Mengjia’s class blew my mind.
For example, it’s been known for some time that changes in a computer’s power consumption can be used as a side channel.
In the figure below, you can see a device’s power consumption, shown in yellow, reliably increasing and decreasing during RSA encryption, enabling you to identify the 1s and 0s that make up a user’s encryption key.
An <a href="https://link.springer.com/book/10.1007/978-0-387-38162-6" target="\_blank">entire book</a> was written about this type of attack over 15 years ago!</p>

<figure>
<img src="https://jackcook.com/img/blog/bigger-fish/power-attack.png">
<figcaption>Credit: <a href="https://en.wikipedia.org/wiki/Power_analysis#/media/File:Power_attack_full.png" target="_blank">Audriusa, 2010</a></figcaption>
</figure>

<p>Similarly, every device, from your laptop to your contactless credit card, emits electromagnetic radiation, since any wire carrying a current creates a small magnetic field (remember the right-hand rule?).
In a similar way to the power-analysis attack described above, you can monitor changes in this EM signal to reverse-engineer encryption keys, user activity, and more.
I could go on and on.
In most cases, though, these types of attacks are impractical to pull off—you need specialized equipment to monitor electromagnetic signals, and it’s hard to look at this signal and tell if someone is encrypting a message or just watching <a href="https://www.youtube.com/watch?v=pRpvdcjkT3k" target="\_blank">cat videos on YouTube</a>.
However, some side-channel attacks are much easier to pull off.
Let’s get back to the topic at hand.</p>

<h2 id="what-is-website-fingerprinting">What is website fingerprinting?</h2>

<p>Now, imagine you have two tabs open in your web browser.
One is a big social media platform that wants to collect as much data about you as possible in order to serve better targeted advertisements.
I don’t want to name an actual social media platform, so I’ll just make one up: let’s call it Facebook.<sup><a href="#fn2" id="ref2">2</a></sup>
In the other tab, you have a website open that you’d prefer “Facebook” didn’t know about—maybe it reveals something about your identity (e.g. <a href="https://en.wikipedia.org/wiki/Truth_Social" target="\_blank">Truth Social</a>) or something you’d otherwise be ashamed for other people to know about (e.g. Taylor Swift fan page).
How could Facebook figure out what website you have open in this other tab?</p>

<p>Facebook could turn to website fingerprinting to solve this problem.
When Mengjia and I discussed my final project, she pointed me to a paper by <a href="https://www.usenix.org/conference/usenixsecurity19/presentation/shusterman" target="\_blank">Shusterman et al.</a> that explores this exact setup, where one website attempts to identify the website open in another tab out of a set of 100 possible websites.
They claimed to do this by taking advantage of a widely-studied side channel: the CPU cache.
Their idea worked as follows: while your computer loads a website (or does anything, for that matter), it saves lots of data in small CPU caches in order to reduce the number of times it needs to load data from RAM or from your hard drive, both of which are much slower.</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/cpu-cache.svg">
  <figcaption>Very simplified explanation of where CPUs get their data.</figcaption>
</figure>

<p>However, caches, like your Wi-Fi router, are shared resources, and they are generally very small.
If RAM is a library that holds hundreds of thousands of books, the CPU cache might be a single bookshelf by the front door with a few of the most popular titles.
When someone requests a book, if it’s on that bookshelf, they will get their book almost instantly.
If not, they can go look for it in the library, but it will take much longer.
The problem here is that your bookshelf will inevitably fill up, because it holds a fraction of the contents of the library.
If you have two programs running on your computer, they will need to share the same CPU cache, which is also being used by your operating system and whatever other applications you have open at the time.</p>

<p>Now, If you think about it, when you open any website, perhaps you’ve opened <a href="https://jackcook.com/" target="\_blank">jackcook.com</a>, it will load in basically the same way every time.
It will reference the same scripts, images, and stylesheets, and it will render the same content on the page.
And in theory, this should translate to very similar cache activity each time the website loads.
This is where website fingerprinting comes in.
To try to take advantage of this CPU-cache side channel, Shusterman et al. designed an attacker that worked as follows:</p>

<ol>
  <li>Allocate an array that’s the same size as the CPU cache<sup><a href="#fn3" id="ref3">3</a></sup> and fill it with ones.</li>
  <li>While a website loads in another tab, every 2 milliseconds, measure how long it takes to loop over the array and access every element.<sup><a href="#fn4" id="ref4">4</a></sup></li>
  <li>Repeat step 2 for 15 seconds.</li>
</ol>

<p>By the end of this process, making one measurement every 2 milliseconds for 15 seconds, the attacker will have 7500 measurements in total.
Now, let’s consider for a moment why an individual measurement might be higher or lower, and remember that you’re opening another website while this happens, the one you don’t want “Facebook” to know about.</p>

<p>Facebook, which I will now refer to as the <em>attacker</em>, and the other website you’re loading, which I will now refer to as the <em>victim</em>, share <em>a lot</em> of resources on your computer.
One of these shared resources is the CPU cache: using the analogy from earlier, imagine the attacker and victim each have their own libraries with their own data, but they have to share space on the same small bookshelf to cache this data.
<cpu data-value="name" data-prefix="Your " data-suffix=" has" data-requiredvalue="cache">A typical CPU might have</cpu> a cache that can hold <cpu data-value="cache" data-requiredvalue="cache">around 6-8 MB</cpu> of data, enough to hold about <cpu data-value="cache/4" data-requiredvalue="cache">2 million</cpu> integers.
When the attacker then creates its <cpu data-value="cache" data-requiredvalue="cache">cache-sized</cpu> array, it will fill this cache, evicting all of the data it currently holds.
But at some point, as the victim website loads, it will have data of its own, which the CPU will put in its cache, evicting some of the attacker’s data.</p>

<p>But then, 2 milliseconds later, the attacker will read all of its data again, and the CPU will first look for this data in the cache.
If any of the attacker’s data was removed from the cache in the last 2 milliseconds, it will take a tiny bit longer to do this, because the CPU will need to spend time looking for it in RAM.
And because the attacker is timing how long this process takes, it will notice this tiny discrepancy.
This process will then repeat itself 7,500 times over the course of the next 15 seconds.
And since websites generally load in the same way every time, these measurements should reflect the unique cache access patterns of the website you’ve opened.
In the figure below, you can see how these measurements look for three different websites, where time increases to the right, and darker colors indicate slower cache accesses.</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/shusterman-fig3.png">
  <figcaption>Figure 3 from <a href="https://www.usenix.org/conference/usenixsecurity19/presentation/shusterman" target="_blank">Shusterman et al., 2019</a></figcaption>
</figure>

<p>Notice how within each website, the traces look very similar, but across different websites, the traces look very different.
This is where website fingerprinting gets its name: each collection of latency measurements, which we call a <em>trace</em>, essentially acts as a “fingerprint” that can be used to identify the website that was loaded.
If I gave you a new trace from one of these websites and asked you to tell me which one it came from, you could probably compare it to the traces above and give me the answer.</p>

<p>Click the start button below to see this process in action by recording <smallscreen data-value="50">100</smallscreen> milliseconds of cache latency measurements on your own device.
Then, hover over or tap on the values in the trace to see what each measurement represents.
This demo works best in Chrome, but it should work fine in other browsers as well.<sup><a href="#fn5" id="ref5">5</a></sup>
<cpu data-value="name" data-prefix="The slider below has been automatically set to your " data-suffix="’s cache size of "></cpu><cpu data-value="cache" data-suffix=", but if">If</cpu> all of your bars have the same color, try adjusting the cache size.</p>

<figure>
  
</figure>

<p>Now, let’s go back to the example from earlier.
Imagine you’re “Facebook,” and you want to identify the websites your users are opening in their other tabs.
How would you do this?
First, you might collect a bunch of traces while opening a lot of different websites, similar to the ones above, which you can use as training data.
With this training data, you can train a machine learning model, which can reliably predict the website that was opened while recording one of these traces.
Then, when users open “Facebook” in the future, “Facebook” can record new traces, feed them into this model, and see if there’s a match.</p>

<p>This is what Shusterman et al. did: in their paper, they collected 100 traces for 100 different websites, yielding a labeled dataset of 10,000 traces, and used it to train a few machine learning models.
They then repeated this process across several different web browsers and operating systems, and found that it was possible to identify the website that was opened out of a set of 100 possible websites with up to 91.4% accuracy.<sup><a href="#fn6" id="ref6">6</a></sup>
Pretty cool, and a little scary!</p>

<figure>
  <div>
    <table>
      <thead>
        <tr>
          <th>Web Browser</th>
          <th>Operating System</th>
          <th>Classification Accuracy</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Chrome</td>
          <td>Linux</td>
          <td>91.4% ± 1.2%</td>
        </tr>
        <tr>
          <td>Chrome</td>
          <td>Windows</td>
          <td>80.0% ± 1.6%</td>
        </tr>
        <tr>
          <td>Firefox</td>
          <td>Linux</td>
          <td>80.0% ± 0.6%</td>
        </tr>
        <tr>
          <td>Firefox</td>
          <td>Windows</td>
          <td>87.7% ± 0.8%</td>
        </tr>
        <tr>
          <td>Safari</td>
          <td>macOS</td>
          <td>72.6% ± 1.3%</td>
        </tr>
        <tr>
          <td>Tor Browser</td>
          <td>Linux</td>
          <td>46.7% ± 4.1%</td>
        </tr>
      </tbody>
    </table>
  </div>
  <figcaption>An abbreviated version of Table 2 from Shusterman et al., 2019</figcaption>
</figure>

<h2 id="my-final-project">My final project</h2>

<p>I spent the next few weeks trying to replicate these results from Shusterman et al.’s paper.
Going into this project, I wasn’t too sure what I would find.
Mengjia and I decided I should look at website fingerprinting because I had past experience with web development and machine learning, and not because there was an obvious unresolved research question.
I collected a bunch of data and tried to distinguish between four websites at a time, which was fairly straightforward.</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/initial-eval.png">
  <figcaption>My very first results, showing 40 traces per website recorded while opening cnn.com, msnbc.com, nytimes.com, and apple.com.</figcaption>
</figure>

<p>Even if you just looked at these traces with your own eyes, there are clear patterns that distinguish these websites from each other.
If I gave you a new trace, you could probably tell me which website it came from.
But of course, models do this faster and more reliably.
By training a simple Random Forest classifier on these traces, I was able to predict the correct website with 98% accuracy.
Scaling this up to 10 websites initially gave me 75% accuracy, but over the next several weeks, I kept experimenting and making improvements until I could reliably classify 10 websites, then 50, then 100.</p>

<p>While many of these experiments led to small improvements, one stood out above the rest.
Remember that in Shusterman et al.’s paper, the attacker works by repeatedly measuring how long it takes to access all of the elements in its array.
For one of my experiments, I tried a different approach: I simply made the attacker count as fast as it could (like literally, execute <code>value++</code> over and over again).
In the demo below, click Start and hover over or tap on the final trace to see how high your computer can count in 5 seconds.</p>

<figure>
  
</figure>

<p>Of course, this number doesn’t tell us too much on its own.
However, if we do this repeatedly, we can see how fast your computer can count over time.
Now see how high your computer can count in one second, five times in a row.</p>

<figure>
  
</figure>

<p>As we make this interval smaller, we can see how this plays out at a finer timescale.
Again, hover over or tap on the values in these traces to see what each measurement represents.</p>

<figure>
  
</figure>

<p>Now, play with the demo above to see if you can get the trace to change.
If you do nothing, the values should stay relatively consistent over time.
But if you click start and then do something else on your device, such as opening a new tab or resizing the browser window, you should see this reflected in the trace.
If you’re having trouble with this, you can see an example below:</p>

<figure>
  <video controls="" preload="auto" data-setup="{&quot;fluid&quot;: true}">
    <source src="https://jackcook.com/img/blog/bigger-fish/google-maps-demo.mp4" type="video/mp4">
    <p>
      To view this video please enable JavaScript, and consider upgrading to a web browser that
      <a href="https://videojs.com/html5-video-support/" target="_blank">supports HTML5 video</a>
    </p>
  </video>
  <figcaption>When I open Google Maps in a new tab, the trace values drop. This enables the attacker to notice that something else is happening on my computer, even though process isolation is supposed to make this impossible.</figcaption>
</figure>

<p>Take a second to think about what’s happening here.
In the video above, over the course of 5 seconds, my computer tried to count as fast as it could.
Every 100 milliseconds, we saved the value it reached, and made it start over again from zero.
As was the case with the cache-based attacker, this trace is sensitive to things that happen outside of the same browser tab: when I opened Google Maps, my computer couldn’t count as high.</p>

<p>This shows that this counter trace can essentially be interpreted as a signal.
We can improve the resolution of this signal by saving the value more often: while I saved the value every 100 milliseconds in my demo to illustrate the concept of our counting-based attack, in our paper, we save the value every 5 milliseconds to get more information in a fixed amount of time.
In the demo above, you can change this value, which we call the <em>period length</em>, to record counter traces at a higher or lower resolution.
A simple version of the trace collection code is shown here in Python, which you can read or try for yourself:</p>

<pre><code>import time</code>
<code></code>
<code>PERIOD_LENGTH = 0.1  # 0.1 second</code>
<code>TRACE_LENGTH = 5  # 5 seconds</code>
<code>start = time.time()</code>
<code>counter = 0</code>
<code>trace = []</code>
<code></code>
<code>while len(trace) &lt; int(TRACE_LENGTH / PERIOD_LENGTH):</code>
<code>    if time.time() - start &gt;= PERIOD_LENGTH:</code>
<code>        trace.append(counter)</code>
<code>        start = time.time()</code>
<code>        counter = 0</code>
<code>        continue</code>
<code>    </code>
<code>    counter += 1</code>
<code></code>
<code>print(trace)</code>
</pre>

<hr>

<p>I ran the experiment, training a model on these counter traces rather than using the cache-latency traces we discussed earlier.
And as it turned out, models trained on the counter traces were more accurate at predicting the website that was opened!</p>

<p>This was a really exciting result!
The theory behind Shusterman et al.’s paper was that their attacker leveraged a CPU-cache-based side channel by repeatedly evicting data from the cache and measuring cache access latency.
But my new attacker simply incremented a counter, without needing to repeatedly evict data from the cache, and it appeared to yield an even better signal.
There was no solid theory behind it.
Why did it work so well?</p>

<p>I presented my findings during the final lecture of Secure Hardware Design, and Mengjia was even more excited than I was.
A few days after that lecture, she sent me the following email:</p>

<blockquote>
  <p>Hi Jack,</p>

  <p>Great job on the 6.888 course project!
As I have commented during the lecture, your work is really impressive.</p>

  <p>I am wondering whether you will be interested in continuing the project in my group.
My group offers [undergraduate research] opportunities.
I am very curious about why your new measurement approach could do a better job than cache-contention attacks.
By digging deep into it and figuring out the reason, we can potentially find something unknown to the public related to how the browser or hardware processor works, and get a potential publication in a top security or computer architecture conference.</p>

  <p>If you are interested, we could chat about details after your finals and the holiday season.</p>

  <p>Thanks,
Mengjia</p>
</blockquote>

<p>I didn’t realize it at the time, but I had accidentally discovered a new side channel.</p>

<h2 id="investigating-the-mystery-side-channel">Investigating the mystery side channel</h2>

<p>I called Mengjia several weeks later, after the end of my <a href="https://jackcook.com/2022/04/11/tmobile.html" target="\_blank">6-week roadtrip across the US</a>, and joined her lab in February 2021.
She introduced me to <a href="https://www.drean.io/" target="\_blank">Jules Drean</a>, a talented graduate student who studies micro-architectural side-channel attacks such as this one, and the three of us immediately got to work trying to understand why my new attacker worked so well.
As it turned out, the picture was complicated, but one thing was clear: machine learning models need to be used carefully.</p>

<p>This would become the biggest lesson of our eventual research paper: in a machine-learning-assisted side-channel attack such as this one, if a model can reliably predict user activity, it proves the <em>presence</em> of a signal, but not the <em>cause</em> of that signal.<sup><a href="#fn7" id="ref7">7</a></sup>
Even though Shusterman et al.’s model could identify the correct victim website 91.4% of the time, that didn’t necessarily mean that their model was picking up on contention over the CPU cache.
And the implications of getting this wrong can be big: researchers look at papers describing attacks when building defenses that make our computers safer.
A more thorough analysis was needed in order to properly identify the side channel, which we set out to provide.</p>

<p>We started by replicating the larger experiments from Shusterman et al.’s paper to understand the similarities and differences between our counting-based attacker and their cache-based attacker.
It turned out that when asked to identify the correct victim website out of 100, my counting-based attacker achieved higher accuracy in nearly every experimental configuration we tried.</p>

<figure>
  <div>
    <table>
      <thead>
        <tr>
          <th>Web Browser</th>
          <th>Operating System</th>
          <th>Cache Attack</th>
          <th><strong>Counting Attack</strong></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Chrome</td>
          <td>Linux</td>
          <td>91.4% ± 1.2%</td>
          <td><strong>96.6%</strong> ± 0.8%</td>
        </tr>
        <tr>
          <td>Chrome</td>
          <td>Windows</td>
          <td>80.0% ± 1.6%</td>
          <td><strong>92.5%</strong> ± 1.0%</td>
        </tr>
        <tr>
          <td>Firefox</td>
          <td>Linux</td>
          <td>80.0% ± 0.6%</td>
          <td><strong>95.3%</strong> ± 0.7%</td>
        </tr>
        <tr>
          <td>Firefox</td>
          <td>Windows</td>
          <td>87.7% ± 0.8%</td>
          <td><strong>91.9%</strong> ± 1.2%</td>
        </tr>
        <tr>
          <td>Safari</td>
          <td>macOS</td>
          <td>72.6% ± 1.3%</td>
          <td><strong>96.6%</strong> ± 0.5%</td>
        </tr>
        <tr>
          <td>Tor Browser</td>
          <td>Linux</td>
          <td>46.7% ± 4.1%</td>
          <td><strong>49.8%</strong> ± 4.2%</td>
        </tr>
      </tbody>
    </table>
  </div>
  <figcaption>A simplified version of Table 1 from our paper, Cook et al., 2022.<sup><a href="#fn8" id="ref8">8</a></sup></figcaption>
</figure>

<p>For some configurations in particular, this discrepancy in performance was huge: in Safari on macOS, the cache attack achieved just 72.6% accuracy, while our counting attack achieved 96.6% accuracy on the same task.
These results really seemed to suggest that the cache may have been interfering with our signal rather than helping it!
But again, we couldn’t say this for sure without a more thorough analysis.
So over the next several weeks we applied the scientific method, modifying one variable at a time, collecting new data, and training new models to eliminate different hypotheses about what was going on.
First, we established a baseline, where we simply ran the attacker in its default configuration without any modifications, and found that we could identify the correct website out of 100 with 95.2% accuracy.</p>

<h3 id="hypothesis-1-cpu-frequency-scaling">Hypothesis 1: CPU frequency scaling</h3>

<p>We then tested our first hypothesis, which was that our counting-based attack was taking advantage of a CPU-frequency-scaling side channel.
CPUs operate at a set frequency, which reflects the amount of work they can do per second.
For example, <cpu data-value="name" data-prefix="your " data-suffix=" operates ">a typical CPU might operate</cpu> at <cpu data-value="frequency">3.0 GHz</cpu>, meaning it completes about <cpu data-value="frequency-math">3 × 10<sup>9</sup> cycles</cpu> per second.
However, modern CPUs adjust this frequency based on the workload demanded of them, speeding up when there’s more work to do, and slowing down when there’s less, in order to save energy.<sup><a href="#fn9" id="ref9">9</a></sup>
This is why your computer might get hot or turn on a fan when you have a lot of applications open: when your CPU scales its frequency up, it performs more operations in a fixed amount of time, generating more heat due to the increased electrical activity.</p>

<p>We hypothesized that while the victim website is loading, the CPU would change its frequency often, enabling it to complete variable amounts of work over time.
And for our attacker, completing more work should enable it to count faster, yielding higher values at specific points in time while the other website loads, or vice versa.
But it turned out this wasn’t the case: we went into <a href="https://en.wikipedia.org/wiki/BIOS" target="\_blank">BIOS</a>, disabled frequency scaling, collected more data, and trained another model.
Compared to the baseline experiment, our accuracy dropped by just one point to 94.2%, indicating that changes in counter values can’t really be explained by changes in CPU frequency.
We started filling out a table to keep track of our results:</p>

<figure>
  <div>
    <table>
      <thead>
        <tr>
          <th>Isolation Mechanism</th>
          <th>Accuracy</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Default</td>
          <td>95.2%</td>
        </tr>
        <tr>
          <td><strong>+ Disable CPU frequency scaling</strong></td>
          <td><strong>94.2%</strong></td>
        </tr>
      </tbody>
    </table>
  </div>
</figure>

<h3 id="hypothesis-2-cpu-core-contention">Hypothesis 2: CPU core contention</h3>

<p>Next, we thought our counting-based attack might have been exploiting a CPU-core-contention side channel.
Your <cpu data-value="name">CPU</cpu> has <cpu data-value="cores">several</cpu> cores, <cpu data-hideifselected="true">often around 4 or 8, </cpu>each of which can execute a fixed number of instructions per second.
However, there are generally more than <cpu data-value="cores">4 or 8</cpu> processes running on your computer, which inevitably means that some processes will have to run on the same core and compete for time.
If the attacker and victim were, by chance, scheduled on the same core, the attacker’s counter values should decrease when the victim tab spends more time loading, providing enough of a signal to tell victim websites apart.</p>

<p>But this was also wrong.
With CPU frequency scaling still disabled, we ran an experiment with Linux’s <code>taskset</code> command, which can be used to force a process to execute on a specific CPU core, and ensured that the attacker and victim tabs ran on separate cores.
Even when the attacker and victim were isolated in this way, the attacker still seemed to have plenty of information: it could still pick the correct victim website out of 100 with 94.0% accuracy.</p>

<figure>
  <div>
    <table>
      <thead>
        <tr>
          <th>Isolation Mechanism</th>
          <th>Accuracy</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Default</td>
          <td>95.2%</td>
        </tr>
        <tr>
          <td>+ Disable CPU frequency scaling</td>
          <td>94.2%</td>
        </tr>
        <tr>
          <td><strong>+ Pin attacker and victim to separate cores</strong></td>
          <td><strong>94.0%</strong></td>
        </tr>
      </tbody>
    </table>
  </div>
</figure>

<p>At this point, we were a little stumped.
CPU caches, frequency, and core resource contention are relatively well-studied side channels, and we had already ruled out all three of them.
We sent out some emails and gave presentations within the department to see if anyone had ideas.
Fortunately, <a href="https://www.fintelia.io/" target="\_blank">Jonathan Behrens</a> answered our call.</p>

<h3 id="hypothesis-3-system-interrupts">Hypothesis 3: System interrupts</h3>

<p>After some further discussion, we hypothesized that our counting-based attacker might be exploiting a system-interrupt-based side channel.
This idea was a bit out of left field: we couldn’t find any prior research that had studied the security properties of system interrupts.
In hindsight, this is a little surprising considering how pervasive they are: your operating system uses system interrupts constantly to communicate with hardware devices, such as your keyboard, mouse, and display.</p>

<p>Compared to software, hardware is fairly unpredictable: anything can happen at any moment.
For example, your operating system has no idea when you’re next going to hit a key on your keyboard.
But once you do, your operating system needs to act as quickly as possible.
It generates a <em>system interrupt</em>, which it dispatches to one of your <cpu data-value="cores" data-suffix=" "></cpu>CPU cores.
Then, once the interrupt arrives at the core, any program currently executing on that core is halted immediately in order to process the interrupt.</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/cook-fig1.png">
  <figcaption>Figure 1 from our paper. As soon as an interrupt is received on the same core as the attacker, the attacker is halted until the interrupt has been processed.</figcaption>
</figure>

<p>We thought that while the victim tab was loading, it would trigger many different kinds of interrupts: from your network card as it uploads and downloads data, from your graphics card as it renders content on your screen, and so on and so forth.
If any of these interrupts is processed on the same CPU core as the attacker program, your operating system would need to halt the attacker each time, preventing it from counting until each interrupt handler (shown above in yellow) has finished processing.
And remember, in the demo from earlier, we learned that less time spent counting leads to lower counter values, which can be highly indicative of activity happening elsewhere on your computer.</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/cook-fig3.png">
  <figcaption>Figure 3 from our paper. Even relatively small changes in counter values can reveal the victim website!</figcaption>
</figure>

<p>It turns out that on Linux, where we were running these experiments, you can monitor system interrupts very easily.
For example, if you run <code>cat /proc/interrupts</code> on Ubuntu, you should get a readout that resembles the table below:</p>

<figure>
  <div>
    <table>
      <thead>
        <tr>
          <th>ID</th>
          <th>Interrupt Type</th>
          <th>Core 1</th>
          <th>Core 2</th>
          <th>Core 3</th>
          <th>Core 4</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><code>16</code></td>
          <td><code>usb1</code> (Mouse)</td>
          <td>31</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
        </tr>
        <tr>
          <td><code>23</code></td>
          <td><code>usb2</code> (Keyboard)</td>
          <td>1943</td>
          <td>934</td>
          <td>0</td>
          <td>0</td>
        </tr>
        <tr>
          <td><code>27</code></td>
          <td><code>enp2s0</code> (Network card)</td>
          <td>0</td>
          <td>376</td>
          <td>0</td>
          <td>10880</td>
        </tr>
        <tr>
          <td><code>28</code></td>
          <td><code>ahci</code> (SATA/hard drive)</td>
          <td>8201</td>
          <td>0</td>
          <td>11531</td>
          <td>0</td>
        </tr>
        <tr>
          <td><code>30</code></td>
          <td><code>i915</code> (Graphics card)</td>
          <td>0</td>
          <td>193</td>
          <td>0</td>
          <td>364</td>
        </tr>
        <tr>
          <td><code>NMI</code></td>
          <td>Local timer interrupts</td>
          <td>22059</td>
          <td>18076</td>
          <td>19010</td>
          <td>27837</td>
        </tr>
        <tr>
          <td><code>IWI</code></td>
          <td>IRQ work interrupts</td>
          <td>5794</td>
          <td>4910</td>
          <td>4950</td>
          <td>7493</td>
        </tr>
        <tr>
          <td><code>RES</code></td>
          <td>Rescheduling interrupts</td>
          <td>1400</td>
          <td>1339</td>
          <td>1359</td>
          <td>1262</td>
        </tr>
        <tr>
          <td><code>CAL</code></td>
          <td>Function call interrupts</td>
          <td>6122</td>
          <td>6547</td>
          <td>6563</td>
          <td>3100</td>
        </tr>
        <tr>
          <td><code>TLB</code></td>
          <td>TLB shootdowns</td>
          <td>295</td>
          <td>377</td>
          <td>285</td>
          <td>290</td>
        </tr>
      </tbody>
    </table>
  </div>
  <figcaption>Each row indicates a different kind of interrupt, and each column indicates the number of times that interrupt has been executed on each core since the computer started up. See a screenshot of the full readout <a href="https://jackcook.com/img/blog/bigger-fish/proc-interrupts.png" target="_blank">here</a> for more detail.</figcaption>
</figure>

<p>This table shows that many interrupts are being processed on all four cores, very likely interfering with the attacker’s counting!
So naturally, the next experiment we would like to run should isolate the attacker from these interrupts, letting the attacker count freely on one core while interrupts are processed on another core.
But after doing some research, we came across a problem.</p>

<p>Linux provides a mechanism to route certain types of interrupts, which we call <em>movable interrupts</em>, to a specific core.
These interrupts have numeric IDs in the table above, and generally come from external hardware devices, such as your keyboard and network card.
However, there are also many types of <em>non-movable interrupts</em> which can’t be routed to a specific core, meaning we can’t isolate them from the attacker.
These interrupts have three-letter IDs in the table above, and are generally used to synchronize activity between your CPU cores, which is why modern operating systems require that they be processed on all of them.
And unfortunately for us, as you can see in the table above, these non-movable interrupts make up the bulk of interrupt activity.</p>

<p>But we didn’t let this deter us.
We used Linux’s <code>irqbalance</code> command, which can be used to force certain interrupts to be processed on a specific CPU core, and routed all movable interrupts to core 1.
Building on the previous experiments, we additionally used <code>taskset</code> to force the attacker and victim to run on cores 2 and 3, while also forcing the CPU to run at a fixed frequency as we described earlier.
Even though we could only isolate movable interrupts, it seemed like we were onto something: the attacker’s accuracy dropped by nearly six points!</p>

<figure>
  <div>
    <table>
      <thead>
        <tr>
          <th>Isolation Mechanism</th>
          <th>Accuracy</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Default</td>
          <td>95.2%</td>
        </tr>
        <tr>
          <td>+ Disable CPU frequency scaling</td>
          <td>94.2%</td>
        </tr>
        <tr>
          <td>+ Pin attacker and victim to separate cores</td>
          <td>94.0%</td>
        </tr>
        <tr>
          <td><strong>+ Isolate movable interrupts</strong></td>
          <td><strong>88.2%</strong></td>
        </tr>
      </tbody>
    </table>
  </div>
  <figcaption>Most of Table 3 from our paper</figcaption>
</figure>

<h3 id="hypothesis-35-non-movable-system-interrupts">Hypothesis 3.5: Non-movable system interrupts</h3>

<p>Of course, this result left us wondering what the attacker’s accuracy would be if we could isolate non-movable interrupts as well.
But again, this type of experiment is impossible: due to fundamental limitations of how operating systems are built, non-movable interrupts must be processed on every core.
If we wanted to understand the impact of these interrupts, we had to use a different approach.</p>

<p>This was where Jonathan’s expertise proved crucial: he suggested we use <a href="https://en.wikipedia.org/wiki/EBPF" target="\_blank">eBPF</a>, a low-level technology that can be used to make small changes to your operating system while it’s still running.
Among the many APIs it provides, eBPF enabled us to record two crucial things:</p>

<ul>
  <li>Every time the attacker program starts and stops</li>
  <li>Every time an interrupt handler starts and stops</li>
</ul>

<p>Remember, CPU frequency scaling is still disabled, meaning that the CPU executes a fixed number of instructions per second.
In theory, this means that if the attacker is uninterrupted, it should always be able to reach the same counter value in a fixed amount of time.
We figured that if we could record every time interval during which the attacker was interrupted, whether to run another program, to process an interrupt, or for some other unknown reason, we could compare this to every interval during which an interrupt was processed, and see if these explained the gaps in the attacker’s execution.</p>

<p>Jonathan <a href="https://github.com/jackcook/bigger-fish/blob/main/ebpf/src/main.rs" target="\_blank">wrote the code</a> to do this with eBPF, recording hundreds of these timestamps while our attacker counted away.
If we go back to the figure from earlier, we measured all of these yellow regions, during which the attacker was interrupted:</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/cook-fig1.png">
  <figcaption>Still figure 1 from our paper</figcaption>
</figure>

<p>We analyzed these gaps in the attacker’s execution, hoping to get an understanding of what was going on, and it turned out that our intuition was right.
Out of all of these gaps that last at least 100 nanoseconds, we found that over 99% of them are spent processing interrupts!
This was the smoking gun we had been looking for all along!</p>

<p>Essentially, during our experiments, the attacker’s CPU core is basically only ever doing one of two things: processing the attacker’s counting code, or processing an interrupt.
And since the CPU is running at a fixed speed, the amount of time spent processing the attacker’s code should be proportional to the number of times it’s able to increment its counter.
In the figure below, you can see this for yourself: while loading a victim website, the attacker’s counter value generally goes up when less time is spent processing interrupts, and vice versa.</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/interrupt-handling-time.png">
  <figcaption>A figure from our ISCA talk, showing that time spent handling interrupts time and counter values are inversely correlated.</figcaption>
</figure>

<p>Now that you understand what’s going on, I encourage you to try this demo again, seeing what happens if you do something in the middle of trace collection that triggers a bunch of interrupts.
Some suggestions include opening a new tab, pressing a bunch of buttons on your keyboard, moving your mouse around really quickly, or opening an application.</p>

<figure>
  
</figure>

<p>You can also try our online demo <a href="https://jackcook.github.io/bigger-fish" target="\_blank">here</a>, or check out our trace collection code <a href="https://github.com/jackcook/bigger-fish" target="\_blank">on GitHub</a>!</p>

<h2 id="theres-always-a-bigger-fish">There’s always a bigger fish</h2>

<p>That was a lot!
My apologies if I lost you in some of the technical details.
Let me take a step back and summarize what we did one more time:</p>

<ul>
  <li>We re-implemented a state-of-the-art cache-based website fingerprinting attack</li>
  <li>We modified it to remove cache accesses, yielding a new counting-based attacker which took advantage of some unknown side channel</li>
  <li>We ruled out several possible side channels, including CPU caches, CPU frequency, and CPU core contention</li>
  <li>We used eBPF to prove that this attack primarily leverages a system-interrupt-based side channel</li>
</ul>

<p>And through this process, we came away with two key findings:</p>

<h3 id="1-system-interrupts-leak-user-activity">1. System interrupts leak user activity</h3>

<p>This was a fairly surprising finding: the security properties of system interrupts had never been demonstrated before.
We became the first group to study this new system-interrupt-based side channel, and we likely won’t be the last: there are tons of directions for future work!
I’ll come back to this in a moment.</p>

<h3 id="2-machine-learning-assisted-side-channel-attacks-need-to-be-analyzed-carefully">2. Machine-learning-assisted side-channel attacks need to be analyzed carefully</h3>

<p>This is arguably our most important takeaway, and almost certainly the reason we ended up winning those awards from Intel and IEEE.
Machine learning models are great at finding patterns in data and can be used regardless of one’s understanding of the side channel being attacked, which leads to the development of powerful attacks that are poorly understood.</p>

<p>Without instrumenting our operating system, we could not have made many conclusions about which side channel our attack was exploiting: it’s impossible to do this with models that can only find correlations!
And it’s important to get this right—an incorrect analysis of an attack can mislead researchers hoping to build defenses, wasting valuable time and energy.</p>

<p>For example, in their paper, Shusterman et al. proposed a defense against their attack that involves repeatedly evicting data from the CPU cache while the attacker tries to collect data.
The idea was driven by their understanding of the side channel being exploited: adding noise to the CPU cache should make it more difficult to exploit a cache-based side channel.
However, we found that a defense that instead generates a bunch of interrupts, such as by making network requests to local IP addresses, defends significantly better against both the cache-based attack and our counting-based attack!</p>

<figure>
  <div>
    <table>
      <thead>
        <tr>
          <th>Attack</th>
          <th>Baseline</th>
          <th>With Cache Noise</th>
          <th>With Interrupt Noise</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Counting Attack (ours)</td>
          <td>95.7%</td>
          <td>92.6%</td>
          <td>62.0%</td>
        </tr>
        <tr>
          <td>Cache Attack (Shusterman et al.)</td>
          <td>78.4%</td>
          <td>76.2%</td>
          <td>55.3%</td>
        </tr>
      </tbody>
    </table>
  </div>
  <figcaption>Table 2 from our paper, which shows that both attacks are affected more by extra interrupts than by extra cache accesses.</figcaption>
</figure>

<p>This is a relatively simple example, but it shows how having a better understanding of the side channel being exploited can increase our ability to defend against it.
In combination with our other findings, it also helped us build our case that Shusterman et al.’s attack primarily exploits signals from interrupts, and not the cache.
We hope this work motivates future researchers to apply these models carefully.</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/bigger-fish.png">
  <figcaption>There’s Always a Bigger Fish</figcaption>
</figure>

<h3 id="other-findings">Other findings</h3>

<p>There are a few more interesting findings in <a href="https://dl.acm.org/doi/pdf/10.1145/3470496.3527416" target="\_blank">our paper</a> if you’re curious to keep reading!
A couple of these include:</p>

<ul>
  <li>We proposed a modification to the clock provided to JavaScript code that completely mitigates our attack</li>
  <li>We ran experiments that isolated the attacker and victim by putting them in separate virtual machines, which should theoretically offer the most isolation possible</li>
  <li>We discussed and analyzed the properties of several types of non-movable interrupts, including how frequently some of them fire and exactly how long they take to process</li>
</ul>

<p>And more! Unfortunately this blogpost is already long enough as it is.</p>

<h2 id="future-work">Future work</h2>

<p>There are a bunch of open questions in this area, even today, two years after we originally published this paper.
Here are a few that are still at the top of my mind.
If you find any of these interesting, please get in touch!</p>

<h3 id="how-should-we-rethink-interrupts">How should we rethink interrupts?</h3>

<p>Similarly to <a href="https://meltdownattack.com/" target="\_blank">Spectre and Meltdown</a>, our attack targets hardware mechanisms that are embedded deep inside basically all modern computers.
It’s currently impossible to implement a defense that isolates non-movable interrupts from an attacker, and it’s unclear how computers would be redesigned in a way that makes that possible.
Figuring this out presents an important direction for future research, especially if attacks such as ours become more accurate in the future.</p>

<h3 id="the-relationship-between-websites-and-interrupts-is-not-well-understood">The relationship between websites and interrupts is not well understood</h3>

<p>Below, you can see a figure from our paper, in which we show how interrupt handling time varies while loading three different websites.
Notice that the behavior, and even the types of interrupts, are different: loading weather.com triggers a lot of rescheduling interrupts, but nytimes.com and amazon.com don’t trigger any!</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/cook-fig5.png">
  <figcaption>Figure 5 from our paper</figcaption>
</figure>

<p>We don’t know why this is: clearly, there is something that weather.com is doing, perhaps loading more video content or more scripts or something, that the other two websites are not.
At a more basic level, we’re not really sure what the relationship is between website activity and triggered interrupts.
What impact does loading one additional image have on the attacker’s counter trace?
What about an advertisement?
What is it exactly that makes the counter traces so distinctive that we can tell them apart so easily?
We didn’t spend time trying to answer these questions, but a better understanding of this relationship would likely help us build better defenses against system-interrupt-based side-channel attacks such as ours.</p>

<h3 id="the-attack-could-be-made-stronger">The attack could be made stronger</h3>

<p>We wrote this paper as more of an “analysis paper,” and not as an “attack paper.”
In theory, the 96.6% accuracy that we achieved when identifying the victim website in Chrome on Linux is a lower bound, not an upper bound.
It’s pretty likely that a better model, or a different methodology, could achieve higher accuracy.
And if it’s possible to achieve higher accuracy on this 100-website task, it’s likely possible to perform well on a 1,000-website task, or on some other privacy-compromising task: figuring out what movie you’re watching, or whether you’re using a VPN, or how often you check Robinhood.</p>

<h3 id="browser-based-defenses-could-be-made-stronger">Browser-based defenses could be made stronger</h3>

<p>All browsers reduce the precision of the clock that they provide to websites via JavaScript: instead of telling you it’s 10:33:13.726142 (13.726142 seconds after the clock strikes 10:33am), your browser might just round to the nearest millisecond and tell you it’s 10:33:13.726.
This is because with access to higher-precision timers, attacks such as ours become much more accurate (see <a href="https://arxiv.org/abs/1502.07373" target="\_blank">Oren et al., 2015</a>).</p>

<p>As a result, Chrome rounds its clock to the nearest 0.1 millisecond and adds some random noise, while Firefox and Safari round to the nearest 1 millisecond.
<a href="https://www.torproject.org/download/" target="\_blank">Tor Browser</a>, the world’s most popular secure web browser, rounds to the nearest 100 milliseconds, reducing our attack’s accuracy from 96.6% (in Chrome) to 49.8%.
There is a tradeoff here: browser-based game engines, for example, need a high-precision timer in order to render and animate content correctly.
As a result, Tor Browser users are unable to play most games, but this is not necessarily a problem for users who care about security.</p>

<p>In Section 6.1 of <a href="https://dl.acm.org/doi/10.1145/3470496.3527416" target="\_blank">our paper</a>, we propose a slight modification to browser clocks that completely mitigates our attack.
I think it’s a step in the right direction, but more work needs to be done to implement this defense into real web browsers, and to examine whether it’s practical for most users.</p>

<h2 id="how-this-paper-changed-my-life">How this paper changed my life</h2>

<p>Before taking Mengjia’s class, the thought of going to graduate school had crossed my mind, but it was not an option I was taking seriously.
One year prior, I had worked at NVIDIA as a deep learning research intern, and I loved it.
After graduating, I probably would have looked for a full-time job there, or at another big tech company, or at some AI startup.</p>

<p>But this project showed me that research can be fun, and maybe even beautiful.
It was a result of myself and three talented researchers coming together, each with a different background and skillset, to learn and create new knowledge together.
This realization changed my life: after graduating from MIT with this paper under my belt, I stuck around for another year to earn my MEng in computer science, which I would not have done if not for this project.
I then applied for a Rhodes scholarship, which I absolutely would not have won had it not been for this project, and which enabled me to spend two years studying at the University of Oxford.
Next year, I will start my six-year PhD in computer science back at MIT, and I could not be more thrilled!</p>

<p>I am grateful to Jules, Jonathan, and especially Mengjia, for making this project possible, and for taking a chance on me—I can only hope that my future research projects will be as exciting and formative as this one.</p>




<p><sup id="fn1">1. Intel still hasn’t updated their website for some reason, but I promise we won. Source: <a href="https://jackcook.com/img/blog/bigger-fish/intel-award.jpeg" target="_blank">trust me bro</a> <a href="#ref1" title="Jump back to footnote 1 in the text.">↩</a></sup>
<sup id="fn2">2. <a href="https://www.youtube.com/shorts/d_lHcJGwnxM" target="_blank">https://www.youtube.com/shorts/d_lHcJGwnxM</a> <a href="#ref2" title="Jump back to footnote 2 in the text.">↩</a></sup>
<sup id="fn3"> 3. I’m just going to refer to it as the CPU cache for simplicity, but if you care about the details, we want to evict data from the last-level cache (LLC), which is the largest and slowest CPU cache. <a href="#ref3" title="Jump back to footnote 3 in the text.">↩</a></sup>
<sup id="fn4">4. You don’t actually need to access every single element: accessing elements at LLC cache line-sized intervals (usually 64 bytes each) is enough to evict all of the data in that cache line. <a href="#ref4" title="Jump back to footnote 4 in the text.">↩</a></sup>
<sup id="fn5">5. All browsers reduce the precision of the clock that they provide to websites via JavaScript: instead of telling you it’s 10:33:13.726142 (13.726142 seconds after the clock strikes 10:33am), your browser might just round to the nearest millisecond and tell you it’s 10:33:13.726. The reason for this is a little crazy: with a higher-precision timer, you can measure the latency of a single memory access (if it takes longer than a few nanoseconds to read an array value, the value was definitely missing from the cache), enabling you to pull off much more accurate cache-based side-channel attacks (see <a href="https://arxiv.org/abs/1502.07373" target="_blank">Oren et al., 2015</a>). All web browsers have since updated to reduce the resolution of their timers, but Chrome’s timer remains the most precise: Chrome rounds to the nearest 100 microseconds, while Firefox and Safari both round to the nearest 1 millisecond. This means that Chrome will give the most accurate timing data for the cache latency demo in this post. <a href="#ref5" title="Jump back to footnote 5 in the text.">↩</a></sup>
<sup id="fn6">6. The numbers in the table are from the closed-world LSTM column in Table 2 of <a href="https://www.usenix.org/conference/usenixsecurity19/presentation/shusterman" target="_blank">Shusterman et al.’s paper</a>. They also report results for an open-world setup, in which the correct website might not be in the training data. <a href="#ref6" title="Jump back to footnote 6 in the text.">↩</a></sup>
<sup id="fn7">7. Note that the inverse is not true: if a model doesn’t achieve high accuracy, it might just mean that your model isn’t good enough, not that there’s no signal. <a href="#ref7" title="Jump back to footnote 7 in the text.">↩</a></sup>
<sup id="fn8">8. See footnote 6. We report open-world results in our paper as well. <a href="#ref8" title="Jump back to footnote 8 in the text.">↩</a></sup>
<sup id="fn9">9. Differences in the data being processed can actually also cause CPU frequency to change! <a href="https://www.hertzbleed.com/" target="_blank">Wang et al., 2022</a> (also selected by IEEE Micro Top Picks) write, “on modern processors, the same program can run at a different CPU frequency (and therefore take a different [amount of] time) when computing, for example, 2022 + 23823 compared to 2022 + 24436.” This opens up yet another side channel. <a href="#ref9" title="Jump back to footnote 9 in the text.">↩</a></sup>
</p>
  
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IronCalc – Open-Source Spreadsheet Engine (338 pts)]]></title>
            <link>https://www.ironcalc.com/</link>
            <guid>42095292</guid>
            <pubDate>Sat, 09 Nov 2024 16:36:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ironcalc.com/">https://www.ironcalc.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42095292">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>      
      <div id="about">
            <div>
              <h4>MIT/Apache 2.0 licensed</h4>
              <p>You can integrate it into your projects, customize it to your needs, and share it openly without any restrictions.</p>
            </div>
            <div>
              <h4>Feature rich</h4>
              <p>You shouldn’t worry that this or that function is not supported.</p>
            </div> 
            
             
            <div>
              <h4>Excel compatible</h4>
              <p>We at IronCalc are in awe of the software created by Microsoft over the years. We want every one to be able to use their spreadsheets.</p>
            </div> 
            <div>
              <h4>Fully tested</h4>
              <p>Modern programming practices should be used covering with tests any feature of the system.</p>
            </div>
            <div>
              <h4>Fast and lightweight</h4>
              <p>The programs shouldn’t be heavier than a few hundred kilobytes.</p>
            </div>
            <div>
              <h4>International from day one</h4>
              <p>Language should no be a barrier to use a spreadsheet.</p>
            </div>
            <div>
              <h4>Well designed</h4>
              <p>It should be nice and friendly to use. Designed with love from the ground up.</p>
            </div> 
             
             
             

          </div>
      
      <div id="why-ironcalc">
          
          
            <p>
              For over 40 years, spreadsheets have been integral to countless applications. Despite numerous proprietary and open-source options, finding a universally accessible, reliable, and high-quality engine remains a challenge. Many existing solutions are expensive, require accounts, or suffer from performance and stability issues.
            </p>
            <p>
              <strong>Our Mission:</strong> To fill the gaps left by the industry and empower every user with a robust, open-source spreadsheet engine that caters to diverse needs. Here's why we are dedicated to this mission:
            </p>
            <h4>Addressing Unmet Needs</h4>
            <p><strong>Empowering SaaS Developers:</strong> Hundreds, if not thousands, of companies have implemented half-baked spreadsheets in their systems. IronCalc aims to provide these businesses with a superior, open-source alternative that enhances their SaaS applications.</p>
            <p><strong>Automated Spreadsheet Processing:</strong> Users need a reliable way to programmatically open, populate, and analyze spreadsheets for large-scale scenarios. IronCalc delivers the performance and functionality required for these complex tasks.</p>
            <p><strong>Global Collaboration:</strong> We envision a world where anyone can use a spreadsheet online and effortlessly share it with friends for collaborative projects, such as planning travel experiences.</p>
            <p><strong>Interactive Blog Integration:</strong> Bloggers should be able to embed interactive spreadsheets in their posts, allowing readers to engage with custom test cases and scenarios.</p>
            <h4>Beyond Code: Advancing Spreadsheet Technology and Community</h4>
            <p>IronCalc's ambition extends beyond providing open-source code. We aim to drive the spreadsheet industry forward through:</p>
            <p><strong>Research and Development:</strong> While many areas of computer science, such as compilers and algorithms, have extensive literature, spreadsheets have been largely overlooked. We want to challenge that by finding collaborators in universities and academic institutions willing to do open research in spreadsheet engines.</p>
            <p><strong>Community and Collaboration:</strong> We can dream bigger, maybe we can organize conferences and maybe support PhD researchers and foster a collaborative environment where ideas and innovations can flourish.</p>
            <p><strong>Building a Knowledge Base:</strong> Our goal is to equip the next generation of spreadsheet developers with a comprehensive set of tools and knowledge, laying a solid foundation for future advancements, not only IronCalc.</p>
            <p>Together, we can push the boundaries of what spreadsheets can achieve, making high-quality, accessible spreadsheet technology available to all.</p>
            
            <p>
            – The IronCalc Team
            </p>

            

        </div>
      <!-- <section id="newsletter">
        <div class="section-content">
          <div class="blue-container">
            <div class="flex-v gap-lg">
              <div class="flex-v gap-sm">
                <h2>Join the waitlist</h2>
                <p>
                  We’ll send you a message when IronCalc is launched!
                </p>
              </div>
              <button class="secondary max-w-200">Join</button>
            </div>
          </div>
        </div>
      </section> -->

      


      

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SQLite does not do checksums (123 pts)]]></title>
            <link>https://avi.im/blag/2024/sqlite-bit-flip/</link>
            <guid>42094663</guid>
            <pubDate>Sat, 09 Nov 2024 14:41:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://avi.im/blag/2024/sqlite-bit-flip/">https://avi.im/blag/2024/sqlite-bit-flip/</a>, See on <a href="https://news.ycombinator.com/item?id=42094663">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><p>SQLite does not do checksums by default. I learned this from <a href="https://fosstodon.org/@AlexMillerDB/109553692861357766">Alex Miller</a>. What does this mean? If there is disk corruption, the database or application won’t be able to know that the database is ‘corrupt’.</p><p>Even a single bit flip can cause havoc. This can happen due to a faulty disk, a bug in the disk driver, or when another application (malicious or otherwise) modifies the database files.</p><p>This is not a bug - it’s properly documented:</p><blockquote><p>SQLite assumes that the detection and/or correction of bit errors caused by cosmic rays, thermal noise, quantum fluctuations, device driver bugs, or other mechanisms, is the responsibility of the underlying hardware and operating system. SQLite does not add any redundancy to the database file for the purpose of detecting corruption or I/O errors. SQLite assumes that the data it reads is exactly the same data that it previously wrote.</p></blockquote><p>I created a <a href="https://gist.github.com/avinassh/0e7e4b0578136a338f1b9a03fba36ead">simple script</a> to demonstrate this:</p><ol><li><p>Create a sample database using <a href="https://gist.github.com/avinassh/0e7e4b0578136a338f1b9a03fba36ead">this script</a>. It creates a bank database and adds a row for Alice with $83K.</p></li><li><p>Flip a single bit:</p><pre><code> printf '\x00\x00\x00\x00\x00\x80' | dd of=bank.db bs=1 seek=$((0x1ffd)) count=1 conv=notrunc
</code></pre></li><li><p>Alice’s balance is now zero. Sorry, Alice.</p></li></ol><p>It passes <code>PRAGMA integrity_check</code> too. Here’s an ASCII animation if you prefer that:</p><h2 id="wal-and-checksums">WAL and Checksums</h2><p>SQLite has checksums for WAL frames. However, when it detects a corrupt frame, it silently ignores the faulty frame and all subsequent frames. It doesn’t even raise an error!</p><p>Ignoring frames might be acceptable, but not raising an error is where it gets me.</p><h2 id="checksum-vfs-shim">Checksum VFS Shim</h2><p>You can use the <a href="https://www.sqlite.org/cksumvfs.html">Checksum VFS Shim</a>, but there’s one important caveat:</p><blockquote><p>Checksumming only works on databases that have a reserve bytes value of exactly 8</p></blockquote><p>The <a href="https://www.sqlite.org/fileformat2.html#resbyte">documentation of reserve bytes</a> explains:</p><blockquote><p>SQLite has the ability to set aside a small number of extra bytes at the end of every page for use by extensions. These extra bytes are used, for example, by the SQLite Encryption Extension to store a nonce and/or cryptographic checksum associated with each page. The “reserved space” size in the 1-byte integer at offset 20 is the number of bytes of space at the end of each page to reserve for extensions. This value is usually 0. The value can be odd.</p></blockquote><p>This means if you’re using any extension that uses reserve bytes, you can’t use the Checksum shim.</p><p>Again, this is not a bug. <a href="https://avi.im/blag/2024/databases-checksum">Most databases (except a few)</a> assume that the OS, filesystem, and disk are sound. Whether this matters depends on your application and the guarantees you need.</p><p>edit: I wrote a <a href="https://avi.im/blag/2024/databases-checksum">follow up post</a>.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientist treated her own cancer with viruses she grew in the lab (465 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-024-03647-0</link>
            <guid>42094573</guid>
            <pubDate>Sat, 09 Nov 2024 14:23:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-024-03647-0">https://www.nature.com/articles/d41586-024-03647-0</a>, See on <a href="https://news.ycombinator.com/item?id=42094573">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    
                        <figure><picture><source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-03647-0/d41586-024-03647-0_27713234.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-03647-0/d41586-024-03647-0_27713234.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px"><img alt="Coloured transmission electron micrograph of cultured measles virus particles." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-03647-0/d41586-024-03647-0_27713234.jpg"><figcaption><p><span>Viruses such as measles (pictured here) can be used to attack cancerous cells. </span><span>Credit: Eye Of Science/Science Photo Library</span></p></figcaption></picture></figure><p>A scientist who successfully treated her own <a href="https://www.nature.com/subjects/breast-cancer" data-track="click" data-label="https://www.nature.com/subjects/breast-cancer" data-track-category="body text link">breast cancer</a> by injecting the tumour with lab-grown viruses has sparked discussion about the ethics of self-experimentation. </p><p>Beata Halassy discovered in 2020, aged 49, that she had breast cancer at the site of a previous mastectomy. It was the second recurrence there since her left breast had been removed, and she couldn’t face another bout of chemotherapy. </p><p>Halassy, a virologist at the University of Zagreb, studied the literature and decided to take matters into her own hands with an unproven treatment. </p><p>A case report published in <i>Vaccines</i> in August<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup> outlines how Halassy self-administered a treatment called <a href="https://www.nature.com/articles/s41571-022-00719-w" data-track="click" data-label="https://www.nature.com/articles/s41571-022-00719-w" data-track-category="body text link">oncolytic virotherapy</a> (OVT) to help treat her own stage 3 cancer. She has now been cancer-free for four years. </p><p>In choosing to <a href="https://www.nature.com/articles/nm0508-471b" data-track="click" data-label="https://www.nature.com/articles/nm0508-471b" data-track-category="body text link">self-experiment</a>, Halassy joins a long line of scientists who have participated in this under-the-radar, stigmatized and ethically fraught practice. “It took a brave editor to publish the report,” says Halassy.</p><h2>Up-and-coming therapy</h2><p>OVT is an emerging field of <a href="https://www.nature.com/subjects/cancer-therapy" data-track="click" data-label="https://www.nature.com/subjects/cancer-therapy" data-track-category="body text link">cancer treatment</a> that uses viruses to both attack cancerous cells and provoke the immune system into fighting them. Most OVT clinical trials so far have been in late-stage, metastatic cancer, but in the past few years they have been directed towards earlier-stage disease. One OVT, called T-VEC, has been in approved in the United States to treat metastatic melanoma, but there are as yet no OVT agents approved to treat breast cancer of any stage, anywhere in the world. </p><p>Halassy stresses that she isn’t a specialist in OVT, but her expertise in cultivating and purifying viruses in the laboratory gave her the confidence to try the treatment. She chose to target her tumour with two different viruses consecutively — a <a href="https://www.nature.com/subjects/measles-virus" data-track="click" data-label="https://www.nature.com/subjects/measles-virus" data-track-category="body text link">measles virus</a> followed by a vesicular stomatitis virus (VSV). Both pathogens are known to infect the type of cell from which her tumour originated, and have already been used in OVT clinical trials. A measles virus has been trialled against metastatic breast cancer.</p><p>Halassy had previous experience working with both viruses, and both have a good safety record. The strain of measles she chose is used extensively in childhood vaccines, and the strain of VSV induces, at worst, mild influenza-like symptoms. </p><figure><picture><source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-03647-0/d41586-024-03647-0_27713236.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-03647-0/d41586-024-03647-0_27713236.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px"><img alt="Portrait of Beata Halassy." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-03647-0/d41586-024-03647-0_27713236.jpg"><figcaption><p><span>Halassy’s experience with self-treatment has changed the focus of her research. </span><span>Credit: Ivanka Popić </span></p></figcaption></picture></figure><p>Over a two-month period, a colleague administered a regime of treatments with research-grade material freshly prepared by Halassy, injected directly into her tumour. Her oncologists agreed to monitor her during the self-treatment, so that she would be able to switch to conventional chemotherapy if things went wrong.</p><p>The approach seemed to be effective: over the course of the treatment, and with no serious side effects, the tumour shrank substantially and became softer. It also detached from the pectoral muscle and skin that it had been invading, making it easy to remove surgically.</p><article data-label="Related"><a href="https://www.nature.com/articles/d41586-024-02613-0" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-03647-0/d41586-024-03647-0_27470032.jpg"><p>How a trove of cancer genomes could improve kids’ leukaemia treatment</p></a></article><p>Analysis of the tumour after removal showed that it was thoroughly infiltrated with immune cells called lymphocytes, suggesting that the OVT had worked as expected and provoked Halassy’s immune system to attack both the viruses and the tumour cells. “An immune response was, for sure, elicited,” says Halassy. After the surgery, she received a year’s treatment with the anticancer drug trastuzumab. </p><p>Stephen Russell, an OVT specialist who runs virotherapy biotech company Vyriad in Rochester, Minnesota, agrees that Halassy’s case suggests the viral injections worked to shrink her tumour and cause its invasive edges to recede. </p><p>But he doesn’t think her experience really breaks any new ground, because researchers are already trying to use OVT to help treat earlier-stage cancer. He isn’t aware of anyone trying two viruses sequentially, but says it isn’t possible to deduce whether this mattered in an ‘<i>n</i> of 1’ study. “Really, the novelty here is, she did it to herself with a virus that she grew in her own lab,” he says.</p><h2>Ethical dilemma</h2><p>Halassy felt a responsibility to publish her findings. But she received more than a dozen rejections from journals — mainly, she says, because the paper, co-authored with colleagues, involved self-experimentation. “The major concern was always ethical issues,” says Halassy. She was particularly determined to persevere after she came across a review highlighting the value of self-experimentation<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup>. </p><p>That journals had concerns doesn’t surprise Jacob Sherkow, a law and medicine researcher at the University of Illinois Urbana-Champaign who has examined the ethics of researcher self-experimentation in relation to COVID-19 vaccines. </p><article data-label="Related"><a href="https://www.nature.com/articles/d41586-023-02075-w" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-03647-0/d41586-024-03647-0_26019860.jpg"><p>Huge leap in breast cancer survival rate</p></a></article><p>The problem is not that Halassy used self-experimentation as such, but that publishing her results could encourage others to reject conventional treatment and try something similar, says Sherkow. People with cancer can be particularly susceptible to trying unproven treatments. Yet, he notes, it’s also important to ensure that the knowledge that comes from self-experimentation isn’t lost. The paper emphasizes that self-medicating with cancer-fighting viruses “should not be the first approach” in the case of a cancer diagnosis. </p><p>“I think it ultimately does fall within the line of being ethical, but it isn’t a slam-dunk case,” says Sherkow, adding that he would have liked to see a commentary fleshing out the ethics perspective, published alongside the case report.</p><p>Halassy has no regrets about self-treating, or her dogged pursuit of publication. She thinks it is unlikely that someone would try to copy her, because the treatment requires so much scientific knowledge and skill. And the experience has given her own research a new direction: in September she got funding to investigate OVT to treat cancer in domestic animals. “The focus of my laboratory has completely turned because of the positive experience with my self-treatment,” she says.</p>
                    
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Memories are not only in the brain, human cell study finds (214 pts)]]></title>
            <link>https://medicalxpress.com/news/2024-11-memories-brain-human-cell.html</link>
            <guid>42094427</guid>
            <pubDate>Sat, 09 Nov 2024 13:53:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medicalxpress.com/news/2024-11-memories-brain-human-cell.html">https://medicalxpress.com/news/2024-11-memories-brain-human-cell.html</a>, See on <a href="https://news.ycombinator.com/item?id=42094427">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/memories-are-not-only.jpg" data-src="https://scx2.b-cdn.net/gfx/news/2024/memories-are-not-only.jpg" data-sub-html="An NYU researcher administers chemical signals to non-neural cells grown in a culture plate. Credit: Nikolay Kukushkin">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/memories-are-not-only.jpg" alt="Memories are not only in the brain, new research finds" title="An NYU researcher administers chemical signals to non-neural cells grown in a culture plate. Credit: Nikolay Kukushkin" width="800" height="530">
             <figcaption>
                An NYU researcher administers chemical signals to non-neural cells grown in a culture plate. Credit: Nikolay Kukushkin
            </figcaption>        </figure>
    </div><p>It's common knowledge that our brains—and, specifically, our brain cells—store memories. But a team of scientists has discovered that cells from other parts of the body also perform a memory function, opening new pathways for understanding how memory works and creating the potential to enhance learning and to treat memory-related afflictions.</p>

                                        
                                                                                  
                                         

                                                                                                                                    <p>"Learning and <a href="https://medicalxpress.com/tags/memory/" rel="tag">memory</a> are generally associated with brains and brain cells alone, but our study shows that other cells in the body can learn and form memories, too," explains New York University's Nikolay V. Kukushkin, the lead author of the <a href="https://www.nature.com/articles/s41467-024-53922-x" target="_blank">study</a>, which appears in the journal <i>Nature Communications</i>.</p>
<p>The research sought to better understand if non-brain cells help with memory by borrowing from a long-established neurological property—the massed-spaced effect—which shows that we tend to retain information better when studied in spaced intervals rather than in a single, intensive session—better known as cramming for a test.</p>
<p>In the research, the scientists replicated learning over time by studying two types of non-brain human cells in a laboratory (one from nerve tissue and one from kidney tissue) and exposing them to different patterns of chemical signals—just like brain cells are exposed to patterns of neurotransmitters when we learn new information.</p>
<p>In response, the non-brain cells turned on a "memory gene"—the same gene that brain cells turn on when they detect a pattern in the information and restructure their connections in order to form memories.</p>
<p>To monitor the memory and learning process, the scientists engineered these non-brain cells to make a glowing protein, which indicated when the memory gene was on and when it was off.</p>

                                                                                                                                                         
                                                                                                                                                                                                <p>The results showed that these cells could determine when the chemical pulses, which imitated bursts of neurotransmitter in the brain, were repeated rather than simply prolonged—just as neurons in our brain can register when we learn with breaks rather than cramming all the material in one sitting.</p>
<p>Specifically, when the pulses were delivered in spaced-out intervals, they turned on the "memory gene" more strongly, and for a longer time, than when the same treatment was delivered all at once.</p>
<p>"This reflects the massed-space effect in action," says Kukushkin, a clinical associate professor of life science at NYU Liberal Studies and a research fellow at NYU's Center for Neural Science. "It shows that the ability to learn from spaced repetition isn't unique to <a href="https://medicalxpress.com/tags/brain+cells/" rel="tag">brain cells</a>, but, in fact, might be a fundamental property of all cells."</p>
<p>The researchers add that the findings not only offer new ways to study memory, but also point to potential health-related gains.</p>
<p>"This discovery opens new doors for understanding how memory works and could lead to better ways to enhance learning and treat memory problems," observes Kukushkin.</p>
<p>"At the same time, it suggests that in the future, we will need to treat our body more like the brain—for example, consider what our pancreas remembers about the pattern of our past meals to maintain healthy levels of blood glucose or consider what a cancer cell remembers about the pattern of chemotherapy."</p>
<p>The work was jointly supervised by Kukushkin and Thomas Carew, a professor in NYU's Center for Neural Science. The study's authors also included Tasnim Tabassum, an NYU researcher, and Robert Carney, an NYU undergraduate researcher at the time of the study.</p>

                                                                                                                                                                            
                                        											<div>
												                                                    <p><strong>More information:</strong>
                                                    N. V. Kukushkin et al, The massed-spaced learning effect in non-neural human cells, <i>Nature Communications</i> (2024). <a data-doi="1" href="https://dx.doi.org/10.1038/s41467-024-53922-x" target="_blank">DOI: 10.1038/s41467-024-53922-x</a>
																								
																								</p>
																							</div>
                                        											
																					
                                                                                                                        
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                 Memories are not only in the brain, human cell study finds (2024, November 8)
                                                 retrieved 9 November 2024
                                                 from https://medicalxpress.com/news/2024-11-memories-brain-human-cell.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I Analyzed 650k TikTok Influencers and This Is What I Found (291 pts)]]></title>
            <link>https://old.reddit.com/r/eCommerceSEO/comments/1gn8egy/ultimate_ecommerce_marketing_tool_influencers/</link>
            <guid>42093911</guid>
            <pubDate>Sat, 09 Nov 2024 11:49:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/eCommerceSEO/comments/1gn8egy/ultimate_ecommerce_marketing_tool_influencers/">https://old.reddit.com/r/eCommerceSEO/comments/1gn8egy/ultimate_ecommerce_marketing_tool_influencers/</a>, See on <a href="https://news.ycombinator.com/item?id=42093911">Hacker News</a></p>
Couldn't get https://old.reddit.com/r/eCommerceSEO/comments/1gn8egy/ultimate_ecommerce_marketing_tool_influencers/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Somebody moved the UK's oldest satellite in the mid 1970s, but no one knows who (105 pts)]]></title>
            <link>https://www.bbc.co.uk/news/articles/cpwrr58801yo</link>
            <guid>42093851</guid>
            <pubDate>Sat, 09 Nov 2024 11:33:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.co.uk/news/articles/cpwrr58801yo">https://www.bbc.co.uk/news/articles/cpwrr58801yo</a>, See on <a href="https://news.ycombinator.com/item?id=42093851">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="main-content" data-testid="main-content"><article><header data-component="headline-block"><h2 id="main-heading" type="headline" tabindex="-1"><span role="text">Somebody moved UK's oldest satellite, and no-one knows who or why</span></h2></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png.webp 800w, https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png.webp 976w" type="image/webp"><img alt="Artist's rendering of the Skynet-1A satellite" loading="eager" src="https://ichef.bbci.co.uk/ace/standard/2560/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png 800w, https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png 976w" width="2560" height="1508.5714285714287"></picture></span><span role="text"><span>Image source, </span>BBC/Gerry Fletcher</span></p><figcaption><span>Image caption, </span><p>Artwork: The half-tonne Skynet-1A satellite was launched in November 1969</p></figcaption></figure></div><div data-component="text-block"><p><b>Someone moved the UK's oldest satellite and there appears to be no record of exactly who, when or why.</b></p><p>Launched in 1969, just a few months after humans first set foot on the Moon, Skynet-1A was put high above Africa's east coast to relay communications for British forces.</p><p>When the spacecraft ceased working a few years later, gravity might have been expected to pull it even further to the east, out over the Indian Ocean.</p><p>But today, curiously, Skynet-1A is actually half a planet away, in a position 22,369 miles (36,000km) above the Americas.</p></div><div data-component="text-block"><p>Orbital mechanics mean it's unlikely the half-tonne military spacecraft simply drifted to its current location.</p><p>Almost certainly, it was commanded to fire its thrusters in the mid-1970s to take it westwards. The question is who that was and with what authority and purpose?</p><p>It's intriguing that key information about a once vital national security asset can just evaporate. But, fascination aside, you might also reasonably ask why it still matters. After all, we're talking about some discarded space junk from 50 years ago.</p><p>"It's still relevant because whoever did move Skynet-1A did us few favours," says space consultant Dr Stuart Eves.</p><p>"It's now in what we call a 'gravity well' at 105 degrees West longitude, wandering backwards and forwards like a marble at the bottom of a bowl. And unfortunately this brings it close to other satellite traffic on a regular basis. </p><p>"Because it's dead, the risk is it might bump into something, and because it's 'our' satellite we're still responsible for it," he explains.</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>BBC/Gerry Fletcher</span></p><figcaption><span>Image caption, </span><p>If a satellite died at 40E it would drift to the nearest gravity well, which is 75E.</p></figcaption></figure></div><div data-component="text-block"><p>Dr Eves has looked through old satellite catalogues, the National Archives and spoken to satellite experts worldwide, but he can find no clues to the end-of-life behaviour of Britain's oldest spacecraft.</p><p>It might be tempting to reach for a conspiracy theory or two, not least because it's hard to hear the name "Skynet" without thinking of the malevolent, self-aware artificial intelligence (AI) system in The Terminator movie franchise.</p><p>But there's no connection other than the name and, in any case, real life is always more prosaic.</p></div><div data-component="text-block"><p>What we do know is that Skynet-1A was manufactured in the US by the now defunct Philco Ford aerospace company and put in space by a US Air Force Delta rocket.</p><p>"The first Skynet satellite revolutionised UK telecommunications capacity, permitting London to securely communicate with British forces as far away as Singapore. However, from a technological standpoint, Skynet-1A was more American than British since the United States both built and launched it," remarked Dr Aaron Bateman in a recent paper on the history of the Skynet programme, which is now on its fifth generation.</p><p>This view is confirmed by Graham Davison who flew Skynet-1A in the early 70s from its UK operations centre at RAF Oakhanger in Hampshire.</p><p>"The Americans originally controlled the satellite in orbit. They tested all of our software against theirs, before then eventually handing over control to the RAF," the long-retired engineer told me.</p><p>"In essence, there was dual control, but when or why Skynet-1A might have been handed back to the Americans, which seems likely - I'm afraid I can't remember," says Mr Davison, who is now in his 80s.</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>Sunnyvale Heritage Park Museum</span></p><figcaption><span>Image caption, </span><p>Could the command to move Skynet-1A have come from the US Air Force's 'Blue Cube'?</p></figcaption></figure></div><div data-component="text-block"><p>Rachel Hill, a PhD student from University College London, has also been scouring the National Archives.</p><p>Her readings have led her to one very reasonable possibility.</p><p>"A Skynet team from Oakhanger would go to the USAF satellite facility in Sunnyvale (colloquially known as the Blue Cube) and operate Skynet during 'Oakout'. This was when control was temporarily transferred to the US while Oakhanger was down for essential maintenance. Perhaps the move could have happened then?” Ms Hill speculated.</p><p>The official, though incomplete, logs of Skynet-1A’s status suggest final commanding was left in the hands of the Americans when Oakhanger lost sight of the satellite in June 1977.</p><p>But however Skynet-1A then got shifted to its present position, it was ultimately allowed to die in an awkward place when really it should have been put in an "orbital graveyard".</p><p>This refers to a region even higher in the sky where old space junk runs zero risk of running into active telecommunications satellites.</p><p>Graveyarding is now standard practice, but back in the 1970s no-one gave much thought to space sustainability.</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>Astroscale</span></p><figcaption><span>Image caption, </span><p>British engineers are developing technologies to snare defunct satellites in low orbits</p></figcaption></figure></div><div data-component="text-block"><p>Attitudes have since changed because the space domain is getting congested.</p><p>At 105 degrees West longitude, an active satellite might see a piece of junk come within 50km of its position up to four times a day.</p><p>That might sound like they’re nowhere near each other, but at the velocities these defunct objects move it’s starting to get a little too close for comfort.</p><p>The Ministry of Defence said Skynet-1A was constantly monitored by the UK's National Space Operations Centre. Other satellite operators are informed if there's likely to be a particularly close conjunction, in case they need to take evasive action.</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>Northrop Grumman</span></p><figcaption><span>Image caption, </span><p>The Americans have already shown it's possible to grab a high-orbiting satellite</p></figcaption></figure></div><div data-component="text-block"><p>Ultimately, though, the British government may have to think about removing the old satellite to a safer location.</p><p>Technologies are being developed to grab junk left in space.</p><p>Already, the UK Space Agency is funding efforts to do this at lower altitudes, and the Americans and the Chinese have shown it's possible to snare ageing hardware even in the kind of high orbit occupied by Skynet-1A.</p><p>"Pieces of space junk are like ticking time bombs," observed Moriba Jah, a professor of aerospace engineering at the University of Texas at Austin.</p><p>"We need to avoid what I call super-spreader events. When these things explode or something collides with them, it generates thousands of pieces of debris that then become a hazard to something else that we care about."</p></div><section data-component="links-block"><p><h2 type="normal">More on this story</h2></p></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Money was never the end goal – mrdoob – threejs creator (115 pts)]]></title>
            <link>https://twitter.com/mrdoob/status/1854662365163536613</link>
            <guid>42093795</guid>
            <pubDate>Sat, 09 Nov 2024 11:21:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/mrdoob/status/1854662365163536613">https://twitter.com/mrdoob/status/1854662365163536613</a>, See on <a href="https://news.ycombinator.com/item?id=42093795">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Mergiraf: a syntax-aware merge driver for Git (340 pts)]]></title>
            <link>https://mergiraf.org/</link>
            <guid>42093756</guid>
            <pubDate>Sat, 09 Nov 2024 11:06:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mergiraf.org/">https://mergiraf.org/</a>, See on <a href="https://news.ycombinator.com/item?id=42093756">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-wrapper">

            <div id="content" class="page">
                    <main>
                        <p><em>Are you held back by conflicts? Then meet</em></p>

<p>Mergiraf can solve <a href="https://mergiraf.org/conflicts.html">a wide range of Git merge conflicts</a>. That's because it's aware of the trees in your files!
Thanks to <a href="https://mergiraf.org/languages.html">its understanding of your language</a>, it can often reconcile the needs of both sides.</p>
<p>You can <a href="https://mergiraf.org/adding-a-language.html">teach Mergiraf a new language</a> in a completely declarative way. It's a nonviolent animal, so it prefers that over imperatives.</p>
<h2 id="demo">Demo</h2>
<p>Configure Git to use Mergiraf instead of its default merge heuristics. This will enhance <code>git merge</code>, <code>revert</code>, <code>rebase</code>, <code>cherry-pick</code> and more.</p>


<p>You can also keep Git's original behaviour and manually invoke Mergiraf after encountering conflicts.</p>


<div>
<p><img src="https://mergiraf.org/img/scene_1.png" alt="A giraffe observes a fighting pair"></p><p><strong>Figure 1:</strong> Two git users making inadequate use of <code>blame</code>, <code>push</code> and <code>pull</code> to resolve a conflict</p>
</div>
<h2 id="ready-to-give-it-a-try"><a href="#ready-to-give-it-a-try">Ready to give it a try?</a></h2>
<p>Head to the <a href="https://mergiraf.org/installation.html">installation</a> page and start merging nonviolently today!</p>
<h2 id="aspirations"><a href="#aspirations">Aspirations</a></h2>
<p>Mergiraf is designed with your needs in mind. Its goals are:</p>
<h3 id="dont-sweep-conflicts-under-the-rug"><a href="#dont-sweep-conflicts-under-the-rug">Don't sweep conflicts under the rug</a></h3>
<p>Syntax-aware merging heuristics can sometimes be a bit too optimistic in considering a conflict resolved. Mergiraf does its best to err on the side of caution and retain conflict markers in the file when encountering suspicious cases.</p>
<p>If it manages to resolve all conflicts on its own, it encourages you to review its mediation work via the <code>mergiraf review</code> command.
If a merge looks faulty, <a href="https://mergiraf.org/usage.html#reporting-a-bad-merge">you can report it easily</a>.</p>
<h3 id="be-fast-enough-for-interactive-use"><a href="#be-fast-enough-for-interactive-use">Be fast enough for interactive use</a></h3>
<div>
<p><img src="https://mergiraf.org/img/scene_2.png" alt="The giraffe surrounds the pair with its neck and they are surprised by its intervention"></p><p><strong>Figure 2:</strong> Mergiraf offers to mediate</p>
</div>
<p>Did you know that giraffes can run as fast as 60 kilometers per hour? Anyways. The operation of merging diverging versions of files happens routinely when working on a code base, often without you noticing as long as there aren't any conflicts. So Mergiraf tries to be quick so as not to interrupt you in your tasks.</p>
<h3 id="be-open-to-other-methods"><a href="#be-open-to-other-methods">Be open to other methods</a></h3>
<p>In many cases, line-based merging works just great and there is no need for tree-munging business. If a line-based merge is conflict-free, then Mergiraf just returns that merge (which is very quick).
One exception to this rule is <a href="https://mergiraf.org/conflicts.html#line-based-merges">when line-based merging creates duplicate keys</a>. In such a case, Mergiraf does a bit more work to resolve the issue or highlight it to you with conflict markers.</p>


                    </main>

                    <nav aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->

                            <a rel="next prefetch" href="https://mergiraf.org/installation.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i></i>
                            </a>

                        
                    </nav>
                </div>

            <nav aria-label="Page navigation">

                    <a rel="next prefetch" href="https://mergiraf.org/installation.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>
            </nav>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: HTML-to-Markdown – convert entire websites to Markdown with Golang/CLI (265 pts)]]></title>
            <link>https://github.com/JohannesKaufmann/html-to-markdown</link>
            <guid>42093511</guid>
            <pubDate>Sat, 09 Nov 2024 09:48:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/JohannesKaufmann/html-to-markdown">https://github.com/JohannesKaufmann/html-to-markdown</a>, See on <a href="https://news.ycombinator.com/item?id=42093511">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">html-to-markdown</h2><a id="user-content-html-to-markdown" aria-label="Permalink: html-to-markdown" href="#html-to-markdown"></a></p>
<p dir="auto">A robust html-to-markdown converter that transforms HTML (even entire websites) into clean, readable Markdown. It supports complex formatting, customizable options, and plugins for full control over the conversion process.</p>
<p dir="auto">Use the fully extendable <a href="#golang-library">Golang library</a> or a quick <a href="#cli---using-it-on-the-command-line">CLI command</a>. Alternatively, try the <a href="https://html-to-markdown.com/demo" rel="nofollow">Online Demo</a> or <a href="https://html-to-markdown.com/api" rel="nofollow">REST API</a> to see it in action!</p>
<p dir="auto">Here are some <em>cool features</em>:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Bold &amp; Italic:</strong> Supports bold and italic—even within single words.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/.github/images/point_bold_italic.png"><img src="https://github.com/JohannesKaufmann/html-to-markdown/raw/main/.github/images/point_bold_italic.png" alt=""></a></p>
</li>
<li>
<p dir="auto"><strong>List:</strong> Handles ordered and unordered lists with full nesting support.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/.github/images/point_list.png"><img src="https://github.com/JohannesKaufmann/html-to-markdown/raw/main/.github/images/point_list.png" alt=""></a></p>
</li>
<li>
<p dir="auto"><strong>Blockquote:</strong> Blockquotes can include other elements, with seamless support for nested quotes.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/.github/images/point_blockquote.png"><img src="https://github.com/JohannesKaufmann/html-to-markdown/raw/main/.github/images/point_blockquote.png" alt=""></a></p>
</li>
<li>
<p dir="auto"><strong>Inline Code &amp; Code Block:</strong> Correctly handles backticks and multi-line code blocks, preserving code structure.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/.github/images/point_code.png"><img src="https://github.com/JohannesKaufmann/html-to-markdown/raw/main/.github/images/point_code.png" alt=""></a></p>
</li>
<li>
<p dir="auto"><strong>Link &amp; Image:</strong> Properly formats multi-line links, adding escapes for blank lines where needed.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/.github/images/point_link_image.png"><img src="https://github.com/JohannesKaufmann/html-to-markdown/raw/main/.github/images/point_link_image.png" alt=""></a></p>
</li>
<li>
<p dir="auto"><strong>Smart Escaping:</strong> Escapes special characters only when necessary, to avoid accidental Markdown rendering.
🗒️ <a href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/ESCAPING.md">ESCAPING.md</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/.github/images/point_escaping.png"><img src="https://github.com/JohannesKaufmann/html-to-markdown/raw/main/.github/images/point_escaping.png" alt=""></a></p>
</li>
<li>
<p dir="auto"><strong>Remove/Keep HTML:</strong> Choose to strip or retain specific HTML tags for ultimate control over output.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/.github/images/point_wrapper.png"><img src="https://github.com/JohannesKaufmann/html-to-markdown/raw/main/.github/images/point_wrapper.png" alt=""></a></p>
</li>
<li>
<p dir="auto"><strong>Plugins:</strong> Easily extend with plugins. Or create custom ones to enhance functionality.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/.github/images/point_strikethrough.png"><img src="https://github.com/JohannesKaufmann/html-to-markdown/raw/main/.github/images/point_strikethrough.png" alt=""></a></p>
</li>
</ul>
<hr>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Golang Library</h2><a id="user-content-golang-library" aria-label="Permalink: Golang Library" href="#golang-library"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="go get -u github.com/JohannesKaufmann/html-to-markdown/v2"><pre>go get -u github.com/JohannesKaufmann/html-to-markdown/v2</pre></div>
<p dir="auto"><em>Or if you want a specific commit add the suffix <code>/v2@commithash</code></em></p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">This is the documentation for the v2 library. For the old version switch to the <a href="https://github.com/JohannesKaufmann/html-to-markdown/tree/v1">"v1" branch</a>.</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage</h3><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><a href="https://pkg.go.dev/github.com/JohannesKaufmann/html-to-markdown/v2" rel="nofollow"><img src="https://camo.githubusercontent.com/4de9ed495aaf54b8396c04b85bc3edb47d7736aab876b29756705763dc8d1ec1/68747470733a2f2f706b672e676f2e6465762f62616467652f6769746875622e636f6d2f4a6f68616e6e65734b6175666d616e6e2f68746d6c2d746f2d6d61726b646f776e2f76322e737667" alt="Go V2 Reference" data-canonical-src="https://pkg.go.dev/badge/github.com/JohannesKaufmann/html-to-markdown/v2.svg"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="package main

import (
	&quot;fmt&quot;
	&quot;log&quot;

	htmltomarkdown &quot;github.com/JohannesKaufmann/html-to-markdown/v2&quot;
)

func main() {
	input := `<strong>Bold Text</strong>`

	markdown, err := htmltomarkdown.ConvertString(input)
	if err != nil {
		log.Fatal(err)
	}
	fmt.Println(markdown)
	// Output: **Bold Text**
}"><pre><span>package</span> main

<span>import</span> (
	<span>"fmt"</span>
	<span>"log"</span>

	htmltomarkdown <span>"github.com/JohannesKaufmann/html-to-markdown/v2"</span>
)

<span>func</span> <span>main</span>() {
	<span>input</span> <span>:=</span> <span>`&lt;strong&gt;Bold Text&lt;/strong&gt;`</span>

	<span>markdown</span>, <span>err</span> <span>:=</span> <span>htmltomarkdown</span>.<span>ConvertString</span>(<span>input</span>)
	<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
		<span>log</span>.<span>Fatal</span>(<span>err</span>)
	}
	<span>fmt</span>.<span>Println</span>(<span>markdown</span>)
	<span>// Output: **Bold Text**</span>
}</pre></div>
<ul dir="auto">
<li>🧑‍💻 <a href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/examples/basics/main.go">Example code, basics</a></li>
</ul>
<p dir="auto">The function <code>htmltomarkdown.ConvertString()</code> is a <em>small wrapper</em> around <code>converter.NewConverter()</code> and the <em>base</em> and <em>commonmark</em> plugins. If you want more control, use the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="package main

import (
	&quot;fmt&quot;
	&quot;log&quot;

	&quot;github.com/JohannesKaufmann/html-to-markdown/v2/converter&quot;
	&quot;github.com/JohannesKaufmann/html-to-markdown/v2/plugin/base&quot;
	&quot;github.com/JohannesKaufmann/html-to-markdown/v2/plugin/commonmark&quot;
)

func main() {
	input := `<strong>Bold Text</strong>`

	conv := converter.NewConverter(
		converter.WithPlugins(
			base.NewBasePlugin(),
			commonmark.NewCommonmarkPlugin(
				commonmark.WithStrongDelimiter(&quot;__&quot;),
				// ...additional configurations for the plugin
			),
		),
	)

	markdown, err := conv.ConvertString(input)
	if err != nil {
		log.Fatal(err)
	}
	fmt.Println(markdown)
	// Output: __Bold Text__
}"><pre><span>package</span> main

<span>import</span> (
	<span>"fmt"</span>
	<span>"log"</span>

	<span>"github.com/JohannesKaufmann/html-to-markdown/v2/converter"</span>
	<span>"github.com/JohannesKaufmann/html-to-markdown/v2/plugin/base"</span>
	<span>"github.com/JohannesKaufmann/html-to-markdown/v2/plugin/commonmark"</span>
)

<span>func</span> <span>main</span>() {
	<span>input</span> <span>:=</span> <span>`&lt;strong&gt;Bold Text&lt;/strong&gt;`</span>

	<span>conv</span> <span>:=</span> <span>converter</span>.<span>NewConverter</span>(
		<span>converter</span>.<span>WithPlugins</span>(
			<span>base</span>.<span>NewBasePlugin</span>(),
			<span>commonmark</span>.<span>NewCommonmarkPlugin</span>(
				<span>commonmark</span>.<span>WithStrongDelimiter</span>(<span>"__"</span>),
				<span>// ...additional configurations for the plugin</span>
			),
		),
	)

	<span>markdown</span>, <span>err</span> <span>:=</span> <span>conv</span>.<span>ConvertString</span>(<span>input</span>)
	<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
		<span>log</span>.<span>Fatal</span>(<span>err</span>)
	}
	<span>fmt</span>.<span>Println</span>(<span>markdown</span>)
	<span>// Output: __Bold Text__</span>
}</pre></div>
<ul dir="auto">
<li>🧑‍💻 <a href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/examples/options/main.go">Example code, options</a></li>
</ul>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">If you use <code>NewConverter</code> directly make sure to also <strong>register the commonmark and base plugin</strong>.</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Plugins</h3><a id="user-content-plugins" aria-label="Permalink: Plugins" href="#plugins"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Published Plugins</h4><a id="user-content-published-plugins" aria-label="Permalink: Published Plugins" href="#published-plugins"></a></p>
<p dir="auto">These are the plugins located in the <a href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/plugin">plugin folder</a>:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Base</td>
<td>Implements basic shared functionality (e.g. removing nodes)</td>
</tr>
<tr>
<td>Commonmark</td>
<td>Implements Markdown according to the <a href="https://spec.commonmark.org/" rel="nofollow">Commonmark Spec</a></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td>GitHubFlavored</td>
<td><em>planned</em></td>
</tr>
<tr>
<td>TaskListItems</td>
<td><em>planned</em></td>
</tr>
<tr>
<td>Strikethrough</td>
<td>Converts <code>&lt;strike&gt;</code>, <code>&lt;s&gt;</code>, and <code>&lt;del&gt;</code> to the <code>~~</code> syntax.</td>
</tr>
<tr>
<td>Table</td>
<td><em>planned</em></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td>VimeoEmbed</td>
<td><em>planned</em></td>
</tr>
<tr>
<td>YoutubeEmbed</td>
<td><em>planned</em></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td>ConfluenceCodeBlock</td>
<td><em>planned</em></td>
</tr>
<tr>
<td>ConfluenceAttachments</td>
<td><em>planned</em></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">Not all the plugins from v1 are already ported to v2. These will soon be implemented...</p>
</div>
<p dir="auto">These are the plugins in other repositories:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>[Plugin Name](Your Link)</td>
<td>A short description</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h4 tabindex="-1" dir="auto">Writing Plugins</h4><a id="user-content-writing-plugins" aria-label="Permalink: Writing Plugins" href="#writing-plugins"></a></p>
<p dir="auto">You want to write custom logic?</p>
<ol dir="auto">
<li>
<p dir="auto">Write your logic and <strong>register</strong> it.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/.github/images/autocomplete_register.png"><img src="https://github.com/JohannesKaufmann/html-to-markdown/raw/main/.github/images/autocomplete_register.png" alt=""></a></p>
<ul dir="auto">
<li>🧑‍💻 <a href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/examples/register/main.go">Example code, register</a></li>
</ul>
</li>
<li>
<p dir="auto"><em>Optional:</em> Package your logic into a <strong>plugin</strong> and publish it.</p>
<ul dir="auto">
<li>🗒️ <a href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/WRITING_PLUGINS.md">WRITING_PLUGINS.md</a></li>
</ul>
</li>
</ol>
<hr>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">CLI - Using it on the command line</h2><a id="user-content-cli---using-it-on-the-command-line" aria-label="Permalink: CLI - Using it on the command line" href="#cli---using-it-on-the-command-line"></a></p>
<p dir="auto">Using the Golang library provides the most customization, while the CLI is the simplest way to get started.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation-1" aria-label="Permalink: Installation" href="#installation-1"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Homebrew Tap</h4><a id="user-content-homebrew-tap" aria-label="Permalink: Homebrew Tap" href="#homebrew-tap"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install JohannesKaufmann/tap/html2markdown"><pre>brew install JohannesKaufmann/tap/html2markdown</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Manually</h4><a id="user-content-manually" aria-label="Permalink: Manually" href="#manually"></a></p>
<p dir="auto">Download the pre-compiled binaries from the <a href="https://github.com/JohannesKaufmann/html-to-markdown/releases">releases page</a> and copy them to the desired location.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Version</h3><a id="user-content-version" aria-label="Permalink: Version" href="#version"></a></p>

<div dir="auto"><p dir="auto">Note</p><p dir="auto">Make sure that <code>--version</code> prints <code>2.X.X</code> as there is a different CLI for V2 of the converter.</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage</h3><a id="user-content-usage-1" aria-label="Permalink: Usage" href="#usage-1"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo &quot;<strong>important</strong>&quot; | html2markdown

**important**"><pre>$ <span>echo</span> <span><span>"</span>&lt;strong&gt;important&lt;/strong&gt;<span>"</span></span> <span>|</span> html2markdown

<span>**</span>important<span>**</span></pre></div>
<div data-snippet-clipboard-copy-content="$ curl --no-progress-meter http://example.com | html2markdown

# Example Domain

This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission.

[More information...](https://www.iana.org/domains/example)"><pre lang="text"><code>$ curl --no-progress-meter http://example.com | html2markdown

# Example Domain

This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission.

[More information...](https://www.iana.org/domains/example)
</code></pre></div>
<p dir="auto"><em>(The cli does not support every option yet. Over time more customization will be added)</em></p>
<hr>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Extending with Plugins</h3><a id="user-content-extending-with-plugins" aria-label="Permalink: Extending with Plugins" href="#extending-with-plugins"></a></p>
<ul dir="auto">
<li>
<p dir="auto">Need your own logic? Write your own code and then <strong>register</strong> it.</p>
<ul dir="auto">
<li>
<p dir="auto">Don't like the <strong>defaults</strong> that the library uses? You can use <code>PriorityEarly</code> to run you logic <em>earlier</em> than others.</p>
</li>
<li>
<p dir="auto">🧑‍💻 <a href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/examples/register/main.go">Example code, register</a></p>
</li>
</ul>
</li>
<li>
<p dir="auto">If you believe that you logic could also benefit others, you can package it up into a <strong>plugin</strong>.</p>
<ul dir="auto">
<li>🗒️ <a href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/WRITING_PLUGINS.md">WRITING_PLUGINS.md</a></li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bugs</h3><a id="user-content-bugs" aria-label="Permalink: Bugs" href="#bugs"></a></p>
<p dir="auto">You found a bug?</p>
<p dir="auto"><a href="https://github.com/JohannesKaufmann/html-to-markdown/issues/new/choose">Open an issue</a> with the HTML snippet that does not produce the expected results. Please, please, plase <em>submit the HTML snippet</em> that caused the problem. Otherwise it is very difficult to reproduce and fix...</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Security</h3><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<p dir="auto">This library produces markdown that is readable and can be changed by humans.</p>
<p dir="auto">Once you convert this markdown back to HTML (e.g. using <a href="https://github.com/yuin/goldmark">goldmark</a> or <a href="https://github.com/russross/blackfriday">blackfriday</a>) you need to be careful of malicious content.</p>
<p dir="auto">This library does NOT sanitize untrusted content. Use an HTML sanitizer such as <a href="https://github.com/microcosm-cc/bluemonday">bluemonday</a> before displaying the HTML in the browser.</p>
<p dir="auto">🗒️ <a href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/SECURITY.md">SECURITY.md</a> if you find a security vulnerability</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Goroutines</h3><a id="user-content-goroutines" aria-label="Permalink: Goroutines" href="#goroutines"></a></p>
<p dir="auto">You can use the <code>Converter</code> from (multiple) goroutines. Internally a mutex is used &amp; there is a test to verify that behaviour.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Escaping &amp; Backslash</h3><a id="user-content-escaping--backslash" aria-label="Permalink: Escaping &amp; Backslash" href="#escaping--backslash"></a></p>
<p dir="auto">Some characters have a special meaning in markdown (e.g. "*" for emphasis). The backslash <code>\</code> character is used to "escape" those characters. That is perfectly safe and won't be displayed in the final render.</p>
<p dir="auto">🗒️ <a href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/ESCAPING.md">ESCAPING.md</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contributing</h3><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">You want to contribute? Thats great to hear! There are many ways to help:</p>
<p dir="auto">Helping to answer questions, triaging issues, writing documentation, writing code, ...</p>
<p dir="auto">If you want to make a code change: Please first discuss the change you wish to make, by opening an issue. I'm also happy to guide you to where a change is most likely needed. There are also extensive tests (see below) so you can freely experiment 🧑‍🔬</p>
<p dir="auto"><em>Note: The outside API should not change because of backwards compatibility...</em></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Testing</h3><a id="user-content-testing" aria-label="Permalink: Testing" href="#testing"></a></p>
<p dir="auto">You don't have to be afraid of breaking the converter, since there are many "Golden File" tests:</p>
<p dir="auto">Add your problematic HTML snippet to one of the <code>.in.html</code> files in the <code>testdata</code> folders. Then run <code>go test -update</code> and have a look at which <code>.out.md</code> files changed in GIT.</p>
<p dir="auto">You can now change the internal logic and inspect what impact your change has by running <code>go test -update</code> again.</p>
<p dir="auto"><em>Note: Before submitting your change as a PR, make sure that you run those tests and check the files into GIT...</em></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">License</h3><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Unless otherwise specified, the project is licensed under the terms of the MIT license.</p>
<p dir="auto">🗒️ <a href="https://github.com/JohannesKaufmann/html-to-markdown/blob/main/LICENSE">LICENSE</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SVDQuant: 4-Bit Quantization Powers 12B Flux on a 16GB 4090 GPU with 3x Speedup (159 pts)]]></title>
            <link>https://hanlab.mit.edu/blog/svdquant</link>
            <guid>42093112</guid>
            <pubDate>Sat, 09 Nov 2024 07:46:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hanlab.mit.edu/blog/svdquant">https://hanlab.mit.edu/blog/svdquant</a>, See on <a href="https://news.ycombinator.com/item?id=42093112">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Muyang Li*, Yujun Lin*, Zhekai Zhang*, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, Song Han</p><p>November 7, 2024</p></div><p>A new post-training training quantization paradigm for diffusion models, which quantize both the weights and activations of FLUX.1 to 4 bits, achieving 3.5× memory and 8.7× latency reduction on a 16GB laptop 4090 GPU.</p><div><p>
  <img src="https://github.com/mit-han-lab/nunchaku/blob/main/assets/demo.gif?raw=true" width="70%">
</p><p>Check our interactive demo at <a href="https://svdquant.mit.edu/">https://svdquant.mit.edu</a>! Our quantization library is at <a href="https://github.com/mit-han-lab/deepcompressor">github.com/mit-han-lab/deepcompressor</a> and inference engine is at <a href="https://github.com/mit-han-lab/nunchaku">github.com/mit-han-lab/nunchaku</a>. Our paper is at <a href="http://arxiv.org/abs/2411.05007">this link</a>.</p><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/672d1bcef3c3ec127e8078fd_672d1b2115081c1e8ac82ea9_teaser.jpeg" loading="lazy" alt=""></p></figure><p>SVDQuant is a post-training quantization technique for 4-bit weights and activations that well maintains visual fidelity. On 12B FLUX.1-dev, it achieves 3.6× memory reduction compared to the BF16 model. By eliminating CPU offloading, it offers 8.7× speedup over the 16-bit model when on a 16GB laptop 4090 GPU, 3× faster than the NF4 W4A16 baseline. On PixArt-∑, it demonstrates significantly superior visual quality over other W4A4 or even W4A8 baselines.</p><h2>Background</h2><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/672a9dcbacb7d59271c5662e_672a9a6c364fccc4e4443187_trend.jpeg" loading="lazy" alt=""></p><figcaption>Computation <em>v.s.</em> parameters for LLMs and diffusion models.</figcaption></figure><p>Diffusion models are revolutionizing AI with their ability to generate high-quality images from text prompts. To improve image quality and improve the alignment between text and image, researchers are scaling up these models. As shown in the right figure, while Stable Diffusion 1.4 has 800 million parameters, newer models like AuraFlow and FLUX.1 reach billions, delivering more refined and detailed outputs. However, scaling brings challenges: these models become computationally heavy, demanding high memory and longer processing times, making them prohibitive for real-time applications.</p><p>As Moore's law slows down, hardware vendors are turning to low-precision inference, such as NVIDIA's new 4-bit floating point (FP4) precision in Blackwell. In large language models (LLMs), quantization has helped reduce model sizes and speed up inference, primarily by addressing latency from loading model weights. Diffusion models, however, are computationally bound, even for single batches, so quantizing weights alone yields limited gains. To achieve measured speedups, both weights and activations must be quantized to the same bit width; otherwise, the lower precision is upcast during computation, negating any performance benefits.</p><p>In this blog, we introduce SVDQuant to quantize both the weights and activations of diffusion models to 4 bits. At such an aggressive level, conventional post-training methods fall short. Unlike smoothing, which redistributes outliers, SVDQuant absorbs them through a high-precision low-rank branch, significantly preserving image quality. Visual examples demonstrate its effectiveness. See the above figure for some visual examples.</p><h2>SVDQuant: Absorbing Outliers via Low-Rank Branch</h2><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/672a66701e9812afb49e0cfa_672a651614cb88ed4fd9dc29_intuition-animate.gif" loading="lazy" alt=""></p></figure><div>


    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LaTeX Rendering Example</title>
    


    <p>
The key idea behind SVDQuant is to introduce an additional low-rank branch that can absorb quantization difficulties in both weights and activations. As shown in the above animation, originally, both the activation \( \boldsymbol{X} \) and weights \( \boldsymbol{W} \) contain massive outliers, making 4-bit quantization challenging. We can first aggregate the outliers by migrating them from activations to weights via smoothing, resulting in the updated activation \( \hat{\boldsymbol{X}} \) and weights \( \hat{\boldsymbol{W}} \). While \( \hat{\boldsymbol{X}} \) becomes easier to quantize, \( \hat{\boldsymbol{W}} \) now becomes more difficult. At the last stage, SVDQuant further decomposes \( \hat{\boldsymbol{W}} \) into a low-rank component \( \boldsymbol{L}_1 \boldsymbol{L}_2 \) and a residual \( \hat{\boldsymbol{W}} - \boldsymbol{L}_1 \boldsymbol{L}_2 \) with Singular Value Decomposition (SVD). As the singular value distribution of \( \hat{\boldsymbol{W}} \) is highly imbalanced, with only the first several values being significantly larger, removing these dominant values can dramatically reduce \( \hat{\boldsymbol{W}} \)’s magnitude and outliers, as suggested by <a href="https://en.wikipedia.org/wiki/Low-rank_approximation">Eckart-Young-Mirsky theorem</a>. Thus, the quantization difficulty is alleviated by the low-rank branch, which runs at 16-bit precision. The below figure illustrates an example value distribution of the input activations and weights in PixArt-∑.
    </p>

</div><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/672abca95c5f67bff08b7664_672abca0fde8974bb6d63e6d_distribution.jpeg" loading="lazy" alt=""></p><figcaption>Example value distribution of inputs and weights in PixArt-∑.</figcaption></figure><h2>Nunchaku: Fusing Low-Rank and Low-Bit Branch Kernels</h2><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/672d1fccad9d41d739c84c2c_672a6ac09cf7fc46071c3fb8_672a6a991adf3346a0ee4dbb_engine.jpeg" loading="lazy" alt=""></p></figure><p>Although the low-rank branch adds only minor computational costs on paper, running it separately can lead to significant latency overhead—about 50% of the 4-bit branch's latency, as shown in figure (a). This is because, despite reduced computation costs with a small rank, the data size of input and output activations remains the same, shifting the bottleneck to memory access instead of computation.</p><p>To address this, we co-designed our inference engine, Nunchaku, with the SVDQuant algorithm. Specifically, we noted that the down projection in the low-rank branch uses the same input as the quantization kernel in the low-bit branch, and the up projection shares the same output as the 4-bit computation kernel, as shown in figure (b). By fusing the down projection with the quantization kernel and the up projection with the 4-bit computation kernel, the low-rank branch can now share activations with the low-bit branch. This eliminates extra memory access and cuts the number of kernel calls in half. As a result, the low-rank branch now adds only 5–10% additional latency, making its cost almost negligible.</p><h2>Performance</h2><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/672d1bcdf3c3ec127e8078f3_672d1b5772f1247fa24e231d_efficiency.jpeg" loading="lazy" alt=""></p></figure><p>SVDQuant reduces the model size of the 12B FLUX.1 by 3.6×. Additionally, Nunchaku further cuts memory usage of the 16-bit model by 3.5× and delivers 3.0× speedups over the NF4 W4A16 baseline on both the desktop and laptop NVIDIA RTX 4090 GPUs. Remarkably, on laptop 4090, it achieves in total 10.1× speedup by eliminating CPU offloading. </p><h2>Quality</h2><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/672ac9098069a7c6f2baedbd_672ac78c0b59dc8da3c15987_visual.jpeg" loading="lazy" alt=""></p></figure><p>On FLUX.1 models, our 4-bit models outperform the NF4 W4A16 baselines, demonstrating superior text alignment and closer similarity to the 16-bit models. For instance, NF4 misinterprets "dinosaur style," generating a real dinosaur. On PixArt-∑ and SDXL-Turbo, our 4-bit results demonstrate noticeably better visual quality than ViDiT-Q's and MixDQ's W4A8 results.</p><p>‍</p><h2>Integrate with LoRA</h2><figure><p><img src="https://cdn.prod.website-files.com/64f4e81394e25710d22d042e/672a6c74c47e63fff4b5b216_672a6c25ef1acf8704ef65af_lora.jpeg" loading="lazy" alt=""></p></figure><p>Traditional quantization methods require fusing LoRA branches and then requantizing the model when integrating LoRAs. Our SVDQuant, however, avoids redundant memory access, making it possible to add a separate LoRA branch directly. The figure above shows visual examples of our INT4 FLUX.1-dev model with LoRAs applied in five distinct styles—<a href="https://huggingface.co/XLabs-AI/flux-RealismLora">Realism</a>, <a href="https://huggingface.co/aleksa-codes/flux-ghibsky-illustration">Ghibsky Illustration</a>, <a href="https://huggingface.co/alvdansen/sonny-anime-fixed">Anime</a>, <a href="https://huggingface.co/Shakker-Labs/FLUX.1-dev-LoRA-Children-Simple-Sketch">Children Sketch</a>, and <a href="https://huggingface.co/linoyts/yarn_art_Flux_LoRA">Yarn Art</a>. Our INT4 model adapts seamlessly to each style, maintaining the image quality of the original 16-bit version.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Texture-Less Text Rendering (186 pts)]]></title>
            <link>https://poniesandlight.co.uk/reflect/debug_print_text/</link>
            <guid>42093037</guid>
            <pubDate>Sat, 09 Nov 2024 07:27:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://poniesandlight.co.uk/reflect/debug_print_text/">https://poniesandlight.co.uk/reflect/debug_print_text/</a>, See on <a href="https://news.ycombinator.com/item?id=42093037">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
  	
<figure>
<img src="https://poniesandlight.co.uk/img/reflect/debug_print_text/look_ma.png" loading="lazy">
</figure>
<div>
<p>Sometimes, all you want is to quickly print some text into a Renderpass. But <a href="https://stackoverflow.com/questions/22080881/how-to-render-text-in-modern-opengl-with-glsl">traditionally, drawing text</a> requires you first to render all possible glyphs of a font into an atlas, to bind this atlas as a texture, and then to render glyphs one by one by drawing triangles on screen, with every triangle picking the correct glyph from the font atlas texture.</p>
<p>This is how <a href="https://github.com/ocornut/imgui">imgui</a> does it, how anyone using <a href="https://github.com/nothings/stb/blob/master/stb_truetype.h">stb_truetype</a> does it, and it’s delightfully close to how <a href="https://en.wikipedia.org/wiki/Typesetting">type setting</a> used to be done ages bygone on physical letterpresses.</p>
<figure>
<img loading="lazy" src="https://poniesandlight.co.uk/img/reflect/debug_print_text/upper_case_and_lower_case.jpg" alt="Composing cases of an early letterpress" title="Composing cases of an early letterpress">
<figcaption>
    Case in point: Some ancient Letterpress Type Cases (public domain) – <a href="https://babel.hathitrust.org/846c4e7f-3fa3-4275-b53c-3183150e9481">source</a><p>In case you wonder – <a href="https://www.mcgill.ca/oss/article/did-you-know-history/why-it-called-upper-and-lower-case">yes</a></p><p>That’s enough (ed).
</p></figcaption>
</figure>
<p>Quaint, correct, but also quite cumbersome.</p>
<p>What if – for quick and dirty debug messaging – there was a simpler way to do this?</p>
<p>Here, I’ll describe a technique for <em>texture-less</em> rendering of debug text. On top of it all, it draws all the text in a single draw call.</p>
<h2 id="the-font-pixels-sans-texture">The Font: Pixels Sans Texture&nbsp;<a href="#the-font-pixels-sans-texture"></a></h2>
<p>How can we get rid of the font atlas texture? We’d need to store a font atlas or something similar <em>directly inside</em> the fragment shader. Obviously, we can’t store <em>bitmaps</em> inside our shaders, but we can store integer constants, which, if you squint hard enough, are nothing but maps of bits. Can we pretend that an integer is a bitmap?</p>
<figure>
<img loading="lazy" src="https://poniesandlight.co.uk/img/reflect/debug_print_text/0x42_as_bitmap.svg" alt="The integer 0x42 as a bitmap" title="The integer 0x42 as a bitmap">
<figcaption>
    An 8 bit integer as a bitmap. The value 66, or <code>0x42</code> in hex notation, translates to <code>0b01000010</code> in binary notation. If we assume that every bit is a pixel  on/off value, we get something like this.
</figcaption>
</figure>
<p>We can draw this to the screen using a GLSL fragment shader by mapping a fragment’s <code>xy</code> position to the bit that is covered by it in the “bitmap”. If the bit is set, we draw in the foreground colour. If the bit is not set, we draw in the background colour.</p>

</div>
<div>
    
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span><span>9
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="glsl"><span><span><span>uint</span> <span>bitmap</span> <span>=</span> <span>0x42</span><span>;</span>
</span></span><span><span><span>vec4</span> <span>col_fg</span> <span>=</span> <span>vec4</span><span>(</span><span>1</span><span>,</span><span>1</span><span>,</span><span>1</span><span>,</span><span>1</span><span>);</span>
</span></span><span><span><span>vec4</span> <span>col_bg</span> <span>=</span> <span>vec4</span><span>(</span><span>0</span><span>,</span><span>0</span><span>,</span><span>0</span><span>,</span><span>1</span><span>);</span>
</span></span><span><span>
</span></span><span><span><span>// vec2 uv is the normalized texture coordinate for the fragment</span>
</span></span><span><span><span>// with the origin top-left</span>
</span></span><span><span><span>uint</span> <span>which_bit</span> <span>=</span> <span>7</span> <span>-</span> <span>min</span><span>(</span><span>7</span><span>,</span><span>floor</span><span>(</span><span>uv</span><span>.</span><span>x</span> <span>*</span> <span>8</span><span>));</span> 
</span></span><span><span>
</span></span><span><span><span>out_color</span> <span>=</span> <span>mix</span><span>(</span><span>col_bg</span><span>,</span> <span>col_fg</span><span>,</span> <span>(</span><span>bitmap</span> <span>&gt;&gt;</span> <span>which_bit</span><span>)</span> <span>&amp;</span> <span>1</span><span>);</span></span></span></code></pre></td></tr></tbody></table>
</div><p><span>glsl</span>
</p></div><p>Now, one byte will only draw one line of pixels for us. If we want to draw nicer glyphs, we will need more bytes. If we allowed <span>16 bytes</span><span> (that’s 16 lines)</span> per glyph, this would give us an 8x16 pixel canvas to work with. A single <code>uvec4</code>, which is a built-in type in GLSL, covers exactly the correct amount of bytes that we need.</p>
<figure>
<img loading="lazy" src="https://poniesandlight.co.uk/img/reflect/debug_print_text/character_A.svg" alt="The Glyph A encoded as an uvec4" title="The Glyph A encoded as an uvec4">
<figcaption>
    The character <code>A</code> encoded in 16 bytes, stored as an <code>uvec4</code>, that’s 4 uints with each 4 bytes.
</figcaption>
</figure>
<p>16 bytes per glyph seems small enough; It should allow us to encode the complete <a href="https://en.wikipedia.org/wiki/ASCII">ASCII</a> subset of 96 printable glyphs in all of 1536 bytes of shader <span>memory</span><span>. (We could probably compress this further, but we would lose simplicity and/or readability)</span>.</p>
<h2 id="where-do-we-get-the-bitmaps-from">Where do we get the bitmaps from?&nbsp;<a href="#where-do-we-get-the-bitmaps-from"></a></h2>
<p>Conveniently, the encoding of a font into bitmaps such as described above is very much the definition of the venerable <a href="https://en.wikipedia.org/wiki/PC_Screen_Font">PSF1 format</a>, give or take a few header bytes. We can therefore harvest the glyph pixels from any PSF1 terminal font by opening it in a hex editor such as <a href="https://imhex.werwolv.net/">ImHex</a>, travelling past the header (4 bytes) and the first section of non-printable glyphs (512 bytes), and then exporting the raw data for the next 96 glyphs (1536 bytes) by using “Copy as → C Array”.</p>
<figure>
<img loading="lazy" src="https://poniesandlight.co.uk/img/reflect/debug_print_text/imhex_screenshot.png" alt="A Screenshot of ImHex" title="A Screenshot of ImHex">
<figcaption>
    The ImHex hex editor has a really useful feature: you can copy binary data as a c-array.
</figcaption>
</figure>
<p>This will give us a nicely formatted array of chars, which we can easily edit into an array of <code>uint</code>s, which we then group into <code>uvec4</code>s. We need to remember that just concatenating the raw chars into <code>uint</code>s flips the endianness of our <code>uint</code>s, but we can always flip this back when we sample the font data…</p>
<p>Once we’re done, this is how our font bitmap data table looks like in the fragment shader:</p>

</div>
<div>
    
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="glsl"><span><span><span>const</span> <span>uvec4</span> <span>font_data</span><span>[</span><span>96</span><span>]</span> <span>=</span> <span>{</span>
</span></span><span><span>  <span>{</span> <span>0x00000000</span><span>,</span> <span>0x00000000</span><span>,</span> <span>0x00000000</span><span>,</span> <span>0x00000000</span> <span>},</span> <span>// 0x1e: SPACE</span>
</span></span><span><span>  <span>{</span> <span>0x00000000</span><span>,</span> <span>0x08080808</span><span>,</span> <span>0x08080800</span><span>,</span> <span>0x08080000</span> <span>},</span> <span>// 0x21: '!'</span>
</span></span><span><span>  <span>{</span> <span>0x00002222</span><span>,</span> <span>0x22220000</span><span>,</span> <span>0x00000000</span><span>,</span> <span>0x00000000</span> <span>},</span> <span>// 0x22: '\'</span>
</span></span><span><span>  <span>{</span> <span>0x00000000</span><span>,</span> <span>0x1212127E</span><span>,</span> <span>0x24247E48</span><span>,</span> <span>0x48480000</span> <span>},</span> <span>// 0x23: '#'</span>
</span></span><span><span>  <span>// ... etc ... </span>
</span></span><span><span>
</span></span><span><span>  <span>{</span> <span>0x00000808</span><span>,</span> <span>0x08080808</span><span>,</span> <span>0x08080808</span><span>,</span> <span>0x08080808</span> <span>},</span> <span>// 0x7C: '|'</span>
</span></span><span><span>  <span>{</span> <span>0x00000030</span><span>,</span> <span>0x08081010</span><span>,</span> <span>0x08040810</span><span>,</span> <span>0x10080830</span> <span>},</span> <span>// 0x7D: '}'</span>
</span></span><span><span>  <span>{</span> <span>0x00000031</span><span>,</span> <span>0x49460000</span><span>,</span> <span>0x00000000</span><span>,</span> <span>0x00000000</span> <span>},</span> <span>// 0x7E: '~'</span>
</span></span><span><span>  <span>{</span> <span>0xFC1B26EF</span><span>,</span> <span>0xC8E04320</span><span>,</span> <span>0x8958625E</span><span>,</span> <span>0x79BAEE7E</span> <span>},</span> <span>// 0x7F: BACKSPACE</span>
</span></span><span><span><span>};</span>                                              </span></span></code></pre></td></tr></tbody></table>
</div><p><span>glsl</span>
</p></div><p>I say table, because the <code>font_data</code> array now stores the bitmaps for 96 character glyphs, indexed by their ASCII value (minus 0x20). This table therefore covers the full printable ASCII range from <code>0x20</code> <kbd>SPACE</kbd> to <code>0x7F</code> <kbd>BACKSPACE</kbd> (inclusive), but in the snippet above I’m showing only 8 of them, to save space.</p>
<p>So far, all this is just so that we don’t have to bind a texture when drawing our text. But how to draw the text itself?</p>
<figure>
<img loading="lazy" src="https://poniesandlight.co.uk/img/reflect/debug_print_text/screenshot_small.png" alt="Text output" title="Text output">
<figcaption>
    This is what we want to print at the end of this process
</figcaption>
</figure>
<h2 id="one-draw-call-thats-all">One Draw Call, That’s All.&nbsp;<a href="#one-draw-call-thats-all"></a></h2>
<p>We’re going to use a single <strong>instanced</strong> draw call.</p>

</div>
<div>
    
<p>With instanced drawing, we don’t have to repeatedly issue draw <em>instructions</em>, since we encode the logic into per-instance data. One draw call contains everything we need, provided it uses two attribute streams. The fist stream, per-draw, has just the necessary information to draw a generic quad. And the second stream, per-instance, packs the two pieces of information that change with every instance of such a quad: First, a position offset, so that we know <em>where in screen space</em> to draw the quad. And second, of course, the text that we want to print.</p>
<p>For the position offset we can use one float each for x and y, which leaves two floats for this particular attribute binding <span>unused</span><span> (attribute bindings in GLSL/Vulkan are at minimum the equivalent of 4 floats wide)</span>. We have more than enough space to use one extra float to pack in a font scale parameter, if we like.</p>

</div>
<div>
  
<p>For the text that we want to print, we have a similarly wasteful situation – the smallest basic vertex attribute data type <a href="https://docs.vulkan.org/spec/latest/chapters/fxvertex.html#fxvertex-attrib-location">is usually 32bit wide</a>, and so it makes sense to make best use of this and pack at least 4 characters at a time. If we do this, we must make sure that the message that we want to print has a length divisible by 4. If it was shorter, we need to fill up the difference with zero byte (<code>\0</code>) characters. Conveniently, the zero byte is also used to signal the end of a c-string.</p>
<p>Our per-instance data looks like this:</p>

</div>
<div>
    
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="cpp"><span><span><span>struct</span> <span>word_data</span> <span>{</span>
</span></span><span><span>  <span>float</span>          <span>pos_and_scale</span><span>[</span> <span>3</span> <span>];</span> <span>// xy position + scale 
</span></span></span><span><span><span></span>  <span>uint32_t</span>       <span>word</span><span>;</span>               <span>// four characters that we want to print
</span></span></span><span><span><span></span><span>};</span></span></span></code></pre></td></tr></tbody></table>
</div><p><span>cpp</span>
</p></div><p>It’s the application’s responsibility to split up the message into chunks of 4 characters, to convert these four characters into an <code>unit32_t</code>, and to store it into a <code>word_data</code> struct together with the position offset for where on screen to render these four characters. Once a <code>word_data</code> is filled, we append it into an array where we accumulate all the data for our text draw calls. Once we are ready to draw, we can then bind this array as a per-instance binding to our debug text drawing pipeline, and draw all text with a single <span>instanced draw call</span><span>, with the number of instances being the number of quads that we want to draw</span>.</p>
<p>More interesting things happen in the vertex and fragment shader of the debug text drawing pipeline.</p>
<h2 id="vertex-shader">Vertex Shader&nbsp;<a href="#vertex-shader"></a></h2>
<p>Our vertex shader produces three outputs.</p>
<p>First, it writes to <code>gl_Position</code> to place the vertices for our triangles on the screen. This operates in <span> NDC </span><span> = Normalised Device </span> “screen space” Coordinates. We calculate an offset for each vertex using the per-instance <code>pos_and_scale</code> attribute data.</p>
<p>The second output of the vertex shader is the word that we want to render: We just pass though the attribute <code>uint</code> as an output to the fragment shader – but we make sure to use the <code>flat</code> qualifier so that it does not get interpolated.</p>
<p>And then, the vertex shader synthesizes texture coordinates (via <code>gl_VertexIndex</code>). It does so pretty cleverly:</p>
<ul>
<li><code>12 &gt;&gt; gl_VertexIndex &amp; 1</code> will give a sequence <code>0, 0, 1, 1</code>,</li>
<li><code> 9 &gt;&gt; gl_VertexIndex &amp; 1</code> will give a sequence <code>1, 0, 0, 1</code>,</li>
</ul>
<p>This creates a sequence of uv coordinates <code>(0,1), (0,0), (1,0), (1,1)</code> in a branchless way.</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span><span>35
</span><span>36
</span><span>37
</span><span>38
</span><span>39
</span><span>40
</span><span>41
</span><span>42
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="glsl"><span><span><span>#version 450 core</span>
</span></span><span><span>
</span></span><span><span><span>#extension GL_ARB_separate_shader_objects : enable</span>
</span></span><span><span><span>#extension GL_ARB_shading_language_420pack : enable</span>
</span></span><span><span>
</span></span><span><span><span>// Inputs </span>
</span></span><span><span><span>// Uniforms - Push Constants</span>
</span></span><span><span><span>layout</span> <span>(</span><span>push_constant</span><span>)</span> <span>uniform</span> <span>Params</span>
</span></span><span><span><span>{</span>
</span></span><span><span>	<span>vec2</span> <span>u_resolution</span><span>;</span> <span>// screen canvas resolution in physical pixels</span>
</span></span><span><span><span>};</span>
</span></span><span><span>
</span></span><span><span><span>// Input Attributes</span>
</span></span><span><span><span>layout</span> <span>(</span><span>location</span> <span>=</span> <span>0</span><span>)</span> <span>in</span> <span>vec3</span> <span>pos</span><span>;</span>      <span>// "vanilla" vertex position attribute - given in pixels</span>
</span></span><span><span><span>layout</span> <span>(</span><span>location</span> <span>=</span> <span>1</span><span>)</span> <span>in</span> <span>uint</span> <span>word</span><span>;</span>     <span>// per-instance: four chars</span>
</span></span><span><span><span>layout</span> <span>(</span><span>location</span> <span>=</span> <span>2</span><span>)</span> <span>in</span> <span>vec3</span> <span>word_pos</span><span>;</span> <span>// per-instance: where to place the word in screen space</span>
</span></span><span><span><span>layout</span> <span>(</span><span>location</span> <span>=</span> <span>3</span><span>)</span> <span>in</span> <span>vec4</span> <span>col_fg</span><span>;</span>   <span>// per-instance: foreground colour</span>
</span></span><span><span><span>layout</span> <span>(</span><span>location</span> <span>=</span> <span>4</span><span>)</span> <span>in</span> <span>vec4</span> <span>col_bg</span><span>;</span>   <span>// per-instance: background colour</span>
</span></span><span><span>
</span></span><span><span><span>// Vertex Outputs </span>
</span></span><span><span><span>struct</span> <span>per_word_data</span> <span>{</span>
</span></span><span><span>	<span>uint</span> <span>msg</span><span>;</span>
</span></span><span><span>	<span>vec4</span> <span>fg_colour</span><span>;</span>
</span></span><span><span>	<span>vec4</span> <span>bg_colour</span><span>;</span>
</span></span><span><span><span>};</span>
</span></span><span><span>
</span></span><span><span><span>out</span> <span>gl_PerVertex</span> <span>{</span> <span>vec4</span> <span>gl_Position</span><span>;</span> <span>};</span>
</span></span><span><span><span>layout</span> <span>(</span><span>location</span> <span>=</span> <span>0</span><span>)</span> <span>out</span> <span>vec2</span> <span>outTexCoord</span><span>;</span>
</span></span><span><span><span>layout</span> <span>(</span><span>location</span> <span>=</span> <span>1</span><span>)</span> <span>flat</span> <span>out</span> <span>per_word_data</span> <span>outMsg</span><span>;</span>
</span></span><span><span>
</span></span><span><span><span>void</span> <span>main</span><span>()</span> 
</span></span><span><span><span>{</span>
</span></span><span><span>	<span>outMsg</span><span>.</span><span>msg</span> <span>=</span> <span>word</span><span>;</span>
</span></span><span><span>	<span>outMsg</span><span>.</span><span>fg_colour</span> <span>=</span> <span>col_fg</span><span>;</span>
</span></span><span><span>	<span>outMsg</span><span>.</span><span>bg_colour</span> <span>=</span> <span>col_bg</span><span>;</span>
</span></span><span><span>
</span></span><span><span>	<span>vec2</span> <span>scale_factor</span> <span>=</span> <span>vec2</span><span>(</span><span>1.</span><span>,</span><span>2.</span><span>)</span><span>/</span><span>(</span><span>u_resolution</span><span>);</span>
</span></span><span><span>	<span>outTexCoord</span> <span>=</span> <span>vec2</span><span>((</span><span>12</span> <span>&gt;&gt;</span> <span>gl_VertexIndex</span><span>)</span> <span>&amp;</span><span>1</span><span>,</span> <span>(</span><span>9</span> <span>&gt;&gt;</span> <span>gl_VertexIndex</span> <span>)</span> <span>&amp;</span><span>1</span><span>);</span>
</span></span><span><span>	<span>vec4</span> <span>position</span> <span>=</span> <span>vec4</span><span>(</span><span>0</span><span>,</span><span>0</span><span>,</span><span>0</span><span>,</span><span>1</span><span>);</span>
</span></span><span><span>	<span>position</span><span>.</span><span>xy</span> <span>=</span> <span>vec2</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>-</span><span>1</span><span>)</span> <span>+</span> <span>(</span><span>pos</span><span>.</span><span>xy</span> <span>*</span> <span>word_pos</span><span>.</span><span>z</span> <span>+</span> <span>word_pos</span><span>.</span><span>xy</span><span>)</span> <span>*</span> <span>scale_factor</span><span>;</span>
</span></span><span><span>	<span>gl_Position</span> <span>=</span> <span>position</span><span>;</span>
</span></span><span><span><span>}</span></span></span></code></pre></td></tr></tbody></table>
</div><p><span>glsl</span>
</p></div><p>If we at this point visualise just the output of the vertex shader, we will get something like this:</p>
<figure>
<img loading="lazy" src="https://poniesandlight.co.uk/img/reflect/debug_print_text/screenshot_small_uv_continuous.png" alt="Quad visualisation with uv coords" title="Quad visualisation with uv coords">
<figcaption>
    Visualisation of per-quad <code>outTexCoord</code> uv coords. Note that these are continuous (smooth).
</figcaption>
</figure>
<h2 id="fragment-shader">Fragment Shader&nbsp;<a href="#fragment-shader"></a></h2>

</div>
<div>
  
<p>Our fragment shader needs three pieces of information to render text, two of which it receives from the vertex shader stage:</p>
<ol>
<li>The fragment’s interpolated uv coordinate, <code>uv</code></li>
<li>The character that we want to draw, <code>in_word</code></li>
<li>The font data array, <code>font_data</code></li>
</ol>
<p>To render a glyph, each fragment must map its uv-coordinate to the correct bit of the glyph bitmap. If the bit at the lookup position is set, then render the fragment in the foreground colour, otherwise render it in background colour.</p>
<p>This mapping works like this:</p>
<p>First, we must map the uv coordinates to <span>word </span><span>– <strong>word</strong> not, <em>world</em>! –</span> pixel coordinates. The nice thing about these two coordinate systems is that they both have their origin at the <span>top left</span><span>, so we only need to bother with scaling, and not origin transformation</span>.</p>
<p>We know that our uv coordinates are normalised floats going from <code>vec2(0.f,0.f)</code> to <code>vec2(1.f,1.f)</code>, while our font pixel coordinates are integers, going from <code>uvec2(0,0)</code> to <code>uvec2(7,15)</code>.</p>
<p>We also must find out which one of the four characters in the word to draw.</p>

</div>
<div>
    
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="glsl"><span><span><span>const</span> <span>uint</span> <span>WORD_LEN</span> <span>=</span> <span>4</span><span>;</span> <span>// 4 characters in a word</span>
</span></span><span><span>
</span></span><span><span><span>// quantize uv coordinate to discrete steps</span>
</span></span><span><span><span>uvec2</span> <span>word_pixel_coord</span> <span>=</span> <span>uvec2</span><span>(</span><span>floor</span><span>(</span><span>uv</span><span>.</span><span>xy</span> <span>*</span> <span>vec2</span><span>(</span> <span>8</span> <span>*</span> <span>WORD_LEN</span><span>,</span> <span>16</span><span>)));</span> 
</span></span><span><span><span>// limit pixel coord range to uvec2(0..31, 0..15)</span>
</span></span><span><span><span>word_pixel_coord</span> <span>=</span> <span>min</span><span>(</span><span>uvec2</span><span>(</span> <span>8</span> <span>*</span> <span>WORD_LEN</span> <span>-</span><span>1</span><span>,</span> <span>16</span> <span>-</span><span>1</span><span>),</span> <span>word_pixel_coord</span><span>);</span>
</span></span><span><span><span>// Find which of the four characters in the word this fragment falls onto</span>
</span></span><span><span><span>uint</span> <span>printable_character</span> <span>=</span> <span>in_word</span> <span>&gt;&gt;</span> <span>(</span><span>WORD_LEN</span> <span>-</span> <span>(</span><span>word_pixel_coord</span><span>.</span><span>x</span> <span>/</span> <span>8</span><span>));</span>
</span></span><span><span><span>// Map fragment coordinate to pixel coordinate inside character bitmap</span>
</span></span><span><span><span>uvec2</span> <span>glyph_pixel_coord</span> <span>=</span> <span>uvec2</span><span>(</span><span>word_pixel_coord</span><span>.</span><span>x</span> <span>%</span> <span>8</span><span>,</span> <span>word_pixel_coord</span><span>.</span><span>y</span><span>);</span></span></span></code></pre></td></tr></tbody></table>
</div><p><span>glsl</span>
</p></div><figure>
<img loading="lazy" src="https://poniesandlight.co.uk/img/reflect/debug_print_text/screenshot_small_uv.png" alt="Quad visualisation of word_pixel_coord" title="Quad visualisation of word_pixel_coord">
<figcaption>
    A visualisation of <code>word_pixel_coord</code> (normalised)
</figcaption>
</figure>
<figure>
<img loading="lazy" src="https://poniesandlight.co.uk/img/reflect/debug_print_text/screenshot_per_char_uv.png" alt="Quad visualisation of glyph_pixel_coord" title="Quad visualisation of glyph_pixel_coord">
<figcaption>
    A visualisation of <code>glyph_pixel_coord</code> (normalised)
</figcaption>
</figure>
<p>Remember, to draw a character, we must look up the character in the font bitmap table, where we must find the correct bit to check based on the uv coordinate of the fragment. You will notice that in the first GLSL example above, we were only worried about the <code>.x</code> coordinate. Now, let’s focus on <code>.y</code>, so that we can draw more lines of pixels by looking up the correct line to sample from.</p>
<p>Let’s do this step by step. First, we fetch the character bitmap from our <code>font_data</code> as an <code>uvec4</code>. Then we use the <code>glyph_pixel_coord.y</code> to pick the correct one of 4 <code>uints</code> that make up the glyph. This will give us four lines of pixels.</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="glsl"><span><span><span>// First, map character ASCII code to an index offset into font_data table. </span>
</span></span><span><span><span>// The first character in the font_data table is 0x20, SPACE.</span>
</span></span><span><span><span>offset</span> <span>=</span> <span>printable_character</span> <span>-</span> <span>0x20</span><span>;</span> 
</span></span><span><span><span>// Then get the bitmap for this glyph</span>
</span></span><span><span><span>uvec4</span> <span>character_bitmap</span> <span>=</span> <span>font_data</span><span>[</span><span>offset</span><span>];</span> 
</span></span><span><span><span>// Find the uint that contains one of the four lines that </span>
</span></span><span><span><span>// are touched by our pixel coordinate</span>
</span></span><span><span><span>uint</span> <span>four_lines</span> <span>=</span> <span>character_bitmap</span><span>[</span><span>glyph_pixel_coord</span><span>.</span><span>y</span> <span>/</span> <span>4</span><span>];</span></span></span></code></pre></td></tr></tbody></table>
</div><p><span>glsl</span>
</p></div><p>Once we have the <code>uint</code> covering four lines, we must pick the correct line from it.</p>
<p>Note that lines are stored in reverse order because after we used ImHex to lift the bitmap bytes out of the font file, we just concatenated the <code>chars</code> into <code>uint</code>. This means that our bitmap <code>uint</code>s have the wrong endianness; We want to keep it like this though, because it is much less work to just concatenate chars copied form ImHex than to manually convert endianness in a text editor.</p>

</div>
<div>
    
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="glsl"><span><span><span>uint</span> <span>current_line</span>  <span>=</span> <span>(</span><span>four_lines</span> <span>&gt;&gt;</span> <span>(</span><span>8</span><span>*</span><span>(</span><span>3</span><span>-</span><span>(</span><span>glyph_pixel_coord</span><span>.</span><span>y</span><span>)</span><span>%</span><span>4</span><span>)))</span> <span>&amp;</span> <span>0xff</span><span>;</span></span></span></code></pre></td></tr></tbody></table>
</div><p><span>glsl</span>
</p></div><p>And, lastly, we must pick the correct bit in the bitmap. Note the <code>7-</code> – this is because bytes are stored with the most significant bit at the highest index. To map this to a left-to-right coordinate system, we must index backwards, again.</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="glsl"><span><span><span>uint</span> <span>current_pixel</span> <span>=</span> <span>(</span><span>current_line</span> <span>&gt;&gt;</span> <span>(</span><span>7</span><span>-</span><span>glyph_pixel_coord</span><span>.</span><span>x</span><span>))</span> <span>&amp;</span> <span>0x01</span><span>;</span></span></span></code></pre></td></tr></tbody></table>
</div><p><span>glsl</span>
</p></div><p>We now can use the current pixel to shade our fragment, so that if the pixel is set in the bitmap, we shade our fragment in the foreground colour, and if it is not set, shade our fragment in the background colour:</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="glsl"><span><span><span>vec3</span> <span>color</span> <span>=</span> <span>mix</span><span>(</span><span>background_colour</span><span>,</span> <span>foreground_colour</span><span>,</span> <span>current_pixel</span><span>);</span></span></span></code></pre></td></tr></tbody></table>
</div><p><span>glsl</span>
</p></div><figure>
<img loading="lazy" src="https://poniesandlight.co.uk/img/reflect/debug_print_text/screenshot_per_char_uv_overlay.png" alt="Quad visualisation" title="Quad visualisation">
<figcaption>
    Text printed with uv coordinates overlaid
</figcaption>
</figure>
<p>What about the fill chars that get inserted if our printable text is too short to be completely divisible by 4? We detect these in the fragment shader: In case were are about to render such a fill character, we should do absolutely nothing, not even draw the background. We can do this by testing <code>printable_character</code>, and issuing a <code>discard</code> in case the printable character is <code>\0</code>.</p>
<h2 id="a-visual-summary">A Visual Summary&nbsp;<a href="#a-visual-summary"></a></h2>
<p>It is said that an image is worth a thousand words. Why not have both? Here is a diagram which summarises the mapping from quad-uv space to glyph bitmap space:</p>
</div>
<figure>
<img src="https://poniesandlight.co.uk/img/reflect/debug_print_text/summary.svg" loading="lazy">
<figcaption>
	    <div><p>Note: Our Fragment position is marked by the blue speck.</p><p>① pick the correct character from our per-quad word. ② calculate the offset into <code>font_data</code> using the character ASCII code. ③ fetch the <code>uvec4</code> that holds the bitmap for our glyph from <code>font_data</code> ④ pick the <code>uint</code> representing the four lines of the glyph that our fragment falls in (via its y-coord) ⑤ pick the correct line using the fragment’s .y coord ⑥ pick the correct bit using the per-glyph <code>x</code> coordinate.
	    </p></div>
</figcaption>
</figure>
<div>
<h2 id="full-implementation--more-source-code">Full Implementation &amp; More Source Code&nbsp;<a href="#full-implementation--more-source-code"></a></h2>
<figure>
<img loading="lazy" src="https://poniesandlight.co.uk/img/island_preview.png" alt="Island preview image" title="Island preview image">
<figcaption>
    You can find an implementation of the technique described above in the source code for <a href="https://github.com/tgfrerer/island/tree/wip/modules/le_debug_print_text">le_print_debug_print_text</a>, which is a new <a href="https://poniesandlight.co.uk/tags/island/">Island</a> module that allows you to easily print debug messages to screen. It has some extra nice bits around text processing and caching which, however, would be too wordy to describe here.
</figcaption>
</figure>
<p>Using this technique, it is now possible, from nearly anywhere in an Island project, to call:</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span><span>2
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="cpp"><span><span><span>char</span> <span>const</span> <span>msg_2</span><span>[]</span> <span>=</span> <span>{</span> <span>70</span><span>,</span> <span>111</span><span>,</span> <span>108</span><span>,</span> <span>107</span><span>,</span> <span>115</span><span>,</span> <span>'!'</span><span>,</span> <span>0</span> <span>};</span>
</span></span><span><span><span>le</span><span>::</span><span>DebugPrint</span><span>(</span> <span>"That's all, %s"</span><span>,</span> <span>msg_2</span> <span>);</span></span></span></code></pre></td></tr></tbody></table>
</div><p><span>cpp</span>
</p></div><p>And see the following result on screen:

</p><figure>
<img loading="lazy" src="https://poniesandlight.co.uk/img/reflect/debug_print_text/that_s_all_folks.png" alt="Image That&amp;rsquo;s all Folks">
</figure>
<h2 id="acknowledgements">Acknowledgements&nbsp;<a href="#acknowledgements"></a></h2>
<ul>
<li>Diagrams drawn with <a href="https://excalidraw.com/">Excalidraw</a></li>
<li>Original source data for the pixel font came from <a href="http://www.fial.com/~scott/tamsyn-font/">Tamsyn</a>, a free pixel font by Scott Fial</li>
</ul>
<h2 id="backlinks">Backlinks&nbsp;<a href="#backlinks"></a></h2>
<p>This article was featured on <a href="https://www.jendrikillner.com/post/graphics-programming-weekly-issue-363/">Graphics Programming Weekly</a>, and discussed on <a href="https://lobste.rs/s/5iiqji/texture_less_text_rendering">Lobste.rs</a>, and <a href="https://news.ycombinator.com/item?id=42093037">Hacker News</a>.</p>
<p>If you like more of this, subscribe to the rss feed, and if you want the very latest, and hear about occasional sortees into generative art and design, follow me on <a href="https://bsky.app/profile/tgfrerer.bsky.social">bluesky</a> or <a href="https://mastodon.social/@tgfrerer">mastodon</a>, or maybe even <a href="https://www.instagram.com/tgfrerer/">Instagram</a>. Shameless plug: my services are also available for contract work.</p>


	</div>
	
	
	
		<div>
			<h3>RSS:</h3>
			<p>Find out first about new posts by subscribing to the <a href="https://poniesandlight.co.uk//reflect/feed.xml">RSS Feed</a> <a href="https://poniesandlight.co.uk//reflect/feed.xml" type="application/rss+xml"><svg style="width: 1em; position:relative; bottom:-0.25em;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Pro 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2022 Fonticons, Inc. --><path d="M64 32C28.7 32 0 60.7 0 96V416c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V96c0-35.3-28.7-64-64-64H64zM96 136c0-13.3 10.7-24 24-24c137 0 248 111 248 248c0 13.3-10.7 24-24 24s-24-10.7-24-24c0-110.5-89.5-200-200-200c-13.3 0-24-10.7-24-24zm0 96c0-13.3 10.7-24 24-24c83.9 0 152 68.1 152 152c0 13.3-10.7 24-24 24s-24-10.7-24-24c0-57.4-46.6-104-104-104c-13.3 0-24-10.7-24-24zm64 120c0 17.7-14.3 32-32 32s-32-14.3-32-32s14.3-32 32-32s32 14.3 32 32z"></path></svg></a></p>
		</div>
	
		<p>
			<h3>Further Posts:</h3>
		</p>
		
        </div></div>]]></description>
        </item>
    </channel>
</rss>