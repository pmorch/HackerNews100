<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 12 Mar 2024 02:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Key Boeing Whistleblower Found Dead from Apparent Suicide (111 pts)]]></title>
            <link>https://thehill.com/policy/transportation/4524968-boeing-whistleblower-found-dead-in-apparent-suicide/</link>
            <guid>39674829</guid>
            <pubDate>Tue, 12 Mar 2024 00:41:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thehill.com/policy/transportation/4524968-boeing-whistleblower-found-dead-in-apparent-suicide/">https://thehill.com/policy/transportation/4524968-boeing-whistleblower-found-dead-in-apparent-suicide/</a>, See on <a href="https://news.ycombinator.com/item?id=39674829">Hacker News</a></p>
Couldn't get https://thehill.com/policy/transportation/4524968-boeing-whistleblower-found-dead-in-apparent-suicide/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing whistleblower found dead in US (221 pts)]]></title>
            <link>https://www.bbc.com/news/business-68534703</link>
            <guid>39673589</guid>
            <pubDate>Mon, 11 Mar 2024 21:45:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/business-68534703">https://www.bbc.com/news/business-68534703</a>, See on <a href="https://news.ycombinator.com/item?id=39673589">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="main-content" data-testid="main-content"><article><header data-component="legacy-header-block"></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 976w" type="image/webp"><img alt="A Boeing 787 Dreamliner plane at the Paris Air Show" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 976w" src="https://ichef.bbci.co.uk/news/976/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg" width="976" height="549" loading="eager"></picture></span><span role="text"><span>Image source, </span>Getty Images</span></p></figure></div><div data-component="text-block"><p><b>A former Boeing employee known for raising concerns about the firm's production standards has been found dead in the US. </b></p></div><div data-component="text-block"><p>John Barnett had worked for Boeing for 32 years, until his retirement in 2017. </p></div><div data-component="text-block"><p>In the days before his death, he had been giving evidence in a whistleblower lawsuit against the company. </p></div><div data-component="text-block"><p>Boeing said it was saddened to hear of Mr Barnett's passing. The Charleston County coroner confirmed his death to the BBC on Monday. </p></div><div data-component="text-block"><p>It said the 62-year-old had died from a "self-inflicted" wound on 9 March and police were investigating. </p></div><div data-component="text-block"><p>Mr Barnett had worked for the US plane giant for 32 years, until his retirement in 2017 on health grounds. </p></div><div data-component="text-block"><p>From 2010, he worked as a quality manager at the North Charleston plant making the 787 Dreamliner, a state-of-the-art airliner used mainly on long-haul routes. </p></div><div data-component="text-block"><p><a href="https://www.bbc.co.uk/news/business-50293927">In 2019, Mr Barnett told the BBC</a> that under-pressure workers had been deliberately fitting sub-standard parts to aircraft on the production line. </p></div><div data-component="text-block"><p>He also said he had uncovered serious problems with oxygen systems, which could mean one in four breathing masks would not work in an emergency.</p></div><div data-component="text-block"><p>He said soon after starting work in South Carolina he had become concerned that the push to get new aircraft built meant the assembly process was rushed and safety was compromised, something the company denied.</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>John Barnett</span></p><figcaption><span>Image caption, </span><p>John Barnett was a former quality control manager at Boeing</p></figcaption></figure></div><div data-component="text-block"><p>He later told the BBC that workers had failed to follow procedures intended to track components through the factory, allowing defective components to go missing. </p></div><div data-component="text-block"><p>He said in some cases, sub-standard parts had even been removed from scrap bins and fitted to planes that were being built to prevent delays on the production line.</p></div><div data-component="text-block"><p>He also claimed that tests on emergency oxygen systems due to be fitted to the 787 showed a failure rate of 25%, meaning that one in four could fail to deploy in a real-life emergency.</p></div><div data-component="text-block"><p>Mr Barnett said he had alerted managers to his concerns, but no action had been taken. </p></div><div data-component="text-block"><p>Boeing denied his assertions. However, a 2017 review by the US regulator, the Federal Aviation Administration (FAA), did uphold some of Mr Barnett's concerns.</p></div><div data-component="text-block"><p>It established that the location of at least 53 "non-conforming" parts in the factory was unknown, and that they were considered lost. Boeing was ordered to take remedial action.</p></div><div data-component="text-block"><p>On the oxygen cylinders issue, the company said that in 2017 it had "identified some oxygen bottles received from the supplier that were not deploying properly". But it denied that any of them were actually fitted on aircraft.</p></div><div data-component="text-block"><p>After retiring, he embarked on a long-running legal action against the company. </p></div><div data-component="text-block"><p>He accused it of denigrating his character and hampering his career because of the issues he pointed out - charges rejected by Boeing.</p></div><div data-component="text-block"><p>At the time of his death, Mr Barnett had been in Charleston for legal interviews linked to that case.</p></div><div data-component="text-block"><p>Last week, he gave a formal deposition in which he was questioned by Boeing's lawyers, before being cross-examined by his own counsel.</p></div><div data-component="text-block"><p>He had been due to undergo further questioning on Saturday. When he did not appear, enquiries were made at his hotel. </p></div><div data-component="text-block"><p>He was subsequently found dead in his truck in the hotel car park. </p></div><div data-component="text-block"><p>Speaking to the BBC, his lawyer described his death as "tragic". </p></div><div data-component="text-block"><p>In a statement Boeing said: "We are saddened by Mr. Barnett's passing, and our thoughts are with his family and friends."</p></div><div data-component="text-block"><p>His death comes at a time when production standards at both Boeing and its key supplier Spirit Aerosystems are under intense scrutiny.</p></div><div data-component="text-block"><p>This follows an incident in early January when an unused emergency exit door blew off a brand-new Boeing 737 Max shortly after take-off from Portland International Airport.</p></div><div data-component="text-block"><p>A preliminary report from the US National Transportation Safety Board suggested that four key bolts, designed to hold the door securely in place, were not fitted.</p></div><div data-component="text-block"><p>Last week, the FAA said a six-week audit of the company had found "multiple instances where the company allegedly failed to comply with manufacturing quality control requirements".</p></div><section data-component="links-block"><p><h2>More on this story</h2></p></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Webb and Hubble confirm Universe's expansion rate (315 pts)]]></title>
            <link>https://www.esa.int/ESA_Multimedia/Images/2024/03/Webb_Hubble_confirm_Universe_s_expansion_rate</link>
            <guid>39673087</guid>
            <pubDate>Mon, 11 Mar 2024 20:49:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.esa.int/ESA_Multimedia/Images/2024/03/Webb_Hubble_confirm_Universe_s_expansion_rate">https://www.esa.int/ESA_Multimedia/Images/2024/03/Webb_Hubble_confirm_Universe_s_expansion_rate</a>, See on <a href="https://news.ycombinator.com/item?id=39673087">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="modal__tab-content--details">
				<p>The rate at which the Universe is expanding, known as the Hubble constant, is one of the fundamental parameters for understanding the evolution and ultimate fate of the cosmos. However, a persistent difference, called the Hubble Tension, is seen between the value of the constant measured with a wide range of independent distance indicators and its value predicted from the afterglow of the Big Bang. The NASA/ESA/CSA James Webb Space Telescope has confirmed that the Hubble Space Telescope’s keen eye was right all along, erasing any lingering doubt about Hubble’s measurements.</p><p>This image of NGC 5468, a galaxy located about 130 million light-years from Earth, combines data from the <a href="https://www.esa.int/Science_Exploration/Space_Science/Hubble_overview" target="_blank">Hubble</a> and <a href="https://www.esa.int/Science_Exploration/Space_Science/Webb" target="_blank">James Webb space</a> telescopes. This is the most distant galaxy in which Hubble has identified Cepheid variable stars. These stars are important milepost markers for measuring the expansion rate of the Universe. The distance calculated from Cepheids has been cross-correlated with a Type Ia supernova in the galaxy. Type Ia supernovae are so bright they are used to measure cosmic distances far beyond the range of the Cepheids, extending measurements of the Universe’s expansion rate deeper into space.</p><p><a href="https://www.esa.int/Science_Exploration/Space_Science/Webb/Webb_Hubble_confirm_Universe_s_expansion_rate" target="_blank">Read more</a></p><p>[<i>Image description</i>: A face-on spiral galaxy with four spiral arms that curve outward in a counterclockwise direction. The spiral arms are filled with young, blue stars and peppered with purplish star-forming regions that appear as small blobs. The middle of the galaxy is much brighter and more yellowish, and has a distinct narrow linear bar angled from 11 o’clock to 5 o’clock. Dozens of red background galaxies are scattered across the image. The background of space is black.]</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Diffusion models from scratch, from a new theoretical perspective (187 pts)]]></title>
            <link>https://www.chenyang.co/diffusion.html</link>
            <guid>39672450</guid>
            <pubDate>Mon, 11 Mar 2024 19:43:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chenyang.co/diffusion.html">https://www.chenyang.co/diffusion.html</a>, See on <a href="https://news.ycombinator.com/item?id=39672450">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
<article>
<h2>Diffusion models from scratch, from a new theoretical perspective</h2>
<section>
\[\newcommand{\Kset}{\mathcal{K}}
\newcommand{\distK}{ {\rm dist}_{\Kset} }
\newcommand{\projK}{ {\rm proj}_{\Kset} }
\newcommand{\eps}{\epsilon}
\newcommand{\Loss}{\mathcal{L}}
\newcommand{\norm}[1]{\left\lVert #1 \right\lVert}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\softmin}{softmin}
\DeclareMathOperator{\distop}{dist}\]
<p>Diffusion models have recently produced impressive results in generative
modeling, in particular sampling from multimodal distributions. Not only has
diffusion models seen widespread adoption in text-to-image generation tools such
as <a href="https://github.com/Stability-AI/stablediffusion">Stable Diffusion</a>, they
also excel in other application domains such as
<a href="https://text-to-audio.github.io/">audio</a>/<a href="https://openai.com/research/video-generation-models-as-world-simulators">video</a>/<a href="https://zero123.cs.columbia.edu/">3D</a>
generation, <a href="https://www.nature.com/articles/s41586-023-06415-8">protein
design</a>, <a href="https://diffusion-policy.cs.columbia.edu/">robotics path
planning</a>, all of which require
sampling from multimodal distributions.</p>
<p>This tutorial aims to introduce diffusion models from an optimization
perspective as introduced in <a href="https://arxiv.org/abs/2306.04848">our paper</a>. It will go over both
theory and code, using the theory to explain how to implement diffusion models
from scratch. By the end of the tutorial, you will learn how to implement
training and sampling code for a toy dataset, which will also work for larger
datasets and models.</p>
<p>In this tutorial we will mainly reference code from
<a href="https://github.com/yuanchenyang/smalldiffusion"><code>smalldiffusion</code></a>. For
pedagogical purposes, the code presented here will be simplified from the
<a href="https://github.com/yuanchenyang/smalldiffusion/blob/main/src/smalldiffusion/">original library code</a>, which is on its own well-commented
and easy to read.</p>
<h4 id="training-diffusion-models">Training diffusion models</h4>
<p>Diffusion models aim to generate samples from a set that is learned from
training examples, which we will denote by \(\mathcal{K}\). For example, if we
want to generate images, \(\mathcal{K} \subset \mathbb{R}^{c\times h \times w}\)
is the set of pixel values that correspond to realistic images. Diffusion models
also work for \(\mathcal{K}\) corresponding to modalities other than images,
such as audio, video, robot trajectories, and even in discrete domains such as
text generation.</p>
<p>In a nutshell, diffusion models are trained by:</p>
<ol>
<li>Sampling \(x_0 \sim \mathcal{K}\), noise level \(\sigma \sim [\sigma_\min,
\sigma_\max]\), noise \(\epsilon \sim N(0, I)\)</li>
<li>Generating noisy data \(x_\sigma = x_0 + \sigma \epsilon\)</li>
<li>Predicting \(\epsilon\) (direction of noise) from \(x_\sigma\) by minimizing squared loss</li>
</ol>
<p>This amounts to training a \(\theta\)-parameterized neural network
\(\epsilon_\theta(x, \sigma)\), by minimizing the loss function</p>
\[\Loss(\theta) = \mathop{\mathbb{E}}
\lVert\epsilon_\theta(x_0 + \sigma_t \epsilon, \sigma_t) - \epsilon \lVert^2\]
<p>In practice, this is done by the following simple <code>training_loop</code>:</p>
<div><pre><code><span>def</span> <span>training_loop</span><span>(</span><span>loader</span>  <span>:</span> <span>DataLoader</span><span>,</span>
                  <span>model</span>   <span>:</span> <span>nn</span><span>.</span><span>Module</span><span>,</span>
                  <span>schedule</span><span>:</span> <span>Schedule</span><span>,</span>
                  <span>epochs</span>  <span>:</span> <span>int</span> <span>=</span> <span>10000</span><span>):</span>
    <span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>())</span>
    <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>epochs</span><span>):</span>
        <span>for</span> <span>x0</span> <span>in</span> <span>loader</span><span>:</span>
            <span>optimizer</span><span>.</span><span>zero_grad</span><span>()</span>
            <span>sigma</span><span>,</span> <span>eps</span> <span>=</span> <span>generate_train_sample</span><span>(</span><span>x0</span><span>,</span> <span>schedule</span><span>)</span>
            <span>eps_hat</span> <span>=</span> <span>model</span><span>(</span><span>x0</span> <span>+</span> <span>sigma</span> <span>*</span> <span>eps</span><span>,</span> <span>sigma</span><span>)</span>
            <span>loss</span> <span>=</span> <span>nn</span><span>.</span><span>MSELoss</span><span>()(</span><span>eps_hat</span><span>,</span> <span>eps</span><span>)</span>
            <span>optimizer</span><span>.</span><span>backward</span><span>(</span><span>loss</span><span>)</span>
            <span>optimizer</span><span>.</span><span>step</span><span>()</span>
</code></pre></div>
<p>The training loop iterates over batches of <code>x0</code>, then samples noise level
<code>sigma</code> and noise vector <code>eps</code> using <code>generate_train_sample</code>:</p>
<div><pre><code><span>def</span> <span>generate_train_sample</span><span>(</span><span>x0</span><span>:</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>,</span> <span>schedule</span><span>:</span> <span>Schedule</span><span>):</span>
    <span>sigma</span> <span>=</span> <span>schedule</span><span>.</span><span>sample_batch</span><span>(</span><span>x0</span><span>)</span>
    <span>eps</span> <span>=</span> <span>torch</span><span>.</span><span>randn_like</span><span>(</span><span>x0</span><span>)</span>
    <span>return</span> <span>sigma</span><span>,</span> <span>eps</span>
</code></pre></div>
<h5 id="noise-schedules">Noise schedules</h5>
<p>In practice, \(\sigma\) is not sampled uniformly from the interval \([\sigma_\min,
\sigma_\max]\), instead this interval is discretized into \(N\) distinct values
called a <em>\(\sigma\) schedule</em>: \(\{ \sigma_t \}_{t=1}^N\), and \(\sigma\) is instead
sampled uniformly from the \(N\) possible values of \(\sigma_t\). We define the
<code>Schedule</code> class that encapsulates the list of possible <code>sigmas</code>, and sample
from this list during training.</p>
<div><pre><code><span>class</span> <span>Schedule</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>sigmas</span><span>:</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>):</span>
        <span>self</span><span>.</span><span>sigmas</span> <span>=</span> <span>sigmas</span>
    <span>def</span> <span>__getitem__</span><span>(</span><span>self</span><span>,</span> <span>i</span><span>)</span> <span>-&gt;</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>:</span>
        <span>return</span> <span>self</span><span>.</span><span>sigmas</span><span>[</span><span>i</span><span>]</span>
    <span>def</span> <span>__len__</span><span>(</span><span>self</span><span>)</span> <span>-&gt;</span> <span>int</span><span>:</span>
        <span>return</span> <span>len</span><span>(</span><span>self</span><span>.</span><span>sigmas</span><span>)</span>
    <span>def</span> <span>sample_batch</span><span>(</span><span>self</span><span>,</span> <span>x0</span><span>:</span><span>torch</span><span>.</span><span>FloatTensor</span><span>)</span> <span>-&gt;</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>:</span>
        <span>return</span> <span>self</span><span>[</span><span>torch</span><span>.</span><span>randint</span><span>(</span><span>len</span><span>(</span><span>self</span><span>),</span> <span>(</span><span>x0</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>],))].</span><span>to</span><span>(</span><span>x0</span><span>)</span>
</code></pre></div>
<p>In this tutorial, we will use a log-linear schedule defined below:</p>
<div><pre><code><span>class</span> <span>ScheduleLogLinear</span><span>(</span><span>Schedule</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>N</span><span>:</span> <span>int</span><span>,</span> <span>sigma_min</span><span>:</span> <span>float</span><span>=</span><span>0.02</span><span>,</span> <span>sigma_max</span><span>:</span> <span>float</span><span>=</span><span>10</span><span>):</span>
        <span>super</span><span>().</span><span>__init__</span><span>(</span><span>torch</span><span>.</span><span>logspace</span><span>(</span><span>math</span><span>.</span><span>log10</span><span>(</span><span>sigma_min</span><span>),</span> <span>math</span><span>.</span><span>log10</span><span>(</span><span>sigma_max</span><span>),</span> <span>N</span><span>))</span>
</code></pre></div>
<p>Other commonly used schedules include <code>ScheduleDDPM</code> for pixel-space diffusion
models and <code>ScheduleLDM</code> for latent diffusion models such as
Stable Diffusion. The following plot compares these three schedules with default
parameters.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/schedule.png" alt="">
</p>
<figcaption>A comparison plot of different diffusion schedules</figcaption>
</figure>
<h5 id="toy-example">Toy example</h5>
<p>In this tutorial we will start with a toy dataset used in one of the first
diffusion papers <a href="https://arxiv.org/abs/1503.03585">[Sohl-Dickstein
et.al. 2015]</a>, where \(\Kset \subset \R^2\)
are points sampled from a spiral. We first construct and visualize this dataset:</p>
<div><pre><code><span>dataset</span> <span>=</span> <span>Swissroll</span><span>(</span><span>np</span><span>.</span><span>pi</span><span>/</span><span>2</span><span>,</span> <span>5</span><span>*</span><span>np</span><span>.</span><span>pi</span><span>,</span> <span>100</span><span>)</span>
<span>loader</span>  <span>=</span> <span>DataLoader</span><span>(</span><span>dataset</span><span>,</span> <span>batch_size</span><span>=</span><span>2048</span><span>)</span>
</code></pre></div>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/swissroll.png" alt="">
</p>
<figcaption>Swissroll toy dataset</figcaption>
</figure>
<p>For this simple dataset, we can implement the denoiser using a multi-layer
perceptron (MLP):</p>
<div><pre><code><span>def</span> <span>get_sigma_embeds</span><span>(</span><span>sigma</span><span>):</span>
    <span>return</span> <span>torch</span><span>.</span><span>cat</span><span>([</span><span>torch</span><span>.</span><span>sin</span><span>(</span><span>torch</span><span>.</span><span>log</span><span>(</span><span>sigma</span><span>)</span><span>/</span><span>2</span><span>),</span>
                      <span>torch</span><span>.</span><span>cos</span><span>(</span><span>torch</span><span>.</span><span>log</span><span>(</span><span>sigma</span><span>)</span><span>/</span><span>2</span><span>)],</span> <span>dim</span><span>=</span><span>1</span><span>)</span>

<span>class</span> <span>TimeInputMLP</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>dim</span><span>,</span> <span>hidden_dims</span><span>):</span>
        <span>super</span><span>().</span><span>__init__</span><span>()</span>
        <span>layers</span> <span>=</span> <span>[]</span>
        <span>for</span> <span>in_dim</span><span>,</span> <span>out_dim</span> <span>in</span> <span>pairwise</span><span>((</span><span>dim</span> <span>+</span> <span>2</span><span>,)</span> <span>+</span> <span>hidden_dims</span><span>):</span>
            <span>layers</span><span>.</span><span>extend</span><span>([</span><span>nn</span><span>.</span><span>Linear</span><span>(</span><span>in_dim</span><span>,</span> <span>out_dim</span><span>),</span> <span>nn</span><span>.</span><span>GELU</span><span>()])</span>
        <span>layers</span><span>.</span><span>append</span><span>(</span><span>nn</span><span>.</span><span>Linear</span><span>(</span><span>hidden_dims</span><span>[</span><span>-</span><span>1</span><span>],</span> <span>dim</span><span>))</span>
        <span>self</span><span>.</span><span>net</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span><span>*</span><span>layers</span><span>)</span>
        <span>self</span><span>.</span><span>input_dims</span> <span>=</span> <span>(</span><span>dim</span><span>,)</span>

    <span>def</span> <span>rand_input</span><span>(</span><span>self</span><span>,</span> <span>batchsize</span><span>):</span>
        <span>return</span> <span>torch</span><span>.</span><span>randn</span><span>((</span><span>batchsize</span><span>,)</span> <span>+</span> <span>self</span><span>.</span><span>input_dims</span><span>)</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>sigma</span><span>):</span>
        <span>sigma_embeds</span> <span>=</span> <span>get_sigma_embeds</span><span>(</span><span>x</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>],</span> <span>sigma</span><span>.</span><span>squeeze</span><span>())</span> <span># shape: b x 2
</span>        <span>nn_input</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>([</span><span>x</span><span>,</span> <span>sigma_embeds</span><span>],</span> <span>dim</span><span>=</span><span>1</span><span>)</span>               <span># shape: b x (dim + 2)
</span>        <span>return</span> <span>self</span><span>.</span><span>net</span><span>(</span><span>nn_input</span><span>)</span>

<span>model</span> <span>=</span> <span>TimeInputMLP</span><span>(</span><span>dim</span><span>=</span><span>2</span><span>,</span> <span>hidden_dims</span><span>=</span><span>(</span><span>16</span><span>,</span><span>128</span><span>,</span><span>128</span><span>,</span><span>128</span><span>,</span><span>128</span><span>,</span><span>16</span><span>))</span>
</code></pre></div>
<p>The MLP takes the concatenation of \(x \in \R^2\) and an embedding of the noise
level \(\sigma\), then predicts the noise \(\epsilon \in \R^2\). Although many
diffusion models use a sinusoidal positional embedding for \(\sigma\), the
simple two-dimensional embedding works just as well:</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/sigma_embedding.png" alt="">
</p>
<figcaption>Two-dimensional \(\sigma_t\) embedding</figcaption>
</figure>
<p>Now we have all the ingredients to train a diffusion model.</p>
<div><pre><code><span>schedule</span> <span>=</span> <span>ScheduleLogLinear</span><span>(</span><span>N</span><span>=</span><span>200</span><span>,</span> <span>sigma_min</span><span>=</span><span>0.005</span><span>,</span> <span>sigma_max</span><span>=</span><span>10</span><span>)</span>
<span>trainer</span>  <span>=</span> <span>training_loop</span><span>(</span><span>loader</span><span>,</span> <span>model</span><span>,</span> <span>schedule</span><span>,</span> <span>epochs</span><span>=</span><span>15000</span><span>)</span>
<span>losses</span>   <span>=</span> <span>[</span><span>ns</span><span>.</span><span>loss</span><span>.</span><span>item</span><span>()</span> <span>for</span> <span>ns</span> <span>in</span> <span>trainer</span><span>]</span>
</code></pre></div>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/training_loss.png" alt="">
</p>
<figcaption>Training loss over 15000 epochs, smoothed with moving average</figcaption>
</figure>
<p>The learned denoiser \(\eps_\theta(x, \sigma)\) can be visualized as a vector
field parameterized by the noise level \(\sigma\), by plotting \(x - \sigma
\eps_\theta(x, \sigma)\) for different \(x\) and levels of \(\sigma\).</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/predicted_eps_field.png" alt="">
</p>
<figcaption>Plot of predicted \(\hat{x}_0 = x - \sigma \eps_\theta(x, \sigma)\) for
different \(x\) and \(\sigma\)</figcaption>
</figure>
<p>In the plots above, the arrows point from each noisy datapoint \(x\) to the
“clean” datapoint predicted by the denoiser with noise level \(\sigma\). At high
levels of \(\sigma\), the denoiser tends to predict the mean of the data, but at
low noise levels the denoiser predicts actual data points, provided that its
input \(x\) is also close to the data.</p>
<p>How do we interpret what the denoiser is learning, and how do we create a
procedure to sample from diffusion models? We will next build a theory of
diffusion models, then draw on this theory to derive sampling algorithms.</p>
<h4 id="denoising-as-approximate-projection">Denoising as approximate projection</h4>
<p>The diffusion training procedure learns a denoiser \(\eps_\theta(x,
\sigma)\). In <a href="https://arxiv.org/abs/2306.04848">our paper</a>, we interpret the learned denoiser as an
approximate projection to the data manifold \(\Kset\), and the goal of the
diffusion process as minimizing the distance to \(\Kset\). This motivates us to
introduce a relative-error approximation model to analyze the convergence of
diffusion sampling algorithms. First we introduce some basic properties of
distance and projection functions.</p>
<h5 id="distance-and-projection-functions">Distance and projection functions</h5>
<p>The <em>distance function</em> to a set \(\Kset \subseteq \R^n\) is defined as</p>
\[\distK(x) := \min \{ \norm{x-x_0} : x_0 \in \Kset \}.\]
<p>The <em>projection</em> of \(x \in \R^n\), denoted \(\projK(x)\), is the set of points
that attain this distance:</p>
\[\projK(x) := \{ x_0 \in \Kset : \distK(x) = \norm{x-x_0} \}\]
<p>If \(\projK(x)\) is unique, the gradient of \(\distK(x)\), the direction of
steepest descent of the distance function, points towards this unique
projection:</p>
<p><strong>Proposition</strong> Suppose \(\Kset\) is closed and \(x \not \in \Kset\). If \(\projK(x)\)
is unique, then</p>
\[\nabla \frac{1}{2} \distK(x)^2 = \distK(x) \nabla \distK(x) = x-\projK(x).\]
<p>This tells us that if we can learn \(\nabla \distK(x)\) for every \(x\), we can
simply move in this direction to find the projection of \(x\) onto \(\Kset\).
One issue with learning this gradient is that \(\distK\) is not differentiable
everywhere, thus \(\nabla \distK\) is not a continuous function. To solve this
problem, we introduce a squared-distance function smoothed by a parameter
\(\sigma\) using the \(\softmin\) operator instead of \(\min\).</p>
\[\distop^2_\Kset(x, \sigma)
:= \substack{\softmin_{\sigma^2} \\ x_0 \in \Kset} \norm{x_0 - x}^2
= {\textstyle -\sigma^2 \log\left(\sum_{x_0 \in \Kset}
\exp\left(-\frac{\norm{x_0 - x}^2}{2\sigma^2}\right)\right)}\]
<p>The following picture from <a href="https://arxiv.org/pdf/2108.10480.pdf">[Madan and Levin
2022]</a> shows the contours of both the
distance function and its smoothed version.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/smoothed_dist_contour.png" alt="">
</p>
<figcaption>Smoothed distance function has continuous gradients</figcaption>
</figure>
<p>From this picture we can see that \(\nabla \distK(x)\) points toward the closest
point to \(x\) in \(\Kset\), and \(\nabla \distop^2(x, \sigma)\) points toward a
weighted average of points in \(\Kset\) determined by \(x\).</p>
<h5 id="ideal-denoiser">Ideal denoiser</h5>
<p>The ideal or optimal denoiser \(\epsilon^*\) for a particular noise level
\(\sigma\) is an exact minimizer of the training loss function. When the data is
a discrete uniform distribution over a finite set \(\Kset\), the ideal
denoiser has an exact closed-form expression given by:</p>
\[\eps^*(x_\sigma, \sigma) = \frac{\sum_{x_0 \in \Kset} (x_\sigma-x_0) \exp(-\norm{x_\sigma-x_0}^2/2\sigma^2)}
{\sigma \sum_{x_0 \in \Kset} \exp(-\norm{x_\sigma-x_0}^2/2\sigma^2)}\]
<p>From the above expression, we see that the ideal denoiser points towards a
weighted mean of all the datapoints in \(\Kset\), where the weight for each
\(x_0 \in \Kset\) determines the distance to \(x_0\). Using this expression, we
can also implement the ideal denoiser, which is computationally tractable for
small datasets:</p>
<div><pre><code><span>def</span> <span>sq_norm</span><span>(</span><span>M</span><span>,</span> <span>k</span><span>):</span>
    <span># M: b x n --(norm)--&gt; b --(repeat)--&gt; b x k
</span>    <span>return </span><span>(</span><span>torch</span><span>.</span><span>norm</span><span>(</span><span>M</span><span>,</span> <span>dim</span><span>=</span><span>1</span><span>)</span><span>**</span><span>2</span><span>).</span><span>unsqueeze</span><span>(</span><span>1</span><span>).</span><span>repeat</span><span>(</span><span>1</span><span>,</span><span>k</span><span>)</span>

<span>class</span> <span>IdealDenoiser</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>dataset</span><span>:</span> <span>torch</span><span>.</span><span>utils</span><span>.</span><span>data</span><span>.</span><span>Dataset</span><span>):</span>
        <span>self</span><span>.</span><span>data</span> <span>=</span> <span>torch</span><span>.</span><span>stack</span><span>(</span><span>list</span><span>(</span><span>dataset</span><span>))</span>

    <span>def</span> <span>__call__</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>sigma</span><span>):</span>
        <span>x</span> <span>=</span> <span>x</span><span>.</span><span>flatten</span><span>(</span><span>start_dim</span><span>=</span><span>1</span><span>)</span>
        <span>d</span> <span>=</span> <span>self</span><span>.</span><span>data</span><span>.</span><span>flatten</span><span>(</span><span>start_dim</span><span>=</span><span>1</span><span>)</span>
        <span>xb</span><span>,</span> <span>db</span> <span>=</span> <span>x</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>],</span> <span>d</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>
        <span>sq_diffs</span> <span>=</span> <span>sq_norm</span><span>(</span><span>x</span><span>,</span> <span>db</span><span>)</span> <span>+</span> <span>sq_norm</span><span>(</span><span>d</span><span>,</span> <span>xb</span><span>).</span><span>T</span> <span>-</span> <span>2</span> <span>*</span> <span>x</span> <span>@</span> <span>d</span><span>.</span><span>T</span>
        <span>weights</span> <span>=</span> <span>torch</span><span>.</span><span>nn</span><span>.</span><span>functional</span><span>.</span><span>softmax</span><span>(</span><span>-</span><span>sq_diffs</span><span>/</span><span>2</span><span>/</span><span>sigma</span><span>**</span><span>2</span><span>,</span> <span>dim</span><span>=</span><span>1</span><span>)</span>
        <span>return </span><span>(</span><span>x</span> <span>-</span> <span>torch</span><span>.</span><span>einsum</span><span>(</span><span>'</span><span>ij,j...-&gt;i...</span><span>'</span><span>,</span> <span>weights</span><span>,</span> <span>self</span><span>.</span><span>data</span><span>))</span><span>/</span><span>sigma</span>
</code></pre></div>
<p>For our toy dataset, we can plot the direction of \(\epsilon^*\) as predicted by
the ideal denoiser for different noise levels \(\sigma\):</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/ideal_eps_field.png" alt="">
</p>
<figcaption>Plot of direction of \(\eps^*(x, \sigma)\) for different \(x\) and
\(\sigma\)</figcaption>
</figure>
<p>From our plots we see that for large values of \(\sigma\), \(\epsilon^*\) points
towards the mean of the data, but for smaller values of \(\sigma\),
\(\epsilon^*\) points towards the nearest data-point.</p>
<p>One insight from our paper is that the ideal denoiser for a fixed \(\sigma\) is
equivalent to the gradient of a \(\sigma\)-smoothed squared-distance function:</p>
<p><strong>Theorem</strong> For all \(\sigma &gt; 0\) and \(x \in \R^n\), we have</p>
\[\frac{1}{2} \nabla_x \distop^2_\Kset(x, \sigma) = \sigma \eps^*(x, \sigma).\]
<p>This tells us that the ideal denoiser found by minimizing the diffusion training
objective \(\Loss(\theta)\) is in fact the gradient of a smoothed
squared-distance function to the underlying data manifold \(\Kset\). This
connection is key to motivating our interpretation that the denoiser is an
approximate projection.</p>
<h5 id="relative-error-model">Relative error model</h5>
<p>In order to analyze the convergence of diffusion sampling algorithms, we
introduced a relative error model which states that the projection predicted by
the denoiser \(x-\sigma \epsilon_{\theta}( x, \sigma)\) well approximates
\(\projK(x)\) when the input to the denoiser \(\sigma\) well estimates
\(\distK(x)/\sqrt{n}\). For constants \(1 &gt; \eta \ge 0\) and \(\nu \ge 1\), we
assume that</p>
\[\norm{ x-\sigma \epsilon_{\theta}( x, \sigma) - \projK(x)} \le \eta \distK(x)\]
<p>when \((x, \sigma)\) satisfies \(\frac{1}{\nu}\distK(x) \le \sqrt{n}\sigma \le
\nu \distK(x)\). In addition to the discussion about ideal denoisers above, this
error model is motivated by the following observations.</p>
<p><em>Low noise</em> When \(\sigma\) is small and the manifold hypothesis holds,
denoising approximates projection because most of the added noise is orthogonal
to the data manifold.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/low_noise.png" alt="">
</p>
<figcaption>When added noise is small, most of noise is orthogonal to tangent space
of manifold. Under the manifold hypothesis, denoising is approximately projection.</figcaption>
</figure>
<p><em>High noise</em> When \(\sigma\) is large relative to the diameter of \(\Kset\),
then any denoiser predicting any weighted mean of the data \(\Kset\) has small
relative error.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/high_noise.png" alt="">
</p>
<figcaption>When added noise is large compared to diameter of data, denoising and
projection point in the same direction</figcaption>
</figure>
<p>We also perform empirical tests of our error model for pre-trained diffusion
models on image datasets. The CIFAR-10 dataset is small enough for tractable
computation of the ideal denoiser. Our experiments show that for this dataset,
the relative error between the exact projection and ideal denoiser output is
small over sampling trajectories.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/ideal_denoiser_error.png" alt="">
</p>
<figcaption>Ideal denoiser well-approximates projection onto the CIFAR-10 dataset
under relative-error model</figcaption>
</figure>

<h4 id="sampling-from-diffusion-models">Sampling from diffusion models</h4>
<p>Given a learned denoiser \(\epsilon_\theta(x, \sigma)\), how do we sample from
it to obtain a point \(x_0 \in \Kset\)? Given noisy \(x_t\) and noise level
\(\sigma_t\), the denoiser \(\eps_\theta(x_t, \sigma_t)\) predicts \(x_0\) via</p>
\[\hat x_0^t := x_t - \sigma_t \eps_\theta(x_t, \sigma_t)\]
<p>Intuition from the relative error assumption tells us that we want to start with
\((x_T, \sigma_T)\) where \(\distK(x_T)/\sqrt{n} \approx \sigma_T\). This is
achieved by choosing \(\sigma_T\) to be large relative to the diameter of
\(\Kset\), and \(x_T\) sampled i.i.d. from \(N(0, \sigma_T)\), a Gaussian with
variance \(\sigma_T\). This ensures that \(x_T\) is far away from
\(\Kset\).</p>
<p>Although \(\hat x_0^T = x_T - \sigma_T \eps_\theta(x_T, \sigma_T)\) has small
relative error, the absolute error \(\distK(\hat x_0^T)\) can still be large as
\(\distK(x_T)\) is large. In fact, at high noise levels, the expression of the
ideal denoiser tells us that \(\hat x_0^T\) should be close to the mean of the
data \(\Kset\). We cannot obtain a sample close to \(\Kset\) with a single call
to the denoiser.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/denoise.png" alt="">
</p>
<figcaption>Sampling process iteratively calls the denoiser based on \(\sigma_t\)
schedule.</figcaption>
</figure>
<p>Thus we want to <em>iteratively call the denoiser</em> to obtain a sequence \(x_T,
\ldots, x_t, \ldots x_0\) using a pre-specified schedule of \(\sigma_t\), hoping
that \(\distK(x_t)\) decreases in concert with \(\sigma_t\).</p>
\[x_{t-1} = x_t - (\sigma_t - \sigma_{t-1}) \eps_\theta(x_t, \sigma_t)\]
<p>This is exactly the deterministic <a href="https://arxiv.org/abs/2010.02502">DDIM sampling algorithm</a>, though
presented in different coordinates through a change of variable. See <a href="https://arxiv.org/abs/2306.04848">Appendix A
of our paper</a> for more details and a proof of equivalence.</p>
<h4 id="diffusion-sampling-as-distance-minimization">Diffusion sampling as distance minimization</h4>
<p>We can interpret the diffusion sampling iterations as gradient descent on the
squared-distance function \(f(x) = \frac{1}{2} \distK(x)^2\). In a nutshell,</p>
<p><strong>DDIM is approximate gradient descent on \(f(x)\) with stepsize \(1-
\sigma_{t-1}/\sigma_t\), with \(\nabla f(x_t)\) estimated by \(\eps_\theta(x_t,
\sigma_t)\).</strong></p>
<p>How should we choose the \(\sigma_t\) schedule? This determines the number and
size of gradient steps we take during sampling. If there are too few steps,
\(\distK(x_t)\) might not decrease and the algorithm may not converge. On the
other hand, if we take many small steps, we need to evaluate the denoiser for as
many times, a computationally expensive operation. This motivates our definition
of admissible schedules.</p>
<p><strong>Definition</strong> An <em>admissible schedule</em> \(\{ \sigma_t \}_{t=0}^T\) ensures
\(\frac{1}{\nu} \distK(x_t) \le \sqrt{n} \sigma_t \le \nu \distK(x_t)\) holds at
each iteration. In particular, a geometrically decreasing (i.e. log-linear)
sequence of \(\sigma_t\) is an admissible schedule.</p>
<p>Our main theorem states that if \(\{\sigma_t\}_{t=0}^T\) is an admissible
schedule and \(\epsilon_\theta(x_t, \sigma_t)\) satisfies our relative error
model, the relative error can be controlled, and the sampling procedure aiming
to minimize distance converges.</p>
<p><strong>Theorem</strong> Let \(x_t\) denote the sequence generated by DDIM and suppose that
\(\nabla \distK(x)\) exists for all \(x_t\) and \(\distK(x_T) = \sqrt n
\sigma_T\). Then</p>
<ol>
<li>\(x_t\) is generated by gradient descent on the squared-distance function
with stepsize \(1 - \sigma_{t-1}/\sigma_{t}\)</li>
<li>\(\distK(x_{t})/\sqrt{n} \approx \sigma_{t}\) for all \(t\)</li>
</ol>
<p>Coming back to our toy example, we can find an admissible schedule by
subsampling from the original log-linear schedule, and implement the DDIM
sampler as follows:</p>
<div><pre><code><span>class</span> <span>Schedule</span><span>:</span>
    <span>...</span>
    <span>def</span> <span>sample_sigmas</span><span>(</span><span>self</span><span>,</span> <span>steps</span><span>:</span> <span>int</span><span>)</span> <span>-&gt;</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>:</span>
        <span>indices</span> <span>=</span> <span>list</span><span>((</span><span>len</span><span>(</span><span>self</span><span>)</span> <span>*</span> <span>(</span><span>1</span> <span>-</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>0</span><span>,</span> <span>steps</span><span>)</span><span>/</span><span>steps</span><span>))</span>
                       <span>.</span><span>round</span><span>().</span><span>astype</span><span>(</span><span>np</span><span>.</span><span>int64</span><span>)</span> <span>-</span> <span>1</span><span>)</span>
        <span>return</span> <span>self</span><span>[</span><span>indices</span> <span>+</span> <span>[</span><span>0</span><span>]]</span>

<span>batchsize</span> <span>=</span> <span>2000</span>
<span>sigmas</span> <span>=</span> <span>schedule</span><span>.</span><span>sample_sigmas</span><span>(</span><span>20</span><span>)</span>
<span>xt</span> <span>=</span> <span>model</span><span>.</span><span>rand_input</span><span>(</span><span>batchsize</span><span>)</span> <span>*</span> <span>sigmas</span><span>[</span><span>0</span><span>]</span>
<span>for</span> <span>sig</span><span>,</span> <span>sig_prev</span> <span>in</span> <span>pairwise</span><span>(</span><span>sigmas</span><span>):</span>
    <span>eps</span> <span>=</span> <span>model</span><span>(</span><span>xt</span><span>,</span> <span>sig</span><span>.</span><span>to</span><span>(</span><span>xt</span><span>))</span>
    <span>xt</span> <span>-=</span> <span>(</span><span>sig</span> <span>-</span> <span>sig_prev</span><span>)</span> <span>*</span> <span>eps</span>
</code></pre></div>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/toy_example_samples_ddim.png" alt="">
</p>
<figcaption>Samples from 20-step DDIM</figcaption>
</figure>

<h4 id="improved-sampler-with-gradient-estimation">Improved sampler with gradient estimation</h4>
<p>Next, we use our interpretation to derive a new efficient sampler. Since
\(\nabla \distK(x)\) is invariant between \(x\) and \(\projK(x)\), we aim to
minimize estimation error \(\sqrt{n} \nabla \distK(x) - \epsilon_{\theta}(x_t,
\sigma_t)\) with the update:</p>
\[\bar\eps_t = \gamma \eps_{\theta}(x_t, \sigma_t) + (1-\gamma) \eps_{\theta}(x_{t+1}, \sigma_{t+1})\]
<p>Intuitively, this update corrects any error made in the previous step using the current estimate:</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/ge_step.png" alt="">
</p>
<figcaption>Our gradient estimation update step</figcaption>
</figure>
<p>This leads to faster convergence compared to the DDIM sampler, as seen from the
samples on our toy model lying closer to the original data.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/toy_example_samples_ge.png" alt="">
</p>
<figcaption>Samples from 20-step gradient estimation sampler</figcaption>
</figure>
<p>Compared to the default DDIM sampler, our sampler can be interpreted as adding
momentum, causing the trajectory to potentially overshoot but converge faster.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/trajectories_varying_gamma.png" alt="">
</p>
<figcaption>Sampling trajectories varying momentum term \(\gamma\)</figcaption>
</figure>
<p>Empirically, adding noise during the generation process also improves the
sampling quality. In order to do so while sticking to our original \(\sigma_t\)
schedule, we need to denoise to a smaller \(\sigma_{t'}\) then add back noise
\(w_t \sim N(0, I)\).</p>
\[x_{t-1} = x_t - (\sigma_t - \sigma_{t'}) \epsilon_\theta(x_t, \sigma_t) + \eta w_t\]
<p>If we assume that \(\mathop{\mathbb{E}}\norm{w_t}^2 = \norm{\epsilon_\theta(x_t,
\sigma_t)}^2\), we should choose \(\eta\) so that the norm of the update is
constant in expectation. This leads to the choice of \(\sigma_{t-1} =
\sigma_t^\mu \sigma_{t-1}^{1-\mu}\) and \(\eta = \sqrt{\sigma_{t-1}^2 -
\sigma_{t'}^2}\) where \(0 \le \mu &lt; 1\). When \(\mu = \frac{1}{2}\), we exactly
recover the <a href="https://arxiv.org/abs/2006.11239">DDPM sampler</a>.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/trajectories_varying_mu.png" alt="">
</p>
<figcaption>Sampling trajectories varying amount of noise added during sampling</figcaption>
</figure>
<p>Our gradient estimation update can be combined with adding noise during
sampling. In summary, our full update step is</p>
\[x_{t-1} = x_t - (\sigma_t - \sigma_{t'}) \bar\eps_t + \eta w_t\]
<p>The full sampler that generalizes DDIM (<code>gam=1, mu=0</code>), DDPM (<code>gam=1, mu=0.5</code>)
and our gradient estimation sampler (<code>gam=2, mu=0</code>) is implemented below.</p>
<div><pre><code><span>@torch.no_grad</span><span>()</span>
<span>def</span> <span>samples</span><span>(</span><span>model</span>      <span>:</span> <span>nn</span><span>.</span><span>Module</span><span>,</span>
            <span>sigmas</span>     <span>:</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>,</span> <span># Iterable with N+1 values for N sampling steps
</span>            <span>gam</span>        <span>:</span> <span>float</span> <span>=</span> <span>1.</span><span>,</span>        <span># Suggested to use gam &gt;= 1
</span>            <span>mu</span>         <span>:</span> <span>float</span> <span>=</span> <span>0.</span><span>,</span>        <span># Requires mu in [0, 1)
</span>            <span>xt</span>         <span>:</span> <span>Optional</span><span>[</span><span>torch</span><span>.</span><span>FloatTensor</span><span>]</span> <span>=</span> <span>None</span><span>,</span>
            <span>batchsize</span>  <span>:</span> <span>int</span> <span>=</span> <span>1</span><span>):</span>
    <span>xt</span> <span>=</span> <span>model</span><span>.</span><span>rand_input</span><span>(</span><span>batchsize</span><span>)</span> <span>*</span> <span>sigmas</span><span>[</span><span>0</span><span>]</span>
    <span>eps</span> <span>=</span> <span>None</span>
    <span>for</span> <span>i</span><span>,</span> <span>(</span><span>sig</span><span>,</span> <span>sig_prev</span><span>)</span> <span>in</span> <span>enumerate</span><span>(</span><span>pairwise</span><span>(</span><span>sigmas</span><span>)):</span>
        <span>eps</span><span>,</span> <span>eps_prev</span> <span>=</span> <span>model</span><span>(</span><span>xt</span><span>,</span> <span>sig</span><span>.</span><span>to</span><span>(</span><span>xt</span><span>)),</span> <span>eps</span>
        <span>eps_av</span> <span>=</span> <span>eps</span> <span>*</span> <span>gam</span> <span>+</span> <span>eps_prev</span> <span>*</span> <span>(</span><span>1</span><span>-</span><span>gam</span><span>)</span>  <span>if</span> <span>i</span> <span>&gt;</span> <span>0</span> <span>else</span> <span>eps</span>
        <span>sig_p</span> <span>=</span> <span>(</span><span>sig_prev</span><span>/</span><span>sig</span><span>**</span><span>mu</span><span>)</span><span>**</span><span>(</span><span>1</span><span>/</span><span>(</span><span>1</span><span>-</span><span>mu</span><span>))</span> <span># sig_prev == sig**mu sig_p**(1-mu)
</span>        <span>eta</span> <span>=</span> <span>(</span><span>sig_prev</span><span>**</span><span>2</span> <span>-</span> <span>sig_p</span><span>**</span><span>2</span><span>).</span><span>sqrt</span><span>()</span>
        <span>xt</span> <span>=</span> <span>xt</span> <span>-</span> <span>(</span><span>sig</span> <span>-</span> <span>sig_p</span><span>)</span> <span>*</span> <span>eps_av</span> <span>+</span> <span>eta</span> <span>*</span> <span>model</span><span>.</span><span>rand_input</span><span>(</span><span>batchsize</span><span>).</span><span>to</span><span>(</span><span>xt</span><span>)</span>
        <span>yield</span> <span>xt</span>
</code></pre></div>
<h4 id="large-scale-examples">Large-scale examples</h4>
<p>The training code above not only works for our toy dataset, they can also be
used to train image diffusion models from scratch. See <a href="https://github.com/yuanchenyang/smalldiffusion/blob/main/examples/fashion_mnist.py">this
example</a> for an example of training on the FashionMNIST
dataset to get a second-place FID score on <a href="https://paperswithcode.com/sota/image-generation-on-fashion-mnist">this
leaderboard</a>:</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/fashion-mnist-samples-large.png" alt="">
</p>
<figcaption>Samples from a diffusion model trained on the FashionMNIST dataset</figcaption>
</figure>
<p>The sampling code works without modifications to sample from state-of-the-art
pretrained latent diffusion models:</p>
<div><pre><code><span>schedule</span> <span>=</span> <span>ScheduleLDM</span><span>(</span><span>1000</span><span>)</span>
<span>model</span>    <span>=</span> <span>ModelLatentDiffusion</span><span>(</span><span>'</span><span>stabilityai/stable-diffusion-2-1-base</span><span>'</span><span>)</span>
<span>model</span><span>.</span><span>set_text_condition</span><span>(</span><span>'</span><span>An astronaut riding a horse</span><span>'</span><span>)</span>
<span>*</span><span>xts</span><span>,</span> <span>x0</span> <span>=</span> <span>samples</span><span>(</span><span>model</span><span>,</span> <span>schedule</span><span>.</span><span>sample_sigmas</span><span>(</span><span>50</span><span>))</span>
<span>decoded</span>  <span>=</span> <span>model</span><span>.</span><span>decode_latents</span><span>(</span><span>x0</span><span>)</span>
</code></pre></div>
<p>We can visualize the different effects of our momentum term \(\gamma\) on high
resolution text-to-image generation.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/sd_examples.jpg" alt="">
</p>
<figcaption>Samples from a pretrained stable diffusion model</figcaption>
</figure>
<h4 id="other-resources">Other resources</h4>
<p>Also recommended are the following blog posts on diffusion models:</p>
<ol>
<li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" target="_blank">What are diffusion models</a> introduces
diffusion models from the discrete-time perspective of reversing a Markov
process</li>
<li><a href="https://yang-song.net/blog/2021/score/" target="_blank">Generative modeling by estimating gradients of the data
distribution</a> introduces diffusion models from
the continuous time perspective of reversing a stochastic differential
equation</li>
<li><a href="https://huggingface.co/blog/annotated-diffusion" target="_blank">The annotated diffusion model</a> goes over
a pytorch implementation of a diffusion model in detail</li>
</ol>

</section>
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The later we meet someone in a sequence, the more negatively we describe them (153 pts)]]></title>
            <link>https://suchscience.org/people-encountered-later-in-a-sequence-described-more-negatively/</link>
            <guid>39672111</guid>
            <pubDate>Mon, 11 Mar 2024 19:05:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://suchscience.org/people-encountered-later-in-a-sequence-described-more-negatively/">https://suchscience.org/people-encountered-later-in-a-sequence-described-more-negatively/</a>, See on <a href="https://news.ycombinator.com/item?id=39672111">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>Imagine you’re the 20th candidate to interview for your dream job, or the 10th hopeful stepping onto the stage to audition for a coveted role. You’re just as qualified and just as talented as those who went before you. </p>



<p>But according to a new study, you might be at a surprising disadvantage, thanks to an unconscious bias in how we perceive and describe others. It found that the later we encounter someone in a sequence, the more negatively we tend to describe them.</p>



<p><a href="https://pubmed.ncbi.nlm.nih.gov/38421750/" target="_blank" rel="noopener">The study</a> was published in the <em>Journal of Personality and Social Psychology</em> on February 29, 2024.</p>



<h2>The Serial Position-Negativity Effect</h2>



<p>The researchers coined this phenomenon the “serial position-negativity effect.” </p>



<p>They hypothesized that when we sequentially encounter people, we focus on distinct attributes that differentiate each new person from those we’ve already met. And because distinct attributes tend to be negative in the grand scheme of things, our descriptions of later-encountered people become increasingly negative.</p>



<p>To test this, the researchers conducted a number of studies. In one, they had 992 participants (recruited from Prolific Academic) describe 20 people based on their Facebook profile pictures. </p>



<p>The participants described the first few individuals quite positively, using an average of 6.2 positive words each. But as they progressed through the sequence, their descriptions became significantly more negative, dipping to an average of just 4.7 positive words by the 20th person.</p>



<p>In another experiment, 987 participants (about evenly split between male and female, with an average age of 42) were shown short video clips of women introducing themselves on the popular TV show <em>The Bachelor</em>. In these clips, each woman tried to make a memorable first impression on the bachelor, often in creative and attention-grabbing ways.</p>



<p>As the study participants progressed through the sequence of videos, their descriptions of the women became increasingly negative: the tenth woman was described significantly more negatively, on average, than the first woman, despite the fact that the order of the videos was randomized for each participant.</p>



<p>And their descriptions also became increasingly specific over time. As participants encountered more women, they focused more on what made each new woman stand out, leading to more unique and ultimately more negative descriptions.</p><!-- wp:shortcode -->


<!-- /wp:shortcode -->



<h2>Further Directions</h2>



<p>The study opens up many possibilities for future research. The researchers next want to examine how our personal quirks and the groups we belong to might amp up or tone down this negative bias towards people we meet later on. </p>



<p>They also want to see how long these snap judgments stick around, and how they might be throwing a wrench into things like job interviews and performance reviews in the real world.</p>



<h2>Implications </h2>



<p>The research suggests that this unconscious bias could disadvantage people who happen to be evaluated later in a sequence, whether it’s job applicants, contestants on a reality show, or Tinder dates. </p>



<p>So the next time you’re in a situation where you’re meeting a lot of new people sequentially, whether a networking event or speed dating night, keep in mind that your impressions may be tainted by this subtle cognitive bias. </p>



<h2>Study information:</h2>



<ul>
<li><strong>Journal: </strong><a href="https://www.apa.org/pubs/journals/psp" target="_blank" rel="noopener">Journal of Personality and Social Psychology</a></li>



<li><strong>Publication Date:</strong> February 29, 2024 (Online ahead of print)</li>



<li><strong>DOI: </strong>10.1037/pspa0000383</li>



<li><strong>Title:</strong> “<a href="https://pubmed.ncbi.nlm.nih.gov/38421750/" target="_blank" rel="noopener">Differentiation in social perception: Why later-encountered individuals are described more negatively</a>“</li>



<li><strong>Authors: </strong>
<ul>
<li>Alex Koch: Booth School of Business, University of Chicago</li>



<li>Andrew Bromley: Booth School of Business, University of Chicago</li>



<li>Johanna Woitzel: Department of Psychology, Ruhr University Bochum</li>



<li>Hans Alves: Department of Psychology, Ruhr University Bochum</li>
</ul>
</li>
</ul>



		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kdenlive 24.02 open source video editor released (206 pts)]]></title>
            <link>https://kdenlive.org/en/2024/03/kdenlive-24-02-0-released/</link>
            <guid>39671218</guid>
            <pubDate>Mon, 11 Mar 2024 17:46:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kdenlive.org/en/2024/03/kdenlive-24-02-0-released/">https://kdenlive.org/en/2024/03/kdenlive-24-02-0-released/</a>, See on <a href="https://news.ycombinator.com/item?id=39671218">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The team is thrilled to introduce the much-anticipated release of Kdenlive 24.02, featuring a substantial upgrade to our frameworks with the adoption of <em>Qt6</em> and <em>KDE Frameworks 6</em>. This significant under-the-hood transformation establishes a robust foundation, shaping the trajectory of Kdenlive for the next decade. The benefits of this upgrade are particularly noteworthy for Linux users, as improved Wayland support enhances the overall experience. Additionally, users on Windows, MacOS, and Linux will experience a substantial performance boost since Kdenlive now runs natively on <em>DirectX</em>, <em>Metal</em>, and <em>Vulkan </em>respectively, replacing the previous abstraction layer reliance on <em>OpenGL</em> and <em>Angle</em>, resulting in a more efficient and responsive application. This upgrade brings significant changes to packaging, featuring the introduction of a dedicated package for <em>Apple Silicon</em>, the discontinuation of <em>PPA</em> support and an enhanced method for installing the <em>Whisper</em> and <em>Vosk</em> speech-to-text engines.</p>
<p>While a significant effort has been invested in providing a stable user experience in this transition, we want to acknowledge that, like any evolving software, there might be some rough edges. Some known issues include: themes and icons not properly applied in Windows and AppImage, text not properly displayed in clips in the timeline when using Wayland and a crash in the Subtitle Manager under MacOS. Worth noting also is the temporary removal of the audio recording feature pending its migration to Qt6. We appreciate your understanding and encourage you to provide feedback in this release cycle so that we can continue refining and improving Kdenlive. In the upcoming release cycles (24.05 and 24.08), our development efforts will concentrate on stabilizing any remaining issues stemming from this upgrade. We’ll also prioritize short-term tasks outlined in our <a href="https://kdenlive.org/en/kdenlive-roadmap/">roadmap</a>, with a specific emphasis on enhancing performance and streamlining the effects workflow.</p>
<p>In terms of performance enhancements, this release introduces optimized RAM usage during the import of clips into the Project Bin. Furthermore, it addresses Nvidia encoding and transcoding issues with recent ffmpeg versions.</p>
<p>To safeguard project integrity, measures have been implemented to prevent corruptions. Projects with non-standard and variable frame rates are not allowed to be created. When rendering a project containing variable frame rate clips, users will receive a warning with the option to transcode these clips, mitigating potential audio-video synchronization issues.</p>
<p>Users can now enjoy the convenience of an automatic update check <strong>without</strong> an active network connection. Glaxnimate animations now default to the rawr format, replacing Lottie. Furthermore, we’ve introduced an FFv1 render preset to replace the previously non-functional Ut Video. And multiple project archiving issues have been fixed.</p>
<p>Beyond performance and stability we’ve managed to sneak in several nifty quality-of-life and usability improvements, the highlights include:</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemma doesn't suck anymore – 8 bug fixes (128 pts)]]></title>
            <link>https://colab.research.google.com/drive/1fxDWAfPIbC-bHwDSVj5SBmEJ6KG3bUu5?usp=sharing</link>
            <guid>39671146</guid>
            <pubDate>Mon, 11 Mar 2024 17:41:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://colab.research.google.com/drive/1fxDWAfPIbC-bHwDSVj5SBmEJ6KG3bUu5?usp=sharing">https://colab.research.google.com/drive/1fxDWAfPIbC-bHwDSVj5SBmEJ6KG3bUu5?usp=sharing</a>, See on <a href="https://news.ycombinator.com/item?id=39671146">Hacker News</a></p>
<div id="readability-page-1" class="page"><div ng-non-bindable="" data-ogsr-up="" id="gb"><p><a aria-label="Sign in" href="https://accounts.google.com/ServiceLogin?passive=true&amp;continue=https%3A%2F%2Fcolab.research.google.com%2Fdrive%2F1fxDWAfPIbC-bHwDSVj5SBmEJ6KG3bUu5%3Fusp%3Dsharing&amp;ec=GAZAqQM" target="_top"><span>Sign in</span></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JSON Canvas – An open file format for infinite canvas data (527 pts)]]></title>
            <link>https://jsoncanvas.org/</link>
            <guid>39670922</guid>
            <pubDate>Mon, 11 Mar 2024 17:22:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jsoncanvas.org/">https://jsoncanvas.org/</a>, See on <a href="https://news.ycombinator.com/item?id=39670922">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="readme" data-node-type="file" data-node-file="readme.md">
          <p>readme</p>
          <h2 id="an-open-file-format-for-infinite-canvas-data">An open file format for infinite canvas data.</h2>

<p>Infinite canvas tools are a way to view and organize information spatially, like a digital whiteboard. Infinite canvases encourage freedom and exploration, and have become a popular interface pattern across many apps.</p>

<p>The JSON Canvas format was created to provide longevity, readability, interoperability, and extensibility to data created with infinite canvas apps. The format is designed to be easy to parse and give users <a href="https://stephango.com/file-over-app">ownership over their data</a>. JSON Canvas files use the <code>.canvas</code> extension.</p>

<p>JSON Canvas was originally created for <a href="https://obsidian.md/blog/json-canvas/">Obsidian</a>. JSON Canvas can be implemented freely as an import, export, and storage format for any app or tool. This site, and all the resources associated with JSON Canvas are <a href="https://github.com/obsidianmd/jsoncanvas">open source</a> under the MIT license.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Are We Watching the Internet Die? (166 pts)]]></title>
            <link>https://www.wheresyoured.at/are-we-watching-the-internet-die/</link>
            <guid>39670900</guid>
            <pubDate>Mon, 11 Mar 2024 17:19:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wheresyoured.at/are-we-watching-the-internet-die/">https://www.wheresyoured.at/are-we-watching-the-internet-die/</a>, See on <a href="https://news.ycombinator.com/item?id=39670900">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      <p>Sometime this month,<a href="https://www.cnbc.com/2024/03/01/reddit-seeking-a-valuation-of-up-to-6point5-billion-in-ipo.html?ref=wheresyoured.at"> <u>Reddit will go public at a valuation of $6.5bn</u></a>.<a href="https://www.cnn.com/2024/02/26/tech/reddit-ipo-users-can-buy-shares/index.html?ref=wheresyoured.at"> <u>Select Redditors were offered the chance to buy stock at the initial listing price</u></a>, which it hasn’t announced yet but is <a href="https://www.ft.com/content/b3199303-d419-482d-96c7-e73c0b0ee8ed?ref=wheresyoured.at"><u>expected to be in the range of $31-34 per share</u></a>. Regardless of the actual price, I wouldn’t be surprised if Reddit shares quickly fall below the IPO price, based on the fact that Reddit is an absolute dog of a company,<a href="https://www.reuters.com/technology/reddit-makes-us-ipo-filing-public-2024-02-22/?ref=wheresyoured.at"> <u>losing $90.8 million on $804 million of revenue in 2023</u></a> and <a href="https://www.cnn.com/2024/02/23/tech/reddit-ipo-filing-business-plan/index.html?ref=wheresyoured.at"><u>never having turned a profit</u></a>.<a href="https://www.sec.gov/Archives/edgar/data/1713445/000162828024006294/reddits-1q423.htm?ref=wheresyoured.at"> <u>Reddit's S1</u></a><u> </u>(the initial registration form for taking a company public) laughably claims that advertising on the site is "rapidly evolving" and that it is "still in the early phases of growing this business,"<a href="https://techcrunch.com/2009/11/12/reddit-advertising/?ref=wheresyoured.at"> <u>with "this business" referring to one that Reddit launched 15 years ago</u></a>.</p><p>The Reddit IPO is one of the biggest swindles in corporate history, where millions of unpaid contributors made billions of posts so that<a href="https://www.bizjournals.com/sanfrancisco/inno/stories/news/2024/02/23/reddit-ceo-steve-huffman-total-compensation-2023.html?ref=wheresyoured.at#:~:text=Bay%20Area%20Inno%20-%20Reddit%20CEO,exceeded%20%24193M%20last%20year"> <u>CEO Steve Huffman could make $193 million</u></a> in 2023<a href="https://www.reuters.com/technology/reddit-lay-off-about-5-workforce-wsj-2023-06-06/?ref=wheresyoured.at"> <u>while laying off 90 people</u> and</a> effectively <a href="https://www.theverge.com/2023/6/15/23762501/reddit-ceo-steve-huffman-interview-protests-blackout?ref=wheresyoured.at"><u>pushing third party apps off of the platform</u></a> by charging exorbitant rates for API access, which in <a href="https://www.theverge.com/2023/6/12/23755974/reddit-subreddits-going-dark-private-protest-api-changes?ref=wheresyoured.at"><u>turn prompted several prolonged “strikes” by users</u></a>, with some of the most popular subreddits going silent for a short period of time. Reddit, in turn, effectively “couped” these subreddits, replacing their longstanding moderators with <a href="https://www.theverge.com/2023/7/20/23802370/reddit-over-reopens-subreddit-protest-male-fashion-advice?ref=wheresyoured.at"><u>ones of its own choosing</u></a> — people who would <a href="https://arstechnica.com/gadgets/2023/07/reddit-calls-for-a-few-new-mods-after-axing-polarizing-some-of-its-best/?ref=wheresyoured.at"><u>happily toe the party line and reopen them to the public</u></a>.&nbsp;</p><p>None of the people that spent hours of their lives lovingly contributing to Subreddits, or performing the vital-but-thankless role of moderation, will make a profit off of Reddit's public listing, but<a href="https://www.cnbc.com/2024/02/22/openai-ceo-sam-altman-stands-to-net-millions-as-reddit-goes-public.html?ref=wheresyoured.at"> <u>Sam Altman will make hundreds of millions of dollars for his $50 million investment from 2014</u></a>.<a href="https://www.reuters.com/technology/reddit-ai-content-licensing-deal-with-google-sources-say-2024-02-22/?ref=wheresyoured.at"> <u>Reddit also announced that it had cut a $60 million deal to allow Google to train its models on Reddit's posts</u></a>, once again offering users nothing in return for their hard work.</p><p><a href="https://www.sec.gov/Archives/edgar/data/1713445/000162828024006294/reddits-1q423.htm?ref=wheresyoured.at#:~:text=As%20a%20way,enjoyable%20as%20possible."><u>Huffman's letter to investors</u></a> waxes poetic about Redditors' "deep sense of ownership over the communities they create," and justifies taking the company public by claiming that he wants "this sense of ownership to be reflected in real ownership" as he offers them a chance to buy non-voting stock in a company that they helped enrich. Huffman ends his letter by saying that Reddit is "one of the internet's largest corpuses of authentic and constantly updated human-generated experience" before referring to it as the company's "data advantage and intellectual property," describing Redditors' posts as "data [that] constantly grows and regenerates as users converse."</p><p>We're at the end of a vast, multi-faceted con of internet users, where ultra-rich technologists tricked their customers into building their companies for free. And while the trade once seemed fair, it's become apparent that these executives see users not as willing participants in some sort of fair exchange, but as veins of data to be exploitatively mined as many times as possible, given nothing in return other than access to a platform that may or may not work properly.</p><p>This is, of course, the crux of Cory Doctorow's<a href="https://en.wikipedia.org/wiki/Enshittification?ref=wheresyoured.at"> <u>Enshittification theory</u></a>, where Reddit has moved from pleasing users to pleasing its business customers to, now, pleasing shareholders at what will inevitably be the cost of the platform's quality.</p><p>Yet what's happening to the web is far more sinister than simple <em>greed</em>, but the destruction of the user-generated internet, where executives think they've found a way to replace human beings making cool things with generative monstrosities trained on datasets controlled and monetized by trillion-dollar firms.</p><p>Their ideal situation isn't one where you visit distinct websites with content created by human beings, but a return to the dark ages of the internet where most traffic ran through a series of heavily-curated portals operated by a few select companies, with results generated based on datasets that are <a href="https://futurism.com/ai-trained-ai-generated-data-interview?ref=wheresyoured.at"><u>increasingly poisoned by generative content</u></a> built to fill space rather than be consumed by a customer.</p><p>The algorithms are easily-tricked, and the tools used to trick them are becoming easier to use and scale.</p><p>And it's slowly killing the internet.</p><h2 id="degenerative-ai"><strong>Degenerative AI</strong></h2><p>After the world's governments began their above-ground nuclear weapons tests in the mid-1940s, radioactive particles made their way into the atmosphere, permanently tainting all modern steel production, making it challenging (or impossible) to build certain machines (such as those that measure radioactivity). As a result, we've a limited supply of something called "<a href="https://qz.com/emails/quartz-obsession/1849564217/low-background-metal-pure-unadulterated-treasure?ref=wheresyoured.at"><u>low-background steel</u></a>," pre-war metal that oftentimes has to be harvested from ships sunk before the first detonation of a nuclear weapon, including <a href="https://www.theatlantic.com/science/archive/2019/10/search-dark-matter-depends-ancient-shipwrecks/600718/?ref=wheresyoured.at"><u>those dating back to the Roman Empire</u></a>.</p><p>Generative AI models are trained by using massive amounts of text scraped from the internet, meaning that the consumer adoption of generative AI has brought a degree of radioactivity to its own dataset. As more internet content is created, either partially or entirely through generative AI, the models themselves will find themselves increasingly inbred, training themselves on content written by their own models which are, on some level,<a href="https://lowbackgroundsteel.ai/?ref=wheresyoured.at"> <u>permanently locked in 2023</u></a>, before the advent of a tool that is specifically intended to replace content created by human beings.</p><p>This is a phenomenon that Jathan Sadowski calls "<a href="https://twitter.com/jathansadowski/status/1625245803211272194?lang=en&amp;ref=wheresyoured.at"><u>Habsburg AI</u></a>," where "a system that is so heavily trained on the outputs of other generative AIs that it becomes an inbred mutant, likely with exaggerated, grotesque features." In reality, a Habsburg AI will be one that is increasingly more <em>generic</em> and <em>empty</em>, normalized into a slop of anodyne business-speak as its models are trained on increasingly-identical content.</p><p>LinkedIn, already a repository of empty-headed corpo-nonsense,<a href="https://techcrunch.com/2023/03/15/linkedin-expands-its-generative-ai-assistant-to-recruitment-ads-and-writing-profiles/?ref=wheresyoured.at"> <u>already lets users write generate messages, profiles and job descriptions using AI</u></a>, and<a href="https://www.linkedin.com/help/linkedin/answer/a5538339?ref=wheresyoured.at#:~:text=For%20example%2C%20some%20of%20our,data%20from%20the%20training%20dataset."> <u>anything you create using these generative features is immediately fed back into Azure's OpenAI models owned by its parent company Microsoft</u></a>, which invested $10 billion in OpenAI in early 2023. While LinkedIn is yet to introduce fully-automated replies,<a href="https://twitter.com/nilansaha/status/1762390930550927823?ref=wheresyoured.at"> <u>Chrome extensions already exist to flood the platform with generic responses</u></a>, feeding more genericisms into the mouth of Microsoft and OpenAI's models.</p><p>Generative AI also naturally aligns with the toxic incentives created by the largest platforms. Google's algorithmic catering to the Search Engine Optimization industry naturally benefits those who can spin up large amounts of "relevant" content rather than content created by humans.<a href="https://searchengineland.com/google-released-massive-search-quality-improvements-with-march-2024-core-update-and-multiple-spam-updates-438144?ref=wheresyoured.at"> <u>While Google has claimed that their upcoming "core" update will help promote "content for people and not to rank in search engines,"</u></a><a href="https://www.theregister.com/2022/08/19/google_search_algorithm/?ref=wheresyoured.at"> <u>it’s made this promise before</u></a>, and I severely doubt anything meaningfully changes. After all, Google makes up more than 85% of all search traffic and<a href="https://www.theverge.com/2023/10/26/23933206/google-apple-search-deal-safari-18-billion?ref=wheresyoured.at"> <u>pays Apple billions a year to make Google search the default on Apple devices</u></a>.</p><p>And because these platforms were built to reward scale and volume far more often than quality, AI naturally rewards those who can find the spammiest ways to manipulate the algorithm. 404 Media reports that<a href="https://www.404media.co/inside-the-world-of-tiktok-spammers-and-the-ai-tools-that-enable-them/?ref=wheresyoured.at"> <u>spammers are making thousands of dollars from TikTok's creator program by making "faceless reels" where AI-generated voices talk over spliced-together videos ripped from YouTube</u></a>, and<a href="https://www.instagram.com/reel/C1ZidLAuksO/?ref=404media.co"> <u>a cottage industry of automation gurus</u></a> are cashing in by helping others flood Facebook, TikTok and Instagram with low-effort videos that are irresistible to algorithms.</p><p>Amazon's Kindle eBook platform has been flooded with AI-generated content that<a href="https://www.wired.com/story/scammy-ai-generated-books-flooding-amazon/?ref=wheresyoured.at"> <u>briefly dominated bestseller lists</u></a>,<a href="https://arstechnica.com/information-technology/2023/09/ai-generated-books-force-amazon-to-cap-ebook-publications-to-3-per-day/?ref=wheresyoured.at"> <u>forcing Amazon to limit authors to publishing three books a day</u></a>.<a href="https://www.wired.com/story/scammy-ai-generated-books-flooding-amazon/?ref=wheresyoured.at"> <u>This hasn't stopped spammers from publishing awkward rewrites and summaries of other people's books</u></a>, and because Amazon's policies don't outright ban AI-generated content, ChatGPT has become an inoperable cancer on the body of the publishing industry.</p><p>"Handmade" goods store Etsy has its own AI problem,<a href="https://www.theatlantic.com/technology/archive/2023/06/ai-chatgpt-side-hustle/674415/?ref=wheresyoured.at"> <u>with The Atlantic reporting last year</u></a> that the platform was now pumped full of AI-generated art, t-shirts and mugs that, in turn, use ChatGPT to optimize listings to rank highly in Google search. As a profitable public company, Etsy has little incentive to change things, even if the artisanal products on the platform are being crowded out by generative art pasted on drop-shipped shirts.<a href="https://www.pymnts.com/artificial-intelligence-2/2023/ebay-to-add-ai-powered-image-based-listing-tool/?ref=wheresyoured.at"> <u>eBay, on the other hand, is leaning into the spam, offering tools to generate entire listings based on a single image using generative AI</u></a>.</p><p>The Wall Street Journal reported last year that<a href="https://www.wsj.com/articles/chatgpt-already-floods-some-corners-of-the-internet-with-spam-its-just-the-beginning-9c86ea25?ref=wheresyoured.at"> <u>magazines are now inundated with AI-generated pitches for articles</u></a>, and<a href="https://www.theguardian.com/technology/2023/feb/21/sci-fi-publisher-clarkesworld-halts-pitches-amid-deluge-of-ai-generated-stories?ref=wheresyoured.at"> <u>renowned sci-fi publisher Clarkesworld was forced to close submissions after receiving an overwhelming amount of AI-generated stories</u></a>. Help A Reporter Out used to be a way for journalists to find potential sources and quotes, except requests are now met with a deluge of AI-generated spam.</p><p>These stories are, of course, all manifestations of a singular problem: that generative artificial intelligence is poison for an internet dependent on algorithms.</p><p>There are simply too many users, too many websites and too many content providers to manually organize and curate the contents of the internet, making algorithms necessary for platforms to provide a service. Generative AI is a perfect tool for soullessly churning out content to match a particular set of instructions — such as those that an algorithm follows — and while an algorithm can theoretically be tuned to evaluate content as "human," so can scaled content be tweaked to make it <em>seem</em> more human.</p><p>Things get worse when you realize that the sheer <em>volume</em> of internet content makes algorithmic recommendations a necessity to sift through an ever-growing pile of crap. Generative AI<a href="https://twitter.com/sugarsmorecake/status/1763941484653343056?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1763941484653343056%7Ctwgr%5Ea872ae864382ff6ce94a7bc72d711eaacb954876%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fwww.404media.co%2Finside-the-world-of-tiktok-spammers-and-the-ai-tools-that-enable-them%2F&amp;ref=wheresyoured.at"> <u>allows creators to weaponize the algorithms' weaknesses</u></a> to monetize and popularize low-effort crap, and ultimately, what is a platform to do? Ban anything that uses AI-generated content? Adjust the algorithm to penalize videos without people's faces? How does a platform judge the difference between a popular video and a video that the platform made popular? And if these videos are made by humans and enjoyed by humans, why <em>should</em> it stop them?</p><p>Google might <em>pretend</em> it cares about the quality of search results, but nothing about search's decade-long decline has suggested it’s actually going to do anything.<a href="https://developers.google.com/search/docs/essentials/spam-policies?hl=en&amp;ref=wheresyoured.at"> <u>Google's spam policies</u></a> have claimed<a href="https://web.archive.org/web/20221207161654/https://developers.google.com/search/docs/essentials/spam-policies?hl=en"> <em><u>for years</u></em></a> that scraped content (outright ripping the contents of another website) was grounds for removal from Google, but even the most cursory glance at any news search shows how often sites thinly rewrite or outright steal others' content. And I can't express enough how bad (yet inevitable) the existence of the<a href="https://www.acumenresearchandconsulting.com/search-engine-optimization-services-market?ref=wheresyoured.at#:~:text=The%20market%20size%20of%20search,USD%2046.7%20Billion%20in%202021."> <u>$40 billion Search Engine Optimization industry</u></a> is, and how much of a boon being able to semi-automate the creation and optimization of content<a href="https://developers.google.com/search/docs/essentials?ref=wheresyoured.at"> <u>to the standards of an algorithm that Google has explained in exhaustive detail</u></a>. While it's plausible that Google might genuinely try and fight the influx of SEO-generated articles, one has to wonder why it’d bother to try now after spending decades catering to the industry.</p><p>As we speak, the battle that platforms are fighting is against <em>generative spam</em>, a cartoonish and obvious threat of outright nonsense, meaningless chum that can and should (and likely will) be stopped. In the process, they're failing to see that this isn't a war against <em>spam</em>, but a war against <em>crap</em>, and the overall normalization and intellectual numbing that comes when content is created to please algorithms and provide a minimum viable product for consumers. Google's "useless" results problem isn't one borne of content that has no meaning, but of content that only sort of helps, that is the "right" result but doesn't actually provide any real thought behind it, like the endless "how to fix error code X" results full of well-meaning and plausibly helpful content that doesn't really help at all.</p><p>The same goes for Etsy and Amazon. While Etsy's "spam" is an existential threat to actual artisans building something with their hands, it's not actual spam — it's cheaply-made crap that nevertheless fulfills a need and <em>sort of </em>fits Etsy's remit. Amazon doesn't have any incentive to get rid of low-quality books that sell for<a href="https://nymag.com/intelligencer/2023/01/why-does-it-feel-like-amazon-is-making-itself-worse.html?ref=wheresyoured.at"> <u>the same reason that it doesn't get rid of its other low-quality items</u></a>. People aren't looking for the best, they're looking to fulfill a need, even if that need is fulfilled with poorly-constructed crap. </p><p>Platforms likely conflate positioning with popularity, failing to see the self-fulfilling prophecy of an algorithm making stuff popular because said stuff is built to please the algorithm creating more demand for content to please the algorithm. "Viral" content is no longer a result of lots of people deciding that they find something interesting — it's a condition created by algorithms manipulated by forces that are getting stronger and more nuanced thanks to generative AI.</p><p>We're watching the joint hyper-scaling and hyper-normalization of the internet, where all popular content begins to look the same to appeal to algorithms run by companies obsessed with growth. Quality control in AI models only exists to stop people from nakedly exploiting the network through unquestionably iniquitous intent, rather than people making shitty stuff that kind of sucks but gets popular because an algorithm says so.</p><p>This isn't a situation where these automated tools are giving life to new forms of art or interesting new concepts, but regurgitations of an increasingly less unique internet, <em>because these models are trained on data drawn from the internet.</em> Like a plant turning to capture sunlight,<a href="https://www.wheresyoured.at/the-anti-economy/"> <u>parts of the internet have already twisted toward the satisfaction of algorithms</u></a>, and as others become dependent on generative AI (<a href="https://slate.com/technology/2024/02/quora-what-happened-ai-decline.html?ref=wheresyoured.at"><u>like Quora, which now promotes ChatGPT-generated answers at the top of results</u></a>), so will the web become more dependent and dictated by automated systems.</p><p>The ultimate problem is that this morass of uselessness will lead companies like Google to force their generative AIs to "fix" the problem by generating answers to sift through the crap.<a href="https://www.theverge.com/2023/8/14/23831391/amazon-review-summaries-generative-ai?ref=wheresyoured.at"> <u>Amazon now summarizes reviews using generative AI</u></a>, legitimizing<a href="https://www.wired.com/story/fake-amazon-reviews-underground-market/?ref=wheresyoured.at"> <u>the thousands of faked and paid-for reviews on the platform</u></a> and presenting them as verified and trusted information from Amazon itself.<a href="https://blog.google/products/search/google-search-generative-ai-international-expansion/?ref=wheresyoured.at"> <u>Google has already been experimenting with its "Search Generative Experience</u></a>" that <a href="https://www.theverge.com/2023/8/15/23833045/google-artificial-intelligence-summary-chrome-sge?ref=wheresyoured.at"><u>summarizes entire articles on iOS and Chrome</u></a>, and Microsoft's Bing search has already integrated summaries from Copilot, with both basing their answers off of a combination of search and training data.</p><p>Yet in doing so, these platforms gain a dangerous hold on the world's information.<a href="https://www.searchenginejournal.com/google-announces-deal-to-show-more-reddit-content/509132/?ref=wheresyoured.at"> <u>Google's deal with Reddit also gave it real time access to Reddit's content</u></a>, allowing it to show Reddit posts natively in search (and directly access Reddit posts data for training purposes). Yet at some point these portals will generate an answer based off of the data they have (or have access to, <a href="https://www.404media.co/tumblr-and-wordpress-to-sell-users-data-to-train-ai-tools/?ref=wheresyoured.at"><u>in the case of Tumblr and Wordpress</u></a>) rather than linking you to a place where you can find an answer by reading something created by another person. There could be a future where the majority of web users experience the web through a series of portals,<a href="https://www.theverge.com/2024/1/28/24053882/arc-search-browser-web-app-ios?ref=wheresyoured.at"> <u>like Arc Search's "browse for me" feature, which visits websites for you and summarizes their information using AI</u></a>.</p><p>Right now, the internet is controlled by a few distinct platforms, each one intent on interrupting the exploratory and creative forces that made the web great. I believe that their goal is to intrude on our ability to browse the internet, to further obfuscate the source of information while paying the platforms for content that their users make for free. Their eventual goal, in my mind, is to remove as much interaction with the larger internet as possible, summarizing and regurgitating as much as they can so that they can control and monetize the results as much as possible.</p><p>On some level, I fear that the current platforms intend to use AI to become something akin to an Internet Service Provider, offering "clean" access to a web that has become too messy and unreliable as a direct result of the platforms' actions, eventually finding ways to monetize your information's prominence in their portals, models and chatbots. As that happens, it will begin to rot out the rest of the internet, depriving media entities and social networks of traffic as executives like Steve Huffman cut further deals to monetize free labor with platforms that will do everything they can to centralize all internet traffic to two or three websites.</p><p>And as the internet becomes dominated by these centralized platforms and the sites they trawl for content, so begins the vicious cycle of the Habsburg AI. OpenAI's ChatGPT and Anthropic's Claude are dependent on a constant flow of training data to improve their models, to the point that it's<a href="https://www.theguardian.com/technology/2024/jan/08/ai-tools-chatgpt-copyrighted-material-openai?ref=wheresyoured.at"> <u>effectively impossible for them to operate without violating copyright</u></a>. As a result, they can't be too picky when it comes to the information they choose, meaning that they're more than likely going to depend on openly-available content from the internet, which as I've suggested earlier will become increasingly normalized by the demands of algorithms and the ease of automating the generic content that satisfies them.</p><p>I am not saying that user-generated content will <em>disappear,</em> but that human beings cannot create content at the scale that automation can, and<a href="https://www.theverge.com/23753963/google-seo-shopify-small-business-ai?ref=wheresyoured.at"> <u>when a large chunk of the internet is content for robots</u></a>, <em>that</em> is the content that will inform tomorrow's models. The only thing that can truly make them better is <em>more stuff</em>, but when the majority of stuff being created isn't good, or interesting, or even written for a human being, ChatGPT or Claude's models will learn the rotten habits of rotten content. This is why so many models' responses sound so similar — they're heavily dependent on the stuff they're fed for their outputs, and so much of their "intelligence" comes from the same training data.</p><p>It's a different flavor of the same problem — these models don't really "know" anything. They're copying other people's homework.</p><blockquote>As an aside, I also fear for the software code that's created by generative AI products like GitHub Co-pilot.<a href="https://www.infoworld.com/article/3713141/github-copilot-makes-insecure-code-even-less-secure-snyk-says.html?ref=wheresyoured.at"> <u>A study by security firm Snyk</u></a> found that GitHub Copilot and other AI-powered coding platforms, which were trained on publicly-available code (and based on the user's own codebase), can replicate existing security issues, proliferating problems rather than fixing them.<a href="https://cyber.nyu.edu/2021/10/15/ccs-researchers-find-github-copilot-generates-vulnerable-code-40-of-the-time/?ref=wheresyoured.at"> <u>NYU's Center for Cybersecurity also found in 2023 study that CoPilot generated code with security vulnerabilities 40% of the time</u></a>.</blockquote><p>These are also the hard limits that you're going to see with generative images and video. While the internet is a giant hole of content you can easily and cheaply consume for training, visual media requires a great deal of significantly more complex data — and that’s on top of the significant and obvious copyright issues. ChatGPT's DALL-E (images) and Sora (video) products are,<a href="https://www.wheresyoured.at/sam-altman-fried/"> <u>as I've noted</u></a>, limited by the availability of ways to teach them as well as the limits of generative AI itself, meaning that video may continue to dominate the internet as text-based content finds itself crowded out by AI-generated content.<a href="https://finance.yahoo.com/news/openai-sam-altman-says-giant-164924270.html?ref=wheresyoured.at"> <u>This may be why Sam Altman is trying to claim that giant AI models are not the future</u></a> — because there may not be enough fuel to grow them much further. After all,<a href="https://cnbc.com/2024/01/18/openai-ceo-on-nyt-lawsuit-ai-models-dont-need-publishers-data-.html?ref=wheresyoured.at"> <u>Altman claims that any one data source "doesn't move the needle" for OpenAI</u></a>.</p><p>There's also no way to escape the fact that these hungry robots require legal plagiarism, and any number of copyright assaults could massively slow their progress.<a href="https://www.axios.com/2024/01/12/ai-forget-unlearn-data-privacy?ref=wheresyoured.at"> <u>It's incredibly difficult to make a model forget information</u></a>, meaning that there may, at some point, be steps <em>back </em>in the development of models if datasets have to be reverted to previous versions with copyrighted materials removed.</p><p>The<a href="https://www.vox.com/technology/2024/1/18/24041598/openai-new-york-times-copyright-lawsuit-napster-google-sony?ref=wheresyoured.at"> <u>numerous</u></a><a href="https://www.nytimes.com/2024/02/28/technology/openai-copyright-suit-media.html?ref=wheresyoured.at"> <u>lawsuits</u></a><a href="https://www.cnbc.com/2024/01/05/microsoft-openai-sued-over-copyright-infringement-by-authors.html?ref=wheresyoured.at"> <u>against</u></a> OpenAI could break the back of the company, and while Altman and other AI fantasists may pretend that these models are an intractable path to the future of society, any force that controls (or makes them pay for) the data that they use will kneecap the company and force them to come up with a way to make these models ethically.</p><p>Yet the world I fear is one where these people are allowed to run rampant, turning unique content into food for an ugly, inbred monster of an internet, one that turns everybody's information sources into semi-personalized versions of the same content. These people have names — Sam Altman of OpenAI, Sundar Pichai of Google, Mark Zuckerberg of Meta (which has its own model called LLaMA), Dario Amodei of Anthropic, and Satya Nadella of Microsoft — and they are responsible for trying to standardize the internet and turn it into a series of toll roads that all lead to the same place.</p><p>And they will gladly misinform and disadvantage billions of people to do so. Their future is one that is less colorful, less exciting, one that caters to the entitled and suppresses the creative. Those who rely on generative AI to create are not creators any more than a person that commissions a portrait is an artist. Altman and his ilk believe they're the new Leonardo Da Vincis, but they're little more than petty kings and rent-seekers trying to steal the world's magic.</p><p>They can, however, be fought. Don't buy their lies. Generative AI might be steeped in the language of high fantasy, but it’s a tool, one that they will not admit is a terribly-flawed and unprofitable way to feed the growth-at-all-costs tech engine. Question everything they say. Don't accept that AI "might one day" be great.&nbsp; Demand that it is today, and reject anything less than perfection from men that make billions of dollars shipping you half-finished shit. Reject their marketing speak and empty fantasizing and interrogate the tools put in front of you, and be a thorn in their side when they try to tell you that mediocrity is the future.</p><p>You are not stupid. You are not "missing anything.” These tools are not magic — they're fantastical versions of autocomplete that can't help but make the same mistakes it's learned from the petabytes of information it's stolen from others.</p>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Prompts as WASM Programs (136 pts)]]></title>
            <link>https://github.com/microsoft/aici</link>
            <guid>39670665</guid>
            <pubDate>Mon, 11 Mar 2024 17:00:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/aici">https://github.com/microsoft/aici</a>, See on <a href="https://news.ycombinator.com/item?id=39670665">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Artificial Intelligence Controller Interface (AICI)</h2><a id="user-content-artificial-intelligence-controller-interface-aici" aria-label="Permalink: Artificial Intelligence Controller Interface (AICI)" href="#artificial-intelligence-controller-interface-aici"></a></p>
<p dir="auto">The Artificial Intelligence Controller Interface (AICI) lets you build Controllers that constrain and direct output of a Large Language Model (LLM) in real time.
Controllers are flexible programs capable of implementing constrained decoding, dynamic editing of prompts and generated text, and coordinating execution across multiple, parallel generations.
Controllers incorporate custom logic during the token-by-token decoding and maintain state during an LLM request. This allows diverse Controller strategies, from programmatic or query-based decoding to multi-agent conversations to execute efficiently in tight integration with the LLM itself.</p>
<p dir="auto"><strong>The purpose of AICI is to make it easy to build and experiment with both existing and entirely new Controller strategies for improving LLM generations.</strong>
By abstracting away implementation details of the underlying LLM inference and serving engine, AICI aims to simplify the development of Controllers, make it easier to
write fast Controllers, and ease compatibility across LLM inference and serving engines.</p>
<p dir="auto">AICI is designed for both local and cloud execution, including (eventually) multi-tenant LLM deployments.
Controllers are implemented as light-weight WebAssembly (Wasm) modules which run on the same machine as the LLM inference engine, utilizing the CPU while the GPU is busy with token generation.
AICI is one layer in the inference stack, and is designed to allow control libraries such as Guidance, LMQL, and others to run on top of it and gain both efficiency and performance improvements, as well as portability across LLM inference and serving engines.</p>
<p dir="auto">AICI currently integrates with llama.cpp, HuggingFace Transformers, and rLLM (custom tch-based LLM inference engine), with vLLM in the works.</p>
<p dir="auto">AICI is:</p>
<ul dir="auto">
<li><a href="#flexibility">Flexible</a>: Controllers can be written in any language that can compile to Wasm (Rust, C, C++, ...),
or be interpreted inside Wasm (Python, JavaScript, ...)</li>
<li><a href="#security">Secure</a>: Controllers are sandboxed and cannot access the filesystem, network, or any other resources</li>
<li><a href="#performance">Fast</a>: Wasm modules are compiled to native code and run in parallel with the LLM inference engine, inducing only a
minimal overhead to the generation process</li>
</ul>
<p dir="auto">AICI is a prototype, designed and built at <a href="https://www.microsoft.com/en-us/research/" rel="nofollow">Microsoft Research</a>.</p>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">We are <a href="https://jobs.careers.microsoft.com/us/en/job/1659267" rel="nofollow">looking for a research intern</a>. You have to be accepted or currently enrolled in a PhD program or an equivalent research-oriented program in Computer Science or related STEM field.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#artificial-intelligence-controller-interface-aici">Artificial Intelligence Controller Interface (AICI)</a></li>
<li><a href="#quickstart-example-walkthrough">QuickStart: Example Walkthrough</a>
<ul dir="auto">
<li><a href="#development-environment-setup">Development Environment Setup</a></li>
<li><a href="#build-and-start-rllm-server-and-aici-runtime">Build and start rLLM server and AICI Runtime</a></li>
<li><a href="#control-ai-output-using-aici-controllers">Control AI output using AICI controllers</a></li>
</ul>
</li>
<li><a href="#comprehensive-guide-exploring-further">Comprehensive Guide: Exploring Further</a></li>
<li><a href="#architecture">Architecture</a></li>
<li><a href="#security">Security</a></li>
<li><a href="#performance">Performance</a></li>
<li><a href="#flexibility">Flexibility</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#trademarks">Trademarks</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">QuickStart: Example Walkthrough</h2><a id="user-content-quickstart-example-walkthrough" aria-label="Permalink: QuickStart: Example Walkthrough" href="#quickstart-example-walkthrough"></a></p>
<p dir="auto">In this quickstart, we'll guide you through the following steps:</p>
<ul dir="auto">
<li>Set up <strong>rLLM Server</strong> and <strong>AICI Runtime</strong>.</li>
<li>Build and deploy a <strong>Controller</strong>.</li>
<li>Use AICI to control LLM output, so you can <strong>customize a LLM to follow specific rules</strong> when generating text.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development Environment Setup</h2><a id="user-content-development-environment-setup" aria-label="Permalink: Development Environment Setup" href="#development-environment-setup"></a></p>
<p dir="auto">To compile AICI components, you need to set up your development environment for Rust. For this quickstart you also need Python 3.11 or later to create a controller.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows WSL / Linux / macOS</h3><a id="user-content-windows-wsl--linux--macos" aria-label="Permalink: Windows WSL / Linux / macOS" href="#windows-wsl--linux--macos"></a></p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto"><strong>Windows users</strong>: please use WSL2 or the included <a href="https://containers.dev/" rel="nofollow">devcontainer</a>. Adding native Windows support <a href="https://github.com/microsoft/aici/issues/42" data-hovercard-type="issue" data-hovercard-url="/microsoft/aici/issues/42/hovercard">is tracked here</a>.</p>
<p dir="auto"><strong>MacOS users</strong>: please make sure you have XCode command line tools installed by running <code>xcode-select -p</code> and, if not installed, run <code>xcode-select --install</code>.</p>
<p dir="auto"><strong>CUDA</strong>: the CUDA build relies on specific libtorch installation. It's highly recommended you use the included devcontainer.</p>
</div>
<p dir="auto">If you're using devcontainer, you can skip to the <a href="#build-and-start-rllm-server-and-aici-runtime">next section</a>.</p>
<p dir="auto">Using the system package manager, install the necessary tools for building code in the repository, including <code>git</code>, <code>cmake</code> and <code>ccache</code>.</p>
<p dir="auto">For instance in WSL / Ubuntu using <code>apt</code>:</p>
<div data-snippet-clipboard-copy-content="sudo apt-get install --assume-yes --no-install-recommends \
    build-essential cmake ccache pkg-config libssl-dev libclang-dev clang llvm-dev git-lfs"><pre><code>sudo apt-get install --assume-yes --no-install-recommends \
    build-essential cmake ccache pkg-config libssl-dev libclang-dev clang llvm-dev git-lfs
</code></pre></div>
<p dir="auto">or using Homebrew on macOS:</p>
<div data-snippet-clipboard-copy-content="brew install git cmake ccache"><pre><code>brew install git cmake ccache
</code></pre></div>
<p dir="auto">Then install <strong>Rust, Rustup and Cargo</strong>, following the instructions provided <a href="https://doc.rust-lang.org/cargo/getting-started/installation.html" rel="nofollow">here</a> and <a href="https://www.rust-lang.org/learn/get-started" rel="nofollow">here</a>:</p>
<div data-snippet-clipboard-copy-content="curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh"><pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
</code></pre></div>
<p dir="auto">After installation, verify that the <code>rustup --version</code> command is accessible by running it from the terminal. If the command isn't recognized, try opening a new terminal session.</p>
<p dir="auto">Next install wasm32-wasi Rust component:</p>
<div data-snippet-clipboard-copy-content="rustup target add wasm32-wasi"><pre><code>rustup target add wasm32-wasi
</code></pre></div>
<p dir="auto">If you already had Rust installed, or are getting complaints from Cargo about outdated versions, run:</p>

<p dir="auto">Last, to work with <strong>Python</strong> controllers and scripts (like this tutorial), run this command to install the required packages:</p>
<div data-snippet-clipboard-copy-content="pip install pytest pytest-forked ujson posix_ipc numpy requests"><pre><code>pip install pytest pytest-forked ujson posix_ipc numpy requests
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build and start rLLM server and AICI Runtime</h2><a id="user-content-build-and-start-rllm-server-and-aici-runtime" aria-label="Permalink: Build and start rLLM server and AICI Runtime" href="#build-and-start-rllm-server-and-aici-runtime"></a></p>
<p dir="auto">The rLLM server has two backends, one based on <code>libtorch</code> and CUDA
(<code>rllm-cuda</code>), and the other based on <code>llama.cpp</code> (<code>rllm-llamacpp</code>).</p>
<p dir="auto">The <code>rllm-cuda</code> backend only works with NVidia GPUs with compute capability 8.0 or later
(A100 and later; RTX 30x0 and later) and requires a fiddly setup of libtorch
-- it's strongly recommended to use the included devcontainer.
While this guide focuses on the <code>rllm-llamacpp</code> backend,
the build steps are the same for <code>rllm-cuda</code>, modulo the folder name.</p>
<p dir="auto">After <a href="#development-environment-setup">dev env setup</a> above,
clone the AICI repository and proceed with the next steps outlined below.</p>
<p dir="auto">Use the following command to build and run <code>aicirt</code> and <code>rllm-llamacpp</code>:</p>
<div data-snippet-clipboard-copy-content="cd rllm/rllm-llamacpp
./server.sh phi2"><pre><code>cd rllm/rllm-llamacpp
./server.sh phi2
</code></pre></div>
<p dir="auto">You can pass other model names as argument (run <code>./server.sh</code> without arguments to see available models).
You can also use a HuggingFace URL to <code>.gguf</code> file or a local path to a <code>.gguf</code> file.
(For <code>rllm-cuda</code> use HuggingFace model id or path to folder).</p>

<p dir="auto">You can find more details about <code>rllm-llamacpp</code> <a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-llamacpp/README.md">here</a>.</p>
<p dir="auto">The rLLM server provides a HTTP interface, utilized for configuration tasks and processing requests. You can also use this interface to promptly verify its status. For instance, if you open <a href="http://127.0.0.1:4242/v1/models" rel="nofollow">http://127.0.0.1:4242/v1/models</a>, you should see:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;object&quot;: &quot;list&quot;,
  &quot;data&quot;: [
    {
      &quot;object&quot;: &quot;model&quot;,
      &quot;id&quot;: &quot;TheBloke/phi-2-GGUF&quot;,
      &quot;created&quot;: 946810800,
      &quot;owned_by&quot;: &quot;owner&quot;
    }
  ]
}"><pre>{
  <span>"object"</span>: <span><span>"</span>list<span>"</span></span>,
  <span>"data"</span>: [
    {
      <span>"object"</span>: <span><span>"</span>model<span>"</span></span>,
      <span>"id"</span>: <span><span>"</span>TheBloke/phi-2-GGUF<span>"</span></span>,
      <span>"created"</span>: <span>946810800</span>,
      <span>"owned_by"</span>: <span><span>"</span>owner<span>"</span></span>
    }
  ]
}</pre></div>
<p dir="auto">confirming that the selected model is loaded.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Control AI output using AICI controllers</h2><a id="user-content-control-ai-output-using-aici-controllers" aria-label="Permalink: Control AI output using AICI controllers" href="#control-ai-output-using-aici-controllers"></a></p>
<p dir="auto">AICI allows hosting custom logic, called <strong>Controllers</strong>, that initiate, terminate, and interact with LLMs token generation. Controllers take input arguments, process them, and return a result with logs, LLM tokens, and variables.</p>
<p dir="auto">The repository includes some examples, in particular:</p>
<ul dir="auto">
<li><strong>jsctrl</strong>: a controller that accepts JavaScript code as input for execution. This code can interact with the model to generate text and tokens.</li>
<li><strong>pyctrl</strong>: a controller that accepts Python code as input for execution. This code can also interact with the model to generate text and tokens.</li>
</ul>
<p dir="auto">In this example we'll utilize <strong>pyctrl</strong> to manage token generation using a simple <strong>Python script</strong>.
If you want, you can <a href="https://github.com/microsoft/aici/blob/main/controllers/pyctrl/README.md">build and upload pyctrl</a>,
however by default the server will automatically
download the <a href="https://github.com/microsoft/aici/releases/latest">latest release</a> of pyctrl from GitHub.</p>
<p dir="auto">In general, controllers require building and deployment, while scripts (Python or JavaScript) are sent with each request.</p>
<p dir="auto">The following illustrates the relationship between the rLLM server, the AICI runtime, and the controller:</p>
<section data-identity="8bb29307-949e-423a-ac8f-c6a02eb10c4e" data-host="https://viewscreen.githubusercontent.com" data-src="https://viewscreen.githubusercontent.com/markdown/mermaid?docs_host=https%3A%2F%2Fdocs.github.com" data-type="mermaid" aria-label="mermaid rendered output container">
  <div dir="auto" data-json="{&quot;data&quot;:&quot;erDiagram\n    Host    ||--|{ CPU : \&quot;\&quot;\n    Host    ||--|{ GPU : \&quot;\&quot;\n    \n    CPU     ||--|| \&quot;rLLM Server\&quot; : execute\n    CPU     ||--|{ \&quot;AICI Runtime\&quot; : execute\n\n    \&quot;AICI Runtime\&quot; ||--|| \&quot;Controller\&quot; : instantiate\n\n    GPU     ||--|{ \&quot;LLM token generation\&quot; : execute\n&quot;}" data-plain="erDiagram
    Host    ||--|{ CPU : &quot;&quot;
    Host    ||--|{ GPU : &quot;&quot;
    
    CPU     ||--|| &quot;rLLM Server&quot; : execute
    CPU     ||--|{ &quot;AICI Runtime&quot; : execute

    &quot;AICI Runtime&quot; ||--|| &quot;Controller&quot; : instantiate

    GPU     ||--|{ &quot;LLM token generation&quot; : execute
">
      <pre lang="mermaid" aria-label="Raw mermaid code">erDiagram
    Host    ||--|{ CPU : ""
    Host    ||--|{ GPU : ""
    
    CPU     ||--|| "rLLM Server" : execute
    CPU     ||--|{ "AICI Runtime" : execute

    "AICI Runtime" ||--|| "Controller" : instantiate

    GPU     ||--|{ "LLM token generation" : execute
</pre>
    </div>
  <span role="presentation">
    <svg style="box-sizing: content-box; color: var(--color-icon-primary);" width="16" height="16" viewBox="0 0 16 16" fill="none" data-view-component="true">
  <circle cx="8" cy="8" r="7" stroke="currentColor" stroke-opacity="0.25" stroke-width="2" vector-effect="non-scaling-stroke" fill="none"></circle>
  <path d="M15 8a7.002 7.002 0 00-7-7" stroke="currentColor" stroke-width="2" stroke-linecap="round" vector-effect="non-scaling-stroke"></path>
</svg>
  </span>
</section>

<p dir="auto"><h3 tabindex="-1" dir="auto">Controlling the LLM token generation</h3><a id="user-content-controlling-the-llm-token-generation" aria-label="Permalink: Controlling the LLM token generation" href="#controlling-the-llm-token-generation"></a></p>
<p dir="auto">Suppose we aim for a model to generate a list, adhering to a specific format and containing only five items.</p>
<p dir="auto">Typically, achieving this involves prompt engineering, crafting the prompt precisely with clear instructions, such as:</p>
<div data-snippet-clipboard-copy-content="What are the five most popular types of vehicles?
Return the result as a numbered list.
Do not add explanations, only the list."><pre><code>What are the five most popular types of vehicles?
Return the result as a numbered list.
Do not add explanations, only the list.
</code></pre></div>
<p dir="auto">The prompt would also vary depending on the model in use, given that each model tends to add explanations and understands instructions in different ways.</p>
<p dir="auto">With AICI, we shift control back to code, and we can simplify the prompt to:</p>
<div data-snippet-clipboard-copy-content="What are the most popular types of vehicles?"><pre><code>What are the most popular types of vehicles?
</code></pre></div>
<p dir="auto">using code to:</p>
<ol dir="auto">
<li>Limit the list to 5 items</li>
<li>Prevent the model from adding some initial explanation</li>
<li>Format to a numbered list</li>
<li>Stop the model from adding some text after the list.</li>
</ol>
<p dir="auto">Let's create a <code>list-of-five.py</code> python file with the following content:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import pyaici.server as aici

# Force the model to generate a well formatted list of 5 items, e.g.
#   1. name 1
#   2. name 2
#   3. name 3
#   4. name 4
#   5. name 5
async def main():
    
    # This is the prompt we want to run.
    # Note how the prompt doesn't mention a number of vehicles or how to format the result.
    prompt = &quot;What are the most popular types of vehicles?\n&quot;

    # Tell the model to generate the prompt string, ie. let's start with the prompt &quot;to complete&quot;
    await aici.FixedTokens(prompt)

    # Store the current position in the token generation process
    marker = aici.Label()

    for i in range(1,6):
      # Tell the model to generate the list number
      await aici.FixedTokens(f&quot;{i}.&quot;)

      # Wait for the model to generate a vehicle name and end with a new line
      await aici.gen_text(stop_at = &quot;\n&quot;)

    await aici.FixedTokens(&quot;\n&quot;)

    # Store the tokens generated in a result variable
    aici.set_var(&quot;result&quot;, marker.text_since())

aici.start(main())"><pre><span>import</span> <span>pyaici</span>.<span>server</span> <span>as</span> <span>aici</span>

<span># Force the model to generate a well formatted list of 5 items, e.g.</span>
<span>#   1. name 1</span>
<span>#   2. name 2</span>
<span>#   3. name 3</span>
<span>#   4. name 4</span>
<span>#   5. name 5</span>
<span>async</span> <span>def</span> <span>main</span>():
    
    <span># This is the prompt we want to run.</span>
    <span># Note how the prompt doesn't mention a number of vehicles or how to format the result.</span>
    <span>prompt</span> <span>=</span> <span>"What are the most popular types of vehicles?<span>\n</span>"</span>

    <span># Tell the model to generate the prompt string, ie. let's start with the prompt "to complete"</span>
    <span>await</span> <span>aici</span>.<span>FixedTokens</span>(<span>prompt</span>)

    <span># Store the current position in the token generation process</span>
    <span>marker</span> <span>=</span> <span>aici</span>.<span>Label</span>()

    <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>1</span>,<span>6</span>):
      <span># Tell the model to generate the list number</span>
      <span>await</span> <span>aici</span>.<span>FixedTokens</span>(<span>f"<span><span>{</span><span>i</span><span>}</span></span>."</span>)

      <span># Wait for the model to generate a vehicle name and end with a new line</span>
      <span>await</span> <span>aici</span>.<span>gen_text</span>(<span>stop_at</span> <span>=</span> <span>"<span>\n</span>"</span>)

    <span>await</span> <span>aici</span>.<span>FixedTokens</span>(<span>"<span>\n</span>"</span>)

    <span># Store the tokens generated in a result variable</span>
    <span>aici</span>.<span>set_var</span>(<span>"result"</span>, <span>marker</span>.<span>text_since</span>())

<span>aici</span>.<span>start</span>(<span>main</span>())</pre></div>
<p dir="auto">Running the script is not too different from sending a prompt. In this case, we're sending control logic and instructions all together.</p>
<p dir="auto">To see the final result, execute the following command:</p>
<div data-snippet-clipboard-copy-content="./aici.sh run list-of-five.py"><pre><code>./aici.sh run list-of-five.py
</code></pre></div>
<p dir="auto">Result:</p>
<div data-snippet-clipboard-copy-content="Running with tagged AICI Controller: gh:microsoft/aici/pyctrl
[0]: FIXED 'What are the most popular types of vehicles?\n'
[0]: FIXED '1.'
[0]: GEN ' Cars\n'
[0]: FIXED '2.'
[0]: GEN ' Motorcycles\n'
[0]: FIXED '3.'
[0]: GEN ' Bicycles\n'
[0]: FIXED '4.'
[0]: GEN ' Trucks\n'
[0]: FIXED '5.'
[0]: GEN ' Boats\n'
[0]: FIXED '\n'
[DONE]
[Response] What are the most popular types of vehicles?
1. Cars
2. Motorcycles
3. Bicycles
4. Trucks
5. Boats

response saved to tmp/response.json
Usage: {'sampled_tokens': 16, 'ff_tokens': 37, 'cost': 69}
Timing: {'http_response': 0.05193686485290527, 'data0': 0.05199289321899414, 'first_token': 0.0658726692199707, 'last_token': 0.1784682273864746}
Tokens/sec: {'prompt': 861.0913072488067, 'sampling': 89.65181217019571}
Storage: {'result': '1. Cars\n2. Motorcycles\n3. Bicycles\n4. Trucks\n5. Boats\n\n'}"><pre><code>Running with tagged AICI Controller: gh:microsoft/aici/pyctrl
[0]: FIXED 'What are the most popular types of vehicles?\n'
[0]: FIXED '1.'
[0]: GEN ' Cars\n'
[0]: FIXED '2.'
[0]: GEN ' Motorcycles\n'
[0]: FIXED '3.'
[0]: GEN ' Bicycles\n'
[0]: FIXED '4.'
[0]: GEN ' Trucks\n'
[0]: FIXED '5.'
[0]: GEN ' Boats\n'
[0]: FIXED '\n'
[DONE]
[Response] What are the most popular types of vehicles?
1. Cars
2. Motorcycles
3. Bicycles
4. Trucks
5. Boats

response saved to tmp/response.json
Usage: {'sampled_tokens': 16, 'ff_tokens': 37, 'cost': 69}
Timing: {'http_response': 0.05193686485290527, 'data0': 0.05199289321899414, 'first_token': 0.0658726692199707, 'last_token': 0.1784682273864746}
Tokens/sec: {'prompt': 861.0913072488067, 'sampling': 89.65181217019571}
Storage: {'result': '1. Cars\n2. Motorcycles\n3. Bicycles\n4. Trucks\n5. Boats\n\n'}
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Comprehensive Guide: Exploring Further</h2><a id="user-content-comprehensive-guide-exploring-further" aria-label="Permalink: Comprehensive Guide: Exploring Further" href="#comprehensive-guide-exploring-further"></a></p>
<p dir="auto">This repository contains a number of components, and which ones you need depends on your use case.</p>
<p dir="auto">You can <strong>use an existing controller module</strong>.
We provide <a href="https://github.com/microsoft/aici/blob/main/controllers/pyctrl">PyCtrl</a> and <a href="https://github.com/microsoft/aici/blob/main/controllers/jsctrl">JsCtrl</a>
that let you script controllers using server-side Python and JavaScript, respectively.
The <a href="https://github.com/microsoft/aici/blob/main/py/pyaici">pyaici</a> package contains <code>aici</code> command line tool that lets you
<a href="https://github.com/microsoft/aici/blob/main/docs/proxy.md">upload and run scripts</a> with any controller
(we also provide <a href="https://github.com/microsoft/aici/blob/main/docs/REST.md">REST API definition</a> for the curious).</p>
<blockquote>
<p dir="auto">🧑‍💻<a href="https://github.com/microsoft/aici/blob/main/controllers/pyctrl">Python code samples for scripting PyCtrl</a> and a <a href="https://github.com/microsoft/aici/blob/main/controllers/jsctrl/samples/hello.js">JavaScript Hello World for JSCtrl</a></p>
</blockquote>
<p dir="auto">We anticipate <a href="#architecture">libraries</a> will be built on top of controllers.
We provide an example in <a href="https://github.com/microsoft/aici/blob/main/py/promptlib">promptlib</a> - a client-side Python library
that generates interacts with <a href="https://github.com/microsoft/aici/blob/main/controllers/declctrl">DeclCtrl</a> via the pyaici package.</p>
<blockquote>
<p dir="auto">🧑‍💻 <a href="https://github.com/microsoft/aici/blob/main/py/promptlib/notebooks/basics_tutorial.ipynb">Example notebook that uses PromptLib to interact with DeclCtrl</a>.</p>
</blockquote>
<p dir="auto">The controllers can be run in a cloud or local AICI-enabled LLM inference engine.
You can <strong>run the provided reference engine (rLLM) locally</strong> with either
<a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-cuda">libtorch+CUDA</a> or <a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-llamacpp">llama.cpp backend</a>.</p>
<p dir="auto">To <strong>develop a new controller</strong>, use a Rust <a href="https://github.com/microsoft/aici/blob/main/controllers/uppercase">starter project</a> that shows usage of <a href="https://github.com/microsoft/aici/blob/main/controllers/aici_abi">aici_abi</a>
library, which simplifies implementing the <a href="https://github.com/microsoft/aici/blob/main/controllers/aici_abi/README.md#low-level-interface">low-level AICI interface</a>.</p>
<blockquote>
<p dir="auto">🧑‍💻<a href="https://github.com/microsoft/aici/blob/main/controllers/uppercase">Sample code for a minimal new controller</a> to get you started</p>
</blockquote>
<p dir="auto">To <strong>add AICI support to a new LLM inference engine</strong>,
you will need to implement LLM-side of the <a href="https://github.com/microsoft/aici/blob/main/docs/aicirt-proto.md">protocol</a>
that talks to <a href="https://github.com/microsoft/aici/blob/main/aicirt">AICI runtime</a>.</p>
<p dir="auto">Finally, you may want to modify any of the provided components - PRs are most welcome!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<p dir="auto">AICI abstracts LLM inference engine from the controller and vice-versa, as in the picture below.
The rounded nodes are aspirational.
Additional layers can be built on top - we provide <a href="https://github.com/microsoft/aici/blob/main/py/promptlib">promptlib</a>,
but we strongly believe that
<a href="https://github.com/guidance-ai/guidance">Guidance</a>,
<a href="https://lmql.ai/" rel="nofollow">LMQL</a>,
<a href="https://github.com/sgl-project/sglang">SGLang</a>,
<a href="https://github.com/outlines-dev/outlines">Outlines</a>,
<a href="https://github.com/1rgs/jsonformer">jsonformer</a>,
<a href="https://github.com/noamgat/lm-format-enforcer">LMFE</a>,
etc.
can also run on top of AICI (either with custom controllers or utilizing PyCtrl or JsCtrl).</p>
<section data-identity="b9f7b882-d4a7-4edf-a14d-a3235821615b" data-host="https://viewscreen.githubusercontent.com" data-src="https://viewscreen.githubusercontent.com/markdown/mermaid?docs_host=https%3A%2F%2Fdocs.github.com" data-type="mermaid" aria-label="mermaid rendered output container">
  <div dir="auto" data-json="{&quot;data&quot;:&quot;graph TD\n    PyCtrl -- AICI --&amp;gt; aicirt[AICI-runtime]\n    JsCtrl -- AICI --&amp;gt; aicirt\n    guidance([GuidanceCtrl]) -- AICI --&amp;gt; aicirt\n    lmql([LMQL Ctrl]) -- AICI --&amp;gt; aicirt\n    aicirt -- POSIX SHM --&amp;gt; rLLM\n    aicirt -- POSIX SHM --&amp;gt; llama[llama.cpp]\n    aicirt -- POSIX SHM --&amp;gt; pyaici\n    pyaici -- Python --&amp;gt; vLLM(vLLM)\n    pyaici -- Python --&amp;gt; hf[HF Transformers]\n&quot;}" data-plain="graph TD
    PyCtrl -- AICI --> aicirt[AICI-runtime]
    JsCtrl -- AICI --> aicirt
    guidance([GuidanceCtrl]) -- AICI --> aicirt
    lmql([LMQL Ctrl]) -- AICI --> aicirt
    aicirt -- POSIX SHM --> rLLM
    aicirt -- POSIX SHM --> llama[llama.cpp]
    aicirt -- POSIX SHM --> pyaici
    pyaici -- Python --> vLLM(vLLM)
    pyaici -- Python --> hf[HF Transformers]
">
      <pre lang="mermaid" aria-label="Raw mermaid code">graph TD
    PyCtrl -- AICI --&gt; aicirt[AICI-runtime]
    JsCtrl -- AICI --&gt; aicirt
    guidance([GuidanceCtrl]) -- AICI --&gt; aicirt
    lmql([LMQL Ctrl]) -- AICI --&gt; aicirt
    aicirt -- POSIX SHM --&gt; rLLM
    aicirt -- POSIX SHM --&gt; llama[llama.cpp]
    aicirt -- POSIX SHM --&gt; pyaici
    pyaici -- Python --&gt; vLLM(vLLM)
    pyaici -- Python --&gt; hf[HF Transformers]
</pre>
    </div>
  <span role="presentation">
    <svg style="box-sizing: content-box; color: var(--color-icon-primary);" width="16" height="16" viewBox="0 0 16 16" fill="none" data-view-component="true">
  <circle cx="8" cy="8" r="7" stroke="currentColor" stroke-opacity="0.25" stroke-width="2" vector-effect="non-scaling-stroke" fill="none"></circle>
  <path d="M15 8a7.002 7.002 0 00-7-7" stroke="currentColor" stroke-width="2" stroke-linecap="round" vector-effect="non-scaling-stroke"></path>
</svg>
  </span>
</section>

<p dir="auto">The <a href="https://github.com/microsoft/aici/blob/main/py/pyaici">pyaici</a> package makes it easier to integrate AICI with Python-based LLM inference engines.
Take a look at integration with <a href="https://github.com/microsoft/aici/blob/main/scripts/py/run_hf.py">HuggingFace Transformers</a>,
though note that it doesn't support forking (generation of multiple sequences in parallel).
The <a href="https://github.com/microsoft/aici/blob/main/scripts/py/vllm_server.py">vLLM REST server</a> is currently out of date.
Please use the <a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-cuda">rLLM-cuda</a> or <a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-llamacpp">rLLM-llama.cpp</a> for now.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security</h2><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<ul dir="auto">
<li><code>aicirt</code> runs in a separate process, and can run under a different user than the LLM engine</li>
<li>Wasm modules are <a href="https://docs.wasmtime.dev/security.html" rel="nofollow">sandboxed by Wasmtime</a></li>
<li>Wasm only have access to <a href="https://github.com/microsoft/aici/blob/main/controllers/aici_abi/src/host.rs"><code>aici_host_*</code> functions</a>,
implemented in <a href="https://github.com/microsoft/aici/blob/main/aicirt/src/hostimpl.rs">hostimpl.rs</a></li>
<li><code>aicirt</code> also exposes a partial WASI interface; however almost all the functions are no-op, except
for <code>fd_write</code> which shims file descriptors 1 and 2 (stdout and stderr) to print debug messages</li>
<li>each Wasm module runs in a separate process, helping with Spectre/Meltdown mitigation
and allowing limits on CPU usage</li>
</ul>
<p dir="auto">In particular, Wasm modules cannot access the filesystem, network, or any other resources.
They also cannot spin threads or access any timers (this is relevant for Spectre/Meltdown attacks).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance</h2><a id="user-content-performance" aria-label="Permalink: Performance" href="#performance"></a></p>
<p dir="auto">Most of computation in AICI Controllers occurs on the CPU, in parallel with the logit generation on the GPU.
The generation occurs in steps, where logits are generated in parallel for a new token for each sequence in a batch
(typically between 1 and 50).
This involves reading the whole model and KV caches for sequences in the batch from the GPU memory.
For optimal batch throughput, the model and KV caches should utilize a major fraction of the GPU memory,
and reading the whole memory takes about 40ms on A100 GPU (80GB).</p>
<p dir="auto">Thus, each step of generation takes on the order of 20-50ms.
With careful engineering,
this is more than enough to compute the set of allowed tokens in Rust compiled to Wasm.
These can be combined either natively in Rust, or via Python or JavaScript interpreters
we provide.</p>
<p dir="auto">For example, computing allowed token set in the 32000-strong vocabulary of Llama model takes:</p>
<ul dir="auto">
<li>about 2.0ms for Yacc grammar of the C programming language</li>
<li>about 0.3ms for a regular expression</li>
<li>about 0.2ms for a substring constraint, from 4kB string</li>
</ul>
<p dir="auto">The above numbers are for a single sequence, however each sequence is processed in separate process,
and thus if there is more cores than sequences (which is typical), they do not change.
They also include overhead of calling into Python interpreter implemented in Wasm, and then back into
Rust-generated Wasm code for the constraint itself.
They are all well within the 20-50ms budget, so do not affect the generation time at all.</p>
<p dir="auto">There is also some overhead in the critical path of sampling. It comes down to about 0.3ms per generation step
when executing 10 sequences in parallel (this is irrespective of the constraint used).
The overhead goes up to around 0.7ms for 40 sequences (though it has not been fully optimized yet).</p>
<p dir="auto">WebAssembly is designed to have minimal overhead, compared to native code.
In our experience, <a href="https://github.com/microsoft/aici/blob/main/controllers/aici_abi/implementation.md#token-trie">highly optimized</a>
Rust code is less than 2x slower when run in
<a href="https://wasmtime.dev/" rel="nofollow">Wasmtime</a> than native.
This is 10-100x better than JavaScript or Python.</p>
<p dir="auto">All measurements done on AMD EPYC 7V13 with nVidia A100 GPU with 80GB of VRAM.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Flexibility</h2><a id="user-content-flexibility" aria-label="Permalink: Flexibility" href="#flexibility"></a></p>
<p dir="auto">The low-level interface that AICI runtime provides allows for:</p>
<ul dir="auto">
<li>interaction with the LLM inference engine before, during, and after every generated token</li>
<li>constraining decoding to a set of tokens</li>
<li>backtracking KV-cache to a previous state</li>
<li>fast-forwarding several tokens at a time (if they are known)</li>
<li>forking generation into multiple branches</li>
<li>communication between forks via shared variables</li>
<li>utility functions for converting between tokens and byte strings</li>
</ul>
<p dir="auto">It can be utilized from any language that compiles to Wasm.</p>
<p dir="auto">This repository provides a Rust library that makes it easy to implement controllers in Rust,
and provides <a href="https://github.com/microsoft/aici/blob/main/controllers/aici_abi/implementation.md">efficient implementations</a>
of specific constraints (<a href="https://github.com/microsoft/aici/blob/main/controllers/aici_abi/README.md#regular-expressions">regular expressions</a>,
<a href="https://github.com/microsoft/aici/blob/main/controllers/aici_abi/README.md#lr1-grammars">yacc grammars</a>, substrings).
We also provide <a href="https://github.com/microsoft/aici/blob/main/controllers/pyctrl">Python</a> and <a href="https://github.com/microsoft/aici/blob/main/controllers/jsctrl">JavaScript</a> interpreters
that allow to glue these constraints together.
All of these can be easily extended.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<ul dir="auto">
<li><a href="https://github.com/microsoft/aici/blob/main/rllm/tch-cuda/kernels/flash_attn">Flash Attention kernels</a> are copied from
<a href="https://github.com/Dao-AILab/flash-attention">flash-attention repo</a>;
see <a href="https://github.com/microsoft/aici/blob/main/rllm/tch-cuda/kernels/flash_attn/LICENSE">BSD LICENSE</a></li>
<li><a href="https://github.com/microsoft/aici/blob/main/rllm/tch-cuda/kernels/vllm">Paged Attention kernels</a> are copied from
<a href="https://github.com/vllm-project/vllm">vLLM repo</a>;
see <a href="https://github.com/microsoft/aici/blob/main/rllm/tch-cuda/kernels/vllm/LICENSE">Apache LICENSE</a></li>
<li><a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-base/src/server/openai">OpenAI API definitions</a> are copied and modified from
<a href="https://github.com/EricLBuehler/candle-vllm">candle-vllm</a>;
see <a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-base/src/server/openai/LICENSE">MIT LICENSE</a></li>
<li><a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-cuda/src/llm/paged/cache_engine.rs">cache_engine.rs</a>,
<a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-base/src/config.rs">config.rs</a>,
and <a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-base/src/scheduler.rs">scheduler.rs</a>
are loosely based on <a href="https://github.com/vllm-project/vllm">vLLM</a></li>
<li><a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-cuda/src/llm/llama.rs">llama.rs</a>, <a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-cuda/src/llm/phi.rs">phi.rs</a>
and <a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-base/src/logits.rs">logits.rs</a> are based on
<a href="https://github.com/huggingface/candle/tree/main/candle-transformers">candle-transformers</a></li>
<li>specific <a href="https://github.com/microsoft/aici/blob/main/controllers/pyctrl/Lib">Python library</a> files are copied from
<a href="https://github.com/RustPython/RustPython">RustPython</a>
(as we only use a subset of them)</li>
<li><a href="https://github.com/guidance-ai/guidance">Guidance</a> files copied under
<a href="https://github.com/microsoft/aici/blob/main/controllers/guidance_ctrl/Lib/guidance">guidance_ctrl</a></li>
<li>the <a href="https://github.com/microsoft/aici/blob/main/controllers/aici_abi/grammars/c.y">example ANSI C grammar</a> is based on
<a href="https://www.lysator.liu.se/c/ANSI-C-grammar-y.html" rel="nofollow">https://www.lysator.liu.se/c/ANSI-C-grammar-y.html</a> by Jeff Lee (from 1985)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <a href="https://cla.opensource.microsoft.com/" rel="nofollow">https://cla.opensource.microsoft.com</a>.</p>
<p dir="auto">When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.</p>
<p dir="auto">This project has adopted the <a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow">Microsoft Open Source Code of Conduct</a>.
For more information see the <a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow">Code of Conduct FAQ</a> or
contact <a href="mailto:opencode@microsoft.com">opencode@microsoft.com</a> with any additional questions or comments.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Trademarks</h2><a id="user-content-trademarks" aria-label="Permalink: Trademarks" href="#trademarks"></a></p>
<p dir="auto">This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
<a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general" rel="nofollow">Microsoft's Trademark &amp; Brand Guidelines</a>.
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[gh-116167: Allow disabling the GIL (383 pts)]]></title>
            <link>https://github.com/python/cpython/pull/116338</link>
            <guid>39670102</guid>
            <pubDate>Mon, 11 Mar 2024 16:21:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/python/cpython/pull/116338">https://github.com/python/cpython/pull/116338</a>, See on <a href="https://news.ycombinator.com/item?id=39670102">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      


    <div>
      <p><a href="#start-of-content">Skip to content</a>
      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p><react-partial partial-name="keyboard-shortcuts-dialog" data-ssr="false">
  
  
  
</react-partial>



      

        

            


<header role="banner" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  

  <div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:python/cpython" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="TexTkrmpiwVRQISjQ9u6_4mK_zmlnys5088MAaIZmTFxAt9kMmMirUgeDCbYYv6w6A0n5ttFgtQ3tzX90RrxJw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="python/cpython" data-current-org="python" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=9DYFdB4AVYJCOlHuSHpLLLMgajv1B0gBMsEWUmYoqphRYw4QoRuVo4Wo4KGGV68fbw8NqrvgBh8t6YUcWWa9pjjnAl%2B%2Fui9J5%2FM7zRkPl4jkjX58M5hbS5%2FmBrcnK%2FAYsQUVc8bNbldATnP71iq9%2FTgt0WXMZCqL6t%2BYqpGOoDw6l41tOoAp3p2VivCLo5geKGu28lKK6INsFm2AewbWbveXI0W%2FQyt9JaayLuYlwER9VJEZhqbQtqH8DAz%2BXam1oywIaR8zuYrriNNieBM4wDrvfb5QI2u6mZVu2GUwswvS3Ih4oDcKZMiKYsp7zPY22yW80s0P7RJWEXhz6zjUTKd4HsKevCX727YBvKEmgr%2Fu7bIpAAEQGznqputFVyCXQWov6ZTWgWU0W%2BryxaGG9R8vObgSfCvfDjyQfp%2FCQGrcvxQKhNT9YGJju34%2B7yCgkQrp2H6F1CQNzc7IT3tPbjteD4dQWJ81BDHPCPGldN3EUbTd6SiPooZuWec%2BqNJFj75WhXW22Jb6cQ%2FMX9kXZlmuGvmBxZ7F%2F8Q%3D--sKPkpGqNwtPVkRS5--KOndQgPYCWF1AMn1mE3UGg%3D%3D&quot; />">
  
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fpull_requests_fragments%2Fpull_request_layout&amp;source=header-repo&amp;source_repo=python%2Fcpython" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/python/cpython/pull/116338&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="5cc4a524fc6f3d5f2a3644821fc97cd1bc2ea91e37bf816d3f32b5af23c94e48" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/voltron/pull_requests_fragments/pull_request_layout;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div>
</header>

      
    </div>

  








    


    
    <include-fragment data-base-src="https://github.com/notifications/beta/shelf"></include-fragment>






  <div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  



      
    

    






  
  <div id="repository-container-header" data-turbo-replace="">

      

        


          <nav data-pjax="#js-repo-pjax-container" aria-label="Repository" data-view-component="true">

  
    <div data-view-component="true">      <action-menu data-select-variant="none" data-view-component="true">
  <focus-group direction="vertical" mnemonics="" retain="">
    <tool-tip id="tooltip-7c6059c6-578c-4aac-bb20-d3c6fa8b96a4" for="action-menu-00e7492a-ec88-43e5-861b-a92fded039ff-button" popover="manual" data-direction="s" data-type="label" data-view-component="true">Additional navigation options</tool-tip>


<anchored-position id="action-menu-00e7492a-ec88-43e5-861b-a92fded039ff-overlay" anchor="action-menu-00e7492a-ec88-43e5-861b-a92fded039ff-button" side="outside-bottom" anchor-offset="normal" popover="auto" data-view-component="true">
  </anchored-position>  </focus-group>
</action-menu></div>
</nav>

  </div>

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div id="repo-content-pjax-container" data-channel="eyJjIjoicHVsbF9yZXF1ZXN0OjE3NTU3MTkxMzkiLCJ0IjoxNzEwMTgwMDA1fQ==--eb5ff5551ddc51b3c705151cd94626da6bcabdb5d105b6f8d1ffcb8de834fb8e" data-url="/python/cpython/pull/116338/partials/title?sticky=true" data-pull-is-open="false" data-gid="PR_kwDOBN0Z8c5opiXj" data-pjax="" data-turbo-frame="">



          
<details>
  <summary data-ga-click="Issues, create new issue, view:issue_show location:issue_header style:button logged_in:false">
    
    New issue
  </summary>
  <details-dialog aria-label="Sign up for GitHub">
            <div>
  <p>
  <strong>Have a question about this project?</strong> Sign up for a free GitHub account to open an issue and contact its maintainers and the community.
  </p>

  <!-- '"` --><!-- </textarea></xmp> -->
  <p>By clicking “Sign up for GitHub”, you agree to our <a href="https://docs.github.com/terms" target="_blank">terms of service</a> and
  <a href="https://docs.github.com/privacy" target="_blank">privacy statement</a>. We’ll occasionally send you account related emails.</p>

  <p>
    Already on GitHub?
    <a data-ga-click="(Logged out) New issue modal, clicked Sign in, text:sign-in" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;new issue modal&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/python/cpython/pull/116338&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="da6957ed603101f7ef551fcae7cb94064e81005d8a56ac7bab72cea9ee98341a" href="https://github.com/login?return_to=%2Fpython%2Fcpython%2Fissues%2Fnew%2Fchoose">Sign in</a>
    to your account
  </p>
</div>
  </details-dialog>
</details>
        
      </div>

</turbo-frame>


    </main>
  </div>

          




    <cookie-consent id="cookie-consent-banner" data-initial-cookie-consent-allowed="" data-cookie-consent-required="true"></cookie-consent>


  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Improving Performance in Firefox and Across the Web with Speedometer 3 (113 pts)]]></title>
            <link>https://hacks.mozilla.org/2024/03/improving-performance-in-firefox-and-across-the-web-with-speedometer-3/</link>
            <guid>39670046</guid>
            <pubDate>Mon, 11 Mar 2024 16:17:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hacks.mozilla.org/2024/03/improving-performance-in-firefox-and-across-the-web-with-speedometer-3/">https://hacks.mozilla.org/2024/03/improving-performance-in-firefox-and-across-the-web-with-speedometer-3/</a>, See on <a href="https://news.ycombinator.com/item?id=39670046">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-main">
  <article role="article">
    <p>In collaboration with the other major browser engine developers, Mozilla is thrilled to <a href="https://browserbench.org/announcements/speedometer3">announce Speedometer 3</a> today. Like previous versions of Speedometer, this benchmark measures <a href="https://www.mozilla.org/en-US/about/webvision/full/#performance">what we think matters most</a> for performance online: responsiveness. But today’s release is more open and more challenging than before, and is the best tool for driving browser performance improvements that we’ve ever seen.</p>
<p>This fulfills the <a href="https://twitter.com/mozhacks/status/1603435347190419456">vision set out in December 2022</a> to bring experts across the industry together in order to rethink how we measure browser performance, guided by a shared goal to reflect the real-world Web as much as possible. This is the first time the Speedometer benchmark, or any major browser benchmark, has been developed through a cross-industry collaboration supported by each major browser engine: Blink, Gecko, and WebKit. Working together means we can build a shared understanding of what matters to optimize, and facilitates broad review of the benchmark itself: both of which make it a stronger lever for improving the Web as a whole.</p>
<p>And we’re seeing results: <a href="https://hacks.mozilla.org/2023/10/down-and-to-the-right-firefox-got-faster-for-real-users-in-2023/">Firefox got faster for real users in 2023</a> as a direct result of <a href="https://hacks.mozilla.org/2023/09/faster-vue-js-execution-in-firefox/">optimizing</a> <a href="https://spidermonkey.dev/blog/2023/11/27/newsletter-firefox-118-121.html">for</a> Speedometer 3. This took a coordinated effort from many teams: understanding real-world websites, building new tools to drive optimizations, and making a huge number of improvements inside Gecko to make web pages run more smoothly for Firefox users. In the process, we’ve shipped <a href="https://mzl.la/4bYLwtn">hundreds of bug fixes</a> across JS, DOM, Layout, CSS, Graphics, frontend, memory allocation, profile-guided optimization, and more.</p>
<p>We’re happy to see core optimizations in all the major browser engines turning into improved responsiveness for real users, and are looking forward to continuing to work together to build performance tests that improve the Web.</p>
    <section>
                                
                                <p><a href="https://hacks.mozilla.org/author/bgrinsteadmozilla-com/">More articles by Brian Grinstead…</a></p>
                  </section>
  </article>
  
  

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Speedometer 3.0: A shared browser benchmark for web application responsiveness (163 pts)]]></title>
            <link>https://browserbench.org/announcements/speedometer3/</link>
            <guid>39670035</guid>
            <pubDate>Mon, 11 Mar 2024 16:17:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://browserbench.org/announcements/speedometer3/">https://browserbench.org/announcements/speedometer3/</a>, See on <a href="https://news.ycombinator.com/item?id=39670035">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>




<p>Since the initial version of the Speedometer benchmark was released in 2014 by the WebKit team, it has become a key tool for browser engines to drive performance optimizations as users and developers continue to demand richer and smoother experiences online.</p>

<p>We’re proud to release Speedometer 3.0 today as a collaborative effort between the three major browser engines: Blink, Gecko, and WebKit. Like previous releases (<a href="https://webkit.org/blog/8063/speedometer-2-0-a-benchmark-for-modern-web-app-responsiveness/">Speedometer 2 in 2018</a> and <a href="https://webkit.org/blog/3395/speedometer-benchmark-for-web-app-responsiveness/">Speedometer 1 in 2014</a>), it’s designed to measure web application responsiveness by simulating user interactions on real web pages. Today’s release of Speedometer 3.0 marks a major step forward in web browser performance testing: it introduces a better way of measuring performance and a more representative set of tests that reflect the modern Web.</p>

<h2>A New Governance Model</h2>

<p>This is the first time the Speedometer benchmark, or any major browser benchmark, has been developed through a cross-industry collaboration supported by each major browser engine: Blink/V8, Gecko/SpiderMonkey, and WebKit/JavaScriptCore. It’s been developed under a new governance model, driven by consensus, and is hosted in a <a href="https://github.com/WebKit/Speedometer/">shared repository</a> that’s open to contribution. This new structure involves a lot of collective effort: discussions, research, debates, decisions, and hundreds of PRs since we announced the project in December 2022. </p>

<h2>A Broader Range of User Experiences</h2>

<p>Speedometer 3 adds many new tests. We started designing this new benchmark by identifying some key scenarios and user interactions that we felt were important for browsers to optimize.</p>

<p>In particular, we added new tests that simulate rendering canvas and SVG charts (React Stockcharts, Chart.js, Perf Dashboard, and Observable Plot), code editing (CodeMirror), WYSIWYG editing (TipTap), and reading news sites (Next.js and Nuxt.js).</p>

<p>We’ve also improved the TodoMVC tests: updating the code to adapt to the most common versions of the most popular frameworks based on data from the the HTTP Archive. The following frameworks and libraries are included: Angular, Backbone, jQuery, Lit, Preact, React, React+Redux, Svelte, and Vue; along with vanilla JavaScript implementations targeting ES5 and ES6, and a Web Components version. We also introduced more complex versions of these tests which are embedded into a bigger DOM tree with many complex CSS rules that more closely emulate the page weight and structure from popular webapps today.</p>

<p>Taken together these exercise a more broad and representative cross section of the engine, providing new opportunities to optimize JS, Layout, CSS, Graphics, and DOM APIs in order to improve user experience on the Web. Take a look at <a href="https://www.browserbench.org/Speedometer3.0/about.html">this page</a> for more details about the tests themselves.</p>

<h2>Improvements to the Test Runner</h2>

<p>The test runner itself in Speedometer 3 has been improved to measure more of the work the browser does in response to user actions, such as painting and asynchronous tasks. Speedometer 2.0 measured the time to run a test script synchronously as "sync" time, as well as any additional work before a 0 second timer fires as "async" time. However, this missed some work browser engines have to do to update the rendering of a web page.</p>

<p>In Speedometer 3.0, we are able to measure this previously-missing rendering work, which creates more opportunities to optimize real-world content. To do this, we measure a test script within a requestAnimationFrame callback as "sync" time, and a 0 second timer scheduled in a second requestAnimationFrame fires as "async" time. This async time is guaranteed to include work from timers in the test itself, as well as page rendering by the browser engine. These changes greatly improve the accuracy of the benchmark, and translate into real-world improvements for users as engines optimize this previously-missing work.</p>

<p>There are some more behind the scenes improvements as well. There’s improved developer tooling so browser engineers can better understand results, profile, and customize the test. We redesigned the test runner architecture to make it easier to write and maintain complex test cases. And there are many code quality improvements and migrations to modern features that weren’t broadly available when Speedometer 2.0 was released, such as native promises, async / await, classes, and modules.</p>

<h2>Improving Web Performance</h2>

<p>The <a href="https://github.com/WebKit/Speedometer/?tab=readme-ov-file#what-is-speedometer">primary goal</a> of Speedometer 3 is to reflect the real-world Web as much as possible, so that users benefit when a browser improves its score on the benchmark. It has already had some success at this before publicly launching, with core optimizations in each major engine throughout the last year turning into responsiveness improvements for users across the Web.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How we engineer feedback at Figma with eng crits (112 pts)]]></title>
            <link>https://www.figma.com/blog/how-we-run-eng-crits-at-figma/</link>
            <guid>39669858</guid>
            <pubDate>Mon, 11 Mar 2024 16:04:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.figma.com/blog/how-we-run-eng-crits-at-figma/">https://www.figma.com/blog/how-we-run-eng-crits-at-figma/</a>, See on <a href="https://news.ycombinator.com/item?id=39669858">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Engineering crits encourage a diversity of perspectives and unblock teams to pursue new ideas. Here’s how we structure and run them&nbsp;at&nbsp;Figma.</p><div colorscheme="[object Object]"><p>At Figma, we’re encouraged to give and get feedback on <a href="https://www.figma.com/blog/welcome-to-the-wip/">in-progress work</a>—the earlier the better. This is especially true on the engineering team. But sharing early work can be daunting. It requires an environment and culture in which individual contributors feel comfortable <a href="https://www.figma.com/blog/designer-developer-handoff-with-figma-and-jira/#present-your-work-even-while-it-s-in-progress">bringing unpolished ideas</a> to the table to ask for feedback and have an open conversation, not face another approval step. Most engineering teams have some sort of approval process, often called a technical review, which is usually reserved for the later stages in a project. The problem with late-stage technical reviews is that when they happen <em>too late, </em>like when a direction or design has already been built out, they can lead to launch-blocking feedback.</p><div><p><a href="https://www.figma.com/blog/design-critiques-at-figma/"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACWElEQVQokW2Sa08TQRSG9ydjt5fdbbfdXui9HwjlUsCWgiByU8AmSARjI+52MrMzi1yiJsZLiNH4yR8wr5kdonzww5OzOZuc82aeY5g2l3fgXv1Lwgrh5DmWexzHBwLrqwJukcN2OVYfctCxABsLbKwJOAUOI2GF8g7cq/GwpKOHTtc4TkcCt1cRgjOBZosjX+I42hP4fhPh9jrC6KlAocxhqFRJJyZOqKpK0Gxr1NZ6k8M/E/j9OQJ/I9Bqc2Q9jv0nAj8/RPj1McLhrkDOiwdS6eSpzHkU6SyVmRzFzAzD3hbDsM/gFmnM/hbDxdsQBzsMhRJF0qbozlJMXjGwcYjegu4ZicxEOvmJrNUJOh0iOx2CuS7B0iJBq02Qzk6QtCeoNwgW5wkaTYKkTTCVJnA9gu4swfwcQaGke0bCotIrU/loyDB+EcqXz0P0VxjyJQrTpshkKYoVhvI0g+2yOEXC0pg2Q8rRqO9EhsLI5Lhc7nFJXwt8fRfJq0mE3cfapHrPVodje1NgOBCxiAexNP5fTJvDKFa4PD4Q8stFhG+XkbwmeqAyVm3w+LHFubZYqeklSpRaaLn/TkvDYZSrXJ6OhPzxPlLGpDgX8c2pNMM+xw2JVHKcHApU67qv/qt7VPZTjl6iqmmHMCyXytU+k8rgJx7Kk6MQtQaFk6dYG2izl0GInU0Gr0xRrVM822YY7TO0O0xdBrIFCnUlpkVhTKV8War4crASYHsjkEsLAVzPR8r20WgGWOsHWB8EaLcDZLI+vJKP3kKAxfkAhZIP0/KR8zRTaR9/ACSBhgxUh+LIAAAAAElFTkSuQmCC" alt="An abstract illustration featuring silhouetted shapes against a solid blue background. The top half shows three simplified, bald human head profiles with different irregular features—one has a wavy outline, another has a heart-shaped indentation, and the third has a split top. Below, there are various abstract shapes resembling melting forms or puddles with parts that interlock like puzzle pieces." data-lqip="true"><img data-loading="true" loading="lazy" alt="An abstract illustration featuring silhouetted shapes against a solid blue background. The top half shows three simplified, bald human head profiles with different irregular features—one has a wavy outline, another has a heart-shaped indentation, and the third has a split top. Below, there are various abstract shapes resembling melting forms or puddles with parts that interlock like puzzle pieces." src="https://cdn.sanity.io/images/599r6htc/localized/82da97f2dad2e60cab257c741cf9cdd27fef6b3f-2784x1566.png?w=2784&amp;h=1566&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/82da97f2dad2e60cab257c741cf9cdd27fef6b3f-2784x1566.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 1392w, https://cdn.sanity.io/images/599r6htc/localized/82da97f2dad2e60cab257c741cf9cdd27fef6b3f-2784x1566.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 2088w, https://cdn.sanity.io/images/599r6htc/localized/82da97f2dad2e60cab257c741cf9cdd27fef6b3f-2784x1566.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=2 2784w"></a></p><p>Noah Levin, Vice President of Product Design at Figma, calls design critiques a “<a href="https://www.figma.com/blog/design-critiques-at-figma/">safe space for exploration and feedback</a>.” It’s not about approval, but giving a designer what they need to move a project forward.</p></div><p>Inspired by the Figma design team’s principles and methods for <a href="https://www.figma.com/blog/design-critiques-at-figma/">running design crits</a>, a core group of Figma engineers, led by Ojan Vafai, set out to introduce a process somewhere in between a design crit and a technical review. This was the genesis of Figma’s <strong>engineering critiques</strong>, dedicated time for the engineering team to brainstorm novel approaches to technical problems, get feedback on existing work, and unblock each other. Today, engineering crits are a core part of our workflow, but it didn’t start out&nbsp;that&nbsp;way.</p><h2 id="lifting-ideas-up"><a href="#lifting-ideas-up">Lifting ideas up</a></h2><p>In the early stages of an engineering team, everyone is both a landscape architect and a gardener—they not only shape the overall concept and design, but also nurture how it comes to life. There’s often a surplus of work and a dearth of resources at this stage. As the team scales, you begin to bring in more people who are unfamiliar with the existing design, but who have agreed to stewarding the concept. Ojan likened this stage to an anecdote about his grandfather: “I have a vivid memory of my grandfather wanting to build a small garden in our backyard. He asked my siblings and I to dig out the rocks to clear the area, but as soon as the soil was prepared, the garden was off limits.” He and his siblings weren’t allowed anywhere near the garden, lest they accidentally step on a burgeoning plant.</p><p>On a high-growth team, the instinct might be the same—to limit others to the perimeter, allowing them only to clear rocks but never plant flowers. On the one hand, you need help building the garden; on the other hand, you’re afraid that someone might trample something that’s been growing for a long time and is about&nbsp;to&nbsp;bloom.</p><div><figure><blockquote><span>The engineering crit plays a very specific role. It’s a place to solicit feedback early and often. It is a forum to get expert support on technical designs. It is not an approval process.</span></blockquote><figcaption><span>Kris Rasmussen, Chief Technology Officer, Figma</span></figcaption></figure></div><p>When we brought the idea of hosting engineering crit to the broader team, many were skeptical. Design crits center visual UX details, so some of us wondered if it was even possible to critique the design of a technical solution in the same way. Similarly, engineering reviews are often based on sharing a detailed eng spec for approval, and the result is thumbs up or thumbs down. With so many of us used to this binary approach, we wondered how we might architect engineering crits to plant flowers together.</p><p>We reflected on the times when we felt like we were really in the flow as a team; brainstorms and retros—which we run in FigJam—came to mind. We also thought about what didn’t quite work in technical reviews. When we ran technical reviews synchronously, we anchored on the input of just a few team leads; they weren’t structured for everyone on the call to weigh in. When we ran them asynchronously in other tools, everyone’s instinct was to share feedback as a comment, which led to unwieldy comment threads that were just a list of disparate pieces of feedback, rather than a conversation. Immediately, the format seemed like the key to unlocking a more collaborative process.</p><div><p><a href="https://www.figma.com/blog/why-roles-are-not-rules/"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAC4jAAAuIwF4pT92AAACPElEQVQokSWPzU6rQABGeVFX7nwB48bowmrQQAckSJtSWwrECA4DAwPy00ILDlhboAt/nubmpusv55x8TJqm6/W6qqo4jgEAZ2dnJycnAIA8z33fn06nEMLVaqWq6sXFxc3NTRAEXdd9fHwghJi6roMgcF2XEDKfz6+vr09PTweDQRAEaZoSQpbLZRRFj4+PLMvKskwICcNQEIS7uzvm6+tL13VZlj3Py7LM87zZbGbbtu/7z8/PoiiOx2OEEITQdV3f94MgME1zOBxKksQ0TaNpGgDA87z9ft/3/Xa7raoKYzwajR4eHjiOwxjvdrv9fh+GIcdxl5eXAABCCJOmqaqqPM8bhtE0zeFw2G63URS9vr4ea67rFkXRtu3hcCCEDAaD8/NzlmXDMGQ8z1NVFQAgy3Icx23b1nVtWdZoNLIsK03T1WpFKf38/NxsNgghTdNeXl5s287z/D8MIVwsFoqiOI6z2Wze3981TRNFEQDw9PSkqmqSJFVVmabJcZwkSRjjPM/rumaCICiKIkkSy7IwxgghnucFQdB1XZIkjuOGw6Hv+2VZGoYhCALP84qi6LqOMWaOl5IkgRASQjDGoihOJpM4jjHGb29vEMIsy6IoMk3Ttm3HcQzDmM1muq4zlNL1ej2fz1mWHY/Hy+WyLEtKadd1u92uLMuiKLIsUxTl9vbWcZyu65qmOaqZ7+9vSulkMrm6urq/vyeE9H3/8/Pz9/fXti1CaLFYRFE0nU6P6+/vb9/3TdNQSv8Br4/ANozyMTIAAAAASUVORK5CYII=" alt="Faces peeking through a brick wall that they're building together." data-lqip="true"><img data-loading="true" loading="lazy" alt="Faces peeking through a brick wall that they're building together." src="https://cdn.sanity.io/images/599r6htc/localized/8e5444ab7a00b475670c9eea1a706122e8944027-2784x1566.png?w=2784&amp;h=1566&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/8e5444ab7a00b475670c9eea1a706122e8944027-2784x1566.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 1392w, https://cdn.sanity.io/images/599r6htc/localized/8e5444ab7a00b475670c9eea1a706122e8944027-2784x1566.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 2088w, https://cdn.sanity.io/images/599r6htc/localized/8e5444ab7a00b475670c9eea1a706122e8944027-2784x1566.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=2 2784w"></a></p><p>Figma CTO Kris Rasmussen shares more about how we <a href="https://www.figma.com/blog/why-roles-are-not-rules/">lift ideas up</a> with engineering crits.</p></div><p>By running engineering crits in FigJam, we could solicit a lot of feedback in a short amount of time, making it easier for more collaborators to jump in. Rather than having one person talk at a time or respond to a long thread of comments, everyone on the team could contribute. And FigJam’s open canvas would allow engineers to share early thinking, inspiration, and even screenshots of in-progress work, alongside helpful context or prompts for the broader team. As soon as we piloted our first few engineering crits with our immediate team of eight to ten people, everyone was bought into the collaborative approach. Once we arrived at a repeatable format, we opened up calendar invites and welcomed more and more people into the process, growing to upwards of&nbsp;200&nbsp;people.</p><h2 id="anatomy-of-an-eng-crit"><a href="#anatomy-of-an-eng-crit">Anatomy of an&nbsp;eng&nbsp;crit</a></h2><div><p>We send a calendar invite to every engineer working on the Figma editor and list them as “optional” so they can pop in or opt out&nbsp;as&nbsp;needed.</p></div><p>Today, the invite list includes the entire organization. While anyone—even cross-functional teams—is free to join, most Figmates opt in if a topic is related to their expertise. Sometimes, we’ll have insights to share on work that doesn’t directly involve us, and other times, many of us will simply be curious about what’s going on in the rest of the organization. My teammate and fellow Software Engineer Shirley Miao and I now host these sessions, which we record for&nbsp;posterity.</p><p>Our goals are&nbsp;to:</p><ul><li>Brainstorm and generate&nbsp;ideas</li><li>Identify difficult engineering challenges</li><li>Validate hypotheses</li><li>Share knowledge</li><li>Call out irreversible decisions</li></ul><p>The engineering crit has become core to the early and middle phases of developing technical designs—and even sometimes in the late phases for a particularly targeted question. It has spread through much of the engineering organization, at every altitude: crits that involve existential questions, ones that center a specific technical challenge, or a series of focused crits to gradually arrive on&nbsp;an&nbsp;approach.</p><div><figure><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAABYlAAAWJQFJUiTwAAAC7ElEQVQ4jYWUSXPcNhBG/ddzSeUSH5L8AB/i3LKc4qTiqGRL5VhSjR1JI42kGQ2HHG7DBdwJEgT5UqQSWbIOQdVXaGwPaADdz7pOUcuauq6psoqdLwjihKZt6XuN7jqUUozzxvYwDAxDP6nXmqaRlEVBFlfEpubZOKB7Td/3KNkg3C2ReUEV3CDFhrYI0DKnqxO0TNFVhtx5lM6WMgwpRUJT5HRVhcrlCBx3HBiLVg25tyS6eUuyesPu+pDIOKUKVhTugsK7QlhXnO79weHPP3Hw6ncOXu9jr1Z0eUpf5p+AI7PXiiJ2iK0zhPmBzfyI1dkxwe0x0eod0XrG7fWMX3/7he9fvuD5t9/xxVfP2XvzliwK6fLsc2BHkQdEwYo0XBJsrzFXc1xjRmy+R9gfcd0FV8ack7MjXvzwki+//obXe/tEnotKxAOXB1Ba4aQ2F84plr8gCtZ4OwMnXONHS0KxJMts2sSnck1WsxM+7O1jz8+RjoXeOY+BrVbc7Ez2zv/ibHmK562xxRYjdyeZhUtchCixo/dtOsdE2Ru0Z9H720kPXB7oeo0V+by/PGdhLHEDh3VocyVsbhIHI/YIs5i2yhnKf1U9sB8/yh0wKBNudhZG6LCJXP42LzkyPzL3l2xchzhOpn85/cX+qZ4ARZPjpDvcOMCMthzdHvLn4hUz55x17BEVKUp34w3drxuGT/YTYN5WRGVClCdYqcvMPuHd8oALd4VVhMRtjuq7B7DhkZ4As7okyAVxlRLUAiNx2DgWOz9ElDllJ9Gje/8LZJjCr5KStCgoZY1ULXUrkWVNU0pape5hY/kc1t/f4QQbUHJAVt0U8KqpaWVDW3eo9r8E0dHrflqo9V38PwSOffcnVE1P6ncIr6ZOHaRYkG5tYqumSNu7bFRVk7I0JY5CyrK426Tvp7ooCp6NRqc7mloRORLfjEidY/L1j/hnJ9jzDLGrybIMIQRxHLO1LK4Wl3ieO6W+kVGWJYZh8A8FPrvSBhVOngAAAABJRU5ErkJggg==" alt="Many cursors float around a FigJam with many sections and sticky notes." width="390" height="316" data-lqip="true"><img data-loading="true" width="390" height="316" loading="lazy" alt="Many cursors float around a FigJam with many sections and sticky notes." src="https://cdn.sanity.io/images/599r6htc/localized/972cfe08ce1d34cb9589d8340b2a7220f8a97b02-2364x1918.png?rect=0,2,2364,1915&amp;w=390&amp;h=316&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/972cfe08ce1d34cb9589d8340b2a7220f8a97b02-2364x1918.png?w=390&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 195w, https://cdn.sanity.io/images/599r6htc/localized/972cfe08ce1d34cb9589d8340b2a7220f8a97b02-2364x1918.png?w=390&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 293w, https://cdn.sanity.io/images/599r6htc/localized/972cfe08ce1d34cb9589d8340b2a7220f8a97b02-2364x1918.png?w=390&amp;q=75&amp;fit=max&amp;auto=format 390w, https://cdn.sanity.io/images/599r6htc/localized/972cfe08ce1d34cb9589d8340b2a7220f8a97b02-2364x1918.png?w=390&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 585w, https://cdn.sanity.io/images/599r6htc/localized/972cfe08ce1d34cb9589d8340b2a7220f8a97b02-2364x1918.png?w=390&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 780w"></p></div><figcaption>Running engineering crits in FigJam welcomes more people into the process, while keeping feedback focused.</figcaption></figure><figure><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAADcklEQVQ4jT2Sy27cRhBF+f9fYGQVOEEcIItYWchObBiQLVkBLHkkWzOSZjRPckgOH93N5qubTfIEkowsDgpVQN26wC2vbRqElIRxQi4K6sZgu57ODRjjnvpHWtNh7fPc9SPDONL3PVVZkWcZMtOo2OF11lI2Bj8pWW0KwqAhzRxJ3hHHLUlmSZUjTlrCqCYRlqId6XoYf4h2XUfXOZwZ8ZxzmG4gzloW94rlncYPLH5s2QYtwaEjlD3buOEh0OySlqwaaN3IMPSM/Q+G4emA96hubYdKFNHDgWityBKDVBYpDbkwZLkle6yyQegWXXeUTUORZRR+RB0ldIVmtBavqhvaqqZNU/RmTzaPEauUOhG0QiJ8STRXSF9jVUlflri6QheS9WzJ7ck3gssFbZQwlhqv0BVNVeOUIpvvuHl/xfT9hN3tnGizYv5lzvnxDbOzBTo4MCjBWGnasmIzDZm8f2D71cckOWiJ1xqDsxYnCzYXM45fvuHol3ecn18xvZ1y+u4zv7044s3vH/C/L2nDPUOe4JQmngsWZyGHaYLLJWj1HMroHL0qiKcLPh+f8vfRZ87/nTK/m3P58ZK/fv2HT0cn7L5eo+Y32N0CF0eIRUx4GaAWCb0soCqfQxmcYyhLsuWaiw+nfHx7wvRiSjjbsL5Y8u3Td+anZ+z+fUv67QQT3NOLFJNJ6kRji4bBWMauw3Pdo8OOXgnWlxNe/fSSly9+5vrsgv10jz+N2N+u2Vydc3d+TDg5xYRbRlP/eJeecRz+xzOdpXcGJw9srif8+eo1f7x6zc31jK0vWO0E4SYgWs7YLWakqy02l4xPey2d0XRtQWee8fJSU9uGtpHILGa3C9jtQ7LH1HXNQZbkuUCL5OlVyrLCmBbnDE2VIQ4bZLRGRlvizQov2EcIrShM9SSSpgqlS7SpyYuSJNMIXaGbEllopK6orKEbLLrOOEQPiGBJuHzg8myCF29ukOkaqRP2+xj/ISD3fYo0JM9z0lwjq5qiKpAifxItTYt9dGgVWoc0ak+0X/NlcoWXhxOK8BoZLdkvQ/zZnnx+j767pthtUbJE1w1FqZAyQ2mFVopGptg6x3Wa3pXoKmMdrvGkDlDpEhGuiLYHglXK4e4O8eUMcTMljwSqKBFKkuYpWZYjd3v06h6TB/SdeUq3aSqWqyX/AUsmqeHH+JthAAAAAElFTkSuQmCC" alt="Many cursors float around a FigJam with many sections and sticky notes." width="804" height="649" data-lqip="true"><img data-loading="true" width="804" height="649" loading="lazy" alt="Many cursors float around a FigJam with many sections and sticky notes." src="https://cdn.sanity.io/images/599r6htc/localized/398b54714391b419158654d219c97966800d951f-2466x1992.png?rect=0,1,2466,1991&amp;w=804&amp;h=649&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/398b54714391b419158654d219c97966800d951f-2466x1992.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 402w, https://cdn.sanity.io/images/599r6htc/localized/398b54714391b419158654d219c97966800d951f-2466x1992.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 603w, https://cdn.sanity.io/images/599r6htc/localized/398b54714391b419158654d219c97966800d951f-2466x1992.png?w=804&amp;q=75&amp;fit=max&amp;auto=format 804w, https://cdn.sanity.io/images/599r6htc/localized/398b54714391b419158654d219c97966800d951f-2466x1992.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 1206w, https://cdn.sanity.io/images/599r6htc/localized/398b54714391b419158654d219c97966800d951f-2466x1992.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1608w"></p></div></figure></div><p>Now that engineering crits are such a central part of our work, we have the process dialed in. Here’s a look at what we expect before, during, and after the&nbsp;session.</p><h4 id="before-the-crit"><a href="#before-the-crit">Before the crit</a></h4><div><p>I’ll admit that we sometimes take a shortcut and paste screenshots of targeted parts of our design doc into a FigJam file, rather than filling out the template from scratch, but this is the exception, not&nbsp;the&nbsp;rule!</p></div><p>To run engineering crits as efficiently as possible, we ask engineers to do some prep work. The must-have is a FigJam file with the design they want critiqued. We use <a href="https://www.figma.com/community/file/1259186143617682581/engineering-crit?searchSessionId=lsuz5497-4bthwob3vxr" target="_blank" rel="noreferrer">this template</a>, which has different prompts than a technical design doc. Reviewers need to quickly get the context they need and give meaningful feedback on it, as we know they don’t have time to digest full&nbsp;PRDs.</p><p>Before we begin, the presenter provides background context and frames the discussion: Are they looking for high-level ideas on large-scale architecture, or are they diving deep on a specific component? They also provide some details on ideas they’ve considered and leave plenty of room for&nbsp;discussion.</p><h4 id="during-the-crit"><a href="#during-the-crit">During the crit</a></h4><p>Engineering crits are explicitly <em>not</em> an approval or decision-making meeting. They are about getting the right feedback to lift ideas up. We ask reviewers to lead with suggestions as opposed to mandates, because we want to ensure we use the time in these meetings to capture as much of the collective learnings and wisdom of the group. We have other forums, including technical reviews, to ensure key feedback is acknowledged and&nbsp;acted&nbsp;on.</p><p>After that, the bulk of the meeting is still synchronous, but silent, as reviewers leave stickies in FigJam. This means we are able to have multiple conversations in parallel—allowing everyone to speak up—and reviewers can focus on the part of the conversation where they have the most expertise. If there’s time at the end of the meeting, the facilitator will turn attendees’ attention to other discussion topics or questions that might help the working&nbsp;group.</p><h4 id="after-the-crit"><a href="#after-the-crit">After the crit</a></h4><p>In most cases, one crit is enough for a team to figure out their approach, but sometimes they come out of the crit concluding that none of the proposed options are feasible. In that case, they’ll go back to the drawing board and attend another&nbsp;crit.</p><p>Of course, the process isn’t perfect, and it’s unrealistic to outline every possible edge case. Engineering crits are also not meant to replace technical reviews entirely. If we’re working on features that touch many different engineering teams or feel particularly high stakes, the engineering crit is a solid stepping stone on the way to a technical review.</p><h2 id="an-example-real-eng-crit"><a href="#an-example-real-eng-crit">An example (real) eng&nbsp;crit</a></h2><p>We recently launched <a href="https://www.figma.com/blog/introducing-ai-to-figjam/">AI in FigJam</a>, which helps teams overcome the “blank canvas” problem. It’s the perfect example of how a project flows through the engineering crit process and comes out better as a result. Here’s a look at how this project evolved at every stage of the engineering crit:</p><div><figure><div><p><img src="data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAANABQDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAQCBQYH/8QAIRAAAQMEAwADAAAAAAAAAAAAAQIDBAAREiEFEzEiQZH/xAAWAQEBAQAAAAAAAAAAAAAAAAACBQb/xAAZEQADAQEBAAAAAAAAAAAAAAAAARECEyH/2gAMAwEAAhEDEQA/AOdcVHLsVeGyVDWqfMRnBfX2h24BzSLUlxS+pgFN/md7q6mERojT7efYoEm6vv8AKrc21TMZymqzP8k0hmQEpFxiD5RUuXnOTZYeeCcygDQt5RQjBqXw/9k=" alt="A series of gardening illustrations corresponding to each stage in the engineering crit process." width="804" height="536" data-lqip="true"><img data-loading="true" width="804" height="536" loading="lazy" alt="A series of gardening illustrations corresponding to each stage in the engineering crit process." src="https://cdn.sanity.io/images/599r6htc/localized/9c4dc06426d780986529b4e3edcae2066d5edbf0-1056x704.jpg?w=804&amp;h=536&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/9c4dc06426d780986529b4e3edcae2066d5edbf0-1056x704.jpg?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 402w, https://cdn.sanity.io/images/599r6htc/localized/9c4dc06426d780986529b4e3edcae2066d5edbf0-1056x704.jpg?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 603w, https://cdn.sanity.io/images/599r6htc/localized/9c4dc06426d780986529b4e3edcae2066d5edbf0-1056x704.jpg?w=804&amp;q=75&amp;fit=max&amp;auto=format 804w, https://cdn.sanity.io/images/599r6htc/localized/9c4dc06426d780986529b4e3edcae2066d5edbf0-1056x704.jpg?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1056w"></p></div><figcaption></figcaption></figure></div><h4 id="step-1-scope"><a href="#step-1-scope">Step 1: Scope</a></h4><p><strong>Goal:</strong> Gut check concepts with a diverse group of people across engineering, design, and product management.</p><p>The AI team kicked off an engineering crit by exploring how we might leverage AI to empower users to work more efficiently in FigJam. We’ve been thinking a lot as a team about how to bring AI into Figma and FigJam, so the design team began dreaming up “what ifs” while the engineering team started reasoning about this problem space at its highest level, offering context around how prompt engineering works. The AI team shared one requirement: AI features need to read and write content on the current file. Other folks with expertise in areas like machine learning helped provide more information from various perspectives, identified challenges, or surfaced relevant solutions we’ve used in the past. The team shared four options, each with pros and cons, and aimed to get a sense for overall feasibility and architecture. Ultimately, they came away with a gut check on which approaches might be realistic, and which might be particularly challenging. Based on the feedback, they were empowered to take one of the options and start running with it, prototyping out an end-to-end functioning flow.</p><h4 id="step-2-iterate"><a href="#step-2-iterate">Step 2: Iterate</a></h4><p><strong>Goal: </strong>Solicit feedback at a regular cadence as the work evolves, addressing new challenges and revisiting early work&nbsp;as&nbsp;needed.</p><p>Next, the team sought feedback from a wider range of engineering teams because building AI functionality into FigJam required touching many different parts of the Figma editor. At this point, they also wanted guidance on how to score AI models. They posed a question to the group: “How might we improve our processes around measuring quality?”</p><p>In response, crit participants shared that the current system for quality measurement was manual: a large spreadsheet with an evaluation of how comprehensive, precise, structurally sound, and useful a model is. Rather than trying to engineer a quantitative approach, the ML team suggested focusing on qualitative attributes by running AI feature bashes for feedback on AI summarization and generation; since the dataset is so small, a more quantitative approach wouldn’t have been statistically rigorous anyway.</p><h4 id="step-3-refine"><a href="#step-3-refine">Step 3: Refine</a></h4><p><strong>Goal:</strong> Focus on polishing specific aspects of the project with relevant experts.</p><p>When it comes to building AI features, the devil is in the details. In this case, the team chose to do another deep dive crit specifically on versioning and got more feedback from the ML platform team on how we want to think about updating the version of our AI prompts and safely roll-out changes. In doing so, they learned about an existing versioning system we already use in <a href="https://www.figma.com/blog/how-figmas-multiplayer-technology-works/">Figma’s multiplayer technology</a>, and went back to the drawing board with this newfound knowledge. This system also helped inform the technical details around data input and summarization.</p><h4 id="step-4-review"><a href="#step-4-review">Step 4: Review</a></h4><p><strong>Goal: </strong>Make any outstanding decisions and review the plan holistically.</p><p>At this point, the team made final decisions informed by earlier crit feedback. In this case, they didn’t need to attend a crit because the directly responsible individual (DRI) had enough information to make a confident decision. They were empowered to move forward, without working their way through a more formal approval checklist.</p><h4 id="step-5-ship"><a href="#step-5-ship">Step 5: Ship</a></h4><p><strong>Goal: </strong>Attend technical reviews as needed, move forward with the project, and schedule any follow-up crit sessions for further&nbsp;work.</p><p>While this project didn’t go through a formal technical review, the working group did follow up with specific subject matter experts and stakeholders. After getting the green light on implementation, the team shipped <a href="https://www.figma.com/blog/introducing-ai-to-figjam/">FigJam AI</a>.</p><div><p><a href="https://www.figma.com/community/file/1259186143617682581" target="_blank" rel="noreferrer"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB2ElEQVQokY2SO28TURCF/e+REA0NBUgU/AEaRAEUNEiIIFkoCpCkMDGK117b613vfc19zId21xCXFEe3uKOjc2a+GapQAmQzqXgoAprR4Q/G9381o0TwSzh+nWSuICwh96CFwfLcOKWMhEh0hWiV6BMppn8zM7KD7jNUL2H1HNav4PARYo1qeUiIUkomBIe1FjEJMYXeCJ0LSJoazcgWmg+wfAx3j9DfT9H9W1TWaEkMDYa3qCC5w4QNJtRI7PESqW1i2QX2xhNT+mv4Hl0+gdULdPcGPV6gskHlgNqK4raINBzDHQf/nTZc04cVne1ZG+FX66kOR7wPU2UdKt4/Q7evoZ+DuwWpwVbo4ZrcLrBmRWN/snUX7Nycpl/QdA07G6htpLXhlHC4sLlCm3fQfkLNJbm/Jro1JRzAbSh+j8jhLOENJlRTQhtZuUKflDJdWdBwP5maS2L7jW4zp9kuCN5Me9RM0UhIHcbXONkTkx13uB0MTcKkMhI4YqNSgbtBzQ/sbk69/MLm/gZjekp5QCbnhPcO7zziEsEleus5Ok/Mw5UHwwHg1IJUY1LXLWg2t7T7ihD8ia9zDhNx4NDnkUEJQkrxjMOR8HzCI1JyIEZ/GnoA+/8EfwAyVlGUDmfRvwAAAABJRU5ErkJggg==" alt="An abstract wireframe in FigJam with a thumbs up." data-lqip="true"><img data-loading="true" loading="lazy" alt="An abstract wireframe in FigJam with a thumbs up." src="https://cdn.sanity.io/images/599r6htc/localized/ae1c410c4aae6881124858b1ce46ed6623a80f61-1706x960.png?w=1706&amp;h=960&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/ae1c410c4aae6881124858b1ce46ed6623a80f61-1706x960.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 853w, https://cdn.sanity.io/images/599r6htc/localized/ae1c410c4aae6881124858b1ce46ed6623a80f61-1706x960.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 1280w, https://cdn.sanity.io/images/599r6htc/localized/ae1c410c4aae6881124858b1ce46ed6623a80f61-1706x960.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1706w"></a></p><p>You can try out engineering crits with your team with <a href="https://www.figma.com/community/file/1259186143617682581" target="_blank" rel="noreferrer">this template</a>, the same one we use&nbsp;at&nbsp;Figma.</p></div><p>Many projects won’t fit neatly into this structure; some steps are collapsed into one crit, while others require a series of dedicated sessions. Still, these five steps are helpful scaffolding for anyone who isn’t sure where to begin. As we continue to evolve the process, our priority is making engineering crits a process that teams look forward to because it accelerates their work, rather than feeling like it gets in&nbsp;the&nbsp;way.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Airbnb is banning indoor security cameras (224 pts)]]></title>
            <link>https://www.theverge.com/2024/3/11/24097107/airbnb-indoor-security-camera-ban</link>
            <guid>39669167</guid>
            <pubDate>Mon, 11 Mar 2024 15:08:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/3/11/24097107/airbnb-indoor-security-camera-ban">https://www.theverge.com/2024/3/11/24097107/airbnb-indoor-security-camera-ban</a>, See on <a href="https://news.ycombinator.com/item?id=39669167">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Airbnb will no longer allow hosts to use indoor security cameras, regardless of where they’re placed or what they’re used for. In <a href="https://news.airbnb.com/an-update-on-our-policy-on-security-cameras/">an update on Monday</a>, Airbnb says the change to “prioritize the privacy” of renters goes into effect on April 30th.</p><p>The vacation rental app <a href="https://web.archive.org/web/20231223165134/https://www.airbnb.com/help/article/3061">previously let</a> hosts install security cameras in “common areas” of listings, including hallways, living rooms, and front doors. Airbnb required hosts to disclose the presence of security cameras in their listings and make them clearly visible, and it prohibited hosts from using cameras in bedrooms and bathrooms.</p><p>But now, hosts can’t use indoor security cameras at all. The change comes after <a href="https://www.newsweek.com/airbnb-camera-hidden-couple-reddit-lawsuit-1743269">numerous reports</a> of <a href="https://www.foxnews.com/tech/couple-finds-hidden-camera-disguised-as-smoke-detector-in-florida-airbnb">guests</a> finding <a href="https://www.buzzfeed.com/bradesposito/people-keep-finding-hidden-cameras-in-their-airbnb">hidden cameras</a> within their rental, leading some vacation-goers to <a href="https://www.theverge.com/23550845/smartphone-hidden-camera-android-ios-how-to">scan their rooms</a> for cameras.</p><p><a href="https://airbnb.pvxt.net/c/482924/264339/4273?u=https%3A%2F%2Fwww.airbnb.com%2Fhelp%2Farticle%2F3061%23%3A~%3Atext%3Dhere%2520to%2520help-%2CWhat%2520we%2520do%2520allow%2Cof%2520a%2520reservation%2520are%2520permitted.">Airbnb’s new policy</a> also introduces new rules for outdoor security cameras, and will now require hosts to disclose their use and locations before guests book a listing. Hosts can’t use outdoor cams to keep tabs on indoor spaces, either, nor can they use them in “certain outdoor areas where there’s a great expectation of privacy,” such as an outdoor shower or sauna. </p><p>Additionally, listings will have to disclose noise decibel monitors, which hosts might use to measure <a href="https://www.theverge.com/circuitbreaker/2018/10/29/18037604/noiseaware-gen-3-indoor-outdoor-security-microphone">whether there’s a party going on</a> in their rental — <a href="https://www.theverge.com/2022/8/17/23309433/airbnb-anti-party-technology-north-america-usa-canada-party-ban">something that Airbnb banned</a> in 2022. “These changes were made in consultation with our guests, hosts, and privacy experts, and we’ll continue to seek feedback to help ensure our policies work for our global community,” Juniper Downs, Airbnb’s head of community policy and partnership, says in a statement. </p><p>Airbnb hosts will have until the end of April to remove the security cameras from inside their listings. If a guest reports the presence of an indoor camera after that, Airbnb says it will investigate and that it could remove the host’s listing or account as a result. The new policy still can’t control the presence of hidden cameras, but it will at least offer some peace of mind knowing that rule-abiding hosts can no longer put cameras anywhere in their rentals.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intel Gaudi2 chips outperform Nvidia H100 on diffusion transformers (138 pts)]]></title>
            <link>https://stability.ai/news/putting-the-ai-supercomputer-to-work</link>
            <guid>39669008</guid>
            <pubDate>Mon, 11 Mar 2024 14:56:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stability.ai/news/putting-the-ai-supercomputer-to-work">https://stability.ai/news/putting-the-ai-supercomputer-to-work</a>, See on <a href="https://news.ycombinator.com/item?id=39669008">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709158639151_29172">
  <p>In our <a href="https://stability.ai/news/using-the-new-ai-supercomputer"><span>last installment</span></a>, we spoke about how we plan to utilize our state-of-the-art AI Supercomputer.</p><p>In this installment, we delve deeper into performance benchmarks and benefits of various compute solutions.</p><p>Our commitment to developing cutting-edge open models in multiple modalities necessitates a compute solution capable of handling diverse tasks with efficiency. To this end, we conducted a performance analysis, training two of our models, including the highly anticipated <a href="https://stability.ai/news/stable-diffusion-3"><span>Stable Diffusion 3</span></a>.</p><p>In our analysis, we compared the training speed of Intel Gaudi 2 accelerators versus Nvidia's A100 and H100, two of the most common choices for startups and developers training LLMs.</p><p><strong>Model 1:</strong></p><p><a href="https://stability.ai/news/stable-diffusion-3"><span><strong>Stable Diffusion 3</strong></span></a> is our most capable text-to-image model, soon to be in <a href="https://stability.ai/stablediffusion3"><span>early preview</span></a>.&nbsp;</p><p>Upon public release of Stable Diffusion 3, it will be available in sizes ranging from 800M to 8B parameters. Our analysis utilized the 2B parameter version and showed pleasantly surprising results.&nbsp;</p><p>We measured the training throughput for the 2B <a href="https://stability.ai/news/stable-diffusion-3-research-paper"><span>Multimodal Diffusion Transformer</span></a> (MMDiT) architecture model with d=24, BFloat16mixed precision, optimized attention (xFormers for A100 and the FusedSDPA for Intel Gaudi). We call this model version MMDiT-ps2-d24.</p><p>First, let’s examine our training benchmark results across 2 nodes, a total of 16 accelerators (Gaudi/GPU). Here’s an excerpt of the raw data:</p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709158639151_25063">
  <p>In this configuration, the Gaudi 2 cluster processed over 3x more images per second, compared to A100-80GB GPUs. This is particularly impressive considering that the A100s have a very optimized software stack.&nbsp;</p><p>On inference tests with the Stable Diffusion 3 8B parameter model the Gaudi 2 chips offer inference speed similar to Nvidia A100 chips using base PyTorch. However, with TensorRT optimization, the A100 chips produce images 40% faster than Gaudi 2. We anticipate that with further optimization, Gaudi 2 will soon outperform A100s on this model. In earlier tests on our SDXL model with base PyTorch, Gaudi 2 generates a 1024x1024 image in 30 steps in 3.2 seconds, versus 3.6 seconds for PyTorch on A100s and 2.7 seconds for a generation with TensorRT on an A100.&nbsp;</p><p>The higher memory and fast interconnect of Gaudi 2, plus other design considerations, make it competitive to run the Diffusion Transformer architecture that underpins this next generation of media models.</p><p><br><strong>Model 2:</strong></p><p><a href="https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models"><span><strong>Stable Beluga 2.5 70B</strong></span></a><strong> </strong>is our fine-tuned version of LLaMA 2 70B, building on the <a href="https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models"><span>Stable Beluga 2</span></a> model which was the first open model to best ChatGPT 3.5 in select benchmarks. We ran this training benchmark on 256 Gaudi 2 accelerators. Running our PyTorch code out of the box, with no extra optimizations, we measured an impressive total average throughput of 116,777 tokens/second. More specifically, this involves using a FP16 datatype, a global batch size of 1024, gradient accumulation steps of 2, and micro batch size of 2.</p><p>On inference tests with our 70B language model on Gaudi 2, it generates 673 tokens/second per accelerator, using an input token size of 128 and output token size of 2048. In comparison to <a href="https://nvidia.github.io/TensorRT-LLM/performance.html"><span>TensorRT-LLM</span></a>, Gaudi 2 appears to be 28% faster than the 525 tokens/second for the A100. We also anticipate further speed improvements with FP8.</p><p>Companies like ours face an increasing demand for more powerful and efficient computing solutions. Our findings underscore the need for alternatives like the Gaudi 2, which not only offers superior performance to other 7nm chips, but also addresses critical market needs such as affordability, reduced lead times, and superior price-to-performance ratios. Ultimately, the opportunity for choice in computing options broadens participation and innovation, thereby making advanced AI technologies more accessible to all.</p><p>Stay tuned for more insights in our next installment of "Behind the Compute."&nbsp;</p><p>To stay updated on our progress follow us on <a href="https://twitter.com/stabilityai"><span>Twitter</span></a>, <a href="https://www.instagram.com/stability.ai/"><span>Instagram</span></a>, <a href="https://www.linkedin.com/company/stability-ai"><span>LinkedIn</span></a>, and join our <a href="https://discord.gg/stablediffusion"><span>Discord Community</span></a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Onedoc (YC W24) – A better way to create PDFs (188 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39668962</link>
            <guid>39668962</guid>
            <pubDate>Mon, 11 Mar 2024 14:52:56 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39668962">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="39671945"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39671945" href="https://news.ycombinator.com/vote?id=39671945&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Congratulations on the launch — it looks fantastic! My company is also developing a similar product. We've chosen to create a visual report designer that enables end-users (non-developers) to create and tweak PDF reports, and integrate with the existing IT infrastructure via the API. Our experience is that users want changes in reports very often and that it's best to allow them do it on their own.<p><a href="https://www.cx-reports.com/" rel="nofollow">https://www.cx-reports.com</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39671939"><td></td></tr>
                <tr id="39671996"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39671996" href="https://news.ycombinator.com/vote?id=39671996&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>We spent many hours designing and generating PDFs at our previous venture.. terrible experience. Which is why we're now focused on solving this issue!</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39670933"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670933" href="https://news.ycombinator.com/vote?id=39670933&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>I've also spent much longer than I'd like on this same problem. Having a lightweight-enough service to convert html-&gt;pdf on the fly, with good fidelity, and that can create an <i>accessible</i> pdf seems to be impossible.<p>If you can nail accessible PDFs then you'd open up a <i>very</i> big government market.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671067"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39671067" href="https://news.ycombinator.com/vote?id=39671067&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>We felt the same, and that's precisely why we built this tool! The key, as you mentioned, is fidelity, especially for designing complex layouts. We hope to bring something new and valuable to the table. And yes, documents are central to many industries including government, legal, banking etc.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39671252"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39671252" href="https://news.ycombinator.com/vote?id=39671252&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Can you directly answer whether your tool generates <i>tagged</i> PDFs?<p>Of course, you can't guarantee that the resulting document is 100% compliant because you can't enforce that the input is valid, but are you at least outputting a complete tag tree with as much semantics as possible given the input?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671430"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39671430" href="https://news.ycombinator.com/vote?id=39671430&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Yes, Onedoc generates tagged PDFs as long as you add a `title` property to the API call to make the PDF UA/1 compliant.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="39670314"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670314" href="https://news.ycombinator.com/vote?id=39670314&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>FYI: the open source state of the art in this area is Playwright (the successor to Puppeteer) with Paged.js (<a href="https://pagedjs.org/" rel="nofollow">https://pagedjs.org/</a>). I highly recommend that everyone check out and donate to paged.js, it's a fantastic project with lots to like. It certainly blows commercial alternatives like Prince XML out of the water.<p>That forms a solid foundation that I find it hard to imagine paying for. The things where you might still command a premium are basically safety mechanisms/CI checks/library components that ensure the PDF renders correctly in the presence of variable-length content, etc. as well as maybe PDF-specific features like metadata and fillable forms. Naive ways to format headers, footers, tables/grids/flexboxes etc. often fail in PDFs because of unexpected layout complications. So having a methodology, process, and validation system for ensuring that a mission critical piece of information appears on a PDF in the presence of these constraints could be attractive.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671264"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39671264" href="https://news.ycombinator.com/vote?id=39671264&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>I think <a href="https://github.com/diegomura/react-pdf">https://github.com/diegomura/react-pdf</a> is closer to what this company is doing.<p>In fact their open source library, <a href="https://github.com/OnedocLabs/react-print-pdf">https://github.com/OnedocLabs/react-print-pdf</a>, seems like a higher-level library that sits above react-pdf. Reminds me a lot of the set of react-pdf based components I built for a corporate job where letting users create PDFs was a huge part of the value proposition.</p><p>They're solving a really cool problem, actually, because building out into certain difficult use cases like SVG support was a huge pain.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39670434"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670434" href="https://news.ycombinator.com/vote?id=39670434&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>We are currently experimenting with this approach. A good thing about paged.js is that we would be able to provide hot-reload and live preview of files without actually converting to PDF.<p>Your second point is very interesting, seems like some kind of .assert('text').isVisible() API. We may want to dig into that further!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39670470"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670470" href="https://news.ycombinator.com/vote?id=39670470&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>(How) does it handle CMYK and print PDFs? I see images of printed books created by Paged.js, were these post-processed, or printed using a printer that does a best-effort RGB conversion?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670569"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39670569" href="https://news.ycombinator.com/vote?id=39670569&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>I'm not sure - we don't do color correction on our PDFs because we don't have photos in them and color rendering is not mission critical - but paged.js is focused on the concern of layout for print media. I would imagine color rendering can be solved orthogonally to what paged.js does for you, as long as you specify the color data in CSS. I'm pretty sure paged.js will pass it through without messing with it, so you're good if the browser that Playwright/puppeteer is driving supports the correct color profile when emitting the PDF. I honestly don't know if browsers have sufficient support for that when emitting a PDF, though.<p>Overall you're right that color correction is another area where you could probably command a premium.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671894"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39671894" href="https://news.ycombinator.com/vote?id=39671894&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>It's certainly an area with more depth than I anticipated when I first started getting into it. Adobe is still pretty much the only one that can get a PDF compliant with print standards.<p>As far as I know, there's no way to currently get colors adhering to print color profiles in CMYK out of browsers.</p><p>Indeed, if color correctness isn't mission critical, I can imagine that going with Paged.js can be a nice experience!</p><p>(Edit: in my experience so far, it's been really really hard to 'correct' colors from an existing PDF in a way that gets a satisfying end result---the colors are usually muted/washed out)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671992"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39671992" href="https://news.ycombinator.com/vote?id=39671992&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>I was curious and searched around and found this presentation: <a href="https://www.w3.org/Graphics/Color/Workshop/slides/Erias.pdf" rel="nofollow">https://www.w3.org/Graphics/Color/Workshop/slides/Erias.pdf</a><p>You're right - although many of the building blocks are there, it appears there is no way to specify a colorspace or print profile when asking Chrome to emit a PDF (and I doubt the other browsers are any better). Skia (the PDF rendering engine that Chromium uses) actually supports colorspace transforms, but Chromium doesn't seem to hook that up to CSS or even support non-RGBA colors in its rendering pipeline.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                                    <tr id="39669425"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669425" href="https://news.ycombinator.com/vote?id=39669425&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>May be this is just me but this looks extremely costly to me! It will cost $2,500 to generate 50,000 PDFs. Are edits/corrections additional cost?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39671397"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39671397" href="https://news.ycombinator.com/vote?id=39671397&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>It sounds like this is as advanced as DocRaptor[1]. They have what I consider to be the best PDF generation API, giving complete control over the documents you need to create. The pricing is similar.<p>If you'd rather do it for free weasyprint[2] is the best open source alternative.</p><p>Another more affordable option you might want to consider is Urlbox[3]. (Disclosure: I work on this)</p><p>Urlbox's rendering engine is based on Chrome. It's been refined over the last 11 years to render pages as images or PDFs[4] that look great. I was a customer for 5 years before I joined the team. Everything we'd tried before Urlbox was a disappointment.</p><p>Urlbox probably can't match the power of either Onedoc or DocRaptor, but pricing starts at less than $0.01 per document and drops significantly with scale. If your PDF looks great when saving as PDF in Chrome it should look identically brilliant with Urlbox.</p><p>[1]: <a href="https://docraptor.com/" rel="nofollow">https://docraptor.com</a>
[2]: <a href="https://weasyprint.org/" rel="nofollow">https://weasyprint.org</a>
[3]: <a href="https://urlbox.com/" rel="nofollow">https://urlbox.com</a>
[4]: <a href="https://urlbox.com/html-to-pdf" rel="nofollow">https://urlbox.com/html-to-pdf</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39669512"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39669512" href="https://news.ycombinator.com/vote?id=39669512&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>This is a good point, and we are still trying to figure out how to price things fairly. Depending on the type of PDF, whether it is a simple receipt or a large multi-pages report, associated costs are very different on our side. At this time, we rely on other proprietary software that we are aiming to replace but that incur high costs on our side as well.<p>Edits and corrections on generated PDFs is not provided as the PDFs are signed as-is, however you can attach the metadata to the PDF and rerender with the modifications.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670331"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39670331" href="https://news.ycombinator.com/vote?id=39670331&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>As a point of reference on pricing, convertAPI charges $0.05 per document conversion at their most expensive tier, and with any level of fixed commitment ($80 - $300 per month) it goes down to $0.016-0.006 per document.<p>Their PDF conversion is pretty good (I use it for PPT/Word -&gt; PDF conversion), though your product is obviously different and has different/better capabilities for programmatic PDF creation. Still, a reference point.</p><p>Pricing page: <a href="https://www.convertapi.com/prices" rel="nofollow">https://www.convertapi.com/prices</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39669654"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39669654" href="https://news.ycombinator.com/vote?id=39669654&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Edits would be limited to certain pages but may spill over (e.g. tables) so the whole PDF need not be generated. Only edited pages can be inserted back to previously generated PDFs. Could be an optimization to reduce cost.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39669631"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39669631" href="https://news.ycombinator.com/vote?id=39669631&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>I second this. Maybe I'm missing something in the value proposition, but we already generate PDFs from .docx/.html templates using open source libraries and Docker microservices.<p>Do not misunderstand. A Stripe for generating PDFs can be great, but for a small team, $0.50/PDF is way more than I can afford (after all, you can create a small number of PDFs without too much fuss). Maybe you are oriented towards large companies?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670787"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39670787" href="https://news.ycombinator.com/vote?id=39670787&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Indeed, and as you mentioned, open-source libraries are always an option. It's worth noting that our open-source library assists in document design, allowing freedom in renderer choice. While the open-source library is aimed at individuals, our API targets businesses of any size. Our pricing can be as low as $0.05 per PDF for high-volume or annual commitments. Additionally, we offer cloud hosting for your documents for up to 90 days, and our pricing includes analytics.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39670373"><td></td></tr>
                  <tr id="39671520"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39671520" href="https://news.ycombinator.com/vote?id=39671520&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Looks awesome, will keep this in mind - every so often you need to create complex documents in code, and it's always a pain. Doing it with a familiar modern programming interface would be nice.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39671577"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39671577" href="https://news.ycombinator.com/vote?id=39671577&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Exactly, that's one of the main reasons we began working on this. We aim to bring the modern web technologies used for website design into the document world. This includes enabling the use of React and, of course, Tailwind, Chakra UI, etc.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39670676"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670676" href="https://news.ycombinator.com/vote?id=39670676&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Love the demo on the homepage with the render button. Really helps explain the product!</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670796"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670796" href="https://news.ycombinator.com/vote?id=39670796&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Thanks! We try to make our product as accessible as possible for anyone to use (or at least to test). It's good to hear that our efforts have been worthwhile!</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39669738"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669738" href="https://news.ycombinator.com/vote?id=39669738&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Really interesting product. I do agree that the pricing seems steep ($0.25/document on Pro on the most generous tier) but I don't know enough about pricing B2B products to know if that would be a blocker.<p>I agree that HTML -&gt; PDF can be a really powerful tool. I worked on the UK government's tool to generate energy efficiency labels for consumer goods [0] and we ended up doing PDF generation with SVG templates, using Open HTML to PDF for the conversion. That ended up working very well, though as you allude to there can be some gotchas (eg unsupported CSS features) that you need to work around.</p><p>A few questions:</p><p>- Do the rendered documents support PDF's various accessibility features?</p><p>- How suitable is this for print PDF generation? For example, what version of the PDF spec do you target? What's your colour profile support like? Do you support the different PDF page boxes (MediaBox, CropBox, BleedBox, TrimBox, ArtBox)?</p><p>[0] <a href="https://github.com/UKGovernmentBEIS/energy-label-service">https://github.com/UKGovernmentBEIS/energy-label-service</a></p><p>[1] <a href="https://github.com/danfickle/openhtmltopdf">https://github.com/danfickle/openhtmltopdf</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670010"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670010" href="https://news.ycombinator.com/vote?id=39670010&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>The pricing does go down for larger volumes and is something we still have to narrow down to the exact place that makes sense to companies and is also viable.<p>- We do not force PDF/* profiles down to the user, but it seems that for most of them PDF/UA-1 would be a sensible default. We can extract most of the tags from the HTML semantics by themselves which makes it much easier.</p><p>- We target the PDF 1.7 spec. Color profiles can be changed and you can use a custom .icc profile, with the corresponding embedding restrictions based on the document format. MediaBox is supported through the @page size property. Bleed, trim and marks can be added using vendor specific css properties. We don't support ArtBox yet but this is something we can look into! So far none of our customers really wanted to take this out to a real print shop, but we would be glad to help people go down this route :)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671287"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39671287" href="https://news.ycombinator.com/vote?id=39671287&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>So are you saying that you don't output tagged PDFs now?<p>For those who don't know, if you use Chromium's print-to-pdf feature you get a tagged PDF. And it's scriptable from the command-line too.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39669806"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669806" href="https://news.ycombinator.com/vote?id=39669806&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>I had to deal a lot with PDF generation over the past few years and I was very unhappy with the eco-system that was available:<p>1. HTML-to-PDF: The web has a great layout system that works well for dynamic content. So using that seems like a good idea. BUT it is not very efficient as a lot of these libraries simply spin up a headless browser or deal with virtual doms.</p><p>2. PDF Libraries (like jsPDF): They mostly just have methods like ".text(x, y, string) which is an absolute pain to work with when building dynamic content or creating complex layouts.</p><p>This was such a pain point in various projects I worked on that I built my own library that has a component system to build dynamic layouts (like tables over multiple pages) and then computes that down to simple jsPDF commands. Giving you the best of both worlds.</p><p>Hope this makes somebody's life a bit easier: <a href="https://github.com/DevLeoko/painless-pdf">https://github.com/DevLeoko/painless-pdf</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670020"><td></td></tr>
                <tr id="39670140"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39670140" href="https://news.ycombinator.com/vote?id=39670140&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Yes, page breaks are probably the most significant difference between the layout of a web page and a PDF document, and thereby a major drawback when using HTML-to-PDF. There is little to no tooling for this in the web.<p>If you want granular control over how your PDF will look with content that is more than one page long, you will have a hard time using html.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670251"><td></td></tr>
            <tr id="39670311"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39670311" href="https://news.ycombinator.com/vote?id=39670311&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>That's what we are trying to solve at Onedoc, we want developers to be able to have full control over the PDF layout as they write content. react-print is built with the intention of creating the illusion that React was meant for PDFs.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="39671294"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39671294" href="https://news.ycombinator.com/vote?id=39671294&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Hmm interesting... I just went through this user experience on iOS generating PDF invoices locally. I attempted the HTML &gt; PDF route, but Webkit is thorny wrt to layouts (as you mentioned). I did settle in with drawing everything from the ground up &gt; which with LLMs wasn't as hairy as it used to be, even got a little Swift framework out of the deal.<p>Am I understanding the docs correctly that you don't have a local library available (the SDKs are just calling the APIs right?)? Mind going through why you chose a remote API?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671497"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39671497" href="https://news.ycombinator.com/vote?id=39671497&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>You are right in the sense we do not provide a local library. We considered the option but would have brought a lot of challenges to accommodate the various runtimes and device capabilities.<p>This may come at a later stage once we have built our own rendering engine though
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39671553"><td></td></tr>
                <tr id="39671571"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39671571" href="https://news.ycombinator.com/vote?id=39671571&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>There are many reasons behind it, to name a few: files are self-contained(*) and easily portable, can guarantee some security features, the format is easily extended, and the ecosystem is very large.<p>It seems that a better format should exist, but the fact that PDF is the de-facto for portable documents make it unlikely things can change overnight.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39669582"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669582" href="https://news.ycombinator.com/vote?id=39669582&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Super interesting and potentially a fit for a project I'm working on right now. What are the benefits of going this route vs styling your page for print (ex. tailwind print modifier) and relying on the browser's print dialogue?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39669652"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39669652" href="https://news.ycombinator.com/vote?id=39669652&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>There is both commonalities and differences! Both approaches rely on web technology to provide the layout and are flexible in terms of frameworks and integrations.<p>Where things differ is that we don't actually use a browser under the hood. This allows a much better control over typesetting and layout - and you can do it on the server. We have also more controls over the outputted PDF and the ability to use more advanced features such as form fields or embedding other files and metadata in the PDF.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39669634"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669634" href="https://news.ycombinator.com/vote?id=39669634&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Is this just a wrapper around Puppeteer that renders a pdf? I do this currently with an AWS lambda that has a chrome-aws-lambda layer.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39669717"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39669717" href="https://news.ycombinator.com/vote?id=39669717&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>We use a dedicated HTML to PDF engine (such as PrinceXML) rather than building on top of a browser. Main issue with browser-backed implementations is that PDFs are often of subpar quality. However, the main good thing is you can rely on the latest CSS features.<p>In the end, what was the main decisive factor is the support for the PrintCSS and PagedMedia specifications, which have been completely discarded by major vendors and only implemented by specific engines.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39669457"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669457" href="https://news.ycombinator.com/vote?id=39669457&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Congrats! My career has also revolved around PDF generation (once for federal compliance at large companies, second for scrubbing data from PDFs for HIPAA compliance and then generating a new pdf based on the scrubbed data). I think I've seen your tool around, I ended up creating a workflow that generated LateX scripts then converted them to pdfs, and the second a python library. The most difficult aspect for our tools was formatting - the pdfs were generally 60-100 pages and tables could show up anywhere and break the page/formatting. Quite curious to see how your company will grow, good luck!</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39669964"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39669964" href="https://news.ycombinator.com/vote?id=39669964&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Curious, which python library did you use to convert to PDFs? currently looking into a couple options myself</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39671056"><td></td></tr>
                        <tr id="39669517"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669517" href="https://news.ycombinator.com/vote?id=39669517&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>It seems TeX/LaTeX is a major inspiration in this, though there can be seen some room for improvement for details like hyphenation, expansion/protusion and microtypography. Not sure if/how a web engine can reach to those points but still it seems this has a potential niche and market outcome, so congrats.<p>Though personally I wish stuff like ConTeXt was more popular and approachable - to my humble knowledge their Lua backend seems to have huge potential, I am doing my invoices with ConTeXt/Lua.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39669561"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39669561" href="https://news.ycombinator.com/vote?id=39669561&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>It definitely is! Typesetting quality was the main reason we chose not to go down the Puppeteer/headless browser route but rather use a completely separate engine where typography is a first-class citizen.<p>We like LaTeX, but even for advanced users laying things out can be a difficult thing. Given that documents are a frontend, we wanted to bring the same tools frontend developers already use.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39669987"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669987" href="https://news.ycombinator.com/vote?id=39669987&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>This looks really interesting! One of the main reasons we've opted to writing a more complex rending code is for speed. We're getting around 500ms for a single document, which is (last I tested) quicker than any headless chrome setup.<p>How long does it take to render using your API? :)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670106"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670106" href="https://news.ycombinator.com/vote?id=39670106&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Rendering time scales with the length / complexity of the document. At the moment, our self-serve API renders slower than a headless chrome setup. We are working on speeding this up as it is currently in the order of seconds.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39670471"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670471" href="https://news.ycombinator.com/vote?id=39670471&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>How is this better than writing out an HTML file, then using headless chrome to export to PDF, like this:<pre><code>    "C:\Program Files\Google\Chrome\Application\chrome.exe" --headless --disable-gpu --print-to-pdf=C:\temp\foo.pdf --no-margins --print-to-pdf-no-header C:\temp\test.mhtml</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670651"><td></td></tr>
            <tr id="39670575"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670575" href="https://news.ycombinator.com/vote?id=39670575&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>This brings its own set of challenges. Headers and footers are strictly limited in terms of features, you cannot add footnotes, the notion of page spreads is harder to implement. Then you need to combine that with having a Chrome instance at hand + exposing the needed assets for URL resolution. Definitely not difficult let alone impossible, but not the easiest way to get started :)</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39671295"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39671295" href="https://news.ycombinator.com/vote?id=39671295&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>The easier way costs $0.05 cents per page. Imagine sending an invoice to your customer and the invoice itself costs 5 cents per page! That's prohibitively expensive for many applications. I wouldn't consider any solution that costs more than 1 cent per page.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39671361"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39671361" href="https://news.ycombinator.com/vote?id=39671361&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>We bill per document, so the number of pages wouldn't impact the pricing. A 5 pages invoice would come at 1 cent per page. However, it seems that each and every company has different needs and the pricing may or may not make sense for them. There are alternative billing options that we are considering but we want to keep it easy to grasp rather than go into billing kilobytes or ms of execution. We would be more than happy to discuss use cases and see what can work for each company :)</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="39670235"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670235" href="https://news.ycombinator.com/vote?id=39670235&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>We're using Gotenberg[1] to convert a rendered web page (with Elixir/Phoenix, in our case) to PDF. Works like a charm and we can use our existing frontend code/styling (including SVG graph generators) which is a huge bonus.<p>1: <a href="https://gotenberg.dev/" rel="nofollow">https://gotenberg.dev/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670326"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670326" href="https://news.ycombinator.com/vote?id=39670326&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>We actually experimented with Gotenberg! Ultimately it is a layer on top of Chromium for conversion and we were dissatisfied with the results. I am curious so as to how are you handling assets and other static media / attachments: do you embed everything in a single HTML file or do you use some kind of bucketing system to resolve URLs?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39671790"><td></td></tr>
                        <tr id="39669684"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669684" href="https://news.ycombinator.com/vote?id=39669684&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>This is definitely a huge market. Are you targeting React developers only? I've successfully used html2pdf in the past, but looking again at their Github, it seems there has been no update in the last three years.<p>I think SOC2 is a must to start engaging with companies. Most PDFs will have sensitive data, and not many companies will feel comfortable sending customer data to a 3rd party platform, so you need security measures and certifications.</p><p>Good luck!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39670222"><td></td></tr>
                <tr id="39670284"><td></td></tr>
                  <tr id="39670911"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670911" href="https://news.ycombinator.com/vote?id=39670911&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>I wonder what YC expects from such investments (considering the multitude of FOSS solutions in this area).</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670992"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670992" href="https://news.ycombinator.com/vote?id=39670992&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>While this may sound a bit counterintuitive (maybe?) we actually pivoted to this field based on YC input and discussions they have had with their previous companies. The multitude of FOSS solutions in this area indicates this is a real problem people are willing to spend time on, and yet there is no go-to solution and every team we have talked to selected different tools based on a very specific requirement.<p>This may not mean success, it means that game is not over in the documents field :)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671047"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39671047" href="https://news.ycombinator.com/vote?id=39671047&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Thanks for the perspective. Indeed, this is an area with real demand. I haven't evaluated YC's recent startups but I trust they do know a bit about what has a better chance in the market. Best of luck :)<p>ps.: As someone with very minimal PDF needs personally and at work, I'd say the beautiful templates are what caught my attention the most.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39670014"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670014" href="https://news.ycombinator.com/vote?id=39670014&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Glad to see people building in the PDF space, which as a format is unfortunately both awful and ubiquitous. Are you planning to build any support for programmatically filling out existing PDF forms? That's a huge pain point our product is facing that doesn't seem easy to solve.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670980"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670980" href="https://news.ycombinator.com/vote?id=39670980&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>I'm facing that same pain point of programmatic PDF filling. I noodled around in the PDF format and learned it's a bit difficult to deal with fonts and formatting. But I think this client-side library works well enough, as a start: <a href="https://pdf-lib.js.org/#fill-form" rel="nofollow">https://pdf-lib.js.org/#fill-form</a><p>I've also heard of one paid API that I forgot but seemed to work well, and this related service <a href="https://www.jotform.com/" rel="nofollow">https://www.jotform.com/</a>, and I also considered porting some server-side libraries to WASM. One day I'll collect all the libraries and findings in a blog post.</p><p>Are you looking to programmatically fill any PDF form by detecting the fields? Or are you filling one known PDF template?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39670221"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670221" href="https://news.ycombinator.com/vote?id=39670221&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Yes, our focus is on programmatic interactions with PDFs, form filling is on our roadmap, alongside programmatic digital signature and many more.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670242"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39670242" href="https://news.ycombinator.com/vote?id=39670242&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Amazing, is there anywhere I can follow along to find out when form filling will be available?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670352"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39670352" href="https://news.ycombinator.com/vote?id=39670352&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Sure! Feel free to join our Discord, we post announcements as soon as new features are released. You can also ask for features, we prioritise these requests with enterprise customer's in our development roadmap.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="39670924"><td></td></tr>
            <tr id="39670708"><td></td></tr>
                <tr id="39670798"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670798" href="https://news.ycombinator.com/vote?id=39670798&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>It is similar to pspdfkit. We add an abstraction layer over the HTML and assets hosting to make it easier to use without having to think too hard about security and serving assets.<p>We also hope to keep the focus on the PDF generation part rather than expanding super-horizontal style to provide all imaginable PDF tools at the expense that none is really good.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39669730"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669730" href="https://news.ycombinator.com/vote?id=39669730&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>So are you using PrinceXML for your "completely separate engine where typography is a first-class citizen"?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670196"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670196" href="https://news.ycombinator.com/vote?id=39670196&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Yes, we use an API layer on top of PrinceXML with additional polyfills to support modern features. This is a meh solution but it allowed us to iterate quickly and get to work with customers without building a full blown PDF engine firsthand. However building this engine ourselves is the key to reduced latency and overall better feature support. But we need to engage with our users first and see exactly where we should head first :)</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39669889"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669889" href="https://news.ycombinator.com/vote?id=39669889&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Can we not have an alternative to PDFs? I get that they're more standardized but why would everyone let adobe have the hammer for a file type that's so important</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670094"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670094" href="https://news.ycombinator.com/vote?id=39670094&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>We quite agree on this - but getting a new alternative out will require a significant critical mass before it can be of any interest. While PDF has its challenges, it remains a light portable format and its security features make it a good fit for binding documents. The ecosystem, although it is dominated by Adobe, also includes other major players and existing integrations.<p>The way we look at it is PDFs allows embedding of other files and metadata. It is easy to provide a platform where we can enrich PDFs to display different contents than the one in the PDF itself. If this gets interesting enough, we can then phase out the PDF in the first place. But this is a long way ahead.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39670615"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670615" href="https://news.ycombinator.com/vote?id=39670615&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>PDF is an incredibly (stupidly) extensible format. There are tons of government forms that (sadly) bake in complex workflows into PDF forms.<p>Given that the whole world has been running on PDFs for decades it's makes more sense to leverage the existing infrastructure and move it towards something more functional over time. Introducing a new format will just lead to another format the achieves 0.5% marketshare and then is abandoned after a few years. Microsoft basically forcing people to use XPS in windows (&gt;70% market share of computing) still wasn't able to achieve meaningful usage or change.</p><p>I expect that PDFs will not go away for 20 years at least, but who knows
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39670421"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670421" href="https://news.ycombinator.com/vote?id=39670421&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>PDF is an open format in the sense that you don't need to pay Adobe a license fee for generating PDFs, or for reading and rendering PDFs. The format is fully documented, although the specification is controlled by Adobe.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39670151"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670151" href="https://news.ycombinator.com/vote?id=39670151&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>For supply chain workflows the ASC X12 Electronic Data Interchange (EDI) industry standard works much better than PDFs. Unfortunately, despite being around for decades in has only been adopted by forward thinking organizations such as Walmart. Most smaller companies and their vendors still haven't implemented EDI.<p><a href="https://developer.walmart.com/home/us-edi/" rel="nofollow">https://developer.walmart.com/home/us-edi/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670591"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39670591" href="https://news.ycombinator.com/vote?id=39670591&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Insanity.<p>EDI is the only place where people are regularly still paying for message by the kilobyte, where unsecured FTP over the open internet is still a norm, and where entire cottage industries exist to support AVOIDING using EDI.</p><p>Source: I work in EDI. it's a pain in the rump.</p><p>Also, EDI is really only good for things like PO's, shipping notices, invoices, sales orders, etc.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671515"><td></td></tr>
            <tr id="39671078"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39671078" href="https://news.ycombinator.com/vote?id=39671078&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>&gt; Also, EDI is really only good for things like PO's, shipping notices, invoices, sales orders, etc.<p>Don't forget health insurance claims, eligibility &amp; benefits, and prior auth requests!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671493"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39671493" href="https://news.ycombinator.com/vote?id=39671493&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>EDI is used in a lot of situations for machine-to-machine communications, but outside USA I believe EDIFACT is much more used (X12 is mostly used in USA).<p>Today many EDIFACT documents have been converted to ebXML: <a href="https://en.wikipedia.org/wiki/EbXML" rel="nofollow">https://en.wikipedia.org/wiki/EbXML</a></p><p>Source: Worked in EDI for a few years
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="39670131"><td></td></tr>
                <tr id="39670652"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39670652" href="https://news.ycombinator.com/vote?id=39670652&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Giving credit where it's due, I can appreciate Microsoft for introducing XPS as an alternative to pdf.<p>There was a time, when not every software had "export to pdf". So, having a "print to pdf" meant installing (often pirated) Adobe Acrobat or installing a sketchy free(ware) printdriver software downloaded from sourceforge.</p><p>MS adding xps print driver to windows enabled sharing docs consistently (within windows ecosystem) without resorting to hacks.</p><p>I don't know why it didn't catch up. May be it was the general mistrust of anything MS,  it arrived too late or it was something else.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670931"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39670931" href="https://news.ycombinator.com/vote?id=39670931&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Indeed, we need to give credit to MS for what they did. However, it didn't catch up as you mentioned, maybe due to timing, skepticism toward MS, or the complexity of moving from Adobe to MS for PDF management. I will dig a bit into it and come back later if I find anything interesting.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="39670584"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670584" href="https://news.ycombinator.com/vote?id=39670584&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>I just wanted to add that if you want to convert plaintext files to pdf, vim has a builtin feature to do so:<pre><code>  vim filename.txt -c "hardcopy &gt; filename.ps | q" &amp;&amp; ps2pdf filename.ps #convert ps to pdf</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39669934"><td></td></tr>
                <tr id="39670148"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670148" href="https://news.ycombinator.com/vote?id=39670148&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>No, we don't currently do that. However, we are considering adding metadata to PDFs, and using pdfmark could be very helpful!</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39670037"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670037" href="https://news.ycombinator.com/vote?id=39670037&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>The problem with using Tailwind is that I can't just say &lt;h1&gt;Some Heading&lt;/h1&gt;. As noted in the Tailwind documents "All heading elements are completely unstyled by default, and have the same font-size and font-weight as normal text."[1]<p>Most of the time when I'm writing HTML I want a set of default styles for the most common elements,
It's tedious and error-prone to have to specify a class <i>every single time</i>.</p><p>1 <a href="https://tailwindcss.com/docs/preflight" rel="nofollow">https://tailwindcss.com/docs/preflight</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670147"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670147" href="https://news.ycombinator.com/vote?id=39670147&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Makes total sense. There is no real requirement to use Tailwind to create the PDFs, we just have grown accustomed to Tailwind :) If you don't use the &lt;Tailwind&gt; tag, the browser defaults are used to generate the PDF.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39670174"><td></td></tr>
            <tr id="39671147"><td></td></tr>
                <tr id="39671451"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39671451" href="https://news.ycombinator.com/vote?id=39671451&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Editors such as Overleaf, and those offered by MS and Adobe, have been around for a long time. Recently, companies like Pandadoc and Docusign have started offering services around PDFs (generation or other aspects of their lifecycle).<p>It might seem odd, given our long history with PDFs, but I believe there's still much to be done with these documents. They're everywhere—invoices, tickets, reports, etc.—yet the technology for generating and managing them hasn't evolved much in years. Our approach is to apply the same modern technologies used for web design to document design.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39671313"><td></td></tr>
                <tr id="39671490"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39671490" href="https://news.ycombinator.com/vote?id=39671490&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>When I was first hired 15 years ago my first task was to create a PDF report. It was easy back then in PHP+fPDF. Two years ago I was hired to work on a Heroku-hosted NodeJS app. I was surprised to find that generating a PDF turned out to be substantially more difficult task, requiring running a browser emulator or connecting to an external service. And now, seeing PDF generation as a premium pay-as-you-go product is just too much.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39671557"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39671557" href="https://news.ycombinator.com/vote?id=39671557&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Makes sense. Actually, if you keep the layout/content very simple, aren't constrained by throughput, and don't need to integrate dynamic data or other similar processes, then simple FOSS could indeed get the job done! That's exactly why we developed the open-source library react-print-pdf</span></p></div></td></tr>
        </tbody></table></td></tr>
                              </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Extropic is building (151 pts)]]></title>
            <link>https://www.extropic.ai/future</link>
            <guid>39668430</guid>
            <pubDate>Mon, 11 Mar 2024 14:09:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.extropic.ai/future">https://www.extropic.ai/future</a>, See on <a href="https://news.ycombinator.com/item?id=39668430">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Mar 11th, 2024								</p><p>/</p><p>Litepaper</p></div><div><p>Message from the team</p><div><p><em>We are very excited to finally share more about what Extropic is building: a full-stack hardware platform to harness matter's natural fluctuations as a computational resource for Generative AI.</em></p><p>What does this novel paradigm of computing practically mean for the world?</p><ul role="list"><li><em>Extends hardware scaling well beyond the constraints of digital computing</em></li><li><em>Enables AI accelerators that are many orders of magnitude faster and more energy efficient than digital processors (CPUs/GPUs/TPUs/FPGAs)</em></li><li><em>Unlocks powerful probabilistic AI algorithms that are not feasible on digital processors</em></li></ul><p><em>Our brief Litepaper (below) provides an early glimpse at our technologies. We hope the following excites you for the journey ahead. Join us as we accelerate towards the thermodynamically intelligent future.</em></p><p><strong>- Gill and Trev</strong></p></div></div><p><span>T</span>he demand for computing power in the AI era is increasing at an unprecedented exponential rate. Luckily, for the past several decades, the miniaturization of CMOS transistor technology following Moore’s law <a href="#references">[1]</a> has allowed much of this exponential growth to be accounted for by increasing computer efficiency.		</p><div><p>Unfortunately, Moore’s law is starting to slow down <a href="#references">[2]</a>. The reason for this is rooted in fundamental physics: transistors are approaching the atomic scale where effects like thermal noise start to forbid rigid digital operation <a href="#references">[3, 4, 5]</a>.</p><p>As a result, the energy requirements of modern AI are beginning to take off. Major players are proposing measures as extreme as building nuclear reactor-powered data centers dedicated to large model training and inference. Continuing this scaling for a few more decades will require infrastructure engineering efforts of unprecedented scale and represents an arduous path forward for scaling humanity’s aggregate intelligence.</p><p>On the other hand, biology is neither rigid nor digital and hosts computing circuitry that is much more efficient than anything humanity has built to date. Inter-cellular chemical reaction networks drive computation in biological systems. Cells are small, and as a result, the number of reactants in these networks is countable <a href="#references">[6, 7]</a>. Therefore, reactions between reactants are genuinely discrete and intrinsically random. The relative effect of this intrinsic randomness scales inversely with the number of reactant molecules, and as such, fluctuations tend to dominate the dynamics of these systems.</p><p>From this, we can say with certainty that there is no fundamental reason for the constraints of digital logic to bind the efficiency of computing devices. The engineering challenge is clear: how can we design a complete AI hardware and software system from the ground up that thrives in an intrinsically noisy environment?</p><p>Energy-Based Models (EBMs) offer hints at a potential solution, as they are a concept that appears both in thermodynamic physics and in fundamental probabilistic machine learning. In physics, they are known as parameterized thermal states, arising from steady-states of systems with tunable parameters. In machine learning, they are known as exponential families.</p><p>Exponential families are known to be the optimal way to parameterize probability distributions, requiring the minimal amount of data to uniquely determine their parameters <a href="#references">[8]</a>. They are thus excellent in the low-data regime, which encompasses scenarios where one needs to model tail events in mission-critical applications, as depicted in figure 1. The way they achieve this is by filling the blanks in data with noise; they seek to maximize their entropy while matching the statistics of the target distribution. This process of hallucinating every possibility that is not included in a dataset and penalizing such occurrences energetically requires the usage of a lot of randomness, both at training and inference time.</p></div><div id="figure-1"><p><span>Figure 1: The principles of Extropic probabilistic AI accelerators</span> A simple example of low-complexity distributional learning failing to capture the effect of a tail event. Low air pressure almost always means rain and high crop yields. However, every so often, very low air pressure corresponds to a hurricane.</p></div><div><p>This requirement of sampling has been the main limiter in production uses of EBMs. The fundamental reason for this is that sampling from generic energy landscapes is very difficult on digital hardware, which must expend substantial electrical energy to generate and shape the entropy required for the diffusion processes. From a hardware perspective, digital sampling seems quite contrived: why put so much effort into building increasingly complex pristine digital computers when the most common and compute-intensive algorithms turn around and pump them full of noise?</p><p>Extropic is shortcutting this inefficiency and unlocking the full potential of generative AI by implementing EBMs directly as parameterized stochastic analog circuits. Extropic accelerators will achieve many orders of magnitude of improvement over digital computers in terms of both runtime and energy efficiency for algorithms based on sampling from complex landscapes.</p><p>The operational principle of Extropic accelerators is analogous to Brownian motion. In Brownian motion, macroscopic but lightweight particles suspended in a fluid experience random forces due to many collisions with microscopic liquid molecules. These collisions lead to the random diffusion of the particles around the vessel. One could imagine anchoring the Brownian particles to the vessel walls and each other with springs, as depicted in Fig. 2 (a). In this case, the springs will resist the random forces, and the particles will prefer to reside in particular parts of the vessel more than others. If one repeatedly sampled the positions of the particles, waiting sufficiently long in between samples (as illustrated in Fig. 2 (b)), one would find that they follow a predictable <em>steady-state</em> probability distribution. If we changed the stiffness of the springs, this distribution would change. This simple mechanical system is a source of programmable randomness.</p></div><div id="figure-2"><p><span>Figure 2: The operational principle of Extropic accelerators &nbsp;<strong>(a)</strong></span> A simple mechanical analogy to Extropic accelerators. Since there are three masses in two dimensions, the steady state of this device would be a probability distribution over a 6-dimensional space. <span>(b)</span> Samples may be drawn from an Extropic accelerator by repeatedly observing the system, waiting at least the equilibration time teq between observations. teq is how long it takes for the noise in the system to destroy all correlations with the previous sample.</p></div><div><p>There is a direct connection between this mechanical picture and the parameterized stochastic analog circuits that make up Extropic accelerators. The lightweight particles represent electrons, and the liquid molecules represent the atoms of the conducting medium, which can transfer energy to the electrons upon collision. The springs represent circuit components that confine the motion of the electrons, such as inductors or transistors. Control voltages/currents can be applied to tune the values of these components, changing the distribution from which the circuit samples.</p><p>Although every circuit is noisy, not every circuit is useful as an Extropic accelerator. Making a noise-dominated yet well-behaved device is challenging from an engineering perspective. Thermal fluctuations are small, so devices must be physically small and low power to be strongly affected by them. For this reason, if one wanted to make an Extropic accelerator out of macroscopic components (on a PCB, for example), one would have to introduce synthetic noise. Doing so erodes the fundamental time and energy savings and ends up performing similarly to running the algorithm digitally.					</p><p>Extropic’s first processors are nano fabricated from aluminum and run at low temperatures where they are superconducting. Fig. 3 shows an early device that tested several possible superconducting neuron designs. Some of these neurons are similar to existing super conducting flux qubits <a href="#references">[9]</a>. These neurons exploit the Josephson effect as a source of nonlinearity, which occurs when two superconductors are near one another. This nonlinearity is required for the device to access non-Gaussian probability distributions, which are necessary to model real-world applications with fat tails. Additionally, digital Gaussian sampling routines are ubiquitous and highly optimized. So, if an analog device is to provide a considerable speedup compared to a traditional processor, non-Gaussianity is required.</p></div><div id="figure-3"><p><span>Figure 3:&nbsp;Microscope image of an Extropic chip.</span> The inset shows two Josephson junctions, which are the devices that provide the processor with its critical nonlinearity.												</p></div><div><p>These neurons provide the basic building blocks that are combined to form a larger superconducting system. In such a larger system, many linear and non-linear neurons are combined together to create a circuit that samples from a rich and high-dimensional distribution. The neuron biases and interaction strengths are all tunable parameters of the distribution, allowing a single device to embody a wide family of probability distributions.</p><p>Extropic’s superconducting chips are entirely passive, meaning we only expend energy when measuring or manipulating its state. This likely makes these neurons the most energy-efficient in the universe. These systems will be highly energy efficient at scale: Extropic targets low-volume, high-value customers like governments, banks, and private clouds with these systems.</p><p>Extropic is also building semiconductor devices that operate at room temperature to extend our reach to a larger market. These devices trade the Josephson junction for the transistor. Doing so sacrifices some energy efficiency compared to superconducting devices. In exchange, it allows one to build them using standard manufacturing processes and supply chains, unlocking massive scale. Since they operate at room temperature, it will be possible to pack them into a GPU-like expansion card form factor. This will allow us to put an Extropic accelerator in every home, enabling everyone to partake in the thermodynamic AI acceleration.</p><p>To support a wide range of hardware substrates, Extropic is also building a software layer that compiles from abstract specifications of EBMs to the relevant hardware control language. This compilation layer is built upon the theoretical framework of factor graphs <a href="#references">[8]</a>. Factor graphs specify how large distributions factorize into local chunks. This allows Extropic accelerators to breakdown and run programs that are too big to fit on any given analog core.				</p><p>Many previous AI accelerator companies have struggled to find an advantage because of the memory-boundedness of deep learning; today’s algorithms spend around 25% of their time moving numbers around in memory. As a result of this, via Amdahl’s law, any chip that accelerates a particular operation (such as matrix multiplication) will struggle to achieve more than a 4x speedup. As Extropic chips natively accelerate a broad class of probabilistic algorithms by running them physically as a rapid and energy-efficient process in their entirety, we are bound to unlock a whole new regime of artificial intelligence acceleration well beyond what was previously thought achievable.</p></div><a href="https://www.extropic.ai/careers" target="_blank"></a><div id="references"><h3>References</h3><a href="https://ieeexplore.ieee.org/document/4785860" target="_blank"></a><a href="https://ieeexplore.ieee.org/document/7878935" target="_blank"></a><a href="https://link.springer.com/article/10.1007/BF01250732" target="_blank"></a><a href="https://ieeexplore.ieee.org/document/752515" target="_blank"></a><a href="https://ieeexplore.ieee.org/document/1182065" target="_blank"></a><a href="https://www.annualreviews.org/doi/10.1146/annurev.physchem.58.032806.104637" target="_blank"></a><a href="https://www.pnas.org/doi/10.1073/pnas.94.3.814" target="_blank"></a><a href="https://mitpress.mit.edu/9780262013192/probabilistic-graphical-models/" target="_blank"></a><a href="https://escholarship.org/uc/item/9844c3h3" target="_blank"></a></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GrapheneOS finds Bluetooth memory corruption via ARM MTE (275 pts)]]></title>
            <link>https://grapheneos.social/@GrapheneOS/112066872276203917</link>
            <guid>39668053</guid>
            <pubDate>Mon, 11 Mar 2024 13:36:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://grapheneos.social/@GrapheneOS/112066872276203917">https://grapheneos.social/@GrapheneOS/112066872276203917</a>, See on <a href="https://news.ycombinator.com/item?id=39668053">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Async tasks in 350 lines of C (108 pts)]]></title>
            <link>https://github.com/rkaehn/cr_task.h</link>
            <guid>39667489</guid>
            <pubDate>Mon, 11 Mar 2024 12:44:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/rkaehn/cr_task.h">https://github.com/rkaehn/cr_task.h</a>, See on <a href="https://news.ycombinator.com/item?id=39667489">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">cr_task.h</h2><a id="user-content-cr_taskh" aria-label="Permalink: cr_task.h" href="#cr_taskh"></a></p>
<p dir="auto">This library provides a minimal set of types and functions for an asynchronous task system in C.
It was designed to be lock-free in the common case, with locks only needed for allocations of backing memory for the task pool and when the worker threads are starved for tasks to execute. It is written in standard C11 with no dependencies besides the C POSIX library.</p>
<p dir="auto">To get started, create an executor with the desired number of worker threads and define a task.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cr_executor_t* exec = cr_executor_create(4);
cr_task_t* task = cr_task_create(exec, func, args);
cr_task_run(task);"><pre><span>cr_executor_t</span><span>*</span> <span>exec</span> <span>=</span> <span>cr_executor_create</span>(<span>4</span>);
<span>cr_task_t</span><span>*</span> <span>task</span> <span>=</span> <span>cr_task_create</span>(<span>exec</span>, <span>func</span>, <span>args</span>);
<span>cr_task_run</span>(<span>task</span>);</pre></div>
<p dir="auto">By default, the task is going to run immediately on <code>cr_task_run</code>. If you want it to run later, call <code>cr_task_wait</code> before <code>run</code>, and <code>cr_task_signal</code> later.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cr_task_t* task = cr_task_create(exec, func, args);
cr_task_wait(task);
cr_task_run(task);
// Later... (possibly on a different thread or inside a task)
cr_task_signal(task);"><pre><span>cr_task_t</span><span>*</span> <span>task</span> <span>=</span> <span>cr_task_create</span>(<span>exec</span>, <span>func</span>, <span>args</span>);
<span>cr_task_wait</span>(<span>task</span>);
<span>cr_task_run</span>(<span>task</span>);
<span>// Later... (possibly on a different thread or inside a task)</span>
<span>cr_task_signal</span>(<span>task</span>);</pre></div>
<p dir="auto">Instead of calling <code>signal</code> yourself, you can also request a signal from another task, creating a dependency. Here, <code>task_1</code> has to finish before <code>task_2</code> can start.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cr_task_t* task_1 = cr_task_create(exec, func, args);
cr_task_t* task_2 = cr_task_create(exec, func, args);
cr_task_wait(task_2);
cr_task_request_signal(task_2, task_1);
cr_task_run(task_2);
// task_2 does not execute yet
cr_task_run(task_1);
// task_1 executes, then task_2"><pre><span>cr_task_t</span><span>*</span> <span>task_1</span> <span>=</span> <span>cr_task_create</span>(<span>exec</span>, <span>func</span>, <span>args</span>);
<span>cr_task_t</span><span>*</span> <span>task_2</span> <span>=</span> <span>cr_task_create</span>(<span>exec</span>, <span>func</span>, <span>args</span>);
<span>cr_task_wait</span>(<span>task_2</span>);
<span>cr_task_request_signal</span>(<span>task_2</span>, <span>task_1</span>);
<span>cr_task_run</span>(<span>task_2</span>);
<span>// task_2 does not execute yet</span>
<span>cr_task_run</span>(<span>task_1</span>);
<span>// task_1 executes, then task_2</span></pre></div>
<p dir="auto">Since telling one task to wait and requesting a signal from another is such a common operation, there is a shorthand function called <code>cr_task_wait_request_signal</code>. <code>wait</code> and <code>signal</code> calls must always be balanced, and as soon as the wait count hits zero and <code>run</code> has been called, the task executes.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cr_task_t* task = cr_task_create(exec, func, args);
cr_task_wait_request_signal(task, task_dep_1);
cr_task_wait_request_signal(task, task_dep_2);
cr_task_run(task);
// task_dep_1 and task_dep_2 execute, then task"><pre><span>cr_task_t</span><span>*</span> <span>task</span> <span>=</span> <span>cr_task_create</span>(<span>exec</span>, <span>func</span>, <span>args</span>);
<span>cr_task_wait_request_signal</span>(<span>task</span>, <span>task_dep_1</span>);
<span>cr_task_wait_request_signal</span>(<span>task</span>, <span>task_dep_2</span>);
<span>cr_task_run</span>(<span>task</span>);
<span>// task_dep_1 and task_dep_2 execute, then task</span></pre></div>
<p dir="auto">When you want to synchronously wait for a task to finish execution, call <code>cr_task_sync</code>. Since a task destroys itself automatically after it has run, you need to call <code>cr_task_retain</code> before, and <code>cr_task_release</code> after waiting. And again, because this pattern of <code>retain</code>, <code>run</code>, <code>sync</code>, and <code>release</code> is relatively common, there is a shorthand called <code>cr_task_run_sync</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cr_task_t* task = cr_task_create(exec, func, args);
cr_task_retain(task);
cr_task_run(task);
cr_task_sync(task);
cr_task_release(task);
// Or the shorthand...
cr_task_run_sync(task);"><pre><span>cr_task_t</span><span>*</span> <span>task</span> <span>=</span> <span>cr_task_create</span>(<span>exec</span>, <span>func</span>, <span>args</span>);
<span>cr_task_retain</span>(<span>task</span>);
<span>cr_task_run</span>(<span>task</span>);
<span>cr_task_sync</span>(<span>task</span>);
<span>cr_task_release</span>(<span>task</span>);
<span>// Or the shorthand...</span>
<span>cr_task_run_sync</span>(<span>task</span>);</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building an Open Source Decentralized E-Book Search Engine (232 pts)]]></title>
            <link>https://github.com/j2qk3b/ebook-demo/blob/main/tutorial.md</link>
            <guid>39666993</guid>
            <pubDate>Mon, 11 Mar 2024 11:56:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/j2qk3b/ebook-demo/blob/main/tutorial.md">https://github.com/j2qk3b/ebook-demo/blob/main/tutorial.md</a>, See on <a href="https://news.ycombinator.com/item?id=39666993">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:j2qk3b/ebook-demo" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="bAZCvTYOEC1SyENw7fiHWar5i4g_5BOabdtPpvB5OUqiOqjlbK5DoCXtmlW_bypQzX_VzwXz_QG6tP0tZpvuDA" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="j2qk3b/ebook-demo" data-current-org="" data-current-owner="j2qk3b" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=9V8qOlU0uL60V2hmJS9JtVN10NVQ2T0zkBzR2JB5SMOIie3pL6Sv6gcvviEJQN%2F69edBzexpUhHPPg%2BjA0aup4mGDvFPZ8P%2BN6CWxCwokOLVWuoU1bOEt4MMeqaI1havQKCgcNWFDpOR8offgQDmbGNr4LUd2iBoLrQhct2EWqemw649Y28VPiGA%2FPcoKwa%2F0%2FvEEpr4jLuHcHaV8DCOZOFOeDOKQOU0lM8SfEuXTKPiETG41IHhPjSibaOrz5XCezH3JVMesEe7RmKttqeIGmT5JnOT3feHxUPw7RQ7bMy%2FCF%2F32ihcy28nN0HUN1RfyjeTlFSypHbOEjX%2Bvv0wIX8gYMj3M5cN5Xq9jTZOmFXsqWljG%2Bp50bKBi6rGb98iFwNCfVSi2L5uM4WUvdlpqA88i%2Bo7%2FUCg4%2FG4cPtW69wXw1wW%2Bf5n9mV7mmbzcZmzD1DOXp3UIx%2F2TT7I8dOgv21e8csCdTypJE7gM1%2BFFHvQ%2BL84w8Ke9Vw1WUlvHjvttK27t0BN%2BG4j9ltsyRGdnHf9ESr8lE0q5Br5xihHc3JJv0CidZtvPSCP--YFyq8JjzuL938XB4--2LSoNQ17ZGyaQ63zQBm0%2BQ%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=j2qk3b%2Febook-demo" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/j2qk3b/ebook-demo/blob/main/tutorial.md&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="8058d1ec797eb6e8a4b31d576edc0b0925b770eb14488aaefac3aabb12c6dca9" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Automakers Are Sharing Consumers' Driving Behavior with Insurance Companies (129 pts)]]></title>
            <link>https://www.nytimes.com/2024/03/11/technology/carmakers-driver-tracking-insurance.html</link>
            <guid>39666976</guid>
            <pubDate>Mon, 11 Mar 2024 11:55:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/03/11/technology/carmakers-driver-tracking-insurance.html">https://www.nytimes.com/2024/03/11/technology/carmakers-driver-tracking-insurance.html</a>, See on <a href="https://news.ycombinator.com/item?id=39666976">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/03/11/technology/carmakers-driver-tracking-insurance.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Flowers for Algernon (1965) [pdf] (339 pts)]]></title>
            <link>https://www.sdfo.org/gj/stories/flowersforalgernon.pdf</link>
            <guid>39666956</guid>
            <pubDate>Mon, 11 Mar 2024 11:53:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sdfo.org/gj/stories/flowersforalgernon.pdf">https://www.sdfo.org/gj/stories/flowersforalgernon.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=39666956">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Teable – Open-Source No-Code Database Fusion of Postgres and Airtable (188 pts)]]></title>
            <link>https://github.com/teableio/teable</link>
            <guid>39666865</guid>
            <pubDate>Mon, 11 Mar 2024 11:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/teableio/teable">https://github.com/teableio/teable</a>, See on <a href="https://news.ycombinator.com/item?id=39666865">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <div dir="auto"><h2 tabindex="-1" dir="auto">
    <themed-picture data-catalyst-inline="true"><picture>
      <source media="(prefers-color-scheme: dark)" srcset="https://github.com/teableio/teable/raw/develop/static/assets/images/teable-vertical-dark.png">
      <img alt="teable logo" height="150" src="https://github.com/teableio/teable/raw/develop/static/assets/images/teable-vertical-light.png">
    </picture></themed-picture>
  </h2><a id="user-content-----------------------" aria-label="Permalink: " href="#----------------------"></a></div>
  <p dir="auto"><h3 tabindex="-1" dir="auto"><strong>Postgres-Airtable Fusion</strong></h3><a id="user-content-postgres-airtable-fusion" aria-label="Permalink: Postgres-Airtable Fusion" href="#postgres-airtable-fusion"></a></p>
  <p dir="auto">Teable is a Super fast, Real-time, Professional, Developer friendly, No-code database built on Postgres. It uses a simple, spreadsheet-like interface to create complex enterprise-level database applications. Unlock efficient app development with no-code, free from the hurdles of data security and scalability. </p>
</div>
<p dir="auto">
  <a href="https://teable.io/" rel="nofollow">Home</a> | <a href="https://help.teable.io/" rel="nofollow">Help</a> | <a href="https://blog.teable.io/" rel="nofollow">Blog</a> | <a href="https://template.teable.io/" rel="nofollow">Template</a> | <a href="https://app.teable.io/share/shr04TEw1u9EOQojPmG/view" rel="nofollow">Roadmap</a> | <a href="https://discord.gg/n2JQqekG" rel="nofollow">Discord </a>
</p>
<p dir="auto">
  <a aria-label="Build" href="https://github.com/teableio/teable/actions?query=Build%20and%20Push%20to%20Docker%20Registry">
    <img alt="build" src="https://camo.githubusercontent.com/51feb6245c6188b4c6eb58922c111c863acbf0e6ad044115abc545e21069aa9d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f746561626c65696f2f746561626c652f646f636b65722d707573682e796d6c3f6c6162656c3d4275696c64266c6f676f3d676974687562267374796c653d666c61742d7175617265266c6162656c436f6c6f723d303030303030" data-canonical-src="https://img.shields.io/github/actions/workflow/status/teableio/teable/docker-push.yml?label=Build&amp;logo=github&amp;style=flat-quare&amp;labelColor=000000">
  </a>
  <a aria-label="Codefactor grade" href="https://www.codefactor.io/repository/github/teableio/teable" rel="nofollow">
    <img alt="Codefactor" src="https://camo.githubusercontent.com/56f625cb617e43b7d96feb176797b27ab934ef6c1533383e2aa16de314798b2d/68747470733a2f2f696d672e736869656c64732e696f2f636f6465666163746f722f67726164652f6769746875622f746561626c65696f2f746561626c653f6c6162656c3d436f6465666163746f72266c6f676f3d636f6465666163746f72267374796c653d666c61742d7175617265266c6162656c436f6c6f723d303030303030" data-canonical-src="https://img.shields.io/codefactor/grade/github/teableio/teable?label=Codefactor&amp;logo=codefactor&amp;style=flat-quare&amp;labelColor=000000">
  </a>
  <a aria-label="CodeClimate maintainability" href="https://codeclimate.com/github/teableio/teable" rel="nofollow">
    <img alt="Maintainability" src="https://camo.githubusercontent.com/02f58b978a0195807709e61e8fb3432a025c9d0c7c5dc776228a02ba1cc06afc/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636c696d6174652f6d61696e7461696e6162696c6974792f746561626c65696f2f746561626c653f6c6162656c3d4d61696e7461696e6162696c697479266c6f676f3d636f64652d636c696d617465267374796c653d666c61742d7175617265266c6162656c436f6c6f723d303030303030" data-canonical-src="https://img.shields.io/codeclimate/maintainability/teableio/teable?label=Maintainability&amp;logo=code-climate&amp;style=flat-quare&amp;labelColor=000000">
  </a>
  <a aria-label="CodeClimate technical debt" href="https://codeclimate.com/github/teableio/teable" rel="nofollow">
    <img alt="Techdebt" src="https://camo.githubusercontent.com/60407bf2806fbd36e678f1e3998e6ffabefe1fbae5a8866767881e0775f9745a/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636c696d6174652f746563682d646562742f746561626c65696f2f746561626c653f6c6162656c3d5465636844656274266c6f676f3d636f64652d636c696d617465267374796c653d666c61742d7175617265266c6162656c436f6c6f723d303030303030" data-canonical-src="https://img.shields.io/codeclimate/tech-debt/teableio/teable?label=TechDebt&amp;logo=code-climate&amp;style=flat-quare&amp;labelColor=000000">
  </a>
  <a aria-label="Codacy grade" href="https://www.codacy.com/gh/teableio/teable/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=teableio/teable&amp;utm_campaign=Badge_Grade" rel="nofollow">
    <img alt="Codacy grade" src="https://camo.githubusercontent.com/feac5e519bc775dbd9168ea74b043bb34c5f78973cafceb4f1c6535b89dd78c9/68747470733a2f2f696d672e736869656c64732e696f2f636f646163792f67726164652f64666639633934346166323834613066616434653136356562313732373436373f6c6f676f3d636f64616379267374796c653d666c61742d737175617265266c6162656c436f6c6f723d303030266c6162656c3d436f64616379" data-canonical-src="https://img.shields.io/codacy/grade/dff9c944af284a0fad4e165eb1727467?logo=codacy&amp;style=flat-square&amp;labelColor=000&amp;label=Codacy">
  </a>
  <a aria-label="Top language" href="https://github.com/teableio/teable/search?l=typescript">
    <img alt="GitHub top language" src="https://camo.githubusercontent.com/d5764ff62c111b52311ce4914e6b48f4cf79c54cae344edfe10f57f31854e8c5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c616e6775616765732f746f702f746561626c65696f2f746561626c653f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d30303026636f6c6f723d626c7565" data-canonical-src="https://img.shields.io/github/languages/top/teableio/teable?style=flat-square&amp;labelColor=000&amp;color=blue">
  </a>
  <a aria-label="Licence" href="https://github.com/teableio/teable/blob/main/LICENSE">
    <img alt="Licence" src="https://camo.githubusercontent.com/29629cb104e1314fe2c260743595d3a21f7582e7c83e06d80ea0f9c7a2ceae0b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f746561626c65696f2f746561626c653f7374796c653d666c61742d7175617265266c6162656c436f6c6f723d303030303030" data-canonical-src="https://img.shields.io/github/license/teableio/teable?style=flat-quare&amp;labelColor=000000">
  </a>
</p>
  <div dir="auto"><h2 tabindex="-1" dir="auto">
    <themed-picture data-catalyst-inline="true"><picture>
      <source media="(prefers-color-scheme: dark)" srcset="https://github.com/teableio/teable/raw/develop/static/assets/images/teable-interface-dark.png">
      <img alt="teable interface" width="100%" src="https://github.com/teableio/teable/raw/develop/static/assets/images/teable-interface-light.png">
    </picture></themed-picture>
  </h2><a id="user-content------------------------1" aria-label="Permalink: " href="#-----------------------1"></a></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Guide</h2><a id="user-content-quick-guide" aria-label="Permalink: Quick Guide" href="#quick-guide"></a></p>
<ol dir="auto">
<li>Looking for a quick experience? Select a scenario from the <a href="https://template.teable.io/" rel="nofollow">template center</a> and click "Use this template".</li>
<li>Seeking high performance? Try the <a href="https://app.teable.io/share/shrVgdLiOvNQABtW0yX/view" rel="nofollow">1 million rows demo</a> to feel the speed of Teable.</li>
<li>Want to learn to use it quickly? Click on this <a href="https://help.teable.io/quick-start/build-a-simple-base" rel="nofollow">tutorial</a></li>
<li>Interested in deploying it yourself? Click <a href="https://railway.app/template/wada5e?referralCode=rE4BjB" rel="nofollow">Deploy on Railway</a></li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">✨Features</h2><a id="user-content-features" aria-label="Permalink: ✨Features" href="#features"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">📊 Spreadsheet-like interface</h4><a id="user-content--spreadsheet-like-interface" aria-label="Permalink: 📊 Spreadsheet-like interface" href="#-spreadsheet-like-interface"></a></p>
<p dir="auto">All you want is here</p>
<ul dir="auto">
<li>Cell Editing: Directly click and edit content within cells.</li>
<li>Formula Support: Input mathematical and logical formulas to auto-calculate values.</li>
<li>Data Sorting and Filtering: Sort data based on a column or multiple columns; use filters to view specific rows of data.</li>
<li>Aggregation Function: Automatically summarize statistics for each column, providing instant calculations like sum, average, count, max, and min for streamlined data analysis.</li>
<li>Data Formatting: formatting numbers, dates, etc.</li>
<li>Grouping: Organize rows into collapsible groups based on column values for easier data analysis and navigation.</li>
<li>Freeze Columns: Freeze the left column of the table so they remain visible while scrolling.</li>
<li>Import/Export Capabilities: Import and export data from other formats, e.g., .csv, .xlsx.</li>
<li>Row Styling &amp; Conditional Formatting: Change row styles automatically based on specific conditions. (coming soon)</li>
<li>Charts &amp; Visualization Tools: Create charts from table data such as bar charts, pie charts, line graphs, etc. (coming soon)</li>
<li>Data Validation: Limit or validate data that are entered into cells. (coming soon)</li>
<li>Undo/Redo: Undo or redo recent changes. (coming soon)</li>
<li>Comments &amp; Annotations: Attach comments to rows, providing explanations or feedback for other users. (coming soon)</li>
<li>Find &amp; Replace: Search content within the table and replace it with new content. (coming soon)</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">🗂️ Multiple Views</h4><a id="user-content-️-multiple-views" aria-label="Permalink: 🗂️ Multiple Views" href="#️-multiple-views"></a></p>
<p dir="auto">Visualize and interact with data in various ways best suited for their specific tasks.</p>
<ul dir="auto">
<li>Grid View: The default view of the table, which displays data in a spreadsheet-like format.</li>
<li>Form View: Input data in a form format, which is useful for collecting data.</li>
<li>Kanban View: Displays data in a Kanban board, which is a visual representation of data in columns and cards. (coming soon)</li>
<li>Calendar View: Displays data in a calendar format, which is useful for tracking dates and events. (coming soon)</li>
<li>Gallery View: Displays data in a gallery format, which is useful for displaying images and other media. (coming soon)</li>
<li>Gantt View: Displays data in a Gantt chart, which is useful for tracking project schedules. (coming soon)</li>
<li>Timeline View: Displays data in a timeline format, which is useful for tracking events over time. (coming soon)</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">🚀 Super Fast</h4><a id="user-content--super-fast" aria-label="Permalink: 🚀 Super Fast" href="#-super-fast"></a></p>
<p dir="auto">Amazing response speed and data capacity</p>
<ul dir="auto">
<li>Millions of data are easily processed, and there is no pressure to filter and sort</li>
<li>Automatic database indexing for maximum speed</li>
<li>Supports batch data operations at one time</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">👨‍💻 Full-featured SQL Support</h4><a id="user-content--full-featured-sql-support" aria-label="Permalink: 👨‍💻 Full-featured SQL Support" href="#-full-featured-sql-support"></a></p>
<p dir="auto">Seamless integration with the software you are familiar with</p>
<ul dir="auto">
<li>BI tools like Metabase PowerBi...</li>
<li>No-code tools like Appsmith...</li>
<li>Direct retrieve data with native SQL</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">🔒 Privacy-First</h4><a id="user-content--privacy-first" aria-label="Permalink: 🔒 Privacy-First" href="#-privacy-first"></a></p>
<p dir="auto">You own your data, in spite of the cloud</p>
<ul dir="auto">
<li>Bring your own database (coming soon)</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">⚡️ Real-time collaboration</h4><a id="user-content-️-real-time-collaboration" aria-label="Permalink: ⚡️ Real-time collaboration" href="#️-real-time-collaboration"></a></p>
<p dir="auto">Designed for teams</p>
<ul dir="auto">
<li>No need to refresh the page, data is updated in real-time</li>
<li>Seamlessly integrate collaboration member invitation and management</li>
<li>Perfect permission management mechanism, from table to column level</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">🧩 Extensions (coming soon)</h4><a id="user-content--extensions-coming-soon" aria-label="Permalink: 🧩 Extensions (coming soon)" href="#-extensions-coming-soon"></a></p>
<p dir="auto">Expand infinite possibilities</p>
<ul dir="auto">
<li>Backend-less programming capability based on React</li>
<li>Customize your own application with extremely low cost</li>
<li>Extremely easy-to-use script extensions mode</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">🤖 Automation (coming soon)</h4><a id="user-content--automation-coming-soon" aria-label="Permalink: 🤖 Automation (coming soon)" href="#-automation-coming-soon"></a></p>
<p dir="auto">Empower data-driven workflows effortlessly and seamlessly</p>
<ul dir="auto">
<li>Design your workflow with AI or Visual programming</li>
<li>Super easy to retrieve data from the table</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">🧠 Copilot (coming soon)</h4><a id="user-content--copilot-coming-soon" aria-label="Permalink: 🧠 Copilot (coming soon)" href="#-copilot-coming-soon"></a></p>
<p dir="auto">Native Integrated AI ability</p>
<ul dir="auto">
<li>Chat 2 App. "Create a project management app for me"</li>
<li>Chat 2 Chart. "Analyze the data in the order table using a bar chart"</li>
<li>Chat 2 View. "I want to see the schedule for the past week and only display participants"</li>
<li>Chat 2 Action. "After the order is paid and completed, an email notification will be sent to the customer"</li>
<li>More actions...</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">🗄️ Support for multiple databases (coming soon)</h4><a id="user-content-️-support-for-multiple-databases-coming-soon" aria-label="Permalink: 🗄️ Support for multiple databases (coming soon)" href="#️-support-for-multiple-databases-coming-soon"></a></p>
<p dir="auto">Choose the SQL database you like</p>
<ul dir="auto">
<li>Sqlite, PostgreSQL, MySQL, MariaDB, TiDB...</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Structure</h2><a id="user-content-structure" aria-label="Permalink: Structure" href="#structure"></a></p>
<p dir="auto"><a href="https://gitpod.io/#https://github.com/teableio/teable" rel="nofollow"><img src="https://camo.githubusercontent.com/1470bc5ed68e2671a712665be23b2c0612229ac893c1eee461399476f43bc28e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4f70656e253230496e2d476974706f642e696f2d2532333139363644323f7374796c653d666f722d7468652d6261646765266c6f676f3d676974706f64" alt="Open in Gitpod" data-canonical-src="https://img.shields.io/badge/Open%20In-Gitpod.io-%231966D2?style=for-the-badge&amp;logo=gitpod"></a></p>
<div data-snippet-clipboard-copy-content=".
├── apps
│   ├── electron            (desktop, include a electron app )
│   ├── nextjs-app          (front-end, include a nextjs app)
│   └── nestjs-backend      (backend, running on server or inside electron app)
└── packages
    ├── common-i18n         (locales)
    ├── core                (share code and interface)
    ├── sdk                 (sdk for extensions)
    ├── db-main-prisma      (schema, migrations, prisma client)
    ├── eslint-config-bases (to shared eslint configs)
    └── ui-lib              (ui component)"><pre><code>.
├── apps
│   ├── electron            (desktop, include a electron app )
│   ├── nextjs-app          (front-end, include a nextjs app)
│   └── nestjs-backend      (backend, running on server or inside electron app)
└── packages
    ├── common-i18n         (locales)
    ├── core                (share code and interface)
    ├── sdk                 (sdk for extensions)
    ├── db-main-prisma      (schema, migrations, prisma client)
    ├── eslint-config-bases (to shared eslint configs)
    └── ui-lib              (ui component)
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Deploy</h2><a id="user-content-deploy" aria-label="Permalink: Deploy" href="#deploy"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deploy with docker</h3><a id="user-content-deploy-with-docker" aria-label="Permalink: Deploy with docker" href="#deploy-with-docker"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="cd dockers/examples/standalone/
docker-compose up -d"><pre><span>cd</span> dockers/examples/standalone/
docker-compose up -d</pre></div>
<p dir="auto">for more details, see <a href="https://github.com/teableio/teable/blob/develop/dockers/examples">dockers/examples</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deploy with Railway</h3><a id="user-content-deploy-with-railway" aria-label="Permalink: Deploy with Railway" href="#deploy-with-railway"></a></p>
<p dir="auto"><a href="https://railway.app/template/wada5e?referralCode=rE4BjB" rel="nofollow"><img src="https://camo.githubusercontent.com/d07713342bc583232f8752c33a6a24e5f367d73725183a63f2f5fdd7c00606a3/68747470733a2f2f7261696c7761792e6170702f627574746f6e2e737667" alt="Deploy on Railway" data-canonical-src="https://railway.app/button.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">1. Initialize</h4><a id="user-content-1-initialize" aria-label="Permalink: 1. Initialize" href="#1-initialize"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Use `.nvmrc` file to specify node version（Requires pre `nvm` tools）
nvm install &amp;&amp; nvm use

# Enabling the Help Management Package Manager
corepack enable

# Install project dependencies
pnpm install

# Build packages
pnpm g:build"><pre><span><span>#</span> Use `.nvmrc` file to specify node version（Requires pre `nvm` tools）</span>
nvm install <span>&amp;&amp;</span> nvm use

<span><span>#</span> Enabling the Help Management Package Manager</span>
corepack <span>enable</span>

<span><span>#</span> Install project dependencies</span>
pnpm install

<span><span>#</span> Build packages</span>
pnpm g:build</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">2. Select Database</h4><a id="user-content-2-select-database" aria-label="Permalink: 2. Select Database" href="#2-select-database"></a></p>
<p dir="auto">we currently support <code>sqlite</code> and <code>postgres</code>, you can switch between them by running the following command</p>

<p dir="auto"><h4 tabindex="-1" dir="auto">3. Custom environment variables（optional）</h4><a id="user-content-3-custom-environment-variablesoptional" aria-label="Permalink: 3. Custom environment variables（optional）" href="#3-custom-environment-variablesoptional"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="cd apps/nextjs-app
copy .env.development .env.development.local"><pre><span>cd</span> apps/nextjs-app
copy .env.development .env.development.local</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">4. Run dev server</h4><a id="user-content-4-run-dev-server" aria-label="Permalink: 4. Run dev server" href="#4-run-dev-server"></a></p>
<p dir="auto">you just need to start backend, it will start next server for frontend automatically, file change will be auto reload</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd apps/nestjs-backend
pnpm dev"><pre><span>cd</span> apps/nestjs-backend
pnpm dev</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why Teable?</h2><a id="user-content-why-teable" aria-label="Permalink: Why Teable?" href="#why-teable"></a></p>
<p dir="auto">No-code tools have significantly speed up how we get things done, allowing non-tech users to build amazing apps and changing the way many work and live. People like using spreadsheet-like UI to handle their data because it's easy, flexible, and great for team collaboration. They also prefer designing their app screens without being stuck with clunky templates.</p>
<p dir="auto">Giving non-techy people the ability to create their software sounds exciting. But that's just the start:</p>
<ul dir="auto">
<li>As businesses expand, their data needs intensify. No one wishes to hear that once their orders reach 100k, they'll outgrow their current interface. Yet, many no-code platforms falter at such scales.</li>
<li>Most no-code platforms are cloud-based. This means your important data sits with the provider, and switching to another platform can be a headache.</li>
<li>Sometimes, no-code tools can't do what you want because of their limitations, leaving users stuck.</li>
<li>If a tool becomes essential, you'll eventually need some tech expertise. But developers often find these platforms tricky.</li>
<li>Maintaining systems with complex setups can be hard for developers, especially if these aren't built using common software standards.</li>
<li>Systems that don't use these standards might need revamping or replacing, costing more in the long run. It might even mean ditching the no-code route and going back to traditional coding.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">What we think the future of no-code products look like</h4><a id="user-content-what-we-think-the-future-of-no-code-products-look-like" aria-label="Permalink: What we think the future of no-code products look like" href="#what-we-think-the-future-of-no-code-products-look-like"></a></p>
<ul dir="auto">
<li>An interface that anyone can use to build applications easily.</li>
<li>Easy access to data, letting users grab, move, and reuse their information as they wish.</li>
<li>Data privacy and choice, whether that's in the cloud, on-premise, or even just on your local.</li>
<li>It needs to work for developers too, not just non-tech users.</li>
<li>It should handle lots of data, so it can grow with your business.</li>
<li>Flexibility to integrate with other software, combining strengths to get the job done.</li>
<li>Last, native AI integration to takes usability to the next level.</li>
</ul>
<p dir="auto">In essence, Teable isn't just another no-code solution, it's a comprehensive answer to the evolving demands of modern software development, ensuring that everyone, regardless of their technical proficiency, has a platform tailored to their needs.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Sponsors ❤️</h2><a id="user-content-sponsors-heart" aria-label="Permalink: Sponsors :heart:" href="#sponsors-heart"></a></p>
<p dir="auto">If you are enjoying some this project in your company, I'd really appreciate a <a href="https://github.com/sponsors/teableio">sponsorship</a>, a <a href="https://ko-fi.com/teable" rel="nofollow">coffee</a> or a dropped star.
That gives me some more time to improve it to the next level.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">AGPL-3.0</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[European Commission's use of Microsoft 365 infringes data protection law for EU (117 pts)]]></title>
            <link>https://www.edps.europa.eu/press-publications/press-news/press-releases/2024/european-commissions-use-microsoft-365-infringes-data-protection-law-eu-institutions-and-bodies_en</link>
            <guid>39666582</guid>
            <pubDate>Mon, 11 Mar 2024 11:02:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.edps.europa.eu/press-publications/press-news/press-releases/2024/european-commissions-use-microsoft-365-infringes-data-protection-law-eu-institutions-and-bodies_en">https://www.edps.europa.eu/press-publications/press-news/press-releases/2024/european-commissions-use-microsoft-365-infringes-data-protection-law-eu-institutions-and-bodies_en</a>, See on <a href="https://news.ycombinator.com/item?id=39666582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-wrapper" data-off-canvas-main-canvas="">
              <main id="content" role="main">
                <section>
                  <a id="main-content" tabindex="-1"></a>
                    
<div id="block-edpsweb-theme-main-page-content">
      

<article>
  
  <div>
    <header>
      
      
      <h3>
        <a href="https://www.edps.europa.eu/press-publications/press-news/press-releases/2024/european-commissions-use-microsoft-365-infringes-data-protection-law-eu-institutions-and-bodies" rel="bookmark"><span>European Commission’s use of Microsoft 365 infringes data protection law for EU institutions and bodies</span>
</a>
      </h3>
      
    </header>
    <div><p><b>Following its investigation, the EDPS has found that the European Commission (Commission) has infringed several key data protection rules when using Microsoft 365. In its decision, the EDPS imposes corrective measures on the Commission</b>.</p>
<p>The EDPS has found that the Commission <b>has infringed several provisions of Regulation (EU) 2018/1725</b>, the EU’s data protection law for EU institutions, bodies, offices and agencies (EUIs), including those <b>on transfers of personal data outside the EU/European Economic Area (EEA)</b>. In particular, the Commission has failed to provide appropriate safeguards to ensure that personal data transferred outside the EU/EEA are afforded an essentially equivalent level of protection as guaranteed in the EU/EEA. Furthermore, in its contract with Microsoft, <b>the Commission did not sufficiently specify what types of personal data are to be collected and for which explicit and specified purposes when using Microsoft 365</b>. The <b>Commission’s infringements as data controller also relate to data processing</b>, including transfers of personal data, carried out <b>on its behalf</b>.</p>
<p><b>Wojciech Wiewiórowski, EDPS, said: </b><i>“It is the responsibility of the EU institutions, bodies, offices and agencies (EUIs) to ensure that any processing of personal data outside and inside the EU/EEA, including in the context of cloud-based services, is accompanied by robust data protection safeguards and measures. This is imperative to ensure that individuals’ information is protected, as required by</i><i> Regulation (EU) 2018/1725, whenever their data is processed by, or on behalf of, an EUI.” </i></p>
<p>The EDPS has therefore decided to order the Commission, effective on <b>9 December 2024</b>, to <b>suspend all data flows </b>resulting from its use of Microsoft 365<b> to Microsoft </b>and to its affiliates and sub-processors<b> located in countries outside the EU/EEA not covered by an adequacy decision</b>. The EDPS has also decided to order the Commission to <b>bring the processing operations</b> resulting from its use of Microsoft 365 <b>into compliance with Regulation (EU) 2018/1725</b>. <b>The Commission must demonstrate compliance with both orders by 9 December 2024</b>.</p>
<p>The EDPS considers that <b>the corrective measures it imposes (<a href="https://www.edps.europa.eu/system/files/2024-03/EDPS-2024-05-European-Commission_s-use-of-M365-infringes-data-protection-rules-for-EU-institutions-and-bodies_EN.pdf">see annex for a detailed excerpt</a>) are appropriate, necessary </b>and<b> proportionate in light of the seriousness and duration of the infringements found. </b></p>
<p>Many of the infringements found concern all processing operations carried out by the Commission, or on its behalf, when using Microsoft 365, and impact a large number of individuals.</p>
<p>The EDPS also takes into account the need <b>not to compromise the Commission’s ability to carry out its tasks in the public interest or to exercise official authority vested in the Commission, and the need to allow appropriate time for the Commission to implement the foreseen suspension of relevant data flows, and to bring the processing of data into compliance with Regulation (EU) 2018/1725.</b></p>
<p>The measures imposed by the EDPS in its decision of 8 March 2024 are without prejudice to any other or further action that the EDPS may undertake.</p>
<p><b><a href="https://www.edps.europa.eu/system/files/2024-03/EDPS-2024-05-European-Commission_s-use-of-M365-infringes-data-protection-rules-for-EU-institutions-and-bodies_EN.pdf">The findings of infringements and corrective measures imposed by the EDPS in its decision can be found in annex</a>. </b></p>
</div>
  </div>
</article>

    </div>


                </section>
              </main>
                                      
                      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[European Commission's use of Microsoft 365 infringes data protection law for EU (146 pts)]]></title>
            <link>https://www.edps.europa.eu/press-publications/press-news/press-releases/2024/european-commissions-use-microsoft-365-infringes-data-protection-law-eu-institutions-and-bodies_en</link>
            <guid>39666561</guid>
            <pubDate>Mon, 11 Mar 2024 10:57:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.edps.europa.eu/press-publications/press-news/press-releases/2024/european-commissions-use-microsoft-365-infringes-data-protection-law-eu-institutions-and-bodies_en">https://www.edps.europa.eu/press-publications/press-news/press-releases/2024/european-commissions-use-microsoft-365-infringes-data-protection-law-eu-institutions-and-bodies_en</a>, See on <a href="https://news.ycombinator.com/item?id=39666561">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-wrapper" data-off-canvas-main-canvas="">
              <main id="content" role="main">
                <section>
                  <a id="main-content" tabindex="-1"></a>
                    
<div id="block-edpsweb-theme-main-page-content">
      

<article>
  
  <div>
    <header>
      
      
      <h3>
        <a href="https://www.edps.europa.eu/press-publications/press-news/press-releases/2024/european-commissions-use-microsoft-365-infringes-data-protection-law-eu-institutions-and-bodies" rel="bookmark"><span>European Commission’s use of Microsoft 365 infringes data protection law for EU institutions and bodies</span>
</a>
      </h3>
      
    </header>
    <div><p><b>Following its investigation, the EDPS has found that the European Commission (Commission) has infringed several key data protection rules when using Microsoft 365. In its decision, the EDPS imposes corrective measures on the Commission</b>.</p>
<p>The EDPS has found that the Commission <b>has infringed several provisions of Regulation (EU) 2018/1725</b>, the EU’s data protection law for EU institutions, bodies, offices and agencies (EUIs), including those <b>on transfers of personal data outside the EU/European Economic Area (EEA)</b>. In particular, the Commission has failed to provide appropriate safeguards to ensure that personal data transferred outside the EU/EEA are afforded an essentially equivalent level of protection as guaranteed in the EU/EEA. Furthermore, in its contract with Microsoft, <b>the Commission did not sufficiently specify what types of personal data are to be collected and for which explicit and specified purposes when using Microsoft 365</b>. The <b>Commission’s infringements as data controller also relate to data processing</b>, including transfers of personal data, carried out <b>on its behalf</b>.</p>
<p><b>Wojciech Wiewiórowski, EDPS, said: </b><i>“It is the responsibility of the EU institutions, bodies, offices and agencies (EUIs) to ensure that any processing of personal data outside and inside the EU/EEA, including in the context of cloud-based services, is accompanied by robust data protection safeguards and measures. This is imperative to ensure that individuals’ information is protected, as required by</i><i> Regulation (EU) 2018/1725, whenever their data is processed by, or on behalf of, an EUI.” </i></p>
<p>The EDPS has therefore decided to order the Commission, effective on <b>9 December 2024</b>, to <b>suspend all data flows </b>resulting from its use of Microsoft 365<b> to Microsoft </b>and to its affiliates and sub-processors<b> located in countries outside the EU/EEA not covered by an adequacy decision</b>. The EDPS has also decided to order the Commission to <b>bring the processing operations</b> resulting from its use of Microsoft 365 <b>into compliance with Regulation (EU) 2018/1725</b>. <b>The Commission must demonstrate compliance with both orders by 9 December 2024</b>.</p>
<p>The EDPS considers that <b>the corrective measures it imposes (<a href="https://www.edps.europa.eu/system/files/2024-03/EDPS-2024-05-European-Commission_s-use-of-M365-infringes-data-protection-rules-for-EU-institutions-and-bodies_EN.pdf">see annex for a detailed excerpt</a>) are appropriate, necessary </b>and<b> proportionate in light of the seriousness and duration of the infringements found. </b></p>
<p>Many of the infringements found concern all processing operations carried out by the Commission, or on its behalf, when using Microsoft 365, and impact a large number of individuals.</p>
<p>The EDPS also takes into account the need <b>not to compromise the Commission’s ability to carry out its tasks in the public interest or to exercise official authority vested in the Commission, and the need to allow appropriate time for the Commission to implement the foreseen suspension of relevant data flows, and to bring the processing of data into compliance with Regulation (EU) 2018/1725.</b></p>
<p>The measures imposed by the EDPS in its decision of 8 March 2024 are without prejudice to any other or further action that the EDPS may undertake.</p>
<p><b><a href="https://www.edps.europa.eu/system/files/2024-03/EDPS-2024-05-European-Commission_s-use-of-M365-infringes-data-protection-rules-for-EU-institutions-and-bodies_EN.pdf">The findings of infringements and corrective measures imposed by the EDPS in its decision can be found in annex</a>. </b></p>
</div>
  </div>
</article>

    </div>


                </section>
              </main>
                                      
                      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A TUI Git client inspired by Magit (110 pts)]]></title>
            <link>https://github.com/altsem/gitu</link>
            <guid>39666520</guid>
            <pubDate>Mon, 11 Mar 2024 10:52:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/altsem/gitu">https://github.com/altsem/gitu</a>, See on <a href="https://news.ycombinator.com/item?id=39666520">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">It's Gitu! - A Git porcelain <em>outside</em> of Emacs</h2><a id="user-content-its-gitu---a-git-porcelain-outside-of-emacs" aria-label="Permalink: It's Gitu! - A Git porcelain outside of Emacs" href="#its-gitu---a-git-porcelain-outside-of-emacs"></a></p>
<p dir="auto"><a href="https://github.com/altsem/gitu/actions/workflows/ci.yml"><img src="https://github.com/altsem/gitu/actions/workflows/ci.yml/badge.svg" alt="CI"></a>
<a href="https://codecov.io/gh/altsem/gitu" rel="nofollow"><img src="https://camo.githubusercontent.com/aea59bf22ba4df39a48b5a78040f11f0013f136caface0ef2f3aa8352042e8a1/68747470733a2f2f636f6465636f762e696f2f67682f616c7473656d2f676974752f67726170682f62616467652e7376673f746f6b656e3d35595750553747574657" alt="codecov" data-canonical-src="https://codecov.io/gh/altsem/gitu/graph/badge.svg?token=5YWPU7GWFW"></a></p>
<p dir="auto">A terminal user interface for Git. Inspired by Magit, and launched straight from the terminal.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/altsem/gitu/blob/master/vhs/rec.gif"><img src="https://github.com/altsem/gitu/raw/master/vhs/rec.gif" data-animated-image=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Features</h3><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto">Gitu aims to implement many of the core features of Magit over time.
It should be familiar to any previous Magit users.</p>
<p dir="auto">A rough list of so-far supported features:</p>
<ul dir="auto">
<li>File/Hunk-level stage/unstage</li>
<li>Show (view commits / open EDITOR at line)</li>
<li>Show branches</li>
<li>Branch:
<ul dir="auto">
<li>checkout</li>
</ul>
</li>
<li>Commit:
<ul dir="auto">
<li>commit, amend, fixup</li>
</ul>
</li>
<li>Fetch:
<ul dir="auto">
<li>all</li>
</ul>
</li>
<li>Log:
<ul dir="auto">
<li>current</li>
</ul>
</li>
<li>Pull / Push:
<ul dir="auto">
<li>remote</li>
</ul>
</li>
<li>Rebase:
<ul dir="auto">
<li>abort, continue, autosquash, interactive</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Keybinds</h3><a id="user-content-keybinds" aria-label="Permalink: Keybinds" href="#keybinds"></a></p>
<p dir="auto">A help-menu can be shown by pressing the <code>h</code> key.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/altsem/gitu/blob/master/vhs/help.png"><img src="https://github.com/altsem/gitu/raw/master/vhs/help.png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Install</h3><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Using Cargo</h4><a id="user-content-using-cargo" aria-label="Permalink: Using Cargo" href="#using-cargo"></a></p>
<p dir="auto">Run the command (recommended):
<code>cargo install gitu --locked</code></p>
<p dir="auto">...or to install from git, run:
<code>cargo install --git https://github.com/altsem/gitu.git --locked</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">The environment variables <code>GIT_EDITOR</code>, <code>VISUAL</code> or <code>EDITOR</code> (checked in this order) dictate which editor Gitu will open.</p>
<p dir="auto">Configuration is also loaded from <code>~/.config/gitu/config.toml</code>,
you could copy the <a href="https://github.com/altsem/gitu/blob/master/src/default_config.toml">default configuration</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Humanity Is Dangerously Pushing Its Ability to Tolerate Heat (144 pts)]]></title>
            <link>https://www.wired.com/story/extreme-heat-tolerance/</link>
            <guid>39666369</guid>
            <pubDate>Mon, 11 Mar 2024 10:17:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/extreme-heat-tolerance/">https://www.wired.com/story/extreme-heat-tolerance/</a>, See on <a href="https://news.ycombinator.com/item?id=39666369">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Humanity’s superpower is sweating—but rising heat could be our kryptonite, and an average temperature rise of 2 degrees Celsius above preindustrial levels could bring regular, fatal heat waves to large parts of the planet, says Tom Matthews, a senior lecturer in environmental geography at King’s College London.</p><p>“We have evolved to cope with the most extreme heat and humidity the planet can throw at us,” he explains. But when our core temperature gets to about 42 degrees Celsius (around 107.5 degrees Fahrenheit), people face heat stroke and probable death as the <a href="https://www.wired.com/story/india-deadly-combination-heat-humidity/">body strains</a> to keep cool and the heart works harder, inducing heart attacks.</p><p>Matthews cites an example from his home country, the UK. In the summer of 2022, the UK broke its high temperature record, surpassing 40 degrees Celsius (104 degrees Fahrenheit). Scientists <a href="https://climate-adapt.eea.europa.eu/en/metadata/publications/heat-related-mortality-in-europe-during-summer-2022">estimate</a> there were roughly 3,500 heat-associated deaths that summer in the UK. Across Europe, they estimate high heat caused more than 60,000 deaths.</p><p>“At 1.5 degrees Celsius of warming, the likes of Lagos, Karachi, [and] Shanghai start to experience heat waves exceeding our limit. At 2 degrees Celsius, the events increase at least 10 times more often, and if we get to 8 degrees Celsius, a large fraction of the Earth’s surface would be too hot for our physiology and would not be habitable,” he says.</p><p>Air conditioning and heat-escape rooms would help, but we might need to abandon intense outdoor work such as rice farming in hotter regions. And these solutions will need to be able to meet demand. “The infrastructure must be able to withstand the surges when everyone turns on the air conditioning, and must be able to withstand hurricanes or floods,” he says.</p><p>Our best hope in the face of inevitable rises in heat? Cooperation. “We’ve built forecasting systems that will warn us when disasters are incoming by working together at enormous scale. We must continue to do the same.”</p><p><em>This article appears in the March/April 2024 issue of WIRED UK magazine.</em></p><p><em>Updated 2-28-2024 11:30 am GMT: This story was updated to correct the estimated excess-death figures associated with the 2022 European heat wave.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[(How to Write a (Lisp) Interpreter (In Python)) (2010) (158 pts)]]></title>
            <link>https://www.norvig.com/lispy.html</link>
            <guid>39665939</guid>
            <pubDate>Mon, 11 Mar 2024 08:47:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.norvig.com/lispy.html">https://www.norvig.com/lispy.html</a>, See on <a href="https://news.ycombinator.com/item?id=39665939">Hacker News</a></p>
<div id="readability-page-1" class="page">


This page has two purposes: to describe how to implement computer
language interpreters in general, and in particular to build an interpreter
for most of the <a href="http://en.wikipedia.org/wiki/Scheme_(programming_language)"><i>Scheme</i></a>
dialect of Lisp using <a href="http://python.org/">Python 3</a> as the implementation language. 
I call my language and interpreter <i>Lispy</i> (<a href="https://www.norvig.com/lis.py"><b>lis.py</b></a>). Years ago, I showed how to write a semi-practical Scheme interpreter  <a (in="" href="https://www.norvig.com/jscheme.html">Java</a> and in <a href="http://books.google.com/books?id=QzGuHnDhvZIC&amp;lpg=PA756&amp;vq=scheme%20interpreter&amp;dq=Paradigms%20of%20Artificial%20Intelligence%20Programming&amp;pg=PA753#v=onepage&amp;q&amp;f=false">in Common
Lisp</a>).  This time around the goal is to demonstrate, as concisely
and simply as possible, what
<a href="http://queue.acm.org/detail.cfm?id=1039523">Alan Kay called</a> "<a href="http://www.righto.com/2008/07/maxwells-equations-of-software-examined.html"><i>Maxwell's Equations of Software</i></a>."

<p>Why does this  matter? As <a href="http://steve-yegge.blogspot.com/2007/06/rich-programmer-food.html">Steve
  Yegge said</a>, <i>"If you don't know how compilers work, then you
  don't know how computers work."</i> Yegge describes 8 problems that
  can be solved with compilers (or equally well with interpreters, or
   with Yegge's
  typical heavy dosage of cynicism).


</p><h2>Syntax and Semantics of Scheme Programs</h2>

The <i>syntax</i> of a language is the arrangement of characters to form correct statements or expressions; the
<i>semantics</i> is the meaning of those statements or expressions.  For example, in the language of
mathematical expressions (and in many programming languages), the syntax for adding one plus two is "1 +
2" and the semantics is the application of the addition operation to the two numbers, yielding the value 3. We say we
are <i>evaluating</i> an expression when we determine its
value; we would say that "1 + 2" evaluates to 3, and write
that as "1 + 2" ⇒ 3.

<p>Scheme syntax is different from most other programming languages. Consider:


</p><blockquote>
<table><tbody><tr><th>Java</th><th>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</th><th>Scheme
    </th></tr><tr><td>
</td></tr><tr><td>
<tt><b>if</b> (x.val() &gt; 0) {
<br>&nbsp;&nbsp;<b>return</b> fn(A[i] + 3 * i, 
<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>new</b> String[] {"one", "two"});
<br>}
  </tt></td><td>&nbsp;
  </td><td><tt>(<b>if</b> (&gt; (val x) 0)
    <br>&nbsp;&nbsp;&nbsp;&nbsp;(fn (+ (aref A i) (* 3 i)) 
<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<b>quote</b> (one two)))
   </tt></td></tr></tbody></table>
</blockquote>

Java has a wide variety
of syntactic conventions (keywords, infix operators, three kinds of brackets,
operator precedence, dot notation, quotes, commas,
semicolons), but Scheme syntax is much simpler: 
<ul>
<li> Scheme programs consist solely of <i>expressions</i>.  There is no statement/expression distinction.
</li><li> Numbers (e.g. <tt>1</tt>) and symbols (e.g. <tt>A</tt>) are called <i>atomic expressions</i>;
they cannot be broken into pieces.  These are similar to their Java counterparts, except that in
Scheme, operators such as <tt>+</tt> and <tt>&gt;</tt> are symbols too, and are treated the same
way as <tt>A</tt> and <tt>fn</tt>.
</li><li> Everything else is a <i>list expression</i>: a "(", followed by zero or more expressions,
followed by a ")".  The first element of the list determines what it means:
<ul>
<li>A list starting with a keyword, e.g. <tt>(if ...)</tt>, is a <i>special form</i>;
the meaning depends on the keyword.
</li><li>A list starting with a non-keyword, e.g. <tt>(fn ...)</tt>, is a function call.
</li></ul>
</li></ul>
The beauty of Scheme is that the full language only needs 5 keywords and 8 syntactic
forms.  In comparison, Python has 33 keywords and <a href="https://docs.python.org/3/reference/grammar.html">110</a>
syntactic forms, 
and Java has 50 keywords and <a href="https://docs.oracle.com/javase/specs/jls/se7/html/jls-18.html">133</a> syntactic forms.
All those parentheses
may seem intimidating, but Scheme syntax has the virtues of
simplicity and consistency. (Some have joked that "Lisp" stands for
"<a href="http://www.google.com/search?q=Lots+of+Irritating+Silly+Parentheses"><i><b>L</b>ots
of <b>I</b>rritating <b>S</b>illy <b>P</b>arentheses</i></a>"; I think it stand for
"<a href="http://www.google.com/search?hl=en&amp;as_q=&amp;as_epq=Lisp+Is+Syntactically+Pure"><i><b>L</b>isp
<b>I</b>s <b>S</b>yntactically <b>P</b>ure</i></a>".)  


<p>In this page we will cover all the important points of the Scheme language and its interpretation
(omitting some minor details), but we will take two steps to get there,
defining a simplified language first, before defining the near-full Scheme language. 

</p><h2>Language 1: Lispy Calculator</h2>

<i>Lispy Calculator</i> is a subset of Scheme using only five syntactic forms (two atomic, two special forms, and the procedure call).
Lispy Calculator lets you do any computation you could do on a typical calculator—as long as you are comfortable with prefix notation. 
And you can do two things that are not offered in typical calculator languages: "if" expressions, and the definition of new variables.  
Here's an example program, that computes the area of a circle of radius 10, using the formula π <i>r</i><sup>2</sup>:

<pre>(define r 10)
(* pi (* r r))
</pre>

Here is a table of all the allowable expressions:

<p>
<table>
  <tbody><tr><th>Expression</th><th>Syntax</th><th>Semantics and Example

  </th></tr><tr><td><a href="http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-7.html#%_sec_4.1.1">variable reference</a></td><td><i>symbol</i></td><td>A symbol is interpreted as a variable name;
  its value is the variable's
  value. <br>Example: <tt>r</tt> ⇒ <tt>10</tt> (assuming <tt>r</tt> was previously defined to be 10)

    </td></tr><tr><td><a href="http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-7.html#%_sec_4.1.2">constant
  literal</a></td><td><i>number</i></td><td>A number 
  evaluates to itself. <br>Examples: <tt>12 ⇒ 12</tt> <i>or</i>
  <tt>-3.45e+6 ⇒ -3.45e+6</tt>


      </td></tr><tr><td><a href="http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-7.html#%_sec_4.1.5">conditional</a></td><td><tt>(if</tt> <i>test conseq
  alt</i><tt>) </tt></td><td>Evaluate <i>test</i>; if true,
  evaluate and return <i>conseq</i>; otherwise  
  <i>alt</i>. <br>Example: <tt>(if (&gt; 10 20) (+ 1 1) (+ 3 3)) ⇒ 6</tt>
    

    </td></tr><tr><td><a href="http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-8.html#%_sec_5.2">definition</a>
</td><td><tt>(define</tt> <i>symbol</i> <i>exp</i><tt>)</tt>
</td><td>Define a new variable and give it
  the value of evaluating the expression <i>exp</i>. 
      <br>Examples: <tt>(define r 10)</tt> 
      
</td></tr><tr><td><a href="http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-7.html#%_sec_4.1.3">procedure
   call</a></td><td><tt>(</tt><i>proc arg...</i><tt>)</tt>	</td><td>If <i>proc</i> is
   anything other than one of the symbols <tt>if,  define,
   </tt> or <tt>quote</tt> then it is treated as a procedure.  Evaluate <i>proc</i>
   and all the <i>args</i>, and then the procedure is applied to the list of <i>arg</i> values. <br>Example: <tt>(sqrt (* 2 8)) ⇒ 4.0</tt>
  
   </td></tr></tbody></table>

   

  </p><p>In the Syntax column of this table, <i>symbol</i> must be a
   symbol,
   <i>number</i> must be an integer or floating point number,
   and the other italicized words can be any
   expression. The notation <i>arg...</i> means zero or more repetitions
   of <i>arg</i>. 
   




</p><h2>What A Language Interpreter Does</h2>

A language interpreter has two parts:
<ol>

  <li> <b>Parsing:</b> The parsing component takes an input program in
the form of a sequence of characters, verifies it according to the
<i>syntactic rules</i> of the language, and translates the program
into an internal representation.  In a simple interpreter the internal
representation is a tree structure (often called an <i>abstract syntax tree</i>) 
that closely mirrors the nested
structure of statements or expressions in the program. In a language
translator called a <i>compiler</i> there is often a series of internal representations,
starting with an abstract syntax tree, and progressing to a
sequence of instructions that can be directly executed by the
computer. The Lispy parser is implemented with the function <tt>parse</tt>.</li><li> <b>Execution:</b> The internal representation is then
  processed according to the <i>semantic rules</i> of the
  language, thereby carrying out the computation. Lispy's execution function is called <tt>eval</tt> (note this shadows
  Python's built-in function of the same name).
  
</li></ol>

Here is a picture of the interpretation process:

<blockquote>program  ➡ <span><tt>parse</tt></span>
➡ abstract-syntax-tree 
➡ <span><tt>eval</tt></span>
➡ result 
</blockquote>

<p>And here is a short example of what we want <tt>parse</tt> and <tt>eval</tt> to be able to do (<tt>begin</tt> evaluates each expression in order and returns the final one):

</p><pre>&gt;&gt; program = "(begin (define r 10) (* pi (* r r)))"

&gt;&gt;&gt; parse(program)
['begin', ['define', 'r', 10], ['*', 'pi', ['*', 'r', 'r']]]

&gt;&gt;&gt; eval(parse(program))
314.1592653589793
</pre>

<h2>Type Definitions</h2>

Let's be explicit about our representations for Scheme objects:

<pre>Symbol = str              # A Scheme Symbol is implemented as a Python str
Number = (int, float)     # A Scheme Number is implemented as a Python int or float
Atom   = (Symbol, Number) # A Scheme Atom is a Symbol or Number
List   = list             # A Scheme List is implemented as a Python list
Exp    = (Atom, List)     # A Scheme expression is an Atom or List
Env    = dict             # A Scheme environment (defined below) 
                          # is a mapping of {variable: value}

</pre>

<h2>Parsing: <tt>parse</tt>, <tt>tokenize</tt> and <tt>read_from_tokens</tt></h2>


Parsing is traditionally separated into two parts: <i>lexical
analysis</i>, in which the input character string is broken up into a
sequence of <i>tokens</i>, and <i>syntactic analysis</i>, in which the
tokens are assembled into an abstract syntax tree.  
The Lispy tokens are parentheses, symbols, and numbers.
There are
many tools for lexical analysis (such as Mike Lesk and Eric Schmidt's
<a href="http://dinosaur.compilertools.net/#lex">lex</a>), but for now we'll
use a very simple tool: Python's <tt>str.split</tt>. The function <tt>tokenize</tt> takes
as input a string of characters; it
adds spaces around each paren, and then calls <tt>str.split</tt> to get a
list of tokens:

<pre>def tokenize(chars: str) -&gt; list:
    "Convert a string of characters into a list of tokens."
    return chars.replace('(', ' ( ').replace(')', ' ) ').split()
</pre>

Here we apply tokenize to our sample program:

<pre>&gt;&gt;&gt; program = "(begin (define r 10) (* pi (* r r)))"
&gt;&gt;&gt; tokenize(program)
['(', 'begin', '(', 'define', 'r', '10', ')', '(', '*', 'pi', '(', '*', 'r', 'r', ')', ')', ')']
</pre>


<p>Our function <tt>parse</tt> will take a string representation of a program as input, call <tt>tokenize</tt>
to get a list of tokens, and then call <tt>read_from_tokens</tt> to assemble an abstract syntax tree.
<tt>read_from_tokens</tt> looks at the first token; if
it is a <tt>')'</tt> that's a syntax error. If it is a <tt>'('</tt>, then we start
building up a list of sub-expressions until we hit a matching <tt>')'</tt>.
Any non-parenthesis token must be a symbol or number. 
We'll let Python make the distinction between them: for each non-paren token,
first try to interpret it as an int, then as a float, and if it is neither
of those, it must be a
symbol. Here is the parser:

</p><pre>def parse(program: str) -&gt; Exp:
    "Read a Scheme expression from a string."
    return read_from_tokens(tokenize(program))

def read_from_tokens(tokens: list) -&gt; Exp:
    "Read an expression from a sequence of tokens."
    if len(tokens) == 0:
        raise SyntaxError('unexpected EOF')
    token = tokens.pop(0)
    if token == '(':
        L = []
        while tokens[0] != ')':
            L.append(read_from_tokens(tokens))
        tokens.pop(0) # pop off ')'
        return L
    elif token == ')':
        raise SyntaxError('unexpected )')
    else:
        return atom(token)

def atom(token: str) -&gt; Atom:
    "Numbers become numbers; every other token is a symbol."
    try: return int(token)
    except ValueError:
        try: return float(token)
        except ValueError:
            return Symbol(token)
</pre>




<tt>parse</tt> works like this:

<pre>&gt;&gt;&gt; program = "(begin (define r 10) (* pi (* r r)))"

&gt;&gt;&gt; parse(program)
['begin', ['define', 'r', 10], ['*', 'pi', ['*', 'r', 'r']]]
</pre>


 
We're almost ready to define <tt>eval</tt>. But we need one more concept first.

<h2>Environments</h2>

An environment is a mapping from variable names to their values.
By default, <tt>eval</tt> will use a global environment that includes the names for a bunch of standard functions (like <tt>sqrt</tt> and <tt>max</tt>,
and also operators like <tt>*</tt>).  This environment can be augmented with user-defined variables,
using the expression <tt>(define <i>symbol value</i>)</tt>.  

<pre>import math
import operator as op

def standard_env() -&gt; Env:
    "An environment with some Scheme standard procedures."
    env = Env()
    env.update(vars(math)) # sin, cos, sqrt, pi, ...
    env.update({
        '+':op.add, '-':op.sub, '*':op.mul, '/':op.truediv, 
        '&gt;':op.gt, '&lt;':op.lt, '&gt;=':op.ge, '&lt;=':op.le, '=':op.eq, 
        'abs':     abs,
        'append':  op.add,  
        'apply':   lambda proc, args: proc(*args),
        'begin':   lambda *x: x[-1],
        'car':     lambda x: x[0],
        'cdr':     lambda x: x[1:], 
        'cons':    lambda x,y: [x] + y,
        'eq?':     op.is_, 
        'expt':    pow,
        'equal?':  op.eq, 
        'length':  len, 
        'list':    lambda *x: List(x), 
        'list?':   lambda x: isinstance(x, List), 
        'map':     map,
        'max':     max,
        'min':     min,
        'not':     op.not_,
        'null?':   lambda x: x == [], 
        'number?': lambda x: isinstance(x, Number),  
		'print':   print,
        'procedure?': callable,
        'round':   round,
        'symbol?': lambda x: isinstance(x, Symbol),
    })
    return env

global_env = standard_env()
</pre>


<h2>Evaluation: <tt>eval</tt></h2>

<p>We are now ready for the implementation of <tt>eval</tt>.
As a refresher, we repeat the table of Lispy Calculator forms:

</p><p>
<table>
  <tbody><tr><th>Expression</th><th>Syntax</th><th>Semantics and Example

  </th></tr><tr><td><a href="http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-7.html#%_sec_4.1.1">variable reference</a></td><td><i>symbol</i></td><td>A symbol is interpreted as a variable name;
  its value is the variable's
  value. <br>Example: <tt>r</tt> ⇒ <tt>10</tt> (assuming <tt>r</tt> was previously defined to be 10)

    </td></tr><tr><td><a href="http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-7.html#%_sec_4.1.2">constant
  literal</a></td><td><i>number</i></td><td>A number 
  evaluates to itself. <br>Examples: <tt>12 ⇒ 12</tt> <i>or</i>
  <tt>-3.45e+6 ⇒ -3.45e+6</tt>


      </td></tr><tr><td><a href="http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-7.html#%_sec_4.1.5">conditional</a></td><td><tt>(if</tt> <i>test conseq
  alt</i><tt>) </tt></td><td>Evaluate <i>test</i>; if true,
  evaluate and return <i>conseq</i>; otherwise  
  <i>alt</i>. <br>Example: <tt>(if (&gt; 10 20) (+ 1 1) (+ 3 3)) ⇒ 6</tt>
    

    </td></tr><tr><td><a href="http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-8.html#%_sec_5.2">definition</a>
</td><td><tt>(define</tt> <i>symbol</i> <i>exp</i><tt>)</tt>
</td><td>Define a new variable and give it
  the value of evaluating the expression <i>exp</i>. 
      <br>Examples: <tt>(define r 10)</tt> 
      
</td></tr><tr><td><a href="http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-7.html#%_sec_4.1.3">procedure
   call</a></td><td><tt>(</tt><i>proc arg...</i><tt>)</tt>	</td><td>If <i>proc</i> is
   anything other than one of the symbols <tt>if,  define,
   </tt> or <tt>quote</tt> then it is treated as a procedure.  Evaluate <i>proc</i>
   and all the <i>args</i>, and then the procedure is applied to the list of <i>arg</i> values. <br>Example: <tt>(sqrt (* 2 8)) ⇒ 4.0</tt>
  
   </td></tr></tbody></table>

</p><p>Here is the code for <tt>eval</tt>, which closely follows the table:

</p><pre>def eval(x: Exp, env=global_env) -&gt; Exp:
    "Evaluate an expression in an environment."
    if isinstance(x, Symbol):        # variable reference
        return env[x]
    elif isinstance(x, Number):      # constant number
        return x                
    elif x[0] == 'if':               # conditional
        (_, test, conseq, alt) = x
        exp = (conseq if eval(test, env) else alt)
        return eval(exp, env)
    elif x[0] == 'define':           # definition
        (_, symbol, exp) = x
        env[symbol] = eval(exp, env)
    else:                            # procedure call
        proc = eval(x[0], env)
        args = [eval(arg, env) for arg in x[1:]]
        return proc(*args)
</pre>

<p><i>We're done!</i> You can see it all in action:

</p><pre>&gt;&gt;&gt; eval(parse("(begin (define r 10) (* pi (* r r)))"))
314.1592653589793
</pre>

<h2>Interaction: A REPL</h2>

It is tedious to have to enter <tt>eval(parse("..."))</tt> all the time.
One of Lisp's great legacies is the notion of an interactive read-eval-print loop:
a way for a programmer to enter an expression, and see it immediately read, evaluated, and printed, without having to go through a lengthy build/compile/run cycle.  So let's define the function <tt>repl</tt> (which stands for read-eval-print-loop), and the function <tt>schemestr</tt> which returns a string representing a Scheme object.

<pre>def repl(prompt='lis.py&gt; '):
    "A prompt-read-eval-print loop."
    while True:
        val = eval(parse(raw_input(prompt)))
        if val is not None: 
            print(schemestr(val))

def schemestr(exp):
    "Convert a Python object back into a Scheme-readable string."
    if isinstance(exp, List):
        return '(' + ' '.join(map(schemestr, exp)) + ')' 
    else:
        return str(exp)
</pre>

Here is <tt>repl</tt> in action:

<pre>&gt;&gt;&gt; repl()
lis.py&gt; (define r 10)
lis.py&gt; (* pi (* r r))
314.159265359
lis.py&gt; (if (&gt; (* 11 11) 120) (* 7 6) oops)
42
lis.py&gt; (list (+ 1 1) (+ 2 2) (* 2 3) (expt 2 3))
lis.py&gt; 
</pre>

<h2>Language 2: Full Lispy</h2>

We will now extend our language with three new special forms, giving us a much more nearly-complete Scheme subset:

<p><table>
  <tbody><tr><th>Expression</th><th>Syntax</th><th>Semantics and Example

          </th></tr><tr><td><a href="http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-7.html#%_sec_4.1.2">quotation</a></td><td><tt>(quote </tt><i>exp</i><tt>)</tt></td><td>
Return the <i>exp</i> literally; do not evaluate it. <br>Example:
	<tt>(quote (+ 1 2)) ⇒ (+ 1 2)</tt> 
    
  </td></tr><tr><td><a href="http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-7.html#%_sec_4.1.6">assignment</a></td><td><tt>(set!</tt> <i>symbol
  exp</i><tt>)</tt></td><td>Evaluate <i>exp</i> and assign that value to
  <i>symbol</i>, which must have been previously defined (with a
  <tt>define</tt> or as a parameter to an enclosing procedure).
    <br>Example: <tt>(set! r2 (* r r))</tt>


      
</td></tr><tr><td><a href="http://www.schemers.org/Documents/Standards/R5RS/HTML/r5rs-Z-H-7.html#%_sec_4.1.4">procedure</a></td><td><tt>(lambda (</tt><i>symbol...</i><tt>)</tt>
  <i>exp</i><tt>)</tt></td><td>Create a procedure
  with parameter(s) named <i>symbol...</i> and <i>exp</i> as the body. <br>Example: <tt>(lambda (r)
  (* pi (* r r)))</tt>

</td></tr></tbody></table>

</p><p>The <tt>lambda</tt> special form (an obscure nomenclature choice that refers to Alonzo Church's <a href="http://en.wikipedia.org/wiki/Lambda_calculus">lambda calculus</a>) creates a procedure.  
We want procedures to work like this:

</p><pre>lis.py&gt; (define circle-area (lambda (r) (* pi (* r r)))
lis.py&gt; (circle-area (+ 5 5))
314.159265359
</pre>

There are two steps here. In the first step,  the <tt>lambda</tt> expression is evaluated to 
create a procedure, one which refers to the global variables <tt>pi</tt> and <tt>*</tt>, takes a single parameter, which it calls <tt>r</tt>.
This procedure is used as the value of the new variable <tt>circle-area</tt>. In the second step,  
the procedure we just defined is the value of <tt>circle-area</tt>, so it is called, with the value 10 as the argument.
We want <tt>r</tt> to take on the value 10,
but it wouldn't do to just set <tt>r</tt> to 10 in the global environment.  
What if we were using <tt>r</tt> for some other purpose? We wouldn't want a call to <tt>circle-area</tt> to alter that value.  Instead, we want to arrange for there to be a <i>local</i> variable named <tt>r</tt> that we can set to 10 without worrying about interfering with any global variable that happens to have the same name.
The process for calling a procedure introduces these new local variable(s), binding each symbol in the parameter list of. the function 
to the corresponding value in the argument list of the function call.

<h2>Redefining <tt>Env</tt> as a Class</h2>

To handle local variables, we will redefine <tt>Env</tt> to be a subclass of <tt>dict</tt>.
When we evaluate <tt>(circle-area (+ 5 5))</tt>, we will fetch the procedure body, <tt>(* pi (* r r))</tt>,
and evaluate it in an environment that has <tt>r</tt> as the sole local variable (with value 10), but also has the global environment as the "outer" 
environment; it is there that we will find the values of <tt>*</tt> and <tt>pi</tt>.  
In other words, we want an environment that looks like this, with the local (blue) environment nested inside the outer (red) global environment:

<p><div>
<tt>pi: 3.141592653589793
<br>*: &lt;built-in function mul&gt;
<br>...
<br>

</tt></div>

</p><p>When we look up a variable in such a nested environment, we look first at the innermost level, but if
we don't find the variable name there, we move to the next outer level.
Procedures and environments are intertwined, so let's define them together:

</p><pre>class Env(dict):
    "An environment: a dict of {'var': val} pairs, with an outer Env."
    def __init__(self, parms=(), args=(), outer=None):
        self.update(zip(parms, args))
        self.outer = outer
    def find(self, var):
        "Find the innermost Env where var appears."
        return self if (var in self) else self.outer.find(var)

class Procedure(object):
    "A user-defined Scheme procedure."
    def __init__(self, parms, body, env):
        self.parms, self.body, self.env = parms, body, env
    def __call__(self, *args): 
        return eval(self.body, Env(self.parms, args, self.env))

global_env = standard_env()
</pre>

We see that every procedure has three components: a list of parameter names,  a body expression, and an environment
that tells us what other variables are accessible from the body. For a procedure defined at the top level this will be
the global environment, but it is also possible for a procedure to refer to the local variables of the environment
in which it was <i>defined</i> (and not the environment in which it is <i>called</i>). 

<p>An environment is a subclass of <tt>dict</tt>, so it has
all the methods that <tt>dict</tt> has. In addition there are two methods: the constructor
<tt>__init__</tt> builds a new environment by taking a list of parameter names
and a corresponding list of argument values, and creating a new environment that has those
{variable: value} pairs as the inner part, and also refers to the given <tt>outer</tt> environment.
The method <tt>find</tt> is used to find the right environment for
a variable: either the inner one or an outer one. 

</p><p>To see how these all go together, here is the new definition of <tt>eval</tt>. Note that
the clause for variable reference has changed: we now have to call <tt>env.find(x)</tt> to find at what level
the variable <tt>x</tt> exists; then we can fetch the value of <tt>x</tt> from that level. (The clause for
<tt>define</tt> has not changed, because a <tt>define</tt> always adds a new variable
to the innermost environment.)  There are two new clauses: for <tt>set!</tt>, we find the environment level
where the variable exists and set it to a new value.  With <tt>lambda</tt>, we create a new procedure object with the given
parameter list, body, and environment.

</p><pre>def eval(x, env=global_env):
    "Evaluate an expression in an environment."
    if isinstance(x, Symbol):    # variable reference
        return env.find(x)[x]
    elif not isinstance(x, List):# constant 
        return x   
    op, *args = x       
    if op == 'quote':            # quotation
        return args[0]
    elif op == 'if':             # conditional
        (test, conseq, alt) = args
        exp = (conseq if eval(test, env) else alt)
        return eval(exp, env)
    elif op == 'define':         # definition
        (symbol, exp) = args
        env[symbol] = eval(exp, env)
    elif op == 'set!':           # assignment
        (symbol, exp) = args
        env.find(symbol)[symbol] = eval(exp, env)
    elif op == 'lambda':         # procedure
        (parms, body) = args
        return Procedure(parms, body, env)
    else:                        # procedure call
        proc = eval(op, env)
        vals = [eval(arg, env) for arg in args]
        return proc(*vals)
</pre>

<p>To appreciate how procedures and environments work together, consider this program and the environment that gets formed when we evaluate <tt>(account1 -20.00)</tt>:

</p><p><table><tbody><tr><td>
<p>
<div><tt>(define <b>make-account</b>
<div>&nbsp;&nbsp;(lambda (<b>balance</b>)
<p>&nbsp;&nbsp;&nbsp;&nbsp;(lambda (<b>amt</b>)
<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(begin <b>(set! balance (+ balance amt))</b> 
<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;balance))))
  </p></div>
<br><tt>(define <b>account1</b> (make-account 100.00))
  <br>(account1 -20.00)
</tt></tt></div>

</p></td><td> &nbsp; </td><td>

<div><b><tt>+</tt></b>: &lt;built-in operator add&gt;
<br><tt><b>make-account</b>: &lt;a Procedure&gt;

<br><tt><tt><b>account1</b>: &lt;a Procedure&gt;
</tt></tt></tt></div>

</td></tr></tbody></table>

</p><p>Each rectangular box represents an environment, and the color of
the box matches the color of the variables that are newly defined in
the environment.  In the last two lines of the program we define <tt>account1</tt> and call
<tt>(account1 -20.00)</tt>; this represents the creation of a bank account
with a 100 dollar opening balance, followed by a 20 dollar withdrawal.
In the process of evaluating <tt>(account1 -20.00)</tt>, we will eval the
expression highlighted in yellow.  There are three variables in that
expression.  <tt>amt</tt> can be found immediately in the innermost
(green) environment. But <tt>balance</tt> is not defined there: we
have to look at the green environment's outer <tt>env</tt>, the blue
one. And finally, the variable <tt>+</tt> is not found in either of
those; we need to do one more outer step, to the global (red) environment.
This process of looking first in inner environments and then in
outer ones is called <i>lexical scoping</i>.  
<tt>Env.find(var)</tt> finds the right environment according to
lexical scoping rules.



</p><p>Let's see what we can do now:

</p><pre>&gt;&gt;&gt; repl()
lis.py&gt; (define circle-area (lambda (r) (* pi (* r r))))
lis.py&gt; (circle-area 3)
28.274333877
lis.py&gt; (define fact (lambda (n) (if (&lt;= n 1) 1 (* n (fact (- n 1))))))
lis.py&gt; (fact 10)
3628800
lis.py&gt; (fact 100)
9332621544394415268169923885626670049071596826438162146859296389521759999322991
5608941463976156518286253697920827223758251185210916864000000000000000000000000
lis.py&gt; (circle-area (fact 10))
4.1369087198e+13
lis.py&gt; (define first car)
lis.py&gt; (define rest cdr)
lis.py&gt; (define count (lambda (item L) (if L (+ (equal? item (first L)) (count item (rest L))) 0)))
lis.py&gt; (count 0 (list 0 1 2 3 0 0))
3
lis.py&gt; (count (quote the) (quote (the more the merrier the bigger the better)))
4
lis.py&gt; (define twice (lambda (x) (* 2 x)))
lis.py&gt; (twice 5)
10
lis.py&gt; (define repeat (lambda (f) (lambda (x) (f (f x)))))
lis.py&gt; ((repeat twice) 10)
40
lis.py&gt; ((repeat (repeat twice)) 10)
160
lis.py&gt; ((repeat (repeat (repeat twice))) 10)
2560
lis.py&gt; ((repeat (repeat (repeat (repeat twice)))) 10)
655360
lis.py&gt; (pow 2 16)
65536.0
lis.py&gt; (define fib (lambda (n) (if (&lt; n 2) 1 (+ (fib (- n 1)) (fib (- n 2))))))
lis.py&gt; (define range (lambda (a b) (if (= a b) (quote ()) (cons a (range (+ a 1) b)))))
lis.py&gt; (range 0 10)
(0 1 2 3 4 5 6 7 8 9)
lis.py&gt; (map fib (range 0 10))
(1 1 2 3 5 8 13 21 34 55)
lis.py&gt; (map fib (range 0 20))
(1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181 6765)
</pre>

We now have a language with procedures, variables, conditionals (<tt>if</tt>), and sequential execution (the <tt>begin</tt> procedure).  If you are familiar with other languages, you might think that a <tt>while</tt> or <tt>for</tt> loop would be needed, but
Scheme manages to do without these just fine. The Scheme report says "Scheme demonstrates that a very small number of rules for forming expressions, with no restrictions on how they are composed, suffice to form a practical and efficient programming language." In Scheme
you iterate by defining recursive functions.

<h2>How Small/Fast/Complete/Good is Lispy?</h2>

In which we judge Lispy on several criteria:

<ul>
<li><b><i>Small:</i></b> Lispy is <i>very</i> small: 117 non-comment
  non-blank lines; 4K of source code. (An earlier version was just 90 lines, but had fewer standard procedures
and was perhaps a bit too terse.)   The smallest version of
  my Scheme in Java, <a href="http://norvig.com/jscheme.html">Jscheme</a>, was 1664 lines and 57K of source. Jscheme was
  originally called SILK (Scheme in Fifty Kilobytes), but I only kept
  under that limit by counting bytecode rather than source code. Lispy does much
  better; I think it meets Alan Kay's 1972 <a href="http://gagne.homedns.org/~tgagne/contrib/EarlyHistoryST.html">claim</a>
  that <i>you could define the
  "most powerful language in the world" in "a page of code."</i> (However,
  I think Alan would disagree, because he would count the Python compiler as part of the code, putting me <i>well</i> over a page.)

<pre>bash$ grep "^\s*[^#\s]" lis.py | wc
     117     497    4276
</pre></li><li><b><i>Fast:</i></b> Lispy computes <tt>(fact 100)</tt> exactly in 0.003
seconds.  That's fast enough for me (although far slower than most
other ways of computing it). </li><li><b><i>Complete:</i></b> Lispy is not very complete compared to the
Scheme standard.  Some major shortcomings:
<ul>
  <li> <b><i>Syntax</i></b>: Missing comments, quote and quasiquote notation, # literals, the derived
  expression types (such as <tt>cond</tt>, derived from <tt>if</tt>,
  or <tt>let</tt>, derived from <tt>lambda</tt>), and dotted list notation.
</li><li> <b><i>Semantics</i></b>: Missing call/cc and tail recursion.  
</li><li> <b><i>Data Types</i></b>: Missing strings, characters, booleans, ports,
  vectors, exact/inexact numbers.
  Python lists are actually closer to Scheme
  vectors than to the Scheme pairs and lists that we implement with them.
</li><li> <b><i>Procedures</i></b>: Missing over 100 primitive procedures.
  
 </li><li> <b><i>Error recovery</i></b>: Lispy does not attempt to detect,
  reasonably report, or recover from errors.  Lispy expects the
  programmer to be perfect.
</li></ul></li><li><b><i>Good:</i></b> That's up to the readers to decide.  I found it
was good for my purpose of explaining Lisp interpreters.
</li></ul>

<h2>True Story</h2>

To back up the idea that it can be very helpful to know how
interpreters work, here's a story.  Way back in 1984 I was writing a
Ph.D. thesis.  This was before LaTeX, before Microsoft Word for Windows—we used
troff. Unfortunately, troff had no facility for forward references
to symbolic labels: I wanted to be able to write "As we will see on
page @theorem-x" and then write something like "@(set theorem-x \n%)" in
the appropriate place (the troff register \n% holds the page number). My
fellow grad student Tony DeRose felt the same need, and together we
sketched out a simple Lisp program that would handle this as a preprocessor.  However,
it turned out that the Lisp we had at the time was good at reading
Lisp expressions, but so slow at reading character-at-a-time non-Lisp
expressions that our program was annoying to use.  
<p>
From there Tony and I split paths.  He reasoned that the hard part was
the interpreter for expressions; he needed Lisp for that, but he knew
how to write a tiny C routine
for reading and echoing the non-Lisp characters and link it in to the Lisp
program.  I didn't know how to do that linking, but I reasoned that writing an
interpreter for this trivial language (all it had was set variable,
fetch variable, and string concatenate) was easy, so I wrote an
interpreter in C. So, ironically, Tony wrote a Lisp program (with one small routine in C) because he was a
C programmer, and I wrote a C program because I was a Lisp programmer.
</p><p>
In the end, we both got our theses done (<a href="http://www.eecs.berkeley.edu/Pubs/TechRpts/1985/6081.html">Tony</a>, <a href="http://www.eecs.berkeley.edu/Pubs/TechRpts/1987/5995.html">Peter</a>).

</p><h2>The Whole Thing</h2>

The whole program is here: <a href="https://www.norvig.com/lis.py">lis.py</a>.

<h2>Further Reading</h2>

   
  
  <p>To learn more about Scheme consult some of the fine books (by
   <a href="http://books.google.com/books?id=xyO-KLexVnMC&amp;lpg=PP1&amp;dq=scheme%20programming%20book&amp;pg=PP1#v=onepage&amp;q&amp;f=false">Friedman
   and Fellesein</a>,
   <a href="http://books.google.com/books?id=wftS4tj4XFMC&amp;lpg=PA300&amp;dq=scheme%20programming%20book&amp;pg=PP1#v=onepage&amp;q&amp;f=false">Dybvig</a>,
   <a href="http://books.google.com/books?id=81mFK8pqh5EC&amp;lpg=PP1&amp;dq=scheme%20programming%20book&amp;pg=PP1#v=onepage&amp;q&amp;f=false">Queinnec</a>,
   <a href="http://www.eecs.berkeley.edu/~bh/ss-toc2.html">Harvey and
   Wright</a> or
   <a href="http://mitpress.mit.edu/sicp/">Sussman and Abelson</a>),
   videos (by <a href="http://groups.csail.mit.edu/mac/classes/6.001/abelson-sussman-lectures/">Abelson
   and Sussman</a>),
   tutorials (by
      <a href="http://www.ccs.neu.edu/home/dorai/t-y-scheme/t-y-scheme.html">Dorai</a>,
   <a href="http://docs.racket-lang.org/quick/index.html">PLT</a>, or
   <a href="http://cs.gettysburg.edu/~tneller/cs341/scheme-intro/index.html">Neller</a>),
   or the
      <a href="http://www.schemers.org/Documents/Standards/R5RS/HTML">reference
   manual</a>.

</p><p>I also have another page describing a <a href="http://norvig.com/lispy2.html">more advanced version of Lispy</a>.


</p><hr>
<i><a href="http://norvig.com/">Peter Norvig</a></i>

<hr> 
 
 
 </div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: RE3 – Reversed Engineered GTA3 Source Code (125 pts)]]></title>
            <link>https://github.com/halpz/re3</link>
            <guid>39665017</guid>
            <pubDate>Mon, 11 Mar 2024 04:53:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/halpz/re3">https://github.com/halpz/re3</a>, See on <a href="https://news.ycombinator.com/item?id=39665017">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/halpz/re3/blob/master/res/images/logo_1024.png?raw=true"><img src="https://github.com/halpz/re3/raw/master/res/images/logo_1024.png?raw=true" alt="re3 logo" width="200"></a></p>
<p dir="auto"><a href="https://actions-badge.atrox.dev/GTAmodding/re3/goto?ref=master" rel="nofollow"><img src="https://camo.githubusercontent.com/1508b6f62669dc0b23cb2a1b91f8aef5dc3b946b611a59dc321682ecfe96dcaa/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e742e7376673f75726c3d6874747073253341253246253246616374696f6e732d62616467652e6174726f782e6465762532464754416d6f6464696e6725324672653325324662616467652533467265662533446d6173746572267374796c653d666c6174" alt="Build Status" data-canonical-src="https://img.shields.io/endpoint.svg?url=https%3A%2F%2Factions-badge.atrox.dev%2FGTAmodding%2Fre3%2Fbadge%3Fref%3Dmaster&amp;style=flat"></a>
<a href="https://discord.gg/RFNbjsUMGg" rel="nofollow"><img src="https://camo.githubusercontent.com/e1d2dd4598108ac98c8d4250d7574a307c1017ef81b319667c53a1de19332324/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646973636f72642d6a6f696e2d3732383944412e7376673f6c6f676f3d646973636f7264266c6f6e6743616368653d74727565267374796c653d666c6174" data-canonical-src="https://img.shields.io/badge/discord-join-7289DA.svg?logo=discord&amp;longCache=true&amp;style=flat"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Intro</h2><a id="user-content-intro" aria-label="Permalink: Intro" href="#intro"></a></p>
<p dir="auto">In this repository you'll find the fully reversed source code for GTA III (<a href="https://github.com/halpz/re3/tree/master/">master</a> branch) and GTA VC (<a href="https://github.com/halpz/re3/tree/miami/">miami</a> branch).</p>
<p dir="auto">It has been tested and works on Windows, Linux, MacOS and FreeBSD, on x86, amd64, arm and arm64.<br>
Rendering is handled either by original RenderWare (D3D8)
or the reimplementation <a href="https://github.com/aap/librw">librw</a> (D3D9, OpenGL 2.1 or above, OpenGL ES 2.0 or above).<br>
Audio is done with MSS (using dlls from original GTA) or OpenAL.</p>
<p dir="auto">The project has also been ported to the <a href="https://github.com/AGraber/re3-nx/">Nintendo Switch</a>,
<a href="https://github.com/Rinnegatamante/re3">Playstation Vita</a> and
<a href="https://github.com/GaryOderNichts/re3-wiiu/">Nintendo Wii U</a>.</p>
<p dir="auto">We cannot build for PS2 or Xbox yet. If you're interested in doing so, get in touch with us.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<ul dir="auto">
<li>re3 requires PC game assets to work, so you <strong>must</strong> own <a href="https://store.steampowered.com/app/12100/Grand_Theft_Auto_III/" rel="nofollow">a copy of GTA III</a>.</li>
<li>Build re3 or download the latest build:
<ul dir="auto">
<li><a href="https://nightly.link/GTAmodding/re3/workflows/re3_msvc_x86/master/re3_Release_win-x86-librw_d3d9-mss.zip" rel="nofollow">Windows D3D9 MSS 32bit</a></li>
<li><a href="https://nightly.link/GTAmodding/re3/workflows/re3_msvc_amd64/master/re3_Release_win-amd64-librw_d3d9-oal.zip" rel="nofollow">Windows D3D9 64bit</a></li>
<li><a href="https://nightly.link/GTAmodding/re3/workflows/re3_msvc_amd64/master/re3_Release_win-amd64-librw_gl3_glfw-oal.zip" rel="nofollow">Windows OpenGL 64bit</a></li>
<li><a href="https://nightly.link/GTAmodding/re3/workflows/build-cmake-conan/master/ubuntu-18.04-gl3.zip" rel="nofollow">Linux 64bit</a></li>
<li><a href="https://nightly.link/GTAmodding/re3/workflows/build-cmake-conan/master/macos-latest-gl3.zip" rel="nofollow">MacOS 64bit x86-64</a></li>
</ul>
</li>
<li>Extract the downloaded zip over your GTA 3 directory and run re3. The zip includes the binary, updated and additional gamefiles and in case of OpenAL the required dlls.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screenshots</h2><a id="user-content-screenshots" aria-label="Permalink: Screenshots" href="#screenshots"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1521437/107704085-fbdabd00-6cbc-11eb-8406-8951a80ccb16.png"><img src="https://user-images.githubusercontent.com/1521437/107704085-fbdabd00-6cbc-11eb-8406-8951a80ccb16.png" alt="re3 2021-02-11 22-57-03-23"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1521437/107703339-cbdeea00-6cbb-11eb-8f0b-07daa105d470.png"><img src="https://user-images.githubusercontent.com/1521437/107703339-cbdeea00-6cbb-11eb-8f0b-07daa105d470.png" alt="re3 2021-02-11 22-43-44-98"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1521437/107703343-cd101700-6cbb-11eb-9ccd-012cb90524b7.png"><img src="https://user-images.githubusercontent.com/1521437/107703343-cd101700-6cbb-11eb-9ccd-012cb90524b7.png" alt="re3 2021-02-11 22-46-33-76"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1521437/107703348-d00b0780-6cbb-11eb-8afd-054249c2b95e.png"><img src="https://user-images.githubusercontent.com/1521437/107703348-d00b0780-6cbb-11eb-8afd-054249c2b95e.png" alt="re3 2021-02-11 22-50-29-54"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Improvements</h2><a id="user-content-improvements" aria-label="Permalink: Improvements" href="#improvements"></a></p>
<p dir="auto">We have implemented a number of changes and improvements to the original game.
They can be configured in <code>core/config.h</code>.
Some of them can be toggled at runtime, some cannot.</p>
<ul dir="auto">
<li>Fixed a lot of smaller and bigger bugs</li>
<li>User files (saves and settings) stored in GTA root directory</li>
<li>Settings stored in re3.ini file instead of gta3.set</li>
<li>Debug menu to do and change various things (Ctrl-M to open)</li>
<li>Debug camera (Ctrl-B to toggle)</li>
<li>Rotatable camera</li>
<li>XInput controller support (Windows)</li>
<li>No loading screens between islands ("map memory usage" in menu)</li>
<li>Skinned ped support (models from Xbox or Mobile)</li>
<li>Rendering
<ul dir="auto">
<li>Widescreen support (properly scaled HUD, Menu and FOV)</li>
<li>PS2 MatFX (vehicle reflections)</li>
<li>PS2 alpha test (better rendering of transparency)</li>
<li>PS2 particles</li>
<li>Xbox vehicle rendering</li>
<li>Xbox world lightmap rendering (needs Xbox map)</li>
<li>Xbox ped rim light</li>
<li>Xbox screen rain droplets</li>
<li>More customizable colourfilter</li>
</ul>
</li>
<li>Menu
<ul dir="auto">
<li>Map</li>
<li>More options</li>
<li>Controller configuration menu</li>
<li>...</li>
</ul>
</li>
<li>Can load DFFs and TXDs from other platforms, possibly with a performance penalty</li>
<li>...</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">To-Do</h2><a id="user-content-to-do" aria-label="Permalink: To-Do" href="#to-do"></a></p>
<p dir="auto">The following things would be nice to have/do:</p>
<ul dir="auto">
<li>Fix physics for high FPS</li>
<li>Improve performance on lower end devices, especially the OpenGL layer on the Raspberry Pi (if you have experience with this, please get in touch)</li>
<li>Compare code with PS2 code (tedious, no good decompiler)</li>
<li><a href="https://web.archive.org/web/20210217192931/https://github.com/GTAmodding/re3/wiki/PS2-port" rel="nofollow">PS2 port</a></li>
<li>Xbox port (not quite as important)</li>
<li>reverse remaining unused/debug functions</li>
<li>compare CodeWarrior build with original binary for more accurate code (very tedious)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Modding</h2><a id="user-content-modding" aria-label="Permalink: Modding" href="#modding"></a></p>
<p dir="auto">Asset modifications (models, texture, handling, script, ...) should work the same way as with original GTA for the most part.</p>
<p dir="auto">CLEO scripts work with <a href="https://github.com/cleolibrary/CLEO-Redux">CLEO Redux</a>.</p>
<p dir="auto">Mods that make changes to the code (dll/asi, limit adjusters) will <em>not</em> work.
Some things these mods do are already implemented in re3 (much of SkyGFX, GInput, SilentPatch, Widescreen fix),
others can easily be achieved (increasing limis, see <code>config.h</code>),
others will simply have to be rewritten and integrated into the code directly.
Sorry for the inconvenience.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building from Source</h2><a id="user-content-building-from-source" aria-label="Permalink: Building from Source" href="#building-from-source"></a></p>
<p dir="auto">When using premake, you may want to point GTA_III_RE_DIR environment variable to GTA3 root folder if you want the executable to be moved there via post-build script.</p>
<p dir="auto">Clone the repository with <code>git clone --recursive https://github.com/halpz/re3.git</code>. Then <code>cd re3</code> into the cloned repository.</p>
<details><summary>Linux Premake</summary>
<p dir="auto">For Linux using premake, proceed: <a href="https://web.archive.org/web/20210217192751/https://github.com/GTAmodding/re3/wiki/Building-on-Linux" rel="nofollow">Building on Linux</a></p>
</details>
<details><summary>Linux Conan</summary>
<p dir="auto">Install python and conan, and then run build.</p>
<div data-snippet-clipboard-copy-content="conan export vendor/librw librw/master@
mkdir build
cd build
conan install .. re3/master@ -if build -o re3:audio=openal -o librw:platform=gl3 -o librw:gl3_gfxlib=glfw --build missing -s re3:build_type=RelWithDebInfo -s librw:build_type=RelWithDebInfo
conan build .. -if build -bf build -pf package"><pre><code>conan export vendor/librw librw/master@
mkdir build
cd build
conan install .. re3/master@ -if build -o re3:audio=openal -o librw:platform=gl3 -o librw:gl3_gfxlib=glfw --build missing -s re3:build_type=RelWithDebInfo -s librw:build_type=RelWithDebInfo
conan build .. -if build -bf build -pf package
</code></pre></div>
</details>
<details><summary>MacOS Premake</summary>
<p dir="auto">For MacOS using premake, proceed: <a href="https://web.archive.org/web/20210717004757/https://github.com/GTAmodding/re3/wiki/Building-on-MacOS" rel="nofollow">Building on MacOS</a></p>
</details>
<details><summary>FreeBSD</summary>
<p dir="auto">For FreeBSD using premake, proceed: <a href="https://web.archive.org/web/20210217192740/https://github.com/GTAmodding/re3/wiki/Building-on-FreeBSD" rel="nofollow">Building on FreeBSD</a></p>
</details>
<details><summary>Windows</summary>
<p dir="auto">Assuming you have Visual Studio 2015/2017/2019:</p>
<ul dir="auto">
<li>Run one of the <code>premake-vsXXXX.cmd</code> variants on root folder.</li>
<li>Open build/re3.sln with Visual Studio and compile the solution.</li>
</ul>
<p dir="auto">Microsoft recently discontinued its downloads of the DX9 SDK. You can download an archived version here: <a href="https://archive.org/details/dxsdk_jun10" rel="nofollow">https://archive.org/details/dxsdk_jun10</a></p>
<p dir="auto"><strong>If you choose OpenAL on Windows</strong> You must read <a href="https://web.archive.org/web/20210217192855/https://github.com/GTAmodding/re3/wiki/Running-OpenAL-build-on-Windows" rel="nofollow">Running OpenAL build on Windows</a>.</p>
</details>
<blockquote>
<p dir="auto">ℹ️ premake has an <code>--with-lto</code> option if you want the project to be compiled with Link Time Optimization.</p>
</blockquote>
<blockquote>
<p dir="auto">ℹ️ There are various settings in <a href="https://github.com/halpz/re3/tree/master/src/core/config.h">config.h</a>, you may want to take a look there.</p>
</blockquote>
<blockquote>
<p dir="auto">ℹ️ re3 uses completely homebrew RenderWare-replacement rendering engine; <a href="https://github.com/aap/librw/">librw</a>. librw comes as submodule of re3, but you also can use LIBRW enviorenment variable to specify path to your own librw.</p>
</blockquote>
<p dir="auto">If you feel the need, you can also use CodeWarrior 7 to compile re3 using the supplied codewarrior/re3.mcp project - this requires the original RW33 libraries, and the DX8 SDK. The build is unstable compared to the MSVC builds though, and is mostly meant to serve as a reference.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">As long as it's not linux/cross-platform skeleton/compatibility layer, all of the code on the repo that's not behind a preprocessor condition(like FIX_BUGS) are <strong>completely</strong> reversed code from original binaries.</p>
<p dir="auto">We <strong>don't</strong> accept custom codes, as long as it's not wrapped via preprocessor conditions, or it's linux/cross-platform skeleton/compatibility layer.</p>
<p dir="auto">We accept only these kinds of PRs;</p>
<ul dir="auto">
<li>A new feature that exists in at least one of the GTAs (if it wasn't in III/VC then it doesn't have to be decompilation)</li>
<li>Game, UI or UX bug fixes (if it's a fix to original code, it should be behind FIX_BUGS)</li>
<li>Platform-specific and/or unused code that's not been reversed yet</li>
<li>Makes reversed code more understandable/accurate, as in "which code would produce this assembly".</li>
<li>A new cross-platform skeleton/compatibility layer, or improvements to them</li>
<li>Translation fixes, for languages original game supported</li>
<li>Code that increase maintainability</li>
</ul>
<p dir="auto">We have a <a href="https://github.com/halpz/re3/blob/master/CODING_STYLE.md">Coding Style</a> document that isn't followed or enforced very well.</p>
<p dir="auto">Do not use features from C++11 or later.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">History</h2><a id="user-content-history" aria-label="Permalink: History" href="#history"></a></p>
<p dir="auto">re3 was started sometime in the spring of 2018,
initially as a way to test reversed collision and physics code
inside the game.
This was done by replacing single functions of the game
with their reversed counterparts using a dll.</p>
<p dir="auto">After a bit of work the project lay dormant for about a year
and was picked up again and pushed to github in May 2019.
At the time I (aap) had reversed around 10k lines of code and estimated
the final game to have around 200-250k.
Others quickly joined the effort (Fire_Head, shfil, erorcun and Nick007J
in time order, and Serge a bit later) and we made very quick progress
throughout the summer of 2019
after which the pace slowed down a bit.</p>
<p dir="auto">Due to everyone staying home during the start of the Corona pandemic
everybody had a lot of time to work on re3 again and
we finally got a standalone exe in April 2020 (around 180k lines by then).</p>
<p dir="auto">After the initial excitement and fixing and polishing the code further,
reVC was started in early May 2020 by starting from re3 code,
not by starting from scratch replacing functions with a dll.
After a few months of mostly steady progress we considered reVC
finished in December.</p>
<p dir="auto">Since then we have started reLCS, which is currently work in progress.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">We don't feel like we're in a position to give this code a license.<br>
The code should only be used for educational, documentation and modding purposes.<br>
We do not encourage piracy or commercial use.<br>
Please keep derivate work open source and give proper credit.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Speech and Language Processing (3rd ed. draft) (167 pts)]]></title>
            <link>https://web.stanford.edu/~jurafsky/slp3/</link>
            <guid>39664782</guid>
            <pubDate>Mon, 11 Mar 2024 03:59:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://web.stanford.edu/~jurafsky/slp3/">https://web.stanford.edu/~jurafsky/slp3/</a>, See on <a href="https://news.ycombinator.com/item?id=39664782">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h2>Speech and Language Processing
<span size="+2">(3rd ed. draft)</span><br>
<span size="+2">
<a href="http://web.stanford.edu/people/jurafsky/">Dan Jurafsky</a> and <a href="http://www.cs.colorado.edu/~martin/">James H. Martin</a>
</span>
</h2>

<h3>
Here's our Feb 3, 2024 release!  We also expect to release Chapter 12 soon in an updated release.

<p>
Individual chapters and updated slides are below;
<a href="https://web.stanford.edu/~jurafsky/slp3/ed3bookfeb3_2024.pdf">here is a single pdf of all the chapters in the Feb 3, 2024 release!</a> <br>

</p><p> <b>Feel free to use the draft chapters and slides in your classes, the resulting feedback we get from you makes the book better!</b><br>
As always, typos and comments very welcome (just email <a href="mailto:slp3edbugs@gmail.com">slp3edbugs@gmail.com</a>
and let us know the date on the draft)!
(Don't bother reporting missing refs due to cross-chapter cross-reference problems  in the indvidual chapter pdfs, those are fixed in the full book draft)<br>
</p></h3><h5>
We've put up a <a href="https://web.stanford.edu/~jurafsky/slp3/thanks.html">list here</a> of the amazing people who have sent so many fantastic suggestions and bug-fixes for improving the book.
We are really grateful to all of you for your help, the book would not be possible without you!
</h5>

<p>
When will the whole book be finished?  Don't ask.  
<br>
</p><p>
If you need last year's Jan 2023 draft chapters, 
they are <a href="https://web.stanford.edu/~jurafsky/slp3/old_jan23/">here</a>;

<table>
<tbody><tr><td>&nbsp;</td></tr>
<tr>
<td></td>
<td><b>Chapter</b></td>
<td><b>Slides </b></td>

<!--<tr><td>&nbsp;</td></tr>-->
</tr><tr><td></td><td><b>Part I: Fundamental Algorithms</b></td></tr>

<tr><td>1:</td><td>Introduction</td>
</tr><tr><td>2:</td><td>
        <a href="https://web.stanford.edu/~jurafsky/slp3/2.pdf">Regular Expressions, Text Normalization, Edit Distance</a></td>
<td>
2: Text Processing
[<a href="https://web.stanford.edu/~jurafsky/slp3/slides/2_TextProc_2023.pptx">pptx</a>] 
[<a href="https://web.stanford.edu/~jurafsky/slp3/slides/2_TextProc_2023.pdf">pdf</a>]<br>
2: Edit Distance
[<a href="https://web.stanford.edu/~jurafsky/slp3/slides/2_EditDistance_2023.pptx">pptx</a>] 
[<a href="https://web.stanford.edu/~jurafsky/slp3/slides/2_EditDistance_2023.pdf">pdf</a>]<br>
</td>
</tr><tr><td>3:</td><td> 
        <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">N-gram Language Models</a></td>
<td>
3: [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/3_LM_2024.pptx">pptx</a>] 
[<a href="https://web.stanford.edu/~jurafsky/slp3/slides/3_LM_2024.pdf">pdf</a>]<br>
</td>
</tr><tr><td>4:</td><td> 
        <a href="https://web.stanford.edu/~jurafsky/slp3/4.pdf"> Naive Bayes, Text Classification,  and Sentiment </a></td>
<td>
4: [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/4_NB_2024.pptx">pptx</a>] 
[<a href="https://web.stanford.edu/~jurafsky/slp3/slides/4_NB_2024.pdf">pdf</a>]<br>
</td>
</tr><tr><td>5:</td><td> 
        <a href="https://web.stanford.edu/~jurafsky/slp3/5.pdf"> Logistic Regression</a></td>
<td>
5: [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/5_LR_Apr_7_2021.pptx">pptx</a>] 
[<a href="https://web.stanford.edu/~jurafsky/slp3/slides/5_LR_Apr_7_2021.pdf">pdf</a>]<br>
</td>
</tr><tr><td>6:</td><td> 
        <a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf"> Vector Semantics and Embeddings</a></td>
<td>
    6: [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/vectorsemantics2024.pptx">pptx</a>] 
[<a href="https://web.stanford.edu/~jurafsky/slp3/slides/vectorsemantics2024.pdf">pdf</a>]
</td>
</tr><tr><td>7:</td><td> 
        <a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf">Neural Networks and Neural Language Models </a>
        </td><td>
    7: [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/7_NN_Apr_28_2021.pptx">pptx</a>] [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/7_NN_Apr_28_2021.pdf">pdf</a>]
</td>
</tr><tr><td>8:</td><td> 
        <a href="https://web.stanford.edu/~jurafsky/slp3/8.pdf">Sequence Labeling for Parts of Speech and Named Entities</a></td>
<td>
    8: (Intro only) [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/8_POSNER_intro_May_6_2021.pptx">pptx</a>] [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/8_POSNER_intro_May_6_2021.pdf">pdf</a>]
</td>
</tr><tr><td>9:</td><td> 
        
        <a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">RNNs and LSTMs</a> </td>
</tr><tr><td>10:</td><td> 
        <a href="https://web.stanford.edu/~jurafsky/slp3/10.pdf">Transformers and Large Language Models</a> </td>
</tr><tr><td>11:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/11.pdf">Fine-tuning and Masked Language Models</a> </td>
</tr><tr><td>12:</td><td> 
        Prompting, In-Context Learning, and Instruct Tuning </td>

</tr><tr><td>&nbsp;</td></tr>
<tr><td></td><td><b>Part II: NLP Applications</b></td></tr>

<tr><td>13:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/13.pdf">Machine Translation</a> </td>
</tr><tr><td>14:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/14.pdf">Question Answering and Information Retrieval</a> </td>
</tr>
<tr><td>15:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/15.pdf">Chatbots and Dialogue Systems</a> </td>
    <td> 
15 [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/24_Dialogue_May_6_2021.pptx">pptx</a>] 
[<a href="https://web.stanford.edu/~jurafsky/slp3/slides/24_Dialogue_May_6_2021.pdf">pdf</a>]
    </td>
</tr>
<tr><td>16:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/16.pdf">Automatic Speech Recognition and Text-to-Speech</a></td>
</tr>

<tr><td>&nbsp;</td></tr>
<tr><td></td><td><b>Part III: Annotating Linguistic Structure</b></td></tr>

<tr><td>17:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/17.pdf">Context-Free Grammars and Constituency Parsing</a></td>
</tr><tr><td>18:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/18.pdf">Dependency Parsing </a></td>
</tr><tr><td>19:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/19.pdf">Information Extraction: Relations, Events, and Time</a></td>
</tr><tr><td>20:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/20.pdf">Semantic Role Labeling and Argument Structure</a></td>
</tr><tr><td>21:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/21.pdf">Lexicons for Sentiment, Affect, and Connotation </a></td>
</tr><tr><td>22:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/22.pdf">Coreference Resolution</a></td>
</tr><tr><td>23:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/23.pdf">Discourse Coherence</a></td>
</tr><tr><td>&nbsp;</td></tr>
<tr><td></td><td><b>Appendix Chapters (will be just on the web)</b></td></tr>
<tr><td>A:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/A.pdf">Hidden Markov Models</a> </td></tr>
<tr><td>B:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/B.pdf">Spelling Correction and the Noisy Channel</a> </td></tr>
<tr><td>C:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/C.pdf">Statistical Constituency Parsing</a> </td>
</tr><tr><td>D:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/D.pdf">Context-Free Grammars</a> </td>
</tr><tr><td>E:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/E.pdf">Combinatory Categorial Grammar</a> </td>
</tr><tr><td>F:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/F.pdf">Logical Representations of Sentence Meaning</a> </td>
</tr><tr><td>G:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/G.pdf">Word Senses and WordNet</a> </td>
</tr><tr><td>H:</td><td> <a href="https://web.stanford.edu/~jurafsky/slp3/H.pdf">Phonetics</a> </td>
<td>
</td></tr></tbody></table>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD announces the Spartan UltraScale+ FPGA family (116 pts)]]></title>
            <link>https://www.cnx-software.com/2024/03/08/amd-announces-the-spartan-ultrascale-fpga-family-for-cost-sensitive-and-io-intensive-applications/</link>
            <guid>39664221</guid>
            <pubDate>Mon, 11 Mar 2024 02:01:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnx-software.com/2024/03/08/amd-announces-the-spartan-ultrascale-fpga-family-for-cost-sensitive-and-io-intensive-applications/">https://www.cnx-software.com/2024/03/08/amd-announces-the-spartan-ultrascale-fpga-family-for-cost-sensitive-and-io-intensive-applications/</a>, See on <a href="https://news.ycombinator.com/item?id=39664221">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<p>The Spartan UltraScale+ FPGA family is the latest inclusion to AMD’s <a href="https://www.xilinx.com/products/silicon-devices/cost-optimized-portfolio.html">Cost-Optimized portfolio</a>, a series of FPGAs designed to balance cost, power, and form factor with affordability. The UltraScale+ FPGA family is designed for cost-sensitive, low-power applications requiring high I/O count and substantial security.</p>
<p><a href="https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale.jpg"><picture><source sizes="(max-width: 720px) 100vw, 720px" type="image/webp" data-srcset="https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-720x405.jpg.webp 720w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-300x169.jpg.webp 300w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-768x432.jpg.webp 768w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale.jpg.webp 1200w"><img decoding="async" title="amd spartan ultrascale+" src="https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-720x405.jpg" alt="amd spartan ultrascale+" width="720" height="405" srcset="https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-720x405.jpg 720w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-300x169.jpg 300w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-768x432.jpg 768w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale.jpg 1200w" sizes="(max-width: 720px) 100vw, 720px" data-eio="l" data-old-src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAtAAAAGVAQAAAAD/UftzAAAAAnRSTlMAAHaTzTgAAAA6SURBVHja7cEBDQAAAMKg909tDwcUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADcGo/3AAHiIfPyAAAAAElFTkSuQmCC" data-src="https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-720x405.jpg" data-srcset="https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-720x405.jpg 720w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-300x169.jpg 300w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-768x432.jpg 768w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale.jpg 1200w"></picture></a></p>
<p>Devices in the Spartan UltraScale+ family offer a high I/O to logic cell ratio for FPGAs built in 28nm and lower process technology (the highest in the industry, according to AMD), consume up to 30% less power than compared to the previous generation, and feature robust security features that outclass the rest of the Cost-Optimized portfolio.</p>
<p>This FPGA family is built on the same UltraScale+ architecture as <a href="https://www.cnx-software.com/2022/11/15/amd-unveils-low-cost-artix-ultrascale-au7p-fpga-and-zynq-ultrascale-zu3t-mpsoc/">previous Artix and Zynq products</a>. They are the first AMD UltraScale+ FPGAs to feature a hardened DDR memory controller and PCIe Gen4 x8 support, “providing both power efficiency and future-ready capabilities for customers.”</p>
<p><a href="https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram.jpg"><picture><source sizes="(max-width: 720px) 100vw, 720px" type="image/webp" data-srcset="https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram-720x566.jpg.webp 720w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram-1200x944.jpg.webp 1200w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram-300x236.jpg.webp 300w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram-768x604.jpg.webp 768w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram.jpg.webp 1280w"><img decoding="async" title="amd spartan ultrascale+ block diagram" src="https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram-720x566.jpg" alt="amd spartan ultrascale+ block diagram" width="720" height="566" srcset="https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram-720x566.jpg 720w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram-1200x944.jpg 1200w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram-300x236.jpg 300w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram-768x604.jpg 768w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram.jpg 1280w" sizes="(max-width: 720px) 100vw, 720px" data-eio="l" data-old-src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAtAAAAI2AQAAAABRMYKuAAAAAnRSTlMAAHaTzTgAAABJSURBVHja7cExAQAAAMKg9U9tDQ+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIB/A8kyAAHAeDrHAAAAAElFTkSuQmCC" data-src="https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram-720x566.jpg" data-srcset="https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram-720x566.jpg 720w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram-1200x944.jpg 1200w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram-300x236.jpg 300w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram-768x604.jpg 768w, https://www.cnx-software.com/wp-content/uploads/2024/03/amd-spartan-ultrascale-block-diagram.jpg 1280w"></picture></a></p>
<p>AMD Spartan UltraScale+ specifications:</p>
<ul>
<li aria-level="1">System Logic Cells – Up to 218K</li>
<li aria-level="1">Memory
<ul>
<li aria-level="2">On-Chip Memory: Block RAM for low latency, high throughput, and UltraRAM</li>
<li aria-level="2">External Memory: LPDDR4x and LPDDR5 up to 4266 Mb/s (hard MC), and DDR4 (soft MC) IP up to 2400 Mb/s</li>
</ul>
</li>
<li aria-level="1">I/O
<ul>
<li aria-level="2">I/O Count: 572</li>
<li aria-level="2">Types:
<ul>
<li aria-level="3">High Density I/O (HDIO) up to 3.3V,</li>
<li aria-level="3">High-Performance I/O (HPIO) up to 1.8V,</li>
<li aria-level="3">XP5IO up to 1.5V, supporting 3200 Mb/s MIPI and 1800 Mb/s LVDS</li>
</ul>
</li>
<li aria-level="2">PCIe: Gen4 x8</li>
<li aria-level="2">Transceivers – Up to 8 GTH transceivers supporting up to 16.3 Gb/s</li>
</ul>
</li>
<li aria-level="1">Security – NIST-approved post-quantum cryptography, unique device identification, permanent tamper penalty for device protection, and side-attack protection via DPA countermeasures</li>
</ul>
<p>The Spartan UltraScale+ FPGA series is primarily targeted for embedded vision, healthcare, industrial networking, robotics, and video applications according to <a href="https://ir.amd.com/news-events/press-releases/detail/1186/amd-extends-market-leading-fpga-portfolio-with-amd-spartan">the press release</a>. The high I/O count will enable the FPGAs to interface with a wide range of sensors and coupled with the programmable logic make it possible to control the sensors in real time and with low latency.</p>
<p>On the software end, the Spartan UltraScale+ FPGA family (along with the rest of AMD’s FPGA and adaptive SoCs lineup) is supported by AMD’s Vivado Design Suite and Vitis Unified Software Platform. This allows both hardware and software designers to leverage the “productivity benefits of these tools via a single designer cockpit from design to verification.”</p>
<p>AMD Spartan UltraScale+ samples and evaluation kits are expected to be available for sampling and evaluation in the first half of 2025. Documentation is currently available and tools support starting with the AMD Vivado Suite in Q4 2024. To learn more about the Spartan UltraScale+ FPGA family and what it offers, be sure to visit the <a href="https://www.xilinx.com/products/silicon-devices/fpga/spartan-ultrascale-plus.html">product page.</a></p>
<div itemtype="http://schema.org/Person" itemscope="" itemprop="author"><p><img decoding="async" src="https://www.cnx-software.com/wp-content/uploads/2023/12/photo_2022-08-11_21-23-31-e1701859611854.jpg" width="100" height="100" alt="" itemprop="image" data-eio="l" data-old-src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAApYAAAL2AQAAAADTfev9AAAAAnRSTlMAAHaTzTgAAABUSURBVHja7cEBDQAAAMKg909tDwcUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPwY+LgAAe5KAhEAAAAASUVORK5CYII=" data-src="https://www.cnx-software.com/wp-content/uploads/2023/12/photo_2022-08-11_21-23-31-e1701859611854.jpg"></p><div><p>Tomisin is a writer specializing in hardware product reviews, comparisons, and explainers. He is very passionate about small form factor and single-board computers.</p></div></div>			<!-- JLA - Donations -->
		<p><strong>Support CNX Software! Donate via <a href="https://www.cnx-software.com/donate-cryptocurrencies/" rel="nofollow noopener">cryptocurrencies</a>, <a href="https://www.patreon.com/cnxsoft" target="_blank" rel="nofollow noopener">become a Patron</a> on Patreon, or purchase goods on <a href="https://amzn.to/3SXubZ0" rel="nofollow">Amazon</a> or <a href="https://s.click.aliexpress.com/e/_DmGIIRT" rel="nofollow">Aliexpress</a></strong></p>
		</div></div>]]></description>
        </item>
    </channel>
</rss>