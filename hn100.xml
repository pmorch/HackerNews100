<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 19 Sep 2023 02:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Striking auto workers want a 40% pay increase–the same rate their CEOs’ pay grew (300 pts)]]></title>
            <link>https://www.cnbc.com/2023/09/15/striking-uaw-auto-workers-want-a-40percent-pay-increase-how-much-they-make-now.html</link>
            <guid>37563231</guid>
            <pubDate>Mon, 18 Sep 2023 22:52:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2023/09/15/striking-uaw-auto-workers-want-a-40percent-pay-increase-how-much-they-make-now.html">https://www.cnbc.com/2023/09/15/striking-uaw-auto-workers-want-a-40percent-pay-increase-how-much-they-make-now.html</a>, See on <a href="https://news.ycombinator.com/item?id=37563231">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="MakeItRegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-1" data-analytics="MakeItRegularArticle-articleBody-5-1"><div><p>Thousands of United Auto Workers members are officially on strike after three Detroit automakers failed to reach an agreement with the union, which represents about 146,000 workers at Ford, GM and Stellantis, by a Thursday night deadline, <a href="https://www.cnbc.com/2023/09/14/uaw-strikes-ford-gm-stellantis.html">CNBC reports</a>.</p><p>One major issue on the table is worker pay. The union proposed 40% hourly pay increases over the next four years. The average U.S. autoworker on a manufacturing production line earns about $28 per hour as of August, according to data from the Bureau of Labor Statistics. That's up $1 from the previous year.</p><p>Autoworker pay at "The Big Three" works on a tiered system, which was introduced in the aftermath of the 2008 auto industry crisis, where more recent hires start at lower rates of pay than more tenured workers.</p><p>Top-tier workers (those hired in 2007 or earlier) earn an average of $33 per hour, <a href="https://www.cbsnews.com/news/how-much-do-uaw-workers-make/" target="_blank">CBS News reports</a>, based on contract summaries for the Big Three. Lower-tier workers (hired after 2007) earn up to $17 an hour.</p><p>Nationwide, autoworkers' average real hourly earnings has fallen 19.3% since 2008, <a href="https://www.epi.org/blog/uaw-automakers-negotiations/" target="_blank">according to research</a> from the left-leaning Economic Policy Institute.</p><p>Meanwhile, Ford CEO Jim Farley earned $21 million in total compensation last year, the Detroit News&nbsp;<a href="https://www.detroitnews.com/story/business/autos/ford/2023/03/31/heres-how-much-ford-ceo-jim-farley-made-last-year/70067820007/#:~:text=Ford%20Motor%20Co.,nearly%20%242.8%20million%20in%20bonuses." target="_blank">reported</a>. Stellantis CEO Carlos Tavares made $24.8 million,&nbsp;<a href="https://www.freep.com/story/money/cars/chrysler/2023/02/25/stellantis-ceo-carlos-tavares-2022-compensation-sec-mike-manley/69942226007/" target="_blank">according</a>&nbsp;to the Detroit Free Press. And GM CEO Mary Barra earned&nbsp;nearly $29 million in 2022 pay, Automotive News&nbsp;<a href="https://www.autonews.com/executives/gm-ceo-mary-barras-2022-compensation-29m" target="_blank">reported</a>.&nbsp;</p><p>"Obviously, CEOs should be the highest-paid person in an enterprise, but then the question is exactly just how much higher than everyone else," Josh Bivens, chief economist at EPI, <a href="https://www.npr.org/2023/09/13/1199168485/uaw-points-to-disparity-between-ceo-and-worker-pay-as-a-reason-for-wage-hike-dem" target="_blank">told NPR</a>.</p><p>CEO pay at the Big Three has grown 40% in the last decade, according to EPI — in line with the UAW's demands for 40% pay increases for autoworkers.</p><p>UAW President Shawn Fain <a href="https://www.cnbc.com/2023/09/13/where-uaw-negotations-stand-ahead-of-likely-strikes.html">said Wednesday</a> Ford has offered a 20% increase over the four years of the deal, followed by GM at 18% and Stellantis at 17.5%. GM raised their offer <a href="https://www.cnbc.com/2023/09/14/uaw-strike-gm-sweetens-wage-and-benefits-offer-to-workers.html">Thursday</a> to a 20% wage increase.</p><p>"I'm extremely frustrated and disappointed," Barra <a href="https://www.cnbc.com/video/2023/09/15/gm-ceo-mary-barra-on-uaw-strike-we-put-a-historic-offer-on-the-table.html">told CNBC Friday morning</a>. "We don't need to be in strike right now. We put a historic offer on the table."</p></div><div id="Placeholder-ArticleBody-Video-107301432" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000315537" aria-labelledby="Placeholder-ArticleBody-Video-107301432"><p><img src="https://image.cnbcfm.com/api/v1/image/107301433-16947802881694780285-31206468990-1080pnbcnews.jpg?v=1694780657&amp;w=750&amp;h=422&amp;vtcrop=y" alt="GM CEO Mary Barra on UAW strike: We put a historic offer on the table"><span></span><span><span data-test="PlayButton"></span></span></p></div><div><p>Profits at the struck auto companies increased 92% from 2013 to 2022, totaling $250 billion, according to EPI. Striking workers say they haven't shared in their company's financial success.</p><p>President Joe Biden <a href="https://www.cnbc.com/2023/09/15/biden-says-record-profits-should-ensure-record-contracts-as-uaw-strikes-ford-gm-and-stellantis-plants.html">weighed in on negotiations</a> Friday, saying, "Auto companies have seen record profits including the last few years because of the extraordinary skill and sacrifices of the UAW workers. Those record profits have not been shared fairly, in my view, with the workers."</p></div><h2><a id="headline0"></a>CEO pay growth outpaces worker wages across industries</h2><div><p>The auto industry is just one example of how executive pay has skyrocketed faster than the typical wage growth for everyday workers.</p><p>The average CEO at a top U.S. company was paid $27.8 million in 2021, including stock awards — 399 times as much as the typical worker — according to research&nbsp;<a href="https://www.epi.org/publication/ceo-pay-in-2021/" target="_blank">published by EPI</a>. From 1978 to 2021, CEO pay grew by 1,460%, adjusted for inflation, versus just 18.1% for the typical worker.</p><p><a href="https://www.cnbc.com/2023/09/13/where-uaw-negotations-stand-ahead-of-likely-strikes.html">The UAW also proposed</a> the elimination of compensation tiers and a restoration of cost-of-living adjustments, as well as other workplace protections like a reduced 32-hour workweek, a shift back to traditional pensions, improved retiree and parental leave benefits, and more.</p><p>"For the first time in our history, we will strike all three of the 'Big Three' at once," Fain said&nbsp;Thursday in live remarks streamed on&nbsp;<a href="https://www.facebook.com/uaw.union/videos/1047762633322736" target="_blank">Facebook&nbsp;</a>and YouTube. "We are using a new strategy, the 'stand-up' strike. We will call on select facilities, locals or units to stand up and go on strike."</p><p>About 12,700 workers will be on strike at three facilities nationwide, starting at GM's plant in Wentzville, Missouri; Ford's plant in Wayne, Michigan; and Stellantis' plant in Toledo, Ohio.</p><p>The targeted strikes aim to bring a work stoppage to key plants that then cause plants further down the line to stop production without needed materials. The strategy is unprecedented: The union may increase the number of strikes based on the status of negotiations.</p><p><em><strong>Want to be smarter and more successful with your money, work &amp; life?&nbsp;</strong></em><a href="https://www.cnbc.com/makeitnewsletter/?__source=makeit%7Cnlarticleteaser"><em><strong>Sign up for our new newsletter</strong></em></a><em><strong>!</strong></em></p><p><em>Want to earn more and land your dream job?&nbsp;</em><a href="https://www.cnbcevents.com/cnbc-make-it-your-money-livestream-2023/?utm_source=DotCom&amp;amp;utm_medium=ArticleMakeIt&amp;amp;utm_campaign=MIYM" target="_blank"><em><strong>Join the free CNBC Make It: Your Money virtual event</strong></em></a><em>&nbsp;on Oct. 17 at 1 p.m. ET to learn how to level up your interview and negotiating skills, build your ideal career, boost your income and grow your wealth.&nbsp;</em><a href="https://www.cnbcevents.com/cnbc-make-it-your-money-livestream-2023/?utm_source=DotCom&amp;amp;utm_medium=ArticleMakeIt&amp;amp;utm_campaign=MIYM" target="_blank"><em>Register</em></a><em>&nbsp;for free today.</em></p><p><em><strong>Check out: </strong></em><a href="https://www.cnbc.com/2023/08/09/how-hollywood-writers-make-ends-meet-100-days-into-the-writers-guild-strike.html"><em><strong>‘Survival jobs,’ ex-careers and side hustles: How Hollywood writers are making ends meet 100 days into the strike</strong></em></a></p></div><div id="Placeholder-ArticleBody-Video-107288090" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000312296" aria-labelledby="Placeholder-ArticleBody-Video-107288090"><p><img src="https://image.cnbcfm.com/api/v1/image/107288231-thumb1_1-2.jpg?v=1692366169&amp;w=750&amp;h=422&amp;vtcrop=y" alt="How a 22-year-old earning $77,000 as a car detailer in West Palm Beach spends his money"><span></span><span><span data-test="PlayButton"></span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Some new snippets from the Snowden documents (125 pts)]]></title>
            <link>https://www.electrospaces.net/2023/09/some-new-snippets-from-snowden-documents.html</link>
            <guid>37562225</guid>
            <pubDate>Mon, 18 Sep 2023 21:20:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.electrospaces.net/2023/09/some-new-snippets-from-snowden-documents.html">https://www.electrospaces.net/2023/09/some-new-snippets-from-snowden-documents.html</a>, See on <a href="https://news.ycombinator.com/item?id=37562225">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-1181374685428086776" itemprop="description articleBody">
<p><span size="2" color="gray">(Updated: September 16, 2023)</span></p>
<p>
It's been more than four years since the <a href="https://theintercept.com/2019/05/29/nsa-data-afghanistan-iraq-mexico-border/" target="_blank">last regular publication</a> of documents from the Snowden trove. Last year, however, some new snippets of information from the Snowden documents appeared in the PhD thesis of hacktivist <a href="https://en.wikipedia.org/wiki/Jacob_Appelbaum" target="_blank">Jacob Appelbaum</a>.</p><p>

The <a href="#new">new information</a> isn't very spectacular and also quite specialistic, but still worth to make it more easily accessible. Also for the record I added some <a href="#remarks">corrections and additions</a> to Appelbaum's discussion of NSA surveillance methods.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCNgdUg8Rgq3ERaS2lzi8Sve5oEwG7rZVOMXK6umGqydzrOxSplgMaEF2q3Alvht9q6ySJrYJ1j-PY45fLFzijvld1MUVbk1Ts1JvbFM0T0jjrOguBhqGG3U9rWAyFHhGaFWIFlaIyhR3DB0E4vKmy__3cs0tscozNYUNLB8Pd2smvgBg09AMpIHbB/s800/appelbaum-thesis-header.jpg" target="_blank"><img alt="" data-original-height="420" data-original-width="800" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgCNgdUg8Rgq3ERaS2lzi8Sve5oEwG7rZVOMXK6umGqydzrOxSplgMaEF2q3Alvht9q6ySJrYJ1j-PY45fLFzijvld1MUVbk1Ts1JvbFM0T0jjrOguBhqGG3U9rWAyFHhGaFWIFlaIyhR3DB0E4vKmy__3cs0tscozNYUNLB8Pd2smvgBg09AMpIHbB/s600/appelbaum-thesis-header.jpg" width="620"></a></p>
<p><span size="2">
    NSA headquarters - Appelbaum's thesis - Eindhoven University of Technology<br>
  </span>
</p>
<p><span size="+2"><b>Jacob Appelbaum</b></span></p><p>

Jacob R. Appelbaum was born in 1983 in California and became a well-known hacker and activist for digital anonymity. He was a member of the <a href="https://en.wikipedia.org/wiki/Cult_of_the_Dead_Cow" target="_blank">Cult of the Dead Cow</a> hacker collective and a core member of the <a href="https://en.wikipedia.org/wiki/Tor_%28network%29" target="_blank">Tor project</a>, which provides a tool for anonymous internet communications.</p><p>

In 2012, Appelbaum moved to Berlin, where he worked closely with Laura Poitras on the NSA documents which she had received from Edward Snowden in May and June 2013. However, he was also involved in the story about the eavesdropping on German chancellor Merkel and the publication of the NSA's <a href="https://en.wikipedia.org/wiki/ANT_catalog" target="_blank">ANT Product Catalog</a>.</p><p>

In both cases the documents were not attributed to Snowden and <a href="https://www.schneier.com/blog/archives/2023/06/snowden-ten-years-later.html" target="_blank">apparantly came</a> from a still unidentified "second source". In his thesis, Appelbaum seems to refer to this source when he mentions "documents exposed by whistleblowers, known and unknown, or other anonymous insiders."</p>
<p>
In 2015, several women accused Appelbaum of sexual abuse and he subsequently lost his position at the Tor project and various other organizations. Appelbaum denied the allegations, but an investigation ordered by the Tor project <a href="https://web.archive.org/web/20190820150807/https://www.nytimes.com/2016/07/28/technology/tor-project-jacob-appelbaum.html" target="_blank">determined</a> that they appeared to be true.</p><p>

Meanwhile Appelbaum had moved to The Netherlands, where he started as a PhD student at the Eindhoven University of Technology (TU/e). There he finished his thesis and received his PhD on March 25, 2022. Currently he <a href="https://research.tue.nl/en/persons/jacob-r-appelbaum" target="_blank">works as a postdoc</a> at the <a href="https://www.win.tue.nl/cc/" target="_blank">Coding Theory and Cryptology</a> group at TU Eindhoven.</p><p><span size="+2"><b>Appelbaum's PhD thesis</b></span></p><p>


The full title of Appelbaum's thesis is "<i>Communication in a world of pervasive surveillance. Sources and methods: Counter-strategies against pervasive surveillance architecture</i>". His promotors were prof.dr. Mark van den Brand, prof.dr. Daniel J. Bernstein and prof.dr. Tanja Lange.</p><p>

The thesis was published on March 25, 2022 and became available for <a href="https://pure.tue.nl/ws/portalfiles/portal/197416841/20220325_Appelbaum_hf.pdf" target="_blank">download</a> as a 24.3 MB pdf-document on September 27, 2022. The contents of this 327-page thesis are as follows:</p><p>

- <b>Chapter 1</b>: Introduction.</p><p>

- <b>Chapter 2</b>: Background on network protocols common to all research.</p><p>

- <b>Chapter 3</b>: Background on cryptography common to all research.</p><p>

- <b>Chapter 4</b>: Review of historical, political, economic, and technical adversarial capabilities (including previously published leaked documents that are from works which Appelbaum has written about in his role as a journalist).</p><p>
 
- <b>Chapter 5</b>: Review of the Domain Name System and an explanation of alternative methods to improve the security and privacy of domain name lookups.</p><p>

- <b>Chapter 6</b>: Examination of a tweak to the WireGuard VPN protocol to protect historic encrypted traffic against future attacks by quantum computers.</p><p>

- <b>Chapter 7</b>: Introduces the Vula protocol, which is a suite of free software tools for automatically protecting network traffic between hosts in the same Local Area Network.</p><p>

- <b>Chapter 8</b>: Introduces REUNION, a privacy-preserving rendezvous protocol. </p><p>


In the preface, Appelbaum writes that his thesis is the culmination of more than a decade of research into the topic of surveillance. He expresses a political and activist aim by saying that the "machinery of mass surveillance is simply too dangerous to be allowed to exist" and that "we must use all of the tools in our toolbox – economic, social, cultural, political, and of course, cryptographic – to blind targeted and mass surveillance."</p><p>

He says more has to be done than simply criticize surveillance practices. Cryptography for example, "allows for resistance in a non-violent manner to the benefit of everyone except the ones who are spying on us." From this perspective Appelbaum's thesis discusses various cryptographic implementations to "protect individual liberty, while aspiring to a broader goal of achieving societal liberty."</p><p><span size="+2"><b>New information from the Snowden documents</b></span></p><p>

Throughout his thesis, Appelbaum reveals some new information from Snowden documents that has not been published, but which he had access to during his research that resulted in various publications in media outlets like Der Spiegel, NDR and Le Monde. The new information is only described, so no new original documents were released.</p><p>

According to Appelbaum: "Many journalists who have worked on the Snowden archive know significantly more than they have revealed in public. It is in this sense that the Snowden archive has almost completely failed to create change: many of the backdoors and sabotage unknown to us before 2013 is still unknown to us today." </p><p><span color="gray">(page 71)</span></p><p>

Appelbaum also provides some new information about the Snowden documents in general, by saying that The Intercept "closed their Snowden archive and reportedly it has been destroyed." </p><p><span color="gray">(page 63, note 17)</span></p>

<p><span size="2">
Below, I provide exact quotes from Appelbaum's thesis, including his sources, which are in square brackets, while I added some additional links for further information.<br>
</span></p><p>

<b>1. BULLRUN: manipulating protocol security</b></p><p>

"How do they accomplish their goals with project <a href="https://en.wikipedia.org/wiki/Bullrun_%28decryption_program%29" target="_blank">BULLRUN</a>? One way is that United States National Security Agency (NSA) participates in Internet Engineering Task Force (IETF) community protocol standardization meetings with the explicit goal of sabotaging protocol security to enhance NSA surveillance capabilities." "Discussions with insiders confirmed what is claimed in as of yet unpublished classified documents from the Snowden archive and other sources." </p><p><span color="gray">(page 6-7, note 8)</span></p><p>



<b>2. Selecting entropic internet traffic</b></p><p>

"There are various rules governing what is <i>selected</i> for long-term data retention in [the NSA's] <i>corporate repositories</i>. One example is that some traffic which is considered <a href="https://en.wikipedia.org/wiki/Entropy_%28information_theory%29" target="_blank">entropic</a> by a standard Shannon Entropy estimate is <i>selected</i> from the network in real time and saved to a database, preserving it for cryptanalysis using future technology." "This statement is based in part on an analysis of as of yet unpublished <a href="https://en.wikipedia.org/wiki/XKeyscore" target="_blank">XKeyscore</a> source code that performs a Shannon Entropy estimate. Some kinds of Internet traffic that is considered entropic is recorded for later analysis." </p><p><span color="gray">(page 9, note 16)</span></p><p>


  
<b>3. Compromised lawful interception systems</b></p><p>
  
"As part of our research, we uncovered evidence that the telecommunications infrastructure in many countries has been compromised by intelligence services. The Snowden archive includes largely unpublished internal NSA documents and presentations that discuss targeting and exploiting not only deployed, live interception infrastructure, but also the vendors of the hardware and software used to build the infrastructure. Primarily these documents remain unpublished because the journalists who hold them fear they will be considered disloyal or even that they will be legally punished. Only a few are available to read in public today." </p><p><span color="gray">(page 41)</span></p><p>

"Targeting lawful interception (LI) equipment is a
<a href="https://www.eff.org/nl/document/20150928-intercept-exploiting-foreign-law-intercept-roundtablepdf" target="_blank">known goal</a> of the NSA. Unpublished NSA documents specifically list their compromise of the Russian SORM LI infrastructure as an NSA success story of compromising civilian telecommunications infrastructure to spy on targets within reach of the Russian <a href="https://en.wikipedia.org/wiki/SORM" target="_blank">SORM</a> system." </p><p><span color="gray">(page 41)</span></p><p>

"The NSA slides have "you talk, we listen" written in Cyrillic on the jackets of two Russian officers." "Review of unpublished Snowden documents about NSA’s activities compromising deployed, lawful interception systems and as well as additional success against the vendors of such hardware or software. Needless to say, a compromised interception system is anything but lawful in the hands of an adversary."  </p><p><span color="gray">(page 41, note 4)</span></p><p>


  
<b>4. Compromised computer hardware</b></p><p>

"While working on documents in the Snowden archive the thesis author learned that an American fabless semiconductor CPU vendor named <a href="https://en.wikipedia.org/wiki/Cavium" target="_blank">Cavium</a> is listed as a successful SIGINT "enabled" CPU vendor. By chance this was the same CPU present in the thesis author's Internet router (UniFi USG3). The entire Snowden archive should be open for academic researchers to better understand more of the history of such behavior." </p><p><span color="gray">(page 71, note 21)</span></p><p>


  
<b>5. PRISM</b></p><p>

"The PRISM slide deck was not published in full, and the public does not fully understand aspects of the program such as the retrieval of voice
content data as seen in <a href="https://en.wikipedia.org/wiki/PRISM#/media/File:Prism-slide-7.jpg" target="_blank">Figure 4.24</a>. Domains hosted by PRISM partners are also subject to selector based surveillance. Several pages of the PRISM slides list targets and related surveillance data, and a majority of them appear to be a matter of political surveillance rather than defense against terrorism. One example that is not well-known except among the journalists who had access to the full PRISM slide deck is the explicit naming of targets. An example shows a suggestion for targeting of the Tibetan Government in Exile through their primary domain name. The tibet.net domain is named as an unconventional example that analysts should be aware of as also falling under the purview of PRISM. The email domain was
hosted by Google Mail, a PRISM partner, at the time of the slide deck creation and it is still currently hosted by Google Mail as of early 2022." </p><p><span color="gray">(page 76)</span></p>

  
<p><b>6. MYSTIC: Country X</b></p><p>

"<a href="https://en.wikipedia.org/wiki/MYSTIC" target="_blank">MYSTIC</a> was revealed to impact a number of countries by name at the time of publication: the Bahamas, Mexico, the Philippines, Kenya and one
mystery country: country X. The Bahamas, and country X are subject to SOMALGET full take data and voice collection. The publisher WikiLeaks observed that the monitoring of an entire country of people is a crime when done by outside parties, essentially an act of war by the surveillance adversary. WikiLeaks then <a href="https://wikileaks.org/WikiLeaks-statement-on-the-mass.html" target="_blank">revealed</a> that the country in question, Country X, was Afghanistan [<a href="https://cryptome.org/2014/05/nsa-mystic-identity.pdf" target="_blank">Yea14</a>]. Through independent review of the Snowden archive, we confirm that this is the identity of Country X, and that WikiLeaks was correct in their claim." </p><p><span color="gray">(page 78)</span></p><p><span size="2">
(Strangely enough, the source provided by Appelbaum ("<a href="https://cryptome.org/2014/05/nsa-mystic-identity.pdf" target="_blank">Yea14</a>") actually shows that already four days <i>before</i> Wikileaks' revelation, collaborative analysis by Paul Dietrich and the author of this weblog had already pointed to Afghanistan as being Country X. In his bibliography, Appelbaum attributes this source document to "John Young and et al." (the owners of the Cryptome website), while it was actually written by and first published on the <a href="https://web.archive.org/web/20160424142120/https://members.efn.org/~paulmd/" target="_blank">blog</a> of Paul Dietrich)<br>
  </span></p><p>


<b>7. Manipulation of DUAL_EC_DRBG</b></p><p>
  
"Many documents released in public from the Snowden archive and additional documents which are still not public make clear that this type of bug is being exploited at scale with help from NSA’s surveillance infrastructure. It is still unclear who authored the changes at Juniper and if bribery from the NSA was involved as with RSA’s deployment of <a href="https://en.wikipedia.org/wiki/Dual_EC_DRBG" target="_blank">DUAL_EC_DRBG</a> to their customers as is discussed in Section 4.4." </p><p><span color="gray">(page 81)</span></p><p>



<b>8. Software backdoors</b></p><p>

"Example from the Snowden Archive of an as of yet unreleased backdoor in fielded software that is most certainly not an exclusively exploitable backdoor by NSA. The software’s secret key generation is sabotaged by design to ensure surveillance of the community of interest. There is a corresponding <a href="https://en.wikipedia.org/wiki/XKeyscore" target="_blank">XKeyscore</a> rule that has not yet been published. The goal of that rule is to gather up all ciphertext using this sabotaged system;
it is clearly part of a larger strategy. As a flag in the ground for later, the thesis author presents the following SHA256 hash: [...]. There are additional examples from other sources that this is the general shape of the game being played with more than a few acts of sabotage by the NSA." </p><p><span color="gray">(page 83, note 27)</span></p><p><span size="+2"><b>Some corrections and additions</b></span></p><p>

Chapter 4 of Appelbaum's thesis is about the "adversary" and describes a wide range of digital surveillance methods which are used by intelligence agencies. He writes a little a bit about the capabilities of Russia and China, but the biggest part is about the methods of the NSA as revealed through the Snowden documents.</p><p> 

In general, this chapter is very similar to for example Glenn Greenwald's book <i>No Place to Hide</i> and Snowden's memoir <i>Permanent Record</i> as it reads like a one-sided accusation against the NSA without much context or the latest information. Chapter 4 also contains small errors which could easily have been prevented. Here I will discuss some examples:</p><p>



- Page 20, note 12: "An example is Suite-A cryptography or Type-1 cryptography, so designated by the NSA. The NSA now calls this the Commercial National Security Algorithm Suite (CNSA)"</p><p>

&gt; Comment: Actually CNSA isn't the new name for the highly secure <a href="https://en.wikipedia.org/wiki/NSA_Suite_A_Cryptography" target="_blank">Suite A</a>, but for the less secure <a href="https://en.wikipedia.org/wiki/NSA_Suite_B_Cryptography" target="_blank">Suite B</a> algorithms.</p><p>



- Page 41: "The BND and the CIA held secret co-ownership of <a href="https://en.wikipedia.org/wiki/Crypto_AG" target="_blank">CryptoAG</a> until 1993, and then the CIA held sole ownership until 2018. The devices were vulnerable by design, which allowed unaffiliated intelligence services, such as the former USSR’s KGB, and the East German Ministry for State Security [MfS], to independently exploit CryptoAG’s intentional flaws." </p><p>

&gt; Comment: This exploitation by the KGB and the MfS was apparently suggested in a German television report, based upon claims by a former Stasi officer, but so far there are no documents that support this claim. See for more information: <a href="https://www.cryptomuseum.com/intel/cia/rubicon.htm" target="_blank">Operation RUBICON</a>.</p><p>



- Page 41: "It does not appear that those party to the Maximator alliance are using their agreement and relative positions to spy on the entire
planet – in stark contrast to the Five-Eyes agreement." </p><p>

&gt; Comment: The <a href="https://www.electrospaces.net/2014/09/nsas-foreign-partnerships.html#2ndparty">Five Eyes</a> and especially NSA and GCHQ have massive capabilities, but spying on "the entire planet" is still rather exaggerated: their collection efforts are limited by <a href="https://www.electrospaces.net/2014/09/nsas-strategic-mission-list.html">national priorities</a>, the locations of where they can access satellite and cable traffic, as well as by technical constraints. While the five members of the European Maximator alliance have/had much smaller capabilities, they could nonetheless intercept and decrypt diplomatic communications from over 60 countries where the weakened encryption devices from Crypto AG were used (see the map below).</p>

<p><a href="https://3.bp.blogspot.com/-Xv7ybswhZMM/XsHbwwcG5dI/AAAAAAAAEkY/TjYYWX26Y34mtll1UbsFEV9YaC1uI7bBgCLcBGAsYHQ/s1600/crypto%2Bag%2Bcustomers.JPG" imageanchor="1" target="_blank"><img src="https://3.bp.blogspot.com/-Xv7ybswhZMM/XsHbwwcG5dI/AAAAAAAAEkY/TjYYWX26Y34mtll1UbsFEV9YaC1uI7bBgCLcBGAsYHQ/s1600/crypto%2Bag%2Bcustomers.JPG" width="500"></a><br>
<span size="2">
The countries that bought and used manipulated Crypto AG devices<br>
<span color="gray">(graphic: The Washington Post - click to enlarge)</span><br>
</span>
</p>
<p>


- Page 47, note 8: "Narus mass surveillance and analysis systems were deployed by the NSA inside AT&amp;T facilities to intercept all
traffic flowing through their large capacity network cables as documented [<a nohref="" title="Mark Klein and James Bamford. Wiring Up the Big Brother Machine–and Fighting it. BookSurge, 2009">KB09</a>] by whistleblower Mark Klein." </p><p>

&gt; Comment: This suggests that the NSA is intercepting American communications, but actually this is part of <a href="https://www.electrospaces.net/2015/09/nsas-legal-authorities.html#upstream">Upstream collection</a>, which is aimed at foreign targets and therefore the NSA <a href="https://www.documentcloud.org/documents/4552325-SSO-NEWS-Relevant-Entries" target="_blank">applies</a> various filter systems to select traffic from countries of interest and discard purely domestic communications.</p>
<p>

- Page 52: "The Foreign Intelligence Surveillance Court (FISC) is largely considered to rubber stamp requests from the FBI. The FBI has routinely misled the FISC, and from the little that is known, the FISC has neither the technical knowledge, nor the general temperament to actually act as a safeguard" </p><p>

&gt; Comment: Since the start of the Snowden revelations, numerous Top Secret documents from the FISC have been <a href="https://icontherecord.tumblr.com/tagged/fisc" target="_blank">declassified</a>, showing that the court examines the NSA's activities in great detail. The idea of being a "rubber stamp" is based upon the fact that the FISC denies just 0.5% of the applications, but later it became clear that American criminal courts <a href="https://www.emptywheel.net/2017/06/28/confirmed-the-fisa-court-is-less-of-a-rubber-stamp-than-title-iii-courts/" target="_blank">only deny</a> a tiny 0.06% of the requests for regular (so-called <a href="https://bja.ojp.gov/program/it/privacy-civil-liberties/authorities/statutes/1284" target="_blank">Title III</a>) wiretaps.</p><p>


    
- Page 53: "The CIA meanwhile, operates their own surveillance capabilities including capabilities that are entirely outside of the
purview of the FISC, even now [<a href="https://www.wyden.senate.gov/news/press-releases/wyden-and-heinrich-newly-declassified-documents-reveal-previously-secret-cia-bulk-collection-problems-with-cia-handling-of-americans-information" target="_blank">cia22</a>]." </p><p>

&gt; Comment: At least one of these cases is about the CIA's use of bulk datasets with financial information, which can of course contain information about Americans, but when the CIA obtained them in ways other than by intercepting communications, the FISC simply has no jurisdiction. It's up to lawmakers to impose privacy safeguards for creating and exchanging such bulk datasets. </p><p>


    
- Page 56: "In the Snowden archive, we see lots of hacking and hacking related programs run by NSA, such as the TURBULENCE [<a href="https://en.wikipedia.org/w/index.php?title=Turbulence_%28NSA%29&amp;oldid=1026069496" target="_blank">Wik21u</a>] program which is made up of modular sub programs [<a href="https://archive.ph/gAojF" target="_blank">Amb13</a>]. Those programs include TURMOIL [<a href="https://arstechnica.com/information-technology/2014/03/nsas-automated-hacking-engine-offers-hands-free-pwning-of-the-world/" target="_blank">Gal14b</a>], TUTELAGE [<a href="https://www.spiegel.de/international/world/new-snowden-docs-indicate-scope-of-nsa-preparations-for-cyber-battle-a-1013409.html" target="_blank">AGG+15a</a>], TURBINE [<a href="https://theintercept.com/2014/03/12/nsa-plans-infect-millions-computers-malware/" target="_blank">GG14</a>, <a href="https://en.wikipedia.org/w/index.php?title=TURBINE_%28US_government_project%29&amp;oldid=950962842" target="_blank">Wik20d</a>], TRAFFICTHIEF [<a href="https://en.wikipedia.org/w/index.php?title=TRAFFICTHIEF&amp;oldid=986162796" target="_blank">Wik20c</a>], and XKeyscore [<a href="https://www.theguardian.com/%20world/2013/jul/31/nsa-top-secret-program-online-data" target="_blank">Gre13d</a>, <a href="https://www.spiegel.de/media/6442ce11-0001-0014-0000-000000034757/media-34757.pdf" target="_blank">Unk13</a>, <a href="https://daserste.ndr.de/panorama/xkeyscorerules100.txt" target="_blank">AGG+14b</a>, <a href="https://nsa.gov1.info/dni/xkeyscore.html." target="_blank">Unk15a</a>] as shown in Figure 4.12 and Figure <a href="https://commons.wikimedia.org/wiki/File:Xkeyscore-worldmap.jpg" target="_blank">4.13</a>, as well as data that was pilfered during those break-ins." </p><p>

&gt; Comment: This suggests that TURBULENCE and its sub-programs are about hacking operations, but actually, TURBULENCE is <a href="https://grid.glendon.yorku.ca/items/show/156" target="_blank">defined</a> as "a next generation mission environment that <a href="https://theintercept.com/document/2018/06/25/sso-dictionary-relevant-entries/" target="_blank">created</a> a unified system for MidPoint and Endpoint SIGINT", or in other words, an overarching framework for bulk and targeted tapping systems. Only the TURBINE sub-program can automatically <a href="https://arstechnica.com/information-technology/2014/03/nsas-automated-hacking-engine-offers-hands-free-pwning-of-the-world/" target="_blank">trigger</a> the implantation of malware into target computer systems. Furthermore, none of the sources mentioned in the thesis say that XKEYSCORE is a sub-program of TURBULANCE and XKEYSCORE is not a hacking tool either. A detailed explanation of the TURBULENCE system is given in an <a href="https://robert.sesek.com/2014/9/unraveling_nsa_s_turbulence_programs.html" target="_blank">article</a> by Robert Sesek, which was apparently not consulted by Appelbaum.</p><p>



- Page 72: "US-984XN is the classified <a href="https://www.electrospaces.net/p/sigint.html">SIGAD</a> while the program name PRISM is unclassified"</p><p>

&gt; Comment: There are no indications that "PRISM" is less secret than any other coverterm which the NSA uses for its collection, processing and analysis programs. That was likely also the reason that the big internet companies involved in this program initially <a href="https://www.theguardian.com/world/2013/jun/06/us-tech-giants-nsa-data" target="_blank">denied</a> that they had ever heard of something called PRISM.</p><p>


- Page 91: "the NSA's Equation Group (EQGRP), which was later renamed Tailored Access Operations (TAO)"</p><p>

&gt; Comment: The name <a href="https://en.wikipedia.org/wiki/Equation_Group" target="_blank">Equation Group</a> was actually <a href="https://web.archive.org/web/20150217023145/https://securelist.com/files/2015/02/Equation_group_questions_and_answers.pdf" target="_blank">coined</a> in February 2015 by the Russian cybersecurity firm Kaspersky for "one of the most sophisticated cyber attack groups in the world". Later on it <a href="https://arstechnica.com/information-technology/2015/02/how-omnipotent-hackers-tied-to-the-nsa-hid-for-14-years-and-were-found-at-last/" target="_blank">became clear</a> that this group was part of the NSA's hacking division TAO.</p><p>


Given how many aspects of the NSA's operations Appelbaum mentions in chapter 4 of his thesis, one could say that it's inevitable that some mistakes are made and some sloppiness occurs. On the other hand, however, this is an academic publication for which the highest standards of accuracy should apply.  </p><p>


  
Finally, Appelbaum's activism is illustrated by the back cover of his thesis, which shows a logo very similar to that of the German terrorist organization <a href="https://en.wikipedia.org/wiki/Red_Army_Faction" target="_blank"><i>Rote Armee Fraktion</i></a> (RAF) from the 1970s, except that the original image of an AK-45 is replaced by that of a computer keyboard:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiy4lkUk-akfN2uYBkPt5xPKu8TAXreBfRyr4aFDGMzIf6PLadDxdeccI5Aq8OcS8gKsuaiI_H0PwwIEpGf5QV2CLvUj3yXrM4DvNWq8SN8e-STsA-4uih8Ky2f8Fy7FaehIBiYNWrCMToJcpgdMuYdrRym__eVoPKS-gcjx_oDlY2xhdiiqsLebh_H/s489/appelbaum-thesis-backcover.jpg"><img alt="" data-original-height="489" data-original-width="350" height="320" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiy4lkUk-akfN2uYBkPt5xPKu8TAXreBfRyr4aFDGMzIf6PLadDxdeccI5Aq8OcS8gKsuaiI_H0PwwIEpGf5QV2CLvUj3yXrM4DvNWq8SN8e-STsA-4uih8Ky2f8Fy7FaehIBiYNWrCMToJcpgdMuYdrRym__eVoPKS-gcjx_oDlY2xhdiiqsLebh_H/s320/appelbaum-thesis-backcover.jpg"></a></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The anatomy of a Godot API call (166 pts)]]></title>
            <link>https://sampruden.github.io/posts/godot-is-not-the-new-unity/</link>
            <guid>37561762</guid>
            <pubDate>Mon, 18 Sep 2023 20:43:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sampruden.github.io/posts/godot-is-not-the-new-unity/">https://sampruden.github.io/posts/godot-is-not-the-new-unity/</a>, See on <a href="https://news.ycombinator.com/item?id=37561762">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Like many people, I’ve spent the last few days looking for the new Unity. Godot has some potential, especially if it can take advantage of an influx of dev talent to drive rapid improvement. Open source is cool like that. However, one major issue holds it back - the binding layer between engine code and gameplay code is structurally built to be slow in ways which are very hard to fix without tearing everything down and rebuilding the entire API from scratch.</p><p>Godot has been used to create some successful games, so clearly this isn’t always a blocker. However Unity has spent the last five years working on speeding up their scripting with crazy projects such as building two custom compilers, SIMD maths libraries, custom collections and allocators, and of course the giant (and very much unfinished) ECS project. It’s been their CTO’s primary focus since 2018. Clearly Unity believed that scripting performance mattered to a significant part of their userbase. Switching to Godot isn’t only like going back five years in Unity - it’s so much worse.</p><p>I started <a href="https://reddit.com/r/godot/comments/16j345n/is_the_c_raycasting_api_as_poor_as_it_first/">a controversial but productive discussion</a> about this on the Godot subreddit a few days ago. This article is a more detailed continuation of my thoughts in that post now that I have a <strong>little</strong> more understanding of how Godot works. <strong>Let’s be clear here: I’m still a Godot newb, and this article <em>will</em> contain mistakes and misconceptions.</strong></p><p><em>Note: The following contains criticisms of the Godot engine’s design and engineering. Although I occasionally use some emotive language to describe my feelings about these things, the Godot developers have put in lots of hard work for the FOSS community and built something that’s loved by many people, and my intent is not to offend or come across as rude to any individuals.</em></p><h2 id="deepdive-into-performing-a-raycast-from-c"><span>Deepdive into performing a raycast from C#</span><a href="#deepdive-into-performing-a-raycast-from-c"><i></i></a></h2><p>We’re going to take a deep dive into how Godot achieves the equivalent of Unity’s <code>Physics2D.Raycast</code>, and what happens under the hood when we use it. To make this a little more concrete, let’s start by implementing a trivial function in Unity.</p><h3 id="unity"><span>Unity</span><a href="#unity"><i></i></a></h3><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
</pre></td><td><pre><span>// Simple raycast in Unity</span>
<span>bool</span> <span>GetRaycastDistanceAndNormal</span><span>(</span><span>Vector2</span> <span>origin</span><span>,</span> <span>Vector2</span> <span>direction</span><span>,</span> <span>out</span> <span>float</span> <span>distance</span><span>,</span> <span>out</span> <span>Vector2</span> <span>normal</span><span>)</span> <span>{</span>
    <span>RaycastHit2D</span> <span>hit</span> <span>=</span> <span>Physics2D</span><span>.</span><span>Raycast</span><span>(</span><span>origin</span><span>,</span> <span>direction</span><span>);</span>
    <span>distance</span> <span>=</span> <span>hit</span><span>.</span><span>distance</span><span>;</span>
    <span>normal</span> <span>=</span> <span>hit</span><span>.</span><span>normal</span><span>;</span>
    <span>return</span> <span>(</span><span>bool</span><span>)</span><span>hit</span><span>;</span>
<span>}</span>
</pre></td></tr></tbody></table></code></p></div><p>Let’s have a quick look at how this is implemented by following the calls.</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td><pre><span>public</span> <span>static</span> <span>RaycastHit2D</span> <span>Raycast</span><span>(</span><span>Vector2</span> <span>origin</span><span>,</span> <span>Vector2</span> <span>direction</span><span>)</span>
 <span>=&gt;</span> <span>defaultPhysicsScene</span><span>.</span><span>Raycast</span><span>(</span><span>origin</span><span>,</span> <span>direction</span><span>,</span> <span>float</span><span>.</span><span>PositiveInfinity</span><span>);</span>

<span>public</span> <span>RaycastHit2D</span> <span>Raycast</span><span>(</span><span>Vector2</span> <span>origin</span><span>,</span> <span>Vector2</span> <span>direction</span><span>,</span> <span>float</span> <span>distance</span><span>,</span> <span>[</span><span>DefaultValue</span><span>(</span><span>"Physics2D.DefaultRaycastLayers"</span><span>)]</span> <span>int</span> <span>layerMask</span> <span>=</span> <span>-</span><span>5</span><span>)</span>
<span>{</span>
    <span>ContactFilter2D</span> <span>contactFilter</span> <span>=</span> <span>ContactFilter2D</span><span>.</span><span>CreateLegacyFilter</span><span>(</span><span>layerMask</span><span>,</span> <span>float</span><span>.</span><span>NegativeInfinity</span><span>,</span> <span>float</span><span>.</span><span>PositiveInfinity</span><span>);</span>
    <span>return</span> <span>Raycast_Internal</span><span>(</span><span>this</span><span>,</span> <span>origin</span><span>,</span> <span>direction</span><span>,</span> <span>distance</span><span>,</span> <span>contactFilter</span><span>);</span>
<span>}</span>

<span>[</span><span>NativeMethod</span><span>(</span><span>"Raycast_Binding"</span><span>)]</span>
<span>[</span><span>StaticAccessor</span><span>(</span><span>"PhysicsQuery2D"</span><span>,</span> <span>StaticAccessorType</span><span>.</span><span>DoubleColon</span><span>)]</span>
<span>private</span> <span>static</span> <span>RaycastHit2D</span> <span>Raycast_Internal</span><span>(</span><span>PhysicsScene2D</span> <span>physicsScene</span><span>,</span> <span>Vector2</span> <span>origin</span><span>,</span> <span>Vector2</span> <span>direction</span><span>,</span> <span>float</span> <span>distance</span><span>,</span> <span>ContactFilter2D</span> <span>contactFilter</span><span>)</span>
<span>{</span>
    <span>Raycast_Internal_Injected</span><span>(</span><span>ref</span> <span>physicsScene</span><span>,</span> <span>ref</span> <span>origin</span><span>,</span> <span>ref</span> <span>direction</span><span>,</span> <span>distance</span><span>,</span> <span>ref</span> <span>contactFilter</span><span>,</span> <span>out</span> <span>var</span> <span>ret</span><span>);</span>
    <span>return</span> <span>ret</span><span>;</span>
<span>}</span>

<span>[</span><span>MethodImpl</span><span>(</span><span>MethodImplOptions</span><span>.</span><span>InternalCall</span><span>)]</span>
<span>private</span> <span>static</span> <span>extern</span> <span>void</span> <span>Raycast_Internal_Injected</span><span>(</span>
    <span>ref</span> <span>PhysicsScene2D</span> <span>physicsScene</span><span>,</span> <span>ref</span> <span>Vector2</span> <span>origin</span><span>,</span> <span>ref</span> <span>Vector2</span> <span>direction</span><span>,</span> <span>float</span> <span>distance</span><span>,</span>
    <span>ref</span> <span>ContactFilter2D</span> <span>contactFilter</span><span>,</span> <span>out</span> <span>RaycastHit2D</span> <span>ret</span><span>);</span>
</pre></td></tr></tbody></table></code></p></div><p>Okay, so it does a tiny amount of work and efficiently shunts the call off to the unmanaged engine core via the extern mechanism. That makes sense, I’m sure Godot will do something similar. Foreshadowing.</p><h3 id="godot"><span>Godot</span><a href="#godot"><i></i></a></h3><p>Let’s do the same thing in Godot, <a href="https://docs.godotengine.org/en/stable/tutorials/physics/ray-casting.html#raycast-query">exactly as the tutorial recommends</a>.</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td><pre><span>// Equivalent raycast in Godot</span>
<span>bool</span> <span>GetRaycastDistanceAndNormal</span><span>(</span><span>Vector2</span> <span>origin</span><span>,</span> <span>Vector2</span> <span>direction</span><span>,</span> <span>out</span> <span>float</span> <span>distance</span><span>,</span> <span>out</span> <span>Vector2</span> <span>normal</span><span>)</span>
<span>{</span>
    <span>World2D</span> <span>world</span> <span>=</span> <span>GetWorld2D</span><span>();</span>
    <span>PhysicsDirectSpaceState2D</span> <span>spaceState</span> <span>=</span> <span>world</span><span>.</span><span>DirectSpaceState</span><span>;</span>
    <span>PhysicsRayQueryParameters2D</span> <span>queryParams</span> <span>=</span> <span>PhysicsRayQueryParameters2D</span><span>.</span><span>Create</span><span>(</span><span>origin</span><span>,</span> <span>origin</span> <span>+</span> <span>direction</span><span>);</span>
    <span>Godot</span><span>.</span><span>Collections</span><span>.</span><span>Dictionary</span> <span>hitDictionary</span> <span>=</span> <span>spaceState</span><span>.</span><span>IntersectRay</span><span>(</span><span>queryParams</span><span>);</span>

    <span>if</span> <span>(</span><span>hitDictionary</span><span>.</span><span>Count</span> <span>!=</span> <span>0</span><span>)</span>
    <span>{</span>
        <span>Variant</span> <span>hitPositionVariant</span> <span>=</span> <span>hitDictionary</span><span>[(</span><span>Variant</span><span>)</span><span>"position"</span><span>];</span>
        <span>Vector2</span> <span>hitPosition</span> <span>=</span> <span>(</span><span>Vector2</span><span>)</span><span>hitPositionVariant</span><span>;</span>
        <span>Variant</span> <span>hitNormalVariant</span> <span>=</span> <span>hitDictionary</span><span>[(</span><span>Variant</span><span>)</span><span>"normal"</span><span>];</span>
        <span>Vector2</span> <span>hitNormal</span> <span>=</span> <span>(</span><span>Vector2</span><span>)</span><span>hitNormalVariant</span><span>;</span>
        
        <span>distance</span> <span>=</span> <span>(</span><span>hitPosition</span> <span>-</span> <span>origin</span><span>).</span><span>Length</span><span>();</span>
        <span>normal</span> <span>=</span> <span>hitNormal</span><span>;</span>
        <span>return</span> <span>true</span><span>;</span>
    <span>}</span>

    <span>distance</span> <span>=</span> <span>default</span><span>;</span>
    <span>normal</span> <span>=</span> <span>default</span><span>;</span>
    <span>return</span> <span>false</span><span>;</span>
<span>}</span>
</pre></td></tr></tbody></table></code></p></div><p>The first thing that we notice is that this is longer. That is not the focus of my criticism, and is partly due to the fact that I’ve formatted this code verbosely in order to make it easier for us to break it down line by line. So let’s do that, what’s actually happening here?</p><p>We start by calling <code>GetWorld2D()</code>. In Godot, physics queries are all performed in the context of a world, and this function gets the world our code is running in. Although this <code>World2D</code> is a managed class type, this function doesn’t do anything crazy like allocate every time we run it. None of these functions are going to do anything crazy like that for a simple raycast, right? Foreshadowing.</p><p>If we look inside these API calls we’ll see that even ostensibly simple ones like this are implemented through some rather convoluted machinery which will have at least a little performance overhead. Let’s dive into <code>GetWorld2D</code> as an example of that by unravelling some of its calls through C#. This is roughly what all the calls that return managed types look like. I’ve added some comments to explain what’s going on.</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
</pre></td><td><pre><span>// This is the function we're diving into.</span>
<span>public</span> <span>World2D</span> <span>GetWorld2D</span><span>()</span>
<span>{</span>
    <span>// MethodBind64 is a pointer to the function we're calling in C++.</span>
    <span>// MethodBind64 is stored in a static variable, so we have to do a memory lookup to retrieve it.</span>
    <span>return</span> <span>(</span><span>World2D</span><span>)</span><span>NativeCalls</span><span>.</span><span>godot_icall_0_51</span><span>(</span><span>MethodBind64</span><span>,</span> <span>GodotObject</span><span>.</span><span>GetPtr</span><span>(</span><span>this</span><span>));</span>
<span>}</span>

<span>// We call into these functions which mediate API calls.</span>
<span>internal</span> <span>unsafe</span> <span>static</span> <span>GodotObject</span> <span>godot_icall_0_51</span><span>(</span><span>IntPtr</span> <span>method</span><span>,</span> <span>IntPtr</span> <span>ptr</span><span>)</span>
<span>{</span>
    <span>godot_ref</span> <span>godot_ref</span> <span>=</span> <span>default</span><span>(</span><span>godot_ref</span><span>);</span>

    <span>// The try/finally machinery is not free. This introduces a state machine.</span>
    <span>// It can also block JIT optimisations.</span>
    <span>try</span>
    <span>{</span>
        <span>// Validation check, even though everything here is internal and should be trusted.</span>
        <span>if</span> <span>(</span><span>ptr</span> <span>==</span> <span>IntPtr</span><span>.</span><span>Zero</span><span>)</span> <span>throw</span> <span>new</span> <span>ArgumentNullException</span><span>(</span><span>"ptr"</span><span>);</span>

        <span>// This calls into another function which performs the actual function pointer call</span>
        <span>// and puts the unmanaged result in godot_ref via a pointer.</span>
        <span>NativeFuncs</span><span>.</span><span>godotsharp_method_bind_ptrcall</span><span>(</span><span>method</span><span>,</span> <span>ptr</span><span>,</span> <span>null</span><span>,</span> <span>&amp;</span><span>godot_ref</span><span>);</span>
        
        <span>// This is some machinery for moving references to managed objects over the C#/C++ boundary.</span>
        <span>return</span> <span>InteropUtils</span><span>.</span><span>UnmanagedGetManaged</span><span>(</span><span>godot_ref</span><span>.</span><span>Reference</span><span>);</span>
    <span>}</span>
    <span>finally</span>
    <span>{</span>
        <span>godot_ref</span><span>.</span><span>Dispose</span><span>();</span>
    <span>}</span>
<span>}</span>

<span>// The function which actually calls the function pointer.</span>
<span>[</span><span>global</span><span>::</span><span>System</span><span>.</span><span>Runtime</span><span>.</span><span>CompilerServices</span><span>.</span><span>MethodImpl</span><span>(</span><span>global</span><span>::</span><span>System</span><span>.</span><span>Runtime</span><span>.</span><span>CompilerServices</span><span>.</span><span>MethodImplOptions</span><span>.</span><span>AggressiveInlining</span><span>)]</span>
<span>public</span> <span>static</span> <span>partial</span> <span>void</span> <span>godotsharp_method_bind_ptrcall</span><span>(</span> <span>global</span><span>::</span><span>System</span><span>.</span><span>IntPtr</span> <span>p_method_bind</span><span>,</span>  <span>global</span><span>::</span><span>System</span><span>.</span><span>IntPtr</span> <span>p_instance</span><span>,</span>  <span>void</span><span>**</span> <span>p_args</span><span>,</span>  <span>void</span><span>*</span> <span>p_ret</span><span>)</span>
<span>{</span>
    <span>// But wait! </span>
    <span>// _unmanagedCallbacks.godotsharp_method_bind_ptrcall is actually another</span>
    <span>// static variable access to retrieve another function pointer.</span>
    <span>_unmanagedCallbacks</span><span>.</span><span>godotsharp_method_bind_ptrcall</span><span>(</span><span>p_method_bind</span><span>,</span> <span>p_instance</span><span>,</span> <span>p_args</span><span>,</span> <span>p_ret</span><span>);</span>
<span>}</span>

<span>// To be honest, I haven't studied this well enough to know exactly what's happening here.</span>
<span>// The basic idea is straightforward - this takes a pointer to an unmanaged GodotObject,</span>
<span>// brings it into .Net, notifies the garbage collector of it so that it can be tracked,</span>
<span>// and casts it to the GodotObject type.</span>
<span>// Fortunately, this doesn't appear to do any allocations. Foreshadowing.</span>
<span>public</span> <span>static</span> <span>GodotObject</span> <span>UnmanagedGetManaged</span><span>(</span><span>IntPtr</span> <span>unmanaged</span><span>)</span>
<span>{</span>
    <span>if</span> <span>(</span><span>unmanaged</span> <span>==</span> <span>IntPtr</span><span>.</span><span>Zero</span><span>)</span> <span>return</span> <span>null</span><span>;</span>

    <span>IntPtr</span> <span>intPtr</span> <span>=</span> <span>NativeFuncs</span><span>.</span><span>godotsharp_internal_unmanaged_get_script_instance_managed</span><span>(</span><span>unmanaged</span><span>,</span> <span>out</span> <span>var</span> <span>r_has_cs_script_instance</span><span>);</span>
    <span>if</span> <span>(</span><span>intPtr</span> <span>!=</span> <span>IntPtr</span><span>.</span><span>Zero</span><span>)</span> <span>return</span> <span>(</span><span>GodotObject</span><span>)</span><span>GCHandle</span><span>.</span><span>FromIntPtr</span><span>(</span><span>intPtr</span><span>).</span><span>Target</span><span>;</span>
    <span>if</span> <span>(</span><span>r_has_cs_script_instance</span><span>.</span><span>ToBool</span><span>())</span> <span>return</span> <span>null</span><span>;</span>

    <span>intPtr</span> <span>=</span> <span>NativeFuncs</span><span>.</span><span>godotsharp_internal_unmanaged_get_instance_binding_managed</span><span>(</span><span>unmanaged</span><span>);</span>
    <span>object</span> <span>obj</span> <span>=</span> <span>((</span><span>intPtr</span> <span>!=</span> <span>IntPtr</span><span>.</span><span>Zero</span><span>)</span> <span>?</span> <span>GCHandle</span><span>.</span><span>FromIntPtr</span><span>(</span><span>intPtr</span><span>).</span><span>Target</span> <span>:</span> <span>null</span><span>);</span>
    <span>if</span> <span>(</span><span>obj</span> <span>!=</span> <span>null</span><span>)</span> <span>return</span> <span>(</span><span>GodotObject</span><span>)</span><span>obj</span><span>;</span>

    <span>intPtr</span> <span>=</span> <span>NativeFuncs</span><span>.</span><span>godotsharp_internal_unmanaged_instance_binding_create_managed</span><span>(</span><span>unmanaged</span><span>,</span> <span>intPtr</span><span>);</span>
    <span>if</span> <span>(!(</span><span>intPtr</span> <span>!=</span> <span>IntPtr</span><span>.</span><span>Zero</span><span>))</span> <span>return</span> <span>null</span><span>;</span>

    <span>return</span> <span>(</span><span>GodotObject</span><span>)</span><span>GCHandle</span><span>.</span><span>FromIntPtr</span><span>(</span><span>intPtr</span><span>).</span><span>Target</span><span>;</span>
<span>}</span>
</pre></td></tr></tbody></table></code></p></div><p>This is actually a substantial amount of overhead. We have a number of layers of pointer chasing indirection between our code and C++. Each of those is a memory lookup, and on top of that we do a bit of work with the validation, <code>try</code> <code>finally</code>, and interpreting the returned pointer. These may sound like tiny inconsequential things, but when every single call into the core and every property/field access on a Godot object does this whole journey, it starts to add up.</p><p>If we look at the next line which accesses the <code>world.DirectSpaceState</code> property we’ll find it does pretty much the same thing. The <code>PhysicsDirectSpaceState2D</code> is once again retrieved from C++ land via this machinery. Don’t worry, I won’t bore you with the details!</p><p>The line after that is the first thing I saw here that really boggled my bonnet.</p><div><p><code><table><tbody><tr><td><pre>1
</pre></td><td><pre><span>PhysicsRayQueryParameters2D</span> <span>queryParams</span> <span>=</span> <span>PhysicsRayQueryParameters2D</span><span>.</span><span>Create</span><span>(</span><span>origin</span><span>,</span> <span>origin</span> <span>+</span> <span>direction</span><span>);</span>
</pre></td></tr></tbody></table></code></p></div><p>What’s the big deal, that’s just a little struct packing some raycast parameters, right? <strong>Wrong</strong>. <code>PhysicsRayQueryParameters2D</code> is a managed class, and this is a full GC garbage generating allocation. That’s a pretty crazy thing to have in a performance sensitive hot path! I’m sure it’s just the one allocation though, right? Let’s have a look inside.</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td><pre><span>// Summary:</span>
<span>//     Returns a new, pre-configured Godot.PhysicsRayQueryParameters2D object. Use it</span>
<span>//     to quickly create query parameters using the most common options.</span>
<span>//     var query = PhysicsRayQueryParameters2D.create(global_position, global_position</span>
<span>//     + Vector2(0, 100))</span>
<span>//     var collision = get_world_2d().direct_space_state.intersect_ray(query)</span>
<span>public</span> <span>unsafe</span> <span>static</span> <span>PhysicsRayQueryParameters2D</span> <span>Create</span><span>(</span><span>Vector2</span> <span>from</span><span>,</span> <span>Vector2</span> <span>to</span><span>,</span> <span>uint</span> <span>collisionMask</span> <span>=</span> <span>uint</span><span>.</span><span>MaxValue</span><span>,</span> <span>Array</span><span>&lt;</span><span>Rid</span><span>&gt;</span> <span>exclude</span> <span>=</span> <span>null</span><span>)</span>
<span>{</span>
    <span>// Yes, this goes through all of the same machinery discussed above.</span>
    <span>return</span> <span>(</span><span>PhysicsRayQueryParameters2D</span><span>)</span><span>NativeCalls</span><span>.</span><span>godot_icall_4_731</span><span>(</span>
        <span>MethodBind0</span><span>,</span>
        <span>&amp;</span><span>from</span><span>,</span> <span>&amp;</span><span>to</span><span>,</span> <span>collisionMask</span><span>,</span>
        <span>(</span><span>godot_array</span><span>)(</span><span>exclude</span> <span>??</span> <span>new</span> <span>Array</span><span>&lt;</span><span>Rid</span><span>&gt;()).</span><span>NativeValue</span>
    <span>);</span>
<span>}</span>
</pre></td></tr></tbody></table></code></p></div><p>Uh oh. Have you spotted it yet?</p><p>That <code>Array&lt;Rid&gt;</code> is a <code>Godot.Collections.Array</code>. That’s another managed class type. Look what happens when we pass in a <code>null</code> value.</p><div><p><code><table><tbody><tr><td><pre>1
</pre></td><td><pre><span>(</span><span>godot_array</span><span>)(</span><span>exclude</span> <span>??</span> <span>new</span> <span>Array</span><span>&lt;</span><span>Rid</span><span>&gt;()).</span><span>NativeValue</span>
</pre></td></tr></tbody></table></code></p></div><p>That’s right, even if we don’t pass an <code>exclude</code> array, it goes ahead and allocates a whole array on the C# heap for us anyway, just so that it can immediately convert it back into a native value representing an empty array.</p><p>In order to pass two simple <code>Vector2</code> values (16 bytes) to a raycast function, we’ve now done two separate garbage creating heap allocations totalling 632 bytes!</p><p>As you’ll see later, we can mitigate this by caching a <code>PhysicsRayQueryParameters2D</code>. However, as you can see from the doc comment I included above, the API clearly expects and recommends creating fresh instances for each raycast.</p><p>Let’s move onto the next line. It can’t get any crazier, right? Foreshadowing.</p><div><p><code><table><tbody><tr><td><pre>1
</pre></td><td><pre><span>Godot</span><span>.</span><span>Collections</span><span>.</span><span>Dictionary</span> <span>hitDictionary</span> <span>=</span> <span>spaceState</span><span>.</span><span>IntersectRay</span><span>(</span><span>queryParams</span><span>);</span>
</pre></td></tr></tbody></table></code></p></div><p>Whelp. That shadowing wasn’t very fore.</p><p>That’s right, our raycast is returning an untyped dictionary. And yes, it creates garbage by allocating in on the managed heap, another 96 bytes. You have my permission to do a bemused and upset type of face now. “Oh, well maybe it at least returns null if it doesn’t hit anything?” you may be thinking. No. If it doesn’t hit anything, it allocates and returns an empty dictionary.</p><p>Let’s jump straight into the C++ implementation here.</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
</pre></td><td><pre><span>Dictionary</span> <span>PhysicsDirectSpaceState2D</span><span>::</span><span>_intersect_ray</span><span>(</span><span>const</span> <span>Ref</span><span>&lt;</span><span>PhysicsRayQueryParameters2D</span><span>&gt;</span> <span>&amp;</span><span>p_ray_query</span><span>)</span> <span>{</span>
    <span>ERR_FAIL_COND_V</span><span>(</span><span>!</span><span>p_ray_query</span><span>.</span><span>is_valid</span><span>(),</span> <span>Dictionary</span><span>());</span>

    <span>RayResult</span> <span>result</span><span>;</span>
    <span>bool</span> <span>res</span> <span>=</span> <span>intersect_ray</span><span>(</span><span>p_ray_query</span><span>-&gt;</span><span>get_parameters</span><span>(),</span> <span>result</span><span>);</span>

    <span>if</span> <span>(</span><span>!</span><span>res</span><span>)</span> <span>{</span>
        <span>return</span> <span>Dictionary</span><span>();</span>
    <span>}</span>

    <span>Dictionary</span> <span>d</span><span>;</span>
    <span>d</span><span>[</span><span>"position"</span><span>]</span> <span>=</span> <span>result</span><span>.</span><span>position</span><span>;</span>
    <span>d</span><span>[</span><span>"normal"</span><span>]</span> <span>=</span> <span>result</span><span>.</span><span>normal</span><span>;</span>
    <span>d</span><span>[</span><span>"collider_id"</span><span>]</span> <span>=</span> <span>result</span><span>.</span><span>collider_id</span><span>;</span>
    <span>d</span><span>[</span><span>"collider"</span><span>]</span> <span>=</span> <span>result</span><span>.</span><span>collider</span><span>;</span>
    <span>d</span><span>[</span><span>"shape"</span><span>]</span> <span>=</span> <span>result</span><span>.</span><span>shape</span><span>;</span>
    <span>d</span><span>[</span><span>"rid"</span><span>]</span> <span>=</span> <span>result</span><span>.</span><span>rid</span><span>;</span>

    <span>return</span> <span>d</span><span>;</span>
<span>}</span>

<span>// This is the params struct that the inernal intersect_ray takes in.</span>
<span>// Nothing too crazy here (although exclude could probably be improved).</span>
<span>struct</span> <span>RayParameters</span> <span>{</span>
    <span>Vector2</span> <span>from</span><span>;</span>
    <span>Vector2</span> <span>to</span><span>;</span>
    <span>HashSet</span><span>&lt;</span><span>RID</span><span>&gt;</span> <span>exclude</span><span>;</span>
    <span>uint32_t</span> <span>collision_mask</span> <span>=</span> <span>UINT32_MAX</span><span>;</span>
    <span>bool</span> <span>collide_with_bodies</span> <span>=</span> <span>true</span><span>;</span>
    <span>bool</span> <span>collide_with_areas</span> <span>=</span> <span>false</span><span>;</span>
    <span>bool</span> <span>hit_from_inside</span> <span>=</span> <span>false</span><span>;</span>
<span>};</span>

<span>// And this is the output. A perfectly reasonable return value for a raycast.</span>
<span>struct</span> <span>RayResult</span> <span>{</span>
    <span>Vector2</span> <span>position</span><span>;</span>
    <span>Vector2</span> <span>normal</span><span>;</span>
    <span>RID</span> <span>rid</span><span>;</span>
    <span>ObjectID</span> <span>collider_id</span><span>;</span>
    <span>Object</span> <span>*</span><span>collider</span> <span>=</span> <span>nullptr</span><span>;</span>
    <span>int</span> <span>shape</span> <span>=</span> <span>0</span><span>;</span>
<span>};</span>
</pre></td></tr></tbody></table></code></p></div><p>As we can see, this is wrapping some perfectly reasonable raycast function in ungodly slow craziness. That internal <code>intersect_ray</code> is the function that should be in the API!</p><p>This C++ code allocates an untyped dictionary on the unmanaged heap. If we dig down into this dictionary, we find a hashmap as expected. It performs six hashmap lookups to initialize this dictionary (some of them may even do additional allocations, but I haven’t dug that deep). But wait, this is an untyped dictionary. How does that work? Well the internal hashmap maps <code>Variant</code> to <code>Variant</code>.</p><p>Sigh. What’s a <code>Variant</code>? Well the implementation is <a href="https://github.com/godotengine/godot/blob/master/core/variant/variant.cpp">quite complicated</a>, but in simple terms it’s a big tagged union type encompassing all possible types the dictionary can hold. We can think of it as being the dynamic untyped type. What we care about is its size, which is 20 bytes.</p><p>Okay, so each of those “fields” we’ve written into the dictionary is now 20 bytes large. Oh, and so are the keys. Those 8 byte <code>Vector2</code> values? 20 bytes each now. That <code>int</code>? 20 bytes. You get the picture.</p><p>If we sum the sizes of the fields in <code>RayResult</code>, we’re looking at 44 bytes (assuming 8 byte pointers). If we sum the sizes of the <code>Variant</code> keys and values of the dictionary, that’s 2 * 6 * 20 = 240 bytes! But wait, it’s a hashmap. Hashmaps don’t store their data compactly, so the true size of that dictionary on the heap is at least 6x larger than the data we want to return, probably much more.</p><p>Okay, let’s go back to C# and see what happens when we return this thing.</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre></td><td><pre><span>// The function we're calling.</span>
<span>public</span> <span>Dictionary</span> <span>IntersectRay</span><span>(</span><span>PhysicsRayQueryParameters2D</span> <span>parameters</span><span>)</span>
<span>{</span>
    <span>return</span> <span>NativeCalls</span><span>.</span><span>godot_icall_1_729</span><span>(</span><span>MethodBind1</span><span>,</span> <span>GodotObject</span><span>.</span><span>GetPtr</span><span>(</span><span>this</span><span>),</span> <span>GodotObject</span><span>.</span><span>GetPtr</span><span>(</span><span>parameters</span><span>));</span>
<span>}</span>

<span>internal</span> <span>unsafe</span> <span>static</span> <span>Dictionary</span> <span>godot_icall_1_729</span><span>(</span><span>IntPtr</span> <span>method</span><span>,</span> <span>IntPtr</span> <span>ptr</span><span>,</span> <span>IntPtr</span> <span>arg1</span><span>)</span>
<span>{</span>
    <span>godot_dictionary</span> <span>nativeValueToOwn</span> <span>=</span> <span>default</span><span>(</span><span>godot_dictionary</span><span>);</span>
    <span>if</span> <span>(</span><span>ptr</span> <span>==</span> <span>IntPtr</span><span>.</span><span>Zero</span><span>)</span> <span>throw</span> <span>new</span> <span>ArgumentNullException</span><span>(</span><span>"ptr"</span><span>);</span>

    <span>void</span><span>**</span> <span>intPtr</span> <span>=</span> <span>stackalloc</span> <span>void</span><span>*[</span><span>1</span><span>];</span>
    <span>*</span><span>intPtr</span> <span>=</span> <span>&amp;</span><span>arg1</span><span>;</span>
    <span>void</span><span>**</span> <span>p_args</span> <span>=</span> <span>intPtr</span><span>;</span>
    <span>NativeFuncs</span><span>.</span><span>godotsharp_method_bind_ptrcall</span><span>(</span><span>method</span><span>,</span> <span>ptr</span><span>,</span> <span>p_args</span><span>,</span> <span>&amp;</span><span>nativeValueToOwn</span><span>);</span>
    <span>return</span> <span>Dictionary</span><span>.</span><span>CreateTakingOwnershipOfDisposableValue</span><span>(</span><span>nativeValueToOwn</span><span>);</span>
<span>}</span>

<span>internal</span> <span>static</span> <span>Dictionary</span> <span>CreateTakingOwnershipOfDisposableValue</span><span>(</span><span>godot_dictionary</span> <span>nativeValueToOwn</span><span>)</span>
<span>{</span>
    <span>return</span> <span>new</span> <span>Dictionary</span><span>(</span><span>nativeValueToOwn</span><span>);</span>
<span>}</span>

<span>private</span> <span>Dictionary</span><span>(</span><span>godot_dictionary</span> <span>nativeValueToOwn</span><span>)</span>
<span>{</span>
    <span>godot_dictionary</span> <span>value</span> <span>=</span> <span>(</span><span>nativeValueToOwn</span><span>.</span><span>IsAllocated</span> <span>?</span> <span>nativeValueToOwn</span> <span>:</span> <span>NativeFuncs</span><span>.</span><span>godotsharp_dictionary_new</span><span>());</span>
    <span>NativeValue</span> <span>=</span> <span>(</span><span>godot_dictionary</span><span>.</span><span>movable</span><span>)</span><span>value</span><span>;</span>
    <span>_weakReferenceToSelf</span> <span>=</span> <span>DisposablesTracker</span><span>.</span><span>RegisterDisposable</span><span>(</span><span>this</span><span>);</span>
<span>}</span>
</pre></td></tr></tbody></table></code></p></div><p>The main things to notice here are that we’re allocating a new managed (garbage creating, yada yada) dictionary in C#, and that it holds a pointer into the one created on the heap in C++. Hey, at least we’re not copying the dictionary contents over! I’ll take wins where I can get them at this point.</p><p>Okay, so what next?</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td><pre><span>if</span> <span>(</span><span>hitDictionary</span><span>.</span><span>Count</span> <span>!=</span> <span>0</span><span>)</span>
<span>{</span>
    <span>// The cast from string to Variant can be implicit - I've made it explicit here for clarity</span>
    <span>Variant</span> <span>hitPositionVariant</span> <span>=</span> <span>hitDictionary</span><span>[(</span><span>Variant</span><span>)</span><span>"position"</span><span>];</span>
    <span>Vector2</span> <span>hitPosition</span> <span>=</span> <span>(</span><span>Vector2</span><span>)</span><span>hitPositionVariant</span><span>;</span>
    <span>Variant</span> <span>hitNormalVariant</span> <span>=</span> <span>hitDictionary</span><span>[(</span><span>Variant</span><span>)</span><span>"normal"</span><span>];</span>
    <span>Vector2</span> <span>hitNormal</span> <span>=</span> <span>(</span><span>Vector2</span><span>)</span><span>hitNormalVariant</span><span>;</span>
    
    <span>distance</span> <span>=</span> <span>(</span><span>hitPosition</span> <span>-</span> <span>origin</span><span>).</span><span>Length</span><span>();</span>
    <span>normal</span> <span>=</span> <span>hitNormal</span><span>;</span>
    <span>return</span> <span>true</span><span>;</span>
<span>}</span>
</pre></td></tr></tbody></table></code></p></div><p>Hopefully we can all follow what’s happening here at this point.</p><p>If our ray didn’t hit anything an empty dictionary is returned, so we check for hits by checking the count.</p><p>If we hit something, for each field we want to read we:</p><ol><li>Cast <code>string</code> keys to C# <code>Variant</code> structs (This also does a call into C++)</li><li>Chase some more function pointers to call into C++ in the way we’ve come to expect by now</li><li>Perform a hashmap lookup to get the <code>Variant</code> holding our value (via function pointer chasing, of course)</li><li>Copy those 20 bytes back into C# world (yes, even though we’re reading <code>Vector2</code> values which are only 8 bytes)</li><li>Extract the <code>Vector2</code> value from the <code>Variant</code> (Yes, it also chases pointers all the way back into C++ to do this conversion)</li></ol><p>Well that’s a lot work for returning a 44 byte struct and reading a couple of fields.</p><h3 id="can-we-do-better"><span>Can we do better?</span><a href="#can-we-do-better"><i></i></a></h3><h4 id="caching-query-parameters"><span>Caching query parameters</span><a href="#caching-query-parameters"><i></i></a></h4><p>If you can remember as far back as <code>PhysicsRayQueryParameters2D</code>, we had the opportunity to avoid some allocations by caching, so let’s do that quickly.</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre></td><td><pre><span>readonly</span> <span>struct</span> <span>CachingRayCaster</span>
<span>{</span>
    <span>private</span> <span>readonly</span> <span>PhysicsDirectSpaceState2D</span> <span>spaceState</span><span>;</span>
    <span>private</span> <span>readonly</span> <span>PhysicsRayQueryParameters2D</span> <span>queryParams</span><span>;</span>

    <span>public</span> <span>CachingRayCaster</span><span>(</span><span>PhysicsDirectSpaceState2D</span> <span>spaceState</span><span>)</span>
    <span>{</span>
        <span>this</span><span>.</span><span>spaceState</span> <span>=</span> <span>spaceState</span><span>;</span>
        <span>this</span><span>.</span><span>queryParams</span> <span>=</span> <span>PhysicsRayQueryParameters2D</span><span>.</span><span>Create</span><span>(</span><span>Vector2</span><span>.</span><span>Zero</span><span>,</span> <span>Vector2</span><span>.</span><span>Zero</span><span>);</span>
    <span>}</span>

    <span>public</span> <span>bool</span> <span>GetDistanceAndNormal</span><span>(</span><span>Vector2</span> <span>origin</span><span>,</span> <span>Vector2</span> <span>direction</span><span>,</span> <span>out</span> <span>float</span> <span>distance</span><span>,</span> <span>out</span> <span>Vector2</span> <span>normal</span><span>)</span>
    <span>{</span>
        <span>Godot</span><span>.</span><span>Collections</span><span>.</span><span>Dictionary</span> <span>hitDictionary</span> <span>=</span> <span>this</span><span>.</span><span>spaceState</span><span>.</span><span>IntersectRay</span><span>(</span><span>this</span><span>.</span><span>queryParams</span><span>);</span>

        <span>if</span> <span>(</span><span>hitDictionary</span><span>.</span><span>Count</span> <span>!=</span> <span>0</span><span>)</span>
        <span>{</span>
            <span>Variant</span> <span>hitPositionVariant</span> <span>=</span> <span>hitDictionary</span><span>[(</span><span>Variant</span><span>)</span><span>"position"</span><span>];</span>
            <span>Vector2</span> <span>hitPosition</span> <span>=</span> <span>(</span><span>Vector2</span><span>)</span><span>hitPositionVariant</span><span>;</span>
            <span>Variant</span> <span>hitNormalVariant</span> <span>=</span> <span>hitDictionary</span><span>[(</span><span>Variant</span><span>)</span><span>"normal"</span><span>];</span>
            <span>Vector2</span> <span>hitNormal</span> <span>=</span> <span>(</span><span>Vector2</span><span>)</span><span>hitNormalVariant</span><span>;</span>
            <span>distance</span> <span>=</span> <span>(</span><span>hitPosition</span> <span>-</span> <span>origin</span><span>).</span><span>Length</span><span>();</span>
            <span>normal</span> <span>=</span> <span>hitNormal</span><span>;</span>
            <span>return</span> <span>true</span><span>;</span>
        <span>}</span>

        <span>distance</span> <span>=</span> <span>default</span><span>;</span>
        <span>normal</span> <span>=</span> <span>default</span><span>;</span>
        <span>return</span> <span>false</span><span>;</span>
    <span>}</span>
<span>}</span>
</pre></td></tr></tbody></table></code></p></div><p>After the first ray, this removes 2/3rds of our per ray C#/GC allocations by count, and 632/738 of our C#/GC allocations by bytes. It’s still not a good situation, but it’s an improvement.</p><h4 id="what-about-gdextension"><span>What about GDExtension?</span><a href="#what-about-gdextension"><i></i></a></h4><p>As you may have heard, Godot also gives us a C++ (or Rust, or other native language) API to allow us to write high performance code. That will come to the rescue here, right? Right?</p><p>Well…</p><p>So it turns out GDExtension exposes the exact same API. Yeah. You can write fast C++ code, but you still only get an API that returns an untyped dictionary of bloated <code>Variant</code> values. It’s a little better because there’s no GC to worry about, but… Yeah. I recommend making another sad face right about now.</p><h4 id="a-whole-different-approach----the-raycast2d-node"><span>A whole different approach - the <code>RayCast2D</code> node</span><a href="#a-whole-different-approach----the-raycast2d-node"><i></i></a></h4><p>But wait! We can take a whole different approach.</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
</pre></td><td><pre><span>bool</span> <span>GetRaycastDistanceAndNormalWithNode</span><span>(</span><span>RayCast2D</span> <span>raycastNode</span><span>,</span> <span>Vector2</span> <span>origin</span><span>,</span> <span>Vector2</span> <span>direction</span><span>,</span> <span>out</span> <span>float</span> <span>distance</span><span>,</span> <span>out</span> <span>Vector2</span> <span>normal</span><span>)</span>
<span>{</span>
    <span>raycastNode</span><span>.</span><span>Position</span> <span>=</span> <span>origin</span><span>;</span>
    <span>raycastNode</span><span>.</span><span>TargetPosition</span> <span>=</span> <span>origin</span> <span>+</span> <span>direction</span><span>;</span>
    <span>raycastNode</span><span>.</span><span>ForceRaycastUpdate</span><span>();</span>

    <span>distance</span> <span>=</span> <span>(</span><span>raycastNode</span><span>.</span><span>GetCollisionPoint</span><span>()</span> <span>-</span> <span>origin</span><span>).</span><span>Length</span><span>();</span>
    <span>normal</span> <span>=</span> <span>raycastNode</span><span>.</span><span>GetCollisionNormal</span><span>();</span>
    <span>return</span> <span>raycastNode</span><span>.</span><span>IsColliding</span><span>();</span>
<span>}</span>
</pre></td></tr></tbody></table></code></p></div><p>Here we have a function which takes a reference to a <code>RayCast2D</code> node in the scene. As the name suggests, this is a scene node that performs raycasts. It’s implemented in C++, and it doesn’t go through the same API with all of the dictionary overhead. This is a pretty clunky way to do raycasts as we need a reference to a node in the scene which we’re happy to mutate, and we have to reposition the node in the scene in order to do a query, but let’s take a look inside.</p><p>First we need to note that, as we’ve come to expect, each of these properties that we’re accessing does a full pointer chasing journey into C++ land.</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td><pre><span>public</span> <span>Vector2</span> <span>Position</span>
<span>{</span>
    <span>get</span> <span>=&gt;</span> <span>GetPosition</span><span>()</span>
    <span>set</span> <span>=&gt;</span> <span>SetPosition</span><span>(</span><span>value</span><span>);</span>
<span>}</span>

<span>internal</span> <span>unsafe</span> <span>void</span> <span>SetPosition</span><span>(</span><span>Vector2</span> <span>position</span><span>)</span>
<span>{</span>
    <span>NativeCalls</span><span>.</span><span>godot_icall_1_31</span><span>(</span><span>MethodBind0</span><span>,</span> <span>GodotObject</span><span>.</span><span>GetPtr</span><span>(</span><span>this</span><span>),</span> <span>&amp;</span><span>position</span><span>);</span>
<span>}</span>

<span>internal</span> <span>unsafe</span> <span>static</span> <span>void</span> <span>godot_icall_1_31</span><span>(</span><span>IntPtr</span> <span>method</span><span>,</span> <span>IntPtr</span> <span>ptr</span><span>,</span> <span>Vector2</span><span>*</span> <span>arg1</span><span>)</span>
<span>{</span>
    <span>if</span> <span>(</span><span>ptr</span> <span>==</span> <span>IntPtr</span><span>.</span><span>Zero</span><span>)</span> <span>throw</span> <span>new</span> <span>ArgumentNullException</span><span>(</span><span>"ptr"</span><span>);</span>

    <span>void</span><span>**</span> <span>intPtr</span> <span>=</span> <span>stackalloc</span> <span>void</span><span>*[</span><span>1</span><span>];</span>
    <span>*</span><span>intPtr</span> <span>=</span> <span>arg1</span><span>;</span>
    <span>void</span><span>**</span> <span>p_args</span> <span>=</span> <span>intPtr</span><span>;</span>
    <span>NativeFuncs</span><span>.</span><span>godotsharp_method_bind_ptrcall</span><span>(</span><span>method</span><span>,</span> <span>ptr</span><span>,</span> <span>p_args</span><span>,</span> <span>null</span><span>);</span>
<span>}</span>
</pre></td></tr></tbody></table></code></p></div><p>Now let’s look at what <code>ForceRaycastUpdate()</code> actually does. I’m sure you can guess the C# by now, so let’s dive straight into the C++.</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
</pre></td><td><pre><span>void</span> <span>RayCast2D</span><span>::</span><span>force_raycast_update</span><span>()</span> <span>{</span>
    <span>_update_raycast_state</span><span>();</span>
<span>}</span>

<span>void</span> <span>RayCast2D</span><span>::</span><span>_update_raycast_state</span><span>()</span> <span>{</span>
    <span>Ref</span><span>&lt;</span><span>World2D</span><span>&gt;</span> <span>w2d</span> <span>=</span> <span>get_world_2d</span><span>();</span>
    <span>ERR_FAIL_COND</span><span>(</span><span>w2d</span><span>.</span><span>is_null</span><span>());</span>

    <span>PhysicsDirectSpaceState2D</span> <span>*</span><span>dss</span> <span>=</span> <span>PhysicsServer2D</span><span>::</span><span>get_singleton</span><span>()</span><span>-&gt;</span><span>space_get_direct_state</span><span>(</span><span>w2d</span><span>-&gt;</span><span>get_space</span><span>());</span>
    <span>ERR_FAIL_NULL</span><span>(</span><span>dss</span><span>);</span>

    <span>Transform2D</span> <span>gt</span> <span>=</span> <span>get_global_transform</span><span>();</span>

    <span>Vector2</span> <span>to</span> <span>=</span> <span>target_position</span><span>;</span>
    <span>if</span> <span>(</span><span>to</span> <span>==</span> <span>Vector2</span><span>())</span> <span>{</span>
        <span>to</span> <span>=</span> <span>Vector2</span><span>(</span><span>0</span><span>,</span> <span>0.01</span><span>);</span>
    <span>}</span>

    <span>PhysicsDirectSpaceState2D</span><span>::</span><span>RayResult</span> <span>rr</span><span>;</span>
    <span>bool</span> <span>prev_collision_state</span> <span>=</span> <span>collided</span><span>;</span>

    <span>PhysicsDirectSpaceState2D</span><span>::</span><span>RayParameters</span> <span>ray_params</span><span>;</span>
    <span>ray_params</span><span>.</span><span>from</span> <span>=</span> <span>gt</span><span>.</span><span>get_origin</span><span>();</span>
    <span>ray_params</span><span>.</span><span>to</span> <span>=</span> <span>gt</span><span>.</span><span>xform</span><span>(</span><span>to</span><span>);</span>
    <span>ray_params</span><span>.</span><span>exclude</span> <span>=</span> <span>exclude</span><span>;</span>
    <span>ray_params</span><span>.</span><span>collision_mask</span> <span>=</span> <span>collision_mask</span><span>;</span>
    <span>ray_params</span><span>.</span><span>collide_with_bodies</span> <span>=</span> <span>collide_with_bodies</span><span>;</span>
    <span>ray_params</span><span>.</span><span>collide_with_areas</span> <span>=</span> <span>collide_with_areas</span><span>;</span>
    <span>ray_params</span><span>.</span><span>hit_from_inside</span> <span>=</span> <span>hit_from_inside</span><span>;</span>

    <span>if</span> <span>(</span><span>dss</span><span>-&gt;</span><span>intersect_ray</span><span>(</span><span>ray_params</span><span>,</span> <span>rr</span><span>))</span> <span>{</span>
        <span>collided</span> <span>=</span> <span>true</span><span>;</span>
        <span>against</span> <span>=</span> <span>rr</span><span>.</span><span>collider_id</span><span>;</span>
        <span>against_rid</span> <span>=</span> <span>rr</span><span>.</span><span>rid</span><span>;</span>
        <span>collision_point</span> <span>=</span> <span>rr</span><span>.</span><span>position</span><span>;</span>
        <span>collision_normal</span> <span>=</span> <span>rr</span><span>.</span><span>normal</span><span>;</span>
        <span>against_shape</span> <span>=</span> <span>rr</span><span>.</span><span>shape</span><span>;</span>
    <span>}</span> <span>else</span> <span>{</span>
        <span>collided</span> <span>=</span> <span>false</span><span>;</span>
        <span>against</span> <span>=</span> <span>ObjectID</span><span>();</span>
        <span>against_rid</span> <span>=</span> <span>RID</span><span>();</span>
        <span>against_shape</span> <span>=</span> <span>0</span><span>;</span>
    <span>}</span>

    <span>if</span> <span>(</span><span>prev_collision_state</span> <span>!=</span> <span>collided</span><span>)</span> <span>{</span>
        <span>queue_redraw</span><span>();</span>
    <span>}</span>
<span>}</span>
</pre></td></tr></tbody></table></code></p></div><p>It looks like there’s a lot going on here, but it’s actually quite simple. If we look carefully we can see that the structure is pretty much the same as our first <code>GetRaycastDistanceAndNormal</code> C# function. It gets the world, gets the state, builds the parameters, calls <code>intersect_ray</code> to do the actual work, then writes the result out to the properties.</p><p>But look! No heap allocations, no <code>Dictionary</code>, and no <code>Variant</code>. This is more like it! We can predict that this will be a lot faster.</p><h3 id="timing-it"><span>Timing it</span><a href="#timing-it"><i></i></a></h3><p>Okay, I’ve made a lot of allusions to all of this overhead being dramatically problematic and we can easily see that it should be, but let’s put some actual numbers to this with benchmarks.</p><p>As we’ve seen above, <code>RayCast2D.ForceRaycastUpdate()</code> is pretty close to a minimalist call to the physics engine’s <code>intersect_ray</code>, so we can use this as a baseline. Remember that even this has some overhead from the pointer chasing function call. I’ve also benchmarked each of the versions of the code we’ve discussed. Each benchmark runs 10,000 iterations of the function under test, with warmup and outlier filtering. I disabled GC collection during the tests. I like to run my game benchmarks on weaker hardware so you may get better results if you repro, but it’s the relative numbers that we care about.</p><p>Our setup is a simple scene containing a single circle collider that our ray always hits. We’re interested in measuring binding overhead, not the performance of the physics engine itself. We’re dealing with timings for individual rays measured in nanoseconds, so these numbers may look inconsequentially small. To better illustrate their significance, I also report “calls per frame” giving the number of times the functions could be called in in a single frame at 60fps and 120fps if the game did nothing but trivial raycasts.</p><div><table><thead><tr><th>Method</th><th>Time (μs)</th><th>Baseline multiple</th><th>Per frame (60fps)</th><th>Per frame (120fps)</th><th>GC alloc (bytes)</th></tr></thead><tbody><tr><td><code>ForceRaycastUpdate</code> (raw engine speed, not useful)</td><td>0.49</td><td>1.00</td><td>34,000</td><td>17,000</td><td>0</td></tr><tr><td><code>GetRaycastDistanceAndNormalWithNode</code></td><td>0.97</td><td>1.98</td><td>17,200</td><td>8,600</td><td>0</td></tr><tr><td><code>CachingRayCaster.GetDistanceAndNormal</code></td><td>7.71</td><td>15.73</td><td>2,200</td><td>1,100</td><td>96</td></tr><tr><td><code>GetRaycastDistanceAndNormal</code></td><td>24.23</td><td>49.45</td><td>688</td><td>344</td><td>728</td></tr></tbody></table></div><p>Those are some significant differences!</p><p>We might expect that the fastest way to do a raycast in a reasonable engine/API is to use the function exposed for doing exactly that, <a href="https://docs.godotengine.org/en/stable/tutorials/physics/ray-casting.html#raycast-query">which is taught as the canonical way in the documentation</a>. As we can see, if we do that, the binding/API overhead makes this 50X slower than the raw physics engine speed. Ouch!</p><p>Using that same API but being sensible (if awkward) about caching, we can get that down to only 16X overhead. This is better, but still awful.</p><p>If our aim here is to get practical performance, we have to sidestep the proper/canonical/advertised API completely, and instead clunkily manipulate scene objects to exploit them to do our query for us. In a sensible world moving objects around in the scene and asking them to do raycasts for us would be slower than calling the raw physics API, but in fact it’s 8X faster.</p><p>Even the node approach is 2X slower than the raw speed of the engine (which we’re actually underestimating). This means that half of the time in that function is being spent on setting two properties and reading three properties. The binding overhead is large enough that five property accesses takes as long as a raycast. Let that sink in. <em>Let’s not even think about the fact that in the real world we may well want to set and read even more properties, such as setting the layer mask and reading the hit collider</em>.</p><p>At the lower end, those numbers are actually very limiting. My current project needs more than 344 raycasts per frame, and of course it does a lot more than just raycasting. This test is a trivial scene with a single collider, if we were making the raycast do actual work in a more complex scene these numbers would be even lower! The documentation’s standard way of doing raycasts would grind my whole game to a halt.</p><p>We also can’t forget about the garbage creating allocations that happen in C#. I usually write games with a zero garbage per frame policy.</p><p><em>Just for fun, I also benchmarked Unity. It does a full useful raycast, with parameter setting and result retrieval, in about 0.52μs. Before Godot’s binding overhead, the core physics engines have comparable speed.</em></p><h2 id="have-i-cherrypicked"><span>Have I cherrypicked?</span><a href="#have-i-cherrypicked"><i></i></a></h2><p>When I posted the reddit thread, a number of people said that the physics API is uniquely bad and that it isn’t representative of the whole engine. I certainly didn’t intentionally cherrypick it - it just so happens that raycasting was the very first thing I attempted when checking out Godot. However, perhaps I’m being a little unfair, so let’s examine that.</p><p>If I had wanted to cherrypick a worse method, I wouldn’t have had to look far. Right next to <code>IntersectRay</code> are <code>IntersectPoint</code> and <code>IntersectShape</code>, both of which share all of the same problems as <code>IntersectRay</code> with the additional craziness that they can have multiple results, so they return a heap allocated managed <code>Godot.Collections.Array&lt;Dictionary&gt;</code>! Oh by the way, that <code>Array&lt;T&gt;</code> is actually a typed wrapper around <code>Godot.Collections.Array</code>, so every 8 byte reference to a dictionary is actually stored as a 20 byte <code>Variant</code>. Clearly I haven’t picked the very worst method in the API!</p><p>If we scan the whole Godot API (via C# reflection) we luckily find that there aren’t that many things which return <code>Dictionary</code>. There’s an eclectic list including the <code>AnimationNode._GetChildNodes</code> method, the <code>Bitmap.Data</code> property, the <code>Curve2D._Data</code> property (and 3D), some things in <code>GLTFSkin</code>, some <code>TextServer</code> stuff, some <code>NavigationAgent2D</code> pieces, etc. None of those are great places to have slow heap allocated dictionaries, but none of them are as bad as the physics API.</p><p>However, in my experience, very few engine APIs get as much use as physics. If I look at the engine API calls in my gameplay code, they’re probably 80% physics and transforms.</p><p>Let’s also remember that <code>Dictionary</code> is only part of the problem. If we look a little wider for things returning <code>Godot.Collections.Array&lt;T&gt;</code> (remember: heap allocated, contents as <code>Variant</code>) we find lots from physics, mesh &amp; geometry manipulation, navigation, tilemaps, rendering, and more.</p><p>Physics may be a particularly bad (but essential) area of the API, but the heap allocated type problems, as well as the general slowness of the pointer chasing, are deeply rooted throughout.</p><h2 id="so-why-are-we-waiting-for-godot"><span>So why are we waiting for Godot?</span><a href="#so-why-are-we-waiting-for-godot"><i></i></a></h2><p>Godot’s primary scripting language is GDScript, a dynamically typed interpreted language where almost all non primitives are heap allocated, i.e. it doesn’t have a struct analogue. That sentence should have set off a cacophony of performance alarms in your head. I’ll give you a moment for your ears to stop ringing.</p><p>If we look at how Godot’s C++ core exposes its API we’ll see something interesting.</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
</pre></td><td><pre><span>void</span> <span>PhysicsDirectSpaceState3D</span><span>::</span><span>_bind_methods</span><span>()</span> <span>{</span>
    <span>ClassDB</span><span>::</span><span>bind_method</span><span>(</span><span>D_METHOD</span><span>(</span><span>"intersect_point"</span><span>,</span> <span>"parameters"</span><span>,</span> <span>"max_results"</span><span>),</span> <span>&amp;</span><span>PhysicsDirectSpaceState3D</span><span>::</span><span>_intersect_point</span><span>,</span> <span>DEFVAL</span><span>(</span><span>32</span><span>));</span>
    <span>ClassDB</span><span>::</span><span>bind_method</span><span>(</span><span>D_METHOD</span><span>(</span><span>"intersect_ray"</span><span>,</span> <span>"parameters"</span><span>),</span> <span>&amp;</span><span>PhysicsDirectSpaceState3D</span><span>::</span><span>_intersect_ray</span><span>);</span>
    <span>ClassDB</span><span>::</span><span>bind_method</span><span>(</span><span>D_METHOD</span><span>(</span><span>"intersect_shape"</span><span>,</span> <span>"parameters"</span><span>,</span> <span>"max_results"</span><span>),</span> <span>&amp;</span><span>PhysicsDirectSpaceState3D</span><span>::</span><span>_intersect_shape</span><span>,</span> <span>DEFVAL</span><span>(</span><span>32</span><span>));</span>
    <span>ClassDB</span><span>::</span><span>bind_method</span><span>(</span><span>D_METHOD</span><span>(</span><span>"cast_motion"</span><span>,</span> <span>"parameters"</span><span>),</span> <span>&amp;</span><span>PhysicsDirectSpaceState3D</span><span>::</span><span>_cast_motion</span><span>);</span>
    <span>ClassDB</span><span>::</span><span>bind_method</span><span>(</span><span>D_METHOD</span><span>(</span><span>"collide_shape"</span><span>,</span> <span>"parameters"</span><span>,</span> <span>"max_results"</span><span>),</span> <span>&amp;</span><span>PhysicsDirectSpaceState3D</span><span>::</span><span>_collide_shape</span><span>,</span> <span>DEFVAL</span><span>(</span><span>32</span><span>));</span>
    <span>ClassDB</span><span>::</span><span>bind_method</span><span>(</span><span>D_METHOD</span><span>(</span><span>"get_rest_info"</span><span>,</span> <span>"parameters"</span><span>),</span> <span>&amp;</span><span>PhysicsDirectSpaceState3D</span><span>::</span><span>_get_rest_info</span><span>);</span>
<span>}</span>
</pre></td></tr></tbody></table></code></p></div><p>This one shared mechanism is used to generate the bindings for all three scripting interfaces; GDSCript, C#, and GDExtensions. <code>ClassDB</code> collects function pointers and metadata about each of the API functions, which is then piped through various code generation systems to create the bindings for each language.</p><p>This means that every API function is designed primarily to serve the limitations of GDScript. <code>IntersectRay</code> returns an untyped dynamic <code>Dictionary</code> because GDScript doesn’t have structs. Our C# and even our GDExtensions C++ code has to pay the catastrophic price for that.</p><p>This way of handling binding via function pointers also leads to significant overhead, as we’ve seen from simple property accesses being slow. Remember, each call first does a memory lookup to find the function pointer it wants to call, then it does another lookup to find the function pointer of a secondary function which is actually responsible for calling the function, then it calls the secondary function passing it the pointer to the primary function. All along that journey there’s extra validation code, branching, and type conversions. C# (and obviously C++) has a fast mechanism for calling into native code via P/Invoke, but Godot simply doesn’t use it.</p><p><strong>Godot has made a philosophical decision to be slow.</strong> The only practical way to interact with the engine is via this binding layer, and its core design prevents it from ever being fast. No amount of optimising the implementation of <code>Dictionary</code> or speeding up the physics engine is going to get around the fact we’re passing large heap allocated values around when we should be dealing with tiny structs. While C# and GDScript APIs remain synchronised, this will always hold the engine back.</p><h2 id="okay-lets-fix-it-then"><span>Okay, let’s fix it then!</span><a href="#okay-lets-fix-it-then"><i></i></a></h2><h3 id="what-can-we-do-without-deviating-from-the-existing-binding-layer"><span>What can we do without deviating from the existing binding layer?</span><a href="#what-can-we-do-without-deviating-from-the-existing-binding-layer"><i></i></a></h3><p>If we assume that we still need to keep all of our APIs GDScript compatible, there are a few areas where we can probably improve things, although it won’t be pretty. Let’s go back to our <code>IntsersectRay</code> example.</p><ul><li><code>GetWorld2D().DirectStateSpace</code> could be compressed to one call instead of two by introducing <code>GetWorld2DStateSpace()</code>.</li><li>The <code>PhysicsRayQueryParameters2D</code> issues could be removed by adding an overload which takes all of the fields as parameters. This would bring us roughly inline with the <code>CachedRayCaster</code> performance (16X baseline) without having to do caching.</li><li>The <code>Dictionary</code> allocation could be removed by allowing us to pass in a cached/pooled dictionary to write into. This is ugly and clumsy compared to a struct, but it would remove the allocation.</li><li>The dictionary lookup process is still ridiculously slow. We might be able to improve on that by instead returning a class with the expected properties. The allocation here could be eliminated with the cached/pooled approach the same way it could with <code>Dictionary</code>.</li></ul><p>These options aren’t pretty or ergonomic for the user, but if we’re in the business of doing ugly patches to get things running, they would probably work. This would fix the allocations but we’d still probably only be about 4X the baseline because of all of the pointer chasing across the boundary and managing of cached values.</p><p>It may also be possible to improve the generated code for all of the pointer chasing shenanigans. I haven’t studied this in detail yet, but if there are wins to find there then they’d apply to the whole API across the board, which would be cool! We could probably at least get away with removing the validation and the <code>try</code> <code>finally</code> in release builds.</p><h3 id="what-if-were-allowed-to-add-additional-apis-for-c-and-gdextensions-which-arent-gdscript-compatible"><span>What if we’re allowed to add additional APIs for C# and GDExtensions which aren’t GDScript compatible?</span><a href="#what-if-were-allowed-to-add-additional-apis-for-c-and-gdextensions-which-arent-gdscript-compatible"><i></i></a></h3><p>Now we’re talking! If we open up this possibility* then in theory we could augment the <code>ClassDB</code> bindings with better ones that deal directly in structs and go through the proper P/Invoke mechanisms. This is the path to viable performance.</p><p>Unfortunately, duplicating the entire API with better versions like this would create quite a mess. There might be ways through this by marking things <code>[Deprecated]</code> and trying to guide the user in the right direction, but issues such as naming clashes would get ugly.</p><p>* Maybe this is already possible, but I haven’t found it yet. Let me know!</p><h3 id="what-if-we-tear-it-all-down-and-start-again"><span>What if we tear it all down and start again?</span><a href="#what-if-we-tear-it-all-down-and-start-again"><i></i></a></h3><p>This option obviously has a lot of short term pain. Godot 4.0 has only recently happened, and now I’m talking about a backcompat breaking complete API redux like a Godot 5.0. However, if I’m honest with myself, I see this as the only viable path to the engine being in a good place in three years time. Mixing fast and slow APIs as discussed above would leave us with headaches for decades - a trap I expect the engine will probably fall into.</p><p><del>In my opinion, if Godot were to go down this route, GDScript should probably be dropped entirely. I don’t really see the point of it when C# exists, and supporting it causes so much hassle. I’m clearly completely at odds with the lead Godot devs and the project philosophy on this point, so I have no expectation that this will happen. Who knows though - Unity eventually dropped UnityScript for full C#, maybe Godot will one day take the same step. Foreshadowing?</del></p><p>Edit: I’m taking the above out for now. I don’t personally care about GDScript, but other people do and I don’t want to take it away from them. I have no objection to C# and GDScript sitting beside each other with different APIs each optimised for the respective language’s needs.</p><h2 id="was-the-title-of-this-article-melodramatic-clickbait"><span>Was the title of this article melodramatic clickbait?</span><a href="#was-the-title-of-this-article-melodramatic-clickbait"><i></i></a></h2><p>Maybe a little. Not a lot.</p><p>There will be people who were making games in Unity who can make those same games in Godot without these issues mattering too much. Godot may be able to capture the lower end of Unity’s market. However, Unity’s recent focus on performance is a good indicator that there’s demand for it. I know that I certainly care about it. Godot’s performance is not just worse than Unity’s, it’s dramatically and systematically worse.</p><p>In some projects 95% of the CPU load is in an algorithm which never touches the engine APIs. In that case, none of this matters. (The GC always matters, but we can use GDExtensions to avoid that.) For many others, good programmatic interaction with physics/collisions and manually modifying the properties of large numbers of objects are essential to the project.</p><p>For many others, it’s important to know that they can do these things if they need to. Maybe you get two years into your project thinking it will barely need raycasts at all, then you make a late game decision to add some custom CPU particles that need to be able to check collisions. It’s a small aesthetic change, but suddenly you need an engine API and you’re in trouble. There’s a lot of talk right now about the importance of being able to trust that your engine will have your back in the future. Unity has that problem with their scummy business practices, Godot has that problem with performance.</p><p>If Godot wants to be able to capture the general Unity market (I don’t actually know that it does want that) it will need to make some rapid and fundamental changes. Many of the things discussed in this article will simply not be acceptable to Unity devs.</p><h2 id="discussion"><span>Discussion</span><a href="#discussion"><i></i></a></h2><p>I <a href="https://old.reddit.com/r/godot/comments/16lti15/godot_is_not_the_new_unity_the_anatomy_of_a_godot/">posted this article on the r/Godot subreddit</a> and there’s quite an active discussion there. If you’ve arrived here from somewhere else and would like to give feedback or be pseudonymously rude to me on the internet, that’s the place to do it.</p><h2 id="acknowledgements"><span>Acknowledgements</span><a href="#acknowledgements"><i></i></a></h2><ul><li>_Mario_Boss on reddit for being the first to bring my attention to the <code>Raycast2D</code> node trick.</li><li>John Riccitiello, for finally giving me a reason to do more research on other engines.</li><li>Mike Bithell, for letting me steal his foreshadowing joke. I didn’t actually ask permission, but he seems too nice to find me and hit me.</li><li>Freya Holmér, because nothing has kept me more entertained while writing this than seeing her complaining about Unreal doing physics in centimetres, and waiting until the moment she shares my horror upon discovering Godot has units like <code>kg pixels^2</code>. Edit: <a href="https://twitter.com/FreyaHolmer/status/1703884185004380338">One of my jokes finally landed.</a></li><li>Clainkey on reddit for pointing out that I mistakenly had nanoseconds where I should have had microseconds.</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FTC warns pharma companies about sham patent listings designed to delay generics (193 pts)]]></title>
            <link>https://www.techdirt.com/2023/09/18/ftc-warns-pharma-companies-that-it-may-go-after-them-for-sham-patent-listings-designed-to-delay-generic-competitors/</link>
            <guid>37561696</guid>
            <pubDate>Mon, 18 Sep 2023 20:38:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2023/09/18/ftc-warns-pharma-companies-that-it-may-go-after-them-for-sham-patent-listings-designed-to-delay-generic-competitors/">https://www.techdirt.com/2023/09/18/ftc-warns-pharma-companies-that-it-may-go-after-them-for-sham-patent-listings-designed-to-delay-generic-competitors/</a>, See on <a href="https://news.ycombinator.com/item?id=37561696">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-420764">


<h3>from the <i>abusing-the-orange-book,-green-with-greed</i> dept</h3>

<p>For many, many years we’ve detailed how big pharma companies, who only care about the monopoly rents they can receive on medicine while under patent, have concocted <a target="_blank" rel="noreferrer noopener" href="https://www.techdirt.com/2019/05/20/big-pharma-companies-accused-conspiring-to-inflate-prices-over-100-generic-drugs-up-to-1000/">all sorts</a> of <a target="_blank" rel="noreferrer noopener" href="https://www.techdirt.com/2009/12/07/pay-for-delay-agreements-again-show-how-pharma-abuses-patent-law-to-harm-us-all/">scams</a> and <a target="_blank" rel="noreferrer noopener" href="https://www.techdirt.com/2021/11/24/why-are-drug-prices-so-high-because-asshole-mckinsey-consultants-figure-out-ways-to-re-patent-same-drugs-over-over/">schemes</a> to <a target="_blank" rel="noreferrer noopener" href="https://www.techdirt.com/2023/02/10/thanks-to-evergreening-and-legal-threats-abbvie-has-earned-114-billion-since-2016-from-a-drug-whose-key-patent-expired-back-then/">avoid</a> having to compete with generic versions, even after their patents have expired (or been invalidated). But one of their older tricks is apparently popular yet again, though the FTC is now warning pharma that it <em>might</em> finally start cracking down.</p>
<p>If it does, it will just be reinforcing the kinds of actions the FTC used to bring. Twenty years ago, the <a target="_blank" rel="noreferrer noopener" href="https://www.ftc.gov/legal-library/browse/cases-proceedings/0110046-bristol-myers-squibb-company-matter">FTC went after Bristol Meyer Squibb</a> for false listings in the Orange Book. The Orange Book, managed by the FDA, is where pharma companies list the FDA-approved drugs they have under patent, which alerts generic drug companies basically not to make generic versions of those drugs.</p>
<p>But, of course, this creates a very tempting scenario: if pharma can get drugs not actually under patent into the Orange Book, they effectively save themselves from generic competition, and they get to profit massively (at the expense of the public and their need for affordable medicine).</p>
<p>However, despite enforcement against such abuse years ago, it seems that the FTC and the FDA have kinda let these things slip over the past few years. And Big Pharma has really taken advantage of that. Thankfully, it looks like the FTC is finally interested in <a target="_blank" rel="noreferrer noopener" href="https://www.ftc.gov/news-events/news/press-releases/2023/09/ftc-issues-policy-statement-brand-pharmaceutical-manufacturers-improper-listing-patents-food-drug">cracking down on this practice again</a>. In <a target="_blank" rel="noreferrer noopener" href="https://www.ftc.gov/system/files/ftc_gov/pdf/p239900orangebookpolicystatement092023.pdf">a new policy statement</a>, it warns pharma companies that it’s looking into the abuse of the Orange Book and sham patent inclusions.</p>
<blockquote>
<p><em>Brand drug manufacturers are responsible for ensuring their patents are properly listed. Yet certain manufacturers have submitted patents for listing in the Orange Book that claim neither the reference listed drug nor a method of using it. When brand drug manufacturers abuse the regulatory processes set up by Congress to promote generic drug competition, the result may be to increase the cost of and reduce access to prescription drugs.</em></p>
<p><em>The goal of this policy statement is to put market participants on notice that the FTC intends to scrutinize improper Orange Book listings to determine whether these constitute unfair methods of competition in violation of Section 5 of the Federal Trade Commission Act.</em></p>
</blockquote>
<p>Of course, this raises some questions, including why do we make the pharma companies themselves the party responsible for making sure their patents are “properly” listed. Why don’t we have at least some process in place for these listings to be reviewed, whether when they’re submitted to the Orange Book or even if another party (such as the generic drug manufacturers) contest an Orange Book listing.</p>
<p>It seems the dumbest possible system is to assume that the Big Pharma companies will be honest in their Orange Book listings.</p>
<p>And, even though the FTC is now putting these companies “on notice,” the fact that the FTC has brought these cases in the past seems like it should be “notice” enough. Instead, it sounds like the FTC let enough pharma companies get away with this for long enough that the big pharma firms felt cleared to abuse the system this way and to delay competition in the marketplace.</p>
<p>The one thing I find interesting in this statement, is that they note that improperly listing things in the Orange Book may “constitute illegal monopolization.”</p>
<blockquote>
<p><em>The improper listing of patents in the Orange Book may also constitute illegal monopolization. Monopolization requires proof of “the willful acquisition or maintenance of [monopoly] power as distinguished from growth or development as a consequence of a superior product, business acumen, or historic accident.” This requires proof that “the defendant has engaged in improper conduct that has or is likely to have the effect of controlling prices or excluding competition,” and courts have recognized that improperly listing patents in the Orange Book may constitute an “improper means” of competition. Accordingly, improperly listing patents in the Orange Book may also be worthy of enforcement scrutiny from government and private enforcers under a monopolization theory. Additionally, the FTC may also scrutinize a firm’s history of improperly listing patents during merger review</em></p>
</blockquote>
<p>This seems exactly correct, but notable in that very few people seem to recognize that (1) patents are government granted monopolies, and thus (2) an abuse of the patent system to get a patent or patent-like protections you don’t deserve are therefore an <em>illegal</em> monopoly seems like an important point. I would hope that this could get expanded to other abuses of patent and copyright law as well.</p>
<p>Still, given that we’ve been facing this and multiple other schemes from Big Pharma to delay generics for decades, I’m not sure anything is really going to change just yet, but at least the FTC is waking up (again?) to this issue. Now let’s see if it actually starts bringing cases…</p>
<p>
Filed Under: <a href="https://www.techdirt.com/tag/competition/" rel="tag">competition</a>, <a href="https://www.techdirt.com/tag/drug-prices/" rel="tag">drug prices</a>, <a href="https://www.techdirt.com/tag/ftc/" rel="tag">ftc</a>, <a href="https://www.techdirt.com/tag/generics/" rel="tag">generics</a>, <a href="https://www.techdirt.com/tag/monopoly/" rel="tag">monopoly</a>, <a href="https://www.techdirt.com/tag/orange-book/" rel="tag">orange book</a>, <a href="https://www.techdirt.com/tag/patents/" rel="tag">patents</a>, <a href="https://www.techdirt.com/tag/pharma/" rel="tag">pharma</a>
<br>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Citing “rapid evolution of generative AI,” Amazon limits new Kindle books (207 pts)]]></title>
            <link>https://www.kdpcommunity.com/s/article/Update-on-KDP-Title-Creation-Limits?language=en_US&amp;forum=KDP%20Forum</link>
            <guid>37561668</guid>
            <pubDate>Mon, 18 Sep 2023 20:37:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.kdpcommunity.com/s/article/Update-on-KDP-Title-Creation-Limits?language=en_US&#x26;forum=KDP%20Forum">https://www.kdpcommunity.com/s/article/Update-on-KDP-Title-Creation-Limits?language=en_US&#x26;forum=KDP%20Forum</a>, See on <a href="https://news.ycombinator.com/item?id=37561668">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="auraLoadingBox"><p><span>Loading</span></p></div><div id="auraErrorMask"><p><span><a id="dismissError">×</a>Sorry to interrupt</span></p><p>CSS Error</p><div><p><a href="https://www.kdpcommunity.com/s/article/Update-on-KDP-Title-Creation-Limits?" id="auraErrorReload">Refresh</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple TV, now with more Tailscale (344 pts)]]></title>
            <link>https://tailscale.com/blog/apple-tv/</link>
            <guid>37560787</guid>
            <pubDate>Mon, 18 Sep 2023 19:24:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tailscale.com/blog/apple-tv/">https://tailscale.com/blog/apple-tv/</a>, See on <a href="https://news.ycombinator.com/item?id=37560787">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>Today we’re expanding the list of devices that can run Tailscale, bringing <a href="https://apps.apple.com/us/app/tailscale/id1470499037?platform=appleTV">secure remote networking to the Apple TV</a>. The newly released tvOS 17 offers support for VPNs, and we’re proud to say Tailscale is among the first to use this new feature. You can now <a href="https://tailscale.com/kb/1280/appletv/">add your Apple TV directly to your tailnet</a>, unlocking three powerful new use cases that we’re excited to share.</p>

    
    

<figure>
        <img src="https://tailscale.com/assets/84113/1695058417-3840x2160bb.png?auto=format" alt="A screenshot of the Tailscale app for Apple TV in light mode." width="3840" height="2160">
    
</figure>

<p>First up, if you already have anything like a “media server” in your life, the benefits of integrating your Apple TV into the same Tailscale network are large. Lots of people already use Tailscale with Plex or Jellyfin servers, <a href="https://perfectmediaserver.com/">homelab set-ups</a>, and <a href="https://tailscale.com/kb/1074/connect-to-your-nas/">NAS devices</a> to securely share their collections and stream from them while on the go. Today’s release makes it that much simpler to do so right on your TV.</p>
<p>With up to three users available <a href="https://tailscale.com/pricing/">on our Free plan</a>, you’ve got tools to make a media drive available to other trusted people in your life. You can share a collection of family photos and home videos into a faraway relative’s tailnet, without worrying about locking down the server for public internet access.</p>
<p>But even if you don’t have a media server to connect to, you can use Tailscale’s Apple TV app to select another device in your tailnet, like a PC, a Raspberry Pi, or even an Android phone, to use as an exit node. This will route all your Apple TV’s traffic through that connection, providing an extra layer of privacy from the local network where you’re using the Apple TV and making your traffic appear to originate from the machine of your choice.</p>
<p>Compare that to a “traditional” VPN option, where your traffic is routed through a commercial data center (which itself may be blocked by sites and services) and where you must trust the VPN provider not to spy on or tamper with your traffic. With a Tailscale exit node, you’re in control and you get the internet connection you’re used to. This new feature could come in handy if you’re traveling with your Apple TV and want to access the same geo-restricted channels you can see from home.</p>
<p>Finally, the new Tailscale client allows an Apple TV to be an exit node itself for other machines in your tailnet. This one might require a little more explaining; after all, not a lot of Apple TV apps advertise features that are most useful when you’re away from your Apple TV.</p>
<p>But look at it this way: your Apple TV device is a capable little computer, and it stays connected to your tailnet even when it’s not in active use. Download and configure Tailscale now and you can securely route any of your other devices’ traffic through your Apple TV&nbsp; — and by extension, through your home internet connection — even when you’re on the other side of the planet. Whether you want another layer of security and privacy on sketchy Wi-Fi networks or just want to connect back through your personal internet connection when you’re on the road, you’re set with the Apple TV as an exit node.</p>
<p>At Tailscale, we’re the kind of nerds who have home server closets and who will stock up on Raspberry Pis just because they’re available again. Our favorite thing about bringing Tailscale to tvOS is you don’t have to be that kind of nerd to be able to tap into the power of Tailscale in your home.</p>
<p>If you’ve got an Apple TV running the new tvOS 17, <a href="https://apps.apple.com/us/app/tailscale/id1470499037?platform=appleTV">download the Tailscale app</a> today!</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Paint on Windows is getting layers and transparency support (198 pts)]]></title>
            <link>https://blogs.windows.com/windows-insider/2023/09/18/paint-app-update-adding-support-for-layers-and-transparency-begins-rolling-out-to-windows-insiders/</link>
            <guid>37559256</guid>
            <pubDate>Mon, 18 Sep 2023 17:34:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blogs.windows.com/windows-insider/2023/09/18/paint-app-update-adding-support-for-layers-and-transparency-begins-rolling-out-to-windows-insiders/">https://blogs.windows.com/windows-insider/2023/09/18/paint-app-update-adding-support-for-layers-and-transparency-begins-rolling-out-to-windows-insiders/</a>, See on <a href="https://news.ycombinator.com/item?id=37559256">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-js="panel-article-header">
					<p><span>
						<span>
							Written By
						</span>
						
					</span>
					<span>
						<span>
							published
						</span>
						<span>
							September 18, 2023
						</span>
					</span></p>
			</div><div data-index="0" data-js="panel" data-type="wysiwyg-with-aside" data-modular-content="" data-modular-content-collection="">
<p>Hello Windows Insiders,</p>
<p>Today we are beginning to roll out an update for the Paint app to Windows Insiders in the Canary and Dev Channels (version 11.2308.18.0 or higher). With this update, we are introducing support for layers and transparency!</p>
<p>You can now add, remove, and manage layers on the canvas to create richer and more complex digital art. With layers, you can stack shapes, text, and other image elements on top of each other. To get started, click on the new Layers button in the toolbar, which will open a panel on the side of the canvas. This is where you can add new layers to the canvas. Try changing the order of layers in this panel to see how the order of stacked image elements on the canvas changes. You can also show or hide and duplicate individual layers or merge layers together.</p>
<figure id="attachment_176579" aria-describedby="caption-attachment-176579"><a href="https://blogs.windows.com/wp-content/uploads/prod/sites/44/2023/09/Paint_Layers.png"><img decoding="async" src="https://blogs.windows.com/wp-content/uploads/prod/sites/44/2023/09/Paint_Layers-1024x642.png" alt="Paint composition of a cat utilizing multiple layers." width="1024" height="642"></a><figcaption id="caption-attachment-176579">Paint composition of a cat utilizing multiple layers.</figcaption></figure>
<p>We are adding support for transparency as well, including the ability to open and save transparent PNGs! When working with a single layer, you will notice a checkerboard pattern on the canvas indicating the portions of the image that are transparent. Erasing any content from the canvas now truly erases the content instead of painting the area white. When working with multiple layers, if you erase content on one layer, you will reveal the content in layers underneath.</p>
<figure id="attachment_176580" aria-describedby="caption-attachment-176580"><a href="https://blogs.windows.com/wp-content/uploads/prod/sites/44/2023/09/Paint_BGLayers.gif"><img decoding="async" loading="lazy" src="https://blogs.windows.com/wp-content/uploads/prod/sites/44/2023/09/Paint_BGLayers.gif" alt="Short animation showing the background getting removed in Paint." width="2063" height="1403"></a><figcaption id="caption-attachment-176580">Short animation showing the background getting removed in Paint.</figcaption></figure>
<p>When you combine layers, transparency, and other tools in Paint, you can create exciting new images and artwork! For example, when combined with the <a href="https://blogs.windows.com/windows-insider/2023/09/07/background-removal-in-paint-begins-rolling-out-to-windows-insiders/">new background removal feature</a>, you can quickly create interesting layered compositions.</p>
<p><strong>FEEDBACK: Please file feedback in&nbsp;</strong><a href="https://aka.ms/paintfeedback"><strong>Feedback Hub (WIN + F)&nbsp;under Apps &gt; Paint</strong></a><strong>.</strong></p>
<p><em>[PLEASE NOTE: We are beginning to roll these experiences out, so they may not be available to all Insiders in the Canary and Dev Channels just yet as we plan to monitor feedback and see how it lands before pushing it out to everyone.]</em></p>
<p>We love getting feedback from the community and are looking forward to your feedback on these updates!</p>
<p>Thanks,<br>
Dave Grochocki, Principal Product Manager Lead – Windows Inbox Apps</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iOS 17 is available today (136 pts)]]></title>
            <link>https://www.apple.com/newsroom/2023/09/ios-17-is-available-today/</link>
            <guid>37559161</guid>
            <pubDate>Mon, 18 Sep 2023 17:27:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/newsroom/2023/09/ios-17-is-available-today/">https://www.apple.com/newsroom/2023/09/ios-17-is-available-today/</a>, See on <a href="https://news.ycombinator.com/item?id=37559161">Hacker News</a></p>
<div id="readability-page-1" class="page">


	
    







 
<nav id="ac-localnav" lang="en-US" role="navigation" aria-label="Newsroom" data-analytics-region="local nav" data-sticky="">
	
    
    
        




    
    

</nav>



<main id="main" role="main"> 



<span id="opens-in-new-window">opens in new window</span>

	

<section>
<article data-analytics-activitymap-region-id="article">






    
    
    











    <div>
        

        <div>
                    
                    
                        <span>UPDATE</span>
                    
                    
                        <span>September 18, 2023</span>
                    
                    
                </div>

        <div>
                
                
                
                    <h2>
                        
    
        iOS 17 is available today
    

                    </h2>
                
            </div>

        

        
            
    
    
    
    
    

        

    </div>







    
    
    






  
    
    
    
    
      <figure aria-label="Media, Contact Posters, Live Stickers, and Live Voicemail are shown on iPhone 15 Pro.">
        <div>
             
              
              <div>
                iOS 17, available today as a free software update,&nbsp;upgrades the communications experience with Contact Posters, a new stickers experience, Live Voicemail, and much more.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2023/09/ios-17-is-available-today/article/Apple-iOS-17-iPhone-15-Pro-3-up.zip" download="" data-analytics-title="Download image" aria-label="Download media, Contact Posters, Live Stickers, and Live Voicemail are shown on iPhone 15 Pro."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div>iOS 17 makes iPhone even more personal and intuitive with major updates to communication apps; StandBy, a new way to experience iPhone when it is charging; easier sharing with AirDrop; and more intelligent input that improves the speed and accuracy of typing. <a href="https://www.apple.com/ios/ios-17" target="_blank">iOS 17</a> is available today as a free software update.&nbsp;
</div>
                 
             
                 <h2><strong>Incoming Calls Get a Major Upgrade&nbsp;</strong>
</h2>
                 
             
                 <div>The Phone app is essential to the iPhone experience, and it receives a big update that makes the calls that matter stand out even more. Personalized Contact Posters provide a new way for users to express themselves by customizing how they appear when they call known contacts, including in third-party calling apps. Contact Posters can be personalized with beautiful treatments for photos, Memoji, and eye-catching typography and font colors.
</div>
                 
             
         </div>
 

    
    
    


    
        
        
        
        
            <figure aria-label="Media, Contact Posters are shown on iPhone 15 Pro.">
                <div>
                         
                            
                            <div>
                                Contact Posters provide users with a new way to express themselves and bring a completely new look to incoming calls.
                            </div>
                        
                        
                        
                        
                        <a href="https://www.apple.com/newsroom/images/2023/09/ios-17-is-available-today/article/Apple-iOS-17-Contact-Posters.zip" download="" data-analytics-title="Download image" aria-label="Download media, Contact Posters are shown on iPhone 15 Pro."></a>
                    </div>
            </figure>
        
    










    
    
    


     
     
    
    
        <div>
             
                 <div>Live Voicemail provides a real-time transcription as someone leaves a voicemail, and gives users the opportunity to pick up while the caller is leaving their message. With the power of the Neural Engine, Live Voicemail transcription is handled on-device and remains private. With Silence Unknown Callers enabled, unknown numbers are transferred directly to Live Voicemail, and calls identified as spam by carriers are instantly declined.
</div>
                 
             
                 <h2><strong>New Ways to Enjoy FaceTime</strong>
</h2>
                 
             
                 <div>Users are now able to leave a video or audio message on FaceTime to capture exactly what they want to say when someone they call is not available. FaceTime calls get more expressive with Reactions such as hearts, balloons, fireworks, and laser beams that can be activated by simple gestures, and are also available with supported third-party video conferencing apps, such as Zoom and Webex by Cisco.
</div>
                 
             
         </div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>FaceTime now supports video and audio messages so when users call someone who is not available, they can share a moment or message that can be enjoyed later.</div>
        
            <a aria-label="Download video: FaceTime video messages on iPhone 15" data-analytics-title="Download video - FaceTime video messages on iPhone 15" download="" href="https://www.apple.com/newsroom/videos/apple-ios-17-facetime/downloads/Apple-iOS-17-FaceTime-video-messages.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>
             
                 <div>FaceTime now features an incredible connected experience with Apple TV 4K. To see friends and family on the big screen, users can use Continuity Camera to start a FaceTime call on iPhone and hand it off to Apple TV, or initiate the FaceTime call directly from their Apple TV. With Center Stage, users stay perfectly framed even as they move around the room.
</div>
                 
             
                 <h2><strong>More Ways to Stay Connected with Messages&nbsp;</strong>
</h2>
                 
             
                 <div>Messages adds new features that make it easier to use and even more fun when connecting with the people who matter the most.&nbsp;
</div>
                 
             
                 <div>Search gets more powerful and precise with search filters; audio messages get automatically transcribed so users can read them in the moment or listen later; replying inline becomes as simple as swiping on a text bubble; and the new expandable menu provides easy access to all iMessage apps, giving Messages a sleeker look.&nbsp;
</div>
                 
             
                 <div>Users now have another way to customize their messages with an all-new stickers experience that adds the ability to create Live Stickers by lifting subjects from photos. Fun effects can be added to stickers, bringing them to life and giving users a way to be creative.
</div>
                 
             
         </div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>An all-new stickers experience adds the ability to create Live Stickers by lifting subjects from photos, and users can add effects that bring the stickers to life.</div>
        
            <a aria-label="Download video: Live Stickers on iPhone 15 Pro" data-analytics-title="Download video - Live Stickers on iPhone 15 Pro" download="" href="https://www.apple.com/newsroom/videos/apple-ios-17-live-stickers/downloads/Apple-iOS-17-Live-Stickers.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>Check In, an important feature built into Messages, lets users notify a family member or friend when they have made it to their destination safely. After a user starts a Check In, their contact will automatically be notified as soon as they arrive. If they are not making progress toward their destination, useful information, such as the device’s location, battery level, and cell service status, is temporarily shared with the selected contact in a secure and private way.
</div>
 

    
    
    


    
        
        
        
        
            <figure aria-label="Media, Check In in Messages is shown on iPhone 15 Pro.">
                <div>
                         
                            
                            <div>
                                Check In, a new feature in Messages, lets users notify a family member or friend when they have made it to their destination safely.
                            </div>
                        
                        
                        
                        
                        <a href="https://www.apple.com/newsroom/images/2023/09/ios-17-is-available-today/article/Apple-iOS-17-Check-in.zip" download="" data-analytics-title="Download image" aria-label="Download media, Check In in Messages is shown on iPhone 15 Pro."></a>
                    </div>
            </figure>
        
    










    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>StandBy Makes iPhone Even More Useful While It’s Charging</strong>
</h2>
                 
             
                 <div>StandBy is a new full-screen experience with glanceable information designed to be viewed from a distance while iPhone is on its side and charging. StandBy is perfect on a desk, nightstand, or kitchen counter, and can be personalized to display a range of clock styles, favorite photos, or widgets, including Smart Stacks, which surface the right widgets at the right time. StandBy also displays full-screen Live Activities, Siri results, incoming calls, and larger notifications. With the Always-On display, StandBy stays on to show useful information, and with Night Mode, StandBy adapts to low light, so clocks, photos, and widgets take on a beautiful red tone. When using a MagSafe charger, StandBy will remember a user’s preferred view for that MagSafe charging dock.&nbsp;
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="ios-17-standby">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-f3e885e38ced6c66f5ef552f795a33c9" href="#gallery-f3e885e38ced6c66f5ef552f795a33c9" data-ac-gallery-trigger="gallery-f3e885e38ced6c66f5ef552f795a33c9"><span>StandBy Music is shown on iPhone 15.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-2231becc565c4696ae2df8ab1ea884d4" href="#gallery-2231becc565c4696ae2df8ab1ea884d4" data-ac-gallery-trigger="gallery-2231becc565c4696ae2df8ab1ea884d4"><span>StandBy clock face is shown on iPhone 15.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-99a69a44da95e810fc5664ce5c666bf3" href="#gallery-99a69a44da95e810fc5664ce5c666bf3" data-ac-gallery-trigger="gallery-99a69a44da95e810fc5664ce5c666bf3"><span>StandBy Live Activities is shown on iPhone 15.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-f3e885e38ced6c66f5ef552f795a33c9" aria-labelledby="gallery-dotnav-f3e885e38ced6c66f5ef552f795a33c9" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:music-iphone-15">
                                
                                <div>
                                    <div>StandBy gives users a full-screen experience with glanceable information designed to be viewed from a distance when iPhone is on its side and charging.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2023/09/ios-17-is-available-today/article/Apple-iOS-17-StandBy-Music.zip" download="" data-analytics-title="Download image" aria-label="Download media, StandBy Music is shown on iPhone 15."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-2231becc565c4696ae2df8ab1ea884d4" aria-labelledby="gallery-dotnav-2231becc565c4696ae2df8ab1ea884d4" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:clock-face-iphone-15">
                                
                                <div>
                                    <div>StandBy gives users a full-screen experience with glanceable information designed to be viewed from a distance when iPhone is on its side and charging.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2023/09/ios-17-is-available-today/article/Apple-iOS-17-StandBy-clock-face.zip" download="" data-analytics-title="Download image" aria-label="Download media, StandBy clock face is shown on iPhone 15."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-99a69a44da95e810fc5664ce5c666bf3" aria-labelledby="gallery-dotnav-99a69a44da95e810fc5664ce5c666bf3" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:live-activities-iphone-15">
                                
                                <div>
                                    <div>StandBy gives users a full-screen experience with glanceable information designed to be viewed from a distance when iPhone is on its side and charging.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2023/09/ios-17-is-available-today/article/Apple-iOS-17-StandBy-Live-Activities-SmartGym.zip" download="" data-analytics-title="Download image" aria-label="Download media, StandBy Live Activities is shown on iPhone 15."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Easier Sharing with AirDrop and NameDrop</strong>
</h2>
                 
             
                 <div>AirDrop makes it easier than ever to share with friends, family, and colleagues. NameDrop, a new AirDrop feature, lets users exchange contact information, including their Contact Poster, simply by bringing their iPhone devices together. With the same gesture, users can also share content or start SharePlay to listen to music, watch a movie, or play a game while in close proximity. And later this year, AirDrop will add the ability to continue transfers over the internet when a user steps out of AirDrop range.
</div>
                 
             
         </div>
 

    
    
    


    
    <div data-component-list="ScrollAnimationDefault AutoPlayVideo">
        <div>NameDrop allows users to exchange contact information by simply bringing their iPhone devices together.</div>
        
            <a aria-label="Download video: NameDrop on iPhone 15" data-analytics-title="Download video - NameDrop on iPhone 15" download="" href="https://www.apple.com/newsroom/videos/apple-ios-17-namedrop/downloads/Apple-iOS-17-NameDrop.zip">
            </a>
        
    </div>






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Comprehensive Updates to Autocorrect and Dictation</strong>
</h2>
                 
             
                 <div>Autocorrect receives a comprehensive update with a new transformer language model, a state-of-the-art on-device machine learning language model for word prediction —&nbsp;improving the experience and accuracy for users every time they type. Sentence-level autocorrections can fix more types of grammatical mistakes, and the refreshed design better supports typing by temporarily underlining corrected words and allowing users to revert back with just a tap. Inline predictive text helps quickly finish sentences, while Dictation’s new speech recognition model brings improved accuracy.&nbsp;
</div>
                 
             
                 <h2><strong>Reflecting on Life’s Moments with Journal</strong>
</h2>
                 
             
                 <div>Journal is a new iPhone app that helps users reflect on everyday moments and special events in their lives. To help inspire a user’s journal entry, personalized suggestions can be intelligently curated from a user’s recent activity, such as photos, people, places, workouts, and more, and scheduled notifications can help build a journaling habit. With the ability to lock the app, the use of on-device processing, and end-to-end encryption, Journal is built to protect a user’s privacy and ensure no one — including Apple — can access a user’s entries. With the new Journaling Suggestions API, developers will be able to add journaling suggestions to their apps. The Journal app and Journaling Suggestions API will be available in a software update later this year.
</div>
                 
             
         </div>
 

    
    
    


    
        
        
        
        
            <figure aria-label="Media, Journal is shown on iPhone 15.">
                <div>
                         
                            
                            <div>
                                Journal, a new app that helps users reflect on everyday moments and special events in their lives, uses on-device machine learning to create personalized suggestions to inspire a user’s journal entry.
                            </div>
                        
                        
                        
                        
                        <a href="https://www.apple.com/newsroom/images/2023/09/ios-17-is-available-today/article/Apple-iOS-17-Journal.zip" download="" data-analytics-title="Download image" aria-label="Download media, Journal is shown on iPhone 15."></a>
                    </div>
            </figure>
        
    










    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Additional Features in iOS 17</strong>
</h2>
                 
             
                 <div><ul>
<li>Profiles in <strong>Safari</strong> keep browsing — such as history, cookies, extensions, Tab Groups, and Favorites — separate for topics like work and personal. Private Browsing now locks when not in use and adds greater protection, both from trackers as a user browses, and from people who might have access to a user’s device.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>For easier and more secure <strong>password and passkeys </strong>sharing, users can share passwords with a group of trusted contacts. Since sharing is through iCloud Keychain, it is end-to-end encrypted.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>The <strong>Health</strong> app offers new mental health features. Users can log their daily moods and momentary emotions; see what might be contributing to their state of mind; and easily access depression and anxiety assessments often used in clinics, plus resources available in their region. Additionally, increasing the distance the device is viewed from can help children lower their risk of myopia and gives adult users the opportunity to reduce digital eyestrain. Screen Distance in <strong>Screen Time</strong> uses the TrueDepth camera to encourage users to move their device farther away after holding it closer than 12 inches from their face for an extended period of time.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Apple Music</strong> adds SharePlay support in the car, making it possible for all passengers to easily control the music right from their own devices, even if they don’t have an Apple Music subscription. Crossfade smoothly transitions between songs, and later this year, users will be able to collaborate on playlists in Apple Music, making listening to music with friends easier than ever before.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>AirPlay</strong>&nbsp;gets even better with iPhone using on-device intelligence to learn a user’s preferences, and later this year, will add support for televisions in hotels, allowing users to easily enjoy their favorite content on the TV when traveling.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>AirPods</strong> receive powerful new features, including Adaptive Audio, Personalized Volume, and Conversation Awareness, that redefine the personal audio experience. Plus, improvements to Automatic Switching and call controls make AirPods even easier to use.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>The <strong>Home</strong> app adds the ability for users to view up to 30 days of activity history across door locks, garage doors, alarm systems, and contact sensors.<sup>1 </sup>Additionally, two popular HomeKit lock features — tap to unlock and PIN codes — will become available for Matter-compatible locks, providing even more ways to connect the home.<sup>2</sup> Grid Forecast is a new tool in the Home app that shows when a customer’s electrical grid has cleaner energy sources available, so they can plan when to charge devices or run appliances.<sup>3</sup></li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Maps</strong> adds offline maps, so users can download a specific area and access turn-by-turn navigation, see their estimated time of arrival, find places in Maps, and more while offline. Maps also makes it easier than ever to discover thousands of trails in parks across the United States, and supports electric vehicle drivers with real-time charging availability information.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>AirTag</strong>&nbsp;can be shared with up to five other people, allowing friends and family to keep track of an item in Find My. Everyone in a group will be able to see an item’s location, play a sound, and use Precision Finding to help pinpoint the location of a shared AirTag when nearby. This also works with all other Find My network accessories.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Grocery lists in <strong>Reminders&nbsp;</strong>automatically group added items into relevant categories to make shopping easier. Users can change how the items are grouped, and the list remembers their preferences.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Visual Look Up</strong> is now available in paused video frames. Users can lift individual or multiple subjects from the background of photos and videos, and identify food, storefronts, signs, and symbols.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Siri&nbsp;</strong>can be activated by simply saying “Siri.” Once activated, users can issue multiple commands in succession without needing to reactivate the assistant.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Photos </strong>uses on-device machine learning to recognize individual cats and dogs in the People album, just like friends or family members.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Privacy</strong> updates include Communication Safety expanding beyond Messages to help keep kids safe when sending and receiving content via AirDrop, Contact Posters, a FaceTime message, and when using the Photos picker to choose content to send. It also expands to cover video content in addition to still images. A new feature, Sensitive Content Warning, helps adult users avoid seeing unwanted nude images and videos. As with Communication Safety, all image and video processing for Sensitive Content Warning occurs on-device, so Apple does not get access to the content.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>The <strong>App Store’s</strong> Today tab gets updated to offer the most dynamic and personalized app discovery experience yet. Users can discover more great apps, games, and in-app events through new tailored recommendations and original stories based on their interests and preferences, as well as helpfully curated and easy-to-browse sections.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Accessibility</strong>&nbsp;updates include Assistive Access, a customizable interface that helps users with cognitive disabilities use iPhone with greater ease and independence; Live Speech, which gives nonspeaking users the option to type and have their words spoken in person, or on phone and FaceTime calls; Personal Voice, which gives users at risk of speech loss the option to create a voice that sounds like theirs; and Point and Speak, which helps users who are blind or have low vision read text on physical objects by pointing.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="iOS-17-additional-features">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-3773aa847c1bc054582a3655c604b02d" href="#gallery-3773aa847c1bc054582a3655c604b02d" data-ac-gallery-trigger="gallery-3773aa847c1bc054582a3655c604b02d"><span>Health tracking a user’s state of mind is shown on iPhone 15 Pro.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-780b225daafb63d0da2fe99f72617711" href="#gallery-780b225daafb63d0da2fe99f72617711" data-ac-gallery-trigger="gallery-780b225daafb63d0da2fe99f72617711"><span>Offline Maps is shown on iPhone 15 Pro.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-7936a40b181a45e0cd51c79c650c7756" href="#gallery-7936a40b181a45e0cd51c79c650c7756" data-ac-gallery-trigger="gallery-7936a40b181a45e0cd51c79c650c7756"><span>Profiles in Safari are shown on iPhone 15 Pro.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-93ffede983ea2c5bbeaeeb203c8c504c" href="#gallery-93ffede983ea2c5bbeaeeb203c8c504c" data-ac-gallery-trigger="gallery-93ffede983ea2c5bbeaeeb203c8c504c"><span>Visual Look Up is shown on iPhone 15 Pro.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-29bd65d17dc4d0ed23fe3d681f028a3f" href="#gallery-29bd65d17dc4d0ed23fe3d681f028a3f" data-ac-gallery-trigger="gallery-29bd65d17dc4d0ed23fe3d681f028a3f"><span>Communication Safety on Messages is shown on iPhone 15 Pro.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-3773aa847c1bc054582a3655c604b02d" aria-labelledby="gallery-dotnav-3773aa847c1bc054582a3655c604b02d" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:health-state-of-mind-iphone-15-pro">
                                
                                <div>
                                    <div>The Health app now offers new mental health features, including the ability for users to log their daily moods and momentary emotions.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2023/09/ios-17-is-available-today/article/Apple-iOS-17-Health-state-of-mind.zip" download="" data-analytics-title="Download image" aria-label="Download media, Health tracking a user’s state of mind is shown on iPhone 15 Pro."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-780b225daafb63d0da2fe99f72617711" aria-labelledby="gallery-dotnav-780b225daafb63d0da2fe99f72617711" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:offline-maps-iphone-15-pro">
                                
                                <div>
                                    <div>Maps adds offline maps, so users can download a specific area and access turn-by-turn navigation, see their estimated time of arrival, find places in Maps, and more while offline.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2023/09/ios-17-is-available-today/article/Apple-iOS-17-Offline-Maps.zip" download="" data-analytics-title="Download image" aria-label="Download media, Offline Maps is shown on iPhone 15 Pro."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-7936a40b181a45e0cd51c79c650c7756" aria-labelledby="gallery-dotnav-7936a40b181a45e0cd51c79c650c7756" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:profiles-safari-iphone-15-pro">
                                
                                <div>
                                    <div>Profiles in Safari keep browsing history, Tab Groups, and Favorites separate for topics like work and personal.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2023/09/ios-17-is-available-today/article/Apple-iOS-17-Safari-Profiles.zip" download="" data-analytics-title="Download image" aria-label="Download media, Profiles in Safari are shown on iPhone 15 Pro."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-93ffede983ea2c5bbeaeeb203c8c504c" aria-labelledby="gallery-dotnav-93ffede983ea2c5bbeaeeb203c8c504c" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:visual-look-up-iphone-15-pro">
                                
                                <div>
                                    <div>Visual Look Up can now identify food, storefronts, signs, and symbols.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2023/09/ios-17-is-available-today/article/Apple-iOS-17-Visual-Look-Up.zip" download="" data-analytics-title="Download image" aria-label="Download media, Visual Look Up is shown on iPhone 15 Pro."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-29bd65d17dc4d0ed23fe3d681f028a3f" aria-labelledby="gallery-dotnav-29bd65d17dc4d0ed23fe3d681f028a3f" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:communication-safety-messages-iphone-15-pro">
                                
                                <div>
                                    <div>A new Privacy feature, Sensitive Content Warning, helps adult users avoid seeing unwanted nude images and videos.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2023/09/ios-17-is-available-today/article/Apple-iOS-17-Messages-Communication-Safety.zip" download="" data-analytics-title="Download image" aria-label="Download media, Communication Safety on Messages is shown on iPhone 15 Pro."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>iOS 17 is a free software update that is available starting today for iPhone Xs and later. For more information, visit <a href="http://apple.com/ios/ios-17" target="_blank">apple.com/ios/ios-17</a>. Some features may not be available in all regions, languages, or on all iPhone models. For more information about availability, visit <a href="https://www.apple.com/ios/feature-availability" target="_blank">apple.com/ios/feature-availability</a>.
</div>
 

    
    
    




    
    
        
    


    
    
    



    
    
    





    
    
    <div>
            <ol>
<li>The ability to view up to 30 days of activity history in the Home app is available for homes using the new home architecture and a HomePod or Apple TV running as a home hub.</li>
<li>Tap to unlock will become available for Matter-compatible locks later this year.</li>
<li>Grid Forecast will be available in the contiguous United States.</li>
</ol>

        </div>



    
    
    






    
















	
	
	
		















	
	

</article>



</section>
</main>


	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: HyperDX – open-source dev-friendly Datadog alternative (446 pts)]]></title>
            <link>https://github.com/hyperdxio/hyperdx</link>
            <guid>37558357</guid>
            <pubDate>Mon, 18 Sep 2023 16:25:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/hyperdxio/hyperdx">https://github.com/hyperdxio/hyperdx</a>, See on <a href="https://news.ycombinator.com/item?id=37558357">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/hyperdxio/hyperdx/blob/main/.github/images/logo_dark.png#gh-dark-mode-only"><img alt="hyperdx logo dark" src="https://github.com/hyperdxio/hyperdx/raw/main/.github/images/logo_dark.png#gh-dark-mode-only"></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/hyperdxio/hyperdx/blob/main/.github/images/logo_light.png#gh-light-mode-only"><img alt="hyperdx logo light" src="https://github.com/hyperdxio/hyperdx/raw/main/.github/images/logo_light.png#gh-light-mode-only"></a>
</p>
<hr>
<h2 tabindex="-1" dir="auto">HyperDX</h2>
<p dir="auto"><a href="https://hyperdx.io/" rel="nofollow">HyperDX</a> helps engineers figure out why production is
broken faster by centralizing and correlating logs, metrics, traces, exceptions
and session replays in one place. An open source and developer-friendly
alternative to Datadog and New Relic.</p>
<p dir="auto">
  <a href="https://www.hyperdx.io/docs" rel="nofollow">Documentation</a> • <a href="https://discord.gg/FErRRKU78j" rel="nofollow">Chat on Discord</a>  • <a href="https://api.hyperdx.io/login/demo" rel="nofollow">Live Demo</a>  • <a href="https://github.com/hyperdxio/hyperdx/issues/new">Bug Reports</a> • <a href="https://github.com/hyperdxio/hyperdx/blob/main/CONTRIBUTING.md">Contributing</a>
</p>
<ul dir="auto">
<li>🕵️ Correlate end to end, go from browser session replay to logs and traces in
just a few clicks</li>
<li>🔥 Blazing fast performance powered by Clickhouse</li>
<li>🔍 Intuitive full-text search and property search syntax (ex. <code>level:err</code>)</li>
<li>🤖 Automatically cluster event patterns from billions of events</li>
<li>📈 Dashboard high cardinality events without a complex query language</li>
<li>🔔 Set up alerts in just a few clicks</li>
<li><code>{</code> Automatic JSON/structured log parsing</li>
<li>🔭 OpenTelemetry native</li>
</ul>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/hyperdxio/hyperdx/blob/main/.github/images/search_splash.png"><img alt="Search logs and traces all in one place" src="https://github.com/hyperdxio/hyperdx/raw/main/.github/images/search_splash.png" title="Search logs and traces all in one place"></a></p>
<h3 tabindex="-1" dir="auto">Additional Screenshots</h3>
<details>
  <summary><b>📈 Dashboards</b></summary>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/hyperdxio/hyperdx/blob/main/.github/images/dashboard.png"><img alt="Dashboard" src="https://github.com/hyperdxio/hyperdx/raw/main/.github/images/dashboard.png"></a>
</details>
<details>
  <summary><b>🤖 Automatic Event Pattern Clustering</b></summary>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/hyperdxio/hyperdx/blob/main/.github/images/pattern3.png"><img alt="Event Pattern Clustering" src="https://github.com/hyperdxio/hyperdx/raw/main/.github/images/pattern3.png"></a>
</details>
<details>
  <summary><b>🖥️ Session Replay &amp; RUM</b></summary>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/hyperdxio/hyperdx/blob/main/.github/images/session.png"><img alt="Event Pattern Clustering" src="https://github.com/hyperdxio/hyperdx/raw/main/.github/images/session.png"></a>
</details>
<h2 tabindex="-1" dir="auto">Spinning Up HyperDX</h2>
<p dir="auto">The HyperDX stack ingests, stores, and searches/graphs your telemetry data.
After standing up the Docker Compose stack, you'll want to instrument your app
to send data over to HyperDX.</p>
<p dir="auto">You can get started by deploying a complete stack via Docker Compose. After
cloning this repository, simply start the stack with:</p>

<p dir="auto">Afterwards, you can visit <a href="http://localhost:8080/" rel="nofollow">http://localhost:8080</a> to access the HyperDX UI.</p>
<blockquote>
<p dir="auto">If your server is behind a firewall, you'll need to open/forward port 8080,
8000 and 4318 on your firewall for the UI, API and OTel collector
respectively.</p>
</blockquote>
<blockquote>
<p dir="auto">We recommend at least 4GB of RAM and 2 cores for testing.</p>
</blockquote>
<p dir="auto"><strong>Enabling Self-instrumentation/Demo Logs</strong></p>
<p dir="auto">To get a quick preview of HyperDX, you can enable self-instrumentation and demo
logs by setting the <code>HYPERDX_API_KEY</code> to your ingestion key (go to
<a href="http://localhost:8080/team" rel="nofollow">http://localhost:8080/team</a> after creating your
account) and then restart the stack.</p>
<p dir="auto">This will redirect internal telemetry from the frontend app, API, host metrics
and demo logs to your new HyperDX instance.</p>
<p dir="auto">ex.</p>
<div dir="auto" data-snippet-clipboard-copy-content="HYPERDX_API_KEY=<YOUR_INGESTION_KEY> docker compose up -d"><pre>HYPERDX_API_KEY=<span>&lt;</span>YOUR_INGESTION_KEY<span>&gt;</span> docker compose up -d</pre></div>
<blockquote>
<p dir="auto">If you need to use <code>sudo</code> for docker, make sure to forward the environment
variable with the <code>-E</code> flag:
<code>HYPERDX_API_KEY=&lt;YOUR_KEY&gt; sudo -E docker compose up -d</code></p>
</blockquote>
<h3 tabindex="-1" dir="auto">Hosted Cloud</h3>
<p dir="auto">HyperDX is also available as a hosted cloud service at
<a href="https://hyperdx.io/" rel="nofollow">hyperdx.io</a>. You can sign up for a free account and start
sending data in minutes.</p>
<h2 tabindex="-1" dir="auto">Instrumenting Your App</h2>
<p dir="auto">To get logs, metrics, traces, session replay, etc into HyperDX, you'll need to
instrument your app to collect and send telemetry data over to your HyperDX
instance.</p>
<p dir="auto">We provide a set of SDKs and integration options to make it easier to get
started with HyperDX, such as
<a href="https://www.hyperdx.io/docs/install/browser" rel="nofollow">Browser</a>,
<a href="https://www.hyperdx.io/docs/install/javascript" rel="nofollow">Node.js</a>, and
<a href="https://www.hyperdx.io/docs/install/python" rel="nofollow">Python</a></p>
<p dir="auto">You can find the full list in <a href="https://www.hyperdx.io/docs" rel="nofollow">our docs</a>.</p>
<p dir="auto"><strong>OpenTelemetry</strong></p>
<p dir="auto">Additionally, HyperDX is compatible with
<a href="https://opentelemetry.io/" rel="nofollow">OpenTelemetry</a>, a vendor-neutral standard for
instrumenting your application backed by CNCF. Supported languages/platforms
include:</p>
<ul dir="auto">
<li>Kubernetes</li>
<li>Javascript</li>
<li>Python</li>
<li>Java</li>
<li>Go</li>
<li>Ruby</li>
<li>PHP</li>
<li>.NET</li>
<li>Elixir</li>
<li>Rust</li>
</ul>
<p dir="auto">(Full list <a href="https://opentelemetry.io/docs/instrumentation/" rel="nofollow">here</a>)</p>
<p dir="auto">Once HyperDX is running, you can point your OpenTelemetry SDK to the
OpenTelemetry collector spun up at <code>http://localhost:4318</code>.</p>
<h2 tabindex="-1" dir="auto">Contributing</h2>
<p dir="auto">We welcome all contributions! There's many ways to contribute to the project,
including but not limited to:</p>
<ul dir="auto">
<li>Opening a PR (<a href="https://github.com/hyperdxio/hyperdx/blob/main/CONTRIBUTING.md">Contribution Guide</a>)</li>
<li><a href="https://github.com/hyperdxio/hyperdx/issues/new">Submitting feature requests or bugs</a></li>
<li>Improving our product or contribution documentation</li>
<li>Voting on <a href="https://github.com/hyperdxio/hyperdx/issues">open issues</a> or contributing use cases to a feature request</li>
</ul>
<h2 tabindex="-1" dir="auto">Motivation</h2>
<p dir="auto">Our mission is to help engineers ship reliable software. To enable that, we
believe every engineer needs to be able to easily leverage production telemetry
to quickly solve burning production issues.</p>
<p dir="auto">However, in our experience, the existing tools we've used tend to fall short in
a few ways:</p>
<ol dir="auto">
<li>They're expensive, and the pricing has failed to scale with TBs of telemetry
becoming the norm, leading to teams aggressively cutting the amount of data
they can collect.</li>
<li>They're hard to use, requiring full-time SREs to set up, and domain experts
to use confidently.</li>
<li>They requiring hopping from tool to tool (logs, session replay, APM,
exceptions, etc.) to stitch together the clues yourself.</li>
</ol>
<p dir="auto">We're still early on in our journey, but are building in the open to solve these
key issues in observability. We hope you give HyperDX a try and let us know how
we're doing!</p>
<h2 tabindex="-1" dir="auto">Open Source vs Hosted Cloud</h2>
<p dir="auto">HyperDX is open core, with most of our features available here under an MIT
license. We have a cloud-hosted version available at
<a href="https://hyperdx.io/" rel="nofollow">hyperdx.io</a> with a few <a href="https://www.hyperdx.io/docs/oss-vs-cloud" rel="nofollow">additional features</a> beyond what's
offered in the open source version.</p>
<p dir="auto">Our cloud hosted version exists so that we can build a sustainable business and
continue building HyperDX as an open source platform. We hope to have more
comprehensive documentation on how we balance between cloud-only and open source
features in the future. In the meantime, we're highly aligned with Gitlab's
<a href="https://handbook.gitlab.com/handbook/company/stewardship/" rel="nofollow">stewardship model</a>.</p>
<h2 tabindex="-1" dir="auto">Contact</h2>
<ul dir="auto">
<li><a href="https://github.com/hyperdxio/hyperdx/issues/new">Open an Issue</a></li>
<li><a href="https://discord.gg/FErRRKU78j" rel="nofollow">Discord</a></li>
<li><a href="mailto:support@hyperdx.io">Email</a></li>
</ul>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto"><a href="https://github.com/hyperdxio/hyperdx/blob/main/LICENSE">MIT</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[California's Delete Act will make erasing consumers' data easier (110 pts)]]></title>
            <link>https://www.techspot.com/news/100180-california-delete-act-make-erasing-consumers-data-easier.html</link>
            <guid>37556802</guid>
            <pubDate>Mon, 18 Sep 2023 14:43:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techspot.com/news/100180-california-delete-act-make-erasing-consumers-data-easier.html">https://www.techspot.com/news/100180-california-delete-act-make-erasing-consumers-data-easier.html</a>, See on <a href="https://news.ycombinator.com/item?id=37556802">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>TechSpot is celebrating its 25th anniversary. TechSpot means tech analysis and advice <a href="https://www.techspot.com/ethics.html" target="_blank">you&nbsp;can&nbsp;trust</a>.</p><div>
<p id="why-it-matters"><strong>What just happened?</strong> A newly approved bill will make life easier for Californian consumers wanting their data removed from online databases. Proponents of the bill are saying that deletion rights are now a "vital necessity," while lawyers and advertisers are depicting a decaying, byzantine Californian digital world. </p>
<p>On September 14, the final day of the 2023 legislative session, the California Senate finally passed <a href="https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB362">Senate Bill 362</a>. Also known as the "Delete Act," the new law is designed to provide consumers in the most populous US state a new right to privacy against the commercial interests of data brokers. The bill will now need to be signed into law by Governor Gavin Newsom, who has until October 14 to comply.</p>
<p>According to Ashkan Soltani, executive director of the California Privacy Protection Agency (CPPA), Newsom is expected to sign the bill in due time. The Delete Act establishes a new "accessible deletion mechanism," which will make deleting consumer information collected by data brokers much easier and faster.</p>
<p>Under the new law, the CPPA will be tasked with developing a new system by 2026. The system will give California residents the ability to make a single data deletion request against all the 500 data brokers officially operating in the state. CPPA will also need to enforce the law, ensuring that the brokers will actually delete a customer's personal information every 45 days after receiving a "verified" deletion request.</p>
<p><picture><source type="image/webp" data-srcset="https://www.techspot.com/images2/news/bigimage/2023/09/2023-09-18-image-5-j_500.webp 500w, https://www.techspot.com/images2/news/bigimage/2023/09/2023-09-18-image-5-j_1100.webp 1100w, https://www.techspot.com/images2/news/bigimage/2023/09/2023-09-18-image-5-j.webp 2500w" data-sizes="(max-width: 960px) 100vw, 680px"><img height="1719" src="https://www.techspot.com/images2/news/bigimage/2023/09/2023-09-18-image-5.jpg" width="2500" alt="" data-src="https://www.techspot.com/images2/news/bigimage/2023/09/2023-09-18-image-5.jpg" data-srcset="https://www.techspot.com/images2/news/bigimage/2023/09/2023-09-18-image-5-j_500.webp 500w, https://www.techspot.com/images2/news/bigimage/2023/09/2023-09-18-image-5-j_1100.webp 1100w, https://www.techspot.com/images2/news/bigimage/2023/09/2023-09-18-image-5-j.webp 2500w" sizes="(max-width: 960px) 100vw, 680px" srcset="https://www.techspot.com/images2/news/bigimage/2023/09/2023-09-18-image-5-j_500.webp 500w, https://www.techspot.com/images2/news/bigimage/2023/09/2023-09-18-image-5-j_1100.webp 1100w, https://www.techspot.com/images2/news/bigimage/2023/09/2023-09-18-image-5-j.webp 2500w"></picture></p>
<p>Democratic senator Josh Becker, who first introduced the Delete Act in California Senate, previously said that the bill would close a loophole within the California Consumer Privacy Act. Before the Delete Act, consumers had to contact every single data broker if they wished for their data to be erased. The Delete Act is based on a very simple premise, <a href="https://iapp.org/news/a/california-legislature-passes-delete-act-for-pi-aggregated-by-data-brokers/">Becker said</a>: "Every Californian should be able to control who has access to their personal information and what they can do with it."</p>
<p>Data brokers spend "their days and nights" building dossiers with millions of people's data about reproductive healthcare, geolocation, and more, Becker stated, even purchasing data so they can later "sell it to the highest bidder." Tom Kemp, who advised lawmakers in drafting the bill and is an investor in multiple data deletion companies, said that in "post-abortion rights America" the selling of "very sensitive data" such as reproductive healthcare or precise geolocation has made things "intolerable for many people."</p>
<p>Opponents of the Delete Act include representatives from Kelley Drye &amp; Warren Partner. The law firm highlighted how the bill would make things much more complicated for companies, who need to invest money to comply with these new laws. Advertisers are fuming, with executives from the Association of National Advertisers (ANA) talking about a scheme conceived to "enrich pay-to-play deletion" services.</p>
<p>The Delete Act will encourage the mass deletion of data that is "the lifeblood of California's digital economy," ANA's executive VP Chris Oswald stated, and the bill also includes "glaring and dramatic failures." Without a "robust data marketplace," the advertisers said, Californians will fall victim to more fraud and identity theft because "their identities can't be verified."</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kargo, a multi-stage application lifecycle orchestrator (109 pts)]]></title>
            <link>https://akuity.io/blog/introducing-kargo/</link>
            <guid>37556719</guid>
            <pubDate>Mon, 18 Sep 2023 14:38:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://akuity.io/blog/introducing-kargo/">https://akuity.io/blog/introducing-kargo/</a>, See on <a href="https://news.ycombinator.com/item?id=37556719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>We are thrilled to announce Kargo, a multi-stage application lifecycle orchestrator for continuously delivering and promoting changes through environments. Kargo, brought to you by the creators of <a href="https://akuity.io/what-is-argo">the Argo Project</a>, is a reimagining of CD pipelines for the cloud-native era, with first-class GitOps support, progressive delivery features, and is 100% open source.</p><p>Kargo, as the name implies, is about transporting “freight” (what we call build and configuration artifacts) to multiple environments with a first-class GitOps approach. GitOps has been transformational in how it has elevated the practice of infrastructure-as-code to the next level. However, the practice of GitOps has created new challenges for traditional CI/CD pipelines. Pull-based GitOps operators, such as Argo CD, meant CI pipelines no longer had direct access to production environments. The async nature of Kubernetes declarative APIs and eventual consistency resulted in imperative processes like testing and analysis difficult to coordinate. Argo CD improved the situation by providing convenient interfaces to Kubernetes clusters, such as health assessments, sync hooks, and waved deployments, but it has not been enough.</p><p>As soon as there is a need to coordinate deployments over multiple environments, GitOps falls short. One of our most common conversations with customers is what to do after Argo CD is up and running. How can platform teams provide a user experience to their application developers and give them the autonomy to move their changes safely into production?</p><p>Until now, the answer to this question has been: cobbled-up scripts and bespoke, fragile automation. If we were to ask ten different organizations how they handle environment promotion, we would get ten different responses, everyone repeating the same patterns and even anti-patterns. Invariably, CI systems are over-leveraged to handle the job of CD.</p><p>At its core, the goals of CI are markedly different from the goals of CD. CI aims to build and produce an artifact as efficiently as possible. On the other hand, the objective of CD is to carry an artifact as safely as possible to production. Whereas CI is generally a short-lived job, CD is often a long, drawn-out process where promoting something from dev to production might take hours, if not days. And until now, adequate tooling has yet to reflect those needs.</p><p>Kargo aims to change all of that. Fundamentally, Kargo takes an entirely different approach to the problem of effecting change to multiple environments. Unlike CI, Kargo deployment pipelines are not generic “jobs” with a beginning, a middle, and an end, relying on executing shell commands against each environment.</p><p>Instead, Kargo Stages are used to model your environments and are the basic building blocks of your deployment pipeline. Stages are independently defined and loosely coupled with other Stages, linked together via subscriptions. They have their own lifecycle, testing, analysis, and conditions for deployment. Artifacts, which we call Freight, are promotable units of deployment that become qualified as they move from Stage to Stage. The final result is a deployment pipeline that is both easier to manage and more powerful than a Jenkinsfile or GitHub action could ever hope to be.</p><figure>
    <span>
      <span></span>
  <img alt="Kargo User Interface" title="Kargo User Interface" src="https://akuity.io/static/4b9e0e66766f2e479561f1e67e74198f/f6a84/kargo-screenshot.png" srcset="https://akuity.io/static/4b9e0e66766f2e479561f1e67e74198f/e85cb/kargo-screenshot.png 480w,https://akuity.io/static/4b9e0e66766f2e479561f1e67e74198f/d9199/kargo-screenshot.png 960w,https://akuity.io/static/4b9e0e66766f2e479561f1e67e74198f/f6a84/kargo-screenshot.png 1629w" sizes="(max-width: 1629px) 100vw, 1629px" loading="lazy" decoding="async">
    </span>
    <figcaption>Kargo User Interface</figcaption>
  </figure><p>Drawing from our experience with Argo CD, we recognize the crucial importance of a positive developer experience. As a result, we are dedicating significant focus to enhancing Kargo's user interface. We anticipate end-user developers to visit Kargo’s interface on a daily basis, as they promote, manage, and observe their environments. Within Kargo’s user interface, developers can understand their environments at a glance: what is running, where artifacts have been deployed, and how things are progressing through the pre-defined Stages.</p><p>What we’re announcing today barely scratches the surface of the problems a tool like Kargo can solve. We’re excited about its future and are looking for key design partners as well as fostering a community around the project. <a href="https://us06web.zoom.us/webinar/register/1116947746742/WN_W434Plb8SnCfRt2jmjsGMg">Join us tomorrow for a live webinar</a> where together with Kelsey Hightower we will be demoing Kargo and talking about its core concepts. You can also get involved and help shape our roadmap by <a href="https://github.com/akuity/kargo">joining us on GitHub</a> or our <a href="https://discord.com/invite/dHJBZw6ewT">Discord server</a>.</p><p>Kargo will soon be a part of the many features of the <a href="https://akuity.io/akuity-platform">Akuity Platform</a>, so if you're interested in testing it out in Beta, <a href="https://akuity.io/kargo-beta/">please reach out and let us know</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[38TB of data accidentally exposed by Microsoft AI researchers (639 pts)]]></title>
            <link>https://www.wiz.io/blog/38-terabytes-of-private-data-accidentally-exposed-by-microsoft-ai-researchers</link>
            <guid>37556605</guid>
            <pubDate>Mon, 18 Sep 2023 14:30:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wiz.io/blog/38-terabytes-of-private-data-accidentally-exposed-by-microsoft-ai-researchers">https://www.wiz.io/blog/38-terabytes-of-private-data-accidentally-exposed-by-microsoft-ai-researchers</a>, See on <a href="https://news.ycombinator.com/item?id=37556605">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2><span></span><a id="executive-summary-0"></a><strong>Executive summary</strong>&nbsp;</h2><ul><li><p>Microsoft’s AI research team, while publishing a bucket of open-source training data on GitHub, accidentally exposed 38 terabytes of additional private data — including a disk backup of two employees’ workstations.&nbsp;</p></li><li><p>The backup includes secrets, private keys, passwords, and over 30,000 internal Microsoft Teams messages.&nbsp;</p></li><li><p>The researchers shared their files using an Azure feature called SAS tokens, which allows you to share data from Azure Storage accounts.&nbsp;</p></li><li><p>The access level can be limited to specific files only; however, in this case, the link was configured to share the entire storage account — including another 38TB of private files.&nbsp;</p></li><li><p>This case is an example of the new risks organizations face when starting to leverage the power of AI more broadly, as more of their engineers now work with massive amounts of training data. As data scientists and engineers race to bring new AI solutions to production, the massive amounts of data they handle require additional security checks and safeguards.&nbsp;</p></li></ul><h2><span></span><a id="introduction-and-microsoft-findings-2"></a><strong>Introduction and Microsoft findings</strong>&nbsp;</h2><p>As part of the Wiz Research Team’s <a href="https://www.youtube.com/watch?v=rbHALyrxj0Y">ongoing work</a> on accidental exposure of cloud-hosted data, the team scanned the internet for misconfigured storage containers. In this process, we found a GitHub repository under the Microsoft organization named <code>robust-models-transfer</code>. The repository belongs to Microsoft’s AI research division, and its purpose is to provide open-source code and AI models for image recognition. Readers of the repository were instructed to download the models from an Azure Storage URL:&nbsp;&nbsp;</p><p>The exposed storage URL, taken from Microsoft’s GitHub repository</p><p>However, this URL allowed access to more than just open-source models. It was configured to grant permissions on the entire storage account, exposing additional private data by mistake.&nbsp;</p><p>Our scan shows that this account contained 38TB of additional data — including Microsoft employees’ personal computer backups. The backups contained sensitive personal data, including passwords to Microsoft services, secret keys, and over 30,000 internal Microsoft Teams messages from 359 Microsoft employees.&nbsp;</p><p>Exposed containers under the 'robustnessws4285631339' storage account </p><p>A small sample of sensitive files found on the computer backups</p><p>Redacted Teams conversation between two Microsoft employees</p><p>In addition to the overly permissive access scope, the token was also misconfigured to allow “full control” permissions instead of read-only. Meaning, not only could an attacker view all the files in the storage account, but they could delete and overwrite existing files as well.&nbsp;</p><p>This is particularly interesting considering the repository’s original purpose: providing AI models for use in training code. The repository instructs users to download a model data file from the SAS link and feed it into a script. The file’s format is <code>ckpt</code>, a format produced by the TensorFlow library. It’s formatted using Python’s <code>pickle</code> formatter, which is <a href="https://huggingface.co/docs/hub/security-pickle">prone to arbitrary code execution</a> by design. Meaning, an attacker could have injected malicious code into all the AI models in this storage account, and every user who trusts Microsoft’s GitHub repository would’ve been infected by it.&nbsp;&nbsp;</p><p>However, it’s important to note this storage account wasn’t directly exposed to the public; in fact, it was a private storage account. The Microsoft developers used an Azure mechanism called “SAS tokens”, which allows you to create a shareable link granting access to an Azure Storage account’s data — while upon inspection, the storage account would still seem completely private.&nbsp;</p><h2><span></span><a id="introduction-to-sas-tokens-16"></a><strong>Introduction to SAS tokens</strong>&nbsp;</h2><p>In Azure, a Shared Access Signature (SAS) token is a signed URL that grants access to Azure Storage data. The access level can be customized by the user; the permissions range between read-only and full control, while the scope can be either a single file, a container, or an entire storage account. The expiry time is also completely customizable, allowing the user to create never-expiring access tokens. This granularity provides great agility for users, but it also creates the risk of granting too much access; in the most permissive case (as we’ve seen in Microsoft’s token above), the token can allow full control permissions, on the entire account, forever – essentially providing the same access level as the account key itself.&nbsp;&nbsp;&nbsp;</p><p>There are 3 types of SAS tokens: Account SAS, Service SAS, and User Delegation SAS. In this blog we will focus on the most popular type – Account SAS tokens, which were also used in Microsoft’s repository.&nbsp;</p><p>Generating an Account SAS is a simple process. As can be seen in the screen below, the user configures the token’s scope, permissions, and expiry date, and generates the token. Behind the scenes, the browser downloads the account key from Azure, and signs the generated token with the key. This entire process is done on the client side; it’s not an Azure event, and the resulting token is not an Azure object.&nbsp;</p><p>Creating a high privilege non-expiring SAS token</p><p>Because of this, when a user creates a highly-permissive non-expiring token, there is no way for an administrator to know this token exists and where it circulates. Revoking a token is no easy task either — it requires rotating the account key that signed the token, rendering all other tokens signed by same key ineffective as well. These unique pitfalls make this service an easy target for attackers looking for exposed data.&nbsp;</p><p>Besides the risk of accidental exposure, the service’s pitfalls make it an effective tool for attackers seeking to maintain persistency on compromised storage accounts. A recent <a href="https://www.microsoft.com/en-us/security/blog/2023/09/07/cloud-storage-security-whats-new-in-the-threat-matrix/#:~:text=Create%20SAS%20Token">Microsoft report</a> indicates that attackers are taking advantage of the service’s lack of monitoring capabilities in order to issue privileged SAS tokens as a backdoor. Since the issuance of the token is not documented anywhere, there is no way to know that it was issued and act against it.&nbsp;&nbsp;</p><h2><span></span><a id="sas-security-risks-24"></a><strong>SAS security risks</strong>&nbsp;</h2><p>SAS tokens pose a security risk, as they allow sharing information with external unidentified identities. The risk can be examined from several angles: permissions, hygiene, management and monitoring.&nbsp;</p><h3><span></span><a id="permissions-27"></a><strong>Permissions</strong>&nbsp;</h3><p>A SAS token can grant a very high access level to a storage account, whether through excessive permissions (like read, list, write or delete), or through wide access scopes that allow users to access adjacent storage containers.&nbsp;&nbsp;</p><h3><span></span><a id="hygiene-29"></a><strong>Hygiene</strong>&nbsp;</h3><p>SAS tokens have an expiry problem — our scans and monitoring show organizations often use tokens with a very long (sometimes infinite) lifetime, as there is no upper limit on a token's expiry. This was the case with Microsoft’s token, which was valid until 2051.&nbsp;</p><h3><span></span><a id="management-and-monitoring-31"></a><strong>Management and monitoring</strong>&nbsp;</h3><p>Account SAS tokens are extremely hard to manage and revoke. There isn't any official way to keep track of these tokens within Azure, nor to monitor their issuance, which makes it difficult to know how many tokens have been issued and are in active use. The reason even issuance cannot be tracked is that SAS tokens are created on the client side, therefore it is not an an Azure tracked activity, and the generated token is not an Azure object. Because of this, even what appears to be a private storage account may potentially be widely exposed.&nbsp;</p><p>As for revocation, there isn't a way to revoke a singular Account SAS; the only solution is revoking the entire account key, which invalidates all the other tokens issued with the same key as well.&nbsp;</p><p>Monitoring the usage of SAS tokens is another challenge, as it requires enabling logging on each storage account separately. It can also be costly, as the pricing depends on the request volume of each storage account. &nbsp;</p><h2><span></span><a id="sas-security-recommendations-35"></a><strong>SAS security recommendations</strong>&nbsp;</h2><p>SAS security can be&nbsp;significantly improved with the following recommendations.</p><h3><span></span><a id="management-37"></a><strong>Management</strong>&nbsp;</h3><p>Due to the lack of security and governance over Account SAS tokens, they should be considered as sensitive as the account key itself. Therefore, it is highly recommended to avoid using Account SAS for external sharing. Token creation mistakes can easily go unnoticed and expose sensitive data.&nbsp;&nbsp;</p><p>For external sharing, consider using a Service SAS with a <a href="https://learn.microsoft.com/en-us/rest/api/storageservices/define-stored-access-policy">Stored Access Policy</a>. This feature connects the SAS token to a server-side policy, providing the ability to manage policies and revoke them in a centralized manner.&nbsp;</p><p>If you need to share content in a time-limited manner, consider using a <a href="https://learn.microsoft.com/en-us/rest/api/storageservices/create-user-delegation-sas">User Delegation SAS</a>, since their expiry time is capped at 7 days. This feature connects the SAS token to Azure Active Directory’s identity management, providing control and visibility over the identity of the token’s creator and its users.&nbsp;</p><p>Additionally, we recommend creating dedicated storage accounts for external sharing, to ensure that the potential impact of an over-privileged token is limited to external data only.&nbsp;</p><p>To avoid SAS tokens completely, organizations will have to <a href="https://learn.microsoft.com/en-us/azure/storage/common/shared-key-authorization-prevent">disable SAS access</a> for each of their storage accounts separately. We recommend using a CSPM to track and enforce this as a policy.&nbsp;</p><p>Another solution to disable SAS token creation is by blocking access to the “<a href="https://learn.microsoft.com/en-us/rest/api/storagerp/storage-accounts/list-keys">list storage account keys</a>” operation in Azure (since new SAS tokens cannot be created without the key), then rotating the current account keys, to invalidate pre-existing SAS tokens. This approach would still allow creation of User Delegation SAS, since it relies on the user’s key instead of the account key.&nbsp;</p><h3><span></span><a id="monitorin-g-44"></a><strong>Monitorin</strong><strong><u>g</u></strong>&nbsp;</h3><p>To track active SAS token usage, you need to <a href="https://learn.microsoft.com/en-us/azure/storage/common/manage-storage-analytics-logs">enable Storage Analytics logs</a> for each of your storage accounts. The resulting logs will contain details of SAS token access, including the signing key and the permissions assigned. However, it should be noted that only actively used tokens will appear in the logs, and that enabling logging comes with extra charges — which might be costly for accounts with extensive activity.&nbsp;</p><p><a href="https://learn.microsoft.com/en-us/azure/storage/blobs/blob-storage-monitoring-scenarios">Azure Metrics</a> can be used to monitor SAS tokens usage in storage accounts. By default, Azure records and aggregates storage account events up to <a href="https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/data-platform-metrics#retention-of-metrics">93 days</a>. Utilizing Azure Metrics, users can look up SAS-authenticated requests, highlighting storage accounts with SAS tokens usage.&nbsp;</p><h3><span></span><a id="secret-scanning-47"></a><strong>Secret scanning</strong>&nbsp;</h3><p>In addition, we recommend using secret scanning tools to detect leaked or over-privileged SAS tokens in artifacts and publicly exposed assets, such as mobile apps, websites, and GitHub repositories — as can be seen in the Microsoft case.&nbsp;&nbsp;</p><p>For more information on cloud secret scanning, please check out our recent talk from the fwd:cloudsec 2023 conference, <a href="https://youtu.be/rbHALyrxj0Y">"Scanning the internet for external cloud exposures"</a>.&nbsp;</p><h3><span></span><a id="for-wiz-customers-50"></a><strong>For Wiz customers</strong>&nbsp;</h3><p>Wiz customers can leverage the Wiz secret scanning capabilities to identify SAS tokens in internal and external assets and explore their permissions. In addition, customers can use the Wiz CSPM to track storage accounts with SAS support.&nbsp;</p><ul><li><p><strong>Detect SAS tokens:</strong> use this <a href="https://app.wiz.io/graph#~(query~(type~(~'SECRET_DATA)~select~true~where~(presignedURL_type~(EQUALS~(~'PresignedURLTypeAzureSASToken)))~relationships~(~(type~(~(type~'PERMITS))~optional~true~with~(type~(~'STORAGE_ACCOUNT)~select~true))~(type~(~(type~'INSTANCE_OF~reverse~true))~with~(type~(~'SECRET_INSTANCE)~select~true~relationships~(~(type~(~(type~'CONTAINS~reverse~true))~with~(type~(~'CLOUD_RESOURCE)~select~true)))))))~view~'table~columns~(~(~'0~49)~(~'1~17)~(~'2~17)~(~'3~17)))">query</a> to surface all SAS tokens in all your monitored cloud environments.&nbsp;</p></li><li><p><strong>Detect high-privilege SAS tokens:</strong> use the following <a href="https://app.wiz.io/graph#~(control~'wc-id-927~view~'table)">control</a> to detect highly-privileged SAS tokens located on publicly exposed workloads.&nbsp;</p></li><li><p><strong>CSPM rule for blocking SAS tokens:</strong> use the following <a href="https://app.wiz.io/graph#~(query~(type~(~'STORAGE_ACCOUNT)~select~true~relationships~(~(type~(~(type~'ALERTED_ON~reverse~true))~with~(type~(~'CONFIGURATION_FINDING)~select~true~where~(configurationRuleShortName~(EQUALS~(~'StorageAccount-026)))))~(type~(~(type~'CONTAINS~reverse~true))~optional~true~with~(type~(~'SUBSCRIPTION)~select~true)))))">Cloud Configuration Rule</a> to track storage accounts allowing SAS token usage.&nbsp;</p></li></ul><h2><span></span><a id="security-risks-in-the-ai-pipeline-53"></a><strong>Security risks in the AI pipeline</strong></h2><p>As companies embrace AI more widely, it is important for security teams to understand the inherent security risks at each stage of the AI development process.&nbsp;</p><p>The incident detailed in this blog is an example of two of these risks.&nbsp;</p><p>The first is <strong>oversharing of data</strong>.<strong> </strong>Researchers collect and share massive amounts of external and internal data to construct the required training information for their AI models. This poses inherent security risks tied to high-scale data sharing. It is crucial for security teams to define clear guidelines for external sharing of AI datasets. As we’ve seen in this case, separating the public AI data set to a dedicated storage account could’ve limited the exposure.&nbsp;</p><p>The second is the risk of <strong>supply chain attacks</strong>. Due to improper permissions, the public token granted write access to the storage account containing the AI models. As noted above, injecting malicious code into the model files could’ve led to a supply chain attack on other researchers who use the repository’s models. Security teams should review and sanitize AI models from external sources, since they can be used as a remote code execution vector.&nbsp;&nbsp;</p><h2><span></span><a id="takeaways-59"></a><strong>Takeaways&nbsp;</strong>&nbsp;</h2><p>The simple step of sharing an AI dataset led to a major data leak, containing over 38TB of private data. The root cause was the usage of Account SAS tokens as the sharing mechanism. Due to a lack of monitoring and governance, SAS tokens pose a security risk, and their usage should be as limited as possible. These tokens are very hard to track, as Microsoft does not provide a centralized way to manage them within the Azure portal. In addition, these tokens can be configured to last effectively forever, with no upper limit on their expiry time. Therefore, using Account SAS tokens for external sharing is unsafe and should be avoided.&nbsp;</p><p>In the wider scope, similar incidents can be prevented by granting security teams more visibility into the processes of AI research and development teams. As we see wider adoption of AI models within companies, it’s important to raise awareness of relevant security risks at every step of the AI development process, and make sure the security team works closely with the data science and research teams to ensure proper guardrails are defined.&nbsp;&nbsp;&nbsp;</p><p>Microsoft's account of this issue is available on the <a href="https://msrc.microsoft.com/blog/2023/09/microsoft-mitigated-exposure-of-internal-information-in-a-storage-account-due-to-overly-permissive-sas-token/">Microsoft Security Response Center blog</a>.</p><h2><span></span><a id="timeline-63"></a><strong>Timeline</strong>&nbsp;</h2><ul><li><p><strong>Jul. 20, 2020</strong> – SAS token first <a href="https://github.com/microsoft/robust-models-transfer/blob/e61568d613c025adfd07f61f2639f3ae78852143/README.md?plain=1#L36">committed</a> to GitHub; expiry set to Oct. 5, 2021&nbsp;</p></li><li><p><strong>Oct. 6, 2021</strong> – SAS token expiry <a href="https://github.com/microsoft/robust-models-transfer/commit/a9e0e80bcd49bd8651c0b3198c7dc89179b2c0ac">updated</a> to Oct. 6, 2051&nbsp;</p></li><li><p><strong>Jun. 22, 2023</strong> – Wiz Research finds and reports issue to MSRC&nbsp;</p></li><li><p><strong>Jun. 24, 2023</strong> – SAS token invalidated by Microsoft&nbsp;</p></li><li><p><strong>Jul. 7, 2023</strong> – SAS token <a href="https://github.com/microsoft/robust-models-transfer/commit/c26ebfff3d01bd2a52a6c14febb8b7ea0234431d">replaced</a> on GitHub&nbsp;</p></li><li><p><strong>Aug. 16, 2023</strong> – Microsoft completes internal investigation of potential impact&nbsp;</p></li><li><p><strong>Sep. 18, 2023</strong> – Public disclosure&nbsp;</p></li></ul><h2><span></span><a id="stay-in-touch-65"></a><strong>Stay in touch!</strong>&nbsp;</h2><p>Hi there! We are Hillai Ben-Sasson (<a href="https://twitter.com/hillai">@hillai</a>), Shir Tamari (<a href="https://twitter.com/shirtamari">@shirtamari</a>), Nir Ohfeld (<a href="https://twitter.com/nirohfeld">@nirohfeld</a>), Sagi Tzadik (<a href="https://twitter.com/sagitz_">@sagitz_</a>) and Ronen Shustin (<a href="https://twitter.com/ronenshh">@ronenshh</a>) from the Wiz Research Team. We are a group of veteran white-hat hackers with a single goal: to make the cloud a safer place for everyone. We primarily focus on finding new attack vectors in the cloud and uncovering isolation issues in cloud vendors.</p><p>We would love to hear from you! Feel free to contact us on Twitter or via email:&nbsp;<a href="mailto:research@wiz.io">research@wiz.io</a>.&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Replanting Logged Forests with Diverse Seedlings Accelerates Restoration (280 pts)]]></title>
            <link>https://www.technologynetworks.com/applied-sciences/news/replanting-logged-forests-with-diverse-mixtures-of-seedlings-accelerates-restoration-378916</link>
            <guid>37556025</guid>
            <pubDate>Mon, 18 Sep 2023 13:44:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.technologynetworks.com/applied-sciences/news/replanting-logged-forests-with-diverse-mixtures-of-seedlings-accelerates-restoration-378916">https://www.technologynetworks.com/applied-sciences/news/replanting-logged-forests-with-diverse-mixtures-of-seedlings-accelerates-restoration-378916</a>, See on <a href="https://news.ycombinator.com/item?id=37556025">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

        

            <div>
                <p><span>Register</span> for free to listen to this article
                </p>             
            </div>
            <p>Thank you. Listen to this article using the player above. <span onclick="closeAudioPlayerThankYouMessage()">✖</span></p>
    </div><div>
        <br>
        
<div>
        <p>
            Want to listen to this article for FREE?
        </p>
</div>

        <p>
            Complete the form below to unlock access to ALL audio articles.
        </p>
    </div><div id="inner_content_body_text" data-content-id="378916">
    <p>Satellite observations of one of the world’s biggest ecological experiments on the island of Borneo have revealed that replanting logged forests with diverse mixtures of seedlings can significantly accelerate their recovery. The results have been published today in the journal&nbsp;<em>Science Advances</em>.</p><p>The experiment was set up by the University of Oxford’s&nbsp;<a href="https://www.biology.ox.ac.uk/people/professor-andrew-hector" target="_blank">Professor Andy Hector</a>&nbsp;and colleagues over twenty years ago as part of the&nbsp;<a href="https://www.searrp.org/" target="_blank">SE Asia Rainforest Research Partnership (SEARRP)</a>. This assessed the recovery of 125 different plots in an area of logged tropical forest that were sown with different combinations of tree species. The results revealed that plots replanted with a mixture of 16 native tree species showed faster recovery of canopy area and total tree biomass, compared to plots replanted with 4 or just 1 species. However, even plots that had been replanted with 1 tree species were recovering more quickly than those left to restore naturally.</p><p>Lead Scientist of the study,&nbsp;<a href="https://www.biology.ox.ac.uk/people/professor-andrew-hector" target="_blank">Professor Andy Hector</a>&nbsp;(Department of Biology, University of Oxford) said: ‘Our new study demonstrates that replanting logged tropical forests with diverse mixtures of native tree species achieves multiple wins, accelerating the restoration of tree cover, biodiversity, and important ecosystem services such as carbon sequestration.’</p><h2>Greater diversity gives greater resilience</h2><p>According to the researchers, a likely reason behind the result is that different tree species occupy different positions, or ‘niches’, within an ecosystem. This includes both the physical and environmental conditions that the species is adapted to, and how it interacts with other organisms. As a result, diverse mixtures complement each other to increase overall functioning and stability of the ecosystem. For instance, some tropical tree species are more tolerant of drought because they produce a greater amount of protective chemicals, giving the forest resilience to periodic times of low rainfall.</p><p>Professor Hector added: ‘Having diversity in a tropical forest can be likened to an insurance effect, similar to having a financial strategy of diverse investment portfolios.’</p><p>In turn, a diverse mix of trees can support a much wider range of animal life. For instance, hornbills specifically require large mature trees with holes where the females can nest.</p><h2>One of the world’s biggest ecological experiments</h2><p>Tropical forests cover just 6% of the planet’s land surface&nbsp;<a href="https://wwf.panda.org/discover/our_focus/forests_practice/importance_forests/tropical_rainforest/" target="_blank">but are home to around 80% of the world’s documented species (WWF),&nbsp;</a>and act as major carbon sinks. However, these critical habitats are disappearing at an alarming rate, chiefly due to logging for timber and conversion to palm oil plantations. Between 2004 and 2017,&nbsp;<a href="https://wwf.panda.org/discover/our_focus/forests_practice/importance_forests/tropical_rainforest/" target="_blank">43 million hectares of tropical forest were lost</a>&nbsp;- an area roughly the size of Morocco (WWF).</p><p>Restoring logged tropical forests is a crucial component of efforts to tackle both the nature and climate crises. Up to now, however, it has been unclear whether this is best achieved through allowing forests to restore themselves naturally (using dormant seeds in the soil) or through active replanting.</p><div><h2>Want more breaking news?</h2><div><p>Subscribe to <i>Technology Networks</i>’ daily newsletter, delivering breaking science news straight to your inbox every day.</p><p><a href="https://go.technologynetworks.com/subscribe-to-breaking-science-news" target="_blank"><span>Subscribe for FREE</span></a></p></div></div><p>To investigate this, the researchers collaborated with local partners to set up the Sabah Biodiversity Experiment on 500 hectares of logged forest in the Malaysian state of Sabah on the island of Borneo. This was divided into 125 experimental plots that were either left to recover naturally or planted with mixtures of either 1, 4, or 16 tree species that are frequently targeted for logging. The 16 species included several endangered species and the worlds’ tallest species of tropical tree (<em>Shorea faguetiana</em>) which can reach over 100 m in height. The first trees were planted in 2002, with nearly 100,000 planted in total over the following years.</p><p>The recovery of the plots was assessed by applying statistical models to aerial images captured by satellites. Within a few years, it became apparent that those with 1 species did worse than those planted with a mixture of 4 species, and those enriched with 16 species did best of all.</p><p>Lead author&nbsp;<a href="https://www.biology.ox.ac.uk/people/ryan-veryard" target="_blank">Ryan Veryard</a>&nbsp;(who analysed the data as part of his PhD at the University of Oxford), said: ‘Importantly, our results show that logged forest can recover so long as it is not converted to agricultural uses like oil palm plantation. They also emphasise the need to conserve biodiversity within undisturbed forests, so that we can restore it in areas that have already been logged.’</p><p>The Sabah Biodiversity Experiment team are now starting a new three-year project funded by the UK Natural Environmental Research Council to take a census of all the surviving trees in the experiment. This will be combined with a wider range of remote sensing methods (including lidar sensors carried by a helicopter and smaller sensors carried by drones) to give a more comprehensive analysis of forest health.</p><p><b>Reference:&nbsp;</b><span>Veryard R, Wu J, O’Brien MJ, et al. Positive effects of tree diversity on tropical forest restoration in a field-scale experiment. </span><i>Sci Adv</i><span>. 2023;9(37):eadf0938. doi: </span><a href="https://doi.org/10.1126/sciadv.adf0938" target="_blank"><span>10.1126/sciadv.adf0938</span></a></p><p><i><span>This article has been republished from the following <a href="https://www.biology.ox.ac.uk/article/replanting-logged-forests-with-diverse-mixtures-of-seedlings-accelerates-restoration" target="_blank">materials.</a> Note: material may have been edited for length and content. For further information, please contact the cited source.</span></i></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Military Asks for Help Locating Missing F-35 Fighter Jet (244 pts)]]></title>
            <link>https://time.com/6315261/missing-military-f35-jet/</link>
            <guid>37555172</guid>
            <pubDate>Mon, 18 Sep 2023 12:20:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://time.com/6315261/missing-military-f35-jet/">https://time.com/6315261/missing-military-f35-jet/</a>, See on <a href="https://news.ycombinator.com/item?id=37555172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body" data-tracking-zone="body">
                  <p><span role="presentation">T</span>he United States’ military is on the hunt for an F-35 fighter jet that has gone missing following an incident that forced the pilot to eject from the advanced stealth aircraft over South Carolina.</p>                  <p>Emergency response teams are trying to find what’s left of the F-35B Lightning II jet, which suffered what the military called a “mishap” on Sunday afternoon, according to social media&nbsp;<a href="https://www.facebook.com/TeamCharleston/?locale=en_GB" target="_blank" rel="noreferrer noopener">posts</a>&nbsp;by Joint Base Charleston, an air base in South Carolina. The unidentified pilot ejected safely and was taken to a local hospital in a stable condition.</p>                                    <p>Joint Base Charleston called on the public to cooperate with military and civilian authorities as the search for the F-35 jet continues. The air base said it was working with Marine Corps Air Station Beaufort to search for the plane north of North Charleston around Lake Moultrie and Lake Marion, based on its last-known location.</p>                  <h3>More From TIME</h3>                    
                  <p>Lockheed Martin Corp is the manufacturer behind the F-35, a single-seat fighter craft used by militaries around the world. The aircraft was a vertical take-off version used by in the US Marine Corps, and the jet is popular for its stealth qualities that make it difficult to detect by radar.&nbsp;</p>                                    <p>The F-35 program, the most expensive US weapons program ever, is projected to cost $400 billion in development and acquisition, plus an additional $1.2 trillion to operate and maintain the fleet over more than 60 years. Each jet can cost more than $160 million, depending on the variant.</p>                  <p>It’s not the first time an F-35 has been in trouble. An F-35B version crashed in 2018 in Beaufort County, South Carolina, because of a manufacturing defect in a fuel tube, according to the Government Accounting Office’s&nbsp;<a href="https://www.gao.gov/assets/gao-19-336sp.pdf" target="_blank" rel="noreferrer noopener">report</a>. The following year, a Japanese F-35A stealth fighter plunged into the ocean during an exercise over the Pacific Ocean, which Japan blamed on pilot disorientation, rather than technical issues.</p>                                    <p>The missing aircraft in the US swiftly drew online mockery, from postings with&nbsp;<em>Missing-Jet</em>&nbsp;fliers on lamp posts and notices on milk cartons, to mashed up&nbsp;<em>Dude, Where’s My F-35</em>&nbsp;movie posters.</p>                  <p>“Now that I got that out of the way. How in the hell do you lose an F-35?” South Carolina Republican Representative Nancy Mace said on social media. “How is there not a tracking device and we’re asking the public to what, find a jet and turn it in?”</p>                  <div>
                        <div>
                          <p>More Must-Reads From TIME</p>
                          <hr>
                        </div>
                        <ul>   	<li>Meet the <a href="https://time.com/collection/time100-next-2023/?utm_source=roundup&amp;utm_campaign=20230202"><strong>2023 TIME100 Next</strong></a>: the Emerging Leaders Shaping the World</li>   	<li><a href="https://time.com/6312770/jalen-hurts-interview-time-100-next/?utm_source=roundup&amp;utm_campaign=20230202"><strong>Jalen Hurts</strong></a> Is Fueled by the Doubters</li>   	<li>Impeachment Experts Say <a href="https://time.com/6313452/impeachment-experts-biden-inquiry-weakest-us-history/?utm_source=roundup&amp;utm_campaign=20230202"><strong>Biden Inquiry May Be Weakest in US History</strong></a></li>   	<li><a href="https://time.com/6311403/martin-scorsese-killers-of-the-flower-moon-interview/?utm_source=roundup&amp;utm_campaign=20230202"><strong>Martin Scorsese Still Has Stories to Tell</strong></a></li>   	<li><a href="https://time.com/6311034/50-50-job-sharing-burn-out/?utm_source=roundup&amp;utm_campaign=20230202"><strong>Burned Out at Work?</strong></a> Find Someone to Split Your Job 50-50 With You</li>   	<li><a href="https://time.com/6313645/jessica-knoll-bright-young-women-ted-bundy/?utm_source=roundup&amp;utm_campaign=20230202"><strong>Jessica Knoll Wants to Correct the Record</strong></a> on Ted Bundy</li>   	<li>The Most Anticipated <strong><a href="https://time.com/6306503/best-books-fall-2023/?utm_source=roundup&amp;utm_campaign=20230202">Books</a>, <a href="https://time.com/6308618/best-movies-fall-2023/?utm_source=roundup&amp;utm_campaign=20230202">Movies</a>, <a href="https://time.com/6309384/best-tv-shows-fall-2023/?utm_source=roundup&amp;utm_campaign=20230202">TV</a>, </strong>and <strong><a href="https://time.com/6309530/best-music-fall-2023/?utm_source=roundup&amp;utm_campaign=20230202">Music</a></strong> of Fall 2023</li>   	<li>Why It Takes <a href="https://time.com/6313270/6313270/?utm_source=roundup&amp;utm_campaign=20230202"><strong>Forever to Get a Doctor's Appointment</strong></a></li>   	<li>Want Weekly Recs on What to Watch, Read, and More? Sign Up for <strong><a href="http://time.com/worth-your-time?utm_source=roundup&amp;utm_campaign=20230202">Worth Your Time</a></strong></li>  </ul>
                  </div>                  <p><strong>Contact us</strong> at <a href="mailto:letters@time.com?subject=(READER%20FEEDBACK)%20U.S.%20Military%20Asks%20for%20Help%20Finding%20Missing%20F-35%20Fighter%20Jet&amp;body=https%3A%2F%2Ftime.com%2F6315261%2Fmissing-military-f35-jet%2F" target="_self" rel="noopener noreferrer">letters@time.com</a>.</p>              
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Problems with homemade billing systems (167 pts)]]></title>
            <link>https://www.getlago.com/blog/the-4-biggest-problems-with-homemade-billing-systems</link>
            <guid>37555139</guid>
            <pubDate>Mon, 18 Sep 2023 12:15:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.getlago.com/blog/the-4-biggest-problems-with-homemade-billing-systems">https://www.getlago.com/blog/the-4-biggest-problems-with-homemade-billing-systems</a>, See on <a href="https://news.ycombinator.com/item?id=37555139">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><a href="https://www.getlago.com/blog"><p>Blog</p></a><p>The 4 biggest problems with homemade billing systems</p></div><div><p>This article was written by <a href="https://www.linkedin.com/in/vincentpochet/" target="_blank">Vincent Pochet</a>, Senior Backend Developer at Lago.</p><p>——</p><p>In 2017, I joined Qonto, a B2B neobank in the making (<a href="https://techcrunch.com/2022/01/10/business-banking-startup-qonto-raises-552-million-at-5-billion-valuation/" target="_blank">now worth more than $5B</a>), and got involved in the design of the billing system.</p><p>As the company was scaling from cradle to unicorn, we faced many billing challenges: monthly and yearly plans, “pay-as-you-go” components (percentage on FX transfers, fixed fee on ATM withdrawals, user seats, printed cards, etc.), creation of invoices at scale, taxes, accounting… It was a never-ending nightmare.</p><p>Now, when someone asks for advice about their billing system, my answer is clear: <strong>DO NOT build it yourself</strong>.</p><p>It may look like an interesting project at the beginning, but you’ll soon regret it. And believe me, it only gets worse. As an engineer, if you’re told it’s a “two-month project”, run as far as you can before it’s too late and you become the “billing guy”!</p><p>‍</p><h3>#1 Pricing changes all the time and billing needs to follow</h3><p>After creating the first billing engine of Qonto, we were pretty happy with what we had built. We could ingest <strong>millions of events every week</strong>, our calculations were correct, we could generate sequential invoices, we were able to apply marketing coupons and we managed to debit our internal ledger. Everything was going well!</p><p>But then…</p><p>‍</p><h4>The marketing team came up with a new yearly plan</h4><p>Billing was performed on a monthly basis. At the end of each month, we had to query the database to compute fees and display them as billing items on the invoices. Customers were able to switch from a “monthly_solo” plan to a “monthly_premium” plan (and vice versa), and the upgrade/downgrade logic was working well (although it had been difficult to implement).</p><p>When the marketing team devised a new yearly plan to secure revenue for the next 12 months, I started to freak out: <strong>the entire billing logic was based on monthly boundaries</strong>. It took us two months to modify this logic. The hardest part was that the subscription fee had to be billed in advance, at the beginning of the annual period, but the overage related to usage-based features still had to be billed on a monthly basis.</p><p>‍</p><h4>The finance team asked to switch from anniversary to calendar dates</h4><p>At the time, customers were billed based on the anniversary date of their contract. For instance, if a customer had signed up for a monthly plan on March 16th, they were billed on the 16th of each month.</p><p>One day, the finance team requested a meeting with the engineering team and the CFO said:</p><p>‍</p><blockquote><em>“It would be easier for everyone if we switched from anniversary dates to calendar dates for subscriptions. Right now it’s a mess from an accounting perspective, so let’s bill everyone at the beginning of the calendar period.”</em></blockquote><p>‍</p><p>In other words, they wanted to switch from this…</p><figure><p><img src="https://uploads-ssl.webflow.com/63569f390f3a7ad4c76d2bd6/6357e464c526ce28312ccbe4_632c2492bb03b806277bb038_anniversary-billing.png" alt="Anniversary billing periods"></p></figure><p>‍</p><p>To this...</p><figure><p><img src="https://uploads-ssl.webflow.com/63569f390f3a7ad4c76d2bd6/6357e464c526ce59c22ccbe5_632c24bc129a6f15e47f56a7_calendar-billing.png" alt="Calendar billing periods"></p></figure><p>‍</p><p><strong>100,000 customers were affected</strong> by this migration. The accounting mess turned into an engineering nightmare, resulting in a three-month project.</p><p>Implementing billing logics takes time and things get incredibly complicated over time. Other typical organic changes include (but are not limited to):</p><p>• New features to be added to the pricing;<br>• New country launch, with a different pricing;<br>• New business line, when you start selling a white-label version of your product for instance;<br>• Changes in tax legislation; and<br>• Custom plans for “Enterprise” customers.</p><p>‍</p><h3>#2 Your billing system needs to scale with your user base</h3><p>A home-made billing system is not scalable if you don’t maintain it. At Qonto, our billing database contained millions of rows. This database was linked to our internal ledger, which was used to deduct fees from our customers’ accounts. Processing high volumes of events is hard, and when something breaks, you need to check whether past events need to be computed again (but you can’t afford to ingest the same event twice).</p><p>Generating invoices is also an important task. It’s not hard to create PDF files with line items on it. We used a library called <a href="https://gotenberg.dev/" target="_blank">Gotenberg</a> to display backend aggregations on a beautiful HTML template. However, it’s hard to process fees and generate millions of invoices at the same time. Your backend queries and aggregates millions of rows asynchronously, and the calculation must be correct for each customer.</p><p>The more complex the pricing, the more complicated the calculations. And the more customers you have, the more IT resources you need.</p><p>‍</p><h3>#3 Grandfathering causes headaches</h3><p>A few weeks ago, <a href="https://www.getlago.com/blog/pricing-billing-open-source-interview-with-algolias-co-founder" target="_blank">we interviewed Nicolas Dessaigne</a>, founder of Algolia and Group Partner at YC. He told us about their terrible experience with pricing and billing. After a series of “minor” changes to their price plans, they decided a few months ago to adopt a full usage-based pricing model and had to rebuild the entire billing logic.</p><p>As most customers had signed long-term contracts based on existing plans, they had to exclude them from the migration. This is called “<strong>grandfathering</strong>”. In this situation, engineers must not only implement new billing rules, but they also need to maintain the old logic for grandfathered plans.</p><p>In 2013, the pricing of Algolia was like this...</p><figure><p><img src="https://uploads-ssl.webflow.com/63569f390f3a7ad4c76d2bd6/6357e464c526ce1d8e2ccbe8_632c24e7491cef6fe07397bc_algolia-2013.png" alt="The pricing of Algolia in 2013"></p></figure><p>‍</p><p>Then in 2015 it was like this...</p><figure><p><img src="https://uploads-ssl.webflow.com/63569f390f3a7ad4c76d2bd6/6357e464c526ce34662ccbe6_632c2505090891f38d817c95_algolia-2015.png" alt="The pricing of Algolia in 2015"></p></figure><p>‍</p><p>And here is the 2022&nbsp;update!</p><figure><p><img src="https://uploads-ssl.webflow.com/63569f390f3a7ad4c76d2bd6/6357e464c526ce64ad2ccbe7_632c25151344fba10f40b3eb_algolia-2022.png" alt="The pricing of Algolia in 2022"></p></figure><p>‍</p><h3>#4 You must be prepared to staff an entire team</h3><p>Billing is never considered an expertise. It’s perceived as a background task and not as a prestigious project that engineers will fight for (they may even leave if they’re forced to work on it for a long time).</p><p>However, sooner or later, billing will be a full-time job for at least one engineer. Because of pricing changes, scalability challenges and grandfathered plans, complexity increases and so does the workload. Billing is built around your company’s product, it’s a living organism, not a feature.</p><p>At Qonto, the billing project was supposed to be completed by a single backend engineer in only two months. One year later, two backend engineers were still working on it full time.</p><p>Then <strong>the team of two backend engineers grew into a team of 20 people</strong>, including product managers, backend engineers and frontend engineers as well. Hiring, onboarding and retaining people to take care of our billing system was a constant challenge. They would have preferred to work on our core product, and our management team also wanted to downsize the team.</p><p>We considered implementing an off-the-shelf billing solution but there was nothing flexible enough and <strong>the switching costs were too high</strong>. Algolia also tried to migrate to Zuora before backing out and rebuilding their billing system for the fourth time. This is a decision you have to make in the early days, otherwise, at some point, your billing will be too complex and won’t fit any software product.</p><p>I left Qonto in March 2021 and I still get phone calls from engineers currently working there and struggling with billing. They learned these four lessons the hard way. They now know what it costs to build a homemade billing system.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The brain is not an onion with a tiny reptile inside (2020) (241 pts)]]></title>
            <link>https://journals.sagepub.com/doi/10.1177/0963721420917687</link>
            <guid>37555118</guid>
            <pubDate>Mon, 18 Sep 2023 12:12:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://journals.sagepub.com/doi/10.1177/0963721420917687">https://journals.sagepub.com/doi/10.1177/0963721420917687</a>, See on <a href="https://news.ycombinator.com/item?id=37555118">Hacker News</a></p>
Couldn't get https://journals.sagepub.com/doi/10.1177/0963721420917687: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[When Zig Outshines Rust – Memory Efficient Enum Arrays (301 pts)]]></title>
            <link>https://alic.dev/blog/dense-enums</link>
            <guid>37555028</guid>
            <pubDate>Mon, 18 Sep 2023 12:00:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alic.dev/blog/dense-enums">https://alic.dev/blog/dense-enums</a>, See on <a href="https://news.ycombinator.com/item?id=37555028">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="blog-content"><p>
					
					Enums (or tagged unions) whose variants vary in size are prone to significant 
					memory fragmentation in Rust. That's because we need to allocate enough 
					data to accommodate the largest variant.

					</p><figure>
						<div>
						<p><b>Figure 1:</b> Consider the following enum:
</p><div>
<pre>pub enum Foo { 
  A(u8), 
  B(u16),
  C(u32),
  D(u64),
}

</pre></div><p>
						Because of the space needed for tagging and alignment, this type is 16 bytes long.
						</p></div>
						<p><img alt="A visualization of an enum with variants of different sizes, and their respective memory fragmentation." src="https://alic.dev/static/dense-enums/img1.webp">
						</p>
					</figure>

					<p>
					This presents real pain when collecting a large number of them into a 
					<span>Vec</span> or <span>HashMap</span>. 
					The padding can be dealt with using some form of 
					<i>struct of arrays</i> (SoA)
					transformation that stores the tag in a separate allocation. 
					However, reducing the variant fragmentation is not so trivial. 
					</p>

					<p>
					You could hand-roll specialized data structures for <i>a particular enum</i>
					that reduce fragmentation to a minimum; but doing this generically 
					for an arbitrary enum with maximum memory efficiency is close to 
					impossible in Rust. The only options we have are proc-macros, which 
					compose poorly (no <span>#[derive]</span> on third-party code or type aliases) 
					and are not type aware (unless using workarounds based on 
					<span>generic_const_expr</span>, which infect the call graph with verbose trait 
					bounds and don't work with generic type parameters). Zig on the 
					other hand let's us perform the wildest data structure transformations 
					in a generic and concise way. 
					</p>

					<p>
					Before I go into the implementation details, I'd like to explain why 
					reducing the aforementioned memory fragmentation is useful in practice. 
					</p>

					<p>Background</p>
					<p>
					To me, one of the biggest motivators for efficient enum arrays 
					has been compilers. One problem that keeps coming up when designing 
					an AST is figuring out how to reduce its memory footprint. Big ASTs 
					can incur a hefty performance penalty during compilation, because 
					memory bandwidth and latency are a frequent bottleneck in compiler 
					frontends. Chandler Carruth's 
					<a href="https://www.youtube.com/watch?v=ZI198eFghJk">video on the Carbon compiler</a> has been 
					making the rounds on language forums. In it he describes how a 
					parsed clang AST regularly consumes 50x more memory than the 
					original source code!
					</p>

					<p>
					Alright, so what does this have to do with enums? Well, the 
					most common way of representing syntax tree nodes is via some kind 
					of recursive (or recursive-like) data structure. Let's define a node 
					for expressions in Rust, using newtype indices for indirection:
					</p>
					<div>
<pre>enum Expr {
    Unit,
    Number,
    Binary(Operation, ExprId, ExprId),
    Ident(Symbol),
    Eval(ExprId, ExprSlice),
    BlockExpression(ExprId, StatementSlice)
}
</pre></div>

					<div>
					<p><b>Note:</b> We can write an AST node in OCaml for comparison:
</p><div>
<pre>type expr = 
  | Unit
  | Number
  | Binary of op * expr * expr
  | Ident of symbol
  | Eval of expr * stmt list

</pre></div><p>
					A big difference compared to Rust is that we can express truly 
					recursive data types without any form of explicit indirection. 
					That's because the runtime system and garbage collector take care 
					of the memory bookkeeping for us.
					</p></div><p>

					The problem we have now is that we want to improve the <i>packing efficiency</i>
					of those enums. A simple <span>Vec(Expr)</span> will consume <span>sizeof(Enum)</span> 
					amount of memory for every element, which corresponds to the size of 
					the largest variant + tag + padding. Luckily, there are some ways 
					of dealing with this.

					</p><p>Reducing Fragmentation</p><p>
					Let's take a simple example of a 3-variant enum with member sizes 
					8, 16 and 32 bits. Storing those in a regular <span>Vec</span> will look like this:

					</p><figure>
						<p><b>Figure 2:</b> 
						Here every element reserves a large amount of space to accommodate the 32-bit variant and to satisfy its alignment. 
						</p>
						<p><img alt="A visualization of an array of enum values, with varying fragmentation levels per element" src="https://alic.dev/static/dense-enums/layout1.webp">
						</p>
					</figure>

					<p>
					The most common way to improve packing efficiency is by just keeping the enum variants as small as possible using tagged indices (*). 
					</p><p>
					(*): For examples in Rust, take a look at the <a href="https://doc.rust-lang.org/stable/nightly-rustc/rustc_data_structures/tagged_ptr/index.html#">tagged_index crate</a> used in the compiler 
					or check out this recent <a href="https://mcyoung.xyz/2023/08/09/yarns/">blog post on small-string optimization</a>. 
					You'll find these optimizations all the time in high-performance code 
					like language runtimes, garbage collectors, compilers, game engines or OS kernels.
					</p>
					

					<p>
					Unfortunately, that doesn't completely solve the fragmentation issue. The other way is to tackle the container type directly! We could use a struct-of-arrays approach to store discriminant and value in two separate allocations. In fact, that's what the self-hosted Zig compiler actually does.
					</p>

					<figure>
						<p><b>Figure 3:</b> 
						The tags and union values are stored in two separate allocations, so we're not paying for padding anymore. However, the union collection still has variant fragmentation. 
						</p>
						<p><img alt="A struct-of-arrays transformation of an enum array" src="https://alic.dev/static/dense-enums/layout2.webp">
						</p>
					</figure>

					<p>
					Because of Zig's staged compilation, we can have container types that 
					perform this SoA transformation generically for any type. In Rust, 
					we're constrained to proc-macros like <span>soa_derive</span> 
					which has several downsides (e.g. we can't place <span>#[derive]</span> 
					on third-party types without changing their source).
					</p>

					<p>Reducing Variant Fragmentation</p><p>
					This SoA transformation reduces a lot of wasted padding introduced by 
					the enum tag, but still isn't optimal. To really get rid of fragmentation 
					in the values, we can create one vector <i>per variant</i>. 

					</p><figure>
						<p><b>Figure 4:</b> 
						Compared to the SoA layout from before, we have a partial order instead of a total order. So upon insertion, we get back a tagged index that holds both the enum tag and the index in the particular variant array. 
						</p>
						<p><img alt="Visualization of a simple AoVA layout" src="https://alic.dev/static/dense-enums/layout6.webp">
						</p>
					</figure><p>

					I don't think there's a name for this collection, so I call it <i>array of variant arrays</i> (or AoVA). This can be implemented in Rust and Zig, using proc-macros and comptime respectively.

					</p><p>Size Equivalence Classes</p>
					<p>
					We could stop here, but let's consider enums that have <i>lots</i> of variants that can be grouped into a small number of clusters with the same type size:
					</p><div>
<div>
<pre>enum Foo {
    A(u8, u8),
    B(u16),
    C(u16),
    D([u8; 2]),
    E(u32),
    F(u16, u16),
    G(u32),
    H(u32),
    I([u8; 4]),
    J(u32, u32),
    K(u32, (u16, u16)),
    L(u64),
    M(u64),
    N(u32, u16, u16),
    O([u8; 8])
}
</pre></div>
					<figure>
						<p><img alt="Naive AoVA layout causes us to create 15 different vectors - one per enum variant" src="https://alic.dev/static/dense-enums/layout15.webp">
						</p>
					</figure>
					</div>

					<p>
					As you can see, the one-vec-per-variant approach would add 15 vectors. 
					It's likely that the number of (re)allocations and system calls would
					increase substantially, and require a lot of memory to amortize
					compared to the naive <span>Vec<foo></foo></span>. 
					The vectors may also be arbitrarily spread in memory, 
					leading to a higher chance of cache conflicts. The AoVA collection itself
					also consumes a lot of memory, bloating any structure it's embedded in. 
					</p>

					<p>
					Now, if we group every variant by size, we get three clusters: 2, 4, and 8 bytes. Such clusters can be allocated together into the same vector - thereby reducing the number of total vectors we have in our container by 80%. So we could realistically store variants of <span>Foo</span> in three clusters:
					</p>
					<div>
<div>
<pre>struct FooVec {
    c_2: Vec&lt;[u8; 2]&gt;, // A - D
    c_4: Vec&lt;[u8; 4]&gt;, // E - I
    c_8: Vec&lt;[u8; 8]&gt;, // J - O
}
</pre></div>
					<figure>
						<p><img alt="The dense AoVA version reduces our vector count to 3" src="https://alic.dev/static/dense-enums/layout5.webp">
						</p>
					</figure>
					</div>

					<p>
					You could say this is a <i>dense</i> version of our AoVA pattern. 
					However, once we colocate different variants in the <i>same allocation</i>,
					we lose the ability to iterate through the vector in a type-safe way. 
					The only way to access elements in such a container is via the tagged 
					pointer that was created upon insertion. If your access pattern does 
					not require blind iteration (which can be the case for flattened, 
					index-based tree structures), this might be a worthwhile trade-off.  
					</p>
					
					<p>
					I've implemented a <a href="https://github.com/dist1ll/osmium">prototype of this data structure in Zig</a>. 
					The most important pieces are the compiler built-ins that allow reflection on
					field types, byte and bit sizes, as well as inspecting the discriminant.
					</p>

					<figure>
					<p><b>Snippet:</b> At its core, it performs straightforward compile-time 
					reflection to compute the clusters and field-to-cluster mappings. We do
					pseudo-dynamic allocation using a stack-allocated vector. 
					The cluster information is used to construct the AoVA data structure.  
					Exact <a href="https://github.com/dist1ll/osmium/blob/127145584fb22b48c05b5dbb14c670001318a81c/src/osmium.zig#L54-L71">source of the snippet is here</a>.
					</p>
<div>
<pre>// determine kind of type (i.e. struct, union, etc.)
switch (@typeInfo(inner)) {
    .Union =&gt; |u| {
        // store mapping from union field -&gt; cluster index
        var field_map = [_]u8{0} ** u.fields.len;

        // iterate over union fields
        for (u.fields, 0..) |field, idx| {
            // compute size
            const space = @max(field.alignment, @sizeOf(field.type));

            // insert into hashtable 
            if (!svec.contains_slow(space)) {
                svec.push(space) catch @compileError(ERR_01);
            }

            field_map[idx] = svec.len - 1;
        }

        // return clusters
        return .{ .field_map = field_map, .sizes = svec };
    },
    else =&gt; @compileError("only unions allowed"),
}
</pre></div>
					</figure>

					<p>
					If you do want type-safe iteration, you could pay the cost of padding, and add the tag back in:
					</p>

					<figure>
						<p><b>Figure 5:</b> 
						We've essentially partitioned the enum on the <i>data-level</i>, 
						leaving the interpretation at the type-level untouched
						</p>
						<p><img alt="Dense AoVA layout with additional tag and padding" src="https://alic.dev/static/dense-enums/layout3.webp">
						</p>
					</figure>

					<p>
					If the padding is too much, you can do an SoA transformation on each of the variant arrays.
					</p>

					<figure>
						<p><b>Figure 6:</b> Here we have a similar partitioning, but without
						the padding. The downside is that we're doubling the vector count.
						</p>
						<p><img alt="Dense AoVA layout, with tag and per-cluster SoA transformation" src="https://alic.dev/static/dense-enums/layout4.webp">
						</p>
					</figure>

					<p>
					So as you can see, there's quite a few trade-offs we can make in this space - 
					and they all depend on the concrete memory layout of our enum. 
					</p>

					<p>
					While creating such data structures is pretty straightforward in Zig, 
					creating any of these examples in Rust using proc macros is basically 
					impossible - the reason being that proc macros don't have access to 
					type information like size or alignment. While you could have a proc 
					macro generate a <span>const fn</span> that computes the clusters for a particular 
					enum, this function cannot be used to specify the length of an array 
					for a <i>generic</i> type.
					</p>

					<p>
					Another limit to Rust's generics is that the implementation of a 
					generic container cannot be conditioned on whether the given type is 
					an enum or a struct. 
					In Zig, we can effectively do something like this:
					</p>

					<figure>
<div>
<pre>// this is pseudocode
struct EfficientContainer<t> {
    if(T.isEnum()) {
        x: EfficientStructArray<t>,
    } else {
        x: EfficientEnumArray<t>,
    }
}
</t></t></t></pre></div>
					</figure>
					<p>
					We can also specialize the flavor of our AoVA implementation based on the enum. Maybe the benefits of colocating different variants only starts to make sense if we reduce the number of vectors by more than 90%. 
					</p>
					
					<p>
					So ultimately we gain a lot of fine-grained control over data structure selection. And if we have good heuristics, we can let the type-aware staging mechanism select the best implementation for us. To me, this represents a huge step in composability for high-performance systems software. 
					</p>

					<p>Bonus: Determining Index Bitwidth at Compile Time</p>
					<p>
					There's another pretty crazy feature that Zig's staged compilation 
					model gets you: if you know the maximum capacity of your data structure 
					at compile time, you can pass that information to the type-constructing function
					and let it determine the bitwidth of the returned tagged index.
					</p>

					<p>
					When this tagged index is included in a subsequent data structure, let's say another enum, this information carries over naturally, and the bits that we didn't need can be used for the discriminant!
					</p>

					<p>
					So what Zig gives you is <i>composable</i> memory efficiency. By being 
					specific about the number of bits you need, different parts of the code 
					can take advantage of that. And with implicit widening integer coercion, 
					dealing with APIs of different bitwidths stays ergonomic. In a way, 
					this reminds me a lot of refinement typing and ranged integers, so this 
					ties in a lot with my <a href="https://alic.dev/blog/custom-bitwidth">post on custom bitwidth integers</a>. 
					</p>
					
					<p>Conclusion</p><p>
					Writing extremely efficient generic data structures in Rust is not 
					always easy - in some cases they incur lots of accidental complexity, 
					in some others they're essentially impossible to implement. I think 
					one of the biggest takeaways for me with regards to staged compilation 
					was the ability to be composable on a memory layout level. If you're 
					developing a systems programming language that embraces efficiency and 
					zero-cost abstractions, you should absolutely take another look at 
					staged programming and in particular Zig's comptime.
					</p><!-- +++++++++ -->
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Japan's Hometown Tax (250 pts)]]></title>
            <link>https://www.kalzumeus.com/2018/10/19/japanese-hometown-tax/</link>
            <guid>37555004</guid>
            <pubDate>Mon, 18 Sep 2023 11:57:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.kalzumeus.com/2018/10/19/japanese-hometown-tax/">https://www.kalzumeus.com/2018/10/19/japanese-hometown-tax/</a>, See on <a href="https://news.ycombinator.com/item?id=37555004">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>This is outside of my normal software-focused beat, but I met some folks who were very interested in public policy recently. I found, to my surprise, that I probably understand one innovative Japanese tax policy better than very well-informed people who geek out about tax policy [<a href="#footnote-0">0</a>].</p>

<p>This post hopefully fixes that bug. (Hat tip to <a href="http://www.gwern.net/">gwern</a> for suggesting I write it up.)</p>

<h2 id="two-countries-in-one-border">Two countries in one border</h2>

<p>The Japanese employment market has a curious feature: there are regions of Japan with extremely high economic productivity (such as Tokyo, Osaka, and Nagoya, but for the purpose of this issue think “Tokyo” and you won’t be wrong) and regions with low economic productivity (substantially everywhere else). This counsels that a young person born and educated in e.g. Gifu move to Tokyo after graduation to earn a living.</p>

<p>Many, many do. While Japan’s overall population is declining, Tokyo’s increases by about 100,000 people per year.</p>

<p>The regions in Japan are not thrilled about this state of affairs for many reasons. Tokyo isn’t just the seat of Japanese commerce; it also houses the government, media, cultural institutions, etc etc. There is a real sense that your children moving to Tokyo causes them to lose connection with their culture and that the rewards from the national enterprise aren’t being allocated fairly. Tokyo, for its perspective, views the regions with the noblesse oblige that you would expect a cosmopolitan center of culture and learning to have with respect to their benighted country bumpkin cousins.</p>

<p>(If this sounds like it echoes the political economy of, say, two large English-speaking nations recently, well, folks greatly overestimate how different Western nations are from each other.)</p>

<h2 id="a-misalignment-in-incentives-for-human-capital-development">A misalignment in incentives for human capital development</h2>

<p>Educating children is incredibly expensive. The regions are quite annoyed that they pay to educate their children but that Tokyo reaps all the benefits. This state of affairs has continued for decades.</p>

<p>But Japan has a policy response for it, and it is sort of beautiful. Called ふるさと納税 (<em>Furusato Nouzei</em> or, roughly, the Hometown Tax System), it works something like this:</p>

<p>A substantial portion of Japan’s income-based taxes are residence taxes, which are paid to the city and prefecture (think state) that one resides in, based on one’s income in the previous year. The rate is a flat 10% of taxed income; due to quirks of calculating this which almost certainly aren’t relevant to you, you can estimate this as 8% of what white collar employees think their salary is.</p>

<p>Furusato Nouzei allows you to donate up to 40% of next year’s residence tax to one or many cities/prefectures of your choice, in return for a 1:1 credit on your tax next year. This is entirely opt-in. Anyone can participate, regardless of where they live.</p>

<p>In principle, the idea is to donate to one’s hometown. Importantly, one actually has unfettered discretion as to which city/prefecture one donates to. This has some very important implications discussed later.</p>

<p>Relevantly to your understanding of the incentives here: most Japanese people do not file taxes every year. Income-based taxes are calculated and remitted by employers directly on the behalf of their employees. Participating in the system requires friction which is somewhat above e.g. changing your direct deposit information but far below e.g. filing a tax return.</p>

<h2 id="what-was-the-idea-here">What was the idea here?</h2>

<p>Tokyo and the regions could have resolved their differences through the democratic process, in which the regions outvote Tokyo and could have altered Japan’s national tax and economic policies to their advantage. Tokyo obviously doesn’t want this, and instead agreed to an opt-in system which allays some of the regions’ concerns.</p>

<p>To the extent that taxpayers donate to their hometowns, Tokyo no longer freerides on the substantial public expenditures required to raise and educate internal migrants.</p>

<p>Putting potentially 40% of Tokyo’s residence tax in play is <em>not a small carrot</em>. Individual residence tax is roughly 45% of the city’s revenue. That works out to roughly <a href="http://www.metro.tokyo.jp/ENGLISH/ABOUT/FINANCIAL/financial02.htm">$30 billion a year.</a></p>

<p>Now if you were a negotiator for Tokyo back in ~2006 when this was being debated, you might have thought “Hmm, while this <em>sounds</em> like it is putting $12 billion a year into play, it’s not actually nearly that bad for us. People have to take affirmative steps to transfer the money to their city of choice, and they have to float the money for most of a year, because of the donate-then-credit mechanism. Uptake on that won’t be that high. Maybe we’ll lose a few tens of millions of dollars; no biggie. Silly country bumpkins; can’t even math.”</p>

<p>But after the system was created, city governments started getting really creative. And what happened next is by parts beautiful and crazy.</p>

<h2 id="incentives-rule-everything-around-me">Incentives rule everything around me</h2>

<p>There exists a culture in Japan of reciprocating gifts. While it varies based on where you live, in the areas I’ve lived, the general rule of thumb is 30%: if you give someone $300 cash on the occasion of their wedding, as is customary for gainfully employed people with respect to someone outside their immediate family, they’re socially obligated to find a way to give you $90 of value back. (The mechanisms for doing this could merit their own post; the word is 返礼品. A dictionary translates this as “quid pro quo”, but the sentiment does not match the common English usage of that phrase. This is simply a ritual; to not participate in it would be non-normative.)</p>

<p>While not formally defined in the legislation for the Furusato Nouzei system, someone at a city government figured that it was just not appropriate to let someone just give ~3% of their salary to the city without receiving a token of appreciation in return. So they sent something back; a can of locally-produced plums, say, to remind you of the tastes of your childhood.</p>

<p>And this was a beautiful idea! It directly improved the ability of the system to cement relationships between internal migrants and their hometowns, one of the declared goals of the system. It motivated people to fill out paperwork and float the city a bit of money for part of a year, because who doesn’t like free plums. (You might sensibly object that they aren’t free given the time value of money, but prevailing interest rates in Japan are indistinguishable from zero.) And it let cities specialize in marketing this initiative.</p>

<p>And specialize they did.</p>

<p>A number of cities in Japan, including my adoptive home town of Ogaki, have made this offer: for a no-cost-to-you donation of $100 or more, the city will send someone out to any grave in the city limits. That person will clean the grave, make an appropriate offering, and send you a photo. This is a beautiful thing.</p>

<p>Most of the gifts are more prosaic. Locally produced food is very popular. If you miss the taste of home, they’ve got you covered.</p>

<p>Cities partnered with local firms to handle the e-commerce aspect, and eventually with platforms to bundle many different items into a single donation; think of it as a shopping cart you could fill with donated money.</p>

<p>And then someone asked a fateful question.</p>

<h2 id="where-is-your-hometown-anyway">Where is your hometown, anyway?</h2>

<p>The Furusato Nouzei system does not define what a “hometown” is. This is mostly by design; Japan historically has a <em>very</em> long-lasting official record of birthplaces which follows one throughout life called the Family Register, and (for reasons outside the scope of this post) it is a major societal issue. Additionally, there was some sentiment that one could have a it-feels-like-home connection to a city that wasn’t necessarily one’s birthplace.</p>

<p>Maybe you were born in Tokyo but lived 30 years in a small town in Aichi, like my wife. Maybe you were born abroad but lived 10 years in Ogaki, like me. Maybe you just loved the onsen in Gero and wanted to subsidize them. The government wasn’t willing to adjudicate one’s “true” hometown; 帰る場所 is where the heart is.</p>

<p>And then some bureaucrat realized that this created a market: you, as a city government, can bid for taxpayers to select you as a hometown.</p>

<h2 id="how-does-that-work">How does that work?</h2>

<p>Well, remember the sites which are acting as brokers for donations? They all have search engines, so that you can search by e.g. who has wagyu available if that is your thing.</p>

<p>Your thing could, plausibly, be travel to your hometown. So your hometown could, plausibly, buy you tickets back to home. But this would be gratuitously operationally intensive.</p>

<p>You have to call city hall. They have to arrange transport. Why do this when Japan is a country with perfectly functioning travel agencies? It would be far better for everyone for your hometown to just send you a gift card to a travel agency.</p>

<p>See where this is going yet? A gift card for e.g. Japan’s largest travel agency is a highly liquid cash equivalent. In addition to using it for any good or service from that travel agency, you could liquidate it for about 97 cents on the dollar in any gift certificate exchange in the country. (These are extraordinarily common in Japan.)</p>

<p>A few rounds of vigorous capitalism later, many rural towns without large expatriate (inpatriate?) populations and without much to differentiate them in terms of local food had bid the consideration for a donation up, up, up. Eventually the central government stepped in and said that the maximum they’d allow is you rewarding a taxpayer with 50% of the donation in consideration.</p>

<p>So, if you “donate” ~3% of your gross salary to one of these cities (which is 1:1 matched by e.g. Tokyo; you’re donating someone else’s money), they will give you ~1.5% of it back in all-but-cash.</p>

<p>In 2008, about 33,000 people participated in the Furusato Nouzei system, principally out of genuine charitable concern. In 2016, it was about 2.2 million. They donated on the order of $2.5 billion. The primary accelerant was the bidding war. A contributor was the popularization of Internet sites to broker the donations, which substantially reduce the friction required to participate.</p>

<p>Running a site is a very good business to be in; it’s like running an e-commerce business with the special wrinkle that your customers are entirely price insensitive. There are a variety of smaller concerns, but the large Japanese Internet giants (Rakuten, Yahoo, etc) all use their massive built-in distribution and relationships to get an edge here. (The business model is simple: take the money from taxpayers, deduct a cut, spend some on gifts authorized by the city, and remit the remainder to the city periodically. You then periodically give the city an Excel file full of taxpayers. The city periodically sends their donors the requisite paperwork to get the tax credit the next year.)</p>

<p>I’d estimate that intermediaries probably soak up somewhere between 5% and 10% of the total donated. This is quite inefficient to accomplish a government-to-government reallocation of resources, but by the standards of Japanese public works projects it is practically free.</p>

<p>(Jokes aside: my estimate is informed by the fact that the margins are rich enough such that the intermediaries will <em>happily</em> support you making a donation on credit cards. The actual numbers are probably in a public disclosure somewhere but I don’t have enough time to go looking.)</p>

<h2 id="is-this-sustainable">Is this sustainable?</h2>

<p>Probably? There has been some talk of rolling back the bidding war via administrative fiat, but the cities are quite opposed to this. It’s a great game theory problem: unless a supermajority of cities collectively agrees to limit gifts to a token number, it’s strongly in a city’s interest to duck the central government’s questionnaires and not express any objection to the status quo. (Also remember that the natural anchor for reciprocation is set quite high across much of Japan; the government might succeed in capping reciprocation at 30% but that might be a hard floor for the ceiling.)</p>

<p>The penetration rate of this system will likely continue quickly increasing. It’s socially viral: a tax optimization that virtually anyone can take advantage of, has the explicit backing of the government, and feels wholesome. If you’re one of the relatively few taxpayers in Japan who has an accountant, expect them to tell you about this in detail and strongly recommend you max our your contribution every year.</p>

<p>Widespread gaming or no, the system pretty much works according to the internal aims. Cities get a list of their internal diaspora, and do make considerably more effort to stay in touch with them than they did previously. (This includes lovely holiday cards and sometimes even I-can’t-believe-they’re-not-alumni-magazines.) You really do get plums from childhood in your mail from your hometown (if you don’t optimize for cash equivalents). Cities with declining local tax bases really do get enough money to do material projects with. Tokyo takes a hit to revenue but can afford it.</p>

<p>And there, that’s Japan’s most novel redistribution program in a nutshell.</p>

<p>If you live in Japan and want to take advantage of this, hit up your local Google; many sites are happy to make it happen for you. (I don’t endorse any in particular, but any of the top brands or organic search results will work substantially as advertised.)</p>

<p><a name="footnote-0"></a>
[0]: I had been of the impression up until today that it was literally not on the English-speaking Internet, but this seems to have changed in the last few years. That said, nobody seems to have written about the policy angle in English yet, so here we are.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: How to do literal web searches after Google destroyed the “ ” feature? (242 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37554736</link>
            <guid>37554736</guid>
            <pubDate>Mon, 18 Sep 2023 11:18:52 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37554736">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="37555385"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555385" href="https://news.ycombinator.com/vote?id=37555385&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>Kagi. Never have I been so happy to send someone $10 every month. When you become the customer, not the product, it’s amazing what can happen.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37555513"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555513" href="https://news.ycombinator.com/vote?id=37555513&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>100% agree on Kagi. Happy customer. Thought it would be just another one of my attempts to use Duck Duck Go that dies after two weeks of !g usage. Turned out Kagi just works. The biggest improvement / gains is on mobile, where you suddenly don't need to scroll through 5 screens of ad results to get to the content.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37556094"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37556094" href="https://news.ycombinator.com/vote?id=37556094&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>I wonder if part of why its better is due to other users providing feedback about results, but also you can pin results from specific domains to the top. Like I can pin any results from StackOverflow, instead of the garbage StackOverflow rip off sites Google keeps giving me, its pretty obvious its ripping off SO because I just read the same thing word for word on StackOverflow three links ago. Thanks Google.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37555920"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555920" href="https://news.ycombinator.com/vote?id=37555920&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>Weird question that I have that I'd love anyone who makes a Kagi account to trial after reading the parent comment to answer:<p>When you make your account, you're given the option to customize. When you do, you can pick things like color theme and how URLs are displayed. On the right hand side of the page there is a preview of what your Kagi searches will look like.</p><p>In my example, the demo Kagi search is Magic The Gathering. I play <i>a lot</i> of Magic The Gathering. I spend most of my time online searching for things related to MtG or brewing decks, second only to things related to software development.</p><p>I imagine it's coincidence. MtG is a pretty nerdy hobby and Kagi seems like a pretty nerdy product. However, it made me uncomfortable enough to ask:</p><p>Is that what it shows for everyone? Or is there some tracking going on already that is being demoed? It's almost certainly the former given the positioning of Kagi in the search market, but I'd like to be sure.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37555940"><td></td></tr>
                <tr id="37556113"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37556113" href="https://news.ycombinator.com/vote?id=37556113&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>Just as an extra data point - same here, got shown MtG results, never played it (though I might have searched about it a few times way back).</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37556153"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37556153" href="https://news.ycombinator.com/vote?id=37556153&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>I'm kind of blown away by how popular that game has gotten over the past few years in North America. I think the pandemic really accelerated the popularity of that and D&amp;D, people are still doing these things after all of that. Even saw someone playing over the phone the other day. I don't seem to remember it being so popular but now it's more than ever and hardly a surprise tbh</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37556066"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37556066" href="https://news.ycombinator.com/vote?id=37556066&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>Can’t second this emotion hard enough, love it, have never looked back, almost never bail out to !g - still use g maps for most location stuff, but all my web search is very comfortably living on kagi</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37556038"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37556038" href="https://news.ycombinator.com/vote?id=37556038&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>You sold me.<p>Fuck google, I have work to do. Thanks for the tip! Nice realizing that they've basically been wasting my time for a while now and that there's a decent alternative available.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37556018"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37556018" href="https://news.ycombinator.com/vote?id=37556018&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>“ Our data includes anonymized API calls to traditional search indexes like Google, Mojeek and Yandex”. They pay google to do this?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37556087"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37556087" href="https://news.ycombinator.com/vote?id=37556087&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>I believe DuckDuckGo does (or at least they did) this with Bing. Starting a new scraper at a scale that users would need to be useful for what they're used to is such a huge jump. I'm sure if Kagi continue to grow they'd prioritize their own scraping too, but that's just not feasible at first.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37556108"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37556108" href="https://news.ycombinator.com/vote?id=37556108&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>Back in the day I'd suggest doing it via Alexa top sites, but now that Alexa is gone, I'm not sure what strategy I would use, but I would want to hit sites that are like the "top 10000 most popular" first, and scrape every inch I could.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37556092"><td></td></tr>
                  <tr id="37555672"><td></td></tr>
                <tr id="37555743"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37555743" href="https://news.ycombinator.com/vote?id=37555743&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>I'm sure many folks, including me, would be willing to help Vlad organize a private acquisition if he suddenly had some pressing need to sell.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37556046"><td></td></tr>
                        <tr id="37555425"><td></td></tr>
            <tr id="37555567"><td></td></tr>
                  <tr id="37555314"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555314" href="https://news.ycombinator.com/vote?id=37555314&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>I agree that as a programmer who frequently needs to search for long literal strings verbatim, Google has become notably less useful than it used to be.<p>I wonder if there is now a gap in the market for some kind of "literal" search engine that makes no attempt to infer meaning on your search terms and simply gives you the closest results? In other words Google ca 2012.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37555357"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555357" href="https://news.ycombinator.com/vote?id=37555357&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span><a href="https://publicwww.com/" rel="nofollow noreferrer">https://publicwww.com/</a> is a great tool for this, though the size of its index leaves a lot to be desired. Still, for enumerating well-SEO'd homepages that use a certain tech stack, it's quite useful!</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37555509"><td></td></tr>
                  <tr id="37555356"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555356" href="https://news.ycombinator.com/vote?id=37555356&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>You have to use a special "verbatim" search product from Google, it isn't the main search box anymore.  Look under Advanced or something.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37555404"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555404" href="https://news.ycombinator.com/vote?id=37555404&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>On the search results page, there's a "Tools" button in the upper-right that expands two dropdowns.  Change "All results" to "Verbatim".</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37555429"><td></td></tr>
                <tr id="37555895"><td></td></tr>
                <tr id="37556215"><td></td></tr>
                              <tr id="37556187"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37556187" href="https://news.ycombinator.com/vote?id=37556187&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>&gt; You have to use a special "verbatim" search product from Google, it isn't the main search box anymore. Look under Advanced or something.<p>I actually use that, but it has its faults. You get more spam results and iffy sites (e.g. Wikipedia clones). It's also missing some of Google's convenient features (like doing unit conversions and arithmetic).</p><p>IIRC, Verbatim mode is closer to the raw results of Google's underlying search engine, before some of the massaging they do. Some of that massaging is bad, but some of it's all right.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37555963"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555963" href="https://news.ycombinator.com/vote?id=37555963&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>Why would they make their very expected functionality something you have to dig around to find now? That just seems like really bad decision making that should have been spotted by someone on top and screams that the people on top are now disconnected from reality.<p>I swear that it used to work for certain strings I'm trying to find now which I was able to find information on and now it isn't even returning, with "verbatim" set, something that is in a very well-known program's documentation. Bing finds like three results. Google has dropped the ball so hard it's embarrassing
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37556208"><td></td></tr>
            <tr id="37556020"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37556020" href="https://news.ycombinator.com/vote?id=37556020&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>It's also possible they they're correct, and the average user of today has much less need for verbatim searches than the average user when the search engine was first designed.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37556072"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37556072" href="https://news.ycombinator.com/vote?id=37556072&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>Nah their product is broken and I'm going to avoid using it from now on except when I need to find something on a map or mess around on SEO for work<p>If they break a key feature to MAKE a verbatim search happen with literally two keys pressed, and are apparently not even indexing what they used to anymore, they're dropping the ball. Most people know the quotation trick now and are probably assuming it still works
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37556064"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37556064" href="https://news.ycombinator.com/vote?id=37556064&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>This is definitely the case. Sit down next to a casual user one day, and you'll find that 'verbatim' is the absolute last thing they need.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37555735"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555735" href="https://news.ycombinator.com/vote?id=37555735&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>Great tip! Seems like this adds the query param "&amp;tbs=li%3A1", so this might be something you can configure as an extra search engine in firefox. But then I am a happy kagi customer, and I was just thinking that I can't recall the last time I had to do a !g to find something. For me, google search si pretty much dead. I only use it now when on someone else's pc</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37555459"><td></td></tr>
            <tr id="37555741"><td></td></tr>
                <tr id="37556206"><td></td></tr>
                        <tr id="37555346"><td></td></tr>
                <tr id="37555401"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555401" href="https://news.ycombinator.com/vote?id=37555401&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>Duck Duck Go has similar problems for me.  It's also recently started sometimes ignoring the "-" when you try to exclude a word.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37555417"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37555417" href="https://news.ycombinator.com/vote?id=37555417&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>We put out a partial fix for that recently and a more complete fix is forthcoming.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37555525"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37555525" href="https://news.ycombinator.com/vote?id=37555525&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>Partial fix for the minus or the double quotes? Both are such a critical part for searching anything these days, it’d be a real shame to lose either.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37555625"><td></td></tr>
                <tr id="37556080"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37556080" href="https://news.ycombinator.com/vote?id=37556080&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>That'll be awesome. The parts of the web that don't respect operands feel awful.<p>Having a minus act like a plus has been particularly tough (for years now, I believe).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="37555931"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37555931" href="https://news.ycombinator.com/vote?id=37555931&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>I think the "-" is case-sensitive sometimes. So if you write "-honey" it'll still return "Honey" results, so you have to write "-honey -Honey".</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37555536"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555536" href="https://news.ycombinator.com/vote?id=37555536&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>I’ve used DDG exclusively for a couple years now and at least with my usecases have found it better than Google in every single way other than images (which I still use it for, but need to do !g to proxy Google maybe 1 in 5 times).</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37556081"><td></td></tr>
                  <tr id="37555386"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555386" href="https://news.ycombinator.com/vote?id=37555386&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>I find better results on it in general.<p>I use Google as a fallback but these days it happens perhaps once every couple months, and mostly I don't get anything out of it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37555476"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37555476" href="https://news.ycombinator.com/vote?id=37555476&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>It depends on the usecase. If you're querying for something local like a restaurant or a store, google wins by orders of magnitude. If you're researching, duckduckgo is fine. Duckduckgo doesn't really compare to Kagi though.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37555471"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37555471" href="https://news.ycombinator.com/vote?id=37555471&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>Yes. IMO DDG has been better for general questions for a long time.<p>Google remained better for programming questions for significantly longer (I speculate this may be because Google's own programmers used it, and complained when the results sucked :-)), but now it's not. Not really.</p><p>Like you, I still use Google as a long shot, but that's become quite rare.</p><p>I sometimes use Bing for Microsoft-specific questions, if DDG doesn't give me what I want. I have the sense that Bing covers Microsoft a little better than the others do. I have no real solid evidence for this, but it seems plausible on the surface.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                                  <tr id="37555313"><td></td></tr>
                <tr id="37555399"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555399" href="https://news.ycombinator.com/vote?id=37555399&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>+1, this is the only way to get old-school(ish) results, even without explicit quoting. Unfortunately, it can't be enforced just by adding parameters to the URL and has to be selected manually every time. (edit: actually it can, see below)<p>Verbatim really highlights how almost useless the default mode has become.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37555518"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37555518" href="https://news.ycombinator.com/vote?id=37555518&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>You can add it with parameters:
tbs=li:1<pre><code>    https://www.google.com/search?tbs=li:1&amp;num=30&amp;safe=off&amp;q=test</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37555626"><td></td></tr>
                  <tr id="37555628"><td></td></tr>
                        <tr id="37555922"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555922" href="https://news.ycombinator.com/vote?id=37555922&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>I see that the quotes in your title are not regular ASCII quotes. I agree that Google's search result quality has been quite horrible, but could the fact that your system is somehow not emitting "real" quotes also be the cause? I've seen plenty of problems caused by those horrid "smart" quotes in the past.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37555289"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555289" href="https://news.ycombinator.com/vote?id=37555289&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>&gt; Search for a domain name with quotation marks for example just recombines the contents of the domain and returns a bunch of unrelated content completely cluttering what I am looking for.<p>site:example.com and -site:example.com still work, I think.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37555374"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555374" href="https://news.ycombinator.com/vote?id=37555374&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>That limits results to pages on example.com (or neg for the second) but that's not what OP is trying to do. Searches for "example.com" should return pages where the exact domain "example.com" appears in the page. It does sort of work for "example.com" itself, because it's so common presumably, but doing it with other domains that are less common is far less likely to produce anything useful.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37555308"><td></td></tr>
                  <tr id="37556027"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37556027" href="https://news.ycombinator.com/vote?id=37556027&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>Quotes don't work but<p>&gt;&gt;&gt;&gt;Search for a domain name with quotation marks for example just recombines the contents of the domain and returns a bunch of unrelated content</p><p>I believe if you search for site:domaingoeshere.com yourqueryhere</p><p>That will spit out results only from that domain. I think that still works ?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37556119"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37556119" href="https://news.ycombinator.com/vote?id=37556119&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>Thank you for asking the important questions, you've saved me a large amount of time here realizing that this isn't just my particular case of search terms being broken somehow. I guess we've all counted on google being reliable all these years and it's kind of a shock that they've gone and tanked their own usefulness after building up like twenty years of trust. Rude.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37555718"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555718" href="https://news.ycombinator.com/vote?id=37555718&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>Just give up on trying to make google work. Google has definitely not improved. Recently I tried searching "hugginface madebyollin" (with a slight type to hugginface instead of huggingface) and it literally didn't show the obvious result. <a href="https://www.google.com/search?q=hugginface+madebyollin" rel="nofollow noreferrer">https://www.google.com/search?q=hugginface+madebyollin</a> (Seems to work in incognito i.e. some A/B variant. But my account doesn't, and it's crazy that that would be an alternative they present to users.) I've switched to duckduckgo and couldn't be happier.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37555880"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555880" href="https://news.ycombinator.com/vote?id=37555880&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>&gt; <i>I used this quite frequently but since Google """"improved"""" it last year (there was a popular HN post complaining about this) it doesn't work anymore</i><p>That's not my experience? The quotes still seem to work for me? Do you have a specific example? And / or can you point to said HN post?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37556118"><td></td></tr>
            <tr id="37555947"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555947" href="https://news.ycombinator.com/vote?id=37555947&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>I can confirm the experience that sometimes searches for quoted terms yield pages that do not contain the quoted term exactly. I don't have a specific example from the top of my head.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37556004"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37556004" href="https://news.ycombinator.com/vote?id=37556004&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>Is it possible that the results simply don't exist and instead of giving you nothing it gives the next possible match?</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37556040"><td></td></tr>
                  <tr id="37555551"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555551" href="https://news.ycombinator.com/vote?id=37555551&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>Can you give an example? Searching for<pre><code>    "news.ycombinator.com"
</code></pre>
works just fine for me. No unrelated content.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37556005"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37556005" href="https://news.ycombinator.com/vote?id=37556005&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>How do the search results for a site: query compare to just quoting what appears to be a DNS domain containing punctuation tokens?<pre><code>  inurl:news.ycombinator.com
  site:news.ycombinator.com
  news.ycombinator.com</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37555359"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555359" href="https://news.ycombinator.com/vote?id=37555359&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>Use Bing? I've found for code/error message snippets in particular Bing is often better, while Google will incorrectly guess I mean some related but different thing and serve the wrong results.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37555839"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555839" href="https://news.ycombinator.com/vote?id=37555839&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>Bing's verbatim feature needs a + in front of the quotes, I believe; but then it will still screw up horribly on things like error codes (you want 1234, it'll give you 1235 and 1233.)</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37555440"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555440" href="https://news.ycombinator.com/vote?id=37555440&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>confirmed (although I use ecosia instead of bing directly). Now its like:<p>semantic search + realtime needs = google;</p><p>raw searches is ecosia(bing)</p><p>and <i>most</i> needs are covered via GPT4 actually. Error debugging + code snippets, general interest explanations, arguing things, evaluating things interactively. Basically everything I don't need hard facts of the last 2 years for.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37555477"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555477" href="https://news.ycombinator.com/vote?id=37555477&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>&gt; <i>Google """"improved"""" it last year</i><p>Google's done a lot of "improvements". I hate to say it but its quote feature has been broken for a decade. You're only now noticing?</p><p>I've tried using other search engines. I've settled on DuckDuckGo. It also does not have a working literal-quote feature. But it's much less infested with SEO garbage.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37556044"><td></td></tr>
            <tr id="37556112"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37556112" href="https://news.ycombinator.com/vote?id=37556112&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>IS that what happened?  This was driving crazy for awhile.  Google you are the worst - why would you do that.  Probably time to move off their other product suite too.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37555677"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555677" href="https://news.ycombinator.com/vote?id=37555677&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>A HN comment recently recommended Yandex describing it as what Google search used to be.<p>After seeing the post and trying Yandex it was absolutely right, it’s what Google search used to be.</p><p>Now whenever I use Google and it’s just a list of ecommerce adds or content farms duplicating the same content without substance, I head to Yandex and get the type of results I used to get from Google.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37555876"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555876" href="https://news.ycombinator.com/vote?id=37555876&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>Guess it's because Yandex gets most of its ad revenue from searches in Russian. Search results for English queries are not so ad-cluttered. For searches in Russian language, Google is still better (shows less ads per page), but tends to prefer .ua websites. I guess their ML predicts European IP + Russian language as a proxy for someone from Ukraine now, which is pretty logical.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37555846"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555846" href="https://news.ycombinator.com/vote?id=37555846&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>Agree. Yandex seems to perform better. 
Heavy use of captcha's (mullvad vpn), but once through that, enjoy the service.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37555907"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555907" href="https://news.ycombinator.com/vote?id=37555907&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>The solution is to simply stop using google and find other search engine, eventually decide to pay for really useful search engines like kagi.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37555558"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555558" href="https://news.ycombinator.com/vote?id=37555558&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>Similar vein, how the hell do I search an image and combine it with text anymore? I hate Lens and there's apparently no way to properly "image" search anymore.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37555332"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555332" href="https://news.ycombinator.com/vote?id=37555332&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>For locking to a particular domain I had always added `site: example.com` to the query, rather than adding domain to a double quote statement.<p>I have used double quotes to limit to a particular _phrase_ as recently as last week. I'm not privy to the improvements you mention, do you have a link?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37555328"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555328" href="https://news.ycombinator.com/vote?id=37555328&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>The minus (-) operator isn't a hard and fast rule anymore either. It's particularly galling when you want to search for something that's similar to something else that's very common.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37555288"><td></td></tr>
                <tr id="37555321"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37555321" href="https://news.ycombinator.com/vote?id=37555321&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>really depends on the query, I have seen it working and I have seen it being totally ignored.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37555715"><td></td></tr>
                        <tr id="37555697"><td></td></tr>
            <tr id="37555899"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555899" href="https://news.ycombinator.com/vote?id=37555899&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><br><div>
                  <p><span>I still try to use + but I guess that hasn't worked since before Google Plus. I never thought I'd want altavista back.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37555507"><td></td></tr>
            <tr id="37555455"><td></td></tr>
            <tr id="37555338"><td></td></tr>
            <tr id="37555333"><td></td></tr>
            <tr id="37555569"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37555569" href="https://news.ycombinator.com/vote?id=37555569&amp;how=up&amp;goto=item%3Fid%3D37554736"></a></center>    </td><td><p><span>It's never stopped working for me, for all queries, and believe me, I use it very often.<p>This post reeks of Kagi spam to me.</p><p>Sorry guys, your product failed, no one is going to pay for search no matter how much Google sucks. Move on.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NSA’s Backdoor Key (2002) (305 pts)]]></title>
            <link>http://www.cypherspace.org/adam/hacks/lotus-nsa-key.html</link>
            <guid>37554504</guid>
            <pubDate>Mon, 18 Sep 2023 10:47:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.cypherspace.org/adam/hacks/lotus-nsa-key.html">http://www.cypherspace.org/adam/hacks/lotus-nsa-key.html</a>, See on <a href="https://news.ycombinator.com/item?id=37554504">Hacker News</a></p>
<div id="readability-page-1" class="page">

<em>This page has also been translated into Russian <a href="http://www.designcontest.com/show/nsa-key-be">here</a><br>
and into Polish <a href="http://www.autoteiledirekt.de/science/backdoor-klucz-nsa-z-lotus-notes">here</a>
</em>



<p>Before the US crypto export regulations were finally disolved the
export version of Lotus Notes used to include a key escrow / backdoor
feature called differential cryptography.  The idea was that they got
permission to export 64 bit crypto if 24 of those bits were encrypted
for the NSA's public key.  The NSA would then only have the small
matter of brute-forcing the remaining 40 bits to get the plaintext,
and everyone else would get a not-that-great 64 bit key space (which
probably already back then NSA would have had the compute power to
brute force also, only at higher cost).</p>

<p>Anyway as clearly inside the application somewhere would be an NSA
public key that the NSA had the private key for, I tried reverse
engineering it to get the public key.</p>

<p>In doing this I discovered that the NSA public key had an
organizational name of "MiniTruth", and a common name of
"Big Brother".  Specifically what I saw in my debugger late
one night, which was spooky for a short moment was:</p>

<block>
<b><tt>O=MiniTruth CN=Big Brother</tt></b>
</block>

<p>Literary note: for those who have not read Orwell's prescient
"1984" the Ministry of Truth was the agency who's job was
propaganda and suppression of truths that did not suit the malignant
fictional future government in the book, and "Big Brother"
was the evil shadowy leader of this government.  The whole book is <a href="http://www.george-orwell.org/1984">online here</a>.</p>

<h2>The NSA's Public Key</h2>

<p>I put this together some years after the reverse-engineering stint,
so there could be errors, but this is from my notes, the raw public
key modulus from the debugger:</p>

<block><b><tt><pre>  8D9D6213D3EF03A7 A5CEAE99B8E9FF06
  12E58ECAAB2939FE 72B41833B8B947A0
  DF8111B561CE67FB 50844623CF88338C
  E7BC80C5ECC31276 6075E13E12E956F6
  59954F68B04F0FEA B6B82EFEC4E07BD8
  4BC41FE3123AF70C 31688BCD5895BB00
</pre></tt></b></block>

<p>I figured it was in little endian format by trial and error; other
formats were easy to factor.  So the big endian hex representation is:

<block><b><tt></tt></b></block></p><b><tt><xmp>
  e = 3
  n = \
  00BB9558CD8B68310CF73A12E31FC44BD87BE0C4FE2EB8B6EA0F4FB0684F9559\
  F656E9123EE175607612C3ECC580BCE78C3388CF23468450FB67CE61B51181DF\
  A047B9B83318B472FE3929ABCA8EE51206FFE9B899AECEA5A703EFD313629D8D
</xmp></tt></b>

<p>where the modulus is 760 bits, and the public key formatted as a
PGP key is (of course I made this user id up -- you can edit it to
whatever you choose it's of course not self-signed):</p>

<block><b><tt><xmp>
Type Bits/KeyID    Date       User ID
pub   760/13629D8D 1998/10/25 Director, NSA <dirnsa@nsa.gov>

-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: 2.6.3i

mQBsAzYyeuIAAAEC+LuVWM2LaDEM9zoS4x/ES9h74MT+Lri26g9PsGhPlVn2VukS
PuF1YHYSw+zFgLznjDOIzyNGhFD7Z85htRGB36BHubgzGLRy/jkpq8qO5RIG/+m4
ma7OpacD79MTYp2NAAIDtB5EaXJlY3RvciwgTlNBIDxkaXJuc2FAbnNhLmdvdj4=
=aoSi
-----END PGP PUBLIC KEY BLOCK-----
</xmp></tt></b></block>

<p>and here's what pgpacket has to say about the contents of that
key:</p>

<block><tt><b><xmp>
---------------------------
Packet Type:Public Key Packet
Length:108
Version Byte:3
Key Created:25 Oct 1998  01:12:02
Valid forever
Algorithm:1 (RSA)
N:0xBB9558CD8B68310CF73A12E31FC44BD87BE0C4FE2EB8B6EA0F4FB0684F9559F6\
56E9123EE175607612C3ECC580BCE78C3388CF23468450FB67CE61B51181DFA0\
47B9B83318B472FE3929ABCA8EE51206FFE9B899AECEA5A703EFD313629D8D
E:0x03
Key ID: 0xA703EFD313629D8D

---------------------------
Packet Type:User ID Packet
Length:30
User ID:"Director, NSA <dirnsa@nsa.gov>"
</xmp></b></tt></block>

<hr>
<em>
Comments, html bugs to  
(<a href="http://www.cypherspace.org/adam/">Adam Back</a>) at
<a href="mailto:adam@cypherspace.org">&lt;adam@cypherspace.org&gt;</a>
</em>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unix shells are generally not viable access control mechanisms any more (151 pts)]]></title>
            <link>https://utcc.utoronto.ca/~cks/space/blog/sysadmin/UnixShellsNoMoreAccessControl</link>
            <guid>37554406</guid>
            <pubDate>Mon, 18 Sep 2023 10:33:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://utcc.utoronto.ca/~cks/space/blog/sysadmin/UnixShellsNoMoreAccessControl">https://utcc.utoronto.ca/~cks/space/blog/sysadmin/UnixShellsNoMoreAccessControl</a>, See on <a href="https://news.ycombinator.com/item?id=37554406">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Unix shells are generally not viable access control mechanisms any more</h2>

	<p><small>September 17, 2023</small></p>
</div><div><p>Once upon a time, if you had a collection of Unix systems, you could
reasonably do a certain amount of access control to your overall
environment by forcing logins to have specific administrative shells.
As a bonus, these administrative shells could print helpful messages
about why the particular login wasn't being allowed to use your system.
This is a quite attractive bundle of features, but unfortunately this
no longer works in a (modern) Unix environment with logins (<a href="https://utcc.utoronto.ca/~cks/space/blog/sysadmin/OurDifferentSysadminEnvironment">such as
we have</a>). There are two core problems.</p>

<p>First, you almost certainly operate a variety of services that
normally only use Unix logins as a source of (password) authentication
and perhaps a UID to operate as, and ignore the login's shell. This
is the common pattern of Samba, IMAP servers, <a href="https://utcc.utoronto.ca/~cks/space/blog/web/ApacheBasicAuthWhy">Apache HTTP Basic
Authentication</a>, and so on. In some cases
you may be able to teach these services to look at the login's shell
and do special things, but some of them are sealed black boxes and
even the ones that can be changed require you to go out of your
way. If you forget one, it fails open (allowing access to people
with an administrative shell that should lock them out).</p>

<p>(One of these services is SSH itself, since you can generally
initiate SSH sessions and ask for port forwarding or other features
that don't cause SSH to run the login shell.)</p>

<p>Second, you may operate general authentication services, such as
LDAP or a Single Sign On system, and if you do these authentication
services are generally blind to what they're being used for and
thus to whether or not a login with a special shell should be allowed
to pass this particular authentication. The only real solution is
to have multiple versions of these authentication systems with
different logins in them, and point systems at different ones based
on exactly who should be allowed to use them.</p>

<p>A similar issue happens with Apache HTTP Basic Authentication in
common configurations, where you have a single authentication realm
with a single Apache htpasswd file that covers an assortment of
different services. If you need certain logins ('locked' logins or
the like) to be excluded from some of these services but not others,
either you need multiple htpasswd files (at least) or you need to
teach each such service to do additional checks.</p>

<p>(In general you're going to have to try to carefully review who
should be able to use which of your services when, and the resulting
matrix is often surprisingly complicated and tangled. Life gets more
complicated if you're using administrative shells for reasons other
than just locking people out with a message, for example to try to
force an initial password change.)</p>

<p>Today, the only two measures of login access control that really
work in a general environment are either scrambling the login's
password (and disable any SSH authorized keys) or excluding the
login entirely from your various authentication data sources (your
LDAP servers, your Apache htpasswd files, and so on). It's a pity
that changing people's shells is no longer enough (it was both easy
and convenient), but that's how the environment has evolved.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What If OpenDocument Used SQLite? (418 pts)]]></title>
            <link>https://www.sqlite.org/affcase1.html</link>
            <guid>37553574</guid>
            <pubDate>Mon, 18 Sep 2023 08:11:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sqlite.org/affcase1.html">https://www.sqlite.org/affcase1.html</a>, See on <a href="https://news.ycombinator.com/item?id=37553574">Hacker News</a></p>
<div id="readability-page-1" class="page">
<div>
<p><a href="https://www.sqlite.org/index.html">
<img src="https://www.sqlite.org/images/sqlite370_banner.gif" alt="SQLite">
</a></p>
<p>
Small. Fast. Reliable.<br>Choose any three.
</p>



</div>






<h2>Introduction</h2>

<p>Suppose the
<a href="http://en.wikipedia.org/wiki/OpenDocument">OpenDocument</a> file format,
and specifically the "ODP" OpenDocument Presentation format, were
built around SQLite.  Benefits would include:
</p><ul>
<li>Smaller documents
</li><li>Faster File/Save times
</li><li>Faster startup times
</li><li>Less memory used
</li><li>Document versioning
</li><li>A better user experience
</li></ul>

<p>
Note that this is only a thought experiment.
We are not suggesting that OpenDocument be changed.
Nor is this article a criticism of the current OpenDocument
design.  The point of this essay is to suggest ways to improve
future file format designs.

</p><h2>About OpenDocument And OpenDocument Presentation</h2>

<p>
The OpenDocument file format is used for office applications:
word processors, spreadsheets, and presentations.  It was originally
designed for the OpenOffice suite but has since been incorporated into
other desktop application suites.  The OpenOffice application has been
forked and renamed a few times.  This author's primary use for OpenDocument is 
building slide presentations with either 
<a href="https://www.neooffice.org/neojava/en/index.php">NeoOffice</a> on Mac, or
<a href="http://www.libreoffice.org/">LibreOffice</a> on Linux and Windows.

</p><p>
An OpenDocument Presentation or "ODP" file is a
<a href="http://en.wikipedia.org/wiki/Zip_%28file_format%29">ZIP archive</a> containing
XML files describing presentation slides and separate image files for the
various images that are included as part of the presentation.
(OpenDocument word processor and spreadsheet files are similarly
structured but are not considered by this article.) The reader can
easily see the content of an ODP file by using the "zip -l" command.
For example, the following is the "zip -l" output from a 49-slide presentation
about SQLite from the 2014
<a href="http://southeastlinuxfest.org/">SouthEast LinuxFest</a>
conference:

</p><blockquote><pre>Archive:  self2014.odp
  Length      Date    Time    Name
---------  ---------- -----   ----
       47  2014-06-21 12:34   mimetype
        0  2014-06-21 12:34   Configurations2/statusbar/
        0  2014-06-21 12:34   Configurations2/accelerator/current.xml
        0  2014-06-21 12:34   Configurations2/floater/
        0  2014-06-21 12:34   Configurations2/popupmenu/
        0  2014-06-21 12:34   Configurations2/progressbar/
        0  2014-06-21 12:34   Configurations2/menubar/
        0  2014-06-21 12:34   Configurations2/toolbar/
        0  2014-06-21 12:34   Configurations2/images/Bitmaps/
    54702  2014-06-21 12:34   Pictures/10000000000001F40000018C595A5A3D.png
    46269  2014-06-21 12:34   Pictures/100000000000012C000000A8ED96BFD9.png
<i>... 58 other pictures omitted...</i>
    13013  2014-06-21 12:34   Pictures/10000000000000EE0000004765E03BA8.png
  1005059  2014-06-21 12:34   Pictures/10000000000004760000034223EACEFD.png
   211831  2014-06-21 12:34   content.xml
    46169  2014-06-21 12:34   styles.xml
     1001  2014-06-21 12:34   meta.xml
     9291  2014-06-21 12:34   Thumbnails/thumbnail.png
    38705  2014-06-21 12:34   Thumbnails/thumbnail.pdf
     9664  2014-06-21 12:34   settings.xml
     9704  2014-06-21 12:34   META-INF/manifest.xml
---------                     -------
 10961006                     78 files
</pre></blockquote>

<p>
The ODP ZIP archive contains four different XML files:
content.xml, styles.xml, meta.xml, and settings.xml.  Those four files
define the slide layout, text content, and styling.  This particular
presentation contains 62 images, ranging from full-screen pictures to
tiny icons, each stored as a separate file in the Pictures
folder.  The "mimetype" file contains a single line of text that says:

</p><blockquote><pre>application/vnd.oasis.opendocument.presentation
</pre></blockquote>

<p>The purpose of the other files and folders is presently 
unknown to the author but is probably not difficult to figure out.

</p><h2>Limitations Of The OpenDocument Presentation Format</h2>

<p>
The use of a ZIP archive to encapsulate XML files plus resources is an
elegant approach to an application file format.
It is clearly superior to a custom binary file format.
But using an SQLite database as the
container, instead of ZIP, would be more elegant still.

</p><p>A ZIP archive is basically a key/value database, optimized for
the case of write-once/read-many and for a relatively small number
of distinct keys (a few hundred to a few thousand) each with a large BLOB
as its value.  A ZIP archive can be viewed as a "pile-of-files"
database.  This works, but it has some shortcomings relative to an
SQLite database, as follows:

</p><ol>
<li><p><b>Incremental update is hard.</b>
</p><p>
It is difficult to update individual entries in a ZIP archive.
It is especially difficult to update individual entries in a ZIP
archive in a way that does not destroy
the entire document if the computer loses power and/or crashes
in the middle of the update.  It is not impossible to do this, but
it is sufficiently difficult that nobody actually does it.  Instead, whenever
the user selects "File/Save", the entire ZIP archive is rewritten.  
Hence, "File/Save" takes longer than it ought, especially on
older hardware.  Newer machines are faster, but it is still bothersome
that changing a single character in a 50 megabyte presentation causes one
to burn through 50 megabytes of the finite write life on the SSD.

</p></li><li><p><b>Startup is slow.</b>
</p><p>
In keeping with the pile-of-files theme, OpenDocument stores all slide 
content in a single big XML file named "content.xml".  
LibreOffice reads and parses this entire file just to display
the first slide.
LibreOffice also seems to
read all images into memory as well, which makes sense seeing as when
the user does "File/Save" it is going to have to write them all back out
again, even though none of them changed.  The net effect is that
start-up is slow.  Double-clicking an OpenDocument file brings up a
progress bar rather than the first slide.
This results in a bad user experience.
The situation grows ever more annoying as
the document size increases.

</p></li><li><p><b>More memory is required.</b>
</p><p>
Because ZIP archives are optimized for storing big chunks of content, they
encourage a style of programming where the entire document is read into
memory at startup, all editing occurs in memory, then the entire document
is written to disk during "File/Save".  OpenOffice and its descendants
embrace that pattern.

</p><p>
One might argue that it is ok, in this era of multi-gigabyte desktops, to
read the entire document into memory.
But it is not ok.
For one, the amount of memory used far exceeds the (compressed) file size
on disk.  So a 50MB presentation might take 200MB or more RAM.  
That still is not a problem if one only edits a single document at a time.  
But when working on a talk, this author will typically have 10 or 15 different 
presentations up all at the same
time (to facilitate copy/paste of slides from past presentation) and so
gigabytes of memory are required.
Add in an open web browser or two and a few other 
desktop apps, and suddenly the disk is whirling and the machine is swapping.
And even having just a single document is a problem when working
on an inexpensive Chromebook retrofitted with Ubuntu.
Using less memory is always better.
</p>

</li><li><p><b>Crash recovery is difficult.</b>
</p><p>
The descendants of OpenOffice tend to segfault more often than commercial
competitors.  Perhaps for this reason, the OpenOffice forks make
periodic backups of their in-memory documents so that users do not lose
all pending edits when the inevitable application crash does occur.
This causes frustrating pauses in the application for the few seconds
while each backup is being made.
After restarting from a crash, the user is presented with a dialog box
that walks them through the recovery process.  Managing the crash
recovery this way involves lots of extra application logic and is
generally an annoyance to the user.

</p></li><li><p><b>Content is inaccessible.</b>
</p><p>
One cannot easily view, change, or extract the content of an 
OpenDocument presentation using generic tools.
The only reasonable way to view or edit an OpenDocument document is to open
it up using an application that is specifically designed to read or write
OpenDocument (read: LibreOffice or one of its cousins).  The situation
could be worse.  One can extract and view individual images (say) from
a presentation using just the "zip" archiver tool.  But it is not reasonable
try to extract the text from a slide.  Remember that all content is stored
in a single "context.xml" file.  That file is XML, so it is a text file.
But it is not a text file that can be managed with an ordinary text
editor.  For the example presentation above, the content.xml file
consist of exactly two lines. The first line of the file is just:

</p><blockquote><pre>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
</pre></blockquote>

<p>The second line of the file contains 211792 characters of
impenetrable XML.  Yes, 211792 characters all on one line.
This file is a good stress-test for a text editor.
Thankfully, the file is not some obscure
binary format, but in terms of accessibility, it might as well be
written in Klingon.
</p></li></ol>

<h2>First Improvement:  Replace ZIP with SQLite</h2>

<p>
Let us suppose that instead of using a ZIP archive to store its files,
OpenDocument used a very simple SQLite database with the following
single-table schema:

</p><blockquote><pre>CREATE TABLE OpenDocTree(
  filename TEXT PRIMARY KEY,  -- Name of file
  filesize BIGINT,            -- Size of file after decompression
  content BLOB                -- Compressed file content
);
</pre></blockquote>

<p>
For this first experiment, nothing else about the file format is changed.
The OpenDocument is still a pile-of-files, only now each file is a row
in an SQLite database rather than an entry in a ZIP archive.
This simple change does not use the power of a relational
database.  Even so, this simple change shows some improvements.

<a name="smaller"></a>

</p><p>
Surprisingly, using SQLite in place of ZIP makes the presentation
file smaller.  Really.  One would think that a relational database file
would be larger than a ZIP archive, but at least in the case of NeoOffice
that is not so.  The following is an actual screen-scrape showing
the sizes of the same NeoOffice presentation, both in its original 
ZIP archive format as generated by NeoOffice (self2014.odp), and 
as repacked as an SQLite database using the 
<a href="http://www.sqlite.org/sqlar/doc/trunk/README.md">SQLAR</a> utility:

</p><blockquote><pre>-rw-r--r--  1 drh  staff  10514994 Jun  8 14:32 self2014.odp
-rw-r--r--  1 drh  staff  10464256 Jun  8 14:37 self2014.sqlar
-rw-r--r--  1 drh  staff  10416644 Jun  8 14:40 zip.odp
</pre></blockquote>

<p>
The SQLite database file ("self2014.sqlar") is about a
half percent smaller than the equivalent ODP file!  How can this be?
Apparently the ZIP archive generator logic in NeoOffice
is not as efficient as it could be, because when the same pile-of-files
is recompressed using the command-line "zip" utility, one gets a file
("zip.odp") that is smaller still, by another half percent, as seen
in the third line above.  So, a well-written ZIP archive
can be slightly smaller than the equivalent SQLite database, as one would
expect.  But the difference is slight.  The key take-away is that an
SQLite database is size-competitive with a ZIP archive.

</p><p>
The other advantage to using SQLite in place of
ZIP is that the document can now be updated incrementally, without risk
of corrupting the document if a power loss or other crash occurs in the
middle of the update.  (Remember that writes to 
<a href="https://www.sqlite.org/atomiccommit.html">SQLite databases are atomic</a>.)   True, all the
content is still kept in a single big XML file ("content.xml") which must
be completely rewritten if so much as a single character changes.  But
with SQLite, only that one file needs to change.  The other 77 files in the
repository can remain unaltered.  They do not all have to be rewritten,
which in turn makes "File/Save" run much faster and saves wear on SSDs.

</p><h2>Second Improvement:  Split content into smaller pieces</h2>

<p>
A pile-of-files encourages content to be stored in a few large chunks.
In the case of ODP, there are just four XML files that define the layout
off all slides in a presentation.  An SQLite database allows storing
information in a few large chunks, but SQLite is also adept and efficient
at storing information in numerous smaller pieces.

</p><p>
So then, instead of storing all content for all slides in a single
oversized XML file ("content.xml"), suppose there was a separate table
for storing the content of each slide separately.  The table schema
might look something like this:

</p><blockquote><pre>CREATE TABLE slide(
  pageNumber INTEGER,   -- The slide page number
  slideContent TEXT     -- Slide content as XML or JSON
);
CREATE INDEX slide_pgnum ON slide(pageNumber); -- Optional
</pre></blockquote>

<p>The content of each slide could still be stored as compressed XML.
But now each page is stored separately.  So when opening a new document,
the application could simply run:

</p><blockquote><pre>SELECT slideContent FROM slide WHERE pageNumber=1;
</pre></blockquote>

<p>This query will quickly and efficiently return the content of the first
slide, which could then be speedily parsed and displayed to the user.
Only one page needs to be read and parsed in order render the first screen,
which means that the first screen appears much faster and
there is no longer a need for an annoying progress bar.

</p><p>If the application wanted
to keep all content in memory, it could continue reading and parsing the
other pages using a background thread after drawing the first page.  Or,
since reading from SQLite is so efficient, the application might 
instead choose to reduce its memory footprint and only keep a single
slide in memory at a time.  Or maybe it keeps the current slide and the
next slide in memory, to facility rapid transitions to the next slide.

</p><p>
Notice that dividing up the content into smaller pieces using an SQLite
table gives flexibility to the implementation.  The application can choose
to read all content into memory at startup.  Or it can read just a
few pages into memory and keep the rest on disk.  Or it can read just
single page into memory at a time.  And different versions of the application
can make different choices without having to make any changes to the
file format.  Such options are not available when all content is in
a single big XML file in a ZIP archive.

</p><p>
Splitting content into smaller pieces also helps File/Save operations
to go faster.  Instead of having to write back the content of all pages
when doing a File/Save, the application only has to write back those
pages that have actually changed.

</p><p>
One minor downside of splitting content into smaller pieces is that
compression does not work as well on shorter texts and so the size of
the document might increase.  But as the bulk of the document space 
is used to store images, a small reduction in the compression efficiency 
of the text content will hardly be noticeable, and is a small price 
to pay for an improved user experience.

</p><h2>Third Improvement:  Versioning</h2>

<p>
Once one is comfortable with the concept of storing each slide separately,
it is a small step to support versioning of the presentation.  Consider
the following schema:

</p><blockquote><pre>CREATE TABLE slide(
  slideId INTEGER PRIMARY KEY,
  derivedFrom INTEGER REFERENCES slide,
  content TEXT     -- XML or JSON or whatever
);
CREATE TABLE version(
  versionId INTEGER PRIMARY KEY,
  priorVersion INTEGER REFERENCES version,
  checkinTime DATETIME,   -- When this version was saved
  comment TEXT,           -- Description of this version
  manifest TEXT           -- List of integer slideIds
);
</pre></blockquote>

<p>
In this schema, instead of each slide having a page number that determines
its order within the presentation, each slide has a unique
integer identifier that is unrelated to where it occurs in sequence.
The order of slides in the presentation is determined by a list of
slideIds, stored as a text string in the MANIFEST column of the VERSION
table.
Since multiple entries are allowed in the VERSION table, that means that
multiple presentations can be stored in the same document.

</p><p>
On startup, the application first decides which version it
wants to display.  Since the versionId will naturally increase in time
and one would normally want to see the latest version, an appropriate
query might be:

</p><blockquote><pre>SELECT manifest, versionId FROM version ORDER BY versionId DESC LIMIT 1;
</pre></blockquote>

<p>
Or perhaps the application would rather use the
most recent checkinTime:

</p><blockquote><pre>SELECT manifest, versionId, max(checkinTime) FROM version;
</pre></blockquote>

<p>
Using a single query such as the above, the application obtains a list
of the slideIds for all slides in the presentation.  The application then
queries for the content of the first slide, and parses and displays that
content, as before.

</p><p>(Aside:  Yes, that second query above that uses "max(checkinTime)"
really does work and really does return a well-defined answer in SQLite.
Such a query either returns an undefined answer or generates an error
in many other SQL database engines, but in SQLite it does what you would 
expect: it returns the manifest and versionId of the entry that has the
maximum checkinTime.)

</p><p>When the user does a "File/Save", instead of overwriting the modified
slides, the application can now make new entries in the SLIDE table for
just those slides that have been added or altered.  Then it creates a
new entry in the VERSION table containing the revised manifest.

</p><p>The VERSION table shown above has columns to record a check-in comment
(presumably supplied by the user) and the time and date at which the File/Save
action occurred.  It also records the parent version to record the history
of changes.  Perhaps the manifest could be stored as a delta from the
parent version, though typically the manifest will be small enough that
storing a delta might be more trouble than it is worth.  The SLIDE table
also contains a derivedFrom column which could be used for delta encoding
if it is determined that saving the slide content as a delta from its
previous version is a worthwhile optimization.

</p><p>So with this simple change, the ODP file now stores not just the most
recent edit to the presentation, but a history of all historic edits.  The
user would normally want to see just the most recent edition of the
presentation, but if desired, the user can now go backwards in time to 
see historical versions of the same presentation.

</p><p>Or, multiple presentations could be stored within the same document.

</p><p>With such a schema, the application would no longer need to make
periodic backups of the unsaved changes to a separate file to avoid lost
work in the event of a crash.  Instead, a special "pending" version could
be allocated and unsaved changes could be written into the pending version.
Because only changes would need to be written, not the entire document,
saving the pending changes would only involve writing a few kilobytes of
content, not multiple megabytes, and would take milliseconds instead of
seconds, and so it could be done frequently and silently in the background.
Then when a crash occurs and the user reboots, all (or almost all)
of their work is retained.  If the user decides to discard unsaved changes, 
they simply go back to the previous version.

</p><p>
There are details to fill in here.
Perhaps a screen can be provided that displays a history changes
(perhaps with a graph) allowing the user to select which version they
want to view or edit.  Perhaps some facility can be provided to merge
forks that might occur in the version history.  And perhaps the
application should provide a means to purge old and unwanted versions.
The key point is that using an SQLite database to store the content,
rather than a ZIP archive, makes all of these features much, much easier
to implement, which increases the possibility that they will eventually
get implemented.

</p><h2>And So Forth...</h2>

<p>
In the previous sections, we have seen how moving from a key/value
store implemented as a ZIP archive to a simple SQLite database
with just three tables can add significant capabilities to an application
file format.
We could continue to enhance the schema with new tables, with indexes
added for performance, with triggers and views for programming convenience,
and constraints to enforce consistency of content even in the face of
programming errors.  Further enhancement ideas include:
</p><ul>
<li> Store an <a href="https://www.sqlite.org/undoredo.html">automated undo/redo stack</a> in a database table so that
     Undo could go back into prior edit sessions.
</li><li> Add <a href="https://www.sqlite.org/fts3.html#fts4">full text search</a> capabilities to the slide deck, or across
     multiple slide decks.
</li><li> Decompose the "settings.xml" file into an SQL table that
     is more easily viewed and edited by separate applications.
</li><li> Break out the "Presentor Notes" from each slide into a separate
     table, for easier access from third-party applications and/or scripts.
</li><li> Enhance the presentation concept beyond the simple linear sequence of
     slides to allow for side-tracks and excursions to be taken depending on
     how the audience is responding.
</li></ul>

<p>
An SQLite database has a lot of capability, which
this essay has only begun to touch upon.  But hopefully this quick glimpse
has convinced some readers that using an SQL database as an application
file format is worth a second look.

</p><p>
Some readers might resist using SQLite as an application
file format due to prior exposure to enterprise SQL databases and
the caveats and limitations of those other systems.  
For example, many enterprise database
engines advise against storing large strings or BLOBs in the database
and instead suggest that large strings and BLOBs be stored as separate
files and the filename stored in the database.  But SQLite 
is not like that.  Any column of an SQLite database can hold
a string or BLOB up to about a gigabyte in size.  And for strings and
BLOBs of 100 kilobytes or less, 
<a href="https://www.sqlite.org/intern-v-extern-blob.html">I/O performance is better</a> than using separate
files.

</p><p>
Some readers might be reluctant to consider SQLite as an application
file format because they have been inculcated with the idea that all
SQL database schemas must be factored into third normal form and store
only small primitive data types such as strings and integers.  Certainly
relational theory is important and designers should strive to understand
it.  But, as demonstrated above, it is often quite acceptable to store
complex information as XML or JSON in text fields of a database.
Do what works, not what your database professor said you ought to do.

</p><h2>Review Of The Benefits Of Using SQLite</h2>

<p>
In summary,
the claim of this essay is that using SQLite as a container for an application
file format like OpenDocument
and storing lots of smaller objects in that container
works out much better than using a ZIP archive holding a few larger objects.
To wit:

</p><ol>
<li><p>
An SQLite database file is approximately the same size, and in some cases
smaller, than a ZIP archive holding the same information.

</p></li><li><p>
The <a href="https://www.sqlite.org/atomiccommit.html">atomic update capabilities</a>
of SQLite allow small incremental changes
to be safely written into the document.  This reduces total disk I/O
and improves File/Save performance, enhancing the user experience.

</p></li><li><p>
Startup time is reduced by allowing the application to read in only the
content shown for the initial screen.  This largely eliminates the
need to show a progress bar when opening a new document.  The document
just pops up immediately, further enhancing the user experience.

</p></li><li><p>
The memory footprint of the application can be dramatically reduced by
only loading content that is relevant to the current display and keeping
the bulk of the content on disk.  The fast query capability of SQLite
make this a viable alternative to keeping all content in memory at all times.
And when applications use less memory, it makes the entire computer more
responsive, further enhancing the user experience.

</p></li><li><p>
The schema of an SQL database is able to represent information more directly
and succinctly than a key/value database such as a ZIP archive.  This makes
the document content more accessible to third-party applications and scripts
and facilitates advanced features such as built-in document versioning, and
incremental saving of work in progress for recovery after a crash.
</p></li></ol>

<p>
These are just a few of the benefits of using SQLite as an application file
format — the benefits that seem most likely to improve the user
experience for applications like OpenOffice.  Other applications might
benefit from SQLite in different ways. See the <a href="https://www.sqlite.org/appfileformat.html">Application File Format</a>
document for additional ideas.

</p><p>
Finally, let us reiterate that this essay is a thought experiment.
The OpenDocument format is well-established and already well-designed.
Nobody really believes that OpenDocument should be changed to use SQLite
as its container instead of ZIP.  Nor is this article a criticism of
OpenDocument for not choosing SQLite as its container since OpenDocument
predates SQLite.  Rather, the point of this article is to use OpenDocument
as a concrete example of how SQLite can be used to build better 
application file formats for future projects.


</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenRA – Classic strategy games rebuilt for the modern era (421 pts)]]></title>
            <link>https://www.openra.net/</link>
            <guid>37553193</guid>
            <pubDate>Mon, 18 Sep 2023 07:07:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openra.net/">https://www.openra.net/</a>, See on <a href="https://news.ycombinator.com/item?id=37553193">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      
        <article>
  
  <div>
    <div>

    <p>Another week, another playtest! Notable changes include:</p>

    <ul>
      <li>Added new Red Alert mission “Soviet13b”</li>
      <li>Fixed dedicated servers not allowing maps with Lua scripts to be played</li>
      <li>Fixed the rare aircraft visual jitter while steering</li>
      <li>Fixed the map importer crashing when trying to convert maps with tile errors</li>
      <li>Fixed the low power notification never playing</li>
    </ul>

    <p>For more information, see the <a href="https://github.com/OpenRA/OpenRA/wiki/Changelog/b9b8aeb58667d08595d6471378fdf0d593b12de2">full changelog</a>.</p>

    <p>And of course the Tiberian Dawn HD playtest can be <a href="https://github.com/OpenRA/TiberianDawnHD/releases/tag/playtest-20230808">downloaded here</a>. For more information see the <a href="https://www.openra.net/news/playtest-20230801/">original playtest-20230801 announcement</a>.</p>

    <p>In an effort of pushing out a swift release we highly encourage everybody to play some games and report any issues you may find in the comments below, on <a href="https://forum.openra.net/">our forum</a>, <a href="https://discord.openra.net/">Community Discord</a>, or <a href="https://github.com/OpenRA/OpenRA/">GitHub</a>. Good luck on the battlefield, commanders!</p>

  </div>

<p>The <a href="https://www.openra.net/news/playtest-20230801/">original playtest-20230801 announcement</a> is included below:</p>

<hr>

<p>As summer is in full swing, we are excited to kick off the playtest series for the upcoming OpenRA release! This time around, it is all about spring cleaning: squashing bugs, implementing minor requested features, and preparing for larger reworks. But fear not! We still have a bunch of cool stuff lined up just for you! Without further ado, here are some notable changes you can expect in <a href="https://www.openra.net/download">Playtest 20230801</a>:</p>

<ul>
  <li>Added new Covert Ops missions for Tiberian Dawn: “Eviction Notice” and “Twist of Fate”</li>
  <li>Added new Red Alert missions “Allies10b” and “Soviet13a” and the Counter-Strike mission “Mousetrap”</li>
  <li>Fixed several palette remapping issues and restored the ability to select darker player colors</li>
  <li>Implemented a new custom Lua wrapper which improves error reporting for map scripters</li>
  <li>Introduced a modular asset installer which allows for installation of the classic assets via Steam or the EA app</li>
  <li>Reworked sprite sequences, significantly improving third party modding support</li>
</ul>

<figure>
  <video autoplay="" loop="" muted="">
    <source src="https://www.openra.net/images/news/20230801-smoke.mp4" type="video/mp4">
    <source src="https://www.openra.net/images/news/20230801-smoke.webm" type="video/webm">
    <img src="https://www.openra.net/images/news/20230801-smoke.png">
  </video>
  <figcaption>One of the smaller features is a new smoke emitter for D2k's damaged vehicles which better matches the original</figcaption>
</figure>

<p>For more information see the <a href="https://github.com/OpenRA/OpenRA/wiki/Changelog/dc43bf4e0213eb50e2f4b6a3090f8bab031079d3">full changelog</a>.</p>

<p>And before we wrap up, we would like mention that this release contains significant progress towards C&amp;C Remastered Collection support, including compatibility with the new EA app. The “Tiberian Dawn HD” playtest is a separate release and can be <a href="https://github.com/OpenRA/TiberianDawnHD/releases/tag/playtest-20230801">downloaded here</a>. This preview is multiplayer-compatible with the main 20230801 playtest. However, please keep in mind that performance, memory usage and loading times have not been optimized yet. The C&amp;C Remastered Collection must be installed through Steam or the EA App, and in case you are using macOS or Linux the project README provides you <a href="https://github.com/OpenRA/TiberianDawnHD#asset-installation">detailed installation instructions</a>.</p>

<p>Stay tuned for more updates and be sure to take part in the playtest. Don’t forget to share your feedback with us on <a href="https://forum.openra.net/">our forum</a>, <a href="https://discord.openra.net/">community Discord server</a>, or <a href="https://github.com/OpenRA/OpenRA/issues">GitHub</a>. Enjoy the summer gaming season with OpenRA!</p>

  </div>
  
</article>
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iPhone 15 Pro facts and estimates (102 pts)]]></title>
            <link>https://leancrew.com/all-this/2023/09/iphone-15-pro-facts-and-estimates/</link>
            <guid>37552605</guid>
            <pubDate>Mon, 18 Sep 2023 05:39:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://leancrew.com/all-this/2023/09/iphone-15-pro-facts-and-estimates/">https://leancrew.com/all-this/2023/09/iphone-15-pro-facts-and-estimates/</a>, See on <a href="https://news.ycombinator.com/item?id=37552605">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="container">
        <p>
          <span><a href="https://leancrew.com/all-this/2023/09/testing-mathml/">Next post</a></span>
          <a href="https://leancrew.com/all-this/2023/09/a-football-score-matrix/">Previous post</a>        </p>
        
        <p>September 13, 2023 at  2:40 PM by Dr. Drang</p>
        <p>During <a href="https://www.apple.com/apple-events/">yesterday’s keynote</a>, I learned some things about the switch from stainless steel to titanium in the iPhone 15 Pro that I’d been guessing about before. I also did some quick and dirty calculations that might explain why <a href="https://www.macworld.com/article/2065050/iphone-15-pro-apple-watch-series-9-ultra-2-hands-on.html">Jason Snell thought</a> the 15 Pro seemed distinctly lighter than the 14 Pro, even though the weight reduction is only 9–10%.</p>

<p>The titanium alloy used in the phone was revealed by Isabel Yang about 57 minutes into the presentation. She called it Grade 5 titanium, which is an <a href="https://www.astm.org/b0265-20a.html">ASTM designation</a>. It’s also known as Ti-6Al-4V, because its major alloying elements are aluminum at 6% and vanadium at 4%. <a href="https://www.podfeet.com/blog/2023/09/titanium-iphone/">Allison Sheridan talked about its properties</a> earlier this month, and I’ve been assuming that it would be the alloy Apple would choose ever since I heard they were switching to titanium for the band.</p>

<p>I guessed it would be Ti-6Al-4V because it’s the garden-variety alloy for titanium. A great material, but not exotic in any way. Apart from many aerospace applications, it’s also used in medical implants, so you know that skin contact won’t be a problem.</p>

<p>Shortly after the introduction of the alloy, Yang talked about how the titanium band is attached to the rest of the phone’s structure, which is aluminum. According to <a href="https://www.apple.com/newsroom/2023/09/apple-unveils-iphone-15-pro-and-iphone-15-pro-max/">Apple’s newsroom</a>:</p>

<blockquote>
  <p>Using an industry-first thermo-mechanical process, the titanium bands encase a new substructure made from 100 percent recycled aluminum, bonding these two metals with incredible strength through solid-state diffusion.</p>
</blockquote>

<p>In other words, the titanium and aluminum are welded together. Not the kind of welding you’re used to, to be sure, but still welding—solid-state welding with no melting of either material. The thermo part of the “thermo-mechanical process” is heating up the materials, and the mechanical part is smushing them together. In essence, this is the oldest form of welding, the kind the village smithy did under the spreading chestnut tree with a forge and a hammer.</p>

<p>I’m sure the process control needed to do solid-state welding with such thin parts is well beyond what other companies can achieve, and I can understand why Apple didn’t want to describe it using a term that conjures up images of sweaty guys in tilt-down helmets making sparks in a dusty manufacturing plant. But it’s still welding.</p>

<p>Finally, we come to Jason Snell’s surprise at how light the 15 Pro seemed when he played with it in the hands-on area. He mentioned this not only in his <a href="https://www.macworld.com/article/2065050/iphone-15-pro-apple-watch-series-9-ultra-2-hands-on.html">Macworld article</a>, but also in <a href="https://www.relay.fm/upgrade/476">the post-keynote episode of <em>Upgrade</em></a>. You wouldn’t expect a change from 206 g for the 14 Pro to 187 g for the 15 Pro would be that noticeable, but Greg Joswiak mentioned it in the keynote and Jason confirmed it. How can that be?</p>

<p>One answer is that people are just more sensitive than we give them credit for being. A 9–10% drop in weight may seem like a small amount to our brains but a large amount to our hands. But because it allowed me to do some simple calculations, I decided to look into another possibility.</p>

<p>Your ability to manipulate a phone is based primarily on its mass, but also on its <a href="https://en.wikipedia.org/wiki/Moment_of_inertia">moment of inertia</a>. And since the reduction in mass when switching from stainless steel to titanium is occurring almost entirely at the perimeter of the phone, the moment of inertia should be reduced more than if the mass were reduced uniformly.</p>

<p>Let’s assume the two phones are the same size<sup id="fnref:smaller"><a href="#fn:smaller" rel="footnote">1</a></sup>, 147.5 mm high by 71.5 mm wide (the 7.85 mm thickness can be ignored). We’ll set the origin at the geometric center of the phone and the x, y, and z axes will be associated with what would normally be called pitch, roll, and yaw. We’ll be doing enough approximating that there’s no point in trying to account for the phone’s rounded corners.</p>

<p><img width="60%" src="https://leancrew.com/all-this/images2023/20230913-iPhone%20cuboid%20with%20axes.png" alt="iPhone cuboid with axes" title="iPhone cuboid with axes"></p>

<p>If the 187 g mass of the 15 Pro were distributed uniformly, its moment of inertia about the x-axis would be</p><p>

\[I_{xx}^{(15)} = \frac{1}{12}(187 \;\mathrm{g})(147.5 \;\mathrm{mm})^2 = 339,035\; \mathrm{gm \cdot mm^2}\]

</p><p>If we assume the 14 Pro’s additional 19 g of mass is distributed uniformly around the perimeter, we can say that the long sides have</p><p>

\[\frac{147.5 \;\mathrm{mm}}{2(147.5 \;\mathrm{mm} + 71.5 \;\mathrm{mm})} (19 \;\mathrm{g}) = 6.4  \;\mathrm{g}\]

</p><p>of extra mass and the short sides have</p><p>

\[\frac{71.5 \;\mathrm{mm}}{2(147.5 \;\mathrm{mm} + 71.5 \;\mathrm{mm})} (19 \;\mathrm{g}) = 3.1 \;\mathrm{g}\]

</p><p>of extra mass. The moment of inertia of the these four lines of additional mass about the x-axis is</p><p>

\[I_{xx}^{(lines)} = 2 \left[ \frac{1}{12}(6.4 \;\mathrm{g})(147.5 \;\mathrm{mm})^2 + (3.1 \;\mathrm{g})\left(\frac{147.5 \;\mathrm{mm}}{2}\right)^2 \right]\]

\[I_{xx}^{(lines)} = 56,929 \;\mathrm{g \cdot mm^2}\]

</p><p>You’ll note the use of the parallel axis theorem in the second term inside the brackets. I’m not calculating the moments of inertia of the top and bottom lines about their own axes because that’s too small to worry about.</p>

<p>Therefore, the moment of inertia of the 14 Pro is</p><p>

\[I_{xx}^{(14)} = I_{xx}^{(15)} + I_{xx}^{(lines)} = 395,964 \;\mathrm{g \cdot mm^2}\]

</p><p>and the reduction in the moment of inertia about the x-axis is</p><p>

\[\frac{I_{xx}^{(14)} - I_{xx}^{(15)}}{I_{xx}^{(14)}} = \frac{56,929}{395,964} = 0.144\]

</p><p>or 14–15%. This reduction, which is more than the mass reduction, would make the iPhone 15 Pro easier to turn, and that may add to the impression that it’s significantly lighter than the 14 Pro.</p>

<p>These calculations were fun, but the initial assumption, that the 15 Pro’s mass is uniformly distributed, is unquestionably wrong. How wrong depends on how non-uniform the mass distribution is, and if I knew that I wouldn’t have had to make the assumption in the first place. My guess is that the assumption is good enough for this kind of back-of-the-envelope calculation.</p>

<p>But even if the numbers are further off than I think, the concept is correct. Reducing the mass at the perimeter, which the change from stainless steel to titanium has done, has definitely reduced the moment of inertia more than a uniform reduction in mass would have. And that will make the 15 Pro easier to manipulate and will contribute—at least somewhat—to the impression of lightness.</p>

<p>You can, of course, do the same sort of calculation for the moments of inertia about the roll and yaw axes. This is left as an exercise for the reader.</p>


        <p>
          <span><a href="https://leancrew.com/all-this/2023/09/testing-mathml/">Next post</a></span>
          <a href="https://leancrew.com/all-this/2023/09/a-football-score-matrix/">Previous post</a>        </p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How FoundationDB works and why it works (2021) (185 pts)]]></title>
            <link>https://blog.the-pans.com/notes-on-the-foundationdb-paper/</link>
            <guid>37552085</guid>
            <pubDate>Mon, 18 Sep 2023 04:00:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.the-pans.com/notes-on-the-foundationdb-paper/">https://blog.the-pans.com/notes-on-the-foundationdb-paper/</a>, See on <a href="https://news.ycombinator.com/item?id=37552085">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
<article>
<section>
<p>FoundationDB is a very impressive database. Its <a href="https://www.foundationdb.org/files/fdb-paper.pdf">paper</a> won the best industry paper award in SIGMOD'21. In this post, I will explain, in detail, how FDB works and discuss a few very interesting design choices they made. It's a dense paper packed with neat ideas. Many details (sometimes even proof of correctness) are not included in the paper. I added proof wherever necessary.</p>
<h2 id="whatisfoundationdb">What is FoundationDB?</h2>
<p>It's a <em>non-sharded</em>, <em>strict serializable</em>, fault tolerant, <em>key-value</em> store that supports point writes, reads and range reads. Notice that it provides a key-value API (not SQL). It's also not sharded, meaning the entire key space is essentially on one logical shard. That's it. Once you have a strict serializable key-value store, you can layer a SQL engine and secondary indexes on top. A strict serializable (can be relaxed if needed obviously) key-value store is the foundation (a smaller reusable component), upon which you can build distributed databases almost<sup><a href="#fn1" id="fnref1">[1]</a></sup> however you want. This is a <em>great</em> design choice.</p>
<h2 id="boldclaims">Bold Claims</h2>
<p>The paper makes a few jaw-dropping claims in the intro.</p>
<ul>
<li>FDB is strict serializable and lock-free. How is this possible?
<ul>
<li>me: Don't you have to take locks to guarantee order?</li>
</ul>
</li>
<li>For a global FDB deployment, it can avoid x-region write latency.
<ul>
<li>me: Hmm... OK. It's possible if you make some assumptions about failure domains.</li>
</ul>
</li>
<li>FDB can tolerate <em>f</em> failures with only <em>f+1</em> replicas.
<ul>
<li>me: You gotta be kidding me!</li>
</ul>
</li>
</ul>
<p>We will come back to all these jaw-dropping claims. If these claims don't excite you, this post and the paper are probably not for you.</p>
<h2 id="architecture">Architecture</h2>
<p><img src="https://blog.the-pans.com/content/images/2021/07/Screen-Shot-2021-07-05-at-11.01.20-AM.png" alt="Screen-Shot-2021-07-05-at-11.01.20-AM" loading="lazy"></p>
<p>FDB takes the idea of microservice to the extreme (they call it decoupling). Almost every single functionality is handled by a distinct service (stateful or stateless). Embrace yourself for many names for different roles.</p>
<h4 id="controlplane">Control Plane</h4>
<p>The most important Control Plane service in the diagram is the <em>Coordinator</em>. It basically stores small metadata about the entire FDB deployment/configuration (e.g. the current epoch, which gets bumped every time a reconfiguration/recovery takes place). <em>ClusterController</em> monitors the health of all servers (presumably via heartbeats, as it's not mentioned in the paper).</p>
<h4 id="dataplane">Data Plane</h4>
<p>FDB breaks the Data Plane into three parts, Transaction System, Log System and Storage System. Log System and Storage System are mostly what you would expect – a distributed log and a disaggregated, sharded storage. The Transaction System is the most interesting one, among which the <em>Sequencer</em> is the most critical service.</p>
<h2 id="transactionmanagement">Transaction Management</h2>
<ol>
<li>A client is always connected to a Proxy, which communicates with FDB internal services.</li>
<li>The Proxy will get a read-version in the form of HLC (hybrid logical clock) from the <em>Sequencer</em>. Notice that there's only one (not logical) physical <em>Sequencer</em> in the system, which serves as a single point of serialization (ordering). The <em>Sequencer</em> conceptually has the following API, which keeps producing monotonically increasing HLCs(or LSNs).<br>
<img src="https://blog.the-pans.com/content/images/2021/07/Screen-Shot-2021-07-17-at-11.58.41-PM.png" alt="Screen-Shot-2021-07-17-at-11.58.41-PM" loading="lazy"> The singularity of the <em>Sequencer</em> is critical, as it provides <em>the</em> ordering of all relevant events of an entire FDB deployment. HLC and LSN are interchangeable for the rest of this post.</li>
<li>The client then issues reads to <em>Storage</em> with the specific read-version, acquired from the <em>Sequencer</em>. Storage System will return data at the specific point-in-time indicated by the read-version (an HLC) – MVCC.</li>
<li>The client then asks <em>Sequencer</em> again for a commit-version.</li>
<li>Next, it sends the write-set <em>and</em> the read-set to the <em>Resolver</em>. <em>Resolvers</em> are range-sharded and they keep a short history of recent writes. Here, it detects if there are any conflicts (i.e. if the data read earlier in the transaction has changed or not, between <code>[read-version, commit-version]</code>).</li>
<li>If no conflict is detected, commit the writes to the Log System with commit-version. Once enough ACKs have been received from the logs, the <em>Proxy</em> returns success to the client.</li>
</ol>
<p>It's basically Optimistic Concurrency Control (OCC), very straightforward. In this example, it actually doesn't even need MVCC. Because if there are conflicts, i.e. what the client read has changed, the transaction will be either aborted or retried anyway. MVCC support in FDB is there to support snapshot reads, and read-only transactions.</p>
<p><em>Edit: Bhaskar Muppana, one of the authors of the paper, pointed <a href="https://www.linkedin.com/feed/update/urn:li:activity:6823635315471863808?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A6823635315471863808%2C6826392929918160896%29">one mistake</a> I had in the following paragraph. It has been corrected. Thanks, Bhaskar.</em></p>
<p><s>The reason why <em>Proxy</em> is needed is that it allows reading uncommitted writes within a transaction, which is a very common semantic provided by almost all relational databases. It's achieved by buffering uncommitted writes locally on the Proxy servers, and merging these writes with storage data for reads on the same keys.</s> Client (notice not <em>Proxy</em>) caches uncommitted writes to support read-uncommitted-writes in the same transaction. This type of read-repair is only feasible for a simple k/v data model. Anything slightly more complicated, e.g. a graph data model, would introduce a significant amount of complexity. Caching is done on the client, so read queries can bypass the entire transaction system. Reads can be served either locally from client cache or from storage nodes.</p>
<p><em>Proxy</em> sits between clients and the transaction system. Its responsibility is to serve as a semi-stateful "client". E.g. it remembers the <em>KCV</em> to aid recovery; it performs batching (of requests from multiple clients) for reducing <em>Sequencer</em> qps.</p>
<p><em>Resolvers</em> are range-sharded, hence they can perform conflict checks in parallel. That means <em>Proxy</em> needs to wait to hear back from all <em>Resolvers</em> involved in each transaction. How does a <em>Resolver</em> get recently <em>committed</em> keys and their versions? It doesn't actually. Otherwise, it would require a distributed transaction between the <em>Log</em> and the <em>Resolver</em>. The workaround is that <em>Resolvers</em> keep a set of recent <strong>write-attempts</strong> and their versions. It's possible that a key appears to be recently written in a <em>Resolver</em> while in fact its transaction is rolled back, leading to false positives. FDB claims the false positives are manageable because:</p>
<ol>
<li>most workloads fall onto one <em>Resolve</em> (hence it would know if it needs to record the commit-attempt or not)</li>
<li>the MVCC window is 5 seconds. Waiting it out is not the end of the world, for conflicts are only around a single key – partial unavailability.</li>
</ol>
<p>How about racing commits? E.g. A Resolver can first admit a transaction with read-version <code>Vr</code> and commit-version <code>Vc</code>. Later, another transaction with commit-version <code>Vr &lt; Vc2 &lt; Vc</code> can hit the Resolver. In this case, the log servers will make sure to accept Vc2 first before accepting Vc, preserving linearizability. All messages in all log servers are sorted in LSN (See 2.4.2, "Similarly, StorageServers pull log data from LogServers in increasing LSNs as well.").</p>
<p><em>Resolver</em> doesn't need locks to detect conflicts. That's why FDB claims itself to be "lock-free". I found this mention of "lock-free" misleading. In order to realize strict serializability, locks must be used <em>somewhere</em> in the system to implement this total ordering. The <em>Sequencer</em> might have locks inside (not mentioned in the paper). Or even if there's only a single IO thread in the <em>Sequencer</em>, the kernel must use locks to maintain a queue of network packets. Even atomic memory operations are essentially implemented by locks in hardware. Without locks, ordering <em>can't</em> be achieved.</p>
<p>I know you are probably thinking about what if <em>Sequencer</em> or <em>Resolver</em> goes down. We will get there.</p>
<h2 id="loggingprotocol">Logging Protocol</h2>
<p><img src="https://blog.the-pans.com/content/images/2021/07/Screen-Shot-2021-07-16-at-11.57.52-AM.png" alt="Screen-Shot-2021-07-16-at-11.57.52-AM" loading="lazy"></p>
<p>FDB's logging protocol, at first glance, seems pretty straightforward. It has a group of logs, and a separate group of storage servers with distinct replication factors and topologies – another example of physical decoupling in FDB. If you dive a little deeper, you will notice that things are much more interesting.</p>
<p>There are multiple <em>LogServers</em>. Remember, FDB provides an abstraction of one logical shard (non-sharded) database. Commonly, people use logs for sequencing (e.g. Raft), in which case, each log is mapped to a logical shard. So how does FDB provide a single shard abstraction with multiple logs? The answer is <strong>FDB decouples <em>sequencing</em> from <em>logging</em></strong>. There is only one <em>Sequencer</em> in the system, which maps to the single shard abstraction (ordering). It enables FDB to have multiple logs, as they do <em>not</em> perform the job of ordering, which logs in other systems usually do. This design is just brilliant. Whoever performs the job of sequencing must be singular. You will get higher throughput and lower latency by having a dedicated role just doing sequencing, and <em>nothing</em> else. In contrast, systems like Raft couple sequencing and logging, which inherently pay a performance cost. In FDB, the <em>Sequencer</em> is physically a singleton – there's a single process. We will talk about failure recovery very soon. Here's what the logging protocol looks like,</p>
<ul>
<li><em>Proxy</em> broadcasts a message to <strong>all</strong> <em>LogServers</em>. Each message will have a common header including <code>LSN</code>, <code>prevLSN</code> and <code>KCV</code>. <code>KCV</code> is the last known committed version from the <em>Proxy</em>. It should always be less than the <code>LSN</code> (<code>KCV &lt; LSN</code>), as the transaction associated with <code>LSN</code> is still in progress (hence not committed). <code>KCV</code> captures an incomplete local view of the world, which will be used later for the recovery process.</li>
<li>Not all broadcast messages include data. E.g. key <code>A</code> is stored on storage shard <code>S</code>. <em>Coordinator</em> maintains an affinity map (each storage server has one preferred LogServer). In this case, data will only be included in the message sent to shard <code>S</code>'s preferred LogServer (and potentially additional LogServers to satisfy replication requirements). All other messages to other LogServers will only include the common header.</li>
<li>Once the data is in the log, it will apply to the storage node. It's not super interesting, although there are some performance improvements mentioned in the paper.</li>
</ul>
<p>There are a few interesting details above. <em>Proxy</em> broadcasts a message to "all" logs. Why all logs instead of only the replicas for the key? Because all messages are applied in order on each log. There can be no gaps. Also, as you will see later, that it's critical for the recovery process. Another related question is why sending messages without data to other logs? This is a performance optimization, as only logs associated with the underlying storage nodes need to have data stored.</p>
<p>Because FDB decouples sequencing from logging, now committing to the logs becomes a distributed transaction problem – e.g. what if log-1 has some data stored, while log-2 doesn't, for an aborted transaction. Does FDB need to perform 2pc? No, actually.</p>
<p>FDB here basically adopts the <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiEw8fzoPLxAhX6KFkFHU1eB14QFjAAegQIBRAD&amp;url=http%3A%2F%2Fcs.yale.edu%2Fhomes%2Fthomson%2Fpublications%2Fcalvin-sigmod12.pdf&amp;usg=AOvVaw0XE8ck-WBihvUiGgAnzr2H">Calvin</a> approach. It's the same idea that Prof. Abadi <a href="https://dbmsmusings.blogspot.com/2019/01/its-time-to-move-on-from-two-phase.html">posted</a>. The keyword is <em>deterministic</em>. We don't need 2pc here, because <em>Proxy</em> writing to multiple logs is performed <em>blindly</em> (a.k.a unconditionally). Sure, errors can happen. If they are transient e.g. timeout, <em>Proxy</em> can always try again – it will never be blocked by another log commit. This is, by the way, why FDB can afford letting <em>Proxies</em> broadcast each message to <em>all</em> logs.</p>
<p>In this case, the only failure scenario that can actually block log commit and require rollback, is when <em>LogServers</em> actually fail (e.g. crash). We will talk about the recovery process soon.</p>
<p>Notice that in the diagram above, there are 5 logs and 10 storage nodes (and one <em>Sequencer</em> not in the diagram). <code>#StorageNode &gt; #Logs &gt; #Sequencer</code> is not a coincidence. A single <em>Sequencer</em> can support multiple logs, as a <em>Sequencer</em> is much more lightweight (not having to write to disk). A single log again can support multiple storage nodes, as logs are relatively cheaper than storage (you can prune logs, sequential writes are cheaper, etc.).</p>
<h2 id="recoveryprocess">Recovery Process</h2>
<p>Finally, the real fun begins. What happens if the <em>Sequencer</em> fails? <em>Resolvers</em> fail? <em>LogServers</em> fail?</p>
<ol>
<li>Failures are detected by the <em>ClusterController</em> (via heartbeats according to the <a href="https://github.com/apple/foundationdb/blob/master/design/recovery-internals.md">source</a>), who explicitly triggers the recovery process.</li>
<li>The <em>Sequencer</em> locks and reads the previous states from <em>Coordinators</em>. It can be a new <em>Sequencer</em> instance, running the recovery process, if the previous <em>Sequencer</em> is in a faulty state. That's why we need to lock the <em>Coordinator</em> here, to avoid multiple <em>Sequencers</em> making topology changes at the same time. The paper doesn't talk about what happens if the recovery Sequencer itself crashes, leaving the Coordinators locked forever. This is a classic consensus problem. Given the Coordinators are already running Paxos, this is not a hard problem to solve. E.g. You can think of each recovery itself being a Paxos Proposal. If a recovery Sequencer died halfway, another Sequencer can always come and discover the previous chosen value if any, or propose a new epoch – starting a new recovery process.</li>
<li>Previous states include information about all old <em>LogServers</em>. The <em>Sequencer</em> then stops old logs from accepting new transactions. The <em>Sequencer</em> knowing it's running a recovery process, obviously won't accept new transactions. But if there's another old <em>Sequencer</em> instance running, it can still accept new transactions. Hence we need to stop the old logs to avoid potential dataloss. Another reason for contacting the old <em>LogServers</em> is to discover the end of the redo log (the last committed LSN).</li>
<li>The <em>Sequencer</em> recruits a new set of <em>Proxies</em>, <em>Resolvers</em>, and <em>LogServers</em>.</li>
<li>Write the new system states to the <em>Coordinators</em>, and release the lock.</li>
<li>Finally, the new <em>Sequencer</em> starts accepting new transactions.</li>
</ol>
<p>Unlike other fault tolerance algorithms, e.g. Paxos, <strong>FDB's recovery process has <em>downtime</em></strong> – from the shutdown of old <em>LogServers</em> to when the new <em>Sequencer</em> starts accepting new transactions. One can argue that this is not really <em>fault tolerant</em>. I personally think it's totally fine to have partial downtime to heal a system (given a short one, which is the case for FDB), e.g. unavailability for a single shard. It's not like systems such as Paxos or Raft have 100% availability even when the simple majority of hosts are always up and running. However FDB has only <em>one</em> logical shard. Its recovery process causes downtime for the entire deployment and the entire key space.</p>
<p><img src="https://blog.the-pans.com/content/images/2021/07/Screen-Shot-2021-07-19-at-11.33.17-AM.png" alt="Screen-Shot-2021-07-19-at-11.33.17-AM" loading="lazy"></p>
<p>P50 downtime for FDB's recovery process is 3.08s. That means no one can write to FDB at all during this window. The paper argues clients can still read from storage nodes unaffected.</p>
<blockquote>
<p>During the recovery, read-write transactions were temporarily blocked and were retried after timeout. However, client reads were not impacted, because they are served by StorageServers.</p>
</blockquote>
<p>This statement seems not completely accurate. Snapshot reads are unaffected. But if <em>Sequencer</em> is down, no read-only transactions can be processed either. FDB is read-heavy. It's very reasonable to assume that most of the production read workloads are snapshot reads (instead of read-only transactions). Claiming minimum client visible impact is reasonable.</p>
<p>If a system allows downtime, it can tolerate a lot of replicas going down without affecting it's availability. E.g. With the simplest leader-follower setup, a single leader can have any number of followers (let's say 10). In this case, with a total replication factor of 11, in theory, we can tolerate up to 10 failures without a single second of downtime.</p>
<p><em>Proxies</em> are easy to recover, as they are mostly stateless (just keeping track of a <code>KCV</code> locally). <em>Resolvers</em> are stateful, but it's safe to discard all <em>Resolvers</em> data from the previous epoch. Because it's impossible to have conflicts spanning across two epochs, as you can't have read-write transactions that span two epochs (as a completely new set of Proxies are recruited). That leaves us with the logs and the <em>Sequencer</em>, which are harder to recover. We just need to know the last committed LSN from the old logs to start a new <em>Sequencer</em> instance. Let's take a closer look at the recovery process of the <em>LogServers</em>.</p>
<h4 id="logrecovery">Log Recovery</h4>
<p><img src="https://blog.the-pans.com/content/images/2021/07/Screen-Shot-2021-07-20-at-3.15.38-PM.png" alt="Screen-Shot-2021-07-20-at-3.15.38-PM" loading="lazy"></p>
<p>For a LogServer deployment with <em>m</em> hosts and replication factor <em>k</em>, here's how the algorithm looks like:</p>
<ul>
<li>Each log maintains <code>DV</code> (Durable Version, the maximum persisted LSN) and <code>KCV</code> (Known last committed version, the latest committed version known from the <em>Proxy</em>). Both <em>DV</em> and <em>KCV</em> are piggy-backed on each message from Proxy to the logs.</li>
<li>The <em>Sequencer</em> attempts to stop <em>all</em> old logs.</li>
<li>After hearing back from <em>m-k+1</em> logs, the <em>Sequencer</em> knows the previous epoch has committed transactions <strong>at least</strong> up to <code>max(KCVs)</code>. We call <code>max(KCVs)</code> here <code>PEV</code> (Previous Epoch's end Version). The new <em>Sequencer</em> will start its epoch from <code>PEV+1</code>.</li>
</ul>
<p>It seems pretty straightforward. However, the paper mentions another variable <code>RV</code> (Recovery Version) to be <code>min(DVs)</code>, and it talks about copying data between <code>[PEV+1, RV]</code> from the old logs to new logs and discarding logs after <em>RV</em>. This is for,</p>
<ol>
<li>providing failure atomicity when log commit fails partially (in which case, we need to perform rollback)</li>
<li>preserving (healing replication factor) for data that are committed at LSN &gt; PEV</li>
</ol>
<h4 id="proofofcorrectness">Proof of Correctness</h4>
<p><em>The paper didn't include any proof for the correctness of the recovery process. I will explain and prove its correctness here.</em></p>
<p>Notice that any log message arrived at the logs have already passed the conflict detection, hence safe to be durably stored. Especially given the fact that data stored might have already been read by another client, we can and should err on the side of preserving more of the old logs except when it's not safe to do so. We know for sure that all transactions before <em>PEV</em> are committed. We just need to find the <em>tail</em> of the redo log, where</p>
<ol>
<li>partial failures might be happening, for which we must rollback, and</li>
<li>no clients have ever seen the data yet (because it's the tail)</li>
</ol>
<p>Let's take a closer look at this diagram.</p>
<p><img src="https://blog.the-pans.com/content/images/2021/07/Screen-Shot-2021-07-20-at-11.16.41-PM.png" alt="Screen-Shot-2021-07-20-at-11.16.41-PM" loading="lazy"></p>
<p>For it to be correct (that we can safely discard data after <em>RV</em> and preserve data before <em>RV</em>), the following conditions need to be met:</p>
<ol>
<li>DV<sub>m</sub>'s cross-range-shard transaction cannot be partially committed (no rollback required).</li>
<li>No clients can read any data written after DV<sub>m</sub>.</li>
</ol>
<p>Let's try proof by contradiction on #1 and #2.</p>
<p>What if DM<sub>m</sub>'s transaction were partially committed (specifically, rollback is required)? Notice that "partial" here specifically means cross-range-shard transaction, e.g. a transaction that sets both key A and key B, key A is stored with data in its logs, but key B is not. In this case, we have to rollback A. Logs can tolerate <em>k-1</em> failures (<em>k</em> being the data replication factor). If at least one surviving B replica has the data for transaction DM<sub>m</sub>, no rollback is required because we have all the data for T<sub>m</sub>, for which we will finish the commit process (heal the replication factor). Otherwise, let's say <em>k-1</em> B replicas that got DM<sub>m</sub> data crashed, there must be at least one B replica up and running that has not received the DM<sub>m</sub> message. On that log, its <em>DV</em> must be smaller than DM<sub>m</sub>. Contradiction.</p>
<p>Now let's move on to #2. Notice that <code>DV &gt;= KCV</code> is true locally for each log, because <code>LSN &gt; KCV</code> is true on each <em>Proxy</em>. Log local <em>DV</em> can sometimes be the same as <em>KCV</em> because reads from the <em>Proxies</em> don't set <em>DV</em> but can bring log local <em>KCV</em> up to date. Even better, <code>max(KCVs) &lt;= min(DVs)</code> should hold always, thanks to the fact that each log commit requires broadcasting the message to <em>all</em> logs. This answers our question earlier why Proxies need to broadcast each log message to <em>all</em> logs. Now it's obvious why #2 holds, because FDB can only serve reads based on <em>KCV</em> (and not <em>DV</em>). So for any <em>DV &gt; DV<sub>m</sub></em>, those transactions must not be committed.</p>
<p>With #1 and #2 combined, we can be confident that we can safely treat <em>RV</em> as the tail of the redo log. The presence of a <em>KCV</em> on a log, implies that the associated transaction is fully replicated (having <em>k</em> copies). It's not covered in the paper, but conceptually we can think of these messages having been durably flushed to storage nodes (safe to discard from the logs). For messages between <code>[PEV+1, RV]</code>, they <em>might</em> be committed transactions. It's safe to err on the side of treating them as committed, as all writes to the logs have already passed conflict detection. These cases would be indistinguishable from reply timeouts. But due to the lack of information, we don't know for sure if messages in the <code>[PEV+1, RV]</code> window have been durably stored in storage nodes or not. The easiest way to handle it is to start the new epoch at <em>PEV+1</em> and replay messages from <code>[PEV+1, RV]</code> from the old logs.</p>
<h2 id="replicationsummary">Replication Summary</h2>
<p>Given the extreme decoupling, each component has its own replication strategy.</p>
<ul>
<li><em>Coordinator</em> runs on Paxos, which can tolerate <em>f</em> failures with a cluster size of <em>2f+1</em>.</li>
<li><em>LogServers</em> can tolerate <em>f</em> failures for replication factor <em>f+1</em>, which is orthogonal to the number of servers. Storage nodes share a similar strategy. E.g. there can be a total of 5 logs, with a replication factor of 3, which can tolerate 2 failures.</li>
</ul>
<h2 id="thoseclaimsatthebeginning">Those Claims at the Beginning</h2>
<p>I think FDB is a great system, probably the best k/v store. However, I do think those bold claims at the beginning of the paper are, frankly, misleading.</p>
<ul>
<li>"FDB provides strict serializability while being lock-free". Well, to be more precise, it's meant to say the conflict detection is lock-free. <em>Sequencer</em> is not lock-free for example.</li>
<li>"For global FDB deployment, it can avoid x-region write latency." Well, it's based on semi-sync in a local region in a different failure domain. It requires assumptions about failure domains.</li>
<li>"FDB can tolerate <em>f</em> failures with only <em>f+1</em> replicas." Well, this only applies to logs and storage nodes. If simply majority of the <em>Coordinator</em> fail, FDB is totally hosed for example.</li>
</ul>
<h2 id="summary">Summary</h2>
<p>FDB is probably the best k/v store for regional deployment out there. It's rock solid thanks to its simulation framework. The FDB team, in my opinion, made all the best design choices, which is just remarkable. The paper is dense. A lot of important details are only briefly mentioned or even omitted, which is probably due to the paper length limit. Congratulations to the FoundationDB team.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>You are still bound by the underlying scaling bottleneck of FDB's singletons, mostly the Sequencer. <a href="#fnref1">↩︎</a></p>
</li>
</ol>
</section>

</section>
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Introduction to Immutable Linux Systems (320 pts)]]></title>
            <link>https://dataswamp.org/~solene/2023-07-12-intro-to-immutable-os.html</link>
            <guid>37551474</guid>
            <pubDate>Mon, 18 Sep 2023 02:19:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dataswamp.org/~solene/2023-07-12-intro-to-immutable-os.html">https://dataswamp.org/~solene/2023-07-12-intro-to-immutable-os.html</a>, See on <a href="https://news.ycombinator.com/item?id=37551474">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article id="20230712">
  <header>
  
    
    <p>Written by <em>Solène</em>, on 12 July 2023.<br>Tags: 
<span><a href="https://dataswamp.org/~solene/tag-immutability.html">#immutability</a></span>


<span><a href="https://dataswamp.org/~solene/tag-linux.html">#linux</a></span>

</p>
    
    
  </header>
  <h2 id="_Introduction">1. Introduction <a href="#_Introduction">§</a></h2>
<p>If you reach this page, you may be interested into this new category of Linux distributions labeled "immutable".
</p>
<p>In this category, one can find by age (oldest → youngest) NixOS, Guix, Endless OS, Fedora Silverblue, OpenSUSE MicroOS, Vanilla OS and many new to come.
</p>
<p>I will give examples of immutability implementation, then detail my thoughts about immutability, and why I think this naming can be misleading.  I spent a few months running all of those distributions on my main computers (NAS, Gaming, laptop, workstation) to be able to write this text.
</p>
<h2 id="_What's_immutability?">2. What's immutability? <a href="#_What's_immutability?">§</a></h2>
<p>The word immutability itself refers to an object that can't change.
</p>
<p>However, when it comes to an immutable operating system, the definition immediately become vague.  What would be an operating system that can't change?  What would you be supposed to do with it?
</p>
<p>We could say that a Linux LIVE-CD is immutable, because every time you boot it, you get the exact same programs running, and you can't change anything as the disk media is read only.  But while the LIVE-CD is running, you can make changes to it, you can create files and directories, install packages, it's not stuck in an immutable state.
</p>
<p>Unfortunately, this example was nice but the immutability approach by those Linux distribution is totally different, so we need to think a bit further.
</p>
<p>There are three common principles in these systems:
</p>
<ul>

  <li>system upgrades aren't done on the live system</li>
  <li>packages changes are applied on the next boot</li>
  <li>you can roll back a change</li>
</ul>

<p>Depending on the implementation, a system may offer more features.  But this list is what a Linux distribution should have to be labelled "immutable" at the moment.
</p>
<h2 id="_Immutable_systems_comparison">3. Immutable systems comparison <a href="#_Immutable_systems_comparison">§</a></h2>
<p>Now we found what are the minimum requirements to be called immutable, let's go through each implementation, by their order of appearance.
</p>
<h2 id="_NixOS_/_Guix">3.1. NixOS / Guix <a href="#_NixOS_/_Guix">§</a></h2>
<p>In this section, I'm mixing NixOS and Guix as they both rely on the same implementation.  NixOS is based on Nix (first appearance in 2003), which has been forked into early 2010s into the Guix package manager to be 100% libre, which gave birth to an eponym operating system also 100% free.
</p>
<p><a href="https://nixos.org/">NixOS official project website</a></p>
<p><a href="https://guix.gnu.org/">Guix official project website</a></p>
<p><a href="https://jonathanlorimer.dev/posts/nix-thesis.html">Jonathan Lorimer's blog post explaining Eelco Dolstra's thesis about Nix</a></p>
<p>These two systems are really different than a traditional Unix like system we are used to, and immutability is a main principle.  To make it quick, they are based on their package manager (being Nix or Guix) that contains every package or built file into a special read-only directory (where only the package manager can write) where each package has its own unique entry, and the operating system itself is a byproduct of the package manager.
</p>
<p>What does that imply?  If the operating system is built, this is because it's made of source code, you literally describe what you want your system to be in a declarative way.  You have to list users, their shells, installed packages, running services and their configurations, partitions to mount with which options etc... Fortunately, it's made a lot easier by the use of modules which provide sane defaults, so if you create a user, you don't have to specify its UID, GID, shell, home etc...
</p>
<p>So, as the system is built and stored in the special read-only directory, all your system is derived from that (using symbolic links), so all the files handled by the package manager are read-only.  A concrete example is that /etc/fstab or /bin/sh ARE read-only, if you want to make a change in those, you have to do it through the package manager.
</p>
<p>I'm not going into details, because this store based package manager is really different than everything else but:
</p>
<ul>

  <li>you can switch between two configurations on the fly as it's just a symlink dance to go from a configuration to another</li>
  <li>you can select your configuration at boot time, so you can roll back to a previous version if something is wrong</li>
  <li>you can't make change to a package file or system file as they are read only</li>
  <li>the mount points except the special store directory are all mutable, so you can write changes in /home or /etc or /var etc... You can remove the system symlinks by a modified version, but you can't modify the symlink source itself.</li>
</ul>

<p>This is the immutability as seen through the Nix lens.
</p>
<p>I've spent a few years running NixOS systems, this is really a blast for me, and the best "immutable" implementation around, but unfortunately it's too different, so its adoption rate is very low, despite all the benefits.
</p>
<p><a href="https://discourse.nixos.org/t/my-issues-when-pushing-nixos-to-companies/28629/1">NixOS forum: My issues when pushing NixOS to companies</a></p>
<h2 id="_Endless_OS">3.2. Endless OS <a href="#_Endless_OS">§</a></h2>
<p>While this one is not the oldest immutable OS around, it's the first one to be released for the average user, while NixOS and Guix are older but for a niche user category.  The company behind Endless OS is trying to offer a solid and reliable system, free and open source, that can works without Internet, to be used in countries with a low Internet / powergrid coverage.  They even provide a version with "offline internet included" containing Wikipedia dumps, class lessons and many things to make a computer useful while offline (I love their work).
</p>
<p><a href="https://www.endlessos.org/">Endless OS official project website</a></p>
<p>Endless OS is based on Debian, but uses the OSTree tool to make it immutable.  OSTree allows you to manage a core system image, and add layers on top of it, think of packages as layers.  But it can also prepare a new system image for the next boot.
</p>
<p>With OSTree, you can apply package changes in a new version of the system that will be available at next boot, and revert to a previous version at boot time.
</p>
<p>The partitions are mounted writable, except for <code>/usr</code>, the land of packages handled by OSTree, which is mounted read-only.  There are no rollbacks possible for <code>/etc</code>.
</p>
<p>Programs meant to be for the user (not the packages to be used by the system like grub, X display or drivers) are installed from Flatpak (which also uses OSTree, but unrelated to the system), this avoids the need to reboot each time you install a new package.
</p>
<p>My experience with Endless OS is mixed, it is an excellent and solid operating system, it's working well, never failed, but I'm just not the target audience.  They provide a modified GNOME desktop that looks like a smartphone menu, because this is what most non-tech users are comfortable with (but I hate it).  And installing DevOps tools isn't practical but not impossible, so I keep Endless OS for my multimedia netbook and I really enjoy it.
</p>
<h2 id="_Fedora_Silverblue">3.3. Fedora Silverblue <a href="#_Fedora_Silverblue">§</a></h2>
<p>This linux distribution is the long descendant of Project Atomic, an old initiative to make Fedora / CentOS/ RHEL immutable.  It's now part of the Fedora releases along with Fedora Workstation.
</p>
<p><a href="https://projectatomic.io/">Project Atomic website</a></p>
<p><a href="https://fedoraproject.org/silverblue/">Fedora Silverblue project website</a></p>
<p>Fedora Silverblue is also using OSTree, but with a twist.  It's using rpm-OSTree, a tool built on top of OSTree to let your RPM packages apply the changes through OSTree.
</p>
<p>The system consists of a single core image for the release, let's say fedora-38, and for each package installed, a new layer is added on top of the core.  At anytime, you can list all the layers to know what packages have been installed on top of the core, if you remove a package, the whole stack is generated again (which is terribly SLOW) without the package, there is absolutely no leftover after a package removal.
</p>
<p>On boot, you can choose an older version of the system, in case something broke after an upgrade.  If you install a package, you need to reboot to have it available as the change isn't applied on the current booted system, however rpm-OSTree received a nice upgrade, you can temporarily merge the changes of the next boot into the live system (using a tmpfs overlay) to use the changes.
</p>
<p>The mountpount management is a bit different, everything is read-only except <code>/etc/</code>, <code>/root</code> and <code>/var</code>, but your home directory is by default in <code>/var/home</code> which sometimes breaks expectations.  There are no rollbacks possible for <code>/etc</code>.
</p>
<p>As installing a new package is slow due to rpm-OSTree and requires a reboot to be fully usable (the live change back port store the extra changes in memory), they recommend to use Flatpak for programs, or <code>toolbox</code>, some kind of wrapper that create a rootless fedora container where you can install packages and use it in your terminal.  toolbox is meant to provide development libraries or tool you wouldn't have in Flatpak, but that you wouldn't want to install in your base Fedora system.
</p>
<p><a href="https://docs.fedoraproject.org/en-US/fedora-silverblue/toolbox/">toolbox website</a></p>
<p>My experience with Fedora Silverblue has been quite good, it's stable, the updates are smooth even if they are slow.  <code>toolbox</code> was working fine despite I don't find this practical.
</p>
<h2 id="_OpenSUSE_MicroOS">3.4. OpenSUSE MicroOS <a href="#_OpenSUSE_MicroOS">§</a></h2>
<p>This spin of OpenSUSE Tumbleweed (rolling-release OpenSUSE) features immutability, but with its own implementation.  The idea of MicroOS is really simple, the whole system except a few directories like <code>/home</code> or <code>/var</code> lives on a btrfs snapshot, if you want to make a change to the system, the current snapshot is forked into a new snapshot, and the changes are applied there, ready for the next boot.
</p>
<p><a href="https://microos.opensuse.org/">OpenSUSE MicroOS official project website</a></p>
<p>What's interesting here is that <code>/etc</code> IS part of the snapshots, and can be roll backed, which wasn't possible in the OSTree based systems.  It's also possible to make changes to any file of the file system (in a new snapshot, not the live one) using a shell, which can be very practical for injecting files to solve a driver issue.  The downside it's not guaranteed that your system is "pure" if you start making changes, because they won't be tracked, the snapshots are just numbered, and you don't know what changes were made in each of them.
</p>
<p>Changes must be done through the command <code>transactional-update</code> which do all the snapshot work for you, and you could either manipulate package by adding/removing a package, or just start a shell in the new snapshot to make all the changes you want.  I said <code>/etc</code> is part of the snapshots, it's true, but it's never read-only, so you could make a change live in <code>/etc</code>, then create a new snapshot, the change would be immediately inherited.  This can create troubles if you roll back to a previous state after an upgrade if you also made changes to <code>/etc</code> just before.
</p>
<p>The default approach of MicroOS is disturbing at first, a reboot is planned every day after a system update, this is because it's a rolling-release system and there are updates every day, and you won't benefit from them until you reboot.  While you can disable this automatic reboot, it makes sense to use the newest packages anyway, so it's something to consider if you plan to use MicroOS.
</p>
<p>There is currently no way to apply the changes into the live system (like Silverblue is offering), it's still experimental, but I'm confident this will be doable soon.  As such, it's recommended to use <code>distrobox</code> to use rootless containers of various distributions to install your favorite tools for your users, instead of using the base system packages.  I don't really like this because this adds maintenance, and I often had issues of distrobox refusing to start a container after a reboot, I had to destroy and recreate it entirely to solve.
</p>
<p><a href="https://github.com/89luca89/distrobox">distrobox GitHub project page</a></p>
<p>My experience with OpenSUSE MicroOS has been wonderful, it's in dual-boot with OpenBSD on my main laptop, it's my Linux Gaming OS, and it's also my NAS operating system, so I don't have to care about updates.  I like that the snapshots system doesn't restrict me, while OSTree systems just doesn't allow you to make changes without installing a package.
</p>
<h2 id="_Vanilla_OS">3.5. Vanilla OS <a href="#_Vanilla_OS">§</a></h2>
<p>Finally, the really new (but mature enough to be usable) system in the immutable family is Vanilla OS based on Ubuntu (but soon on Debian), using ABroot for immutability.  With Vanilla OS, we have another implementation that really differs from what we saw above.
</p>
<p><a href="https://vanillaos.org/">Vanilla OS project website</a></p>
<p>ABroot named is well thought, the idea is to have a root partition A, another root partition B, and a partition for persistent data like <code>/home</code> or <code>/var</code>.
</p>
<p>Here is the boot dance done by ABroot:
</p>
<ul>

  <li>first boot is done on A, it's mounted in read-only</li>
  <li>changes to the system like new packages or file changes in <code>/etc</code> are done on B (and can be applied live using a tmpfs overlay)</li>
  <li>upon reboot, if previous boot was A, you boot on B, then if the boot is successful, ABroot scan for all the changes between A and B, and apply all the changes from B to A</li>
  <li>when you are using your system, until you make a change, A and B are always identical</li>
</ul>

<p>This implementation has downsides, you can only roll back a change until you boot on the new version, then the changes are also applied on the previous boot, and you can't roll back.  This implementation mostly protects you from a failing upgrade, or if you made changes and tried them live, but you prefer to rollback.
</p>
<p>Vanilla OS features the package manager apx, written by distrobox author.  That's for sure an interesting piece of software, allowing your non-root user to install packages from many distributions (arch linux, fedora, ubuntu, nix, etc...) and integrates them into the system as if they were installed locally.  I suppose it's some kind of layer on top of distrobox.
</p>
<p><a href="https://github.com/Vanilla-OS/apx">apx package manager GitHub project page</a></p>
<p>My experience wasn't very good, I didn't find ABroot to be really useful, and the version 22.10 I tried was using an old Ubuntu LTS release which didn't make my gaming computer really happy.  The overall state of Vanilla OS, ABroot and apx is that they are young, I think it can become a great distribution, but it still has some rough edges.
</p>
<h2 id="_Alpine_Linux_(with_LBU)">3.6. Alpine Linux (with LBU) <a href="#_Alpine_Linux_(with_LBU)">§</a></h2>
<p>I've been told that it was possible to achieve immutability on Alpine Linux using the "lbu" command.
</p>
<p><a href="https://wiki.alpinelinux.org/wiki/Alpine_local_backup">Alpine Linux wiki: Local backup</a></p>
<p>I don't want to go much into details, but here is the short version: you can use Alpine Linux installer as a base system to boot from, and create tarballs of "saved configurations" that are automatically applied upon boot (it's just tarred directories and some automation to install packages).  At every boot, everything is untarred again, and packages are installed again (you should use an apk cache directory), everything in live memory, fully writable.
</p>
<p>What does this achieve?  You always start from a clean state, changes are applied on top of it at every boot, you can roll back the changes and start fresh again.  Immutability as we defined above here isn't achieved because changes are applied on the base system, but it's quite close to fulfill (my own) requirements.
</p>
<p>I've been using it a few days only, not as my main system, and it requires a very good understanding of what you are doing because the system is fully in memory, and you need to take care about what you want to save/restore, which can create big archives.
</p>
<p>On top of that, it's poorly documented.
</p>
<h2 id="_Pros,_Cons_and_Facts">4. Pros, Cons and Facts <a href="#_Pros,_Cons_and_Facts">§</a></h2>
<p>Now I gave some details about all the major immutable systems (Linux based) around, I think it's time to list the real pros and cons I found from my experimentation.
</p>
<h2 id="_Pros">4.1. Pros <a href="#_Pros">§</a></h2>
<ul>

  <li>you can roll back changes if something went wrong.</li>
  <li>transactional-updates allows you to keep the system running correctly during packages changes.</li>
</ul>

<h2 id="_Cons">4.2. Cons <a href="#_Cons">§</a></h2>
<ul>

  <li>configuration management tool (ansible, salt, puppet etc..) integrate VERY badly, they received updates to know how to apply package changes, but you will mostly hit walls if you want to manage those like regular systems.</li>
  <li>having to reboot after a change is annoying (except for NixOS and Guix which don't require rebooting for each change).</li>
  <li>OSTree based systems aren't flexible, my netbook requires some extra files in alsa directories to get sound (fortunately Endless OS have them!), you just can't add the files without making a package deploying them.</li>
  <li>blind rollbacks, it's hard to figure what was done in each version of the system, so when you roll back it's hard to figure what you are doing exactly.</li>
  <li>it can be hard to install programs like Nix/Guix which require a directory at the root of the file system, or install non-packaged software system-wide (this is often bad practice, but sometimes a necessary evil).</li>
</ul>

<h2 id="_Facts">4.3. Facts <a href="#_Facts">§</a></h2>
<ul>

  <li>immutability is a lie, many parts of the systems are mutable, although I don't know how to describe this family with a different word (transactional something?).</li>
  <li>immutable doesn't imply stateless.</li>
  <li>NixOS / Guix are doing it right in my opinion, you can track your whole system through a reliable package manager, and you can use a version control system on the sources, it has the right philosophy from the ground up.</li>
  <li>immutability is often associated with security benefits, I don't understand why.  If someone obtains root access on your system, they can still manipulate the live system and have fun with the <code>/boot</code> partition, nothing prevent them to install a backdoor for the next boot.</li>
  <li>immutability requires discipline and maintenance, because you have to care about the versioning, you have extra programs like apx / distrobox / devbox that must be updated in parallel of the system (while this is all integrated into NixOS/Guix).</li>
</ul>

<h2 id="_Conclusion">5. Conclusion <a href="#_Conclusion">§</a></h2>
<p>Immutable operating systems are making the news in our small community of open source systems, but behind this word lies many implementations with different use cases.  The word immutable certainly creates expectations from users, but it's really nothing more than transactional updates for your operating system, and I'm happy we can have this feature now.
</p>
<p>But transactional updates aren't new, I think it started a while ago with Solaris and ZFS allowing you to select a system snapshot at boot time, then I'm quite sure FreeBSD implemented this a decade ago, and it turns out that on any linux distribution with regular btrfs snapshots you could select a snapshot at boot time.
</p>
<p><a href="https://dataswamp.org/~solene/2023-01-04-boot-on-btrfs-snapshot.html">Previous blog post about booting on a BTRFS snapshot without any special setup</a></p>
<p>In the end, what's REALLY new is the ability to apply a transactional change on a non-live environment, integrates this into the bootloader, and give the user the tooling to handle this easily.
</p>
<h2 id="_Going_further">6. Going further <a href="#_Going_further">§</a></h2>
<p>I recommend reading the blog post "“Immutable” → reprovisionable, anti-hysteresis" by Colin Walters.
</p>
<p><a href="https://blog.verbum.org/2020/08/22/immutable-%E2%86%92-reprovisionable-anti-hysteresis/">“Immutable” → reprovisionable, anti-hysteresis</a></p>

</article>
</div></div>]]></description>
        </item>
    </channel>
</rss>