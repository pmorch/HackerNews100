<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 02 Apr 2025 01:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[DEDA – Tracking Dots Extraction, Decoding and Anonymisation Toolkit (137 pts)]]></title>
            <link>https://github.com/dfd-tud/deda</link>
            <guid>43551397</guid>
            <pubDate>Tue, 01 Apr 2025 21:11:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/dfd-tud/deda">https://github.com/dfd-tud/deda</a>, See on <a href="https://news.ycombinator.com/item?id=43551397">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DEDA - tracking Dots Extraction, Decoding and Anonymisation toolkit</h2><a id="user-content-deda---tracking-dots-extraction-decoding-and-anonymisation-toolkit" aria-label="Permalink: DEDA - tracking Dots Extraction, Decoding and Anonymisation toolkit" href="#deda---tracking-dots-extraction-decoding-and-anonymisation-toolkit"></a></p>
<p dir="auto">Document Colour Tracking Dots, or yellow dots, are small systematic dots which encode information about the printer and/or the printout itself. This process is integrated in almost every commercial colour laser printer. This means that almost every printout contains coded information about the source device, such as the serial number.</p>
<p dir="auto">On the one hand, this tool gives the possibility to read out and decode these forensic features and on the other hand, it allows anonymisation to prevent arbitrary tracking.</p>
<p dir="auto">If you use this software, please cite the paper:
Timo Richter, Stephan Escher, Dagmar Schönfeld, and Thorsten Strufe. 2018. Forensic Analysis and Anonymisation of Printed Documents. In Proceedings of the 6th ACM Workshop on Information Hiding and Multimedia Security (IH&amp;MMSec '18). ACM, New York, NY, USA, 127-138. DOI: <a href="https://doi.org/10.1145/3206004.3206019" rel="nofollow">https://doi.org/10.1145/3206004.3206019</a></p>
<hr>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installation</h4><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<ul dir="auto">
<li>Install Python 3</li>
<li>Install Deda</li>
</ul>
<p dir="auto">From PyPI:
<code>$ pip3 install --user deda</code></p>
<p dir="auto">Or from current directory:
<code>$ pip3 install --user .</code></p>
<ul dir="auto">
<li>Optional requirement by deda_anonmask_apply (Unix and GNU/Linux only):
<code>$ pip3 install --user wand</code></li>
</ul>
<p dir="auto">Without Wand, pages containing white areas on images cannot be anonymised.</p>
<hr>
<p dir="auto"><h4 tabindex="-1" dir="auto">Graphical User Interface</h4><a id="user-content-graphical-user-interface" aria-label="Permalink: Graphical User Interface" href="#graphical-user-interface"></a></p>
<ul dir="auto">
<li>To open the GUI type:
<code>$ deda_gui</code></li>
</ul>
<hr>
<p dir="auto"><h4 tabindex="-1" dir="auto">Terminal Application</h4><a id="user-content-terminal-application" aria-label="Permalink: Terminal Application" href="#terminal-application"></a></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">1. Reading tracking data</h5><a id="user-content-1-reading-tracking-data" aria-label="Permalink: 1. Reading tracking data" href="#1-reading-tracking-data"></a></p>
<p dir="auto">Tracking data can be read and sometimes be decoded from a scanned image. For good results the input shall use a lossless compression (e.g. png) and 300 dpi. Make sure to set a neutral contrast
<code>$ deda_parse_print INPUTFILE</code></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">2. Find a divergent printer in a set of scanned documents</h5><a id="user-content-2-find-a-divergent-printer-in-a-set-of-scanned-documents" aria-label="Permalink: 2. Find a divergent printer in a set of scanned documents" href="#2-find-a-divergent-printer-in-a-set-of-scanned-documents"></a></p>
<p dir="auto"><code>$ deda_compare_prints INPUT1 INPUT2 [INPUT3] ...</code></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">3. Analysing an unknown tracking pattern</h5><a id="user-content-3-analysing-an-unknown-tracking-pattern" aria-label="Permalink: 3. Analysing an unknown tracking pattern" href="#3-analysing-an-unknown-tracking-pattern"></a></p>
<p dir="auto">New patterns might not be recognised by parse_print. The dots can be extracted
for further analysis.<br>
<code>$ deda_extract_yd INPUTFILE</code></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">4. Create your own tracking dots</h5><a id="user-content-4-create-your-own-tracking-dots" aria-label="Permalink: 4. Create your own tracking dots" href="#4-create-your-own-tracking-dots"></a></p>
<p dir="auto">If you want to create your own tracking dots matrix and add it to a pdf
document, pass the contents as parameters (see <code>deda_create_dots -h</code>).
<code>$ deda_create_dots PDFINPUT</code></p>
<p dir="auto">The calibration page (<code>$ deda_anonmask_create -w</code>) may be used as an input.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">5. Anonymise a scanned image</h5><a id="user-content-5-anonymise-a-scanned-image" aria-label="Permalink: 5. Anonymise a scanned image" href="#5-anonymise-a-scanned-image"></a></p>
<p dir="auto">This (mostly) removes tracking data from a scan:<br>
<code>$ deda_clean_document INPUTFILE OUTPUTFILE</code></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">6. Anonymise a document for printing</h5><a id="user-content-6-anonymise-a-document-for-printing" aria-label="Permalink: 6. Anonymise a document for printing" href="#6-anonymise-a-document-for-printing"></a></p>
<ul dir="auto">
<li>
<p dir="auto">Save your document as a PDF file and call it DOCUMENT.PDF.</p>
</li>
<li>
<p dir="auto">Print the testpage.pdf file created by<br>
<code>$ deda_anonmask_create -w</code><br>
without any page margin.</p>
</li>
<li>
<p dir="auto">Scan the document (300 dpi) and pass the lossless file to<br>
<code>$ deda_anonmask_create -r INPUTFILE</code><br>
This creates 'mask.json', the individual printer's anonymisation mask.</p>
</li>
<li>
<p dir="auto">Now apply the anonymisation mask:<br>
<code>$ deda_anonmask_apply mask.json DOCUMENT.PDF</code>
This creates 'masked.pdf', the anonymised document. It may be printed with a
zero page margin setting.</p>
</li>
</ul>
<p dir="auto">Check whether a masked page covers your printer's tracking dots by using a
microscope. The mask's dot radius, x and y offsets can be customised and
passed to <code>deda_anonmask_apply</code> as parameters.</p>
<p dir="auto">Note that if DOCUMENT.PDF contains graphics with white or light coloured parts, these can only be masked if "wand" is installed (see above).</p>
<hr>
<p dir="auto"><h4 tabindex="-1" dir="auto">Troubleshooting</h4><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">deda_parse_print: command not found</h5><a id="user-content-deda_parse_print-command-not-found" aria-label="Permalink: deda_parse_print: command not found" href="#deda_parse_print-command-not-found"></a></p>
<p dir="auto">Possible solutions:</p>
<ul dir="auto">
<li>Install deda accordig to chapter 0</li>
<li>Execute
<code>$ export PATH="$PATH:$(python -c 'import site,os; print(os.path.join(site.USER_BASE, "bin"))')"</code></li>
</ul>
<p dir="auto"><h5 tabindex="-1" dir="auto">Deda does not recognise my tracking dots</h5><a id="user-content-deda-does-not-recognise-my-tracking-dots" aria-label="Permalink: Deda does not recognise my tracking dots" href="#deda-does-not-recognise-my-tracking-dots"></a></p>
<p dir="auto">Set up your scan program so that it does not eliminate the paper structure nor tracking dots by some threshold and check again. Remember that monochrome pages as well as inkjet prints might not contain tracking dots.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">My printer does not print tracking dots. Can I hide this fact?</h5><a id="user-content-my-printer-does-not-print-tracking-dots-can-i-hide-this-fact" aria-label="Permalink: My printer does not print tracking dots. Can I hide this fact?" href="#my-printer-does-not-print-tracking-dots-can-i-hide-this-fact"></a></p>
<p dir="auto">If there are really no tracking dots, you can either create your own ones (<code>deda_create_dots</code>) or print the calibration page (<code>deda_anonmask_create -w</code>) with another printer and use the mask for your own printer. You can use the anonymised version of the tracking dots or just copy them (<code>deda_anonmask_create --copy</code>). See chapters "Anonymise a document for printing" and "Create your own tracking dots".</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Install Error: command 'x86_64-linux-gnu-gcc' failed with exit status 1</h5><a id="user-content-install-error-command-x86_64-linux-gnu-gcc-failed-with-exit-status-1" aria-label="Permalink: Install Error: command 'x86_64-linux-gnu-gcc' failed with exit status 1" href="#install-error-command-x86_64-linux-gnu-gcc-failed-with-exit-status-1"></a></p>
<p dir="auto">This may be caused by the eel dependency which is needed for the GUI. Try
<code>$ sudo apt-get install build-essential autoconf libtool pkg-config python3.6-dev gcc &amp;&amp; pip3 install --user eel</code></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">wand.exceptions.PolicyError: attempt to perform an operation not allowed by the security policy PDF' @ error/constitute.c/IsCoderAuthorized/408</h5><a id="user-content-wandexceptionspolicyerror-attempt-to-perform-an-operation-not-allowed-by-the-security-policy-pdf--errorconstituteciscoderauthorized408" aria-label="Permalink: wand.exceptions.PolicyError: attempt to perform an operation not allowed by the security policy PDF' @ error/constitute.c/IsCoderAuthorized/408" href="#wandexceptionspolicyerror-attempt-to-perform-an-operation-not-allowed-by-the-security-policy-pdf--errorconstituteciscoderauthorized408"></a></p>
<p dir="auto">This is being caused by ImageMagick. Either remove Wand (<code>pip3 uninstall wand</code>) or add <code>&lt;policy domain="coder" rights="read | write" pattern="PDF" /&gt;</code> just before <code>&lt;/policymap&gt;</code> in /etc/ImageMagick-*/policy.xml. See also <a href="https://stackoverflow.com/questions/52998331/imagemagick-security-policy-pdf-blocking-conversion" rel="nofollow">https://stackoverflow.com/questions/52998331/imagemagick-security-policy-pdf-blocking-conversion</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dave Täht has died (159 pts)]]></title>
            <link>https://libreqos.io/2025/04/01/in-loving-memory-of-dave/</link>
            <guid>43550098</guid>
            <pubDate>Tue, 01 Apr 2025 18:40:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://libreqos.io/2025/04/01/in-loving-memory-of-dave/">https://libreqos.io/2025/04/01/in-loving-memory-of-dave/</a>, See on <a href="https://news.ycombinator.com/item?id=43550098">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="21df1089" data-element_type="widget" data-widget_type="text-editor.default">
									<p>04/01/2025</p><p>We are devastated to report that Dave Täht has passed away.</p><p>Dave was an amazing man, helping the world with FQ-CoDel and CAKE, fighting bufferbloat and trying to make the world a better place. Always willing to help, and without him – LibreQoS (and the other QoE solutions out there) wouldn’t exist.</p><p>Dave was an inspiration, and we all miss him. We’re reaching out to family and close friends to see if there’s anything we can do to help.</p><p>Dave was an inspiration to us. Dave’s contributions to Linux, FQ-CoDel, and CAKE improved internet connectivity around the world for millions of people. Because of him, millions of people now have access to reliable video calls – and in turn, access to loved ones, healthcare, and community. One of Robert’s ISP customers is a kind paraplegic woman who lives in a far-flung rural Colonia near El Paso, Texas. Her reliable access to her doctors through telemedicine, and to her family through FaceTime, was only made possible because of his algorithms. There are millions of cases like hers, where Dave’s contributions have silently enabled human connection and safety. Everything Dave contributed to the world of technology was free and open source, for the betterment of humanity.</p><p>Dave is the reason that Starlink was able to tackle its latency issues – enabling a generation of young entrepreneurs across the developing world, such as these young folks pictured in the Phillipines, to start their own ISPs to expand internet access to their communities. Dave started work on FQ-CoDel in part because of his own journey working to expand internet access in Nicaragua, so we know he saw that his work had come full-circle and helped so many.</p><p>We’re incredibly grateful to have Dave as our friend, mentor, and as someone who continuously inspired us – showing us that we could do better for each other in the world, and leverage technology to make that happen. He will be dearly missed.</p><p><strong>PS:</strong> Dave is forever in our hearts and souls, in our routers and… in production. <strong><a href="https://github.com/LibreQoE/LibreQoS/pull/684" rel="nofollow">https://github.com/LibreQoE/LibreQoS/pull/684</a> </strong>We will miss you so much, Dave.</p><p>– Robert, Herbert, and Frank with LibreQoS</p>								</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Myst Graph: A New Perspective on Myst (149 pts)]]></title>
            <link>https://glthr.com/myst-graph-1</link>
            <guid>43549293</guid>
            <pubDate>Tue, 01 Apr 2025 17:19:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://glthr.com/myst-graph-1">https://glthr.com/myst-graph-1</a>, See on <a href="https://news.ycombinator.com/item?id=43549293">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content-parent"><div data-node-type="callout">
<p>👋</p>
<p><em>Hi HackerNews! Please note that </em><a target="_blank" href="https://glthr.com/myst-graph-2"><em>a second article</em></a><em> uses graph analysis to reveal new findings about Myst (e.g., unreachable views, most connected locations).</em></p>
</div>

<p>Upon reflection, <a target="_blank" href="https://en.wikipedia.org/wiki/Myst">Myst</a> has long been more analogous to a graph than a traditional linear game, owing to the relative freedom it affords players. This is particularly evident in its first release (Macintosh, 1993), which was composed of interconnected HyperCard cards.</p>
<p>It is now literally one. Here is Myst as a graph:</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209037941/20772834-e8ee-4c58-9fad-660cfe6b3f09.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>The Myst graph, a comprehensive representation of the game’s structure that maps the connections between various views and locations, <strong>is downloadable</strong> <a target="_blank" href="https://github.com/glthr/DeMystify/blob/054bcb5de98e977bd5635b2a2664bebf343b56ea/graphs/myst_graph.pdf"><strong>as a poster-size PDF</strong></a>.  </p>
<h2 id="heading-introduction">Introduction</h2>
<h2 id="heading-origins-of-the-myst-graph">Origins of the Myst Graph</h2>
<p>I fondly recall creating, just like many others (including <a target="_blank" href="http://www.imajeenyus.com/computer/20120814_selenitic_age_map/selenitic_age_maze_map.pdf">more skilled cartographers</a>), a rudimentary <em>topological map</em> of <a target="_blank" href="https://dni.fandom.com/wiki/Mazerunner">the infamous Myst Mazerunner puzzle</a> when I was young. The idea recently struck me: could this approach be extended to the entire game? The HyperCard implementation’s deterministic connection of cards seemed tailor-made for an even more abstract map: a <em>graph</em>.  After all, graphs are a powerful tool for information analysis. By having the ability to represent the original Myst as a network of interconnected nodes and edges, we could gain a deeper understanding of how the different aspects of the game are related and uncover new insights into its underlying mechanics.</p>
<p>However, I did not see myself creating it manually; it would have been a monumental and error-prone task. Fortunately, I soon discovered, thanks to the efforts of <a target="_blank" href="https://mastodon.social/@uliwitness@chaos.social">Uli Kusterer</a> (Youtube channel: <a target="_blank" href="https://www.youtube.com/watch?v=CjrBwRnIVG4&amp;list=PLZjGMBjt_VVCl3cftfjeO8qQsSxgDQYO1">“Masters of the Void”</a>) and the <a target="_blank" href="https://www.youtube.com/watch?v=lacwEuMaQvQ">“Reverse Engineering Myst”</a> presentation at <a target="_blank" href="https://mysterium.net/">Mysterium Con</a> 2024, that the game’s source code was accessible. That changed everything: how about generating the graph programmatically?</p>
<h2 id="heading-roadmap">Roadmap</h2>
<p><em>(1)</em> In this article, I introduce the concept of the Myst graph, explaining why it is an important tool for analyzing the game, and walk readers through how to interpret the graph, highlighting key concepts and insights. <em>(2)</em> <strong>A second article,</strong> <a target="_blank" href="https://glthr.com/myst-graph-2"><strong>already published</strong></a><strong>, explores new findings that have emerged from analyzing the Myst graph. This article builds on the present article, providing additional context and explanations for the insights gained from the graph.</strong> <em>(3)</em> A third article, not published yet, will detail the technical approach and open source “DeMystify,” the program I created to generate the graph. This will provide readers with hands-on experience exploring the Myst graph using code. <em>(4)</em> Last, a fourth article will discuss how this graph can be used as a starting point for new projects and speculate on what other insights might be gained by further analysis.</p>
<h2 id="heading-minor-terminology-clarifications">Minor Terminology Clarifications</h2>
<p>Before diving into the presentation of the graph, let’s clarify some words. “<em>Dunny</em>” refers to D’ni: this is how the D’ni stack was initially named. Also, I may refer to the Myst Age as the “<em>Myst Island</em>” for variation. When discussing graph theory, I will use the following terms: <em>nodes</em> will refer to the vertices, while <em>edges</em> represent the connections between them. For those unfamiliar with graph theory, just remember that edges are arrows pointing from one box (or view, or card) to another, the nodes, illustrating a relationship between the two.</p>
<h2 id="heading-disclaimer">Disclaimer</h2>
<p>This project is a personal initiative to analyze and understand the classic Myst game. It is an unofficial and nonlucrative open-source effort created for educational purposes only. This project is not affiliated with Cyan Worlds, Inc. or the original creators of Myst. Any opinions, insights, or analyses presented on this blog are my personal views and do not reflect any official positions or statements of Cyan Worlds, Inc., its affiliates, or any organizations I might be affiliated with.</p>
<p>Before analyzing this graph, please be aware that this series of articles will contain spoilers for the game. If you are new to Myst or plan to play it soon, I recommend skipping these articles and experiencing it firsthand: Myst remains such a fantastic game!</p>
<h2 id="heading-basic-properties-of-the-myst-graph">Basic Properties of the Myst Graph</h2>
<h2 id="heading-nodes-and-edges">Nodes and Edges</h2>
<p>The game comprises 6 HyperCard stacks, one for each Age, totaling 1,355 cards. The graph abstracts these into 1,364 nodes connected by 3,189 edges.</p>
<p>The number of nodes does not coincide with the number of cards and stacks because 3 nodes are <em>virtual</em>. They represent cards that were not shipped in the release. They only exist because they are referred to by actual cards. These virtual nodes are Mechanical Age 17673 and 20348, and Selenitic Age 31832.</p>
<p>In this version of the graph, edges do not encode gameplay constraints. A card is considered connected to another if it references that card; the specifics of how the player interacts with the card to reach the other card—whether by simply clicking or solving a complex puzzle—are left implicit. (This choice is discussed later.)</p>
<h2 id="heading-paths">Paths</h2>
<p>Paths are sequences of edges from node <em>A</em> to node <em>B</em>. Because the gameplay has not been integrated into the graph, they do not necessarily represent feasible direct paths in the game. For instance, a puzzle on the path may require performing actions outside the path. In other words, the paths evoked in these articles abstract away all the constraints of the game, except for the connections between cards.</p>
<p>Theoretically, the <em>shortest path</em> between the starting point and the (good) ending consists of 24 edges. From a pure graph distance point of view, reaching the beginning of the game and its ending only takes 24 movements or a change of direction. The <em>most distant nodes</em> (in other words, the longest shortest path in the graph) are separated by 130 edges. In the context of the game, <a target="_blank" href="https://glthr.com/myst-graph-2">these paths are detailed in the second article</a>.</p>
<h2 id="heading-clusters">Clusters</h2>
<p>Interestingly, while the file that encodes the graph does not contain any information about clusters (except for isolated clusters, as explained in the second article, and for the colors of the nodes, for rendering purposes), the rendering engine that transforms it into a PDF, creates an organic (albeit imperfect) formation of clusters: the Myst Island in the middle, and the Ages at the periphery. This is explainable: Myst Island refers to all Ages; each Age only refers to Myst Island.</p>
<p>There is one disconnected cluster outside a few isolated nodes—that, consequently, cannot be reached from within the game. This isolated cluster comprises 3 nodes, including 2 virtual ones, from the Mechanical Age: originally, 17552 had directed edges with 20348 and 17673, both absent from the Mechanical Age stack.</p>
<h2 id="heading-legend-how-to-read-the-graph">Legend: How to Read the Graph</h2>
<h2 id="heading-nodes">Nodes</h2>
<h3 id="heading-colors">Colors</h3>
<p>Each Age has its own color to facilitate the identification of stack-based clusters.</p>
<h3 id="heading-nodes-labels">Nodes Labels</h3>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209347182/07be4e6f-ef5a-423a-b356-707561202f3f.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>When a node abstracts a stack, the name of the stack (corresponding to the Age) is prefixed with “[Stack]”. For instance, “[Stack] Myst” is the stack containing the cards belonging to the Myst Island.</p>
<p>Nodes representing cards are labeled following this pattern: “{Stack}:{ID} [({Original name})] \ {Image name}”.</p>
<p>The cards are identified by an ID (see <a target="_blank" href="https://hypercard.org/HyperTalk%20Reference%202.4.pdf#page=280">page 280 of this reference</a>), a HyperCard-generated integer. Example: “Mechanical Age:12345” refers to card ID 12345 belonging to the Mechanical Age stack. These IDs are unique <em>per stack</em>. Since each stack has its own set of unique IDs, there can be collisions across stacks where two or more IDs are identical between different stacks. This is evident in the Myst graph, where some IDs appear twice across Ages (8059, 8794, 9075, 18304, 19870, 21064, 32302, 33578, 34421, 39710, 52457, 55809, and 74269). As HyperCard automatically increments these IDs within each stack, one can reasonably infer that the greater an ID value is, the closer in time it was created relative to the initial release. However, this relationship may not hold for the underlying assets, as they were rendered separately through a different process. In other words, a “recent” (relative to the release) card does not necessarily mean that the asset it represents was also rendered close to the release.</p>
<p>Cards can have been voluntarily named. When that occurs, the original name is specified between parentheses after the ID. For example, “Myst:81655 (AchenarLose)” was named “AchenarLose” by the Myst creators.</p>
<p>Last, cards represent a view or an image, so they need a reference to an asset filename. This information is displayed just below the label. So “Myst:8336 (dock) \ <em>Dock1-E</em>” means that card ID 8336, belonging to the Myst Island and named initially “dock” features an image called “<em>Dock1-E</em>”. This specific node plays an important role in the game, as it is where the player starts.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209447972/35be9198-659d-4aba-8134-e326d8c41f65.png?auto=compress,format&amp;format=webp" alt=""></p>
<h3 id="heading-special-nodes">Special Nodes</h3>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209475311/765207ee-1398-4fb2-a176-fba8c7b3a2c5.png?auto=compress,format&amp;format=webp" alt=""></p>
<p><strong>Sink nodes</strong> serve as destinations, receiving connections but not providing any exit paths. They can be reached within the game, contrary to the following categories of nodes. <strong>Source nodes</strong> act as starting points, sending out connections and initiating new paths without incoming edges. They are unreachable in the game. <strong>Isolated nodes</strong> are unreachable from the rest of the graph, having no incoming or outgoing edges that could be followed.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209487825/0f17f99e-67a5-4b9b-8983-b8f2d4e19427.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>More specific to the game, nodes with <strong>blue borders</strong> represent views containing blue pages, while those with <strong>red borders</strong> show views having red pages. The unique, <strong>purple-bordered</strong> node denotes a view encompassing blue and red pages.</p>
<h2 id="heading-edges">Edges</h2>
<h3 id="heading-directions">Directions</h3>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209522084/2b46ae44-812d-4996-ab70-286d53a18dbe.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>The Myst graph is a directed graph. The quasi-totality of the edges is either <strong>directed</strong> (they connect two nodes in one direction) or <strong>bidirectional</strong> (they connect two nodes in both directions). Rarely (3 occurrences), some edges are <strong>self-loops</strong>, connecting a node to itself.</p>
<p>In the game, a <em>bidirectional</em> edge typically represents a direct change of orientation (cardinal points, looking up and down). Movements are usually represented by <em>directed</em> edges, as walking backward is impossible, except for some exceptions (backtracking edges; see below).</p>
<p>Here is a concrete demonstration. In the village of the Channelwood Age, changing between one of the staircase ascending views (17225) and its corresponding pathway view (73038) can be done in a single click as the edge is bidirectional. However, once the player takes the stairs to go up (73346), it is impossible to backtrack directly to the original staircase ascending view without first turning around to look down (73587), then reaching the original pathway view (73038) and last turning around again (17225).</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209541672/1d322a52-aec4-436e-97d2-7b3482a33c9b.png?auto=compress,format&amp;format=webp" alt=""></p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209546442/b749a6d0-83bf-422d-b064-b604212c0dbc.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>To summarize, and with some exceptions, from a navigational perspective, while bidirectional edges can be interpreted as walking, directed edges can be interpreted as changing directions.</p>
<h3 id="heading-special-edges">Special Edges</h3>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209571761/5a268f32-8899-4ff6-afc8-1fec1a61fb1e.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>Edges connecting two nodes from different Ages (<strong>cross-Ages edges</strong>) are rendered with a thick line. Edges that connect exclusively two nodes are <strong>backtracking edges</strong>. They act as a device to zoom in/zoom out (one click to enter the next node, one click to go back to the previous node) and are represented by edges with inversed arrow heads and tails. Last, some of the connections are commented out in the source code of Myst: they characterize <strong>disabled edges</strong>, represented by dashed lines with a tee arrow shape.</p>
<h3 id="heading-transitivity">Transitivity</h3>
<p>In Myst, there are three forms of transitivity.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209612487/02eb6814-2c5c-4d6f-93eb-58fd0e203c22.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>First, by default, intermediary nodes are <strong>fully transitive</strong> (edges directions notwithstanding, obviously). If node B can access A, and C and D can access B, then C and D can access A. This is the common navigational mechanism.</p>
<p>Second, some edges are <strong>intransitive</strong>: while B can access A and, reciprocally, A can access B, C and D can access B too, but they cannot access A. This relationship characterizes the backtracking edges.</p>
<p>Lastly, some edges are <strong>partially transitive</strong>: if node B can access A, C, and D can access B, only D can access A. In the realm of Myst, this is only true of the edges making the player return to Myst after completing an Age. Upon returning to Myst Island with the book, they first systematically see the ceiling of the library (Myst:44018). When they click the ceiling, they automatically face the bookshelf (Myst:46439). (The mechanism by which this is done with HyperTalk, the scripting language of HyperCard, will be detailed in the third article.)</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1743209635499/d455df43-c947-45e0-9150-5e2d22912715.png?auto=compress,format&amp;format=webp" alt=""></p>
<h2 id="heading-limitations">Limitations</h2>
<p>First, as explained in the previous section, the graph does not capture the gameplay mechanics. Although this approach has its limitations, it provides a significant advantage. By separating the gameplay from the graph structure, we can already gain novel insights into Myst without dealing with the complexities of puzzle design. Of course, nothing prevents future versions of the graph from including this logic.</p>
<p>Second, the rendered clusters are not entirely separate from each other. They overlap in their boundaries. For example, Myst:38896 and Myst:30143 encroach on the D’ni Age. While it would be possible to constrain the graph so there is no overlapping, I made the rendering engine as Age agnostic as possible for this initial version. As “DeMystify”, the graph generator, will be released, readers can try different arrangements; I would be very interested in seeing their results.</p>
<p>Third, the graph only captures straightforward relationships between cards and stacks. Analyzing the scripts more granularly and identifying complex relationships, such as image substitutions, would be interesting. This lack of granularity is evident for the books from the bookshelf: the current graph does not sufficiently capture the pages. A future version may improve this aspect.</p>
<h2 id="heading-next-steps">Next Steps</h2>
<p>Now that we have had a chance to familiarize ourselves with the Myst graph, its structure, and how it relates to the game, l<strong>et’s dive deeper</strong> <a target="_blank" href="https://glthr.com/myst-graph-2"><strong>with a second article</strong></a><strong>, which explores some of the insights and discoveries made possible through this new conceptual tool</strong>. The article will take us through the hidden connections and relationships between various game elements, revealing new aspects of Myst that were previously unknown or unseen.</p>
<p>The graph will make one with the game.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Qwen-2.5-32B is now the best open source OCR model (159 pts)]]></title>
            <link>https://github.com/getomni-ai/benchmark/blob/main/README.md</link>
            <guid>43549072</guid>
            <pubDate>Tue, 01 Apr 2025 17:00:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/getomni-ai/benchmark/blob/main/README.md">https://github.com/getomni-ai/benchmark/blob/main/README.md</a>, See on <a href="https://news.ycombinator.com/item?id=43549072">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true" aria-labelledby="file-name-id-wide file-name-id-mobile"><article itemprop="text"><p dir="auto"><a href="https://getomni.ai/ocr-benchmark" rel="nofollow"><img src="https://camo.githubusercontent.com/628b0587ccd21fe4c90b6d165c5ab572fd0a2a1c78d59c4ce1587d2cc6f79d17/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f6f6d6e692d6f63722d62656e63686d61726b2e706e67" alt="Omni OCR Benchmark" data-canonical-src="https://omniai-images.s3.us-east-1.amazonaws.com/omni-ocr-benchmark.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Omni OCR Benchmark</h2><a id="user-content-omni-ocr-benchmark" aria-label="Permalink: Omni OCR Benchmark" href="#omni-ocr-benchmark"></a></p>
<p dir="auto">A benchmarking tool that compares OCR and data extraction capabilities of different large multimodal models such as gpt-4o, evaluating both text and json extraction accuracy. The goal of this benchmark is to publish a comprehensive benchmark of OCRaccuracy across traditional OCR providers and multimodal Language Models. The evaluation dataset and methodologies are all Open Source, and we encourage expanding this benchmark to encompass any additional providers.</p>
<p dir="auto"><a href="https://getomni.ai/blog/benchmarking-open-source-models-for-ocr" rel="nofollow"><strong>Open Source LLM Benchmark Results (Mar 2025)</strong></a> | <a href="https://huggingface.co/datasets/getomni-ai/ocr-benchmark" rel="nofollow"><strong>Dataset</strong></a></p>
<p dir="auto"><a href="https://getomni.ai/ocr-benchmark" rel="nofollow"><strong>Benchmark Results (Feb 2025)</strong></a> | <a href="https://huggingface.co/datasets/getomni-ai/ocr-benchmark" rel="nofollow"><strong>Dataset</strong></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/30934424/429220141-2be179ad-0abd-4f0e-b73a-7d5a70390367.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM1NTA1MDMsIm5iZiI6MTc0MzU1MDIwMywicGF0aCI6Ii8zMDkzNDQyNC80MjkyMjAxNDEtMmJlMTc5YWQtMGFiZC00ZjBlLWI3M2EtN2Q1YTcwMzkwMzY3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDAxVDIzMzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTE2OWRhMDYyMTJkY2Y0Y2IyNTNjZGRhM2I0ZGI4ZGJjMzUxNGFiY2UzZTU4MmMzYTc3ZjA2NDc5NmU5YzMwN2MmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.2QjIz9LPDIfSr2nqi-Sm9fshhY-h3mhkhQfh9BFDfyI"><img src="https://private-user-images.githubusercontent.com/30934424/429220141-2be179ad-0abd-4f0e-b73a-7d5a70390367.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM1NTA1MDMsIm5iZiI6MTc0MzU1MDIwMywicGF0aCI6Ii8zMDkzNDQyNC80MjkyMjAxNDEtMmJlMTc5YWQtMGFiZC00ZjBlLWI3M2EtN2Q1YTcwMzkwMzY3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDAxVDIzMzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTE2OWRhMDYyMTJkY2Y0Y2IyNTNjZGRhM2I0ZGI4ZGJjMzUxNGFiY2UzZTU4MmMzYTc3ZjA2NDc5NmU5YzMwN2MmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.2QjIz9LPDIfSr2nqi-Sm9fshhY-h3mhkhQfh9BFDfyI" alt="image"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Methodology</h2><a id="user-content-methodology" aria-label="Permalink: Methodology" href="#methodology"></a></p>
<p dir="auto">The primary goal is to evaluate JSON extraction from documents. To evaluate this, the Omni benchmark runs <strong>Document ⇒ OCR ⇒ Extraction</strong>. Measuring how well a model can OCR a page, and return that content in a format that an LLM can parse.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3888f1759b84adb355b4f91d0da2ee6fc772fc7b44fed778aff22d8e6b8cb0f1/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f6d6574686f646f6c6f67792d6469616772616d2e706e67"><img src="https://camo.githubusercontent.com/3888f1759b84adb355b4f91d0da2ee6fc772fc7b44fed778aff22d8e6b8cb0f1/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f6d6574686f646f6c6f67792d6469616772616d2e706e67" alt="methodology" data-canonical-src="https://omniai-images.s3.us-east-1.amazonaws.com/methodology-diagram.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Evaluation Metrics</h2><a id="user-content-evaluation-metrics" aria-label="Permalink: Evaluation Metrics" href="#evaluation-metrics"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">JSON accuracy</h3><a id="user-content-json-accuracy" aria-label="Permalink: JSON accuracy" href="#json-accuracy"></a></p>
<p dir="auto">We use a modified <a href="https://github.com/zgrossbart/jdd">json-diff</a> to identify differences between predicted and ground truth JSON objects. You can review the <a href="https://github.com/getomni-ai/benchmark/blob/main/src/evaluation/json.ts">evaluation/json.ts</a> file to see the exact implementation. Accuracy is calculated as:</p>
<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="48ab4fa3f8d3dd346b396ed5891458ab">$$\text{Accuracy} = 1 - \frac{\text{number of difference fields}}{\text{total fields}}$$</math-renderer>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1cb5a1143e4200a729f6c296a5306caceb6de7dec623d0f59da135530a4fa6d6/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f6a736f6e5f61636375726163792e706e67"><img src="https://camo.githubusercontent.com/1cb5a1143e4200a729f6c296a5306caceb6de7dec623d0f59da135530a4fa6d6/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f6a736f6e5f61636375726163792e706e67" alt="json-diff" data-canonical-src="https://omniai-images.s3.us-east-1.amazonaws.com/json_accuracy.png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Text similarity</h3><a id="user-content-text-similarity" aria-label="Permalink: Text similarity" href="#text-similarity"></a></p>
<p dir="auto">While the primary benchmark metric is JSON accuracy, we have included <a href="https://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow">levenshtein distance</a> as a measurement of text similarity between extracted and ground truth text.
Lower distance indicates higher similarity. Note this scoring method heavily penalizes accurate text that does not conform to the exact layout of the ground truth data.</p>
<p dir="auto">In the example below, an LLM could decode both blocks of text without any issue. All the information is 100% accurate, but slight rearrangements of the header text (address, phone number, etc.) result in a large difference on edit distance scoring.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d1d49357f37a1fa8e1ba6673a550fbbcc70d4245bf41c6be215d8ce25927dbc2/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f656469745f64697374616e63652e706e67"><img src="https://camo.githubusercontent.com/d1d49357f37a1fa8e1ba6673a550fbbcc70d4245bf41c6be215d8ce25927dbc2/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f656469745f64697374616e63652e706e67" alt="text-similarity" data-canonical-src="https://omniai-images.s3.us-east-1.amazonaws.com/edit_distance.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running the benchmark</h2><a id="user-content-running-the-benchmark" aria-label="Permalink: Running the benchmark" href="#running-the-benchmark"></a></p>
<ol dir="auto">
<li>Clone the repo and install dependencies: <code>npm install</code></li>
<li>Prepare your test data
<ol dir="auto">
<li>For local data, add individual files to the <code>data</code> folder.</li>
<li>To pull from a DB, add <code>DATABASE_URL</code> in your <code>.env</code></li>
</ol>
</li>
<li>Copy the <code>models.example.yaml</code> file to <code>models.yaml</code>. Set up API keys in <code>.env</code> for the models you want to test. Check out the <a href="#supported-models">supported models</a> here.</li>
<li>Run the benchmark: <code>npm run benchmark</code></li>
<li>Results will be saved in the <code>results/&lt;timestamp&gt;/results.json</code> file.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported models</h2><a id="user-content-supported-models" aria-label="Permalink: Supported models" href="#supported-models"></a></p>
<p dir="auto">To enable specific models, create a <code>models.yaml</code> file in the <code>src</code> directory. Check out the <a href="https://github.com/getomni-ai/benchmark/blob/main/src/models.example.yaml">models.example.yaml</a> file for the required variables.</p>
<div dir="auto" data-snippet-clipboard-copy-content="models:
  - ocr: gemini-2.0-flash-001 # The model to use for OCR
    extraction: gpt-4o # The model to use for JSON extraction

  - ocr: gpt-4o
    extraction: gpt-4o
    directImageExtraction: true # Whether to use the model's native image extraction capabilities"><pre><span>models</span>:
  - <span>ocr</span>: <span>gemini-2.0-flash-001 </span><span><span>#</span> The model to use for OCR</span>
    <span>extraction</span>: <span>gpt-4o </span><span><span>#</span> The model to use for JSON extraction</span>

  - <span>ocr</span>: <span>gpt-4o</span>
    <span>extraction</span>: <span>gpt-4o</span>
    <span>directImageExtraction</span>: <span>true </span><span><span>#</span> Whether to use the model's native image extraction capabilities</span></pre></div>
<p dir="auto">You can view configuration for each model in the <a href="https://github.com/getomni-ai/benchmark/blob/main/src/models">src/models/</a> folder.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Closed-source LLMs</h3><a id="user-content-closed-source-llms" aria-label="Permalink: Closed-source LLMs" href="#closed-source-llms"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model Provider</th>
<th>Models</th>
<th>OCR</th>
<th>JSON Extraction</th>
<th>Required ENV Variables</th>
</tr>
</thead>
<tbody>
<tr>
<td>Anthropic</td>
<td><code>claude-3-5-sonnet-20241022</code></td>
<td>✅</td>
<td>✅</td>
<td><code>ANTHROPIC_API_KEY</code></td>
</tr>
<tr>
<td>OpenAI</td>
<td><code>gpt-4o</code></td>
<td>✅</td>
<td>✅</td>
<td><code>OPENAI_API_KEY</code></td>
</tr>
<tr>
<td>Gemini</td>
<td><code>gemini-2.0-flash-001</code>, <code>gemini-1.5-pro</code>, <code>gemini-1.5-flash</code></td>
<td>✅</td>
<td>✅</td>
<td><code>GOOGLE_GENERATIVE_AI_API_KEY</code></td>
</tr>
<tr>
<td>Mistral</td>
<td><code>mistral-ocr</code></td>
<td>✅</td>
<td>❌</td>
<td><code>MISTRAL_API_KEY</code></td>
</tr>
<tr>
<td>OmniAI</td>
<td><code>omniai</code></td>
<td>✅</td>
<td>✅</td>
<td><code>OMNIAI_API_KEY</code>, <code>OMNIAI_API_URL</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Open-source LLMs</h3><a id="user-content-open-source-llms" aria-label="Permalink: Open-source LLMs" href="#open-source-llms"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model Provider</th>
<th>Models</th>
<th>OCR</th>
<th>JSON Extraction</th>
<th>Required ENV Variables</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gemma 3</td>
<td><code>google/gemma-3-27b-it</code></td>
<td>✅</td>
<td>❌</td>
<td></td>
</tr>
<tr>
<td>Qwen 2.5</td>
<td><code>qwen2.5-vl-32b-instruct</code>, <code>qwen2.5-vl-72b-instruct</code></td>
<td>✅</td>
<td>❌</td>
<td></td>
</tr>
<tr>
<td>Llama 3.2</td>
<td><code>meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo</code>, <code>meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo</code></td>
<td>✅</td>
<td>❌</td>
<td></td>
</tr>
<tr>
<td>ZeroX</td>
<td><code>zerox</code></td>
<td>✅</td>
<td>✅</td>
<td><code>OPENAI_API_KEY</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Cloud OCR Providers</h3><a id="user-content-cloud-ocr-providers" aria-label="Permalink: Cloud OCR Providers" href="#cloud-ocr-providers"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model Provider</th>
<th>Models</th>
<th>OCR</th>
<th>JSON Extraction</th>
<th>Required ENV Variables</th>
</tr>
</thead>
<tbody>
<tr>
<td>AWS</td>
<td><code>aws-text-extract</code></td>
<td>✅</td>
<td>❌</td>
<td><code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, <code>AWS_REGION</code></td>
</tr>
<tr>
<td>Azure</td>
<td><code>azure-document-intelligence</code></td>
<td>✅</td>
<td>❌</td>
<td><code>AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT</code>, <code>AZURE_DOCUMENT_INTELLIGENCE_KEY</code></td>
</tr>
<tr>
<td>Google</td>
<td><code>google-document-ai</code></td>
<td>✅</td>
<td>❌</td>
<td><code>GOOGLE_LOCATION</code>, <code>GOOGLE_PROJECT_ID</code>, <code>GOOGLE_PROCESSOR_ID</code>, <code>GOOGLE_APPLICATION_CREDENTIALS_PATH</code></td>
</tr>
<tr>
<td>Unstructured</td>
<td><code>unstructured</code></td>
<td>✅</td>
<td>❌</td>
<td><code>UNSTRUCTURED_API_KEY</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<ul dir="auto">
<li>LLMS are instructed to use the following <a href="https://github.com/getomni-ai/benchmark/blob/main/src/models/shared/prompt.ts">system prompts</a> for OCR and JSON extraction.</li>
<li>For Google Document AI, you need to include <code>google_credentials.json</code> in the <code>data</code> folder.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmark Dashboard</h2><a id="user-content-benchmark-dashboard" aria-label="Permalink: Benchmark Dashboard" href="#benchmark-dashboard"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/getomni-ai/benchmark/blob/main/assets/dashboard-gif.gif"><img src="https://github.com/getomni-ai/benchmark/raw/main/assets/dashboard-gif.gif" alt="dashboard" data-animated-image=""></a></p>
<p dir="auto">You can use benchmark dashboard to easily view the results of each test run. Check out the <a href="https://github.com/getomni-ai/benchmark/blob/main/dashboard/README.md">dashboard documentation</a> for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License - see the LICENSE file for details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A man powers home for eight years using a thousand old laptop batteries (347 pts)]]></title>
            <link>https://techoreon.com/a-man-powers-home-8-years-laptop-batteries/</link>
            <guid>43548217</guid>
            <pubDate>Tue, 01 Apr 2025 15:49:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techoreon.com/a-man-powers-home-8-years-laptop-batteries/">https://techoreon.com/a-man-powers-home-8-years-laptop-batteries/</a>, See on <a href="https://news.ycombinator.com/item?id=43548217">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
<p><strong>A man has managed to power his home for eight years with a system using more than 1,000 recycled laptop batteries</strong>. This ingenious project, based on the use of electronic waste, has proven to be an environmentally friendly and economical solution, without the need to even replace batteries over the years.</p>



<p>This system also uses solar panels, which were the origin of his renewable energy project that he started a long time ago and which has been enough for him to live during this time.</p>



<h3><mark>How Does This DIY Power System Work?</mark></h3>



<p>The project began in November 2016, when the creator, known with the alias <em>Glubux</em> on online forums, began sharing his plans in the <a href="https://secondlifestorage.com/index.php?threads%2Fglubuxs-powerwall.126%2F=" target="_blank" rel="noopener"><em>Second Life Storage community</em></a>. From the outset, his goal was clear: to generate energy for his home without relying on the electrical grid, through a combination of solar panels and recycled batteries.</p>




<p>In its early stages, it used a basic 1.4 kW solar panel system, along with an old 24V 460Ah forklift battery, charge controllers, and a 3 kVA inverter. However, its vision was to <strong>expand the system and take it beyond</strong> what it had initially achieved.</p>



<p>The centerpiece of their system is more than 1,000 secondhand laptop batteries. For many, old computer batteries are considered waste, but for Glubux, they represented an opportunity to create a completely independent as well as renewable energy source.</p>



<p>Reusing these batteries is a great idea and is an example of how it’s possible to give a second life to electronic waste, a sector in which the UN has noted that <strong>less than a quarter</strong> of the e-waste generated globally is properly collected and recycled.</p>




<p>The system was initially modest, but over time, Glubux began adding more and more recycled batteries. Soon,&nbsp;his installation grew from a small setup to a self-powered system consisting of 650 batteries.</p>



<p>This growth forced the creator to build a separate warehouse, located about 50 meters from his home, to store the batteries and the new charge controllers and inverters. The warehouse became a workshop where he assembled the battery packs, grouping them together to create blocks with a capacity of approximately 100 Ah each.</p>



<p>At first, he faced some obstacles. The battery discharge rates were uneven due to differences in the battery cells used, <strong>causing some to drain faster than others</strong>. However, the solution came with rearranging and adjusting the cells to ensure the packs worked more efficiently.</p>



<p>Glubux even began disassembling entire laptop batteries, removing individual cells and organizing them into custom racks. This task, which likely required a great deal of manual labor and technical knowledge, was key to making the system work effectively and sustainably.</p>



<h3><mark>How this System Has Lasted Eight Years?</mark></h3>



<p><strong>The most amazing thing about this project</strong> is that, despite the initial difficulties and the experimental nature of the system, it has continued to operate uninterruptedly today. In its eight years of operation, not a single battery cell has needed to be replaced, a remarkable achievement considering the operating conditions and the nature of the recycled batteries.</p>



<p>In addition, over the years, Glubux has improved and expanded its solar panel system. Currently, its installation features 24 solar panels, each measuring 440W, allowing it to generate sufficient power even during the coldest months.</p>




<p><strong>Despite being an unusual system</strong>, with recycled and homemade components, no major problems have been reported, such as fires or swollen batteries, which is a common issue with some second-hand electronic devices.</p>



<p>Glubux, for its part, continues to operate with complete confidence in its installation, which has not only been able to supply all of its home’s electricity, but also allows the operation of equipment such as the washing machine.</p>



<hr>







<!-- CONTENT END 1 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RubyUI (Former PhlexUI): Ruby Gem for RubyUI Components (115 pts)]]></title>
            <link>https://github.com/ruby-ui/ruby_ui</link>
            <guid>43548108</guid>
            <pubDate>Tue, 01 Apr 2025 15:39:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ruby-ui/ruby_ui">https://github.com/ruby-ui/ruby_ui</a>, See on <a href="https://news.ycombinator.com/item?id=43548108">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">RubyUI (former PhlexUI) 🚀</h2><a id="user-content-rubyui-former-phlexui-" aria-label="Permalink: RubyUI (former PhlexUI) 🚀" href="#rubyui-former-phlexui-"></a></p>
<p dir="auto">Beautifully designed components that you can copy and paste into your apps. Accessible. Customizable. Open Source.</p>
<p dir="auto">This is NOT a component library. It's a collection of re-usable components that you can generate or copy and paste into your apps.</p>
<p dir="auto">Pick the components you need. Copy and paste the code into your project and customize to your needs. The code is yours.</p>
<p dir="auto">Use this as a reference to build your own component libraries.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Key Features:</h3><a id="user-content-key-features" aria-label="Permalink: Key Features:" href="#key-features"></a></p>
<ul dir="auto">
<li><strong>Built for Speed</strong> ⚡: RubyUI leverages Phlex, which is up to 12x faster than traditional Rails ERB templates.</li>
<li><strong>Stunning UI</strong> 🎨: Design beautiful, streamlined, and customizable UIs that sell your app effortlessly.</li>
<li><strong>Stay Organized</strong> 📁: Keep your UI components well-organized and easy to manage.</li>
<li><strong>Customer-Centric UX</strong> 🧑‍💼: Create memorable app experiences for your users.</li>
<li><strong>Completely Customizable</strong> 🔧: Full control over the design of all components.</li>
<li><strong>Minimal Dependencies</strong> 🍃: Uses custom-built Stimulus.js controllers to keep your app lean.</li>
<li><strong>Reuse with Ease</strong> ♻️: Build components once and use them seamlessly across your project.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">How to Use:</h3><a id="user-content-how-to-use" aria-label="Permalink: How to Use:" href="#how-to-use"></a></p>
<ol dir="auto">
<li><strong>Find the perfect component</strong> 🔍: Browse live-embedded components on our documentation page.</li>
<li><strong>Copy the snippet</strong> 📋: Easily copy code snippets for quick implementation.</li>
<li><strong>Make it yours</strong> 🎨: Customize components using Tailwind utility classes to fit your specific needs.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation 🚀</h2><a id="user-content-installation-" aria-label="Permalink: Installation 🚀" href="#installation-"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">1. Install the gem</h3><a id="user-content-1-install-the-gem" aria-label="Permalink: 1. Install the gem" href="#1-install-the-gem"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="bundle add ruby_ui --group development --require false"><pre>bundle add ruby_ui --group development --require <span>false</span></pre></div>
<p dir="auto">or add it to your Gemfile:</p>
<div dir="auto" data-snippet-clipboard-copy-content="gem &quot;ruby_ui&quot;, group: :development, require: false"><pre><span>gem</span> <span>"ruby_ui"</span><span>,</span> <span>group</span>: <span>:development</span><span>,</span> <span>require</span>: <span>false</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">2. Run the installer:</h3><a id="user-content-2-run-the-installer" aria-label="Permalink: 2. Run the installer:" href="#2-run-the-installer"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="bin/rails g ruby_ui:install"><pre>bin/rails g ruby_ui:install</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">3. Done! 🎉</h3><a id="user-content-3-done-" aria-label="Permalink: 3. Done! 🎉" href="#3-done-"></a></p>
<p dir="auto">You can generate your components using <code>ruby_ui:component</code> generator.</p>
<div dir="auto" data-snippet-clipboard-copy-content="bin/rails g ruby_ui:component Accordion"><pre>bin/rails g ruby_ui:component Accordion</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation 📖</h2><a id="user-content-documentation-" aria-label="Permalink: Documentation 📖" href="#documentation-"></a></p>
<p dir="auto">Visit <a href="https://rubyui.com/docs/introduction" rel="nofollow">https://rubyui.com/docs/introduction</a> to view the full documentation, including:</p>
<ul dir="auto">
<li>Detailed component guides</li>
<li>Themes</li>
<li>Lookbook</li>
<li>Getting started guide</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Speed Comparison 🏎️</h2><a id="user-content-speed-comparison-️" aria-label="Permalink: Speed Comparison 🏎️" href="#speed-comparison-️"></a></p>
<p dir="auto">RubyUI, powered by Phlex, outperforms alternative methods:</p>
<ul dir="auto">
<li>Phlex: Baseline 🏁</li>
<li>ViewComponent: ~1.5x slower 🚙</li>
<li>ERB Templates: ~5x slower 🐢</li>
</ul>
<p dir="auto">See the original <a href="https://github.com/KonnorRogers/view-layer-benchmarks">view layers benchmark</a> by @KonnorRogers and its <a href="https://github.com/KonnorRogers/view-layer-benchmarks/forks">variations</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Importmap notes:</h2><a id="user-content-importmap-notes" aria-label="Permalink: Importmap notes:" href="#importmap-notes"></a></p>
<p dir="auto">If you run into importmap issues this stackoverflow question might help:
<a href="https://stackoverflow.com/questions/70548841/how-to-add-custom-js-file-to-new-rails-7-project/72855705" rel="nofollow">https://stackoverflow.com/questions/70548841/how-to-add-custom-js-file-to-new-rails-7-project/72855705</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License 📜</h2><a id="user-content-license-" aria-label="Permalink: License 📜" href="#license-"></a></p>
<p dir="auto">Licensed under the <a href="https://github.com/shadcn/ui/blob/main/LICENSE.md">MIT license</a>.</p>
<hr>
<p dir="auto">© 2024 RubyUI. All rights reserved. 🔒</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We can, must, and will simulate nematode brains (101 pts)]]></title>
            <link>https://asteriskmag.com/issues/09/we-can-must-and-will-simulate-nematode-brains</link>
            <guid>43547813</guid>
            <pubDate>Tue, 01 Apr 2025 15:16:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://asteriskmag.com/issues/09/we-can-must-and-will-simulate-nematode-brains">https://asteriskmag.com/issues/09/we-can-must-and-will-simulate-nematode-brains</a>, See on <a href="https://news.ycombinator.com/item?id=43547813">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<div data-mode="add-marker">
		<p><img id="marker" src="https://asteriskmag.com/assets/img/asterisk_mark.png" title="save highlight"></p><!-- <a href="https://asteriskmag.com/about/#highlights"><img id="help" src="https://asteriskmag.com/assets/img/asterisk_help.png" title="about highlights"></a> -->
		
	</div>

	<section>
				
		 			<h2>
				   
					<span>Michael Skuhersky</span>
							</h2>
			</section>
	
			<section id="rangyscope">
					<p>Scientists have spent over 25 years trying — and failing — to build computer simulations of the smallest brain we know. Today, we finally have the tools to pull it off.</p>
				<div>
											<div><p>A near-perfect simulation of the human brain would have profound implications for humanity. It could offer a pathway for us to transcend the biological limitations that have constrained human potential, and enable unimaginable new forms of intelligence, creativity, and exploration. This represents the next phase in human evolution, freeing our cognition and memory from the limits of our organic structure.</p><p>Unfortunately, it’s also a long way off. The human brain contains on the order of one hundred billion neurons — interconnected by up to a quadrillion synapses. Reverse-engineering this vast network would require computational resources far exceeding what’s currently available. Scientists seeking a proof of concept for whole brain emulation have had to turn to simpler model organisms. And by far the simplest available brain — at just 300 neurons — belongs to the nematode <em>Caenorhabditis elegans</em>.&nbsp;</p><p>Scientists have been working on the problem of simulating <em>C. elegans </em>in some form or another for over 25 years. So far, they’ve been met with little success. But with today’s technology, the task is finally possible, and —&nbsp;as I’ll argue —&nbsp;necessary.&nbsp;</p></div>
											<div><figure>
      <p><img src="https://asteriskmag.com/media/pages/issues/09/we-can-must-and-will-simulate-nematode-brains/6edcf61517-1737579357/skuhersky-04.png" alt="">
  </p>
    <p>
    Motion patterns of <em>C. elegans</em>. Credit: Hiroshima University, Osaka University  </p>
  </figure>
</div>
											<p><h2><strong>A brief history of worm brains</strong></h2>
</p>
											<div><p>The biologist Sydney Brenner became interested in <em>C. elegans </em>as a model organism for developmental biology in the 1970s. Its simplicity and small size made it an ideal lab subject. In 1986, John C. White, a scientist in Brenner’s research group, produced a nearly complete map of the neural connections that make up the <em>C. elegans</em> brain — what scientists now call the connectome. As computers became more accessible, other scientists started building on Brenner’s work. Ernst Neibur and Paul Erdös kicked things off with a <a href="https://www.cell.com/biophysj/fulltext/S0006-3495(91)82149-X">biophysical model</a> of nematode locomotion in 1991. Two different teams (one at <a href="https://web.archive.org/web/20110817114324/http://www.csi.uoregon.edu/projects/celegans/">the University of Oregon</a> and the other in <a href="https://pubmed.ncbi.nlm.nih.gov/9847421/">Japan</a>) published plans for building more ambitious models in the late 1990s. Both would have utilized White’s work on neural circuitry.&nbsp; Unfortunately, neither got off the ground.&nbsp;</p><p>In 2004, the Virtual <em>C. elegans </em>project at Hiroshima University got somewhat farther: they released <a href="http://www.bsys.hiroshima-u.ac.jp/pub/pdf/J/J_152.pdf">two</a> <a href="http://www.bsys.hiroshima-u.ac.jp/pub/pdf/J/J_153.pdf">papers</a> describing their model, which simulated the nematode’s motor control circuits. The simulated nematode could respond to virtual pokes on its head, but it didn’t do much else. And even this was, arguably, not a true simulation. Although the researchers had a map of the nematode’s neurons, they didn’t know their innate biophysical parameter — that is, the precise electrical characteristics of the connections between them. Instead, the researchers used machine learning to produce a set of values for each neuron that made their simulated nematode respond to a poke like a real one would. As a result, this approach was not entirely grounded in biological reality — a recurring theme that would surface in several future simulation attempts.</p><p>That is where things stood at the dawn of the 2010s. While <a href="https://etheses.whiterose.ac.uk/1377/">work</a> <a href="https://www.researchgate.net/publication/228374526_A_Biologically_Accurate_3D_Model_of_the_Locomotion_of_Caenorhabditis_Elegans">continued</a> on simulating nematode locomotion, there was no progress on simulating a nematode’s brain —&nbsp;let alone a realistic one. Then, on January 1st, 2010, the engineer Giovanni Idili tweeted at the official account of the Whole Brain Catalogue, a project to consolidate data from mouse brains: <a href="https://x.com/John_Idol/status/7279117575">“new year's resolution: simulate the whole C.Elegans brain (302 neurons)!”</a> U.C. San Diego neuroscience grad student Stephen Larson noticed the tweet and, by August, Larson was pitching the idea at conferences. By early 2011, Larson and Idili had put together a team to start work on what would become the OpenWorm project —&nbsp;the efforts of a decentralized group of academics with the goal of creating a complete, realistic, and open source model of <em>C. elegans.&nbsp;</em></p><p>This was a heady time to be interested in simulating extremely tiny brains. Over the next few years, OpenWorm published a series of papers and model updates. In 2013, they hosted their first conference in Paris and landed an optimistic story in <em>The Atlantic </em>(title: “<a href="https://www.theatlantic.com/technology/archive/2013/05/is-this-virtual-worm-the-first-sign-of-the-singularity/275715/">Is This Virtual Worm The First Sign of the Singularity?</a>”). Meanwhile, the researcher David Dalrymple was working on a parallel project at MIT, which he dubbed Nemaload. OpenWorm scientists largely used data from dead nematodes but Dalrymple wanted to use the then-new technique of optogenetics to study living specimens. Optogenetics allows scientists to control neurons and other cells with light. In this case, the technique could be used to collect data on how a nematode’s brain responds to different states by perturbing it thousands-upon-thousands of times. In a 2011 <a href="https://www.lesswrong.com/posts/XhHetxjWxZ6b85HK9/whole-brain-emulation-looking-at-progress-on-c-elgans?commentId=wwwhhRufNfuNTSmQy">comment </a>on LessWrong, Dalrymple wrote “I would be Extremely Surprised, for whatever that's worth, if this is still an open problem in 2020.”&nbsp;</p><p>It’s now 2025, and nematode simulation remains an open problem. Dalrymple abandoned Nemaload in 2012. OpenWorm still exists but has not made substantial progress over the past ten years towards creating a truly scientific whole brain simulation, due to a lack of available data<em>. </em>Occasionally, <a href="https://www.frontiersin.org/journals/computational-neuroscience/articles/10.3389/fncom.2019.00008/full">more modern (though still heavily assumption-based) simulations</a> are published, including <a href="https://www.nature.com/articles/s43588-024-00738-w%23MOESM1">integrative models that strive to make fewer assumptions</a>. We’re not quite back where we were in the 2010s: we have much better data on the <em>C. elegans</em> nervous system and — as I’ll discuss later — much better tools to study it. But we aren’t much closer to simulating a whole brain.&nbsp;</p><p>What went wrong? Why has it taken over 25 years to build a working computer simulation of one of the simplest brains known to mankind? And, more importantly, why do I think that this time we can actually pull it off?</p></div>
											<p><h2><strong>Why we got stuck</strong></h2>
</p>
											<div><p>Before explaining what happened, we should ask a more fundamental question: what does it mean to successfully simulate a brain? This is a topic where it’s important to be specific. The term "simulation" in academic neuroscience often evokes the <a href="https://www.scientificamerican.com/article/why-the-human-brain-project-went-wrong-and-how-to-fix-it/">notorious failures</a> of the Human Brain Project. In 2013, neuroscientist Henry Markram secured about 1 billion euros from the European Union to "simulate the human brain" — a proposal widely deemed unrealistic even at the time. The project faced significant challenges and ultimately did not meet its ambitious yet vague goals. These events cast something of a stigma on brain simulation research, making it especially important for those in the field to set clearer, more realistic goals with concrete milestones along the way.&nbsp;</p><p>What makes a good simulation is a debate in itself, so I’ll just share my view: a good simulation of a nervous system is one that both accurately replicates its functionality and reliably predicts the future activity of a real system under the same initial conditions. That is, a simulated nematode in a simulated plate of agar should behave the same way as a real nematode in a real plate of agar. If we disturb the simulation —&nbsp;say, by poking or shining a light on it — it should respond the same way the real nematode would. And it should keep acting like a real nematode over time, instead of accumulating more error as time goes on.&nbsp;</p><p>This definition can help us clarify what is and isn’t simulation. Last October, a consortium of scientists across 127 institutions published the <a href="https://www.nature.com/immersive/d42859-024-00053-4/index.html">complete connectome</a> of the fruit fly, <em>Drosophila melanogaster. </em>This is a massive accomplishment by any objective standard: it is only the second complete connectome assembled, after that of <em>C. elegans</em>, and contains over 140,000 neurons (as compared to <em>C. elegans</em>’s 300). The success of the project, called FlyWire, has rekindled interest in brain simulation. And, in a sense, the FlyWire connectome can be used to simulate a fruit fly. When Philip Shiu, a researcher on the project, test-‘fired’ the neurons responsible for sensing sugar, the model predicted that other neurons that extend the fly’s proboscis would fire, as they would in a real fly. Other researchers have since used Shiu’s model to accurately predict neural patterns involved in the fly’s sense of taste, grooming, and locomotion.&nbsp;</p><p>Shiu’s model represents an important advance in our understanding of fruit fly brains, but it isn’t really a simulation. (Nor is it trying to be;&nbsp;Shiu himself has been <a href="https://news.berkeley.edu/2024/10/02/researchers-simulate-an-entire-fly-brain-on-a-laptop-is-a-human-brain-next/">clear</a> that the model is extremely simplified and makes assumptions about key parameters governing how neurons behave). While the model can successfully predict the behavior of particular groups of neurons, it cannot mimic the exact functionality of an entire fly brain. That’s because the FlyWire model is missing the same thing as OpenWorm (and other attempts to simulate nematodes) did: good data on the relationship between neural structure and neural function.&nbsp;</p><p>Think of the connectome as a map of the brain. It can tell us how neurons connect to each other through electrical and chemical synapses. But despite revealing which neurons connect to one another, it doesn’t tell us anything about how those connections work. To fully model a brain, we need to understand the biophysical parameters governing each neuron’s behavior. This includes not only the variable strength of synapses (in neuroscience, these are called weights) but also the cells’ membrane properties, such as capacitance and the shapes of dendrites and axons, which affect how electrical signals propagate. We need to know both a neuron’s firing threshold as well as how that threshold changes as the animal learns new things (learning involves shifts in both synaptic weights and the intrinsic properties of neurons themselves). A simulation based only on a static connectome can’t learn —&nbsp;so it won’t behave very much like the real creature it’s trying to simulate.&nbsp;</p><p>Unfortunately, learning the dynamic biophysical features of a living brain is much harder than understanding its structure (which, as we’ve seen, is hard enough). The primary technique used to map a connectome is electron microscopy. Because electrons have a wavelength up to one hundred thousand times smaller than that of visible light, they can be used to produce images at a much higher resolution than light microscopes. But electron microscopy has a serious disadvantage. It can only be used on sliced brain tissue, so it can’t tell us how a living brain responds to stimuli or changes over time. The technique can give us extremely detailed, high quality images, but can’t tell us a neuron’s electrical characteristics, like the strength of its synapses or how its membranes store electrical charge.</p><p>For decades, the only way to learn such things was through a technique called patch clamping. The advantage of patch clamping is that it is highly accurate. The disadvantage is that it requires the painstaking placement of electrodes on each individual neuron. With effort, it’s feasible to patch clamp about three neurons at once, making it a less-than-ideal choice for capturing information about neural activity throughout the whole brain. &nbsp;</p><p>This is where things stood when earlier attempts to simulate <em>C. elegans </em>stalled out. It was a problem of timing:&nbsp; In 2013, the tools that would let us understand what happens inside neurons either didn’t exist, or weren’t ready for practical use.</p></div>
											<p><h2><strong>New ways to see</strong></h2>
</p>
											<div><p>As <em>C. elegans </em>simulation research was losing steam, other researchers pushed forward in advancing the ability to observe cells. First, advances in optical microscopy made it possible to capture fast, relatively sharp images of living cells without destroying them. Since the late 1950s, biologists have relied on confocal microscopes, which use a tiny pinhole to block out-of-focus light. This creates higher resolution images, but the method is also slow, since capturing a whole sample means scanning it point-by-point. This is a serious problem for studying traits that change rapidly (like neuronal activity). This is where modern techniques like light sheet microscopy prove particularly useful. Instead of focusing light through a point, light sheet microscopes use a laser sheet to illuminate an entire 2D cross-section of a sample.The process is dramatically faster and gentler on tissue than traditional confocal methods.&nbsp;</p><p>Light sheet microscopes have existed since the 1990s, but early versions of the technology struggled to capture fast intracellular processes. That changed with a series of innovations in the early 2010s. First, new techniques were developed to allow optical microscopy below the diffraction limit (the smallest distance between two points at which they can still be distinguished by an optical system). For visible light, this distance is between 200 and 250 nanometers — too big to distinguish most cellular features. That changed with the introduction of <a href="https://pubmed.ncbi.nlm.nih.gov/20643879/">super-resolution microscopy</a> which featured resolutions of 100 nanometers and below. Another major advance was DiSPIM,<sup>
    <!-- <a id="fnref-1" href="#fn-1"> -->
    <span id="fnref-1">
        1    </span>
    <!-- </a> -->
</sup>
 invented in 2014. In light sheet microscopes, the light illuminating an image has to be perpendicular to the camera picking it up. Originally, this meant that the camera and the light sheet were part of separate assemblies. DiSPIM microscopes use two perpendicular lens assemblies, each equipped with a light source and a camera. This approach doubled the speed with which the microscope could capture images of living samples, and ensured that images could be reconstructed at the same resolution across all three dimensions. In 2015, a group at Columbia University developed a method called SCAPE,<sup>
    <!-- <a id="fnref-2" href="#fn-2"> -->
    <span id="fnref-2">
        2    </span>
    <!-- </a> -->
</sup>
 which used an oblique sheet of light to scan and image a sample using a single lens assembly. SCAPE is even faster than earlier light sheet techniques, making it particularly useful for tracking rapid neuronal activity. &nbsp;</p><p>Another set of innovations has to do with what the microscopes are looking at. All the methods we’ve discussed depend on fluorescent reporters — engineered proteins that fluoresce under certain conditions, such as the presence of a specific protein or the expression of a particular gene. In our case, that trigger is calcium. When a neuron fires, calcium ions flood into the cell, making calcium influx a reliable proxy for neuronal activity. The key breakthrough here was the development of the GCaMP6 family of reporters by a team at the Janelia Research Campus between 2013 and 2015. This new generation of calcium indicators were brighter and more sensitive than earlier versions, quickly becoming the go-to tool for imaging neuronal circuits in living organisms. While GCaMP6 revolutionized calcium-based imaging, even more precise measurements could come from fluorescent reporters that respond to voltage directly. These already exist for larger organisms and are actively being developed for use in <em>C. elegans</em>.</p><p>Today, the combination of calcium imaging and microscopy techniques like DiSPIM and SCAPE means that we can see how neurons behave throughout the entire <em>C. elegans </em>brain — in real time. The next challenge is to actually do it. And to do it a lot. Our understanding of the <em>C. elegans </em>connectome has improved significantly since White’s groundbreaking work in 1986. White’s connectome was a mosaic of five individual worms. However, the same neuron in different animals might differ in size or capacity for electric charge. To fully understand the <em>C. elegans </em>brain and its operation during a broad range of behaviors, we need to collect data from thousands of individuals.&nbsp;</p><p>There’s the question of what to do with the data once we have it. This is another area where recent advances –&nbsp;this time, in machine learning — make the process much more feasible. For all its biological complexity, the <em>C. elegans </em>brain still consists of just 300 neurons — tiny compared to state-of-the-art large language models. Using <a href="https://arxiv.org/abs/2006.10782">symbolic regression</a>, a machine learning technique for discovering mathematical formulas that explain observed data, we can take our data on neuronal activity and use it to derive key parameters like capacitance and synaptic strength for every single neuron and every single neuronal connection. These equations would likely resemble the biophysical models that scientists have already derived from patch-clamp experiments, but inferred directly from whole-brain data.</p></div>
											<p><h2><strong>Fish, flies, and beyond</strong></h2>
</p>
											<div><p>I don’t mean to suggest that building an accurate <em>C. elegans </em>simulation will be easy. There are many considerations that the technologies I’ve described may not account for, from extra-synaptic signalling to the role of specific neuron morphology (not to mention the fact that neurons and synapses change over the course of a nematode’s life). But with modern techniques, which continue to rapidly improve, I do believe that it is possible.&nbsp;</p><p>And if we want to one day build simulations of larger animals — including humans — I also believe that it is necessary. The optical microscopy techniques that let us observe the neural activity of living organisms have one key limitation: depth. Light can only penetrate so far into tissue. With current techniques, that limit is roughly 750 microns, a bit less than a millimeter. To build an accurate whole brain simulation, we need activity data from a whole brain — which means that we’re currently limited to brains less than a millimeter deep. In other words, <em>C. elegans</em>, larval zebrafish, and fly brains are our only options. By investigating small organisms, we can develop new methods that allow us to predict neural activity by looking at the brain’s structure and other indirect forms of data. These techniques will make it possible for us to model more complex brains, including those that are too large for us to image their activity directly.&nbsp;</p><p>My research focuses on creating a scientifically-grounded simulation of <em>C. elegans</em> by integrating these recently developed microscopy, fluorescent reporter, and machine learning methods into a cohesive pipeline and methodological framework. The idea is to create a proven simulation creation blueprint that can then be applied to more complex brains. But achieving a successful simulation of <em>C. elegans</em> would be a remarkable scientific accomplishment on its own. More importantly, it would help us begin to decipher how the structure of a brain relates to the dynamic processes unfolding within it. Over time, this understanding will open the doors to simulating more complex organisms, ultimately including humans. We have a long journey ahead of us, but now is the best time to begin — expeditiously, and with tractable, well-defined milestones along the way.</p></div>
										 
				</div>
		
	</section>
	 	<section>
		 		 <p><strong>Michael Skuhersky</strong> holds a PhD in neuroscience from MIT and is currently founding a nonprofit research institute focused on brain simulation.</p>		 		 		 </section>
	 	<section>            
		<p>
			Published March 2025		</p>
		
		<p>Have something to say? Email us at <a href="mailto:letters@asteriskmag.com">letters@asteriskmag.com</a>.</p>		                        
	</section>	
	
	
	<!--end published content, not coming soon-->

	<!--tags-->
		<section>
		<h4>Further Reading</h4>                
			<p>
				More:  
									<span data-no="tag-1">science</span>
									<span data-no="tag-2">technology</span>
							</p>
			<!--related articles-->
			             
	</section>
	 
	
	  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Systems Correctness Practices at AWS: Leveraging Formal and Semi-Formal Methods (122 pts)]]></title>
            <link>https://queue.acm.org/detail.cfm?id=3712057</link>
            <guid>43547593</guid>
            <pubDate>Tue, 01 Apr 2025 14:59:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://queue.acm.org/detail.cfm?id=3712057">https://queue.acm.org/detail.cfm?id=3712057</a>, See on <a href="https://news.ycombinator.com/item?id=43547593">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<div>
		
		<p><a href="https://queue.acm.org/"><img src="https://queue.acm.org/img/acmqueue_logo.gif"></a>

	</p></div>

<!--
<p style='text-align:center;'>
<a href='/app/' target='_new'><img src='/app/2021_03-04_lrg.png' with=90 height=120 style='float:right;width:90px;height:120px;' alt='March/April 2021 issue of acmqueue' /></a>
<b><a href='/app/'>The March/April 2021 issue of acmqueue is out now</a></b>
<br />
<br />
<a href='https://cdn.coverstand.com/3rd_pty/acm/login.html?&btx_i=705849'>Subscribers and ACM Professional members login here</a>
<br clear=all />
<hr style='display:block;color:red;margin:5px;' />
</p>
-->




<p><label>February 4, 2025<br><b><a href="https://queue.acm.org/issuedetail.cfm?issue=3714453">Volume 22, issue 6 </a></b></label></p><p>
<!-- // Check for existence of associated MP3 file-->

 &nbsp;
	
			<a href="https://portal.acm.org/citation.cfm?id=3712057">
				<img src="https://queue.acm.org/img/icon_pdf.png" alt="Download PDF version of this article">
				PDF
			</a>
		
</p>


 
   
  <h2>Leveraging Formal and Semi-formal Methods</h2> 
  <h3>Marc Brooker and Ankush Desai</h3> 
  <p>AWS (Amazon Web Services) strives to deliver reliable services that customers can trust completely. This demands maintaining the highest standards of security, durability, integrity, and availability—with systems correctness serving as the cornerstone for achieving these priorities. An April 2015 paper published in <i>Communications of the ACM</i>, titled "How Amazon Web Services Uses Formal Methods," highlighted the approach for ensuring the correctness of critical services that have since become among the most widely used by AWS customers.<sup>21</sup> </p> 
  <p>Central to this approach was TLA+,<sup>14</sup> a formal specification language developed by Leslie Lamport. Our experience at AWS with TLA+ revealed two significant advantages of applying formal methods in practice. First, we could identify and eliminate subtle bugs early in development—bugs that would have eluded traditional approaches like testing. Second, we gained the deep understanding and confidence needed to implement aggressive performance optimizations while maintaining systems correctness. </p> 
  <p>Moreover, 15 years ago, AWS's software testing practice relied primarily on build-time unit testing, often against mocks, and limited deployment-time integration testing. Since then, we have significantly evolved our correctness practices, integrating both formal and semi-formal approaches into the development process. As AWS has grown, formal methods have become increasingly valuable—not only for ensuring correctness but also for performance improvements, particularly in verifying the correctness of both low- and high-level optimizations. This systematic approach toward systems correctness has become a force multiplier at AWS's scale, enabling faster development cycles through improved developer velocity while delivering more cost-effective services to customers.</p> 
  <p>This article surveys the portfolio of formal methods used across AWS to deliver complex services with high confidence in its correctness. We consider an umbrella definition of formal methods that encompasses these rigorous techniques—from traditional formal approaches such as theorem proving,<sup>7,10</sup> deductive verification,<sup>18</sup> and model checking<sup>8,14</sup> to more lightweight semi-formal approaches such as property-based testing,<sup>6,19</sup> fuzzing,<sup>9</sup> and runtime monitoring.<sup>11</sup></p> 
   
  <h3>The P Programming Language</h3> 
  <p>As the use of formal methods was expanded beyond the initial teams at AWS in the early 2010s, we discovered that many engineers struggled to learn and become productive with TLA+. This difficulty seemed to stem from TLA+'s defining feature: It is a high-level, abstract language that more closely resembles mathematics than the imperative programming languages most developers are familiar with. While this mathematical nature is a significant strength of TLA+, and we continue to agree with Lamport's views on the benefits of mathematical thinking,<sup>15</sup> we also sought a language that would allow us to model check (and later prove) key aspects of systems designs while being more approachable to programmers.</p> 
  <p>We found this balance in the P programming language.<sup>8</sup> P is a state-machine-based language for modeling and analysis of distributed systems. Using P, developers model their system designs as communicating state machines, a mental model familiar to Amazon's developer population, most of whom develop systems based on microservices and SOAs (service-oriented architectures). P has been developed at AWS since 2019 and is maintained as a strategic open-source project.<sup>22</sup> Teams across AWS that build some of its flagship products—from storage (e.g., Amazon S3, EBS), to databases (e.g., Amazon DynamoDB, MemoryDB, Aurora), to compute (e.g., EC2, IoT)—have been using P to reason about the correctness of their system designs.</p> 
  <p>For example, P was used in migrating S3 (Simple Storage Service) from eventual to strong read-after-write consistency.<sup>1</sup> A key component of S3 is its index subsystem, an object metadata store that enables fast data lookups. To achieve strong consistency, the S3 team had to make several nontrivial changes to the S3 index protocol stack.<sup>25</sup> Because these changes were difficult to get right at S3 scale, and the team wanted to deliver strong consistency with high confidence in correctness, they used P to formally model and validate the protocol design. P helped eliminate several design-level bugs early in the development process and allowed the team to deliver risky optimizations with confidence, as they could be validated using model checking.</p> 
  <p>In 2023, the P team at AWS built PObserve, which provides a new tool for validating the correctness of distributed systems both during testing and in production. With PObserve, we take structured logs from the execution of distributed systems and validate post-hoc that they match behaviors allowed by the formal P specification of the system. This allows for bridging the gap between the P specification of the system design and the production implementation (typically in languages like Rust or Java). While there are significant benefits from verifying protocols at design time, runtime monitoring of the same properties for the implementation makes the investment in formal specification much more valuable and addresses classic concerns with the deployment of formal methods in practice (i.e., connecting design-time validation with system implementation).</p> 
   
  <h3>Lightweight Formal Methods</h3> 
  <p>Another way that AWS has brought formal methods closer to its engineering teams is through the adoption of <i>lightweight formal methods</i>.</p> 
   
  <h4>Property-based testing</h4> 
  <p>The most notable single example of leveraging light-weight formal method is in Amazon S3's ShardStore, where the team used property-based testing throughout the development cycle both to test correctness and to speed up the development (described in detail by Bornholt, et al.<sup>4</sup>). The key idea in their approach was combining property-based testing with developer-provided correctness specifications, coverage-guided fuzzing (an approach where the distribution of inputs is guided by code coverage metrics), failure injection (where hardware and other system failures are simulated during testing), and minimization (where counterexamples are automatically reduced to aid human-guided debugging).</p> 
   
  <h4>Deterministic simulation </h4> 
  <p>Another lightweight method widely used at AWS is deterministic simulation testing, in which a distributed system is executed on a single-threaded simulator with control over all sources of randomness such as thread scheduling, timing, and message delivery order. Tests are then written for particular failure or success scenarios, such as the failure of a participant at a particular stage in a distributed protocol. The nondeterminism in the system is controlled by the test framework, allowing developers to specify orderings that they believe are interesting (such as ones that have caused bugs in the past). The scheduler in the testing framework can also be extended for fuzzing of orderings or exploring all possible orderings to be tested. </p> 
  <p>Deterministic simulation testing moves testing of system properties, like behavior under delay and failure, closer to build time instead of integration testing. This accelerates development and provides for more complete behavioral coverage during testing. Some of the work done at AWS on build-time testing of thread ordering and systems failures has been open-sourced as part of the shuttle (<a href="https://github.com/awslabs/shuttle">https://github.com/awslabs/shuttle</a>) and turmoil (<a href="https://github.com/tokio-rs/turmoil">https://github.com/tokio-rs/turmoil</a>) projects.</p> 
   
  <h4>Continuous fuzzing or random test-input generation</h4> 
  <p>Continuous fuzzing, especially coverage-guided scalable test-input generation, is also effective for testing systems correctness at integration time. During the development of Amazon Aurora's data-sharding feature (Aurora Limitless Database<sup>3),</sup> for example, we made extensive use of fuzzing to test two key properties of the system. First, by fuzzing SQL queries (and entire transactions), we validated that the logic partitioning SQL execution over shards is correct. Large volumes of random SQL schemas, datasets, and queries are synthesized and run through the engines under test, and the results compared with an oracle based on the nonsharded version of the engine (as well as other approaches to validation, like those pioneered by SQLancer<sup>23</sup>). </p> 
  <p>Fuzzing, combined with fault injection testing, is also useful for testing other aspects of database correctness such as atomicity, consistency, and isolation. In database testing, transactions are automatically generated, their correct behavior is defined using a formally specified correctness oracle, and then all possible interleaving of transactions and statements within the transaction are executed against the system under test. We also use post-hoc validation of properties like isolation (following approaches such as Elle<sup>13).</sup></p> 
   
  <h3>Fault Injection as a Service</h3> 
  <p>In early 2021 AWS launched FIS (Fault Injection Service)<sup>2</sup> with the goal of making testing based on fault injection accessible to a wide range of AWS customers. FIS allows customers to inject simulated faults, from API errors to I/O pauses and failed instances, into test or production deployments of their infrastructure on AWS. Injecting faults allows customers to validate that the resiliency mechanisms they have built into their architectures (such as failovers and health checks) actually work to improve availability and do not introduce correctness problems. Fault-injection testing based on FIS is widely used by AWS customers, and internally within Amazon. For example, Amazon.com ran 733 FIS-based fault-injection experiments in preparation for Prime Day 2024.</p> 
  <p>In 2014, Yuan, et al. found that 92 percent of catastrophic failures in tested distributed systems were triggered by incorrect handling of nonfatal errors. Many distributed-systems practitioners who were told about this research were surprised the percentage wasn't higher. Happy-case catastrophic failures are rare simply because the happy case of systems is executed often, tested better (both implicitly and explicitly), and is significantly simpler than the error cases. Fault-injection testing and FIS make it much easier for practitioners to test the behavior of their systems under faults and failures, closing the gap between happy-case and error-case bug density.</p> 
  <p>While fault injection is not considered a formal method, it can be combined with formal specifications. Defining the expected behavior using a formal specification, and then comparing results during and after fault injection to the specified behavior, allows for catching a lot more bugs than simply checking for errors in metrics and logs (or having a human look and say, "Yup, that looks about right").</p> 
   
  <h3>Metastability and Emergent System Behavior</h3> 
  <p>Over the past decade, there has been an emerging interest in a particular class of systems failure: those where some triggering event (like an overload or a cache emptying) causes a distributed system to enter a state where it doesn't recover without intervention (such as reducing load below normal). This class of failures, dubbed <i>metastable failures</i>,<sup>5</sup> is one of the most important contributors to unavailability in cloud systems. Figure 1 (adapted from Bronson, et al.<sup>5</sup>) illustrates a common type of metastable behavior: Load increases on the system are initially met with increasing goodput, followed by saturation, followed by congestion and goodput dropping to zero (or near zero). From there, the system cannot return to healthy state by slightly reducing load. Instead, it must follow the dotted line and may not recover until load is significantly reduced. This type of behavior is present in even simple systems. For example, it can be triggered in most systems with timeout-and-retry client logic.</p> 
  <p><img src="https://dl.acm.org/cms/attachment/html/10.1145/3712057/assets/html/brooker1.png" alt="Systems Correctness Practices at AWS"></p> 
  <p>Traditional formal approaches to modeling distributed systems typically focus on <i>safety</i> (nothing bad happens) and <i>liveness</i> (something good eventually happens), but metastable failures remind us that systems have a variety of behaviors that cannot be neatly categorized this way. We have increasingly turned to discrete-event simulation to understand the emergent behavior of systems, investing both in custom-built systems simulations and tooling that allow the use of existing system models (built in languages like TLA+ and P) to simulate system behavior. Extending exhaustive model checkers, like TLA+'s TLC with probabilistic simulations, also allows for the generation of statistical results such as posterior latency distributions, making model checking useful for tasks like understanding the achievability of latency SLAs (service-level agreements).</p> 
   
  <h3>Formal Proof</h3> 
  <p>In some cases, the formal methods enumerated so far in this article are not sufficient. For critical security boundaries such as authorization and virtualization, for example, proofs of correctness can be both desirable and worth the significant investment needed to create them.</p> 
  <p>In 2023, AWS introduced the Cedar authorization policy language for writing policies that specify fine-grained permissions. Cedar was designed for automated reasoning and formal proof.<sup>12,24</sup> The language was designed to be well-suited for proof, and the implementation was built in the verification-aware programming language Dafny. Using Dafny, the team was able to prove that the implementation satisfies a variety of security properties. This type of proof goes beyond testing. It is a proof in the mathematical sense. The team also applied a differential testing approach using the Dafny code as a correctness oracle to verify the correctness of the production-ready Rust implementation. Publishing the Dafny code and test procedures as open source, along with the Cedar implementation, allows Cedar users to check the team's work on correctness.</p> 
  <p>Another example is the Firecracker VMM (virtual machine monitor). Firecracker uses a low-level protocol called <i>virtio</i> to expose emulated hardware devices (such as a network card or solid-state drive) to guest kernels running inside the VM. This emulated device is a critical security boundary because it is the most complex interaction between the untrusted guest and trusted host. The Firecracker team used a tool called Kani<sup>20</sup> that is able to reason formally about Rust code to prove key properties of this security boundary. Again, proof here goes beyond testing and ensures that the critical properties of this boundary are held no matter what the guest attempts to do.</p> 
  <p>Proofs around the behaviors of programs are an important part of AWS's software correctness program, and so we support development on tools such as Kani, Dafny,<sup>18</sup> and Lean,<sup>16</sup> and the underlying tools—like SMT (satisfiability modulo theories) solvers—that power them.</p> 
  <p>The ability to use formal models and specifications—for model-checking systems at design time, for validating in-production behavior using runtime monitoring by serving as a correctness oracle, for simulating emergent systems behavior, and for building proofs of critical properties—allows AWS to amortize the engineering effort of developing these specifications over a larger amount of business and customer value.</p> 
   
  <h3>Benefits Beyond Correctness</h3> 
  <p>Finally, as discussed in the aforementioned 2015 paper, formal methods are a crucial part of safely improving the performance of cloud systems. Modeling a key commit protocol for the Aurora relational database engine in P and TLA+ allowed us to identify an opportunity to reduce the cost of distributed commits from two to 1.5 network roundtrips without sacrificing any safety properties. These kinds of stories are usual for teams that adopt formal methods, driven by at least two different dynamics. </p> 
  <p>First, the act of deeply thinking about and formally writing down distributed protocols forces a structured way of thinking that leads to deeper insights about the structure of protocols and the problem to be solved. </p> 
  <p>Second, having the ability to formally check (and, in some cases, prove) that proposed design optimizations are correct allows naturally conservative distributed-systems engineers to be bolder in their protocol design choices without increasing risk and boosting the developer velocity toward delivering reliable services.</p> 
  <p>These productivity and cost benefits are limited not only to high-level design optimizations but also to low-level code that normally gets ignored. In one example, the AWS team identified optimizations to the implementation of the RSA (Rivest-Shamir-Adleman) public-key encryption scheme on our ARM-based Graviton 2 processor, which could improve throughput by up to 94 percent.<sup>17</sup> </p> 
  <p>Using the HOL Light interactive theorem prover, the team was able to prove the correctness of these optimizations. Given the high percentage of cloud CPU cycles spent on cryptography, this type of optimization can significantly reduce infrastructure costs and aid sustainability while at the same time improving customer-visible performance.</p> 
   
  <h3>Challenges and Opportunities for the Future</h3> 
  <p>Despite significant success in scaling formal and semi-formal testing methods across AWS over the past 15 years, several challenges persist, particularly in industrial adoption of formal methods. The primary barriers for formal methods tools include their steep learning curve and the specialized domain expertise required. Additionally, many of these tools remain academic in nature and lack user-friendly interfaces.</p> 
  <p>Even well-established semi-formal approaches face adoption challenges. For example, deterministic simulation, a key distributed-systems testing technique used successfully at AWS and in projects like FoundationDB, remains unfamiliar to many experienced distributed-systems developers joining AWS. Similar gaps exist in the adoption of other proven methodologies such as fault-injection testing, property-based testing, and fuzzing. The challenge is educating distributed-systems developers about these testing methods and tools, teaching the art of rigorous thinking.</p> 
  <p>The education gap begins at the academic level, where even basic formal reasoning approaches are rarely taught, making it difficult for graduates from top institutions to adopt these tools. Although formal methods and automated reasoning are crucial for industry applications, they continue to be viewed as niche fields. We anticipate that increased industry adoption of formal methods and automated reasoning will attract more talent to this domain.</p> 
  <p>Metastability and other emergent properties of large-scale systems represent another critical research area facing similar awareness challenges. Common practices that lead to metastable system behavior, such as "retry N times on timeout," continue to be widely recommended despite their known issues. Current tools and techniques for understanding emergent system behavior are still in their early stages, making system stability modeling both expensive and complex. The ongoing research in this area holds promising potential for advancement.</p> 
  <p>Looking ahead, we believe large language models and AI assistants will significantly help address the adoption challenges of formal methods in practice. Just as AI-assisted unit testing has gained popularity, these tools are expected soon to help developers create formal models and specifications, making these advanced techniques more accessible to the broader developer community.</p> 
   
  <h3>Conclusion</h3> 
  <p>Building reliable and secure software requires a range of approaches to reason about systems correctness. Alongside industry-standard testing methods (such as unit and integration testing), AWS has adopted model checking, fuzzing, property-based testing, fault-injection testing, deterministic simulation, event-based simulation, and runtime validation of execution traces. Formal methods have been an important part of the development process—perhaps most importantly, formal specifications as test oracles that provide the correct answers for many of AWS's testing practices. Correctness testing and formal methods remain key areas of investment at AWS, accelerated by the excellent returns seen on investments in these areas already.</p> 
   
  <h4>References</h4> 
  <p>1. Amazon Web Services. 2020. Amazon S3 now delivers strong read-after-write consistency automatically for all applications; <a href="https://aws.amazon.com/about-aws/whats-new/2020/12/amazon-s3-now-delivers-strong-read-after-write-consistency-automatically-for-all-applications">https://aws.amazon.com/about-aws/whats-new/2020/12/amazon-s3-now-delivers-strong-read-after-write-consistency-automatically-for-all-applications</a>. (Announces automatic strong read-after-write consistency for all S3 operations across all AWS regions.)</p> 
  <p>2. Amazon Web Services. 2021. Announcing general availability of AWS Fault Injection Simulator, a fully managed service to run controlled experiments; <a href="https://aws.amazon.com/about-aws/whats-new/2021/03/aws-announces-service-aws-fault-injection-simulator/">https://aws.amazon.com/about-aws/whats-new/2021/03/aws-announces-service-aws-fault-injection-simulator/</a>.</p> 
  <p>3. Amazon Web Services. 2023. Announcing Amazon Aurora Limitless Database; <a href="https://aws.amazon.com/about-aws/whats-new/2023/11/amazon-aurora-limitless-database/">https://aws.amazon.com/about-aws/whats-new/2023/11/amazon-aurora-limitless-database/</a>.</p> 
  <p>4. Bornholt, J., Joshi, R., Astrauskas, V., Cully, B., Kragl, B., Markle, S., Sauri, K., Schleit, D., Slatton, G., Tasiran, S., Van Geffen, J., Warfield, A. 2021. Using lightweight formal methods to validate a key-value storage node in Amazon S3. In <i>Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles</i>, 836–850; <a href="https://dl.acm.org/doi/10.1145/3477132.3483540">https://dl.acm.org/doi/10.1145/3477132.3483540</a>.</p> 
  <p>5. Bronson, N., Aghayev, A., Charapko, A., Zhu, T. 2021. Metastable failures in distributed systems. In <i>Proceedings of the Workshop on Hot Topics in Operating Systems</i>, 221–227; <a href="https://dl.acm.org/doi/10.1145/3458336.3465286">https://dl.acm.org/doi/10.1145/3458336.3465286</a>.</p> 
  <p>6. Claessen, K., Hughes, J. 2000. QuickCheck: A lightweight tool for random testing of Haskell programs. In <i>Proceedings of the Fifth ACM SIGPLAN International Conference on Functional Programming</i>, 268–279; <a href="https://dl.acm.org/doi/10.1145/351240.351266">https://dl.acm.org/doi/10.1145/351240.351266</a>. </p> 
  <p>7. de Moura, L., Ullrich, S. 2021. The Lean 4 theorem prover and programming language. In <i>Automated Deduction – CADE 28</i> (28th International Conference on Automated Deduction, volume 12699 of <i>Lecture Notes in Computer Science</i>, 625–635. Springer; <a href="https://dl.acm.org/doi/10.1007/978-3-030-79876-5_37">https://dl.acm.org/doi/10.1007/978-3-030-79876-5_37</a>.</p> 
  <p>8. Desai, A., Gupta, V., Jackson, E., Qadeer, S., Rajamani, S., Zufferey, D. 2013. P: safe asynchronous event-driven programming. <i>ACM SIGPLAN Notices</i> 48(6), 321–332; <a href="https://dl.acm.org/doi/10.1145/2499370.2462184">https://dl.acm.org/doi/10.1145/2499370.2462184</a>.</p> 
  <p>9. Fioraldi, A., Maier, D., Eißfeldt, H., Heuse, M. 2020. AFL++: Combining incremental steps of fuzzing research. In <i>14th Usenix Workshop on Offensive Technologies</i>; <a href="https://www.usenix.org/conference/woot20/presentation/fioraldi">https://www.usenix.org/conference/woot20/presentation/fioraldi</a>.</p> 
  <p>10. Harrison, J. 2009. HOL Light: an overview. In <i>Proceedings of the 22nd International Conference on Theorem Proving in Higher Order Logics</i>, Munich, Germany, ed., S. Berghofer, T. Nipkow, C. Urban, and M. Wenzel, 60–66, volume 5674 of <i>Lecture Notes in Computer Science</i>. Springer-Verlag; <a href="https://link.springer.com/chapter/10.1007/978-3-642-03359-9_4">https://link.springer.com/chapter/10.1007/978-3-642-03359-9_4</a>.</p> 
  <p>11. Havelund, K., Rosu, G. 2019. Runtime verification – 17 years later. In <i>Runtime Verification</i>, 18th International Conference, RV 2018, Limassol, Cyprus, ed., C. Colombo and M. Leucker, 3–17, volume 11237 of <i>Lecture Notes in Computer Science</i>. Springer; <a href="https://link.springer.com/book/10.1007/978-3-030-03769-7">https://link.springer.com/book/10.1007/978-3-030-03769-7</a>.</p> 
  <p>12. Hicks, M. 2023. How we built Cedar with automated reasoning and differential testing. Amazon Science; <a href="https://www.amazon.science/blog/how-we-built-cedar-with-automated-reasoning-and-differential-testing">https://www.amazon.science/blog/how-we-built-cedar-with-automated-reasoning-and-differential-testing</a>.</p> 
  <p>13. Kingsbury, K., Alvaro, P. 2020. Elle: inferring isolation anomalies from experimental observations. <i>Proceedings of the VLDB Endowment</i> 14(3), 268–280; <a href="https://dl.acm.org/doi/10.14778/3430915.3430918">https://dl.acm.org/doi/10.14778/3430915.3430918</a>.</p> 
  <p>14. Lamport, L. 2002. <i>Specifying Systems: The TLA+ Language and Tools for Hardware and Software Engineers</i>. Addison-Wesley Professional.</p> 
  <p>15. Lamport, L. 2015. Who builds a house without drawing blueprints? <i>Communications of the ACM</i> 58(4), 38–41; <a href="https://dl.acm.org/doi/10.1145/2736348">https://dl.acm.org/doi/10.1145/2736348</a>.</p> 
  <p>16. Lean Prover Community. 2024. Lean 4. GitHub; <a href="https://github.com/leanprover/lean4">https://github.com/leanprover/lean4</a>.</p> 
  <p>17. Lee, J., Becker, H., Harrison, J. 2024. Formal verification makes RSA faster—and faster to deploy. Amazon Science; <a href="https://www.amazon.science/blog/formal-verification-makes-rsa-faster-and-faster-to-deploy">https://www.amazon.science/blog/formal-verification-makes-rsa-faster-and-faster-to-deploy</a>.</p> 
  <p>18. Leino, K. R. M. 2010. Dafny: an automatic program verifier for functional correctness. In <i>Proceedings of the 16th International Conference on Logic for Programming, Artificial Intelligence, and Reasoning</i>, 348–370. Springer-Verlag; <a href="https://dl.acm.org/doi/10.5555/1939141.1939161">https://dl.acm.org/doi/10.5555/1939141.1939161</a>.</p> 
  <p>19. MacIver, D. R., Hatfield-Dodds, Z., et al. 2019. Hypothesis: a new approach to property-based testing. <i>Journal of Open Source Software</i> 4(43), 1891; <a href="https://joss.theoj.org/papers/10.21105/joss.01891.pdf">https://joss.theoj.org/papers/10.21105/joss.01891.pdf</a>.</p> 
  <p>20. Monteiro, F., Roy, P. 2023. Using Kani to validate security boundaries in AWS Firecracker. Technical report, Amazon Web Services; <a href="https://model-checking.github.io/kani-verifier-blog/2023/08/31/using-kani-to-validate-security-boundaries-in-aws-firecracker.html">https://model-checking.github.io/kani-verifier-blog/2023/08/31/using-kani-to-validate-security-boundaries-in-aws-firecracker.html</a>.</p> 
  <p>21. Newcombe, C., Rath, T., Zhang, F., Munteanu, B., Brooker, M., Deardeuff, M. 2015. How Amazon Web Services uses formal methods. <i>Communications of the ACM </i>58(4), 66–73. <a href="https://dl.acm.org/doi/10.1145/2699417">https://dl.acm.org/doi/10.1145/2699417</a>.</p> 
  <p>22. P Team. 2024. P: a programming language for formal specification of distributed systems. <a href="https://github.com/p-org/P">https://github.com/p-org/P</a>. (Used extensively in AWS and Microsoft for formal verification of distributed systems.)</p> 
  <p>23. Rigger, M., Su, Z. 2020. Testing database engines via pivoted query synthesis. In 14th Usenix Symposium on Operating Systems Design and Implementation, Article 38, 667–682; <a href="https://dl.acm.org/doi/10.5555/3488766.3488804">https://dl.acm.org/doi/10.5555/3488766.3488804</a>. </p> 
  <p>24. Rungta, N. 2024. Trillions of formally verified authorizations a day! Splash keynote; <a href="https://2024.splashcon.org/details/splash-2024-keynotes/3/Trillions-of-Formally-Verified-Authorizations-a-day-">https://2024.splashcon.org/details/splash-2024-keynotes/3/Trillions-of-Formally-Verified-Authorizations-a-day-</a>.</p> 
  <p>25. Vogels, W. 2021. Diving deep on S3 consistency. All Things Distributed; <a href="https://www.allthingsdistributed.com/2021/04/s3-strong-consistency.html">https://www.allthingsdistributed.com/2021/04/s3-strong-consistency.html</a>. (Blog post describing the implementation of strong consistency in Amazon S3.)</p> 
   
  <p><b>Marc&nbsp;Brooker&nbsp;</b>is a Distinguished Engineer at Amazon Web Services, where he focusses on AI and databases. He's interested in distributed systems, serverless, systems correctness, and formal methods. Marc holds a Ph.D. in Electrical Engineering from the University of Cape Town.</p> 
  <p><b>Ankush Desai </b>is a Principal Applied Scientist at Amazon Web Services, where he focusses on building tools and techniques that help developers deliver distributed services with high assurance of correctness. He's interested in improving developer productivity, formal methods, systematic testing, fuzzing, and distributed systems. Ankush holds a PhD in Computer Science from the University of California, Berkeley.</p> 
  <p>Copyright © 2024 held by owner/author. Publication rights licensed to ACM.</p>  
  

	<div>
	
		<p><img src="https://queue.acm.org/img/q%20stamp_small.jpg" width="26" height="45" alt="acmqueue"></p><p>
	
	<em>Originally published in Queue vol. 22, no. 6</em>—
 	<br>
	Comment on this article in the <a href="http://portal.acm.org/citation.cfm?id=3712057">ACM Digital Library</a></p></div>
	



<br>
<!--
<a href="https://twitter.com/share" class="twitter-share-button" data-via="ACMQueue">Tweet</a>
-->


<br>

<!--
<fb:like></fb:like>
-->

<br>



<!-- these get hooked up to js events -->


<!-- FB Like -->
<!--
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "connect.facebook.net/en_US/all.js#xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

<div id="fb-root"></div>
-->

<!-- Place this tag after the last +1 button tag. -->

<!--
<script type="text/javascript">
  (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/plusone.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script>

<br />
<script src="https://connect.facebook.net/en_US/all.js#xfbml=1"></script>

<script>
FB.Event.subscribe('edge.create', function(targetUrl) {
  _gaq.push(['_trackSocial', 'facebook', 'like', targetUrl]);
});
</script>
-->



<hr noshade="" size="1"><p>




More related articles:

	  </p><p>
	  <span>Achilles Benetopoulos</span> - <a href="https://queue.acm.org/detail.cfm?id=3712258"><b>Intermediate Representations for the Datacenter Computer</b></a>
	  <br>
	  We have reached a point where distributed computing is ubiquitous. In-memory application data size is outstripping the capacity of individual machines, necessitating its partitioning over clusters of them; online services have high availability requirements, which can be met only by deploying systems as collections of multiple redundant components; high durability requirements can be satisfied only through data replication, sometimes across vast geographical distances.
	  </p>
	  

	  <p>
	  <span>David R. Morrison</span> - <a href="https://queue.acm.org/detail.cfm?id=3711677"><b>Simulation: An Underutilized Tool in Distributed Systems</b></a>
	  <br>
	  Simulation has a huge role to play in the advent of AI systems: We need an efficient, fast, and cost-effective way to train AI agents to operate in our infrastructure, and simulation absolutely provides that capability.
	  </p>
	  

	  <p>
	  <span>Matt Fata, Philippe-Joseph Arida, Patrick Hahn, Betsy Beyer</span> - <a href="https://queue.acm.org/detail.cfm?id=3264508"><b>Corp to Cloud: Google’s Virtual Desktops</b></a>
	  <br>
	  Over one-fourth of Googlers use internal, data-center-hosted virtual desktops. This on-premises offering sits in the corporate network and allows users to develop code, access internal resources, and use GUI tools remotely from anywhere in the world. Among its most notable features, a virtual desktop instance can be sized according to the task at hand, has persistent user storage, and can be moved between corporate data centers to follow traveling Googlers. Until recently, our virtual desktops were hosted on commercially available hardware on Google’s corporate network using a homegrown open-source virtual cluster-management system called Ganeti. Today, this substantial and Google-critical workload runs on GCP (Google Compute Platform).
	  </p>
	  

	  <p>
	  <span>Pat Helland</span> - <a href="https://queue.acm.org/detail.cfm?id=3025012"><b>Life Beyond Distributed Transactions</b></a>
	  <br>
	  This article explores and names some of the practical approaches used in the implementation of large-scale mission-critical applications in a world that rejects distributed transactions. Topics include the management of fine-grained pieces of application data that may be repartitioned over time as the application grows. Design patterns support sending messages between these repartitionable pieces of data.
	  </p>
	  <br>


<hr noshade="" size="1">





<hr noshade="" size="1">

	<p>
	<a href="#"><img src="https://queue.acm.org/img/logo_acm.gif"></a>
	<br>
	© ACM, Inc. All Rights Reserved.
	</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Why hasn’t AMD made a viable CUDA alternative? (161 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43547309</link>
            <guid>43547309</guid>
            <pubDate>Tue, 01 Apr 2025 14:37:54 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43547309">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="43547461"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547461" href="https://news.ycombinator.com/vote?id=43547461&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>There is more than one way to answer this.</p><p>They have made an alternative to the CUDA language with HIP, which can do most of the things the CUDA language can.</p><p>You could say that they haven't released supporting libraries like cuDNN, but they are making progress on this with AiTer for example.</p><p>You could say that they have fragmented their efforts across too many different paradigms but I don't think this is it because Nvidia also support a lot of different programming models.</p><p>I think the reason is that they have not prioritised support for ROCm across all of their products. There are too many different architectures with varying levels of support. This isn't just historical. There is no ROCm support for their latest AI Max 395 APU. There is no nice cross architecture ISA like PTX. The drivers are buggy. It's just all a pain to use. And for that reason "the community" doesn't really want to use it, and so it's a second class citizen.</p><p>This is a management and leadership problem. They need to make using their hardware easy. They need to support all of their hardware. They need to fix their driver bugs.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547568"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547568" href="https://news.ycombinator.com/vote?id=43547568&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>This ticket, finally closed after being open for <i>2 years</i>, is a pretty good micocosm of this problem:</p><p><a href="https://github.com/ROCm/ROCm/issues/1714" rel="nofollow">https://github.com/ROCm/ROCm/issues/1714</a></p><p>Users complaining that the docs don't even specify which cards work.</p><p>But it goes deeper - a valid complaint is that "this only supports one or two consumer cards!" A common rebuttal is that it works fine on lots of AMD cards if you set some environment flag to force the GPU architecture selection.  The fact that this is <i>so close</i> to working on a wide variety of hardware, and yet doesn't, is exactly the vibe you get with the whole ecosystem.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549097"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549097" href="https://news.ycombinator.com/vote?id=43549097&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>What I don't get is why they don't at least assign a dev or two to make the poster child of this work: llama.cpp</p><p>It's the first thing anyone tries when trying to dabble in AI or compute on the gpu, yet it's a clusterfuck to get to work. A few blessed cards work, with proper drivers and kernel; others just crash, perform horribly slow, or output GGGGGGGGGGGGGG to every input (I'm not making this up!) Then you LOL, dump it and go buy nvidia et voila, stuff works first try.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548203"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548203" href="https://news.ycombinator.com/vote?id=43548203&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I suspect part of it is also that Nvidia actually does a lot of things in firmware that can be upgraded. The new Nvidia Linux drivers (the "open" ones) support Turing cards from 2018. That means chips that old already do much of the processing in firmware.</p><p>AMD keeps having issues because their drivers talk to the hardware directly so their drivers are massive bloated messes, famous for pages of auto-generated register definitions. Likely it's much more difficult to fix anything.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547940"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547940" href="https://news.ycombinator.com/vote?id=43547940&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Geez. If I were Berkshire Hathaway looking to invest in the GPU market, this would be a major red flag in my fundamentals analysis.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547700"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547700" href="https://news.ycombinator.com/vote?id=43547700&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I had a similar (I think) experience when building LLVM from source a few years ago.</p><p>I kept running into some problem with LLVM's support for HIP code, even though I had not interest in having that functionality.</p><p>I realize this isn't exactly an AMD problem, but IIRC it was they were who contributed the troublesome code to LLVM, and it remained unfixed.</p><p>Apologies if there's something unfair or uninformed in what I wrote, it's been a while.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547988"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547988" href="https://news.ycombinator.com/vote?id=43547988&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>That reeks of gross incompetence somewhere in the organization. Like a hosting company that has a customer dealing with very poor performance, over pays greatly to avoid it while the whole time nobody even thinks to check what the linux swap file is doing.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547799"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547799" href="https://news.ycombinator.com/vote?id=43547799&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt;This is a management and leadership problem.</p><p>It's easy (and mostly correct) to blame management for this, but it's such a foundational issue that even if everyone up to the CEO pivoted on every topic, it wouldn't change anything. They simply don't have the engineering talent to pull this off, because they somehow concluded that making stuff open source means someone else will magically do the work for you. Nvidia on the other hand has accrued top talent for more than a decade and carefully developed their ecosystem to reach this point. And there are only so many talented engineers on the planet. So even if AMD leadership wakes up tomorrow, they won't go anywhere for a looong time.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547827"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547827" href="https://news.ycombinator.com/vote?id=43547827&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; This is a management and leadership problem. They need to make using their hardware easy. They need to support all of their hardware. They need to fix their driver bugs.</p><p>Yes. This kind of thing is unfortunately endemic in hardware companies, which don't "get" software. It's cultural and requires (a) a leader who does Get It and (b) one of those Amazon memos stating "anyone who does not Get With The Program will be fired".</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547675"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547675" href="https://news.ycombinator.com/vote?id=43547675&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>It is a little bit more complicated than ROCm simply not having support because ROCm has at a point claimed support, and they've had to walk it back painfully (multiple times). Its not a driver issue, nor a hardware issue on their side.</p><p>There has been a long-standing issue between AMD and its mainboard manufacturers. The issue has to do with features required for ROCm, namely PCIe Atomics. AMD has been unable or unwilling to hold the mainboard manufacturers to account for advertising features the mainboard does not support.</p><p>The CPU itself must support this feature, but the mainboard must as well (in firmware).</p><p>One of the reasons why ROCm hasn't worked in the past is because the mainboard manufacturers have claimed and advertised support for PCIe Atomics, and the support they've claimed has been shown to be false, and the software fails in non-deterministic ways when tested. This is nightmare fuel for the few AMD engineers tasked with ROCm.</p><p>PCIe Atomics requires non-translated direct IO to operate correctly, and in order to support the same CPU models from multiple generations they've translated these IO lines in firmware.</p><p>This has left most people that query their system to check this showing PCIAtomics is supported, while when actual tests that rely on that support are done they fail, in chaotic ways. There is no technical specification or advertising that the mainboard manufacturers provide showing whether this is supported. Even the boards with multiple x16 slots and the many technologies related to it such as Crossfire/SLI/mGPU brandings these don't necessarily show whether PCIAtomics is properly supported.</p><p>In other words, the CPU is supported, the firmware/mainboard fail with no way to differentiate between the two at the upper layers of abstraction.</p><p>All in all. You shouldn't be blaming AMD for this. You should be blaming the three mainboard manufacturers who chose to do this. Some of these manufacturers have upper end boards where they actually did do this right they just chose to not do this for any current gen mainboard costing less than ~$300-500.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549200"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549200" href="https://news.ycombinator.com/vote?id=43549200&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>There are so many hardware certification programs out there, why doesn't AMD run one to fix this?</p><p>Create a "ROCm compatible" logo and a list of criteria. Motherboard manufacturers can send a pre-production sample to AMD along with a check for some token amount (let's say $1000). AMD runs a comprehensive test suite to check actual compatibility, if it passes the mainboard is allowed to be advertised and sold with the previously mentioned logo. Then just tell consumers to look for that logo if they want to use ROCm. If things go wrong on a mainboard without the certification, communicate that it's probably the mainboard's fault.</p><p>Maybe add some kind of versioning scheme to allow updating requirements in the future</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43549341"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549341" href="https://news.ycombinator.com/vote?id=43549341&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>AIUI, AMD documentation claims that the requirement for PCIe Atomics is due to ROCm being based on Heterogeneous System Architecture, <a href="https://en.wikipedia.org/wiki/Heterogeneous_System_Architecture" rel="nofollow">https://en.wikipedia.org/wiki/Heterogeneous_System_Architect...</a> which allows for a sort of "unified memory" (strictly speaking, a unified address space) across CPU and GPU RAM.  Other compute API's such as CUDA, OpenCL, SYCL or Vulkan Compute don't have HSA as a strict requirement but ROCm apparently does.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547796"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547796" href="https://news.ycombinator.com/vote?id=43547796&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Look, this sounds like a frustrating nightmare, but the way it seems to us consumers is that AMD chose to rely on poorly implemented and supported technology, and Nvidia didn't. I can't blame AMD for the poor support by motherboards manufacturers but I can and will blame AMD for relying on it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548795"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43548795" href="https://news.ycombinator.com/vote?id=43548795&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>While we won't know for sure, unless someone from AMD comments on this; in fairness there may not have been any other way.</p><p>Nvidia has a large number of GPU related patents.</p><p>The fact that AMD chose to design their system this way, in such a roundabout and brittle manner, which is contrary to how engineer's approach things, may have been a direct result of being unable to design such systems any other way because of broad patents tied to the interface/GPU.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549136"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43549136" href="https://news.ycombinator.com/vote?id=43549136&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I feel like this issue is to at least some extent a red herring. Even accepting that  ROCm doesn't work on some motherboards, this can't explain why so few of AMD's GPUs have official ROCm support.</p><p>I notice that at one point there was a ROCm release which said it didn't require atomics for gfx9 GPUs, but the requirement was reintroduced in a later version of ROCm. Not sure what happened there but this seems to suggest AMD might have had a workaround at some point (though possibly it didn't work).</p><p>If this really is due to patent issues AMD can likely afford to licence or cross-license the patent given potential upside.</p><p>It would be in line with other decisions taken by AMD if they took this decision because it works well with their datacentre/high-end GPUs, and they don't (or didn't) really care about offering GPGPU to the mass/consumer GPU market.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549331"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_43549331" href="https://news.ycombinator.com/vote?id=43549331&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; I feel like this issue is to at least some extent a red herring.</p><p>I don't see that, these two issues adequately explain why so few GPUs have official support. They don't want to get hit with a lawsuit, as a result of issues outside their sphere of control.</p><p>&gt; If this really is due to patent issues AMD can likely afford to license or cross-license the patent given potential upside.</p><p>Have you ever known any company willing to cede market dominance and license or cross-license a patent letting competition into a market that they hold an absolute monopoly over, let alone in an environment where antitrust is non-existent and fang-less?</p><p>There is no upside for NVIDIA to do that. If you want to do serious AI/ML work you currently need to use NVIDIA hardware, and they can charge whatever they want for that.</p><p>The moment you have a competitor, demand is halved at a bare minimum depending on how much the competitor undercuts you by. Any agreement on coordinating prices leads to price-fixing indictments.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43547751"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547751" href="https://news.ycombinator.com/vote?id=43547751&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>How does NVIDIA manage this issue? I wonder whether they have a very different supply chain or just design software that puts less trust in the reliability of those advertised features.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549279"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43549279" href="https://news.ycombinator.com/vote?id=43549279&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I should point out here, if nobody has already; Nvidia's GPU designs are <i>extremely</i> complicated compared to what AMD and Apple ship. The "standard" is to ship a PCIe card with display handling drivers and some streaming multiprocessor hardware to process your framebuffers. Nvidia goes even further by adding additional accelerators (ALUs by way of CUDA core and tensor cores), onboard RTOS management hardware (what Nvidia calls GPU System Processor), and more complex userland drivers that very well might be able to manage atomics without any PCIe standards.</p><p>This is also one of the reasons AMD and Apple can't simply turn their ship around right now. They've both invested heavily in simplifying their GPU and removing a lot of the creature-comforts people pay Nvidia for. 10 years ago we could at least all standardize on OpenCL, but these days it's all about proprietary frameworks and throwing competitors under the bus.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547816"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547816" href="https://news.ycombinator.com/vote?id=43547816&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Its an open question they have never answered afaik.</p><p>I would speculate that their design is self-contained in hardware.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547777"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547777" href="https://news.ycombinator.com/vote?id=43547777&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>So .. how's Nvidia dealing with this? Or do they benefit from motherboard manufacturers doing preferential integration testing?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43549371"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43549371" href="https://news.ycombinator.com/vote?id=43549371&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I made a post here a while back suggesting an investment strategy of spending one billion on AMD shares and one billion on software developers to 3rd party write a quality support stack for their hardware. I'm still not sure if its a crazy idea.</p><p>Actually it might be better to spend 1B on shares and 10x 100M on development and take ten attempts in parallel and use the best of them.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547919"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547919" href="https://news.ycombinator.com/vote?id=43547919&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I want to argue that graphics cards are really 3 markets: integrated, gaming (dedicated), and compute. Not only do these have different hardware (fixed function, ray tracing cores, etc.) but also different programming and (importantly) distribution models. NVIDIA went from 2 to 3. Intel went from 1 to 2, and bought 3 (trying to merge). AMD started with 2 and went to 1 (around Llano) and attempted the same thing as NVIDIA via GCN (please correct me if I'm wrong).</p><p>My understanding is that the reason is that the real market for 3 (GPUs for compute) didn't show up until very late, so AMD's GCN bet didn't pay off. Even in 2021, NVIDIA's revenue from gaming was above data center revenue (a segment they basically had no competition in, and 100% of their revenue was from CUDA). AMD meanwhile won the battle for Playstation and Xbox consoles, and was executing a turnaround in data centers with EPYC and CPUs (with Zen). So my guess as to why they might have underinvested is basically: for much of the 2010s they were just trying to survive, so they focused on battles they could win that would bring them revenue.</p><p>This high level prioritization would explain a lot of "misexecution", e.g. if they underhired for ROCm, or prioritized APU SDK experience over data center, their testing philosophy ("does this game work ok? great").</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547462"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547462" href="https://news.ycombinator.com/vote?id=43547462&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>They likely haven't put even close to enough money behind it. This isn't a unique situation - you'll see in corporate america a lot of CEOs who say "we are investing in X" and they really believe they are. But the required size is billions (like, hundreds of really insanely talented engineers being paid 500k-1m, lead by a few being paid $3-10m), and they are instead investing low 10's of millions.</p><p>They can't bring themselves to put so much money into it that it would be an obvious fail if it didn't work.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547563"><td></td></tr>
                <tr id="43547645"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547645" href="https://news.ycombinator.com/vote?id=43547645&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>The big players are all investing in building chips themselves.</p><p>And probably not putting enough money behind it... it takes enormous courage as a CEO to walk into a boardroom and say "I'm going to spend $50 billion, I think it will probably work, I'm... 60% certain".</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547778"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547778" href="https://news.ycombinator.com/vote?id=43547778&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>You're probably correct, but I feel like I have to raise the issue of Zuckerberg spending a comparable amount on VR which was much more speculative.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548018"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43548018" href="https://news.ycombinator.com/vote?id=43548018&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Zuck is founder and owner. So is Huang (Nvidia CEO). They call all the shots.</p><p>Whereas AMD's CEO was appointed, and can be fired. Huge difference in their risk appetite.</p><p>I'm reminded of pg's article "founder mode": <a href="https://paulgraham.com/foundermode.html" rel="nofollow">https://paulgraham.com/foundermode.html</a></p><p>I think some companies simply aren't capable of taking big risks and innovating in big ways, for this reason.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547864"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43547864" href="https://news.ycombinator.com/vote?id=43547864&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Zuckerberg owns Facebook though. It’s a lot easier to make bold decisions when you’re the majority shareholder.</p><p>Edit: though emphasis should be put on “easIER” because it’s still far from easy.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547935"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_43547935" href="https://news.ycombinator.com/vote?id=43547935&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>This. Without knowing the guy, he seems to be a) very comfortable taking a lot of risk and b) it's actually not that risky for him to blow $20 billion.</p><p>There aren't many cases like this. Larry/Sergey were more than comfortable risking $10 billion here and there.</p></div></td></tr>
        </tbody></table></td></tr>
                                    <tr id="43547934"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547934" href="https://news.ycombinator.com/vote?id=43547934&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>It amazes me how much these companies make actually gets spent on R&amp;D, you see the funnel charts on reddit and I am like what the hell. Microsoft only spends ~6bn USD on R&amp;D with a total 48bn of revenue and 15bn in profits?</p><p>What the hell is going on, they should be able to keep an army of PhDs doing pointless research even if only one paper in 10 years comes to a profitable product. But instead they are cutting down workforce like there is no tomorrow...</p><p>(I know, I know, market dynamics, value extraction, stock market returns)</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549373"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549373" href="https://news.ycombinator.com/vote?id=43549373&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>R and D in the financial statements I've seen basically covers the entire product, engineering etc org. Lots and lots of people, but not what regular people consider RnD.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548046"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548046" href="https://news.ycombinator.com/vote?id=43548046&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>well, look at Meta... they're spending Billions with a capital B on stuff and they get slaughtered every earnings call because it hasn't paid off yet. if Zuckerberg wasn't the majority share holder it probably wouldn't be sustainable.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43549332"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43549332" href="https://news.ycombinator.com/vote?id=43549332&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I've been telling people for years that NVIDIA is actually a software company, but nobody ever listens.  My argument is that their silicon is nothing special and could easily be replicated by others, and therefore their real value is in their driver+CUDA layer.</p><p>(Maybe "nothing special" is a little bit strong, but as a chip designer I've never seen the actual NVIDIA chips as all that much of a moat.  What makes it hard to find alternatives to NVIDIA is their driver and CUDA stack.)</p><p>Curious to hear others' opinions on this.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43549300"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43549300" href="https://news.ycombinator.com/vote?id=43549300&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>If AMD developers use AI deployed on nvidia hardware to create tools that complete against nvidia as a company but overall improves outcomes because of competition, would this be an example of co evolution observable in human time standards... I feel like ai is evolving, taking a stable form in this complex multi dimension multi paradigm sweet spot of an environment we have created, on top of this technical, social and governmental infrastructure and we're watching it live on discovery tech filtered into a 2d video narrated by some idiot who has no right to be as confident as he sounds. I'm sorry I'm on withdrawal from quitting mass media and I'm very bored.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549395"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43549395" href="https://news.ycombinator.com/vote?id=43549395&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt;I'm sorry I'm on withdrawal from quitting mass media and I'm very bored.</p><p>Good choice!  So many people doing that these days.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547586"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547586" href="https://news.ycombinator.com/vote?id=43547586&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>CUDA is an entire ecosystem - not a single programming language extension (C++) or a single library, but a collection of libraries &amp; tools for specific use cases and optimizations (cuDNN, CUTLASS, cuBLAS, NCCL, etc.). There is also tooling support that Nvidia provides, such as profilers, etc. Many of the libraries build on other libraries. Even if AMD had the decent, reliable language extensions for general-purpose GPU programming, they still don't have the libraries and the supporting ecosystem to provide anything to the level that CUDA provides today, which is a decade plus of development effort from Nvidia to build.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548095"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43548095" href="https://news.ycombinator.com/vote?id=43548095&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>The counter point is they could make a higher level version of CUDA which wouldn't necessitate all the other supporting libraries. The draw of cuBLAS is that CUDA is a confusing pain. It seems reasonable to think they could write a better, higher level language (in the same vein as triton) and not have to write as many support libraries</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548849"><td></td></tr>
                        <tr id="43547594"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547594" href="https://news.ycombinator.com/vote?id=43547594&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Back in 2015, they were a quarter or two from bankruptcy, saved by the XBOX and Playstation contracts.  Those years saw several significant layoffs, and talent leaving for greener pastures.    Lisa Su has done a great job at rebuilding the company.   But not in a position to hire 2000 engineers x few million comp (~$4 billion annually) even if there were people readily available.</p><p>"it'd still be a good investment." - that's definitely not a sure thing.  Su isn't a risk taker, seems to prefer incremental growth, mainly focused on the CPU side.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548052"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43548052" href="https://news.ycombinator.com/vote?id=43548052&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Where does the idea that engineers cost "a few million" come from? You might pay that much to senior engineering management, big names who can attract other talent, but normal engineers cost much less than a million dollars a year.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548486"><td></td></tr>
            <tr id="43547672"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547672" href="https://news.ycombinator.com/vote?id=43547672&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>This is the difference between Jensen and Su. It’s not that Jensen is a risk taker. No. Jensen focused on incremental growth of the core business while slowly positioning the company for growth in other verticals as well should the landscape change.</p><p>Jensen never said… hey I’m going to bet it all on AI and cuda. Let’s go all in. This never happened. Both Jensen and Su are not huge risk takers imo.</p><p>Additionally there’s a lot of luck involved with the success of NVIDIA.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549302"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549302" href="https://news.ycombinator.com/vote?id=43549302&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I think this broaches the real matter, which is that nVidia's core business is GPUs while AMD's core business is CPUs. And, frankly, AMD has lately been doing a great job at its core business. The problem is that GPUs are now much more profitable than CPUs, both in terms of unit economics and growth potential. So they are winning a major battle (against Intel) even as they are losing a different one (against nVidia). I'm not sure there's a strategy they could have adopted to win both at the same time.</p><p>However, the next big looming problem for them is likely to be the shrinking market for x86 vs. the growing market for Arm etc. So they might very well have demonstrated great core competence, that ends up being completely swept away by not just one but two major industry shifts.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43549345"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43549345" href="https://news.ycombinator.com/vote?id=43549345&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>In turn I will raise you the following: Why are GPU ISA trade secrets at all? Why not open them up like CPU ISAs, get rid of specialized cores and let compiler writers port their favorite languages to compile into native GPU programs? Everyone will be happy. Game devs will be happy with more control over the hardware, Compiler devs will be happy to run haskell or prolog natively on GPUs, ML devs will be happier, NVIDIA/AMD will be happier with taking the MainStage.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548054"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43548054" href="https://news.ycombinator.com/vote?id=43548054&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>CUDA isn't the moat people think it is. NVIDIA absolutely has the best dev ergonomics for machine learning, there's no question about that. Their driver is also far more stable than AMD's. But AMD is also improving, they've made some significant strides over the last 12-18 months.</p><p>But I think more importantly, what is often missed in this analysis is that most programmers doing ML work aren't writing their own custom kernels. They're just using pytorch (or maybe something even more abstracted/multi-backend like keras 3.x) and let the library deal with implementation details related to their GPU.</p><p>That doesn't mean there aren't footguns in that particular land of abstraction, but the delta between the two providers is not nearly as stark as its often portrayed. At least not for the average programmer working with ML tooling.</p><p>(EDIT: also worth noting that the work being done in the MLIR project has a role to play in closing the gap as well for similar reasons)</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548642"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43548642" href="https://news.ycombinator.com/vote?id=43548642&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; But I think more importantly, what is often missed in this analysis is that most programmers doing ML work aren't writing their own custom kernels. They're just using pytorch (or maybe something even more abstracted/multi-backend like keras 3.x) and let the library deal with implementation details related to their GPU.</p><p>That would imply that AMD could just focus on implementing good PyTorch support on their hardware and they would be able to start taking market share. Which doesn't sound like much work compared with writing a full CUDA competitor. But that does not seem to be the strategy, which implies it is not so simple?</p><p>I am not an ML engineer so don't have first hand experience, but those I have talked to say they depend on a lot more than just one or two key libraries. But my sample size is small. Interested in other perspectives...</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549088"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43549088" href="https://news.ycombinator.com/vote?id=43549088&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; But that does not seem to be the strategy, which implies it is not so simple?</p><p>That is exactly what has been happening [1], and not just in pytorch. Geohot has been very dedicated in working with AMD to upgrade their station in this space [2]. If you hang out in the tinygrad discord, you can see this happening in real time.</p><p>&gt; those I have talked to say they depend on a lot more than just one or two key libraries.</p><p>Theres a ton of libraries out there yes, but if we're talking about python and the libraries in question are talking to GPUs its going to be exceedingly rare that theyre not using one of these under the hood: pytorch, tensorflow, jax, keras, et al.</p><p>There are of course exceptions to this, particularly if you're not using python for your ML work (which is actually common for many companies running inference at scale and want better runtime performance, training is a different story). But ultimately the core ecosystem does work just fine with AMD GPUs, provided you're not doing any exotic custom kernel work.</p><p>(EDIT: just realized my initial comment unintentionally borrowed the "moat" commentary from geohot's blog. A happy accident in this case, but still very much rings true for my day to day ML dev experience)</p><p>[1] <a href="https://github.com/pytorch/pytorch/pulls?q=is%3Aopen+is%3Apr+label%3A%22module%3A+rocm%22" rel="nofollow">https://github.com/pytorch/pytorch/pulls?q=is%3Aopen+is%3Apr...</a></p><p>[2] <a href="https://geohot.github.io//blog/jekyll/update/2025/03/08/AMD-YOLO.html" rel="nofollow">https://geohot.github.io//blog/jekyll/update/2025/03/08/AMD-...</a></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547535"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547535" href="https://news.ycombinator.com/vote?id=43547535&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>I can't contribute much to this discussion due to bias and NDAs, but I just wanted to mention, technically HIP is our CUDA competitor. ROCm is the foundation that HIP is being built on.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547554"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547554" href="https://news.ycombinator.com/vote?id=43547554&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>I wonder what the purpose is behind creating a whole new API? Why not just focus on getting Vulkan compute on AMD GPUs to have the data throughput of CUDA?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548619"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548619" href="https://news.ycombinator.com/vote?id=43548619&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I don’t know answer to your question, but I recalled something relevant. Some time ago, Microsoft had a tech which compiled almost normal looking C++ into Direct3D 11 compute shaders: <a href="https://learn.microsoft.com/en-us/cpp/parallel/amp/cpp-amp-overview?view=msvc-170" rel="nofollow">https://learn.microsoft.com/en-us/cpp/parallel/amp/cpp-amp-o...</a> The compute kernels are integrated into CPU-running C++ in the similar fashion CUDA does.</p><p>As you see, the technology deprecated in Visual Studio 2022. I don’t know why but I would guess people just didn’t care. Maybe because it only run on Windows.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547492"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547492" href="https://news.ycombinator.com/vote?id=43547492&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>AMD have actually made several attempts at it.</p><p>The first time, they went ahead and killed off their effort to consolidate on OpenCL. OpenCL went terribly (in no small part because NVIDIA held out on OpenCL 2 support) and that set AMD back a long ways.</p><p>Beyond that, AMD does not have a strong software division or one with the teeth to really influence hardware to their needs . They have great engineers but leadership doesn’t know how to get them to where they need to be.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547900"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547900" href="https://news.ycombinator.com/vote?id=43547900&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>This is it, it's an organisational skill issue. To be fair, being a HW company and a SW company at the same time is very difficult.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548029"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548029" href="https://news.ycombinator.com/vote?id=43548029&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>It is but you have to be.</p><p>It’s been key to the success of their peers. NVIDIA and Apple are the best examples but even Intel to a smaller degree.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547545"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547545" href="https://news.ycombinator.com/vote?id=43547545&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>The idea that CUDA is the main reason behind Nvidia dominance seems strange to me. If most of the money is coming from Facebook and Microsoft they have their own teams writing code at a lower level than CUDA anyway. Even deepseek was writing stuff lower than that.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43549250"><td></td></tr>
                  <tr id="43547853"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547853" href="https://news.ycombinator.com/vote?id=43547853&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>AMD <i>was</i> investing in a drop-in CUDA compatibility layer &amp; cross-compiler!</p><p>Perhaps in keeping with the broader thread here, they had only ever funded a single contract developer working on it, and then discontinued the project (for who-knows-what legal or political reasons). But the developer had specified that he could open-source the pre-AMD state if the contract was dissolved, and he did exactly that! The project is active with an actively contributing community, and is rapidly catching up to where it was.</p><p><a href="https://www.phoronix.com/review/radeon-cuda-zluda" rel="nofollow">https://www.phoronix.com/review/radeon-cuda-zluda</a></p><p><a href="https://vosen.github.io/ZLUDA/blog/zludas-third-life/" rel="nofollow">https://vosen.github.io/ZLUDA/blog/zludas-third-life/</a></p><p><a href="https://vosen.github.io/ZLUDA/blog/zluda-update-q4-2024/" rel="nofollow">https://vosen.github.io/ZLUDA/blog/zluda-update-q4-2024/</a></p><p>IMO it's vital that even if NVIDIA's future falters in some way, the (likely) collective millennia of research built on top of CUDA will continue to have a path forward on other constantly-improving hardware.</p><p>It's frustrating that AMD will benefit from this without contributing - but given the entire context of this thread, maybe it's best that they <i>aren't</i> actively managing the thing that gives their product a future!</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547906"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547906" href="https://news.ycombinator.com/vote?id=43547906&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>ZLUDA is built on HIP which is built on ROCm.   Both of the latter are significant efforts that AMD is pouring significant resources into.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43548633"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43548633" href="https://news.ycombinator.com/vote?id=43548633&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>CUDA is over a decade of investment. I left CUDA toolkit team in 2014 and it was probably around 10 years old back then. Can't build something comparable fast.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547506"><td></td></tr>
            <tr id="43547335"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547335" href="https://news.ycombinator.com/vote?id=43547335&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>The answer is in the question, because if they had the foresight to do such a thing the tech would already be here, instead they thought 1 dimensionally about their product, were part of the group that fumbled OpenCL and now they're a decade behind playing catch up.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547379"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547379" href="https://news.ycombinator.com/vote?id=43547379&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>A good group can catch up significantly in 2 years. They will still be behind, but if they are cheaper (or just you can buy them) that would still go a long way.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547589"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547589" href="https://news.ycombinator.com/vote?id=43547589&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>I think even with the trashy api and drivers if they release graphic cards with 4x the memory of the nvidia equivalents the community would put the effort to make them work.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547800"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547800" href="https://news.ycombinator.com/vote?id=43547800&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Yeah. Easier said than done, I know, but they need to not just catch up to nVidia but leapfrog them somehow.</p><p>I would have said that releasing cards with 32GB+ of onboard RAM, or better yet 128GB, would have gotten things moving. They'd be able to run/train models that nVidia's consumer cards couldn't.</p><p>But I think nVidia closed that gap with their "Project Digits" (or whatever the final name is) PCs.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547724"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547724" href="https://news.ycombinator.com/vote?id=43547724&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>I think that it is really hard to be cheaper in the ways that really matter. Performance per watt matters a lot here, and NVidia is excellent at this. It doesn't seem like anyone else will be able to compete within at least the next couple of years.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547858"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547858" href="https://news.ycombinator.com/vote?id=43547858&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>"good group" is carrying a lot of weight here.   You can't buy that.    You can buy good small groups, but AMD needs a good large group, and that can't be bought.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547882"><td></td></tr>
                  <tr id="43547416"><td></td></tr>
            <tr id="43548746"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43548746" href="https://news.ycombinator.com/vote?id=43548746&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Another possible reason might be outreach. NVIDIA spends big money on getting people to use their products. I have worked at two HPC centers and at both we had NVIDIA employees stationed there, whose job it was to help us get the most out of the hardware. Besides that, they also organize Hackatrons and they have dedicated software developer programs for each common application, be it LLMs, Weather Prediction or Protein Folding, not to mention dedicated libraries for pretty much every domain.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43549019"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43549019" href="https://news.ycombinator.com/vote?id=43549019&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Maybe this is an overly cynical response but the answer is simply that they cannot (at least not immediately). They have not invested enough into engineering talent with this specific goal in mind.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547831"><td></td></tr>
            <tr id="43547510"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547510" href="https://news.ycombinator.com/vote?id=43547510&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; The delta between NVIDIA's value and AMD's is bigger than the annual GDP of Spain.</p><p>Nvidia is massively overvalued right now. AI has rocketed them into absolute absurdity, and it's not sustainable. Put aside the actual technology for a second and realize that public image of AI is at rock bottom. Every single time a company puts out AI-generated materials, they receive immense public backlash. That's not going away any time soon and it's only likely to get worse.</p><p>Speaking as someone that's not even remotely anti-AI, I wouldn't touch the shit with a 10 foot pole because of how bad the public image is. The moment that capital realizes this, that bubble is going to pop and it's going to pop hard.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547590"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547590" href="https://news.ycombinator.com/vote?id=43547590&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Interesting perspective, I haven't noticed much if any public backlash against AI generation. What are some examples?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547761"><td></td></tr>
                <tr id="43548218"><td></td></tr>
                  <tr id="43547884"><td></td></tr>
                  <tr id="43547711"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547711" href="https://news.ycombinator.com/vote?id=43547711&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; AI has rocketed them into absolute absurdity, and it's not sustainable</p><p>Why isn't it sustainable? Their biggest customers all have strong finances and legitimate demand. Google and Facebook would happily run every piece of user generated content through an LLM if they had enough GPUs. Same with Microsoft and every enterprise document.</p><p>The VC backed companies and Open AI are more fragile, but they're comparatively small customers.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548050"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548050" href="https://news.ycombinator.com/vote?id=43548050&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>IMO the closest analogue for Nvidia now is Cisco during the dot-com boom. Cisco sold the physical infrastructure required for Internet companies to operate. Investors all bought in because they figured it was a safe bet. Individual companies may come and go, but if the Internet keeps growing, companies will always need to buy networking equipment. Despite the Internet being way bigger than it was in 2000, and Cisco being highly profitable, Cisco's share price has never exceeded the peak it was at during the dot-com boom.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548097"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43548097" href="https://news.ycombinator.com/vote?id=43548097&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Google may well want to run more of their content through an LLM, but they will not be using Nvidia hardware to do it, they'll be using their TPUs.</p><p>Amazon are on their third generation of in-house AI chips and Anthropic will be using those chips to train the next generation of Claude.</p><p>In other words, their biggest customers are looking for cheaper alternatives and are already succeeding in finding them.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547801"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547801" href="https://news.ycombinator.com/vote?id=43547801&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; Google and Facebook would happily run every piece of user generated content through an LLM if they had enough GPUs. Same with Microsoft and every enterprise document.</p><p>.. But how much actual value derives from this?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547917"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547917" href="https://news.ycombinator.com/vote?id=43547917&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Youtube could conceivably put multi language subtitles on every video. Potentially even dub them.</p><p>But the "real value" would come from making adverts better targeted and more interactive. It's hard to quantity as a person outside of the companies, but the intuition for a positive value is pretty strong.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548093"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43548093" href="https://news.ycombinator.com/vote?id=43548093&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; Youtube could conceivably put multi language subtitles on every video.</p><p>They already do this, it's opt-in.</p><p>&gt; But the "real value" would come from making adverts better targeted and more interactive.</p><p>Is there any evidence to suggest that a transformer would be better at collaborative filtering than the current deep learning system that was custom engineered and built for this?</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43547559"><td></td></tr>
                <tr id="43547716"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547716" href="https://news.ycombinator.com/vote?id=43547716&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I see this form of argument sometimes here but I really don’t get it.</p><p>Lots of people don’t play the stock market or just invest in funds. It seems like just a way of challenging somebody that looks vaguely clever, or calls them out in a “put your money where your mouth is” sense, but actually presents no argument.</p><p>Anyway, if you want to short Nvidia you have to know when their bubble is going to pop to get much benefit out of it, right? The market can remain stupid for longer than you can remain solvent or whatever.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547942"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547942" href="https://news.ycombinator.com/vote?id=43547942&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Spot on on the timing being important. I don't think you need to fine-tune it that much; short and hold until the pop happens. If you hold off for a <i>the pop could happen at an indefinite time; maybe very far from now</i>, then I think that invalidates the individual prediction.</p><p>One frustrating aspect of investing is that confident information is tough to come by. It's my take that if you have any (I personally rarely do), you should act on it. So, when someone claims confidently (e.g. with adjectives that imply confidence) that something's going to happen, then that's better than the default.</p><p>I don't have the insight the claimer does; my thought is: "I am jealous. I with I could be that confident about a stock's trajectory. I would act on it."</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548338"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43548338" href="https://news.ycombinator.com/vote?id=43548338&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I was a student up until 2009; watching people talk about buying houses for 50K and selling them for 100K, everyone talking about easy money.</p><p>I knew things were bad when a friend of my sister was complaining that her father(a building framer) was not able to get a loan for a 500K house, something that his colleagues had been able to get. It took another 6 months before the collapse started to hit and the banks when up.</p><p>Timing is hard.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547659"><td></td></tr>
                <tr id="43547867"><td></td></tr>
                <tr id="43548013"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43548013" href="https://news.ycombinator.com/vote?id=43548013&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; But they didn't say soon.</p><p>Setting an indefinite timeline devalues any claim. You could prove this to yourself using <i>Reductio ad absurdum</i>, or by applying it to various general cases.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547984"><td></td></tr>
                        <tr id="43547619"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547619" href="https://news.ycombinator.com/vote?id=43547619&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>You imply you either would believe his word or would short nvidia yourself if he said he was.  If not, why not?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547856"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547856" href="https://news.ycombinator.com/vote?id=43547856&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Close - If I had the degree of confidence that post implies about Nvidia being overvalued, I would take an aggressive short position.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547996"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43547996" href="https://news.ycombinator.com/vote?id=43547996&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>&gt; Lots of very smart people have lost a lot of money by being completely right about the destination, but wrong about the path and how long it will take to get there.</p><p>If you make a habit of this and still lose money, then either you statistically were very unlucky, or did not have a history of being right.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547944"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43547944" href="https://news.ycombinator.com/vote?id=43547944&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p><i>I would take an aggressive short position.</i></p><p>Lots of very smart people have lost a lot of money by being completely right about the destination, but wrong about the path and how long it will take to get there.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547847"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547847" href="https://news.ycombinator.com/vote?id=43547847&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I forbid myself from speculative trading as a consequence of idiosyncratic principles that I live my life by. One of many symbolic rejections of toxic profiteering that infests our neo-mercantile society. I have enough digits in my bank account that adding any more would be unambiguously greedy and distasteful, so in the end it would be violating my principles simply to debase myself. No thanks.</p><p>Anyways you'd need some kind of window of when a stock is going to collapse to short it. Good luck predicting this one.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547885"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547885" href="https://news.ycombinator.com/vote?id=43547885&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I respect, and adore your philosophy.</p><p>For a short, I think you don't need that strong of a window. For an options combination, yes.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43547803"><td></td></tr>
                  <tr id="43547630"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547630" href="https://news.ycombinator.com/vote?id=43547630&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>One thing I've learned the hard way is that industry trends -- and the stock valuations that go with them -- can stay irrational far longer than you can imagine.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43548927"><td></td></tr>
                  <tr id="43547728"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547728" href="https://news.ycombinator.com/vote?id=43547728&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>HIP is definitely a viable option. In fact with some effort you can port large CUDA projects to be compilable with the HIP/ AMD-clang toolchain. This way you don’t have to rewrite the world from scratch in a new language but still be able to run GPU workloads on AMD hardware.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547450"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547450" href="https://news.ycombinator.com/vote?id=43547450&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>I don't think its that bad. The focus will turn to inference going forward and that eventually means a place for AMD and maybe even Intel. Eventually it will be all about the efficiency of inference in watts.</p><p>That switch will reduce the NVIDIA margins by a lot. NVIDIA probably has 2 years left of being the only one with golden shovels.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547549"><td></td></tr>
                  <tr id="43547627"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547627" href="https://news.ycombinator.com/vote?id=43547627&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>What about Rust for GPU programming? I wonder why AMD doesn't back such kind of effort as an alternative.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547340"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547340" href="https://news.ycombinator.com/vote?id=43547340&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Leadership. At the end of the day, the buck stops with leadership.</p><p>If they wanted to prioritize this, they would. They're simply not taking it seriously.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547363"><td></td></tr>
                <tr id="43547469"><td></td></tr>
            <tr id="43547604"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547604" href="https://news.ycombinator.com/vote?id=43547604&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>Why, because 90% of her job is talking to and appeasing shareholders, grand standing with fat whales, and what else.. what do you think a CEO at these companies actually does? They aren't in the trenches of each subdivision nurturing and cracking whips. She likely attends a 2 hour briefing with a line item: CUDA parity project: on schedule release date not set</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43547560"><td></td></tr>
            <tr id="43547520"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547520" href="https://news.ycombinator.com/vote?id=43547520&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>HIP is now somewhat viable (and ROCm is now all HIP).</p><p>But — too late. First versions of ROCm were terrible. Too much boilerplate. 1200 lines of template-heavy C++ for a simple FFT. Can't just start hacking around.</p><p>Since then, the CUDA way is cemented in minds of developers. Intel now has oneAPI, and it is not too bad, and hackable, but there is no hardware and no one will learn it. And HIP is "CUDA-like", so why not CUDA, unless you _have to_ use AMD hardware.</p><p>Tl;dr first versions of ROCm were bad. Now they are better, but it is too late.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547893"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43547893" href="https://news.ycombinator.com/vote?id=43547893&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>AMD's CEO is the cousin of Nvidias CEO.</p><p>Neither will encroach too much on the others turf.    The two companies don't want to directly compete on the things that really drive the share price.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43547347"><td></td></tr>
                <tr id="43547359"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43547359" href="https://news.ycombinator.com/vote?id=43547359&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>The question was why don't they have anything as good as CUDA, not why don't they adopt CUDA itself.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547402"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43547402" href="https://news.ycombinator.com/vote?id=43547402&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div><p>Isn't the "goodness" of CUDA really down to its mass adoption -- and therefore its community and network effects -- not strictly its technical attributes?</p><p>If I recall, there are various "GPU programming" and "AI" efforts that have existed for AMD GPUs, but none of them have had the same success in large part because they're simply non-"standard?"</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43547524"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43547524" href="https://news.ycombinator.com/vote?id=43547524&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>I thought OpenCL was supposed to be the "standard"?  From the Wikipedia page, it's largely vendor neutral and not that much younger than CUDA (initial release Aug 2009 vs Feb 2007).  Maybe some more knowledgeable people can comment why it seems to have been outcompeted by the proprietary CUDA?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43548063"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43548063" href="https://news.ycombinator.com/vote?id=43548063&amp;how=up&amp;goto=item%3Fid%3D43547309"></a></center>    </td><td><br><div>
                  <p>CUDA has a comparatively nicer user experience. If you would like to understand tacitly and have an nvidia GPU, try writing a simple program using both. (Something highly parallel, like nbody, for example)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                                    </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bletchley code breaker Betty Webb dies aged 101 (333 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c78jd30ywv8o</link>
            <guid>43546236</guid>
            <pubDate>Tue, 01 Apr 2025 12:55:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c78jd30ywv8o">https://www.bbc.com/news/articles/c78jd30ywv8o</a>, See on <a href="https://news.ycombinator.com/item?id=43546236">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div data-testid="byline-new" data-component="byline-block"><p><span>Aida Fofana</span></p><p><span>BBC News, West Midlands<!-- --></span></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/ae8b/live/9ed85c70-0ee3-11f0-bb66-1953ea6c7fa4.png.webp" loading="eager" alt="BBC Pictured is Betty Webb with short curly white hair and in a purple coast with army pin badges on the right lapel. She is slightly smiling and looking towards the camera."><span>BBC</span></p></div><p data-component="caption-block"><figcaption>Bletchley Park code breaker Betty Webb has died at the age of 101<!-- --></figcaption></p></figure><div data-component="text-block"><p>A decorated World War Two code breaker who spent her youth deciphering enemy messages at Bletchley Park has died at the age of 101.<!-- --></p><p>Charlotte "Betty" Webb MBE - who was among the last surviving Bletchley code breakers - died on Monday night, the Women's Royal Army Corps Association confirmed.<!-- --></p><p>Mrs Webb, from Wythall in Worcestershire, joined operations at the Buckinghamshire base at the age of 18, later going on to help with Japanese codes at The Pentagon in the US. She was awarded France's highest honour - the Légion d'Honneur - in 2021. <!-- --></p><p>The Women's Royal Army Corps Association described Mrs Webb as a woman who "inspired women in the Army for decades".<!-- --></p></div><div data-component="text-block"><p>Bletchley Park Trust CEO Iain Standen said Mrs Webb will not only be remembered for her work but "also for her efforts to ensure that the story of what she and her colleagues achieved is not forgotten."<!-- --></p><p>"Betty's passion for preserving the history and legacy of Bletchley Park has undoubtedly inspired many people to engage with the story and visit the site," he said in a statement.<!-- --></p><p>Tributes to Mrs Webb have begun to be posted on social media, including one from historian and author Dr Tessa Dunlop who said she was with her in her final hours.<!-- --></p><p>Describing Mrs Webb as "the very best", she said on X: "She is one of the most remarkable woman I have ever known."<!-- --></p><p>Mrs Webb told the BBC in 2020 that she had "never heard of Bletchley", Britain's wartime code-breaking centre, before starting work there as a member of the ATS, the Auxiliary Territorial Service.<!-- --></p><p>She had been studying at a college near Shrewsbury, Shropshire, when she volunteered as she said she and others on the course felt they "ought to be serving our country rather than just making sausage rolls".<!-- --></p><p>Her mother had taught her to speak German as a child and ahead of her posting remembered being "taken into the mansion [at Bletchley] to read the Official Secrets Act".<!-- --></p><p>"I realised that from then on there was no way that I was going to be able to tell even my parents where I was and what I was doing until 1975 [when restrictions were lifted]," she recalled.<!-- --></p><p>She would tell the family with whom she lodged that she was a secretary.<!-- --></p></div><p data-component="caption-block"><figcaption>Listen on BBC Sounds: Mrs Webb went to work at Bletchley Park when she was 18<!-- --></figcaption></p><div data-component="text-block"><p>When the War ended in Europe in May of 1945, she went to work at the Pentagon after spending four years at Bletchley, which with its analysis of German communications had served as a vital cog in the Allies' war machine.<!-- --></p><p>At the Pentagon she would paraphrase and transcribe already-decoded Japanese messages. She said she was the only member of the ATS to be sent to Washington, describing it as a "tremendous honour".<!-- --></p><p>Mrs Webb, in 2020, recalled she had had no idea the Americans planned to end the conflict by dropping atomic weapon on Japanese cities, describing the weapons' power as "utterly awful"<!-- --></p><p>After the Allies' final victory, it took Mrs Webb several months to organise return passage to the UK, where she worked as a secretary at a school in Shropshire.<!-- --></p><p>The head teacher there had also worked at Bletchley so knew of her professionalism, whereas other would-be employers, she recalled, were left stumped by her being unable to explain - due to secrecy requirements - her previous duties.<!-- --></p><p>More than half a century later, in 2021, Mrs Webb was one of 6,000 British citizens to receive the Légion d'Honneur, following a decision by President François Hollande in 2014 to recognise British veterans who helped liberate France.<!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/87d9/live/c8583c40-0ee9-11f0-ba12-8d27eb561761.png.webp" loading="lazy" alt="PA Media Mrs Webb sat in the front row in a red skirt suit surrounded by other people in large hats, floral print dresses and trouser suits at the Kings Coronation."><span>PA Media</span></p></div><p data-component="caption-block"><figcaption>Betty Webb, seen in the front row in a red suit, was invited to the Coronation<!-- --></figcaption></p></figure><div data-component="text-block"><p>In 2023, she and her niece were among 2,200 people from 203 countries invited to Westminster Abbey to see King Charles III's coronation.<!-- --></p><p>The same year she celebrated her 100th birthday at Bletchley Park with a party. <!-- --></p><p>She and her guests were treated to a fly-past by a Lancaster bomber. She said at the time: "It was for me - it's unbelievable isn't it? Little me."<!-- --></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why F#? (311 pts)]]></title>
            <link>https://batsov.com/articles/2025/03/30/why-fsharp/</link>
            <guid>43546004</guid>
            <pubDate>Tue, 01 Apr 2025 12:34:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://batsov.com/articles/2025/03/30/why-fsharp/">https://batsov.com/articles/2025/03/30/why-fsharp/</a>, See on <a href="https://news.ycombinator.com/item?id=43546004">Hacker News</a></p>
<div id="readability-page-1" class="page"><section itemprop="text">
        
        <p>If someone had told me a few months ago I’d be playing with .NET again after a
15+ years hiatus I probably would have laughed at this.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup> Early on in my
career I played with .NET and Java, and even though .NET had done some things
better than Java (as it had the opportunity to learn from some early Java
mistakes), I quickly settled on Java as it was a truly portable environment.</p>

<p>I guess everyone who reads my blog knows that in the past few years I’ve been
playing on and off with OCaml and I think it’s safe to say that it has become
one of my favorite programming languages - alongside the likes of Ruby and
Clojure. My work with OCaml drew my attention recently to F#, an ML targeting
.NET, developed by Microsoft. The functional counterpart of the
(mostly) object-oriented C#. The newest ML language created…</p>

<h2 id="what-is-f">What is F#?</h2>

<blockquote>
  <p>Unfortunately, no one can be told what the Matrix is. You have to see it for yourself.</p>

  <p>– Morpheus, The Matrix</p>
</blockquote>

<p>Before we start discussing F#, I guess we should answer first the question
“What is F#?”. I’ll borrow a bit from the <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/what-is-fsharp">official page</a> to answer it.</p>

<p>F# is a universal programming language for writing succinct, robust and performant code.</p>

<p>F# allows you to write uncluttered, self-documenting code, where your focus remains on your problem domain, rather than the details of programming.</p>

<p>It does this without compromising on speed and compatibility - it is open-source, cross-platform and interoperable.</p>

<div><pre><code><span>open</span> <span>System</span> <span>// Gets access to functionality in System namespace.</span>

<span>// Defines a list of names</span>
<span>let</span> <span>names</span> <span>=</span> <span>[</span> <span>"Peter"</span><span>;</span> <span>"Julia"</span><span>;</span> <span>"Xi"</span> <span>]</span>

<span>// Defines a function that takes a name and produces a greeting.</span>
<span>let</span> <span>getGreeting</span> <span>name</span> <span>=</span> <span>$</span><span>"Hello, {name}"</span>

<span>// Prints a greeting for each name!</span>
<span>names</span>
<span>|&gt;</span> <span>List</span><span>.</span><span>map</span> <span>getGreeting</span>
<span>|&gt;</span> <span>List</span><span>.</span><span>iter</span> <span>(</span><span>fun</span> <span>greeting</span> <span>-&gt;</span> <span>printfn</span> <span>$</span><span>"{greeting}! Enjoy your F#"</span><span>)</span>
</code></pre></div>

<p><strong>Trivia:</strong> F# is the language that made the pipeline operator (<code>|&gt;</code>) popular.</p>

<p>F# has numerous features, including:</p>

<ul>
  <li>Lightweight syntax</li>
  <li>Immutable by default</li>
  <li>Type inference and automatic generalization</li>
  <li>First-class functions</li>
  <li>Powerful data types</li>
  <li>Pattern matching</li>
  <li>Async programming</li>
</ul>

<p>A full set of features are documented in the <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/language-reference/">F# language guide</a>.</p>

<p>Looks pretty promising, right?</p>

<p>F# 1.0 was officially released in May 2005 by Microsoft Research. It was
initially developed by Don Syme at Microsoft Research in Cambridge and evolved
from an earlier research project called “Caml.NET,” which aimed to bring OCaml
to the .NET platform.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup> F# was officially moved from Microsoft Research to
Microsoft (as part of their developer tooling division) in 2010 (timed
with the release of F# 2.0).</p>

<p>F# has been steadily evolving since those early days and the most recent release
<a href="https://learn.microsoft.com/en-us/dotnet/fsharp/whats-new/fsharp-9">F# 9.0</a> was
released in November 2024.  It seems only appropriate that F# would come to my
attention in the year of its 20th birthday!</p>

<p>There were several reasons why I wanted to try out F#:</p>

<ul>
  <li>.NET became open-source and portable a few years ago and I wanted to check the progress on that front</li>
  <li>I was curious if F# offers any advantages over OCaml</li>
  <li>I’ve heard good things about the F# tooling (e.g. Rider and Ionide)</li>
  <li>I like playing with new programming languages</li>
</ul>

<p>Below you’ll find my initial impressions for several areas.</p>

<h2 id="the-language">The Language</h2>

<p>As a member of the ML family of languages, the syntax won’t surprise
anyone familiar with OCaml. As there are quite few people familiar with
OCaml, though, I’ll mention that Haskell programmers will also feel right at
home with the syntax. And Lispers.</p>

<p>For everyone else - it’d be fairly easy to pick up the basics.</p>

<div><pre><code><span>// function application</span>
<span>printfn</span> <span>"Hello, World!"</span>

<span>// function definition</span>
<span>let</span> <span>greet</span> <span>name</span> <span>=</span>
    <span>printfn</span> <span>"Hello, %s!"</span> <span>name</span>

<span>greet</span> <span>"World"</span>

<span>// whitespace is significant, like in Python</span>
<span>let</span> <span>foo</span> <span>=</span>
    <span>let</span> <span>i</span><span>,</span> <span>j</span><span>,</span> <span>k</span> <span>=</span> <span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>)</span>

    <span>// Body expression:</span>
    <span>i</span> <span>+</span> <span>2</span> <span>*</span> <span>j</span> <span>+</span> <span>3</span> <span>*</span> <span>k</span>

<span>// conditional expressions</span>
<span>let</span> <span>test</span> <span>x</span> <span>y</span> <span>=</span>
  <span>if</span> <span>x</span> <span>=</span> <span>y</span> <span>then</span> <span>"equals"</span>
  <span>elif</span> <span>x</span> <span>&lt;</span> <span>y</span> <span>then</span> <span>"is less than"</span>
  <span>else</span> <span>"is greater than"</span>

<span>printfn</span> <span>"%d %s %d."</span> <span>10</span> <span>(</span><span>test</span> <span>10</span> <span>20</span><span>)</span> <span>20</span>

<span>// Looping over a list.</span>
<span>let</span> <span>list1</span> <span>=</span> <span>[</span> <span>1</span><span>;</span> <span>5</span><span>;</span> <span>100</span><span>;</span> <span>450</span><span>;</span> <span>788</span> <span>]</span>
<span>for</span> <span>i</span> <span>in</span> <span>list1</span> <span>do</span>
   <span>printfn</span> <span>"%d"</span> <span>i</span>

<span>// Looping over a sequence of tuples</span>
<span>let</span> <span>seq1</span> <span>=</span> <span>seq</span> <span>{</span> <span>for</span> <span>i</span> <span>in</span> <span>1</span> <span>..</span> <span>10</span> <span>-&gt;</span> <span>(</span><span>i</span><span>,</span> <span>i</span><span>*</span><span>i</span><span>)</span> <span>}</span>
<span>for</span> <span>(</span><span>a</span><span>,</span> <span>asqr</span><span>)</span> <span>in</span> <span>seq1</span> <span>do</span>
  <span>printfn</span> <span>"%d squared is %d"</span> <span>a</span> <span>asqr</span>

<span>// A simple for...to loop.</span>
<span>let</span> <span>function1</span> <span>()</span> <span>=</span>
  <span>for</span> <span>i</span> <span>=</span> <span>1</span> <span>to</span> <span>10</span> <span>do</span>
    <span>printf</span> <span>"%d "</span> <span>i</span>
  <span>printfn</span> <span>""</span>

<span>// A for...to loop that counts in reverse.</span>
<span>let</span> <span>function2</span> <span>()</span> <span>=</span>
  <span>for</span> <span>i</span> <span>=</span> <span>10</span> <span>downto</span> <span>1</span> <span>do</span>
    <span>printf</span> <span>"%d "</span> <span>i</span>
  <span>printfn</span> <span>""</span>

<span>// Records</span>

<span>// Labels are separated by semicolons when defined on the same line.</span>
<span>type</span> <span>Point</span> <span>=</span> <span>{</span> <span>X</span><span>:</span> <span>float</span><span>;</span> <span>Y</span><span>:</span> <span>float</span><span>;</span> <span>Z</span><span>:</span> <span>float</span> <span>}</span>

<span>// You can define labels on their own line with or without a semicolon.</span>
<span>type</span> <span>Customer</span> <span>=</span>
    <span>{</span> <span>First</span><span>:</span> <span>string</span>
      <span>Last</span><span>:</span> <span>string</span>
      <span>SSN</span><span>:</span> <span>uint32</span>
      <span>AccountNumber</span><span>:</span> <span>uint32</span> <span>}</span>

<span>let</span> <span>mypoint</span> <span>=</span> <span>{</span> <span>X</span> <span>=</span> <span>1</span><span>.</span><span>0</span><span>;</span> <span>Y</span> <span>=</span> <span>1</span><span>.</span><span>0</span><span>;</span> <span>Z</span> <span>=</span> <span>-</span><span>1</span><span>.</span><span>0</span> <span>}</span>

<span>// Discriminated Union</span>
<span>type</span> <span>Shape</span> <span>=</span>
    <span>|</span> <span>Circle</span> <span>of</span> <span>radius</span><span>:</span> <span>float</span>
    <span>|</span> <span>Rectangle</span> <span>of</span> <span>width</span><span>:</span> <span>float</span> <span>*</span> <span>height</span><span>:</span> <span>float</span>

<span>// Functing using pattern matching</span>
<span>let</span> <span>area</span> <span>shape</span> <span>=</span>
    <span>match</span> <span>shape</span> <span>with</span>
    <span>|</span> <span>Circle</span> <span>radius</span> <span>-&gt;</span> <span>System</span><span>.</span><span>Math</span><span>.</span><span>PI</span> <span>*</span> <span>radius</span> <span>*</span> <span>radius</span>
    <span>|</span> <span>Rectangle</span> <span>(</span><span>width</span><span>,</span> <span>height</span><span>)</span> <span>-&gt;</span> <span>width</span> <span>*</span> <span>height</span>

<span>let</span> <span>circle</span> <span>=</span> <span>Circle</span> <span>5</span><span>.</span><span>0</span>
<span>let</span> <span>rectangle</span> <span>=</span> <span>Rectangle</span><span>(</span><span>4</span><span>.</span><span>0</span><span>,</span> <span>3</span><span>.</span><span>0</span><span>)</span>

<span>printfn</span> <span>"Circle area: %f"</span> <span>(</span><span>area</span> <span>circle</span><span>)</span>
<span>printfn</span> <span>"Rectangle area: %f"</span> <span>(</span><span>area</span> <span>rectangle</span><span>)</span>
</code></pre></div>

<p>Nothing shocking here, right?</p>

<p>Here’s another slightly more involved example:</p>

<div><pre><code><span>open</span> <span>System</span>

<span>// Sample data - simple sales records</span>
<span>type</span> <span>SalesRecord</span> <span>=</span> <span>{</span> <span>Date</span><span>:</span> <span>DateTime</span><span>;</span> <span>Product</span><span>:</span> <span>string</span><span>;</span> <span>Amount</span><span>:</span> <span>decimal</span><span>;</span> <span>Region</span><span>:</span> <span>string</span> <span>}</span>

<span>// Sample dataset</span>
<span>let</span> <span>sales</span> <span>=</span> <span>[</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>1</span><span>,</span> <span>15</span><span>);</span> <span>Product</span> <span>=</span> <span>"Laptop"</span><span>;</span> <span>Amount</span> <span>=</span> <span>1200</span><span>m</span><span>;</span> <span>Region</span> <span>=</span> <span>"North"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>);</span>  <span>Product</span> <span>=</span> <span>"Phone"</span><span>;</span>  <span>Amount</span> <span>=</span> <span>800</span><span>m</span><span>;</span>  <span>Region</span> <span>=</span> <span>"South"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>1</span><span>,</span> <span>20</span><span>);</span> <span>Product</span> <span>=</span> <span>"Tablet"</span><span>;</span> <span>Amount</span> <span>=</span> <span>400</span><span>m</span><span>;</span>  <span>Region</span> <span>=</span> <span>"North"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>2</span><span>,</span> <span>18</span><span>);</span> <span>Product</span> <span>=</span> <span>"Laptop"</span><span>;</span> <span>Amount</span> <span>=</span> <span>1250</span><span>m</span><span>;</span> <span>Region</span> <span>=</span> <span>"East"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>1</span><span>,</span> <span>5</span><span>);</span>  <span>Product</span> <span>=</span> <span>"Phone"</span><span>;</span>  <span>Amount</span> <span>=</span> <span>750</span><span>m</span><span>;</span>  <span>Region</span> <span>=</span> <span>"West"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>2</span><span>,</span> <span>12</span><span>);</span> <span>Product</span> <span>=</span> <span>"Tablet"</span><span>;</span> <span>Amount</span> <span>=</span> <span>450</span><span>m</span><span>;</span>  <span>Region</span> <span>=</span> <span>"North"</span> <span>}</span>
    <span>{</span> <span>Date</span> <span>=</span> <span>DateTime</span><span>(</span><span>2023</span><span>,</span> <span>1</span><span>,</span> <span>28</span><span>);</span> <span>Product</span> <span>=</span> <span>"Laptop"</span><span>;</span> <span>Amount</span> <span>=</span> <span>1150</span><span>m</span><span>;</span> <span>Region</span> <span>=</span> <span>"South"</span> <span>}</span>
<span>]</span>

<span>// Quick analysis pipeline</span>
<span>let</span> <span>salesSummary</span> <span>=</span>
    <span>sales</span>
    <span>|&gt;</span> <span>List</span><span>.</span><span>groupBy</span> <span>(</span><span>fun</span> <span>s</span> <span>-&gt;</span> <span>s</span><span>.</span><span>Product</span><span>)</span>                          <span>// Group by product</span>
    <span>|&gt;</span> <span>List</span><span>.</span><span>map</span> <span>(</span><span>fun</span> <span>(</span><span>product</span><span>,</span> <span>items</span><span>)</span> <span>-&gt;</span>                          <span>// Transform each group</span>
        <span>let</span> <span>totalSales</span> <span>=</span> <span>items</span> <span>|&gt;</span> <span>List</span><span>.</span><span>sumBy</span> <span>(</span><span>fun</span> <span>s</span> <span>-&gt;</span> <span>s</span><span>.</span><span>Amount</span><span>)</span>
        <span>let</span> <span>avgSale</span> <span>=</span> <span>totalSales</span> <span>/</span> <span>decimal</span> <span>(</span><span>List</span><span>.</span><span>length</span> <span>items</span><span>)</span>
        <span>let</span> <span>topRegion</span> <span>=</span>
            <span>items</span>
            <span>|&gt;</span> <span>List</span><span>.</span><span>groupBy</span> <span>(</span><span>fun</span> <span>s</span> <span>-&gt;</span> <span>s</span><span>.</span><span>Region</span><span>)</span>                   <span>// Nested grouping</span>
            <span>|&gt;</span> <span>List</span><span>.</span><span>maxBy</span> <span>(</span><span>fun</span> <span>(_,</span> <span>regionItems</span><span>)</span> <span>-&gt;</span>
                <span>regionItems</span> <span>|&gt;</span> <span>List</span><span>.</span><span>sumBy</span> <span>(</span><span>fun</span> <span>s</span> <span>-&gt;</span> <span>s</span><span>.</span><span>Amount</span><span>))</span>
            <span>|&gt;</span> <span>fst</span>

        <span>(</span><span>product</span><span>,</span> <span>totalSales</span><span>,</span> <span>avgSale</span><span>,</span> <span>topRegion</span><span>))</span>
    <span>|&gt;</span> <span>List</span><span>.</span><span>sortByDescending</span> <span>(</span><span>fun</span> <span>(_,</span> <span>total</span><span>,</span> <span>_,</span> <span>_)</span> <span>-&gt;</span> <span>total</span><span>)</span>      <span>// Sort by total sales</span>

<span>// Display results</span>
<span>salesSummary</span>
<span>|&gt;</span> <span>List</span><span>.</span><span>iter</span> <span>(</span><span>fun</span> <span>(</span><span>product</span><span>,</span> <span>total</span><span>,</span> <span>avg</span><span>,</span> <span>region</span><span>)</span> <span>-&gt;</span>
    <span>printfn</span> <span>"%s: $%M total, $%M avg, top region: %s"</span>
        <span>product</span> <span>total</span> <span>avg</span> <span>region</span><span>)</span>
</code></pre></div>

<p>Why don’t you try saving the snippet above in a file called <code>Sales.fsx</code> and running it like this:</p>



<p>Now you know that F# is a great choice for ad-hoc scripts! Also, running <code>dotnet fsi</code> by itself
will pop an F# REPL where you can explore the language at your leisure.</p>

<p>I’m not going to go into great details here, as much of what I wrote about OCaml
<a href="https://batsov.com/articles/2022/08/29/ocaml-at-first-glance/">here</a> applies to F# as well.
I’d also suggest this quick <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/tour">tour of F#</a>
to get a better feel for its syntax.</p>

<p><strong>Tip:</strong> Check out the <a href="https://fsprojects.github.io/fsharp-cheatsheet/">F# cheatsheet</a>
if you’d like to see a quick syntax reference.</p>

<p>One thing that made a good impression to me is the focus of the language designers on
making F# approachable to newcomers, by providing a lot of small quality of life improvements
for them. Below are few examples, that probably don’t mean much to you, but would mean something
to people familiar with OCaml:</p>

<div><pre><code><span>// line comments</span>
<span>(* the classic ML comments are around as well *)</span>

<span>// mutable values</span>
<span>let</span> <span>mutable</span> <span>x</span> <span>=</span> <span>5</span>
<span>x</span> <span>&lt;-</span> <span>6</span>

<span>// ranges and slices</span>
<span>let</span> <span>l</span> <span>=</span> <span>[</span><span>1</span><span>..</span><span>2</span><span>..</span><span>10</span><span>]</span>
<span>name</span><span>[</span><span>5</span><span>..]</span>

<span>// C# method calls look pretty natural</span>
<span>let</span> <span>name</span> <span>=</span> <span>"FOO"</span><span>.</span><span>ToLower</span><span>()</span>

<span>// operators can be overloaded for different types</span>
<span>let</span> <span>string1</span> <span>=</span> <span>"Hello, "</span> <span>+</span> <span>"world"</span>
<span>let</span> <span>num1</span> <span>=</span> <span>1</span> <span>+</span> <span>2</span>
<span>let</span> <span>num2</span> <span>=</span> <span>1</span><span>.</span><span>0</span> <span>+</span> <span>2</span><span>.</span><span>5</span>

<span>// universal printing</span>
<span>printfn</span> <span>"%A"</span> <span>[</span><span>1</span><span>..</span><span>2</span><span>..</span><span>100</span><span>]</span>
</code></pre></div>

<p>I guess some of those might be controversial, depending on whether you’re a ML
language purist or not, but in my book anything that makes ML more popular is a
good thing.</p>

<p>Did I also mention it’s easy to work with unicode strings and regular expressions?</p>

<p>Often people say that F# is mostly a staging ground for future C# features, and perhaps that’s true.
I haven’t observed both languages long enough to have my own opinion on the subject, but I was impressed
to learn that <code>async/await</code> (of C# and later JavaScript fame) originated in… F# 2.0.</p>

<blockquote>
  <p>It all changed in 2012 when C#5 launched with the introduction of what has now
become the popularized <code>async/await</code> keyword pairing. This feature allowed you to
write code with all the benefits of hand-written asynchronous code, such as not
blocking the UI when a long-running process started, yet read like normal
synchronous code. This <code>async/await</code> pattern has now found its way into many
modern programming languages such as Python, JS, Swift, Rust, and even C++.</p>

  <p>F#’s approach to asynchronous programming is a little different from <code>async/await</code>
but achieves the same goal (in fact, <code>async/await</code> is a cut-down version of F#’s
approach, which was introduced a few years previously, in F#2).</p>

  <p>– Isaac Abraham, F# in Action</p>
</blockquote>

<p>Time will tell what will happen, but I think it’s unlikely that C# will ever be able to fully replace F#.</p>

<p>I’ve also found this <a href="https://www.reddit.com/r/fsharp/comments/xlegmc/why_doesnt_microsoft_use_f/">encouraging comment from 2022</a> that Microsoft might be willing to invest more in F#:</p>

<blockquote>
  <p>Some good news for you. After 10 years of F# being developed by 2.5 people
internally and some random community efforts, Microsoft has finally decided to
properly invest in F# and created a full-fledged team in Prague this
summer. I’m a dev in this team, just like you I was an F# fan for many years
so I am happy things got finally moving here.</p>
</blockquote>

<p>Looking at the changes in F# 8.0 and F 9.0, it seems the new full-fledged team
has done some great work!</p>

<h2 id="ecosystem">Ecosystem</h2>

<p>It’s hard to assess the ecosystem around F# after such a brief period, but overall it seems to
me that there are fairly few “native” F# libraries and frameworks out there and most people
rely heavily on the core .NET APIs and many third-party libraries and frameworks geared towards C#.
That’s a pretty common setup when it comes to hosted languages in general, so nothing surprising here as well.</p>

<p>If you’ve ever used another hosted language (e.g. Scala, Clojure, Groovy) then you probably know what
to expect.</p>

<p><a href="https://github.com/fsprojects/awesome-fsharp">Awesome F#</a> keeps track of popular F# libraries, tools and frameworks. I’ll highlight here the web development and data science libraries:</p>

<p><strong>Web Development</strong></p>

<ul>
  <li><strong>Giraffe</strong>: A lightweight library for building web applications using ASP.NET Core. It provides a functional approach to web development.</li>
  <li><strong>Suave</strong>: A simple and lightweight web server library with combinators for routing and task composition. (Giraffe was inspired by Suave)</li>
  <li><strong>Saturn</strong>: Built on top of Giraffe and ASP.NET Core, it offers an MVC-style framework inspired by Ruby on Rails and Elixir’s Phoenix.</li>
  <li><strong>Bolero</strong>: A framework for building client-side applications in F# using WebAssembly and Blazor.</li>
  <li><strong>Fable</strong>: A compiler that translates F# code into JavaScript, enabling integration with popular JavaScript ecosystems like React or Node.js.</li>
  <li><strong>Elmish</strong>: A model-view-update (MVU) architecture for building web UIs in F#, often used with Fable.</li>
  <li><strong>SAFE Stack</strong>: An end-to-end, functional-first stack for building cloud-ready web applications. It combines technologies like Saturn, Azure, Fable, and Elmish for a type-safe development experience.</li>
</ul>

<p><strong>Data Science</strong></p>

<ul>
  <li><strong>Deedle</strong>: A library for data manipulation and exploratory analysis, similar to pandas in Python.</li>
  <li><strong>DiffSharp</strong>: A library for automatic differentiation and machine learning.</li>
  <li><strong>FsLab</strong>: A collection of libraries tailored for data science, including visualization and statistical tools.</li>
</ul>

<p>I haven’t played much with any of them at this point yet, so I’ll reserve any
feedback and recommendations for some point in the future.</p>

<h2 id="documentation">Documentation</h2>

<p>The official documentation is pretty good, although I find it kind of weird that
some of it is hosted on <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/what-is-fsharp">Microsoft’s site</a>
and the rest is on <a href="https://fsharp.org/">https://fsharp.org/</a> (the site of the F# Software Foundation).</p>

<p>I really liked the following parts of the documentation:</p>

<ul>
  <li><a href="https://learn.microsoft.com/en-us/dotnet/fsharp/style-guide/">F# Style Guide</a></li>
  <li><a href="https://github.com/fsharp/fslang-design">F# Design</a> - a repository of RFCs (every language should have one of those!)</li>
  <li><a href="https://fsharp.github.io/fsharp-core-docs/">F# Standard Library API</a></li>
</ul>

<p><a href="https://fsharpforfunandprofit.com/">https://fsharpforfunandprofit.com/</a> is another good learning resource. (even if it seems a bit dated)</p>



<p>F# has a somewhat troubled dev tooling story, as historically
support for F# was great only in Visual Studio, and somewhat subpar
elsewhere. Fortunately, the tooling story has improved a lot in the past
decade:</p>

<blockquote>
  <p>In 2014 a technical breakthrough was made with the creation of the
FSharp.Compiler.Service (FCS) package by Tomas Petricek, Ryan Riley, and Dave
Thomas with many later contributors. This contains the core implementation of
the F# compiler, editor tooling and scripting engine in the form of a single
library and can be used to make F# tooling for a wide range of
situations. This has allowed F# to be delivered into many more editors,
scripting and documentation tools and allowed the development of alternative
backends for F#. Key editor community-based tooling includes Ionide, by
Krzysztof Cieślak and contributors, used for rich editing support in the
cross-platform VSCode editor, with over 1M downloads at time of writing.</p>

  <p>– Don Syme, The Early History of F#</p>
</blockquote>

<p>I’ve played with the F# plugins for several editors:</p>

<ul>
  <li>Emacs (<code>fsharp-mode</code>)</li>
  <li>Zed (third-party plugin)</li>
  <li>Helix (built-in support for F#)</li>
  <li>VS Code (<a href="https://ionide.io/">Ionide</a>)</li>
  <li>Rider (JetBrains’s .NET IDE)</li>
</ul>

<p>Overall, Rider and VS Code provide the most (and the most polished) features,
but the other options were quite usable as well.  That’s largely due to the fact
that the F# LSP server <code>fsautocomplete</code> (naming is hard!) is quite robust and
any editor with good LSP support gets a lot of functionality for free.</p>

<p>Still, I’ll mention that I found the tooling lacking in some regards:</p>

<ul>
  <li><code>fsharp-mode</code> doesn’t use TreeSitter (yet) and doesn’t seem to be very actively developed (looking at the code - it seems it was derived from <code>caml-mode</code>)</li>
  <li>Zed’s support for F# is quite spartan</li>
  <li>In VS Code shockingly the expanding and shrinking selection is broken, which is quite odd for what is supposed to be the flagship editor for F#</li>
</ul>

<p>I’m really struggling with VS Code’s keybindings (too many modifier keys and functions keys for my taste) and editing model, so I’ll likely stick with Emacs going forward. Or I’ll finally spend more quality time with neovim!</p>

<p>It seems that everyone is using the same code formatter (<code>Fantomas</code>), including the F# team, which is great!
The linter story in F# is not as great (seems the only popular linter <a href="https://fsprojects.github.io/FSharpLint/">FSharpLint</a> is abandonware these days), but when your
compiler is so good, you don’t really need a linter as much.</p>

<p>Oh, well… It seems that Microsoft are not really particularly invested in
supporting the tooling for F#, as pretty much all the major projects in this
space are community-driven.</p>

<p>Using AI coding agents (e.g. Copilot) with F# worked pretty well, but I didn’t
spend much time on this front.</p>

<p>In the end of the day any editor will likely do, as long as you’re using LSP.</p>

<p>By the way, I had an interesting observation while programming in F# (and OCaml for that matter) -
that when you’re working with a language with a really good type system you don’t really need that much
from your editor. Most the time I’m perfectly happy with just some inline type information (e.g. something like CodeLenses), auto-completion and the ability to easily send code to <code>fsi</code>. Simplicity continues
to be the ultimate sophistication…</p>

<p>Other tools that should be on your radar are:</p>

<ul>
  <li><a href="https://fsprojects.github.io/Paket/">Paket</a> - Paket is a dependency manager for .NET projects. Think of it as something like <code>bundler</code>, <code>npm</code> or <code>pip</code>, but for .NET’s NuGet package ecosystem.</li>
  <li><a href="https://fake.build/">FAKE</a> -  A DSL for build tasks and more, where you can use F# to specify the tasks. Somewhat similar to Ruby’s <code>rake</code>. Some people claim that’s the easiest way to sneak F# into an existing .NET project.</li>
</ul>

<h2 id="use-cases">Use Cases</h2>

<p>Given the depth and breath of .NET - I guess that sky is the limit for you!</p>

<p>Seems to me that F# will be a particularly good fit for data analysis and manipulation, because
of features like <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/tutorials/type-providers/">type providers</a>.</p>

<p>Probably a good fit for backend services and even full-stack apps, although I haven’t really played
with the F# first solutions in this space yet.</p>

<p>Fable and Elmish make F# a viable option for client-side programming and might offer
another easy way to sneak F# into your day-to-day work.</p>

<p><strong>Note:</strong> Historically, Fable has been used to target JavaScript but since Fable
4, you can also target other languages such as TypeScript, Rust, Python, and
more.</p>

<p>Here’s how easy it is to transpile an F# codebase into something else:</p>

<div><pre><code><span># If you want to transpile to JavaScript</span>
dotnet fable

<span># If you want to transpile to TypeScript</span>
dotnet fable <span>--lang</span> typescript

<span># If you want to transpile to Python</span>
dotnet fable <span>--lang</span> python
</code></pre></div>

<p>Cool stuff!</p>



<p>My initial impression of the community is that it’s fairly small, perhaps even
smaller than that of OCaml.  The F# Reddit and Discord (the one listed on
Reddit) seem like the most active places for F# conversations. There’s supposed
to be some F# Slack as well, but I couldn’t get an invite for it. (seems the
automated process for issuing those invites has been broken for a while)</p>

<p>I’m still not sure what’s the role Microsoft plays in the community, as I
haven’t seen much from them overall.</p>

<p>For a me a small community is not really a problem, as long as the community is
vibrant and active. Also - I’ve noticed I always feel more connected to smaller
communities. Moving from Java to Ruby back in the day felt like night and day as
far as community engagement and sense of belonging go.</p>

<p>I didn’t find many books and community sites/blogs dedicated to F#, but I didn’t
really expect to in the first place.</p>

<p>The most notable community initiatives I discovered were:</p>

<ul>
  <li><a href="https://amplifyingfsharp.io/">Amplifying F#</a> - an effort to promote F# and to get more businesses involved with it</li>
  <li><a href="https://fsharpforfunandprofit.com/">F# for Fun and Profit</a> - a collection of tutorials and essays on F#</li>
  <li><a href="https://fslab.org/">F# Lab</a> - The community driven toolkit for datascience in F#</li>
  <li><a href="https://sergeytihon.com/category/f-weekly/">F# Weekly</a> - a weekly newsletter about the latest developments in the world of F#</li>
</ul>

<p>Seems to me that more can be done to promote the language and engage new programmers and businesses
with it, although that’s never easy 20 years into the existence of some project. I continue to be
somewhat puzzled as to why Microsoft doesn’t market F# more, as I think it could be a great
marketing vehicle for them.</p>

<p>All in all - I don’t feel qualified to comment much on the F# community at this point.</p>

<h2 id="the-popularity-contest">The Popularity Contest</h2>

<p>Depending on the type of person you are you may or may not care about a a programming language’s
“popularity”. People often ask my why I spent a lot of time with languages that are unlikely to
ever result in job opportunities for me, e.g.:</p>

<ul>
  <li>Emacs Lisp</li>
  <li>Clojure</li>
  <li>OCaml</li>
  <li>F#</li>
</ul>

<p>Professional opportunities are important, of course, but so are:</p>

<ul>
  <li>having fun (and the F in F# stands for “fun”)</li>
  <li>learning new paradigms and ideas</li>
  <li>challenging yourself to think and work differently</li>
</ul>

<p>That being said, F# is not a popular language by most conventional metrics. It’s not highly ranked
on TIOBE, StackOverflow or most job boards. But it’s also not less popular than most “mainstream”
functional programming languages. The sad reality is that functional programming is still not
mainstream and perhaps it will never be.</p>

<p>A few more resources on the subject:</p>

<ul>
  <li><a href="https://medium.com/@lanayx/about-f-popularity-c9b78ed89252">About F#’s popularity</a></li>
  <li><a href="https://hamy.xyz/blog/2024-11_fsharp-popularity">How Popular is F# in 2024</a>
    <ul>
      <li>Here’s also a <a href="https://www.youtube.com/watch?v=JioaHcy_QE0&amp;t=1s">video</a> for the article above</li>
    </ul>
  </li>
</ul>

<h2 id="f-vs-ocaml">F# vs OCaml</h2>

<blockquote>
  <p>The early conception of F# was simple: to bring the benefits of OCaml to .NET and .NET to OCaml: a
marriage between strongly typed functional programming and .NET. Here “OCaml” meant both the
core of the language itself, and the pragmatic approach to strongly-typed functional programming
it represented. The initial task was relatively well-defined: I would re-implement the core of the
OCaml language and a portion of its base library to target the .NET Common Language Runtime.
The implementation would be fresh, i.e. not using any of the OCaml codebase, for legal clarity.</p>

  <p>– Don Syme, creator of F#, The Early History of F#</p>
</blockquote>

<p>F# was derived from OCaml, so the two languages share a lot of DNA. Early on
F# made some efforts to support as much of OCaml’s syntax as possible, and it
even allowed the use of <code>.ml</code> and <code>.mli</code> file extensions for F# code. Over time
the languages started to diverge a bit, though.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup></p>

<p>Creating a language that’s independent from OCaml, of course, was something
intended from the very beginning. That’s also reflected in the decision
to chose the name F#, even if early versions of the language were called “Caml.NET”:</p>

<blockquote>
  <p>Although the first version of F# was initially presented as “Caml-for-.NET”,
in reality it was always a new language, designed for .NET from day 1. F# was
never fully compatible with any version of OCaml, though it shared a compatible
subset, and it took Caml-Light and OCaml as its principal sources of design
guidance and inspiration.</p>

  <p>– Don Syme, The Early History of F#</p>
</blockquote>

<p>If you ask most people about the pros and cons of F# over OCaml you’ll probably
get the following answers.</p>

<p><strong>F# Pros</strong></p>

<ul>
  <li>Runs on .NET
    <ul>
      <li>Tons of libraries are at disposal</li>
    </ul>
  </li>
  <li>Backed by Microsoft</li>
  <li>Arguably it’s a bit easier to learn by newcomers (especially those who have only experience with OO programming)
    <ul>
      <li>The syntax is slightly easier to pick up (I think)</li>
      <li>The compiler errors and warnings are “friendlier” (easier to understand)</li>
      <li>It’s easier to debug problems (partially related to the previous item)</li>
    </ul>
  </li>
  <li>Strong support for <a href="https://learn.microsoft.com/en-us/dotnet/fsharp/tutorials/async">async programming</a></li>
  <li>Has some cool features, absent in OCaml, like:
    <ul>
      <li>Anonymous Records</li>
      <li>Active Patterns</li>
      <li>Computational expressions</li>
      <li>Sequence comprehensions</li>
      <li>Type Providers</li>
      <li>Units of measure</li>
    </ul>
  </li>
</ul>

<p><strong>F# Cons</strong></p>

<ul>
  <li>Runs on .NET
    <ul>
      <li>The interop with .NET influenced a lot of language design decisions (e.g. allowing <code>null</code>)</li>
    </ul>
  </li>
  <li>Backed by Microsoft
    <ul>
      <li>Not everyone likes Microsoft</li>
      <li>Seems the resources allocated to F# by Microsoft are modest</li>
      <li>It’s unclear how committed Microsoft will be to F# in the long run</li>
    </ul>
  </li>
  <li>Naming conventions: I like <code>snake_case</code> way more than <code>camelCase</code> and <code>PascalCase</code></li>
  <li>Misses some cool OCaml features
    <ul>
      <li>First-class modules and functors</li>
      <li>GADTs</li>
    </ul>
  </li>
  <li>Doesn’t have a friendly camel logo</li>
  <li>The name F# sounds cool, but is a search and filename nightmare (and you’ll see FSharp quite often in the wild)</li>
</ul>

<p>Both F# and OCaml can also target JavaScript runtimes as well - via <a href="https://fable.io/">Fable</a> on
the F# side and Js_of_ocaml and Melange on the OCaml side. Fable seems like a
more mature solution at a cursory glance, but I haven’t used any of the three
enough to be able to offer an informed opinion.</p>

<p>In the end of the day both remain two fairly similar robust, yet niche,
languages, which are unlikely to become very popular in the future. I’m guessing
working professionally with F# is more likely to happen for most people, as .NET
is super popular and I can imagine it’d be fairly easy to sneak a bit of F# here
in there in established C# codebases.</p>

<p>One weird thing I’ve noticed with F# projects is that they still use XML project
manifests, where you have to list the source files manually in the order in
which they should be compiled (to account for the dependencies between them). I
am a bit shocked that the compiler can’t handle the dependencies automatically,
but I guess that’s because in F# there’s not direct mapping between source files
and modules. At any rate - I prefer the OCaml compilation process (and Dune) way
more.</p>

<p>As my interest in MLs is mostly educational I’m personally leaning towards OCaml, but if I had to build
web services with an ML language I’d probably pick F#. I also have a weird respect for every language
with its own runtime, as this means that it’s unlikely that the runtime will force some compromises
on the language.</p>

<h2 id="closing-thoughts">Closing thoughts</h2>

<p>All in all I liked F# way more than I expected to! In a way it reminded me of my
experience with Clojure back in the day in the sense that Clojure was the most
practical Lisp out there when it was released, mostly because of its great
interop with Java.</p>

<p>I have a feeling that if .NET was portable since day 1 probably ClojureCLR would have become
as popular as Clojure, and likely F# would have developed a bigger community and
broader usage by now. I’m fairly certain I would have never dabbled in .NET again
if it hadn’t been for .NET Core, and I doubt I’m the only one.</p>

<p>Learning OCaml is definitely not hard, but I think that people interested to learn some ML
dialect might have an easier time with F#. And, as mentioned earlier, you’ll probably have an
easier path to “production” with it.</p>

<p>I think that everyone who has experience with .NET will benefit from learning F#.
Perhaps more importantly - everyone looking to do more with an ML family language
should definitely consider F#, as it’s a great language in its own right, that gives
you access to one of the most powerful programming platforms out there.</p>

<p>Let’s not forget about <a href="https://fable.io/">Fable</a>, which makes it possible for you leverage
F# in JavaScript, Dart, Rust and Python runtimes!</p>

<p>So, why F#? In the F# community there’s the saying that the “F” in F# stands for
“Fun”. In my brief experience with F# I found this to be very true! I’ll go a
step further and make the claim that F# is both seriously <strong>fun</strong> and seriously
practical!</p>

<p>Also if your code compiles - it will probably work the way you expect it to. I
hear that’s generally considered a desirable thing in the world of programming!</p>

<p>That’s all I have for you today. Please, share in the comments what do you love about F#!</p>

<p>In sane type systems we trust!</p>

<h2 id="discussions">Discussions</h2>

<ul>
  <li><a href="https://news.ycombinator.com/item?id=43546004">Hacker News</a></li>
  <li><a href="https://lobste.rs/s/kubso9/why_f">Lobsters</a></li>
</ul>



        
      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Electron Band Structure in Germanium, My Ass (725 pts)]]></title>
            <link>https://pages.cs.wisc.edu/~kovar/hall.html</link>
            <guid>43545917</guid>
            <pubDate>Tue, 01 Apr 2025 12:25:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pages.cs.wisc.edu/~kovar/hall.html">https://pages.cs.wisc.edu/~kovar/hall.html</a>, See on <a href="https://news.ycombinator.com/item?id=43545917">Hacker News</a></p>
<div id="readability-page-1" class="page">
<base target="top">
<basefont size="3">

<b></b>
<p><b> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Abstract: The exponential dependence of resistivity on temperature 
in germanium is found to be a great big lie.  My careful theoretical modeling and painstaking experimentation reveal 1) that my equipment 
is crap, as are all the available texts on the subject and 2) that this whole exercise was a complete waste of my 
time.
</b>
 </p>

<h3><u>Introduction</u></h3> 

<p> 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  Electrons in germanium are confined to 
well-defined energy bands that are separated by "forbidden regions" of zero charge-carrier density.  You can 
read about it yourself if you want to, although I don't recommend it.  You'll have to wade through an obtuse, convoluted discussion about considering an arbitrary number of 
non-coupled harmonic-oscillator potentials and taking limits and so on.  The upshot is that if you heat up a sample of germanium, electrons will jump from a
non-conductive energy band to a conductive one, thereby creating a measurable change in resistivity.  This 
relation between temperature and resistivity can be shown to be exponential in certain temperature regimes 
by waving your hands and chanting "to first order".  
 </p>
<h3><u>Experiment procedure</u></h3> 

<p>
&nbsp; &nbsp; &nbsp; I sifted through the box of germanium crystals and chose the one that appeared 
to be the least cracked.  Then I soldered wires onto the crystal in the spots shown in figure 2b of Lab Handout 
32.  Do you have any idea how hard it is to solder wires to germanium?  I'll tell you: real goddamn hard.  The 
solder simply won't stick, and you can forget about getting any of the grad students in the solid state labs to 
help you out. 
<br> 
&nbsp; &nbsp; &nbsp;  Once the wires were in place, I attached them as appropriate to the second-rate  
equipment I scavenged from the back of the lab, none of which worked properly.  I soon wised up and swiped 
replacements from the well-stocked research labs.   This is how they treat undergrads around here: they give 
you broken tools and then don't understand why  you don't get any results.
 <br>

<table>
<tbody><tr>
<td rowspan="3">&nbsp;</td>
<td>
<img src="https://pages.cs.wisc.edu/~kovar/fittedHall.gif" width="351" height="285">
</td></tr>
<tr><td></td></tr>
<tr><td>
<b><span face="Arial, Geneva, Helvetica, sans-serif" size="2">
Fig. 1: Check this shit out.
</span></b>
</td></tr></tbody></table>


&nbsp; &nbsp; &nbsp;  In order to control the temperature of the germanium, I attached the crystal to a 
copper rod, the upper end of which was attached to a heating coil and the lower end of which was dipped in 
a thermos of liquid nitrogen.   Midway through the project, the thermos began leaking.  That's right: I pay a cool ten grand a quarter to 
come here, and yet they can't spare the five bucks to ensure that I have a working thermos.  
</p>
<h3><u>Results</u></h3> 

<p>
&nbsp; &nbsp; &nbsp; Check this shit out (Fig. 1).  That's bonafide, 100%-real data, my friends.  I took it 
myself over the course of two weeks.  And this was not a leisurely two weeks, either; I busted my
ass day and night in order to provide you with nothing but the best data possible.   Now, let's look a bit more closely
at this data, remembering that it is absolutely first-rate.  Do you see the exponential dependence?  I sure don't.  I see a bunch of crap.<br>
&nbsp; &nbsp; &nbsp; Christ, this was such a waste of my time. <br>
&nbsp; &nbsp; &nbsp;  Banking on my hopes that whoever grades this will just look at the pictures, I drew an 
exponential through my noise.  I believe the apparent legitimacy is enhanced by the fact that I used a complicated computer program
to make the fit.  I understand this is the same process by which the top quark was discovered.
</p>
<h3><u>Conclusion</u></h3> 

<p>
&nbsp; &nbsp; &nbsp; Going into physics was the biggest mistake of my life.  I should've declared CS.  I still 
wouldn't have any women, but at least I'd be rolling in cash.
</p>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[CERN scientists find evidence of quantum entanglement in sheep (290 pts)]]></title>
            <link>https://home.cern/news/news/physics/cern-scientists-find-evidence-quantum-entanglement-sheep</link>
            <guid>43545349</guid>
            <pubDate>Tue, 01 Apr 2025 11:08:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://home.cern/news/news/physics/cern-scientists-find-evidence-quantum-entanglement-sheep">https://home.cern/news/news/physics/cern-scientists-find-evidence-quantum-entanglement-sheep</a>, See on <a href="https://news.ycombinator.com/item?id=43545349">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <figure id="CERN-PHOTO-201706-157-2"><a href="https://cds.cern.ch/images/CERN-PHOTO-201706-157-2" title="View on CDS"><img alt="Life at CERN" src="https://cds.cern.ch/images/CERN-PHOTO-201706-157-2/file?size=large"></a><figcaption>The CERN flock of sheep on site in 2017.<span> (Image: CERN)</span></figcaption></figure>
<p>Quantum entanglement is a fascinating phenomenon where two particles’ states are tied to each other, no matter how far apart the particles are. In 2022, the <a href="https://home.cern/news/news/knowledge-sharing/cern-congratulates-winners-2022-nobel-prize-physics">Nobel Prize in Physics</a> was awarded to Alain Aspect, John F. Clauser and Anton Zeilinger for groundbreaking experiments involving entangled photons. These experiments confirmed the predictions for the manifestation of entanglement that had been made by the <a href="https://home.cern/news/news/physics/fifty-years-bells-theorem">late CERN theorist John Bell</a>. This phenomenon has so far been observed in a wide variety of systems, such as in top quarks at <a href="https://home.cern/news/press-release/physics/lhc-experiments-cern-observe-quantum-entanglement-highest-energy-yet">CERN’s Large Hadron Collider</a> (LHC) in 2024. Entanglement has also found several important societal applications, such as quantum cryptography and quantum computing. Now, it also explains the famous herd mentality of sheep.</p>

<p>A flock of sheep (ovis aries) has roamed the CERN site during the spring and summer months <a href="https://cds.cern.ch/record/970008?ln=en">for over 40 years</a>. Along with the CERN shepherd, they help to maintain the vast expanses of grassland around the LHC and are part of the Organization’s long-standing <a href="https://home.cern/news/news/cern/environmental-awareness-exploring-cerns-biodiversity">efforts to protect the site’s biodiversity</a>. In addition, their <a href="https://www.nature.com/articles/s41567-022-01769-8">flocking behaviour</a> has been of great interest to CERN's physicists. It is well known that sheep <a href="https://physicsworld.com/a/field-work-the-physics-of-sheep-from-phase-transitions-to-collective-motion/">behave like particles</a>: their stochastic behaviour has been studied by zoologists and physicists alike, who noticed that a flock’s ability to quickly change phase is similar to that of atoms in a solid and a liquid. Known as the Lamb Shift, this can cause them to get themselves into bizarre situations, such as walking in a circle for <a href="https://www.abc.net.au/news/science/2022-11-22/sheep-circling-mystery-could-have-simple-explanation/101682672">days on end.</a></p>

<p>Now, new research has shed light on the reason for these extraordinary abilities. Scientists at CERN have found evidence of quantum entanglement in sheep. Using sophisticated modelling techniques and specialised trackers, the findings show that the brains of individual sheep in a flock are quantum-entangled in such a way that the sheep can move and vocalise simultaneously, no matter how far apart they are. The evidence has several ramifications for ovine research and has set the baa for a new branch of quantum physics.</p>

<p>“The fact that we were having our lunch next to the flock was a shear coincidence,” says Mary Little, leader of the <a href="https://greybook.cern.ch/experiment/detail?id=HERD">HERD collaboration</a>, describing how the project came about. “When we saw and herd their behaviour, we wanted to investigate the movement of the flock using the technology at our disposal at the Laboratory.”</p>

<p>Observing the sheep’s ability to simultaneously move and vocalise together caused one main question to aries: since the sheep behave like subatomic particles, could quantum effects be the reason for their behaviour?</p>

<p>“Obviously, we couldn’t put them all in a box and see if they were dead or alive,” said Beau Peep, a researcher on the project. “However, by assuming that the sheep were spherical, we were able to model their behaviour in almost the exact same way as we model subatomic particles.”</p>

<p>Using sophisticated trackers, akin to those in the LHC experiments, the physicists were able to locate the precise particles in the sheep’s brains that might be the cause of this entanglement. Dubbed “moutons” and represented by the Greek letter lambda, l, these particles are leptons and are close relatives of the muon, but fluffier.</p>

<p>The statistical significance of the findings is 4 <a href="https://home.cern/resources/faqs/five-sigma">sigma</a>, which is enough to show evidence of the phenomenon. However, it does not quite pass the baa to be classed as an observation.</p>

<p>“More research is needed to fully confirm that this was indeed an observation of ovine entanglement or a statistical fluctuation,” says Ewen Woolly, spokesperson for the HERD collaboration. “This may be difficult, as we have found that the research makes physicists become inexplicably drowsy.”</p>

<p>“While entanglement is now the leading theory for this phenomenon, we have to take everything into account,” adds Dolly Shepherd, a CERN theorist. “Who knows, maybe further variables are hidden beneath their fleeces. Wolves, for example.”</p>

<figure id="CERN-HOMEWEB-PHO-2025-028-1"><a href="https://cds.cern.ch/images/CERN-HOMEWEB-PHO-2025-028-1" title="View on CDS"><img alt="home.cern,Life at CERN" src="https://cds.cern.ch/images/CERN-HOMEWEB-PHO-2025-028-1/file?size=large"></a>
<figcaption>Theoretical physicist John Ellis, pioneer of the penguin diagram, with its updated sheep version. Scientists at CERN find evidence of quantum entanglement in sheep in 2025, the year declared by the United Nations as the <a href="https://home.cern/news/news/knowledge-sharing/official-launch-quantum-year">International Year of Quantum Science and Technology.</a>&nbsp;<span>(Image: CERN)</span></figcaption></figure>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Self-Hosting like it's 2025 (202 pts)]]></title>
            <link>https://kiranet.org/self-hosting-like-its-2025/</link>
            <guid>43544979</guid>
            <pubDate>Tue, 01 Apr 2025 10:11:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kiranet.org/self-hosting-like-its-2025/">https://kiranet.org/self-hosting-like-its-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=43544979">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[US accidentally sent Maryland father to Salvadorian prison, can't get him back (195 pts)]]></title>
            <link>https://www.independent.co.uk/news/world/americas/us-politics/trump-el-salvador-abrego-garcia-b2725002.html</link>
            <guid>43544534</guid>
            <pubDate>Tue, 01 Apr 2025 09:14:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.independent.co.uk/news/world/americas/us-politics/trump-el-salvador-abrego-garcia-b2725002.html">https://www.independent.co.uk/news/world/americas/us-politics/trump-el-salvador-abrego-garcia-b2725002.html</a>, See on <a href="https://news.ycombinator.com/item?id=43544534">Hacker News</a></p>
Couldn't get https://www.independent.co.uk/news/world/americas/us-politics/trump-el-salvador-abrego-garcia-b2725002.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA['A hostile state': Why some travellers are avoiding the US (116 pts)]]></title>
            <link>https://www.bbc.com/travel/article/20250328-the-people-boycotting-travel-to-the-us</link>
            <guid>43544293</guid>
            <pubDate>Tue, 01 Apr 2025 08:39:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/travel/article/20250328-the-people-boycotting-travel-to-the-us">https://www.bbc.com/travel/article/20250328-the-people-boycotting-travel-to-the-us</a>, See on <a href="https://news.ycombinator.com/item?id=43544293">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0l11gb2.jpg.webp 160w,https://ichef.bbci.co.uk/images/ic/240xn/p0l11gb2.jpg.webp 240w,https://ichef.bbci.co.uk/images/ic/320xn/p0l11gb2.jpg.webp 320w,https://ichef.bbci.co.uk/images/ic/480xn/p0l11gb2.jpg.webp 480w,https://ichef.bbci.co.uk/images/ic/640xn/p0l11gb2.jpg.webp 640w,https://ichef.bbci.co.uk/images/ic/800xn/p0l11gb2.jpg.webp 800w,https://ichef.bbci.co.uk/images/ic/1024xn/p0l11gb2.jpg.webp 1024w,https://ichef.bbci.co.uk/images/ic/1376xn/p0l11gb2.jpg.webp 1376w,https://ichef.bbci.co.uk/images/ic/1920xn/p0l11gb2.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0l11gb2.jpg.webp" loading="eager" alt="Getty Images A line of American flags on poles through a wire fence (Credit: Getty Images)"><span>Getty Images</span></p></div><p data-component="caption-block"><figcaption>(Credit: Getty Images)<!-- --></figcaption></p></figure><p><b id="as-the-list-of-nations-issuing-travel-warnings-to-the-us-grows,-some-visitors-are-opting-to-boycott-it-entirely.-here's-why-many-foreigners-are-changing-their-travel-plans-and-what-this-could-mean-for-americans.">As the list of nations issuing travel warnings to the US grows, some visitors are opting to boycott it entirely. Here's why many foreigners are changing their travel plans and what this could mean for Americans.<!-- --></b></p><p>The cold shoulder has been particularly noticeable from the US's northern neighbour, Canada, which sends more than <!-- --><a target="_blank" href="https://www.ustravel.org/press/potential-results-decline-canadian-travel-united-states#:~:text=Canada%20is%20the%20top%20source,spending%20and%2014%2C000%20job%20losses.">20 million visitors<!-- --></a> to the country per year – more than any other nation. In response to Trump's proposed <!-- --><a target="_self" href="https://www.bbc.com/news/articles/cn93e12rypgo">tariffs<!-- --></a> and repeated threats to <!-- --><a target="_self" href="https://www.bbc.com/news/articles/czx82j5wd8vo">annex the nation<!-- --></a>, former prime minister Justin Trudeau recently urged his fellow Canadians: "Now is the time to choose Canada," adding, "it might mean changing your summer vacation plans to stay here in Canada."<!-- --></p><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0l11ghw.jpg.webp 160w,https://ichef.bbci.co.uk/images/ic/240xn/p0l11ghw.jpg.webp 240w,https://ichef.bbci.co.uk/images/ic/320xn/p0l11ghw.jpg.webp 320w,https://ichef.bbci.co.uk/images/ic/480xn/p0l11ghw.jpg.webp 480w,https://ichef.bbci.co.uk/images/ic/640xn/p0l11ghw.jpg.webp 640w,https://ichef.bbci.co.uk/images/ic/800xn/p0l11ghw.jpg.webp 800w,https://ichef.bbci.co.uk/images/ic/1024xn/p0l11ghw.jpg.webp 1024w,https://ichef.bbci.co.uk/images/ic/1376xn/p0l11ghw.jpg.webp 1376w,https://ichef.bbci.co.uk/images/ic/1920xn/p0l11ghw.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0l11ghw.jpg.webp" loading="lazy" alt="Alamy Justin Trudeau recently urged Canadians to stay in Canada instead of travelling to the US (Credit: Alamy)"><span>Alamy</span></p></div><p data-component="caption-block"><figcaption>Justin Trudeau recently urged Canadians to stay in Canada instead of travelling to the US (Credit: Alamy)<!-- --></figcaption></p></figure><p>The appeal seems to have caught on, as many infuriated Canadians are now <!-- --><a target="_blank" href="https://www.wsj.com/lifestyle/travel/trump-canada-vacation-travel-plans-1428d574">boycotting US holidays<!-- --></a>. In February, border crossings were down by more than 20%, <!-- --><a target="_blank" href="https://www150.statcan.gc.ca/n1/daily-quotidien/250310/dq250310d-eng.htm">according to Statistics Canada<!-- --></a>. The US Travel Association estimates that even a 10% reduction in Canadian visitors could result in $2.1bn in lost spending and 14,000 job losses.<!-- --></p><p>While some Canadians are snubbing the US because of policy changes, others say they simply don't feel as safe as they once did.<!-- --></p><p>"My partner and I decided not to go ahead with our planned vacations to the US this year," said Canadian travel journalist Kate Dingwall. "I worry about the border and getting stuck somehow, especially with how prickly Trump is to Canada. There's just a sense of uneasiness around visiting America at the moment."&nbsp;<!-- --></p><p>Keith Serry, a writer and comedian based in Montreal, Quebec, cancelled five April appearances in New York City (including four shows at the upcoming <!-- --><a target="_blank" href="https://frigid.nyc/new-york-city-fringe/">New York City Fringe<!-- --></a> festival) due to the tense political situation.<!-- --></p><p>"This decision will, of course, rob me of the opportunity to share my art with many of you in New York I've grown to know and love," he wrote on his <!-- --><a target="_blank" href="https://www.facebook.com/volumeknob/posts/pfbid0eCymJvBJNoTt2D5zM45upv4RznMsqN6REKag4eEdE4h3umj7CVPhVuZ65KVxJmBpl?rdid=aN7hg2gEiAusfGtb">Facebook page<!-- --></a>. "That said, the honest truth is that I just don't feel safe travelling to the States right now. In addition, I feel a powerful disinclination to spending my money in any way that might aid the economy of a hostile state."<!-- --></p><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0l11gmr.jpg.webp 160w,https://ichef.bbci.co.uk/images/ic/240xn/p0l11gmr.jpg.webp 240w,https://ichef.bbci.co.uk/images/ic/320xn/p0l11gmr.jpg.webp 320w,https://ichef.bbci.co.uk/images/ic/480xn/p0l11gmr.jpg.webp 480w,https://ichef.bbci.co.uk/images/ic/640xn/p0l11gmr.jpg.webp 640w,https://ichef.bbci.co.uk/images/ic/800xn/p0l11gmr.jpg.webp 800w,https://ichef.bbci.co.uk/images/ic/1024xn/p0l11gmr.jpg.webp 1024w,https://ichef.bbci.co.uk/images/ic/1376xn/p0l11gmr.jpg.webp 1376w,https://ichef.bbci.co.uk/images/ic/1920xn/p0l11gmr.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0l11gmr.jpg.webp" loading="lazy" alt="Alamy Some international travellers have been inexplicably detained by US officials (Credit: Alamy)"><span>Alamy</span></p></div><p data-component="caption-block"><figcaption>Some international travellers have been inexplicably detained by US officials (Credit: Alamy)<!-- --></figcaption></p></figure><p>In recent months, some Canadians have faced tougher border security than ever before – even those with clean records and valid documents. One woman made international headlines as <!-- --><a target="_blank" href="https://www.theguardian.com/us-news/2025/mar/19/canadian-detained-us-immigration-jasmine-mooney">she was detained by ICE for two weeks<!-- --></a> in reportedly bleak conditions after her visa was revoked.<!-- --></p><p>"We're a country that prides itself, imperfectly but intentionally, on values like inclusion, equity and human rights. When those values feel out of step with what's happening across the border, it becomes harder to justify participation," said Amar Charles Marouf, a Canadian citizen who works as growth strategist at the international law firm Gowling WLG. "The political climate raises broader questions. What kind of treatment are we normalising? What assumptions are being made about who is welcome and who isn’t?"<!-- --></p><p>It's not that Canadians aren't traveling internationally; many are simply swapping their traditional destinations. Marouf says Mexico, South America and Europe all feel more welcoming at the moment. "California will be there when this is all over, but I just feel like there's less risk going to Portugal instead," added Dingwall.<!-- --></p><p>Some international properties have seen a bump in traffic, directly attributable to Canadians now spurning the US. "In the last several weeks, we've received 10-plus leads from Canadians seeking to relocate events from the United States this summer," said Diarmaid O'Sullivan, director of sales &amp; marketing at <!-- --><a target="_blank" href="https://www.thehamiltonprincess.com/">Hamilton Princess Hotel &amp; Beach Club Bermuda<!-- --></a>. "This is a mix of leisure travellers, including those planning weddings, and companies relocating business events. This represents an approximately 20% increase in forecasted revenue from the Canadian market."<!-- --></p><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0l11gtf.jpg.webp 160w,https://ichef.bbci.co.uk/images/ic/240xn/p0l11gtf.jpg.webp 240w,https://ichef.bbci.co.uk/images/ic/320xn/p0l11gtf.jpg.webp 320w,https://ichef.bbci.co.uk/images/ic/480xn/p0l11gtf.jpg.webp 480w,https://ichef.bbci.co.uk/images/ic/640xn/p0l11gtf.jpg.webp 640w,https://ichef.bbci.co.uk/images/ic/800xn/p0l11gtf.jpg.webp 800w,https://ichef.bbci.co.uk/images/ic/1024xn/p0l11gtf.jpg.webp 1024w,https://ichef.bbci.co.uk/images/ic/1376xn/p0l11gtf.jpg.webp 1376w,https://ichef.bbci.co.uk/images/ic/1920xn/p0l11gtf.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0l11gtf.jpg.webp" loading="lazy" alt="Alamy Ironically, the US had recently been touted as the world's top tourism market (Credit: Alamy)"><span>Alamy</span></p></div><p data-component="caption-block"><figcaption>Ironically, the US had recently been touted as the world's top tourism market (Credit: Alamy)<!-- --></figcaption></p></figure><p>Interestingly, all of this is happening just as the US had been experiencing record growth in the travel and tourism sector. Marta Soligo, professor at UNLV and director of Tourism Research at the UNLV Office of economic development, points to The World Travel &amp; Tourism Council (WTTC)'s <!-- --><a target="_blank" href="https://wttc.org/research/economic-impact">2024 Economic Impact Trends Report<!-- --></a>, which showed <!-- --><a target="_self" href="https://www.bbc.com/travel/article/20240617-why-the-us-is-the-top-country-for-tourism-in-2024">the US as the world's top travel and tourism market<!-- --></a>. Not only did the industry achieve an unprecedented economic impact of $2.36tn in the US, but the US Bureau of Labor Statistics projections indicate that the US economy will add more than 800,000 jobs in the leisure and hospitality sector.<!-- --></p><p>"Such numbers help us understand the importance of the sector in the US, showing that a decrease in tourism from top-origin countries could significantly damage the US economy," said Soligo. "The consequences of these issues can impact both large US-based corporations, such as hotel chains, and small businesses alike."&nbsp;<!-- --></p><p>The change in travel patterns is already having a tangible effect on the US economy. <!-- --><a target="_blank" href="https://www.tourismeconomics.com/press/latest-research/escalating-trade-war-threatens-us-travel-sector/">Tourism Economics<!-- --></a> recently updated its inbound US travel forecast from a predicted 8.8% growth, to a 5.1% decline, attributing the change to "strained" travel sentiment, "sweeping tariffs" and "exchange rate shifts" making travel to the US more expensive.&nbsp;<!-- --></p><p>"The combination of travel bans and a reduction of US travel could have a material impact on tourism and economic development," said Jeff Le, former deputy cabinet secretary for the State of California and the state's federal coordinator during the first Trump Administration.<!-- --></p><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0l11gzq.jpg.webp 160w,https://ichef.bbci.co.uk/images/ic/240xn/p0l11gzq.jpg.webp 240w,https://ichef.bbci.co.uk/images/ic/320xn/p0l11gzq.jpg.webp 320w,https://ichef.bbci.co.uk/images/ic/480xn/p0l11gzq.jpg.webp 480w,https://ichef.bbci.co.uk/images/ic/640xn/p0l11gzq.jpg.webp 640w,https://ichef.bbci.co.uk/images/ic/800xn/p0l11gzq.jpg.webp 800w,https://ichef.bbci.co.uk/images/ic/1024xn/p0l11gzq.jpg.webp 1024w,https://ichef.bbci.co.uk/images/ic/1376xn/p0l11gzq.jpg.webp 1376w,https://ichef.bbci.co.uk/images/ic/1920xn/p0l11gzq.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0l11gzq.jpg.webp" loading="lazy" alt="Alamy International travellers to California contributed $24bn in 2023 (Credit: Alamy)"><span>Alamy</span></p></div><p data-component="caption-block"><figcaption>International travellers to California contributed $24bn in 2023 (Credit: Alamy)<!-- --></figcaption></p></figure><p>In California alone, international visitors provided more than $24bn in local economies in 2023, reports Le. "Losing this would likely hurt both people and local government funds," he said. "It is harder to measure reputational loss, but I cannot imagine active boycotts help."<!-- --></p><p>While the broad impact of boycotting travel to the US could have substantial national and governmental effects, some advocacy organisations also warn that the loss of tourism dollars is most likely to be felt by individuals.<!-- --></p><p>"It's US workers and small businesses who get affected by US travel boycotts," said Thomas F Goodwin, leader of the <!-- --><a target="_blank" href="https://www.exhibitionsconferencesalliance.org/">Exhibitions and Conferences Alliance<!-- --></a>. Goodwin notes that more than 99% of the US business and professional events industry is made up of small businesses, and more than 80% of all exhibitors are US and international small businesses. "When international business travellers forgo coming to the US, everyone from exposition booth builders and general service contractors to venue caterers and individual skilled labourers suffer – not politicians or the government. Boycotts have a knock-on effect on hotels, taxis, restaurants, local entertainers, high street commerce and more."<!-- --></p><p>There is also a cost that is harder to quantify: the global influence of American culture.<!-- --></p><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0l11h6t.jpg.webp 160w,https://ichef.bbci.co.uk/images/ic/240xn/p0l11h6t.jpg.webp 240w,https://ichef.bbci.co.uk/images/ic/320xn/p0l11h6t.jpg.webp 320w,https://ichef.bbci.co.uk/images/ic/480xn/p0l11h6t.jpg.webp 480w,https://ichef.bbci.co.uk/images/ic/640xn/p0l11h6t.jpg.webp 640w,https://ichef.bbci.co.uk/images/ic/800xn/p0l11h6t.jpg.webp 800w,https://ichef.bbci.co.uk/images/ic/1024xn/p0l11h6t.jpg.webp 1024w,https://ichef.bbci.co.uk/images/ic/1376xn/p0l11h6t.jpg.webp 1376w,https://ichef.bbci.co.uk/images/ic/1920xn/p0l11h6t.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0l11h6t.jpg.webp" loading="lazy" alt="Alamy Hotels, restaurants and local businesses are likely to feel the effect of travel boycotts (Credit: Alamy)"><span>Alamy</span></p></div><p data-component="caption-block"><figcaption>Hotels, restaurants and local businesses are likely to feel the effect of travel boycotts (Credit: Alamy)<!-- --></figcaption></p></figure><p>"Visitor numbers declining in the US sends a signal that the US is losing its soft power – the influence it once held through openness, cultural leadership and global goodwill," said Neri Karra Sillaman, entrepreneurship expert at the University of Oxford and author of the forthcoming book <!-- --><a target="_blank" href="https://www.pioneersbook.com/">Pioneers: 8 Principles of Business Longevity from Immigrant Entrepreneurs<!-- --></a>. "If this trend continues, it may force tourism boards or even local governments to create counter narratives to win back trust. I am not sure if it will be effective, however, since at the end of the day, it is the policies of the government that give the ultimate direction and influence perception."&nbsp;<!-- --></p><p>Sillaman notes that travel boycotts also have a long-term cost to culture, the economy and overall innovation. "When academics, scientists, artists, designers and entrepreneurs start to choose other countries instead of US, the US is going to lose more than just visitors," she said. "In the long term, it's going to lose its competitiveness, goodwill and it will turn into a closed society that stifles growth and innovation."<!-- --></p><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/images/ic/160xn/p0l11hgh.jpg.webp 160w,https://ichef.bbci.co.uk/images/ic/240xn/p0l11hgh.jpg.webp 240w,https://ichef.bbci.co.uk/images/ic/320xn/p0l11hgh.jpg.webp 320w,https://ichef.bbci.co.uk/images/ic/480xn/p0l11hgh.jpg.webp 480w,https://ichef.bbci.co.uk/images/ic/640xn/p0l11hgh.jpg.webp 640w,https://ichef.bbci.co.uk/images/ic/800xn/p0l11hgh.jpg.webp 800w,https://ichef.bbci.co.uk/images/ic/1024xn/p0l11hgh.jpg.webp 1024w,https://ichef.bbci.co.uk/images/ic/1376xn/p0l11hgh.jpg.webp 1376w,https://ichef.bbci.co.uk/images/ic/1920xn/p0l11hgh.jpg.webp 1920w" src="https://ichef.bbci.co.uk/images/ic/480xn/p0l11hgh.jpg.webp" loading="lazy" alt="Alamy Fewer travellers suggest the US may be losing its power and influence (Credit: Alamy)"><span>Alamy</span></p></div><p data-component="caption-block"><figcaption>Fewer travellers suggest the US may be losing its power and influence (Credit: Alamy)<!-- --></figcaption></p></figure><p>This human impact of boycotts also shows up in a myriad of ways, noted Missouri lawyer John Beck. "I've worked with hundreds of non-citizens over the last 15 years and seen firsthand how policies between governments can result in individuals – regular travellers, students, families – getting caught in the middle," he said. "It's rarely about the politics. It's about missed weddings, lost business deals or being unable to visit a dying parent."<!-- --></p><p>Beck says he has worked with at least 80 clients who have either delayed or cancelled US travel due to real or perceived hostility over the past few years. This causes complications with split families, international companies who can't move talent quickly and lost business due to the lack of ease of movement. Even though the geopolitical stakes are high, there is a human aspect on both sides of the border.<!-- --></p><p>"Most Americans have little to do with the policies that frustrate global travellers," he said. "They just want safety, fairness and dignity. That's what we need to protect."<!-- --></p><p>--<!-- --></p><p><i id="if-you-liked-this-story,">If you liked this story, <!-- --></i><a target="_self" href="https://cloud.email.bbc.com/SignUp10_08?&amp;at_bbc_team=studios&amp;at_medium=Onsite&amp;at_objective=acquisition&amp;at_ptr_name=bbc.com&amp;at_link_origin=featuresarticle&amp;at_campaign=essentiallist&amp;at_campaign_type=owned&amp;&amp;&amp;&amp;&amp;"><b id="sign-up-for-the-essential-list-newsletter"><i id="sign-up-for-the-essential-list-newsletter">sign up for The Essential List newsletter<!-- --></i></b></a><i id="–-a-handpicked-selection-of-features,-videos-and-can't-miss-news,-delivered-to-your-inbox-twice-a-week."> – a handpicked selection of features, videos and can't-miss news, delivered to your inbox twice a week.<!-- --></i>&nbsp;<!-- --></p><p><i id="for-more-travel-stories-from-the-bbc,-follow-us-on">For more Travel stories from the BBC, follow us on <!-- --></i><a target="_blank" href="https://www.facebook.com/BBCTravel/"><b id="facebook"><i id="facebook">Facebook<!-- --></i></b></a><i id=",">, <!-- --></i><a target="_blank" href="https://twitter.com/BBC_Travel"><b id="x"><i id="x">X<!-- --></i></b></a><i id="and"> and <!-- --></i><a target="_blank" href="https://www.instagram.com/bbc_travel/"><b id="instagram"><i id="instagram">Instagram<!-- --></i></b></a><i id=".">.<!-- --></i>&nbsp;<!-- --></p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The April Fools joke that might have got me fired (437 pts)]]></title>
            <link>http://oldvcr.blogspot.com/2025/04/the-april-fools-joke-that-might-have.html</link>
            <guid>43543743</guid>
            <pubDate>Tue, 01 Apr 2025 07:11:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://oldvcr.blogspot.com/2025/04/the-april-fools-joke-that-might-have.html">http://oldvcr.blogspot.com/2025/04/the-april-fools-joke-that-might-have.html</a>, See on <a href="https://news.ycombinator.com/item?id=43543743">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-6562913850949730620" itemprop="description articleBody"><p>
Everyone should pull one great practical joke in their lifetimes. This one was mine, and I think it's past the statute of limitations. The story is true. Only the names are redacted to protect the guilty.
</p><p>
My first job out of college was a database programmer, even though my undergraduate degree had nothing to do with computers and my current profession still mostly doesn't. The reason was that the University I worked for couldn't afford competitive wages, but they did offer various fringe benefits, and they were willing to train someone who at least had decent working knowledge. I, as a newly minted graduate of the august University of California system, had decent working knowledge at least of BSD/386 and SunOS, but more importantly also had the glowing recommendation of my predecessor who was being promoted into a new position. I was hired, which was their first mistake.
</p><p>
<a name="more"></a>
The system I was hired to work on was an HP 9000 K250, one of Hewlett-Packard's big PA-RISC servers. I wish I had a photograph of it, but all I have are a couple bad scans of some bad Polaroids of my office and none of the server room. The server room was downstairs from my office back in the days when server rooms were on-premises, complete with a swipe card lock and a halon system that would give you a few seconds of grace before it flooded everything. The K250 hulked in there where it had recently replaced what I think was an Encore mini of some sort (probably a Multimax, since it was a few years old and the 88K Encores would have been too new for the University), along with the AIX RS/6000s that provided student and faculty shell accounts and E-mail, the bonded T1 lines, some of the terminal servers, the massive Cabletron routers and a lot of the telco stuff. One of the tape reels from the Encore hangs on my wall today as a memento.
</p><p>
The K250 and the Encore it replaced (as well as the L-Class that later replaced the K250 when I was a consultant) ran an all-singing, all-dancing student information system called CARS. CARS is still around, renamed <a href="https://jenzabar.com/">Jenzabar</a>, though I suspect that many of its underpinnings remain if you look under the table. In those days CARS was a massive overlay that was loaded atop the operating system and database, which when I started were, respectively, HP/UX 10.20 and Informix. (I'm old.) It used Informix tables, screens and stored procedures plus its own text UI libraries to run code written variously as Perform screens, SQL, C-shell scripts and plain old C or ESQL/C. Everything was tracked in RCS using overgrown <tt>Makefile</tt>s. I had the admin side (resource management, financials, attendance trackers, etc.) and my office partner had the academic side (mostly grades and faculty tracking). My job was to write and maintain this code and shortly after to help the University create custom applications in CARS' brand-spanking new web module, which chose the new hotness in scripting languages, i.e., Perl. Fortuitously I had learned Perl in, appropriately enough, a computational linguistics course.
</p><p>
CARS also managed most of the printers on campus except for the few that the RS/6000s controlled directly. Most of the campus admin printers were HP LaserJet 4 units of some derivation equipped with JetDirect cards for networking. These are great warhorse printers, some of the best laser printers HP ever made. I suspect there were line printers other places, but those printers were largely what existed in the University's offices.
</p><p>
It turns out that the <tt>READY</tt> message these printers show on their VFD panels is changeable. I don't remember where I read this, probably idly paging through the manual over a lunch break, but initially the only fun things I could think of to do was to have the printer say hi to my boss when she sent jobs to it, stuff like that (whereupon she would tell me to get back to work). Then it dawned on me: because I had access to the printer spools on the K250, and the spool directories were conveniently named the same as their hostnames, I knew where each and every networked LaserJet on campus was. I was young, rash and motivated. This was a hack I just couldn't resist. It would be even better than what had been my favourite joke at my alma mater, where campus services, notable for posting various service suspension notices, posted one April Fools' Day that gravity itself would be suspended to various buildings. I felt sure this hack would eclipse that too.
</p><p>
The plan on April Fools' Day was to get into work at OMG early o'clock and iterate over every entry in the spool, sending it a sequence that would change the <tt>READY</tt> message to <tt>INSERT 5 CENTS</tt>. This would cause every networked LaserJet on campus to appear to ask for a nickel before you printed anything. The script was very simple (this is the actual script, I saved it):
</p><div><pre>#!/bin/csh -f

cd /opt/carsi/spool
foreach i (*)
        echo '^[%-12345X@PJL RDYMSG DISPLAY="INSERT 5 CENTS"' | netto $i 9100
end
</pre></div>
<p>
The <tt>^[</tt> was a literal ASCII 27 ESCape character, and <tt>netto</tt> was a simple <tt>netcat</tt>-like script I had written in these days before <tt>netcat</tt> was widely used. That's it.

Now, let me be clear: the printer was <em>still</em> ready! The effect was merely cosmetic! It would still print if you sent jobs to it! Nevertheless, to complete the effect, this message was sent out on the campus-wide administration mailing list (which I also saved):
</p><div><pre>To: xxx@xxx.xxx
Date: xxx, 1 Apr xxxx 05:41:34 -0800 (PST)
Subject: IMPORTANT NOTE ON PRINTER POLICY

Due to the increasing costs of service commitments for campus printers,
all printers on campus will be reprogrammed for pay-per-page service
to defray these mounting expenses, effective immediately.

Most printers will now require a 5 cent deposit per page for printing. This
may be paid on account or through special coin acceptors to be installed
on the unit by technicians through the end of this week. If your office has
not yet established an account, your printer will automatically request you to 
insert 5 cents into the slot per page to be printed. Please check your
printer's LCD [sic] display to see if your printer requires the 5 cents per
page before using your printer.

Additional printers will be retrofitted as soon as possible. Technicians
will be contacting departments with specific details.

All accounts will be maintained on CARS. Do not call the Helpdesk. To
establish or verify your department's printer account, please call me at
xxxx.

Please also direct all questions regarding this new policy to me as well.

We apologise for the inconvenience and hope that the new cost requirement
will not adversely affect your department's productivity.
</pre></div>
<p>
At the end of the day I would reset everything back to <tt>READY</tt>, smile smugly, and continue with my menial existence. That was the plan.
</p><p>
Having sent this out, I fielded a few anxious calls, who laughed uproariously when they realized, and I reset their printers manually afterwards. The people who knew me, knew I was a practical joker, took note of the date, and sent approving replies. One of the best was sent to me later in the day by intercampus mail, printed on their laser printer, with a nickel taped to it.
</p><p>
Unfortunately, not everybody on campus knew me, and those who did not not only did <em>not</em> call me, but instead called university administration directly. By 8:30am it was chaos in the main office and this filtered up to the head of HR, who most definitely <em>did</em> know me, and told me I'd better send a retraction before the CFO got in or I was in big trouble. That went wrong also, because my retraction said that campus administration was not considering charging per-page fees when in fact they actually were, so I had to retract it and send a <em>new</em> retraction that didn't call attention to that fact. I also ran the script to reset everything early. Eventually the hubbub finally settled down around noon. Everybody in the office thought it was very funny. Even my boss, who officially disapproved, thought it was somewhat funny.
</p><p>
The other thing that went wrong, as if all that weren't enough, was that the director of IT — which is to say, my boss's boss — was away on vacation when all this took place. (Read E-mail remotely? Who does <em>that</em>?) I compounded this situation with the tactical error of going skiing over the coming weekend and part of the next week, most of which I spent snowplowing down the bunny slopes face first, so that he discovered all the angry E-mail in his box without me around to explain myself. (My office partner remembers him coming in wide-eyed asking, "what did he <em>do</em>??") When I returned, it was icier in the office than it had been on the mountain. The assistant director, who thought it was funny, was in trouble for not putting a lid on it, and I was in really big trouble for doing it in the first place. I was appropriately contrite and made various apologies and was an uncharacteristically model employee for an unnaturally long period of time.
</p><p>
The Ice Age eventually thawed and the incident was officially dropped except for a "poor judgment" on my next performance review and the satisfaction of what was then considered the best practical joke ever pulled on campus. Indeed, everyone agreed it was much more technically accomplished than the previous award winner, where someone had supposedly gotten it around the grounds that the security guards at the entrance would be charging a nominal admission fee per head. Years later they still said it was legendary.
</p><p>
I like to think they still do.
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Nue – Apps lighter than a React button (677 pts)]]></title>
            <link>https://nuejs.org/blog/large-scale-apps/</link>
            <guid>43543241</guid>
            <pubDate>Tue, 01 Apr 2025 05:47:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nuejs.org/blog/large-scale-apps/">https://nuejs.org/blog/large-scale-apps/</a>, See on <a href="https://news.ycombinator.com/item?id=43543241">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    <article>
      <header>
  <time datetime="2025-04-01T00:00:00.000Z">April 1, 2025</time>

  

  

</header>
      <p>On this release we're showing what you can do by taking the modern web standards — HTML, CSS, and JS — to their absolute peak:</p>

<p>The entire app is significantly lighter than a React button:</p>
<figure><picture><source srcset="https://nuejs.org/img/react-button-vs-nue-spa.png" media="(max-width: 750px)" type="image/png">
<source srcset="https://nuejs.org/img/react-button-vs-nue-spa-big.png" media="(min-width: 750px)" type="image/png">
<img loading="lazy" src="https://nuejs.org/img/react-button-vs-nue-spa-big.png" width="704" height="394"></picture></figure>
<p>See benchmark and details <a href="https://nuejs.org/docs/react-button-vs-nue.html">here ›</a></p>
<h2 id="going-large-scale"><a href="#going-large-scale" title="Going large-scale"></a>Going large-scale</h2>
<p>Here’s the same app, now with a <strong>Rust</strong> computation engine and <strong>Event Sourcing</strong> for instant search and other operations over <strong>150,000</strong> records — far past where JavaScript (and React) would crash with a stack overflow error:</p>
<p>

  

  <bunny-player custom="bunny-player">
  
</bunny-player>

  <figcaption>
    Instant operations across 150.000 records with Rust/WASM
  </figcaption>

  

</p>
<p>See this demo <a href="https://mpa.nuejs.org/app/?rust">live ›</a></p>

<p>Nue crushes HMR and build speed records and sets you up with a millisecond feedback loop for your day-to-day VSCode/Sublime file-save operations:</p>
<p>

  

  <bunny-player custom="bunny-player">
  
</bunny-player>

  <figcaption>
    Immediate feedback for design and component updates, preserving app state
  </figcaption>

  

</p>
<video src="https://nuejs.org/img/mpa-build.mp4" type="video/mp4" autoplay="" loop="" muted="" width="350"></video>

<p>Here's what this means:</p>
<h3 id="for-rust-go-and-js-engineers"><a href="#for-rust-go-and-js-engineers" title="For Rust, Go, and JS engineers"></a>For Rust, Go, and JS engineers</h3>
<p>This is a wake-up call for Rust, Go, and JS engineers stuck wrestling with React idioms instead of leaning on timeless software patterns. Nue emphasizes a model-first approach, delivering modular design with simple, testable functions, true static typing, and minimal dependencies. Nue is a liberating experience for system devs whose skills can finally shine in a separated model layer.</p>
<h3 id="for-design-engineers"><a href="#for-design-engineers" title="For Design Engineers"></a>For Design Engineers</h3>
<p>This is a wake-up call for design engineers bogged down by React patterns and <a href="https://github.com/shadcn-ui/ui/tree/main/apps/v4/registry/new-york-v4">40,000+ line</a> design systems. Build radically simpler systems with modern CSS (@layers, variables, calc()) and take control of your typography and whitespace.</p>
<h3 id="for-ux-engineers"><a href="#for-ux-engineers" title="For UX Engineers"></a>For UX Engineers</h3>
<p>This is a wake-up call for UX engineers tangled in React hooks and utility class walls instead of owning the user experience. Build apps as light as a React button to push the web—and your skills—forward.</p>
<h2 id="faq-wth-is-nue"><a href="#faq-wth-is-nue" title="FAQ: WTH is Nue?"></a>FAQ: WTH is Nue?</h2>
<p>Nue is a web framework focused on web standards, currently in active development. We aim to reveal the hidden complexity that’s become normalized in modern web development. When a single button outweighs an entire application, something’s fundamentally broken.</p>
<p>Nue drives the inevitable shift. We’re rebuilding tools and frameworks from the ground up with a cleaner, more robust architecture. Our goal is to restore the joy of web development for all key skill sets: frontend architects, design engineers, and UX engineers.</p>
      
    </article>

    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Duolingo-style exercises but with real-world content like the news (429 pts)]]></title>
            <link>https://app.fluentsubs.com/exercises/daily</link>
            <guid>43543235</guid>
            <pubDate>Tue, 01 Apr 2025 05:46:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://app.fluentsubs.com/exercises/daily">https://app.fluentsubs.com/exercises/daily</a>, See on <a href="https://news.ycombinator.com/item?id=43543235">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[An 'Administrative Error' Sends a Maryland Father to a Salvadoran Prison (115 pts)]]></title>
            <link>https://www.theatlantic.com/politics/archive/2025/03/an-administrative-error-sends-a-man-to-a-salvadoran-prison/682254/</link>
            <guid>43542333</guid>
            <pubDate>Tue, 01 Apr 2025 02:51:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/politics/archive/2025/03/an-administrative-error-sends-a-man-to-a-salvadoran-prison/682254/">https://www.theatlantic.com/politics/archive/2025/03/an-administrative-error-sends-a-man-to-a-salvadoran-prison/682254/</a>, See on <a href="https://news.ycombinator.com/item?id=43542333">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-event-module="article body" data-flatplan-body="true"><p data-flatplan-paragraph="true">The Trump administration acknowledged in a court <a data-event-element="inline link" href="https://storage.courtlistener.com/recap/gov.uscourts.mdd.578815/gov.uscourts.mdd.578815.11.0.pdf">filing</a> Monday that it had grabbed a Maryland father with protected legal status and mistakenly deported him to El Salvador, but said that U.S. courts lack jurisdiction to order his return from the megaprison where he’s now locked up.</p><p data-flatplan-paragraph="true">The case appears to be the first time the Trump administration has admitted to errors when it sent three planeloads of Salvadoran and Venezuelan deportees to El Salvador’s grim “Terrorism Confinement Center” on March 15. Attorneys for several Venezuelan deportees have said that the Trump administration falsely labeled their clients as gang members because of their tattoos. Trump officials have disputed those claims.</p><p data-flatplan-paragraph="true">But in Monday’s court filing, attorneys for the government admitted that the Salvadoran man, Kilmar Abrego Garcia, was deported accidentally. “Although ICE was aware of his protection from removal to El Salvador, Abrego Garcia was removed to El Salvador because of an administrative error,” the government told the court. Trump lawyers said the court has no ability to bring him back now that Abrego Garcia is in Salvadoran custody.</p><p data-flatplan-paragraph="true">Simon Sandoval-Moshenberg, Abrego Garcia’s attorney, said he’s never seen a case in which the government knowingly deported someone who had already received protected legal status from an immigration judge. He is asking the court to order the Trump administration to ask for Abrego Garcia’s return and, if necessary, to withhold payment to the Salvadoran government, which says it’s charging the United States $6 million a year to jail U.S. deportees.</p><p data-flatplan-paragraph="true">Trump administration attorneys told the court to dismiss the request on multiple grounds, including that Trump’s “primacy in foreign affairs” outweighs the interests of Abrego Garcia and his family.</p><p data-flatplan-paragraph="true">“They claim that the court is powerless to order any relief,’’ Sandoval-Moshenberg told me. “If that’s true, the immigration laws are meaningless—all of them—because the government can deport whoever they want, wherever they want, whenever they want, and no court can do anything about it once it’s done.”</p><p data-flatplan-paragraph="true">Court <a data-event-element="inline link" href="https://storage.courtlistener.com/recap/gov.uscourts.mdd.578815/gov.uscourts.mdd.578815.1.0.pdf">filings</a> show Abrego Garcia came to the United States at age 16 in 2011 after fleeing gang threats in his native El Salvador. In 2019 he received a form of protected legal status known as “withholding of removal” from a U.S. immigration judge who found he would likely be targeted by gangs if deported back.</p><p data-flatplan-paragraph="true">Abrego Garcia, who is married to a U.S. citizen and has a 5-year-old disabled child who is also a U.S. citizen, has no criminal record in the United States, according to his attorney. The Trump administration does not claim he has a criminal record, but called him a “danger to the community” and an active member of MS-13, the Salvadoran gang that Trump has declared a Foreign Terrorist Organization.</p><p data-flatplan-paragraph="true">Sandoval-Moshenberg said those charges are false, and the gang label stems from a 2019 incident when Abrego Garcia and three other men were detained in a Home Depot parking lot by a police detective in Prince George’s County, Maryland. During questioning, one of the men told officers Abrego Garcia was a gang member, but the man offered no proof and police said they didn’t believe him, filings show. Police did not identify him as a gang member.</p><p data-flatplan-paragraph="true">Abrego Garcia was not charged with a crime, but he was handed over to U.S. Immigration and Customs Enforcement after the arrest to face deportation. In those proceedings, the government claimed that a reliable informant had identified him as a ranking member of MS-13. Abrego Garcia and his family hired an attorney and fought the government’s attempt to deport him. He received “withholding of removal” six months later, a protected status.</p><p data-flatplan-paragraph="true">It is not a path to permanent U.S. residency, but it means the government won’t deport him back to his home country because he’s more likely than not to face harm there.</p><p data-flatplan-paragraph="true">Abrego Garcia has had no contact with any law enforcement agency since his release, according to his attorney. He works full time as a union sheetmetal apprentice, has complied with requirements to check in annually with ICE, and cares for his five-year-old son, who has autism and a hearing defect, and is unable to communicate verbally.</p><p data-flatplan-paragraph="true">On March 12 Abrego Garcia had picked up his son after work from the boy’s grandmother’s house when ICE officers stopped the car, saying his protected status had changed. Officers waited for Abrego Garcia’s wife to come to the scene and take care of the boy, then drove him away in handcuffs. Within two days he had been transferred to an ICE staging facility in Texas, along with other detainees the government was preparing to send to El Salvador. Trump had invoked the Alien Enemies Act of 1798, and the government planned to deport two planeloads of Venezuelans along with a separate group of Salvadorans.</p><p data-flatplan-paragraph="true">Abrego Garcia’s family has had no contact with him since he was sent to the megaprison in El Salvador, known as the CECOT. His wife spotted her husband in news photographs released by Salvadoran President Nayib Bukele on the morning of March 16, after a U.S. District Judge had told the Trump administration to halt the flights.</p><p data-flatplan-paragraph="true">“Oopsie,” Bukele <a data-event-element="inline link" href="https://x.com/nayibbukele/status/1901238762614517965">wrote</a> on social media, taunting the judge.</p><p data-flatplan-paragraph="true">Abrego Garcia’s wife recognized her husband’s decorative arm tattoo and scars, according to the court filing. The image showed Salvadoran guards in black ski masks frog-marching him into the prison, with his head shoved down toward the floor. The CECOT is the same prison Department of Homeland Security Secretary Kristi Noem visited last week, recording videos for social media while standing in front of a cell packed with silent detainees.</p><p data-flatplan-paragraph="true">If the government wants to deport someone with protected status, the standard course would be to reopen the case and introduce new evidence arguing for deportation. The deportation of a protected status holder has even stunned some government attorneys I’ve been in touch with who are tracking the case, who declined to be named because they weren’t authorized to speak to the press. “What. The. Fuck,” one texted me.</p><p data-flatplan-paragraph="true">Sandoval-Moshenberg told the court he believes Trump officials deported his client “through extrajudicial means because they believed that going through the immigration judge process took too long, and they feared that they might not win all of their cases.’’</p><p data-flatplan-paragraph="true">Officials at ICE and the Department of Homeland Security did not respond to a request for comment. The Monday court filing by the government indicates officials knew Abrego Garcia had legal protections shielding him from deportation to El Salvador.</p><p data-flatplan-paragraph="true">“ICE was aware of this grant of withholding of removal at the time [of] Abrego Garcia’s removal from the United States. Reference was made to this status on internal forms,” the government told the court in its filing.</p><p data-flatplan-paragraph="true">Abrego Garcia was not on the initial manifest of the deportation flight, but was listed “as an alternate,” the government attorneys explained. As other detainees were removed from the flight for various reasons, Abrego Garcia “moved up the list.’’</p><p data-flatplan-paragraph="true">The flight manifest “did not indicate that Abrego-Garcia should not be removed,’’ the attorneys said. “Through administrative error, Abrego-Garcia was removed from the United States to El Salvador. This was an oversight.” But despite this, they told the court that Abrego Garcia’s deportation was carried out ‘’in good faith.’’</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Get the hell out of the LLM as soon as possible (308 pts)]]></title>
            <link>https://sgnt.ai/p/hell-out-of-llms/</link>
            <guid>43542259</guid>
            <pubDate>Tue, 01 Apr 2025 02:34:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sgnt.ai/p/hell-out-of-llms/">https://sgnt.ai/p/hell-out-of-llms/</a>, See on <a href="https://news.ycombinator.com/item?id=43542259">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <figure>
  <img src="https://sgnt.ai/hell.jpeg" alt="Get out of there">
</figure>
<p>Don’t let an LLM make decisions or implement business logic: they suck at that. I build NPCs for an online game, and I get asked a lot “How did you get ChatGPT to do that?” The answer is invariably: “I didn’t, and also you shouldn’t”.</p>
<p>In most applications, the LLM should be the user-interface only between the user and an API into your application logic. The LLM shouldn’t be implementing any logic. Get the hell out of the LLM as soon as possible, and stay out as long as you can.</p>
<h2 id="y-tho">Y Tho?</h2>
<p>This is best illustrated by a contrived example: you want to write a chess-playing bot you access over WhatsApp. The user sends a description of what they want to do (“use my bishop to take the knight”), and the bot plays against them.</p>
<p>Could you get the LLM to be in charge of maintaining the state of the chess board and playing convincingly? <a href="https://dynomight.net/chess/">Possibly, maybe</a>. Would you? Hell no, for some intuitive reasons:</p>
<ul>
<li><strong>Performance</strong>: It’s impressive that LLMs might be able to play chess at all, but they suck at it (as of 2025-04-01). A specialized chess engine is always going to be a faster, better, cheaper chess player. Even modern chess engines like Stockfish that incorporate neural networks are still purpose-built specialized systems with well-defined inputs and evaluation functions - not general-purpose language models trying to maintain game state through text.</li>
<li><strong>Debugging and adjusting</strong>: It’s impossible to reason about and debug <em>why</em> the LLM made a given decision, which means it’s very hard to change <em>how</em> it makes those decisions if you need to tweak them. You don’t understand the journey it took through the high-dimensional semantic space to get to your answer, and it’s really poor at explaining it too. Even purpose-built neural networks like those in chess engines can be challenging for observability, and a general LLM is a nightmare, despite Anthropic’s <a href="https://www.anthropic.com/research/tracing-thoughts-language-model">great strides in this area</a></li>
<li><strong>And the rest…</strong>: testing LLM outputs is much harder than unit-testing known code-paths; LLMs are much worse at math than your CPU; LLMs are insufficiently good at picking random numbers; version-control and auditing becomes much harder; monitoring and observability gets painful; state management through natural language is fragile; you’re at the mercy of API rate limits and costs; and security boundaries become fuzzy when everything flows through prompts.</li>
</ul>
<h2 id="examples"><strong>Examples</strong></h2>
<p>The chess example illustrates the fundamental problem with using LLMs for core application logic, but this principle extends far beyond games. In any domain where precision, reliability, and efficiency matter, you should follow the same approach:</p>
<ol>
<li>The user says they want to attack player X with their vorpal sword? The LLM shouldn’t be the system figuring out is the user has a vorpal sword, or what the results of that would be: the LLM is responsible for translating the free-text the user gave you into an API call <em>only</em> and translating the result into text for the user</li>
<li>You’re building a negotiation agent that should respond to user offers? The LLM isn’t in charge of the negotiation, just in charge of packaging it up, passing it off to the negotiating engine, and telling the user about the result</li>
<li>You need to make a random choice about how to respond to the user? The LLM doesn’t get to choose</li>
</ol>
<h2 id="reminder-of-what-llms-are-good-at"><strong>Reminder of what LLMs are good at</strong></h2>
<p>While I’ve focused on what LLMs shouldn’t do, it’s equally important to understand their strengths so you can leverage them appropriately:</p>
<p>LLMs excel at transformation and at categorization, and have a pretty good grounding in “how the world works”, and this is where you in your process you should be deploying them.</p>
<p>The LLM is good at taking “hit the orc with my sword” and turning it into <code>attack(target="orc", weapon="sword")</code>. Or taking <code>{"error": "insufficient_funds"}</code> and turning it into “You don’t have enough gold for that.”</p>
<p>The LLM is good at figuring out what the hell the user is trying to do and routing it to the right part of your system. Is this a combat command? An inventory check? A request for help?</p>
<p>Finally, the LLM is good at knowing about human concepts, and knowing that a “blade” is probably a sword and “smash” probably means attack.</p>
<p>Notice that all these strengths involve transformation, interpretation, or communication—not complex decision-making or maintaining critical application state. By restricting LLMs to these roles, you get their benefits without the pitfalls described earlier.</p>
<h2 id="the-future"><strong>The future</strong></h2>
<p>What LLMs can and can’t do is ever-shifting and reminds me of the “<a href="https://en.wikipedia.org/wiki/God_of_the_gaps">God of the gaps</a>”. a term from theology where each mysterious phenomenon was once explained by divine intervention—until science filled that gap. Likewise, people constantly identify new “human-only” tasks to claim that LLMs aren’t truly intelligent or capable. Then, just a few months later, a new model emerges that handles those tasks just fine, forcing everyone to move the goalposts again, examples <em>passim</em>. It’s a constantly evolving target, and what seems out of reach today may be solved sooner than we expect.</p>
<p>And so like in our chess example, we will probably soon end up with LLMs that can handle all of our above examples reasonably well. I suspect however that most of the drawbacks won’t go away: your non-LLM logic that you pass off to is going to be easier to reason about, easier to maintain, cheaper to run, and more easily version-controlled.</p>
<p>Even as LLMs continue to improve, the fundamental architectural principle remains: use LLMs for what they’re best at—the interface layer—and rely on purpose-built systems for your core logic.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The case against conversational interfaces (263 pts)]]></title>
            <link>https://julian.digital/2025/03/27/the-case-against-conversational-interfaces/</link>
            <guid>43542131</guid>
            <pubDate>Tue, 01 Apr 2025 02:14:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://julian.digital/2025/03/27/the-case-against-conversational-interfaces/">https://julian.digital/2025/03/27/the-case-against-conversational-interfaces/</a>, See on <a href="https://news.ycombinator.com/item?id=43542131">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
 		
<p><span>01</span> Intro</p>



<p>Conversational interfaces are a bit of a meme. Every couple of years a shiny new AI development emerges and people in tech go <em>“This is it! The next computing paradigm is here! We’ll only use natural language going forward!”</em>. But then nothing actually changes and we continue using computers the way we always have, until the debate resurfaces a few years later.</p>



<p>We’ve gone through this cycle a couple of times now: Virtual assistants (Siri), smart speakers (Alexa, Google Home), chatbots (<a href="https://medium.com/chris-messina/conversational-commerce-92e0bccfc3ff">“conversational commerce”</a>), <a href="http://julian.digital/2020/04/19/airpods-as-a-platform/">AirPods-as-a-platform</a>, and, most recently, large language models.</p>



<p>I’m not entirely sure where this obsession with conversational interfaces comes from. Perhaps it’s a type of anemoia, a nostalgia for a future we saw in StarTrek that never became reality. Or maybe it’s simply that people look at the term <em>“<strong>natural</strong> language”</em> and think <em>“well, if it’s <strong>natural</strong> then it must be the logical end state”</em>.</p>



<p>I’m here to tell you that it’s not.</p>



<p><span>02</span> Data transfer mechanisms</p>



<p>When people say <em>“natural language”</em> what they mean is written or verbal communication. Natural language is a way to exchange ideas and knowledge between humans. In other words, it’s a data transfer mechanism.</p>



<p>Data transfer mechanisms have two critical factors: speed and lossiness.</p>



<p>Speed determines how quickly data is transferred from the sender to the receiver, while lossiness refers to how accurately the data is transferred. In an ideal state, you want data transfer to happen at maximum speed (instant) and with perfect fidelity (lossless), but these two attributes are often a bit of a trade-off. </p>



<p>Let’s look at how well natural language does on the speed dimension:</p>



<p><img decoding="async" src="https://julian.digital/wp-content/uploads/2025/02/datatransfer.png"></p><p>The first thing I should note is that these data points are <a href="https://www.researchgate.net/publication/332380784_How_many_words_do_we_read_per_minute_A_review_and_meta-analysis_of_reading_rate">very</a>, <a href="https://irisreading.com/what-is-the-average-reading-speed/">very</a> <a href="https://virtualspeech.com/blog/average-speaking-rate-words-per-minute">simplified</a> <a href="https://en.wikipedia.org/wiki/Words_per_minute">averages</a>. The important part to take away from this table is not the accuracy of individual numbers, but the overall pattern: We are significantly faster at receiving data (reading, listening) than sending it (writing, speaking). This is why we can listen to podcasts at 2x speed, but not record them at 2x speed.</p>



<p>To put the writing and speaking speeds into perspective, <strong>we form thoughts at 1,000-3,000 words per minute</strong>. Natural language might be natural, but it’s a bottleneck.</p>



<p>And yet, if you think about your day-to-day interactions with other humans, most communication feels really fast and efficient. That’s because natural language is only one of many data transfer mechanisms available to us.</p>



<p>For example, instead of saying <em>“I think what you just said is a great idea”</em>, I can just give you a thumbs up. Or nod my head. Or simply smile. </p>



<p>Gestures and facial expressions are effectively data compression techniques. They encode information in a more compact, but lossier, form to make it faster and more convenient to transmit.</p>



<p><img decoding="async" src="https://julian.digital/wp-content/uploads/2025/01/thumbsup.png"></p><p>Natural language is great for data transfer that requires high fidelity (or as a data storage mechanism for async communication), but whenever possible we switch to other modes of communication that are faster and more effortless. Speed and convenience always wins.</p>



<p>My favorite example of truly effortless communication is a memory I have of my grandparents. At the breakfast table, my grandmother never had to ask for the butter – my grandfather always seemed to pass it to her automatically, because after 50+ years of marriage he just sensed that she was about to ask for it. It was like they were communicating telepathically.  </p>



<p>*That* is the type of relationship I want to have with my computer!</p>



<p><span>03</span> Human Computer Interaction</p>



<p>Similar to human-to-human communication, there are different data transfer mechanisms to exchange information between humans and computers. In the early days of computing, users interacted with computers through a command line. These text-based commands were effectively a natural language interface, but required precise syntax and a deep understanding of the system.</p>



<p>The introduction of the GUI primarily solved a discovery problem: Instead of having to memorize exact text commands, you could now navigate and perform tasks through visual elements like menus and buttons. This didn’t just make things easier to discover, but also more convenient: It’s faster to click a button than to type a long text command.</p>



<p>Today, we live in a productivity equilibrium that combines graphical interfaces with keyboard-based commands.</p>



<p>We still use our mouse to navigate and tell our computers what to do next, but routine actions are typically communicated in form of quick-fire keyboard presses: <span>⌘</span><span>b</span> to format text as bold, <span>⌘</span><span>t</span> to open a new tab, <span>⌘</span><span>c</span>/<span>v</span> to quickly copy things from one place to another, etc.</p>



<p>These shortcuts are not natural language though. They are another form of data compression. Like a thumbs up or a nod, they help us to communicate faster.</p>



<p>Modern productivity tools take these data compression shortcuts to the next level. In tools like Linear, Raycast or Superhuman every single command is just a keystroke away. Once you’ve built the muscle memory, the data input feels completely effortless. It’s almost like being handed the butter at the breakfast table without having to ask for it.</p>



<p>Touch-based interfaces are considered the third pivotal milestone in the evolution of human computer interaction, but they have always been more of an augmentation of desktop computing rather than a replacement for it. Smartphones are great for “away from keyboard” workflows, but important productivity work still happens on desktop.</p>



<p><a href="https://x.com/blakeir/status/1838365114312872320">
  <img decoding="async" src="https://julian.digital/wp-content/uploads/2025/03/blake-1.png">
</a></p><p>That’s because text is not a mobile-native input mechanism. A physical keyboard can feel like a natural extension of your mind and body, but typing on a phone is always a little awkward – and it shows in data transfer speeds: <a href="https://userinterfaces.aalto.fi/typing37k/">Average typing speeds on mobile are just 36 words-per-minute</a>, notably slower than the ~60 words-per-minute on desktop.</p>



<p>We’ve been able to replace natural language with mobile-specific data compression algorithms like emojis or Snapchat selfies, but we’ve never found a mobile equivalent for keyboard shortcuts. Guess why we still don’t have a truly mobile-first productivity app after almost 20 years since the introduction of the iPhone?</p>



<p><img decoding="async" src="https://julian.digital/wp-content/uploads/2025/02/emojis.png"></p><p><em>“But what about speech-to-text,”</em> you might say, pointing to <a href="https://www.npr.org/2023/04/16/1170232936/voice-notes-messages-trend">reports</a> about increasing usage of voice messaging. It’s true that speaking (150wpm) is indeed a faster data transfer mechanism than typing (60wpm), but that doesn’t automatically make it a better method to interact with computers.</p>



<p>We keep telling ourselves that previous voice interfaces like Alexa or Siri didn’t succeed because the underlying AI wasn’t smart enough, but that’s only half of the story. The core problem was never the quality of the output function, but the inconvenience of the input function: A natural language prompt like <em>“Hey Google, what’s the weather in San Francisco today?”</em> just takes 10x longer than simply tapping the weather app on your homescreen.</p>



<p>LLMs don’t solve this problem. The quality of their output is improving at an astonishing rate, but the input modality is a step backwards from what we already have. Why should I have to describe my desired action using natural language, when I could simply press a button or keyboard shortcut? Just pass me the goddamn butter.</p>



<p><span>04</span> Conversational UI as Augmentation</p>



<p>None of this is to say that LLMs aren’t great. I love LLMs. I use them all the time. In fact, I wrote this very essay with the help of an LLM. </p>



<p>Instead of drafting a first version with pen and paper (my preferred writing tools), I spent an entire hour walking outside, talking to ChatGPT in Advanced Voice Mode. We went through all the fuzzy ideas in my head, clarified and organized them, explored some additional talking points, and eventually pulled everything together into a first outline.</p>



<p>This wasn’t just a one-sided “<em>Hey, can you write a few paragraphs about x</em>” <a href="https://x.com/julianlehr/status/1855858599156932773">prompt</a>. It felt like a genuine, in-depth conversation and exchange of ideas with a true thought partner. Even weeks later, I’m still amazed at how well it worked. It was one of those rare, magical moments where software makes you feel like you’re living in the future.</p>



<p>In contrast to typical human-to-computer commands, however, this workflow is not defined by speed. Like writing, my ChatGPT conversation is a thinking process – not an interaction that happens post-thought.</p>



<p>It should also be noted that ChatGPT does not substitute any existing software workflows in this example. It’s a completely new use case.</p>



<p>This brings me to my core thesis: The inconvenience and inferior data transfer speeds of conversational interfaces make them an unlikely replacement for existing computing paradigms – but what if they complement them?</p>



<p>The most convincing conversational UI I have seen to date was at a hackathon where a team turned <a href="https://upsidelab.io/blog/design-voice-user-interface-starcraft">Amazon Alexa into an in-game voice assistant for StarCraft II</a>. Rather than replacing mouse and keyboard, voice acted as an <em>additional</em> input mechanism. It increased the bandwidth of the data transfer.</p>



<p><img decoding="async" src="https://julian.digital/wp-content/uploads/2025/03/starcraft-2.png"></p><p>You could see the same pattern work for any type of knowledge work, where voice commands are available <em>while</em> you are busy doing other things. We will not replace Figma, Notion, or Excel with a chat interface. It’s not going to happen. Neither will we forever continue the status quo, where we constantly have to switch back and forth between these tools and an LLM.</p>



<p>Instead, AI should function as an always-on command meta-layer that spans across all tools. Users should be able to trigger actions from anywhere with simple voice prompts without having to interrupt whatever they are currently doing with mouse and keyboard.</p>



<p>For this future to become an actual reality, AI needs to work at the OS level. It’s not meant to be an interface for a single tool, but an interface across tools. <a href="https://kwokchain.com/2019/08/16/the-arc-of-collaboration/">Kevin Kwok famously wrote</a> that <em>“productivity and collaboration shouldn’t be two separate workflows”</em>. And while he was referring to human-to-human collaboration, the statement is even more true in a world of human-to-AI collaboration, where the lines between productivity and coordination are becoming increasingly more blurry. </p>



<p><img decoding="async" src="https://julian.digital/wp-content/uploads/2025/03/metalayer-1.png"></p><p>The second thing we need to figure out is how we can compress voice input to make it faster to transmit. What’s the voice equivalent of a thumbs-up or a keyboard shortcut? Can I prompt Claude faster with simple sounds and whistles? Should ChatGPT have access to my camera so it can change its answers in realtime based on my facial expressions?</p>



<p>Even as a secondary interface, speed and convenience is all that matters.</p>



<p><span>05</span> Closing thoughts</p>



<p>I admit that the title of this essay is a bit misleading (made you click though, didn’t it?). This isn’t really a case against conversational interfaces, it’s a case against zero-sum thinking.</p>



<p>We spend too much time thinking about AI as a substitute (for interfaces, workflows, and jobs) and too little time about AI as a complement. Progress rarely follows a simple path of replacement. It unlocks new, previously unimaginable things rather than merely displacing what came before.</p>



<p>The same is true here. The future isn’t about replacing existing computing paradigms with chat interfaces, but about enhancing them to make human-computer interaction feel effortless – like the silent exchange of butter at a well-worn breakfast table.</p>







<p><em>Thanks to Blake Robbins, Chris Paik, Jackson Dahl, Johannes Schickling, Jordan Singer, and signüll for reading drafts of this post.</em></p>




 	</div></div>]]></description>
        </item>
    </channel>
</rss>