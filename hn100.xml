<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 11 Dec 2025 21:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Going Through Snowden Documents, Part 1 (128 pts)]]></title>
            <link>https://libroot.org/posts/going-through-snowden-documents-part-1/</link>
            <guid>46235412</guid>
            <pubDate>Thu, 11 Dec 2025 18:52:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://libroot.org/posts/going-through-snowden-documents-part-1/">https://libroot.org/posts/going-through-snowden-documents-part-1/</a>, See on <a href="https://news.ycombinator.com/item?id=46235412">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>We are building a comprehensive archive and analysis project examining published documents leaked by Edward Snowden. Our methodology involves systematically reviewing each available document with particular attention to small details and information that has received little or no public attention since the initial 2013 disclosures. Throughout this process, we will publish posts highlighting interesting previously unreported findings. The main project will hopefully be complete and made public in mid-to-late 2026.</p><p>This is Part 1 of our "Going Through Snowden Documents" series.</p><p>Document: <a href="https://web.archive.org/web/20181005121544/https://assets.documentcloud.org/documents/2116005/cne-analysis-in-xks.pdf" target="_blank">CNE Analysis in XKEYSCORE</a></p><p>Classification: TOP SECRET//COMINT//REL TO USA, AUS, CAN, GBR, NZL</p><p>Date: October 15, 2009</p><p>Published by: The Intercept (<a href="https://theintercept.com/2015/07/01/nsas-google-worlds-private-communications/" target="_blank" rel="noreferrer">July 1</a> and <a href="https://theintercept.com/2015/07/02/look-under-hood-xkeyscore/" target="_blank" rel="noreferrer">July 2, 2015</a>)</p><p>Authors: Morgan Marquis-Boire, Glenn Greenwald, and Micah Lee</p><p>While The Intercept published this document, the accompanying articles focus on NSA's XKEYSCORE system broadly and does not analyze this specific document. The document appears only in the "Documents published with this article" sections without dedicated coverage. Academic searches, news archives, and general web searches reveal virtually no subsequent analysis or citation of this document. This pattern of important documents published but never publicly analyzed is unfortunately very common in the published Snowden documents.</p><h2>Table of Contents</h2><div><ul><li><a href="#Norinco">CNE operation against Chinese defense contractor Norinco</a></li><li><a href="#redaction-fail">Redaction failure exposing NSA agent username</a></li><li><a href="#mexican-feds">CNE operation against Mexican federal law enforcement</a></li><li><a href="#iran-customs-and-rails">CNE operation against Iran's customs and rails</a></li><li><a href="#programs">New surveillance program codenames</a><ul><li><a href="#turbochaser">TURBOCHASER</a></li><li><a href="#tucker">TUCKER</a></li><li><a href="#shadowquest-waytide-greenchaos">SHADOWQUEST, WAYTIDE, GREENCHAOS</a></li></ul></li><li><a href="#other">Other</a></li><ul><li><a href="#foggybottom">FOGGYBOTTOM: HTTP activity surveillance</a></li><li><a href="#windows-registry">Windows registry surveillance</a></li><li><a href="#keylogger">Multi-lingual keylogger capabilities</a></li><li><a href="#vpn-in-docs">"vpn in docs"</a></li></ul></ul></div><p>This October 2009 33-page document, in a slideshow format, is an internal NSA training presentation demonstrating how analysts use XKEYSCORE to search and analyze data collected through Computer Network Exploitation (CNE), the NSA's term for active hacking operations. While framed as instructional examples showing various search capabilities, the screenshots display real surveillance operations with identifiable targets and captured data.</p><p>The screenshots in the document are such poor quality that, at times, reading the text is very difficult. However, by examining the context and surrounding text (or surrounding pages), the text can be inferred with a very high probability. This has certainly contributed to why many documents have not been studied more thoroughly in public, as many are similarly low quality with scrambled text.</p><h2 id="Norinco">CNE operation against Chinese defense contractor Norinco</h2><p>One of the most significant previously unreported findings in this document is evidence of NSA surveillance targeting Norinco, China North Industries Corporation, one of the world's largest state-owned defense contractors. Norinco ranks among the world's top 100 defense companies by revenue and serves as a major exporter of military equipment to Pakistan, Iran, Venezuela, Zimbabwe, and dozens of other countries, many of which have contentious relationships with the United States.</p><p>On page 18, a screenshot from XKEYSCORE's Metaviewer interface displays a "Histogram of @Domain" view with a bar graph showing email volume across 10 domain names followed by a data table with formatted surveillance results. The query appears to be a converged search combining multiple distinct surveillance targets: Mexican federal agencies (ssp.gob.mx at 452 emails, pfp.local at 158 emails), Norinco-related domains (mail.norinco.cn, businessmonitor.com, bmi.msgfocus.com, zhenhuaoil.com, and lms-ms-daemon, each showing 3 emails), and two additional targets (steels-net.cu and inwind.it, each with 1 email). This convergence of seemingly unrelated targets in a single query demonstrates XKEYSCORE's ability to simultaneously analyze multiple surveillance operations.</p><p>The first five entries in the results table contain:</p><pre>Email User Name | Datetime            | Highlights | @Domain             | Subject                               | Chain
[REDACTED]      | 2009-10-10 05:15:10 | CNE        | mail.norinco.cn     | 28-10 senior contacts in India for zh | 0kqe00g01mrdii@mail.norinco.cn&amp;kate.strut
[REDACTED]      | 2009-10-10 05:15:10 | CNE        | businessmonitor.com | 28-10 senior contacts in India for zh | 0kqe00g01mrdii@mail.norinco.cn&amp;kate.strut
[REDACTED]      | 2009-10-10 05:15:10 | CNE        | bmi.msgfocus.com    | 28-10 senior contacts in India for zh | 0kqe00g01mrdii@mail.norinco.cn&amp;kate.strut
[REDACTED]      | 2009-10-10 05:15:10 | CNE        | zhenhuaoil.com      | 28-10 senior contacts in India for zh | 0kqe00g01mrdii@mail.norinco.cn&amp;kate.strut
[REDACTED]      | 2009-10-10 05:15:10 | CNE        | lms-ms-daemon       | 28-10 senior contacts in India for zh | 0kqe00g01mrdii@mail.norinco.cn&amp;kate.strut
</pre><p><img src="https://libroot.org/public/post-imgs/p18.jpg" alt="Screenshot from the page 18 showing emails related to Norinco"></p><p>All entries are marked with the "CNE" highlight tag, indicating the data came from CNE operations, active hacking intrusions rather than passive network intercepts. Critically, all five entries share an identical "Chain" value indicating this is a single email captured at multiple points as it traversed Norinco's email infrastructure. The multiple domains – businessmonitor.com (newsletter sender), bmi.msgfocus.com (newsletter delivery service), mail.norinco.cn (Norinco's mail server), zhenhuaoil.com (Norinco's subsidiary), and lms-ms-daemon (the <a href="https://docs.oracle.com/cd/E19563-01/819-4428/bgaih/index.html" target="_blank" rel="noreferrer">default domain name</a> for Sun Java Messaging Server commonly used in enterprise email infrastructure) – represent the newsletter email's routing path through Norinco's network. This indicates that NSA achieved deep network penetration with visibility across multiple servers and routing points within Norinco's corporate email infrastructure, not just a single interception point. The compromise extended to Zhenhua Oil (Norinco's oil exploration subsidiary), indicating enterprise-wide access.</p><h2 id="redaction-fail">Redaction failure exposing NSA agent username</h2><p>Most XKEYSCORE search interfaces display a welcoming message showing the analyst's internal NSA username. In the document all usernames have been redacted from the screenshots except one left unredacted by mistake.</p><p><img src="https://libroot.org/public/post-imgs/r1.jpg" alt="Screenshot showing redacted NSA username"></p><p>On page 9, the username "<strong>cryerni</strong>" is visible in the screenshot.</p><p><img src="https://libroot.org/public/post-imgs/r2.jpg" alt="Screenshot showing NSA agent username"></p><p>This username most likely belongs to the NSA analyst who created the presentation. The seven-character length matches the redacted name on the first page, based on the surrounding unredacted font. The length of seven characters also matches to other NSA agents' usernames in other documents (more on that in upcoming parts).</p><h2 id="mexican-feds">CNE operation against Mexican federal law enforcement</h2><p>On the page 18, the XKEYSCORE Metaviewer displays email extraction results showing surveillance of Mexican federal law enforcement from domains ssp.gob.mx (Secretaría de Seguridad Pública) and pfp.local (Policía Federal Preventiva). Email subjects include:</p><pre>101009 EII LA PAZ, BAJA CALIFORNIA
101009 EII MEXICALI, BAJA CALIFORNIA
101009 EII CIUDAD JUÁREZ, CHIHUAHUA
</pre><p><img src="https://libroot.org/public/post-imgs/p18_2.jpg" alt="Screenshot from the page 18 showing emails related to ssp.gob.mx and pfp.local"></p><p>"EII" likely stands for "Estructura de Información de Inteligencia" or similar internal reporting format. The dates (101009 = October 10, 2009) and locations indicate daily intelligence reports from Mexican federal police units in Baja California's border region and Ciudad Juárez, one of Mexico's most violent cities during the peak of cartel warfare under President Felipe Calderón's military-led offensive against drug cartels.</p><p>NSA surveillance of these communications likely supported US counter-narcotics operations, identified compromised Mexican officials, and monitored cartel structures and government response capabilities. However, this represents surveillance of a nominal ally's law enforcement agencies without apparent Mexican government knowledge or consent. All entries were marked "CNE," again indicating active computer compromise rather than passive intercept.</p><h2 id="iran-customs-and-rails">CNE operation against Iran's customs and rails</h2><p>Another interesting finding appears on page 17, showing document metadata extraction results with the name "Iran OP Customs and Rail Extracted Docs". The results table displays documents captured from a file path containing "lap top drive" and "Private Inbox", with all entries marked "CNE" in the Highlights column, indicating NSA compromised a portable computer likely belonging to someone working in Iranian transportation or customs infrastructure. The implant performed a complete directory walk and extracted Word documents from the user's private folders.</p><p><img src="https://libroot.org/public/post-imgs/p17.jpg" alt="Screenshot from the page 17 titled 'XK Metaviewer Iran OP Customs and Rail Extracted Docs'"><img src="https://libroot.org/public/post-imgs/p17_2.jpg" alt="Screenshot from the page 17 table showing the surveillance results"></p><h2 id="programs">New surveillance program codenames</h2><p>Several program codenames mentioned in this document don't appear in any other published Snowden documents or in previous reporting. No mention either in <a href="https://www.electrospaces.net/p/nicknames-and-codewords.html">websites</a> <a href="https://www.electrospaces.net/p/abbreviations-and-acronyms.html">documenting</a> <a href="https://gist.github.com/dustyfresh/a98abad0dd63b3b4e5bc8ff6e3cf0781">all</a> <a href="https://christopher-parsons.com/resources/the-sigint-summaries/nsa-summaries/" target="_blank">the</a> <a href="https://www.electrospaces.net/p/gchq-nicknames-and-codewords.html" target="_blank">codenames</a> <a href="https://christopher-parsons.com/resources/the-sigint-summaries/gchq-covernames-programs-and-suggested-use-implementation/" target="_blank">found</a> in Snowden documents and in other NSA/GCHQ related articles and documents.</p><p id="turbochaser"><strong>TURBOCHASER</strong> - The document describes TURBOCHASER as an NSA database for "profiles" and for "future tasking", appearing alongside MARINA (the well-documented NSA metadata repository). The name suggests rapid-cycling or high-speed processing ("turbo") of pursuit targets ("chaser"). Based on context, TURBOCHASER likely handled specific metadata types or geographic regions that MARINA didn't cover. The document's brief mention provides no additional details.</p><p id="tucker"><strong>TUCKER</strong> - References in the document suggest TUCKER is an exploitation framework comparable to UNITEDRAKE (the well-documented full-featured Windows implant). The document lists TUCKER's sub-projects including OLYMPUS, EXPANDINGPULLY, and UNIX, indicating TUCKER was a platform hosting multiple specialized payloads and/or (post-)exploitation tools.</p><p id="shadowquest-waytide-greenchaos"><strong>SHADOWQUEST</strong>, <strong>WAYTIDE</strong>, <strong>GREENCHAOS</strong> - These appear as collection source identifiers in the document. The document shows them as input sources feeding CNE data into XKEYSCORE. Notably, FOXACID, the well-documented NSA exploit server system used to deliver malware to targets, also appears in this context with the suffix FOXACID6654, suggesting it functioned not just as an exploitation delivery mechanism but also as a collection source identifier once targets were compromised. This reveals FOXACID's dual role: initial compromise vector and ongoing data collection infrastructure.</p><p>The input sources shown include:</p><ul><li>FOXACID6654 - collecting wireless survey data</li><li>SHADOWQUEST35 - collecting wireless survey data</li><li>WAYTIDE1173 - collecting wireless intelligence</li><li>GREENCHAOS15 - source of the Chinese keylogger data</li></ul><p>The numeric suffixes (6654, 35, 1173, 15) likely designates a specific server or operational instance, possibly corresponding to geographic regions, operational theaters, or specific TAO teams.</p><h2 id="other">Other</h2><p>Finally, the document showcases several detailed cases of NSA's CNE capabilities, confirming and adding specific context to techniques that have been reported on more generally since 2013.</p><h3 id="foggybottom">FOGGYBOTTOM: HTTP activity surveillance</h3><p><img src="https://libroot.org/public/post-imgs/foggy.jpg" alt="Screenshot showing the captured HTTP activity"></p><p>Pages 19-20 showcase FOGGYBOTTOM for monitoring HTTP activity captured through CNE operations. FOGGYBOTTOM is a computer implant plug-in that records logs of internet browsing histories and collects login details and passwords used to access websites and email accounts. These pages show detailed browser surveillance of a target identified by case notation YM.VALAGWAADTC (Yemen) on October 14, 2009. The system captured:</p><ul><li>Multiple Facebook login attempts (login.facebook.com with "login_attempt=1" POST requests)</li><li>Arabic-language Facebook browsing (ar-ar.facebook.com)</li><li>Saudi Arabian Google searches (www.google.com.sa with "hl=ar" indicating Arabic language)</li><li>Yemeni news sites (www.14october.com, www.26sep.net, www.althawranews.net)</li><li>Arabic sports forums (forum.kooora.com - a popular Middle Eastern sports discussion site)</li></ul><p>The surveillance captured not just URLs but complete HTTP request details including POST data and URL parameters. The "dnt_payload/browser" formatter shows the target's local time, timezone offset, and HTTP POST form data. Since this data comes from a CNE implant running on the compromised computer itself – not passive network interception – it captures web traffic before encryption occurs. The implant sees the browsing data whether the connection uses HTTP or HTTPS, providing complete visibility into all browsing activity including encrypted sessions that would be opaque to network-level surveillance.</p><h3 id="windows-registry">Windows registry surveillance</h3><p><img src="https://libroot.org/public/post-imgs/reg.jpg" alt="Screenshot showing the Windows registry entries"></p><p>Page 26 demonstrates XKEYSCORE's capability to search and analyze Windows registry data extracted from compromised machines. The screenshots show registry queries returning UserAssist keys; Windows registry entries that record every program a user has executed, how many times, and when they last ran it. This data is maintained by Windows for user interface optimization but becomes a detailed forensic record when captured by NSA implants.</p><h3 id="keylogger">Multi-lingual keylogger capabilities</h3><p><img src="https://libroot.org/public/post-imgs/keylogger.jpg" alt="Screenshot of keystroke data captured from a target in China using QQ Messenger and Microsoft Excel"></p><p>Pages 24-25 demonstrate XKEYSCORE's keylogger capabilities with actual captured keystrokes from a compromised computer identified as GREENCHAOS15 in China. The target was using QQ.exe (China's largest instant messaging platform owned by Tencent), Microsoft Excel, and Microsoft Access. The keylogger captured complete Chinese character input, control key sequences, hexadecimal codes for special characters, window titles showing conversation participants, and even deleted text and editing actions. In Excel, the system recorded every keystroke including numerical entries, navigation inputs (Delete, NumPad entries), and cell references (D4, H2, D53, etc.), showing the target working on a spreadsheet titled "3C证书导入工作周报0928-1001.xls" (3C Certificate Import Work Weekly Report 09/28-10/01). The target appeared to be an office worker handling administrative tasks related to China's 3C certification system (China Compulsory Certificate for product safety/quality). This demonstrates NSA's ability to capture multi-lingual keystrokes across all applications with complete context preservation.</p><h3 id="vpn-in-docs">"vpn in docs"</h3><p><img src="https://libroot.org/public/post-imgs/vpn.jpg" alt="XKEYSCORE results flagging the keywords 'vpn' and 'pptp' found within captured documents and emails"></p><p>The document also demonstrates how XKEYSCORE uses a generic "tech strings" search to automatically identify and flag arbitrary keywords that an analyst queries. This feature appears to function as a catchall system for finding terms of interest in data streams that lack a more specific parser. The examples show XKEYSCORE tagging the strings "vpn" and "pptp" inside a wide variety of captured data. This includes the content of emails (email_body), the body of local documents (document_body with file paths like C:\TNI-095CC.DOC), and other raw data payloads exfiltrated from implants (tech_body). As nearly all entries are highlighted with "CNE," this reveals that NSA implants actively scan a target's private files and communications for these keywords. The resulting intelligence allows analysts to discover a target's security posture, identify potential vulnerabilities, and find information such as credentials or server details that can be leveraged to gain access to privileged systems or map internal networks.</p><p>This document is a good example of the significant intelligence hiding in plain sight within the published Snowden documents. A detailed review can reveal significant, previously unreported intelligence operations, such as the CNE op against a major Chinese defense contractor. These findings underscore the importance of a systematic review of the documents. Also, it's important to acknowledge the inherent limitations of analyzing any single document in isolation like we did in this post. A single document analysis offers only a snapshot with limited context.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rivian Unveils Custom Silicon, R2 Lidar Roadmap, and Universal Hands Free (106 pts)]]></title>
            <link>https://riviantrackr.com/news/rivian-unveils-custom-silicon-r2-lidar-roadmap-universal-hands-free-and-its-next-gen-autonomy-platform/</link>
            <guid>46234920</guid>
            <pubDate>Thu, 11 Dec 2025 18:17:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://riviantrackr.com/news/rivian-unveils-custom-silicon-r2-lidar-roadmap-universal-hands-free-and-its-next-gen-autonomy-platform/">https://riviantrackr.com/news/rivian-unveils-custom-silicon-r2-lidar-roadmap-universal-hands-free-and-its-next-gen-autonomy-platform/</a>, See on <a href="https://news.ycombinator.com/item?id=46234920">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>RJ opened the first ever <a href="https://riviantrackr.com/news/rivian-to-showcase-ai-driven-driving-tech-at-december-autonomy-day/" title="Rivian to Showcase AI-Driven Driving Tech at December Autonomy Day">Autonomy and AI Day</a> explaining why Rivian believes it is positioned to lead in this next phase of the industry. The company is leaning hard into compute, custom hardware, large scale AI systems, and a shared data foundation that touches every part of the ownership experience.</p>



<p>Let’s break it all down.</p>



<div>
<h2><strong>Meet the Rivian Autonomy Processor</strong></h2>



<p>One of the biggest announcements was RAP1, Rivian’s first in house processor built on a 5nm multi chip module. It delivers 1600 sparse INT8 TOPS and can push 5 billion pixels per second inside the new Gen 3 Autonomy Computer. Rivian even built its own AI compiler and platform software to support it. This shows Rivian is no longer just integrating off the shelf chips, it is now designing silicon specifically for its autonomy roadmap.</p>



<figure>
<figure data-wp-context="{&quot;imageId&quot;:&quot;693b37db17e35&quot;}" data-wp-interactive="core/image" data-wp-key="693b37db17e35"><img fetchpriority="high" decoding="async" width="2133" height="1200" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" data-id="15369" src="https://riviantrackr.com/wp-content/uploads/2025/12/ACM3-3.jpg" alt="Rivian Autonomy Processor" srcset="https://riviantrackr.com/wp-content/uploads/2025/12/ACM3-3.jpg 2133w, https://riviantrackr.com/wp-content/uploads/2025/12/ACM3-3-300x169.jpg 300w, https://riviantrackr.com/wp-content/uploads/2025/12/ACM3-3-1024x576.jpg 1024w, https://riviantrackr.com/wp-content/uploads/2025/12/ACM3-3-150x84.jpg 150w, https://riviantrackr.com/wp-content/uploads/2025/12/ACM3-3-768x432.jpg 768w, https://riviantrackr.com/wp-content/uploads/2025/12/ACM3-3-1536x864.jpg 1536w, https://riviantrackr.com/wp-content/uploads/2025/12/ACM3-3-2048x1152.jpg 2048w" sizes="(max-width: 2133px) 100vw, 2133px"><figcaption>Click to enlarge</figcaption></figure>



<figure data-wp-context="{&quot;imageId&quot;:&quot;693b37db180c8&quot;}" data-wp-interactive="core/image" data-wp-key="693b37db180c8"><img decoding="async" width="2133" height="1200" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" data-id="15370" src="https://riviantrackr.com/wp-content/uploads/2025/12/ACM3-2-1.jpg" alt="Rivian Autonomy Processor" srcset="https://riviantrackr.com/wp-content/uploads/2025/12/ACM3-2-1.jpg 2133w, https://riviantrackr.com/wp-content/uploads/2025/12/ACM3-2-1-300x169.jpg 300w, https://riviantrackr.com/wp-content/uploads/2025/12/ACM3-2-1-1024x576.jpg 1024w, https://riviantrackr.com/wp-content/uploads/2025/12/ACM3-2-1-150x84.jpg 150w, https://riviantrackr.com/wp-content/uploads/2025/12/ACM3-2-1-768x432.jpg 768w, https://riviantrackr.com/wp-content/uploads/2025/12/ACM3-2-1-1536x864.jpg 1536w, https://riviantrackr.com/wp-content/uploads/2025/12/ACM3-2-1-2048x1152.jpg 2048w" sizes="(max-width: 2133px) 100vw, 2133px"><figcaption>Click to enlarge</figcaption></figure>
</figure>
</div>



<div>
<h2><strong><strong>Autonomy Computer and LiDAR on R2</strong></strong></h2>



<p>The ACM3 (Autonomy Compute Module 3) autonomy computer will debut on R2 starting at the end of 2026, but Rivian made it clear that R2 will launch initially without LiDAR. What Rivian confirmed today is that LiDAR will be added later in the program. This lines up with what we <a href="https://riviantrackr.com/news/could-lidar-be-coming-to-the-rivian-r2/" title="Could LiDAR Be Coming to the Rivian R2?">explored back in May</a> when we spotted early signs that Rivian was evaluating LiDAR as a redundancy and ground truth layer for future autonomy. Rivian has now officially validated that LiDAR is coming to R2 down the road, where it will join cameras and radar to create a richer, more resilient perception stack.</p>



<figure>
<figure data-wp-context="{&quot;imageId&quot;:&quot;693b37db185e9&quot;}" data-wp-interactive="core/image" data-wp-key="693b37db185e9"><img decoding="async" width="1800" height="1200" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" data-id="15372" src="https://riviantrackr.com/wp-content/uploads/2025/12/R1_Lidar_1.jpg" alt="Rivian R2 with LiDAR" srcset="https://riviantrackr.com/wp-content/uploads/2025/12/R1_Lidar_1.jpg 1800w, https://riviantrackr.com/wp-content/uploads/2025/12/R1_Lidar_1-300x200.jpg 300w, https://riviantrackr.com/wp-content/uploads/2025/12/R1_Lidar_1-1024x683.jpg 1024w, https://riviantrackr.com/wp-content/uploads/2025/12/R1_Lidar_1-150x100.jpg 150w, https://riviantrackr.com/wp-content/uploads/2025/12/R1_Lidar_1-768x512.jpg 768w, https://riviantrackr.com/wp-content/uploads/2025/12/R1_Lidar_1-1536x1024.jpg 1536w" sizes="(max-width: 1800px) 100vw, 1800px"><figcaption>Click to enlarge</figcaption></figure>



<figure data-wp-context="{&quot;imageId&quot;:&quot;693b37db187fb&quot;}" data-wp-interactive="core/image" data-wp-key="693b37db187fb"><img loading="lazy" decoding="async" width="1800" height="1200" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" data-id="15371" src="https://riviantrackr.com/wp-content/uploads/2025/12/R2_Lidar_5.jpg" alt="Rivian R2 with LiDAR" srcset="https://riviantrackr.com/wp-content/uploads/2025/12/R2_Lidar_5.jpg 1800w, https://riviantrackr.com/wp-content/uploads/2025/12/R2_Lidar_5-300x200.jpg 300w, https://riviantrackr.com/wp-content/uploads/2025/12/R2_Lidar_5-1024x683.jpg 1024w, https://riviantrackr.com/wp-content/uploads/2025/12/R2_Lidar_5-150x100.jpg 150w, https://riviantrackr.com/wp-content/uploads/2025/12/R2_Lidar_5-768x512.jpg 768w, https://riviantrackr.com/wp-content/uploads/2025/12/R2_Lidar_5-1536x1024.jpg 1536w" sizes="auto, (max-width: 1800px) 100vw, 1800px"><figcaption>Click to enlarge</figcaption></figure>
</figure>
</div>



<div>
<h2><strong><strong><strong>Large Driving Model and Rivian’s Data Loop</strong></strong></strong></h2>



<p>Rivian explained how its autonomy stack is powered by a self improving data loop feeding the company’s Large Driving Model, which is trained similarly to an LLM. Reinforcement learning distills high quality driving behavior into efficient onboard models. Every release improves the system, and Rivian laid out a trajectory that moves toward point to point, eyes off and eventually personal Level 4.</p>



<figure><video height="540" width="960" autoplay="" loop="" muted="" src="https://riviantrackr.com/wp-content/uploads/2025/12/Multi-Modal-Sensors.mov" playsinline=""></video></figure>
</div>



<div>
<h2><strong><strong><strong><strong>Universal Hands Free Coming to Gen 2</strong></strong></strong></strong></h2>



<p>Rivian confirmed that a major software update will bring <a href="https://riviantrackr.com/news/rj-scaringe-confirms-universal-hands-free-on-gen-2-r1-before-autonomy-day/" title="RJ Scaringe Confirms Universal Hands Free on Gen 2 R1 Before Autonomy Day">Universal Hands Free</a> to Gen 2 R1T and R1S. This hands free experience will cover over 3.5 million miles of roads across the US and Canada as long as there are clearly painted lane lines. It is a huge expansion of the assisted driving envelope for current owners.</p>



<figure><video height="720" width="1280" autoplay="" loop="" muted="" src="https://riviantrackr.com/wp-content/uploads/2025/12/Universal-Hands-Free.mov" playsinline=""></video></figure>
</div>



<div>
<h2><strong><strong><strong><strong><strong>Autonomy+ Sub Launching in 2026</strong></strong></strong></strong></strong></h2>



<p>Rivian also announced <a href="https://riviantrackr.com/news/introducing-rivians-cutting-edge-autonomy-platform/" title="Introducing Rivian’s Cutting-Edge Autonomy Platform">Autonomy+</a>, an autonomy tier with continuously expanding features launching early 2026. </p>



<p>Pricing is $2,500 one time or $49.99 per month.</p>



<figure data-wp-context="{&quot;imageId&quot;:&quot;693b37db19325&quot;}" data-wp-interactive="core/image" data-wp-key="693b37db19325"><img loading="lazy" decoding="async" width="2376" height="876" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://riviantrackr.com/wp-content/uploads/2025/12/Screenshot-2025-12-05-at-1.40.05-PM.jpg" alt="Rivian Autonomy" srcset="https://riviantrackr.com/wp-content/uploads/2025/12/Screenshot-2025-12-05-at-1.40.05-PM.jpg 2376w, https://riviantrackr.com/wp-content/uploads/2025/12/Screenshot-2025-12-05-at-1.40.05-PM-300x111.jpg 300w, https://riviantrackr.com/wp-content/uploads/2025/12/Screenshot-2025-12-05-at-1.40.05-PM-1024x378.jpg 1024w, https://riviantrackr.com/wp-content/uploads/2025/12/Screenshot-2025-12-05-at-1.40.05-PM-150x55.jpg 150w, https://riviantrackr.com/wp-content/uploads/2025/12/Screenshot-2025-12-05-at-1.40.05-PM-768x283.jpg 768w, https://riviantrackr.com/wp-content/uploads/2025/12/Screenshot-2025-12-05-at-1.40.05-PM-1536x566.jpg 1536w, https://riviantrackr.com/wp-content/uploads/2025/12/Screenshot-2025-12-05-at-1.40.05-PM-2048x755.jpg 2048w" sizes="auto, (max-width: 2376px) 100vw, 2376px"><figcaption>Click to enlarge</figcaption></figure>
</div>



<div>
<h2><strong><strong><strong><strong><strong><strong>Rivian Unified Intelligence</strong></strong></strong></strong></strong></strong></h2>



<p>Rivian is reorganizing its entire platform around Rivian Unified Intelligence, a data foundation that ties together telemetry, cloud models, service systems and customer facing features. It is the backbone for predictive maintenance, smarter diagnostics and upcoming AI driven tools.</p>
</div>



<div>
<h2><strong><strong><strong><strong><strong><strong><strong>Rivian Assistant Coming in 2026</strong></strong></strong></strong></strong></strong></strong></h2>



<p>Rivian also officially unveiled its new Rivian Assistant, a next generation voice experience arriving early 2026 on Gen 1 and Gen 2 R1 vehicles. The assistant uses a blend of edge models and in vehicle intelligence to understand your schedule, recognize context, and handle everyday requests.</p>



<p>On R2, it will even run fully offline thanks to a more powerful infotainment computer, reducing latency and keeping more of the experience on device.</p>



<figure data-wp-context="{&quot;imageId&quot;:&quot;693b37db199a4&quot;}" data-wp-interactive="core/image" data-wp-key="693b37db199a4"><img loading="lazy" decoding="async" width="2248" height="1160" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://riviantrackr.com/wp-content/uploads/2025/12/Rivian-Assistant.jpg" alt="" srcset="https://riviantrackr.com/wp-content/uploads/2025/12/Rivian-Assistant.jpg 2248w, https://riviantrackr.com/wp-content/uploads/2025/12/Rivian-Assistant-300x155.jpg 300w, https://riviantrackr.com/wp-content/uploads/2025/12/Rivian-Assistant-1024x528.jpg 1024w, https://riviantrackr.com/wp-content/uploads/2025/12/Rivian-Assistant-150x77.jpg 150w, https://riviantrackr.com/wp-content/uploads/2025/12/Rivian-Assistant-768x396.jpg 768w, https://riviantrackr.com/wp-content/uploads/2025/12/Rivian-Assistant-1536x793.jpg 1536w, https://riviantrackr.com/wp-content/uploads/2025/12/Rivian-Assistant-2048x1057.jpg 2048w" sizes="auto, (max-width: 2248px) 100vw, 2248px"><figcaption>Click to enlarge</figcaption></figure>
</div>



<div>
<h2><strong><strong><strong><strong><strong><strong><strong><strong>AI Powered Service and Diagnostics</strong></strong></strong></strong></strong></strong></strong></strong></h2>



<p>Rivian is embedding AI into the service workflow. Technicians will have access to an AI driven expert system that analyzes telemetry and vehicle history to pinpoint issues faster and more accurately. These same tools will eventually power the mobile app as well, making self service diagnostics significantly smarter.</p>



<figure data-wp-context="{&quot;imageId&quot;:&quot;693b37db19de2&quot;}" data-wp-interactive="core/image" data-wp-key="693b37db19de2"><img loading="lazy" decoding="async" width="1024" height="683" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://riviantrackr.com/wp-content/uploads/2025/04/f16306f390fdf1bc0d3345414d6bdd3e_u52271.jpg" alt="Rivian Service Center" srcset="https://riviantrackr.com/wp-content/uploads/2025/04/f16306f390fdf1bc0d3345414d6bdd3e_u52271.jpg 1024w, https://riviantrackr.com/wp-content/uploads/2025/04/f16306f390fdf1bc0d3345414d6bdd3e_u52271-300x200.jpg 300w, https://riviantrackr.com/wp-content/uploads/2025/04/f16306f390fdf1bc0d3345414d6bdd3e_u52271-150x100.jpg 150w, https://riviantrackr.com/wp-content/uploads/2025/04/f16306f390fdf1bc0d3345414d6bdd3e_u52271-768x512.jpg 768w" sizes="auto, (max-width: 1024px) 100vw, 1024px"><figcaption>Click to enlarge</figcaption></figure>
</div>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-5.2 (185 pts)]]></title>
            <link>https://openai.com/index/introducing-gpt-5-2/</link>
            <guid>46234874</guid>
            <pubDate>Thu, 11 Dec 2025 18:12:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/introducing-gpt-5-2/">https://openai.com/index/introducing-gpt-5-2/</a>, See on <a href="https://news.ycombinator.com/item?id=46234874">Hacker News</a></p>
Couldn't get https://openai.com/index/introducing-gpt-5-2/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-5.2 (468 pts)]]></title>
            <link>https://openai.com/index/introducing-gpt-5-2/</link>
            <guid>46234788</guid>
            <pubDate>Thu, 11 Dec 2025 18:04:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/introducing-gpt-5-2/">https://openai.com/index/introducing-gpt-5-2/</a>, See on <a href="https://news.ycombinator.com/item?id=46234788">Hacker News</a></p>
Couldn't get https://openai.com/index/introducing-gpt-5-2/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Litestream VFS (151 pts)]]></title>
            <link>https://fly.io/blog/litestream-vfs/</link>
            <guid>46234710</guid>
            <pubDate>Thu, 11 Dec 2025 17:59:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fly.io/blog/litestream-vfs/">https://fly.io/blog/litestream-vfs/</a>, See on <a href="https://news.ycombinator.com/item?id=46234710">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <figure>
                <img src="https://fly.io/blog/litestream-vfs/assets/litestream-vfs.jpg" alt="">
                <figcaption>
                  <span>Image by</span>
                  
<svg role="img" style="pointer-events: none; width: 17px; height: 17px;" viewBox="0 0 20 20" fill="currentColor" fill-rule="evenodd">
  <g buffered-rendering="static">
    <path fill-rule="evenodd" d="M1 8a2 2 0 012-2h.93a2 2 0 001.664-.89l.812-1.22A2 2 0 018.07 3h3.86a2 2 0 011.664.89l.812 1.22A2 2 0 0016.07 6H17a2 2 0 012 2v7a2 2 0 01-2 2H3a2 2 0 01-2-2V8zm13.5 3a4.5 4.5 0 11-9 0 4.5 4.5 0 019 0zM10 14a3 3 0 100-6 3 3 0 000 6z" clip-rule="evenodd"></path>
  </g>
</svg>

                    <a href="https://annieruygtillustration.com/" target="_blank">
                      Annie Ruygt
                    </a>
                </figcaption>
            </figure>
          <p><strong>I’m Ben Johnson, and I work on Litestream at Fly.io. Litestream is the missing backup/restore system for SQLite. It’s free, open-source software that should run anywhere, and</strong> <a href="https://fly.io/blog/litestream-v050-is-here/" title=""><strong>you can read more about it here</strong></a><strong>.</strong></p>
<p>Again with the sandwiches: assume we’ve got a SQLite database of sandwich ratings, and we’ve backed it up with <a href="https://fly.io/blog/litestream-v050-is-here/" title="">Litestream</a> to an S3 bucket.</p>

<p>Now, on our local host, load up AWS credentials and an S3 path into our environment. Open SQLite and:</p>
<div>
    <pre><code id="code-9g4bsd2q">$ sqlite3
SQLite version 3.50.4 2025-07-30 19:33:53
sqlite&gt; .load litestream.so
sqlite&gt; .open file:///my.db?vfs=litestream
</code></pre>
  </div>
<p>SQLite is now working from that remote database, defined by the Litestream backup files in the S3 path we configured. We can query it:</p>
<div>
    <pre><code id="code-3hthyxxm">sqlite&gt; SELECT * FROM sandwich_ratings ORDER BY RANDOM() LIMIT 3 ; 
22|Veggie Delight|New York|4
30|Meatball|Los Angeles|5
168|Chicken Shawarma Wrap|Detroit|5
</code></pre>
  </div>
<p>This is Litestream VFS. It runs SQLite hot off an object storage URL. As long as you can load the shared library our tree builds for you, it’ll work in your application the same way it does in the SQLite shell.</p>

<p>Fun fact: we didn’t have to download the whole database to run this query. More about this in a bit.</p>

<p>Meanwhile, somewhere in prod, someone has it in for meatball subs and wants to knock them out of the bracket – oh, fuck:</p>
<div>
    <pre><code id="code-6bnwo2d5">sqlite&gt; UPDATE sandwich_ratings SET stars = 1 ;
</code></pre>
  </div>
<p>They forgot the <code>WHERE</code> clause!</p>
<div>
    <pre><code id="code-vosvwspc">sqlite&gt; SELECT * FROM sandwich_ratings ORDER BY RANDOM() LIMIT 3 ; 
97|French Dip|Los Angeles|1
140|Bánh Mì|San Francisco|1
62|Italian Beef|Chicago|1
</code></pre>
  </div>
<p>Italian Beefs and Bánh Mìs, all at 1 star. Disaster!</p>

<p>But wait, back on our dev machine:</p>
<div>
    <pre><code id="code-45me1co3">sqlite&gt; PRAGMA litestream_time = '5 minutes ago'; 
sqlite&gt; select * from sandwich_ratings ORDER BY RANDOM() LIMIT 3 ; 
30|Meatball|Los Angeles|5
33|Ham &amp; Swiss|Los Angeles|2
163|Chicken Shawarma Wrap|Detroit|5
</code></pre>
  </div>
<p>We’re now querying that database from a specific point in time in our backups. We can do arbitrary relative timestamps, or absolute ones, like <code>2000-01-01T00:00:00Z</code>.</p>

<p>What we’re doing here is instantaneous point-in-time recovery (PITR), expressed simply in SQL and SQLite pragmas.</p>

<p>Ever wanted to do a quick query against a prod dataset, but didn’t want to shell into a prod server and fumble with the <code>sqlite3</code> terminal command like a hacker in an 80s movie? Or needed to do a quick sanity check against yesterday’s data, but without doing a full database restore? Litestream VFS makes that easy. I’m so psyched about how it turned out.</p>
<h2 id="how-it-works"><a href="#how-it-works" aria-label="Anchor"></a><span>How It Works</span></h2>
<p><a href="https://fly.io/blog/litestream-v050-is-here/" title="">Litestream v0.5</a> integrates <a href="https://github.com/superfly/ltx" title="">LTX</a>, our SQLite data-shipping file format. Where earlier Litestream blindly shipped whole raw SQLite pages to and from object storage, LTX ships ordered sets of pages. We built LTX for <a href="https://fly.io/docs/litefs/" title="">LiteFS</a>, which uses a FUSE filesystem to do transaction-aware replication for unmodified applications, but we’ve spent this year figuring out ways to use LTX in Litestream, without all that FUSE drama.</p>

<p>The big thing LTX gives us is “compaction”. When we restore a database from object storage, we want the most recent versions of each changed database page. What we don’t want are all the intermediate versions of those pages that occurred prior to the most recent change.</p>

<p>Imagine, at the time we’re restoring, we’re going to need pages 1, 2, 3, 4, and 5. Depending on the order in which pages were written, the backup data set might look something like <code>1 2 3 5 3 5 4 5 5</code>. What we want is the <em>rightmost</em>  5, 4, 3, 2, and 1, without wasting time on the four “extra” page 5’s and the one “extra” page 3. Those “extra” pages are super common in SQLite data sets; for instance, every busy table with an autoincrementing primary key will have them.</p>

<p>LTX lets us skip the redundant pages, and the algorithm is trivial: reading backwards from the end of the sequence, skipping any page you already read. This drastically accelerates restores.</p>

<p>But LTX compaction isn’t limited to whole databases. We can also LTX-compact sets of LTX files. That’s the key to how PITR restores with Litestream now work.</p>

<p>In the diagram below, we’re taking daily full snapshots. Below those snapshots are “levels” of changesets: groups of database pages from smaller and smaller windows of time. By default, Litestream uses time intervals of 1 hour at the highest level, down to 30 seconds at level 1. L0 is a special level where files are uploaded every second, but are only retained until being compacted to L1.</p>

<p><img src="https://fly.io/blog/litestream-vfs/assets/litestream-restore.png"></p>

<p>Now, let’s do a PITR restore. Start from the most proximal snapshot. Then determine the minimal set of LTX files from each level to reach the time you are restoring to.</p>

<p><img src="https://fly.io/blog/litestream-vfs/assets/litestream-restore-path.png"></p>

<p>We have another trick up our sleeve.</p>

<p>LTX trailers include a small index tracking the offset of each page in the file. By fetching <em>only</em> these index trailers from the LTX files we’re working with (each occupies about 1% of its LTX file), we can build a lookup table of every page in the database. Since modern object storage providers all let us fetch slices of files, we can perform individual page reads against S3 directly.</p>

<p><img alt="Anatomy of an LTX file" src="https://fly.io/blog/litestream-vfs/assets/litestream-ltx.png"></p>
<h2 id="how-its-implemented"><a href="#how-its-implemented" aria-label="Anchor"></a><span>How It’s Implemented</span></h2>
<p>SQLite has a plugin interface for things like this: <a href="https://sqlite.org/vfs.html" title="">the “VFS” interface.</a> VFS plugins abstract away the bottom-most layer of SQLite, the interface to the OS. If you’re using SQLite now, you’re already using some VFS module, one SQLite happens to ship with.</p>

<p>For Litestream users, there’s a catch. From the jump, we’ve designed Litestream to run alongside unmodified SQLite applications. Part of what makes Litestream so popular is that your apps don’t even need to know it exists. It’s “just” a Unix program.</p>

<p>That Litestream Unix program still does PITR restores, without any magic. But to do fast PITR-style queries straight off S3, we need more. To make those queries work, you have to load and register Litestream’s VFS module.</p>

<p>But that’s all that changes.</p>

<p>In particular: Litestream VFS doesn’t replace the SQLite library you’re already using. It’s not a new “version” of SQLite. It’s just a plugin for the SQLite you’re already using.</p>

<p>Still, we know that’s not going to work for everybody, and even though we’re really psyched about these PITR features, we’re not taking our eyes off the ball on the rest of Litestream. You don’t have to use our VFS library to use Litestream, or to get the other benefits of the new LTX code.</p>

<p>The way a VFS library works, we’re given just a couple structures, each with a bunch of methods defined on them. We override only the few methods we care about. Litestream VFS handles only the read side of SQLite. Litestream itself, running as a normal Unix program, still handles the “write” side. So our VFS subclasses just enough to find LTX backups and issue queries.</p>

<p>With our VFS loaded, whenever SQLite needs to read a page into memory, it issues a <code>Read()</code> call through our library. The read call includes the byte offset at which SQLite expected to find the page. But with Litestream VFS, that byte offset is an illusion.</p>

<p>Instead, we use our knowledge of the page size along with the requested page number to do a lookup on the page index we’ve built. From it, we get the remote filename, the “real” byte offset into that file, and the size of the page. That’s enough for us to use the <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/range-get-olap.html" title="">S3 API’s <code>Range</code> header handling</a> to download exactly the block we want.</p>

<p>To save lots of S3 calls, Litestream VFS implements an LRU cache. Most databases have a small set of “hot” pages —  inner branch pages or the leftmost leaf pages for tables with an auto-incrementing ID field. So only a small percentage of the database is updated and queried regularly.</p>
<div><p><strong>We’ve got one last trick up our sleeve.</strong></p>

<p>Quickly building an index and restore plan for the current state of a database is cool. But we can do one better.</p>

<p>Because Litestream backs up (into the L0 layer) once per second, the VFS code can simply poll the S3 path, and then incrementally update its index. <strong>The result is a near-realtime replica.</strong> Better still, you don’t need to stream the whole database back to your machine before you use it.</p>
</div><h2 id="eat-your-heart-out-marty-mcfly"><a href="#eat-your-heart-out-marty-mcfly" aria-label="Anchor"></a><span>Eat Your Heart Out, Marty McFly</span></h2>
<p>Litestream holds backup files for every state your database has been in, with single-second resolution, for as long as you want it to. Forgot the <code>WHERE</code> clause on a <code>DELETE</code> statement? Updating your database state to where it was an hour (or day, or week) ago is just a matter of adjusting the LTX indices Litestream manages.</p>

<p>All this smoke-and-mirrors of querying databases without fully fetching them has another benefit: it starts up really fast! We’re living an age of increasingly ephemeral servers, what with the AIs and the agents and the clouds and the hoyvin-glavins. Wherever you find yourself, if your database is backed up to object storage with Litestream, you’re always in a place where you can quickly issue a query.</p>

<p>As always, one of the big things we think we’re doing right with Litestream is: we’re finding ways to get as much whiz-bang value as we can (instant PITR reading live off object storage: pretty nifty!) while keeping the underlying mechanism simple enough that you can fit your head around it.</p>

<p>Litestream is solid for serious production use (we rely on it for important chunks of our own Fly.io APIs). But you could write Litestream yourself, just from the basic ideas in these blog posts. We think that’s a point in its favor. We land there because the heavy lifting in Litestream is being done by SQLite itself, which is how it should be.</p>

          
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Days since last GitHub incident (179 pts)]]></title>
            <link>https://github-incidents.pages.dev/</link>
            <guid>46233798</guid>
            <pubDate>Thu, 11 Dec 2025 16:52:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github-incidents.pages.dev/">https://github-incidents.pages.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=46233798">Hacker News</a></p>
<div id="readability-page-1" class="page">
    Days since last Github service disruption: 0
  


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Things I want to say to my boss (202 pts)]]></title>
            <link>https://www.ithoughtaboutthatalot.com/2025/the-things-i-want-to-say-to-my-boss</link>
            <guid>46233570</guid>
            <pubDate>Thu, 11 Dec 2025 16:35:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ithoughtaboutthatalot.com/2025/the-things-i-want-to-say-to-my-boss">https://www.ithoughtaboutthatalot.com/2025/the-things-i-want-to-say-to-my-boss</a>, See on <a href="https://news.ycombinator.com/item?id=46233570">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>I’m sitting down to write this in a gap between jobs. The downtime is strange, like the world has stopped moving but my thoughts haven’t caught up. Other than replaying the shit that went down during the last six months – or to put it more bluntly, the reasons I left, I don’t quite know what to do with myself.</p><p>What happened wasn’t unique. And that’s the part that bothers me most.&nbsp;</p><p>It’s the same stuff I hear from friends, colleagues, people I trust across the industry.</p></div><p>I know this is anonymous, but if you think this is about you, then I hope you do your team a favour and listen.</p><div><p>It’s the performance of ‘care’ from leadership. Saying one thing loudly and proudly, yet doing another quietly, repeatedly.</p><p>I know this is anonymous, but if you think this is about you, then I hope you do your team a favour and listen.</p><h2><strong>The things I wish I could say</strong></h2><p>You can’t fake care. People feel it. In small moments, in the gaps between your words, in the way you prioritise your business over their wellbeing. Care is a practice, not a performance. If you only care when outsiders are watching, you’re just performing.&nbsp;</p><p>Communication isn’t optional or a one-way thing. Consistency and honesty build trust. Inconsistency and silence destroy it. If you communicate more externally than with your team, your culture will break down slowly over time.&nbsp;</p><p>Ideas stop being shared because “what’s the point?” It’s not like you’re really listening. Meetings become quieter because speaking up feels risky. Colleagues start shrinking, not because their talent fades, but because the space to use it gets narrower.</p></div><div><p>I hope you learn that leadership is more than LinkedIn posts and conference talks.&nbsp;</p><p>It’s the day-to-day choices you make when nobody’s applauding.</p></div><div><p>Burnout isn’t a sign of commitment, it’s a sign of organisational failure. If your best people are exhausted, withdrawn, or like shadows of who they once were, that’s not a resource problem. That’s a You problem.</p><p>By the time you notice a culture is broken, the damage has already been done. People have mentally checked out, or quietly left, or stayed but stopped believing.</p><h2><strong>What I hope (though I’m not holding my breath)</strong></h2><p>I hope you learn that leadership is more than LinkedIn posts and conference talks.&nbsp;</p><p>It’s the day-to-day choices you make when nobody’s applauding. It’s the way you treat people when they’re tired, honest, unwell or “inconvenient”. It’s whether your words match your actions, and whether you’re brave enough to admit when they don’t.</p><p>I hope you realise that people don’t leave because they’re unwilling. They leave because you didn’t take care of them. You don’t get to call yourself “people-first” when every decision proves otherwise.&nbsp;</p><p>I hope you learn that if you focus on making money instead of the team lining your pockets, you will end up with a broken team and no money.</p><h2><strong>What good leadership actually looks like</strong></h2><p>Good leadership isn’t complicated, but it is demanding. It asks more of you than your job title does. It asks for self-awareness, not slogans. It asks you to trade the armour of performance for the discomfort of being accountable.</p></div><div><p>In the end, good leadership is never proven by what you say about yourself. It’s proven by what people say when you’re not in the room.</p><p>And trust me, they’re talking.</p></div><div><p>It’s showing up before the crisis, not after. It’s noticing when someone’s energy changes and checking in, not waiting for them to break. It’s understanding the difference between being busy and being present.</p><p>It’s making decisions with people, not about them. It’s protecting your team from unnecessary chaos rather than generating it. It’s recognising that transparency isn’t a risk, but how trust stays alive.</p><p>It’s creating conditions where people want to speak — not because they’re brave, but because it’s safe. Where the loudest voices don’t automatically win.</p><p>It’s understanding that care is not soft. It’s not indulgent. It’s not a blocker to delivery. It’s the foundation that makes delivery possible. Care is the thing that keeps people willing to stay, to try, to believe. Care is taking responsibility for the things you say and do, and the culture that results in.</p><p>If you want loyalty, creativity, honesty, energy, you must earn them. You earn them by being the kind of leader whose actions make it obvious that people matter. Not because it’s good PR. Because it’s your job. And because people matter, and they deserve it.</p><p>In the end, good leadership is never proven by what you say about yourself. It’s proven by what people say when you’re not in the room.</p><p>And trust me, they’re talking.</p><p>I've given you too much of my time, attention and energy in 2025. So in 2026, I plan to do the opposite and not give you any more.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iPhone Typos? It's Not Just You – The iOS Keyboard Is Broken [video] (337 pts)]]></title>
            <link>https://www.youtube.com/watch?v=hksVvXONrIo</link>
            <guid>46232528</guid>
            <pubDate>Thu, 11 Dec 2025 15:25:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=hksVvXONrIo">https://www.youtube.com/watch?v=hksVvXONrIo</a>, See on <a href="https://news.ycombinator.com/item?id=46232528">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Disney making $1B investment in OpenAI, will allow characters on Sora AI (318 pts)]]></title>
            <link>https://www.cnbc.com/2025/12/11/disney-openai-sora-characters-video.html</link>
            <guid>46231585</guid>
            <pubDate>Thu, 11 Dec 2025 14:12:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/12/11/disney-openai-sora-characters-video.html">https://www.cnbc.com/2025/12/11/disney-openai-sora-characters-video.html</a>, See on <a href="https://news.ycombinator.com/item?id=46231585">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="Placeholder-ArticleBody-Video-108240561" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000398052" aria-labelledby="Placeholder-ArticleBody-Video-108240561"><p><img src="https://image.cnbcfm.com/api/v1/image/108240562-17654620331765462030-42932533857-1080pnbcnews.jpg?v=1765462032&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Disney and OpenAI reach three-year licensing agreement"><span></span><span></span></p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/DIS/">The Walt Disney Co.</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> on Thursday announced it will make a $1 billion equity investment in <a href="https://www.cnbc.com/2025/12/09/openai-slack-ceo-denise-dresser-chief-revenue-officer.html">OpenAI</a> and will allow users to make videos with its copyrighted characters on its <a href="https://www.cnbc.com/2025/11/04/openai-sora-android.html">Sora</a> app.</p><p>OpenAI launched Sora in September, and it allows users to create&nbsp;<a href="https://www.cnbc.com/2025/10/02/openai-invite-code-sora-2-censorship.html">short videos</a>&nbsp;by simply typing in a prompt. </p><p>As part of the startup's new three-year licensing agreement with Disney, Sora users will be able make content with&nbsp;more than 200 characters across Disney, Marvel, Pixar and Star Wars starting next year. </p><p>"The rapid advancement of <a href="https://www.cnbc.com/ai-artificial-intelligence/">artificial intelligence</a> marks an important moment for our industry, and through this collaboration with OpenAI we will thoughtfully and responsibly extend the reach of our storytelling through generative AI, while respecting and protecting creators and their works," Disney CEO <a href="https://www.cnbc.com/bob-iger/">Bob Iger</a> said in a statement.</p><p>As part of the agreement, Disney said it will receive warrants to purchase additional equity and will become a major OpenAI customer. </p><p>Disney is deploying OpenAI's chatbot, ChatGPT, to its employees and will work with its technology to build new tools and experiences, according to a release. </p><p>When Sora launched this fall, the app rocketed to <a href="https://www.cnbc.com/2025/10/03/openai-sora-apple-app-store.html">the top</a> of <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-8"><a href="https://www.cnbc.com/quotes/AAPL/">Apple's</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> App Store and generated a <a href="https://www.cnbc.com/2025/10/04/sora-openai-video-app.html">storm of controversy</a> as users flooded the platform with videos of popular brands and characters. </p><p>The <a href="https://www.cnbc.com/2025/10/07/openais-sora-2-must-stop-allowing-copyright-infringement-mpa-says.html">Motion Picture Association</a>&nbsp;said in October that OpenAI needed to take "immediate and decisive action" to prevent copyright infringement on Sora. </p><p>OpenAI CEO Sam Altman said more "granular control" over character generation was coming, according to <a href="https://blog.samaltman.com/sora-update-number-1" target="_blank">a blog post</a> following the launch.</p></div><div id="ArticleBody-InlineImage-108240672" data-test="InlineImage"><p>Disney CEO Bob Iger and OpenAI CEO Sam Altman appearing on CNBC on Dec. 11th, 2025.</p><p>CNBC</p></div><div><p>As AI startups have rapidly changed the way that people can interact with content online, media companies, including Disney, have kicked off a series of fresh legal battles to try and protect their intellectual property.</p><p>Disney sent a cease and desist letter to <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-12"><a href="https://www.cnbc.com/quotes/GOOGL/">Google</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> late on Wednesday alleging the company infringed its copyrights on a "massive scale." In the letter, which was viewed by CNBC, Disney said Google has been using its copyrighted works to train models and distributing copies of its protected content without authorization.</p><p>CNBC has reached out to Google for comment on the letter.</p><p>Universal and Disney have sued the AI image creator&nbsp;<a href="https://www.cnbc.com/2025/06/11/disney-universal-midjourney-ai-copyright.html">Midjourney</a>, alleging that the company improperly used and distributed AI-generated characters from their movies. Disney also sent a&nbsp;<a href="https://www.cnbc.com/2025/09/30/disney-cease-and-desist-characterai-copyright.html">cease and desist</a>&nbsp;letter to&nbsp;<a href="https://www.cnbc.com/2025/08/01/human-ai-relationships-love-nomi.html">Character.AI</a>&nbsp;in September, warning the startup to stop using its copyrighted characters without authorization.</p><p>Disney's deal with OpenAI suggests the company isn't ruling out AI platforms entirely.</p></div><div id="RegularArticle-RelatedContent-1"><h2>Read more CNBC tech news</h2><div><ul><li><a href="https://www.cnbc.com/2025/12/11/oracle-shares-plummet-dragging-down-ai-stocks-nvidia-coreweave.html">Oracle shares plummet, dragging down AI stocks</a></li><li><a href="https://www.cnbc.com/2025/12/11/big-tech-microsoft-amazon-google-india-billions-in-investment.html">Over $50 billion in under 24 hours: Why Big Tech is doubling down on investing in India</a></li><li><a href="https://www.cnbc.com/2025/12/10/ciscos-stock-closes-at-record-for-first-time-since-dot-com-peak-2000.html">Cisco's stock closes at record for first time since dot-com peak in 2000</a></li><li><a href="https://www.cnbc.com/2025/12/10/nvidia-backed-starcloud-trains-first-ai-model-in-space-orbital-data-centers.html">Nvidia-backed Starcloud trains first AI model in space as orbital data center race heats up</a></li></ul></div></div><div><p>The companies said they have affirmed a commitment to the use of AI that "protects user safety and the rights of creators" and "respects the creative industries," according to the release. </p><p>OpenAI has also agreed to maintain "robust controls" to prevent illegal or harmful content from being generated on its platforms. </p><p>Some of the characters available through the deal include Mickey Mouse, Ariel, Cinderella, Iron Man and Darth Vader. Disney and OpenAI said the agreement does not include any talent likeness or voices. </p><p>Users will also be able to draw from the same intellectual property while using ChatGPT Images, where they can use natural language prompts to create images.&nbsp;</p><p>"Disney is the global gold standard for storytelling, and we're excited to partner to allow Sora and ChatGPT Images to expand the way people create and experience great content," Altman said in a statement.</p><p>Curated selections of Sora videos will also be available to watch on Disney's streaming platform Disney+.</p><p><em>Disclosure: Comcast is the parent company of NBCUniversal, which owns CNBC. Versant would become the new parent company of CNBC upon Comcast's planned spinoff of Versant.</em></p><p><strong>WATCH: </strong><a href="https://www.cnbc.com/video/2025/10/10/why-sora-2-by-openai-has-hollywood-worried.html">We tested OpenAI’s Sora 2 AI-video app to find out why Hollywood is worried</a></p></div><div id="Placeholder-ArticleBody-Video-108210371" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000391611" aria-labelledby="Placeholder-ArticleBody-Video-108210371"><p><img src="https://image.cnbcfm.com/api/v1/image/108210376-OpenAI_Hollywood_C_Clean.jpg?v=1760071837&amp;w=750&amp;h=422&amp;vtcrop=y" alt="We tested OpenAI’s Sora 2 AI-video app to find out why Hollywood is worried"><span></span><span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Craft software that makes people feel something (189 pts)]]></title>
            <link>https://rapha.land/craft-software-that-makes-people-feel-something/</link>
            <guid>46231274</guid>
            <pubDate>Thu, 11 Dec 2025 13:45:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rapha.land/craft-software-that-makes-people-feel-something/">https://rapha.land/craft-software-that-makes-people-feel-something/</a>, See on <a href="https://news.ycombinator.com/item?id=46231274">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>So, I woke up today. Got my coffee, family went to sleep, and I have a free afternoon.</p>

<p>I thought about writing something. I may delete this article, but if you are reading this, it means I went through with it.</p>

<p>Recently, people have been asking me why I’m pausing Boo to work on a programming language. I think it would actually be cool to write down how I feel.</p>

<p>Boo is a code editor I created solely for myself; I never had the intention of making it a mainstream editor. Of course, it would be fun if people used it, but that was never my goal. This year I got it working in a functional state, where I can actually use it for my daily work. It has innovative human-keyboard navigation and replaces the LSP system with something faster and less costly for the OS. So why on earth am I not open-sourcing it? That’s what people keep asking me.</p>

<p>First, let’s go step by step.</p>

<p>My mind isn’t really moved by the idea that it would be a success or a failure — the end user of Boo is me. I don’t feel it’s there yet; in fact, I think software should inspire us. Working on Rio Terminal and Boo in my free time — both written in Rust and sharing many similarities — affects my joy, because it starts to become something automatic. Both have similar architecture, language, release process, and etcetera.</p>

<p>Since I was a kid, I liked to build Lego blocks. That’s probably what I did the most besides playing football or video games. The fun thing about Lego is that one day you can build a castle, and the next day you can build a ship. Not necessarily using the same pieces and colors — you can actually add a lot of stuff that’s external to what you have, like a wood stick.</p>

<p>When programming becomes repetitive, the odds of you creating something that makes people go “wow” are reduced quite a bit. It isn’t a rule, of course. You need to be inspired to make inspiring software.</p>

<p>I always use the example of <a href="https://en.wikipedia.org/wiki/The_Legend_of_Zelda:_Breath_of_the_Wild">The Legend of Zelda: Breath of the Wild</a>. This game is so well crafted that I know people who don’t even like video games but bought a console just to play it — and once they finished, they sold everything. This is what I’m talking about: taking time to build something so that once people try it, they remember it for as long as they live.</p>

<p>Boo isn’t a business. I don’t need or want to make money out of it. I don’t have a deadline, nor do I want to create another VS Code. I don’t feel like forcing it to happen.</p>

<p>In that case, I don’t necessarily need to stop building Lego blocks, right? I’ll just park it there, and when the inspiration comes back, I’ll pick it up where it was. That being said, I paused Boo, and I am working on my own programming language. Eventually, my idea is to rewrite Boo to use it.</p>

<p>“Wow! That’s a lot of work.” Indeed. But it’s my hobby stuff. I’ve always loved programming languages, and I am having a blast learning more about binaries and compilers. So, I don’t really feel I need to follow people’s cake recipe for success. That’s how my mind works, and I will stick with it.</p>

<p>By the way, this article was written using Boo.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[French supermarket's Christmas advert is worldwide hit (without AI) [video] (120 pts)]]></title>
            <link>https://www.youtube.com/watch?v=Na9VmMNJvsA</link>
            <guid>46231187</guid>
            <pubDate>Thu, 11 Dec 2025 13:35:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=Na9VmMNJvsA">https://www.youtube.com/watch?v=Na9VmMNJvsA</a>, See on <a href="https://news.ycombinator.com/item?id=46231187">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Meta shuts down global accounts linked to abortion advice and queer content (326 pts)]]></title>
            <link>https://www.theguardian.com/global-development/2025/dec/11/meta-shuts-down-global-accounts-linked-to-abortion-advice-and-queer-content</link>
            <guid>46230072</guid>
            <pubDate>Thu, 11 Dec 2025 11:26:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/global-development/2025/dec/11/meta-shuts-down-global-accounts-linked-to-abortion-advice-and-queer-content">https://www.theguardian.com/global-development/2025/dec/11/meta-shuts-down-global-accounts-linked-to-abortion-advice-and-queer-content</a>, See on <a href="https://news.ycombinator.com/item?id=46230072">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Meta has removed or restricted dozens of accounts belonging to abortion access providers, queer groups and reproductive health organisations in the past weeks in what campaigners call one of the “biggest waves of censorship” on its platforms in years.</p><p>The takedowns and restrictions began in October and targeted the Facebook, <a href="https://www.theguardian.com/technology/instagram" data-link-name="in body link" data-component="auto-linked-tag">Instagram</a> and WhatsApp accounts of more than 50 organisations worldwide, some serving tens of thousands of people – in what appears to be a growing push by Meta to limit reproductive health and queer content across its platforms. Many of these were from Europe and the UK, however the bans also affected groups serving women in Asia, Latin America and the Middle East.</p><p>Repro Uncensored, an NGO tracking digital censorship against movements focused on gender, health and justice, said that it had tracked 210 incidents of account removals and severe restrictions affecting these groups this year, compared with 81 last year.</p><p>Meta denied an escalating trend of censorship. “Every organisation and individual on our platforms is <a href="https://transparency.meta.com/policies/community-standards/" data-link-name="in body link">subject to the same set of rules</a>, and any claims of enforcement based on group affiliation or advocacy are baseless,” it said in a statement, adding that its policies on abortion-related content had not changed.</p><figure id="ade83e20-9ddd-4875-994b-bd0be693edef" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/976a0fd5e30e02c3c3dc2ac1e6cdb93f0ca63219/0_0_1195_831/master/1195.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/976a0fd5e30e02c3c3dc2ac1e6cdb93f0ca63219/0_0_1195_831/master/1195.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/976a0fd5e30e02c3c3dc2ac1e6cdb93f0ca63219/0_0_1195_831/master/1195.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/976a0fd5e30e02c3c3dc2ac1e6cdb93f0ca63219/0_0_1195_831/master/1195.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/976a0fd5e30e02c3c3dc2ac1e6cdb93f0ca63219/0_0_1195_831/master/1195.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/976a0fd5e30e02c3c3dc2ac1e6cdb93f0ca63219/0_0_1195_831/master/1195.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="A black box that says ‘We suspended your account, The Queer Agenda’ in the middle of squares of social media content" src="https://i.guim.co.uk/img/media/976a0fd5e30e02c3c3dc2ac1e6cdb93f0ca63219/0_0_1195_831/master/1195.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="309.4518828451883" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>In a recent purge queer and sex-positive accounts were banned.</span> Photograph: Courtesy of Repro Uncensored</figcaption></figure><p>Campaigners say the actions indicate that Meta is taking its Trump-era approach to women’s health and LGBTQ+ issues global. Earlier this year, <a href="https://www.theguardian.com/world/2025/jan/29/abortion-pills-instagram-shadow-banning" data-link-name="in body link">it appeared to “shadow-ban” or remove the accounts</a> of organisations on Instagram or Facebook helping Americans to find abortion pills. Shadow-banning is when a social media platform severely restricts the visibility of a user’s content without telling the user.</p><p>In this latest purge, it blocked abortion hotlines in countries where abortion is legal, banned queer and sex-positive accounts in Europe, and removed posts with even non-explicit, cartoon depictions of nudity.</p><p>“Within this last year, especially since the new US presidency, we have seen a definite increase in accounts being taken down – not only in the US, but also worldwide as a ripple effect,” said Martha Dimitratou, executive director of Repro Uncensored.</p><figure id="7b7aef26-bc34-49f9-bdcc-ae4b35df01d8" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-3"><picture><source srcset="https://i.guim.co.uk/img/media/9cbcb14759839e4807385822bff8b17500bd20b3/0_0_7819_5213/master/7819.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/9cbcb14759839e4807385822bff8b17500bd20b3/0_0_7819_5213/master/7819.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/9cbcb14759839e4807385822bff8b17500bd20b3/0_0_7819_5213/master/7819.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/9cbcb14759839e4807385822bff8b17500bd20b3/0_0_7819_5213/master/7819.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/9cbcb14759839e4807385822bff8b17500bd20b3/0_0_7819_5213/master/7819.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/9cbcb14759839e4807385822bff8b17500bd20b3/0_0_7819_5213/master/7819.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="Mark Zuckerberg and Donald Trump sit at a table laughing; Trump has his hand on Zuckerberg’s back." src="https://i.guim.co.uk/img/media/9cbcb14759839e4807385822bff8b17500bd20b3/0_0_7819_5213/master/7819.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="296.68563754955875" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>US president Donald Trump jokes with Meta CEO Mark Zuckerberg, left, as he hosts tech leaders for a dinner in the state dining room of the White House in Washington DC in September 2025. </span> Photograph: Saul Loeb/AFP/Getty Images</figcaption></figure><p>“This has been, to my knowledge, at least one of the biggest waves of censorship we are seeing,” she said.</p><p><br>
 Campaigners have accused Meta of being condescending and unresponsive, with the company offering only vague reasons why certain accounts were taken down – and appearing unwilling to engage.</p><p>In one email shared with the Guardian, a Meta consultant appears to invite a number of reproductive health organisations to a closed-door online briefing about “the challenges that you are facing with Meta’s content moderation policies”.</p><p>The email says the meeting “will not be an opportunity to raise critiques of Meta’s practices or to offer recommendations for policy changes”.</p><p>Dimitratou said such closed-door meetings had happened before, saying they “reinforce the power imbalance that allows big tech to decide whose voices are amplified and whose are silenced”.</p><p>In another instance, a Meta employee counselled an affected organisation in a personal message to simply move away from the platform entirely and start a mailing list, saying that bans were likely to continue. Meta said it did not send this message.</p><p>Meta’s recent takedowns are part of a broader pattern of the company purging accounts, and then – at times – appearing to backtrack after public pressure, said Carolina Are, a fellow at Northumbria University’s Centre for Digital Citizens.</p><p>“It wouldn’t be as much of a problem if platforms’ appeals actually worked, but they don’t. And appeals are the basis of any democratic justice system,” she added.</p><p>Meta said that it aimed to reduce enforcement mistakes against accounts on its platform, but added that the appeals process for banned accounts had become frustratingly slow.</p><p>Organisations affected by the bans include Netherlands-registered Women Help Women, a nonprofit offering information about abortion to women worldwide, including in Brazil, the Philippines and Poland. It fields about 150,000 emails from women each year, said its executive director, Kinga Jelinska.</p><figure id="e2e87436-142f-43c1-8a29-3b8c38178c5b" data-spacefinder-role="supporting" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-4"><picture><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=380&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 1300px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 1300px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=380&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 1300px)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=300&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 980px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 980px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=300&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 980px)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="A black box that says ‘We suspended your page’ in the middle of squares of social media content" src="https://i.guim.co.uk/img/media/ad4ef5cc572acfe48bcfaa2b1bfe5bc3dcc41e80/0_0_1259_889/master/1259.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="314.2216044479746" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>The feminist group Women Help Women had their page banned by Meta in November, but it has since been reinstated.</span> Photograph: Courtesy of Repro Uncensored</figcaption></figure><p>Women Help Women has been on <a href="https://www.theguardian.com/technology/facebook" data-link-name="in body link" data-component="auto-linked-tag">Facebook</a> for 11 years, said Jelinska, and while its account had been suspended before, this was the first time it was banned outright. The ban could be “life-threatening”, she said, pushing some women towards dangerous, less reliable information sources. Little explanation was given for the ban.</p><p>A message from Meta to the group dated 13 November said its page “does not follow our Community Standards on prescription drugs”, adding: “We know this is disappointing, but we want to keep Facebook safe and welcoming for everyone.”</p><p>“It’s a very laconic explanation, a feeling of opacity,” Jelinska said. “They just removed it. That’s it. We don’t even know which post it was about.”</p><p>Meta said more than half of the accounts flagged by Repro Uncensored have been reinstated, including Women Help Women which it said was taken down in error. “The disabled accounts were correctly removed for violating a variety of our policies including our <a href="https://transparency.meta.com/policies/community-standards/human-exploitation/" data-link-name="in body link">Human Exploitation</a> policy,” it added.</p><p>Jacarandas was founded by a group of young feminists when abortion was <a href="https://www.theguardian.com/global-development/2022/feb/22/colombia-legalises-abortion-in-move-celebrated-as-historic-victory-by-campaigners" data-link-name="in body link">decriminalised in Colombia in 2022</a>, to advise women and girls on how to get a free, legal abortion. The group’s executive director, Viviana Monsalve, said its WhatsApp helpline had been blocked then reinstated three times since October. The WhatsApp account is currently banned and Monsalve said they had received little information from Meta about whether this would continue.</p><p>“We wrote [Meta] an email and said, ‘hey, we are a feminist organisation. We work in abortion. <a href="https://www.theguardian.com/world/abortion" data-link-name="in body link" data-component="auto-linked-tag">Abortion</a> is allowed in Colombia up to 24 weeks. It’s allowed to give information about it,’” said Monsalve.</p><p>Without Meta’s cooperation, Monsalve said it was difficult to plan for the future. “You are not sure if [a ban] will happen tomorrow or after tomorrow, because they didn’t answer anything.”</p><figure id="c3516402-bc9c-4ff6-b3c1-41c5dc0fb476" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:27,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Meta and Google accused of restricting reproductive health information&quot;,&quot;elementId&quot;:&quot;c3516402-bc9c-4ff6-b3c1-41c5dc0fb476&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/global-development/2024/mar/27/meta-and-google-accused-of-restricting-reproductive-health-information&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>Meta said: “Our policies and enforcement regarding abortion medication-related content have not changed: we allow posts and ads promoting healthcare services like abortion, as well as discussion and debate around them, as long as they follow our policies.”</p><p>While groups such as Jacarandas and Women Help Women had their accounts removed outright, other groups said that they increasingly faced Meta restricting their posts and shadow-banning their content.</p><p>Fatma Ibrahim, the director of the Sex Talk Arabic, a UK-based platform which offers Arabic-language content on sexual and reproductive health, said that the organisation had received a message almost every week from Meta over the past year saying that its page “didn’t follow the rules” and would not be suggested to other people, based on posts related to sexuality and sexual health.</p><figure id="3efc4ddb-c1c7-4bb7-ac98-61189cfd0704" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-5"><picture><source srcset="https://i.guim.co.uk/img/media/72d60b6755dcd49881de61a16b2ad609c37a28a7/0_0_1629_1192/master/1629.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/72d60b6755dcd49881de61a16b2ad609c37a28a7/0_0_1629_1192/master/1629.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/72d60b6755dcd49881de61a16b2ad609c37a28a7/0_0_1629_1192/master/1629.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/72d60b6755dcd49881de61a16b2ad609c37a28a7/0_0_1629_1192/master/1629.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/72d60b6755dcd49881de61a16b2ad609c37a28a7/0_0_1629_1192/master/1629.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/72d60b6755dcd49881de61a16b2ad609c37a28a7/0_0_1629_1192/master/1629.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="An illustration of a naked man and woman walking along a path with an arm around each others’ wait while pink hearts float around, one covering their bottoms." src="https://i.guim.co.uk/img/media/72d60b6755dcd49881de61a16b2ad609c37a28a7/0_0_1629_1192/master/1629.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="325.6230816451811" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>An Instagram post from The Sex Talk Arabic that triggered a nudity warning and was removed by Meta.</span> Photograph: Courtesy of Thesextalkarabic</figcaption></figure><p>Two weeks ago, these messages escalated to a warning, in which Meta noted its new policies on nudity and removed a post from the Sex Talk Arabic’s page. The offending <a href="https://www.instagram.com/p/DJMherqt6rA/?igsh=MW9zd254dTJ0MXhtbQ%3D%3D" data-link-name="in body link">post was an artistic depiction of a naked couple</a>, obscured by hearts.</p><p>Ibrahim said the warning was “condescending”, and that Meta’s moderation was US-centric and lacked context.</p><p>“Despite the profits they make from our region, they don’t invest enough to understand the social issues women fight against and why we use social media platforms for such fights,” she said.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A “frozen” dictionary for Python (178 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/1047238/25c270b077849dc0/</link>
            <guid>46229467</guid>
            <pubDate>Thu, 11 Dec 2025 09:51:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/1047238/25c270b077849dc0/">https://lwn.net/SubscriberLink/1047238/25c270b077849dc0/</a>, See on <a href="https://news.ycombinator.com/item?id=46229467">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<div>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider <a href="https://lwn.net/subscribe/">subscribing to LWN</a>.  Thank you
for visiting LWN.net!
</p></div>
</blockquote>

<p>
Dictionaries are ubiquitous in Python code; they are the data structure of
choice for a wide variety of tasks.  But dictionaries are mutable, which
makes them problematic for sharing data in concurrent code.  Python has
added various concurrency features to the language over the last decade or
so—<a href="https://lwn.net/Articles/726600/">async</a>, <a href="https://lwn.net/Articles/947138/">free threading without the global interpreter lock</a>
(GIL), and <a href="https://lwn.net/Articles/941090/">independent subinterpreters</a>—but users must work out their own
solution for an immutable dictionary that can be safely shared by
concurrent code.  There are existing modules that could be used, but a recent proposal, <a href="https://peps.python.org/pep-0814/">PEP 814</a> ("Add frozendict
built-in type"), looks to bring the feature to the language itself.
</p>

<p>
Victor Stinner <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854">announced
the PEP</a> that he and Donghee Na have authored in a post to the PEPs
category of the <a href="https://discuss.python.org/">Python discussion
forum</a> on November 13.  The idea has come up before, including in <a href="https://peps.python.org/pep-0416/">PEP 416</a>, which has essentially
the same title as 814 and was authored by Stinner back in 2012.  It was
rejected by Guido van Rossum at the time, in part due to its target: a <a href="https://lwn.net/Articles/574215/">Python sandbox</a> that never
really panned out.
</p>

<h4><tt>frozendict</tt></h4>

<p>
The idea is fairly straightforward: add <tt>frozendict</tt> as a new
immutable type to the
language's <a href="https://docs.python.org/3/library/builtins.html#module"><tt>builtins</tt>
module</a>.  As Stinner put it:
</p><blockquote>
We expect <tt>frozendict</tt> to be safe by design, as it prevents any unintended modifications. This addition benefits not only CPython's standard library, but also third-party maintainers who can take advantage of a reliable, immutable dictionary type.
</blockquote>


<p>
While <tt>frozendict</tt> has a lot in common with the <a href="https://docs.python.org/3/library/stdtypes.html#mapping-types-dict"><tt>dict</tt></a>
built-in type, it is <a href="https://peps.python.org/pep-0814/#inherit-from-dict">not a subclass of <tt>dict</tt></a>; instead, it
is a subclass of the base <a href="https://docs.python.org/3/library/functions.html#object"><tt>object</tt></a>
type.  The <tt>frozendict()</tt> constructor can be used to create one in
various ways:
</p><pre>    fd = frozendict()           # empty
    fd = frozendict(a=1, b=2)   # frozen { 'a' : 1, 'b' : 2 }
    d = { 'a' : 1, 'b' : 2 }
    fd = frozendict(d)          # same
    l = [ ( 'a', 1 ), ( 'b', 2 ) ]
    fd = frozendict(l)          # same
    fd2 = frozendict(fd)        # same
    assert d == fd == fd2       # True
</pre>


<p>
As with dictionaries, the keys for a <tt>frozendict</tt> must be immutable,
thus <a href="https://docs.python.org/3/glossary.html#term-hashable">hashable</a>,
but the values may or may not be.  For example, a list is a legitimate type
for a value in either type of dictionary, but it is mutable, making the
dictionary as a whole (frozen or not) mutable.  However, if all of the
values stored in a <tt>frozendict</tt> are immutable, it is also immutable,
so it can be hashed and used in places where that is required
(e.g. dictionary keys, set elements, or entries in a <a href="https://docs.python.org/3/library/functools.html#functools.lru_cache"><tt>functools.lru_cache</tt></a>).
</p>

<p>
As might be guessed, based on the last line of the example above, frozen
dictionaries that are hashable can be compared for equality with other
dictionaries of either type.  In addition, neither the <a href="https://docs.python.org/3/library/functions.html#hash"><tt>hash()</tt></a>
value nor the equality test depend on the insertion order of the
dictionary, though that order is preserved in a frozen dictionary (as it is
in the regular variety).  So:
</p><pre>    d = { 'a' : 1, 'b' : 2 }
    fd = frozendict(d)
    d2 = { 'b' : 2, 'a' : 1 }
    fd2 = frozendict(d2)
    assert d == d2 == fd == fd2

    # frozendict unions work too, from the PEP
    &gt;&gt;&gt; frozendict(x=1) | frozendict(y=1)
    frozendict({'x': 1, 'y': 1})
    &gt;&gt;&gt; frozendict(x=1) | dict(y=1)
    frozendict({'x': 1, 'y': 1})
</pre><p>
For the unions, a new frozen dictionary is created in both cases; the
"</p><tt>|=</tt><p>" union-assignment operator also works by generating a new
</p><tt>frozendict</tt><p> for the result.
</p>

<p>
Iteration over a <tt>frozendict</tt> works as expected; the type implements
the <a href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Mapping"><tt>collections.abc.Mapping</tt></a>
abstract base class, so <tt>.items()</tt> returns an iterable of key-value
tuples, while <tt>.keys()</tt> and <tt>.values()</tt> provide the keys and
values of the frozen dictionary.
For the most part, a
<tt>frozendict</tt> acts like a <tt>dict</tt> that cannot change; the
specific differences between the two are <a href="https://peps.python.org/pep-0814/#differences-between-dict-and-frozendict">listed
in the PEP</a>.  It also contains a <a href="https://peps.python.org/pep-0814/#possible-candidates-for-frozendict-in-the-stdlib">lengthy
list</a> of places in the standard library where a <tt>dict</tt> could be switched to a
<tt>frozendict</tt> to "<q>enhance safety and prevent unintended modifications</q>".
</p>

<h4>Discussion</h4>

<p>
The reaction to the PEP was generally positive, with the usual suggestions
for tweaks and more substantive additions to the proposal.  Stinner kept
the discussion focused on the proposal at hand for the most part.  One part
of the proposal was troubling to some: converting a <tt>dict</tt> to a
<tt>frozendict</tt> was described as an O(n) shallow copy.  Daniel F
Moisset <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854/19">thought</a>
that it would make sense to have an in-place transformation that could be
O(1) instead.  He proposed adding a <tt>.freeze()</tt> method that would
essentially just change the type of a <tt>dict</tt> object to
<tt>frozendict</tt>. 
</p>

<p>
However, changing the type of an existing object is fraught with peril, as Brett
Cannon <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854/20">described</a>:

</p><blockquote>
But now you have made that dictionary frozen for everyone who holds a reference to it, which means side-effects at a distance in a way that could be unexpected (e.g. context switch in a thread and now suddenly you're going to get an exception trying to mutate what was a dict a microsecond ago but is now frozen). That seems like asking for really nasty debugging issues just to optimize some creation time.
</blockquote>



<p>
The PEP is not aimed at performance, he continued, but is meant to help
"<q>lessen bugs in concurrent code</q>".  Moisset <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854/23">noted</a>,
that dictionaries can already change in unexpected ways via
<a href="https://docs.python.org/3/library/stdtypes.html#dict.clear"><tt>.clear()</tt></a>
or <a href="https://docs.python.org/3/library/stdtypes.html#dict.update"><tt>.update()</tt></a>,
thus the debugging issues already exist.  He recognized that the
authors may not want to tackle that as part of the PEP, but wanted to try
to ensure that an O(1) transformation was not precluded in the future.
</p>

<p>
Cannon's <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854/47">strong
objection</a> is to changing the type of the object directly.  <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854/24">Ben
Hsing</a> and <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854/48">"Nice
Zombies"</a> proposed ways to construct a new <tt>frozendict</tt> without
requiring the shallow copy—thus O(1)—by either moving the hash table to a
newly created <tt>frozendict</tt>, while clearing the dictionary, or by
using a copy-on-write scheme for the table.  As Steve Dower <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854/58">noted</a>,
that optimization can be added later as long as the PEP does not specify
that the operation <i>must</i> be O(n), which would be a silly thing to do,
but that it sometimes happens "<q>because it makes people stop
complaining</q>", he said in a footnote.  In light of the discussion, the
PEP <a href="https://peps.python.org/pep-0814/#method-to-convert-dict-to-frozendict">specifically
defers</a> that optimization to a later time, suggesting that it could also
be done for other frozen types (<a href="https://docs.python.org/3/library/stdtypes.html#tuple"><tt>tuple</tt></a>
and <a href="https://docs.python.org/3/library/stdtypes.html#frozenset"><tt>frozenset</tt></a>),
perhaps by resurrecting <a href="https://peps.python.org/pep-0351/">PEP
351</a> ("The freeze protocol").
</p>

<p>
On December 1, Stinner <a href="https://discuss.python.org/t/pep-814-add-frozendict-built-in-type/104854/89">announced</a>
that the PEP had been <a href="https://github.com/python/steering-council/issues/325">submitted to
the steering council</a> for pronouncement.  Given that Na is on the
council, though will presumably recuse himself from deciding on this PEP,
he probably has a pretty good sense for how it might be received by the group.
So it seems likely that the PEP has a good chance of being approved.  The
availability of the
free-threaded version of the language (i.e. without the GIL) means that more
multithreaded Python programs are being created, so having a safe way to share
dictionaries 
between threads will be a boon.
</p><br clear="all"><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Archives/PythonIndex/">Python</a></td><td><a href="https://lwn.net/Archives/PythonIndex/#Dictionaries">Dictionaries</a></td></tr>
            <tr><td><a href="https://lwn.net/Archives/PythonIndex/">Python</a></td><td><a href="https://lwn.net/Archives/PythonIndex/#Python_Enhancement_Proposals_PEP-PEP_814">Python Enhancement Proposals (PEP)/PEP 814</a></td></tr>
            </tbody></table><br clear="all">

               <br clear="all">
               <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Cost of a Closure in C (174 pts)]]></title>
            <link>https://thephd.dev/the-cost-of-a-closure-in-c-c2y</link>
            <guid>46228597</guid>
            <pubDate>Thu, 11 Dec 2025 07:21:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thephd.dev/the-cost-of-a-closure-in-c-c2y">https://thephd.dev/the-cost-of-a-closure-in-c-c2y</a>, See on <a href="https://news.ycombinator.com/item?id=46228597">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <header id="main">
        
    </header>

    <section>
    
            <p>I had a vague idea that closures could have a variety of performance implications; I did not believe that so many of the chosen and potential designs for C and C++ extensions ones, however, were so…<!--more--> suboptimal.</p>

<p>But, before we get into how these things perform and what the cost of their designs are, we need to talk about what Closures are.</p>

<h2 id="closures">“Closures”?</h2>

<p>Closures in this instance are programming language constructs that includes data alongside instructions that are not directly related to their input (arguments) and their results (return values). They can be seen as a “generalization” of the concept of a function or function call, in that a function call is a “subset” of closures (e.g., the set of closures that do not include this extra, spicy data that comes from places outside of arguments and returns). These generalized functions and generalized function objects hold the ability to do things like work with “instance” data that is not passed to it directly (i.e., variables surrouding the closure off the stack) and, usually, some way to carry around more data than is implied by their associated function signature.</p>

<p>Pretty much all recent and modern languages include something for Closures unless they are deliberately developing for a target audience or for a source code design that is too “low level” for such a concept (such as Stack programming languages, Bytecode languages, or ones that fashion themselves as assembly-like or close to it). However, we’re going to be focusing on and looking specifically at Closures in C and C++, since this is going to be about trying to work with and – eventually – standardize something for ISO C that works for everyone.</p>

<p>First, let’s show a typical problem that arises in C code to show why closure solutions have popped up all over the C ecosystem, then talk about it in the context of the various solutions.</p>

<h2 id="the-closure-problem">The Closure Problem</h2>

<p>The closure problem can be neatly described by as “how do I get extra data to use within this <code>qsort</code> call?”. For example, consider setting this variable, <code>in_reverse</code>, as part of a bit of command line shenanigans, to change how a sort happens:</p>

<div><pre><code><span>#include</span> <span>&lt;stdlib.h&gt;</span><span>
#include</span> <span>&lt;string.h&gt;</span><span>
#include</span> <span>&lt;stddef.h&gt;</span><span>
</span>
<span>static</span> <span>int</span> <span>in_reverse</span> <span>=</span> <span>0</span><span>;</span>

<span>int</span> <span>compare</span><span>(</span><span>const</span> <span>void</span><span>*</span> <span>untyped_left</span><span>,</span> <span>const</span> <span>void</span><span>*</span> <span>untyped_right</span><span>)</span> <span>{</span>
  <span>const</span> <span>int</span><span>*</span> <span>left</span> <span>=</span> <span>untyped_left</span><span>;</span>
  <span>const</span> <span>int</span><span>*</span> <span>right</span> <span>=</span> <span>untyped_right</span><span>;</span>
  <span>return</span> <span>(</span><span>in_reverse</span><span>)</span> <span>?</span> <span>*</span><span>right</span> <span>-</span> <span>*</span><span>left</span> <span>:</span> <span>*</span><span>left</span> <span>-</span> <span>*</span><span>right</span><span>;</span>
<span>}</span>

<span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>*</span> <span>argv</span><span>[])</span> <span>{</span>
  <span>if</span> <span>(</span><span>argc</span> <span>&gt;</span> <span>1</span><span>)</span> <span>{</span>
    <span>char</span><span>*</span> <span>r_loc</span> <span>=</span> <span>strchr</span><span>(</span><span>argv</span><span>[</span><span>1</span><span>],</span> <span>'r'</span><span>);</span>
    <span>if</span> <span>(</span><span>r_loc</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
      <span>ptrdiff_t</span> <span>r_from_start</span> <span>=</span> <span>(</span><span>r_loc</span> <span>-</span> <span>argv</span><span>[</span><span>1</span><span>]);</span>
      <span>if</span> <span>(</span><span>r_from_start</span> <span>==</span> <span>1</span> <span>&amp;&amp;</span> <span>argv</span><span>[</span><span>1</span><span>][</span><span>0</span><span>]</span> <span>==</span> <span>'-'</span> <span>&amp;&amp;</span> <span>strlen</span><span>(</span><span>r_loc</span><span>)</span> <span>==</span> <span>1</span><span>)</span> <span>{</span>
        <span>in_reverse</span> <span>=</span> <span>1</span><span>;</span>
      <span>}</span> 
    <span>}</span>
  <span>}</span>
  <span>int</span> <span>list</span><span>[]</span> <span>=</span> <span>{</span> <span>2</span><span>,</span> <span>11</span><span>,</span> <span>32</span><span>,</span> <span>49</span><span>,</span> <span>57</span><span>,</span> <span>20</span><span>,</span> <span>110</span><span>,</span> <span>203</span> <span>};</span>
  <span>qsort</span><span>(</span><span>list</span><span>,</span> <span>(</span><span>sizeof</span><span>(</span><span>list</span><span>)</span><span>/</span><span>sizeof</span><span>(</span><span>*</span><span>list</span><span>)),</span> <span>sizeof</span><span>(</span><span>*</span><span>list</span><span>),</span> <span>compare</span><span>);</span>
	
  <span>return</span> <span>list</span><span>[</span><span>0</span><span>];</span>
<span>}</span>
</code></pre></div>

<p>This uses a <code>static</code> variable to have it persist between both the <code>compare</code> function calls that <code>qsort</code> makes and the <code>main</code> call which (potentially) changes its value to be <code>1</code> instead of <code>0</code>. Unfortunately, this isn’t always the best idea for more complex programs that don’t fit within a single snippet:</p>

<ul>
  <li>it is impossible to have different “copies” of a <code>static</code> variable, meaning all mutations done in all parts of the program that can see <code>in_reverse</code> are responsible for knowing the state before and after (e.g., heavily stateful programming of state that you may not own / cannot see);</li>
  <li>working on <code>static</code> data may produce thread contention/race conditions in more complex programs;</li>
  <li>using <code>_Thread_local</code> instead of <code>static</code> only solves the race condition problem but does not solve the “shared across several places on the same thread” problem;</li>
  <li>referring to specific pieces of data or local pieces of data (like <code>list</code> itself) become impossible;</li>
</ul>

<p>and so on, and so forth. This is the core of the problem here. It becomes more pronounced when you want to do things with function and data that are a bit more complex, such as <a href="https://rosettacode.org/wiki/Man_or_boy_test">Donald Knuth’s “Man-or-Boy” test code</a>.</p>

<p>The solutions to these problems come in 4 major flavors in C and C++ code.</p>

<ul>
  <li>Just reimplement the offending function to take a userdata pointer so you can pass whatever data you want (typical C solution, e.g. going from <code>qsort</code> as the sorting function to BSD’s <code>qsort_r</code><sup id="fnref:bsd-qsort_r" role="doc-noteref"><a href="#fn:bsd-qsort_r" rel="footnote">1</a></sup> or Annex K’s <code>qsort_s</code><sup id="fnref:annex-k-qsort_s" role="doc-noteref"><a href="#fn:annex-k-qsort_s" rel="footnote">2</a></sup>).</li>
  <li>Use GNU Nested Functions to just Refer To What You Want Anyways.</li>
  <li>Use Apple Blocks to just Refer To What You Want Anyways.</li>
  <li>Use C++ Lambdas and some elbow grease to just Refer To What You Want Anyways.</li>
</ul>

<p>Each solution has drawbacks and benefits insofar as usability and design, but as a quick overview we’ll show what it’s like using <code>qsort</code> (or <code>qsort_r</code>/<code>qsort_s</code>, where applicable). Apple Blocks, for starters, looks like this:</p>

<div><pre><code><span>#include</span> <span>&lt;stdlib.h&gt;</span><span>
#include</span> <span>&lt;string.h&gt;</span><span>
#include</span> <span>&lt;stddef.h&gt;</span><span>
</span>
<span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>*</span> <span>argv</span><span>[])</span> <span>{</span>
	<span>// local, non-static variable</span>
	<span>int</span> <span>in_reverse</span> <span>=</span> <span>0</span><span>;</span>

	<span>// value changed in-line</span>
	<span>if</span> <span>(</span><span>argc</span> <span>&gt;</span> <span>1</span><span>)</span> <span>{</span>
		<span>char</span><span>*</span> <span>r_loc</span> <span>=</span> <span>strchr</span><span>(</span><span>argv</span><span>[</span><span>1</span><span>],</span> <span>'r'</span><span>);</span>
		<span>if</span> <span>(</span><span>r_loc</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
			<span>ptrdiff_t</span> <span>r_from_start</span> <span>=</span> <span>(</span><span>r_loc</span> <span>-</span> <span>argv</span><span>[</span><span>1</span><span>]);</span>
			<span>if</span> <span>(</span><span>r_from_start</span> <span>==</span> <span>1</span> <span>&amp;&amp;</span> <span>argv</span><span>[</span><span>1</span><span>][</span><span>0</span><span>]</span> <span>==</span> <span>'-'</span> <span>&amp;&amp;</span> <span>strlen</span><span>(</span><span>r_loc</span><span>)</span> <span>==</span> <span>1</span><span>)</span> <span>{</span>
				<span>in_reverse</span> <span>=</span> <span>1</span><span>;</span>
			<span>}</span> 
		<span>}</span>
	<span>}</span>
	
	<span>int</span> <span>list</span><span>[]</span> <span>=</span> <span>{</span> <span>2</span><span>,</span> <span>11</span><span>,</span> <span>32</span><span>,</span> <span>49</span><span>,</span> <span>57</span><span>,</span> <span>20</span><span>,</span> <span>110</span><span>,</span> <span>203</span> <span>};</span>
	
	<span>qsort_b</span><span>(</span><span>list</span><span>,</span> <span>(</span><span>sizeof</span><span>(</span><span>list</span><span>)</span><span>/</span><span>sizeof</span><span>(</span><span>*</span><span>list</span><span>)),</span> <span>sizeof</span><span>(</span><span>*</span><span>list</span><span>),</span>
		<span>// Apple Blocks are Block Expressions, meaning they do not have to be stored</span>
		<span>// in a variable first</span>
		<span>^</span><span>(</span><span>const</span> <span>void</span><span>*</span> <span>untyped_left</span><span>,</span> <span>const</span> <span>void</span><span>*</span> <span>untyped_right</span><span>)</span> <span>{</span>
			<span>const</span> <span>int</span><span>*</span> <span>left</span> <span>=</span> <span>untyped_left</span><span>;</span>
			<span>const</span> <span>int</span><span>*</span> <span>right</span> <span>=</span> <span>untyped_right</span><span>;</span>
			<span>return</span> <span>(</span><span>in_reverse</span><span>)</span> <span>?</span> <span>*</span><span>right</span> <span>-</span> <span>*</span><span>left</span> <span>:</span> <span>*</span><span>left</span> <span>-</span> <span>*</span><span>right</span><span>;</span>
		<span>}</span>
	<span>);</span>
	
	<span>return</span> <span>list</span><span>[</span><span>0</span><span>];</span>
<span>}</span>
</code></pre></div>

<p>and GNU Nested Functions look like this:</p>

<div><pre><code><span>#include</span> <span>&lt;stdlib.h&gt;</span><span>
#include</span> <span>&lt;string.h&gt;</span><span>
#include</span> <span>&lt;stddef.h&gt;</span><span>
</span>
<span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>*</span> <span>argv</span><span>[])</span> <span>{</span>
	<span>// local, non-static variable</span>
	<span>int</span> <span>in_reverse</span> <span>=</span> <span>0</span><span>;</span>

	<span>// modify variable in-line</span>
	<span>if</span> <span>(</span><span>argc</span> <span>&gt;</span> <span>1</span><span>)</span> <span>{</span>
		<span>char</span><span>*</span> <span>r_loc</span> <span>=</span> <span>strchr</span><span>(</span><span>argv</span><span>[</span><span>1</span><span>],</span> <span>'r'</span><span>);</span>
		<span>if</span> <span>(</span><span>r_loc</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
			<span>ptrdiff_t</span> <span>r_from_start</span> <span>=</span> <span>(</span><span>r_loc</span> <span>-</span> <span>argv</span><span>[</span><span>1</span><span>]);</span>
			<span>if</span> <span>(</span><span>r_from_start</span> <span>==</span> <span>1</span> <span>&amp;&amp;</span> <span>argv</span><span>[</span><span>1</span><span>][</span><span>0</span><span>]</span> <span>==</span> <span>'-'</span> <span>&amp;&amp;</span> <span>strlen</span><span>(</span><span>r_loc</span><span>)</span> <span>==</span> <span>1</span><span>)</span> <span>{</span>
				<span>in_reverse</span> <span>=</span> <span>1</span><span>;</span>
			<span>}</span> 
		<span>}</span>
	<span>}</span>
	
	<span>int</span> <span>list</span><span>[]</span> <span>=</span> <span>{</span> <span>2</span><span>,</span> <span>11</span><span>,</span> <span>32</span><span>,</span> <span>49</span><span>,</span> <span>57</span><span>,</span> <span>20</span><span>,</span> <span>110</span><span>,</span> <span>203</span> <span>};</span>
	
	<span>// GNU Nested Function definition, can reference `in_reverse` directly</span>
	<span>// is a declaration/definition, and cannot be used directly inside of `qsort`</span>
	<span>int</span> <span>compare</span><span>(</span><span>const</span> <span>void</span><span>*</span> <span>untyped_left</span><span>,</span> <span>const</span> <span>void</span><span>*</span> <span>untyped_right</span><span>)</span> <span>{</span>
		<span>const</span> <span>int</span><span>*</span> <span>left</span> <span>=</span> <span>untyped_left</span><span>;</span>
		<span>const</span> <span>int</span><span>*</span> <span>right</span> <span>=</span> <span>untyped_right</span><span>;</span>
		<span>return</span> <span>(</span><span>in_reverse</span><span>)</span> <span>?</span> <span>*</span><span>right</span> <span>-</span> <span>*</span><span>left</span> <span>:</span> <span>*</span><span>left</span> <span>-</span> <span>*</span><span>right</span><span>;</span>
	<span>}</span>
	<span>// use in the sort function without the need for a `void*` parameter</span>
	<span>qsort</span><span>(</span><span>list</span><span>,</span> <span>(</span><span>sizeof</span><span>(</span><span>list</span><span>)</span><span>/</span><span>sizeof</span><span>(</span><span>*</span><span>list</span><span>)),</span> <span>sizeof</span><span>(</span><span>*</span><span>list</span><span>),</span> <span>compare</span><span>);</span>
	
	<span>return</span> <span>list</span><span>[</span><span>0</span><span>];</span>
<span>}</span>
</code></pre></div>

<p>or, finally, C++-style Lambdas:</p>

<div><pre><code><span>#define __STDC_WANT_LIB_EXT1__ 1
</span>
<span>#include</span> <span>&lt;stdlib.h&gt;</span><span>
#include</span> <span>&lt;string.h&gt;</span><span>
#include</span> <span>&lt;stddef.h&gt;</span><span>
</span>
<span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>*</span> <span>argv</span><span>[])</span> <span>{</span>
	<span>int</span> <span>in_reverse</span> <span>=</span> <span>0</span><span>;</span>
	
	<span>if</span> <span>(</span><span>argc</span> <span>&gt;</span> <span>1</span><span>)</span> <span>{</span>
		<span>char</span><span>*</span> <span>r_loc</span> <span>=</span> <span>strchr</span><span>(</span><span>argv</span><span>[</span><span>1</span><span>],</span> <span>'r'</span><span>);</span>
		<span>if</span> <span>(</span><span>r_loc</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
			<span>ptrdiff_t</span> <span>r_from_start</span> <span>=</span> <span>(</span><span>r_loc</span> <span>-</span> <span>argv</span><span>[</span><span>1</span><span>]);</span>
			<span>if</span> <span>(</span><span>r_from_start</span> <span>==</span> <span>1</span> <span>&amp;&amp;</span> <span>argv</span><span>[</span><span>1</span><span>][</span><span>0</span><span>]</span> <span>==</span> <span>'-'</span> <span>&amp;&amp;</span> <span>strlen</span><span>(</span><span>r_loc</span><span>)</span> <span>==</span> <span>1</span><span>)</span> <span>{</span>
				<span>in_reverse</span> <span>=</span> <span>1</span><span>;</span>
			<span>}</span> 
		<span>}</span>
	<span>}</span>
	
	<span>// lambdas are expressions, but we can assign their unique variable types with `auto`</span>
	<span>auto</span> <span>compare</span> <span>=</span> <span>[</span><span>&amp;</span><span>](</span><span>const</span> <span>void</span><span>*</span> <span>untyped_left</span><span>,</span> <span>const</span> <span>void</span><span>*</span> <span>untyped_right</span><span>)</span> <span>{</span>
		<span>const</span> <span>int</span><span>*</span> <span>left</span> <span>=</span> <span>(</span><span>const</span> <span>int</span><span>*</span><span>)</span><span>untyped_left</span><span>;</span>
		<span>const</span> <span>int</span><span>*</span> <span>right</span> <span>=</span> <span>(</span><span>const</span> <span>int</span><span>*</span><span>)</span><span>untyped_right</span><span>;</span>
		<span>return</span> <span>(</span><span>in_reverse</span><span>)</span> <span>?</span> <span>*</span><span>right</span> <span>-</span> <span>*</span><span>left</span> <span>:</span> <span>*</span><span>left</span> <span>-</span> <span>*</span><span>right</span><span>;</span>
	<span>};</span>

	<span>int</span> <span>list</span><span>[]</span> <span>=</span> <span>{</span> <span>2</span><span>,</span> <span>11</span><span>,</span> <span>32</span><span>,</span> <span>49</span><span>,</span> <span>57</span><span>,</span> <span>20</span><span>,</span> <span>110</span><span>,</span> <span>203</span> <span>};</span>	

	<span>// C++ Lambdas don't automatically make a trampoline, so we need to provide</span>
	<span>// one ourselves for the `qsort_s/r` case so we can call the lambda</span>
	<span>auto</span> <span>compare_trampoline</span> <span>=</span> <span>[](</span><span>const</span> <span>void</span><span>*</span> <span>left</span><span>,</span> <span>const</span> <span>void</span><span>*</span> <span>right</span><span>,</span> <span>void</span><span>*</span> <span>user</span><span>)</span> <span>{</span>
		<span>typeof</span><span>(</span><span>compare</span><span>)</span><span>*</span> <span>p_compare</span> <span>=</span> <span>user</span><span>;</span>
		<span>return</span> <span>(</span><span>*</span><span>p_compare</span><span>)(</span><span>left</span><span>,</span> <span>right</span><span>);</span>
	<span>};</span>
	<span>qsort_s</span><span>(</span><span>list</span><span>,</span> <span>(</span><span>sizeof</span><span>(</span><span>list</span><span>)</span><span>/</span><span>sizeof</span><span>(</span><span>*</span><span>list</span><span>)),</span> <span>sizeof</span><span>(</span><span>*</span><span>list</span><span>),</span> <span>compare_trampoline</span><span>,</span> <span>&amp;</span><span>compare</span><span>);</span>

	<span>return</span> <span>list</span><span>[</span><span>0</span><span>];</span>
<span>}</span>
</code></pre></div>

<p>To solve this gaggle of problems, pretty much every semi-modern language (that isn’t assembly-adjacent or based on some kind of state/stack programming) provide some idea of being able to associate some set of data with one or more function calls. And, particularly for Closures, this is done in a local way without passing it as an explicit argument. As it turns out, all of those design choices – including the ones in C – have pretty significant consequences on not just usability, but performance.</p>

<h2 id="not-a-big-overview">Not A Big Overview</h2>

<p>This article is <strong>NOT</strong> going to talk in-depth about the <strong>design</strong> of all of the alternatives or other languages. We’re focused on the actual cost of the extensions and what they mean. A detailed overview of the design tradeoffs, their security implications, and other problems, can be read at the <a href="https://thephd.dev/future_cxx/papers/C%20-%20Functions%20with%20Data%20-%20Closures%20in%20C.html">ISO C Proposal for Functions with Closures here</a>; it also gets into things like Security Implications, ABI, current implementation impact, and more of the various designs. The discussion in the paper is pretty long and talks about the dozens of aspects of each solution down to both the design aspect and the implementation quirks. We encourage you to dive into that proposal and read it to figure out if there’s something more specific you care about insofar as some specific design portion. But, this article is going to be concerned about one thing and one thing only:</p>

<h2 id="purrrrrrrformance-3">Purrrrrrrformance <strong>:3</strong>!</h2>

<p>In order to measure this cost, we are going to take Knuth’s Man-or-Boy test and benchmark various styles of implementation in C and C++ using various different extensions / features for the Closure problem. The Man-or-Boy test is an efficient measure of how well your programming language can handle referring to <em>specific</em> entities while engaging in a large degree of recursion and self-reference. It can stress test various portions of how your program creates and passes around data associated with a function call, and if your programming language design is so goofy that it can’t refer to a specific instance of a variable or function argument, it will end up producing the wrong answer and breaking horrifically.</p>

<h2 id="anatomy-of-a-benhcmark-raw-c">Anatomy of a Benhcmark: Raw C</h2>

<p>Here is the core of the Man-or-Boy test, as implemented in raw C. This implementation<sup id="fnref:idk-benchmarks-closures" role="doc-noteref"><a href="#fn:idk-benchmarks-closures" rel="footnote">3</a></sup> and all the others are available online for us all to scrutinize and yell at me for messing up, to make sure I’m not slandering your favorite solution for Closures in this space.</p>

<div><pre><code><span>// ...</span>

<span>static</span> <span>int</span> <span>eval</span><span>(</span><span>ARG</span><span>*</span> <span>a</span><span>)</span> <span>{</span>
	<span>return</span> <span>a</span><span>-&gt;</span><span>fn</span><span>(</span><span>a</span><span>);</span>
<span>}</span>

<span>static</span> <span>int</span> <span>B</span><span>(</span><span>ARG</span><span>*</span> <span>a</span><span>)</span> <span>{</span>
	<span>int</span> <span>k</span>    <span>=</span> <span>*</span><span>a</span><span>-&gt;</span><span>k</span> <span>-=</span> <span>1</span><span>;</span>
	<span>ARG</span> <span>args</span> <span>=</span> <span>{</span> <span>B</span><span>,</span> <span>&amp;</span><span>k</span><span>,</span> <span>a</span><span>,</span> <span>a</span><span>-&gt;</span><span>x1</span><span>,</span> <span>a</span><span>-&gt;</span><span>x2</span><span>,</span> <span>a</span><span>-&gt;</span><span>x3</span><span>,</span> <span>a</span><span>-&gt;</span><span>x4</span> <span>};</span>
	<span>return</span> <span>A</span><span>(</span><span>&amp;</span><span>args</span><span>);</span>
<span>}</span>

<span>static</span> <span>int</span> <span>A</span><span>(</span><span>ARG</span><span>*</span> <span>a</span><span>)</span> <span>{</span>
	<span>return</span> <span>*</span><span>a</span><span>-&gt;</span><span>k</span> <span>&lt;=</span> <span>0</span> <span>?</span> <span>eval</span><span>(</span><span>a</span><span>-&gt;</span><span>x4</span><span>)</span> <span>+</span> <span>eval</span><span>(</span><span>a</span><span>-&gt;</span><span>x5</span><span>)</span> <span>:</span> <span>B</span><span>(</span><span>a</span><span>);</span>
<span>}</span>

<span>// ...</span>
</code></pre></div>

<p>You will notice that there is a big, fat, ugly <code>ARG*</code> parameter hanging around all of these functions. That is because, as stated before, plain ISO C cannot handle passing the data around unless it’s part of a function’s arguments. Because the actual core of the Man-or-Boy experiment is the ability to refer to specific values of <code>k</code> that exist during the recursive run of the program, we need to actually <strong>modify the function signature</strong> and thereby cheat some of the implicit Man-or-Boy requirements of not passing the value in directly. Here’s what <code>ARG</code> looks like:</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>arg</span> <span>{</span>
	<span>int</span> <span>(</span><span>*</span><span>fn</span><span>)(</span><span>struct</span> <span>arg</span><span>*</span><span>);</span>
	<span>int</span><span>*</span> <span>k</span><span>;</span>
	<span>struct</span> <span>arg</span> <span>*</span><span>x1</span><span>,</span> <span>*</span><span>x2</span><span>,</span> <span>*</span><span>x3</span><span>,</span> <span>*</span><span>x4</span><span>,</span> <span>*</span><span>x5</span><span>;</span>
<span>}</span> <span>ARG</span><span>;</span>

<span>static</span> <span>int</span> <span>f_1</span><span>(</span><span>ARG</span><span>*</span> <span>_</span><span>)</span> <span>{</span>
	<span>return</span> <span>-</span><span>1</span><span>;</span>
<span>}</span>

<span>static</span> <span>int</span> <span>f0</span><span>(</span><span>ARG</span><span>*</span> <span>_</span><span>)</span> <span>{</span>
	<span>return</span> <span>0</span><span>;</span>
<span>}</span>

<span>static</span> <span>int</span> <span>f1</span><span>(</span><span>ARG</span><span>*</span> <span>_</span><span>)</span> <span>{</span>
	<span>return</span> <span>1</span><span>;</span>
<span>}</span>

<span>static</span> <span>int</span> <span>eval</span><span>(</span><span>ARG</span><span>*</span> <span>a</span><span>)</span> <span>{</span>
	<span>// ...</span>
<span>}</span>
<span>// ...</span>
</code></pre></div>

<p>And this is how it gets used in the main body of the function in order to compute the right answer and benchmark it:</p>

<div><pre><code><span>static</span> <span>void</span> <span>normal_functions_rosetta</span><span>(</span><span>benchmark</span><span>::</span><span>State</span><span>&amp;</span> <span>state</span><span>)</span> <span>{</span>
	<span>const</span> <span>int</span> <span>initial_k</span>  <span>=</span> <span>k_value</span><span>();</span>
	<span>const</span> <span>int</span> <span>expected_k</span> <span>=</span> <span>expected_k_value</span><span>();</span>
	<span>int64_t</span> <span>result</span>       <span>=</span> <span>0</span><span>;</span>

	<span>for</span> <span>(</span><span>auto</span> <span>_</span> <span>:</span> <span>state</span><span>)</span> <span>{</span>
		<span>int</span> <span>k</span>     <span>=</span> <span>initial_k</span><span>;</span>
		<span>ARG</span> <span>arg1</span>  <span>=</span> <span>{</span> <span>f1</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span> <span>};</span>
		<span>ARG</span> <span>arg2</span>  <span>=</span> <span>{</span> <span>f_1</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span> <span>};</span>
		<span>ARG</span> <span>arg3</span>  <span>=</span> <span>{</span> <span>f_1</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span> <span>};</span>
		<span>ARG</span> <span>arg4</span>  <span>=</span> <span>{</span> <span>f1</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span> <span>};</span>
		<span>ARG</span> <span>arg5</span>  <span>=</span> <span>{</span> <span>f0</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span> <span>};</span>
		<span>ARG</span> <span>args</span>  <span>=</span> <span>{</span> <span>B</span><span>,</span> <span>&amp;</span><span>k</span><span>,</span> <span>&amp;</span><span>arg1</span><span>,</span> <span>&amp;</span><span>arg2</span><span>,</span> <span>&amp;</span><span>arg3</span><span>,</span> <span>&amp;</span><span>arg4</span><span>,</span> <span>&amp;</span><span>arg5</span> <span>};</span>
		<span>int</span> <span>value</span> <span>=</span> <span>A</span><span>(</span><span>&amp;</span><span>args</span><span>);</span>
		<span>result</span> <span>+=</span> <span>value</span> <span>==</span> <span>expected_k</span> <span>?</span> <span>1</span> <span>:</span> <span>0</span><span>;</span>
	<span>}</span>

	<span>if</span> <span>(</span><span>result</span> <span>!=</span> <span>state</span><span>.</span><span>iterations</span><span>())</span> <span>{</span>
		<span>state</span><span>.</span><span>SkipWithError</span><span>(</span><span>"failed: did not produce the right answer!"</span><span>);</span>
	<span>}</span>
<span>}</span>

<span>BENCHMARK</span><span>(</span><span>normal_functions_rosetta</span><span>);</span>
</code></pre></div>

<p>Everything within the <code>for (auto _ : state) { ... }</code> is benchmarked. For those paying attention to the code and find it looking familiar, it’s because that code is the basic structure all Google Benchmark<sup id="fnref:google-benchmark" role="doc-noteref"><a href="#fn:google-benchmark" rel="footnote">4</a></sup> code finds itself looking like. I’ve wanted to swap to Catch2<sup id="fnref:catch2-benchmark" role="doc-noteref"><a href="#fn:catch2-benchmark" rel="footnote">5</a></sup> for a long time now to change to their benchmarking infrastructure, but I’ve been stuck on Google Benchmark because I’ve made a lot of graph-making tools based on its JSON output and I have not vetted Catch2’s JSON output yet to see if it has all of the necessary bits ‘n’ bobbles I use to de-dedup runs and compute statistics.</p>

<p>Everything outside is setup (the part above the <code>for</code> loop) or teardown/test correction (the part below the <code>for</code> loop). The initialization of the <code>ARG args</code>s cannot be moved outside of the measuring loop because each invocation of <code>A</code> – the core of the Man-or-Boy experiment – modifies the <code>k</code> of the ARG parameter, so all of them have to be inside. Conceivably, <code>arg1 .. 5</code> could be moved out of the loop, but I am very tired of looking at the eight or nine variations of this code so someone else can move it and tell me if Clang or GCC has lots of compiler optimization sauce and doesn’t understand that those 5 <code>argI</code>s can be hoisted out of the loop.</p>

<p>The value <code>k</code> is <code>10</code>, and <code>expected_k</code> is <code>-67</code>. The expected, returned <code>k</code> value is dependent on the input <code>k</code> value, which controls how deep the Man-or-Boy test would recurse on itself to produce its answer. Therefore, to prevent GCC and Clang and other MEGA POWERFUL PILLAR COMPILERS from optimizing the entire thing out and just replacing the benchmark loop with <code>ret -67</code>, both <code>k_value()</code> and <code>expected_k_value()</code> come from a Dynamic Link Library (<code>.dylib</code> on MacOS, <code>.so</code> on *nix platforms, <code>.dll</code> on Windows platforms) to make sure that NO amount of optimization (Link Time Optimization/Link Time Code Generation, Inlining Optimization, Cross-Translation Unit Optimization, and Automatic Constant Expression Optimization) from C or C++ compilers could fully preempt all forms of computation.</p>

<p>This allows us to know, for sure, that we’re actually measuring something and not just testing how fast a compiler can load a number into a register and test it against <code>state.iterations()</code>. And, since we know for sure, we can now talk the general methodology.</p>

<h2 id="methodology">Methodology</h2>

<p>The tests were ran on a dying 13-inch 2020 MacBook Pro M1 that has suffered several toddler spills and two severe falls. It has 16 GB of RAM and is son MacOS 15.7.2 Sequoia at the time the test was taken, using the stock MacOS AppleClang Compiler and the stock <code>brew install gcc</code> compiler in order to produce the numbers seen on December 6th, 2025.</p>

<p>There 2 measures being conducted: Real Time and CPU Time. The time is gathered by running a single iteration of the code within the <code>for</code> loop anywhere from a couple thousand to hundreds of thousands of times to produce confidence in that run of the benchmark. This is then averaged to produce the first point. The process is repeated 50 times, repeating that many iterations to build further confidence in the measurement. All 50 means are used as the points for the values, and the average of all of those 50 means is then used as the height of a bar in a bar graph.</p>

<p>The bars are presented side-by-side as a horizontal bar chart with 11 categories of C or C++ code being measured. The 11 categories are:</p>

<ol>
  <li><code>no-op</code>: Literally doing nothing. It’s just there to test environmental noise and make sure none of our benchmarks are so off-base that we’re measuring noise rather than computation. Helps keep us grounded in reality.</li>
  <li><code>Lambdas (No Function Helpers)</code>: a solution using C++-style lambdas. Rather than using helper functions like <code>f0</code>, <code>f1</code>, and <code>f_1</code>, we compute a raw lambda that stores the value meant to be returned for the Man-or-Boy test (<code>return i;</code>) in the lambda itself and then pass that uniquely-typed lambda to the core of the test. The entire test is templated and uses a fake <code>recursion</code> template parameter to halt the recursion after a certain depth.</li>
  <li><code>Lambdas</code>: The same as above but actually using <code>int f0(void)</code>, etc. helper functions at the start rather than lambdas. Reduces inliner pressure by using “normal” types which do not add to the generated number of lambda-typed, recursive, templated function calls.</li>
  <li><code>Lambdas (std::function_ref)</code>: The same as above, but rather than using a function template to handle each uniquely-typed lambda like a precious baby bird, it instead erases the lambda behind a <code>std::function_ref&lt;int(void)&gt;</code>. This allows the recursive function to retain exactly one signature.</li>
  <li><code>Lambdas (std::function)</code>: The same as above, but replaces <code>std::function_ref&lt;int(void)&gt;</code> with <code>std::function&lt;int(void)&gt;</code>. This is its allocating, C++03-style type.</li>
  <li><code>Lambdas (Rosetta Code)</code>: The code straight out of the C++11 Rosetta Code Lambda section on the Man-or-Boy Rosetta Code implementation.</li>
  <li><code>Apple Blocks</code>: Uses Apple Blocks to implement the test, along with the <code>__block</code> specifier to refer directly to certain variables on the stack.</li>
  <li><code>GNU Nested Functions (Rosetta Code)</code>: The code straight out of the C Rosetta Code section on the Man-or-Boy Rosetta Code implementation.</li>
  <li><code>GNU Nested Functions</code>: GNU Nested Functions similar to the Rosetta Code implementation, but with some slight modifications in a hope to potentially alleviate some stack pressure if possible by using regular helper functions like <code>f0</code>, <code>f1</code>, and <code>f_1</code>.</li>
  <li><code>Custom C++ Class</code>: A custom-written C++ class using a discriminated union to decide whether its doing a straight function call or attemping to engage in the Man-or-Boy recursion.</li>
  <li><code>C++03 shared_ptr (Rosetta Code)</code>: A C++ class using <code>std::enable_shared_from_this</code> and <code>std::shared_ptr</code> with a virtual function call to invoke the “right” function call during recursion.</li>
</ol>

<p>The two compilers tested are Apple Clang 17 and GCC 15. There are two graph images because one is for Apple Clang and the other is for GCC. This is particularly important because neither compiler implements the other’s closure extension (Clang does Apple Blocks but not Nested Functions, while GCC does Nested Functions in exclusively its C frontend but does not implement Apple Blocks<sup id="fnref:gcc-apple-blocks" role="doc-noteref"><a href="#fn:gcc-apple-blocks" rel="footnote">6</a></sup>).</p>

<h2 id="the-results">The Results</h2>

<p>Ta-da!</p>

<p><img src="https://thephd.dev/assets/img/2025/12/appleclang17_closure_linear.png" alt=""></p>

<p><sub><sub><sub><a href="https://thephd.dev/assets/img/2025/12/appleclang17_closure_linear.png.txt">For the vision-impaired, a text description is available.</a></sub></sub></sub></p>

<p><img src="https://thephd.dev/assets/img/2025/12/gcc15_closure_linear.png" alt=""></p>

<p><sub><sub><sub><a href="https://thephd.dev/assets/img/2025/12/gcc15_closure_linear.png.txt">For the vision-impaired, a text description is available.</a></sub></sub></sub></p>

<p>… Oh. That looks <strong>awful</strong>.</p>

<p>It turns out that some solutions are so dogwater that it completely screws up our viewing graphs. But, it does let us know that Lambdas used the Rosetta Code style are so unbelievably awful that it is several orders of magnitude more expensive than any other solution presented! One has to wonder what the hell is going on in the code snippet there, but first we need to make the graphs more legible. To do this we’re going to be using the (slightly deceptive) <strong>LOGARITHMIC SCALING</strong>. This is a bit deadly to do because it tends to mislead people about how much of a change there is, so please pay attention to the <strong>potential order of magnitude gains and losses</strong> when going from one bar graph to another.</p>

<p><img src="https://thephd.dev/assets/img/2025/12/appleclang17_closure_logarithmic.png" alt="">
<sub><sub><sub><a href="https://thephd.dev/assets/img/2025/12/appleclang17_closure_logarithmic.png.txt">For the vision-impaired, a text description is available.</a></sub></sub></sub></p>

<p><img src="https://thephd.dev/assets/img/2025/12/gcc15_closure_logarithmic.png" alt=""></p>

<p><sub><sub><sub><a href="https://thephd.dev/assets/img/2025/12/gcc15_closure_logarithmic.png.txt">For the vision-impaired, a text description is available.</a></sub></sub></sub></p>

<p>There we go. Now we can talk about the various solutions and – in particular – why “lambdas” have 4 different entries with such wildly differing performance profiles. First up, let’s talk about the clear performance winners.</p>

<h2 id="lambdas-on-top">Lambdas: On Top!</h2>

<p>Not surprising to anyone who has been checked in to C++, lambdas that are used directly and not type-erased are on top. This means there’s a one-to-one mapping between a function call and a given bit of execution. We are cheating by using a constant parameter to stop the uniquely-typed lambdas being passed into the functions from recursing infinitely, which makes the Man-or-Boy function look like this:</p>

<div><pre><code><span>template</span> <span>&lt;</span><span>int</span> <span>recursion</span> <span>=</span> <span>0</span><span>&gt;</span>
<span>static</span> <span>int</span> <span>a</span><span>(</span><span>int</span> <span>k</span><span>,</span> <span>const</span> <span>auto</span><span>&amp;</span> <span>x1</span><span>,</span> <span>const</span> <span>auto</span><span>&amp;</span> <span>x2</span><span>,</span> <span>const</span> <span>auto</span><span>&amp;</span> <span>x3</span><span>,</span> <span>const</span> <span>auto</span><span>&amp;</span> <span>x4</span><span>,</span> <span>const</span> <span>auto</span><span>&amp;</span> <span>x5</span><span>)</span> <span>{</span>
	<span>if</span> <span>constexpr</span> <span>(</span><span>recursion</span> <span>==</span> <span>11</span><span>)</span> <span>{</span>
		<span>::</span><span>std</span><span>::</span><span>cerr</span> <span>&lt;&lt;</span> <span>"This should never happen and this code should never have been generated."</span> <span>&lt;&lt;</span> <span>std</span><span>::</span><span>endl</span><span>;</span>
		<span>::</span><span>std</span><span>::</span><span>terminate</span><span>();</span>
		<span>return</span> <span>0</span><span>;</span>
	<span>}</span>
	<span>else</span> <span>{</span>
		<span>auto</span> <span>B</span> <span>=</span> <span>[</span><span>&amp;</span><span>](</span><span>this</span> <span>const</span> <span>auto</span><span>&amp;</span> <span>self</span><span>)</span> <span>{</span> <span>return</span> <span>a</span><span>&lt;</span><span>recursion</span> <span>+</span> <span>1</span><span>&gt;</span><span>(</span><span>--</span><span>k</span><span>,</span> <span>self</span><span>,</span> <span>x1</span><span>,</span> <span>x2</span><span>,</span> <span>x3</span><span>,</span> <span>x4</span><span>);</span> <span>};</span>
		<span>return</span> <span>k</span> <span>&lt;=</span> <span>0</span> <span>?</span> <span>x4</span><span>()</span> <span>+</span> <span>x5</span><span>()</span> <span>:</span> <span>B</span><span>();</span>
	<span>}</span>
<span>}</span>
</code></pre></div>

<p>Every <code>B</code> is its own unique type and we are not erasing that unique type when using the expression as an initializer to <code>B</code>. This means that when we call <code>a</code> again with <code>B</code> (the <code>self</code> in this lambda here using Deduced This, a C++23 feature that cannot be part of the C version of lambdas) which means we need to use <code>auto</code> parameters (a shortcut way of writing template parameters) to take it. But, since every parameter is unique, and every <code>B</code> is unique, calling this recursively means that, eventually, C++ compilers will actually just completely crash out/toss out-of-memory errors/say we’ve compile-time recursed too hard, or similar. That’s why the compile-time <code>if constexpr</code> on the extra, templated <code>recursion</code> parameter needs to have some arbitrary limit. Because we know <code>k</code> starts at 10 for this test, we just have some bogus limit of “11”.</p>

<p>This results in a very spammy recursive chain of function calls, where the actual generated names of these template functions is <strong>far</strong> more complex than <code>a</code> and can run the compiler into the ground / cause quite a bit of instantiations if you let <code>recursion</code> get to a high enough value. But, once you add the limit, the compiler gets perfect information about this recursive call all the way to every leaf, and thus is able to not only optimize the hell out of it, but refuse to generate the other frivolous code it knows won’t be useful.</p>

<h3 id="lambdas-are-also-fast-even-when-type-erased">Lambdas are also Fast, even when Type-Erased</h3>

<p>You can observe a slight bump up in performance penalty when a Lambda is erased by a <code>std::function_ref</code>. This is a low-level, non-allocating, non-owning, slim “view” type that is analogous to what a language-based wide function pointer type would be in C. From this, it allows us to <em>guess</em> how good Lambdas in C would be even if you had to hide them behind a non-unique type.</p>

<p>The performance metrics are about equivalent to if you hand-wrote a C++ class with a custom <code>operator()</code> that uses a discriminated union, no matter which compiler gets used to do it. It’s obviously not as fast as having access to a direct function call and being able to slurp-inline optimize, but the performance difference is acceptable when you do not want to engage in a large degree of what is called “monomorphisation” of a genric routine or type. And, indeed, outside of macros, C has no way of doing this innately that isn’t runtime-based.</p>

<p>A very strong contender for a good solution!</p>

<h3 id="lambdas-on-bottom-too">Lambdas: On…. Bottom, too?</h3>

<p>One must wonder, then, why the <code>std::function</code> Lambdas and the Rosetta Code Lambdas are either bottom-middle-of-the-road or absolutely-teary-eyed-awful.</p>

<p>Starting off, the <code>std::function</code> Lambdas are bad because of exactly that: <code>std::function</code>. <code>std::function</code> is not a “cheap” closure; it is a potentially-allocating, meaty, owning function abstraction. This means that it’s safe to make one and pass it around and store it and call it later; the cost of this is, obviously, that you’re allocating (when the type is big enough) for that internal storage. Part of this is alleviated by using <code>const std::function&lt;int(void)&gt;&amp;</code> parameters, taking things by reference and only generating a new object when necessary. This prevents copying on every function call. Both the Rosetta Lambdas and regular <code>std::function</code> Lambdas code does the by-reference parameters bit, though, so where does the difference come in? It actually has to do with the Captures. Here’s how <code>std::function</code> Lambdas defines the recursive, self-referential lambda and uses it:</p>

<div><pre><code><span>using</span> <span>f_t</span> <span>=</span> <span>std</span><span>::</span><span>function</span><span>&lt;</span><span>int</span><span>(</span><span>void</span><span>)</span><span>&gt;</span><span>;</span>

<span>inline</span> <span>static</span> <span>int</span> <span>A</span><span>(</span><span>int</span> <span>k</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x1</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x2</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x3</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x4</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x5</span><span>)</span> <span>{</span>
	<span>f_t</span> <span>B</span> <span>=</span> <span>[</span><span>&amp;</span><span>]</span> <span>{</span> <span>return</span> <span>A</span><span>(</span><span>--</span><span>k</span><span>,</span> <span>B</span><span>,</span> <span>x1</span><span>,</span> <span>x2</span><span>,</span> <span>x3</span><span>,</span> <span>x4</span><span>);</span> <span>};</span>
	<span>return</span> <span>k</span> <span>&lt;=</span> <span>0</span> <span>?</span> <span>x4</span><span>()</span> <span>+</span> <span>x5</span><span>()</span> <span>:</span> <span>B</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>And, here is how the Rosetta Code Lambdas defines the recursive, self-referential lambda and uses it:</p>

<div><pre><code><span>using</span> <span>f_t</span> <span>=</span> <span>std</span><span>::</span><span>function</span><span>&lt;</span><span>int</span><span>(</span><span>void</span><span>)</span><span>&gt;</span><span>;</span>

<span>inline</span> <span>static</span> <span>int</span> <span>A</span><span>(</span><span>int</span> <span>k</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x1</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x2</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x3</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x4</span><span>,</span> <span>const</span> <span>f_t</span><span>&amp;</span> <span>x5</span><span>)</span> <span>{</span>
	<span>f_t</span> <span>B</span> <span>=</span> <span>[</span><span>=</span><span>,</span> <span>&amp;</span><span>k</span><span>,</span> <span>&amp;</span><span>B</span><span>]</span> <span>{</span> <span>return</span> <span>A</span><span>(</span><span>--</span><span>k</span><span>,</span> <span>B</span><span>,</span> <span>x1</span><span>,</span> <span>x2</span><span>,</span> <span>x3</span><span>,</span> <span>x4</span><span>);</span> <span>};</span>
	<span>return</span> <span>k</span> <span>&lt;=</span> <span>0</span> <span>?</span> <span>x4</span><span>()</span> <span>+</span> <span>x5</span><span>()</span> <span>:</span> <span>B</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>The big problem here is in the use of the <code>=</code>. What <code>=</code> by itself in the front of a lambda capture clause means is “copy all the visible variables in and hold onto that copy” (unless the capture for that following variable is “overridden” by a <code>&amp;var</code>, address capture). Meanwhile, the <code>&amp;</code> is the opposite: it means “refer to all the visible variables directly by their address and do not copy them in”. So, while the <code>std::function</code> Lambda is (smartly) referring to stuff directly without copying because we know for the Man-or-Boy test that referring to things directly is not an unsafe operation, the general <code>=</code> causes that for the several dozen recursive iterations through the function, it is copying all five allocating <code>std::function</code> arguments. So the first call creates a <code>B</code> that copies everything in, and then passes that in, and then the next call copies the previous <code>B</code> and the 4 normal functions, and then passes that in to the next <code>B</code>, and then it copies <strong>both</strong> previous <code>B</code>’s, and this stacks for the depth of the callgraph (some 10 times since <code>k = 10</code> to start).</p>

<p>You can imagine how much that completely screws with the performance, and it explains why the Rosetta Code Lambdas code behaves so poorly in terms of performance. But, this also raises a question: if referring to everything by-reference saves so much speed, then why does GNU Nested Functions – in all its variants – perform so poorly? After all, Nested Functions capture everything by reference / by address, exactly like a lambda does with <code>[&amp;]</code>.</p>

<p>Similarly, if allocating over and over again was so expensive, how come Apple Blocks and C++03 <code>shared_ptr</code> Rosetta Code-style versions of the Man-or-Boy test don’t perform nearly as badly as the Rosetta Code Lambdas? Are we not copying the value of the arguments into a newly created Apple Block and, thusly, tanking the performance metrics? Well, as it turns out, there’s many reasons for these things, so let’s start with GNU Nested Functions.</p>

<h2 id="nested-functions-and-the-stack">Nested Functions and The Stack</h2>

<p>I’ve written about it <a href="https://thephd.dev/lambdas-nested-functions-block-expressions-oh-my">dozens of times</a> now, but the prevailing and most common implementation of Nested Functions is with an executable stack. The are <a href="https://thephd.dev/_vendor/future_cxx/papers/C%20-%20Functions%20with%20Data%20-%20Closures%20in%20C.html#intro-nested.functions-security">a lot of security and other implications for this</a>, but all you need to understand is that the <em>reason</em> GCC did this is because it was an at-the-time slick encoding of both the <em>location</em> of the variables and the <em>routine</em> itself. Allocating a chunk of data off of the current programming stack means that the “environment context”/”this closure” pointer has the same anchoring address as the routine itself. This means you can encode both the location of the data to know what to access <em>and</em> the address of a function’s entry point into a single thing that works with your typical setup-and-call convention that comes with invoking a standard ISO C function pointer.</p>

<p>But think about that, briefly, in terms of optimization.</p>

<p>You are using the function’s stack frame at that precise point in the program as the “base address” for this executable code. That base address also means that all the variables associated with it need to be <strong>reachable</strong> from that base address: i.e., that things are not stuffed in registers, but that you are referring to the same variables as modified by the enclosing function around your nested function. Principally, this means that your function needs to have all of the following now so that GNU Nested Functions <em>actually</em> work.</p>

<ul>
  <li>A stack that is executable so that the base address used for the trampoline can be run succinctly.</li>
  <li>A real function frame that exists somewhere in memory to serve as the base address for the trampoline.</li>
  <li>Real objects in memory backing the names of the captured variables accesses.</li>
</ul>

<p>This all seems like regular consequences, until you tack on the second order affects from the point of optimization.</p>

<ul>
  <li>A stack that now has both data and instructions all blended into itself.</li>
  <li>A real function frame, which means no ommission of a frame pointer and no collapsing / inlining of that function frame.</li>
  <li>Real objects that all have their address taken that are tied to the function frame, which must be memory-accessible and which the compiler now has a hard time telling if they can simply be exchanged through registers or if the need to <strong>actually</strong> sit somewhere in memory.</li>
</ul>

<p>In other words: GNU Nested Functions have created the perfect little storm for what might be the best optimizer-murderer. The reason it performs so drastically poorly (worse than even allocating lambdas inside of a <code>std::function</code> or C++03-style virtual function calls inside of a bulky, nasty C++ <code>std::shared_ptr</code>) by a whole order of magnitude or more is that everything about Nested Functions and their current implementation is basically Optimizer Death. If the compiler can’t see through everything – and the Man-or-Boy test with a non-constant value of <code>k</code> and <code>expected_k</code> – GNU Nested Functions deteriorate rapidly. It takes every core optimization technique that we’ve researched and maximized on in the last 30 years and puts a shotgun to the side of its head once it can’t pre-compute <code>k</code> and <code>expected_k</code>.</p>

<p>The good news is that GCC has completed a new backing implementation for GNU Nested Functions, which uses a heap-based trampoline. Such a trampoline does not interfere with the stack, would allow for omission of frame pointers while referring directly to the data itself (which may prevent the wrecking of specific kinds of inlining optimizations), and does not need an executable stack (just a piece of memory from ✨somewhere✨ it can mark executable). This may have performance closer to Apple Blocks, but we don’t have a build of the latest GCC to test it with. But, when we do, we can simply add the compilation flag <code>-ftrampoline-impl=heap</code> to the two source files in CMake and then let the benchmarks run again to see how it stacks up!</p>

<p>Finally, there is a <em>minor</em> performance degredation because our benchmarking software is in C++ and this extension exists exclusively in the C frontend of GCC. That means I have to use an <code>extern</code> function call within the benchmark loop to get to the actual code. Within the function call, however, all of this stuff should be optimized down, so the cost of a <em>single</em> function call’s stack frame shouldn’t be so awful, but I expect to try to dig into this better to help make sure the <code>extern</code> of a C function call isn’t making things dramatically worse than they are. Given it’s a different translation unit and it’s <strong>not</strong> being compiled as a separate static or dynamic library, it should still link together and optimize cleanly, but given how bad it’s performing? Every possible issue is on the table.</p>

<h2 id="what-about-apple-blocks">What about Apple Blocks?</h2>

<p>Apple Blocks are not the fastest, but they the best of the C extensions while being the worst of the “fast” solutions. They are not faster than just hacking the <code>ARG*</code> into the function signature and using regular normal C function calls, unfortunately, and that’s likely due to their shared, heap-ish nature. The saddest part about Apple Blocks is that it works using a Blocks Runtime that is already as optimized as it can possibly be: Clang and Apple both document that whie the Blocks Runtime does manage an Automatic Reference Counted (ARC) Heap of Block pointers, when a Block is first created it will literally have its memory stored on the stack rather than in the heap. In order to move it to the heap, one must call <code>Block_copy</code> to trigger the “normal” heap-based shenanigans. We never call <code>Block_copy</code>, so this is with as-fast-as-possible variable access and management with few allocations.</p>

<p>It’s very slightly disappointing that: normal C functions with an <code>ARG*</code> blob; a custom C++ class using a discriminated union and <code>operator()</code>; any mildly conscientious use of lambdas; and, any other such shenanigans perform better than the very best Apple Blocks has to offer. One has to imagine that all of the ARC management functions made to copy the <code>int^(void)</code> “hat-style” function pointers, even if they end up not doing much for the data stored on the stack, impacted the results here. But, this is also somewhat good news: because Apple Block hat pointers are cheaply-copiable entities (they are just pointers to a Block object), it means that even if we copy all of the arguments into the closure every function call, that copying is about as cheap as it can get. Obivously, as regular “Lambdas” and “Lambas (No Function Helpers)” demonstrate, being able to just slurp everything up by address/by reference – including visible function arguments – with <code>[&amp;]</code> saves us a teensy, tiny bit of time<sup id="fnref:apple-blocks-parameters" role="doc-noteref"><a href="#fn:apple-blocks-parameters" rel="footnote">7</a></sup>.</p>

<p>The cheapness of <code>int^(void)</code> hat-pointer function types is likely the biggest saving grace for Apple Blocks in this benchmark. In the one place we need to be careful, we rename the input argument <code>k</code> to <code>arg_k</code> and then make a <code>__block</code> variable to actually refer to a shared <code>int k</code> (and get the right answer):</p>

<div><pre><code><span>static</span> <span>int</span> <span>a</span><span>(</span><span>int</span> <span>arg_k</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x1</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x2</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x3</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x4</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x5</span><span>)</span> <span>{</span>
	<span>__block</span> <span>int</span> <span>k</span>    <span>=</span> <span>arg_k</span><span>;</span>
	<span>__block</span> <span>fn_t</span> <span>^</span> <span>b</span> <span>=</span> <span>^</span><span>(</span><span>void</span><span>)</span> <span>{</span> <span>return</span> <span>a</span><span>(</span><span>--</span><span>k</span><span>,</span> <span>b</span><span>,</span> <span>x1</span><span>,</span> <span>x2</span><span>,</span> <span>x3</span><span>,</span> <span>x4</span><span>);</span> <span>};</span>
	<span>return</span> <span>k</span> <span>&lt;=</span> <span>0</span> <span>?</span> <span>x4</span><span>()</span> <span>+</span> <span>x5</span><span>()</span> <span>:</span> <span>b</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>All of the <code>x1</code>, <code>x2</code>, and <code>x3</code> – like the bad Lambda case – are copied over and over and over again. One could change the name of all the arugments <code>arg_xI</code> and then have an <code>xI</code> variable inside that is marked <code>__block</code>, but that’s more effort and very unlikely to have any serious impact on the code while possibly degrading performance for the setup of multiple shared variables that all have to also be ARC-reference-counted and be stored inside each and every new <code>b</code> block that is created.</p>

<h2 id="a-brief-aside-self-referencing-functionsclosures">A Brief Aside: Self-Referencing Functions/Closures</h2>

<p>It’s also important to note that just writing this:</p>

<div><pre><code><span>static</span> <span>int</span> <span>a</span><span>(</span><span>int</span> <span>arg_k</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x1</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x2</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x3</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x4</span><span>,</span> <span>fn_t</span> <span>^</span> <span>x5</span><span>)</span> <span>{</span>
	<span>__block</span> <span>int</span> <span>k</span>    <span>=</span> <span>arg_k</span><span>;</span>
	<span>fn_t</span> <span>^</span> <span>b</span> <span>=</span> <span>^</span><span>(</span><span>void</span><span>)</span> <span>{</span> <span>return</span> <span>a</span><span>(</span><span>--</span><span>k</span><span>,</span> <span>b</span><span>,</span> <span>x1</span><span>,</span> <span>x2</span><span>,</span> <span>x3</span><span>,</span> <span>x4</span><span>);</span> <span>};</span>
	<span>return</span> <span>k</span> <span>&lt;=</span> <span>0</span> <span>?</span> <span>x4</span><span>()</span> <span>+</span> <span>x5</span><span>()</span> <span>:</span> <span>b</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>(no <code>__block</code> on the <code>b</code> variable) is actually a huge bug. Apple Blocks, like older C++ Lambdas, cannot technically refer to “itself” inside. You have to refer to the “self” by capturing the variable it set to. For those who use C++ and are familiar with the lambdas over there, it’s like making sure you capture the variable you initialize with the lambda by reference while <em>also</em> making sure it has a concrete type. It can only be escaped by using <code>auto</code> and Deducing This, or some other combination of referential-use. That is:</p>

<ul>
  <li><code>auto x = [&amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }</code> does not compile, as the type <code>auto</code> isn’t figured out yet;</li>
  <li><code>std::function_ref&lt;int(int)&gt; x = [&amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }</code> compiles but due to C++ shenanigans produces a dangling reference to a temporary lambda that dies after the full expression (the initialization);</li>
  <li><code>std::function&lt;int(int)&gt; x = [&amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }</code> compiles and works with no segfaults because <code>std::function</code> allocates, and the reference to itself <code>&amp;x</code> is just fine.</li>
  <li>and, finally, <code>auto x = [](this const auto&amp; self, int v) { if (v != limit) self(v + 1); return v + 8; }</code> which compiles and works with no segfaults because the invisible <code>self</code> parameter is just a reference to the current object.</li>
</ul>

<p>The problem with the most recent Apple Blocks snippet just above is that it’s the equivalent of doing</p>

<ul>
  <li><code>std::function&lt;int(int)&gt; x = [x](int v) { if (v != limit) x(v + 1); return v + 8; }</code></li>
</ul>

<p>Notice that there’s no <code>&amp;x</code> in the lambda initializer’s capture list. It’s copying an (uninitialized) variable by-value into the lambda. This is what Apple Blocks set into a variable that does not have a <code>__block</code> specifier, like in our bad code case with <code>b</code>.</p>

<p>All variations of this on all implementations which allow for self-referencing allow this and compile some form of this. You would imagine some implementations would warn about this, but this is leftover nonsense from allowing a variable to refer to itself in its initialization. The obvious reason this happens in C and C++ is because you can create self-referential structures, but unfortunately neither languages provided a safe way to do this generally. C++23’s Deducing This does not work inside of regular functions and non-objects, so good luck applying to other places and other extensions. The only extension which does not suffer this problem is GNU Nested Functions, because it creates a function declaration / definition rather than a variable with an initializer. Thus, this code from the benchmarks works:</p>

<div><pre><code><span>inline</span> <span>static</span> <span>int</span> <span>gnu_nested_functions_a</span><span>(</span><span>int</span> <span>k</span><span>,</span> <span>int</span> <span>xl</span><span>(</span><span>void</span><span>),</span> <span>int</span> <span>x2</span><span>(</span><span>void</span><span>),</span> <span>int</span> <span>x3</span><span>(</span><span>void</span><span>),</span> <span>int</span> <span>x4</span><span>(</span><span>void</span><span>),</span> <span>int</span> <span>x5</span><span>(</span><span>void</span><span>))</span> <span>{</span>
	<span>int</span> <span>b</span><span>(</span><span>void</span><span>)</span> <span>{</span>
		<span>return</span> <span>gnu_nested_functions_a</span><span>(</span><span>--</span><span>k</span><span>,</span> <span>b</span><span>,</span> <span>xl</span><span>,</span> <span>x2</span><span>,</span> <span>x3</span><span>,</span> <span>x4</span><span>);</span>
	<span>}</span>
	<span>return</span> <span>k</span> <span>&lt;=</span> <span>0</span> <span>?</span> <span>x4</span><span>()</span> <span>+</span> <span>x5</span><span>()</span> <span>:</span> <span>b</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>And it has the semantics one would expect, unlike how Blocks, Lambdas, or others with default by-value copying works.</p>

<p>In the general case, this is what the paper <code>__self_func</code> was going to solve<sup id="fnref:__self_func" role="doc-noteref"><a href="#fn:__self_func" rel="footnote">8</a></sup>, but… that’s going to need some time for me to convince WG14 that maybe it IS actually a good idea. We can probably just keep writing the buggy code a few dozen more times for the recursion case and keep leaving it error prone, but I’ll try my best to convince them one more time that the above situation is very not-okay.</p>

<h2 id="thinking-it-over">Thinking It Over</h2>

<p>While the Man-or-Boy test isn’t exactly the end-all, be-all performance test, due to flexing both (self)-referential data and utilization of local copies with recursion, it is surprisingly suitable for figuring out if a closure design is decent enough in a mid to high-level programming language. It also gives me some confidence that, at the very least, the baseline for performance of statically-known, compile-time understood, non-type erased, callable Closure object will have the best implementation quality and performance tradeoffs for a language like ISO C no matter the compiler implementation.</p>

<p>In the future, at some point, I’ll have to write about <strong>why</strong> that is. It’s a bit upside-down from the perspective of readers of this blog to <strong>first</strong> address performance and then later write about the design, but it’s nice to make sure we’re not designing ourselves into a bad performance corner at the offset of this whole adventure.</p>

<h2 id="learned-insights">Learned Insights</h2>

<p>Surprising nobody, the more information the compiler is allowed to accrue (the Lambda design), the better its ability to make the code fast. What might be slightly more surprising is that a <strong>slim</strong>, <strong>compact</strong> layer of type erasure – not a bulky set of Virtual Function Calls (C++03 <code>shared_ptr</code> Rosetta Code design) – does not actually cost much at all (Lambdas with <code>std::function_ref</code>). This points out something else that’s part of the ISO C proposal for Closures (but not formally in its wording): Wide Function Pointers.</p>

<p>The ability to make a thin <code>{ some_function_type* func; void* context; }</code> type backed by the compiler in C would be extremely powerful. Martin Uecker has a proposal that has received interest and passing approval in the Committee, but it would be nice to <a href="https://thephd.dev/_vendor/future_cxx/papers/C%20-%20Functions%20with%20Data%20-%20Closures%20in%20C.html#appendix-wide.function.pointer">move it along in a nice direction</a>. My suggestion is having <code>%</code> as a modifier, so it can be used easily since wide function pointers are an extremely prevalent concept. Being able to write something like the following would be very easy and helpful.</p>

<div><pre><code><span>typedef</span> <span>int</span><span>(</span><span>compute_fn_t</span><span>)(</span><span>int</span><span>);</span>

<span>int</span> <span>do_computation</span><span>(</span><span>int</span> <span>num</span><span>,</span> <span>compute_fn_t</span><span>%</span> <span>success_modification</span><span>);</span>
</code></pre></div>

<p>A wide function pointer type like this would also be traditionally convertible from a number of already-existing extensions, too, where GNU Nested Functions, Apple Blocks, C++-style Lambdas, and more could create the appropriate wide function pointer type to be cheaply used. Additionally, it also works for FFI: things like Go closures already use GCC’s <code>__builtin_call_with_static_chain</code> to transport through their Go functions in C. Many other functions from other languages could be cheaply and efficiently bridged with this, without having to come up with hairbrained schemes about where to put a <code>void* userdata</code> or some kind of implicit context pointer / implicit environment pointer.</p>

<h2 id="existing-extensions">Existing Extensions?</h2>

<p>Unfortunately – except for the borland closure annotation – there’s too many things that are performance-stinky about both GNU Nested Functions and Apple Blocks. It’s no wonder GCC is trying to add <code>-ftrampoline-impl=heap</code> to the story of GNU Nested Functions; they might be able to tighten up that performance and make it more competitive with Apple Blocks. But, unfortunately, since it is heap-based, there’s a real chance that its <strong>maximum</strong> performance ceiling is only as good as Apple Blocks, and <strong>not</strong> as good as a C++-style Lambda.</p>

<p>Both GNU Nested Functions and Apple Blocks – as they are implemented – do not really work well in ISO C. GNU Nested Functions because their base design and most prevalent implementation are performance-awful, but also Apple Blocks because of the copying and indirection runtime of Blocks that manage ARC pointers providing a hard upper limit on how good the performance can actually be in complex cases.</p>

<p>Regular C code, again, performs middle-of-the-road here. It’s not the worst of it, but it’s not the best at all, which means there’s some room beneath how we could go having the C code run. While it’s hard to fully trust the Rosetta Code Man-or-Boy code for C as the best, it is a pretty clear example of how a “normal” C developer would do it and how it’s not actually able to hit maximum performance for this situation.</p>

<p>I wanted to add a version of regular C code that used a dynamic array with <code>static</code>s to transfer data, or a bunch of <code>thread_local</code>s, but I could not bring myself to actually care enough to write a complex association scheme from a specific invocation of the recursive function <code>a</code> and the slot of dynamic data that represented the closure’s data. I’m sure there’s schemes for it and I could think of a few, but at that point it’s such a violent contortion to get a solution that going that I figured it simply wasn’t worth the effort. But, as always,</p>

<p>pull requests are welcome. 💚</p>

<ul>
  <li>Banner and Title Photo by <a href="https://www.pexels.com/photo/person-holding-black-card-holder-928181/">Lukas, from Pexels</a></li>
</ul>







    
    </section>

    <!-- Social media shares -->
    






    <!-- Category and Tag list -->
    <div data-testid="tag-list">
    <ul>
      
        <li>Tags</li>
      

      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#C">
          <p><i></i> C</p>
        </a></li>
      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#C+standard">
          <p><i></i> C standard</p>
        </a></li>
      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#C%2B%2B">
          <p><i></i> C++</p>
        </a></li>
      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#blocks">
          <p><i></i> blocks</p>
        </a></li>
      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#closures">
          <p><i></i> closures</p>
        </a></li>
      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#functions">
          <p><i></i> functions</p>
        </a></li>
      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#performance">
          <p><i></i> performance</p>
        </a></li>
      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#purrformance">
          <p><i></i> purrformance</p>
        </a></li>
      
        <li><a data-testid="tag-link" href="https://thephd.dev/tags#%F0%9F%93%8A">
          <p><i></i> 📊</p>
        </a></li>
      
    </ul>
  </div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Incomplete list of mistakes in the design of CSS (180 pts)]]></title>
            <link>https://wiki.csswg.org/ideas/mistakes</link>
            <guid>46227619</guid>
            <pubDate>Thu, 11 Dec 2025 04:20:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wiki.csswg.org/ideas/mistakes">https://wiki.csswg.org/ideas/mistakes</a>, See on <a href="https://news.ycombinator.com/item?id=46227619">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page">

<p>
That should be corrected if anyone invents a time machine. :P
</p>
<ul>
<li><p><code>white-space: nowrap</code> should be <code>white-space: no-wrap</code></p>
<ul>
<li><p> and line wrapping behavior should not have been added to <code>white-space</code></p>
</li>
</ul>
</li>
<li><p><code>animation-iteration-count</code> should just have been <code>animation-count</code> (like <code>column-count</code>!)</p>
</li>
<li><p><code>vertical-align</code> should not apply to table cells. Instead the CSS3 alignment properties should exist in Level 1.</p>
</li>
<li><p><code>vertical-align: middle</code> should be <code>text-middle</code> or <code>x-middle</code> because it's not really in the middle, and such a name would better describes what it does.</p>
</li>
<li><p> Percentage heights should be calculated against <code>fill-available</code> rather than being undefined in auto situations.</p>
</li>
<li><p> Table layout should be sane.</p>
</li>
<li><p> Box-sizing should be <code>border-box</code> by default.</p>
</li>
<li><p><code>background-size</code> with one value should duplicate its value, not default the second one to <code>auto</code>. Ditto <code>translate()</code>.</p>
</li>
<li><p><code>background-position</code> and <code>border-spacing</code> (all 2-axis properties) should take *vertical* first, to match with the 4-direction properties like <code>margin</code>.</p>
</li>
<li><p> Not quite a mistake, because it was a reasonable default for the 90s, but it would be more helpful since then if `background-repeat` defaulted to `no-repeat`.</p>
</li>
<li><p> The 4-value shorthands like <code>margin</code> should go counter-clockwise (so that the inline-start value is before the block-end and inline-end values instead of after them).</p>
</li>
<li><p><code>z-index</code> should be called <code>z-order</code> or <code>depth</code> and should Just Work on all elements (like it does on flex items).</p>
</li>
<li><p><code>word-wrap</code>/<code>overflow-wrap</code> should not exist. Instead, <code>overflow-wrap</code> should be a keyword on 'white-space', like <code>nowrap</code> (<code>no-wrap</code>).</p>
</li>
<li><p> The top and bottom margins of a single box should never have been allowed to collapse together automatically as this is the <strong>root of all margin-collapsing evil</strong>.</p>
</li>
<li><p> Partial collapsing of margins instead of weird rules to handle min/max-heights?</p>
</li>
<li><p> Tables (like other non-blocks, e.g. flex containers) should form pseudo-stacking contexts.</p>
</li>
<li><p> The <code>currentColor</code> keyword should have retained the dash, <code>current-color</code>, as originally specified. Likewise all other color multi-word keyword names.</p>
</li>
<li><p> There should have been a predictable color naming system (like CNS) instead of the arbitrary X11 names which were eventually adopted.</p>
</li>
<li><p><code>border-radius</code> should have been <code>corner-radius</code>.</p>
</li>
<li><p> Absolutely-positioned replaced elements should stretch when opposite offset properties (e.g. left+right) are set, instead of being start-aligned.</p>
</li>
<li><p> The <code>hyphens</code> property should be called <code>hyphenate</code>. (It's called <code>hyphens</code> because the XSL:FO people objected to <code>hyphenate</code>.)</p>
</li>
<li><p><code>rgba()</code> and <code>hsla()</code> should not exist, <code>rgb()</code> and <code>hsl()</code>  should have gotten an optional fourth parameter instead (and the alpha value should have used the same format as R, G, and B or S and L).</p>
</li>
<li><p> Descendant combinator should have been <code>»</code> and indirect sibling combinator should have been <code>++</code>, so there's some logical relationships among the selectors' ascii art</p>
</li>
<li><p> The <code>*-blend-mode</code> properties should've just been <code>*-blend</code></p>
</li>
<li><p> The syntax of unicode ranges should have consistent with the rest of <abbr title="Cascading Style Sheets">CSS</abbr>, like <code>u0001-u00c8</code>.</p>
</li>
<li><p> Unicode ranges should not have had a separate microsyntax with their own tokenization or token handling.  The tokenization hacks required either to make selectors (e.g., u+a) handle things that are unicode-range tokens, or make unicode-range handle the other huge range of tokens (numbers and dimensions with and without scientific notation, identifiers, +) are both horrible.</p>
</li>
<li><p><code>font-family</code> should have required the font name to be quoted (like all other values that come from “outside” <abbr title="Cascading Style Sheets">CSS</abbr>).  The rules for handling unquoted font names make parsing <code>font</code> stupid, as it requires a <code>font-size</code> value for disambiguation.</p>
</li>
<li><p> Flexbox should have been less crazy about <code>flex-basis</code> vs <code>width</code>/<code>height</code>.  Perhaps: if <code>width</code>/<code>height</code> is <code>auto</code>, use <code>flex-basis</code>; otherwise, stick with <code>width</code>/<code>height</code> as an inflexible size.  (This also makes min/max width/height behavior fall out of the generic definition.)</p>
</li>
<li><p><del><code>:empty</code> should have been <code>:void</code>, and <code>:empty</code> should select items that contain only white space</del> FIXED in the <abbr title="specification">spec</abbr>, waiting for implementations to check for Web-compat…</p>
</li>
<li><p><code>table-layout: fixed; width: auto</code> should result in a fill-available table with fixed-layout columns.</p>
</li>
<li><p><code>text-orientation</code> should have had <code>upright</code> as the initial value (given the latest changes to 'writing-mode').</p>
</li>
<li><p> The <code>@import</code> rule is required to (a) always hit the network unless you specify cache headers, and (b) construct fresh CSSStyleSheet objects for every import, even if they're identical. It should have had more aggressive <abbr title="Uniform Resource Locator">URL</abbr>-based deduping and allowed sharing of stylesheet objects.</p>
</li>
<li><p> Selectors have terrible future-proofing. We should have split on top-level commas, and only ignored unknown/invalid segments, not the entire thing.</p>
</li>
<li><p><code>:link</code> should have had the <code>:any-link</code> semantics all along.</p>
</li>
<li><p> The <code>flex</code> shorthand (and <code>flex-shrink</code> and <code>flex-grow</code> longhands) should accept <code>fr</code> units instead of bare numbers to represent flex fractions.</p>
</li>
<li><p> The <code>display</code> property should be called <code>display-type</code>.</p>
</li>
<li><p> The <code>list-style</code> properties should be called <code>marker-style</code>, and <code>list-item</code> renamed to <code>marked-block</code> or something.</p>
</li>
<li><p> The <code>text-overflow</code> property should always apply, not be dependent on <code>overflow</code></p>
</li>
<li><p><code>line-height: &lt;percentage&gt;</code> should compute to the equivalent <code>line-height: &lt;number&gt;</code>, so that it effectively inherits as a percentage not a length</p>
</li>
<li><p><code>::placeholder</code> should be <code>::placeholder-text</code> and <code>:placeholder-shown</code> should be <code>:placeholder</code></p>
</li>
<li><p><code>overflow: scroll</code> should introduce a stacking context</p>
</li>
<li><p><code>size</code> should have been a shorthand for <code>width</code> and <code>height</code> instead of an <code>@page</code> property with a different definition</p>
</li>
<li><p> We probably should have avoided mixing keywords (<code>span</code>) with idents in the <a href="https://github.com/w3c/csswg-drafts/issues/1137" title="https://github.com/w3c/csswg-drafts/issues/1137" rel="ugc nofollow">grid properties</a>, possibly by using functional notation (like <code>span(2)</code>).</p>
</li>
<li><p> Comments shouldn't have been allowed basically everywhere in <abbr title="Cascading Style Sheets">CSS</abbr> (compare to <abbr title="HyperText Markup Language">HTML</abbr>, which basically only allows them where content goes), because it makes them basically unrepresentable in the object model, which in turn makes building editing directly on top of the object model impossible</p>
</li>
<li><p> The alignment properties in Flexbox should have been writing-mode relative, not flex-flow relative, and thus could have reasonably understandable names like <code>align-inline-*</code> and <code>align-block-*</code>.</p>
</li>
<li><p><code>shape-outside</code> should have had <code>wrap-</code> in the name somehow, as people assume the shape should also clip the content as in <code>clip-path</code>.</p>
</li>
<li><p> It shouldn't be <code>!important</code> —&nbsp;that reads to engineers as “not important”. We should have picked another way to write this.</p>
</li>
</ul>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vibe coding is mad depressing (233 pts)]]></title>
            <link>https://law.gmnz.xyz/vibe-coding-is-mad-depressing/</link>
            <guid>46227422</guid>
            <pubDate>Thu, 11 Dec 2025 03:50:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://law.gmnz.xyz/vibe-coding-is-mad-depressing/">https://law.gmnz.xyz/vibe-coding-is-mad-depressing/</a>, See on <a href="https://news.ycombinator.com/item?id=46227422">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    

    
        

        <p>
            <i>
                <time datetime="2025-12-11T03:28Z">
    11 Dec, 2025
</time>
            </i>
        </p>
    

    <p>I’ve been in the mobile development industry for almost 15 years, and this AI/LLM era might be the worst.</p>
<p>My work are mostly freelance, gigs, hourly, milestones, and I could say 90% of my experience are greenfield projects. I don’t have any apps on my own, I make a living coding apps for others.</p>
<h3 id="before-ai">Before AI</h3><p>Back in the day, during a client kickoff they usually hand me a document with a UI prototype and a list of features. Then, you start from scratch, File - New Project. <code>git init</code> that shit and you’re on your way.</p>
<p>Everything was calm, clients just wanted a weekly or monthly feedback because they know how hard mobile development is you know. No pressure. You can focus on the great work, clean code, proper variable naming, proper git commit, all that stuffs.</p>
<p>In 2-3 months you get an alpha or beta build out, and clients are very happy. They can’t believe there idea, has now transformed into something they can play with.</p>
<h3 id="start-of-ai-era">Start of AI era</h3><p>Fast forward to today, or maybe it started around 2-3 years ago. Nothing wrong with it at first. Like any freelancer, I try to adopt with the latest trends.</p>
<p>At first it was just code snippets.</p>
<blockquote>
<p>Hey! I asked AI for this code, do you think this will work? I think you should use it.</p>
</blockquote>
<p>Okay, so this non-technical person is sending me codes now.</p>
<p>I mostly reply with</p>
<blockquote>
<p>It’s alright I got some working code blocks that worked in production perfectly fine. Thanks though!</p>
</blockquote>
<p>But, then this code snippets get larger and larger as time goes on. I'm thankful for this suggestions of course. But, it's just additional work when you're coding and you get this AI source code and then you have to think on how to merge this code with a different coding style and variable names into your codebase.</p>
<h3 id="vibe-coding-era">Vibe Coding era</h3><p>The first clues started when a client, who I thought was a software developer, starts merging his own code through the <code>main</code> branch, without warning. No pull request, just straight <code>git push --force origin main</code>.</p>
<p>As I started to checkout what the code was about, I started seeing this emojis inside the <code>print()</code> statements. I thought, this is so odd and unprofessional.</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/lawgimenez/17am.webp" alt="Screenshot 2025-12-11 at 10"></p>
<p>I tried to Google search the macOS shortcut for emojis, to match this person's vibe. This fella must really like emojis you know. It turns out, AI code has a lot of emojis along with it.</p>
<p>The other sign was how the branching, and merging works with AI. And maybe feature request? I really don't know. For example, one vibe coded project has 1,227 branches and counting. I haven't merged one yet, I let the client deal with that.</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/lawgimenez/41am.webp" alt="Screenshot 2025-12-11 at 10"></p>
<p>Last time, I checked this Xcode project did not compiled. Or anything close to it.</p>
<p>And the last thing that made me snapped was, all this vibed source code were located inside one file <code>ContentView</code>. To anyone who's not familiar, <code>ContentView</code> is the first SwiftUI file created when you start a new Xcode project.</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/lawgimenez/20pm.webp" alt="Screenshot 2025-12-09 at 12"></p>
<p>All the UI logic, view models, model are located inside that file. Worst part, this is currently live in the App Store.</p>
<h3 id="conclusion">Conclusion</h3><p>I totally get it, everyone has to make a living. Creating an app is one of them. I just feel sad with how AI has bastardized my profession, which I worked hard for the last 15 years. There is no best practices anymore, no proper process, no meaningful back and forth. Just dealing with thousands and thousands of lines of code at every project kickoff.</p>


    

    
        

        
            


        

        
            
        
    


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Patterns.dev (538 pts)]]></title>
            <link>https://www.patterns.dev/</link>
            <guid>46226483</guid>
            <pubDate>Thu, 11 Dec 2025 01:18:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.patterns.dev/">https://www.patterns.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=46226483">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <h3>
            We offer a modern perspective on patterns
          </h3>
          <p>
            A common critique of design patterns is that they needlessly add
            complexity.
          </p>
          <p>
            Our perspective is that patterns are valuable for solving specific
            problems, often helping to 
            <strong>communicate comminalities in code problems</strong> for humans.
            If a project doesn't have those problems, there isn't a need
            to apply them. Patterns can also be very language or framework-specific
            (e.g. React), which can often mean thinking beyond the scope of just
            the original GoF design patterns.
          </p>
        </div><div>
          <h3>
            We help you scale your webapps for performance
          </h3>
          <p>
            Learn about web performance patterns for loading your code more
            efficiently. Unsure how to think about modern approaches to loading
            or rendering user-experiences? We've got you covered.
          </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When would you ever want bubblesort? (2023) (106 pts)]]></title>
            <link>https://buttondown.com/hillelwayne/archive/when-would-you-ever-want-bubblesort/</link>
            <guid>46224311</guid>
            <pubDate>Wed, 10 Dec 2025 21:45:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://buttondown.com/hillelwayne/archive/when-would-you-ever-want-bubblesort/">https://buttondown.com/hillelwayne/archive/when-would-you-ever-want-bubblesort/</a>, See on <a href="https://news.ycombinator.com/item?id=46224311">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
        
            <p>There are very few universal rules in software engineering, but there are are a lot of <em>near</em>-universal principles. Things like "prefer composition to inheritance" is near-universal. I love finding the rare situations where these principles don't hold, like where you do want <a href="https://buttondown.email/hillelwayne/archive/when-to-prefer-inheritance-to-composition/" target="_blank">inheritance over composition</a>. A similar near-universal principle is "don't use <a href="https://en.wikipedia.org/wiki/Bubble_sort" target="_blank">bubblesort</a>". Some would even say it's a universal rule, with Donald Knuth writing "bubble sort seems to have nothing to recommend it, except a catchy name and the fact that it leads to some interesting theoretical problems".<sup id="fnref:cite"><a href="#fn:cite">1</a></sup> But Knuth's <a href="https://en.wikipedia.org/wiki/Knuth_reward_check" target="_blank">been wrong before</a>, so let's see if this universal rule is only <em>near</em>-universal.</p>
<p>Theoretically, bubblesort is faster than quick or mergesort for small arrays. This makes it useful as part of a larger sorting strategy: most of the fast-in-principle sorting algorithms work by recursively sorting subpartitions of an array, ie if you apply quicksort to 2^20 random integers, at some point you're sorting 2^17 8-integer subpartitions. Switching over to bubblesort for those subpartitions would be a nice optimization. </p>
<p>Many production sorting algorithms do use a hybrid approach, but they overwhelmingly use <a href="https://en.wikipedia.org/wiki/Insertion_sort" target="_blank">insertion sort</a> instead. Insertion sort is very fast for small arrays and it's also <a href="https://nicknash.me/2012/10/12/knuths-wisdom/" target="_blank">better at using the hardware</a>. On some very particular hardwares bubblesort stills ends up better, like in this <a href="http://www.sci.utah.edu/~wald/Publications/2019/rtgems/ParticleSplatting.pdf" target="_blank">NVIDIA study</a>, but you probably don't have that hardware.</p>
<p>So that's one use-case, albeit one still dominated by a different algorithm. It's interesting that NVIDIA used it here because gamedev has a situation that's uniquely useful to bubblesort, based on two of its properties:</p>
<ol>
<li>While the algorithm is very slow overall, each individual step is very fast and easily suspendable.</li>
<li>Each swap leaves the array more ordered than it was before. Other sorts can move values <em>away</em> from their final positions in intermediate stages.</li>
</ol>
<p>This makes it really good when you want to do a fixed amount of sorting work per frame. Say you have a bunch of objects on a screen, where some objects can occlude others. You want to render the objects closest to the camera <em>first</em> because then you can determine which objects it hides, and then save time rendering those objects. There's no correctness cost for rendering objects out of order, just a potential performance cost. So while your array doesn't <em>need</em> to be ordered, the more ordered it is the happier you are. But you also can't spend too much time running a sorting algorithm, because you have a pretty strict realtime constraint. Bubble sort <a href="https://discussions.unity.com/t/depth-sorting-of-billboard-particles-how-can-i-do-it/5053" target="_blank">works pretty well here</a>. You can run it a little bit of a time at each frame and get a better ordering than when you started.</p>
<p>That reminds me of one last use-case I've heard, apocryphally. Let's say you have a random collection of randomly-colored particles, and you want to animate them sorting into a rainbow spectrum. If you make each frame of the animation one pass of bubblesort, the particles will all move smoothly into the right positions. I couldn't find any examples in the wild, so with the help of GPT4 I hammered out a crappy visualization. Code is <a href="https://gist.github.com/hwayne/85488f755066d8aa57cd147875e97b72" target="_blank">here</a>, put it <a href="https://editor.p5js.org/" target="_blank">here</a>.</p>
<p>(After doing that I suspect this isn't actually done in practice, in favor of running a better sort to calculate each particles final displacement and then animating each particles moving directly, instead of waiting to move for each bubblesort pass. I haven't mocked out an example but I think that'd look a lot smoother.)</p>
<p>So there you go, three niche use cases for bubblesort. You'll probably never need it.</p>
<hr>
<h3>New Quanta Article!</h3>
<p>Okay so I didn't actually write this one, but I played a role in it happening! A while back a friend visited, and we were chatting about his job at quanta. At the time he was working on this <a href="https://www.quantamagazine.org/complexity-theorys-50-year-journey-to-the-limits-of-knowledge-20230817/" target="_blank">mammoth article on metacomplexity theory</a>, so naturally the topic of <a href="https://buttondown.email/hillelwayne/archive/problems-harder-than-np-complete/" target="_blank">problems harder than NP-complete</a> came up and I recommend he check out Petri net reachability. So he did, and then he wrote <a href="https://www.quantamagazine.org/an-easy-sounding-problem-yields-numbers-too-big-for-our-universe-20231204/" target="_blank">An Easy-Sounding Problem Yields Numbers Too Big for Our Universe</a>. Gosh this is so exciting! </p>

        
    
</div></div>]]></description>
        </item>
    </channel>
</rss>