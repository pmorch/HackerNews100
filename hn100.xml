<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 08 Apr 2025 21:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Obituary for Cyc (102 pts)]]></title>
            <link>https://yuxi-liu-wired.github.io/essays/posts/cyc/</link>
            <guid>43625474</guid>
            <pubDate>Tue, 08 Apr 2025 19:13:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://yuxi-liu-wired.github.io/essays/posts/cyc/">https://yuxi-liu-wired.github.io/essays/posts/cyc/</a>, See on <a href="https://news.ycombinator.com/item?id=43625474">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
  

<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">





<section id="abstract">
<h2 data-anchor-id="abstract">Abstract</h2>
<p>The legendary Cyc project, Douglas Lenat’s 40-year quest to build artificial general intelligence by scaling symbolic logic, has failed. Based on extensive archival research, this essay brings to light its secret history so that it may be widely known.</p>
<p>Lenat’s journey began with his PhD work on automated mathematical discovery through heuristic search. He observed that such systems initially make promising discoveries but quickly “run out of steam” as they exhaust their initial pool of heuristic rules. His follow-up system EURISKO, famous for winning tournament competitions by finding unconventional tactics, faced similar limitations. These experiences convinced Lenat that true AI needed a vast foundation of common sense knowledge to avoid intellectual exhaustion.</p>
<p>In 1985, he launched Cyc to manually encode millions of facts and rules about common sense, predicting that once this “knowledge pump” was primed, the system would begin true machine learning by reading natural language texts and conducting autonomous scientific experiments. Cyc grew to contain approximately 30 million assertions at a cost of $200 million and 2,000 person-years. Yet despite Lenat’s repeated predictions of imminent breakthrough, it never came.</p>
<p>In terms of funding, Cycorp was probably half-funded by the military and intelligence before 2010, and entirely funded by commercial applications since 2016. Cycorp achieved long-term financial stability that is uncommon for a small technology company, but all known commercial uses of its system involve standard methods in expert systems, data integration, and information retrieval, functionally the same as similar services offered by established corporations like Oracle and IBM. No evidence suggests that Cyc’s purported higher intelligence provided any competitive advantage.</p>
<p>By academic standards, the Cyc project is highly insular. Publications involving Cyc typically described methods for entering information <em>into</em> the system, rarely addressing applications <em>out of</em> it. Outside Cycorp, Cyc saw minimal use in AI research or even in knowledge retrieval, its most adjacent field. Academics found the system difficult to use, and it never performed on public benchmarks. Spin-off projects like OpenCyc and various semantic web initiatives all eventually shut down without notable success.</p>
<p>The secretive nature of Cyc has multiple causes. Lenat personally did not release the source code of his PhD project or EURISKO, remained unimpressed with open source, and disliked academia as much as academia disliked him. Most open information concerning Cyc had been deliberately removed circa 2015, at the moment when Cycorp pivoted to commercial applications.</p>
<p>Lenat had a single philosophical vision for AI that he pursued for 40 years. Guided by it, he had consistently rejected every alternative vision for AI, including the heuristic search approach, the expert systems approach, the robotics approach, and the neural network approach. All were rejected as either “shallow pattern-matching” seeking a “free lunch” on one end, or a “mystical worship of physical embodiment” on the other end.</p>
<p>As of 2025, 9 years after the knowledge pump had been primed, there is still no sign that Cyc would ever achieve general intelligence. The long slow failure of Lenat’s project is a strong indictment against the symbolic-logical approach to AI.</p>
<p>More archival material available on <a href="https://github.com/yuxi-liu-wired/cyc-archive/tree/main">GitHub</a>.</p>
</section>
<section id="in-lieu-of-an-introduction">
<h2 data-anchor-id="in-lieu-of-an-introduction">In lieu of an introduction</h2>
<p>Many years later, surrounded by the humming servers of the knowledge base, Douglas Lenat was to remember that distant afternoon when he taught Cyc that everyone can only see their own dream.</p>
</section>
<section id="automated-mathematician">
<h2 data-anchor-id="automated-mathematician">Automated Mathematician</h2>
<p>In the land of AI, there had been legends, of people and systems that were before their time, that showed sparks of brilliance but without followups, and Douglas Lenat was a man of three.</p>
<p>Lenat’s first legend was his Automated Mathematician (AM), a system that, according to the legends, discovered mathematical concepts autonomously by distilling concepts out of patterns and remixing and stirring together previous concepts <span data-cites="lenatAMArtificialIntelligence1976">(<a href="#ref-lenatAMArtificialIntelligence1976" role="doc-biblioref">Lenat 1976</a>)</span>. As we will see, this legend is mostly true, though with a caveat that leads to the topic of his second legend.</p>
<p>AM was his 1976 PhD thesis project, and it was one of many “automated discovery” systems in the 1970s – the only one still remembered nowadays. Though neural networks and other “self-organized” machine learning methods had mostly <a href="https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/">died out in the 1960s</a>, machine learning did not die, and indeed, was going through a logical spring, under the banner of “automated discovery”.</p>
<section id="automated-discovery">
<h3 data-anchor-id="automated-discovery">Automated discovery</h3>
<p>Nowadays, when we think of “machine learning”, we think of random forests, neural networks, and such. But back then, machine learning was understood as “logical AI gone meta”. Well, to go meta, we must first understand: What <em>is</em> logical AI? Simon and Newell, the two giants of logical AI, had shown the way: logical AI is heuristic search.</p>
<ul>
<li>To play chess, one simply writes a program that does <a href="https://en.wikipedia.org/wiki/Alpha-beta_pruning">alpha-beta search</a> over the game tree, guided by heuristics of piece-worth, mobility, and such.</li>
<li>To solve planar geometry problems, one simply writes a program that constructs a path in the space of geometric arguments, until one connects the axioms with the conclusions.</li>
<li>To solve problems of type X, one simply writes a program that walks through the space of possible solutions to X, and code in the heuristic rules, its north star, so that it will not be lost in the space of solutions.</li>
</ul>
<p>Well, chess is a game, and so is Euclidean geometry, symbolic integration, and… perhaps scientific discovery itself? The famed scientific method, to deserve the name “method”, ought to be just one more problem for logical AI to solve. It merely remained to go meta, to heuristically search over the space of heuristic searches.</p>
<p>During 1977–1983, <a href="http://www.isle.org/~langley/">Pat Langley</a> wrote a series of programs named BACON, after <a href="https://en.wikipedia.org/wiki/Francis_Bacon">Francis Bacon</a>, a father of scientific empiricism. It purported to discover scientific laws from mere data. For example, when given a table of the moles <span>\(N\)</span>, pressures <span>\(P\)</span>, volumes <span>\(V\)</span>, and temperatures <span>\(T\)</span> of gas samples, BACON would first try out simple equations involving two of the terms. It would discover that the data for <span>\(PV\)</span> appears more “clusterly” than either <span>\(P\)</span> or <span>\(V\)</span>, so it would make a new quantity <span>\(PV\)</span>, and add that to the table. It would then repeat this process, discovering that <span>\(PV/T\)</span> is an interesting quantity, and finally that <span>\(PV/NT\)</span> is a <em>constant</em> quantity – the <a href="https://en.wikipedia.org/wiki/Ideal_gas_law">ideal gas law</a> discovered! <span data-cites="langleyDataDrivenDiscoveryPhysical1981">(<a href="#ref-langleyDataDrivenDiscoveryPhysical1981" role="doc-biblioref">Langley 1981</a>)</span></p>
<p>In the logical AI framework, the problem space of BACON is the space of all elementary functions involving the table columns, and a <em>solution</em> is a (nontrivial) function that results in a nearly constant column. At each step, the program tries out simple combinations of the current columns, and heuristically pick the one most nearly constant.</p>
<p>As another example, consider the famed <a href="https://en.wikipedia.org/wiki/Dendral">Dendral and Meta-Dendral</a>.</p>
<p>Dendral was an artificial model of how professional chemists read out a molecular structure from its atomic spectroscopic data. Its problem space is the space of all possible ways to cut up a molecule into fragments, and ways for the fragments to give and take their atoms. Its heuristics are the rules for generating chemically plausible and implausible cleavages and transfers. For example, it is plausible for a protein to be cleaved at the <code>-CO-*-NH-</code> peptide bond, but implausible to be cleaved at the double bond between <code>C</code> and <code>O</code>. The goal is, given molecular spectroscopic data for a single molecule, to construct a molecular structure and a sequence of cleavages and transfers, such that it would produce the data.</p>
<p>With Meta-Dendral, the problem space goes meta, becoming the space of possible chemical <em>rules</em>. Its goal is to find plausible rules (plausible according to heuristic meta-rules) that can explain a large collection of molecular structures and their spectroscopic data. Meta-Dendral proved useful for working chemists by discovering some cleavage rules for a family of <a href="https://en.wikipedia.org/wiki/Steroid">steroids</a>. For details, see my essay on <a href="https://yuxi-liu-wired.github.io/essays/posts/expert-systems/">expert systems</a>.</p>
<p>In general, such a system begins with some simple rules that allow the system to a low score according to some criteria. and as it runs, it builds, prunes, and modifies the rules, so that in the end, its rule set allows it to achieve a high score. The following tabulates a few <span data-cites="walkerHowFeasibleAutomated1987">(<a href="#ref-walkerHowFeasibleAutomated1987" role="doc-biblioref">Walker 1987</a>)</span>:</p>
<table>
<colgroup>
<col>
<col>
<col>
<col>
<col>
<col>
</colgroup>
<thead>
<tr>
<th>System Name</th>
<th>Date</th>
<th>Task</th>
<th>Data</th>
<th>Rules</th>
<th>Discovery Method</th>
</tr>
</thead>
<tbody>
<tr>
<td>Meta-Dendral</td>
<td>1976</td>
<td>Discover molecular cleavage and transfer rules for mass spectrometry</td>
<td>Molecular structures and their spectra</td>
<td>Molecular cleavage and transfer rules</td>
<td>Use meta-heuristic rules to generate possible rules, and test on data</td>
</tr>
<tr>
<td>Bacon</td>
<td>1977–1983</td>
<td>Discover physical laws</td>
<td>Numeric data from experiments</td>
<td>Elementary functions</td>
<td>Symbolic regression</td>
</tr>
<tr>
<td>RX</td>
<td>1982</td>
<td>Discover drug side effects and interactions</td>
<td>Patient information database</td>
<td>Drug effect and interaction rules</td>
<td>Symbolic regression with time-lag</td>
</tr>
</tbody>
</table>
</section>
<section id="the-working-of-am">
<h3 data-anchor-id="the-working-of-am">The working of AM</h3>
<blockquote>
<p>The honor of your machine is preserved.</p>
<p>— Paul Erdős, after examining <span data-cites="lenatAMArtificialIntelligence1976">(<a href="#ref-lenatAMArtificialIntelligence1976" role="doc-biblioref">Lenat 1976</a>, appendix 4)</span>.</p>
</blockquote>
<p>Within this context, AM is different. It is still a logical AI with a problem space and a heuristic search. However, there is no data: It was mostly “self-play”.</p>
<p>To start AM, Lenat began by entering 115 concepts in set theory and ~250 heuristic rules, and thence AM ran, discovering more and more constructions. Most would be trivial, but a few would be interesting, and these interesting constructions would be stored, allowing further constructions upon them. It secured a place as a minor legend, reportedly rediscovering many concepts, such as the natural numbers, the prime numbers, and the Goldbach conjecture.</p>
<p>A <strong>concept</strong> is essentially a <a href="https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)">frame</a>, which are essentially objects in <a href="https://en.wikipedia.org/wiki/Object-oriented_programming">object-oriented programming</a>. A concept has 25 possible <strong>facets</strong>, which are “slots”, or “data fields”. Not all need to be filled.</p>
<div id="callout-1" title="An example concept">
<p>The following is an example concept with many facets populated <span data-cites="lenatRoleHeuristicsLearning1983">(<a href="#ref-lenatRoleHeuristicsLearning1983" role="doc-biblioref">Lenat 1983c</a>, table 9.9)</span>:</p>
<ul>
<li><code>NAME</code>: <code>Generalize-rare-predicate</code></li>
<li><code>ABBREVIATION</code>: <code>GRP</code></li>
<li><code>STATEMENT</code>:
<ul>
<li><code>English</code>: If a predicate is rarely true, Then create generalizations of it</li>
<li><code>IF-potentially-relevant</code>:</li>
<li><code>IF-just-finished-a-task-dealing-with</code>: a predicate <code>P</code></li>
<li><code>IF-about-to-work-on-task-dealing-with</code>: an agenda <code>A</code></li>
<li><code>IF-in-the-middle-of-a-task-dealing-with</code>: <em>never</em></li>
<li><code>IF-truly-relevant</code>: <code>P</code> returns True less than <code>5%</code> of Average Predicate</li>
<li><code>IF-resources-available</code>: at least <code>10</code> CPU seconds, at least <code>300</code> cells</li>
<li><code>THEN-add-task-to-agenda</code>: Fill in entries for <code>Generalizations</code> slot of <code>P</code></li>
<li><code>THEN-conjecture</code>:
<ul>
<li><code>P</code> is less interesting than expected</li>
<li><code>Generalizations</code> of <code>P</code> may be better than <code>P</code></li>
<li><code>Specializations</code> of <code>P</code> may be very bad</li>
</ul></li>
<li><code>THEN-modify-slots</code>:
<ul>
<li>Reduce Worth of <code>P</code> by <code>10%</code></li>
<li>Reduce Worth of <code>Specializations(P)</code> by <code>50%</code></li>
<li>Increase Worth of <code>Generalizations(P)</code> by <code>20%</code></li>
</ul></li>
<li><code>THEN-print-to-user</code>: <code>English(GRP)</code> with “a predicate” replaced by <code>P</code></li>
<li><code>THEN-define-new-concepts</code>:</li>
</ul></li>
<li><code>CODED-IF-PART</code>: <code>λ(P) ... &lt;LISP function conjoining all the IF- parts&gt;</code></li>
<li><code>CODED-THEN-PART</code>: <code>λ(P) ... &lt;LISP function appending all the THEN- parts&gt;</code></li>
<li><code>CODED-IF-THEN-PARTS</code>: <code>λ(P) ... &lt;LISP function combining the previous 2 slots&gt;</code></li>
<li><code>COMPILED-CODED-IF-THEN-PARTS</code>: <code>#30875</code></li>
<li><code>SPECIALIZATIONS</code>: <code>Generalize-rare-set-predicate</code>
<ul>
<li><code>Boundary-Specializations</code>: <code>Enlarge-domain-of-predicate</code></li>
</ul></li>
<li><code>GENERALIZATIONS</code>: <code>Modify-predicate</code>, <code>Generalize-concept</code>
<ul>
<li><code>Immediate-Generalizations</code>: <code>Generalize-rare-contingent-piece-of-knowledge</code></li>
<li><code>Siblings</code>: <code>Generalize-rare-heuristic</code></li>
</ul></li>
<li><code>IS-A</code>: <code>Heuristic</code></li>
<li><code>EXAMPLES</code>:
<ul>
<li><code>Good-Examples</code>: Generalize <code>Set-Equality</code> into <code>Same-Length</code></li>
<li><code>Bad-Examples</code>: Generalize <code>Set-Equality</code> into <code>Same-First-Element</code></li>
</ul></li>
<li><code>CONJECTURES</code>: Special cases of this are more powerful than <code>Generalizations</code>
<ul>
<li><code>Good-Conjec-Units</code>: <code>Specialize</code>, <code>Generalize</code></li>
</ul></li>
<li><code>ANALOGIES</code>: <code>Weaken-overconstrained-problem</code></li>
<li><code>WORTH</code>: <code>600</code></li>
<li><code>VIEW</code>: <code>Enlarge-structure</code></li>
<li><code>ORIGIN</code>: Specialization of <code>Modify-predicate</code> via empirical induction
<ul>
<li><code>Defined-using</code>: <code>Specialize</code></li>
<li><code>Creation-date</code>: <code>6/1/78 11:30</code></li>
</ul></li>
<li><code>HISTORY</code>:
<ul>
<li><code>N-Good-Examples</code>: <code>1</code>, <code>N-Bad-Examples</code>: <code>1</code></li>
<li><code>N-Good-Conjectures</code>: <code>3</code>, <code>N-Bad-Conjectures</code>: <code>1</code></li>
<li><code>N-Good-Tasks-Added</code>: <code>2</code>, <code>N-Bad-Tasks-Added</code>: <code>0</code></li>
<li><code>Avg-Cpu-Time</code>: <code>9.4</code> seconds, <code>Avg-List-Cells</code>: <code>200</code></li>
</ul></li>
</ul>
</div>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/AM_ontology.png"></p>
<figcaption>The network of concepts at the beginning of AM. <code>|</code> means “is a”. <code>|||</code> means “is an example of”. <span data-cites="lenatAMArtificialIntelligence1976">(<a href="#ref-lenatAMArtificialIntelligence1976" role="doc-biblioref">Lenat 1976, 106</a>)</span></figcaption>
</figure>
</div>
<p>AM has an <strong>agenda</strong>: a list of <strong>tasks</strong>, each with a list of <strong>reasons</strong> for the task. Each task is of the form “Perform operation <code>O</code> to facet <code>F</code> of concept <code>C</code>”. A task’s <code>Worth</code> is the sum of its reasons’ worths. AM always performs the task with the highest worth.</p>
<p>To perform a task, AM looks for <strong>heuristic rules</strong> whose conditions are (mostly) satisfied, and whose worth is (pretty) high. Each heuristic rule is of form “if <code>&lt;condition&gt;</code>, then run <code>&lt;actions&gt;</code>”. Each action has 3 kinds of possible effects:</p>
<ul>
<li>Add a new task to agenda.</li>
<li>Create a new concept.</li>
<li>Add or delete an entry to a facet of a concept.</li>
</ul>
<p>Some example heuristic rules:</p>
<ul>
<li>If the task is to fill in examples of X, and X is a special case of Y, then for each example of Y, check if it a definition of X. If so, then add it to the list of examples of X.</li>
<li>If some but not most examples of X are also examples of Y, then create a new concept “X and Y”.</li>
<li>If very few examples of X are found, then add the following task to the agenda: “Generalize the concept X”, for the following reason: “X are quite rare; a slightly less restrictive concept might be more interesting”.</li>
</ul>
<p>At this point, the careful reader would notice several problems:</p>
<p>How does AM know that the concept should be called “Prime Numbers”? Ah, that’s because Lenat would regularly interrupt and inspect AM, and if Lenat notices that AM has rediscovered, say, prime numbers, he would rename that from something like <code>concept-421</code> to <code>prime-numbers</code>.</p>
<p>To discover prime numbers, AM must have a way to check if a Lisp object is a prime number or not. That is, the definition must also be a program. So…? Ah, the famous <a href="https://en.wikipedia.org/wiki/Homoiconicity">homoiconicity</a> of Lisp came to the rescue! A definition, as stored within a facet of a concept, is a data, but for Lisp, data is program, and program data. Consequently, AM can run a subroutine that enumerates possible programs <em>as data</em>, and for each, interpret it <em>as program</em>, until AM hits upon a program that works (or times out).</p>
<p>How does it check that two definitions actually define the same thing? In general, this is impossible by <a href="https://en.wikipedia.org/wiki/Rice's_theorem">Rice’s theorem</a>, so Lenat must have used some heuristic rules. I looked, but can’t find Lenat explaining this anywhere. It seemed like a trick of the hand.</p>
<p>But most serious of all issues is that the most critical part of AM was not the concepts it discovered – after all, mathematicians did not need a computer to inform them that prime numbers are interesting. The most critical part was surely the heuristic rules by which AM worked, and many were entirely hand-waved. The most detailed description was in <span data-cites="lenatAMArtificialIntelligence1976">(<a href="#ref-lenatAMArtificialIntelligence1976" role="doc-biblioref">Lenat 1976</a>, appendix 3)</span>, and it is still not described to a level of detail that may allow reimplementation.</p>
<p>Consider rule 75: “A constructive existence conjecture is interesting if it is frequently used.” How frequent is “frequent”? What threshold of frequency triggers the increase in interestingness, and by how much? There are even vaguer rules, such as rule 69: “Formulate a parameterized conjecture, a ‘template’, which gets slowly specialized or instantiated into a definite conjecture.”.</p>
<p>Now, these would have not been a problem if AM was just a forgotten system, but it was not. The achievements of AM was impressive enough that it was an instant celebrity among the AI people, winning an <a href="https://en.wikipedia.org/wiki/IJCAI_Computers_and_Thought_Award">IJCAI Award</a> just 1 year after publication <span data-cites="lenatUbiquityDiscovery1977">(<a href="#ref-lenatUbiquityDiscovery1977" role="doc-biblioref">Lenat 1977</a>)</span>. Anecdotes even suggested that the AI mathematician was here:<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div><p><sup>1</sup>&nbsp;This anecdote corroborates <span data-cites="lenatAMArtificialIntelligence1976">(<a href="#ref-lenatAMArtificialIntelligence1976" role="doc-biblioref">Lenat 1976</a>, appendix 4.6)</span>.</p></div><blockquote>
<p>On one memorable occasion, one of my advisors, George Polya, was looking at its results, and remarked “That reminds me of something a student of a friend of mine once did.” He rummaged through an old trunk, found the relevant correspondence, and it turned out that his friend was G. H. Hardy, and the student was Srinivasa Ramanujan! Even though that regularity (involving highly composite numbers) has no practical significance, Polya and I were happy to see AM behaving much like the young self-taught Indian genius had, in his explorations in search for interesting regularities.</p>
<p><span data-cites="lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">Lenat 2022a</a>)</span></p>
</blockquote>
</section>
<section id="the-end-of-am">
<h3 data-anchor-id="the-end-of-am">The end of AM</h3>
<p>The swirling controversy came to a head with the publication of <span data-cites="ritchieAMCaseStudy1984">(<a href="#ref-ritchieAMCaseStudy1984" role="doc-biblioref">Ritchie and Hanna 1984</a>)</span>, which argued that AM was badly documented. That, its control structure probably was not “simply pick the task with the highest worth”, but more complicated. This called into question the Lenat claim that the heuristic rules were the load-bearing parts of AM, which Lenat had implied by emphasizing its almost trivially simple control structure. Further, some crucial <em>quantitative</em> heuristic rules were probably hidden behind vague <em>qualitative</em> handwaves like “A nonconstructive existence conjecture is interesting” (rule 74). Now, handwaving would have not been a problem if they were intended as merely glosses over the source code, but the source code was also unpublished, making it impossible for other other researchers to reproduce or extend the work, or to reinterpret AM’s workings.</p>
<p>In short, because of the various issues, AM was an event that happened, but not an entity that could be built upon. One could not build upon it directly in source code, since it’s unavailable. One could not build upon it by reimplementing the pseudocode, since the rules were vaguely specified, and the control structure was probably wrong. One could not build upon it by reimplementing the high-level ideas, since it’s unclear which part, out of the several dozen tightly integrated parts of AM, was responsible for AM’s good outputs, and which were just implementation details. In our language, there was no ablation study.</p>
<p>Lenat quickly replied with <span data-cites="lenatWhyAmEurisko1984">(<a href="#ref-lenatWhyAmEurisko1984" role="doc-biblioref">Lenat and Brown 1984</a>)</span>. He dismissed the criticism as mostly miscommunication, and went on to describe the <em>real</em> lesson of AM. If I were to be dab, Lenat was saying that it is fine for AM’s source,<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> or even pseudocode, to be unavailable, because Lenat had learned the lessons, and you, dear reader, need only listen to the lessons from him.</p>
<div><p><sup>2</sup>&nbsp;Though Lenat admitted that “the code ought to have been provided” <span data-cites="lenatWhyAmEurisko1984">(<a href="#ref-lenatWhyAmEurisko1984" role="doc-biblioref">Lenat and Brown 1984</a>)</span> for AM, he would never publish the code, not with AM, nor with EURISKO. He had often claimed it had been long lost, yet the source code for AM and EURISKO had <a href="https://white-flame.com/am-eurisko.html">recently been found</a>, right where it should be – the <a href="https://www.saildart.org/DBL">DBL folder in the Stanford AI Laboratory backup data</a>. It could only be found because Lenat had died, thus releasing the password protection on the folder. I wonder if Lenat had lied, and simply wished to protect his source code. It would correlate with his later behavior.</p></div><blockquote>
<p>The AM thesis never explained, precisely, how concepts such as ‘not very often’ and ‘related to’ were implemented. By and large, these omissions were due to the fact that the code Lenat wrote for these predicates was quite trivial… inevitable, yet regrettable process of simplifying large pieces of code, translating them to brief English phrases. This process left out many exceptional cases, and made the English condensations less accurate… Some problems that Ritchie and Hanna cite… are simply errors of mis-reading what was stated in the thesis or articles… A few of the problems raised in Ritchie and Hanna’s article are, annoyingly, genuine inconsistencies in the thesis document, such as whether or not facets had subfacets. These reflect the fact that AM was a running and evolving program, changing daily in small ways even as the thesis document was being written… the changes in representation were driven simply by AM’s running out of list space in 1975 <a href="https://en.wikipedia.org/wiki/Interlisp">INTERLISP</a> code; we were forced to shift representations time and time again just to gain a few hundred precious list cells.</p>
<p><span data-cites="lenatWhyAmEurisko1984">(<a href="#ref-lenatWhyAmEurisko1984" role="doc-biblioref">Lenat and Brown 1984</a>)</span></p>
</blockquote>
<p>And what were the lessons?</p>
<p>1th lesson: AM exhausts itself. Roughly speaking, each new interesting discovery depends on ~24 heuristics, and each heuristic has a hand in ~24 discoveries. Therefore, with ~N heuristic rules, there would be ~N interesting discoveries. Because AM cannot discover heuristic rules, with ~300 starting heuristic rules, it would run out of interesting discoveries and “die of boredom”. Lenat could help AM by adding new heuristics, and AM would make some new discoveries, but this never lasted.</p>
<blockquote>
<p>Eventually, AM acquired an uncommon ailment for a computing system: intellectual exhaustion. Having explored the esoteric reaches of mathematics, AM suddenly downshifted into a preoccupation with rudimentary arithmetic. Finally, with the remark, “Warning! No task on the agenda has priority over 200.”, the system virtually expired, as though from boredom.</p>
<p><span data-cites="hiltzikBirthThinkingMachine2001">(<a href="#ref-hiltzikBirthThinkingMachine2001" role="doc-biblioref">Hiltzik 2001</a>)</span></p>
</blockquote>
<p>2th lesson: Representation matters a lot. AM worked so well for mathematics, because AM used Lisp code as data. Lisp is the perfect tool if you want to search over the space of interesting mathematical functions. You can modify a Lisp expression, and get a different mathematical function that is possibly interesting. In contrast, if you were to modify assembly code, you’d most likely end up with nonsense. Indeed, Lenat found that he could not extend AM to “go meta” and discover new heuristics, because Lisp is good for math, not “heuretics” (the study of heuristics). Modifying a Lisp expression for a heuristic most likely ends up with nonsense, much like modifying assembly code for a mathematical function.</p>
<blockquote>
<p>It was only because of the intimate relationship between LISP and Mathematics that the mutation operators (loop unwinding, recursion elimination, composition, argument elimination, function substitution, etc.) turned out to yield a high ‘hit rate’ of viable, useful new math concepts when applied to previously-known, useful math concepts – concepts represented as LISP functions. But no such deep relationship existed between LISP and Heuretics, and when the basic automatic programming (mutations) operators were applied to viable, useful heuristics, they almost always produced useless (often worse than useless) new heuristic rules.</p>
<p>…</p>
<p>We did not perceive until writing this paper that the way in which <code>Similar-To</code>, <code>Not-Often</code>, <code>Notice-Regularity</code>, and scores of other ‘primitives’ were coded do themselves embody a large amount of heuristic knowledge. We exploited the structure of (or, if you prefer, partially encoded) the domain of elementary mathematics, in the process of making <em>trivial yet adequate</em> LISP versions of those extremely complex and subtle notions (such as similarity of concepts).</p>
<p><span data-cites="lenatWhyAmEurisko1984">(<a href="#ref-lenatWhyAmEurisko1984" role="doc-biblioref">Lenat and Brown 1984</a>)</span></p>
</blockquote>
<p>How do we know that Lenat learned the lessons?</p>
</section>
</section>
<section id="eurisko">
<h2 data-anchor-id="eurisko">EURISKO</h2>
<p>Though 1981 is still near, EURISKO is already shrouded in a reverential mystery like legends do, among the Deep Blue, the Samuel Checkers Player, and the apprentice’s broom. A symbol, a moral archetype. A program that discovered loopholes in a sci-fi ship-building tournament, allowing its creator to win twice in a row using fleets so unaesthetic that the people running the tournament threatened to stop the tournaments if EURISKO would enter and win a 3th one, so it retired, the Honorary Admiral, EURISKO the Undefeated.</p>
<p>Brilliant, dramatic, but what do we glean from it, other than a moral play about the power of thinking sideways and <a href="https://gwern.net/unseeing">seeing through</a>? Quite a lot.</p>
<section id="the-lessons">
<h3 data-anchor-id="the-lessons">The lessons</h3>
<p>As we saw, AM raised many questions, and Lenat wrote EURISKO to answer them. Specifically, he wanted to see if EURISKO could avoid intellectual exhaustion if it could <em>also</em> discover heuristics. If AM exhausted itself because it has used up the worth of its heuristics (1th lesson), then why not let the computer discover more heuristics? Since AM could not efficiently search over heuristic rules using LISP (2th lesson), Lenat designed a new language called RLL (“Representation Language Language”), over which heuristic rules are efficient to search.</p>
<p>As in AM, each heuristic in EURISKO had a level of <code>Worth</code>. Higher-worth heuristics were more likely to be invoked. Each heuristic had a <code>CreditTo</code>, so that if a heuristic rule rose in worth, its <code>CreditTo</code> would also rise in worth. When EURISKO was born, and saw itself, all the heuristic rules it had were branded with <code>CreditTo = DBLenat</code>, but this would soon change, as heuristics begat heuristics.</p>
<p>Yet while Lenat solved the problems raised by the first two lessons of AM, EURISKO failed, from which Lenat learned two further lessons. As we will see, it turned out that EURISKO <em>did</em> exhaust itself eventually after all. Self-discovery of heuristic rules eventually ceased, because self-discovery of heuristic rules relied on meta-heuristic rules about heuristic rules, and <em>those</em> rules run out of steam after a dozen or so uses.</p>
<p>Lenat concluded that he could program 100 heuristics and get a system that could discover 1000 rules, or program 100 meta-heuristics and get a system that could discover 1000 heuristics and 10000 rules, but none of these would be truly autonomous, and Lenat wished for more. He wished for something that could be an equal to humanity, that would grow up and explore into the great beyond, where it could no longer rely on humans for help. Lenat concluded that there really is no way to get a working automated discovery program without doing the hard work of hand-coding in a lot of common sense, and that there would be a point at which this system would finally achieve escape velocity, and would never be exhausted again.</p>
<p>Why would common sense help? Lenat observed how humans don’t seem to get stuck like EURISKO. He concluded that humans don’t run out of steam because they have a vast store of common sense knowledge about the world, from which they can draw upon for analogies, those far-flung flights of fancies that, in sufficient quantities, allows us to generate genuinely new ideas forever. For example, we can analogize the military with the medical, so that a doctor can “fighting an infection by an encircling movement with antibiotics”. This is the 3th lesson.</p>
<p>Why would analogies give genuinely new ideas? Well, intelligence is messy! If everything is so uniform, then there is no way to make a far-flung analogy – everything is pretty much the same already. Besides, just look at all the broken dreams of logical AI – their corpses tell us that no elegant theory of intelligence exists. This is the 4th lesson. Messiness is a hideous strength.</p>
<blockquote>
<p>The apparent adhoc-ness in both the heuristics’ content themselves, and in the control knowledge guiding the application of those heuristics, is clearly the source of many methodological objections to the work. But we believe that this adhocracy – indeed, adhocracy controlling adhocracy – may be the source of EURISKO’s underlying potential especially as a model for cognition.</p>
<p><span data-cites="lenatWhyAmEurisko1984">(<a href="#ref-lenatWhyAmEurisko1984" role="doc-biblioref">Lenat and Brown 1984</a>)</span></p>
</blockquote>
<p>Those are the lessons Lenat drew, but what did EURISKO really do?</p>
<p>Other than winning the sci-fi naval battle tournaments, EURISKO also worked on number theory, set theory, simulation of evolution,[^lenat-eurisko-biology] and metal-oxide (MOS) design. In MOS, it designed some new circuit elements that were verified by physical fabrication, such as a more efficient flip-flop that was “difficult to produce masks for and difficult to fabricate, but extremely small and fast”.</p>
<div title="Lenat's meta-Darwinian evolution theory">
<div data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">

<p>
Lenat’s meta-Darwinian evolution theory
</p>

</div>
<div id="callout-2">
<p>Lenat had a novel idea about how evolution works, one that I have not seen anywhere else. His idea was that purely random mutations are too slow for evolutionary progress, so that evolution must have gone “meta” as well. A substantial portion of the genome is probably not coding for direct protein transcription, but rather for a kind of heuristics, so that instead of trying out all possible mutations, evolution only tries out mutations that are likely useful. And of course, this would have gone meta, with meta-heuristic genes coding for useful ways to mutate the heuristic genes. <span data-cites="lenatPlausibleMutationDNA1980">(<a href="#ref-lenatPlausibleMutationDNA1980" role="doc-biblioref">Lenat 1980</a>)</span></p>
<blockquote>
<p>Here is a mechanism which embodies the heuristic “If a gene has mutated successfully several times in the recent past, then increase its chance of mutating in the next generation, and conversely.” All we need to posit is that somehow a short, noncoding sequence—we’ll call it an <em>asterisk—is</em> added to a gene each time it mutates [and] some mechanism (for example, stereochemical) whereby genes with many asterisks are more likely to be mutated, duplicated, and so on, than genes with few or none. Since the asterisks provide no specific benefits to the individual, they will gradually be lost over time, so that when a gene no longer should be mutated, its asterisk count will slowly decline over several generations.</p>
<p>…</p>
<p>… recombination among <a href="https://en.wikipedia.org/wiki/Intron">introns</a> modulates the evolution of a gene. Let’s look at an example of this: it is extremely important to keep the <a href="https://en.wikipedia.org/wiki/Hemoglobin_subunit_alpha"><code>a</code></a>, <a href="https://en.wikipedia.org/wiki/Hemoglobin_subunit_beta"><code>b</code></a>, and <a href="https://en.wikipedia.org/wiki/HBD"><code>d</code></a> globin genes separate, but their internal structure is very similar. To inhibit recombination, the spacers between them can be made very different, and the introns within them can diverge dramatically (since mutations in introns are not as deleterious to the functioning of the gene as mutations to the coding regions). In fact, there is evidence that both of these kinds of divergence do occur for the globins.</p>
<p><span data-cites="lenatRoleHeuristicsLearning1983">(<a href="#ref-lenatRoleHeuristicsLearning1983" role="doc-biblioref">Lenat 1983c</a>)</span></p>
</blockquote>
<p>At the start, only random mutations would be selected for, but eventually heuristics would arise that create likely advantageous mutations, and then evolution would go meta by one level: it would select for good heuristics. Over the natural history of earth, this can go meta for as many levels as time allows.</p>
<p>The paper goes into further speculative details, and makes a half-humorous–half-horror suggestion that perhaps evolution has worked for so long that the genome now contains a sophisticated intelligent designer, bootstrapped from the billions of years of heuristics-upon-heuristics backstopped by natural selection. The tiny designer would design the progeny’s genome, so that almost nothing in the mutation is “random”. This designer might regard the human experiment as a failure because of how hard humans are driving the natural world far out of its expectation.</p>
<p>For example, the designer might have learned a rule that is like “If the environment temperature is varying outside of this bound, then undo the previous mutation. We have clearly created an offspring that is wandering too much.” Then it would find the human experiment a mistake.</p>
<blockquote>
<p>By now a large knowledge base may exist about ecology, geology, glaciation, seasons, gravity, predation, symbiosis, causality, conservation, behavior, evolution and knowledge itself. In a small number of generations, man has managed to invalidate many of these bits of knowledge, this model of the world. If the heuristics can trace this breakdown to the increasing size of our brains, they might take quick corrective action, preserving homeostasis and the validity of their knowledge base by drastically decreasing human brain size over just a few generations. While this is of course a fanciful tongue-in-cheek extreme case, it (and the longer example above) demonstrates the power, the coordination, that a body of heuristics could evince if it were guiding the process of evolution.</p>
<p><span data-cites="lenatRoleHeuristicsLearning1983">(<a href="#ref-lenatRoleHeuristicsLearning1983" role="doc-biblioref">Lenat 1983c</a>)</span></p>
</blockquote>
<p>Imagine a species getting so good at adaptation that it is considered a mistake and thus undone. Wouldn’t that be the greatest <a href="https://en.wiktionary.org/wiki/lusus_naturae">joke of nature</a>?</p>
</div>
</div>
<p>But we’re really here to hear about the cool naval battles, not more abstract logics. Unfurl the photonic sails. We go.</p>
</section>
<section id="the-evervictorious">
<h3 data-anchor-id="the-evervictorious">The Evervictorious</h3>
<p>Lenat did not describe why, one July 4 weekend of 1981, he and EURISKO decided to enter the The Trillion Credit Squadron tournament. Perhaps for glory, or the thrill of the hunt. In any case, they did, and thus carved their legend.</p>
<p>The tournament was for the tabletop RPG game <a href="https://en.wikipedia.org/wiki/Traveller_(role-playing_game)"><em>Traveller</em></a>. Despite purportedly about space battles in a galactic empire, the game was essentially just a simplified version of WWII-era navy battles draped in space-opera garments. In the tournament, pairs of starship generals line up their fleets and fight until one side loses all their ships or surrender. Each fleet must be built with a budget of 1 trillion credits (thus the name), can contain ≤ 100 ships, and has certain other minor constraints.</p>
<p>At the start, Lenat went through the rule books and coded the rules into EURISKO. Then, every evening EURISKO would run through its tasks, including Traveller games, MOS design, math problems, and so on. And every morning, Lenat would check on the last night’s results, removing some heuristics that EURISKO discovered that he deemed bad, and adding some others. Manual intervention was necessary since otherwise EURISKO can be stuck with bad heuristics for a long time, and because of weird meta-bugs. Lenat estimated that the final EURISKO had accumulated 1300 CPU-hours of runtime in total on a <a href="https://archive.computerhistory.org/resources/access/text/2010/06/102660634-05-07-acc.pdf">Xerox 1100 Lisp machine</a>, and the Traveller win was “60/40% Lenat/EURISKO”.</p>
<p>Some heuristics that Lenat hardcoded at the start were:</p>
<ul>
<li>R7: If <span>\(f\)</span> is an interesting function of type <span>\(A \times A \to B\)</span>, then <span>\(g(a) := f(a,a)\)</span> is possibly an interesting function of type <span>\(A \to B\)</span>, and should be studied.</li>
<li>R9: If <span>\(f: A \to B\)</span> is an interesting function, and <span>\(S \subset B\)</span> is extremal in some sense, then <span>\(f^{-1}(S)\)</span> is possibly an interesting subset of <span>\(A\)</span> and should be studied.</li>
<li>R16 (conjecturing): If the first few examples of a concept <span>\(C\)</span> have just been found, then examine a typical one, and see what properties if satisfies; then see if any of those properties is satisfied by all examples of <span>\(C\)</span> .</li>
</ul>
<p>These were used by EURISKO for fleet design:</p>
<blockquote>
<p>One type of craft which is commonly included is a fighter, which is carried into the area by a carrier… Following R7, the possibility was considered of building fighters that could transport themselves into the battle area; due to the way the constraints were set up, this turned out to be a very powerful–if bizarre–design tactic. Essentially, each fighter was equipped with just enough ‘sailing’ and ‘launching’ equipment for it not to need a carrier. Once airborne, this excess equipment was jettisoned… This design tactic caused the rules publishers to modify the constraints, so that in 1982 one could not legally build such a thing.</p>
<p>…</p>
<p>The constraints specified a minimum fractional tonnage which had to be held back, away from battle [under the pretense of “<a href="https://en.wikipedia.org/wiki/Depot_ship">fuel tenders</a>”]. R7 caused us to consider using warships for that purpose, and indeed that proved a useful decision: whenever some front-line ships were moderately (but not totally) damaged, they traded places with the tenders in the rear lines. This maneuver was explicitly permitted in the rules, but no one had ever employed it except in desperation near the end of a nearly-stalemated battle, when little besides tenders were left intact. Due to the unintuitive and undesirable power of this design, the tournament directors altered the rules so that in 1982 and succeeding years the act of “trading places” is not so instantaneous. The rules modifications introduced more new synergies (loopholes) than they eliminated, and one of those involved having a ship which, when damaged, fired on (and sunk) itself so as not to reduce the overall fleet agility.</p>
<p>…</p>
<p>In the naval fleet design task, R9 was used quite heavily. The functions <span>\(f\)</span> in that simulated world apply to the design and behavior of fleets and of individual ships: <code>FleetComposition</code>, <code>Agility</code>, <code>Armor</code>, <code>WeaponVariety</code>, <code>TimeToEngage</code>, etc… the ultimate design did settle on a fleet containing almost all identical ships, each with nearly minimal agility, maximal armor, maximal weapon variety, almost all of which engaged with the enemy immediately, etc. One extremal ship employed in the 1981 tournament was a tiny but incredibly agile ship, with no offense whatsoever, that simply could not be hit. Although this was no longer legal in 1982, a ship with massive offensive capability and no defense was instrumental in that new fleet.</p>
<p>…</p>
<p>[For R16,] once a new design was tested in simulated combat, several characteristics of the conflict were noted (speed of victory, final state of the victor, amount of tactical decision-making required, etc.). These were formed into proto-conjectures, which were then tested by subsequent mock battles, and any which held over most of the simulations were believed as empirically valid.</p>
<p><span data-cites="lenatTheoryFormationHeuristic1983">(<a href="#ref-lenatTheoryFormationHeuristic1983" role="doc-biblioref">Lenat 1983b</a>)</span></p>
</blockquote>
<p>Other than hard-coded heuristics, EURISKO discovered many heuristics on its own (of course), such as the “nearly extreme” rule: In almost all Traveller TCS fleet design situations, the right decision is to go for the <em>nearly</em> extremal design.</p>
<blockquote>
<p>Thus, the final ships had Agility 2 (slightly above the absolute minimum), one weapon of each type of small weapons (rather than 0 or many), the fleet had almost as many ships as it could legally have but not quite (96 instead of 100), etc. Big weapons (enormous spinal mounts capable of blasting another ship to pieces with a single shot) were gradually phased out, in favor of an enormous number of small missile weapons. The fleet had almost all (75) ships of this type though there was one ship which was small and super agile and purely defensive (and literally unhittable by any reasonable enemy ship), and a couple monstrous hulks which had no chance of defense against normal ships, but which had weapons just barely accurate enough to hit any enemy ships that were (of course!) small and agile and purely defensive.</p>
<p><span data-cites="lenatTheoryFormationHeuristic1983">(<a href="#ref-lenatTheoryFormationHeuristic1983" role="doc-biblioref">Lenat 1983b</a>)</span></p>
</blockquote>
<p>One might have questioned the wisdom of running EURISKO every night on <em>all</em> the problem domains, from Traveller to MOS design, but it found good use of heuristic rules learned in one domain applied to another, what we’d now call “transfer learning”</p>
<blockquote>
<p>In working on the design of integrated circuits, for example, EURISKO stumbled on the fact that symmetry is a desirable property for such chips, although it did not understand why; when it was later instructed to design fleets for the Traveller game, EURISKO decided to make them symmetrical and justified its decision by referring to its earlier experience in designing circuits.</p>
<p><span data-cites="lenatComputerSoftwareIntelligent1984">(<a href="#ref-lenatComputerSoftwareIntelligent1984" role="doc-biblioref">Lenat 1984</a>)</span></p>
</blockquote>
<p>Fortunately for EURISKO and Lenat, the navy battles were “tactically trivial”, thus reducing the task to merely <a href="https://en.wikipedia.org/wiki/Nonlinear_programming">nonlinear optimization</a>. Also importantly, the problem was hard enough, and nonlinear enough, for EURISKO to show its edge over linear programming and human intuition.</p>
<blockquote>
<p>with 50 parameters per ship, about 10 values for each parameter (sometimes fewer, often an infinite number), and up to 100 distinct ships to design and include in each fleet, any systematic or Monte Carlo analysis of the problem is unlikely to succeed. In fact, the designers had done a detailed linear programming model of the game, and their computer runs convinced them that a fleet of about 20 behemoths was the optimal design. This was close to the starting fleet design the author supplied to EURISKO, and it was also close to the designs that most of the tournament entrants came up with.</p>
<p><span data-cites="lenatEURISKOProgramThat1983">(<a href="#ref-lenatEURISKOProgramThat1983" role="doc-biblioref">Lenat 1983a</a>)</span></p>
</blockquote>
<p>At the 1981 championship, EURISKO’s fleet of 96 ships consisted of the following:</p>
<ul>
<li>75 “Eurisko class” ships, which were very slow, heavily armored, and carried many small missiles – not unlike a sea urchin. It was discovered thanks to the “nearly extreme” heuristic.</li>
<li>7 “Wasp class” ships, which were small (1000 tons) but the most agile (Agility 6). These ships had virtually no offensive capability but were practically impossible to hit, serving as “stalemate guarantors”. If all other ships in EURISKO’s fleet were destroyed, these agile ships would remain, forcing a draw since enemy ships couldn’t destroy what they couldn’t hit. This concept emerged from EURISKO observing battle simulations where extremely agile ships survived. They were carried aboard the Queller and Garter class ships.</li>
<li>3 “Bee class” tiny ships (99 tons), which were the original accidentally-discovered stalemate guarantor, the “lifeboat” that EURISKO incorporated into every subsequent design, even while it refined the concept of the stalemate guarantor into the Wasp class. Despite heavy armor (Factor A=10), their Agility 0 made them less effective than Wasps at avoiding enemy fire, but presumably their tiny size made them hard to hit. They were carried aboard the Queller class ships.</li>
<li>3 “Queller class” ships, which were specifically designed as hard counters to stalemate guarantors. Each carried a single massive particle accelerator, which was not used by human-designed fleets, since that was ineffective against normally armored ships but excellent against small targets due to its broad beam and ease of aiming. These ships were hard counters to stalemate guarantors.</li>
<li>4 “Cisor class” ships. These “monstrous hulks” were heavily armored vessels that were <em>also</em> hard counters to stalemate guarantors. It had Agility 0, which means it has no chance of avoiding normal ships, but presumably it would survive long enough to destroy any stalemate guarantor, if any existed on the opposing side.</li>
<li>4 “Garter class” ships, which implemented the “warship as fuel tender” concept. They were reasonably agile (Agility 4) and could rotate between combat and reserve roles. When front-line ships became damaged, they would trade places with these capable warships held in reserve, allowing fresh ships to enter combat while damaged ones withdrew for repairs.</li>
</ul>
<div title="Reconstructing EURISKO's fleet">
<div data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">

<p>
Reconstructing EURISKO’s fleet
</p>

</div>
<div id="callout-3">
<p>The fleet was actually precisely reported in <a href="https://the-eye.eu/public/Books/rpg.rem.uz/Traveller/00%20-%20Other%20Materials/Journal%20of%20the%20Travellers%20Aid%20Society/JTAS/JTAS%2010.pdf"><em>The Journal of the Travellers’ Aid Society</em>, #10, pp.&nbsp;38-9</a>, which Yuxi had <a href="https://yuxi-liu-wired.github.io/essays/posts/cyc/code/eurisko's%20fleet.txt">transcribed to plaintext</a>. Not being an honorary Admiral, agi convened with a council of 4 other LLMs (Gemini-2.5, Claude-3.7, OpenAI-o1, DeepSeek-R1) to figure out how to correlate Lenat’s verbal description in <span data-cites="lenatEURISKOProgramThat1983">(<a href="#ref-lenatEURISKOProgramThat1983" role="doc-biblioref">Lenat 1983a</a>)</span> with the precise report.</p>
<p>In fact, it’s quite confusing even in the original paper. All agreed that the 75 “Eurisko class” ships corresponded to the “Eurisko class” in the report. But that’s where clarity ended.</p>
<p>The “stalemate guarantor” was described as “one ship which was small and super agile and purely defensive (and literally unhittable by any reasonable enemy ship)”. However, in the report, <em>every single class</em> contained more than one ship. There were two classes that seemed like the stalemate guarantor: the Wasp class with 1000 tons and Agility 6, and the Bee class with 99 tons and Agility 0. The Bee class had the smallest tonnage but no Agility, while the Wasp class was second-smallest and had the highest Agility. The Council voted Wasp as the “stalemate guarantor” at 3 Yea (Yuxi, Claude, o1), 1 Nay (Gemini), and R1 abstaining due to the server being busy (abstaining is typical behavior for the Chinese during <a href="https://en.wikipedia.org/wiki/Permanent_members_of_the_United_Nations_Security_Council">Big-Five votes</a>). Those who voted “Yea” were unable to respond to Gemini’s objection as to what EURISKO used the Bee class for, and motioned to discuss the second issue.</p>
<p>The hard counter to the stalemate guarantor was described even less clearly, and it seemed like there were <em>two</em> classes of them!</p>
<blockquote>
<p>a couple monstrous hulks which had no chance of defense against normal ships, but which had weapons just barely accurate enough to hit any enemy ships that were (of course!) small and agile and purely defensive. … this new ship had moderate size, no armor, the largest possible guidance computer, the slowest possible engines for its size and equipment, and one single, enormous accelerator weapon–a weapon usually ignored because its broad beam glances harmlessly off large armor-plated ships, but which is very easy to aim.</p>
<p><span data-cites="lenatEURISKOProgramThat1983">(<a href="#ref-lenatEURISKOProgramThat1983" role="doc-biblioref">Lenat 1983a</a>)</span></p>
</blockquote>
<p>The first hard counter matched both the Cisor class and the Queller class, both of which had over 19,000 tons and Agility 0. The second hard counter seemed like the Garter class, with 12,000 tons and Agility 4.</p>
<p>Yuxi lamented that it would have saved the Council a great deal of trouble if the report had detailed what weapons the ships used, but alas, they only reported on the “batteries”. It was at this point that o1 and Gemini raised that the “Batteries Bearing” did not actually stand for “carrying around electric batteries”, but rather “the angle which the artillery could span its fire”. Yuxi expressed astonishment that they could recall the rules of <em>Classic Traveller</em>, and sent the entire <span data-cites="TravellerBook51980">(<a href="#ref-TravellerBook51980" role="doc-biblioref"><em>Traveller <span>Book</span> 5: <span>High Guard</span></em> 1980</a>)</span> to Gemini for a translation, in response to which Gemini suggested that Yuxi RTFM, “Specifically, pages 21–37 and 50–52 are crucial.”.</p>
<p>The Council’s final conclusion was that probably there were two types of hard counters, with the Cisor being the “monstrous hulk” and the Queller being the one with the “enormous accelerator”.</p>
<p>Reading Gemini’s report, Claude raised the issue of what the Garter class was supposed to be. At that point, Yuxi re-attended to the fact that <span data-cites="lenatTheoryFormationHeuristic1983">(<a href="#ref-lenatTheoryFormationHeuristic1983" role="doc-biblioref">Lenat 1983b</a>)</span> described a slightly different fleet, one with “fuel tenders”, which <span data-cites="lenatEURISKOProgramThat1983">(<a href="#ref-lenatEURISKOProgramThat1983" role="doc-biblioref">Lenat 1983a</a>)</span> did not mention at all. Upon presenting the quote mentioning “fuel tenders”, Gemini argued that the Garter class is the fuel tender, and the Council concurred.</p>
<p>The Council reached no concensus as to the purpose of the Bee, though some suspected that the Bee was <em>another</em> type of stalemate guarantor, possibly the <em>original</em> accidentally-discovered stalemate guarantor:</p>
<blockquote>
<p>Some of the strangest elements of the final fleet were discovered accidentally rather than as the result of a long, continuous evolution process. The usefulness of a tiny defensive ship was apprehended after a ‘lifeboat’ was the only survivor from one side’s fleet, yet round after round it could not be hit at all. That design was immortalized into a design strategy (“Include one such ship in your fleet!”), and a very general rule began looking for ships that <em>could</em> destroy it.</p>
<p><span data-cites="lenatEURISKOProgramThat1983">(<a href="#ref-lenatEURISKOProgramThat1983" role="doc-biblioref">Lenat 1983a</a>)</span></p>
</blockquote>
<p>Yuxi raised one final issue, that of what the fighter “equipped with just enough ‘sailing’ and ‘launching’ equipment for it not to need a carrier” described in <span data-cites="lenatTheoryFormationHeuristic1983">(<a href="#ref-lenatTheoryFormationHeuristic1983" role="doc-biblioref">Lenat 1983b</a>)</span> corresponded to. Gemini concluded that no such thing existed in the fleet as reported, since the 3 Bees and 7 Wasps were the only fighters in the fleet, and they were carried by exactly enough carriers (3 Quellers carring 1 Wasp and 1 Bee each, 4 Garters carrying 1 Wasp each). Yet “This design tactic caused the rules publishers to modify the constraints, so that in 1982 one could not legally build such a thing.” strongly suggested that EURISKO actually used such a ship at one point in the 1981 competition. Possibly EURISKO used more than one fleet design, and only the design used in the final battle was reported. Yuxi acknowledged absent endorsement and motioned that the meeting adjourn.</p>
<p>The final report from the Council <a href="https://yuxi-liu-wired.github.io/essays/posts/cyc/code/Traveller%20High%20Guard%20Analysis.md">is attached</a>.</p>
</div>
</div>
<p>Whereas most battles took 2–4 hours, EURISKO’s opponents resigned in a few minutes, because typical opponents had around 20 large ships against EURISKO’s 96 ships. Each round would destroy EURISKO’s 15 ships and 5 of opponent’s ships. One round was enough for the opponent to realize this, and resign.</p>
<p>That is, except the very last round, where EURISKO faced against a human opponent with <em>basically the same design</em>, except they didn’t have the stalemate guarantor. So if EURISKO seemed to be losing, it can retreat all its fleet and bring out the stalemate guarantor, repair the fleet to full health, then do it again. EURISKO could keep doing this until it gets a lucky dice roll to win.</p>
<blockquote>
<p>Its second opponent did some calculations and resigned without ever firing a shot. The subsequent opponents resigned during their first or second round of combat with this fleet. EURISKO’s few specialty ships remained unused until the final round of the tournament, battling for 1st versus 2nd place. That opponent also had ships with heavy armor, few large weapons, low agility, etc. He was lacking any fast ships or fast-ship-killers, though. The author simply pointed out to him that if EURISKO were losing then (according to the TCS rules) our side need put only our fast ship out the front line, withdraw all the others and repair them, and – once they were finished repairing themselves – effectively start the battle all over again. This could go on ad infinitum, until such time as EURISKO appeared to be winning, and in that case we would let the battle continue to termination. The opponent did a few calculations and surrendered without fighting.</p>
<p>The tournament directors were chagrined that a bizarre fleet such as this one captured the day, <em>and</em> a similar fleet (though not so extreme) took second place. The rules for future years’ TCS tournaments were changed to eliminate the design singularities which EURISKO found. For example, repairing of damaged ships was prohibited, so the utility of the unhittable ship became negligible.</p>
<p><span data-cites="lenatEURISKOProgramThat1983">(<a href="#ref-lenatEURISKOProgramThat1983" role="doc-biblioref">Lenat 1983a</a>)</span></p>
</blockquote>
<p>Notice that this complicates the typical narrative about a computer beating out humans. As it turns out, Traveller was just a very exploitable game. Even a human discovered that loophole.</p>
<p>At the 1982 championship, rules were changed to plug the loopholes, and the rules were published only a week before the event. Fortunately, loophole-plugging begat even more loopholes, as any programmer who has ever rush-debugged could attest, and EURISKO’s general heuristics (such as the “nearly extreme” rule) remained valid, so it worked out another winning fleet quickly without needing another 1300 CPU-hours.</p>
<blockquote>
<p>Coincidentally, just as the defensive ship made a difference in the 1981 final round, the offensive ships made a difference in the 1982 final round. In each case, their presence caused the opponent to resign without firing a shot… Just as most ‘experienced’ players jeered at the 1981 fleet because it had practically no large weapons, they jeered at the 1982 fleet because it was unarmored <em>and</em> it still had no large weapons, even though the rules changes had made them much cheaper.</p>
<p><span data-cites="lenatEURISKOProgramThat1983">(<a href="#ref-lenatEURISKOProgramThat1983" role="doc-biblioref">Lenat 1983a</a>)</span></p>
</blockquote>
<p>Unfortunately there is no information whatsoever about what EURISKO’s 1982 fleet was made of. It seems to be lost in the star-dust of history.</p>
<p>As one expect from a program reasoning about and making its own rules, it stumbled into meta-bugs. The simplest example was one that kept triggering itself, creating an infinite loop. <span data-cites="johnsonMachineryMindNew1986">(<a href="#ref-johnsonMachineryMindNew1986" role="doc-biblioref">Johnson 1986, chap. 10</a>)</span> Others were more amusing.</p>
<blockquote>
<p>One of the first heuristics that EURISKO synthesized (H59) quickly attained nearly the highest Worth possible (999). Quite excitedly, we examined it and could not understand at first what it was doing that was so terrific. We monitored it carefully, and finally realized how it worked: whenever a new conjecture was made with high worth, this rule put its own name down as one of the discoverers! It turned out to be particularly difficult to prevent this generic type of finessing of EURISKO’s evaluation mechanism. Since the rules had full access to EURISKO’s code, they would have access to any safeguards we might try to implement. We finally opted for having a small ‘meta-level’ of protected code that the rest of the system could not modify.</p>
<p>The second ‘bug’ is even stranger. A heuristic arose which (as part of a daring but ill-advised experiment EURISKO was conducting) said that all machine-synthesized heuristics were terrible and should be eliminated. Luckily, EURISKO chose this very heuristic as one of the first to eliminate, and the problem solved itself.</p>
<p><span data-cites="lenatEURISKOProgramThat1983">(<a href="#ref-lenatEURISKOProgramThat1983" role="doc-biblioref">Lenat 1983a</a>)</span></p>
</blockquote>
<blockquote>
<p>Often I’d find it in a mode best described as “dead”. Sometime during the night, EURISKO would decide that the best thing to do was to commit suicide and shut itself off. More precisely, it modified its own judgmental rules in a way that valued “making no errors at all” as highly as “making productive new discoveries”. As soon as EURISKO did this, it found it could successfully meet its new goal by doing nothing at all for the rest of the night… I eventually had to add a new heuristic to EURISKO-one it couldn’t modify in any way-to explicitly forbid this sort of suicide.</p>
<p><span data-cites="storkHALsLegacy2001s1998">(<a href="#ref-storkHALsLegacy2001s1998" role="doc-biblioref">Stork 1998, 194</a>)</span></p>
</blockquote>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/Eurisko_GUI.png"></p>
<figcaption>The only known image of EURISKO, reasoning about the Traveller game, probably on the Xerox 1100 Lisp machine. It was probably running over the <a href="https://en.wikipedia.org/wiki/Genera_(operating_system)">Genera Operating System</a>. Lenat claimed EURISKO ran for 1300 CPU-hours in total. <span data-cites="lenatComputerSoftwareIntelligent1984">(<a href="#ref-lenatComputerSoftwareIntelligent1984" role="doc-biblioref">Lenat 1984</a>)</span></figcaption>
</figure>
</div>
<p>In his review article, Lenat made a brief philosophical comment that EURISKO is the new perceptron:</p>
<blockquote>
<p>… the paradigm underlying AM and EURISKO may be thought of as the new generation of perceptrons, perceptrons based on collections or societies of evolving, self-organizing, symbolic knowledge structures. In classical perceptrons, all knowledge had to be encoded as topological networks of linked neurons, with weights on the links. The representation scheme being used by EURISKO provides much more powerful linkages, taking the form of heuristics about concepts, including heuristics for how to use and evolve heuristics. Both types of perceptrons rely on the law of large numbers, on a kind of local-global property of achieving adequate performance through the interactions of many small, relatively simple parts.</p>
<p>The classical perceptrons did hill-climbing, in spaces whose topology was defined explicitly by weights on arcs between nodes (nodes which did straightforward Boolean combinations plus thresholding). The EURISKO style of system does hill-climbing at both the object- (performance-program) and meta- (control decision) levels, in spaces whose terrain is defined implicitly, symbolically, by the contents of the nodes (nodes which are full-fledged concepts, at both object- and meta-levels). The new scheme fully exploits the same source of power (synergy through abundance) yet it is free from many of the limitations of the classical perceptron scheme.</p>
<p><span data-cites="lenatWhyAmEurisko1984">(<a href="#ref-lenatWhyAmEurisko1984" role="doc-biblioref">Lenat and Brown 1984</a>)</span></p>
</blockquote>
<p>If this sounds familiar, it is because this Lenat had the same idea as Marvin Minsky, a friend of his,<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> and he was writing in 1984, at the second coming of neural networks. Minsky would soon write his <em>Society of Mind</em> in 1986, which the Cyc project resembled, then <a href="https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/#minsky-and-papert-struck-back">re-reject neural networks by writing a long epilogue in 1988</a> to his infamous <em>Perceptrons</em> (1969), often blamed for the first neural network winter. Indeed, Lenat’s objection to neural networks was essentially the same as Minsky’s, if you compare that with Minsky’s epilogue. Lenat’s approach to Cyc was the same as the Society of Mind of Minsky. Reciprocating, Minsky had often called the field of AI “brain-dead”, holding Lenat’s Cyc as the only one worth mentioning. <span data-cites="baardAIFounderBlasts2003">(<a href="#ref-baardAIFounderBlasts2003" role="doc-biblioref">Baard 2003</a>)</span></p>
<div><div id="fn3"><p><sup>3</sup>&nbsp;I’m not sure where to put this anecdote about Minsky that Lenat told, but I want to put it somewhere:</p>
<blockquote>
<p>… when he was at Lincoln Labs about 50 years ago. And in those days computer time was so precious that you submitted a deck of computer cards and the very first card said ‘how many CPU seconds to allow the program to run?’ And so he built a program that essentially would beg for time. So it would say ‘give 30 seconds’ on the job control card, but then once it started, all it would do is sit there for 15 seconds doing nothing. Then it would ring a bell on the Teletype console in the machine room and call the operator’s attention and say ‘I need 20 more seconds please.’ Then it would just sit there for another 15 seconds and do that again and say ‘I need another minute please.’ And so at the end finally after like half an hour, the operator just killed that particular job. And Marvin would storm into the poor operator’s room and say “Hey I put 15 seconds on the job control card. You’re charging me for half an hour of CPU time,” and the poor operator would say “well your program kept asking for it,” and Marvin would say, “it always does that.”</p>
<p><span data-cites="lenatEpisode89Conversation2019">(<a href="#ref-lenatEpisode89Conversation2019" role="doc-biblioref">Lenat 2019b</a>)</span></p>
</blockquote>
<p>Though now that I’ve finished the essay, this feels like a metaphor for the Cyc project itself.</p></div><p><sup>4</sup>&nbsp;Though Lenat admitted that “the code ought to have been provided” <span data-cites="lenatWhyAmEurisko1984">(<a href="#ref-lenatWhyAmEurisko1984" role="doc-biblioref">Lenat and Brown 1984</a>)</span> for AM, he would never publish the code, not with AM, nor with EURISKO. He had often claimed it had been long lost, yet the source code for AM and EURISKO had <a href="https://white-flame.com/am-eurisko.html">recently been found</a>, right where it should be – the <a href="https://www.saildart.org/DBL">DBL folder in the Stanford AI Laboratory backup data</a>. It could only be found because Lenat had died, thus releasing the password protection on the folder. I wonder if Lenat had lied, and simply wished to protect his source code. It would correlate with his later behavior.</p></div><p>Continuing the trend of AM, Lenat never published the source code for EURISKO,<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a> and indeed, the only known attempt at reimplementation was <span data-cites="haaseInventionExplorationDiscovery1990">(<a href="#ref-haaseInventionExplorationDiscovery1990" role="doc-biblioref">Haase 1990</a>)</span>, which had no offspring.</p>
</section>
</section>
<section id="the-saga-of-cyc">
<h2 data-anchor-id="the-saga-of-cyc">The saga of Cyc</h2>
<blockquote>
<p>He divided the universe into forty categories or classes, which were then subdivided into differences, and subdivided in turn into species. To each class he assigned a monosyllable of two letters; to each difference, a consonant; to each species, a vowel. For example, <code>de</code> means element; <code>deb</code>, the first of the elements, fire; <code>deba</code>, a portion of the element of fire, a flame. In a similar language invented by Letellier (1850), a means animal; ab, mam malian; abo, carnivorous; <code>aboj</code>, feline; <code>aboje</code>, cat; <code>abi</code>, herbivorous; <code>abiv</code>, equine; etc… children could learn this language without knowing that it was artificial; later, in school, they would discover that it was also a universal key and a secret encyclopedia.</p>
<p>Having defined Wilkins’ procedure, we must examine a problem that is impossible or difficult to postpone: the merit of the forty-part table on which the language is based. Let us consider the eighth category: stones. Wilkins divides them into common (flint, gravel, slate); moderate (marble, amber, coral); precious (pearl, opal); transparent (amethyst, sapphire); and insoluble (coal, fuller’s earth, and arsenic). The ninth category is almost as alarming as the eighth. It reveals that metals can be imperfect (vermilion, quicksilver); artificial (bronze, brass); recremental (filings, rust); and natural (gold, tin, copper). The whale appears in the sixteenth category: it is a viviparous, oblong fish.</p>
<p>— Borges, <em>The <a href="https://en.wikipedia.org/wiki/An_Essay_Towards_a_Real_Character,_and_a_Philosophical_Language">analytical language</a> of <a href="https://en.wikipedia.org/wiki/John_Wilkins">John Wilkins</a></em></p>
</blockquote>
<p>Unfortunately, EURISKO ran out of steam just like AM. Taking the 4 lessons of AM and EURISKO, Lenat concluded that there would be no free lunch. Intelligence is a lot of work. You need to put in the right representational language for reasoning and discovery – not just about Lisp or RLL or mathematics, but a lot more. You need to put in a lot of loosely organized, kind of correct heuristic rules – not just a few eternal truths of discovery. You need to put in a lot of facts. You need to interact with the program, to help it along and to be helped along, not just to sit and watch.</p>
<p>So Lenat began paying for his lunch.</p>
<p>In 1984, he started the Cyc project, an ambitious attempt to scale symbolic AI up to the real world. Like most expert systems, the Cyc project consists of a giant knowledge base that encodes all of common sense, upon which inference engines run. Unlike most expert systems, the ambition of Cyc was universal: Its knowledge base would not be restricted to expert knowledge in a particular domain, but <em>all</em> common sense knowledge in <em>all</em> domains that humans have ever common-sensed. It would take a decade, but considering the payoff, it would be completely worth it.</p>
<blockquote>
<p>AI has for many years understood enough about representation and inference to tackle this project, but no one has sat down and done it… only by pulling together the latest human interface tools, Lisp machines, ideas of enforced semantics, and funding for a decade-long effort could we attempt a project of this scale.</p>
<p><span data-cites="lenatCycUsingCommon1985">(<a href="#ref-lenatCycUsingCommon1985" role="doc-biblioref">Lenat, Prakash, and Shepherd 1985</a>)</span></p>
</blockquote>
<p>The game plan was simple:</p>
<ol type="1">
<li>“Prime the knowledge pump” by manually encoding a large enough knowledge base of common senses in a logical language. Also, construct a translator between the logical language of the Cyc and the natural language of humans.</li>
<li>Obtain an AI with common sense and natural language, allowing it to learn by reading what people have written down and conversing with people.</li>
<li>When it reaches the human frontier of knowledge, it will start performing experiments to go beyond it.</li>
</ol>
<p>This would solve in one go three problems that plagued the 1980s expert systems:</p>
<ol type="1">
<li>No more of the famous brittleness of expert systems, such as stating that a rusty car had measles just because the user stated that it had reddish spots, because Cyc would have <em>all</em> the common senses.</li>
<li>No more of “running out of steam” like EURISKO and AM. Once it has enough knowledge, it would be able to machine-learn, and thus get past the knowledge bottleneck.</li>
<li>No more of siloed experts unable to communicate across the vast gap between their little knowledge domains. Expert systems would finally be able to talk with each other if they would all be based on Cyc’s universal knowledge base.</li>
</ol>
<p>A metaphor that Lenat had used often is that of priming a water pump. The more knowledge Cyc has, the easier it is for Cyc to learn more. At first, it must be spoon-fed knowledge with every entry entered by hand. As it builds a basic understanding of the world, it would be able to parse sentences half-way from natural language to logic, and the ontologists would help finish the job, and the more it knew, the better it could parse, saving more time, until it would start parsing without human help. On that day, the “knowledge pump” would finally, triumphantly, be primed, and Cyc would start pumping and pumping, and more knowledge would just keep pouring out without any exhaustion, ushering a new golden age.</p>
<section id="cyc-by-1993">
<h3 data-anchor-id="cyc-by-1993">Cyc by 1993</h3>
<p>The first published plan of Cyc was in 1985 <span data-cites="lenatCycUsingCommon1985">(<a href="#ref-lenatCycUsingCommon1985" role="doc-biblioref">Lenat, Prakash, and Shepherd 1985</a>)</span>, with 3 stages:</p>
<ol type="1">
<li>(1985–1988): by hand, encode 400 encyclopedia articles. They estimated there are about 400 “kinds” of articles, and they planned to get one in each kind.</li>
<li>(1988–1993): encode 30000 encyclopedia articles. This step should be fast, since each new article can be copy-and-edited from a previously encoded similar one.</li>
<li>(1993–?): use Cyc to solve AI problems and apply Cyc for commercial purposes.</li>
</ol>
<p>By checking Most articles had just 1 paragraph, and 1 paragraph took about 1 person-day to encode, they estimated that phases 1 and 2 would take 150 person-years to complete, so with a crew of 20, the plan seemed doable.</p>
<p>At this point, Cyc was still based on frames, like AM and EURISKO. Each frame corresponded roughly to a concept. Each new article encoded took “dozens” of new frames, but they expected it to drop to around 0.1 frames per article after the first 1000s of articles, indicating they expected the final Cyc to contain about 50K concepts.</p>
<p>Such a project was way beyond a typical academic project, or even a commercial project. Fortunately, the Japan scare of the 1980s created a flood of funding for AI projects, and Cyc got funding for the next 10 years, freed from both academic and commercial interests.</p>
<p>5 years later, Lenat optimistically wrote the “midterm” report that the project was still on schedule, that the knowledge pump would be primed by 1995. <span data-cites="lenatCycMidtermReport1990">(<a href="#ref-lenatCycMidtermReport1990" role="doc-biblioref">Lenat and Guha 1990</a>)</span></p>
<p>The most important change was what Lenat called “from the black to the white”. Instead of encoding what is written in the encyclopedias, they really should encode what is <em>not</em> written. Encyclopedias don’t teach what everyone knows, but to make sense of it, one must already know what “everyone knows”. What Cyc should have is not the black ink, but the white space around the black ink.</p>
<blockquote>
<p>Lenat began building Cyc by setting himself a seemingly modest challenge. He picked a pair of test sentences that Cyc would eventually have to understand: “Napoleon died in 1821. Wellington was greatly saddened.” To comprehend them, Cyc would need to grasp such basic concepts as death, time, warfare, and France, as well as the sometimes counterintuitive aspects of human emotion, such as why Wellington would be saddened by his enemy’s demise. Lenat and a few collaborators began writing these concepts down and constructing a huge branching-tree chart to connect them. They produced a gigantic list of axiomatic statements–fundamental assumptions–that described each concept in Cyc’s database: its properties, how it interacted with other things. “We took enormous pieces of white paper,” Lenat remembers, “and filled walls, maybe 150 feet long by about 8 feet high, with little notes and circles and arrows and whatnot.”</p>
<p><span data-cites="thompsonKnowItAllMachine2001">(<a href="#ref-thompsonKnowItAllMachine2001" role="doc-biblioref">Thompson 2001</a>)</span></p>
</blockquote>
<p>They have also estimated that it takes about 80 minutes to encode a single rule, including the overhead for knowledge elicitation up front, and the overhead for debugging and testing <span data-cites="lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">Lenat 2022a</a>)</span>.</p>
<p>With these lessons learned, they confidently claimed that they have encoded 1M assertions, 50K concepts, with 0.1% of common sense done. They expected that 10–50% was necessary for knowledge pump to be primed, which they expected to be done by 1995 – still on schedule, and they looked forward to the day, around 2000, when “no one would even think about having a computer that doesn’t have Cyc running on it”. <span data-cites="lenatBuildingLargeKnowledgebased1989">(<a href="#ref-lenatBuildingLargeKnowledgebased1989" role="doc-biblioref">Lenat and Guha 1989</a>)</span></p>
<p>They expected that the finished Cyc would know 1M concepts. Why 1M? They held their own “mini-<a href="https://en.wikipedia.org/wiki/Dartmouth_Conference">Dartmouth conference</a>” and found that multiple estimates all suggest the 1M number <span data-cites="lenatThresholdsKnowledge1991">(<a href="#ref-lenatThresholdsKnowledge1991" role="doc-biblioref">Lenat and Feigenbaum 1991</a>)</span>:</p>
<ul>
<li>Alan Kay: 30K encyclopedia articles with 30 concepts per article gives 0.9M concepts.</li>
<li>Japanese Electronic Dictionary Research Project: in several languages, an educated speaker knows about 200K words.</li>
<li>Marvin Minsky: 0.2M waking hours between birth and age 21. Assuming 4 new concepts per hour, then 0.8M concepts.</li>
<li>There are about 1 trillion brain cells. Assuming each brain cell is responsible for a one-step inference between two concepts, then there are 1M concepts.</li>
</ul>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/cyc_project_ontology.png"></p>
<figcaption>The state of knowledge base of Cyc one year after its founding, in 1985. <span data-cites="lenatCycUsingCommon1985">(<a href="#ref-lenatCycUsingCommon1985" role="doc-biblioref">Lenat, Prakash, and Shepherd 1985, fig. 1</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="cyc-by-2000">
<h3 data-anchor-id="cyc-by-2000">Cyc by 2000</h3>
<p>Around 1990, a large rewrite occurred, resetting some progress. This was expected. As Lenat later claimed, they had met about 150 technical obstacles along the way, and had cleared them all away by 1990, and it remained to just add more knowledge. <span data-cites="lenatBuildingMachineSmart2009 lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatBuildingMachineSmart2009" role="doc-biblioref">Lenat 2009</a>, <a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">2022a</a>)</span> The most important technical obstacles that they conquered were as follows.</p>
<p>Instead of an object-oriented frame-and-slots language like EURISKO and AM, use a fully general higher-order logic language. This is necessary because people can do common sense higher-order reasoning: modals, reflection, pros and cons, counterfactual hypotheticals, contexts as first-class objects in our ontology, several different useful “species” of negation, etc. <span data-cites="lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">Lenat 2022a</a>)</span></p>
<p>Instead of searching for the right representation, use as many representations for concepts and rules as you need. Each is represented in at least two ways. This is necessary for efficient inference. Similarly, use as many inference engines as needed, since the general logic engine is too slow. They had already 20 at that point, and they would eventually end up with &gt;1100. <span data-cites="lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">Lenat 2022a</a>)</span></p>
<p>Don’t try to make the perfect “<a href="https://en.wikipedia.org/wiki/Upper_ontology">upper ontology</a>”. It just has to be good enough. A “suboptimal” one only causes a constant factor <span>\(O(1)\)</span> of waste in computational space and time.</p>
<blockquote>
<p>We even wasted quite a bit of time trying to get the very most general tip of Cyc’s concept network “right”… at the 1986 Fifth-Generation Project conference in Tokyo, when we saw the ontology built by Japan’s answer to Cyc, named Electronic Dictionary Research (EDR). Their topmost distinction was between things with souls and things without souls. And large trees were in the former category, whereas small trees were in the latter category… They and their EDR system knew that both types of trees needed water and sunlight and had roots, etc., they just had to represent each of those assertions as two separate rules instead of one, as we did in Cyc. No big deal.</p>
<p>The important lesson was: Making suboptimal ontology choices just means that your ontology and knowledge base might have to be bigger, more verbose, to make up for those missed generalization opportunities.</p>
<p><span data-cites="lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">Lenat 2022a</a>)</span></p>
</blockquote>
<p>In the progress report in 1995, they stated that they had manually re-entered in the new language 100K concepts and 1M assertions into Cyc in the new language, at the price of 100 person-years. Furthermore, the knowledge pump was close to being primed, and they expect Cyc to start learning on its own by reading (“natural language understanding”) and discovery (“machine learning”) sometime in the next 10 years. <span data-cites="lenatCycLargescaleInvestment1995">(<a href="#ref-lenatCycLargescaleInvestment1995" role="doc-biblioref">Lenat 1995b</a>)</span></p>
<p>At this point, the previous governmental funding had mostly stopped, so Lenat spun out Cyc into a for-profit company, Cycorp, to continue the work. Most academic publications ceased at this point, and I had to rely on OSINT/cyberstalking at this point to piece together what happened afterwords, by watching every Lenat talk, reading every news report, and digging up every gossip by ex-Cyclists in long-dead forums.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<div><p><sup>5</sup>&nbsp;Most papers published after that point were slim on details, and mostly about yet new exciting ways for them to ingest more data from the Internet, or about yet more ways to use their knowledge base. I could find no information about how the inference engines worked, and very little details of commercial applications. Most of the “applications” were vaporware, with dead links everywhere.</p></div></section>
<section id="cyc-in-the-2000s">
<h3 data-anchor-id="cyc-in-the-2000s">Cyc in the 2000s</h3>
<p>It was 2001. The Internet was thriving even though <a href="https://en.wikipedia.org/wiki/Dot-com_bubble">most dot-com startups had died</a>, and Lenat planned to harness the wisdom of the online crowd – much like Wikipedia, the Free Encyclopedia, which launched in the same year. Cyc at that point had 1.5M assertions, and had begun to be partly “tutored” in natural language by the Cyclists, which was more pleasant than typing everything up in pure CycL. Lenat optimistically predicted that the Internet crowd would be entering 10M assertions in 2002, accelerating from there, so that Cyc would know 100M assertions by 2007, at which point it would know as many things as a typical human. Then by 2011, Cyc would have learned, by reading, everything that humanity collectively knows, and must extend the knowledge frontier by doing novel experiments. <span data-cites="thompsonKnowItAllMachine2001 anthesComputerizingCommonSense2002">(<a href="#ref-thompsonKnowItAllMachine2001" role="doc-biblioref">Thompson 2001</a>; <a href="#ref-anthesComputerizingCommonSense2002" role="doc-biblioref">Anthes 2002b</a>)</span></p>
<p>To drum up the support, Cycorp released OpenCyc in 2001, a small subset of Cyc. Lenat planned to migrate everything to the public mode. But OpenCyc would always lag the true Cyc by 24 to 30 months. <span data-cites="anthesComputerizingCommonSense2002">(<a href="#ref-anthesComputerizingCommonSense2002" role="doc-biblioref">Anthes 2002b</a>)</span> Unfortunately, this was not to be the case. The last version of OpenCyc was released in 2012, and it quietly shut down with no fanfare, probably in 2017-03, with a <a href="https://web.archive.org/web/20170422212642/http://opencyc.org/">curt message</a>:</p>
<blockquote>
<p>Part of the Cyc technology was released, starting in 2001, as OpenCyc, which provided an API, RDF endpoint, and data dump, under appropriate Apache and Creative Commons open source licenses. Its distribution was discontinued in early 2017 because such “fragmenting” led to divergence, and led to confusion amongst its users and the technical community generally that that OpenCyc fragment was Cyc.</p>
</blockquote>
<p>OpenCyc was primarily a subset of the knowledge base. As described <a href="https://web.archive.org/web/20040310001548/http://www.cyc.com/doc/handbook/oe/10-notable-quoted-collections.html">in the Handbook</a>, the base contained 3 sections:</p>
<ul>
<li>The public section was a “large section, constituting much of the Cyc upper-ontology as well as some middle- and lower-ontology”.</li>
<li>The proprietary section was an “even larger section of the Knowledge Base, subsuming the public release and containing a good deal more, has been sanctioned for release to corporations and individuals who are co-participants with Cyc in various DARPA contracts. This portion of the Knowledge Base is generally referred to as the Integrated Knowledge Base or IKB.”.</li>
<li>A small classified section was for Cycorp itself, “that should not be released to anyone outside the company. Sometimes, this is because the information is pertinent to commercial contracts that are subject to non-disclosure; sometimes, it is because the terms in question are considered experimental in one way or another, and therefore not suitable for immediate release.”</li>
</ul>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/opencyc-kb-browser.gif"></p>
<figcaption>The knowledge base of OpenCyc as of 2010. <a href="https://web.archive.org/web/20100526095325/http://www.opencyc.com/">Source</a></figcaption>
</figure>
</div>
<p>Concurrent with OpenCyc, there was also ResearchCyc, which contained the non-proprietary parts of Cyc, but only available for research purposes. It shutdown <a href="https://web.archive.org/web/20190601000000*/http://www.cyc.com/researchcyc">sometime in 2019</a>, without even a curt message.</p>
<p>As one can expect from Lenat’s previous non-releases, there was and has never been any release of Cyc itself, especially because Cyc is a commercial endeavor, as is necessary to sustain the 2000 person-year project.</p>
<p>In 2001, at the start of the <a href="https://en.wikipedia.org/wiki/Semantic_Web">Semantic Web</a> hype (the original “Web 3.0”, before it came to mean cryptographic blockchains), Cycorp began seriously engaging with the many Semantic Web initiatives.</p>
<p>Squinting a bit, the visions of the semantic web and the vision of the Cyc were the same: Both wish to draw connections between data, such that computer programs can chain together multiple operations on data, synthesize them, and give the user what they meant, instead of what they literally typed out. Cycorp regarded the semantic web effort as doing essentially the same thing, except with a less expressive frame language (not even first-order logic), on the Internet scale. During the 2000s, papers from Cycorp often talked of integrating Cyc with the semantic web by encoding knowledge in DAML, RDF, OWL, XML, or some other boring acronym.</p>
<p>Cycorp participated in the <a href="https://web.archive.org/web/20120609070903/http://suo.ieee.org/">Standard Upper Ontology Working Group (SUO WG)</a>, which, like most Working Groups, petered out in 2003 among motions, procedures, and consensus-buildings, filled with meticulous wisdoms and benevolent safeguarding of the metaphysics of mankind. What it did provide is <a href="https://web.archive.org/web/20030515043435/http://www.cyc.com/SUO/opencyc-ontology.txt">the earliest copy</a> of OpenCyc I have found, in 2003, which I have <a href="https://github.com/yuxi-liu-wired/cyc-archive/blob/main/other_files/opencyc-ontology.txt">backed up</a> for safekeeping.</p>
<p>With the breakout success of the <a href="https://en.wikipedia.org/wiki/ESP_game">ESP game</a> in 2003, “games with a purpose” was all the rage, so Cyc attempted to keep up with the times with its own <a href="https://web.archive.org/web/20120902022607/http://game.cyc.com/">FACTory</a> in 2005. It never got out of the <code>Beta version 1.0.1</code> and shut down sometime after 2012. According to the <a href="https://web.archive.org/web/20061205102605/http://207.207.9.186/helpfiles/HowToPlay.html">tutorial</a>, it was a single player game, where the player just selects whether the statement is true, false, doesn’t make sense, or the player doesn’t know. The answers are scored based on the majority of answers. Though Lenat stated in a lecture that they had players that raked up 50 hours a month to the point that Lenat felt concerned about it, I could not find any mention of how much data they have gathered from this game in total. Nevertheless, this appeared to be the only time Cyc crowd-sourced data.</p>
<p>In 2006, Cycorp spun out The Cyc Foundation, a non-profit organization promote the OpenCyc + Semantic Web combo. As usual, nothing ever came of it. The <a href="https://web.archive.org/web/20110610022626/http://www.cycfoundation.org/blog/?p=45">last blogpost went up in 2011-06</a>, and the website shutdown in 2015.</p>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/Cyclopedia_mockeup.png"></p>
<figcaption>The Cyc + Wikipedia = “Cyclopedia” mockup <a href="https://web.archive.org/web/20071102082440/http://www.cycfoundation.org/blog/?page_id=15">posted in 2007 by The Cyc Foundation</a>. Despite being “fairly close to releasing a beta version of Cyclopedia”, they never did.</figcaption>
</figure>
</div>
<p>In 2008, Cycorp tried again by putting up a copy of OpenCyc with a user interface and an API, branded as “<a href="https://web.archive.org/web/20080826011848/http://sw.opencyc.org/">OpenCyc for the Semantic Web</a>”. The API would allow web agents to call on OpenCyc and use the replies to do Semantic Web things, making it “the backbone of semantic web”… at least that was the hope. An exhaustive Google search turned up zero actual applications. It shut down in 2017 when OpenCyc did.</p>
<p>Though deprived of its purported backbone, from our vantage point, the Semantic Web has arrive as promised, but instead of the dream of Cyc-like thinkers performing long queries over databases, we have forgetful agent swarms talking with each other with API calls, dying, <a href="https://en.wikipedia.org/wiki/REST">REST</a>ing, and reincarnating, a game of <a href="https://en.wikipedia.org/wiki/Memento_(film)"><em>Memento</em></a> on the global scale.</p>
<p>In 2008, Cycorp tried again by joining the Large Knowledge Collider (LarKC)<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> project at Europe, “a platform for massive distributed incomplete reasoning that will remove the scalability barriers of currently existing reasoning systems for the Semantic Web”. The <a href="https://active09.ijs.si/wp-content/uploads/2009/06/michael-witbrock.pdf">hope</a> was to produce a common knowledge base with over 1 billion triples, such that it can easily be scaled up further, and easy to produce expert systems based on it. A few papers and conferences later, the project <a href="https://web.archive.org/web/20150206003436/http://www.larkc.eu/">ended in 2011</a>.</p>
<div><p><sup>6</sup>&nbsp;The name is an obvious shade to the <a href="https://en.wikipedia.org/wiki/Large_Hadron_Collider#Inaugural_tests_(2008)">Large Hadron Collider</a>, which began operation in 2009 and was briefly in the popular imagination.</p></div><p>There was <em>another</em> <a href="https://web.archive.org/web/20120126094742/http://blog.cyc.com/">blog</a> by Cycorp starting in 2008, that stopped updating in 2011 after 11 unremarkable posts. It was supposedly written by Cycorp, though its style was the same forgettably respectable style as that of the Cyc Foundation and parliamentary proceedings.</p>
<p>There was a Twitter bot <code>@cyc_ai​</code>, which started in 2008 and <a href="https://web.archive.org/web/20150524003739/https://twitter.com/cyc_ai">stopped in 2011</a> after 15764 inane tweets in the format of “I just leaned <code>&lt;statement&gt;</code>, true or false?”. According to Internet Archive, it shut down sometime before 2017.</p>
</section>
<section id="cyc-is-done">
<h3 data-anchor-id="cyc-is-done">Cyc is done?</h3>
<p>As you might have noticed if you clicked on any of the above links, almost all the links are dead now. By checking the last known good copy of the various websites on Internet Archive, I noticed that there was a “massive extinction event” during 2014–2016, when Cycorp purged most of the open information about Cyc from the Internet. No more <a href="https://web.archive.org/web/20100526095325/http://www.opencyc.com/">OpenCyc</a>, <a href="https://web.archive.org/web/20091110010118/http://www.cyc.com/cycdoc/walkthroughs/oeintro_cats_frames_long.html">tutorials</a>, <a href="https://web.archive.org/web/20090219032021/http://www.cyc.com:80/cycdoc/ref/subl-reference.html">references</a>, <a href="https://web.archive.org/web/20090213151624/http://www.cyc.com:80/cycdoc/vocab/actor-vocab-complete.html">vocabulary lists</a>, <a href="https://web.archive.org/web/20040107124240/http://www.cyc.com/doc/handbook/oe/oe-handbook-toc-opencyc.html">The Ontological Engineer’s handbook (version 0.7)</a>…<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a> everything was gone, except marketing material. Fortunately, the Internet Archive exists, and I scraped much of the primary material from it and <a href="https://github.com/yuxi-liu-wired/cyc-archive/tree/main">uploaded them to GitHub</a> for safekeeping.</p>
<div id="fn7"><p><sup>7</sup>&nbsp;There is a funny anecdote about this book:</p>
<blockquote>
<p>I coined the phrase “ontological engineer” in the mid-1980’s, shortly after embarking on the construction of Cyc, because I didn’t like the pejorative tone of “knowledge enterer” or “knowledge worker”, and the term “knowledge engineer” had already been taken (to mean someone who builds expert systems). Based on that, when Addison-Wesley published our 1989 Cyc book (<em>Building Large Knowledge-Based Systems</em>), the editor playfully inserted a forward reference, at the front, under Other Publications, to the 1997 <em>Ontological Engineer’s Handbook</em>. Of course the joke was on us when, starting in 1997, we began to be deluged by requests for that nonexistent work.</p>
<p><span data-cites="lenatAppliedOntologyIssues2005">(<a href="#ref-lenatAppliedOntologyIssues2005" role="doc-biblioref">Lenat 2005</a>)</span></p>
</blockquote>
</div><p>Why?</p>
<p>I believe it was for commercial reasons. This mass extinction event closely corresponded to the commercialization wave in 2016, the year in which Lenat finally declared the Cyc project “done” and set about commercializing it, both via Cycorp and via Lucid.ai, a company founded in 2015.</p>
<blockquote>
<p>“Part of the reason is the doneness of Cyc,” explains Lenat, who left his post as a professor at Stanford to start the project in late 1984. “Not that there’s nothing else to do,” he says. But he notes that most of what is left to be added is relevant to a specific area of expertise, such as finance or oncology. Among other projects, [Lucid] is developing a personal assistant equipped with Cyc’s general knowledge. This could perhaps lead to something similar to Siri. … the CEO of Lucid says the new company is in talks with various others interested in using the Cyc knowledge base. Lucid has been working with the Cleveland Clinic, for example, to help automate the process of finding patients for clinical studies.</p>
<p><span data-cites="knightAI30Years2016">(<a href="#ref-knightAI30Years2016" role="doc-biblioref">Knight 2016</a>)</span></p>
</blockquote>
<p>This timing of commercialization coincides suspiciously with the end of almost all projects except ResearchCyc. Even <a href="https://web.archive.org/web/20160825080811/http://www.cyc.com/documentation/">the documentation</a> <a href="https://web.archive.org/web/20170802104610/http://www.cyc.com/documentation/">went offline</a> in 2016 and could now only be accessed for commercially registered accounts! It looks like a total internal pivot as the company focused on commercialization and shut down all services that were not high priority for the bottom line.</p>
</section>
<section id="how-expensive-was-the-lunch">
<h3 data-anchor-id="how-expensive-was-the-lunch">How expensive was the lunch?</h3>
<p>After cyberstalking the Cyc for days, I had captured every numerical datapoint that has ever dripped out of Cycorp over its entire existence, and combined them into a <a href="https://yuxi-liu-wired.github.io/essays/posts/cyc/code/Cycorp%20claims.xlsx">spreadsheet that you can analyze yourself</a>. In short, Cyc has grown to 30M assertions over the years, and is still incomplete, if completeness were measured by its original standard – a self-learning AGI. Despite Lenat’s 2016 claim that it was “done”, there is no self-learning or AGI in sight.</p>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/Cyc_progress_history.svg"></p>
<figcaption>The progress of Cyc project over the period of 1989–2022. The number of assertions grew to 30M, the cost grew to $200M, with 2000 person-years. Source: <a href="https://yuxi-liu-wired.github.io/essays/posts/cyc/code/Cycorp%20claims.xlsx">my obsessive cyberstalking</a>.</figcaption>
</figure>
</div>
<p>Looking at this diagram, we notice 3 things:</p>
<p>One, the growth of assertions is roughly exponential, doubling every 6 years. At this rate, in 2032 Cyc can expect to reach 100M assertions, the hoped-for point at which Cyc would know as much as a typical human. This might be the dream of Lenat, but I bet it will just become a slightly bigger enterprise solution to expertise management.</p>
<p>Two, the cost of human labor has remained stable throughout the existence of Cycorp, at $100K/person-year.</p>
<p>Three, The cost of assertions was $5/assertion before 2010, but $1/assertion after 2015, around the time when Lenat declared the project mostly complete. This matches the estimate of <span data-cites="paulheimHowMuchTriple2018">(<a href="#ref-paulheimHowMuchTriple2018" role="doc-biblioref">Paulheim 2018</a>)</span>. We might call this a success of “priming the knowledge pump”, or this might just because after 2015, most new assertions came from adding new assertions specialized to particular commercial applications, and thus were easier to handle. Formalizing business rules already written down is easier than extracting the intuitive metaphysics of fruits vs vegetables, after all. In any case, a drop in price by a factor of 5 over 30 years is unimpressive, and indistinguishable from more prosaic arguments about <a href="https://en.wikipedia.org/wiki/Learning_curve">learning curve</a> effects observed in most industries: The more you make, the cheaper you can make.</p>
</section>
</section>
<section id="overview-of-cyc-the-system">
<h2 data-anchor-id="overview-of-cyc-the-system">Overview of Cyc, the system</h2>
<p>The Cyc system consists of three main kinds of parts:</p>
<ol type="1">
<li>The iterations of CycL, based on SubLisp, which is itself based on Lisp.</li>
<li>The knowledge bases, consisting of millions of assertions written in CycL.</li>
<li>The ~1000 inference engines, written in SubLisp.</li>
</ol>
<p>To increase speed and compatibility, the programs written in CycL are compiled to Java, which is then compiled to bytecode.</p>
<p>The CycL language is intended to be the high-level language that humans can read and write, at the “Epistemological Level” (EL). Most inference engines do not process CycL directly, but process low-level translations of CycL at the “Heuristic Level” (HL). Each sentence at the EL can be translated into a multitude of HL sentences, since different translations allow different inference engines to process it, so that hopefully at least one of those would process it efficiently. The two sides are connected by an interface called the “Canonicalizer”.</p>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/Cyc_EL_HL.png"></p>
<figcaption>How Cyc processes a user’s input as of 1990. The user enters at the EL-KB (epistemological level knowledge base), which gets translated down to the HL, where multiple modules for generating and comparing arguments for and against a given proposition are run. <span data-cites="lenatCycMidtermReport1990">(<a href="#ref-lenatCycMidtermReport1990" role="doc-biblioref">Lenat and Guha 1990, fig. 1</a>)</span></figcaption>
</figure>
</div>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/EL_vs_HL.gif"></p>
<figcaption>The expressiveness-efficiency tradeoff. The HL is highly efficient for the machine to infer with, but very hard for humans to express what they are thinking of in. It is the opposite for the EL, and even more so for natural languages like ENglish. <a href="https://web.archive.org/web/20041225143106/http://www.cyc.com:80/cycdoc/handbook/images/elhl.gif">Source</a></figcaption>
</figure>
</div>
<section id="cycl-language">
<h3 data-anchor-id="cycl-language">CycL language</h3>
<p>Since the CycL language has had been developed since the late 1980s, and Cycorp published very little, I am basing this section mostly on <a href="https://web.archive.org/web/20040107124240/http://www.cyc.com/doc/handbook/oe/oe-handbook-toc-opencyc.html">the Ontological Engineer’s Handbook (version 0.7)</a>, which seemed to contain the most details, even though technically it only describes the 2002 version of CycL.</p>
<p>Like Common Lisp, the CycL language has almost no syntax:</p>
<ul>
<li>An <strong>expression</strong> is from one of the following types: <code>(#$relation &lt;arg1&gt; … &lt;argn&gt;)</code>, <code>?variable</code>, <code>#$Term</code>, text string, and rational number.</li>
<li>A <code>#$relation</code> is either a function or a predicate. It always begins with a lowercase.</li>
<li>If <code>#$relation</code> is a function, then <code>(#$relation &lt;arg1&gt; … &lt;argn&gt;)</code> is an <strong>object</strong>, aka a <strong>term</strong>. Most of Cyc’s knowledge base consists of assertions. Each assertion has a truth value.</li>
<li>If <code>#$relation</code> is a predicate, then <code>(#$relation &lt;arg1&gt; … &lt;argn&gt;)</code> is an <strong>assertion</strong>, aka a <strong>sentence</strong>. Most of Cyc’s knowledge base consists of assertions.</li>
<li>For a few special cases of <code>#$relation</code> such as <code>#$implies</code>, <code>#$forAll</code>, and <code>#$thereExists</code>,<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a> <code>(#$relation &lt;arg1&gt; … &lt;argn&gt;)</code> is a <strong>rule</strong> of inference.</li>
<li>Each assertion has a truth value.</li>
<li>There are 5 truth values: default true, monotonically true, default false, monotonically false, and undefined. The “default true” is used to allow for “nonmonotonic reasoning”. For example, it allows us to say that “Pengu is a bird, so Pengu can fly”, and then, if someone else adds “and Pengu is a penguin”, we can say, “in that case, Pengu cannot fly”. Since the “Pengu can fly” statement is only “default true” and “Pengu cannot fly” is only “default false”, we would not suffer from the principle of explosion.</li>
<li>A <code>#$Term</code>, aka an <strong>atomic term</strong>, aka a <strong>concept</strong>, can be thought of as a “word” in the vocabulary of CycL. It begins with an uppercase. Interpreted in object-oriented programming, some terms correspond to classes, while others correspond to objects. For example, <code>#$Socrates</code> is an object, while <code>#$Human</code> is a class. The difference is that we can say <code>(#$isa #$Socrates #$Human)</code> but not <code>(#$isa #$Socrates #$Plato)</code>. The concepts can be as abstract as <code>#$AnimalWalkingProcess</code> (the concept of any possible walking by any animal) and as granular as <code>#$Walking00036</code> (the walk I took on the afternoon of 1897-02-03 in Paris).</li>
</ul>
<div><p><sup>8</sup>&nbsp;A technical detail is that as soon as you enter an expression that uses <code>#$thereExists</code>, Cyc automatically “<a href="https://en.wikipedia.org/wiki/Skolem_normal_form">Skolemizes</a>” that expression, because existential quantifiers are technically inconvenient. For example, if I were to say “There exists a time at which Socrates dies.”, then it would be very awkward when someone asks “When?” and I have no recourse but to reply “… there exists!”. Skolemization means that I would instead define a function <code>#$DateOfDeath</code>. Then any time someone asks “When?”, I need simply reply “<code>(#$DateOfDeath #$Socrates)</code>!”.</p></div><p>This is enough for us to start writing some assertions that would imply “Socrates is mortal”.</p>
<div id="cb1"><pre><code><span id="cb1-1">(#$isa #$Socrates #$MaleHuman)</span>
<span id="cb1-2">(#$isa #$MaleHuman #$Predicate)</span>
<span id="cb1-3">(#$genls #$MaleHuman #$Human) <span>; #$genls means "generalizes"</span></span>
<span id="cb1-4">(#$genls #$MaleHuman #$MaleAnimal)</span>
<span id="cb1-5"></span>
<span id="cb1-6">(#$genls #$Person #$Individual)</span>
<span id="cb1-7">(#$isa #$Individual #$FirstOrderCollection) <span>; Things can go meta</span></span>
<span id="cb1-8">(#$isa #$FirstOrderCollection #$SecondOrderCollection)</span>
<span id="cb1-9">(#$isa #$FirstOrderCollection #$MetaClass) </span>
<span id="cb1-10">(#$isa #$MetaClass #$MetaClass) <span>; Very meta</span></span>
<span id="cb1-11"></span>
<span id="cb1-12">(#$forAll ?X</span>
<span id="cb1-13">  (#$sameAs (#$MotherFn (#$MotherFn ?X)) </span>
<span id="cb1-14">    (#$MaternalGrandMotherFn ?X)))</span>
<span id="cb1-15"></span>
<span id="cb1-16"><span>; Humans are mortal</span></span>
<span id="cb1-17">(#$isa #$Mortal #$Predicate)</span>
<span id="cb1-18">(#$implies (#$isa ?X #$Human) (#$Mortal ?X))</span>
<span id="cb1-19"></span>
<span id="cb1-20"><span>; For X to be mortal means there exists a death event X is subject to</span></span>
<span id="cb1-21">(#$implies</span>
<span id="cb1-22">  (#$Mortal ?X)</span>
<span id="cb1-23">  (#$exists ?DE</span>
<span id="cb1-24">    (#$and</span>
<span id="cb1-25">      (#$isa ?DE #$DeathEvent)</span>
<span id="cb1-26">      (#$subject ?DE ?X))))</span></code></pre></div>
<p>Assertions can be packaged into a <strong>microtheory</strong> (Mt, aka a <strong>context</strong>), which could be thought of as <a href="https://en.wikipedia.org/wiki/Scope_(computer_science)">scopes</a>, or <a href="https://en.wikipedia.org/wiki/Module_(programming)">modules</a>. Each assertion can belong to exactly one microtheory, though you can have two assertions that are literally the same occur in two microtheories. For example, “Socrates is alive.” is true in the context of 500 BC, but not in the context of 1995. For a single problem-and-answer session with a user, Cyc typically sets up a temporary context that is deleted after the session.</p>
<p>Microtheories are necessary because human beliefs are incompatible, and there are a lot of humans. For example (very relevant, considering how much Cyc was involved with the War on Terror), <code>#$ChristianMt</code> and <code>#$IslamMt</code> could have:</p>
<div id="cb2"><pre><code><span id="cb2-1">(#$isa #$ChristianMt #$Microtheory)</span>
<span id="cb2-2">(#$isa #$IslamMt #$Microtheory)</span>
<span id="cb2-3"></span>
<span id="cb2-4"><span>; #$ist means "is true in the context of"</span></span>
<span id="cb2-5">(#$ist (#$ChristianMt)</span>
<span id="cb2-6">  (#$not (#$sameAs #$God #$Allah)))</span>
<span id="cb2-7">(#$ist (#$ChristianMt)</span>
<span id="cb2-8">  (#$sonOf #$Jesus #$God))</span>
<span id="cb2-9"><span>; the Trinity is left as an exercise</span></span>
<span id="cb2-10"></span>
<span id="cb2-11">(#$ist (#$IslamMt)</span>
<span id="cb2-12">  (#$sameAs #$God #$Allah))</span>
<span id="cb2-13"><span>; In Islam, God has no son</span></span>
<span id="cb2-14">(#$ist (#$IslamMt)</span>
<span id="cb2-15">  (#$not (#$sonOf ?X #$God)))</span>
<span id="cb2-16">(#$ist (#$IslamMt)</span>
<span id="cb2-17">  (#$prophetOf #$Jesus #$God))</span></code></pre></div>
<p>Microtheories can contain each other. For example, both are <code>#$AbrahamicMt</code>, which allows us to make assertions that apply to both.</p>
<div id="cb3"><pre><code><span id="cb3-1">(#$isa #$AbrahamicMt #$Microtheory)</span>
<span id="cb3-2">(#$genls #$ChristianMt #$AbrahamicMt)</span>
<span id="cb3-3">(#$genls #$IslamMt #$AbrahamicMt)</span>
<span id="cb3-4"></span>
<span id="cb3-5"><span>; Abraham is a prophet</span></span>
<span id="cb3-6">(#$ist (#$AbrahamicMt)</span>
<span id="cb3-7">    (#$prophetOf #$Abraham #$God))</span>
<span id="cb3-8"></span>
<span id="cb3-9"><span>; God exists uniquely</span></span>
<span id="cb3-10">(#$ist (#$AbrahamicMt)</span>
<span id="cb3-11">  (#$exists ?X</span>
<span id="cb3-12">    (#$and</span>
<span id="cb3-13">      (#$isa ?X #$God)</span>
<span id="cb3-14">      (#$not</span>
<span id="cb3-15">        (#$exists ?Y</span>
<span id="cb3-16">          (#$and</span>
<span id="cb3-17">            (#$isa ?Y #$God)</span>
<span id="cb3-18">            (#$not</span>
<span id="cb3-19">              (#$sameAs ?X ?Y))))))))</span></code></pre></div>
<p>As of 2010, there were over 20K microtheories arranged in a hierarchy, with <code>BaseKB</code> at the top. It was unfortunately a mess, as Lenat promised. Some microtheories were 50 levels deep down the hierarchy!</p>
<blockquote>
<p>… determining the hierarchical structure of the Mt’s is difficult, manual, and error prone because all of the Mt’s in ResearchCyc are direct subtypes of <code>BaseKB</code>, even if they are also its indirect subtypes. For example, Mt <code>ClothingGMt</code> describes general information about clothing and is a direct subtype of <code>BaseKB</code>. It is also an indirect subtype of <code>BaseKB</code> because it is a subtype of <code>ArtifactGMt</code>, also a subtype of <code>ArtifactGVocabularyMt</code>, which, in turn, is a subtype of <code>BaseKB</code>… Choosing the right microtheory can easily take several minutes for a trained ontologist who is familiar with the Cyc ontology and experienced in how best to organize knowledge for maximum utility.</p>
<p><span data-cites="conesaUsabilityUpperLevel2010">(<a href="#ref-conesaUsabilityUpperLevel2010" role="doc-biblioref">Conesa, Storey, and Sugumaran 2010</a>)</span></p>
</blockquote>
</section>
<section id="ontology">
<h3 data-anchor-id="ontology">Ontology</h3>
<p>The Ontology of Cyc is the graph of all concepts in Cyc, with one directed edge per <code>(#$genls #$Thing1 #$Thing2)</code> assertion. At least, that’s the simplest possible way to say it. In fact, the graph is more complicated, since CycL is a higher-order logic, which allows it to talk about the relation between relations between predicates, etc. For example, both <code>#$SetOrCollection</code> and <code>#$MathematicalObject</code> are sub-concepts of <code>#$MathematicalThing</code>, but we also need to specify that any <code>#$MathematicalThing</code> is either a <code>#$SetOrCollection</code> xor a <code>#$MathematicalObject</code>.</p>
<p>The ontology contains multiple levels. At the upper level are the most metaphysical concepts, starting with <code>#$Thing</code>, going down to the middle level of <code>#$Language</code> and <code>#$MilitaryOrganization</code>.</p>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/Cyc_Upper_Ontology_2010.png"></p>
<figcaption>The upper ontology of Cyc as of 2010. <span data-cites="foxvogCyc2010">(<a href="#ref-foxvogCyc2010" role="doc-biblioref">Foxvog 2010, fig. 1</a>)</span></figcaption>
</figure>
</div>
<p>Below the upper ontology are domain-specific knowledge, divided into large microtheories (minitheories?). There would be knowledge about mortgages, computer security, weapons systems, pathology, etc. Below those are the domain-specific facts and data, divided into microtheories. The entire thing is structured like a pyramid.</p>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/Cyc_ontology_pyramid.png"></p>
<figcaption>The cyc ontology pyramid.</figcaption>
</figure>
</div>
<p>As an example, here is how <code>#$Philosopher</code> is described in <a href="https://yuxi-liu-wired.github.io/essays/posts/cyc/code/opencyc-ontology.txt">OpenCyc</a>:</p>
<blockquote>
<p>A specialization of <code>#$Person</code>; in the context of <code>#$HumanActivitiesMt</code> this collection is an instance of <code>#$PersonTypeByActivity</code>, in the context of <code>#$JobMt</code> it is an instance of <code>#$PersonTypeByOccupation</code>. Each instance of <code>#$Philosopher</code> is a person who habitually thinks about philosophical matters such as what is or might be, what we can know, how we can know anything, etc. In the contemporary era most philosophers are academics or professionals, but a significant number (now and historically) don’t fit this profile.</p>
</blockquote>
<p>And here’s the <code>#$Thing</code>:</p>
<blockquote>
<p><code>#$Thing</code> is the “universal collection”: the collection which, by definition, contains everything there is. Every thing in the Cyc ontology – every <code>#$Individual</code> (of any kind), every <code>#$Set-Mathematical</code>, and every <code>#$Collection</code> – is an instance of (see <code>#$isa</code>) <code>#$Thing</code>. Similarly, every collection is a subcollection of (see <code>#$genls</code>) <code>#$Thing</code>. Trivially, <code>#$Thing</code> is both an instance of and a subcollection of itself, and is not a subcollection of any other collection. (Note that the above reference to “every thing in the Cyc ontology” is <em>not</em> meant to be limited to things actually <em>reified</em> in the Cyc system, but includes (e.g.) every instance – reified or not, known or not – of every collection recognized by Cyc.)</p>
</blockquote>
<p>And if the parenthetical note sounds a bit theological, note that among those that Cycorp had hired included philosophers, botanists, chemists, and of course, theologians. Interestingly, they didn’t ask botanists to encode what they know about botany, but about what they know about non-botany. Lenat’s theory was that botanists’ understanding of botany is <em>not</em> commonsensical. Instead, what botany he wanted Cyc to encode is common sense botany: how non-botanists think about plants – even, and especially, those botanical beliefs that a botanist would consider wrong, such as <code>(#$not (#$isa #$Banana #$BotanicalBerry))</code>.</p>
<p>Unfortunately for ontology, while everyone has a common sense, few can pull it out of their heads and push it into a computer. Let’s consider how Cyc encodes events, which Cyc calls “<a href="https://web.archive.org/web/20040310001317/http://www.cyc.com/doc/handbook/oe/08-cycl-representation-choices.html">Davidsonian semantics</a>”, since it’s how <a href="https://en.wikipedia.org/wiki/Donald_Davidson_(philosopher)">Donald Davidson</a> represented <a href="https://en.wikipedia.org/wiki/Event_(philosophy)">events</a> – yes, Cyc hired a lot of philosophy PhDs.</p>
<p>To represent “John gave Mary a book.”, you can write something like <code>(#$Give #$John #$Mary #$Book1)</code>. The problem is that this prevents you from adding more details, because for the assertion to be syntactically correct, we must make <code>#$Give</code> with arity exactly 3. And then we’d be stuck if we wanted to write “John gave Mary a book yesterday.”. And even if we redefine <code>#$Give</code> to have arity 4, what if you <em>also</em> want to say it happened in the library, or that it was a gift, or that John was happy about it? You’d have to change the definition of the predicate <code>#$Give</code> again and again, and there is no end to this.</p>
<p>To solve this problem, Cyc treats the event itself as an <code>#$Event</code>. That is, it reifies the process as an object. Now, instead of cramming everything into one assertion, you can construct an endless sequence of assertions, limited only by your patience:</p>
<div id="cb4"><pre><code><span id="cb4-1">(#$isa #$Event123 #$Event)</span>
<span id="cb4-2">(#$Sender #$Event123 #$John)</span>
<span id="cb4-3">(#$Receiver #$Event123 #$Mary)</span>
<span id="cb4-4">(#$GivenObject #$Event123 #$Book134)</span>
<span id="cb4-5">...</span></code></pre></div>
<p>Such problems are fairly subtle, and the world is a very big place. Fortunately, Lenat was a master of ontology, so it all worked out in the end.</p>
<blockquote>
<p>… during a short stint working with Doug Lenat’s Cyc project. At the time, they were trying to encode all of botany and had a small staff of professional botanists doing knowledge entry. Naturally it was quite difficult for the botanists to try to translate their knowledge into the formalisms required by Cyc, and they would regularly puzzle over various questions… and if they could not come to a consensus, would have to take it before the Master, Doug Lenat, who would think for a bit, maybe draw some diagrams on a whiteboard, and come up with the Right Representation.</p>
<p>— <a href="https://hyperphor.com/ammdi/alpha-ontologist">AMMDI: alpha ontologist</a> (2023-10-07)</p>
</blockquote>
</section>
<section id="inference-engines">
<h3 data-anchor-id="inference-engines">Inference engines</h3>
<p>To a mathematician, 1 and 1 trillion are the same – both are finite. To computer scientists, even <span>\(x^2\)</span> and <span>\(x^3\)</span> are different. Much work on Cyc was not on building the 100M-assertion knowledge base, but on building inference engines that allow fast inferences when there are 100M assertions to pick from.</p>
<p>Every logical system must face an impossible trilemma:</p>
<ul>
<li>an <strong>expressive</strong> language that can represent what people would ever want to say in practice;</li>
<li>an <strong>efficient</strong> inference engine on the language that runs fast enough for practical inferences;</li>
<li>a <strong>complete</strong> inference engine that can perform all inferences that are logically valid.</li>
</ul>
<p>Why a trilemma?</p>
<p>Suppose we want to allow Cyc express all common sense assertions, then since humans in their daily life say high-order statements like “Are you implying that you <em>meant</em> to make me upset by spreading rumors about her, when you had known all along that I would soon hear about it?”, Cyc needs to use a higher-order language. Now this immediately makes it impossible to have an inference engine that is both complete and computable, since we have a Gödel-style incompleteness theorem <span data-cites="shapiroFoundationsFoundationalismCase1991">(<a href="#ref-shapiroFoundationsFoundationalismCase1991" role="doc-biblioref">Shapiro 1991</a>, theorem 4.14)</span>.</p>
<p>Lenat chose to use a fully higher-order language, giving up completeness, and then try his best improving efficiency.</p>
<p>Like most expert systems, the Cyc has a general <a href="https://en.wikipedia.org/wiki/Resolution_(logic)">resolution</a>-based inference engine. Unlike most expert systems, its knowledge base is large and higher-ordered, so its general engine runs too slowly for most queries. Thus, the developers kept adding more specialized modules (“pattern-specific heuristic modules”), each capable of efficiently inferring on a few microtheories. If a specialized engine fails to make progress, a more general engine can be the fall-back, all the way up to the most general one.</p>
<p>As the simplest example of how inference engine can work, consider the following example of backward-chaining inference, in a semantic web. The system is asked basically “Why does Clyde want to possess a crescent wrench?”, and it eventually replies “Because Clyde has not eaten lately.”. Lenat expected that the ontology of Cyc would eventually power generally intelligent agents, which would use forward-chaining to construct goals from current states, and use backward-chaining to explain why others have their goals.</p>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/Cyc_triple_reasoning.png"></p>
<figcaption>A sketch of how to reason by backward-chaining together triples in a semantic web. Figure from <span data-cites="wallichSiliconBabies1991">(<a href="#ref-wallichSiliconBabies1991" role="doc-biblioref">Wallich 1991</a>)</span>. I can’t help but wonder if Lenat meant for a subtle joke against the <em>Elephants don’t play chess</em> by <span data-cites="brooksElephantsDonPlay1990">(<a href="#ref-brooksElephantsDonPlay1990" role="doc-biblioref">Brooks 1990</a>)</span> by adding in a “dead end” branch that ends up concluding Clyde is an elephant.</figcaption>
</figure>
</div>
<p>Most inference engines are highly specialized. They can only reason within a few microtheories, but very well. Here is a toy example for reasoning with mutually exclusive categories. We can define <code>#$Mutex</code> as</p>
<div id="cb5"><pre><code><span id="cb5-1">(#$implies</span>
<span id="cb5-2">  (#$and (#$isa ?X ?A) (#$Mutex ?A ?B))</span>
<span id="cb5-3">         (#$not (#$isa ?X ?B)))</span></code></pre></div>
<p>Then this allows the general reasoning engine to reason about mutual exclusivity by always reducing to this definition. However, we can accelerate this by adding in some “lemmas”:</p>
<div id="cb6"><pre><code><span id="cb6-1">(#$implies (#$Mutex ?A ?B) (#$Mutex ?B ?A))</span>
<span id="cb6-2">(#$implies</span>
<span id="cb6-3">  (#$and (#$Mutex ?A ?B) (#$genls ?C ?A))</span>
<span id="cb6-4">  (#$Mutex ?C ?B))</span>
<span id="cb6-5">(#$implies</span>
<span id="cb6-6">  (#$and (#$Mutex ?A ?B) (#$isa ?X ?A))</span>
<span id="cb6-7">  (#$not (#$isa ?X ?B)))</span></code></pre></div>
<p>After producing enough lemmas, we can then write a specialized inference engine that would be invoked whenever <code>#$Mutex</code> appears in an expression, and it would attempt some inference and simplifications by applying these lemmas. Indeed, most inference engines were constructed in this way: Cyc would try to solve a problem, and fails by timing out. The ontologists at Cyc would call up a human expert and ask, “How did you do this?” and the expert would explain how they would solve it with quick rules of thumb, which the ontologists would write into Cyc, resulting in more assertions, and possibly more inference engines. This is essentially the same as “knowledge elicitation” used for making expert systems.</p>
<p>Other than these inference engines (or “workers”), there are also many “tacticians”, modules that pick engines that are probably good for solving a problem, and a general all-powerful strategist at the very top. It was reported in that there were 1 strategist, 4 tacticians, and 1097 workers. The strategist and tacticians have parameters that can vary, presumably by the user or the ontologists in order to adapt to specific tasks. <span data-cites="lenatEfficientPathfindingVery2007">(<a href="#ref-lenatEfficientPathfindingVery2007" role="doc-biblioref">Lenat et al. 2007</a>)</span></p>
<p>The control structure of Cyc is a commercial secret.<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a> I could only find two brief explanations of it, one in a <a href="https://web.archive.org/web/20130708080327/http://www.cyc.com/sites/default/files/storage/white-papers/The%20Cyc%20Blackboard%20System%20v1.0.pdf">2010 Cycorp white paper</a>, another in an essay published near the end of his life <span data-cites="lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">Lenat 2022a</a>)</span>. According to these, Cyc coordinates the ~1000 inference engines with blackboard architecture similar to the one used in Hearsay-II speech recognition system <span data-cites="ermanHearsayIISpeechUnderstandingSystem1980">(<a href="#ref-ermanHearsayIISpeechUnderstandingSystem1980" role="doc-biblioref">Erman et al. 1980</a>)</span>. Lenat described it as:</p>
<div><p><sup>9</sup>&nbsp;<a href="https://web.archive.org/web/20130707133552/http://www.cyc.com/documentation/control-structure">The SubLisp reference</a> was titled “control structure”, and got me excited for a moment. But I was disappointed to find that it described the control structure of the SubLisp programming language (mostly the same as that of Common Lisp), <em>not</em> of the Cyc system.</p></div><blockquote>
<p>there is a whole battery of specialized inference engines and representations… and, when making progress, broadcasting the results… Whenever progress is made, all of them stop and work on the now-simpler subproblem. Some of the inference engines are very general, and work on general representations–e.g., a theorem prover that works on first-order logic. The more specialized inference engines are much faster whenever they do apply… In 1986, Cyc had two such representations; by 1990, there were 20, each with its own inference engine; today Cyc has over 1100. They work together as a community of agents, communicating by posting their intermediate results on a sort of blackboard that all the other agents can watch and react to if/when/as they see an opportunity that fits their specialty.</p>
<p><span data-cites="lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">Lenat 2022a</a>)</span></p>
</blockquote>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/Blackboard_architecture.png"></p>
<figcaption>The blackboard architecture used in Hearsay-II. In this example, a 3-dimensional blackboard allows different modules to read and write to different parts of the blackboard, and collaboratively recognize speech. <span data-cites="lenatComputerSoftwareIntelligent1984">(<a href="#ref-lenatComputerSoftwareIntelligent1984" role="doc-biblioref">Lenat 1984</a>)</span></figcaption>
</figure>
</div>
<p>You can imagine a shared working space – the blackboard – where agents can scan and find tasks to do, do them, and then post the outputs to the blackboard. Each agent has their own “places to watch”. They notice (or get notified) what is posted to their watched areas, and ignore what is outside.<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a> When an agent completes whatever task it is running, it posts the outputs to particular areas of the board to notify certain agents, “Hey, check this out! I bet you’ll find this helpful.”. To prevent race conditions or general conflict between agents, an agent can claim a lock on something, so that other agents can’t work on it until it times out, or releases the lock.</p>
<div><p><sup>10</sup>&nbsp;Lenat seemed to suggest that each inference engine watches <em>the entire board</em>, but every large-scale blackboard system I know of doesn’t do that. Hearsay-II certainly did not. It is slow and creates much of unintended complexities. Imagine a microservice where every process pings every process whenever it outputs something, or a conference where everyone broadcasts to everyone.</p></div><p>You can imagine this as a <a href="https://en.wikipedia.org/wiki/Microservices">microservices framework</a>, with over 1100 microservices, <a href="https://en.wikipedia.org/wiki/Multicast">multicast</a>, with SubLisp as the <a href="https://en.wikipedia.org/wiki/Serialization">serialization language</a>. Indeed, <a href="https://www.planettechnews.com/planettech-interviews-michael-stewart-founder-chairman-and-ceo-of-lucid-ai/">apparently</a> some of the inference engines used neural networks, so their numerical outputs <em>definitely</em> had to be serialized somehow into SubLisp.</p>
<p>The blackboard is also connected to the “external world” through API calls. For example, SQL queries run by an external database can return data that is posted to a region of the blackboard, and what Cyc writes to the blackboard can be read out for external use.</p>
<p>According to a <a href="https://www.youtube.com/watch?v=KSrUHGaUE_c">2006 lecture</a>, the blackboard has 12 “dimensions”, because each assertion occurs in a kind of context, and there are 12 dimensions to the context: <code>Anthropacity</code>, <code>Time</code>, <code>GeoLocation</code>, <code>TypeOfPlace</code>, <code>TypeOfTime</code>, <code>Culture</code>, <code>Sophistication/Security</code>, <code>TopicGranularity</code>, <code>Modality/Disposition/Epistemology</code>, <code>Argument-Preference</code>, and <code>Justification</code>. For example, “Ronald Reagan is president” is true in the context <code>Time = 1985, GeoLocation = UnitedStates</code>.</p>
<p>Other than the slightly anarchic microservice structure, Cyc could also use <a href="https://www.cs.umd.edu/projects/shop/">Simple Hierarchical Ordered Planner</a> to run the engines in order, if the workflow for how the engines should be run is known.</p>
<p>It was known very early on that the most general inference engine is the slowest, which is why they settled on a multi-layered structure of inference engines. Each inference is handled by the lowest engines first, and upon failing, the higher-levels try it.</p>
<p>Further, to allow the tacticians know who to call, and the engines to know whether it is powerful enough to handle the query, the user can specify in a query over 150 adjustable parameters, such as max time limit, max number of answers desired, max number of backward-chaining steps, whether to introduce new terms, etc. In 2007, they found 6 combinations of these parameters that could answer almost all queries they tested with, which allowed them to clean up the user interface. Not a form with 150 blanks, but just a pick-1-in-6. <span data-cites="lenatACS2022Invited2022">(<a href="#ref-lenatACS2022Invited2022" role="doc-biblioref">Lenat 2022b</a>)</span></p>
<p>Though there is a most general inference engine on the very top, they had noticed by 2007 that was so slow that turning it off made the system go <em>faster</em>! So they turned it off entirely in 2010. <span data-cites="lenatACS2022Invited2022 lenatGettingGenerativeAI2023">(<a href="#ref-lenatACS2022Invited2022" role="doc-biblioref">Lenat 2022b</a>; <a href="#ref-lenatGettingGenerativeAI2023" role="doc-biblioref">Lenat and Marcus 2023</a>)</span></p>
<p>The heaven is high and the emperor is far away. The monologic is dead. Long live the swarm.</p>
</section>
<section id="querying">
<h3 data-anchor-id="querying">Querying</h3>
<p>As a user, the knowledge base would be worth nothing if you cannot ask it questions and receive answers. The Cyc system, like most logical programming systems, answers queries by <a href="https://en.wikipedia.org/wiki/Unification_(computer_science)">logical unification</a>. That is, it converts a query into a logical formula <span>\(\phi(x)\)</span> with a certain unknown <span>\(x\)</span>, then returns all <span>\(x\)</span> that makes <span>\(\phi(x)\)</span> true, or if no such <span>\(x\)</span> exists, then returns <code>FALSE</code>.</p>
<p>For a query like</p>
<div id="cb7"><pre><code><span id="cb7-1">(#$geopoliticalSubdivision #$Canada ?WHAT)</span></code></pre></div>
<p>the Cyc system applies various inference engines to find all bindings (aka substitutions) of the variable <code>?WHAT</code> that make the statement true in the knowledge base. The results will include all Canadian provinces and territories that have been asserted explicitly or can be inferred through rules.</p>
<p>Cyc provides 3 types of queries:</p>
<ul>
<li><code>ASK</code>: General-purpose queries that generate bindings for free variables along with a formal proof.</li>
<li><code>PROVE</code>: Conditional queries that handle universal quantification (<code>#$forAll</code>) by constructing temporary microtheories for temporary hypotheticals.</li>
<li><code>QUERY</code>: A wrapper around both <code>ASK</code> and <code>PROVE</code> in the Cyc Browser.</li>
</ul>
<p>To illustrate how querying works in practice, consider a scenario where we want to find all Canadian provinces. An <code>ASK</code> query would look like:</p>
<div id="cb8"><pre><code><span id="cb8-1">(#$isa ?WHAT #$CanadianProvince)</span></code></pre></div>
<p>Cyc processes this by looking for all terms that satisfy the predicate. The system might use transitivity rules, inheritance mechanisms, and various specialized inference engines to gather results. Behind the scenes, Cyc might employ backward-chaining to start from the query and work backward to known facts, or forward-chaining to derive new facts from existing ones, depending on which approach its tacticians determine is more efficient. The reply would be something like</p>
<div id="cb9"><pre><code><span id="cb9-1">((?WHAT . #$Manitoba-CanadianProvince))</span>
<span id="cb9-2">((?WHAT . #$BritishColumbia-CanadianProvince))</span>
<span id="cb9-3">((?WHAT . #$Alberta-CanadianProvince))</span>
<span id="cb9-4">((?WHAT . #$Ontario-CanadianProvince))</span>
<span id="cb9-5">((?WHAT . #$Quebec-CanadianProvince))</span>
<span id="cb9-6">((?WHAT . #$NovaScotia-CanadianProvince))</span>
<span id="cb9-7">...</span></code></pre></div>
<p>When Cyc performs a query, it can also provide a deductive trace (forma proof) showing how it reached its conclusion. For example, when proving that Manitoba is a Canadian province, the deductive trace might look like:</p>
<div id="cb10"><pre><code><span id="cb10-1">lispCopyQuery: (#$isa #$Manitoba-CanadianProvince #$CanadianProvince)</span>
<span id="cb10-2">  <span>1.</span> (#$isa #$Manitoba-CanadianProvince #$CanadianProvince)</span>
<span id="cb10-3">     [Direct assertion in PoliticalGeographyMt]</span>
<span id="cb10-4">  <span>2.</span> Therefore, Manitoba-CanadianProvince is a CanadianProvince.</span>
<span id="cb10-5">     [TRUE]</span>
<span id="cb10-6"></span>
<span id="cb10-7">Query: (#$geopoliticalSubdivision #$Canada #$Manitoba-CanadianProvince)</span>
<span id="cb10-8">  <span>1.</span> (isa Manitoba-CanadianProvince CanadianProvince)</span>
<span id="cb10-9">     [Established above]</span>
<span id="cb10-10">  <span>2.</span> (implies </span>
<span id="cb10-11">      (#$isa ?X #$CanadianProvince)</span>
<span id="cb10-12">      (geopoliticalSubdivision Canada ?X))</span>
<span id="cb10-13">     [Rule in PoliticalGMt]</span>
<span id="cb10-14">  <span>3.</span> Therefore, (#$geopoliticalSubdivision #$Canada #$Manitoba-CanadianProvince)</span>
<span id="cb10-15">     [Modus Ponens: <span>1</span>,<span>2</span>]</span>
<span id="cb10-16">     [TRUE]</span></code></pre></div>
<p>For more complex queries involving logical implications, <code>PROVE</code> creates hypothetical microtheories to test universal claims. Such microtheories are useful, since the hypotheticals being considered are usually temporary and should not be mixed with the permanent assertions in the knowledge base. For example, to ask “Is every Canadian province a political subdivision of Canada?”, we use:</p>
<div id="cb11"><pre><code><span id="cb11-1">(#$implies</span>
<span id="cb11-2">  (#$isa ?WHAT #$CanadianProvince)</span>
<span id="cb11-3">  (#$geopoliticalSubdivision #$Canada ?WHAT))</span></code></pre></div>
<p><code>PROVE</code> handles this by creating a temporary “possible world” where it assumes the existence of a generic Canadian province and tests whether it must also be a political subdivision of Canada.</p>
<p>As previously stated, each time the user begins a question-and-answer session with the Cyc system, it creates a new microtheory just for this session, which allows it to keep track of what has been asked and responded to, so that the user can refer to things like “your previous answer”. Such a microtheory is usually deleted after the session is over, though it can also be saved if the user wishes to continue the session at a later date.</p>
<p>The querying interface also provides controls for managing the inference process, such as limiting search depth, maximal time allowed, the inference engines to be used, the parameters for the strategist and the tacticians etc. These can help the Cyc system to solve difficult queries when the default settings are insufficient.</p>
</section>
<section id="natural-language-processing">
<h3 data-anchor-id="natural-language-processing">Natural language processing</h3>
<p>When it comes to the use of natural languages, there are two problems. The easy problem is to convert sentences in CycL into natural language. This is fairly simple and has been done since around late 1990s, simply by constructing assertions between a concept and a concept of the English word for the concept. <span data-cites="guhaReCycLingPaper1993">(<a href="#ref-guhaReCycLingPaper1993" role="doc-biblioref">Guha and Lenat 1993</a>)</span> Concretely, it involves entering sentences of the form (simplified):</p>
<div id="cb12"><pre><code><span id="cb12-1">(#$rootString #$TheEnglishWordPen <span>"pen"</span>)</span>
<span id="cb12-2">(#$denotation #$TheEnglishWordPen #$WritingPen)</span>
<span id="cb12-3">(#$rootString #$TheFrenchWordPlume <span>"plume"</span>)</span>
<span id="cb12-4">(#$denotation #$TheFrenchWordPlume #$WritingPen)</span>
<span id="cb12-5">...</span></code></pre></div>
<p>and similarly for ordering the words according to syntax and grammar.</p>
<p>Sure, such generated language will sound a bit wooden and formalistic, and would not win any literary award, but it solves the problem. The hard problem is to parse sentences in natural language into CycL, along with all its messiness.</p>
<p>Information about how Cyc solves the hard problem is sparse, but from what I gathered, it uses a tiered system from the fast and inaccurate to the slow and accurate.</p>
<p>Underlying all of its NLP is an English dictionary, also represented as concepts and assertions in CycL. The dictionary contains about 200K words and phrases, and more assertions about them. For example, the word “light” is represented as a concept <code>Light-theWord</code>, and there would be assertions stating that it can be a verb, or a noun, or an adjective, etc.</p>
<p>At the fastest tier are keyword matching, or concept spotting. For example, for parsing terrorism articles, it can just quickly match for the existence of keywords in a sentence. Seeing a sentence that looks like “… al-Qaeda … embassy … grenade … suicide attack…”, the system can assume, with high probability, what the sentence means, and generate its CycL representation.</p>
<p>At a slower tier is using extraction templates. For example, “[A] was involved in kidnapping [B]” would be matched to a template that looks for the fragment “involved in kidnapping”, which then would parse [A] as the <code>Perpetrator</code> while [B] as the <code>Victim</code>.</p>
<p>At a slower tier is example-based machine translation by syntax templates. For example, in “… the heart of Baghdad …”, the first pass parses Badhdad as a city, then a syntax template activates and it parses the “heart” as <code>Downtown</code>. This can be coded as</p>
<div id="cb13"><pre><code><span id="cb13-1">&lt;<span>template</span>&gt;</span>
<span id="cb13-2">    &lt;<span>nlPattern</span><span> class=</span><span>"140080"</span>&gt;the heart of $City#0&lt;/<span>nlPattern</span>&gt;</span>
<span id="cb13-3">    &lt;<span>cyclPattern</span>&gt;</span>
<span id="cb13-4">        (#$equalSymbols ?D (#$DowntownFn $City#0))</span>
<span id="cb13-5">    &lt;/<span>cyclPattern</span>&gt;</span>
<span id="cb13-6">    &lt;<span>variable</span>&gt;?D&lt;/<span>variable</span>&gt;</span>
<span id="cb13-7">    &lt;<span>type</span>&gt;#$Downtown&lt;/<span>type</span>&gt;</span>
<span id="cb13-8">&lt;/<span>template</span>&gt;</span></code></pre></div>
<p>At the slowest tier is <a href="https://web.archive.org/web/20130511232427/http://www.cyc.com/cyc/nl">full syntax tree parsing</a>. In this tier, the sentence is fully parsed to a syntax tree using a manually written transformational grammar under the <a href="https://en.wikipedia.org/wiki/Government_and_binding_theory">government and binding theory</a>. It is then parsed semantically using <a href="https://en.wikipedia.org/wiki/Montague_grammar">Montague grammar</a>.</p>
<p>For translating CycL back to English, Cyc uses a simple generative grammar template. For example, <code>(#$genls #$Dog #$Mammal)</code> would be transformed to “Dogs are mammals.”, and so on. This is substantially easier than translating English to CycL. Whereas the CycL-to-English part is already workable mostly in 2001, the English-to-CycL part is still ongoing work, a consummation devoutly to be wished. Even though Lenat originally thought natural language understanding would be finished soon after priming the knowledge pump, it has already been 8 years since the priming, and Cyc is still not reading the world’s writings and learning autonomously.</p>
</section>
<section id="machine-learning">
<h3 data-anchor-id="machine-learning">Machine learning</h3>
<p>Cyc has used statistical and machine learning methods in minor parts, such as using neural networks, n-gram methods, random forest, support vector machines, etc, to automatically extract templates for natural language processing, classify the microtheories that a sentence belongs to, etc.</p>
<p>However, none of these were at all what Lenat meant when he talked of “machine learning”, by which he meant a Cyc machine, with knowledge pump fully primed, would begin to perform experiments and learn by automated discovery, much like AM and EURISKO were meant to do. While this has always been the “phase 3” of the Cyc project, to this day, Cyc has made no progress in this area.</p>
</section>
</section>
<section id="cycops-what-is-it-good-for">
<h2 data-anchor-id="cycops-what-is-it-good-for">CycOps, what is it good for?</h2>
<section id="overview">
<h3 data-anchor-id="overview">Overview</h3>
<p>In America, at least before the Deep Learning era, the military funded most large-scale efforts in AI, and that includes the Cyc project. Indeed, the Cyc project initiated in 1984-07 under the <a href="https://en.wikipedia.org/wiki/Microelectronics_and_Computer_Technology_Corporation">Microelectronics and Computer Consortium</a> (MCC), which, like the Strategic Computing Initiative,<!-- todo: link to SCI --> was formed in reaction to the threat of the Japanese <a href="https://en.wikipedia.org/wiki/Fifth_Generation_Computer_Systems">Fifth Generation Computer Systems</a> project. Though not <em>directly</em> funded by the government, its head was <a href="https://en.wikipedia.org/wiki/Bobby_Ray_Inman">Bobby Inman</a>, who had previously held high positions in the Navy, the NSA, and the CIA, so draw your own conclusion.</p>
<p>In 1995-01, as MCC wound down, the Cyc group formed Cycorp Inc., a for-profit company, to continue their mission. Immediately after that point, academic publication and generally candid conversation almost ceased. Who bought the services of Cyc, and for what? The details are slim. Trade secrets, no doubt. Confirmed results:<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<div><p><sup>11</sup>&nbsp;I swear I’m not a policy wonk, but 3 days of cyberstalking does take its toll.</p></div><ul>
<li><a href="https://en.wikipedia.org/wiki/Lycos">Lycos search engine</a>, to disambiguate search terms. It ended in 2001. (<a href="https://web.archive.org/web/20150905165226/http://www.cyc.com/about/media-coverage/computer-save-world/">Source</a>)</li>
<li>Cleveland Clinic, starting in 2007, to parse clinician queries into database queries and returns the result. <span data-cites="lenatHarnessingCycAnswer2010 pierceSemanticDBSemanticWeb2012">(<a href="#ref-lenatHarnessingCycAnswer2010" role="doc-biblioref">Lenat et al. 2010</a>; <a href="#ref-pierceSemanticDBSemanticWeb2012" role="doc-biblioref">Pierce et al. 2012</a>)</span></li>
<li>Department of Defense, in 2001, to “clean dataset”. <span data-cites="thompsonKnowItAllMachine2001">(<a href="#ref-thompsonKnowItAllMachine2001" role="doc-biblioref">Thompson 2001</a>)</span></li>
<li>GlaxoSmithKline, in 2001, to “clean dataset” <span data-cites="thompsonKnowItAllMachine2001">(<a href="#ref-thompsonKnowItAllMachine2001" role="doc-biblioref">Thompson 2001</a>)</span>, probably meaning to manage a thesaurus of pharmaceutical chemicals. (<a href="https://web.archive.org/web/20130720051038/http://www.cyc.com/enterprise-solutions/success-stories/pharmaceutical-thesaurus-management">Source</a>).</li>
<li>Goldman Sachs, sometime around 2016, to “monitor the inner workings of its technological infrastructure” and detect insider trading. <span data-cites="metzOneGeniusLonely2016 shilohHeTaughtAI2023">(<a href="#ref-metzOneGeniusLonely2016" role="doc-biblioref">Metz 2016</a>; <a href="#ref-shilohHeTaughtAI2023" role="doc-biblioref">Shiloh 2023</a>)</span></li>
<li><a href="https://web.archive.org/web/20101224222259/http://cyc.com/cyc/applications/CycSecure/">CycSecure</a>, a network vulnerability assessment tool, first beta in 2002. <span data-cites="anthesCycUse2002">(<a href="#ref-anthesCycUse2002" role="doc-biblioref">Anthes 2002a</a>)</span> Trialed at the US Strategic Command Computer Emergency Response Team at some unknown point before 2005. <span data-cites="shepardKnowledgebasedApproachNetwork2005">(<a href="#ref-shepardKnowledgebasedApproachNetwork2005" role="doc-biblioref">Shepard et al. 2005</a>)</span></li>
<li>The CIA and the Department of Defense, probably to identify terrorist threats. <span data-cites="shilohHeTaughtAI2023">(<a href="#ref-shilohHeTaughtAI2023" role="doc-biblioref">Shiloh 2023</a>)</span></li>
<li>The NSA, to “identify terrorist threats in international communications data”. <span data-cites="metzOneGeniusLonely2016">(<a href="#ref-metzOneGeniusLonely2016" role="doc-biblioref">Metz 2016</a>)</span></li>
<li>Paul Allen had funded Cycorp sometime before 2001 for unknown purposes and for an unknown sum. <span data-cites="hiltzikBirthThinkingMachine2001">(<a href="#ref-hiltzikBirthThinkingMachine2001" role="doc-biblioref">Hiltzik 2001</a>)</span> In 2003, he funded it by $0.7M as part of his project of “Digital Aristotle”, to create a tutoring AI. <span data-cites="richmanAllenClaimsSuccess2003 friedlandProjectHaloDigital2004">(<a href="#ref-richmanAllenClaimsSuccess2003" role="doc-biblioref">Richman 2003</a>; <a href="#ref-friedlandProjectHaloDigital2004" role="doc-biblioref">Friedland et al. 2004</a>)</span></li>
<li>The <a href="https://en.wikipedia.org/wiki/MIPT_Terrorism_Knowledge_Base">Terrorism Knowledge Base</a> (2004–2008).</li>
<li>The <a href="https://en.wikipedia.org/wiki/Total_Information_Awareness">Total Information Awareness</a> project funded Cycorp for $9.8 million in 2003 for a “prototype database” and a system that could “identify phone-calling patterns as they might exist among potential terrorists overseas”. <span data-cites="crensonBigBrotherCould2003">(<a href="#ref-crensonBigBrotherCould2003" role="doc-biblioref">Crenson 2003</a>)</span></li>
<li>Electronic Surveillance System for the Early Notification of Community-Based Epidemics-II (ESSENCE-II), around 2006. Its title is self-explanatory. <span data-cites="abbottIntegratedBiologicalWarfare2007">(<a href="#ref-abbottIntegratedBiologicalWarfare2007" role="doc-biblioref">Abbott et al. 2007</a>)</span></li>
<li>Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) program, sometime between 2001 and 2004. Its title is self-explanatory. <span data-cites="lenatBuildingMachineSmart2009">(<a href="#ref-lenatBuildingMachineSmart2009" role="doc-biblioref">Lenat 2009</a>)</span></li>
<li>Seven unnamed big companies, for unspecified expert system applications <span data-cites="cycorpCycTechnologyOverview2021">(<a href="#ref-cycorpCycTechnologyOverview2021" role="doc-biblioref">Cycorp 2021</a>)</span>, but probably “common-sense platform for their applications, and as an interlingua to fully, semantically integrate all the data they generate and all the data they license from third parties”. <span data-cites="lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">Lenat 2022a</a>)</span></li>
<li>Unnamed semiconductor foundry, for an expert system for root-cause analysis of fabrication yields. (<a href="https://web.archive.org/web/20130805145356/http://www.cyc.com/enterprise-solutions/success-stories/semiconductor-yield-management">Source</a>)</li>
<li>Unnamed big bank, for an expert system for IT support and inventory management. (<a href="https://web.archive.org/web/20130805145023/http://www.cyc.com/enterprise-solutions/success-stories/it-inventory-catalog-and-configuration-management">Source</a>)</li>
<li>Unnamed big bank, for an expert system for IT personnel expertise management. (<a href="https://web.archive.org/web/20130619021204/http://www.cyc.com/enterprise-solutions/success-stories/help-desk-expertise-management">Source</a>)</li>
<li>Unnamed oil company, for an expert system for monitoring and predicting breakdowns at oil pumping facilities. (<a href="https://web.archive.org/web/20130805144921/http://www.cyc.com/enterprise-solutions/success-stories/facilities-status-monitoring-and-alerting">Source</a>)</li>
</ul>
<p>Looking at the list, we see that much of Cycorp funding came from the American intelligence community, especially between 2001 and 2010, during the heights of <a href="https://en.wikipedia.org/wiki/War_on_terror">War on Terror</a>, as the American state struggled to expand its sovereign eye over the expanding cyberspace. Indeed, one of the early success was when it “predicted anthrax might be sent through the mail six months before trove of knowledge about past terrorist activities, tactics, and weapons”. <span data-cites="hawkinsPredictingTerroristsNext2003">(<a href="#ref-hawkinsPredictingTerroristsNext2003" role="doc-biblioref">Hawkins 2003</a>)</span> Though the success did not help anyone, it was great advertisement.<a href="#fn12" id="fnref12" role="doc-noteref"><sup>12</sup></a> Corroborating, Lenat in a <a href="https://youtu.be/gAtn-4fhuWA?si=gAQ-TISZxxgeD1VN&amp;t=1856">2006 Google Talk</a> showed screenshots of Cyc answering “Which American city would be most vulnerable to an anthrax attack during summer?”. (The answer was “Phoenix”.)</p>
<div id="fn12"><p><sup>12</sup>&nbsp;But there’s also this:</p>
<blockquote>
<p>Once, developing a scenario for a terrorist attack on Hoover Dam, it hypothesized a school of 1,000 al Qaeda-trained dolphins bearing explosives.</p>
<p><span data-cites="hawkinsPredictingTerroristsNext2003">(<a href="#ref-hawkinsPredictingTerroristsNext2003" role="doc-biblioref">Hawkins 2003</a>)</span></p>
</blockquote>
</div><div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/Cyc_anthrax_phoenix.png"></p>
<figcaption>Screenshot of Cyc answering “Which American city would be most vulnerable to an anthrax attack during summer?”. The system replied “Phoenix”, with reasoning. <span data-cites="lenatGoogleTechTalksComputers2006">(<a href="#ref-lenatGoogleTechTalksComputers2006" role="doc-biblioref">Lenat 2006</a>)</span></figcaption>
</figure>
</div>
<p>The total funding of the project is hard to know, although we know that in 2002, its <a href="https://stanfordmag.org/contents/wise-up-dumb-machine">total cost had been $60M</a>, of which <a href="https://web.archive.org/web/20120502151103/http://www.opencyc.org/cyc/company/news/APArticle060902">$25M came from the military</a>, so I think it’s fair to say 50% came from the military. This is corroborated in 2005:</p>
<blockquote>
<p>In 1996, we got our first substantial government contract,” Lenat recalls. Since then, Cycorp has collected about half of its revenue from U.S. government agencies and the rest from companies, mostly for building “semantic maps” that help users pull information from various databases with a single query. By taking on paying projects, Cycorp has been able to stay profitable and debt-free. All of the firm’s stock is owned by its employees, making Cycorp answerable only to Cycorp. “But,” Lenat admits, “we have had to tack with the funding winds. Maybe 50 percent of the funding we get pushes us forward in the direction that we need to go.”</p>
<p>Cycorp doesn’t even want to be distracted by the rigors of the retail software business; instead, it licenses Cyc for use in third-party software packages… The time may come, Lenat says, when a greatly expanded Cyc will underlie countless software applications. But reaching that goal could easily take another two decades.</p>
<p><span data-cites="woodCycorpCostCommon2005">(<a href="#ref-woodCycorpCostCommon2005" role="doc-biblioref">Wood 2005</a>)</span></p>
</blockquote>
<p>Out of all these applications, only two had been reported in detail.</p>
</section>
<section id="terrorism-knowledge-base">
<h3 data-anchor-id="terrorism-knowledge-base">Terrorism Knowledge Base</h3>
<p>The first application is the Terrorism Knowledge Base (TKB), created in 2004 and shut down in 2008. <span data-cites="cycorpTerrorismKnowledgeBase2008">(<a href="#ref-cycorpTerrorismKnowledgeBase2008" role="doc-biblioref">Cycorp 2008</a>)</span> During the aftermath of 9/11, the American government funded a massive expansion of surveillance and data processing, and the Cycorp took on several of such contracts. The TKB is the only one about which we know in some detail.</p>
<p>The system can be browsed like a local Wikipedia – if Wikipedia were focused entirely on terrorism. TKB contained &gt;2000 terrorists, &gt;700 terrorist groups, &gt;6500 terrorist attacks, and &gt;200,000 assertions such as “Xavier Djaffor participated in the Jihad from 1996 to 2000” and “Lashkar-e-Taiba is an Islamist terror group founded in 1990”.</p>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/TKB_browser.png"></p>
<figcaption>Browsing the TKB about <a href="https://en.wikipedia.org/wiki/Imad_Mughniyeh">Imad Fayez Mughniyeh</a>. <span data-cites="cycorpTerrorismKnowledgeBase2008">(<a href="#ref-cycorpTerrorismKnowledgeBase2008" role="doc-biblioref">Cycorp 2008</a>)</span></figcaption>
</figure>
</div>
<p>A user query would be processed in 4 steps.</p>
<ol type="1">
<li>User enters question in formal, but still natural, English.</li>
<li>Cyc parses the question by keyword matching, template matching, and syntactic rules, then applies domain and common sense constraints to fix the parse, then retrieves some CycL fragments that are the closest matches to what the user entered.</li>
<li>The user clicks on the fragments they meant. Cyc synthesizes a full query in CycL. The user optionally modifies the CycL query.</li>
<li>Cyc runs inference engines to retrieve the answer along with a logic chain for the answer.</li>
</ol>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/TKB_query.png"></p>
<figcaption>Asking the TKB about “What terrorists and biological agents are such that the terrorist is capable of learning to make the biological agent?”. Its returns included <a href="https://en.wikipedia.org/wiki/Masami_Tsuchiya_(terrorist)">土谷正実</a> from <a href="https://en.wikipedia.org/wiki/Aum_Shinrikyo">Aum Shinrikyo</a>. <span data-cites="cycorpTerrorismKnowledgeBase2008">(<a href="#ref-cycorpTerrorismKnowledgeBase2008" role="doc-biblioref">Cycorp 2008</a>)</span></figcaption>
</figure>
</div>
<p>Like querying, data entry also has a light amount of parsing, and the system attempts to fill a form with it. The user can then fix the form. An intelligence specialist, lightly trained in using it, could enter up to 100 assertions per hour.</p>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/TKB_data_entry.png"></p>
<figcaption>The data entry form. The system is prompting the user to disambiguate “skating boy” between “boy who is a doer of skating” and “boy who performs skating professionally”. <span data-cites="cycorpTerrorismKnowledgeBase2008">(<a href="#ref-cycorpTerrorismKnowledgeBase2008" role="doc-biblioref">Cycorp 2008</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="cleveland-clinic">
<h3 data-anchor-id="cleveland-clinic">Cleveland Clinic</h3>
<p>The second application took place at the Cleveland Clinic, sometime during 2007–2010.<a href="#fn13" id="fnref13" role="doc-noteref"><sup>13</sup></a> The system was called “Semantic Research Assistant” (SRA), and it could answer queries about <a href="https://en.wikipedia.org/wiki/Cardiothoracic_surgery">cardiothoracic surgery</a>, <a href="https://en.wikipedia.org/wiki/Cardiac_catheterization">cardiac catheterization</a>, and <a href="https://en.wikipedia.org/wiki/Percutaneous_coronary_intervention">percutaneous coronary intervention</a> – basically, surgery-relevant questions about the heart.</p>
<div><p><sup>13</sup>&nbsp;This paper is the only substantial public information about this collaboration I can find. Therefore, I can only confirm that Cyc had been used in Cleveland Clinic during 2007–2010. It may have lasted to at least 2021, since <span data-cites="cycorpCycTechnologyOverview2021">(<a href="#ref-cycorpCycTechnologyOverview2021" role="doc-biblioref">Cycorp 2021</a>)</span> still cited this application.</p></div><div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/Semantic_Research_Assistant.png"></p>
<figcaption>The 4-step process of the SRA. It is essentially the same process as in the TKB. <span data-cites="lenatHarnessingCycAnswer2010">(<a href="#ref-lenatHarnessingCycAnswer2010" role="doc-biblioref">Lenat et al. 2010, fig. 2</a>)</span></figcaption>
</figure>
</div>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/SRA_logical_proof.png"></p>
<figcaption>A logical justification constructed by Cyc by backward chaining on a user query. The original query is not shown, but it probably should be “For each instance of pericardial aortic valve replacement event in 2008, which event was it, and which type of pericardial aortic valve prosthesis was it?”. <span data-cites="lenatHarnessingCycAnswer2010">(<a href="#ref-lenatHarnessingCycAnswer2010" role="doc-biblioref">Lenat et al. 2010, fig. 4</a>)</span></figcaption>
</figure>
</div>
<p>There was one subsequent report on the system in 2012, which described the SemanticDB project at Cleveland, of which Cyc was only a part. The SemanticDB system contains a database of 120M semantic triples (which Lenat had long dismissed as being too limited). In the system, Cyc parses <a href="https://en.wikipedia.org/wiki/Cohort_analysis">cohort identification queries</a> written in English into formal queries, then queries the database in <a href="https://en.wikipedia.org/wiki/SPARQL">SPARQL</a>, does some further inference, and shows the result. <span data-cites="pierceSemanticDBSemanticWeb2012">(<a href="#ref-pierceSemanticDBSemanticWeb2012" role="doc-biblioref">Pierce et al. 2012</a>)</span> Unsurprisingly, Oracle Semantic Technologies <a href="https://download.oracle.com/otndocs/tech/semantic_web/pdf/oow10_semtech_clvclnc_booth.pdf">was also involved</a>.</p>
<p>In a <a href="https://web.archive.org/web/20210430003227/https://files.gotocon.com/uploads/slides/conference_13/724/original/AI_GOTO%20Lenat%20keynote%2030%20April%202019%20hc.pdf">presentation in 2019</a>, Lenat claimed it required 120K new assertions for the Cleveland project, or 0.5% of the total knowledge base. However, 95% of the assertions that Cyc called up for answering queries for the project required knowledge Cyc already had, indicating large knowledge reuse (indicating that the whole project required about 2M rules).</p>
</section>
<section id="is-that-all">
<h3 data-anchor-id="is-that-all">Is that all?</h3>
<p>Well, I tried my best to look for more applications, but the fact is that there were so few of them. Out of all the confirmed instances of applications, the above two were the <em>only</em> ones reported in detail.</p>
<p>Indeed, just as OpenCyc failed to bring about the Semantic Web or impress the general public, ResearchCyc failed to bring about a revolution in knowledge engineering or impress the general academia, or essentially anyone outside of Cycorp itself. Even Ernest Davis and Gary Marcus, highly sympathetic to the symbolic approach to AI, found little evidence for the success of Cyc, not because Cyc had provably failed, but simply because there was too little evidence in any direction, success or failure.</p>
<blockquote>
<p>… it is in fact very difficult for an outsider to determine what has been accomplished here. In its first 15 years, CYC published astonishingly little. Since about 2002, somewhat more has been published, but still very little, considering the size of the project. No systematic evaluation of the contents, capacities, and limitations of CYC has been published. A number of organizations have done private evaluations but the results were not published.</p>
<p>It is not, for example, at all clear what fraction of CYC actually deals with commonsense inference, and what fraction deals with specialized applications such as medical records or terrorism. It is even less clear what fraction of commonsense knowledge of any kind is in CYC. … There are not even very many specific examples of commonsense reasoning carried out by CYC that have been published.</p>
<p><span data-cites="davisCommonsenseReasoningCommonsense2015">(<a href="#ref-davisCommonsenseReasoningCommonsense2015" role="doc-biblioref">Davis and Marcus 2015</a>)</span></p>
</blockquote>
<blockquote>
<p>On the whole, it is fair to say that the AI community regards CYC as a very elaborate failure. Domingos (2015 p.&nbsp;51) characterizes it as “the most notorious failure in the history of AI”. Domingos is a researcher in machine learning and has little use for any kind of knowledge-based methods, so his phrasing is certainly harsh, but in our experience, this opinion, more or less, is common even in the knowledge representation (KR) community. … it would be very helpful, and it would, we believe, significantly improve CYC’s standing in the AI community, if the CYC team could demonstrate some specific task where CYC really visibly shines. … let them design their own task. As Watson demonstrates, passing a self-imposed task can be impressive enough, depending on the task, and in any case it is much better than nothing. At the moment, a person who is asked, “What interesting thing has been done with CYC?” is largely at a loss for an answer.</p>
<p><span data-cites="davisEvaluatingCYCPreliminary2016">(<a href="#ref-davisEvaluatingCYCPreliminary2016" role="doc-biblioref">Davis 2016</a>)</span></p>
</blockquote>
<p>There is a kind of insularity in Cycorp that starts to affect you if you look too closely into it. I know I was affected. Many times I had opened a paper that purported to show the application of Cyc, and was disappointed to find that it was yet another paper about the application of a method to ingest knowledge <em>into</em> Cyc, rather than a method to apply knowledge <em>out from</em> Cyc. I came to dread the literature review, as Cyc in my mind took on the sinister appearance of a black hole at the center of the knowledge graph, a cocoon that would never metamorphose into a butterfly.</p>
</section>
</section>
<section id="everyone-can-only-see-their-own-dream">
<h2 data-anchor-id="everyone-can-only-see-their-own-dream">Everyone can only see their own dream</h2>
<section id="lenats-tenets">
<h3 data-anchor-id="lenats-tenets">Lenat’s tenets</h3>
<p>In <span data-cites="lenatThresholdsKnowledge1991">(<a href="#ref-lenatThresholdsKnowledge1991" role="doc-biblioref">Lenat and Feigenbaum 1991</a>)</span>, a paper coauthored with Feigenbaum, Lenat gave the most comprehensive statement for where he stands philosophically, which he held onto for the rest of his life:</p>
<ul>
<li>Knowledge Principle. A system exhibits intelligent understanding and action at a high level of competence primarily because of the knowledge that it can bring to bear: the concepts, facts, representations, methods, models, metaphors, and heuristics about its domain of endeavor.</li>
<li>Explicit Knowledge Principle. While knowledge may be compiled to opaque lumps of code for efficiency, there should always be a declarative version of that, so that they can be subject to meta-reasoning.</li>
<li>Breadth Hypothesis. Intelligent performance often requires the problem solver to fall back on increasingly general knowledge, and/or to analogize to specific knowledge from far-flung domains.</li>
<li>Empirical Inquiry Hypothesis. The most profitable way to investigate AI is to embody our hypotheses in programs, and gather data by running the programs. The surprises usually suggest revisions that start the cycle over again. Progress depends on these experiments being able to falsify our hypotheses. Falsification is the most common and yet most crucial of surprises. In particular, these programs must be capable of behavior not expected by the experimenter.</li>
<li>Difficult Problems Hypothesis. There are too many ways to solve simple problems. Raising the level and breadth of competence we demand of a system makes it easier to test – and raise – its intelligence.</li>
<li>Knowledge Is All There Is Hypothesis. No sophisticated, as-yet-unknown <em>control structure</em> is required for intelligent behavior.</li>
<li>The Local Consistency Hypothesis. There is no need–and probably not even any possibility–of achieving a global consistent unification of several expert systems’ KBs (or, equivalently, for one very large KB). Large systems need local consistency.</li>
<li>The Coherence Hypothesis. Moreover, whenever two large internally consistent chunks C1, C2 are similar, their heuristics and analogies should cohere; e.g., if the “going up” metaphor usually means “getting better” for C1, then it should again mean “getting better” for C2, or else it should not apply at all there.</li>
</ul>
<p>Lenat is the very example of a <a href="https://en.wikipedia.org/wiki/The_Hedgehog_and_the_Fox">hedgehog</a>: a single philosophy, a vision for AGI, pursued for 40 years. One does not pursue a single vision without rejecting alternative visions, and Lenat has been explicit in rejecting every alternative route to AGI, using his sharp tongue <span data-cites="lenatThresholdsKnowledge1991 thompsonKnowItAllMachine2001 lenatVoiceTurtleWhatever2008 lenatGettingGenerativeAI2023">(<a href="#ref-lenatThresholdsKnowledge1991" role="doc-biblioref">Lenat and Feigenbaum 1991, sec. A.2</a>; <a href="#ref-thompsonKnowItAllMachine2001" role="doc-biblioref">Thompson 2001</a>; <a href="#ref-lenatVoiceTurtleWhatever2008" role="doc-biblioref">Lenat 2008</a>; <a href="#ref-lenatGettingGenerativeAI2023" role="doc-biblioref">Lenat and Marcus 2023</a>)</span>:</p>
<ul>
<li>Logical AI in the style of Simon and Newell’s General Problem Solver. Such an elegant framework would not work beyond toy problem domains, by the Knowledge Principle.</li>
<li>Highly accurate models of human behavior, in the style of Simon and Newell’s <em>Human Problem Solving</em> or the <a href="https://en.wikipedia.org/wiki/Soar_(cognitive_architecture)">SOAR architecture</a>. Duplicating human cognitive architecture constitutes cargo cult science. AGI need not have the <a href="https://en.wikipedia.org/wiki/The_Magical_Number_Seven%2C_Plus_or_Minus_Two">magic number 7 ± 2</a>.</li>
<li>Physical embodiment. It might be great fun to make robots, but physical embodiment is neither necessary nor sufficient for “grounding” the knowledge base, by the <a href="https://en.wikipedia.org/wiki/Physical_symbol_system">Physical Symbol System Hypothesis</a>. A “mystical worship of physical embodiment” would only delay AGI. In particular, the <a href="https://en.wikipedia.org/wiki/Subsumption_architecture">subsumption architecture</a> does not lead to AGI.</li>
<li>Genetic algorithms and other evolutionary algorithms. It gets stuck in local minima too often, and runs too slowly.</li>
<li>Creating tiny morsels of little expert systems, and hope that bit by bit, AGI would emerge out of that. Remember that plateau-hopping requires breadth. Without an overarching plan, they will not fit together, like how the 1980s expert systems could never talk to each other.</li>
<li>Logical machine learning without a large knowledge base already in place. It makes for good demos, but quickly exhausts itself. These are examples of the illusory hope for “free lunch” or elegant “Maxwell’s equations of thinking”, a severe case of laziness and “physics envy”. Researchers should stop sitting on their asses mad with “physics envy”, and start the dirty work of coding.</li>
<li>Statistical machine learning, pattern matching, neural networks, and other self-organization methods. Just wait for enough compute and data, then magically a large model would learn on its own? Yet more wishful thinking for “free lunch”, caused by laziness and “physics envy”.</li>
<li>Any form of machine learning without a large knowledge base to begin with. This is impossible because learning is possible only at the fringe of knowing. Any attempt to learn without a large starting knowledge base is, again, trying to get a “free lunch”.</li>
<li>Wait until philosophers have figured out the one true ontology for the world, then build the Cyc accordingly. Philosophers suffered from “Hamlet syndrome”, unwilling to take decisive action, satisfied with publishing tiny morsels of ontologies that don’t cover the whole world, or grand ontologies that cover a caricature of the whole world.</li>
</ul>
<p>Lenat had his own grand historical vision for AI, which I call the 3 Optima Theory.</p>
<p>With a little hard work (about 6 person-months), one can get a knowledge-free system working, such as self-organized neural networks, Simon and Newell’s General Problem Solver, etc. This allows the researcher to publish a quick paper, a student to earn their PhD degree, and so on. Putting in more hard work does not result in a better system, but usually makes things worse as the code becomes bloated and unmanageable. Academic myopia stops people from trying to get out of this local maximum, since people just want to get published papers.</p>
<p>With a lot more hard work (about 10 person-years), one can get a system with a lot of specialized knowledge working. This is where the commercialized expert systems live. However, the general consensus is that as an expert system grows beyond 10K rules, it starts to suffer from its weight of all the rules. Standard expert systems were built for special fields, so people would use a simple language that works, but eventually collapses under the weight of 100K rules. Commercial myopia stops people from trying to get out of this local maximum, since people just want to sell products, and 50K rules is good enough for the customer.</p>
<p>The problem is that all these efforts are wasted. Specialized expert systems cannot be glued together efficiently, because each of them lives in a differently simplified world. That is, “plateau-hopping requires breadth”. The AI field as a whole would stagnate. The only way out of this is to go for the full common sense, to invest in the 2000 person-years of effort, and make a Cyc. After that, all the expert systems can interface with Cyc, and with each other using CycL, and all the computers can be preinstalled with their digital common sense.</p>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/Lenat_3_optima_theory.svg"></p>
<figcaption>Lenat’s 3 Optima Theory for the grand history of AI.</figcaption>
</figure>
</div>
</section>
<section id="a-hostile-assessment-of-cyc">
<h3 data-anchor-id="a-hostile-assessment-of-cyc">A hostile assessment of Cyc</h3>
<blockquote>
<p><strong>bogosity</strong>: At CMU, bogosity is measured with a bogometer; in a seminar, when a speaker says something bogus, a listener might raise his hand and say “My bogometer just triggered”… The agreed-upon unit of bogosity is the microLenat.</p>
<p><strong>microLenat</strong>: The unit of bogosity. Abbreviated µL or mL in ASCII. Consensus is that this is the largest unit practical for everyday use. The microLenat, originally invented by David Jefferson, was promulgated as an attack against noted computer scientist Doug Lenat by a tenured graduate student at CMU. Doug had failed the student on an important exam because the student gave only “AI is bogus” as his answer to the questions. The slur is generally considered unmerited, but it has become a running gag nevertheless.</p>
<p>— <a href="http://www.catb.org/jargon/html/M/microLenat.html">The Jargon File</a></p>
</blockquote>
<p>It is hard to interpret the state of Cyc today, if we take Lenat’s word for it:<a href="#fn14" id="fnref14" role="doc-noteref"><sup>14</sup></a></p>
<div><p><sup>14</sup>&nbsp;Feel free to stop reading at this point. It is about to get polemical. Indeed, I have noticed that articles have the nefarious tendency to start from the personal (“the <a href="https://en.wikipedia.org/wiki/Narrative_hook">hook</a>”), then become informative, and then subtly slide to the polemical (“the call to action”). In fact, I have mastered the art of first scrolling the page with my eyes blurred so that the <a href="https://en.wikipedia.org/wiki/Hook_(boxing)">hook</a> cannot land on my head, and then, as soon as the text utters the first syllable of the ca–[tab closed]</p></div><ul>
<li>There were 150 technical challenges to knowledge engineering and representation at the start of Cyc in 1984, but they were all solved by 1990. <span data-cites="lenatBuildingMachineSmart2009 lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatBuildingMachineSmart2009" role="doc-biblioref">Lenat 2009</a>, <a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">2022a</a>)</span></li>
<li>Cyc could already be tutored in (constrained) natural language in 2001. <span data-cites="anthesComputerizingCommonSense2002">(<a href="#ref-anthesComputerizingCommonSense2002" role="doc-biblioref">Anthes 2002b</a>)</span></li>
<li>The upper ontology has remained stable for years as of 2015. <span data-cites="lenat50ShadesSymbolic2015">(<a href="#ref-lenat50ShadesSymbolic2015" role="doc-biblioref">Lenat 2015</a>)</span></li>
<li>The knowledge pump is 95% primed in 2015, when there were just 15M assertions, <span data-cites="lenat50ShadesSymbolic2015">(<a href="#ref-lenat50ShadesSymbolic2015" role="doc-biblioref">Lenat 2015</a>)</span>, and as of 2021, there were over 25M assertions <span data-cites="cycorpCycTechnologyOverview2021">(<a href="#ref-cycorpCycTechnologyOverview2021" role="doc-biblioref">Cycorp 2021</a>)</span>.</li>
<li>SubLisp is easy to learn, and knowledge engineering in SubLisp is 1000× more efficient than in a modern language like Python. <span data-cites="lenatCycQuestSolve2021">(<a href="#ref-lenatCycQuestSolve2021" role="doc-biblioref">Lenat 2021</a>)</span></li>
<li>The Cycorp had been profitable since its inception, had never taken on debt, had been almost entirely employee-owned, had always had only around 50–200 employees, a mostly flat corporate structure, and could remain profitable entirely on doing business with non-government corporations in 2022. <span data-cites="lenatCycQuestSolve2021 lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCycQuestSolve2021" role="doc-biblioref">Lenat 2021</a>, <a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">2022a</a>)</span></li>
<li>Cyc has natural language understanding of pragmatics, while statistical machine learning systems have none. <span data-cites="lenatSometimesVeneerIntelligence2017 lenatNotGoodGold2019">(<a href="#ref-lenatSometimesVeneerIntelligence2017" role="doc-biblioref">Lenat 2017</a>, <a href="#ref-lenatNotGoodGold2019" role="doc-biblioref">2019a</a>)</span></li>
</ul>
<p>At this point, Cyc is supposed to be out there doing true machine learning (not the shallow veneer of “machine learning” that those statistical and connectionist researchers are faking). Reading and studying human text written in natural language. True intelligence. General intelligence. The foundation of a thousand expert systems. The backbone of the Semantic Web. The flowering of a new age of reason.</p>
<p>And yet 9 years after Cyc is declared “done”, Cyc is still stuck inside the walls of Cycorp doing nothing of the kind. What is stopping Cyc from learning?</p>
<p>The finances are healthy. Cycorp is not subject to perverse interests of the market or middle managers. There are fewer employees than <a href="https://en.wikipedia.org/wiki/Dunbar's_number">Dunbar’s number</a>. They are aligned to the corporate mission. SubLisp is a great language. All technical challenges to knowledge engineering and representation had been solved by 1990. The knowledge pump is over 160% primed.</p>
<p>What. Is. Stopping. Cyc. From. Learning??</p>
<p>According to the final work of Lenat <span data-cites="lenatGettingGenerativeAI2023">(<a href="#ref-lenatGettingGenerativeAI2023" role="doc-biblioref">Lenat and Marcus 2023</a>)</span>, the only holdup is natural language understanding. The knowledge pump had been thoroughly primed, but Cyc still couldn’t learn by reading human texts because natural language understanding is still too hard. Cyc can read CycL perfectly well – the interlingua – but it is stubbornly difficult to parse English into the interlingua. But what <em>is</em> lacking in the parser? Why is it hard to parse?</p>
<p>This situation is clarified if we consider a classic model of machine translation, that of the <a href="https://en.wikipedia.org/wiki/Vauquois_triangle">Vauquois triangle</a>: We need to translate from one language (let’s say, English) to another (let’s say, Japanese). We can translate word-for-word, which corresponds to the base of the triangle. This direct approach is of course simple but brittle. Not only does it ignore different word-ordering across languages, it would also completely fail to disambiguate homonyms, such as “fly” as a noun versus “fly” as a verb. Stopgap measures such as using n-grams lead to combinatorial explosions.</p>
<p>At a higher level, we can take account of syntax. We first parse the English sentence into a syntax tree, do a word-replacement, transform the syntax tree according to Japanese syntax, and finally generate the Japanese sentence from it. This approach would solve the “fly” vs “fly” problem, since one is a verb and another is a noun, which would be clear according to the syntax tree. However, this fails to disambiguate the word “pen” in the two sentences:</p>
<ul>
<li>The ink is in the <u>pen</u>.</li>
<li>The sheep is in the <u>pen</u>.</li>
</ul>
<p>In both cases, the word “pen” has exactly the same syntactic category, but has a different meaning, and it requires some understanding of how the world works (i.e.&nbsp;that the writing-pen is too small to hold a sheep, while it is highly unlikely for someone to put ink into an animal-pen). Similarly, in the <a href="https://en.wikipedia.org/wiki/Winograd_schema_challenge">Winograd schema challenge</a>, the task is to disambiguate what a pronoun refers to, such as deciding what the word “they” refers to in each of the two sentences:</p>
<ul>
<li>The city councilmen refused the demonstrators a permit because <u>they</u> feared violence.</li>
<li>The city councilmen refused the demonstrators a permit because <u>they</u> advocated violence.</li>
</ul>
<p>Indeed, Lenat had often claimed that the Winograd schema challenge is a touchstone for true language understanding, something that would prove that Cyc really understands, while unmasking the other systems’ veneers of intelligence. And how must the Winograd schema challenge be solved? Lenat’s solution is to go to the pinnacle of the triangle – interlingua, the King of Kings, the Language among Languages, a completely universal conceptual representation for all humans might wish to mean by their speech.</p>
<p>Lenat intended the CycL to be the interlingua.</p>
<p>With this framework, the problems of machine translation and understanding are unified: To understand English, it remains to parse English into the interlingua. To translate Japanese to English, it remains to parse Japanese to interlingua, then verbalize interlingua into English.</p>
<div>
<figure>
<p><img src="https://yuxi-liu-wired.github.io/essays/posts/cyc/figure/Vauquois%20triangle.png"></p>
<figcaption>The Vauquois triangle of translation.</figcaption>
</figure>
</div>
<p>Recall that Lenat had always argued that “just letting a system learn on its own by natural language understanding” is a free lunch, and that NLU requires a significant portion (~10–50%) of common sense already encoded. But if that’s the case, then there is a near-contradiction here:</p>
<ol type="1">
<li>The CycL language is enough to represent common sense language about the world. On the Vauquois triangle, all natural languages are joined at the top by a common interlingua, which is the CycL.</li>
<li>By the Winograd schema challenge, translation requires common sense. Therefore, Japanese → English translation requires common sense.</li>
<li>CycL → English requires no common sense. The result would sound kind of wooden and robotic, but it doesn’t require any understanding: Just follow the syntax substitution rules. Indeed, CycL → English was already working since ~2000. Therefore, CycL → English requires <em>no</em> common sense.</li>
<li>Therefore, by the conservation of common sense, it takes <em>exactly</em> the same amount of common sense to perform Japanese → English translation and Japanese → CycL translation.</li>
<li>By the no free lunch hypothesis, neural networks trained from scratch can’t have common sense. Thus, neural networks should fail at Japanese → CycL translation.</li>
<li>But in Lenat’s last paper, he argued that the only problem stopping Cyc from learning from natural language was that NLU did not work well enough yet, and <em>hoped</em> that neural networks could perform the natural language → CycL translation.</li>
</ol>
<p>On the topic of interlingua, it is interesting that ABBYY<!-- todo: link to ABBYY --> was almost a twin of Cycorp. Whereas Cycorp began building an ontology for the common sense world since 1984, spent $200M, and got stuck on NLU since around 2010, ABBYY began building an interlingua-based machine translation system since the 1990s, and spent over $80M. By the early 2010s, they realized that they could not compete with Google statistical machine translation, and pivoted to doing NLU with semantic graph technology based on the knowledge base they produced for the sake of interlingua. <span data-cites="skorinkinABBYYsBitterLesson2024">(<a href="#ref-skorinkinABBYYsBitterLesson2024" role="doc-biblioref">Skorinkin 2024</a>)</span></p>
<p>Indeed, interlingua-based machine translation projects used to be common, but essentially went extinct (except for ABBYY) after the rise of statistical machine translation in the 1990s <span data-cites="hutchinsMachineTranslationHistory2023">(<a href="#ref-hutchinsMachineTranslationHistory2023" role="doc-biblioref">Hutchins 2023</a>)</span>.</p>
<p>I suspect that it is not simply the problem of getting a better English → CycL translator, and then Cyc would finally begin learning, but that much knowledge in sentences doesn’t translate to interlingua.<a href="#fn15" id="fnref15" role="doc-noteref"><sup>15</sup></a> If the failures of all interlingua machine translation systems is not enough evidence, then consider some more facts about Cyc’s NLU:</p>
<div><p><sup>15</sup>&nbsp;Or that the interlingua exists, but it is not symbolic-logical, but linear-algebraic. Indeed, the successful neural machine translation systems’ latent spaces may be that consummate interlingua so devoutly wished for, but such an interlingua is very far from what you’d see in Cyc or ABBYY.</p></div><ul>
<li>Exhaustively searching the literature, I only found four (four!!) examples that Cycorp gave for English → CycL: “A girl is on a white lounge chair” <span data-cites="prattCYCReportPratt1994">(<a href="#ref-prattCYCReportPratt1994" role="doc-biblioref">Pratt 1994</a>)</span>, “Bill Clinton sleeps.”, “An AI researcher is a kind of computer scientist.” <span data-cites="pantonCommonSenseReasoning2006">(<a href="#ref-pantonCommonSenseReasoning2006" role="doc-biblioref">Panton et al. 2006</a>)</span>, and “Did you touch a blue object located in the capital of France on September 25th, 2022?” <span data-cites="lenatGettingGenerativeAI2023">(<a href="#ref-lenatGettingGenerativeAI2023" role="doc-biblioref">Lenat and Marcus 2023</a>)</span>. They were quite easy and unambiguous examples, almost as if they began with a CycL sentence, and then converted it to English. None involves the “many AI-complete elements, such as correct disambiguation, understanding of idioms, metaphor, sarcasm, foreshadowing, irony, subtext, and so on.” <span data-cites="lenatGettingGenerativeAI2023">(<a href="#ref-lenatGettingGenerativeAI2023" role="doc-biblioref">Lenat and Marcus 2023</a>)</span>.</li>
<li><span data-cites="guhaEnablingAgentsWork1994">(<a href="#ref-guhaEnablingAgentsWork1994" role="doc-biblioref">Guha and Lenat 1994</a>)</span> stated that in 1994-03 “The Syntax module can properly handle about 75% of the sentences found in the news stories of a typical issue of the newspaper USA Today. And in cases in which Cyc knows all the proper nouns in the sentence, the Semantics module can properly handle most of the sentences parsable by the syntax module… as good as what our knowledge enterers independently come up with, when asked to manually translate the material into CycL.”.</li>
<li><span data-cites="pantonCommonSenseReasoning2006">(<a href="#ref-pantonCommonSenseReasoning2006" role="doc-biblioref">Panton et al. 2006</a>)</span> stated that in 2006, a search-and-verify system for English → CycL, that combined syntactic parsing, statistical parsing, and Cyc verification, resulted in “sentences that were correct, according to human review, approximately 50% of the time”.</li>
<li><span data-cites="sarjantAllYouCan2009">(<a href="#ref-sarjantAllYouCan2009" role="doc-biblioref">Sarjant et al. 2009</a>)</span> increased the common-sense knowledge in ResearchCyc by 30% in 2009, by guess-and-verify, where the Cyc does verification, and the guess was done by simplistic methods like regex parsing, <a href="https://en.wikipedia.org/wiki/Help:Infobox">infobox</a> pairing, etc.</li>
<li>Some governmental experimental uses of Cyc, such as ESSENCE-II and Total Information Awareness, might have involved some NLU, but I cannot find details concerning how much NLU was involved.</li>
<li>Only <em>one</em> of the commercial applications of Cyc may have plausibly required NLU.
<ul>
<li>In <a href="https://cyc.com/mathcraft/">Mathcraft</a>, the “learning by teaching” game powered by Cyc, students are only allowed to pick from choices generated by the Cyc itself. There is no free-form natural language input at all.</li>
<li>In the Cleveland Clinic application <span data-cites="lenatHarnessingCycAnswer2010">(<a href="#ref-lenatHarnessingCycAnswer2010" role="doc-biblioref">Lenat et al. 2010</a>)</span>, the user enters queries already in a constrained language (like “aortic valve replacement patients with a pericardial aortic valve”), and then compose a CycL translation by clicking from Cyc’s parser’s suggestions.</li>
<li>The same is true for the Terrorism Knowledge Base application.</li>
<li>“Maintaining persistent user models in order to support extended, months-long online chats with their famous characters” (<a href="https://cyc.com/wp-content/uploads/2021/04/Cyc-Technology-Overview.pdf">Source</a>) seems to involve NLU. However, there is no information whatsoever as to how much NLU this project involved. I can’t find out which company hired Cycorp for this, or who the “famous characters” were. It is not even clear if users are allowed to enter free-form input during online chat, or if it is another application like Mathcraft, where you can only pick from choices generated by Cyc itself. Besides, even if this case involved free-form input, there is still a problem. We know that self-organized NN can work as chatbots. So if Lenat was right to say that self-organized NN don’t understand (as he had been saying for 40 years), then this application didn’t require NLU after all.</li>
</ul></li>
</ul>
<p>Let’s take it another way: If there is No Free Lunch to NLU, then what is Cyc’s score on Winograd schema benchmark? Where is the Cyc-translator? Forget about Google Neural Translate – is it even better than ABBYY’s? Where’s ChatCyc? Why does none of Cyc’s commercial applications involve NLU in any significant amount, while most commercial applications of NLU use statistical or neural machine learning?</p>
<p>All evidences point to the conclusion that a sentence that can be parsed to CycL is already bureaucratic and formalistic, with the mark of interlingua written on its brow. For those, Cyc could already understand in the early 2000s, and yet, Cyc is still here, not machine-learning, lacking … what? <code>&lt;sarcasm&gt;</code>A sarcasm parser?<code>&lt;/sarcasm&gt;</code></p>
<div title="An information-theoretic estimate">
<div data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">

<p>
An information-theoretic estimate
</p>

</div>
<div id="callout-4">
<p>The <a href="https://yuxi-liu-wired.github.io/essays/posts/perplexity-turing-test/#entropy-of-natural-languages">minimal description length of English</a> is about 0.8 bits per character, and each English sentence contains about 100 characters, giving 80 bits of incompressible information. Since there are 27 characters (we are just counting the 26 lowercase letters and the whitespace), this means that the compressible information is about <span>\(\log_2 27 - 0.8 = 4.0\)</span> bits per character.</p>
<p>We can think of writing as a process of selection: Out of all the possible letters you can pick next, you picked this <em>particular</em> letter, and you do it again and again. Intelligence is good selection, the careful elimination of bad choices. In this particular case, the bad choices to be eliminated come out to about 3.2 bits per character.</p>
<p>Now, one might argue that the last bit is the deepest, but it is hard to square the two claims:</p>
<ul>
<li>Self-organized NN could compress natural text to a bit rate of ~1.0 bits per character, so they already capture <span>\(3.0/3.2 = 94\%\)</span> of the selection.</li>
<li>Self-organized NN don’t understand.</li>
</ul>
<p>Stated in another way, this means 94% of the selection that humans perform while writing is mindless, mere remembering and espousing without understanding or inferring. This is even harder to square with Lenat’s assertion that 99% of language understanding is pragmatics, which self-organized NN don’t have. <span data-cites="lenatSometimesVeneerIntelligence2017">(<a href="#ref-lenatSometimesVeneerIntelligence2017" role="doc-biblioref">Lenat 2017</a>)</span></p>
<p>So does this mean that 93% of pragmatics is mindless, or are we going to just throw out the information-theoretic understanding of language as another incommensurable paradigm shift? It would fit the theme of Cyc. Indeed, they had long excised the remnants of probabilistic reasoning when they removed <a href="https://en.wikipedia.org/wiki/Mycin">certainty factors</a> at some point before 1989, since they found it inadequate compared to reasoning by logical unification <span data-cites="guhaReCycLingPaper1993">(<a href="#ref-guhaReCycLingPaper1993" role="doc-biblioref">Guha and Lenat 1993</a>)</span>. Cry “Kuhnian” and let slip the dogs of logic…</p>
</div>
</div>
<p>Take another look at <a href="#cycops-what-is-it-good-for">the list of known applications of Cyc</a>. Then take a look at what was even planned for as “short-term uses” of Cyc even back in 1993:</p>
<blockquote>
<p>the most promising short-term uses of Cyc are not what are traditionally considered AI problems. Instead, they are in relatively “mundane” problems such as making spreadsheets smarter, providing better access to a heterogeneous set of databases, directed marketing of goods and services, etc.</p>
<p><span data-cites="guhaReCycLingPaper1993">(<a href="#ref-guhaReCycLingPaper1993" role="doc-biblioref">Guha and Lenat 1993</a>)</span></p>
</blockquote>
<p>It is quite prophetic that such “short-term uses” of Cyc are still the <em>only</em> uses of Cyc so far. Is 35 years considered “short-term”? Does this look like a path towards AGI, or does this look no different from building custom-made expert systems for specialized purposes, something that those generic professionals of Oracle, IBM, or Accenture have been doing for decades? Perhaps they did have a product differentiation in being able to consistently find particularly good knowledge engineers, and in programming in SubLisp, a particularly efficient language compared to Python or Java, which allowed them to stay in business despite having just a hundred-person crew. It is not something to be dismissed, but is this a path towards AGI, or a veneer of AGI cast over enterprise solutions? <code>&lt;sarcasm&gt;</code>An IBM Data Integration Solutions<sup>®</sup> with better cover art?<code>&lt;/sarcasm&gt;</code></p>
<p>Perhaps Lenat could be eternally optimistic despite all the missed predictions and the lack of AGI. Perhaps it is a <a href="https://en.wikipedia.org/wiki/Selection_bias">selection effect</a>. One does not undertake a 40-year project for AGI without being delusionally optimistic about the prospect – the same could be said of great leaders and startup founders.</p>
<p>Or perhaps he was disconnected from what was really going on down there at Cycorp. In a <a href="https://news.ycombinator.com/item?id=21781597">HackerNews discussion</a>, some ex-Cyclists wrote what they thought about Cycorp.</p>
<p>On the pro side were many. The corporate culture was highly intellectual and philosophical, as one expects for a company that does computable ontology for a living: “it can be pretty fun to be in meetings where you try to explain Davidsonian ontology to perplexed business people”. The company had solved many technical problems in large scale inference, and remained profitable, with successful commercial applications.</p>
<p>On the con side were many. The codebase was creaking under 30 years of technical debt:</p>
<blockquote>
<p>I spent some entire days just scrolling through different versions of entire systems that duplicate massive chunks of functionality, written 20 years apart, with no indication of which (if any) still worked or were the preferred way to do things.</p>
</blockquote>
<p>The technical solutions and commercial applications were closely guarded secrets, so the outside world does not know. Lenat was unimpressed with open source and so did not commit resources to OpenCyc, tutorials, easier third-party integration, software development kit, or other outreach projects (except those regularly scheduled newspaper reports for publicity). The Cyc culture was also insular, with a true believer’s mentality:</p>
<blockquote>
<p>… veterans there sort of feel like the broader AI community turned their back on symbolic reasoning in the 80s (fair) and they’re generally not very impressed by the current trends within the AI community, particularly w.r.t. advances in ML (perhaps unfairly so), so they’re going to just keep doing their thing until they can’t be ignored anymore.</p>
</blockquote>
<p>Most pertinent to the dream of AGI, it was unclear, down in the trenches, whether Cyc was really doing common sense reasoning, or just a particularly good base for developing expert systems from. It also wasn’t clear if common sense reasoning was really necessary for the successful commercial projects in the first place. This has caused some Cyclists to become ex-Cyclists from disillusionment.</p>
<blockquote>
<p>I personally suspect that some of Cycorp’s clients would do better with domain-specific solutions because they don’t realize how much of their problem could be solved that way and how much of the analysis coming from Cyc is actually the result of subject matter experts effectively building domain-specific solutions the hard way inside of Cyc. With a lot of Cycorp projects, it’s hard to point your finger at exactly where the “AI” is happening… The degree to which it’s effective seemed to me to be a case-by-case thing. While working there I tended to suspect that Cyc people underestimated the degree to which you could get a large fraction of their results using something like <a href="https://en.wikipedia.org/wiki/Datomic">Datomic</a> and it was an open question (to me at least) whether the extra 10% or whatever was worth how much massively more complicated it is to work with Cyc.</p>
</blockquote>
<p>Structurally, the Cycorp had two levels. At the upper level are Lenat, Witbrock, and such keepers of the faith, who kept the ceaseless striving for that elusive dream alive. At the lower level are the working ontological engineers who just had to deliver the product, AGI or not.</p>
<blockquote>
<p>It turns out there’s a kind of reality distortion field around the management there, despite their best intentions - partially maintained by the management’s own steadfast belief in the idea that what Cyc does is what it ought to be doing, but partially maintained by a layer of people that actively isolate the management from understanding the dirty work that goes into actually making projects work or appear to. So while a certain amount of “common sense” knowledge factors into the reasoning processes, a great amount of Cyc’s output at the project level really comes from hand-crafted algorithms implemented either in the inference engine or the ontology.</p>
<p>Over the years, the Cyc as its actually implemented has drifted pretty far from the Cyc that people like Doug Lenat believe in, and the degree to which they’re willing or able to acknowledge that seems to sort of drift around, often dependent on factors like mood. Doug would show up and be very confused about why some things were hard because he just believes that Cyc works differently than it does in practice, and people had project deadlines, so they often implemented features via hacks to shape inference or hand-built algorithms to deliver answers that Doug thought ought to be derived from principles via inference. Doug thinks way more stuff that Cyc does is something that it effectively learned to do by automatically deriving a way to solve the general form of a problem, rather than a programmer up late hand-coding things to make a demo work the next day, and the programmers aren’t going to tell him because there’s a demo tomorrow too and it’s not working yet.</p>
</blockquote>
<p>But that’s enough about unverified rumors, and I apologize. It would have been better for epistemic hygiene if there were more open information about Cycorp. Let’s turn to a Cycoanalysis of Lenat’s rhetorics, which is more well-documented.</p>
</section>
<section id="lenat-against-the-world">
<h3 data-anchor-id="lenat-against-the-world">Lenat against the world</h3>
<p>According to multiple reports, Lenat was charismatic, able to sell his vision of AGI to many people. Having read most documents produced by Lenat or Cycorp over the 40 years, I have discovered that there is a consistent list of themes that Lenat just kept repeating over his career. Each theme has a double structure: a technical statement that has an emotionally neutral valence, and a moral coloring that provides the call to action, the charisma, the coherence for the employees to align to his vision.</p>
<table>
<caption>The doubled structure of Lenat’s rhetoric</caption>
<colgroup>
<col>
<col>
</colgroup>
<thead>
<tr>
<th>technical statement</th>
<th>moral coloring</th>
</tr>
</thead>
<tbody>
<tr>
<td>the No Free Lunch Hypothesis</td>
<td>we do honest hard work like the proverbial ant, you are lazy like the proverbial cricket (and the AI Winter is coming)</td>
</tr>
<tr>
<td>there are no “Maxwell’s Equations of Thought”</td>
<td>we are self-assured, you suffer from physics-envy</td>
</tr>
<tr>
<td>the Empirical Inquiry Hypothesis; Cyc is unaesthetic; we are building the Cyc profitably, not publishing academic papers</td>
<td>we are strong engineers that get things done, you are weak aesthetes playing the academic game</td>
</tr>
<tr>
<td>the Physical Symbol System Hypothesis</td>
<td>we are building real intelligence, you are just playing with robots</td>
</tr>
<tr>
<td>the Breadth Hypothesis</td>
<td>our systems are intelligent, yours are autistic idiot savants</td>
</tr>
<tr>
<td>the Explicit Knowledge Principle</td>
<td>our systems understand deeply, yours pattern-match shallowly</td>
</tr>
<tr>
<td>we think you are putting pattern-matchers in places that require deep understanding</td>
<td>we are trustworthy, you are reckless</td>
</tr>
<tr>
<td>we believe the Cyc is the only current effort towards AGI</td>
<td>we are ambitious, you are academic careerists</td>
</tr>
<tr>
<td>writing the Cyc costs a lot and is unpopular with the academia</td>
<td>we rebel and think freely, you sheepishly follow the crowd</td>
</tr>
<tr>
<td>Cycorp hires anyone – including high school dropouts – good at encoding common sense</td>
<td>we are egalitarian, you are elitists</td>
</tr>
<tr>
<td>Cycorp has always had just ~100 people, and has been mostly forgotten now</td>
<td>we are the elect, you will see</td>
</tr>
</tbody>
</table>
<p>And more than technical, moral, and personal conviction is on the line: If Cyc really would take 1000 person-years (20 years with 50 philosopher PhDs), then it would cost about $100 million just in human labor. The Cycorp, if it were to survive, has a strong commercial interest in rejecting all alternatives. It can be very hard to get someone to understand something, when their <a href="https://en.wikipedia.org/wiki/Product_differentiation">product differentiation</a> depends on them not understanding it.</p>
<p>Lenat’s rejections progressed with time as each new challenger arose, applying the same tenets in different decades.</p>
<p>In the 1980s, like other expert systems people, he aimed his rejection at the previous logical AI methods exemplified by Simon and Newell. Logical AI was a dream that a graduate student might build an AGI during a thesis period, if only they knew the “Maxwell’s equations of thinking”. Of course, such attempts failed, because there are no such equations. He took a little effort towards rejecting the <em>other</em> logical AI approach exemplified by <span data-cites="newellHumanProblemSolving1972">(<a href="#ref-newellHumanProblemSolving1972" role="doc-biblioref">Newell and Simon 1972</a>)</span>, by constructing models that reproduced every little detail of how humans really perform in psychometric experiments, such as their reaction times, their uhhs and oopses. Admitting its interest to psychologists, he considered it a distraction for machine intelligence.</p>
<p>In the 1990s, as the expert system hype died down, he turned his criticism towards expert systems. He recalled that, back when he was young, before academia had rejected him, he thought automated discovery with AI, such as AM and EURISKO, would lead the way to self-improving learning machines. But then he was disabused of this. BACON discovered Kepler’s three laws “only” from data, but that’s because Pat Langley was careful in presenting nothing but the relevant data. The cost to discover Kepler’s laws on the filtered dataset? A few CPU-hours. The cost to filter the dataset? 10 Kepler-years. Similarly, AM started out with the set-theory axioms and discovered prime numbers and some famous conjectures, but quickly ended up enumerating boring complications. Lenat had to keep adding in more heuristics to get something out of it. Similarly, EURISKO would run overnight and Lenat would check its outputs in the morning, remove some bad ideas, add some good ones, and so on. The <em>Traveller</em> 1981 win was “60/40% Lenat/EURISKO” after all.</p>
<p>Generalizing, Lenat argued that there is a common thread across all these machine learning systems. They would all start out discovering many interesting basic things, but quickly “run out of steam” enumerating boring complications. Lenat called it as systems putting up a “veneer of intelligence” while they were really just “discharging potential energy that was stored in them”. That is, the creators secretly put into the program with their own expert knowledge somehow, either through the right rules, heuristics, dataset, features, or some other thing. Once the expert knowledge is “exhausted”, no more discoveries could be made, and the veneer wears off. However, it makes for impressive demos, leading to cycles of hype and bust. The only escape is to prime the knowledge pump. If the knowledge base is large enough, then it wouldn’t run out of steam. <span data-cites="lenatVoiceTurtleWhatever2008">(<a href="#ref-lenatVoiceTurtleWhatever2008" role="doc-biblioref">Lenat 2008</a>)</span></p>
<p>Lenat’s approach was not welcomed by the academics, and the feeling was mutual. AI researchers thought the Cyc project was hyped, and were unhappy with the secretive nature of Cycorp. Philosophers considered the Cyc project premature – how could Lenat build an ontology for the world when philosophers hadn’t even figured out what the ontology is? Lenat shot back, calling academics lazy, abstract, and unable to persist through decades of hard engineering work. <span data-cites="thompsonKnowItAllMachine2001">(<a href="#ref-thompsonKnowItAllMachine2001" role="doc-biblioref">Thompson 2001</a>)</span> Among the academics, the only one that still supported him was Marvin Minsky, who had no problem calling the rest of AI research “brain-dead since the 1970s”, especially robotics: “Graduate students are wasting 3 years of their lives soldering and repairing robots, instead of making them smart.”. <span data-cites="baardAIFounderBlasts2003">(<a href="#ref-baardAIFounderBlasts2003" role="doc-biblioref">Baard 2003</a>)</span></p>
<p>Interesting. Why the hate towards robotics? Well, during the 1990s, there were two main challengers to his idea of a symbolic-logical system. On the one side, there was the challenge of bottom-up non-symbolic reasoning promoted by Rodney Brooks’ subsumption architecture <span data-cites="brooksElephantsDonPlay1990">(<a href="#ref-brooksElephantsDonPlay1990" role="doc-biblioref">Brooks 1990</a>)</span>, and the statistical machine learning methods like support vector machines. He did not have much to say about the statistical methods – not yet – but he did reject the subsumption architecture as a mistaken attempt to reach AGI through robotics, much as Minsky did. Motors, sensors, etc, are simply not needed – common sense, specified in logical language, is all you need. The hard work needed to get the robots to do anything is, you see, the <em>wrong</em> kind of hard work.</p>
<p>On the other side, though most expert system researchers had shrunken their ambition in the winter chill, from AGI down to mere commercial survival, a few still believed in the mission. They thought that by building little systems, brick by brick, we can build towards a generally intelligent system. This is basically a “Society of Mind” approach of Marvin Minsky, and even though Lenat and Minsky liked each other’s research, Lenat rejected this approach as well. One cannot settle for building common sense bit by bit, expecting a finished system to emerge, but must braid the whole thing under one roof, one upper ontology. The ontology does not have to be perfect or efficient, but there has to be one. Without it, the Society of Mind would fragment into a Tower of Babel, with little expert systems of incompatible ontologies, just like how Feigenbaum’s dream of a “Library of Congress” of knowledge bases failed to materialize.</p>
<p>In the 2000s, big data arrived with the Internet, and statistical learning became dominant. No doubt trying to preempt customers’ “Why don’t I just Google it?”, he turned his firepower towards statistical learning systems. He never tired of pointing out that, if you make an even slightly complex query like “Is the Space Needle taller than the Eiffel Tower?”, Google will happily serve up results saying “The Space Needle is 605 feet high.” and “The Eiffel Tower is 1,063 feet high.”, unable to actually answer your question. Despite having 15,000 servers, Google only ran dumb statistical algorithms, while Cyc running on a single server could answer it. Google-style statistical machine learning, like its trillion-token statistical machine translations systems <span data-cites="brantsLargeLanguageModels2007">(<a href="#ref-brantsLargeLanguageModels2007" role="doc-biblioref">Brants et al. 2007</a>)</span>, was just pattern matching, yet another grasping after a free lunch. Such systems could not truly understand. As an alternative, he held out Cyc as the foundation to the <a href="https://en.wikipedia.org/wiki/Semantic_Web">Semantic Web</a>, which would build a system that <em>would</em> truly understand.</p>
<p>Curiously, right from the start, Lenat considered self-organized neural networks as not a viable path towards general intelligence, but for <em>the same reasons</em> as to why the logical AI programs of Simon and Newell would fail! From our perspective, they couldn’t be more different, yet to Lenat, neural nets, General Problem Solvers, n-gram models, whatever, are all just “explicit-knowledge-free systems”, <a href="https://en.wikipedia.org/wiki/Neats_and_scruffies">too neat, not scruffy</a>, and would fail for the exact same reason.</p>
<blockquote>
<p>they are unaesthetic! And they entail person-centuries of hard knowledge-entry work. Until we are forced to them, Occam’s Razor encourages us to try more elegant solutions, such as training a neural net “from scratch”; or getting an infant-simulator and then “talking to it”. Only as these fail do we turn, unhappily, to the “hand-craft a huge KB” tactic.</p>
<p>…</p>
<p>Our position regarding the aesthetes: There is a methodological difference between our “scruffy” way of doing AI and the aesthetes’ “neat” way… If only there were a secret ingredient for intelligence–Maxwell’s equations of thought. If only we could axiomatize the world in a small set of axioms, and deduce everything. If only our learning program could start from scratch. If only our neural nets were big or cerebellar or hyperlinear enough. If only the world were like that. But it isn’t. The evidence indicates that almost all the power is in the bulk knowledge. As Whitehead remarked, “God is in the details.”</p>
<p><span data-cites="lenatThresholdsKnowledge1991">(<a href="#ref-lenatThresholdsKnowledge1991" role="doc-biblioref">Lenat and Feigenbaum 1991</a>)</span></p>
</blockquote>
<p>With the second neural network winter, he did not pay more attention to them, but as they arose yet again in the 2010s, with some exasperation, he would remind the world that, no, nothing has changed. Neural nets had already failed and they would fail. Thinking that “one large net for everything” would just work is yet another example of the logical AI fallacy that “If only we have the Maxwell’s equations of learning, it will just work!”. They are always “remembering and espousing”, but never “understanding and inferring”, and can only ever be the “right brain” to Cyc’s “left brain” <span data-cites="lenatGettingGenerativeAI2023">(<a href="#ref-lenatGettingGenerativeAI2023" role="doc-biblioref">Lenat and Marcus 2023</a>)</span>. As Deep Learning kept blowing past expectations, he rehashed the same 1980s argument with escalating apocalypse:</p>
<blockquote>
<p>If computers were human, they’d present themselves as autistic, schizophrenic, or otherwise brittle. It would be unwise or dangerous for that person to take care of children and cook meals, but it’s on the horizon for home robots. That’s like saying, ‘We have an important job to do, but we’re going to hire dogs and cats to do it.’</p>
<p><span data-cites="loveMostAmbitiousArtificial2014">(<a href="#ref-loveMostAmbitiousArtificial2014" role="doc-biblioref">Love 2014</a>)</span></p>
</blockquote>
<blockquote>
<p>No matter how good your elegant theory of&nbsp;<em>syntax</em>&nbsp;and&nbsp;<em>semantics</em>&nbsp;is, there’s always this annoying residue of&nbsp;<em>pragmatics</em>, which ends up being the lower 99% of the iceberg.&nbsp; You can wish it weren’t so, and ignore it, which is easy to do because it’s out of sight (it’s not explicitly there in the letters, words, and sentences on the page, it’s lurking in the empty spaces around the letters, words, and sentences.)&nbsp; But lacking it, to any noticeable degree, gets a person labeled&nbsp;<em>autistic</em>. They may be otherwise quite smart and charming (such as Raymond in&nbsp;<em>Rain Man</em> and Chauncey Gardiner in&nbsp;<em>Being There</em>), but it would be frankly dangerous to let them drive your car, mind your baby, cook your meals, act as your physician, manage your money, etc.&nbsp;And yet those are the very applications the world is blithely handing over to severely autistic AI programs!</p>
<p><span data-cites="lenatSometimesVeneerIntelligence2017">(<a href="#ref-lenatSometimesVeneerIntelligence2017" role="doc-biblioref">Lenat 2017</a>)</span></p>
</blockquote>
<blockquote>
<p>We would not be comfortable giving a severely neurologically-impaired person – say someone with no functioning left brain hemisphere – real-time decision-making authority over our family members’ health, our life savings, our cars, or our missile defense systems. Yet we are hurtling in that direction with today’s AI’s which are impaired in almost exactly that same fashion! They – those people and those AI programs – have trouble doing multi-step abstract reasoning, and that limitation makes their decision-making and behavior brittle, especially when confronted by unfamiliar, unexpected and unusual situations… Machine learning algorithms have scarcely changed at all, in the last 40 years… Current AI’s can form and recognize patterns, but they don’t really <em>understand</em> anything. That’s what we humans use our left brain hemispheres for.</p>
<p>… Researchers and application builders tolerate their AI systems having just the thinnest veneer of intelligence, and that may be adequate for fast internet searching or party conversation or New York Times op-ed pieces, but that simple representation leads to inferences and answers which fall far short of the levels of competence and insight and adaptability that expert humans routinely achieve at complicated tasks, and leads to shallow explanations and justifications of those answers. There is a way out of that trap, though it’s not pleasant or elegant or easy. The solution is not a machine-learning-like “free lunch” or one clap-of-thunder insight about a clever algorithm: it requires a lot of hard work…</p>
<p><span data-cites="lenatNotGoodGold2019">(<a href="#ref-lenatNotGoodGold2019" role="doc-biblioref">Lenat 2019a</a>)</span></p>
</blockquote>
<p>Concurrently, on the Cycorp website, two white papers published in 2021-04 reiterated their product differentiation against the false promises of <a href="https://cyc.com/wp-content/uploads/2021/04/Cyc-Technology-Overview.pdf">neural networks</a> and <a href="https://cyc.com/wp-content/uploads/2021/04/bayesnetspaper.pdf">Bayesian networks</a>.<a href="#fn16" id="fnref16" role="doc-noteref"><sup>16</sup></a> Neural networks posed a great commercial threat to their business, and the Bayesian networks, by promising to half-open the neural network black box, threatened their business as well. The papers argued that since both were not rule-based logical systems, they were not Actually Intelligent. After such fear-uncertainty-doubt, they reassured the reader that true AI needs both the left brain and the right brain, and they sell the finest left brains on the planet.</p>
<div><p><sup>16</sup>&nbsp;Read <a href="https://cyc.com/wp-content/uploads/2021/04/Cyc-Technology-Overview.pdf">it</a> yourself (<span>\(\sim 3 \times 10^6 \mathrm{\mu Lenat}\)</span>) to see how hard they had to work that product differentiation. Calling it “<strong>A</strong>ctually <strong>I</strong>ntelligent”, claiming “ML can <em>never</em> give an explicit step-by-step explanation of its line of reasoning behind a conclusion, but Cyc <em>always</em> can.”, and insinuating that <em>nobody</em> could do Natural Language Understanding yet because none of those newfangled neural networks had any pragmatics… And this was uploaded in 2021-04, a year after GPT-3! It was written in the same <a href="https://en.wikipedia.org/wiki/Fear%2C_uncertainty%2C_and_doubt">FUD</a>dy voice of those that still sold machine translation services after Google Neural Translate, transcription services after OpenAI Whisper, or copywriting services after ChatGPT.</p></div><p>The same accusation of brain-damage that he leveled at neural networks was in fact a rehash of the exact same argument he had made against statistical machine learning systems like Cleverbot, Google, and Amazon recommender systems <span data-cites="loveMostAmbitiousArtificial2014">(<a href="#ref-loveMostAmbitiousArtificial2014" role="doc-biblioref">Love 2014</a>)</span>, since he made no distinction between statistical methods, be it keyword matching, n-gram models, or neural networks. They are all the same veneer of intelligence, same free lunch, same shallowness.</p>
<p>Lenat could apply the same criticisms with the same counterexamples over his 40 years of career, without needing to inspect the details of these machine learning architectures, because he had the following fully general proof, which you can discover by intersecting the previous paragraphs in this section:</p>
<ol type="1">
<li>Unless common sense is fully represented and integrated, an AI system is an idiot savant at most. <span data-cites="pantonCommonSenseReasoning2006">(<a href="#ref-pantonCommonSenseReasoning2006" role="doc-biblioref">Panton et al. 2006</a>)</span></li>
<li>Machine-learning common sense from scratch is impossible, because learning occurs at the fringe of what one already knows. <span data-cites="lenatCycLargescaleInvestment1995">(<a href="#ref-lenatCycLargescaleInvestment1995" role="doc-biblioref">Lenat 1995b</a>)</span></li>
<li>Therefore…</li>
</ol>
<p>Hedgehogs. Hedgehogs are all the same. They have one big idea, one big proof, one big theory, and continue going on with it for decades. Chomsky did it, Minsky did it, and Lenat did it too. Benefit: If they got it right, they really got it right. Cost: If they got it wrong, then they would sound like a broken record.</p>
<p>For example, Lenat called expert systems “brittle” and “idiot savants” in the 1980s, and statistical machine learning systems “brittle” (probably also “idiot savant”) in the 2000s, and neural networks “brittle” and “autistic” since 2015 until his death.<a href="#fn17" id="fnref17" role="doc-noteref"><sup>17</sup></a></p>
<div><p><sup>17</sup>&nbsp;Some words and phrases reappear so often in Lenat’s writings that I termed them “Lenatisms” (<span>\(\sim 10^6 \mathrm{\mu Lenat/word}\)</span>) and came to hate them as much as I hate GPTisms like “delve” and “crucial”: free lunch, hard work, physics envy, Maxwell’s equations, clever algorithm, measles, idiot-savant, autistic, veneer of intelligence, shallow, pattern matching, brittle, understand, trustworthy, left brain, hemisphere.</p></div><p>Similarly, he kept talking about the Winograd schema challenge, and how logically encoded common sense is the only way to solve it. He started talking about it in the 1990s <span data-cites="lenatCycLargescaleInvestment1995">(<a href="#ref-lenatCycLargescaleInvestment1995" role="doc-biblioref">Lenat 1995b</a>)</span>. He was still telling Stephen Wolfram in 2019 that, surely, if Cyc would team up with Wolfram Alpha, then they could finally solve the challenge for good <span data-cites="wolframRememberingDougLenat2023">(<a href="#ref-wolframRememberingDougLenat2023" role="doc-biblioref">Wolfram 2023</a>)</span>. He was <em>still</em> <a href="https://x.com/CycorpAI/status/1238183980580642816">tweeting about it even on 2020-03-12</a>, about <span data-cites="sakaguchiWinoGrandeAdversarialWinograd2021">(<a href="#ref-sakaguchiWinoGrandeAdversarialWinograd2021" role="doc-biblioref">Sakaguchi et al. 2021</a>)</span> which showed stated that modern LLMs (GPT-2, BERT, and a few others) was underperforming WinoGrade, a larger version of the previous Winograd benchmark. Etc, etc.</p>
<p>Not just his arguments were repetitive, but his “war stories” too. In 1994, Cyc could retrieve images by semantic search, so for example, it would retrieve an image of a rock climber if queried “an adventurous man” <span data-cites="lenatArtificialIntelligence1995">(<a href="#ref-lenatArtificialIntelligence1995" role="doc-biblioref">Lenat 1995a</a>)</span>. Great demo for 1994, and he would harp on this throughout the 2000s in his presentations, presumably to product-differentiate against Google-like Image Search engines. Similarly, he told of a story of an expert system diagnosing his rusty car with measles first in 1987 <span data-cites="lenatThresholdsKnowledge1991">(<a href="#ref-lenatThresholdsKnowledge1991" role="doc-biblioref">Lenat and Feigenbaum 1991</a>)</span>, and would keep telling that story throughout his presentations in the 2000s, and he was still telling the story (and about the Winograd Schema) <a href="https://voicesinai.com/episode/episode-89-a-conversation-with-doug-lenat/">in 2019</a>.</p>
<p>In his last paper, coauthored with Gary Marcus, he updated his critique of statistical machine learning to the LLM age. Again the shallowness, brittleness, free lunch, etc.</p>
<blockquote>
<p>Given the arduous nature of the reasoning required… it is understandable almost all AI researchers and developers have gone in the opposite direction, abandoning or trivializing symbolic representation and reasoning, and instead seeking one or another sort of “free lunch” in the form of perceptrons, multi-layer neural networks and, most recently, LLMs… limiting an AI to such a narrow “baby talk” language would be a huge barrier to it ever becoming a trustworthy general AI.</p>
<p><span data-cites="lenatGettingGenerativeAI2023">(<a href="#ref-lenatGettingGenerativeAI2023" role="doc-biblioref">Lenat and Marcus 2023</a>)</span></p>
</blockquote>
<p>I am struck by the irony that a veteran of logical AI would call neural networks “brittle”, or make an appeal to sunk cost. Lenat had devoted 2000 person-years to the project, therefore a “free lunch” shouldn’t work, nevermind the fact that these “free” lunches took 20 years of gritty battles to build the datasets,<!-- todo: add the link to essay on datasets once it's done. --> struggles with the cussedness of CUDA,<!-- todo: add the link to essay on CUDA once it's done. --> waking up to yet another divergent overnight training run, staring at tensors filled with <code>NaNs</code>, and eventually <a href="https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/">cost $100 million</a> per serving, roughly the total budget of Cycorp through its life. How dare you to go “free lunch” on us… But <em>de mortuis nil nisi bonum</em>.</p>
<p>Lenat died in 2023, unmourned on <a href="https://lucid.ai/">Lucid AI</a> and <a href="https://cyc.com/">Cycorp</a>, who, like ABBYY, still proudly advertise their product differentiation to this day.</p>
</section>
</section>
<section id="in-lieu-of-a-conclusion">
<h2 data-anchor-id="in-lieu-of-a-conclusion">In lieu of a conclusion</h2>
<p>Napoleon died in 1821. Wellington was greatly saddened.</p>
<blockquote>
<p>Plutarch has related that Julius Caesar wept for the death of Pompey; Aurelian did not weep for the death of John, but he felt what a man would feel when rid of an incurable disease that had become a part of his life.</p>
<p>— Borges, <em>The Theologians</em></p>
</blockquote>
<p><em>Come as you are, as you were<br>
As I want you to be<br>
As a friend, as a friend<br>
As an old enemy<br>
Take your time, hurry up<br>
Choice is yours, don’t be late<br>
Take a rest, as a friend<br>
As an old memoria…</em></p>


<!-- -->


</section>


<section role="doc-bibliography" id="quarto-appendix"><h2>References</h2><div id="refs" data-entry-spacing="0" role="list">
<p>
Abbott, Frank T, Apperson H Johnson, Stephen D Prior, and Donald D Steiner. 2007. <span>“Integrated <span>Biological Warfare Technology Platform</span> (<span>IBWTP</span>). <span>Intelligent Software Supporting Situation Awareness</span>, <span>Response</span>, and <span>Operations</span>,”</span> January.
</p>


<div id="ref-baardAIFounderBlasts2003" role="listitem"><p>
Baard, Mark. 2003. <span>“<span>AI Founder Blasts Modern Research</span>.”</span> <em>Wired</em>, May. <a href="https://www.wired.com/2003/05/ai-founder-blasts-modern-research/">https://www.wired.com/2003/05/ai-founder-blasts-modern-research/</a>.
</p></div>
<p>
Brants, Thorsten, Ashok Popat, Peng Xu, Franz Josef Och, and Jeffrey Dean. 2007. <span>“Large Language Models in Machine Translation.”</span> In <em>Proceedings of the 2007 <span>Joint Conference</span> on <span>Empirical Methods</span> in <span>Natural Language Processing</span> and <span>Computational Natural Language Learning</span> (<span>EMNLP-CoNLL</span>)</em>, 858–67. <a href="https://aclanthology.org/D07-1090.pdf">https://aclanthology.org/D07-1090.pdf</a>.
</p>
<p>
Brooks, Rodney A. 1990. <span>“Elephants Don’t Play Chess.”</span> <em>Robotics and Autonomous Systems</em> 6 (1-2): 3–15. <a href="https://doi.org/10/bk6">https://doi.org/10/bk6</a>.
</p>
<p>
Conesa, Jordi, Veda C. Storey, and Vijayan Sugumaran. 2010. <span>“Usability of Upper Level Ontologies: <span>The</span> Case of <span>ResearchCyc</span>.”</span> <em>Data &amp; Knowledge Engineering</em>, Including <span>Special Section</span>: 12th <span>International Conference</span> on <span>Applications</span> of <span>Natural Language</span> to <span>Information Systems</span> (<span>NLDB</span>’07) – <span>Three</span> selected and extended papers, 69 (4): 343–56. <a href="https://doi.org/10.1016/j.datak.2009.08.002">https://doi.org/10.1016/j.datak.2009.08.002</a>.
</p>

<div id="ref-cycorpTerrorismKnowledgeBase2008" role="listitem"><p>
Cycorp. 2008. <span>“Terrorism <span>Knowledge Base</span> (<span>TKB</span>): <span>Final Technical Report</span>.”</span> AFRL-RI-RS-TR-2008-125. <a href="http://archive.org/details/DTIC_ADA481467">http://archive.org/details/DTIC_ADA481467</a>.
</p></div>

<div id="ref-davisEvaluatingCYCPreliminary2016" role="listitem"><p>
Davis, Ernest. 2016. <span>“Evaluating <span>CYC</span>: <span>Preliminary Notes</span>.”</span> <a href="https://cs.nyu.edu/~davise/papers/CYCEval.pdf">https://cs.nyu.edu/~davise/papers/CYCEval.pdf</a>.
</p></div>
<p>
Davis, Ernest, and Gary Marcus. 2015. <span>“Commonsense Reasoning and Commonsense Knowledge in Artificial Intelligence.”</span> <em>Communications of the ACM</em> 58 (9): 92–103. <a href="https://doi.org/10.1145/2701413">https://doi.org/10.1145/2701413</a>.
</p>
<p>
Erman, Lee D., Frederick Hayes-Roth, Victor R. Lesser, and D. Raj Reddy. 1980. <span>“The <span>Hearsay-II Speech-Understanding System</span>: <span>Integrating Knowledge</span> to <span>Resolve Uncertainty</span>.”</span> <em>ACM Computing Surveys</em> 12 (2): 213–53. <a href="https://doi.org/10.1145/356810.356816">https://doi.org/10.1145/356810.356816</a>.
</p>
<p>
Foxvog, Douglas. 2010. <span>“Cyc.”</span> In <em>Theory and <span>Applications</span> of <span>Ontology</span>: <span>Computer Applications</span></em>, edited by Roberto Poli, Michael Healy, and Achilles Kameas, 259–78. Dordrecht: Springer Netherlands. <a href="https://doi.org/10.1007/978-90-481-8847-5_12">https://doi.org/10.1007/978-90-481-8847-5_12</a>.
</p>
<p>
Friedland, Noah S., Paul G. Allen, Gavin Matthews, Michael Witbrock, David Baxter, Jon Curtis, Blake Shepard, Pierluigi Miraglia, Jurgen Angele, and Steffen Staab. 2004. <span>“Project Halo: <span>Towards</span> a Digital Aristotle.”</span> <em>AI Magazine</em> 25 (4): 29–29. <a href="https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/1783">https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/1783</a>.
</p>
<div id="ref-guhaReCycLingPaper1993" role="listitem"><p>
Guha, R. V., and Douglas B. Lenat. 1993. <span>“Re: <span>CycLing</span> Paper Reviews.”</span> <em>Artificial Intelligence</em> 61 (1): 149–74. <a href="https://doi.org/10.1016/0004-3702(93)90100-P">https://doi.org/10.1016/0004-3702(93)90100-P</a>.
</p></div>
<div id="ref-guhaEnablingAgentsWork1994" role="listitem"><p>
———. 1994. <span>“Enabling Agents to Work Together.”</span> <em>Communications of the ACM</em> 37 (7): 126–42. <a href="https://doi.org/10.1145/176789.176804">https://doi.org/10.1145/176789.176804</a>.
</p></div>
<div id="ref-haaseInventionExplorationDiscovery1990" role="listitem"><p>
Haase, Kenneth W. 1990. <span>“Invention and Exploration in Discovery.”</span> Thesis, Massachusetts Institute of Technology. <a href="https://dspace.mit.edu/handle/1721.1/14257">https://dspace.mit.edu/handle/1721.1/14257</a>.
</p></div>
<div id="ref-hawkinsPredictingTerroristsNext2003" role="listitem"><p>
Hawkins, Dana. 2003. <span>“Predicting Terrorists Next Moves with Common-Sense Computing.”</span> <em>U.S. News &amp; World Report</em>, April. <a href="https://web.archive.org/web/20070328224448/http://www.usnews.com/usnews/culture/articles/030407/7data.b.htm">https://web.archive.org/web/20070328224448/http://www.usnews.com/usnews/culture/articles/030407/7data.b.htm</a>.
</p></div>
<div id="ref-hiltzikBirthThinkingMachine2001" role="listitem"><p>
Hiltzik, Michael a. 2001. <span>“Birth of a <span>Thinking Machine</span>.”</span> <em>Los Angeles Times</em>, June. <a href="https://www.latimes.com/archives/la-xpm-2001-jun-21-mn-12881-story.html">https://www.latimes.com/archives/la-xpm-2001-jun-21-mn-12881-story.html</a>.
</p></div>
<p>
Hutchins, W. John. 2023. <span>“Machine <span>Translation</span>: <span>History</span> of <span>Research</span> and <span>Applications</span>.”</span> In <em>Routledge <span>Encyclopedia</span> of <span>Translation Technology</span></em>, 2nd ed. Routledge.
</p>
<p>
Johnson, George. 1986. <em>Machinery of the Mind: Inside the New Science of Artificial Intelligence</em>. Redmond, Washington: Tempus.
</p>
<div id="ref-knightAI30Years2016" role="listitem"><p>
Knight, Will. 2016. <span>“An <span>AI</span> with 30 <span>Years</span>’ <span>Worth</span> of <span>Knowledge Finally Goes</span> to <span>Work</span>.”</span> <em>MIT Technology Review</em>, March. <a href="https://www.technologyreview.com/2016/03/14/108873/an-ai-with-30-years-worth-of-knowledge-finally-goes-to-work/">https://www.technologyreview.com/2016/03/14/108873/an-ai-with-30-years-worth-of-knowledge-finally-goes-to-work/</a>.
</p></div>
<div id="ref-langleyDataDrivenDiscoveryPhysical1981" role="listitem"><p>
Langley, Pat. 1981. <span>“Data-<span>Driven Discovery</span> of <span>Physical Laws</span>.”</span> <em>Cognitive Science</em> 5 (1): 31–54. <a href="https://doi.org/10.1111/j.1551-6708.1981.tb00869.x">https://doi.org/10.1111/j.1551-6708.1981.tb00869.x</a>.
</p></div>
<div id="ref-lenatAMArtificialIntelligence1976" role="listitem"><p>
Lenat, Douglas B. 1976. <em><span>AM</span>: An Artificial Intelligence Approach to Discovery in Mathematics as Heuristic Search.</em> Stanford University. <a href="https://search.proquest.com/openview/7bf7ad428cf88bc7205ff59d0edaed81/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y">https://search.proquest.com/openview/7bf7ad428cf88bc7205ff59d0edaed81/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y</a>.
</p></div>
<div id="ref-lenatUbiquityDiscovery1977" role="listitem"><p>
———. 1977. <span>“The Ubiquity of Discovery.”</span> <em>Artificial Intelligence</em> 9 (3): 257–85. <a href="https://www.sciencedirect.com/science/article/pii/0004370277900248">https://www.sciencedirect.com/science/article/pii/0004370277900248</a>.
</p></div>
<p>
———. 1980. <span>“The <span>Plausible Mutation</span> of <span>DNA</span>.”</span>
</p>
<div id="ref-lenatEURISKOProgramThat1983" role="listitem"><p>
———. 1983a. <span>“<span>EURISKO</span>: A Program That Learns New Heuristics and Domain Concepts: The Nature of Heuristics <span>III</span>: Program Design and Results.”</span> <em>Artificial Intelligence</em> 21 (1-2): 61–98. <a href="https://www.sciencedirect.com/science/article/pii/S0004370283800058">https://www.sciencedirect.com/science/article/pii/S0004370283800058</a>.
</p></div>
<div id="ref-lenatTheoryFormationHeuristic1983" role="listitem"><p>
———. 1983b. <span>“Theory Formation by Heuristic Search: <span>The</span> Nature of Heuristics <span>II</span>: Background and Examples.”</span> <em>Artificial Intelligence</em> 21 (1-2): 31–59. <a href="https://www.sciencedirect.com/science/article/pii/S0004370283800046">https://www.sciencedirect.com/science/article/pii/S0004370283800046</a>.
</p></div>
<p>
———. 1983c. <span>“The Role of Heuristics in Learning by Discovery: <span>Three</span> Case Studies.”</span> In <em>Machine <span>Learning</span></em>, edited by Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell, 243–306. San Francisco (CA): Morgan Kaufmann. <a href="https://doi.org/10.1016/B978-0-08-051054-5.50013-3">https://doi.org/10.1016/B978-0-08-051054-5.50013-3</a>.
</p>
<div id="ref-lenatComputerSoftwareIntelligent1984" role="listitem"><p>
———. 1984. <span>“Computer Software for Intelligent Systems.”</span> <em>Scientific American</em> 251 (3): 204–14. <a href="https://www.jstor.org/stable/24920354">https://www.jstor.org/stable/24920354</a>.
</p></div>
<div id="ref-lenatArtificialIntelligence1995" role="listitem"><p>
———. 1995a. <span>“Artificial <span>Intelligence</span>.”</span> <em>Scientific American</em> 273 (3): 80–82. <a href="https://www.jstor.org/stable/24981725">https://www.jstor.org/stable/24981725</a>.
</p></div>
<p>
———. 1995b. <span>“Cyc: A Large-Scale Investment in Knowledge Infrastructure.”</span> <em>Communications of the ACM</em> 38 (11): 33–38. <a href="https://doi.org/10.1145/219717.219745">https://doi.org/10.1145/219717.219745</a>.
</p>
<p>
———. 2005. <span>“Applied Ontology Issues.”</span> <em>Applied Ontology: An Interdisciplinary Journal of Ontological Analysis and Conceptual Modeling</em> 1 (1): 9–12. <a href="https://doi.org/10.3233/APO-2005-000002">https://doi.org/10.3233/APO-2005-000002</a>.
</p>
<div id="ref-lenatGoogleTechTalksComputers2006" role="listitem"><p>
———. 2006. <span>“Google <span>TechTalks</span>: <span>Computers</span> Versus <span>Common Sense</span>.”</span> <a href="https://www.youtube.com/watch?v=gAtn-4fhuWA">https://www.youtube.com/watch?v=gAtn-4fhuWA</a>.
</p></div>
<div id="ref-lenatVoiceTurtleWhatever2008" role="listitem"><p>
———. 2008. <span>“The Voice of the Turtle: <span>Whatever</span> Happened to Ai?”</span> <em>AI Magazine</em> 29 (2): 11–22. <a href="https://onlinelibrary.wiley.com/doi/abs/10.1609/aimag.v29i2.2106">https://onlinelibrary.wiley.com/doi/abs/10.1609/aimag.v29i2.2106</a>.
</p></div>
<p>
———. 2009. <span>“Building a <span>Machine Smart Enough</span> to <span>Pass</span> the <span>Turing Test</span>.”</span> In <em>Parsing the <span>Turing Test</span>: <span>Philosophical</span> and <span>Methodological Issues</span> in the <span>Quest</span> for the <span>Thinking Computer</span></em>, edited by Robert Epstein, Gary Roberts, and Grace Beber, 261–82. Dordrecht: Springer Netherlands. <a href="https://doi.org/10.1007/978-1-4020-6710-5_16">https://doi.org/10.1007/978-1-4020-6710-5_16</a>.
</p>
<div id="ref-lenat50ShadesSymbolic2015" role="listitem"><p>
———. 2015. <span>“50 <span>Shades</span> of <span>Symbolic Representation</span> and <span>Reasoning</span>.”</span> <a href="https://www.youtube.com/watch?v=4mv0nCS2mik">https://www.youtube.com/watch?v=4mv0nCS2mik</a>.
</p></div>
<div id="ref-lenatSometimesVeneerIntelligence2017" role="listitem"><p>
———. 2017. <span>“Sometimes the <span>Veneer</span> of <span>Intelligence</span> Is <span>Not Enough</span>.”</span> <em>COGNITIVE WORLD</em>. <a href="https://cognitiveworld.com/articles/sometimes-veneer-intelligence-not-enough">https://cognitiveworld.com/articles/sometimes-veneer-intelligence-not-enough</a>.
</p></div>


<div id="ref-lenatCycQuestSolve2021" role="listitem"><p>
———. 2021. <span>“Cyc and the <span>Quest</span> to <span>Solve Common Sense Reasoning</span> in <span>AI</span>.”</span> <a href="https://lexfridman.com/douglas-lenat/">https://lexfridman.com/douglas-lenat/</a>.
</p></div>
<div id="ref-lenatCreating30MillionRuleSystem2022" role="listitem"><p>
———. 2022a. <span>“Creating a 30-<span>Million-Rule System</span>: <span>MCC</span> and <span>Cycorp</span>.”</span> <em>IEEE Annals of the History of Computing</em> 44 (1): 44–56. <a href="https://doi.org/10.1109/MAHC.2022.3149468">https://doi.org/10.1109/MAHC.2022.3149468</a>.
</p></div>
<div id="ref-lenatACS2022Invited2022" role="listitem"><p>
———. 2022b. <span>“[<span>ACS</span> 2022] <span>Invited Talk</span>: <span>Computers</span> Versus <span>Common Sense</span>.”</span> <a href="https://www.youtube.com/watch?v=VjkbmLjwXO8">https://www.youtube.com/watch?v=VjkbmLjwXO8</a>.
</p></div>
<div id="ref-lenatWhyAmEurisko1984" role="listitem"><p>
Lenat, Douglas B., and John Seely Brown. 1984. <span>“Why <span>AM</span> and <span>EURISKO</span> Appear to Work.”</span> <em>Artificial Intelligence</em> 23 (3): 269–94. <a href="https://doi.org/10.1016/0004-3702(84)90016-X">https://doi.org/10.1016/0004-3702(84)90016-X</a>.
</p></div>
<div id="ref-lenatThresholdsKnowledge1991" role="listitem"><p>
Lenat, Douglas B., and Edward A. Feigenbaum. 1991. <span>“On the Thresholds of Knowledge.”</span> <em>Artificial Intelligence</em> 47 (1): 185–250. <a href="https://doi.org/10.1016/0004-3702(91)90055-O">https://doi.org/10.1016/0004-3702(91)90055-O</a>.
</p></div>
<div id="ref-lenatEfficientPathfindingVery2007" role="listitem"><p>
Lenat, Douglas B., Keith Goolsbey, Kevin Knight, and Pace Smith. 2007. <span>“Efficient <span>Pathfinding</span> in <span>Very Large Data Spaces</span> : <span>Defense Technical Information Center</span>.”</span> ADA475387. <a href="https://archive.org/details/DTIC_ADA475387/page/n1/mode/2up">https://archive.org/details/DTIC_ADA475387/page/n1/mode/2up</a>.
</p></div>
<p>
Lenat, Douglas B., and R. V. Guha. 1989. <em>Building Large Knowledge-Based Systems: Representation and Inference in the <span>Cyc</span> Project</em>. Reading, Mass: Addison-Wesley Pub. Co.
</p>
<div id="ref-lenatCycMidtermReport1990" role="listitem"><p>
Lenat, Douglas B., and Ramanathan V. Guha. 1990. <span>“Cyc: <span>A</span> Midterm Report.”</span> <em>AI Magazine</em> 11 (3): 32–32. <a href="https://ojs.aaai.org/index.php/aimagazine/article/view/842">https://ojs.aaai.org/index.php/aimagazine/article/view/842</a>.
</p></div>
<p>
Lenat, Douglas B., and Gary Marcus. 2023. <span>“Getting from <span>Generative AI</span> to <span>Trustworthy AI</span>: <span>What LLMs</span> Might Learn from <span>Cyc</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2308.04445">https://doi.org/10.48550/arXiv.2308.04445</a>.
</p>
<p>
Lenat, Douglas B., Mayank Prakash, and Mary Shepherd. 1985. <span>“Cyc: <span>Using</span> Common Sense Knowledge to Overcome Brittleness and Knowledge Acquisition Bottlenecks.”</span> <em>AI Magazine</em> 6 (4): 65–65. <a href="https://ojs.aaai.org/index.php/aimagazine/article/view/510/0">https://ojs.aaai.org/index.php/aimagazine/article/view/510/0</a>.
</p>
<p>
Lenat, Douglas B., Michael Witbrock, David Baxter, Eugene Blackstone, Chris Deaton, Dave Schneider, Jerry Scott, and Blake Shepard. 2010. <span>“Harnessing <span>Cyc</span> to Answer Clinical Researchers’ Ad Hoc Queries.”</span> <em>AI Magazine</em> 31 (3): 13–32. <a href="https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2299">https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2299</a>.
</p>
<p>
Love, Dylan. 2014. <span>“The <span>Most Ambitious Artificial Intelligence Project In The World Has Been Operating In Near Secrecy For</span> 30 <span>Years</span>.”</span> <em>Business Insider</em>, July. <a href="https://www.businessinsider.com/cycorp-ai-2014-7">https://www.businessinsider.com/cycorp-ai-2014-7</a>.
</p>
<div id="ref-metzOneGeniusLonely2016" role="listitem"><p>
Metz, Cade. 2016. <span>“One <span>Genius</span>’ <span>Lonely Crusade</span> to <span>Teach</span> a <span>Computer Common Sense</span>.”</span> <em>Wired</em>, March. <a href="https://www.wired.com/2016/03/doug-lenat-artificial-intelligence-common-sense-engine/">https://www.wired.com/2016/03/doug-lenat-artificial-intelligence-common-sense-engine/</a>.
</p></div>
<p>
Newell, Allen, and Herbert Alexander Simon. 1972. <em>Human Problem Solving</em>.
</p>
<p>
Panton, Kathy, Cynthia Matuszek, Douglas B. Lenat, Dave Schneider, Michael Witbrock, Nick Siegel, and Blake Shepard. 2006. <span>“Common <span>Sense Reasoning</span> – <span>From Cyc</span> to <span>Intelligent Assistant</span>.”</span> In <em>Ambient <span>Intelligence</span> in <span>Everyday Life</span>: <span>Foreword</span> by <span>Emile Aarts</span></em>, edited by Yang Cai and Julio Abascal, 1–31. Berlin, Heidelberg: Springer. <a href="https://doi.org/10.1007/11825890_1">https://doi.org/10.1007/11825890_1</a>.
</p>
<div id="ref-paulheimHowMuchTriple2018" role="listitem"><p>
Paulheim, Heiko. 2018. <span>“How Much Is a Triple.”</span> In <em><span>IEEE International Semantic Web Conference</span></em>. <a href="http://www.heikopaulheim.com/docs/iswc_bluesky_cost2018.pdf">http://www.heikopaulheim.com/docs/iswc_bluesky_cost2018.pdf</a>.
</p></div>
<p>
Pierce, Christopher D., David Booth, Chimezie Ogbuji, Chris Deaton, Eugene Blackstone, and Doug Lenat. 2012. <span>“<span>SemanticDB</span>: <span>A Semantic Web Infrastructure</span> for <span>Clinical Research</span> and <span>Quality Reporting</span>.”</span> <em>Current Bioinformatics</em> 7 (3): 267–77. <a href="https://doi.org/10.2174/157489312802460730">https://doi.org/10.2174/157489312802460730</a>.
</p>
<div id="ref-prattCYCReportPratt1994" role="listitem"><p>
Pratt, Vaughan. 1994. <span>“<span>CYC Report</span> – <span>V</span>. <span>Pratt</span>.”</span> <a href="http://boole.stanford.edu/cyc.html">http://boole.stanford.edu/cyc.html</a>.
</p></div>
<div id="ref-richmanAllenClaimsSuccess2003" role="listitem"><p>
Richman, Dan. 2003. <span>“Allen Claims Success in Work on Computers That Can Reason.”</span> <em>Seattle Post-Intelligencer</em>, June. <a href="https://www.seattlepi.com/business/article/allen-claims-success-in-work-on-computers-that-1117119.php">https://www.seattlepi.com/business/article/allen-claims-success-in-work-on-computers-that-1117119.php</a>.
</p></div>
<div id="ref-ritchieAMCaseStudy1984" role="listitem"><p>
Ritchie, Graeme D., and F. Keith Hanna. 1984. <span>“<span>AM</span>: <span>A</span> Case Study in <span>AI</span> Methodology.”</span> <em>Artificial Intelligence</em> 23 (3): 249–68. <a href="https://www.sciencedirect.com/science/article/pii/0004370284900158">https://www.sciencedirect.com/science/article/pii/0004370284900158</a>.
</p></div>
<p>
Sakaguchi, Keisuke, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. <span>“<span>WinoGrande</span>: An Adversarial Winograd Schema Challenge at Scale.”</span> <em>Communications of the ACM</em> 64 (9): 99–106. <a href="https://doi.org/10.1145/3474381">https://doi.org/10.1145/3474381</a>.
</p>
<p>
Sarjant, Samuel, Catherine Legg, Michael Robinson, and Olena Medelyan. 2009. <span>“" <span>All You Can Eat</span>" <span>Ontology-Building</span>: <span>Feeding Wikipedia</span> to <span>Cyc</span>.”</span> In <em>2009 <span>IEEE</span>/<span>WIC</span>/<span>ACM International Joint Conference</span> on <span>Web Intelligence</span> and <span>Intelligent Agent Technology</span></em>, 1:341–48. IEEE. <a href="https://ieeexplore.ieee.org/abstract/document/5286048/">https://ieeexplore.ieee.org/abstract/document/5286048/</a>.
</p>
<p>
Shapiro, Stewart. 1991. <em>Foundations <span>Without Foundationalism</span>: <span>A Case</span> for <span>Second-Order Logic</span></em>. Oxford <span>Logic Guides</span> v.17. Oxford: Oxford University Press, UK.
</p>
<p>
Shepard, Blake, Cynthia Matuszek, C. Bruce Fraser, William Wechtenhiser, David Crabbe, Zelal Güngördü, John Jantos, et al. 2005. <span>“A Knowledge-Based Approach to Network Security: Applying <span>Cyc</span> in the Domain of Network Risk Assessment.”</span> In <em>Proceedings of the 17th Conference on <span>Innovative</span> Applications of Artificial Intelligence - <span>Volume</span> 3</em>, 1563–68. <span>IAAI</span>’05. Pittsburgh, Pennsylvania: AAAI Press.
</p>
<div id="ref-shilohHeTaughtAI2023" role="listitem"><p>
Shiloh, Kali. 2023. <span>“He <span>Taught AI</span> the <span>Facts</span> of <span>Life</span>.”</span> <em>Stanford Magazine</em>. <a href="https://stanfordmag.org/contents/he-taught-ai-the-facts-of-life">https://stanfordmag.org/contents/he-taught-ai-the-facts-of-life</a>.
</p></div>
<div id="ref-skorinkinABBYYsBitterLesson2024" role="listitem"><p>
Skorinkin, Daniil. 2024. <span>“<span>ABBYY’s Bitter Lesson: How Linguists Lost the Last Battle for NLP</span>.”</span> <em>System Block</em>. <a href="https://sysblok.ru/blog/gorkij-urok-abbyy-kak-lingvisty-proigrali-poslednjuju-bitvu-za-nlp/">https://sysblok.ru/blog/gorkij-urok-abbyy-kak-lingvisty-proigrali-poslednjuju-bitvu-za-nlp/</a>.
</p></div>
<div id="ref-storkHALsLegacy2001s1998" role="listitem"><p>
Stork, David G., ed. 1998. <em><span>HAL</span>’s <span>Legacy</span>: 2001’s <span>Computer</span> as <span>Dream</span> and <span>Reality</span></em>. The MIT Press. <a href="https://doi.org/10.7551/mitpress/3404.001.0001">https://doi.org/10.7551/mitpress/3404.001.0001</a>.
</p></div>
<div id="ref-thompsonKnowItAllMachine2001" role="listitem"><p>
Thompson, Clive. 2001. <span>“The <span>Know-It-All Machine</span>.”</span> <em>Lingua Franca</em> 11 (6). <a href="http://linguafranca.mirror.theinfo.org/print/0109/cover.html">http://linguafranca.mirror.theinfo.org/print/0109/cover.html</a>.
</p></div>
<p><em>Traveller <span>Book</span> 5: <span>High Guard</span></em>. 1980. 2nd ed. Game Designers’ Workshop.
</p>
<div id="ref-walkerHowFeasibleAutomated1987" role="listitem"><p>
Walker, Michael G. 1987. <span>“How <span>Feasible Is Automated Discovery</span>?”</span> <em>IEEE Expert</em> 2 (1): 69–82. <a href="https://doi.org/10.1109/MEX.1987.4307036">https://doi.org/10.1109/MEX.1987.4307036</a>.
</p></div>
<div id="ref-wallichSiliconBabies1991" role="listitem"><p>
Wallich, Paul. 1991. <span>“Silicon <span>Babies</span>.”</span> <em>Scientific American</em> 265 (6): 124–35. <a href="https://www.jstor.org/stable/24938839">https://www.jstor.org/stable/24938839</a>.
</p></div>

<div id="ref-woodCycorpCostCommon2005" role="listitem"><p>
Wood, Lamont. 2005. <span>“Cycorp: <span>The Cost</span> of <span>Common Sense</span>.”</span> <em>MIT Technology Review</em>, March. <a href="https://www.technologyreview.com/2005/03/01/274581/cycorp-the-cost-of-common-sense-2/">https://www.technologyreview.com/2005/03/01/274581/cycorp-the-cost-of-common-sense-2/</a>.
</p></div>
</div></section></main> <!-- /main -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apache ECharts (497 pts)]]></title>
            <link>https://echarts.apache.org/en/index.html</link>
            <guid>43624220</guid>
            <pubDate>Tue, 08 Apr 2025 17:23:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://echarts.apache.org/en/index.html">https://echarts.apache.org/en/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=43624220">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><div id="home-section"><p>Apache ECharts</p><p>An Open Source JavaScript Visualization Library</p><p><a href="https://echarts.apache.org/handbook/en/get-started"><svg width="22px" height="19px" viewBox="0 0 17 22" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path fill="#fff" d="M10.9,16.2 C10.9,16.8 10.5,17.2 9.9,17.2 L5.3,17.2 C4.7,17.2 4.3,16.8 4.3,16.2 C4.3,15.6 4.7,15.2 5.3,15.2 L9.9,15.2 C10.5,15.2 10.9,15.7 10.9,16.2 Z M13.6,7.5 L5.3,7.5 C4.7,7.5 4.3,7.9 4.3,8.5 C4.3,9.1 4.7,9.5 5.3,9.5 L13.6,9.5 C14.2,9.5 14.6,9.1 14.6,8.5 C14.6,7.9 14.2,7.5 13.6,7.5 Z M17.9,6 L17.9,19 C17.9,20.7 16.6,22 14.9,22 L3.9,22 C2.2,22 0.9,20.7 0.9,19 L0.9,7.5 L0.9,6 L0.9,5 L0.9,4 L0.9,2 C0.9,0.9 1.8,0 2.9,0 L12.9,0 C14,0 14.9,0.9 14.9,2 L14.9,3 C16.6,3 17.9,4.3 17.9,6 Z M2.9,3 L3.9,3 L12.9,3 L12.9,2 L2.9,2 L2.9,3 Z M15.9,6 C15.9,5.4 15.5,5 14.9,5 L3.9,5 C3.3,5 2.9,5.4 2.9,6 L2.9,19 C2.9,19.6 3.3,20 3.9,20 L14.9,20 C15.5,20 15.9,19.6 15.9,19 L15.9,6 Z M13.6,11.5 L5.3,11.5 C4.7,11.5 4.3,11.9 4.3,12.5 C4.3,13.1 4.7,13.5 5.3,13.5 L13.6,13.5 C14.2,13.5 14.6,13.1 14.6,12.5 C14.6,11.9 14.2,11.5 13.6,11.5 Z"></path></svg><span>Get Started</span></a><a href="https://echarts.apache.org/examples/en/index.html"><svg width="25px" height="19px" viewBox="0 0 17 22" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path fill="#F72C5B" d="M21,0 L6,0 C4.9,0 4,0.9 4,2 L4,4 L2,4 C0.9,4 0,4.9 0,6 L0,18 C0,19.1 0.9,20 2,20 L17,20 C18.1,20 19,19.1 19,18 L19,16 L21,16 C22.1,16 23,15.1 23,14 L23,2 C23,0.9 22.1,0 21,0 Z M17,18 L2,18 L2,6 L17,6 L17,18 Z M21,14 L19,14 L19,6 C19,4.9 18.1,4 17,4 L6,4 L6,2 L21,2 L21,14 Z M4,15 L4,12 C4,11.4 4.4,11 5,11 C5.6,11 6,11.4 6,12 L6,15 C6,15.6 5.6,16 5,16 C4.4,16 4,15.6 4,15 Z M13,15 L13,12 C13,11.4 13.4,11 14,11 C14.6,11 15,11.4 15,12 L15,15 C15,15.6 14.6,16 14,16 C13.4,16 13,15.6 13,15 Z M7,15 L7,9 C7,8.4 7.4,8 8,8 C8.6,8 9,8.4 9,9 L9,15 C9,15.6 8.6,16 8,16 C7.4,16 7,15.6 7,15 Z M10,15 L10,11 C10,10.4 10.4,10 11,10 C11.6,10 12,10.4 12,11 L12,15 C12,15.6 11.6,16 11,16 C10.4,16 10,15.6 10,15 Z"></path></svg><span>Demo</span></a><a href="https://github.com/apache/echarts" target="_blank"><svg width="22px" height="22px" viewBox="0 0 22 22" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g transform="translate(-262.000000, -2480.000000)" fill="#081642"><g transform="translate(252.000000, 2470.000000)"><path d="M29.9006449,27.904446 C28.5016932,29.8668872 26.6944882,31.2248797 24.4790301,31.9784237 C24.2211218,32.0272886 24.0325745,31.9931534 23.9133882,31.876018 C23.7942019,31.7588827 23.7346171,31.6119968 23.7346338,31.4353603 L23.7346338,28.3376019 C23.7346338,27.3880159 23.4865017,26.6930482 22.9902375,26.2526987 C23.5347812,26.1938141 24.024319,26.1058042 24.458851,25.9886688 C24.893383,25.8715335 25.3422785,25.680801 25.8055376,25.4164714 C26.2687966,25.1521418 26.6556591,24.8267354 26.9661251,24.4402521 C27.276591,24.0537688 27.5296112,23.5398222 27.7251857,22.8984123 C27.9207602,22.2570024 28.0185475,21.5203805 28.0185475,20.6885464 C28.0185475,19.504056 27.6414612,18.4959023 26.8872886,17.6640854 C27.240536,16.7733669 27.2023419,15.7752328 26.7727063,14.6696833 C26.5050218,14.5813565 26.1181593,14.6352226 25.6121189,14.8312818 C25.1060784,15.0273409 24.6669591,15.2428141 24.2947609,15.4777015 L23.7502256,15.8300324 C22.8622108,15.5757225 21.9454689,15.4485675 21,15.4485675 C20.0545311,15.4485675 19.1377892,15.5757225 18.2497744,15.8300324 C18.0969813,15.7223001 17.8940788,15.5901353 17.641067,15.433538 C17.3880551,15.2769408 16.9892689,15.0884007 16.4447085,14.8679176 C15.9001481,14.6474346 15.4894467,14.5813479 15.2126043,14.6696576 C14.7921266,15.7758408 14.7588206,16.7739749 15.1126863,17.6640597 C14.3585137,18.4958938 13.9814275,19.5040475 13.9814275,20.6885208 C13.9814275,21.5203548 14.0792147,22.2544676 14.2747892,22.8908591 C14.4703637,23.5272505 14.720944,24.0411971 15.0265302,24.4326988 C15.3321164,24.8242005 15.7165391,25.1521076 16.1797981,25.41642 C16.6430571,25.6807325 17.0919527,25.871465 17.5264847,25.9886174 C17.9610167,26.1057699 18.4505545,26.1937799 18.9950982,26.2526473 C18.6131238,26.60467 18.3790461,27.1085884 18.292865,27.7644025 C18.0924107,27.8621152 17.8775846,27.935404 17.6483866,27.984269 C17.4191886,28.0331339 17.1469167,28.0575663 16.831571,28.0575663 C16.5162253,28.0575663 16.203612,27.9523346 15.8937309,27.7418712 C15.5838499,27.5314078 15.3189144,27.225424 15.0989244,26.8239198 C14.9174043,26.5107253 14.6857747,26.2564154 14.4040358,26.0609899 C14.1222969,25.8655645 13.8860801,25.7481209 13.6953854,25.708659 L13.4084408,25.6644956 C13.2079866,25.6644956 13.069557,25.6864188 12.9931521,25.7302654 C12.9167472,25.7741119 12.8929083,25.8304873 12.9216353,25.8993914 C12.9503623,25.9682956 12.9934529,26.0368829 13.050907,26.1051533 C13.1083611,26.1734238 13.1703941,26.2319915 13.2370061,26.2808564 L13.336924,26.3541538 C13.5471712,26.4518665 13.7549701,26.6378974 13.9603209,26.9122466 C14.1656716,27.1865958 14.3160164,27.4362043 14.4113555,27.661072 L14.554364,27.9983735 C14.67843,28.3704354 14.8886689,28.6714094 15.1850804,28.9012955 C15.481492,29.1311815 15.8014334,29.2780674 16.1449045,29.3419532 C16.4883756,29.405839 16.8202406,29.4399742 17.1404995,29.4443589 C17.4607584,29.4487435 17.7256939,29.4315218 17.935306,29.3926936 L18.2644137,29.3335008 C18.2644137,29.7055627 18.2668619,30.1412107 18.2717584,30.6404447 C18.2766548,31.1396787 18.279103,31.4040083 18.279103,31.4334334 C18.279103,31.6094533 18.21707,31.7563392 18.093004,31.8740912 C17.9689379,31.9918431 17.7779507,32.0259784 17.5200424,31.9764969 C15.3045843,31.22297 13.4973793,29.8649774 12.0984276,27.9025191 C10.6994759,25.9400608 10,23.7305118 10,21.2738721 C10,19.2281225 10.4916769,17.3417791 11.4750308,15.6148418 C12.4583846,13.8879045 13.7928551,12.5202092 15.4784422,11.5117558 C17.1640293,10.5033024 19.0045486,9.99938397 21,10.0000006 C22.9954514,10.0006172 24.8359707,10.5045356 26.5215578,11.5117558 C28.2071449,12.518976 29.5416154,13.8866713 30.5249692,15.6148418 C31.5083231,17.3430123 32,19.2293557 32,21.2738721 C31.9990725,23.7324558 31.2995966,25.9420048 29.9006449,27.904446 Z"></path></g></g></g></svg></a></p></div><section id="new-version-section">📣 Apache ECharts 5.6 is out! See <a href="https://echarts.apache.org/handbook/en/basics/release-note/5-6-0?ref=banner" target="_blank">what's new</a></section><div id="feature-section"><div id="index-feature-1"><h3>Flexible Chart Types</h3><p>Apache ECharts provides more than 20 chart types available out of the box, along with a dozen components, and each of them can be arbitrarily combined to use.</p></div><div id="index-feature-2"><h3>Powerful Rendering Engine</h3><p>Easily switch between Canvas and SVG rendering. Progressive rendering and stream loading make it possible to render 10 million data in realtime.</p></div><div id="index-feature-3"><h3>Professional Data Analysis</h3><p>Manage data through datasets, which support data transforms like filtering, clustering, and regression to help analyze multi-dimensional analysis of the same data.</p></div><div id="index-feature-4"><h3>Elegant Visual Design</h3><p>The default design follows visualization principles, supports responsive design. Flexible configurations make it easy to customize.</p></div><div id="index-feature-5"><h3>A Healthy Community</h3><p>The active open source community ensures the healthy development of the project and contributes a wealth of third-party extensions.</p></div><div id="index-feature-6"><h3>Accessibility-Friendly</h3><p>Automatically generated chart descriptions and decal patterns help users with disabilities understand the content and the stories behind the charts.</p></div></div><div id="publication"><h2>ECharts: A Declarative Framework for Rapid Construction of Web-based Visualization</h2><p>You are welcomed to cite the following paper whenever you use ECharts in your R&amp;D projects, products, research papers, technical reports, news reports, books, presentations, teaching, patents, and other related intelligence activities.</p></div><div id="about-section"><h3>Follow</h3><hr><p>Follow us to get more updates in time.</p><div><a id="btn-weixin"><div><p><img src="https://echarts.apache.org/en/images/ercode.jpg"></p></div></a></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Thank HN: The puzzle game I posted here 6 weeks ago got licensed by The Atlantic (551 pts)]]></title>
            <link>https://www.theatlantic.com/games/bracket-city/</link>
            <guid>43622719</guid>
            <pubDate>Tue, 08 Apr 2025 15:11:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/games/bracket-city/">https://www.theatlantic.com/games/bracket-city/</a>, See on <a href="https://news.ycombinator.com/item?id=43622719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><nav aria-labelledby="site-navigation" data-event-module="site nav" id="main-navigation"><div><p><a href="#main-content">Skip to content</a></p><h2 id="site-navigation">Site Navigation</h2><div><ul><li><a href="https://www.theatlantic.com/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 87.83 134"><title>The Atlantic</title><path d="M24.48 95.13c-.56 0-.74-.37-.74-.93l13.08-55.88c.19-.94.93-.94 1.12 0L50.09 94.2c0 .56-.19.93-.75.93zM48.22.19a22.54 22.54 0 01-7.66 5.05c-.75.19-.94.37-1.13 1.12l-26.72 112.5c-2 9-4.67 10.66-11.77 11.22a.88.88 0 00-.94.93v2.06a.88.88 0 00.92.93h25.6a.88.88 0 00.93-.93V131a.88.88 0 00-.93-.93c-9.53 0-10.47-2.81-8.6-10.66l4.49-19.25a1.18 1.18 0 011.12-.93h26.74a1.19 1.19 0 011.13.93l5 23.18c1.12 5-.75 6.17-7.1 6.73a.88.88 0 00-.93.93v2.06a.88.88 0 00.93.93h37.62a.88.88 0 00.94-.93V131a.88.88 0 00-.94-.93c-5.79-.56-8.22-1.5-9.34-6.73L49.34.57c-.19-.56-.75-.75-1.12-.38"></path></svg></a></li><li></li><li></li><li><a href="https://www.theatlantic.com/most-popular/">Popular</a></li><li><a href="https://www.theatlantic.com/latest/">Latest</a></li><li><a href="https://www.theatlantic.com/newsletters/">Newsletters</a></li></ul><div><ul data-event-element="account links"><li><a href="https://accounts.theatlantic.com/login/">Sign In</a></li><li><a href="https://www.theatlantic.com/subscribe/navbar/">Subscribe</a></li></ul></div></div></div></nav></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Better typography with text-wrap pretty (105 pts)]]></title>
            <link>https://webkit.org/blog/16547/better-typography-with-text-wrap-pretty/</link>
            <guid>43622703</guid>
            <pubDate>Tue, 08 Apr 2025 15:10:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webkit.org/blog/16547/better-typography-with-text-wrap-pretty/">https://webkit.org/blog/16547/better-typography-with-text-wrap-pretty/</a>, See on <a href="https://news.ycombinator.com/item?id=43622703">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-16547">
            
            

            <div>
                <menu><menuitem></menu>                
                <p>Support for <code>text-wrap: pretty</code> just shipped in <a href="https://developer.apple.com/safari/resources/">Safari Technology Preview,</a> bringing an unprecedented level of polish to typography on the web. Let’s take a look at what the WebKit version of <code>pretty</code> does — it’s probably a lot more than you expect. Then, we’ll compare it to <code>balance</code> and the other <code>text-wrap</code> values to better understand when to use which one.</p>
<p>Ideas of what makes for “good” typography are deeply rooted in eras when type was set by hand using metal, wood, or ink. Typesetters took great care when deciding if a word should go on the end of one line, the beginning of the next, or be broken with a hyphen. Their efforts improved comprehension, reduced eye-strain, and simply made the reading experience more pleasant. While beauty can be subjective, with disagreements about what’s “better”, there are also strongly-held typographic traditions around the globe, representing various languages and scripts. These traditions carry people’s culture from one generation to the next, through the centuries.</p>
<p>In digital typography, a computer places all the words, not humans. Often, as a web designer or developer, you are creating a template to be filled with different versions of content. There is no “hand tweaking” typography on the web, especially when the layout is fluid, reflowing to fit different shapes and sizes of screens. So what can we do now to better express the expectations of quality from traditional typography, while still relying on the mechanization brought by today’s computers?</p>
<p>One solution is <code>text-wrap:pretty</code>. It’s intended to bring a new level of polish to type on the web by leveraging paragraph-based algorithms to solve long-standing problems.</p>
<h2><a name="better-typography"></a>Better typography</h2>
<p>There are several things certain typographic traditions teach you to do:</p>
<ol>
<li><strong>Avoid short last lines.</strong> You want to avoid leaving a single word by itself on the end of a paragraph. It can look quite strange, and make space between paragraphs look mistakenly large. </li>
<li><strong>Avoid “bad rag”.</strong> You can look at the ragged edge of the text (the inline-end side) and note whether the general length of lines is kind of consistent, or whether the rag is very jagged. When hand setting, typographers would move words around to minimize visual differences between adjacent lines  — to avoid bag rag. “Good rag” increases readability of the text, and makes the entire block of text look more pleasing. </li>
<li><strong>Avoid poor hyphenation.</strong> For languages that can be hyphenated, hyphenation helps create good rag. It also breaks a word into pieces, and places those pieces as far apart as possible in the inline dimension. This adds to the cognitive load when reading. It’s best to minimize the use of hyphenation and to avoid hyphenating two lines in a row. </li>
<li><strong>Avoid <a href="https://en.wikipedia.org/wiki/River_(typography)">typographic rivers</a>.</strong> If you know to look for rivers, you might start to notice that sometimes the spaces between words randomly line up across lines, creating a path of whitespace through the paragraph. Rivers can be distracting.</li>
</ol>
<p>You can see these four problems in the following example. The text is identical on both the left and the right.</p>
<figure><picture><source src="https://webkit.org/wp-content/uploads/bad-typography-SM-dark.png" type="image/png" media="(prefers-color-scheme: dark)"><img fetchpriority="high" decoding="async" src="https://webkit.org/wp-content/uploads/bad-typography-SM-light.png" alt="A screenshot of paragraph text that demonstrates a short last line, bad rag, bad hyphenation, and a typographic river." width="1652" height="896" srcset="https://webkit.org/wp-content/uploads/bad-typography-SM-light.png 1652w, https://webkit.org/wp-content/uploads/bad-typography-SM-light-300x163.png 300w, https://webkit.org/wp-content/uploads/bad-typography-SM-light-1024x555.png 1024w, https://webkit.org/wp-content/uploads/bad-typography-SM-light-768x417.png 768w, https://webkit.org/wp-content/uploads/bad-typography-SM-light-1536x833.png 1536w" sizes="(max-width: 1652px) 100vw, 1652px"></picture></figure>
<p>Designers and typographers often use hyphenation and/or justification to help improve rag, but on the web, neither provides satisfying results. Until recently, it’s been impossible to do much of anything about short lines, bad rag, or rivers on the web. We’ve just lived with them.</p>
<h3><a name="web-line-layout-since-1991"></a>Web line layout since 1991</h3>
<p>For over 30 years, the web had only one technique for determining where to wrap text.</p>
<p>The browser starts with the first line of text, and lays out each word or syllable, one after another until it runs out of room. As soon as it has no more space to fit another word/syllable, it wraps to the next line (if <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/text-wrap-mode">wrapping</a> is allowed). Then it starts on the next line, fitting all the content it can… then when it runs out of room, it wraps… and starts working on the next line.</p>
<p>It’s always thinking about only one line at a time. It wraps whenever it needs, after it’s fit the maximum amount of content on the previous line. If hyphenation is turned on, it will hyphenate whatever word is last on the line, at whatever point leaves as much of the word on the previous line as possible. Nothing else is taken into consideration — which is why text on the web has bad rag, rivers, short last lines, and hyphenation that makes no sense.</p>
<p>This is not required by the fact that text is laid out by a computer. For decades, software like Adobe InDesign and LaTeX has evaluated multiple lines of text at a time as they decide where to end one line and begin the next. It’s just that the web didn’t use a multiline algorithm. Until now.</p>
<p>We are excited to bring this capability to the web for the first time, in Safari Technology Preview 216.</p>
<h2><a name="text-wrap-pretty"></a>text-wrap: pretty</h2>
<p>Now, the web has the ability to evaluate the whole passage of text when figuring out where best to wrap. You can ask browsers to do this by using <code>text-wrap: pretty</code>. WebKit is not the first browser engine to implement, but we are the first browser to use it to evaluate and adjust the entire paragraph. And we are the first browser to use it to improve rag. We chose to take a more comprehensive approach in our implementation because we want you to be able to use this CSS to make your text easier to read and softer on the eyes, to provide your users with better readability and accessibility. And simply, to make something beautiful.</p>
<p>Safari Technology Preview 216 prevents short lines, improves the rag, and reduces the need for hyphenation — across all of the text, no matter how long it is. We are not yet making adjustments to prevent rivers, although we’d love to in the future.</p>
<p>While <a href="https://caniuse.com/mdn-css_properties_text-wrap_pretty">support for <code>pretty</code></a> shipped in Chrome 117, Edge 177, and Opera 103 in Fall 2023, and Samsung Internet 24 in 2024, the Chromium version is more limited in what it accomplishes. According to <a href="https://developer.chrome.com/blog/css-text-wrap-pretty/">an article</a> by the Chrome team, Chromium only makes adjustments to the last four lines of a paragraph. It’s focused on preventing short last lines. It also adjusts hyphenation if consecutive hyphenated lines appear at the end of a paragraph.</p>
<p>The purpose of <code>pretty</code>, as designed by the CSS Working Group, is for <em>each browser to do what it can</em> to improve how text wraps. The <a href="https://drafts.csswg.org/css-text-4/#text-wrap-style">CSS Text Level 4 specification</a> currently defines it like this, (where “user agent” means the web browser, emphasis added):</p>
<blockquote><p>
  The user agent may among other things attempt to avoid excessively short last lines… but it should also improve the layout in additional ways. <strong>The precise set of improvements is user agent dependent, and may include</strong> things such as: reducing the variation in length between lines; avoiding typographic rivers; prioritizing different classes of soft wrap opportunities, hyphenation opportunities, or justification opportunities; avoiding hyphenation on too many consecutive lines.
</p></blockquote>
<p>The use of the word “may” is very meaningful here. It’s a clear statement that each browser gets to decide for itself exactly what <code>pretty</code> should do. There is no mandate for every browser to make the same choices. In fact, a browser team might decide in 2025 to handle some aspects of improving these qualities, and then change what their implementation does in the future.</p>
<p>Because of the way Chrome’s implementation of <code>pretty</code> has been taught, a lot of web developers expect this value is only supposed to prevent short last lines. But that was never the intention. In fact, the CSS Working Group defined a different value for such a purpose. It was <a href="https://github.com/w3c/csswg-drafts/issues/11283">just renamed</a> last week to <code>text-wrap: avoid-short-last-lines</code>.</p>
<h3><a name="take-a-look"></a>Take a look</h3>
<p>You can try out <code>text-wrap: pretty</code> today in Safari Technology Preview 216. Check out <a href="https://codepen.io/jensimmons/pen/xxvoqNM?editors=1100">this demo</a> where you can toggle <code>pretty</code> on and off to see its effects. You can also toggle hyphenation or justification to try out any combination. Show guides and show ghosts will help you understand what’s happening. Or try out <code>text-wrap: balance</code> to see the difference. The demo has content in English, Arabic, German, Chinese and Japanese so you can see the effects in different writing systems.</p>
<figure><picture><source src="https://webkit.org/wp-content/uploads/demo-dark.png" type="image/png" media="(prefers-color-scheme: dark)"><img decoding="async" src="https://webkit.org/wp-content/uploads/demo-light.png" alt="Screenshot of the demo showing a control panel of options that include: text-wrap: pretty, hyphenate, justify, show guides, show ghosts, text-wrap: balance. With a bunch of text on the page which will change as different options are applied. " width="2760" height="1784" srcset="https://webkit.org/wp-content/uploads/demo-light.png 2760w, https://webkit.org/wp-content/uploads/demo-light-300x194.png 300w, https://webkit.org/wp-content/uploads/demo-light-1024x662.png 1024w, https://webkit.org/wp-content/uploads/demo-light-768x496.png 768w, https://webkit.org/wp-content/uploads/demo-light-1536x993.png 1536w, https://webkit.org/wp-content/uploads/demo-light-2048x1324.png 2048w" sizes="(max-width: 2760px) 100vw, 2760px"></picture><figcaption>Try <a href="https://cdpn.io/pen/debug/xxvoqNM">this demo</a> in Safari Technology Preview 216.</figcaption></figure>
<p>Here’s a sample of text in English, without applying <code>text-wrap</code>. This is the default wrapping algorithm the web has had for years. I’ve turned on “show guides” to mark the edges of the text box. The green line marks the inline-end edge of the box — where the line layout algorithm is aiming for each line to reach. The browser wraps when the text reaches this green line.</p>
<figure><picture><source src="https://webkit.org/wp-content/uploads/demo-1-dark.png" type="image/png" media="(prefers-color-scheme: dark)"><img decoding="async" src="https://webkit.org/wp-content/uploads/demo-1-light.png" alt="Three paragraphs of text, with a green vertical line marking the inline end edge of the text box." width="1978" height="1258" srcset="https://webkit.org/wp-content/uploads/demo-1-light.png 1978w, https://webkit.org/wp-content/uploads/demo-1-light-300x191.png 300w, https://webkit.org/wp-content/uploads/demo-1-light-1024x651.png 1024w, https://webkit.org/wp-content/uploads/demo-1-light-768x488.png 768w, https://webkit.org/wp-content/uploads/demo-1-light-1536x977.png 1536w" sizes="(max-width: 1978px) 100vw, 1978px"></picture></figure>
<p>And here’s how the same text looks instead with <code>text-wrap: pretty</code> applied. The green line has moved. Now, the browser instead aims to wrap each line sooner than the maximum limit of the text box. It wraps within the range, definitely after the magenta line, and definitely before the red line. This improves the rag.</p>
<figure><picture><source src="https://webkit.org/wp-content/uploads/demo-2-dark.png" type="image/png" media="(prefers-color-scheme: dark)"><img loading="lazy" decoding="async" src="https://webkit.org/wp-content/uploads/demo-2-light.png" alt="Demo of the same text, only now with better rag. The &quot;guides&quot; show three vertical lines about 50 pixels apart —&nbsp;the far right edge is marked with a red line. 45 pixels in from the left is a vertical green line. And another 45 pixels left of that line is a vertical magenta line." width="1978" height="1258" srcset="https://webkit.org/wp-content/uploads/demo-2-light.png 1978w, https://webkit.org/wp-content/uploads/demo-2-light-300x191.png 300w, https://webkit.org/wp-content/uploads/demo-2-light-1024x651.png 1024w, https://webkit.org/wp-content/uploads/demo-2-light-768x488.png 768w, https://webkit.org/wp-content/uploads/demo-2-light-1536x977.png 1536w" sizes="auto, (max-width: 1978px) 100vw, 1978px"></picture></figure>
<p>You can also “show ghosts” to view the unprettified version as a ghost in the background.</p>
<figure><picture><source src="https://webkit.org/wp-content/uploads/demo-3-dark.png" type="image/png" media="(prefers-color-scheme: dark)"><img loading="lazy" decoding="async" src="https://webkit.org/wp-content/uploads/demo-3-light.png" alt="The same text, only now the well wrapped version is in normal color text, with the previous, badly wrapped version directly behind the good version, in a faint cyan color. It's a ghost of the before version, making it clear which lines have changed and how." width="1978" height="1258" srcset="https://webkit.org/wp-content/uploads/demo-3-light.png 1978w, https://webkit.org/wp-content/uploads/demo-3-light-300x191.png 300w, https://webkit.org/wp-content/uploads/demo-3-light-1024x651.png 1024w, https://webkit.org/wp-content/uploads/demo-3-light-768x488.png 768w, https://webkit.org/wp-content/uploads/demo-3-light-1536x977.png 1536w" sizes="auto, (max-width: 1978px) 100vw, 1978px"></picture></figure>
<p>You can also toggle hyphenation and justification on and off to compare different combinations. Resize the browser window, and see what happens at different widths of text.</p>
<p>You might notice that since Safari Technology Preview applies <code>pretty</code> to every line of text, not just the last four lines, it has a more significant impact on the text. The block of text becomes more of a solid rectangle.</p>
<p>You really have to typeset body text with <code>text-wrap: pretty</code> to see just how much of a difference it makes. It’s subtle, but remarkable. Combine this <a href="https://12daysofweb.dev/2024/css-margin-trim-line-height-units/">with paragraph margins of <code>1lh</code></a>, and the text starts looking fantastic.</p>
<p>So why doesn’t every browser do everything it can to make the text look better? Because of performance.</p>
<h3><a name="performance"></a>Performance</h3>
<p>Many developers are understandably concerned about the performance of <code>text-wrap: pretty</code>. While better typography is visually pleasing — it cannot come at the expense of a slower page. Each browser engineering team must think about the hardware and chips their users have when deciding how to set limits.</p>
<p>We are thrilled with the work we’ve done to ensure Safari users don’t experience a negative performance impact, even when web developers apply <code>text-wrap: pretty</code> to a lot of content on the web page.</p>
<p>One thing to know as a developer, the performance of <code>text-wrap</code> is not affected by how many elements on the page it’s applied to. Perf concerns emerge as the <code>pretty</code> algorithm takes more and more lines into consideration as it calculates what to do. In WebKit-based browsers or apps, your text element would need to be many hundreds or thousands of lines long to see a performance hit — and that kind of content is unusual on the web. If your content is broken up into typical-length paragraphs, then you have no reason to worry. Use <code>text-wrap: pretty</code> as much as you want, and rely on our browser engineers to ensure your users will not experience any downsides.</p>
<p>We might decide add a mechanism to take really long paragraphs and break them up into more reasonable chunks, where WebKit evaluates each chunk separately. If we do so, then even 1,000-line paragraphs won’t affect performance. That’s the approach Adobe InDesign takes. It improves the layout of all lines of text, but it doesn’t evaluate an infinite number of lines inside each paragraph all at once, in one pass. There also might be other ways the WebKit team discovers to balance beauty and speed, ensuring <code>pretty</code> does the most to improve all of your text without affecting the users experience of our incredibly fast browser.</p>
<p>Test out <code>text-wrap: pretty</code> in Safari Technology Preview 216 today, and let us know if you can trigger a performance impact. File an issue at <a href="http://bugs.webkit.org/">bugs.webkit.org</a>, so we can consider such feedback as we polish this feature before shipping in Safari itself.</p>
<h3><a name="when-to-use-pretty-vs-balance"></a>When to use pretty vs balance?</h3>
<p>Clearly, <code>text-wrap: pretty</code> is designed to make body text more beautifully typeset. But is that the only use case for it? What about <code>text-wrap: balance</code>? When should we use <code>pretty</code> or <code>balance</code>?</p>
<p>There are people who will give you an overly simple answer like “pretty is for paragraphs and balance is for headlines” — but that’s likely bad advice. Let’s look at what <code>balance</code> does in contrast to <code>pretty</code> and how to decide which one to use on headlines, captions, teasers, and other kinds of shorter, wrapped text.</p>
<h2><a name="text-wrap-balance"></a>text-wrap: balance</h2>
<p>Basically, the <code>text-wrap: balance</code> rule tells the browser to wrap in such places to make every line be about the same length as the others. I think of it like folding a piece of paper into halves, or thirds, or quarters.</p>
<p>For example, here’s a headline with the default of <code>text-wrap: auto</code>. You can see that the word “enough” ends up as the first word of the second line, simply because there wasn’t enough space for it on the first line after “with”. Each line is laid out one-by-one, with no regard for the others. This causes this headline to end up with the word “itself” alone on the last line.</p>
<figure><picture><source src="https://webkit.org/wp-content/uploads/headline-auto-dark.png" type="image/png" media="(prefers-color-scheme: dark)"><img loading="lazy" decoding="async" src="https://webkit.org/wp-content/uploads/headline-auto-light.png" alt="" width="1934" height="486" srcset="https://webkit.org/wp-content/uploads/headline-auto-light.png 1934w, https://webkit.org/wp-content/uploads/headline-auto-light-300x75.png 300w, https://webkit.org/wp-content/uploads/headline-auto-light-1024x257.png 1024w, https://webkit.org/wp-content/uploads/headline-auto-light-768x193.png 768w, https://webkit.org/wp-content/uploads/headline-auto-light-1536x386.png 1536w" sizes="auto, (max-width: 1934px) 100vw, 1934px"></picture></figure>
<p>Here’s a similar headline with <code>text-wrap: balance</code> applied. There’s no longer a single word by itself on the last line. That’s great! But that’s not all. The last line is now the about same length as the other two. The first line is made significantly shorter so that its length is “balanced” with the length of the others. All three lines basically have the same length.</p>
<p>You’ll notice that as a result, the visual imprint of the headline / the chunk of “ink” on the page is now significantly narrower than the overall space available in its container (marked here with a cyan line).</p>
<figure><picture><source src="https://webkit.org/wp-content/uploads/headline-balance-dark.png" type="image/png" media="(prefers-color-scheme: dark)"><img loading="lazy" decoding="async" src="https://webkit.org/wp-content/uploads/headline-balance-light.png" alt="A balanced headline, where all three lines are about the same length. All of them are about two-thirds as wide as the box they are in, leaving a lot of white space on the right. The first line is actually the shortest line. " width="1934" height="488" srcset="https://webkit.org/wp-content/uploads/headline-balance-light.png 1934w, https://webkit.org/wp-content/uploads/headline-balance-light-300x76.png 300w, https://webkit.org/wp-content/uploads/headline-balance-light-1024x258.png 1024w, https://webkit.org/wp-content/uploads/headline-balance-light-768x194.png 768w, https://webkit.org/wp-content/uploads/headline-balance-light-1536x388.png 1536w" sizes="auto, (max-width: 1934px) 100vw, 1934px"></picture></figure>
<p>This can be fantastic for your design. You can apply <code>balance</code> to headlines, captions, teasers, any shorter types of copy and it will have the same effect. It will stack the lines of text so they’re all about the same length — they are balanced.  And once the text has been wrapped, it will likely not fill the box anymore. It will be narrower than the available space.</p>
<p>Sometimes, that’s not a desirable effect. Perhaps your headline sits under a teaser image, and the design calls for the text to be the same width as the image. You’ll notice that in this example, the first line ends up shorter than the rest. Balancing the text might come at too high a cost. Perhaps all you wanted to do is avoid a short last line.</p>
<p>You can instead use <code>text-wrap: pretty</code> on such headlines. This will avoid the short last line, while also still filling the container in the inline direction.</p>
<figure><picture><source src="https://webkit.org/wp-content/uploads/headline-pretty-dark.png" type="image/png" media="(prefers-color-scheme: dark)"><img loading="lazy" decoding="async" src="https://webkit.org/wp-content/uploads/headline-pretty-light.png" alt="A similar headline, with three words on the last line. The first line does stretch all the way across the box, and the second line is a bit shorter so that some of its words can fill in the last line. " width="1934" height="484" srcset="https://webkit.org/wp-content/uploads/headline-pretty-light.png 1934w, https://webkit.org/wp-content/uploads/headline-pretty-light-300x75.png 300w, https://webkit.org/wp-content/uploads/headline-pretty-light-1024x256.png 1024w, https://webkit.org/wp-content/uploads/headline-pretty-light-768x192.png 768w, https://webkit.org/wp-content/uploads/headline-pretty-light-1536x384.png 1536w" sizes="auto, (max-width: 1934px) 100vw, 1934px"></picture></figure>
<p>You can <a href="https://codepen.io/jensimmons/pen/YzMgvjr?editors=1100">try out these examples yourself</a> in Safari Technology Preview 126+ and Chrome/Edge 130+ to dive more into the effect of <code>text-wrap</code> on long, medium, and short headlines. Drag the corner of the boxes to see just how differently they handle wrapping.</p>
<p>What are the performance considerations for <code>text-wrap: balance</code>? Again, the CSS web standard leaves it to the browser engine to decide what kind of limits should be in place to ensure the users experience is not negatively impacted. Browsers do not have to make the same choices as each other.</p>
<p>The Chromium implementation limits the number of lines that are balanced to four to ensure Chrome, Edge and other Chromium browsers will still be fast. The WebKit implementation doesn’t need to limit the number of lines. Every line will be balanced with all of the others.</p>
<p>So if “pretty is for body text and balance is for headlines” is too simplistic a recommendation to be good advice, what might be a better way to think about the choice?</p>
<p>I think about it like this:</p>
<ul>
<li><code>pretty</code> can be applied to anything on the page — body text, headlines, captions, teasers, etc. Look to see what it does and if you like the effect. If you have <em>incredibly</em> long paragraphs (or better said: long body text without any paragraphs breaking it up, think in the hundreds or thousands of lines of text), test performance first. Also, if you are animating text in such a way that it rewraps as it animates, test to see if that’s a good idea.</li>
<li><code>balance</code> should be used for anything where you <em>want</em> all the lines to be the same length — especially headlines and captions, etc. And where you do not mind if the overall group of lines is narrower than its container. Don’t use it on long passages of text; that doesn’t make sense. </li>
<li><code>auto</code> is the default, which currently considers just one line at a time as layout is calculated, like the web has since 1991 (see below).</li>
<li><code>stable</code> should be used for editable text and more (see below).</li>
</ul>
<p>Unconvinced that <code>text-wrap: balance</code> won’t usually make sense on long passages of text? Well, you can try it out in <a href="https://codepen.io/jensimmons/full/xxvoqNM">this same demo</a>.</p>
<figure><picture><source src="https://webkit.org/wp-content/uploads/balanced-body-text-dark.png" type="image/png" media="(prefers-color-scheme: dark)"><img loading="lazy" decoding="async" src="https://webkit.org/wp-content/uploads/balanced-body-text-light.png" alt="The same demo, now with `text-wrap: balance` applied to the paragraphs. Each paragraph is now wildly different widths from the others. This is not useful for anything. " width="3248" height="1382" srcset="https://webkit.org/wp-content/uploads/balanced-body-text-light.png 3248w, https://webkit.org/wp-content/uploads/balanced-body-text-light-300x128.png 300w, https://webkit.org/wp-content/uploads/balanced-body-text-light-1024x436.png 1024w, https://webkit.org/wp-content/uploads/balanced-body-text-light-768x327.png 768w, https://webkit.org/wp-content/uploads/balanced-body-text-light-1536x654.png 1536w, https://webkit.org/wp-content/uploads/balanced-body-text-light-2048x871.png 2048w" sizes="auto, (max-width: 3248px) 100vw, 3248px"></picture></figure>
<p>See how the overall width of each paragraph is adjusted so that all the lines of text are about the same length, with no regard for how wide they are overall compared to their container? That’s what balancing does. And in the above example, that means each paragraph is a radically different width than the others. Which is odd. Only use it when you want that effect. Otherwise, you probably want to use another option.</p>
<p>What do the other values for <code>text-wrap</code> do? Let’s go through them.</p>
<h2><a name="text-wrap-avoid-short-last-lines"></a>text-wrap: avoid-short-last-lines</h2>
<p>The <code>avoid-short-last-lines</code> value is the newest one in the <a href="https://drafts.csswg.org/css-text-4/">CSS Text Module Level 4</a> specification. It’s not yet been implemented in any browser. It will focus on just avoiding short last lines, leaving <code>pretty</code> to do so much more.</p>
<h2><a name="text-wrap-auto"></a>text-wrap: auto</h2>
<p>The default value of <code>text-wrap</code> currently does what the web has done since 1991, where each line of text is laid out by itself, with no consideration for multiple lines. (This is often called a “first-fit” or “greedy” algorithm.)</p>
<p>This, however, could change in the future! There may come a day when browsers decide to change the default, and apply some kind of multiline line-layout algorithm to all existing content on the web. This would improve all content, even old websites, even websites where the developer didn’t know about <code>text-wrap: pretty</code>.</p>
<h2><a name="text-wrap-stable"></a>text-wrap: stable</h2>
<p>If you’ve tried <code>text-wrap: stable</code>, you might think “it doesn’t do anything! What is this?” Basically, right now, <code>stable</code> does the same thing as <code>auto</code>. It uses the original first-fit wrapping algorithm, where each line is laid out to fully fill that line with content, and only wrap where necessary.</p>
<p>This is an especially good choice of wrapping algorithms when the content itself is editable. If your user is writing text, you don’t want words/syllables jumping around, changing the wrapping as they type. To ensure your content won’t shift due to edits on subsequent lines, or in any case where you want OG line wrapping, apply <code>text-wrap: stable</code>.</p>
<p>This is also a good choice if you are animating text in such a way that it keeps re-wrapping. It will ensure the fastest wrapping algorithm is used at all times — important if the calculations are going to be done over and over in rapid succession.</p>
<p>By explicitly choosing <code>text-wrap: stable</code> you are ensuring this content will continue to wrap using the original algorithm, even if browsers redefine what <code>auto</code> does.</p>
<p>The <code>stable</code> value is already <a href="https://caniuse.com/mdn-css_properties_text-wrap_stable">well supported</a>.</p>
<h2><a name="text-wrap-mode-and-text-wrap-style"></a>text-wrap-mode and text-wrap-style</h2>
<p>The <code>text-wrap</code> property is actually a short-hand for two longhands. The <code>text-wrap-style</code> property is for choosing the wrapping algorithm that’s used, while <code>text-wrap-mode</code> lets you turn wrapping on and off.</p>
<pre><code><span>text-wrap-style</span>: <span>auto</span> | <span>stable</span> | <span>balance</span> | <span>pretty</span> | <span>avoid-short-last-lines</span>
<span>text-wrap-mode</span>: <span>wrap</span> | <span>nowrap</span> 
</code></pre>
<p>By having both the  <code>text-wrap-mode</code> and <code>text-wrap-style</code> properties, you have the flexibility to change the style of wrapping independently from whether or not content wraps, and let these choices cascade independently.</p>
<p>This means you can also use the shorthand to simply turn off wrapping with <code>text-wrap: nowrap</code>. Or, use <code>text-wrap: wrap</code> to turn wrapping back on. Test out how it works in <a href="https://codepen.io/jensimmons/pen/xbxJxRx?editors=1100">this demo</a>.</p>
<p><a href="https://caniuse.com/?search=text-wrap">Support for</a> the <code>text-wrap-mode</code> and <code>text-wrap-style</code> longhands, along with the <code>nowrap</code> and <code>wrap</code> values, became “Baseline Newly Available” (aka, available in all major browsers) in October 2024, when Chromium added support in Chrome/Edge 130. To ensure full support for wrapping for people with older browsers, you can always provide a fallback to the older<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/white-space"><code>white-space: nowrap | normal</code></a>. (Although when you do, take care to also check your white space collapsing behavior, since it’s affected by <code>white-space</code>.)</p>
<h2><a name="what-do-you-think"></a>What do you think?</h2>
<p>Try out <code>text-wrap: pretty</code> in <a href="https://developer.apple.com/safari/resources/">Safari Technology Preview 216</a> today. We’d love to hear what you think. Find me, Jen Simmons, on <a href="https://bsky.app/profile/jensimmons.bsky.social">Bluesky</a> or <a href="https://front-end.social/@jensimmons">Mastodon</a> to share your feedback. If you find a bug or problem, please file a <a href="https://bugs.webkit.org/">WebKit bug report</a>.</p>

                            </div>
        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Connecting an IBM 3151 terminal to a mainframe [video] (113 pts)]]></title>
            <link>https://www.youtube.com/watch?v=V14ac9cRi9Q</link>
            <guid>43621007</guid>
            <pubDate>Tue, 08 Apr 2025 12:36:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=V14ac9cRi9Q">https://www.youtube.com/watch?v=V14ac9cRi9Q</a>, See on <a href="https://news.ycombinator.com/item?id=43621007">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Meta got caught gaming AI benchmarks (252 pts)]]></title>
            <link>https://www.theverge.com/meta/645012/meta-llama-4-maverick-benchmarks-gaming</link>
            <guid>43620452</guid>
            <pubDate>Tue, 08 Apr 2025 11:29:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/meta/645012/meta-llama-4-maverick-benchmarks-gaming">https://www.theverge.com/meta/645012/meta-llama-4-maverick-benchmarks-gaming</a>, See on <a href="https://news.ycombinator.com/item?id=43620452">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="zephr-anchor"><p>Over the weekend, Meta dropped two new <a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/">Llama 4 models</a>: a smaller model named Scout, and Maverick, a mid-size model that the company claims can beat GPT-4o and Gemini 2.0 Flash “across a broad range of widely reported benchmarks.”</p><p>Maverick quickly secured the number-two spot on LMArena, the AI benchmark site where humans compare outputs from different systems and vote on the best one. In Meta’s <a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/">press release</a>, the company highlighted Maverick’s ELO score of 1417, which placed it above OpenAI’s 4o and just under Gemini 2.5 Pro. (A higher ELO score means the model wins more often in the arena when going head-to-head with competitors.)</p><p>The achievement seemed to position Meta’s open-weight Llama 4 as a serious challenger to the state-of-the-art, closed models from OpenAI, Anthropic, and Google. Then, AI researchers digging through Meta’s documentation discovered something unusual.</p><p>In fine print, Meta acknowledges that the version of Maverick tested on LMArena isn’t the same as what’s available to the public. According to Meta’s own materials, it deployed an <a href="https://x.com/natolambert/status/1908913635373842655">“experimental chat version”</a> of Maverick to LMArena that was specifically “optimized for conversationality,” <em>TechCrunch</em> first <a href="https://techcrunch.com/2025/04/06/metas-benchmarks-for-its-new-ai-models-are-a-bit-misleading/">reported</a>.</p><p>“Meta’s interpretation of our policy did not match what we expect from model providers,” LMArena <a href="https://x.com/lmarena_ai/status/1909397817434816562">posted</a> on X two days after the model’s release. “Meta should have made it clearer that ‘Llama-4-Maverick-03-26-Experimental’ was a customized model to optimize for human preference. As a result of that, we are updating our leaderboard policies to reinforce our commitment to fair, reproducible evaluations so this confusion doesn’t occur in the future.“</p><p>A spokesperson for Meta, Ashley Gabriel, said in an emailed statement that “we experiment with all types of custom variants.”</p><p>“‘Llama-4-Maverick-03-26-Experimental’ is a chat optimized version we experimented with that also performs well on LMArena,” Gabriel said. “We have now released our open source version and will see how developers customize Llama 4 for their own use cases. We’re excited to see what they will build and look forward to their ongoing feedback.”</p><p>While what Meta did with Maverick isn’t explicitly against LMArena’s rules, the site has shared concerns <a href="https://blog.lmarena.ai/blog/2024/policy/?utm_source=chatgpt.com">about gaming the system</a> and taken steps to “prevent overfitting and benchmark leakage.” When companies can submit specially-tuned versions of their models for testing while releasing different versions to the public, benchmark rankings like LMArena become less meaningful as indicators of real-world performance.</p><p>”It’s the most widely respected general benchmark because all of the other ones suck,” independent AI researcher Simon Willison tells <em>The Verge</em>. “When Llama 4 came out, the fact that it came second in the arena, just after Gemini 2.5 Pro — that really impressed me, and I’m kicking myself for not reading the small print.”</p><p>Shortly after Meta released Maverick and Scout, the AI community started <a href="https://x.com/Yuchenj_UW/status/1909061004207816960">talking about a rumor</a> that Meta had also trained its Llama 4 models to perform better on benchmarks while hiding their real limitations. VP of generative AI at Meta, Ahmad Al-Dahle, addressed the accusations <a href="https://x.com/Ahmad_Al_Dahle/status/1909302532306092107">in a post on X</a>: “We’ve also heard claims that we trained on test sets -- that’s simply not true and we would never do that. Our best understanding is that the variable quality people are seeing is due to needing to stabilize implementations.”</p><div><p>“It’s a very confusing release generally.”</p></div><p>Some <a href="https://x.com/kalomaze/status/1908706389922324599">also noticed</a> that Llama 4 was released at an odd time. Saturday doesn’t tend to be when big AI news drops. After someone on Threads asked why Llama 4 was released over the weekend, Meta CEO Mark Zuckerberg <a href="https://www.threads.net/@zuck/post/DIFAsupTS7Z">replied</a>: “That’s when it was ready.”</p><p>“It’s a very confusing release generally,” says Willison, who <a href="https://simonwillison.net/">closely follows and documents AI models</a>. “The model score that we got there is completely worthless to me. I can’t even use the model that they got a high score on.”</p><p>Meta’s path to releasing Llama 4 wasn’t exactly smooth. According <a href="https://www.theinformation.com/articles/meta-nears-release-new-ai-model-performance-hiccups?rc=mshudk">to a recent report</a> from <em>The Information</em>, the company repeatedly pushed back the launch due to the model failing to meet internal expectations. Those expectations are especially high after DeepSeek, an open-source AI startup from China, released an open-weight model that generated a ton of buzz.</p><p>Ultimately, using an optimized model in LMArena puts developers in a difficult position. When selecting models like Llama 4 for their applications, they naturally look to benchmarks for guidance. But as is the case for Maverick, those benchmarks can reflect capabilities that aren’t actually available in the models that the public can access.</p><p>As AI development accelerates, this episode shows how benchmarks are becoming battlegrounds. It also shows how Meta is eager to be seen as an AI leader, even if that means gaming the system.</p><p><strong>Update, April 7th:</strong> <em>The story was updated to add Meta’s statement.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brazil's government-run payments system has become dominant (323 pts)]]></title>
            <link>https://www.economist.com/the-americas/2025/04/03/brazils-government-run-payments-system-has-become-dominant</link>
            <guid>43620279</guid>
            <pubDate>Tue, 08 Apr 2025 10:59:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/the-americas/2025/04/03/brazils-government-run-payments-system-has-become-dominant">https://www.economist.com/the-americas/2025/04/03/brazils-government-run-payments-system-has-become-dominant</a>, See on <a href="https://news.ycombinator.com/item?id=43620279">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><div><p><span><a href="https://www.economist.com/the-americas" data-analytics="sidebar:section"><span>The Americas</span></a></span><span> | <!-- -->Pix a winner</span></p></div><h2>Pix has spiced up Brazil’s fusty banking sector, but it gives the central bank a worrying amount of power</h2></section><div><div><p><time datetime="2025-04-03T13:02:23.882Z"> <!-- -->Apr 3rd 2025</time><span>|</span><span>São Paulo</span></p></div><section><p data-component="paragraph"><span data-caps="initial">I</span><small>n november 2020 </small>the Central<small> </small>Bank of Brazil<small> </small>(<small>BCB</small>) launched Pix, a digital payment system, into the teeth of the covid-19 pandemic. Avoiding physical contact at a time when that was much desired, instantaneous, free and easy-to-use, Pix took off. Users need the recipient’s national <small>ID</small> number, phone number or a <small>QR</small> code to move money. By 2024 (see chart) it had become Brazil’s most popular payment technology, displacing both cash and cards. The number of transactions increased from 9bn in 2021 to 63bn in 2024, moving 26trn reais ($4.5trn). No country has adopted such a system faster.</p></section><p><h3 id="article-tags">Explore more</h3><nav aria-labelledby="article-tags"><a href="https://www.economist.com/topics/world" data-analytics="tags:world"><span>World</span></a><a href="https://www.economist.com/topics/the-americas" data-analytics="tags:the_americas"><span>The Americas</span></a><a href="https://www.economist.com/topics/finance" data-analytics="tags:finance"><span>Finance</span></a><a href="https://www.economist.com/topics/brazil" data-analytics="tags:brazil"><span>Brazil</span></a></nav></p><p>This article appeared in the The Americas section of the print edition under the headline “Pix perfect”</p><div data-test-id="chapterlist" data-tracking-id="content-well-chapter-list"><div><hr data-testid="rule-accent"><div><h3><a href="https://www.economist.com/the-americas" text="The Americas" data-analytics="chapter_list_header:The Americas">The Americas</a></h3><p><span>April 5th 2025</span></p></div></div><ul><li><a href="https://www.economist.com/the-americas/2025/04/03/brazils-government-run-payments-system-has-become-dominant" id="6b721b5c-700e-4ba4-87ed-74b363e4f180" data-analytics="article:reports_headline:1" data-test-id="chapterlist-link-0"><span data-testid="right-economist-red-false"><span>→</span></span><span>Brazil’s government-run payments system has become dominant</span></a></li><li><a href="https://www.economist.com/the-americas/2025/04/03/the-liberal-partys-polling-surge-is-canadas-largest-ever" id="6558af76-ec98-4400-af76-efc605b49959" data-analytics="article:reports_headline:2" data-test-id="chapterlist-link-1"><span data-testid="right-london-5-false"><span>→</span></span><span>The Liberal Party’s polling surge is Canada’s largest ever</span></a></li><li><a href="https://www.economist.com/the-americas/2025/04/03/peruvians-long-for-a-bukele-like-strongman-to-beat-crime" id="7b3edc3f-e135-4975-a873-ba6eb652ced3" data-analytics="article:reports_headline:3" data-test-id="chapterlist-link-2"><span data-testid="right-london-5-false"><span>→</span></span><span>Peruvians long for a Bukele-like strongman to beat crime</span></a></li><li><a href="https://www.economist.com/the-americas/2025/04/03/latin-american-migrants-transfer-money-like-never-before" id="3cf5a0c2-3692-49f7-8e82-5614b2768dd6" data-analytics="article:reports_headline:4" data-test-id="chapterlist-link-3"><span data-testid="right-london-5-false"><span>→</span></span><span>Latin American migrants transfer money like never before</span></a></li></ul></div><div orientation="vertical" data-test-id="vertical"><div orientation="vertical"><figure><img loading="lazy" width="1280" height="1709" decoding="async" data-nimg="1" sizes="300px" srcset="https://www.economist.com/cdn-cgi/image/width=16,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg 16w, https://www.economist.com/cdn-cgi/image/width=32,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg 32w, https://www.economist.com/cdn-cgi/image/width=48,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg 48w, https://www.economist.com/cdn-cgi/image/width=64,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg 64w, https://www.economist.com/cdn-cgi/image/width=96,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg 96w, https://www.economist.com/cdn-cgi/image/width=128,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg 128w, https://www.economist.com/cdn-cgi/image/width=256,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg 256w, https://www.economist.com/cdn-cgi/image/width=360,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg 360w, https://www.economist.com/cdn-cgi/image/width=384,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg 384w, https://www.economist.com/cdn-cgi/image/width=480,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg 480w, https://www.economist.com/cdn-cgi/image/width=600,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg 600w, https://www.economist.com/cdn-cgi/image/width=834,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg 834w, https://www.economist.com/cdn-cgi/image/width=960,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg 960w, https://www.economist.com/cdn-cgi/image/width=1096,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg 1096w, https://www.economist.com/cdn-cgi/image/width=1280,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg 1280w, https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg 1424w" src="https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20250405_DE_EU.jpg"></figure></div><div orientation="vertical"><h3 orientation="vertical">From the April 5th 2025 edition</h3><p orientation="vertical">Discover stories from this section and more in the list of contents</p><p><a href="https://www.economist.com/weeklyedition/2025-04-05" data-analytics="sidebar:weekly_edition"><span data-testid="right-economist-red-true"><span>⇒</span></span><span>Explore the edition</span></a></p></div></div><div><a href="https://s100.copyright.com/AppDispatchServlet?publisherName=economist&amp;publication=economist&amp;title=Brazil%E2%80%99s%20government-run%20payments%20system%20has%20become%20dominant&amp;publicationDate=2025-04-03&amp;contentID=%2Fcontent%2Flmv7se5saq15s7jcr4sa61ivhb3qamib&amp;type=A&amp;orderBeanReset=TRUE" target="_blank" rel="noreferrer" data-analytics="end_of_article:reuse_this_content"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" data-testid="renew-outline"><path fill="var(--mb-colour-base-chicago-45)" d="M5.1 16.05a8.25 8.25 0 0 1-.825-1.95A7.696 7.696 0 0 1 4 12.05c0-2.233.775-4.133 2.325-5.7C7.875 4.783 9.767 4 12 4h.175l-1.6-1.6 1.4-1.4 4 4-4 4-1.4-1.4 1.6-1.6H12c-1.667 0-3.083.588-4.25 1.763C6.583 8.938 6 10.367 6 12.05c0 .433.05.858.15 1.275.1.417.25.825.45 1.225l-1.5 1.5ZM12.025 23l-4-4 4-4 1.4 1.4-1.6 1.6H12c1.667 0 3.083-.587 4.25-1.762C17.417 15.063 18 13.633 18 11.95c0-.433-.05-.858-.15-1.275-.1-.417-.25-.825-.45-1.225l1.5-1.5c.367.633.642 1.283.825 1.95.183.667.275 1.35.275 2.05 0 2.233-.775 4.133-2.325 5.7C16.125 19.217 14.233 20 12 20h-.175l1.6 1.6-1.4 1.4Z"></path></svg><span>Reuse this content</span></a></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tailscale has raised $160M (276 pts)]]></title>
            <link>https://tailscale.com/blog/series-c</link>
            <guid>43620141</guid>
            <pubDate>Tue, 08 Apr 2025 10:36:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tailscale.com/blog/series-c">https://tailscale.com/blog/series-c</a>, See on <a href="https://news.ycombinator.com/item?id=43620141">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Tailscale has raised $160 million USD ($230 million CAD) in our Series C, led by Accel with participation from CRV, Insight Partners, Heavybit, and Uncork Capital. Existing angel investor George Kurtz - CEO of Crowdstrike is also included in this round, as well as Anthony Casalena - CEO of Squarespace, who joins as a new investor for Series C.</p><p>There’s a lot packed into that sentence. But the real question is — why should you care?</p><p><img _type="asset" video="[object Object]" alt="$160 Million Series C" loading="lazy" width="2304" height="1240" decoding="async" data-nimg="1" srcset="https://cdn.sanity.io/images/w77i7m8x/production/a2d4cd932d9b8c035b001b367707d080a8f66f85-2304x1240.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 1x" src="https://cdn.sanity.io/images/w77i7m8x/production/a2d4cd932d9b8c035b001b367707d080a8f66f85-2304x1240.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format"></p><h3 id="venture-money-is-a-tool"><a href="#venture-money-is-a-tool">Venture money is a tool.<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3><p>When we started Tailscale in 2019, we weren't even sure we wanted to be a venture-backed company. We just wanted to fix networking. Or, more specifically, make networking disappear — reduce the number of times anyone had to think about NAT traversal or VPN configurations ever again.</p><p>That might sound simple, but it wasn’t. Here we are, six years later, and millions of people rely on Tailscale every day, connecting their homelabs, their apps, their companies, their <a target="" rel="noreferrer" href="https://tailscale.com/blog/ai-normal">AI workloads</a>. Some use it because they love networking and want better tools. Many use it because they have better things to do – they don’t want to think about networking at all.</p><p>Either way, the outcome is the same: things connect, securely and privately, without the traditional headaches.</p><p><img _type="asset" video="[object Object]" alt="Identity first, Decentralized, Empowered" loading="lazy" width="2304" height="1240" decoding="async" data-nimg="1" srcset="https://cdn.sanity.io/images/w77i7m8x/production/976379382bac07b73dead2b27f3f75bac6665e08-2304x1240.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 1x" src="https://cdn.sanity.io/images/w77i7m8x/production/976379382bac07b73dead2b27f3f75bac6665e08-2304x1240.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format"></p><h3 id="this-round-isnt-about-us-its-about-what-we-build-next-for-you"><a href="#this-round-isnt-about-us-its-about-what-we-build-next-for-you">This round isn’t about us — it’s about what we build next, for you.<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3><p>Even though we already had a long runway, we raised this Series C because we realized the world had started raining opportunities. We want to go faster where it matters:</p><ul><li>Removing friction</li><li>Scaling the network without scaling complexity</li><li>Making identity, not IP addresses, the core of secure connectivity</li></ul><p>The Internet wasn’t built with identity in mind. It was built for location — packets sent between machines, not people. Everything that came after — VPNs, firewalls, Zero Trust — are attempts to patch over that original gap.</p><p>We think there’s a better way forward. We're calling it <strong>identity-first networking.</strong></p><p>When you connect to something with Tailscale, you’re not just an IP connecting to a server at some IP. You’re connecting to your app, your teammate, your service — wherever it happens to be running right now. That’s how it should work.</p><p><img _type="asset" video="[object Object]" alt="Product Innovation, Expansion, Team Growth" loading="lazy" width="2304" height="1240" decoding="async" data-nimg="1" srcset="https://cdn.sanity.io/images/w77i7m8x/production/f60ab1adce50919784344b614048d6ae35b3723a-2304x1240.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 1x" src="https://cdn.sanity.io/images/w77i7m8x/production/f60ab1adce50919784344b614048d6ae35b3723a-2304x1240.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format"></p><h3 id="why-now-why-raise-this-much"><a href="#why-now-why-raise-this-much"><strong>Why now? Why raise this much?</strong><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3><p>The last year made the need for this even more obvious. The AI industry, in particular, is struggling to rapidly mature its underlying infrastructure. Connecting GPUs across clouds, securing workloads across continents, migrating between cloud providers — it’s messy, it’s hard, and it breaks all the time.</p><p>A surprising number of leading AI companies — Perplexity, Mistral, Cohere, Groq, Hugging Face — are now building on Tailscale to solve exactly this.</p><p>It’s not just AI. Companies like Instacart, SAP, Telus, Motorola, and Duolingo and <a target="" rel="noreferrer" href="https://tailscale.com/blog/welcome-grace-lin-10000-customers">thousands of others</a> use Tailscale to make their hybrid, remote, and cloud networks sane again.</p><p>This new funding helps us support all of that, faster. We're going to grow our engineering and product teams to unlock more markets faster. We're also investing further in our <a target="" rel="noreferrer" href="https://tailscale.com/blog/free-plan">free support for free customers</a> promise and our <a target="" rel="noreferrer" href="https://tailscale.com/blog/community-projects">backward compatibility forever</a> platform. Business is booming, and taking investment now lets us stay focused on making the network <em>just work</em>, whether you’re a startup, a Fortune 500, or a person running a Minecraft server.</p><p><img _type="asset" video="[object Object]" alt="Accel, CRV, Heavybit, Insight Partners, Uncork" loading="lazy" width="2304" height="1240" decoding="async" data-nimg="1" srcset="https://cdn.sanity.io/images/w77i7m8x/production/77caf25ede5967ca1b64325c52e8516c9aea3c3f-2304x1240.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 1x" src="https://cdn.sanity.io/images/w77i7m8x/production/77caf25ede5967ca1b64325c52e8516c9aea3c3f-2304x1240.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format"></p><h3 id="whos-behind-this-round"><a href="#whos-behind-this-round"><strong>Who’s behind this round?</strong><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3><p>We’re lucky to have <a target="" rel="noreferrer" href="https://www.accel.com/people/amit-kumar">Accel’s Amit Kumar</a> — who led our Series A — leading this round too, now from their growth fund. And we’re excited to welcome Anthony Casalena of Squarespace, alongside returning investors CRV, Heavybit, Insight, and Uncork, and George Kurtz - CEO of Crowdstrike.</p><p>The mix here matters. These are people who understand that the network is the right place for the security and identity layer. The boundary is shifting from the datacenter to the device — and from the device to the person holding it, or the container running on it.</p><p><img _type="asset" video="[object Object]" alt="Connected Nodes" loading="lazy" width="2304" height="1240" decoding="async" data-nimg="1" srcset="https://cdn.sanity.io/images/w77i7m8x/production/0eddc5196ffca9df679d1a05ad530c4566d2e936-2304x1240.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 1x" src="https://cdn.sanity.io/images/w77i7m8x/production/0eddc5196ffca9df679d1a05ad530c4566d2e936-2304x1240.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format"></p><h3 id="thanks-for-being-here"><a href="#thanks-for-being-here"><strong>Thanks for being here.</strong><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3><p>We wouldn’t be at this point without the thousands of businesses — and the millions of people — who've bet on us so far. You believed networking could be better, even when you didn’t want to have to think about it.</p><p>That’s fine. We think about it so you don’t have to.</p><p>Thanks for being part of this. More soon.</p><p>— Avery</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Unstoppable force' of solar power propels world to 40% clean electricity (155 pts)]]></title>
            <link>https://news.sky.com/story/unstoppable-force-of-solar-power-propels-world-to-40-clean-electricity-report-finds-13344230</link>
            <guid>43620007</guid>
            <pubDate>Tue, 08 Apr 2025 10:11:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.sky.com/story/unstoppable-force-of-solar-power-propels-world-to-40-clean-electricity-report-finds-13344230">https://news.sky.com/story/unstoppable-force-of-solar-power-propels-world-to-40-clean-electricity-report-finds-13344230</a>, See on <a href="https://news.ycombinator.com/item?id=43620007">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-name="ui-article-body" data-highlight-intro="true">
      
      <p>Clean power provided 40% of the world's electricity last year for the first time since the 1940s, new figures show.</p><p>Clean energy comes from nuclear and renewable sources like wind and solar.</p>
<p>The milestone has been reached thanks to the "staggering" rise of solar, which has doubled in just three years, energy thinktank Ember said in its new report.</p><p>And solar was the fastest-growing electricity source for the 20th year in a row.</p><p>It now provides 7% of the world's electricity.</p>
<p>But it remains eclipsed by wind, which grew to 8% last year, and nuclear to 9%.</p><p>Hydropower - produced by running water, usually from rivers or reservoirs, and the world's oldest and largest single source of renewable power - has hovered at 14%.</p><p>Phil MacDonald, Ember's managing director, said: "Paired with battery storage, solar is set to be an unstoppable force.</p><p>"As the fastest-growing and largest source of new electricity, it is critical in meeting the world's ever-increasing demand for electricity."</p>        
        <div data-account-id="6058004172001" data-amp-iframe-embed="" data-asset-id="c4001865-9ca9-416c-bd77-8a7481b392f7" data-asset-path="" data-asset-version="" data-autoplay="false" data-auth-config="" data-caption="BP slashes renewables investments and boosts fossil fuel production" data-clip-type="" data-component-name="sdc-site-video" data-component-name-alias="ui-sitewide-video" data-competition="" data-copy-url-text="URL copied to clipboard" data-fn="sdc-site-video" data-id="id_c4001865-9ca9-416c-bd77-8a7481b392f7" data-is-live-stream="false" data-lite="true" data-options="" data-originator-id="51" data-originator-handle="brightcove-news-gb" data-package-name="" data-provider="brightcove" data-player-id="yjBoKQ5XA" data-playsinline="" data-sensitive="false" data-sdc-id="6840489" data-sdc-video-id="c4001865-9ca9-416c-bd77-8a7481b392f7" data-sport-category="" data-state="loading" data-token-state="none" data-video-ad-unit="" data-video-blacklisted-originator-ids="" data-video-id="ref:c4001865-9ca9-416c-bd77-8a7481b392f7" data-video-type="" data-auto-pause-on-not-visible="true" tabindex="-1" data-closed-captions-position="middle-third" data-show-closed-captions="true" data-show-pip="true" data-show-live-stream-scrubber="true" data-autoload="false">
            <div data-role="bridge-controller">
                <p data-role="accessibility-message">Please use Chrome browser for a more accessible video player</p>
              
              <video id="id_c4001865-9ca9-416c-bd77-8a7481b392f7" data-embed="default" data-application-id="" controls="" playsinline=""></video>
            </div>
        
            <figcaption>
              
              <span data-role="caption-text">BP slashes renewables investments and boosts fossil fuel production</span>
            </figcaption>
          </div>
<p>Nuclear energy is considered a clean energy source because it produces almost zero greenhouse gases or other air pollutants.</p><p>The report's findings come as US President Donald Trump, in charge of the world's second most polluting country, <strong><a href="https://news.sky.com/story/the-climate-rules-trump-has-ripped-up-already-13293428" target="_blank">restricts offshore wind farms</a> </strong>and seeks to expand the mining and burning of coal in a bid to fuel booming AI data centres.</p><p>China and <a href="https://news.sky.com/topic/india-5990/1" target="_blank"><strong>India</strong></a>, the world's largest and third largest polluting countries respectively, are steaming ahead with their clean power plans.</p><p>More than half the world's new solar electricity came from <a href="https://news.sky.com/topic/china-5869/1" target="_blank"><strong>China</strong></a> last year - though it is still <strong><a href="https://news.sky.com/story/new-coal-plants-in-china-soar-despite-president-xi-s-pledge-to-strictly-control-dirtiest-fuel-13112101" target="_blank">building new coal power</a></strong> plants too.</p><p>Professor Xunpeng Shi, president of the International Society for Energy Transition Studies (ISETS), said: "The future of the global power system is being shaped in Asia, with China and India at the heart of the energy transition."</p><p><strong>Electricity demand on the rise</strong></p><p>Despite the rise in renewable power, electricity from more polluting fossil fuels crept up by 1.4% last year due to surging demand, meaning emissions from the sector rose too to an all-time high.</p><p>Ember had previously predicted that these emissions <a href="https://news.sky.com/story/renewable-power-reaches-record-30-of-global-electricity-13131048" target="_blank"><strong>would peak in 2023</strong></a>.</p>     <a href="https://news.sky.com/download-app" target="blank" data-tracking-label="ui-app-promo-download-link" data-type="" data-component-name="ui-app-promo">
        
    </a>


<p>But a series of heatwaves thwarted those expectations, as the world turned up the air conditioning to cope with hot weather turbocharged by <a href="https://news.sky.com/topic/climate-change-6161" target="_blank"><strong>climate change</strong></a> and the <a href="https://news.sky.com/story/2023-was-worlds-hottest-year-on-record-and-2024-could-be-worse-13044196" target="_blank"><strong>El Nino weather phenomenon</strong></a>, which has since subsided.</p><p>Increases in AI, electric vehicles and heat pumps are also driving up electricity demand.</p>     
<p>Ember forecasts the growth in clean power will soon outpace the growth in demand, helping to displace fossil fuels from the system.</p><p>Bruce Douglas, chief executive of the Global Renewables Alliance, said: "Despite geopolitical and economic headwinds, the renewables industry delivered an additional 858 TWh of generation to the system last year - more than the combined annual electricity consumption of the UK and France."</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Overwhelmingly Negative and Demoralizing Force (202 pts)]]></title>
            <link>https://aftermath.site/ai-video-game-development-art-vibe-coding-midjourney</link>
            <guid>43619759</guid>
            <pubDate>Tue, 08 Apr 2025 09:22:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aftermath.site/ai-video-game-development-art-vibe-coding-midjourney">https://aftermath.site/ai-video-game-development-art-vibe-coding-midjourney</a>, See on <a href="https://news.ycombinator.com/item?id=43619759">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>We’re a few years into a supposed artificial intelligence revolution, which could and should have been about reducing mundane tasks and freeing everyone up to do more interesting things with their time. Instead, thanks to the bloodthirsty nature of modern capitalism and an ideological crusade being waged by the tech industry, we’re now facing a world where many people’s livelihoods–like video game developers–are under direct threat.</p><p>These tumultuous times are of course being reported on everywhere you look, <a href="https://aftermath.site/tag/ai" target="_blank" rel="noreferrer noopener">including on this very website</a>, but one area I’ve been curious about recently aren’t the broader moral and legal battles, but what the struggle looks like for the average dev who is now having to encounter AI in their workplace.</p><p>For this piece, I spoke with a number of people working in the video game industry or very close to it, including artists, game designers, and software developers. I asked them to tell their stories about their daily interactions and struggles with&nbsp;artificial intelligence in the workplace, and what it means for the jobs they've been trained and hired to do.</p><p>Each person’s name and workplace has been changed to protect their identity.&nbsp;</p><figure><blockquote><p>[He] can’t even write a fucking email without using Chat GPT...</p></blockquote></figure><p><strong>Bradley </strong>is a veteran artist who works at a AAA video game studio. They describe the company as having been founded relatively recently, with investment coming from traditional AAA executives who “want to make money, and they are trying to figure out what game to make for that”.&nbsp;</p><p>They say their project’s art direction has been largely fuelled by AI prompts and generated imagery, all driven by their head Art Director, who is himself an experienced video game artist, but who now “can’t even write a fucking email without using Chat GPT”.&nbsp;</p><p>“In my interview with the company I was very clear that I do not like AI-generated content”, Bradley says, and adds that the team initially said that was fine, the tech was only being used for pitching purposes. But Bradley says the reality has proven vastly different, and that the studio has made increasing use of AI-generated imagery in their team's art pipeline.</p><p>“I have no idea how he ended up as an art director when he can’t visualise what he wants in his head unless can see some end results”, Bradley says. Rather than beginning with sketches and ideas, then iterating on those to produce a more finalised image or vision, Bradley says his boss will just keep prompting an AI for images until he finds one he likes, and then the art team will have to backwards engineer the whole thing to make it work.</p><p>“He doesn't know that the important thing isn't just the end result, it's the journey and the questions you answer along the way”. Bradley says that the studio’s management have become so enamoured with the technology that without a reliance on AI-generated imagery for presentations and pitches they would not be at the stage they are now, which is dealing with publishers and investors.&nbsp;</p><p>Bradley says they’ve threatened to quit if further attempts are made to use AI in the art pipeline. While the art team continues to resist the use of AI in creating actual concept pieces and assets for the game, it’s a different story when it comes to pitching the project to those publishers and investors.</p><p>“They’re finding it difficult to get funding because the game is being treated as a car salesman’s pitch, using AI-generated imagery to sell it without any actual substance or meat to back the vision up. The whole game is resting on a prompt ‘what if a game was…’, but with no idea if that would be fun, or how to make it fun. It’s madness”.&nbsp;</p><figure><blockquote><p>I had a meeting with the CEO where he told me he noticed I wasn't using the Chat GPT account the company had given me. I wasn't really aware the company was tracking that.</p></blockquote></figure><p><strong>Mitch </strong>has been working in software for over seven years. Having previously been employed in the defense industry, their last job was writing developer tools for a small tech startup creating an app. The company was built with some initial interest in artificial intelligence, though Mitch says this was met with “varying levels of scepticism” by employees.&nbsp;</p><p>“I was the most opposed”, Mitch says, “to the point where I felt it was a detriment. One team member who only ended up working there for a few months was very positive on it. The CEO was sceptical but felt like it could be a massive benefit if used correctly, and the other team members saw it as a fun toy that could occasionally be useful. So it was pretty broad across the board”.</p><p>Some at the company were uneasy with the technology early on, when the team initially had just a small AI plugin through which users could chat with the app. “On the way to the airport with a co-worker we talked about the introduction of these AI-generated plugins”, Mitch says. “Both of us felt incredibly uneasy with them. Despite [my coworker] finding AI to be a fun toy, he was worried that it wasn't what he had initially signed up for, and I had more or less the same sentiment. It didn't really boil over into anything at this point since the feature didn't go anywhere, but it was still notable”.</p><p>A few months later, things started to change. Mitch says the first signs of a deepening reliance on AI came when the company’s CEO was found to be rewriting parts of their app so that it would be easier for AI models to understand and help with. “Then”, Mitch says, “I had a meeting with the CEO where he told me he noticed I wasn't using the Chat GPT account the company had given me. I wasn't really aware the company was tracking that”.</p><p>“Anyway, he told me that I would need to start using Chat GPT to speed up my development process. Furthermore, he said I should start using <a href="https://en.wikipedia.org/wiki/Claude_(language_model)" target="_blank" rel="noreferrer noopener">Claude</a>, another AI tool, to just wholesale create new features for the app. He walked me through setting up the accounts and had me write one with Claude while I was on call with him. I’m still not entirely sure why he did that, but I think it may have been him trying to convince himself that it would work.”</p><p>Mitch describes this increasing reliance on AI to be not just “incredibly boring”, but ultimately pointless. “Sure, it was faster, but it had a completely different development rhythm”, they say. “In terms of software quality, I would say the code created by the AI was worse than code written by a human–though not drastically so–and was difficult to work with since most of it hadn’t been written by the people whose job it was to oversee it”.&nbsp;</p><p>“One thing to note is that just the thought of using AI to generate code was so demotivating that I think it would counteract any of the speed gains that the tool would provide, and on top of that would produce worse code than I didn’t understand. And that’s not even mentioning the ethical concerns of a tool built on plagiarism.”</p><p>Mitch says that over the 18 months they worked with the company, its CEO had transformed from someone mildly positive about AI to emphasising to workers that “AI was the future”. “In particular, he was of the belief that developers who are able to use AI to accelerate their process are going to win out in the future. I have seen this sentiment in other places worded as ‘an AI won't replace you, but a programmer who knows how to use AI will’. He didn’t believe that 18 months ago, but he absolutely believes it now”.&nbsp;</p><p>The AI’s poor code and a personal dislike of the technology meant that Mitch soon simply began ignoring their CEO’s instructions to use Chat GPT and Claude. A few months later, the entire company went bust. &nbsp;</p><figure><blockquote><p>The only issue I am facing as a professional art director in games is that I just want them to leave me and my art teams alone.</p></blockquote></figure><p><strong>Francis </strong>is both a consultant and artist in the video game industry, and so has worked alongside art teams as well as working with agencies, studios and publishers about their creative processes. “This means I have had some truly wild conversations about AI in a professional context that make me want to walk into the sea”, they say.</p><p>Since AI-generated imagery first blew up a few years ago, Francis has found that discussions both with consulting clients and potential employers for their own art has “shifted”. They believe that most employers and outsiders they speak to don’t see themselves as part of what Francis calls an ‘AI will replace artists!’ crowd, but are more like a ‘if we use AI it’ll make an artist’s life easier’ crowd, who think using AI in art will protect their artists, not displace them.</p><p>“What follows from these discussions is me explaining why, usually over hours rather than minutes, that these tools have no place in a professional game development pipeline or production and actually hinder the development of visuals”, Francis says. “I also find myself explaining to them how the iteration and 'idea' phase of a project is where the best stuff happens, how exploring things through artistic labor is where your best ideas come to fruition, and why would we want an AI (that we don't even own) to do that for us with art that isn't ours to use?”</p><p>“I am yet to have a team or gamerunner push back on me once I actually explain how these AI art generators work and how they don't contribute in a helpful way to a project, but I have a sense of dread that it is only a matter of time until that changes, especially given that I've gone the majority of my career with no mention of them to every second conversation having it mentioned.”</p><p>This all sounds <em>slightly </em>more optimistic than what other people I spoke with expressed, but Francis also added that a close family member is “neck-deep in the technology side of the AI industry”, and that “while conversation with them mostly gives me the desire to purchase cigarettes for the first time in a very long time, it also gives me insight into how these people feel when they say they are 'helping' my industry and 'innovating' to make artists' lives better.”</p><p>Francis says their understanding of the AI-pusher’s outlook is that they see the entire game-making process as a problem, one that AI tech companies alone can solve. “When I’m told 'Think of how much time you could be spending instead on making the actual game!', those who have drank the AI Kool-Aid don't understand that all this brainstorming and iteration <em>is </em>making the game, it’s a crucial everyday part of game development (and human interaction) and is not a problem to be solved.”</p><p>“My experience here is unfortunately not singular”, Francis says. “I have had many discussions with other game developers who interact with AI engineers and savants who believe our industry pipelines need 'fixing' by them and them alone. The only other similar experience I can think of as a comparison is a snake oil salesman insisting that their magic tonic will fix all of my problems that don't exist but they insist that I have.”</p><p>Francis believes that there’s a common thread running through both their lines of work: that those selling AI tools and those advocating for their use, whatever their best intentions, think that AI is solving a huge problem games industry workers are facing. “The only issue I am facing as a professional art director in games is that I just want them to leave me and my art teams alone so we can make cool art. There is no problem to be solved here”.</p><figure><img alt="" src="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2025/04/kickout.jpg?w=1440&amp;h=810&amp;crop=1" srcset="https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2025/04/kickout.jpg?resize=480,270&amp;quality=75 480w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2025/04/kickout.jpg?resize=960,540&amp;quality=75 960w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2025/04/kickout.jpg?resize=640,360&amp;quality=75 640w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2025/04/kickout.jpg?resize=1280,720&amp;quality=75 1280w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2025/04/kickout.jpg?resize=768,432&amp;quality=75 768w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2025/04/kickout.jpg?resize=1536,864&amp;quality=75 1536w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2025/04/kickout.jpg?resize=1024,576&amp;quality=75 1024w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2025/04/kickout.jpg?resize=2048,1152&amp;quality=75 2048w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2025/04/kickout.jpg?resize=1280,720&amp;quality=75 1280w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2025/04/kickout.jpg?resize=2560,1440&amp;quality=75 2560w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2025/04/kickout.jpg?resize=1440,810&amp;quality=75 1440w, https://lede-admin.aftermath.site/wp-content/uploads/sites/55/2025/04/kickout.jpg?resize=2880,1620&amp;quality=75 2880w" sizes="100vw" loading="lazy"><figcaption><a href="https://www.shutterstock.com/image-vector/robot-kick-female-employee-out-office-2229371365" target="_blank" rel="noreferrer noopener">Drawlab19</a></figcaption></figure><figure><blockquote><p>It was a huge waste of time, and really felt like an affront to my own expertise, which is... why I was hired.</p></blockquote></figure><p><strong>Ricky </strong>is a senior game designer who says they were often asked to use AI on their last project, initially with the goal of saving time on basic tasks. One example they shared was being encouraged to use Chat GPT to do things like “generate outlines of existing games and their systems so that we could use them as references and discussion points. The issue? [The outlines] were often wrong, and I'd spend more time fixing them than if I'd just done it the old-fashioned way”.</p><p>Since the AI was generating responses for games Ricky was already familiar with, they could easily see that those responses were fundamentally incorrect. ”When I say they were wrong”, Ricky says, ” I mean they were talking about systems that didn't exist, items that didn't exist, and outlines of enemies that had incorrect explanations of their behaviour”.</p><p>“There were more superfluous issues as well -- the whole thing read like marketing speak. There were buzzwords everywhere, it wasn't concise, and if I were to try and fix these issues using different prompts, or continuing my conversation with [the AI], well, then I'm just spending time prompt-engineering instead of just writing the fucking thing myself. It was a huge waste of time, and really felt like an affront to my own expertise, which is... why I was hired”.</p><p>Ricky says they were also asked to use Chat GPT as an “idea generator”, to help with tasks like “coming up with a bunch of interesting puzzles” or “creating ideas for enemies”, supposedly to both save time and provide inspirational starting points.&nbsp;</p><p>“It really weighed on the creativity of my role, and again, spat in the face of my expertise”, they say. “It wasn't just this though; the tool itself lacks the intent, context, and limitations of what we're doing. It doesn't have other aspects of the project, influences, references, or personal experiences in the back of its mind, because it doesn't have a mind. Whenever we design something for a game, it's drawn from somewhere, influenced by other things, and filtered through our own experiences as a human. These AI ideas lose ALL of that, turning it into an omega-corporate ‘back of the box’ list of ideas and features”.</p><figure><blockquote><p>I raised the concern that we shouldn't be doing any of this without consent from those actors...</p></blockquote></figure><p><strong>Sally </strong>is an animator and 3D artist whose most recent video game work was as part of an indie studio working on a VR game. They say that AI tools made deep inroads into life at the studio, beginning with the game’s creative directors and lead designers “playing around with Midjourney” before quickly deciding that they wouldn’t need to hire concept artists, then moving on to declaring that they wouldn’t need to pay <em>anyone at all</em> to create the game’s 2D assets, because those could just be generated by Midjourney then edited in Photoshop.&nbsp;</p><p>Sally began voicing their concerns directly with management when AI moved into the game’s voice work, as bosses began toying with new AI-generated lines trained on work previously performed by voice actors. “I raised the concern that we shouldn't be doing any of this without consent from those actors”, Sally says, “and was met with a very aggressive shutdown response which was that ‘it wasn't going to ship’ and would only be used as an internal tool, so it shouldn't be an issue at all”.</p><p>“This is pretty awful for the voice actors because there’s not really anything they can do if a studio decides to do something like this. [Studios] already have a bunch of their voice work, and often voice actors will just send the entire recording of their lines over at once. So a studio will have multiple takes of each line, and sometimes different variations of the same line, which is kind of a perfect example for something you could feed through AI”.</p><p>Sally was recently laid off from the studio, along with a few of their coworkers, and says that as far as they’re aware the studio’s plan for its next game is to use AI-generated animations trained on motion capture to replace the role of a human animator. “I think the last thing I want to add is just how frustrating it is having AI-generated art in a game that’s about to ship, alongside all the hard work that we human artists put into it. All because one of the directors wanted to cut down on ‘wasting time’ or ‘hiring one other person’”.&nbsp;</p><figure><blockquote><p>[AI] has been an overwhelmingly negative and demoralizing force in my own personal workplace, no question about it.</p></blockquote></figure><p><strong>Audrey </strong>is a veteran concept artist who has worked both as a freelancer and in-house at studios in the video game industry. With qualifications in both art and game design, they say that over the last few years the emergence of AI tech in the art world “has been an overwhelmingly negative and demoralizing force in my own personal workplace, no question about it”.</p><p>“My own use of AI in my current job has been minimal and against my own wishes and ethics”, they say. “Once in a blue moon I have been given a task that necessitated using AI to make very VERY quick concepts. And when I say ‘necessitated’, I would be told explicitly to use AI and make a ton of images to pick from, and I could then ‘make them look good later’. Depressing doesn’t begin to cover it”.</p><p>Audrey says that their work deadlines are “being completely fucked up as well”, as they will usually need to completely redraw AI-generated images that colleagues in other departments have relied on as placeholder art, which then shifts their own workflow and priorities.&nbsp;</p><p>Audrey says their team’s overall response to AI tech has been “mixed”. “Only a few specific people higher up like to use AI and will use it regularly, while the majority of the team is neutral or completely against using it”, they say. “My own personal opinion was quite vocal. I tried to be clear about my stance on using AI, and so did the rest of the art team. I gave multiple reasons for our company to stop or minimize using it; I’d ask about replacing images that were AI placeholders or if there was a way we could use art already made instead, all of which had mixed results”.</p><figure><blockquote><p>They never hired anyone for the artist position, and are using AI-generated art for their games.</p></blockquote></figure><p><strong>Alfie </strong>is an experienced 2D artist in the video game industry. They recently interviewed at a startup games studio with millions in funding, and came away deeply affected by AI without even working there.</p><p>“This corporate, tech-backed startup near me were looking to hire an artist”, they say. “I went through their entire recruitment process, which took around five weeks, including a trip in person to visit them for several hours. I even took part in a brainstorming session for about an hour, where I ended up coming up with a production plan with some basic schedules. After around three weeks of not hearing anything back from them, they finally got in touch to say they wouldn’t be progressing with my application”.</p><p>Alfie wonders if the studio might have simply been using them for some free art and production consultation. “They never hired anyone for the artist position, and are using AI-generated art for their games”, they say. “There are zero artists on their team”.</p><p>Alfie has since joined a new studio, which <em>had </em>been experimenting with AI-generated imagery a few years back but abandoned its use entirely after finding the results neither accurate nor specific enough for in-game purposes. This studio changed course entirely, and Alfie’s hiring was part of a heavy investment in human art.</p><p>“I guess they realised that AI imagery can't solve all of your problems, and they realise just how precious real human artists are”, Alfie says. “A human artist will research the material properly, with all the nuances of human emotion and logic, and ultimately produce art assets that are historically accurate, game specific and have soul”.</p><figure><blockquote><p>...we have no other choice but to look for mundane work and give up our passion...</p></blockquote></figure><p><strong>Douglas </strong>is a voice actor whose jobs include recording lines for indie video games, and says their experiences discussing AI with potential employers "feels like screaming into a void". "I've gotten through to small indie teams", they say, "and some publishers have been empathetic and understanding when I try steering them away from AI, since it hurts already-struggling independent artists and actors. But it's always the larger corporations that are eerily silent and flat out seem to ignore anyone who wants to talk about the subject".</p><p>As an example, Douglas says Audible has recently <a href="https://www.theverge.com/2024/9/9/24239903/amazon-audible-audiobook-narrators-ai-generated-voice-clones" target="_blank" rel="noreferrer noopener">begun trialling a system where actors can submit their recordings to an AI program</a> that will duplicate their voice, then sell it to book authors for their audiobooks. Those actors will then receive a small percentage of any income the AI 'voice' generates, but Douglas can easily see that becoming problematic for the market, with existing (and upcoming) voice actors able to be easily replaced by anyone looking for a "quick and easy" solution.</p><p>I asked Douglas how they and their peers are feeling about this, <a href="https://aftermath.site/sag-aftra-voice-actor-strike-genshin-impact-destiny-2" target="_blank" rel="noreferrer noopener">and other challenges being presented by AI in the voice acting space</a>, and their response was blunt. "Myself and a lot of my fellow voice actors are in the same boat", they say. "None of us feel like submitting our voice to an AI is worth the benefit, because it will destroy a lot of the human emotion we put into our work to help bring a story to life".</p><p>"A lot of us worked tirelessly to make even a fraction of a living doing something we genuinely love, and we're afraid of that being taken away from us, so it's looking like we have no other choice but to look for mundane work and give up our passion".</p><hr><p>These are just some people’s personal experiences, a small snapshot of how AI is affecting the games industry. While <a href="https://gdconf.com/news/gdc-2025-state-game-industry-devs-weigh-layoffs-ai-and-more" target="_blank" rel="noreferrer noopener">surveys and numbers</a> can provide some sense of scale, individual experiences can highlight the precise ways AI is encroaching into people’s workplaces and lives, how they feel about it, and how they’re responding.</p><p>As grim as some of these responses feel, I hope there’s at least some small solace to be taken from the fact that so many artists remain firm in their resistance to AI technology that devalues their work, and are championing human creators in the face of it.</p><hr><p><a rel="noreferrer noopener" href="https://aftermath.site/baseball-week-2025" target="_blank">Inside Baseball Week</a>&nbsp;is our annual week of stories about the lesser-known parts of game development, the ins and outs of games journalism, and a peek behind the curtain at Aftermath. It's part of our second, even more ambitious subscription drive, which you can&nbsp;<a rel="noreferrer noopener" href="https://aftermath.site/aftermath-site-subscriber-goals-2025" target="_blank">learn more about here</a>. If you like what you see, please consider&nbsp;<a rel="noreferrer noopener" href="https://aftermath.site/products" target="_blank">subscribing</a>!</p></div><div><h2>Stay in touch</h2><p>Sign up for our free newsletter</p></div><div><h2><span>More from Aftermath</span></h2><div><div><a tabindex="0" href="https://aftermath.site/manga-no-name-ending-edgerunners"><h3>The Truth Behind The Sudden Conclusion Of Cyberpunk: Edgerunners Showrunner’s Short-Lived Manga Series</h3></a><p>Edgerunners showrunner Rafał Jaki mythbusts the ins-and-outs of creating his historic manga series, No\Name</p></div><div><a tabindex="0" href="https://aftermath.site/aftermath-site-subscriber-goals-2025"><h3>Aftermath Vs 2025</h3></a><p>We've got even bigger plans than last year, and we need your help</p></div></div><a tabindex="0" href="https://aftermath.site/all">See all posts<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="#000" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5 12h13m-6-7 7 7-7 7"></path></svg></a></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Less Htmx Is More (158 pts)]]></title>
            <link>https://unplannedobsolescence.com/blog/less-htmx-is-more/</link>
            <guid>43619581</guid>
            <pubDate>Tue, 08 Apr 2025 08:42:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://unplannedobsolescence.com/blog/less-htmx-is-more/">https://unplannedobsolescence.com/blog/less-htmx-is-more/</a>, See on <a href="https://news.ycombinator.com/item?id=43619581">Hacker News</a></p>
<div id="readability-page-1" class="page"><header>
  <h2><a href="https://unplannedobsolescence.com/">Unplanned Obsolescence</a></h2>
  <nav>
  <ul>
    <li><a href="https://unplannedobsolescence.com/blog">Blog</a>
    </li><li><a href="https://unplannedobsolescence.com/about">About</a>
    </li><li><a href="https://unplannedobsolescence.com/atom.xml">RSS</a>
  </li></ul>
  </nav>
</header>

<hr>




<p><strong>October 02, 2024</strong></p>

<p>It’s been two years since I wrote my first production webservice with <a href="https://htmx.org/">htmx</a>.
Two years is not a very long time, but early indicators suggest that the software projects I’ve written with htmx are a much better experience for users, and orders of magnitude easier to maintain, than the software projects they replaced.
They are likely to remain useful for longer than anything else I’ve ever written (so far).
Pretty good!</p>
<p>Like any new tool, especially a tool that got popular <a href="https://risingstars.js.org/2023/en#section-framework">as quickly as htmx</a>, there are differing schools of thought on how best to use it.
My approach—which I believe necessary to achieve the results described above—requires you to internalize something that htmx certainly hints at, but doesn’t enforce: use plain HTML <em>wherever possible</em>.</p>
<p>Once you get the hang of it, htmx starts pushing you in this direction anyway, and you start reaching for htmx less and less.
It requires a mindset shift though, especially if you’re not accustomed to <a href="https://unplannedobsolescence.com/blog/behavior-belongs-in-html/">building page behavior with HTML features</a>.</p>
<h2 id="how-should-we-use-htmx"><a href="#how-should-we-use-htmx" aria-label="Anchor link for: how-should-we-use-htmx">How should we use htmx?</a></h2>
<p>In my opinion, most websites should be using htmx for either:</p>
<ol>
<li>Updates that users would not expect to see on a refresh (or a new page load)</li>
<li>Updates that would <em>also</em> be present on a refresh (or a new page load)</li>
</ol>
<p>Everything else should use <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/a">regular links</a> and <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/form">regular forms</a> that do standard, full-page navigations.</p>

<p>Let’s say you’re making a website that shows today’s baseball games, and you want it to update the stats live.
Here’s how I would approach that.</p>
<p>The website’s home page should have all the currently-playing games on it, showing the live score for each one.
Each of those live scoreboards uses htmx to <a href="https://htmx.org/attributes/hx-trigger/#polling">poll the server</a> at regular intervals for updates.
Clicking on the scoreboard title (which is a regular <code>&lt;a&gt;</code> link) takes you to that game’s page, at its own URL.
The game page has not just the score, but the pitch count, the game’s full box score, and so on;
these update with htmx as well.</p>
<p>The idea here is that the website still has a sound URL structure, which is managed by the core browser functionality, while interactivity is carefully layered on top, with targeted updates.
Exactly what merits a targeted update versus a new page depends on what you’re building, but you should have a mental model that distinguishes between them in some capacity.</p>

<p>Unfortunately, a lot of the beginner guides suggest that you can get started easily by “upgrading” all your links with <code>hx-boost</code>.
I disagree with this.
While htmx is amazing for targeted page updates, I highly discourage using it to take over <em>all</em> page navigation.</p>
<h2 id="what-is-hx-boost"><a href="#what-is-hx-boost" aria-label="Anchor link for: what-is-hx-boost">What is hx-boost?</a></h2>
<p><a href="https://htmx.org/attributes/hx-boost/"><code>hx-boost</code></a> is an attribute that converts a “regular” link into a “boosted” link:</p>
<pre data-lang="html"><code data-lang="html"><span>&lt;!-- normal link --&gt;
</span><span>&lt;</span><span>a </span><span>href</span><span>=</span><span>example.com</span><span>&gt;Example&lt;/</span><span>a</span><span>&gt;
</span><span>
</span><span>&lt;!-- boosted link --&gt;
</span><span>&lt;</span><span>a </span><span>href</span><span>=</span><span>example.com </span><span>hx-boost</span><span>=</span><span>true</span><span>&gt;Example&lt;/</span><span>a</span><span>&gt;
</span></code></pre>
<p>Instead of doing a full page navigation when the “boosted” link is clicked, htmx will issue an HTTP request to the link’s URL and replace the <code>&lt;body&gt;</code> of the page with the content of the response.
In theory, this feels “smoother” because it only repaints part of the page, mimicking the feel of a Single-Page Application (SPA).</p>
<h2 id="what-s-wrong-with-hx-boost"><a href="#what-s-wrong-with-hx-boost" aria-label="Anchor link for: what-s-wrong-with-hx-boost">What’s wrong with hx-boost?</a></h2>
<p>The problems it solves are better solved by other means, and it creates a lot of problems on its own.</p>
<p>Use <code>hx-boost</code> long enough, and something will go wrong.
You’ll click the back button and see only a partial page update;
you’ll refresh the page and it’ll go blank;
another library that you’re using will conk out;
elements will enter or exit the DOM in a way that you did not expect.</p>
<p>From a coding perspective, this is not anyone’s fault—the features promised by <code>hx-boost</code> are impossible.
<code>hx-boost</code> uses the JavaScript <a href="https://developer.mozilla.org/en-US/docs/Web/API/History_API">History API</a>, which exists to let single-page apps (SPAs) hook into session management functionality, most notably the browser’s forward and back buttons.
In practice, this is virtually impossible to get right, and is so annoying to implement that htmx creator Carson Gross made <a href="https://htmx.org/img/memes/javascripthistory.png">a meme</a> about it.</p>
<p>The core problem is that with normal page navigation, each link you click resets the JavaScript environment and triggers a full set of page <a href="https://developer.mozilla.org/en-US/docs/Web/API/Document/DOMContentLoaded_event">lifecycle events</a>.
This is a very good thing.
It means that every additional script you include on the page has a standardized way to keep track of what’s happening.
If you replace this process with an ad-hoc, scripting-based navigation, you remove access to that common language for every other library on your page.
You also initiate a long-lived JavaScript environment that is likely to eventually enter a bad state of some kind.</p>
<p>This problem is inherent to SPAs, and it can only be resolved by not writing SPAs.
So don’t use the attribute that turns your htmx site into an SPA.</p>
<h2 id="what-should-i-do-instead"><a href="#what-should-i-do-instead" aria-label="Anchor link for: what-should-i-do-instead">What should I do instead?</a></h2>
<p>Use <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/a">regular links</a>. <code>hx-boost</code> promises to enhance the experience of a regular link; skip the middleman and just use them.</p>
<p><a href="https://unplannedobsolescence.com/blog/hard-page-load/">Regular links are a better user experience and developer experience</a>, full stop.</p>
<h2 id="what-about-the-benefits-of-hx-boost"><a href="#what-about-the-benefits-of-hx-boost" aria-label="Anchor link for: what-about-the-benefits-of-hx-boost">What about the benefits of hx-boost?</a></h2>
<p>The first time you use <code>hx-boost</code>, it feels magical to have the page update “seamlessly” like that, but you can achieve all the same benefits, without the headaches, using browser features.</p>
<h3 id="send-cache-headers-to-re-use-css-and-js-across-page-loads"><a href="#send-cache-headers-to-re-use-css-and-js-across-page-loads" aria-label="Anchor link for: send-cache-headers-to-re-use-css-and-js-across-page-loads">Send cache headers to re-use CSS and JS across page loads</a></h3>
<p>Basically all static file servers support <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/ETag">ETags</a>.
When the server sends the browser a file, it can also send a unique string that identifies <em>that version</em> of the file.
The next time you try to load that file (after, for instance, navigating to a new page that uses the same CSS), the browser asks your server, “is it still this one?”, and sends that ETag string.
If the file hasn’t changed, the server just responds with a <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/304">304 Not Modified</a> header and the browser users its cached version.</p>
<p>In most cases, this is process adds essentially nothing to your load times.
The browser has to talk to the server anyway to get whatever info is on the next page, and it’s <a href="https://www.rfc-editor.org/rfc/rfc2616#section-8.1">re-using the same TCP connection</a> to do so.
The GET -&gt; 304 back-and-forth is a handful of extra bytes on an already-open socket.</p>
<p>But if you don’t want the browser to even <em>ask</em>, you can do that do by setting a <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control#response_directives">cache control header</a>.</p>
<p>Here’s how I load htmx in all the websites where I use it.
We’ll use version 1.9.3 as an example.
I include a script tag like this in the header:</p>
<pre data-lang="html"><code data-lang="html"><span>&lt;</span><span>script </span><span>src</span><span>="</span><span>/htmx-1.9.3.js</span><span>"&gt;&lt;/</span><span>script</span><span>&gt;
</span></code></pre>
<p>When the user loads the page for the very first time, their browser sends the HTTP request <code>GET /htmx-1.9.3.js</code> to my server.
The server will send back something like this in response:</p>
<pre data-lang="http"><code data-lang="http"><span>HTTP/2 200
</span><span>accept-ranges: bytes
</span><span>cache-control: public, max-age=31536000
</span><span>last-modified: Fri, 06 Sep 2024 22:09:43 GMT
</span><span>etag: W/"24b79-191c962d458"
</span><span>content-type: application/javascript; charset=UTF-8
</span></code></pre>
<p>That says: “download htmx 1.9.3 from my server exactly once, and then never ask me for it again for <em>a full calendar year</em>.”
From that point on, for one year, every time that browser loads a page at the same domain that includes htmx 1.9.3, the browser won’t even ask the server for it, it’ll just use the saved version.
If I want to upgrade everyone to a new version, I just change the version number in the URL:</p>
<pre data-lang="html"><code data-lang="html"><span>&lt;!-- From this... --&gt;
</span><span>&lt;</span><span>script </span><span>src</span><span>="</span><span>/htmx-1.9.3.js</span><span>"&gt;&lt;/</span><span>script</span><span>&gt;
</span><span>
</span><span>&lt;!-- ...to this --&gt;
</span><span>&lt;</span><span>script </span><span>src</span><span>="</span><span>/htmx-1.9.4.js</span><span>"&gt;&lt;/</span><span>script</span><span>&gt;
</span></code></pre>
<p>The next time each of my users loads that page, their browsers will see that the page requires a new file it doesn’t have, and ask for the server for it again.</p>
<p>If I don’t even want to include a version number—maybe for a file like <code>stylesheet.css</code>—I can use a URL query.</p>
<pre data-lang="html"><code data-lang="html"><span>&lt;!-- The browser will consider these two different files,
</span><span>      but your server will know that they're the same --&gt;
</span><span>&lt;</span><span>link </span><span>rel</span><span>="</span><span>stylesheet</span><span>" </span><span>href</span><span>="</span><span>/stylesheet.css</span><span>"&gt;
</span><span>&lt;</span><span>link </span><span>rel</span><span>="</span><span>stylesheet</span><span>" </span><span>href</span><span>="</span><span>/stylesheet.css?id=1</span><span>"&gt;
</span></code></pre>
<p>Again, basically every static file server supports this pattern.</p>
<h3 id="use-same-origin-links-to-get-partial-page-updates"><a href="#use-same-origin-links-to-get-partial-page-updates" aria-label="Anchor link for: use-same-origin-links-to-get-partial-page-updates">Use same-origin links to get partial page updates</a></h3>
<p>This website (<a href="https://unplannedobsolescence.com/">unplannedobsolescence.com</a>) uses exclusively regular links, and if you click around up top you’ll see that the header largely stays in place.
This happens automatically now, for same-origin links to pages with the same structure and stylesheets (like I showed you above).</p>
<p>Here’s the Chrome team <a href="https://developer.chrome.com/blog/paint-holding">announcing this feature</a>:</p>
<blockquote>
<p>Try Paint Holding in Chrome Canary (Chrome 76) and let us know what you think. Developers shouldn’t have to worry about making any modifications to their pages to take advantage of it.</p>
</blockquote>
<p>Chrome 76 came out four years ago, in 2019.
Everyone who built their website with regular links got a significant, free performance upgrade to their website pushed out to billions of people;
the same is not true for everyone who tried to replace that functionality with JavaScript.</p>
<h3 id="leverage-html-for-free-performance-upgrades"><a href="#leverage-html-for-free-performance-upgrades" aria-label="Anchor link for: leverage-html-for-free-performance-upgrades">Leverage HTML for free performance upgrades</a></h3>
<p>Using standard HTML features allows the browser to optimize performance and UX in ways that JavaScript is categorically incapable of.
Every time the browser updates it is getting better at loading, parsing, and rendering webpages.
Page history, loading bars, the back button, the cancel button, the URL bar, etc., all work correctly, by default, every time, on every browser.</p>
<p><a href="https://unplannedobsolescence.com/blog/hard-page-load/#in-the-long-run-the-browser-always-wins">In the long run, the browser always wins.</a></p>
<h2 id="why-does-hx-boost-exist-then"><a href="#why-does-hx-boost-exist-then" aria-label="Anchor link for: why-does-hx-boost-exist-then">Why does hx-boost exist then?</a></h2>
<p>htmx was created during a period in which it seemed like SPAs were the inevitable future of web development.
To compete in that environment, it had to demonstrate that it could replicate what most people considered to be the killer feature of SPAs: not repainting the whole page.
If this was ever necessary—I’m skeptical—it’s sure not necessary anymore.</p>
<p>Now that htmx has proven itself in the mindshare ecosystem, and developers are starting to trust multi-page websites again, I think the time has come to make the harder, but ultimately more impactful case: HTML and HTTP have the features required to build the vast, vast majority of website functionality; they’re easier to use than the scripting alternatives, and they last longer with much less maintenance.</p>

<p>Building good websites requires dropping the sugar high of <code>hx-boost</code> and saying “here’s how to use a cache header.”</p>
<h2 id="is-there-ever-a-time-i-should-use-htmx-to-make-an-spa"><a href="#is-there-ever-a-time-i-should-use-htmx-to-make-an-spa" aria-label="Anchor link for: is-there-ever-a-time-i-should-use-htmx-to-make-an-spa">Is there ever a time I should use htmx to make an SPA?</a></h2>
<p>My friend <a href="https://aramzs.xyz/">Aram</a> made a website called <a href="https://songobsessed.com/">Song Obsessed</a> that has a persistent music player which holds its state even as you navigate around the site.
<code>hx-boost</code> is a good fit for this because it allows you to construct your website as a series of URLs; you can just slap <code>hx-boost</code> on everything and, with a little tweaking, you can get htmx to leave the music player alone while replacing the rest of the page.
You still lost the reliability inherent in the hard page load, but you get genuinely novel functionality in exchange, which is a good trade in this case.
Until HTML has an API to keep live content persistent across page navigations, some SPA functionality is required to make that happen.</p>
<p>SPAs are an <em>advanced</em> tool that the industry deceptively marketed as a simple one.
Aram is a highly experienced web developer who’s using <code>hx-boost</code> to push the boundaries of what’s possible with page navigations;
Most people, who just want to add a little interactivity to their webpage, should <a href="https://grugbrain.dev/#grug-on-complexity">stick with the simplest tool available</a>: a <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/a">regular link</a>.</p>
<p><em>Thanks to Carson Gross his for feedback on a draft of this article. Comments available on <a href="https://lobste.rs/s/1uv7e4/less_htmx_is_more">lobste.rs</a></em></p>
<h2 id="notes"><a href="#notes" aria-label="Anchor link for: notes">Notes</a></h2>
<ul>
<li><a href="https://alexanderpetros.com/triptych/">Triptych</a>—the HTML proposals that Carson and I are working on—would render htmx obsolete for the type of website I describe here.
More advanced htmx features, like the ones used to great effect by <a href="https://david.guillot.me/en/posts/tech/following-up-mother-of-all-htmx-demos/">David Guillot and Contexte</a>, will still require htmx for the foreseeable future.</li>
<li>To add PUT (and DELETE) support to “regular” forms with htmx, add <a href="https://htmx.org/attributes/hx-put/"><code>hx-put</code></a> to the form, and then have the server respond with status code 200 and an <a href="https://htmx.org/headers/hx-redirect/"><code>HX-Redirect</code></a> header;
instead of doing partial page replacement, htmx will tell the browser to do a full-page navigation.
This mimics the POST-Redirect-GET pattern, but uses a header instead of a 303 response.</li>
<li>Ideally, htmx would be able to intercept a normal 303 response and use the <code>location</code> header, instead of a custom header, but because of limitations in the fetch API (<code>manual</code> redirects <a href="https://developer.mozilla.org/en-US/docs/Web/API/RequestInit#redirect">hide all the headers</a>), it can’t.
I don’t totally understand what security purpose this serves, to be honest, but it’s a bit of a shame, because it means that <a href="https://github.com/alexpetros/triptych?tab=readme-ov-file#limitations">you can’t make a proper polyfill</a> for PUT and DELETE forms.</li>
<li>Both <a href="https://dev.37signals.com/a-happier-happy-path-in-turbo-with-morphing/">turbo</a> and <a href="https://data-star.dev/essays/another_dependency#fn:1">datastar</a> use Carson’s <a href="https://github.com/bigskysoftware/idiomorph">idiomorph</a> algorithm to merge updates into the page, but Carson ended up rejecting idiomorph as the default merging algorithm for htmx, because it was too complicated—even though he’s the one who created it in the first place!
The default <a href="https://htmx.org/attributes/hx-swap/">htmx swap strategy</a> is to just wipe away what’s inside the <code>innerHTML</code> and replace it with the response—not unlike how the default page navigation is to wipe away the environment and give you a fresh one.
Simple; effective.</li>
</ul>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intelligence Evolved at Least Twice in Vertebrate Animals (174 pts)]]></title>
            <link>https://www.quantamagazine.org/intelligence-evolved-at-least-twice-in-vertebrate-animals-20250407/</link>
            <guid>43619548</guid>
            <pubDate>Tue, 08 Apr 2025 08:36:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/intelligence-evolved-at-least-twice-in-vertebrate-animals-20250407/">https://www.quantamagazine.org/intelligence-evolved-at-least-twice-in-vertebrate-animals-20250407/</a>, See on <a href="https://news.ycombinator.com/item?id=43619548">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>
            Complex neural circuits likely arose independently in birds and mammals, suggesting that vertebrates evolved intelligence multiple times.        </p>
        
    </div><div>
            <h2>Introduction</h2>
            <div data-role="selectable">
    <p>Humans tend to put our own intelligence on a pedestal. Our brains can do math, employ logic, explore abstractions and think critically. But we can’t claim a monopoly on thought. Among a variety of nonhuman species known to display intelligent behavior, birds have been shown time and again to have advanced cognitive abilities. <a href="https://www.science.org/content/article/ravens-humans-and-apes-can-plan-future">Ravens plan</a> for the future, <a href="https://www.quantamagazine.org/animals-can-count-and-use-zero-how-far-does-their-number-sense-go-20210809/">crows count</a> and <a href="https://www.allaboutbirds.org/news/unique-beak-evolved-with-tool-use-in-new-caledonian-crow/">use tools</a>, <a href="https://www.audubon.org/news/wild-cockatoos-and-humans-compete-rubbish-prize-potential-arms-race">cockatoos open and pillage</a> booby-trapped garbage cans, and <a href="https://knowablemagazine.org/content/article/mind/2019/chickadee-memory-food">chickadees keep track</a> of tens of thousands of seeds cached across a landscape. Notably, birds achieve such feats with brains that look completely different from ours: They’re smaller and lack the highly organized structures that scientists associate with mammalian intelligence.</p>
<p>“A bird with a 10-gram brain is doing pretty much the same as a chimp with a 400-gram brain,” said <a href="https://hahn-institute.de/en/research/group/biopsychology-guentuerkuen">Onur Güntürkün</a>, who studies brain structures at Ruhr University Bochum in Germany. “How is it possible?”</p>
<p>Researchers have long debated about the relationship between avian and mammalian intelligences. One possibility is that intelligence in vertebrates — animals with backbones, including mammals and birds — evolved once. In that case, both groups would have inherited the complex neural pathways that support cognition from a common ancestor: a lizardlike creature that lived 320 million years ago, when Earth’s continents were squished into one landmass. The other possibility is that the kinds of neural circuits that support vertebrate intelligence evolved independently in birds and mammals.</p>
<p>It’s hard to track down which path evolution took, given that any trace of the ancient ancestor’s actual brain vanished in a geological blink. So biologists have taken other approaches — such as comparing brain structures in adult and developing animals today — to piece together how this kind of neurobiological complexity might have emerged.</p>
<p>A series of studies <a href="http://dx.doi.org/10.1126/science.adv2609">published in <em>Science</em></a> in February 2025 provides the best evidence yet that birds and mammals did not inherit the neural pathways that generate intelligence from a common ancestor, but rather evolved them independently. This suggests that vertebrate intelligence arose not once, but multiple times. Still, their neural complexity didn’t evolve in wildly different directions: Avian and mammalian brains display surprisingly similar circuits, the studies found.</p>
<p>“It’s a milestone in the quest to understand and to integrate the different ideas about the evolution” of vertebrate intelligence, said Güntürkün, who was not involved in the new research.</p>
</div>
    </div><div data-role="selectable">
    <p>The findings emerge in a world enraptured by artificial forms of intelligence, and they could teach us something about how complex circuits in our own brains evolved. Perhaps most importantly, they could help us step “away from the idea that we are the best creatures in the world,” said <a href="https://www.kuleuven.be/wieiswie/en/person/00132293">Niklas Kempynck</a>, a graduate student at KU Leuven who led one of the studies. “We are not this optimal solution to intelligence.”</p>
<p>Birds got there too, on their own.</p>
<h2><strong>Pecking Disorder </strong></h2>
<p>For the first half of the 20th century, neuroanatomists assumed that birds were simply not that smart. The creatures lack anything resembling a neocortex — the highly ordered outermost structure in the brains of humans and other mammals where language, communication and reasoning reside. The neocortex is organized into six layers of neurons, which receive sensory information from other parts of the brain, process it and send it out to regions that determine our behavior and reactions.</p>

<p>“For the longest time, it was thought that this is the center of cognition, and you need this kind of anatomy to develop advanced cognitive abilities,” said <a href="https://home.kaessmannlab.org/members">Bastienne Zaremba</a>, a postdoctoral researcher studying the evolution of the brain at Heidelberg University.</p>
<p>Rather than neat layers, birds have “unspecified balls of neurons without landmarks or distinctions,” said <a href="https://www.achucarro.org/people/fernando-garcia-moreno/">Fernando García-Moreno</a>, a neurobiologist at the Achucarro Basque Center for Neuroscience in Spain. These structures compelled neuroanatomists a century ago to suggest that much of bird behavior is reflexive, and not driven by learning and decision-making. This “implies that what a mammal can learn easily, a bird will never learn,” Güntürkün said.</p>
<p>The conventional thinking started to change in the 1960s when Harvey Karten, a young neuroanatomist at the Massachusetts Institute of Technology, mapped and compared brain circuits in mammals and pigeons, and later in owls, chickens and other birds. What he found was a surprise: The brain regions thought to be involved only in reflexive movements were built from neural circuits — networks of interconnected neurons — that resembled those found in the mammalian neocortex. This region in the bird brain, the dorsal ventricular ridge (DVR), seemed to be comparable to a neocortex; it just didn’t look like it.</p>
<p>In 1969, Karten wrote a “very influential paper that completely changed the discussion in the field,” said <a href="https://www.tosches-lab.com/">Maria Tosches</a>, who studies vertebrate brain development at Columbia University. “His work was really revolutionary.” He concluded that because avian and mammalian circuits are similar, they were <a href="https://doi.org/10.1111/j.1749-6632.1969.tb20442.x">inherited from a common ancestor</a>. That thinking dominated the field for decades, said Güntürkün, a former postdoc in Karten’s lab. It “sparked quite a lot of interest in the bird brain.”</p>

<p>A few decades later, Luis Puelles, an anatomist at the University of Murcia in Spain, drew the opposite conclusion to Karten. By comparing embryos at various stages of development, he found that the mammalian neocortex and the avian DVR developed from distinct areas of the embryo’s pallium — a brain region shared by all vertebrates. He concluded that the structures must have evolved independently</p>
<p>Karten and Puelles were “giving completely different answers to this big question,” Tosches said. The debate continued for decades. During this time, biologists also began to appreciate bird intelligence, starting with their studies of Alex, an African gray parrot who could count and identify objects. They realized just how smart birds could be.</p>
<p>However, neither group seemed to want to resolve the discrepancy between their two theories of how vertebrate palliums evolved, according to García-Moreno. “No, they kept working on their own method,” he said. One camp continued to compare the circuitry in adult vertebrate brains; the other focused on embryonic development.</p>
<p>In the new studies, he said, “we tried to put everything together.”</p>
<h2><strong>Same but Not the Same</strong></h2>
<p>Two new studies, which were conducted by independent teams of researchers, relied on the same powerful tool for identifying cell types, known as single-cell RNA sequencing. This technique lets researchers compare neuronal circuits, as Karten did, not only in adult brains but all the way through embryonic development, following Puelles. In this way, they could see where the cells started growing in the embryo and where they ended up in the mature animal&nbsp;— a developmental journey that can reveal evolutionary pathways.</p>
<p>For their study, García-Moreno and his team wanted to watch how brain circuitry develops. Using RNA sequencing and other techniques, they tracked cells in the palliums of chickens, mice and geckos at various embryonic stages to time-stamp when different types of neurons were generated and where they matured.</p>
<p>They found that the mature circuits <a href="http://dx.doi.org/10.1126/science.adp3411">looked remarkably alike across animals</a>, just as Karten and others had noted, but they were built differently, as Puelles had found. The circuits that composed the mammalian neocortex and the avian DVR developed at different times, in different orders and in different regions of the brain.</p>
</div><div data-role="selectable">
    <p>At the same time, García-Moreno was collaborating with Zaremba and her colleagues at Heidelberg University. Using RNA sequencing, they created “the most comprehensive atlas of the bird pallium that we have,” said Tosches, who wrote <a href="https://dx.doi.org/10.1126/science.adv2609">a related perspective piece</a>&nbsp;published in <em>Science</em>. By comparing the bird pallium to lizard and mouse palliums, they also found that the neocortex and DVR were <a href="http://dx.doi.org/10.1126/science.adp5182">built with similar circuitry</a> — however, the neurons that composed those neural circuits were distinct.</p>
<p>“How we end up with similar circuitry was more flexible than I would have expected,” Zaremba said. “You can build the same circuits from different cell types.”</p>
<p>Zaremba and her team also found that in the bird pallium, neurons that start development in different regions can mature into the same type of neuron in the adult. This pushed against previous views, which held that distinct regions of the embryo must generate different types of neurons.</p>

<p>In mammals, brain development follows an intuitive path: The cells in the embryo’s amygdala region at the start of development end up in the adult amygdala. The cells in the embryo’s cortex region end up in the adult cortex. But in birds, “there is a fantastic reorganization of the forebrain,” Güntürkün said, that is “nothing that we had expected.”</p>
<p>Taken together, the studies provide the clearest evidence yet that birds and mammals independently evolved brain regions for complex cognition. They also echo <a href="https://www.quantamagazine.org/gene-expression-in-neurons-solves-a-brain-evolution-puzzle-20230214/">previous research from Tosches’ lab</a>, which found that the mammalian neocortex evolved independently from the reptile DVR.</p>
<p>Still, it seems likely there was some inheritance from a common ancestor. In a third study that used deep learning, Kempynck and his co-author Nikolai Hecker found that mice, chickens and humans <a href="http://dx.doi.org/10.1126/science.adp3957">share some stretches of DNA</a> that influence the development of the neocortex or DVR, suggesting that similar genetic tools are at work in both types of animals. And as previous studies had suggested, the research groups found that inhibitory neurons, or those that silence and modulate neural signals, were conserved across birds and mammals.</p>
<p>The findings haven’t completely resolved Karten and Puelles’ debate, however. Whose ideas were closer to the truth? Tosches said that Puelles was right, while Güntürkün thought the findings better reflect Karten’s ideas, though would partly please Puelles. García-Moreno split the difference: “Both of them were right; none of them was wrong,” he said.</p>
<h2><strong>How To Build Intelligence</strong></h2>

<p>Intelligence doesn’t come with an instruction manual. It is hard to define, there are no ideal steps toward it, and it doesn’t have an optimal design, Tosches said. Innovations can happen throughout an animal’s biology, whether in new genes and their regulation, or in new neuron types, circuits and brain regions. But similar innovations can evolve multiple times independently — a phenomenon known as convergent evolution — and this is seen across life.</p>
<p>“One of the reasons I kind of like these papers is that they really highlight a lot of differences,” said <a href="https://mcd.ucsc.edu/faculty/colquitt.html">Bradley Colquitt</a>, a molecular neuroscientist at the University of California, Santa Cruz. “It allows you to say: What are the different neural solutions that these organisms have come up with to solve similar problems of living in a complex world and being able to adapt in a rapidly changing terrestrial environment?”</p>
<p>Octopuses and squids, independently of mammals, evolved camera-like eyes. Birds, bats and insects all took to the skies on their own. Ancient people in Egypt and South America independently built pyramids — the most structurally efficient shape that will stand the test of time, García-Moreno said: “If they make a tower, it will fall. If they make a wall, it won’t work.”</p>
<p>Similarly, “there’s limited degrees of freedom into which you can generate an intelligent brain, at least within vertebrates,” Tosches said. Drift outside the realm of vertebrates, however, and you can generate an intelligent brain in much weirder ways — from our perspective, anyway. “It’s a wild west,” she said. Octopuses, for example, “evolved intelligence in a way that’s completely independent.” Their cognitive structures look nothing like ours, except that they’re built from the same broad type of cell: the neuron. Yet octopuses have been caught performing incredible feats such as escaping aquarium tanks, solving puzzles, unscrewing jar lids and carrying shells as shields.</p>
        
        
<p>It would be exciting to figure out how octopuses evolved intelligence using really divergent neural structures, Colquitt said. That way, it might be possible to pinpoint any absolute constraints on evolving intelligence across all animal species, not just vertebrates.</p>
<p>Such findings could eventually reveal shared features of various intelligences, Zaremba said. What are the building blocks of a brain that can think critically, use tools or form abstract ideas? That understanding could help in the search for extraterrestrial intelligence — and help improve our artificial intelligence. For example, the way we currently think about using insights from evolution to improve AI is very anthropocentric. “I would be really curious to see if we can build like artificial intelligence from a bird perspective,” Kempynck said. “How does a bird think? Can we mimic that?”</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UK Effort to Keep Apple Encryption Fight Secret Is Blocked (164 pts)]]></title>
            <link>https://www.msn.com/en-us/money/other/uk-effort-to-keep-apple-encryption-fight-secret-is-blocked/ar-AA1CsokD</link>
            <guid>43619315</guid>
            <pubDate>Tue, 08 Apr 2025 07:46:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.msn.com/en-us/money/other/uk-effort-to-keep-apple-encryption-fight-secret-is-blocked/ar-AA1CsokD">https://www.msn.com/en-us/money/other/uk-effort-to-keep-apple-encryption-fight-secret-is-blocked/ar-AA1CsokD</a>, See on <a href="https://news.ycombinator.com/item?id=43619315">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Building the System/360 Mainframe Nearly Destroyed IBM (126 pts)]]></title>
            <link>https://spectrum.ieee.org/building-the-system360-mainframe-nearly-destroyed-ibm</link>
            <guid>43619229</guid>
            <pubDate>Tue, 08 Apr 2025 07:30:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/building-the-system360-mainframe-nearly-destroyed-ibm">https://spectrum.ieee.org/building-the-system360-mainframe-nearly-destroyed-ibm</a>, See on <a href="https://news.ycombinator.com/item?id=43619229">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
        IBM spent US&nbsp;$5&nbsp;billion to build the System/360, introduced in 1964. These 9-track magnetic tape drives were among the S/360’s 150-product line.
    </p><div data-headline="Building the System/360 Mainframe Nearly Destroyed IBM"><p><strong>A short list of the most transformative products</strong> of the past century and a half would include the lightbulb, Ford’s Model T—and the <a href="https://ethw.org/IBM_System/360">IBM System/360</a>. This <a href="https://spectrum.ieee.org/tag/mainframe">mainframe</a> series forever changed the computer industry and revolutionized how businesses and governments worked, enhancing productivity and making countless new tasks possible.</p><p>In the years leading up to its 7 April 1964 launch, however, the 360 was one of the scariest dramas in American business. It took a nearly fanatical commitment at all levels of <a href="https://www.ibm.com/">IBM</a> to bring forth this remarkable collection of machines and software. While the technological innovations that went into the S/360 were important, how they were created and deployed bordered on disaster. The company experienced what <a href="https://pdfs.semanticscholar.org/c41e/a89f526b1b54036320c7e15067122c4ff794.pdf">science policy expert Keith Pavitt called “tribal warfare”</a>: people clashing and collaborating in a rapidly growing company with unstable, and in some instances unknown, technologies, as uncertainty and ambiguity dogged all the protagonists.</p><p>Ultimately, <a href="https://spectrum.ieee.org/tag/ibm">IBM</a> was big and diverse enough in talent, staffing, financing, and materiel to succeed. In an almost entrepreneurial fashion, it took advantage of emerging technologies, no matter where they were located within the enterprise. In hindsight, it seemed a sloppy and ill-advised endeavor, chaotic in execution and yet brilliantly successful. We live in an age that celebrates innovation, so examining cases of how innovation is done can only illuminate our understanding of the process.</p><p><strong>By the end of the 1950s,</strong> computer users faced a seemingly intractable problem. Had it not been solved, it would have prevented computers from becoming widespread, and any thoughts of living in an Information Age would have been fiction.</p><p><img alt="IBM 1401" data-rm-shortcode-id="86125ce55d2e371a1f33b289111d3f3e" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/ibm-1401.jpg?id=25588279&amp;width=980" height="594" id="98290" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/ibm-1401.jpg?id=25588279&amp;width=980" width="1240"><small placeholder="Add Photo Caption...">The S/360 was designed to replace IBM’s 1401 mainframe, which was popular but couldn’t be expanded or upgraded.</small><small placeholder="Add Photo Credit...">Photo: IBM</small></p><p>Organizations were acquiring computers in great numbers, automating many of the old punch card operations and doing more with data processing. The popularity of the IBM 1401 illustrates the rapid adoption of computing. Over 12,000 of these systems were sold from their introduction in 1959 to 1971, when IBM retired the line.</p><p>With the 1401 so dominating the computer business, any problems with it were serious. One of them was that the 1401 was too small.</p><p>Users found these machines so useful that they kept piling more work on them, reaching the system’s capacity. They then had three options: move to a bigger IBM system, such as an IBM 7000, install a competitor’s system, or acquire more 1401s. None of these options was attractive. To change to a bigger system required rewriting software, since the old software would not work on a different type of machine. The cost of rewriting could easily exceed the financial benefits of moving to a bigger machine. Such a change also called for retraining staff or hiring new staff familiar with the new system. Adding more units of the same system was equally unattractive because each unit required duplicate staff, equipment, and maintenance of hardware and software. Customers wanted systems that were “upgradable” or “compatible,” such that as their needs grew, they could bring in larger machines but still run the same software and peripheral equipment. In the 1950s and early 1960s, it was a wish, and for vendors an aspiration.</p><p>IBM had worse problems than its customers did. The 1401s were proving so popular that engineers in Endicott, N.Y., which had developed the system, resisted attempts by their counterparts in Poughkeepsie to build larger computers, leading to growing rivalry between the two groups. As one engineer recalled, “So intense was it that sometimes it seemed to exceed the rivalry with external competitors.” Systems made by Poughkeepsie would not run programs written for the 1400 series. Customers wanting to move from the smaller 1400s to the larger Poughkeepsie machines put increasing pressure on IBM to provide compatibility. Senior management had to contend with the expenses of sustaining R&amp;D for a half-dozen incompatible product lines and training IBMers to sell and maintain so many systems.</p><p>Consensus grew that IBM needed to get down to one system to simplify production, reduce the cost of R&amp;D, and be more competitive against a growing array of rivals. If customers had to upgrade in the early 1960s, they could just as easily move to a competitor’s machine, since they would have to rewrite their software anyway.</p><p>The power of compatibility was demonstrated in the fall of 1960, when IBM introduced the more powerful 1410 to replace the 1401. Software and peripheral equipment for the 1401 worked with the newer machine. Customers and IBM sales loved that fact. Poughkeepsie’s engineers were close to completing work on a set of four computers known as the 8000s that were compatible with the 7000s.</p><p><img alt="T. Vincent Learson" data-rm-shortcode-id="7d6a6c0225c17110a7c16387640e7b3e" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/t-vincent-learson.jpg?id=25588280&amp;width=980" height="620" id="c72f0" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/t-vincent-learson.jpg?id=25588280&amp;width=980" width="620"><small placeholder="Add Photo Caption...">To get the S/360 off the ground, T. Vincent Learson compelled engineering factions within IBM to cooperate.</small><small placeholder="Add Photo Credit...">Photo: IBM</small></p><p><a href="https://www.nytimes.com/1996/11/06/nyregion/t-vincent-learson-84-ibm-chief-dies.html"><strong>T. Vincent Learson</strong></a>—known as Vin or T.V.—was in charge of future product development as the vice president of manufacturing and development. A gifted problem solver, he knew he had to move quickly to break down the rivalry between Endicott and Poughkeepsie. IBM’s CEO at the time, <a href="https://history.computer.org/pioneers/watson-jr.html">Thomas J. Watson Jr.</a>, later described what happened: “He did it by applying a management technique called ‘abrasive interaction.’ This means forcing people to swap sides: taking the top engineer from the small-computer division and making him boss of the best development team in the large-computer division. A lot of people thought this made about as much sense as electing Khrushchev president.”</p><p>Learson replaced the Poughkeepsie manager in charge of the 8000 project with Bob O. Evans, who had served as the engineering manager for the 1401 and 1410. Evans favored compatibility across all future products. After 90 days in his new role, Evans recommended that work on the 8000s be stopped and that both sites begin working “to develop a total cohesive product line.” He also proposed a bold new base technology for all future systems, called <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.4832&amp;rep=rep1&amp;type=pdf">Solid Logic Technology (SLT)</a>, to make IBM’s machines more competitive.</p><p><a href="https://ethw.org/Frederick_P._Brooks,_Jr">Frederick P. Brooks Jr.</a>, who led the design team for the 8000, fought back. Evans and Brooks were formidable opponents. The two engineers both had years of experience running engineering and product development activities at IBM, and they were articulate and highly respected by their staffs and senior management. Brooks was not as high ranking as Evans, so Learson brought in <a href="https://history.computer.org/pioneers/haddad.html">Jerrier A. Haddad</a>, who had spent the previous two years in charge of the Advanced Engineering Development Division, to study the proposed approaches of Evans and Brooks. Haddad recommended going with Evans’s ideas, and Learson killed the 8000 project in May 1961.</p><p>Bob Evans immediately asked Brooks to develop the plan for a compatible family of computers. Brooks was flabbergasted, but he accepted, and with that the two engineering communities stopped feuding and began collaborating. There were still opponents in the company, but no matter—the trajectory toward a common system had been set.</p><p><img alt="From left: Bob O. Evans, Frederik P. Brooks Jr., and Jerrier A. Haddad." data-rm-shortcode-id="c3653f34b1da04fd4f3bc04c2afe3cf6" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/from-left-bob-o-evans-frederik-p-brooks-jr-and-jerrier-a-haddad.jpg?id=25588281&amp;width=980" height="530" id="0bc0b" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/from-left-bob-o-evans-frederik-p-brooks-jr-and-jerrier-a-haddad.jpg?id=25588281&amp;width=980" width="1240"><small placeholder="Add Photo Caption...">Bob O. Evans (left) recommended killing IBM’s larger 8000 mainframe in favor of a new line of compatible computers. Frederick P. Brooks Jr. (middle), who led the 8000’s design, objected, but Jerrier A. Haddad (right) sided with Evans. Evans then asked Brooks to work on the new line, which became the S/360.</small><small placeholder="Add Photo Credit...">Photos: IBM</small></p><p>Learson also assigned <a href="https://en.wikipedia.org/wiki/John_Haanstra">John W. Haanstra</a>, president of the General Products Division, which produced the 1400s, to chair an internal task force called SPREAD (for Systems Programming, Research, Engineering, and Development), with Evans as vice chair. Brooks later joined the task force. In December 1961, the group presented its technical recommendations.</p><p>Their report called for five compatible computers, labeled <em><a href="https://spectrum.ieee.org/tag/processors">processors</a></em> (defined as the computer, its memory, and channels to connect to peripheral equipment). The software and <a href="https://spectrum.ieee.org/tag/peripherals">peripherals</a> for one processor were to work with all other processors. The plan called for using standard hardware and software interfaces between computers and peripherals, such as between <a href="https://spectrum.ieee.org/tag/disk-drives">disk drives</a> and tape drives connecting to computers, so that the peripherals did not have to be swapped out when a new processor was installed. The recommendations became the basis for the System/360.</p><p>Because so much would be new, the processors would not be compatible with IBM’s existing products. That was an enormously important point. Customers moving to the new IBM machines would have to rewrite existing software just once to get on the path of the new system.</p><p>Then, IBM got a lucky break. As one engineer wrote, “Almost miraculously [Evans’s] vision of the new product line was saved by a last-minute technical accomplishment. In mid-1963, engineers in the Poughkeepsie and Endicott laboratories had begun exploring the possibility of adding special microcode to the control stores of computers to improve their performance when simulating earlier IBM computers.” This function would allow 1401 software to run in the two smaller models of the proposed new system, only faster. Sales got on board, and its executives began pressuring R&amp;D and manufacturing management for early introduction of the new processors.</p><p>Watson recognized what was at stake, as he recalled in his memoirs:</p><blockquote>
  From the beginning we faced two risks, either of which was enough to keep us awake at night. First there was the task of coordinating the hardware and software design work for the new line. We had engineering teams all over America and Europe working simultaneously on six new processors and dozens of new peripherals…but in the end all of this hardware would have to plug together. The software was a bigger hurdle still. In order for System/360 to have a common personality, hundreds of <a href="https://spectrum.ieee.org/tag/programmers">programmers</a> had to write millions of lines of computer code. Nobody had ever tackled that complex a programming job, and the engineers were under great pressure to get it done. 
</blockquote><p>A second set of problems involved manufacturing the electronic components for the new systems. The electronics industry was starting to work on <a href="https://spectrum.ieee.org/tag/integrated-circuits">integrated circuits</a>, and the new computers were going to be filled with these new components. To be independent, IBM had to make its own. It proved to be an expensive proposition.</p><p>Eventually, the corporate management committee, including Watson and the board of directors, sucked in a deep breath and approved the SPREAD recommendations. IBM was off to the races in the wildest ride of its history.</p><p><strong>IBM could not hide what was going on.</strong> New employees flocked to Endicott, Poughkeepsie, and other labs and plants. Customers heard rumors, the computer press was speculating, and executives at <a href="https://spectrum.ieee.org/tag/ge">GE</a>, <a href="https://spectrum.ieee.org/tag/honeywell">Honeywell</a>, Sperry Univac, and elsewhere were trying to anticipate what IBM would do.</p><p>At IBM, nobody seemed satisfied with progress on the new system. Engineering, manufacturing, sales, and corporate staff were in many cases working 100-hour weeks. Engineers moved cots into their offices. When Watson stopped in to see how programming was going, an engineer yelled at him to get out so he could work. The chairman of IBM beat a hasty retreat.</p><p><img alt="Left, sign in Grand Central terminal announcing train to take reporters to Poughkeepsie for the 360 press event. Right, Thoma J. Watson Jr." data-rm-shortcode-id="086f1db8b94567b6506513c03c83cb6c" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/left-sign-in-grand-central-terminal-announcing-train-to-take-reporters-to-poughkeepsie-for-the-360-press-event-right-thoma-j.jpg?id=25588282&amp;width=980" height="913" id="1b6e2" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/left-sign-in-grand-central-terminal-announcing-train-to-take-reporters-to-poughkeepsie-for-the-360-press-event-right-thoma-j.jpg?id=25588282&amp;width=980" width="1240"><small placeholder="Add Photo Caption...">On 7 April 1964, a train from New York City’s Grand Central Terminal shuttled reporters to Poughkeepsie, where IBM chairman Thomas J. Watson Jr. officially unveiled the System/360.</small><small placeholder="Add Photo Credit...">Photos: IBM</small></p><p>It all became public at noon eastern time in the United States on 7 April 1964. Over 100,000 customers, reporters, and technologists met in 165 U.S. cities, while others gathered around the world over the next few days to hear the news. As Watson declared at a press conference in Poughkeepsie, it was “the most important product announcement in the company’s history.”</p><p><img alt="IBM 2311 disk drive" data-rm-shortcode-id="accf8a0dfe1ef9043a12820d0c5c1b31" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/ibm-2311-disk-drive.jpg?id=25588283&amp;width=980" height="929" id="ba126" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/ibm-2311-disk-drive.jpg?id=25588283&amp;width=980" width="620"><small placeholder="Add Photo Caption...">Among the System/360’s 44 peripherals was the 2311 disk storage drive. Each removable disk pack stored 7.25 megabytes.</small><small placeholder="Add Photo Credit...">Photo: Mark Richards/Computer History Museum</small></p><p>On that day, IBM introduced a mind-boggling 150 new products: 6 computers; 44 peripherals, including tape drives, disk drives, <a href="https://spectrum.ieee.org/tag/printers">printers</a>, and control units; and a promise to provide the software necessary to make everything work together. The press packet was an inch thick, and manuals describing all the machines, components, software, and their installation and operation filled more than 50 linear feet.</p><p>The central feature of the System/360 was, of course, its compatibility. A growing data center could install a small 360 computer and later upgrade to a larger one without rewriting software or replacing peripheral equipment. Once familiar with the system, one did not have to learn a great deal more to handle an upgrade. The name 360 was chosen to suggest the idea of 360 degrees, covering everything.</p><p>In the first month following the S/360 announcement, customers worldwide ordered over 100,000 systems. To put that number in perspective, in that same year in the <a href="https://spectrum.ieee.org/tag/united-kingdom">United Kingdom</a>, all of Western Europe, the United States, and Japan, there were slightly more than 20,000 computers of any kind installed. The first deliveries of the smaller machines were promised for the third quarter of 1965, and deliveries of the larger ones in the first quarter of 1966. The delay between announcement and shipping date gave customers time to decide which models to acquire, get them approved and budgeted, plan on where to house them, train staff, complete software remediation, and so forth. With the April announcement, IBM bought itself two years to make good on its promises and knock competitors back on their heels.</p><p><strong>From 7 April</strong> to when the company started delivering machines to customers, IBM entered the most dangerous, intense, and challenging era of its history. The company spent US $5 billion (about $40 billion today) to develop the System/360, which at the time was more than IBM made in a year, and it would eventually hire more than 70,000 new workers. Every IBMer believed that failure meant the death of IBM.</p><p>As Watson later recalled, “Not all of the equipment on display [on 7 April] was real; some units were just mockups made of wood. We explained that to our guests, so there was no deception. But it was a dangerous cutting of corners—not the way I think business ought to be done—and an uncomfortable reminder to me of how far we had to go before we could call the program a success.”</p><p>Watson assigned his brother, Arthur, to manage engineering and manufacturing going forward. Learson would run sales for the new system, “twisting the tails of our salesmen.” Tom Watson Jr. thought Learson had the more difficult task. The risk of customers converting to someone else’s machines rather than to the S/360 greatly concerned Watson.</p><p><img alt="Manufacturing in Poughkeepsie" data-rm-shortcode-id="161cd1b24e220f10c54e50964f57a000" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/manufacturing-in-poughkeepsie.jpg?id=25588284&amp;width=980" height="930" id="1ea91" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/manufacturing-in-poughkeepsie.jpg?id=25588284&amp;width=980" width="1240"><small placeholder="Add Photo Caption...">IBM promised to begin delivering the first S/360 machines in the third quarter of 1965. Production problems emerged almost immediately.</small><small placeholder="Add Photo Credit...">Photo: IBM</small></p><p>As the number of orders for the S/360 kept increasing, manufacturing was asked in 1965 to double production. One production manager said it could not be done and was replaced. Quality declined. Some of the electronic circuits within an SLT, for example, were not complete, so <a href="https://spectrum.ieee.org/tag/electrons">electrons</a> could not go where they were supposed to. By the end of the year, the quality control department had impounded 25 percent of all SLT modules, bringing production to a halt.</p><p><img alt="System 360 SLT held within a few fingers." data-rm-shortcode-id="d05052a761c06073e78ab09ab9426283" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/system-360-slt-held-within-a-few-fingers.jpg?id=25588285&amp;width=980" height="1240" id="35a40" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/system-360-slt-held-within-a-few-fingers.jpg?id=25588285&amp;width=980" width="1240"><small placeholder="Add Photo Caption...">With the S/360, IBM introduced its solid logic technology (SLT), a precursor to integrated circuits. Doubling the production of the S/360 in 1965 led to defects in a quarter of the SLT modules.</small><small placeholder="Add Photo Credit...">Photo: IBM</small></p><p>After the problems were solved, manufacturing proceeded in 1966, resulting in 90 million SLT modules bring produced, compared to just 36 million the previous year. IBM opened a new plant in East Fishkill, just south of Poughkeepsie, which made more semiconductor devices than all other manufacturers worldwide combined. Production also expanded to new facilities in Burlington, Vt., and in Corbeil-Essonnes, France.</p><p>To resolve manufacturing problems with the ferrite-core memories, IBM set up a plant in Boulder, Colo., in 1965. But it took the craftsmanship of workers in Japan to get the production of memories up to the required amounts and quality.</p><p><img alt="IBM System 360 ferrite-core memory." data-rm-shortcode-id="5ef6aa819c7095d3b4fd4732ca8beeba" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/ibm-system-360-ferrite-core-memory.jpg?id=25588286&amp;width=980" height="930" id="b0e43" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/ibm-system-360-ferrite-core-memory.jpg?id=25588286&amp;width=980" width="1240"><small placeholder="Add Photo Caption...">The S/360’s ferrite-core memory also proved extremely tricky to manufacture. This plane contains 1,536 memory cores.</small><small placeholder="Add Photo Credit...">Photo: Mark Richards/Computer History Museum</small></p><p>As manufacturing became a worldwide effort, new problems arose in coordinating activities and fabricating machines. Arthur Watson had some experience managing IBM’s small <a href="https://spectrum.ieee.org/tag/factories">factories</a> outside the United States but none with resolving engineering problems, let alone massive global problems in development and manufacturing. He was out of his league, and his brother challenged him to resolve the problems. Meanwhile, Learson and his sales teams wanted additional improvements to the product line. Relations between Learson and Arthur completely deteriorated. In October 1964, IBM announced significant delays in shipping products.</p><p>Tom removed Arthur from his job and turned over his responsibilities to Learson, who in turn brought in four engineering managers to punch through the problems. Nicknamed the “four horsemen,” they had full authority worldwide for getting the S/360 manufactured and delivered to customers. Their collection of problems, one of the managers noted later, was “an absolute nightmare,” “a gray blur of 24-hour days, seven days a week—never being home.” And yet, in five months, they had worked out enough of the problems to start meeting delivery dates. In January 1966, Learson became president of IBM.</p><p><img alt="The \u201cfour horseman\u201d were Henry E. Cooley, Clarence E. Frizzell, John W. Gibson, and John Haanstra." data-rm-shortcode-id="21af284dac8eb993e691ee12f3176f8e" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/the-u201cfour-horseman-u201d-were-henry-e-cooley-clarence-e-frizzell-john-w-gibson-and-john-haanstra.jpg?id=25588287&amp;width=980" height="416" id="2ee96" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/the-u201cfour-horseman-u201d-were-henry-e-cooley-clarence-e-frizzell-john-w-gibson-and-john-haanstra.jpg?id=25588287&amp;width=980" width="1240"><small placeholder="Add Photo Caption...">Facing unacceptable production delays, Learson brought in (left to right) Henry E. Cooley, Clarence E. Frizzell, John W. Gibson, and John Haanstra, who sorted out manufacturing problems worldwide and got the S/360 back on track.</small><small placeholder="Add Photo Credit...">Photos: IBM</small></p><p>Arthur was shunted into the role of vice chairman. His career was broken, and he retired in 1970. In his memoirs, Tom Watson Jr. admitted to being in a nearly continuous panic from 1964 to 1966 and deeply regretted his treatment of Arthur. “I felt nothing but shame and frustrations at the way I’d treated him…. As it was, we remade the computer industry with the System/360, and objectively it was the greatest triumph of my business career. But whenever I look back on it, I think about my brother I injured.”</p><p><strong>Software problems also slowed production</strong> of the 360. The <a href="https://spectrum.ieee.org/tag/software-development">software development</a> staff was described as being in “disarray” as early as 1963. The <a href="https://spectrum.ieee.org/tag/operating-system">operating system</a>, called OS/360, struggled to run more than one job at a time, which was essential to making the S/360 fast and productive. Other problems surfaced with telecommunications and with application programs. Programming support became another contentious issue.</p><p>Fred Brooks volunteered to help, and IBM added 1,000 people to the operating system project, costing the company more for software in one year than had been planned for the entire development of the S/360 system. But throwing more programmers at the project did not help. Based on the S/360 experience, Brooks would later expand on that theme in <em>The Mythical Man-Month</em> (Addison-Wesley, 1975), still one of the most widely read books on computing. The software would take years to complete, but in the end it worked well enough to keep the shipping delay to one month.</p><p><img alt="Delivery of the IBM System/360 in to Tokai Bank Japan." data-rm-shortcode-id="649276b9ee9b3b38b3e180081613f1b9" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/delivery-of-the-ibm-system-360-in-to-tokai-bank-japan.jpg?id=25588288&amp;width=980" height="665" id="230f2" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/delivery-of-the-ibm-system-360-in-to-tokai-bank-japan.jpg?id=25588288&amp;width=980" width="1240"><small placeholder="Add Photo Caption...">A System/360 arrives at Tokai Bank in Japan. Demand for computing grew enormously in the years following the S/360’s launch.</small><small placeholder="Add Photo Credit...">Photo: IBM</small></p><p>Despite the costs and anxiety, in 1965—the year IBM had committed to shipping the first units to customers—it managed “by some miracle” (Watson’s words) to deliver hundreds of medium-size S/360s. Their quality did not always match the original design specifications. Shortages of parts, other parts that did not work, and software filled with bugs spread to many countries. Almost every initial customer encountered problems.</p><p>Branch offices were hiring systems engineers to help. SEs were college graduates, usually with technical degrees, who knew how to debug software and assisted the sales force in selling and supporting computers. The SEs heroically tackled S/360’s software problems, while field engineers, who installed equipment, fixed hardware problems. Salesmen calmed their customers, while branch managers worked to keep their staffs motivated and focused.</p><p>And despite the many problems, “customers were still ordering 360s faster than we could build them,” Watson recalled, forcing delivery dates out as much as three years. By the end of 1966, customers had taken delivery of nine models of the S/360, for a total of 7,700.</p><p><strong>IBM’s competitors responded.</strong> Burroughs, GE, Honeywell, NCR, and Sperry Rand, operating largely in the United States, CII in France, and ICT (later ICI) in Great Britain introduced systems compatible with one another’s machines, but not compatible with IBM’s. A second, smaller group chose to manufacture machines that were compatible with IBM’s, including RCA and others in Europe and Japan, relying on RCA’s licenses.</p><p><img alt="1968. Flanked by a display console with an array of lights and switches, an operator uses a monitor to check the performance of the Model 91. This System/360 installed at NASA's Goddard Space Flight Center in Greenbelt, Md." data-rm-shortcode-id="3c1c2b8d1847a353491060e465ea0e99" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/1968-flanked-by-a-display-console-with-an-array-of-lights-and-switches-an-operator-uses-a-monitor-to-check-the-performance-of.jpg?id=25588289&amp;width=980" height="798" id="d6c27" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/1968-flanked-by-a-display-console-with-an-array-of-lights-and-switches-an-operator-uses-a-monitor-to-check-the-performance-of.jpg?id=25588289&amp;width=980" width="1240"><small placeholder="Add Photo Caption...">NASA purchased a number of S/360s, including this one at <a href="https://spectrum.ieee.org/tag/goddard">Goddard</a>&nbsp;<a href="https://spectrum.ieee.org/tag/space-flight">Space Flight</a> Center. Several others at <a href="https://spectrum.ieee.org/tag/mission-control">mission control</a> in Houston were used to monitor <a href="https://spectrum.ieee.org/tag/apollo">Apollo</a> 11.</small><small placeholder="Add Photo Credit...">Photo: IBM</small></p><p>Five years later, the worldwide inventory of installed IBM computers had grown to $24 billion, while that of competitors had reached $9 billion. In other words, IBM’s S/360 increased overall demand for computing so massively that it raised all boats. The industry’s annual growth in the second half of the 1960s was in double digits year over year, as many thousands of organizations expanded their use of computers. Demand for computing grew because of the technological innovations brought forth by IBM, but also because users were accumulating enough experience to understand a computer’s value in driving down costs and performing new functions.</p><p>IBM also grew, more than doubling from 127,000 employees worldwide in 1962 to 265,000 by the end of 1971. Revenue rose from $3.2 billion in 1964 to $8.2 billion in 1971.</p><p><img alt="Fisheye view of the IBM System 360" data-rm-shortcode-id="9575ea4bbcd9562577dd7834054669c2" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/fisheye-view-of-the-ibm-system-360.jpg?id=25588290&amp;width=980" height="1240" id="5b214" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/fisheye-view-of-the-ibm-system-360.jpg?id=25588290&amp;width=980" width="1240"><small placeholder="Add Photo Caption...">The S/360 reinforced IBM’s domination of the global computer business. One executive, asked whether the company would ever take on another such project, replied, “Hell no, never again.”</small><small placeholder="Add Photo Credit...">Photo: IBM</small></p><p>Because the S/360 was the heart of much computing by the end of the 1960s, its users constituted a world of their own. Thousands of programmers only knew how to use software that ran on S/360s. Additional thousands of data-processing personnel had worked only with IBM equipment, including keypunch machines, printers, tape drives, disk drives, and software, which in many instances took years to master. By the early 1970s the computing space was largely an IBM world on both sides of the Atlantic, in the emerging markets in Latin America, and in Japan.</p><p>Years later, when asked whether IBM would ever engage in such a massive project again, one executive barked out, “Hell no, never again.” Watson tilted toward a similar reaction. Commenting in 1966, he said, “At our size, we can’t go 100 percent with anything new again,” meaning anything that big. After the 360, Watson made it a policy “never to announce a new technology which will require us to devote more than 25 percent of our production to that technology.”</p><p>The generation that brought out the S/360 remained unique in the company, a special clan bonded ferociously to IBM. Careers had been made and broken, personal lives and marriages upended. IBMers did not know at the time how extensively their products would change the world, but we do.</p><p><img alt="Left, IBM: The Rise and Fall and Reinvention of a Global Icon. Right: Author James W. Cortada." data-rm-shortcode-id="2be47b1560c38ee120c90e582a19933f" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/left-ibm-the-rise-and-fall-and-reinvention-of-a-global-icon-right-author-james-w-cortada.jpg?id=25588291&amp;width=980" height="620" id="b081a" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/left-ibm-the-rise-and-fall-and-reinvention-of-a-global-icon-right-author-james-w-cortada.jpg?id=25588291&amp;width=980" width="1240"></p><p><em>This article is based on excerpts from </em><a href="https://mitpress.mit.edu/books/ibm">IBM: The Rise and Fall and Reinvention of a Global Icon</a><em> (MIT Press, 2019).</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[India's repair culture gives new life to dead laptops (287 pts)]]></title>
            <link>https://www.theverge.com/tech/639126/india-frankenstein-laptops</link>
            <guid>43618105</guid>
            <pubDate>Tue, 08 Apr 2025 03:27:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/tech/639126/india-frankenstein-laptops">https://www.theverge.com/tech/639126/india-frankenstein-laptops</a>, See on <a href="https://news.ycombinator.com/item?id=43618105">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="zephr-anchor"><p>In a dimly lit, cluttered workshop in Delhi’s Nehru Place, the air hums with the sound of whirring drills and the crackle of soldering irons. Sushil Prasad, a 35-year-old technician, wipes the sweat off his brow as he carefully pieces together the guts of an old laptop. It is a daily ritual — resurrecting machines by stitching together motherboards, screens, and batteries scavenged from other trashed older laptops and e-waste — to create functional, low-cost devices.</p><div><p>“India has always had a repair culture … but companies are pushing planned obsolescence”</p></div><p>“Right now, there is a huge demand for such ‘hybrid’ laptops,” Prasad says, his hands swapping out a damaged motherboard. “Most people don’t care about having the latest model; they just want something that works and won’t break the bank.”</p><div><div><p><a href="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0,0,100,100" data-pswp-height="1800" data-pswp-width="2700" target="_blank" rel="noreferrer"><img alt="One of the streets where laptops are repaired at Nehru Place." data-chromatic="ignore" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 639px) 100vw, (max-width: 1023px) 50vw, 700px" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=256 256w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=376 376w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=384 384w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=415 415w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=480 480w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=540 540w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=640 640w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=750 750w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=828 828w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1080 1080w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1200 1200w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1440 1440w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1920 1920w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2048 2048w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400 2400w" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9989.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400"></a></p></div><p><figcaption><em>One of the streets where laptops are repaired at Nehru Place. </em></figcaption></p></div><div><p>Across India, in metro markets from Delhi’s Nehru Place to Mumbai’s Lamington Road, technicians like Prasad are repurposing broken and outdated laptops that many see as junk. These “Frankenstein” machines — hybrids of salvaged parts from multiple brands — are sold to students, gig workers, and small businesses, offering a lifeline to those priced out of India’s growing digital economy.</p><p>“We take usable components from different older or discarded systems to create a new functioning unit. For instance, we salvage parts from old laptop motherboards, such as capacitors, mouse pads, transistors, diodes, and certain ICs and use them in the newly refurbished ones,” says Prasad.</p></div><div><div><p><a href="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0,0.055555555555557,100,99.888888888889" data-pswp-height="1798" data-pswp-width="2697" target="_blank" rel="noreferrer"><img alt="Prasad holding a laptop motherboard at his shop." data-chromatic="ignore" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 639px) 100vw, (max-width: 1023px) 50vw, 700px" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=256 256w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=376 376w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=384 384w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=415 415w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=480 480w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=540 540w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=640 640w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=750 750w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=828 828w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=1080 1080w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=1200 1200w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=1440 1440w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=1920 1920w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=2048 2048w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=2400 2400w" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9359.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=2400"></a></p></div><p><figcaption><em>Prasad holding a laptop motherboard at his shop. </em></figcaption></p></div><p>As he explains, Manohar Singh, the owner of the workshop-slash-store where Prasad works, flips open a refurbished laptop while sitting on a rickety stool. The screen flickers to life, displaying a crisp image. He smiles — a sign that another machine has been successfully revived.</p><p>“We literally make them out of scrap! We also take in second-hand laptops and e-waste from countries like Dubai and China, fix them up, and sell them at half the price of a new one,” he explains.</p><p>“A college student or a freelancer can get a good machine for INR 10,000 [about $110 USD] instead of spending INR 70,000 [about $800 USD] on a brand-new one. For many, that difference means being able to work or study at all.”</p><div><div><p><a href="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0,0,100,100" data-pswp-height="1800" data-pswp-width="2700" target="_blank" rel="noreferrer"><img alt="A wide view of a bustling street in Nehru Place, home to hundreds of shops and workshops dealing in all types of electronic products, attracting thousands of customers daily." data-chromatic="ignore" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 639px) 100vw, (max-width: 1023px) 50vw, 700px" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=256 256w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=376 376w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=384 384w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=415 415w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=480 480w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=540 540w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=640 640w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=750 750w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=828 828w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1080 1080w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1200 1200w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1440 1440w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1920 1920w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2048 2048w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400 2400w" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9733.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400"></a></p></div><p><figcaption><em>A wide view of a bustling street in Nehru Place, home to hundreds of shops and workshops dealing in all types of electronic products, attracting thousands of customers daily.</em></figcaption></p></div><p>Singh recalls a young engineering student who visited his shop last year, desperate for a laptop to complete his coursework. “He had saved up for months but was still short of money. I put together a machine for him from spare parts, and he left with tears in his eyes. That is when you know this work matters.”</p><p>But this booming market does not exist in isolation. It is entangled with a much larger battle, one between small repair technicians and global technology giants. While these Frankenstein laptops are a lifeline for many, the repair industry itself faces significant roadblocks. Many global manufacturers deliberately make repairs difficult by restricting access to spare parts, using proprietary screws, and implementing software locks that force customers to buy new devices instead of fixing old ones.</p><div><div><p><a href="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0,0.055555555555557,100,99.888888888889" data-pswp-height="1798" data-pswp-width="2697" target="_blank" rel="noreferrer"><img alt="Two boys transport discarded computer CPUs on a bicycle in Seelampur, one of India’s largest e-waste hubs, where informal recycling is a major source of livelihood." data-chromatic="ignore" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 639px) 100vw, (max-width: 1023px) 50vw, 700px" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=256 256w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=376 376w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=384 384w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=415 415w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=480 480w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=540 540w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=640 640w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=750 750w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=828 828w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=1080 1080w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=1200 1200w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=1440 1440w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=1920 1920w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=2048 2048w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=2400 2400w" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9183.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=2400"></a></p></div><p><figcaption><em>Two boys transport discarded computer CPUs on a bicycle in Seelampur, one of India’s largest e-waste hubs, where informal recycling is a major source of livelihood.</em></figcaption></p></div><p>Satish Sinha, associate director at Toxics Link, a nonprofit working on waste management, believes repair technicians like Prasad and Singh are on the front lines of a larger battle.</p><p>“India has always had a repair culture, from fixing old radios to hand-me-down phones. But companies are pushing planned obsolescence, making repairs harder and forcing people to buy new devices instead,” Sinha says. </p><div><div><p><a href="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0,0.055555555555557,100,99.888888888889" data-pswp-height="1798" data-pswp-width="2697" target="_blank" rel="noreferrer"><img alt="Prasad, a technician, works on discarded boards at his shop in Nehru Place." data-chromatic="ignore" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 639px) 100vw, (max-width: 1023px) 50vw, 700px" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=256 256w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=376 376w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=384 384w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=415 415w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=480 480w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=540 540w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=640 640w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=750 750w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=828 828w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=1080 1080w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=1200 1200w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=1440 1440w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=1920 1920w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=2048 2048w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=2400 2400w" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9211.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=2400"></a></p></div><p><figcaption><em>Prasad, a technician, works on discarded boards at his shop in Nehru Place.</em></figcaption></p></div><p>“We need to encourage such reuse of materials. These repaired or new hybrid devices minimize waste by extending a product’s lifespan and reducing overall market waste. Reusing components cuts down on the need for new materials, lowering energy use, resource extraction, and environmental impact,” Sinha adds.</p><div><p>“These parts would end up in a landfill otherwise.”</p></div><p>The Indian government has <a href="https://pib.gov.in/PressReleasePage.aspx?PRID=1841403&amp;utm_">started discussions</a> on right-to-repair laws, inspired by similar efforts in the European Union and the United States. However, progress remains slow, and repair shops continue to operate in legal limbo, often forced to source different parts from informal and e-waste markets.</p><p>As a result, many repair technicians have no choice but to rely on informal supply chains, with markets like Delhi’s Seelampur — India’s largest e-waste hub — becoming a critical way to source spare parts. Seelampur processes approximately <a href="https://www.defindia.org/wp-content/uploads/2021/10/Seelampur-Indias-Digital-Trashcan.pdf">30,000 tonnes</a> (33,069 tons) of e-waste daily, providing employment to <a href="https://www.thenationalnews.com/news/asia/2025/03/09/india-e-waste-industry-hazards-delhi-seelampur/">nearly 50,000</a> informal workers who extract valuable materials from it. The market is a chaotic maze of discarded electronics, where workers sift through mountains of broken circuit boards, tangled wires, and cracked screens, searching for usable parts. </p><p>Farooq Ahmed, an 18-year-old scrap dealer, has spent the last four years sourcing laptop components for technicians like Prasad. “We find working RAM sticks, motherboards with minor faults, batteries that still hold charge and sell it to different electronic workshops,” he says. “These parts would end up in a landfill otherwise.”</p><div><div><p><a href="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0,0.055555555555557,100,99.888888888889" data-pswp-height="1798" data-pswp-width="2697" target="_blank" rel="noreferrer"><img alt="A close-up view of discarded phone displays and circuit boards in a sack in Seelampur." data-chromatic="ignore" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 639px) 100vw, (max-width: 1023px) 50vw, 700px" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=256 256w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=376 376w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=384 384w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=415 415w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=480 480w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=540 540w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=640 640w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=750 750w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=828 828w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=1080 1080w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=1200 1200w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=1440 1440w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=1920 1920w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=2048 2048w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=2400 2400w" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9205.jpg?quality=90&amp;strip=all&amp;crop=0%2C0.055555555555557%2C100%2C99.888888888889&amp;w=2400"></a></p></div><p><figcaption><em>A close-up view of discarded phone displays and circuit boards in a sack in Seelampur.</em></figcaption></p></div><p>But while e-waste salvaging provides cheap repair materials, it comes at a steep price. Without proper safety measures, workers handle toxic materials such as lead, mercury, and cadmium daily. “I cough a lot,” Ahmed admits with a sheepish grin. “But what can I do? This work feeds my family.”</p><p>Despite the dangers, the demand for Frankenstein systems continues to grow. And as India’s digital economy expands, the need for such affordable technology will only increase. Many believe that integrating the repair sector into the formal economy could bring about a win-win situation, reducing e-waste, creating jobs, and making technology more accessible.</p><div><div><p><a href="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0,0,100,100" data-pswp-height="1800" data-pswp-width="2700" target="_blank" rel="noreferrer"><img alt="Agents wait in an alley of a three-story building in Nehru Place, guiding customers to electronics shops as part of the bustling market’s competitive trade." data-chromatic="ignore" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 639px) 100vw, (max-width: 1023px) 50vw, 700px" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=256 256w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=376 376w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=384 384w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=415 415w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=480 480w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=540 540w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=640 640w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=750 750w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=828 828w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1080 1080w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1200 1200w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1440 1440w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1920 1920w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2048 2048w, https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400 2400w" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/04/257643_Frankenstein_laptops_Delhi_DPandit_9940.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400"></a></p></div><p><figcaption><em>Agents wait in an alley of a three-story building in Nehru Place, guiding customers to electronics shops as part of the bustling market’s competitive trade.</em></figcaption></p></div><p>“If the government recognizes independent repair businesses, gives them access to spare parts, and sets quality standards, we can transform this industry,” Sinha says.</p><p>But for now, in dimly lit workshops across the country, men like Prasad and Singh continue their work, reviving the dead, bridging the digital divide, and proving that, in India, the repair ecosystem is set to thrive.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Any program can be a GitHub Actions shell (262 pts)]]></title>
            <link>https://yossarian.net/til/post/any-program-can-be-a-github-actions-shell/</link>
            <guid>43617493</guid>
            <pubDate>Tue, 08 Apr 2025 01:20:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://yossarian.net/til/post/any-program-can-be-a-github-actions-shell/">https://yossarian.net/til/post/any-program-can-be-a-github-actions-shell/</a>, See on <a href="https://news.ycombinator.com/item?id=43617493">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>In GitHub Actions, you can use the <code>shell</code> keyword to specify the shell
that runs a given <code>run:</code> block. This keyword is optional for workflows
but mandatory for action definitions.</p>
<p>The shell normally defaults to something sensible for your runner,
e.g. <code>bash</code> on Linux and macOS, and <code>pwsh</code> on Windows. But it can also be
specified, and <a href="https://docs.github.com/en/actions/writing-workflows/workflow-syntax-for-github-actions#defaultsrunshell">GitHub documents</a> that specifying it explicitly also
implies some flags of their choosing:</p>
<pre><code><span><span><span>#</span> setting bash explicitly implies bash --noprofile --norc -eo pipefail
</span><span>-</span> <span><span>shell</span></span><span>:</span> <span>bash</span>
  <span><span>run</span></span><span>:</span> <span>|</span>
<span>    echo "Hello, world!"
</span></span></code></pre>
<p>Based on that, you might think that there's a fixed number of valid
<code>shell</code> values<sup><a href="#fn-toolcache" id="fnref-toolcache" data-footnote-ref="">1</a></sup>, which GitHub tracks and adds their special flags to.
But you'd be wrong!</p>
<p>As it turns out, you can set <code>shell</code> to any executable on the <code>$PATH</code>,
and GitHub will happily use it to execute the <code>run</code> block. If the command
doesn't already take a single file as input, you need to pass it
the special <code>{0}</code> argument, which GitHub replaces with the temporary file
that it generates the template-expanded <code>run</code> block into.</p>
<p>Thanks to this, we can do all kinds of cursed things.</p>
<p>Using C as our step runner <a href="https://github.com/woodruffw-experiments/actions-experiments/pull/3">works fine</a>:</p>
<pre><code><span><span>-</span> <span><span>run</span></span><span>:</span> <span>sudo apt install -y tcc</span>
<span>-</span> <span><span>shell</span></span><span>:</span> <span>tcc -run {0}</span>
  <span><span>run</span></span><span>:</span> <span>|</span>
<span>    #include &lt;stdio.h&gt;
    int main() {
      printf("Hello, world!\n");
      return 0;
    }
</span></span></code></pre>
<p>...and so does <a href="https://github.com/woodruffw-experiments/actions-experiments/pull/5">dynamically modifying</a> the <code>$PATH</code> via <code>$GITHUB_PATH</code>:</p>
<pre><code><span><span>-</span> <span><span>run</span></span><span>:</span> <span>|</span>
<span>    touch ./bash
    chmod +x ./bash
    echo '#!/bin/sh' &gt; ./bash
    echo 'echo hello from fake bash' &gt;&gt; ./bash
    echo "${PWD}" &gt;&gt; "${GITHUB_PATH}"
</span><span>-</span> <span><span>run</span></span><span>:</span> <span>|</span>
<span>    echo "this doesn't do what you expect"
</span>  <span><span>shell</span></span><span>:</span> <span>bash</span>
</span></code></pre>
<p>Does this matter from a security perspective? It's hard to say — there
are plenty of other ways in which file writes imply execution in GitHub Actions,
including <code>GITHUB_ENV</code> itself.</p>
<p>On the other hand, it's surprising (IMO) that GitHub does <code>$PATH</code> lookups even
for their "well-known" shell values; I would expect those to be fixed to
specific values (e.g. <code>bash</code> to <code>/bin/bash</code>), especially since they inject
flags into the <code>run</code>'s command line based on those well-known values.</p>
<section data-footnotes="">
<ol>
<li id="fn-toolcache">
<p>Or, more generally, that GitHub will accept any pre-registered tool in its toolcache. That, for example, is how I thought <code>shell: python</code> worked, before I noticed this. <a href="#fnref-toolcache" data-footnote-backref="" data-footnote-backref-idx="1" aria-label="Back to reference 1">↩</a></p>
</li>
</ol>
</section>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[North Korean IT workers have infiltrated the Fortune 500 (141 pts)]]></title>
            <link>https://www.yahoo.com/news/thousands-north-korean-workers-infiltrated-110000417.html</link>
            <guid>43617352</guid>
            <pubDate>Tue, 08 Apr 2025 00:47:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.yahoo.com/news/thousands-north-korean-workers-infiltrated-110000417.html">https://www.yahoo.com/news/thousands-north-korean-workers-infiltrated-110000417.html</a>, See on <a href="https://news.ycombinator.com/item?id=43617352">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-article-body="true"><ul><li><p><strong>Fortune 500 companies have unwittingly hired thousands of software engineers</strong> who claim to be American developers but are actually North Korean citizens using stolen or fake identities. Through legitimate employment, the IT workers are <a href="https://ofac.treasury.gov/media/923126/download?inline" rel="nofollow noopener" target="_blank" data-ylk="slk:illegally funneling their salaries;elm:context_link;itc:0;sec:content-canvas">illegally funneling their salaries</a> to Kim Jong Un’s regime to fund prohibited weapons of mass destruction and ballistic missile programs. The U.S. Treasury, State Department, and FBI collectively estimate the IT workers scam has generated hundreds of millions each year since 2018.</p></li></ul><p>About 95% of the résumés Harrison Leggio gets in response to job postings for his crypto startup g8keep are from North Korean engineers pretending to be American, the founder estimates. He even once interviewed a job seeker who claimed to have worked at the same Manhattan-based cryptocurrency exchange as he did, during the time he worked there.</p><p>Turns out it was all a ruse: The programming languages the engineer said the company used were incorrect, and he claimed to have floated among teams rather than embedding in a single group, which “wasn’t a thing there,” said Leggio.</p><p>Nowadays, Leggio told <em>Fortune</em> he won’t even set up an interview with a candidate who seems promising on paper unless they agree to one final step.</p><p>“Say something negative about Kim Jong Un,” Leggio tells potential job candidates, referring to the third-generation authoritarian Supreme Leader of North Korea, officially the Democratic People’s Republic of Korea (DPRK). Through research, Leggio learned insulting the DPRK’s Supreme Leader is forbidden, and North Korean citizens could face serious punishment for showing anything less than reverence.</p><p>“The first time I ever did it, the person started freaking out and cursing,” said Leggio.</p><p>The job seeker subsequently blocked Leggio across all social media platforms. Now Leggio makes the same request before every single interview. Other startups and founders he knows are asking the same thing of job seekers, he said.</p><h2>A yacht versus a missile</h2><p>The UN estimated the North Korean IT worker scam has generated <a href="https://fortune.com/2024/08/10/nashville-man-north-korean-remote-work-jobs-weapons-program-laptop-farm/?utm_source=search&amp;utm_medium=advanced_search&amp;utm_campaign=search_link_clicks" rel="nofollow noopener" target="_blank" data-ylk="slk:$250 million to $600 million;elm:context_link;itc:0;sec:content-canvas">$250 million to $600 million</a>&nbsp;every year since 2018. As a result, cybersecurity experts of all stripes have banded together to <a href="https://www.recordedfuture.com/research/inside-the-scam-north-koreas-it-worker-threat" rel="nofollow noopener" target="_blank" data-ylk="slk:share information;elm:context_link;itc:0;sec:content-canvas">share information</a> about the strategies, profiles, VPNs, and signs to watch for. But AI has emboldened the North Korean scheme, allowing the IT workers to develop scripts so they can hold down as many as six or seven jobs at a time, disguise their appearance, and even alter their voices so they don’t have an accent—or so they sound like a woman instead of a man. Experts <a href="https://cloud.google.com/blog/topics/threat-intelligence/dprk-it-workers-expanding-scope-scale" rel="nofollow noopener" target="_blank" data-ylk="slk:predict;elm:context_link;itc:0;sec:content-canvas">predict</a> the scope and scale will expand in 2025, moving across Europe and Asia with well-honed social engineering tactics paired with more aggressive job hunting at European defense and government companies.</p><p>Michael Barnhart, an intelligence leader at <a href="https://fortune.com/company/alphabet/" rel="nofollow noopener" target="_blank" data-ylk="slk:Google;elm:context_link;itc:0;sec:content-canvas">Google</a> Cloud who has been <a href="https://cloud.google.com/blog/topics/threat-intelligence/mitigating-dprk-it-worker-threat" rel="nofollow noopener" target="_blank" data-ylk="slk:tracking;elm:context_link;itc:0;sec:content-canvas">tracking</a> North Korean threats for years, explained the scheme this way: North Korean engineers, deployed to locations in China and Russia, use AI to create bios with eye-catching company experience highlighted. They work in teams to apply for jobs en masse, using stolen American identities, or with the help of <a href="https://www.justice.gov/archives/opa/pr/justice-department-disrupts-north-korean-remote-it-worker-fraud-schemes-through-charges-and" rel="nofollow noopener" target="_blank" data-ylk="slk:facilitators;elm:context_link;itc:0;sec:content-canvas">facilitators</a> in the U.S. or abroad. Some IT workers have even created <a href="https://www.justice.gov/archives/opa/media/1320161/dl?inline" rel="nofollow noopener" target="_blank" data-ylk="slk:front companies;elm:context_link;itc:0;sec:content-canvas">front companies</a> to pose as legitimate recruiting firms or web-design agencies, for instance, that larger Fortune 500 companies then hire—not realizing it’s a North Korean front, said Barnhart.</p><p>“Right now, we have North Korean IT workers adapting so much that they’re not even doing IT work anymore,” he told <em>Fortune</em>.</p><div id="recirc-8d154401-37c0-38b5-b071-eb5cc8cc8ee4"><h2>More in World</h2></div><p>Among global companies, security teams have implemented different systems and strategies for rooting out North Korean IT workers seeking jobs as well as those already employed and working at companies, Barnhart said. And the stakes couldn’t be higher. The FBI <a href="https://www.ic3.gov/PSA/2025/PSA250123" rel="nofollow noopener" target="_blank" data-ylk="slk:reported;elm:context_link;itc:0;sec:content-canvas">reported</a> the money funds nuclear weapons and operations, and the intelligence and data the IT workers illegally pilfer from companies is directed toward extortion, espionage, and data theft.</p><p>“There are criminals who steal your money to get yachts, but in this case, your money isn’t going to a Lamborghini—it’s going back to fund nuclear munitions,” said Barnhart. “A yacht versus a missile—attribution matters.”</p><h2>300-plus incidents in 2024</h2><p>Bojan Simic, CEO of identity-verification firm Hypr, built a product specifically for companies to verify people's identities because of the North Korean threat, he told <em>Fortune</em>. As a tech founder, he also deals with the issue within his own company. Simic accidentally hired an engineer who did a great job during the interview, but then the person who showed up to be onboarded on their first day wasn’t the person he hired. The engineer also failed a geolocation check, Simic said, and appeared to be in Spain when he claimed to live in Poland.</p><p>Emi Chiba, a senior principal analyst at <a href="https://fortune.com/company/gartner/" rel="nofollow noopener" target="_blank" data-ylk="slk:Gartner;elm:context_link;itc:0;sec:content-canvas">Gartner</a> who has been researching the issue, told <em>Fortune</em> security experts should partner with internal human-resources teams to periodically re-verify the identities of employees and strengthen <a href="https://www.farnsworthintelligence.com/north-korean-it-farms-ioc-document-and-recommendations" rel="nofollow noopener" target="_blank" data-ylk="slk:recruiting practices;elm:context_link;itc:0;sec:content-canvas">recruiting practices</a>. The goal is to ensure job candidates aren’t hiding their locations overseas and pretending to be based in the U.S. Those practices range from camera-on video interviews to using identity verification tools with geolocation features to compare a government ID with a selfie, which would help match people to their identities and locations, she said.</p><p>“One of the biggest things you can do to combat this is training up HR staff,” added Barnhart.</p><p>Despite the efforts to disrupt it, cybersecurity company CrowdStrike <a href="https://www.crowdstrike.com/en-us/global-threat-report/" rel="nofollow noopener" target="_blank" data-ylk="slk:reported;elm:context_link;itc:0;sec:content-canvas">reported</a> North Korean IT workers, a group it calls Famous Chollima, were behind 304 incidents in 2024, and its activities ramped up during the latter half of the year. In its latest assessment, CrowdStrike predicted Famous Chollima will continue its campaigns in 2025 given the financial success it’s seen and limited impact from federal prosecutions and government indictments last year.</p><p>Adam Meyers, senior vice president of CrowdStrike’s counter adversary team, told <em>Fortune </em>Famous Chollima has two main tentacles. One is a <a href="https://fortune.com/crypto/2023/12/14/north-korea-lazarus-crypto-hack-immunefi-2023-cybercrime/" rel="nofollow noopener" target="_blank" data-ylk="slk:malware operation;elm:context_link;itc:0;sec:content-canvas">malware operation</a> that focuses on intelligence collection and <a href="https://fortune.com/2025/02/26/north-korea-hackers-crypto-heist-bybit-dubai-ether/?utm_source=search&amp;utm_medium=suggested_search&amp;utm_campaign=search_link_clicks" rel="nofollow noopener" target="_blank" data-ylk="slk:crypto theft;elm:context_link;itc:0;sec:content-canvas">crypto theft</a>, like the $1.5 billion cryptocurrency heist from a Dubai exchange. The other is the IT workers scam in which North Korean engineers get legitimate jobs and remit their salaries to North Korea to fund <a href="https://www.hrnk.org/documentations/slaves-bomb-role-north-koreas-nuclear-scientists/" rel="nofollow noopener" target="_blank" data-ylk="slk:nuclear weapons;elm:context_link;itc:0;sec:content-canvas">nuclear weapons</a>, operations, and trade. The two prongs also work together to share intelligence.</p><p>In the IT worker scheme, once someone involved gets an interview, North Koreans use remote-desktop tools to help coach people through the Q&amp;A with a recruiter.</p><p>Aidan Raney, founder of Farnsworth Intelligence, posed as an American willing to help North Koreans to investigate the issue for a client who almost hired a fake engineer. During the course of two video calls with three or four people who all said their names were “Ben,” Raney learned the details. “The Bens” would handle all the upfront work for him—creating a fake <a href="https://fortune.com/company/linkedin/" rel="nofollow noopener" target="_blank" data-ylk="slk:LinkedIn;elm:context_link;itc:0;sec:content-canvas">LinkedIn</a> profile to verify his new identity for U.S. recruiters, formulating a bio, and sending it out to dozens of job postings with a new Gmail address they set up.</p><p>The Bens even modified Raney’s headshot to a black-and-white photo so it wouldn’t resemble his usual picture, Raney told <em>Fortune</em>. If Raney got a job, he would show up for meetings, like a morning stand-up or scrum, and go about his day while a North Korean engineer handled the workload. Raney would be allowed to keep 30% of the salary but had to transfer 70% to the Bens using crypto, Paypal, or Payoneer.</p><p>“What they were trying to do was use my identity to bypass background checks, and so they wanted this fake persona they created to be extremely close to the real-life version,” said Raney.</p><p>The Bens got Raney an interview, and while it was ongoing, they used a remote-desktop application to set up a notepad on Raney’s screen so they could write out responses to the questions from the interviewer, Raney explained. And it worked: Raney got a verbal offer for a job with a private government contractor that paid $80,000 a year.</p><p>He then had to immediately turn around and tell the company he couldn’t accept the offer and apologize for claiming their time.</p><p>“Now that they’re using real Americans who have verified identities and documents and they’re using their real faces—everything looks real,” said Raney. “There would be nothing stopping them from being hired.”</p><p>In the past two years, the Department of Justice has <a href="https://www.justice.gov/opa/media/1380081/dl" rel="nofollow noopener" target="_blank" data-ylk="slk:indicted;elm:context_link;itc:0;sec:content-canvas">indicted</a> dozens of North Korean citizens and unnamed coconspirators in the scheme, charging them for stealing American identities, conspiracy to violate U.S. sanctions, wire fraud, and money laundering. The FBI’s cybercrime <a href="https://www.fbi.gov/wanted/cyber/dprk-it-workers" rel="nofollow noopener" target="_blank" data-ylk="slk:wanted list;elm:context_link;itc:0;sec:content-canvas">wanted list</a> includes at least 14 North Korean IT workers sought by authorities, and the State Department announced a reward of <a href="https://rewardsforjustice.net/rewards/yanbian-silverstar-and-volasys-silverstar/" rel="nofollow noopener" target="_blank" data-ylk="slk:up to $5 million;elm:context_link;itc:0;sec:content-canvas">up to $5 million</a> for information on those involved.</p><p>Relatedly, a Nashville <a href="https://www.justice.gov/archives/opa/pr/justice-department-disrupts-north-korean-remote-it-worker-fraud-schemes-through-charges-and" rel="nofollow noopener" target="_blank" data-ylk="slk:man;elm:context_link;itc:0;sec:content-canvas">man</a> was arrested and an Arizona <a href="https://www.justice.gov/usao-dc/pr/arizona-woman-pleads-guilty-fraud-scheme-illegally-generated-17-million-revenue-north" rel="nofollow noopener" target="_blank" data-ylk="slk:woman;elm:context_link;itc:0;sec:content-canvas">woman</a> pleaded guilty for running “laptop farms” as part of the scheme. The laptop-farm keepers work with the North Korean gangs to <a href="https://www.justice.gov/archives/opa/pr/justice-department-announces-arrest-premises-search-and-seizures-multiple-website-domains" rel="nofollow noopener" target="_blank" data-ylk="slk:keep laptops;elm:context_link;itc:0;sec:content-canvas">keep laptops</a> shipped from various U.S. companies at their homes for a monthly fee, in exchange for accepting the devices and installing remote-desktop software so the IT workers can work outside the U.S., authorities alleged.</p><p>In the Arizona case, a 49-year-old woman outside Phoenix helped North Korean coconspirators get jobs at Fortune 500 banks, a television network, aerospace manufacturer, car manufacturer, and a Silicon Valley tech company, court documents show. Using 60 stolen identities, she helped the IT workers get jobs at 300 companies that paid them millions for their work.</p><p>“That a woman living her quiet life in the outskirts of Phoenix can allegedly get so entangled in something like this clearly indicates our adversaries are getting more sophisticated and stealthier, so it’s critical that businesses and citizens be hypervigilant with their cyber activities,” said FBI Special Agent Akil Davis of the Phoenix Field Office last year.</p><p>Ultimately, companies have to do more than just shipping a laptop out to a remote worker, said Chiba of Gartner.</p><p>“It reminds me of trying to get into a club; the bouncer is looking between you and your ID to see if it’s you and if it has the right photo,” Chiba said. “If the ID is checked once and only that once, and that is the only mitigation tactic, it’s probably not enough to catch someone.”</p><p>In a statement, Payoneer told <em>Fortune</em> it proactively works to combat fraud and financial crime on its platform through robust compliance systems and that it works closely with both regulators and law enforcement.</p><p>This story was originally featured on <a href="https://fortune.com/2025/04/07/north-korean-it-workers-infiltrating-fortune-500-companies/" rel="nofollow noopener" target="_blank" data-ylk="slk:Fortune.com;elm:context_link;itc:0;sec:content-canvas">Fortune.com</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Framework temporarily pausing some laptop sales in the US due to tariffs (174 pts)]]></title>
            <link>https://fosstodon.org/@frameworkcomputer/114297967333461078</link>
            <guid>43617207</guid>
            <pubDate>Tue, 08 Apr 2025 00:16:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fosstodon.org/@frameworkcomputer/114297967333461078">https://fosstodon.org/@frameworkcomputer/114297967333461078</a>, See on <a href="https://news.ycombinator.com/item?id=43617207">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[What Was Quartz? (136 pts)]]></title>
            <link>https://www.zachseward.com/what-was-quartz/</link>
            <guid>43616604</guid>
            <pubDate>Mon, 07 Apr 2025 22:24:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.zachseward.com/what-was-quartz/">https://www.zachseward.com/what-was-quartz/</a>, See on <a href="https://news.ycombinator.com/item?id=43616604">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <article>
      


      <section>
        <p>"It's impossible to kill a media brand," Jim Spanfeller told me on my first day working for him, as we sat in his corner office. He had just bought the business news organization, <a href="https://qz.com/?ref=zachseward.com">Quartz</a>, that I had <a href="https://www.zachseward.com/thank-you-quartz/" rel="noreferrer">spent the past decade</a> building and, most recently, trying desperately to save from oblivion. So I was inclined to believe him.</p><p>But I knew it wasn't true. Jim and his <a href="https://www.greathillpartners.com/?ref=zachseward.com">private-equity-backed</a> digital-media conglomerate <a href="https://g-omedia.com/?ref=zachseward.com">G/O Media</a> had already destroyed several of their properties, some all at once (<a href="https://www.npr.org/2019/11/01/775548069/after-days-of-resignations-the-last-of-the-deadspin-staff-have-quit?ref=zachseward.com">Deadspin</a>), most of the others by sapping resources, antagonizing their staff, and undermining the editorial visions that once made them great (<a href="https://www.thewrap.com/jezebel-editor-in-chief-laura-bassett-quits/?ref=zachseward.com">Jezebel</a>, <a href="https://www.gawkerarchives.com/media/what-happened-at-the-root?ref=zachseward.com">The Root</a>). It would take him three years to do the same to Quartz.</p><p>The end came on Friday, <a href="https://bsky.app/profile/gmgunion.bsky.social/post/3llz7s5jeok2c?ref=zachseward.com">when G/O fired</a> the few remaining writers at Quartz and <a href="https://finance.yahoo.com/news/redbrick-acquires-quartz-g-o-192800730.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAJr-lIw5hUIOZAtf2jxkJ_405wdnXtaEEzpxsQIiysw-ByR1A3LFzdYqEXcChDmP4ShKL3W7xbAUQsj7Ko1U7-jt1WLcsWNoJYB2BpmEWRbSZcuCuh-4XgOWLjIu55vdotvmgkqFXP71_YyHV9LIgjgtDBEA9rExP-CQ8x-XdLar&amp;ref=zachseward.com">sold the carcass</a> to a Canadian firm that appears mostly interested in the email list. I thought back to Jim's comment. His cynical view of digital media was good neither for journalism nor business, but maybe Jim was right about that one thing. What is Quartz now if not a "media brand" that refuses to die?</p>
<!--kg-card-begin: html-->
<p>§</p>
<!--kg-card-end: html-->
<p>We <a href="https://qz.com/6014/hello-world-a-welcome-letter-from-our-editor-in-chief?ref=zachseward.com" rel="noreferrer">launched Quartz</a> in 2012 with <a href="https://www.adweek.com/performance-marketing/the-atlantic-names-gideon-lichfield-global-news-editor-of-quartz/?ref=zachseward.com">prestigious editorial hires</a>, a <a href="https://www.niemanlab.org/2012/09/quartz-the-new-biz-news-site-is-a-technological-and-structural-innovator-with-only-a-few-hiccups/?ref=zachseward.com">slick new website</a>, <a href="https://allthingsd.com/20120924/quartz-shoots-for-tablet-and-mobile-readers-but-doesnt-arm-itself-with-an-app/?ref=zachseward.com">bold pronouncements</a> about the future of media, and a <a href="https://www.nytimes.com/2012/09/24/business/media/with-digital-only-quartz-atlantic-to-cover-business-world.html?unlocked_article_code=1.9U4.DvAc.tBenqzXRhaA0&amp;smid=url-share&amp;ref=zachseward.com">splashy profile</a> in The New York Times in which we likened ourselves to a "pirate ship attacking the Royal Navy." It sure felt that way. From a <a href="https://www.niemanlab.org/2015/05/quartz-is-an-api-the-path-ahead-for-the-business-site-thats-reshaping-digital-news/?ref=zachseward.com#:~:text=That%20image%20reminds%20me%20of%20another%20one%2C%20a%20photograph%20that%20Kevin%20%5BDelaney%2C%20Quartz%20editor%2Din%2Dchief%20and%20president%5D%20took%20of%20the%20first%20Quartz%20office%20in%20SoHo%20about%20three%20years%20ago.%20Look%20at%20all%20that%20room!">spare SoHo loft</a>, and then increasingly <a href="https://www.officelovin.com/2017/10/tour-quartzs-sleek-new-nyc-office/?ref=zachseward.com">fancier offices</a>, we set out to be compelling, entertaining, serious, and excellent all at the same time. Everything was up for reinvention, from the <a href="https://martinbelam.com/2013/smartden-kevin-delaney/?ref=zachseward.com">structure of our stories</a> to the <a href="https://digiday.com/media/quartz-guide-making-ads-dont-suck/?ref=zachseward.com">design of our ads</a> to the <a href="https://www.fastcompany.com/3054334/traditional-homepages-are-obsolete-says-quartz-heres-what-they-built-instead?ref=zachseward.com">need for a traditional homepage</a>. We believed that a news organization should stand for something, which in our case was <a href="https://qz.com/832998/the-case-for-free-trade-open-borders-and-the-new-global-economy?ref=zachseward.com" rel="noreferrer">globalism</a>. Most of all, we put a strong <a href="https://awards.journalists.org/entries/qz-com/?ref=zachseward.com">emphasis on quality</a> at a time when online journalism was still considered inferior to print publications that happened to have websites.</p><p>I think, in all those respects, we succeeded. Nothing about what ultimately transpired makes me regret the choices we made or the fun that we had. Quartz produced a lot of <a href="https://blog.qz.com/quartz-awards-2019-edition-ab067db5b1a9?ref=zachseward.com">great journalism</a>, served <a href="https://www.zachseward.com/presumed-audiences-in-digital-media/">our readers</a> well, and <a href="https://www.nytimes.com/2019/10/07/business/media/quartz-delaney-out.html?ref=zachseward.com#:~:text=%E2%80%9CI%E2%80%99m%20really%20proud%20of%20how%20Quartz%20has%20helped%20to%20fundamentally%20redefine%20the%20media%20industry%E2%80%99s%20approach%20to%20journalism%2C%20product%2C%20and%20advertising%20%E2%80%94%20and%20connected%20with%20tens%20of%20millions%20of%20readers%20around%20the%20world%20along%20the%20way%2C%E2%80%9D%20Mr.%20Delaney%20said%20in%20a%20statement.">widely influenced</a> the rest of our industry. If that's all Quartz was in the end, I'll take it.</p><p>Still, we also hoped to endure on the scale of centuries, just like rival news organizations — in particular, The Financial Times, The Economist, and The Wall Street Journal — that we viewed as our Goliaths. For a stretch in the middle there, it <a href="https://mashable.com/archive/quarts-tops-economist-traffic?ref=zachseward.com">even seemed possible</a>. But Quartz never made money. We grew, between 2012 and 2018, to nearly 250 employees and $35 million in annual revenue. The dismal economics of digital media meant losing more than $40 million over that stretch just to grow unsustainably large. A trade publication, which <a href="https://digiday.com/media/is-quartz-the-very-model-of-a-modern-publisher/?ref=zachseward.com">once called</a> Quartz "the very model a modern publisher," started <a href="https://digiday.com/future-of-tv/quartz-sees-video-subscriptions-driver/?ref=zachseward.com">literally portraying us</a> as pushing a boulder up a hill. Someone on our staff printed out the illustration and stuck it to a wall in our office, where it remained until the end.</p>
<!--kg-card-begin: html-->
<p>§</p>
<!--kg-card-end: html-->
<p>Twenty-eighteen would prove to be the peak of the endeavor and, not incidentally, the year our original owner, David Bradley, <a href="https://www.nytimes.com/2018/07/02/business/dealbook/quartz-atlantic-media-uzabase.html?ref=zachseward.com">decided to sell</a>. Quartz had <a href="https://www.ft.com/content/3260d3a0-9f86-11e5-beba-5e33e2b79e46?ref=zachseward.com">attracted takeover interest</a> over the years from major media companies like <a href="https://www.iac.com/?ref=zachseward.com">IAC</a> and <a href="https://www.nytco.com/?ref=zachseward.com">The New York Times</a> and even, to our astonishment, the luxury fashion house <a href="https://www.lvmh.com/en?ref=zachseward.com">LVMH</a>. Their interest ultimately fizzled. It was a small Japanese financial data firm, <a href="https://www.uzabase.com/en/?ref=zachseward.com">Uzabase</a>, that made the highest and final bid, at $86 million.</p><p>As business journalists, we of course recognized that Quartz was fundamentally a financial asset subject to the whims of market forces beyond our control. I may have known it intellectually, but still viewed Quartz <a href="https://qz.com/1930012/join-our-mission-to-make-business-better-as-quartz-becomes-an-independent-media-company?ref=zachseward.com">as a movement</a> out to prove something more noble. So when Uzabase <a href="https://www.wsj.com/articles/quartz-is-put-on-the-block-just-two-years-after-sale-11602034051?ref=zachseward.com">gave up on us</a> two years later, at the height of the pandemic, I quixotically decided to <a href="https://blog.qz.com/quartz-acquired-by-co-founder-and-ceo-zach-seward-in-management-buyout-1cd3eda1fd81?ref=zachseward.com">buy the company</a> myself. The price tag was <a href="https://nypost.com/2020/11/10/financial-website-quartz-may-have-been-sold-for-1-sources-say/?ref=zachseward.com">next-to-nothing</a> but also required financing the buyout and taking on the risk of surviving, for the first time, as an independent company.</p><p>The two years we spent on our own was the most challenging stretch of my tenure at Quartz. I started having panic attacks. One morning shortly after the buyout, I fainted and fell limp to the floor of my bedroom. But it was also a freeing time. A <a href="https://blog.qz.com/quartz-welcomes-new-reporters-based-in-taipei-nairobi-new-york-and-san-francisco-301a89ec7fd0?ref=zachseward.com">new generation</a> of employees breathed life into our journalism and <a href="https://qz.com/1999070/quartz-essentials-our-way-of-extracting-knowledge-from-the-news?ref=zachseward.com">products</a>, while financial independence gave the company, which I incorporated as a <a href="https://en.wikipedia.org/wiki/Benefit_corporation?ref=zachseward.com">public benefit</a> corporation, <a href="https://qz.com/1878256/what-is-the-purpose-of-business-journalism?ref=zachseward.com">new purpose</a>. The <a href="https://en.wikipedia.org/wiki/Paycheck_Protection_Program?ref=zachseward.com">Paycheck Protection Program</a>, for small businesses affected by the pandemic, helped keep us afloat.</p><p>Investors who specialized in "distressed assets" would call from time-to-time. They all had the same idea: Fire most of the staff and reboot Quartz as an email-only publication focused on aggregation of business news. As a business model, it might have made sense, but it wasn't appealing. Muddling along with few or no journalists would be a worse outcome, I felt, than just shutting the place down.</p><p>That is how I came to realize Quartz was, and always had been, <a href="https://qzlife.tumblr.com/?ref=zachseward.com">its people</a>. The hundreds of people who built and constantly rebuilt Quartz, who did the work, who supported each other through all the changes, who gave the place <a href="https://blog.qz.com/quartz-life-end-of-the-year-awards-bd8695496fe9?ref=zachseward.com#:~:text=In%20celebration%20and%20recognition%20of%20this%20work%2C,Force%20of%20Ideas%2C%20and%20Spirit%20of%20Generosity.">its values</a>, and who brought those values to other companies when they moved on. (Quartz alumni are all over today's great media companies, 22 of us at The Times alone.) We really did believe in what we were doing.</p>
<!--kg-card-begin: html-->
<p>§</p>
<!--kg-card-end: html-->
<p>So how did such an earnest enterprise end up in the maws of private equity? By 2022, we were running short of cash and didn't have anyone willing to put up more money, especially as <a href="https://www.nytimes.com/2019/02/01/business/media/buzzfeed-digital-media-wrong.html?ref=zachseward.com">enthusiasm waned</a> for the entire digital-media sector. We put together a quick M&amp;A process and made clear that preference would go to anyone willing to take on all of the roughly 80 people still working at Quartz.</p><p>G/O was the only suitor willing to make that commitment, and still bid three times more than the next-highest offer. That meant there was enough cash in the deal to share <a href="https://talkingbiznews.com/media-news/quartz-ceo-seward-to-staff-youll-get-a-cut-of-the-sale/?ref=zachseward.com">more than $1 million</a> of the proceeds with employees, who each got a stake in Quartz when we went independent. It was a far better outcome than I thought possible when we started the process, just desperate to survive.</p><p>My own investment would also turn out well, thanks to G/O's stubborn insistence that it only wanted Quartz's assets and not the corporate entity, which for complicated accounting reasons was still pretty valuable. I mention all these details, I must admit, out of spite. Jim would often talk about his feeling that journalists were bad businesspeople and taunt us about past decisions by asking, "How'd that work out for you?" Now G/O is in retreat, and today's strongest for-profit media companies are <a href="https://www.theinformation.com/?ref=zachseward.com">run</a> <a href="https://www.dowjones.com/?ref=zachseward.com">by</a> <a href="https://www.axios.com/?ref=zachseward.com">former</a> <a href="https://www.nytco.com/?ref=zachseward.com">reporters</a>.</p><p>At one point late in the sale process, Jim acted like he was no longer interested, and stopped returning my calls. That forced us to stare down the prospect of running completely out of cash and collapsing, <a href="https://www.theguardian.com/media/2024/jan/31/the-messenger-news-startup-shutters?ref=zachseward.com">Messenger-style</a>, as Jim well knew. We had no choice but to call his bluff. He went ahead with the purchase.</p>
<!--kg-card-begin: html-->
<p>§</p>
<!--kg-card-end: html-->
<p>I put on a <a href="https://medium.com/@quartz/quartz-acquired-by-g-o-media-f22e0da98010?ref=zachseward.com">brave face</a>, but we were under no illusions about the likely fate of Quartz under G/O's ownership. The sale immediately prompted obituaries; New York magazine <a href="https://nymag.com/intelligencer/2022/04/quartz-media-history.html?ref=zachseward.com">deemed us</a> "the history of internet media in just 10 years," which I still take as a compliment, even though we wanted 100 years.</p><p>It's still not quite clear to me what Jim thought he was buying or why he was so desperate to have it. In the subject line of his email announcing the deal, he spelled our name "<a href="https://www.vice.com/en/article/jim-spanfeller-proudly-announces-purchase-of-quarts-its-quartz/?ref=zachseward.com">Quarts</a>," and that set the tone for the level of care in what he had bought. But borrowing money was still incredibly cheap at that time (interest rates were <a href="https://www.thestreet.com/fed/fed-rate-hikes-2022-2023-timeline-discussion?ref=zachseward.com">about to surge</a>, another case of good timing), and I guess the allure of an existing media brand was too hard to resist. In retrospect, "it's impossible to kill" sounds to me less like an adage and more like a challenge.</p><p>Everyone who could quit did so as soon as they could. A few diehards held on longer, and were tortured or fired, or both, for their sacrifice. I left on the day my contract allowed it, exactly a year after the sale. G/O ultimately filled up the site with <a href="https://qz.com/7-weird-tourist-attractions-u-s-mexico-thailand-1851772879?ref=zachseward.com">2000s-era slideshows</a> and <a href="https://futurism.com/quartz-ai-generated-articles-slop-errors?ref=zachseward.com">AI-generated earnings stories</a>. It took another two years for me to process the loss and for G/O to complete its demolition.</p><p>The media business often feels like a battle between idealists and cynics. Most of my <a href="https://www.thecity.nyc/?ref=zachseward.com">favorite</a> <a href="https://documentedny.com/?ref=zachseward.com">news</a> <a href="https://outliermedia.org/?ref=zachseward.com">startups</a> of the current era have chosen the non-profit path, which has its own major challenges, but at least cynicism is not one of them. Quartz is now a <a href="https://www.techtarget.com/whatis/feature/Notable-zombie-brands?ref=zachseward.com">zombie brand</a>, which is the most cynical move in media.</p><p>I never wanted to write this piece while good people were still working at Quartz. Now it can be laid to rest.</p>
      </section>


        
          
        
        <div>
                <h2>Read more from Zach</h2>
                <p>Sign up to receive occasional emails with new posts.</p>
                
<form data-members-form="signup">
  <p><label for="subscribe-box-email">Your email address</label>
    
    
  </p>

  <p>Please check your inbox and click the link to confirm your subscription.</p>
  <p>Please enter a valid email address!</p>
  <p>An error occurred, please try again later.</p>
</form>              </div>

        
    </article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB (103 pts)]]></title>
            <link>https://arxiv.org/abs/2504.01157</link>
            <guid>43616241</guid>
            <pubDate>Mon, 07 Apr 2025 21:39:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2504.01157">https://arxiv.org/abs/2504.01157</a>, See on <a href="https://news.ycombinator.com/item?id=43616241">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2504.01157">View PDF</a>
    <a href="https://arxiv.org/html/2504.01157v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Knowledge-intensive analytical applications retrieve context from both structured tabular data and unstructured, text-free documents for effective decision-making. Large language models (LLMs) have made it significantly easier to prototype such retrieval and reasoning data pipelines. However, implementing these pipelines efficiently still demands significant effort and has several challenges. This often involves orchestrating heterogeneous data systems, managing data movement, and handling low-level implementation details, e.g., LLM context management.
<br>To address these challenges, we introduce FlockMTL: an extension for DBMSs that deeply integrates LLM capabilities and retrieval-augmented generation (RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling chained predictions through tuple-level mappings and reductions. Drawing inspiration from the relational model, FlockMTL incorporates: (i) cost-based optimizations, which seamlessly apply techniques such as batching and caching; and (ii) resource independence, enabled through novel SQL DDL abstractions: PROMPT and MODEL, introduced as first-class schema objects alongside TABLE. FlockMTL streamlines the development of knowledge-intensive analytical applications, and its optimizations ease the implementation burden.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Anas Dorbani [<a href="https://arxiv.org/show-email/498468e8/2504.01157" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Tue, 1 Apr 2025 19:48:17 UTC (446 KB)<br>
</p></div></div>]]></description>
        </item>
    </channel>
</rss>