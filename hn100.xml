<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 10 May 2024 16:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[HNInternal: Show HN: A web debugger an ex-Cloudflare team has been working on for 4 years (222 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40318542</link>
            <guid>40318542</guid>
            <pubDate>Fri, 10 May 2024 13:08:38 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40318542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td colspan="2"></td><td><div><p>Hey HN, I wanted to show you a product a small team and I have been working on for 4 years. <a href="https://jam.dev/" rel="nofollow">https://jam.dev</a></p><p>It’s called Jam and it prevents product managers (like I used to be) from being able to create vague and un-reproducible bug tickets (like I used to create).</p><p>It’s actually really hard as a non-engineer to file useful bug tickets for engineers. Like, sometimes I thought I included a screenshot, but the important information the engineer needed was what was actually right outside the boundary of the screenshot I took. Or I'd write that something "didn't work" but the engineer wasn't sure if I meant that it returned an error or if it was unresponsive. So the engineer would be frustrated, I would be frustrated, and fixing stuff would slow to a halt while we went back and forth to clarify how to repro the issue over async Jira comments.</p><p>It’s actually pretty crazy that while so much has changed in how we develop software (heck, we have types in javascript now*), the way we capture and report bugs is just as manual and lossy as it was in the 1990’s. We can run assembly in the browser but there’s still no tooling to help a non-engineer show a bug to an engineer productively.</p><p>So that’s what Jam is. Dev tools + video in a link. It’s like a shareable HAR file synced to a video recording of the session. And besides video, you can use it to share an instant replay of a bug that just happened — basically a 30 second playback of the DOM as a video.</p><p>We’ve spent a lot of time adding in a ton of niceties, like Jam writes automatic repro steps for you, and Jam’s dev tools use the same keyboard shortcuts you’re used to in Chrome dev tools, and our team’s personal favorite: Jam parses GraphQL responses and pulls out mutation names and errors (which is important because GraphQL uses one endpoint for all requests and always returns a 200, meaning you usually have to sift through every GraphQL request when debugging to find the one you’re looking for)</p><p>We’re now 2 years in to the product being live and people have used Jam to fix more than 2 million bugs - which makes me so happy - but there’s still a ton to do. I wanted to open up for discussion here and get your feedback and opinions how can we make it even more valuable for you debugging?</p><p>The worst part of the engineering job is debugging and not even being able to repro the issue, it’s not even really engineering, it’s just a communication gap, one that we should be able to solve with tools. So yeah excited to get your feedback and hear your thoughts how we can make debugging just a little less frustrating.</p><p>(Jam is free to use forever — there is a paid tier for features real companies would need, but we’re keeping a large free plan forever. We learned to build products at Cloudflare and free tier is in our ethos, both my co-founder and I and about half the team is ex-Cloudflare) and what we loved there is how much great feedback we’d get because the product was mostly  free to use. We definitely want to keep that going at Jam.)</p><p>By the way, we’re hiring engineers and if this is a problem that excites you, we’d love to chat: jam.dev/careers</p></div></td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Popover API (220 pts)]]></title>
            <link>https://developer.mozilla.org/en-US/docs/Web/API/Popover_API</link>
            <guid>40317740</guid>
            <pubDate>Fri, 10 May 2024 11:41:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.mozilla.org/en-US/docs/Web/API/Popover_API">https://developer.mozilla.org/en-US/docs/Web/API/Popover_API</a>, See on <a href="https://news.ycombinator.com/item?id=40317740">Hacker News</a></p>
<div id="readability-page-1" class="page"><article lang="en-US"><header><details><summary><span role="img" aria-label="Baseline Check"></span><h2>Baseline<!-- --> <span>2024</span></h2><p>Newly available</p><span></span></summary></details></header><p>The <strong>Popover API</strong> provides developers with a standard, consistent, flexible mechanism for displaying popover content on top of other page content. Popover content can be controlled either declaratively using HTML attributes, or via JavaScript.</p><section aria-labelledby="concepts_and_usage"><h2 id="concepts_and_usage"><a href="#concepts_and_usage">Concepts and usage</a></h2><div><p>A very common pattern on the web is to show content over the top of other content, drawing the user's attention to specific important information or actions that need to be taken. This content can take several different names — overlays, popups, popovers, dialogs, etc. We will refer to them as popovers through the documentation. Generally speaking, these can be:</p>
<ul>
  <li><strong>modal</strong>, meaning that while a popover is being shown, the rest of the page is rendered non-interactive until the popover is actioned in some way (for example an important choice is made).</li>
  <li><strong>non-modal</strong>, meaning that the rest of the page can be interacted with while the popover is being shown.</li>
</ul>
<p>Popovers created using the Popover API are always non-modal. If you want to create a modal popover, a <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/dialog"><code>&lt;dialog&gt;</code></a> element is the right way to go. There is significant overlap between the two — you might for example want to create a popover that persists, but control it using declarative HTML. You can turn a <code>&lt;dialog&gt;</code> element into a popover (<code>&lt;dialog popover&gt;</code> is perfectly valid) if you want to combine popover control with dialog semantics.</p>
<p>Typical use cases for the popover API include user-interactive elements like action menus, custom "toast" notifications, form element suggestions, content pickers, or teaching UI.</p>
<p>You can create popovers in two different ways:</p>
<ul>
  <li>Declaratively, via a set of new HTML attributes. A simple popover with a toggle button can be created using the following code:
    <div><pre data-signature="KdnYlrjDZk+9KBeXtIzbj2uatLAvVEOa7d4apg36UVg="><code><span><span><span>&lt;</span>button</span> <span>popovertarget</span><span><span>=</span><span>"</span>mypopover<span>"</span></span><span>&gt;</span></span>Toggle the popover<span><span><span>&lt;/</span>button</span><span>&gt;</span></span>
<span><span><span>&lt;</span>div</span> <span>id</span><span><span>=</span><span>"</span>mypopover<span>"</span></span> <span>popover</span><span>&gt;</span></span>Popover content<span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
</code></pre></div>
  </li>
  <li>Via a JavaScript API. For example, <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/togglePopover"><code>HTMLElement.togglePopover()</code></a> can be used to toggle a popover between shown and hidden.</li>
</ul>
<p>There are also new events to react to a popover being toggled, and CSS features to aid in styling popovers. All the new features are listed below.</p>
<p>See <a href="https://developer.mozilla.org/en-US/docs/Web/API/Popover_API/Using">Using the popover API</a> for a detailed guide to using this API.</p></div></section><section aria-labelledby="html_attributes"><h2 id="html_attributes"><a href="#html_attributes">HTML attributes</a></h2><div><dl>
  <dt id="popover"><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/popover"><code>popover</code></a></dt>
  <dd>
    <p>A global attribute that turns an element into a popover element; takes a popover state (<code>"auto"</code> or <code>"manual"</code>) as its value.</p>
  </dd>
  <dt id="popovertarget"><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/button#popovertarget"><code>popovertarget</code></a></dt>
  <dd>
    <p>Turns a <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/button"><code>&lt;button&gt;</code></a> or <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/input"><code>&lt;input&gt;</code></a> element into a popover control button; takes the ID of the popover element to control as its value.</p>
  </dd>
  <dt id="popovertargetaction"><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/button#popovertargetaction"><code>popovertargetaction</code></a></dt>
  <dd>
    <p>Specifies the action to be performed (<code>"hide"</code>, <code>"show"</code>, or <code>"toggle"</code>) on the popover element being controlled by a control <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/button"><code>&lt;button&gt;</code></a> or <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/input"><code>&lt;input&gt;</code></a>.</p>
  </dd>
</dl></div></section><section aria-labelledby="css_features"><h2 id="css_features"><a href="#css_features">CSS features</a></h2><div><dl>
  <dt id="backdrop"><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/::backdrop"><code>::backdrop</code></a></dt>
  <dd>
    <p>The <code>::backdrop</code> pseudo-element is a full-screen element placed directly behind popover elements, allowing effects to be added to the page content behind the popover(s) if desired (for example blurring it out).</p>
  </dd>
  <dt id="popover-open"><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/:popover-open"><code>:popover-open</code></a></dt>
  <dd>
    <p>The <code>:popover-open</code> pseudo-class matches a popover element only when it is in the showing state — it can be used to style popover elements when they are showing.</p>
  </dd>
</dl></div></section><section aria-labelledby="interfaces"><h2 id="interfaces"><a href="#interfaces">Interfaces</a></h2><div><dl>
  <dt id="toggleevent"><a href="https://developer.mozilla.org/en-US/docs/Web/API/ToggleEvent"><code>ToggleEvent</code></a></dt>
  <dd>
    <p>Represents an event notifying the user when a popover element's state toggles between showing and hidden. It is the event object for the <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/beforetoggle_event" title="beforetoggle"><code>beforetoggle</code></a> and <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/toggle_event" title="toggle"><code>toggle</code></a> events, which fire on popovers when their state changes.</p>
  </dd>
</dl></div></section><section aria-labelledby="extensions_to_other_interfaces"><h2 id="extensions_to_other_interfaces"><a href="#extensions_to_other_interfaces">Extensions to other interfaces</a></h2></section><section aria-labelledby="instance_properties"><h3 id="instance_properties"><a href="#instance_properties">Instance properties</a></h3><div><dl>
  <dt id="htmlelement.popover"><a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/popover"><code>HTMLElement.popover</code></a></dt>
  <dd>
    <p>Gets and sets an element's popover state via JavaScript (<code>"auto"</code> or <code>"manual"</code>), and can be used for feature detection. Reflects the value of the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/popover"><code>popover</code></a> global HTML attribute.</p>
  </dd>
  <dt id="htmlbuttonelement.popovertargetelement"><a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLButtonElement/popoverTargetElement"><code>HTMLButtonElement.popoverTargetElement</code></a> and <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLInputElement/popoverTargetElement"><code>HTMLInputElement.popoverTargetElement</code></a></dt>
  <dd>
    <p>Gets and sets the popover element being controlled by the control button. The JavaScript equivalent of the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/button#popovertarget"><code>popovertarget</code></a> HTML attribute.</p>
  </dd>
  <dt id="htmlbuttonelement.popovertargetaction"><a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLButtonElement/popoverTargetAction"><code>HTMLButtonElement.popoverTargetAction</code></a> and <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLInputElement/popoverTargetAction"><code>HTMLInputElement.popoverTargetAction</code></a></dt>
  <dd>
    <p>Gets and sets the action to be performed (<code>"hide"</code>, <code>"show"</code>, or <code>"toggle"</code>) on the popover element being controlled by the control button. Reflects the value of the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/button#popovertargetaction"><code>popovertargetaction</code></a> HTML attribute.</p>
  </dd>
</dl></div></section><section aria-labelledby="instance_methods"><h3 id="instance_methods"><a href="#instance_methods">Instance methods</a></h3><div><dl>
  <dt id="htmlelement.hidepopover"><a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/hidePopover"><code>HTMLElement.hidePopover()</code></a></dt>
  <dd>
    <p>Hides a popover element by removing it from the top layer and styling it with <code>display: none</code>.</p>
  </dd>
  <dt id="htmlelement.showpopover"><a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/showPopover"><code>HTMLElement.showPopover()</code></a></dt>
  <dd>
    <p>Shows a popover element by adding it to the top layer.</p>
  </dd>
  <dt id="htmlelement.togglepopover"><a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/togglePopover"><code>HTMLElement.togglePopover()</code></a></dt>
  <dd>
    <p>Toggles a popover element between the showing and hidden states.</p>
  </dd>
</dl></div></section><section aria-labelledby="events"><h3 id="events"><a href="#events">Events</a></h3><div><dl>
  <dt id="htmlelement"><a href="#htmlelement"><code>HTMLElement</code></a> <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/beforetoggle_event" title="beforetoggle"><code>beforetoggle</code></a> event</dt>
  <dd>
    <p>Fired just before a popover element's state changes between showing and hidden, or vice versa.</p>
  </dd>
  <dt id="htmlelement_2"><a href="#htmlelement_2"><code>HTMLElement</code></a> <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/toggle_event" title="toggle"><code>toggle</code></a> event</dt>
  <dd>
    <p>Fired just after a popover element's state changes between showing and hidden, or vice versa. This event already existed to signal state changes on <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/details"><code>&lt;details&gt;</code></a> elements, and it seemed logical to extend it for popover elements.</p>
  </dd>
</dl></div></section><section aria-labelledby="examples"><h2 id="examples"><a href="#examples">Examples</a></h2><div><p>See our <a href="https://mdn.github.io/dom-examples/popover-api/" target="_blank">Popover API examples landing page</a> to access the full collection of MDN popover examples.</p></div></section><h2 id="specifications"><a href="#specifications">Specifications</a></h2><table><thead><tr><th scope="col">Specification</th></tr></thead><tbody><tr><td><a href="https://html.spec.whatwg.org/multipage/popover.html#dom-popover">HTML Standard<!-- --> <br><small># <!-- -->dom-popover</small></a></td></tr></tbody></table><h2 id="browser_compatibility"><a href="#browser_compatibility">Browser compatibility</a></h2><p>BCD tables only load in the browser</p><section aria-labelledby="see_also"><h2 id="see_also"><a href="#see_also">See also</a></h2><div><ul>
  <li><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes/popover"><code>popover</code></a> HTML global attribute</li>
  <li><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/button#popovertarget"><code>popovertarget</code></a> HTML attribute</li>
  <li><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/button#popovertargetaction"><code>popovertargetaction</code></a> HTML attribute</li>
  <li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/::backdrop"><code>::backdrop</code></a> CSS pseudo-element</li>
  <li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/:popover-open"><code>:popover-open</code></a> CSS pseudo-class</li>
</ul></div></section></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is the largest root of a random polynomial more likely to be real than complex? (115 pts)]]></title>
            <link>https://mathoverflow.net/questions/470951/is-the-largest-root-of-a-random-polynomial-more-likely-to-be-real-than-complex</link>
            <guid>40316788</guid>
            <pubDate>Fri, 10 May 2024 09:01:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mathoverflow.net/questions/470951/is-the-largest-root-of-a-random-polynomial-more-likely-to-be-real-than-complex">https://mathoverflow.net/questions/470951/is-the-largest-root-of-a-random-polynomial-more-likely-to-be-real-than-complex</a>, See on <a href="https://news.ycombinator.com/item?id=40316788">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
                
<p><em>This question might be hard because <a href="https://math.stackexchange.com/questions/4908114/are-the-root-of-a-polynomial-with-the-largest-or-the-smallest-modulus-more-likel">it got <span>$35$</span> upvotes in MSE and also had a <span>$200$</span> points bounty</a> by Jyrki Lahtonen but it was unanswered. So I am posting it in MO.</em></p>
<p>The number of real roots of a random polynomial with real coefficients is much smaller than the number of complex roots. Assume that the coefficients are independently and uniformly random in <span>$(-1,1)$</span> for if not then we can divide each coefficient by the coefficient with the largest absolutely value to scale each coefficient to <span>$(-1,1)$</span>. The number of real roots of a polynomial of degree <span>$n$</span> is asymptotic to <span>$\displaystyle \frac{2\log n}{\pi} + o(1)$</span>. This means that the number of complex roots is approximately <span>$\displaystyle n - \frac{2\log n}{\pi}$</span>. Similar asymptotics hold for <a href="https://arxiv.org/pdf/1409.4128" rel="noreferrer">other distribution of the coefficients</a>.</p>
<blockquote>
<p>**Definition **: <em>The largest (or smallest) root of a polynomial is the root with the largest (or smallest) modulus.</em></p>
</blockquote>
<p><a href="https://i.sstatic.net/TpOFTU8J.png" rel="noreferrer"><img src="https://i.sstatic.net/TpOFTU8J.png" alt="Roots scatter plot"></a></p>
<p>The above graph shows the roots one such polynomial with degree <span>$101$</span>. The largest root is in the top right corner in green.</p>
<p>We can ask if the largest (or the smallest) root more likely to be real or complex? Since there are exponentially more complex roots than real roots as seen from the above asymptotic, my naive guess was that the largest (or the smallest) root is more likely to be complex. However experimental data proved to be quite counterintuitive.</p>
<p><a href="https://i.sstatic.net/nuuqhaXP.png" rel="noreferrer"><img src="https://i.sstatic.net/nuuqhaXP.png" alt="Probability that the largest root is real"></a></p>
<p>The data shows that:</p>
<ol>
<li>Probability that the largest (or smallest) root is real is greater than the probability that it is complex.</li>
<li>And this probability decreases to some value near <span>$1/2$</span> as <span>$n \to \infty$</span> as shown in the above graph (created using a Monte Carlo simulation with <span>$10^5$</span> trials for each value of <span>$n$</span>).</li>
<li><strong>Note</strong>: Instead of uniform distribution, if we assume that the coefficients are normally distributed with mean <span>$0$</span> and standard deviation <span>$1$</span> and scaled to <span>$(-1,1)$</span>, the above observation and limiting probabilities hold.</li>
</ol>
<p>It is counterintuitive that despite being much (exponentially) fewer in number, real roots are more likely to contain the both largest and the smallest roots of a random polynomial. <em>In this sense, the largest and the smallest roots are both biased towards reals</em>.</p>
<p><strong>Question 1</strong>: What is the reason for this bias?</p>
<p><strong>Question 2</strong>: Does the probability that the largest (or the smallest) root of a polynomial of degree <span>$n$</span> is real converge (to some value near <span>$\frac{1}{2}$</span> as <span>$n \to \infty$</span>)?</p>
<p><strong>Note</strong>: We can quantify the observed bias as follows. Let <span>$P(L|R)$</span> be the probability that a root is the largest given that it is real and let <span>$P(L|C)$</span> be the probability that a root is the largest given that it is complex. Similarly, let <span>$P(S|R)$</span> be the probability that a root is the smallest given that it is real and let <span>$P(S|C)$</span> be the probability that a root is the smallest given that it is complex. Then the experimental data says that</p>
<p><span>$$
P(L|R) = P(S|R) \approx \frac{\pi}{4\log n},
$$</span></p>
<p><span>$$
P(L|C) = P(S|C) \approx \frac{\pi}{2n\pi - 4\log n}.
$$</span></p>
<p><strong>Update</strong>: In the <a href="https://math.stackexchange.com/questions/4908114/is-the-largest-root-of-a-random-polynomial-more-likely-to-be-real-than-complex">linked MSE post</a>, it has now been proved that the probability that the largest root is real is at least</p>
<p><span>$$
\frac{23-16\sqrt{2}}{6} \approx 6.2 \%
$$</span></p>
<p><strong>Related</strong>: <a href="https://math.stackexchange.com/questions/4906738/what-is-the-probability-that-the-absolute-value-of-the-roots-of-a-polynomial-of">What is the probability that the absolute value of the roots of a polynomial of degree <span>$n$</span> is greater than <span>$x$</span></a>?</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Elvish, expressive programming language and a versatile interactive shell (130 pts)]]></title>
            <link>https://elv.sh</link>
            <guid>40316010</guid>
            <pubDate>Fri, 10 May 2024 06:38:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://elv.sh">https://elv.sh</a>, See on <a href="https://news.ycombinator.com/item?id=40316010">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
    <article>
      
      <div>
        <div>
<p><strong>Elvish</strong> (<em>noun</em>):</p>
<ol>
<li>
<p>A powerful scripting language.</p>
</li>
<li>
<p>A shell with useful interactive features built-in.</p>
</li>
<li>
<p>A statically linked binary for Linux, BSDs, macOS or Windows.</p>
</li>
</ol>
</div>
<section>
<header>
<p>Powerful modern shell scripting</p>
</header>
<div>
<p>Write readable and maintainable scripts - no cryptic operators, no
double-quoting every variable.</p>
<pre><header><p>jpg-to-png.elv <a href="https://elv.sh/learn/scripting-case-studies.html#jpg-to-png.elv">(explainer)</a></p>
</header><code><span>for</span> <span>x</span> <span>[</span>*.jpg<span>]</span> <span>{</span>
  <span>gm</span> convert <span>$x</span> <span>(</span><span>str:trim-suffix</span> <span>$x</span> .jpg<span>)</span>.png
<span>}</span>
</code></pre>
<p>Power up your workflows with data structures and functional programming.</p>
<pre><header><p>update-servers-in-parallel.elv <a href="https://elv.sh/learn/scripting-case-studies.html#update-servers-in-parallel.elv">(explainer)</a></p>
</header><code><span>var</span> <span>hosts</span> <span>=</span> <span>[</span><span>[</span><span>&amp;</span>name=a <span>&amp;</span>cmd=<span>'apt update'</span><span>]</span>
             <span>[</span><span>&amp;</span>name=b <span>&amp;</span>cmd=<span>'pacman -Syu'</span><span>]</span><span>]</span>
<span># peach = "parallel each"</span>
<span>peach</span> <span>{</span><span>|</span>h<span>|</span> <span>ssh</span> root@<span>$h</span><span>[</span>name<span>]</span> <span>$h</span><span>[</span>cmd<span>]</span> <span>}</span> <span>$hosts</span>
</code></pre>
<p>Catch errors before code executes.</p>
<pre><header><p>Terminal: elvish <a href="https://elv.sh/learn/scripting-case-studies.html#catching-errors-early">(explainer)</a></p>
</header><code>~&gt; <span>var</span> <span>project</span> <span>=</span> ~/project
~&gt; <span>rm</span> -rf <span>$projetc</span>/bin
compilation error: variable $projetc not found
</code></pre>
</div>
</section>
<section>
<header>
<p>Run it anywhere</p>
</header>
<div>
<p>Elvish comes in a single statically linked binary for your laptop, your server,
your PC, or your Raspberry Pi.</p>
<pre><header><p>Terminal: Raspberry Pi</p>
</header><code>~&gt; <span>wget</span> dl.elv.sh/linux-arm64/elvish-HEAD.tar.gz
~&gt; <span>tar</span> -C /usr/local/bin -xvf elvish-HEAD.tar.gz
elvish
~&gt; <span>elvish</span>
</code></pre>
<p>Use Elvish in your CI/CD pipelines. Convenient shell syntax and modern
programming language - why not both?</p>
<pre><header><p>github-actions.yaml</p>
</header><code>steps:
  - uses: elves/setup-elvish@v1
    with:
      elvish-version: HEAD
  - name: Run something with Elvish
    shell: elvish {0}
    run: |
      echo Running Elvish $version
</code></pre>
</div>
</section>
<section>
<header>
<p>Interactive shell with batteries included</p>
</header>
<div>
<p>Press <kbd>Ctrl-L</kbd> for directory history, and let Elvish find
<code>java/com/acme/project</code> for you.</p>
<pre><header><p>Terminal: elvish - directory history <a href="https://elv.sh/learn/tour.html#directory-history">(more)</a></p>
</header><code>~&gt;                                          <span>elf@host</span>
<span> LOCATION </span> j
<span> 10 ~/java/com/acme/project/utilities               </span>
 10 ~/java/com/acme/project
 10 /opt/java
</code></pre>
<p>Press <kbd>Ctrl-R</kbd> for command history. That beautiful <code>ffmpeg</code> command you
crafted two months ago is still there.</p>
<pre><header><p>Terminal: elvish - command history <a href="https://elv.sh/learn/tour.html#command-history">(more)</a></p>
</header><code>~&gt;                                          <span>elf@host</span>
<span> HISTORY (dedup on) </span> ff                 <span>Ctrl-D</span> dedup
  34 ffmpeg -i input.mp4 -c:v libx264 -c:a aac outpu
<span>  35 ffmpeg -i input.mp4 -vf "transpose=1,scale=640:</span>
</code></pre>
<p>Press <kbd>Ctrl-N</kbd> for the builtin file manager. Explore directories and
files without leaving the comfort of your shell.</p>
<pre><header><p>Terminal: elvish - file manager <a href="https://elv.sh/learn/tour.html#navigation-mode">(more)</a></p>
</header><code>~/elvish&gt;                                   <span>elf@host</span>
<span> NAVIGATING </span>             <span>Ctrl-H</span> hidden <span>Ctrl-F</span> filter
<span> bash  </span> <span> 1.0-release.m </span><span> </span> 1.0 has not been released y
<span> elvis </span>  CONTRIBUTING. <span> </span>
<span> zsh   </span>  Dockerfile    <span> </span>
         LICENSE       <span> </span>
         Makefile      <span>│</span>
         PACKAGING.md  <span>│</span>
         README.md     <span>│</span>
         SECURITY.md   <span>│</span>
</code></pre>
</div>
</section>


      </div>
      
    </article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Roman Tyrian purple snail dye found in UK for first time (130 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cjje132jvygo</link>
            <guid>40315970</guid>
            <pubDate>Fri, 10 May 2024 06:31:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cjje132jvygo">https://www.bbc.com/news/articles/cjje132jvygo</a>, See on <a href="https://news.ycombinator.com/item?id=40315970">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Player-Driven Emergence in LLM-Driven Game Narrative (101 pts)]]></title>
            <link>https://arxiv.org/abs/2404.17027</link>
            <guid>40315434</guid>
            <pubDate>Fri, 10 May 2024 04:05:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2404.17027">https://arxiv.org/abs/2404.17027</a>, See on <a href="https://news.ycombinator.com/item?id=40315434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Peng,+X">Xiangyu Peng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Quaye,+J">Jessica Quaye</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+W">Weijia Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Brockett,+C">Chris Brockett</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dolan,+B">Bill Dolan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jojic,+N">Nebojsa Jojic</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=DesGarennes,+G">Gabriel DesGarennes</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lobb,+K">Ken Lobb</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+M">Michael Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Leandro,+J">Jorge Leandro</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jin,+C">Claire Jin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rao,+S">Sudha Rao</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2404.17027">View PDF</a>
    <a href="https://arxiv.org/html/2404.17027v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>We explore how interaction with large language models (LLMs) can give rise to emergent behaviors, empowering players to participate in the evolution of game narratives. Our testbed is a text-adventure game in which players attempt to solve a mystery under a fixed narrative premise, but can freely interact with non-player characters generated by GPT-4, a large language model. We recruit 28 gamers to play the game and use GPT-4 to automatically convert the game logs into a node-graph representing the narrative in the player's gameplay. We find that through their interactions with the non-deterministic behavior of the LLM, players are able to discover interesting new emergent nodes that were not a part of the original narrative but have potential for being fun and engaging. Players that created the most emergent nodes tended to be those that often enjoy games that facilitate discovery, exploration and experimentation.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Sudha Rao [<a href="https://arxiv.org/show-email/4788f6e5/2404.17027">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 25 Apr 2024 20:39:44 UTC (1,287 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The most backdoor-looking bug I've ever seen (2021) (128 pts)]]></title>
            <link>https://words.filippo.io/dispatches/telegram-ecdh/</link>
            <guid>40315274</guid>
            <pubDate>Fri, 10 May 2024 03:33:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://words.filippo.io/dispatches/telegram-ecdh/">https://words.filippo.io/dispatches/telegram-ecdh/</a>, See on <a href="https://news.ycombinator.com/item?id=40315274">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <!--kg-card-begin: markdown--><p>This is the story of a bug that was discovered and fixed in Telegram's self-rolled cryptographic protocol about seven years ago. The bug didn't get any press, and no one seems to know about it, probably because it was only published in Russian.</p>
<p>To this day, it's the most backdoor-looking bug I've ever seen.</p>
<p>Google Translate does a good enough job on the <a href="https://habrahabr.ru/post/206900/?ref=words.filippo.io">original article</a>, which is still available on Habr, but I'm going to walk you through it along with some context.</p>
<p>Telegram is a popular chat app that uses its own... bizarre protocol to encrypt chats, called MTProto. The protocol is used both to encrypt all messages to the Telegram server, and to encrypt opt-in 1:1 end-to-end "Secret Chats".<sup><a href="#fn1" id="fnref1">[1]</a></sup> In text I can't do justice to the facial expressions of cryptographers when you mention Telegram's protocol, so just believe me that it's <em>weird</em>.</p>
<p>The current consensus seems to be that the latest version is not broken in known ways that are severe or relevant enough to affect end users, assuming the implementation is correct. That is about as safe as leaving exposed wires around your house because they are either not live or placed high enough that no one should touch them.</p>
<p>The original version was, however, completely broken, in the most puzzling of ways.</p>
<p>End-to-end Telegram chat sessions use <a href="https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange?ref=words.filippo.io">finite-field Diffie-Hellman</a><sup><a href="#fn2" id="fnref2">[2]</a></sup> to establish a shared key between the two participants. The negotiation happens through messages relayed by the Telegram server. Diffie-Hellman is a fundamental building block of many cryptosystems, and it allows two parties to establish a shared secret that any eavesdroppers can't derive. It is however only one part of a secure key exchange, because an attacker capable of intercepting the messages could simply establish two separate sessions with the two parties, carrying out a <a href="https://en.wikipedia.org/wiki/Person-in-the-middle_attack?ref=words.filippo.io">Person-in-the-Middle</a> attack. The parties need some way to verify they derived the same secret. In TLS, they use a signature from a certificate. In most secure chat apps, there is a fingerprint ("Safety Numbers" in Signal) that the two parties can compare out-of-band.<sup><a href="#fn3" id="fnref3">[3]</a></sup> What's important is that if the two sides derived the same secret, they can be sure no one else has access to it.</p>
<p>The Telegram key exchange is described in <a href="https://web.archive.org/web/20131220000537/https://core.telegram.org/api/end-to-end#key-generation">the "Key Generation" section of Telegram's end-to-end API docs</a>. Concretely, Alice requests the DH parameters <code>(p, g)</code> from Telegram, painstakingly verifies them, computes a random <code>a</code> value, and sends <code>g^a mod p</code> to Telegram. Bob receives <code>(p, g, g^a mod p)</code>, similarly computes <code>b</code> and <code>g^b mod p</code>, and sends the latter back (along with a truncated hash of the derived key, for some reason).</p>
<p>Now, normally the two sides would compute the shared key as <code>(g^a)^b mod p</code> and <code>(g^b)^a mod p</code>. Instead, the original version of MTProto computed it as</p>
<pre><code>(g^a)^b mod p XOR nonce
</code></pre>
<p>where <code>nonce</code> was an arbitrary, supposedly random value sent by the server along with the peer's public contribution.</p>
<p>This was a completely non-standard and useless addition, and all it did was let the server perform an undetected Person-in-the-Middle attack. Let's see how.</p>
<p>In a normal PitM, the server negotiates two separate Diffie-Hellman sessions with Alice and Bob, who end up with different shared keys, which they could detect by comparing fingerprints.</p>
<pre><code>Alice                     Telegram              Bob

a = random()       
A = g^a mod p       -&gt;
                        t = random()
                        T = g^t mod p -&gt;
                                          b = random()
                                      &lt;-  B = g^b mod p
                                          key = T^b mod p
                    &lt;-  T
key = T^a mod p

                    T^a mod p != T^b mod p
</code></pre>
<p>With the nonce addition, however, the server could "fix" Alice's key to match Bob's by manipulating Alice's nonce. The two parties would end up with the same fingerprint, and couldn't tell that an attack happened, but the server (and no one else) would know the shared key, allowing it to decrypt all messages.</p>
<pre><code>nonce_bob = random()
key_bob = T^b mod p  XOR  nonce_bob

nonce_alice = A^t mod p  XOR  B^t mod p  XOR  nonce_bob
key_alice = T^a mod p  XOR  nonce_alice =
  T^a mod p  XOR  (A^t mod p  XOR  B^t mod p  XOR  nonce_bob) =
  B^t mod p  XOR  nonce_bob = key_bob
</code></pre>
<p>Why do I say this addition was useless? Because it literally had no purpose! Indeed, the vulnerability was <a href="https://web.archive.org/web/diff/20131220000537/20131225140924/http://core.telegram.org/api/end-to-end">fixed by silently removing the nonce step from the docs</a>.<sup><a href="#fn4" id="fnref4">[4]</a></sup> <a href="https://core.telegram.org/constructor/encryptedChatRequested?layer=11&amp;ref=words.filippo.io">A later API revision</a> removed the nonce parameter with the caption "Improve secret chats". All <a href="https://web.archive.org/web/20131028041748/http://core.telegram.org/constructor/encryptedChatRequested">the original API reference</a> said about the nonce is "Random server sequence for calculation of key".</p>
<p>I never heard a plausible explanation for why the designers of MTProto went out of their way to add useless complexity to their protocol, with the only outcome of making undetectable interception possible.</p>
<p><strong>Edit (2021-01-11)</strong>: <a href="https://twitter.com/asdofindia/status/1348491279798128641?ref=words.filippo.io">@asdofindia linked me on Twitter</a> to <a href="https://telegram.org/blog/crowdsourcing-a-more-secure-future?ref=words.filippo.io">an official statement by Telegram about this</a> that I couldn't find anymore. It claims the nonce was there to protect clients with weak random number generators. Here's what I had buried into a footnote when I couldn't find a citation to attribute that explanation to Telegram:</p>
<blockquote>
<p>This doesn't make sense for a number of reasons: 1) clients with weak randomness are likely to be toast anyway, because Telegram's bizarro not-a-MAC relies on randomness in the payload to avoid an offline decryption oracle (there is a plaintext hash of the payload on the wire, I told you this was weird!); 2) the API also allows clients to request random bytes from the server to XOR with their secret share; and 3) defending against weak randomness by relying on a server contribution defends against everything but the server, which is the relevant attacker in the end-to-end setting. (Said another way, anyone that can intercept client-server messages can see the extra randomness, making it moot.) Non-practitioners might think this is a reasonable defense in depth, belts and suspenders kind of thing, but in cryptography engineering adding complexity to defend against scenarios that lead to compromise anyway is simply pointless.</p>
</blockquote>
<p>Anyway, it's been a while, the world is a different place now, and maybe <a href="https://en.wikipedia.org/wiki/Hanlon%27s_razor?ref=words.filippo.io">Hanlon's razor</a> cuts deeper than I thought. I think there are better reasons not to use Telegram today than this old bug<sup><a href="#fn1" id="fnref1:1">[1:1]</a></sup>, but it's still what I think about every time people talk about far-fetched "bugdoors". The bar is high!</p>
<h2 id="the-picture">The picture</h2>
<p>In other news, this newsletter is going to pivot into Rome photoblogging. (Not really, if you made it this far and like cryptography engineering, you should <a href="https://buttondown.email/cryptography-dispatches?tag=header&amp;ref=words.filippo.io">subscribe</a> or <a href="https://twitter.com/FiloSottile?ref=words.filippo.io">follow me on Twitter</a>.)</p>
<p><img src="https://words.filippo.io/content/images/2022/01/ee618b89-a8fa-45a2-af01-6f9955d2c99a.jpeg" alt="St. Peter's reflecting in the Tevere" loading="lazy"></p>
<hr>
<section>
<ol>
<li id="fn1"><p>By the way, aside from all the cryptographic weirdness and the unexplained backdoor-looking bug, the real reason you should not trust Telegram's encryption is that it's off by default, inconvenient to use, and simply unavailable in groups, meaning most messages flow unencrypted on Telegram's servers. Nonetheless, Telegram markets itself as a secure chat app, with misleading copy along the lines of "everything is encrypted, Secret Chats are just <em>more</em> encrypted!" They explain in their FAQ that it's all about backups, and that other more secure apps "<a href="https://telegram.org/faq?ref=words.filippo.io#dev_page_content:~:text=Other%20apps%20ignore%20the%20need%20for,before%20ever%20reaching%20a%20million%20users.">never reach a million users</a>". <a href="https://twitter.com/signalapp/status/1347240006444675072?ref=words.filippo.io">In other news</a>. <a href="#fnref1">↩︎</a> <a href="#fnref1:1">↩︎</a></p>
</li>
<li id="fn2"><p>Diffie-Hellman over finite fields is how it was originally designed, but today we'd use Elliptic-Curve Diffie-Hellman, which is faster, has smaller outputs, and is safer. FFDH has many of <a href="https://buttondown.email/cryptography-dispatches/archive/557475c5-9781-47e0-a640-5734bc849bc7?ref=words.filippo.io">the same issues as DSA</a> (FFDH is to DSA like ECDH is to ECDSA and EdDSA.) Current-day MTProto 2.0 still uses FFDH, but that's far from the most anachronistic choice in it. <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p>This is admittedly not a particularly strong authentication strategy, but it relies on the assumption that even if 1% of users check their fingerprints, systematic PitM is likely to be detected, and high-risk users can be extra careful and consistently check fingerprints. I hope solutions like key transparency can improve this picture in the coming years without changing the default UX. <a href="#fnref3">↩︎</a></p>
</li>
<li id="fn4"><p>Can we talk about how cool the Wayback Machine Compare feature is? Now is a good time to <a href="https://archive.org/donate/">donate to the Internet Archive</a>, by the way. <a href="#fnref4">↩︎</a></p>
</li>
</ol>
</section>
<!--kg-card-end: markdown-->
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CS388: Natural Language Processing (122 pts)]]></title>
            <link>https://www.cs.utexas.edu/~gdurrett/courses/online-course/materials.html</link>
            <guid>40314236</guid>
            <pubDate>Fri, 10 May 2024 00:09:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/materials.html">https://www.cs.utexas.edu/~gdurrett/courses/online-course/materials.html</a>, See on <a href="https://news.ycombinator.com/item?id=40314236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper">


<p>These are the course materials for an online masters course in NLP. All lectures are videos available on YouTube.</p>

<p><b>Note on enrollment for on-campus students:</b> This course is listed in the course catalog as "Natural Language Processing-WB". It is a partially asynchronous
course taught for certain online masters programs at UT ("Option III" programs, as the university calls them). If you are a student enrolled on-campus at UT Austin,
you are <b>not</b> eligible to take this course. This is a hard requirement from
the university due to the fact that this course is part of an Option III program. There is an on-campus version of CS388 that is typically
taught once per year by either me, Eunsol Choi, or Ray Mooney, which you are eligible to take (or CS371N if you're an undergraduate student). Regardless, you are free to consult the materials here!

</p><h2>Assignments</h2>

<p><b><a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/a1.pdf">Assignment 1: Linear Sentiment Classification</a></b> <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/a1-distrib.tgz">[code and dataset download]</a> [see edX for code walkthrough and debugging tips]
</p><p><b><a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/a2.pdf">Assignment 2: Feedforward Neural Networks, Word Embeddings, and Generalization</a></b> <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/a2-distrib.tgz">[code and dataset download]</a> [see edX for code walkthrough and debugging tips]
</p><p><b><a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/a3.pdf">Assignment 3: Transformer Language Modeling</a></b> <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/a3-distrib.tgz">[code and dataset download]</a> [see edX for code walkthrough and debugging tips]
</p><p><b><a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/a4.pdf">Assignment 4: Factuality and ChatGPT</a></b> <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/a4-distrib.tgz">[code and dataset download]</a>
</p><p><b><a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/fp.pdf">Final Project: Dataset Artifacts</a></b> <a href="https://github.com/gregdurrett/fp-dataset-artifacts">[code and dataset download]</a> <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/project-ex-1.pdf">[example 1]</a> <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/project-ex-2.pdf">[example 2]</a> <b><a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/peer-assessment.pdf">[peer assessment instructions]</a></b>
<!--<p><b><a href="a6.pdf">[SPRING 2021 VERSION] Assignment 6 (Final Project): Domain Adaptation for Question Answering</a></b> <a href="https://github.com/gregdurrett/nlp-qa-finalproj/">[code and dataset download]</a>-->

</p><h2>Lecture Videos and Readings</h2>

<p><b><a href="https://www.youtube.com/playlist?list=PLofp2YXfp7TZZ5c7HEChs0_wfEfewLDs7">YouTube playlist containing all videos</a></b></p>

<p><b><a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/slides-notes.tgz">Download the slides and handwritten notes here (88MB tgz)</a></b></p>

<table bordercolor="#111111" id="AutoNumber5">
  <tbody>
    <tr>
      <td><b>Topics and Videos</b></td>
      <td><b>Readings</b></td>
    </tr>
    <tr><td colspan="2"><b>Week 1: Intro and Linear Classification</b></td></tr>
    <tr>
     <td><a href="https://youtu.be/Mz8-LTednt4">Course Preview</a>
     </td>
      <td></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/k5p8teUNHX4">Introduction</a>
      </td>
      <td><p>Note: this introduction video is from an older run of the class and references an outdated schedule. Please refer
          to the new course structure here.
          </p>
      </td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/DVxR3AwdxoA">Linear Binary
          Classification</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          2.0-2.5, 4.2-4.4.1</a><p>
        <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/perc-lr-connections.pdf">Perceptron and logistic regression</a></p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/0jSElGFUxro">
          Sentiment Analysis and Basic Feature Extraction</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          4.1</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/_We4tlPkaj0">Basics of
          Learning, Gradient Descent</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/tMGv5ZcuVP4">Perceptron</a>
      </td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/hhTkyP7EzGw">Perceptron as
          Minimizing Loss</a>
      </td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/0naHFT07ja8">Logistic
          Regression</a>
      </td>
      <td><a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/perc-lr-connections.pdf">Perceptron and LR connections</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/cKbnEmjxnOY">Sentiment
          Analysis</a>
      </td>
      <td><a href="https://www.aclweb.org/anthology/W02-1011/" target="_blank">Thumbs up? Sentiment Classification using
          Machine Learning Techniques</a> Bo Pang et al., 2002<p>
        <a href="https://www.aclweb.org/anthology/P12-2018/" target="_blank">Baselines and Bigrams: Simple, Good
          Sentiment and Topic Classification</a> Sida Wang and Christopher Manning, 2012</p><p>
        <a href="https://www.aclweb.org/anthology/D14-1181/" target="_blank">Convolutional Neural Networks for Sentence
          Classification</a> Yoon Kim, 2014</p><p>
        <a href="https://github.com/sebastianruder/NLP-progress/blob/master/english/sentiment_analysis.md" target="_blank">[GitHub] NLP Progress on Sentiment Analysis</a></p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/65ui-GdtY0Q">Optimization
          Basics</a>
      </td>
      <td></td>
    </tr>

    <tr><td colspan="2"><b>Week 2: Multiclass and Neural Classification</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/My6GaGhqxdI">Multiclass
          Classification</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 4.2</a><p>
        <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/multiclass.pdf">Multiclass lecture note</a></p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/EA627DC7k6M">Multiclass
          Perceptron and Logistic Regression</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/va2i7LXt9zI">Multiclass
          Classification Examples</a></td>
      <td><a href="https://www.aclweb.org/anthology/D15-1075/" target="_blank">A large annotated corpus for learning
          natural language inference</a> Sam Bowman et al., 2015<p>
        <a href="https://www.aclweb.org/anthology/D13-1193/" target="_blank">Authorship Attribution of
          Micro-Messages</a> Roy Schwartz et al., 2013
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/N4f2-S19LME">Fairness in
          Classification</a></td>
      <td><a href="https://arxiv.org/pdf/1811.10104.pdf" target="_blank">50 Years of Test (Un)fairness: Lessons for
          Machine Learning</a> Ben Hutchinson and Margaret Mitchell, 2018<p>
        <a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G" target="_blank">[Article] Amazon scraps secret AI recruiting tool that showed bias against women</a></p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/DU_p-RBy5gM">Neural
          Networks</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/rdohzaGa8aE">Neural Network
          Visualization</a></td>
      <td><a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" target="_blank">[Blog] Neural Networks,
          Manifolds, and Topology</a> Chris Olah</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/8WhPYIWyR5g">Feedforward
          Neural Networks, Backpropagation</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          Chapter 3.1-3.3</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/IRZCQO18QAI">Neural Net
          Implementation</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/KPZb2rYS4BE">Neural Net
          Training, Optimization</a></td>
      <td><a href="https://dl.acm.org/doi/10.5555/2627435.2670313">Dropout: a simple way to prevent neural networks from
          overfitting</a> Nitish Srivastava et al., 2014 <p>

        <a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
          Internal Covariate Shift</a> Sergey Ioffe and Christian Szegedy, 2015</p><p>

        <a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a> Durk Kingma and Jimmy Ba,
        2015</p><p>

        <a href="https://papers.nips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html">The Marginal
          Value of Adaptive Gradient Methods in Machine Learning</a> Ashia Wilson et al., 2017
      </p></td>
    </tr>

    <tr><td colspan="2"><b>Week 3: Word Embeddings</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/8EqQROdVPyM">Word
          Embeddings</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/hznxqCIrzSQ">Skip-gram</a>
      </td>
      <td><a href="https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Distributed
          Representations of Words and Phrases and their Compositionality</a> Tomas Mikolov et al., 2013</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/gpP-depOUwg">Other Word
          Embedding Methods</a></td>
      <td><a href="https://papers.nips.cc/paper/2008/hash/1e056d2b0ebd5c878c550da6ac5d3724-Abstract.html" target="_blank">A Scalable Hierarchical Distributed Language Model</a> Andriy Mnih and Geoff Hinton, 2008<p>
        <a href="https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf" target="_blank">Neural Word Embedding as Implicit Matrix Factorization</a> Omer Levy and Yoav Goldberg, 2014</p><p>
        <a href="https://www.aclweb.org/anthology/D14-1162/" target="_blank">GloVe: Global Vectors for Word
          Representation</a> Jeffrey Pennington et al., 2014</p><p>
        <a href="https://arxiv.org/abs/1607.04606" target="_blank">Enriching Word Vectors with
          Subword Information</a> Piotr Bojanowski et al., 2016
    </p></td></tr>

    <tr>
      <td><a href="https://youtu.be/J_227g77Jqg">Bias in Word
          Embeddings</a></td>
      <td><a href="https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf" target="_blank">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word
          Embeddings</a> Tolga Bolukbasi et al., 2016<p>
        <a href="https://www.aclweb.org/anthology/N19-1062/" target="_blank">Black is to Criminal as Caucasian is to
          Police: Detecting and Removing Multiclass Bias in Word Embeddings</a> Thomas Manzini et al., 2019</p><p>
        <a href="https://www.aclweb.org/anthology/N19-1061/" target="_blank">Lipstick on a Pig: Debiasing Methods Cover
          up Systematic Gender Biases in Word Embeddings But do not Remove Them</a> Hila Gonen and Yoav Goldberg, 2019
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/3pwwdHuH0I4">Applying
          Embeddings, Deep Averaging Networks</a></td>
      <td><a href="https://www.aclweb.org/anthology/P15-1162/" target="_blank">Deep Unordered Composition Rivals
          Syntactic Methods for Text Classification</a> Mohit Iyyer et al., 2015</td>
    </tr>


    <tr><td colspan="2"><b>Week 4: Language Modeling and Self-Attention</b></td></tr>

    <tr>
      <td><a href="https://youtu.be/J-yHbD8LYCM">
          n-gram LMs</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          6.1</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/Yfug5eIQh5w">
          Smoothing in n-gram LMs</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          6.2</a></td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/ImW4vJ5XZQc">
          LM Evaluation</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          6.4</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/59NrmwAdOWA">
          Neural Language Models</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/xvnnA04JVQo">
          RNNs and their Shortcomings</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          6.3</a><p>
<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">[Blog] Understanding LSTMs</a> Chris Olah</p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/q7HY7tpWWi8">
          Attention</a></td>
      <td><a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate
</a> Dzmitry Bahdanau et al., 2015</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/10l2NXStROU">
          Self-Attention</a></td>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a> Ashish Vaswani et al.,
        2017</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/nHXrdLMo8Uk">
          Multi-Head Self-Attention</a></td>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a> Ashish Vaswani et al.,
        2017<p>
        <a href="http://jalammar.github.io/illustrated-transformer/">[Blog] The Illustrated Transformer</a> Jay Alammar</p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/a8sTGth7PoU">
          Position Encodings</a></td>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a> Ashish Vaswani et al.,
        2017<p>
        <a href="https://arxiv.org/abs/2108.12409" target="_blank">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</a> Ofir Press et al., 2021</p><p>
        <a href="https://arxiv.org/abs/2305.19466" target="_blank">The Impact of Positional Encoding on Length Generalization in Transformers</a> Amirhossein Kazemnejad et al., 2023
      </p></td>
    </tr>

    <tr><td colspan="2"><b>Week 5: Transformers and Decoding</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/sLsUD-RcDqg">
          Transformer Architecture</a></td>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a> Ashish Vaswani et al.,
        2017
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/1Efx04lHa7w">
          Using Transformers</a></td>
      <td>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/htyspM3FrMg">
          Transformer Language Modeling</a></td>
      <td>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/DPvDL8L4Dqo">
          Transformer Extensions</a></td>
      <td>
        <a href="https://arxiv.org/abs/2001.08361" target="_blank">Scaling Laws for Neural Language Models</a> Jared Kaplan et al., 2020<p>
        <a href="https://arxiv.org/abs/2009.06732" target="_blank">Efficient Transformers: A Survey</a> Yi Tay et al., 2020</p><p>
        <a href="https://arxiv.org/abs/2009.14794" target="_blank">Rethinking Attention with Performers</a> Krzysztof Choromanski et al., 2021</p><p>
        <a href="https://arxiv.org/abs/2004.05150" target="_blank">Longformer: The Long-Document Transformer</a> Iz Beltagy et al., 2021

      </p></td>
    </tr>


    <tr>
      <td><a href="https://youtu.be/wltqDbhlcJ0">
          Beam Search</a></td>
      <td>
      </td>
    </tr>


    <tr>
      <td><a href="https://youtu.be/JETxaSaj6_k">
          Nucleus Sampling</a></td>
      <td>
         <a href="https://arxiv.org/abs/1904.09751" target="_blank">The Curious Case of Neural Text Degeneration</a> Ari Holtzman et al., 2019
      </td>
    </tr>


    <tr><td colspan="2"><b>Week 6: Pre-training, seq2seq LMs</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/dya_QNFvtiQ">
          BERT: Masked Language Modeling</a></td>
      <td><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank">BERT: Pre-training of Deep Bidirectional
          Transformers for Language Understanding</a> Jacob Devlin et al., 2019</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/g96oi4ihc_E">
          BERT: Model and Applications</a></td>
      <td><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank">BERT: Pre-training of Deep Bidirectional
          Transformers for Language Understanding</a> Jacob Devlin et al., 2019<p>
        <a href="https://www.aclweb.org/anthology/W19-4302/" target="_blank">To Tune or Not to Tune? Adapting Pretrained
          Representations to Diverse Tasks</a> Matthew Peters et al., 2019</p><p>
        <a href="https://arxiv.org/pdf/1804.07461.pdf" target="_blank">GLUE: A Multi-Task Benchmark and Analysis
          Platform for Natural Language Understanding</a> Alex Wang et al., 2019</p><p>
        <a href="https://arxiv.org/abs/1906.04341" target="_blank">What Does BERT Look At? An Analysis of BERT's Attention
           </a> Kevin Clark et al., 2019
        <a href="https://arxiv.org/pdf/1907.11692.pdf" target="_blank">RoBERTa: A Robustly Optimized BERT Pretraining
          Approach</a> Yinhan Liu et al., 2019
      </p></td>
    </tr>


    <tr>
      <td><a href="https://youtu.be/TKZkvqb-qpM">
          Seq2seq Models</a></td>
      <td>
      </td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/M9L3gk4ITec">
          BART</a></td>
      <td><a href="https://arxiv.org/abs/1910.13461" target="_blank">BART: Denoising Sequence-to-Sequence Pre-training
          for Natural Language Generation, Translation, and Comprehension</a> Mike Lewis et al., 2019
      </td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/b6KFaT8mK4g">
          T5</a></td>
      <td>
        <a href="https://arxiv.org/pdf/1910.10683.pdf" target="_blank">Exploring the Limits of Transfer Learning with a
          Unified Text-to-Text Transformer</a> Colin Raffel et al., 2020<p>
        <a href="https://arxiv.org/abs/2005.00700" target="_blank">UnifiedQA: Crossing Format Boundaries With a Single QA System</a> Daniel Khashabi et al., 2020</p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/WA16JelEkkg">
          Word Piece and Byte Pair Encoding</a></td>
      <td><a href="https://arxiv.org/pdf/1508.07909.pdf" target="_blank">Neural Machine Translation of Rare Words with
          Subword Units</a> Rico Sennrich et al., 2016<p>
        <a href="https://arxiv.org/pdf/2004.03720.pdf" target="_blank">Byte Pair Encoding is Suboptimal for Language
          Model Pretraining</a> Kaj Bostrom and Greg Durrett, 2020
      </p></td>
    </tr>
    <tr><td colspan="2"><b>Week 7-8: Structured Prediction: Part-of-speech, Syntactic Parsing</b>
      <p>Note: this unit was previously presented as Week 4 right after classification. There are a few references
          to it being our first brush with structured models. In this structure of the course, it's still true that it's our first exposure to
          models dealing with linguistic structure as opposed to surface-level sequential structure (i.e., token sequences in generation).</p>
      </td></tr>
    <tr>
      <td><a href="https://youtu.be/Llw6qfeAWDs">Part-of-Speech
          Tagging</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 8.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/yQZ0mDW-U3g">Sequence
          Labeling, Tagging with Classifiers</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 7.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/FeLtLLbn4qU">Hidden Markov
          Models</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 7.4</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/dVF7LZkbl9g">
          HMMs: Parameter Estimation</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 7.4.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/Ks7IrsjhqSo">
          HMMs: Viterbi Algorithm</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 7.3</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/wijpAX_LLXo">
          HMMs for POS Tagging</a></td>
      <td><a href="https://arxiv.org/abs/cs/0003055" target="_blank">TnT - A Statistical Part-of-Speech Tagger</a>
        Thorsten Brants, 2000<p>
        <a href="https://www.aclweb.org/anthology/W00-1308/" target="_blank">Enriching the Knowledge Sources Used in a
          Maximum Entropy Part-of-Speech Tagger</a> Kristina Toutanvoa and Christopher Manning, 2000</p><p>
        <a href="https://link.springer.com/chapter/10.1007/978-3-642-19400-9_14" target="_blank">Part-of-Speech Tagging
          from 97% to 100%: Is It Time for Some Linguistics?</a> Christopher Manning, 2011</p><p>
        <a href="https://www.aclweb.org/anthology/D17-1309.pdf" target="_blank">Natural Language Processing with Small
          Feed-Forward Networks</a> Jan Botha et al., 2017
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/zDPUKQKDaMM">
          Constituency Parsing</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 10.1-10.2</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/q3dLP9YQLPA">
          Probabilistic Context-Free Grammars</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 10.3-10.4</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/QeDb6mSDSqs">
          CKY Algorithm</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 10.3.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/f1o1_bPWzM0">
          Refining Grammars</a></td>
      <td><a href="https://www.aclweb.org/anthology/P03-1054/" target="_blank">Accurate Unlexicalized Parsing</a> Dan Klein
        and Chris Manning, 2003<p>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 10.5</a></p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/dbDjKCc4R3E">
          Dependencies</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 11.1</a><p>
<a href="https://www.aclweb.org/anthology/Q13-1002/" target="_blank">Finding Optimal 1-Endpoint-Crossing
          Trees</a> Emily Pitler et al., 2013</p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/ypoaw7lJ6Rk">
          Transition-based Dependency Parsing</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 11.3</a>
      </td>
    </tr>
    <tr><td colspan="2"><b>Week 9: Modern Large Language Models</b></td></tr>

    <tr>
      <td><a href="https://youtu.be/jn41DLgnqek">
          GPT-3</a></td>
      <td>
       <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a> Alec Radford et al., 2019<p>
       <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> Tom B. Brown et al., 2020</p><p>
       <a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">Llama 2: Open Foundation and Fine-Tuned Chat Models</a> Hugo Touvron et al., 2023</p><p>Llama 2 is one of the latest models with publicly available weights (although it is not fully open-source, as many details of the training are not public).</p>
      </td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/YCq6b31Jb6E">
          Zero-shot Prompting</a></td>
      <td>
       <a href="https://arxiv.org/abs/2212.04037">Demystifying Prompts in Language Models via Perplexity Estimation</a> Hila Gonen et al., 2022
      </td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/JSBjj09xJeM">
          Few-shot Prompting</a></td>
      <td>
       <a href="https://arxiv.org/abs/2102.09690">Calibrate Before Use: Improving Few-Shot Performance of Language Models</a> Tony Z. Zhao et al., 2021<p>
       <a href="https://arxiv.org/abs/2211.09110">Holistic Evaluation of Language Models</a> Percy Liang et al., 2022</p><p>
       <a href="https://arxiv.org/abs/2202.12837">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</a> Sewon Min et al., 2022
      </p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/mUthsZ_Aivo">
          Understanding ICL: Induction Heads</a></td>
      <td>
        <a href="https://arxiv.org/abs/2209.11895">In-context Learning and Induction Heads</a> Catherine Olsson et al., 2022
      </td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/YT3VSlDjrVU">
          Instruction Tuning</a></td>
      <td>
        <a href="https://arxiv.org/abs/2110.08207">Multitask Prompted Training Enables Zero-Shot Task Generalization</a> Victor Sanh et al., 2021<p>
        <a href="https://arxiv.org/abs/2210.11416">Scaling Instruction-Finetuned Language Models</a> Hyung Won Chung et al., 2022
      </p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/DwAdhx6GFh8">
          Reinforcement Learning from Human Feedback (RLHF)</a></td>
      <td>
        <a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a> Long Ouyang et al., 2022<p>
        <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">[Website] Stanford Alpaca: An Instruction-following LLaMA Model</a> Rohan Taori et al., 2023
      </p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/bQZvmQUlqcs">
          Factuality of LLMs</a></td>
      <td>
        <a href="https://arxiv.org/abs/2212.07981">Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation</a> Yixin Liu et al., 2023<p>
        <a href="https://arxiv.org/abs/2303.01432">WiCE: Real-World Entailment for Claims in Wikipedia</a> Ryo Kamoi et al., 2023</p><p>
        <a href="https://arxiv.org/abs/2111.09525">SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization</a> Philippe Laban et al., 2022</p><p>
        <a href="https://arxiv.org/abs/2305.14251">FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</a> Sewon Min et al., 2023</p><p>
        <a href="https://arxiv.org/abs/2210.08726">RARR: Researching and Revising What Language Models Say, Using Language Models</a> Luyu Gao et al., 2022
      </p></td>
    </tr>
    <tr><td colspan="2"><b>Week 10: Explanations</b></td></tr>


    <tr>
      <td><a href="https://youtu.be/Nr0_xYEso-4">
          Explainability in NLP</a></td>
      <td><a href="https://arxiv.org/pdf/1606.03490.pdf" target="_blank">The Mythos of Model Interpretability</a> Zach Lipton,
        2016<p>
        <a href="https://www.aclweb.org/anthology/P15-1162/" target="_blank">Deep Unordered Composition Rivals Syntactic
          Methods for Text Classification</a> Mohit Iyyer et al., 2015</p><p>
        <a href="https://arxiv.org/pdf/1812.08951.pdf" target="_blank">Analysis Methods in Neural Language Processing: A
          Survey</a> Yonatan Belinkov and Jim Glass, 2019
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/ZVElc4CvHpk">
          Local Explanations: Highlights</a></td>
      <td><a href="https://arxiv.org/pdf/1602.04938.pdf" target="_blank">"Why Should I Trust You?" Explaining the
          Predictions of Any Classifier</a> Marco Tulio Ribeiro et al., 2016<p>
        <a href="https://arxiv.org/pdf/1703.01365.pdf" target="_blank">Axiomatic Attribution for Deep Networks</a>
        Mukund Sundararajan et al., 2017
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/a6u6WM5wcLQ">
          Model Probing</a></td>
      <td><a href="https://arxiv.org/pdf/1905.05950.pdf" target="_blank">BERT Rediscovers the Classical NLP Pipeline</a>
        Ian Tenney et al., 2019<p>
        <a href="https://arxiv.org/pdf/1905.06316.pdf" target="_blank">What Do You Learn From Context? Probing For
          Sentence Structure In Contextualized Word Represenations</a> Ian Tenney et al., 2019
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/RXYaMZcDIWU">
          Annotation Artifacts</a></td>
      <td><a href="https://www.aclweb.org/anthology/N18-2017/" target="_blank">Annotation Artifacts in Natural Language
          Inference Data</a> Suchin Gururangan et al., 2018<p>
        <a href="https://www.aclweb.org/anthology/S18-2023/" target="_blank">Hypothesis Only Baselines in Natural
          Language Inference</a> Adam Poliak et al., 2018</p><p>
        <a href="https://www.aclweb.org/anthology/P18-1176/" target="_blank">Did the Model Understand the Question?</a>
        Pramod Kaushik Mudrakarta et al., 2018</p><p>
        <a href="https://www.aclweb.org/anthology/D18-1009.pdf" target="_blank">Swag: A Large-Scale Adversarial Dataset
          for Grounded Commonsense Inference</a> Rowan Zellers et al., 2018
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/bXHM5t_ejsc">
          Text Explanations</a></td>
      <td>
        <a href="https://arxiv.org/pdf/1603.08507.pdf" target="_blank">Generating Visual Explanations</a> Lisa-Anne Hendricks et
        al., 2016<p>
        <a href="https://arxiv.org/abs/1812.01193" target="_blank">e-SNLI: Natural Language Inference with Natural Language Explanations</a> Oana-Maria Camburu et
        al., 2018</p><p>
        <a href="https://arxiv.org/pdf/2004.05569.pdf" target="_blank">Explaining Question Answering Models through Text
          Generation</a> Veronica Latcinnik and Jonathan Berant, 2020
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/tNGu3EqJbKc">
          Chain-of-thought</a></td>
      <td>
        <a href="https://arxiv.org/abs/1705.04146" target="_blank">Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems</a> Wang Ling et al., 2017<p>
        <a href="https://arxiv.org/abs/2201.11903" target="_blank">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a> Jason Wei et al., 2022</p><p>
        <a href="https://arxiv.org/abs/2205.03401" target="_blank">The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning</a> Xi Ye and Greg Durrett, 2022</p><p>
        <a href="https://arxiv.org/abs/2205.11916" target="_blank">Large Language Models are Zero-Shot Reasoners</a> Takeshi Kojima et al., 2022
      </p></td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/9sFyzMywKmo">
          Chain-of-thought: Extensions and Analysis</a></td>
      <td>
        <a href="https://arxiv.org/pdf/2211.13892.pdf" target="_blank">Complementary Explanations for Effective In-Context Learning</a> Xi Ye et al., 2023<p>
        <a href="https://arxiv.org/abs/2211.10435" target="_blank">PAL: Program-aided Language Models</a> Luyu Gao et al., 2022</p><p>
        <a href="https://arxiv.org/abs/2210.03350" target="_blank">Measuring and Narrowing the Compositionality Gap in Language Models</a> Ofir Press et al., 2022
      </p></td>
    </tr>

    <tr><td colspan="2"><b>Week 11: Question Answering, Dialogue Systems</b></td></tr>
    
    <tr>
      <td><a href="https://youtu.be/gnUSE0fCbso">
          Reading comprehension intro</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/JRI3RwRBnMY">
          Reading comprehension: setup and baselines</a></td>
      <td><a href="https://www.aclweb.org/anthology/D13-1020.pdf" target="_blank">MCTest: A Challenge Dataset for the
          Open-Domain Machine Comprehension of Text</a> Matthew Richardson et al., 2013<p>
        <a href="https://www.aclweb.org/anthology/D16-1264/" target="_blank">SQuAD: 100,000+ Questions for Machine
          Comprehension of Text</a> Pranav Rajpurkar et al., 2016
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/F8hWZ4xaVkA">
          BERT for QA</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/tCvAHmrxPvY">
          Problems with Reading Comprehension</a></td>
      <td><a href="https://www.aclweb.org/anthology/D17-1215/" target="_blank">Adversarial Examples for Evaluating
          Reading Comprehension Systems</a> Robin Jia and Percy Liang, 2017</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/P-j_zeS0Pa8">
          Open-domain QA</a></td>
      <td>
        <a href="https://arxiv.org/abs/1704.00051" target="_blank">Reading Wikipedia to Answer Open-Domain Questions</a> Danqi Chen et al., 2017<p>
        <a href="https://www.aclweb.org/anthology/P19-1612.pdf" target="_blank">Latent Retrieval for Weakly Supervised
          Open Domain Question Answering</a> Kenton Lee et al., 2019</p><p>
        <a href="https://ai.google.com/research/NaturalQuestions" target="_blank">[Website] Natural Questions</a> Tom Kwiatkowski et al., 2019</p><p>Most modern open-domain QA systems are either "closed-book" models like ChatGPT or "open-book" models that do retrieval, similar
          to the Chen et al. and Lee et al. papers above. These are typically described under the general framework of <a href="https://arxiv.org/pdf/2005.11401.pdf">retrieval-augmented generation</a> and an example of
          how these systems work is <a href="https://arxiv.org/abs/2112.09332">WebGPT</a> (similar to the "new Bing" chatbot).</p>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/jpRwa2iE_z8">
          Multi-hop QA</a></td>
      <td>
        <a href="https://arxiv.org/abs/1809.09600" target="_blank">HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering
             </a> Zhilin Yang et al., 2018<p>
        <a href="https://www.aclweb.org/anthology/N19-1405/" target="_blank">Understanding Dataset Design Choices for
          Multi-hop Reasoning</a> Jifan Chen and Greg Durrett, 2019</p><p>
        <a href="https://openreview.net/forum?id=SJgVHkrYDH" target="_blank">Learning to Retrieve Reasoning Paths over
          Wikipedia Graph for Question Answering</a> Akari Asai et al., 2020</p><p>Modern QA systems operating over the web are largely multi-hop by default; multi-hop QA has been subsumed by open-domain QA to a large extent. For a more recent multi-hop QA dataset, see <a href="https://arxiv.org/abs/2205.12665">QAMPARI</a></p>

      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/vAZ7VlLXReE">
          Dialogue: Chatbots</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/JXfAkX7kvnM">
          Task-Oriented Dialogue</a></td>
      <td><a href="https://arxiv.org/pdf/1811.01241.pdf" target="_blank">Wizards of Wikipedia: Knowledge-Powered
          Conversational Agents</a> Emily Dinan et al., 2019<p>
          <a href="https://arxiv.org/abs/2009.11423" target="_blank">Task-Oriented Dialogue as Dataflow Synthesis</a> Semantic Machines, 2020</p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/Hc7P3QukmJk" target="_blank">
          Neural Chatbots</a></td>
      <td>
        <a href="https://arxiv.org/abs/1506.06714">A Neural Network Approach to Context-Sensitive Generation of Conversational Responses</a> Alessandro Sordoni et al., 2015<p>
        <a href="https://arxiv.org/abs/1510.03055">A Diversity-Promoting Objective Function for Neural Conversation Models</a> Jiwei Li et al., 2016</p><p>
        <a href="https://arxiv.org/pdf/2004.13637.pdf">Recipes for building an open-domain chatbot</a> Stephen Roller et al., 2020</p><p>Note: an updated version of BlenderBot is described in <a href="https://arxiv.org/abs/2208.03188">Kurt Shuster et al.</a>.
              Other chatbots discussed, like <a href="https://character.ai/">character.ai</a>, can be found online and you can play with them, but less information
              about their precise internals is available in published papers.</p>
      </td>
          
    </tr>

    <tr><td colspan="2"><b>Week 12: Machine Translation, Summarization</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/9KAZ4-gKj9g">
          Machine Translation Intro</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 18.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/Oup0DEYJXEQ">
          MT: Framework and Evaluation</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 18.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/dzOuPhBmFtE">
          MT: Word alignment</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/mbtk3VCG_2A">
          MT: IBM Models</a></td>
      <td><a href="https://www.aclweb.org/anthology/C96-2141.pdf" target="_blank">HMM-Based Word Alignment in
          Statistical Translation</a> Stephan Vogel et al., 1996</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/0k8b5jGk-h4">
          Phrase-based Machine Translation</a></td>
      <td><a href="http://homepages.inf.ed.ac.uk/pkoehn/publications/pharaoh-amta2004.pdf" target="_blank">Pharaoh: A
          Beam Search Decoder for Phrase-Based Statistical Machine Translation Models</a> Philipp Koehn, 2004<p>
        <a href="https://www.aclweb.org/anthology/P03-1021/" target="_blank">Minimum Error Rate Training in Statistical
          Machine Translation</a> Franz Och, 2003</p><p>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 18.4</a></p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/bcP4b_4HQ8A">
          Neural and Pre-Trained Machine Translation</a></td>
      <td>
        <a href="https://arxiv.org/abs/1905.11901" target="_blank">Revisiting Low-Resource Neural Machine Translation: A Case Study</a> Rico Sennrich and Biao Zhang, 2019<p>
        <a href="https://aclanthology.org/2020.acl-main.688/" target="_blank">In Neural Machine Translation, What Does Transfer Learning Transfer?</a> Alham Fikri Aji et al., 2020</p><p>
        <a href="https://arxiv.org/abs/2001.08210" target="_blank">Multilingual Denoising Pre-training for Neural Machine Translation</a> Yinhan Liu et al., 2020</p><p>
        <a href="https://arxiv.org/abs/2302.14520" target="_blank">Large Language Models Are State-of-the-Art Evaluators of Translation Quality</a> Tom Kocmi and Christian Federmann, 2023
      </p></td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/lBDI1CBNe_U">
          Summarization Intro</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/QWt2E3m00kA">
          Extractive Summarization</a></td>
      <td><a href="https://dl.acm.org/doi/10.1145/290941.291025" target="_blank">The use of MMR, diversity-based
          reranking for reordering documents and producing summaries</a> Jaime Carbonell and Jade Goldstein, 1998<p>
        <a href="https://arxiv.org/abs/1109.2128" target="_blank">LexRank: Graph-based Lexical
          Centrality as Salience in Text Summarization</a> Gunes Erkan and Dragomir Radev, 2004</p><p>
        <a href="https://www.aclweb.org/anthology/W09-1802/" target="_blank">A Scalable Global Model for
          Summarization</a> Dan Gillick and Benoit Favre, 2009</p><p>
        <a href="https://www.aclweb.org/anthology/W17-4511/" target="_blank">Revisiting the Centroid-based Method: A
          Strong Baseline for Multi-Document Summarization</a> Demian Gholipour Ghalandari, 2017
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/feLTtTilycY">
          Pre-trained Summarization and Factuality</a></td>
      <td><a href="https://www.aclweb.org/anthology/2020.acl-main.703/" target="_blank">BART: Denoising
          Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a> Mike Lewis et
        al., 2019<p>
        <a href="https://arxiv.org/abs/1912.08777" target="_blank">PEGASUS:
          Pre-training with Extracted Gap-sentences for Abstractive Summarization</a> Jingqing Zhang et al., 2020</p><p>
        <a href="https://arxiv.org/pdf/2010.05478.pdf" target="_blank">Evaluating Factuality in Generation with
          Dependency-level Entailment</a> Tanya Goyal and Greg Durrett, 2020</p><p>
        <a href="https://arxiv.org/abs/2004.04228" target="_blank">Asking and Answering Questions to Evaluate the Factual Consistency of Summaries</a> Alex Wang et al., 2020</p><p>Note: while the specific fine-tuned modeling approaches and factuality detection systems are no longer state-of-the-art as stated in the video,
              they are representative of ideas from pre-training 
              that are still used today. For discussion of how LLMs relate to summarization, see <a href="https://arxiv.org/abs/2209.12356">News Summarization and Evaluation in the Era of GPT-3
</a> by Tanya Goyal, Junyi Jessy Li, and Greg Durrett</p>
      </td>
    </tr>

    <tr><td colspan="2"><b>Week 13-14: Multilinguality, Language Grounding, Ethical Issues</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/ettP9Ayrho8">
          Morphology</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/rTSCLfxdxrI">
          Cross-lingual Tagging and Parsing</a></td>
      <td><a href="https://www.aclweb.org/anthology/P11-1061/" target="_blank">Unsupervised Part-of-Speech Tagging with
          Bilingual Graph-Based Projections</a> Dipanjan Das and Slav Petrov, 2011<p>
        <a href="https://www.aclweb.org/anthology/D11-1006/" target="_blank">Multi-Source Transfer of Delexicalized
          Dependency Parsers</a> Ryan McDonald et al., 2011
      </p></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/KNBRb8sjzOA">
          Cross-lingual Pre-training</a></td>
      <td><a href="https://arxiv.org/pdf/1602.01925.pdf" target="_blank">Massively Multilingual Word Embeddings</a>
        Waleed Ammar et al., 2016<p>
        <a href="https://www.aclweb.org/anthology/Q19-1038.pdf" target="_blank">Massively Multilingual Sentence
          Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond</a> Mikel Artetxe and Holger Schwenk, 2019</p><p>
        <a href="https://www.aclweb.org/anthology/P19-1493.pdf" target="_blank">How multilingual is Multilingual
          BERT?</a> Telmo Pires et al., 2019
      </p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/ayNoMmoXnd8">
          Language Grounding</a></td>
      <td>
        <a href="https://aclanthology.org/2020.acl-main.463/">Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data</a> Emily Bender and Alexander Koller, 2020<p>
        <a href="https://arxiv.org/abs/2104.10809">Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand?</a> Will Merrill et al., 2021</p><p>
        <a href="https://arxiv.org/abs/2209.12407">Entailment Semantics Can Be Extracted from an Ideal Language Model</a> Will Merrill et al., 2022</p><p>
        <a href="https://arxiv.org/abs/2004.10151">Experience Grounds Language</a> Yonatan Bisk et al., 2020
      </p></td>
    </tr>
    <tr>
      <td><a href="https://www.youtube.com/watch?v=pkdV-iddZxk">
          Language and Vision</a></td>
      <td>
       <a href="https://arxiv.org/abs/1505.00468" target="_blank">VQA: Visual Question Answering</a> Aishwarya Agrawal et al., 2015<p>
       <a href="https://arxiv.org/abs/2103.00020" target="_blank">Learning Transferable Visual Models From Natural Language Supervision</a> Alex Radford et al., 2021
      </p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/tTkAjNkXvH4">
          Ethics: Bias</a></td>
      <td>
        <a href="https://aclanthology.org/P16-2096.pdf" target="_blank">The Social Impact of Natural Language Processing</a> Dirk Hovy and Shannon Spruit, 2016<p>
        <a href="https://arxiv.org/pdf/1707.09457.pdf" target="_blank">Men Also Like Shopping:
Reducing Gender Bias Amplification using Corpus-level Constraints</a> Jieyu Zhao et al., 2017
      </p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/0haVbW2ouzw">
          Ethics: Exclusion</a></td>
      <td>
        <a href="https://arxiv.org/abs/2205.12247" target="_blank">GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models</a> Da Yin et al., 2022<p>
        <a href="https://arxiv.org/abs/2109.13238" target="_blank">Visually Grounded Reasoning across Languages and Cultures</a> Fangyu Liu et al., 2021
      </p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/2PSzHb08Xm4">
          Ethics: Dangers of Automation</a></td>
      <td>
       <a href="https://dl.acm.org/doi/10.1145/3442188.3445922" target="_blank">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</a> Emily Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell, 2021<p>
       <a href="https://arxiv.org/abs/2009.11462" target="_blank">RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models</a> Samuel Gehman et al., 2020
      </p></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/Hp8qfbsp57M">
          Ethics: Unethical Use and Paths Forward</a></td>
      <td>
       <a href="https://arxiv.org/pdf/1803.09010.pdf">Datasheets for Datasets</a> Timnit Gebru et al., 2018<p>
       <a href="https://dl.acm.org/doi/pdf/10.1145/3351095.3372873">Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing</a> Deb Raji et al., 2020
      </p></td>
    </tr>

  </tbody>
</table>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wprs – rootless remote desktop for Wayland (and X11, via XWayland) applications (135 pts)]]></title>
            <link>https://github.com/wayland-transpositor/wprs</link>
            <guid>40313798</guid>
            <pubDate>Thu, 09 May 2024 23:02:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/wayland-transpositor/wprs">https://github.com/wayland-transpositor/wprs</a>, See on <a href="https://news.ycombinator.com/item?id=40313798">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">wprs</h2><a id="user-content-wprs" aria-label="Permalink: wprs" href="#wprs"></a></p>
<p dir="auto">Like <a href="https://en.wikipedia.org/wiki/Xpra" rel="nofollow">xpra</a>, but for Wayland, and written in
Rust.</p>
<p dir="auto">wprs implements rootless remote desktop access for remote Wayland (and X11, via
XWayland) applications.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto"><code>cargo build --profile=release-lto  # or release, but debug is unusably slow</code></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">On the remote host, enable wprsd:</p>
<div dir="auto" data-snippet-clipboard-copy-content="loginctl enable-linger
systemctl --user enable wprsd.service
systemctl --user start wprsd.service"><pre>loginctl enable-linger
systemctl --user <span>enable</span> wprsd.service
systemctl --user start wprsd.service</pre></div>
<p dir="auto">On the local host:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# starts application on the remote host (starts ssh connection, forwards sockets, starts wprsc, runs application)
wprs <remote_host> run <application>

# stops local wprs connections, leaving remote session running (tear down ssh connection and forwarded sockets, stops wprsc)
wprs <remote_host> detach

# attaches to remote wprs session (starts ssh connection, forwards sockets, starts wprsc)
wprs <remote_host> attach"><pre><span><span>#</span> starts application on the remote host (starts ssh connection, forwards sockets, starts wprsc, runs application)</span>
wprs <span>&lt;</span>remote_host<span>&gt;</span> run <span>&lt;</span>application<span>&gt;</span>

<span><span>#</span> stops local wprs connections, leaving remote session running (tear down ssh connection and forwarded sockets, stops wprsc)</span>
wprs <span>&lt;</span>remote_host<span>&gt;</span> detach

<span><span>#</span> attaches to remote wprs session (starts ssh connection, forwards sockets, starts wprsc)</span>
wprs <span>&lt;</span>remote_host<span>&gt;</span> attach</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">System Tuning</h2><a id="user-content-system-tuning" aria-label="Permalink: System Tuning" href="#system-tuning"></a></p>
<p dir="auto">Increasing linux's socket buffer limits as described in
<a href="https://wiki.archlinux.org/title/sysctl#Increase_the_memory_dedicated_to_the_network_interfaces" rel="nofollow">https://wiki.archlinux.org/title/sysctl#Increase_the_memory_dedicated_to_the_network_interfaces</a>
will result in improved performance.</p>
<p dir="auto">TODO: test ssh socket forwarding performance with different values of
wmem_default. wprs uses setsockopt to increase its buffer size, but it doesn't
seem that ssh does.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration Files</h2><a id="user-content-configuration-files" aria-label="Permalink: Configuration Files" href="#configuration-files"></a></p>
<p dir="auto">You can create configuration files for <code>wprsc</code> and <code>wprsd</code> instead of passing additional
arguments to <code>wprs</code>. To see what options are available, run <code>wprsc --help</code> and
<code>wprsd --help</code>.</p>
<p dir="auto">To generate the default configs, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# on your local machine
wprsc --print-default-config-and-exit=true > ~/.config/wprs/wprsc.ron"><pre><span><span>#</span> on your local machine</span>
wprsc --print-default-config-and-exit=true <span>&gt;</span> <span>~</span>/.config/wprs/wprsc.ron</pre></div>
<p dir="auto">and</p>
<div dir="auto" data-snippet-clipboard-copy-content="# on your remote machine
wprsd --print-default-config-and-exit=true > ~/.config/wprs/wprsd.ron"><pre><span><span>#</span> on your remote machine</span>
wprsd --print-default-config-and-exit=true <span>&gt;</span> <span>~</span>/.config/wprs/wprsd.ron</pre></div>
<p dir="auto">Then update the <code>wprsc.ron</code> and <code>wprsd.ron</code> files with your desired settings.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Current Limitations</h2><a id="user-content-current-limitations" aria-label="Permalink: Current Limitations" href="#current-limitations"></a></p>
<p dir="auto">Currently only the the Core and XDG shell protocols are implemented. In
particular, hardware rendering/dmabuf support is not yet implemented.</p>
<ul dir="auto">
<li>Damage passthough is not yet implemented.</li>
<li>Touch event support is not yet implemented.</li>
<li>Drag-and-drop may be wonky in some cases.</li>
<li>XWayland drag-and-drop is not (yet?) implemented.</li>
<li>webauthn security keys don't yet work in browsers</li>
</ul>
<p dir="auto">Generally, wprs will aim to support as many protocols as feasible, it's a
question of time and prioritization.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<p dir="auto">On the remote (server) side, <code>wprsd</code> implements a wayland compositor using
<a href="https://github.com/Smithay/smithay">Smithay</a>. Instead of compositing and
rendering though, wprsd serializes the state of the wayland session and sends it
to the connected wprsc client using a custom protocol.</p>
<p dir="auto">On the local (client) side, <code>wprsc</code> implements a wayland client (using the
<a href="https://github.com/Smithay/client-toolkit">Smithay Client Toolkit</a> that creates
local wayland objects that correspond to remote wayland objects. For example, if
a remote application running against wprsd creates a surface and an
xdg-toplevel, wprsc will create a surface with the same contents, an
xdg-toplevel with the same metadata, etc.. From the local compositor's point of
view, wprsc is just a normal application with a bunch of windows. Input and
other events from the local compositor that wprsc are serialized and sent to
wprsd, which forwards them to the appropriate application (the owner of the
surface which the wprsc surface which received the events corresponds to).</p>
<p dir="auto">wprs supports session resumption (wprsc disconnection and later reconnection and
wprsc restarts). The wayland protocol is not natively resumable in this way
because it relies on shared state between the compositor and client
applications. By implementing a wayland compositor locally relative to the
application, wprsd stores all state necessary for wayland applications and is
also able to store sufficient state (e.g., the buffer contents for each surface
as of the last commit) for a newly-connected wprsc to correctly set up all
necessary wayland objects. wprsc is stateless, but wprsd is not, so a wprsd
restart will still terminate all wayland applications running against it, like
with any other wayland compositor.</p>
<p dir="auto">Communication between wprsd and wprsc happens over unix domain sockets; wprsd
creates a socket and wprsc connects to it. The default mode of operation is to,
on the client side, use ssh to forward a local socket to the remote wprsd
socket, but a different transport could be used with, for example, socat or a
custom proxy application. A launcher script (<code>wprs</code>) is provided which sets up
the ssh socket forwarding.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Protocol</h3><a id="user-content-protocol" aria-label="Permalink: Protocol" href="#protocol"></a></p>
<p dir="auto">The custom protocol used to serialize and transmit wayland state between wprsc
and wprsd is a simplified version of the wayland protocol. Wayland objects are
represented as rust types and serialized using
<a href="https://github.com/rkyv/rkyv">rkyv</a>. Unlike the wayland protocol, the wprs
protocol tries to be idempotent when possible. For example, instead of the
repeated back-and-forth involved in created a surface, creating an xdg-surface,
creating an xdg-toplevel, waiting for it to be configured, creating a buffer,
attaching the buffer, and comitting it, wprsd will send a single commit message
to wprsc with the complete state of the surface (surface's attached buffer
contents (if any), its role (if any) and any associated metadata, etc.) and
wprsc will execute the appropriate dance with the local compositor.</p>
<p dir="auto">Frame callbacks are scheduled locally by wprsd at the configured framerate, they
are not forwarded from wprsc as that would introduce an unacceptable amount of
frame latency due to network round-trips. When no wprsc is connected, wprsd
pauses sending frame callbacks to wayland applications.</p>
<p dir="auto">Buffer compression is handled using a custom multithreaded and SIMD-accelerated
lossless image compression algorithm:</p>
<ol dir="auto">
<li>Transpose the image from an <a href="https://en.wikipedia.org/wiki/AoS_and_SoA" rel="nofollow">array of structures to a struct of
arrays</a>. This makes the subequent
steps significantly faster by letting them be implemented with SIMD
instructions and additionally improves the compression ratio because each
color channel is more closely spatially correlated with itself than with the
other
channels.</li>
<li>Apply an adjacent (wrapping) difference to each color channel (differential
pulse-code modulation). This improves the compression ratio by taking
advantage of spatial correlation and transforms (for example) a solid-colored
line into a single color byte and then a sequence of 0-bytes, or a gradient
into a sequence of 1-bytes, etc.</li>
<li>Transform each color channel into a
<a href="https://en.wikipedia.org/wiki/Y%E2%80%B2UV" rel="nofollow">YUV</a>-like color space: <code>y := g, u := b - g, v := r - g, a := a</code>. This improves the compression ratio in a
similar way as the previous step but by taking advantage of cross-color
correlation.</li>
<li>Compress the data with zstd.
This algorithm was designed for reasonably good compression ratios while being
extremely last: single-digit milliseconds per frame. Decompression is done by
inverting those steps.</li>
</ol>
<p dir="auto">This protocol is <em>not stable</em>: there is no guarantee that different versions of
wprsc and wprsd, or wprsc and wprsd built with different versions of
dependencies or even rustc will be compatible. This may change in the future,
but it will not happen soon.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Comparison to Waypipe</h3><a id="user-content-comparison-to-waypipe" aria-label="Permalink: Comparison to Waypipe" href="#comparison-to-waypipe"></a></p>
<p dir="auto"><a href="https://gitlab.freedesktop.org/mstoeckl/waypipe" rel="nofollow">Waypipe</a>'s model is analogous
to X forwarding, while wprs's model is analgous to Xpra. Waypipe ~transparently
forwards messages between the local compositor and the remote application, so
the client ends up being stateful and sessions can only be resumed through
network reconnections, not client restarts. There are tradeoffs to the two
approaches. Waypipe's approach is partially forward-compatible: it can support
new wayland protocols automatically, however those protocols may be broken if
they use shared resources in a way that waypipe doesn't know how to handle.
wprs, on the other hand, requires explicit implementation for every wayland
protocol.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">XWayland</h3><a id="user-content-xwayland" aria-label="Permalink: XWayland" href="#xwayland"></a></p>
<p dir="auto">XWayland support is implemented as a separate binary, <code>xwayland-xdg-shell</code>. The
binary implements a wayland compositor (but only for the protocol features used
by xwayland) and client, just like wprsd and wprsc, but in a single binary (so
skipping the serialization/deserialization). This is the same model as
<a href="https://github.com/talex5/wayland-proxy-virtwl#xwayland-support">xwayland-proxy-virtwl</a>,
which is itself inspired by
<a href="https://chromium.googlesource.com/chromiumos/platform2/+/main/vm_tools/sommelier/" rel="nofollow">sommelier</a>.
xwayland-xdg-shell was primarily written (instead of just using
xwayland-proxy-virtwl) so as to share a common design/codebase with wprs and to
make use of common wayland development in the form of Smithay and its wayland
crates. Additionally, xwayland-xdg-shell is more narrowly focused and its sole
purpose is xwayland support, not virtio-gpu or virtwl.</p>
<p dir="auto">Like xwayland-proxy-virtwl, xwayland-xdg-proxy can be used to implement external
xwayland support for any wayland compositor instead of re-implementing it inside
the compositor. Aside from eliminating the need to implement xwayland support in
every compositor, this approach has been reported to result in better xwayland
scaling than native xwayland support in some compositor, and it allows xwayland
applications to be treated more like regular wayland applications instead of
getting special access.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Security</h3><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<p dir="auto">wprsd is a wayland compositor, so it has access to all surfaces displayed by
applications running against it and it can inject input into them. Any process
which implements the wprs protocol and connects to the wprs socket will have the
same access. For that reason, the wprs socket is created in a directory which
only the user has access to ($XDG_RUNTIME_DIR) and the socket itself is only
readable/writable by the user. Malicious applications running as the same user
as wprsd can still access this socket, but at that point you have bigger
problems.</p>
<p dir="auto">wprs does not do any auth of its own, it relies entirely on whatever transport
is being used (ssh, in the default case).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Thanks</h2><a id="user-content-thanks" aria-label="Permalink: Thanks" href="#thanks"></a></p>
<p dir="auto">Huge thanks to the following excellent projects for making this project
significantly easier than it otherwise would have been:</p>
<ul dir="auto">
<li><a href="https://github.com/Smithay">Smithay</a></li>
<li><a href="https://github.com/rkyv/rkyv">rkyv</a></li>
<li><a href="https://github.com/tokio-rs/tracing">tracing</a></li>
<li><a href="https://github.com/wolfpld/tracy">Tracy</a></li>
</ul>
<p dir="auto">Thanks to <a href="https://gitlab.freedesktop.org/mstoeckl/waypipe" rel="nofollow">Waypipe</a> and
<a href="https://github.com/talex5/wayland-proxy-virtwl#xwayland-support">xwayland-proxy-virtwl</a>
for paving the way in this problem space.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple apologizes for iPad 'Crush' ad that 'missed the mark' (421 pts)]]></title>
            <link>https://www.theverge.com/2024/5/9/24153113/apple-ipad-ad-crushing-apology</link>
            <guid>40313733</guid>
            <pubDate>Thu, 09 May 2024 22:50:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/5/9/24153113/apple-ipad-ad-crushing-apology">https://www.theverge.com/2024/5/9/24153113/apple-ipad-ad-crushing-apology</a>, See on <a href="https://news.ycombinator.com/item?id=40313733">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Apple has apologized after a commercial meant to showcase its <a href="https://www.theverge.com/24151128/apple-ipad-pro-2024-hands-on">brand-new iPad Pro</a> drew widespread criticism among the creative community. In a statement <a href="https://adage.com/article/digital-marketing-ad-tech-news/apple-apologizes-ipad-pro-crushed-ad-it-missed-mark/2559321">provided to <em>Ad Age</em></a>, Tor Myhren, Apple’s vice president of marketing, said the company “missed the mark.”</p><p>“Creativity is in our DNA at Apple, and it’s incredibly important to us to design products that empower creatives all over the world,” Myhren told <em>Ad Age</em>. “Our goal is to always celebrate the myriad of ways users express themselves and bring their ideas to life through iPad. We missed the mark with this video, and we’re sorry.”</p><p>On Tuesday, Apple introduced the <a href="https://www.theverge.com/24151128/apple-ipad-pro-2024-hands-on">M4-powered iPad Pro</a>, which the company described as its thinnest product ever. To advertise all the creative possibilities with the iPad, it released a “Crush!” commercial that shows things like a piano, record player, paint, and other works flattening under the pressure of a hydraulic press. At the end, only one thing remains: an iPad Pro.</p><p>The ad rubbed some creatives the wrong way. Hugh Grant <a href="https://twitter.com/HackedOffHugh/status/1788183871504204257">called it</a> a “destruction of human experience,” while <em>Handmaid’s Tale</em> director Reed Morano <a href="https://twitter.com/reedmorano/status/1788298509780685261">told Apple CEO</a> Tim Cook to “read the room” in a post on X. Apple didn’t immediately respond to <em>The Verge</em>’s request for comment.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The world has probably passed peak pollution (173 pts)]]></title>
            <link>https://www.sustainabilitybynumbers.com/p/peak-pollution</link>
            <guid>40313451</guid>
            <pubDate>Thu, 09 May 2024 22:11:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sustainabilitybynumbers.com/p/peak-pollution">https://www.sustainabilitybynumbers.com/p/peak-pollution</a>, See on <a href="https://news.ycombinator.com/item?id=40313451">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>The health impacts of air pollution are often underrated. There are a </span><a href="https://ourworldindata.org/data-review-air-pollution-deaths" rel="">range of estimates</a><span> for how many people die prematurely from local air pollution every year.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-144199988" href="https://www.sustainabilitybynumbers.com/p/peak-pollution#footnote-1-144199988" target="_self" rel="">1</a></span><span> All are in the low millions. The World Health Organization </span><a href="https://www.who.int/health-topics/air-pollution" rel="">estimates around</a><span> 7 million.</span></p><p>The good news, then, is that the world is probably passed “peak pollution”. I say “probably” because confidently declaring a peak is, apparently, the best way to make sure it doesn’t happen.</p><p><span>Here, I’m talking specifically about emissions of harmful </span><em>local</em><span> air pollutants: gases like nitrogen oxides (NOx), sulphur dioxide which causes acid rain, carbon monoxide, black carbon, organic carbon, non-methane volatile organic compounds. I’m not talking about greenhouse gases.</span></p><p><span>The </span><a href="https://github.com/JGCRI/CEDS/tree/master" rel="">Community Emissions Data System (CEDS)</a><span> recently extended its long-term dataset on emissions of air pollutants up to the end of 2022.</span></p><p><span>I updated this data in our explorer tool </span><a href="https://ourworldindata.org/explorers/air-pollution" rel="">on Our World in Data</a><span> (where you can explore the trends by country).</span></p><p>What’s striking is that emissions appear to have peaked for all of these pollutants, with the exception of ammonia, which is almost entirely produced by agriculture. Organic carbon and NMVOCs are not quite out of the clear yet, but might not reach their previous peaks again.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png" width="1456" height="1028" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1028,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Of course, emissions are not falling everywhere. They’ve fallen steeply in richer countries like the US and much of Europe. And the big turning point for the global figures has been the rapid turnaround in China. Emissions have declined rapidly in the last decade, with huge gains for public health.</p><p><span>It’s in low and lower-middle income countries where emissions are still rising, and pollution levels in cities are the highest. This is not surprising: air pollution is one of the few areas where the “</span><a href="https://en.wikipedia.org/wiki/Kuznets_curve#Environmental_Kuznets_curve" rel="">Environmental Kuznets Curve</a><span>” tells a pretty accurate and consistent story.</span></p><p>Air pollution increases as countries develop, gain access to energy, and industrialise. They then fall once a country gets rich enough to impose pollution standards and limits without infringing on development and the move away from energy poverty.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png" width="1456" height="1028" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1028,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The goal now is to see if countries can move through this curve much faster – and with lower levels of pollution – than countries like the US or the UK did. This should be doable: we’ve learned a lot over the last 50 years about how to produce energy with less pollution, what technologies work and don’t work, and have reduced the costs of solutions that were expensive in their early days.</p><p><span>Note that this is not a finger-pointing exercise where rich countries tell poorer ones not to pollute. We’re mostly talking about </span><em>local</em><span> air pollution. The negative impacts of pollution are felt by domestic populations. It’s about how we ensure that the poorest countries can gain access to energy, alleviate poverty, and develop while limiting the number of people who die prematurely from air pollution in the process.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cubic millimetre of brain mapped in spectacular detail (164 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-024-01387-9</link>
            <guid>40313193</guid>
            <pubDate>Thu, 09 May 2024 21:36:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-024-01387-9">https://www.nature.com/articles/d41586-024-01387-9</a>, See on <a href="https://news.ycombinator.com/item?id=40313193">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>Researchers have mapped a tiny piece of the human brain in astonishing detail. The resulting cell atlas, which was described today in <i>Science</i><sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup> and is <a href="https://h01-release.storage.googleapis.com/data.html" data-track="click" data-label="https://h01-release.storage.googleapis.com/data.html" data-track-category="body text link">available online</a>, reveals new patterns of connections between brain cells called neurons, as well as cells that wrap around themselves to form knots, and pairs of neurons that are almost mirror images of each other.</p><p>The 3D map covers a volume of about one cubic millimetre, one-millionth of a whole brain, and contains roughly 57,000 cells and 150 million synapses — the connections between neurons. It incorporates a colossal 1.4 petabytes of data. “It’s a little bit humbling,” says Viren Jain, a neuroscientist at Google in Mountain View, California, and a co-author of the paper. “How are we ever going to really come to terms with all this complexity?”</p><h2>Slivers of brain</h2><p>The brain fragment was taken from a 45-year-old woman when she underwent surgery to treat her epilepsy. It came from the cortex, a part of the brain involved in learning, problem-solving and processing sensory signals. The sample was immersed in preservatives and stained with heavy metals to make the cells easier to see. Neuroscientist Jeff Lichtman at Harvard University in Cambridge, Massachusetts, and his colleagues then cut the sample into around 5,000 slices — each just 34 nanometres thick — that could be imaged using electron microscopes.</p><p>Jain’s team then built artificial-intelligence models that were able to stitch the microscope images together to reconstruct the whole sample in 3D. “I remember this moment, going into the map and looking at one individual synapse from this woman’s brain, and then zooming out into these other millions of pixels,” says Jain. “It felt sort of spiritual.”</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-01387-9/d41586-024-01387-9_27068610.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-01387-9/d41586-024-01387-9_27068610.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Rendering of a neuron with a round base and many branches, on a black background." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-01387-9/d41586-024-01387-9_27068610.jpg">
  <figcaption>
   <p><span>A single neuron (white) shown with 5,600 of the axons (blue) that connect to it. The synapses that make these connections are shown in green.</span><span>Credit: Google Research &amp; Lichtman Lab (Harvard University). Renderings by D. Berger (Harvard University)</span></p>
  </figcaption>
 </picture>
</figure><p>When examining the model in detail, the researchers discovered unconventional neurons, including some that made up to 50 connections with each other. “In general, you would find a couple of connections at most between two neurons,” says Jain. Elsewhere, the model showed neurons with tendrils that formed knots around themselves. “Nobody had seen anything like this before,” Jain adds.</p><p>The team also found pairs of neurons that were near-perfect mirror images of each other. “We found two groups that would send their dendrites in two different directions, and sometimes there was a kind of mirror symmetry,” Jain says. It is unclear what role these features have in the brain.</p><h2>Proofreaders needed</h2><p>The map is so large that most of it has yet to be manually checked, and it could still contain errors created by the process of stitching so many images together. “Hundreds of cells have been ‘proofread’, but that’s obviously a few per cent of the 50,000 cells in there,” says Jain. He hopes that others will help to proofread parts of the map they are interested in. The team plans to produce similar maps of brain samples from other people — but a map of the entire brain is unlikely in the next few decades, he says.</p><p>“This paper is really the tour de force creation of a human cortex data set,” says Hongkui Zeng, director of the Allen Institute for Brain Science in Seattle. The vast amount of data that has been made freely accessible will “allow the community to look deeper into the micro-circuitry in the human cortex”, she adds.</p><p>Gaining a deeper understanding of how the cortex works could offer clues about how to treat some psychiatric and neurodegenerative diseases. “This map provides unprecedented details that can unveil new rules of neural connections and help to decipher the inner working of the human brain,” says Yongsoo Kim, a neuroscientist at Pennsylvania State University in Hershey.</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sioyek is a PDF viewer with a focus on textbooks and research papers (288 pts)]]></title>
            <link>https://github.com/ahrm/sioyek</link>
            <guid>40313143</guid>
            <pubDate>Thu, 09 May 2024 21:28:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ahrm/sioyek">https://github.com/ahrm/sioyek</a>, See on <a href="https://news.ycombinator.com/item?id=40313143">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Sioyek</h2><a id="user-content-sioyek" aria-label="Permalink: Sioyek" href="#sioyek"></a></p>
<p dir="auto">Sioyek is a PDF viewer with a focus on textbooks and research papers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contents</h2><a id="user-content-contents" aria-label="Permalink: Contents" href="#contents"></a></p>
<ul dir="auto">
<li><a href="#install">Installation</a></li>
<li><a href="#documentation">Documentation</a></li>
<li><a href="#feature-video-overview">Video Demo</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#build-instructions">Build Instructions</a></li>
<li><a href="#donation">Buy Me a Coffee (or a Book!)</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Official packages</h3><a id="user-content-official-packages" aria-label="Permalink: Official packages" href="#official-packages"></a></p>
<p dir="auto">There are installers for Windows, macOS and Linux. See <a href="https://github.com/ahrm/sioyek/releases">Releases page</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Homebew Cask</h3><a id="user-content-homebew-cask" aria-label="Permalink: Homebew Cask" href="#homebew-cask"></a></p>
<p dir="auto">There is a homebrew cask available here: <a href="https://formulae.brew.sh/cask/sioyek" rel="nofollow">https://formulae.brew.sh/cask/sioyek</a>. Install by running:</p>
<div data-snippet-clipboard-copy-content="brew install --cask sioyek"><pre><code>brew install --cask sioyek
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Third-party packages for Linux</h3><a id="user-content-third-party-packages-for-linux" aria-label="Permalink: Third-party packages for Linux" href="#third-party-packages-for-linux"></a></p>
<p dir="auto">If you prefer to install sioyek with a package manager, you can look at this list. Please note that they are provided by third party packagers. USE AT YOUR OWN RISK! If you're reporting a bug for a third-party package, please mention which package you're using.</p>
<table>
<thead>
<tr>
<th>Distro</th>
<th>Link</th>
<th>Maintainer</th>
</tr>
</thead>
<tbody>
<tr>
<td>Flathub</td>
<td><a href="https://flathub.org/apps/details/com.github.ahrm.sioyek" rel="nofollow">sioyek</a></td>
<td><a href="https://flathub.org/apps/details/com.github.ahrm.sioyek" rel="nofollow">@nbenitez</a></td>
</tr>
<tr>
<td>Alpine</td>
<td><a href="https://pkgs.alpinelinux.org/packages?name=sioyek" rel="nofollow">sioyek</a></td>
<td><a href="https://github.com/jirutka">@jirutka</a></td>
</tr>
<tr>
<td>Arch</td>
<td><a href="https://aur.archlinux.org/packages/sioyek" rel="nofollow">AUR sioyek</a></td>
<td><a href="https://github.com/goggle">@goggle</a></td>
</tr>
<tr>
<td>Arch</td>
<td><a href="https://aur.archlinux.org/packages/sioyek-git/" rel="nofollow">AUR sioyek-git</a></td>
<td><a href="https://github.com/hrdl-github">@hrdl-github</a></td>
</tr>
<tr>
<td>Arch</td>
<td><a href="https://aur.archlinux.org/packages/sioyek-appimage/" rel="nofollow">AUR sioyek-appimage</a></td>
<td><a href="https://github.com/DhruvaSambrani">@DhruvaSambrani</a></td>
</tr>
<tr>
<td>Debian</td>
<td><a href="https://packages.debian.org/sioyek" rel="nofollow">sioyek</a></td>
<td><a href="https://github.com/viccie30">@viccie30</a></td>
</tr>
<tr>
<td>NixOS</td>
<td><a href="https://search.nixos.org/packages?channel=unstable&amp;show=sioyek&amp;from=0&amp;size=50&amp;sort=relevance&amp;type=packages&amp;query=sioyek" rel="nofollow">sioyek</a></td>
<td><a href="https://github.com/podocarp">@podocarp</a></td>
</tr>
<tr>
<td>openSUSE</td>
<td><a href="https://build.opensuse.org/package/show/Publishing/sioyek" rel="nofollow">Publishing</a></td>
<td><a href="https://github.com/uncomfyhalomacro">@uncomfyhalomacro</a></td>
</tr>
<tr>
<td>openSUSE</td>
<td><a href="https://build.opensuse.org/package/show/openSUSE:Factory/sioyek" rel="nofollow">Factory</a></td>
<td><a href="https://github.com/uncomfyhalomacro">@uncomfyhalomacro</a></td>
</tr>
<tr>
<td>Ubuntu</td>
<td><a href="https://packages.ubuntu.com/sioyek" rel="nofollow">sioyek</a></td>
<td><a href="https://github.com/viccie30">@viccie30</a></td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">You can view the official documentation <a href="https://sioyek-documentation.readthedocs.io/en/latest/" rel="nofollow">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Feature Video Overview</h2><a id="user-content-feature-video-overview" aria-label="Permalink: Feature Video Overview" href="#feature-video-overview"></a></p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=yTmCI0Xp5vI" rel="nofollow"><img src="https://camo.githubusercontent.com/3bbdb00658bc336a5da25c541e8141b7d38738232eba44310c960639633bb396/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f79546d43493058703576492f302e6a7067" alt="Sioyek feature overview" data-canonical-src="https://img.youtube.com/vi/yTmCI0Xp5vI/0.jpg"></a></p>
<p dir="auto">For a more in-depth tutorial, see this video:</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=RaHRvnb0dY8" rel="nofollow"><img src="https://camo.githubusercontent.com/b1817153034c5bf93e9b14a37292ae3bf2e949d900169928f9720ece7832d452/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f52614852766e62306459382f302e6a7067" alt="Sioyek Tutorial" data-canonical-src="https://img.youtube.com/vi/RaHRvnb0dY8/0.jpg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quick Open</h3><a id="user-content-quick-open" aria-label="Permalink: Quick Open" href="#quick-open"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description recent_docs.mp4">recent_docs.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125321111-9b29dc00-e351-11eb-873e-94ea30016a05.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTExMS05YjI5ZGMwMC1lMzUxLTExZWItODczZS05NGVhMzAwMTZhMDUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9M2I5NGE4NzVlMDQ4NjA4YjRmZjFkNzUzODVkZWVmMzMwNTYwZDU2Y2Y2ZmZjNWM3ZjUzNmM4MjExNzg0OWI0YiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.CgEbCL-dJokkAf-td825PN-E02l_-7J2M1w6T2cQgIo" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125321111-9b29dc00-e351-11eb-873e-94ea30016a05.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTExMS05YjI5ZGMwMC1lMzUxLTExZWItODczZS05NGVhMzAwMTZhMDUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9M2I5NGE4NzVlMDQ4NjA4YjRmZjFkNzUzODVkZWVmMzMwNTYwZDU2Y2Y2ZmZjNWM3ZjUzNmM4MjExNzg0OWI0YiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.CgEbCL-dJokkAf-td825PN-E02l_-7J2M1w6T2cQgIo" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">You can quickly search and open any file you have previously interacted with using sioyek.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Table of Contents</h3><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description toc.mp4">toc.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125321313-cf050180-e351-11eb-9275-c2759c684af5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTMxMy1jZjA1MDE4MC1lMzUxLTExZWItOTI3NS1jMjc1OWM2ODRhZjUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OTZhOGM4NmVhNGQyOWE4MTZjYTllMjhkZDBiN2FlOGUwOGU0MjlkYmVmODk1NGJhMmZiODA1NTM3MWIyMDBmYiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.e26EJM2s5-akKXn_7bEAC91YYYBtavP0lVelx94xGT8" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125321313-cf050180-e351-11eb-9275-c2759c684af5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTMxMy1jZjA1MDE4MC1lMzUxLTExZWItOTI3NS1jMjc1OWM2ODRhZjUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OTZhOGM4NmVhNGQyOWE4MTZjYTllMjhkZDBiN2FlOGUwOGU0MjlkYmVmODk1NGJhMmZiODA1NTM3MWIyMDBmYiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.e26EJM2s5-akKXn_7bEAC91YYYBtavP0lVelx94xGT8" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">You can search and jump to table of contents entries.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Smart Jump</h3><a id="user-content-smart-jump" aria-label="Permalink: Smart Jump" href="#smart-jump"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description jump.mp4">jump.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125321419-e5ab5880-e351-11eb-9688-95374a22774f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTQxOS1lNWFiNTg4MC1lMzUxLTExZWItOTY4OC05NTM3NGEyMjc3NGYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YzhjYjZjZjQ1ZGU5OTlhNTFkZjU1NjJiNjk1ZjIyYWYzMTA2M2EyNDBmNzE3M2FlZjZjMDJiMzhlNDQxNDlmOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.mcFpCPbTqmL1-BVLl5ZJ4LzElUkQRX-sk34sPf8c3OY" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125321419-e5ab5880-e351-11eb-9688-95374a22774f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTQxOS1lNWFiNTg4MC1lMzUxLTExZWItOTY4OC05NTM3NGEyMjc3NGYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YzhjYjZjZjQ1ZGU5OTlhNTFkZjU1NjJiNjk1ZjIyYWYzMTA2M2EyNDBmNzE3M2FlZjZjMDJiMzhlNDQxNDlmOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.mcFpCPbTqmL1-BVLl5ZJ4LzElUkQRX-sk34sPf8c3OY" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">You can jump to any referenced figure or bibliography item <em>even if the PDF file doesn't provide links</em>. You can also search the names of bibliography items in google scholar/libgen by middle clicking/shift+middle clicking on their name.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Overview</h3><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description overview.mp4">overview.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/154683015-0bae4f92-78e2-4141-8446-49dd7c2bd7c9.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzE1NDY4MzAxNS0wYmFlNGY5Mi03OGUyLTQxNDEtODQ0Ni00OWRkN2MyYmQ3YzkubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGJhYTQ1ZmI0M2JiMDU4MDZmMDY0ZGMxMjZiNjg4NWE0OGNlMzJmMGNjYzk0NzgxOTBhYzVhZTJhMzFjYTIxMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.73JERWG0v6CTuvhWn5BzMFMGTGWOqNXl7OgjBWog3Cg" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/154683015-0bae4f92-78e2-4141-8446-49dd7c2bd7c9.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzE1NDY4MzAxNS0wYmFlNGY5Mi03OGUyLTQxNDEtODQ0Ni00OWRkN2MyYmQ3YzkubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGJhYTQ1ZmI0M2JiMDU4MDZmMDY0ZGMxMjZiNjg4NWE0OGNlMzJmMGNjYzk0NzgxOTBhYzVhZTJhMzFjYTIxMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.73JERWG0v6CTuvhWn5BzMFMGTGWOqNXl7OgjBWog3Cg" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">You can open a quick overview of figures/references/tables/etc. by right clicking on them (Like Smart Jump, this feature works even if the document doesn't provide links).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mark</h3><a id="user-content-mark" aria-label="Permalink: Mark" href="#mark"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description mark.mp4">mark.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125321811-505c9400-e352-11eb-85e0-ffc3ae5f8cb8.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTgxMS01MDVjOTQwMC1lMzUyLTExZWItODVlMC1mZmMzYWU1ZjhjYjgubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MzlhZjA5YzFlOGYwNjIyMTUwMTY2YmU5ZTdmM2JhOWRiYWY5NjY0ZDljNWJjZTc0OGRhNTY5NjI0YmRjODEyNSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.AmelCLzLeOa-CMTR1mZdOh2LpzR787V2c46m0RFUDRI" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125321811-505c9400-e352-11eb-85e0-ffc3ae5f8cb8.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTgxMS01MDVjOTQwMC1lMzUyLTExZWItODVlMC1mZmMzYWU1ZjhjYjgubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MzlhZjA5YzFlOGYwNjIyMTUwMTY2YmU5ZTdmM2JhOWRiYWY5NjY0ZDljNWJjZTc0OGRhNTY5NjI0YmRjODEyNSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.AmelCLzLeOa-CMTR1mZdOh2LpzR787V2c46m0RFUDRI" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Sometimes when reading a document you need to go back a few pages (perhaps to view a definition or something) and quickly jump back to where you were. You can achieve this by using marks. Marks are named locations within a PDF file (each mark has a single character name for example 'a' or 'm') which you can quickly jump to using their name. In the aforementioned example, before going back to the definition you mark your location and later jump back to the mark by invoking its name. Lower case marks are local to the document and upper case marks are global (this should be very familiar to you if you have used vim).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bookmarks</h3><a id="user-content-bookmarks" aria-label="Permalink: Bookmarks" href="#bookmarks"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description bookmarks.mp4">bookmarks.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125322503-1a6bdf80-e353-11eb-8018-5e8fc43b8d05.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMjUwMy0xYTZiZGY4MC1lMzUzLTExZWItODAxOC01ZThmYzQzYjhkMDUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjcyZGI1Y2QzMzdlZTEwMTJkMzAyZjE5ZmVhZTEwZjkyNjY4N2YwNzBiMjdhMGFjMTA4MzA3ODhjNjM5YTM4ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.eXA1H8apV3pXKK-90BZi2Y5BkjkJw0VeEWVkzYQ6tQ4" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125322503-1a6bdf80-e353-11eb-8018-5e8fc43b8d05.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMjUwMy0xYTZiZGY4MC1lMzUzLTExZWItODAxOC01ZThmYzQzYjhkMDUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjcyZGI1Y2QzMzdlZTEwMTJkMzAyZjE5ZmVhZTEwZjkyNjY4N2YwNzBiMjdhMGFjMTA4MzA3ODhjNjM5YTM4ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.eXA1H8apV3pXKK-90BZi2Y5BkjkJw0VeEWVkzYQ6tQ4" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Bookmarks are similar to marks except they are named by a text string and they are all global.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Highlights</h3><a id="user-content-highlights" aria-label="Permalink: Highlights" href="#highlights"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description highlights.mp4">highlights.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/130956728-7e0a87fa-4ada-4108-a8fc-9d9d04180f56.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEzMDk1NjcyOC03ZTBhODdmYS00YWRhLTQxMDgtYThmYy05ZDlkMDQxODBmNTYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTk0ZThiZTZiZmU0ZjhhMzhiOTcwOGFlY2FmMzEwZTU0YmQ4MGFmYTgxYjBlY2E0MTlkYjNkNDViZThhZDI2MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.HbkEAIDP6hnEgafwmccUAEOeyipy5fPzhHHkKfiAaGo" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/130956728-7e0a87fa-4ada-4108-a8fc-9d9d04180f56.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEzMDk1NjcyOC03ZTBhODdmYS00YWRhLTQxMDgtYThmYy05ZDlkMDQxODBmNTYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTk0ZThiZTZiZmU0ZjhhMzhiOTcwOGFlY2FmMzEwZTU0YmQ4MGFmYTgxYjBlY2E0MTlkYjNkNDViZThhZDI2MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.HbkEAIDP6hnEgafwmccUAEOeyipy5fPzhHHkKfiAaGo" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Highlight text using different kinds of highlights. You can search among all the highlights.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Portals (this feature is most useful for users with multiple monitors)</h3><a id="user-content-portals-this-feature-is-most-useful-for-users-with-multiple-monitors" aria-label="Permalink: Portals (this feature is most useful for users with multiple monitors)" href="#portals-this-feature-is-most-useful-for-users-with-multiple-monitors"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description portal.mp4">portal.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125322657-41c2ac80-e353-11eb-985e-8f3ce9808f67.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMjY1Ny00MWMyYWM4MC1lMzUzLTExZWItOTg1ZS04ZjNjZTk4MDhmNjcubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGMxZWU2NTNmYzdiN2NmODI1ZmQzNDcwN2E4OTUwNmY2ZmEyN2NmNDFiM2IwOTQyMjQ2NTEwZjhiZmFhMGMwYyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.tnn-Y723I7iEw-gZjhO75Rwk5mFab8XLd-BKNDCrQEE" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125322657-41c2ac80-e353-11eb-985e-8f3ce9808f67.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMjY1Ny00MWMyYWM4MC1lMzUzLTExZWItOTg1ZS04ZjNjZTk4MDhmNjcubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGMxZWU2NTNmYzdiN2NmODI1ZmQzNDcwN2E4OTUwNmY2ZmEyN2NmNDFiM2IwOTQyMjQ2NTEwZjhiZmFhMGMwYyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.tnn-Y723I7iEw-gZjhO75Rwk5mFab8XLd-BKNDCrQEE" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Suppose you are reading a paragraph which references a figure which is not very close to the current location. Jumping back and forth between the current paragraph and the figure can be very annoying. Using portals, you can link the paragraph's location to the figure's location. Sioyek shows the closest portal destination in a separate window (which is usually placed on a second monitor). This window is automatically updated to show the closest portal destination as the user navigates the document.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description config.mp4">config.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125337160-e4832700-e363-11eb-8801-0bee58121c2d.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMzNzE2MC1lNDgzMjcwMC1lMzYzLTExZWItODgwMS0wYmVlNTgxMjFjMmQubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NGE3NmViNjU3YjQxZTQ3ZTYzODEzNmY3NDIxMTE4ZmE3YTlmOWZkYjY4MThiZTc2YzJhYjNlZDFmYzZjY2I5NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.ruxUPlLfOjuotsHBJRfFgE5VcBmMNTjluhNgVTLDRDU" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125337160-e4832700-e363-11eb-8801-0bee58121c2d.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMzNzE2MC1lNDgzMjcwMC1lMzYzLTExZWItODgwMS0wYmVlNTgxMjFjMmQubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NGE3NmViNjU3YjQxZTQ3ZTYzODEzNmY3NDIxMTE4ZmE3YTlmOWZkYjY4MThiZTc2YzJhYjNlZDFmYzZjY2I5NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.ruxUPlLfOjuotsHBJRfFgE5VcBmMNTjluhNgVTLDRDU" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">You can customize all key bindings and some UI elements by editing <code>keys_user.config</code> and <code>prefs_user.config</code>. The default configurations are in <code>keys.config</code> and <code>prefs.config</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build Instructions</h2><a id="user-content-build-instructions" aria-label="Permalink: Build Instructions" href="#build-instructions"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Linux</h3><a id="user-content-linux" aria-label="Permalink: Linux" href="#linux"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Fedora</h4><a id="user-content-fedora" aria-label="Permalink: Fedora" href="#fedora"></a></p>
<p dir="auto">Run the following commands to install dependencies, clone the repository and compile sioyek on Fedora (tested on Fedora Workstation 36).</p>
<div data-snippet-clipboard-copy-content="sudo dnf install qt5-qtbase-devel qt5-qtbase-static qt5-qt3d-devel harfbuzz-devel
git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
./build_linux.sh"><pre><code>sudo dnf install qt5-qtbase-devel qt5-qtbase-static qt5-qt3d-devel harfbuzz-devel
git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
./build_linux.sh
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Generic distribution</h4><a id="user-content-generic-distribution" aria-label="Permalink: Generic distribution" href="#generic-distribution"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Install Qt 5 and make sure <code>qmake</code> is in <code>PATH</code>.</p>
<p dir="auto">Run <code>qmake --version</code> to make sure the <code>qmake</code> in path is using Qt 5.x.</p>
</li>
<li>
<p dir="auto">Install <code>libharfbuzz</code>:</p>
</li>
</ol>
<div data-snippet-clipboard-copy-content="sudo apt install libharfbuzz-dev"><pre><code>sudo apt install libharfbuzz-dev
</code></pre></div>
<ol start="3" dir="auto">
<li>Clone the repository and build:</li>
</ol>
<div data-snippet-clipboard-copy-content="git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
./build_linux.sh"><pre><code>git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
./build_linux.sh
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows</h3><a id="user-content-windows" aria-label="Permalink: Windows" href="#windows"></a></p>
<ol dir="auto">
<li>Install Visual Studio (tested on 2019, other relatively recent versions should work too)</li>
<li>Install Qt 5 and make sure qmake is in <code>PATH</code>.</li>
<li>Clone the repository and build using 64 bit Visual Studio Developer Command Prompt:</li>
</ol>
<div data-snippet-clipboard-copy-content="git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
build_windows.bat"><pre><code>git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
build_windows.bat
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mac</h3><a id="user-content-mac" aria-label="Permalink: Mac" href="#mac"></a></p>
<ol dir="auto">
<li>Install Xcode.</li>
<li>Clone the repository and build: (The code below is in Zsh, which is the default shell on macOS.)</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="(
setopt PIPE_FAIL PRINT_EXIT_VALUE ERR_RETURN SOURCE_TRACE XTRACE

git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
chmod +x build_mac.sh

brew install 'qt@5' freeglut mesa harfbuzz

export PATH=&quot;/opt/homebrew/opt/qt@5/bin:$PATH&quot;
#: The above is needed to make =qmake= from =qt= be found.
#: Find the path using =brew info 'qt@5'=.

MAKE_PARALLEL=8 ./build_mac.sh

mv build/sioyek.app /Applications/
sudo codesign --force --sign - --deep /Applications/sioyek.app
)"><pre>(
setopt PIPE_FAIL PRINT_EXIT_VALUE ERR_RETURN SOURCE_TRACE XTRACE

git clone --recursive https://github.com/ahrm/sioyek
<span>cd</span> sioyek
chmod +x build_mac.sh

brew install <span><span>'</span>qt@5<span>'</span></span> freeglut mesa harfbuzz

<span>export</span> PATH=<span><span>"</span>/opt/homebrew/opt/qt@5/bin:<span>$PATH</span><span>"</span></span>
<span><span>#</span>: The above is needed to make =qmake= from =qt= be found.</span>
<span><span>#</span>: Find the path using =brew info 'qt@5'=.</span>

MAKE_PARALLEL=8 ./build_mac.sh

mv build/sioyek.app /Applications/
sudo codesign --force --sign - --deep /Applications/sioyek.app
)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Donation</h2><a id="user-content-donation" aria-label="Permalink: Donation" href="#donation"></a></p>
<p dir="auto">If you enjoy sioyek, please consider donating to support its development.</p>
<p dir="auto"><a href="https://www.buymeacoffee.com/ahrm" rel="nofollow"><img src="https://camo.githubusercontent.com/4412aa44a78a18c03862fd7da2de5bd81e3817a3adec90fdd41671170a206abd/68747470733a2f2f63646e2e6275796d6561636f666665652e636f6d2f627574746f6e732f64656661756c742d6f72616e67652e706e67" alt="Buy Me A Coffee" height="41" width="174" data-canonical-src="https://cdn.buymeacoffee.com/buttons/default-orange.png"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How bad are satellite megaconstellations for astronomy? (104 pts)]]></title>
            <link>https://www.leonarddavid.com/blinded-by-the-light-megaconstellation-clash-with-astronomical-peer-groups/</link>
            <guid>40312469</guid>
            <pubDate>Thu, 09 May 2024 20:11:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.leonarddavid.com/blinded-by-the-light-megaconstellation-clash-with-astronomical-peer-groups/">https://www.leonarddavid.com/blinded-by-the-light-megaconstellation-clash-with-astronomical-peer-groups/</a>, See on <a href="https://news.ycombinator.com/item?id=40312469">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<div id="attachment_41665"><p><a href="https://www.leonarddavid.com/wp-content/uploads/2024/05/wait-a-minute-elevator-leonard-scaled.jpg"><img fetchpriority="high" decoding="async" aria-describedby="caption-attachment-41665" src="https://www.leonarddavid.com/wp-content/uploads/2024/05/wait-a-minute-elevator-leonard-263x350.jpg" alt="" width="263" height="350" srcset="https://www.leonarddavid.com/wp-content/uploads/2024/05/wait-a-minute-elevator-leonard-263x350.jpg 263w, https://www.leonarddavid.com/wp-content/uploads/2024/05/wait-a-minute-elevator-leonard-768x1024.jpg 768w, https://www.leonarddavid.com/wp-content/uploads/2024/05/wait-a-minute-elevator-leonard-150x200.jpg 150w, https://www.leonarddavid.com/wp-content/uploads/2024/05/wait-a-minute-elevator-leonard-1152x1536.jpg 1152w, https://www.leonarddavid.com/wp-content/uploads/2024/05/wait-a-minute-elevator-leonard-1536x2048.jpg 1536w, https://www.leonarddavid.com/wp-content/uploads/2024/05/wait-a-minute-elevator-leonard-scaled.jpg 1920w" sizes="(max-width: 263px) 100vw, 263px"></a></p><p id="caption-attachment-41665">Wait a minute.<br>Image credit: Barbara David</p></div>
<p>Over the last number of years, our planet has become encircled by Starlink, OneWeb, and other “megaconstellation” satellites.</p>
<p>Yes, the emergence of those megaconstellations offers great benefit for humanity. But in a wait-a-minute pause, there are also substantial costs, including the imposition on humankind’s ongoing and growing thirst for astronomical peering into the surrounding universe.</p>
<p>That’s the view of David Koplow, the Scott K. Ginsburg Professor of Law at Georgetown University Law Center in Washington, D.C.</p>
<div id="attachment_31178"><p><a href="https://www.leonarddavid.com/wp-content/uploads/2022/02/starlink-2.jpg"><img decoding="async" aria-describedby="caption-attachment-31178" src="https://www.leonarddavid.com/wp-content/uploads/2022/02/starlink-2-350x233.jpg" alt="" width="350" height="233" srcset="https://www.leonarddavid.com/wp-content/uploads/2022/02/starlink-2-350x233.jpg 350w, https://www.leonarddavid.com/wp-content/uploads/2022/02/starlink-2-1024x682.jpg 1024w, https://www.leonarddavid.com/wp-content/uploads/2022/02/starlink-2-200x133.jpg 200w, https://www.leonarddavid.com/wp-content/uploads/2022/02/starlink-2-768x512.jpg 768w, https://www.leonarddavid.com/wp-content/uploads/2022/02/starlink-2.jpg 1280w" sizes="(max-width: 350px) 100vw, 350px"></a></p><p id="caption-attachment-31178">Starlink constellation pass overhead near Carson National Forest, New Mexico, photographed soon after launch. &nbsp;<br>SpaceX Starlink Satellites over Carson National Forest, New Mexico, photographed soon after launch.<br>Credit: Mike Lewinsky/Creative Commons Attribution 2.0</p></div>
<p>“We are just beginning to appreciate how bad the disruption can be for land-based and space-based telescopes, and as more and more satellite overflights occur, the problems will only intensify,” Koplow told <em>Inside Outer Space</em>.</p>
<p><strong>Legal rights</strong></p>
<p>Koplow’s concerns have been voiced in several scholarly works, the titles of which underscore his qualms, such as: “<em>Large Constellations of Small Satellites: The Good, the Bad, the Ugly and the Illegal</em>,” as well as “<em>Blinded by the Light: Resolving the Conflict Between Satellite Megaconstellations and Astronomy</em>.”</p>
<div id="attachment_22571"><p><a href="https://www.leonarddavid.com/wp-content/uploads/2020/02/starlink-n.jpg"><img decoding="async" aria-describedby="caption-attachment-22571" src="https://www.leonarddavid.com/wp-content/uploads/2020/02/starlink-n-350x240.jpg" alt="" width="350" height="240" srcset="https://www.leonarddavid.com/wp-content/uploads/2020/02/starlink-n-350x240.jpg 350w, https://www.leonarddavid.com/wp-content/uploads/2020/02/starlink-n-1024x703.jpg 1024w, https://www.leonarddavid.com/wp-content/uploads/2020/02/starlink-n-200x137.jpg 200w, https://www.leonarddavid.com/wp-content/uploads/2020/02/starlink-n-768x527.jpg 768w, https://www.leonarddavid.com/wp-content/uploads/2020/02/starlink-n.jpg 1138w" sizes="(max-width: 350px) 100vw, 350px"></a></p><p id="caption-attachment-22571">Starlink satellites visible in a mosaic of an astronomical image.<br>Courtesy of NSF’s<br>National Optical-Infrared Astronomy Research Laboratory/NSF/AURA/CTIO/DELVE)</p></div>
<p>&nbsp;“The world has mostly been assuming that the relevant international law basically allows the satellite companies to do whatever they want in space, while forcing the observatories to adapt as well as they can,” Koplow advised.&nbsp;</p>
<p>But in reality, Koplow continues, the legal regime is not so one-sided. “Astronomers also have legal rights to free use of space, and they need not stand by idly while their profession is damaged.”</p>
<p><strong>Hair on fire</strong></p>
<p>Koplow points out that in 2019 the world of optical and radio astronomy changed abruptly and massively when the first SpaceX batch of 60 Starlink satellites was lofted.</p>
<p>“Jolted by the sudden brightness of those spacecraft, and alarmed by the prospect of their legions of successors, observatories scrambled to respond,” Koplow observes.</p>
<p>They did so by studying and documenting the true dimensions of the problem, beginning to invent or conceptualize mitigation measures, and entering into discussions with SpaceX and other companies.</p>
<p>“Some astronomers see this as a true ‘hair on fire’ emergency, heralding irretrievable losses to space science; others present a more sanguine face, depicting this as yet another challenge to be surmounted in surveying a decreasingly pristine sky,” Koplow remarks.</p>
<div id="attachment_36401"><p><a href="https://www.leonarddavid.com/wp-content/uploads/2023/03/ASTRONOMY-IMAGE-ESO-scaled.jpg"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-36401" src="https://www.leonarddavid.com/wp-content/uploads/2023/03/ASTRONOMY-IMAGE-ESO-350x173.jpg" alt="" width="350" height="173" srcset="https://www.leonarddavid.com/wp-content/uploads/2023/03/ASTRONOMY-IMAGE-ESO-350x173.jpg 350w, https://www.leonarddavid.com/wp-content/uploads/2023/03/ASTRONOMY-IMAGE-ESO-1024x507.jpg 1024w, https://www.leonarddavid.com/wp-content/uploads/2023/03/ASTRONOMY-IMAGE-ESO-200x99.jpg 200w, https://www.leonarddavid.com/wp-content/uploads/2023/03/ASTRONOMY-IMAGE-ESO-768x381.jpg 768w, https://www.leonarddavid.com/wp-content/uploads/2023/03/ASTRONOMY-IMAGE-ESO-1536x761.jpg 1536w, https://www.leonarddavid.com/wp-content/uploads/2023/03/ASTRONOMY-IMAGE-ESO-2048x1015.jpg 2048w" sizes="(max-width: 350px) 100vw, 350px"></a></p><p id="caption-attachment-36401">Image credit: ESO/P. Horálek</p></div>
<p><strong>Incipient clash</strong></p>
<p>That said, the astronomical community has related that the time and the financial costs of conducting effective astronomy will rise considerably, Koplow says, “and that some important data will simply be irretrievable, with concomitant losses for science and the future exploration and use of space.”</p>
<p>In his “Blinded by the Light” treatise, Koplow describes the incipient clash between satellite megaconstellations and astronomy, assesses the relevant international and domestic legal authorities, and proposes compromise solutions to mitigate the damage.</p>
<p>“Overall, the thesis is that a better balance must be struck between these competing types of space activities,” Koplow adds, “without ceding to either a comprehensive right to proceed in disregard of the key functions of the other.”</p>
<div id="attachment_12276"><p><a href="https://www.leonarddavid.com/wp-content/uploads/2017/10/oneweb.jpg"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-12276" src="https://www.leonarddavid.com/wp-content/uploads/2017/10/oneweb-350x144.jpg" alt="" width="350" height="144" srcset="https://www.leonarddavid.com/wp-content/uploads/2017/10/oneweb-350x144.jpg 350w, https://www.leonarddavid.com/wp-content/uploads/2017/10/oneweb-200x82.jpg 200w, https://www.leonarddavid.com/wp-content/uploads/2017/10/oneweb-768x316.jpg 768w, https://www.leonarddavid.com/wp-content/uploads/2017/10/oneweb-1024x421.jpg 1024w, https://www.leonarddavid.com/wp-content/uploads/2017/10/oneweb.jpg 1200w" sizes="(max-width: 350px) 100vw, 350px"></a></p><p id="caption-attachment-12276">Credit: OneWeb</p></div>
<p><strong>Voluntary measures</strong></p>
<p>Koplow acknowledges that some satellite companies have voluntarily invested considerable corporate talent and money in efforts to mitigate their interference with astronomy.&nbsp;</p>
<p>“But these voluntary measures are not adequate to solve the problem, they are not durable and reliable, and they have not been adopted by all the companies,” Koplow suggests.</p>
<p>“A stronger response is necessary,” Koplow concludes.</p>
<p>To gain access to “<em>Blinded by the Light: Resolving the Conflict Between Satellite Megaconstellations and Astronomy” </em>go to: <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4346299">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4346299</a></p>
<p><em>To review the paper “Three Things I Hate About Large Constellations of Small Satellites” </em>go to: <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4503593">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4503593</a></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The history of 'OK' (2023) (104 pts)]]></title>
            <link>https://people.howstuffworks.com/history-ok.htm</link>
            <guid>40312434</guid>
            <pubDate>Thu, 09 May 2024 20:07:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://people.howstuffworks.com/history-ok.htm">https://people.howstuffworks.com/history-ok.htm</a>, See on <a href="https://news.ycombinator.com/item?id=40312434">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="editorial-body">

					
	
				
	
																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																									
				


	


	<div id="page0" data-slide="0" data-track-gtm="Content">	
					<div>
<figure>
			
		
	
								
		
		
																																																								
			<picture>
				<source media="(max-width: 320px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjMyMH0sInRvRm9ybWF0IjoiYXZpZiJ9fQ==" type="image/avif"><source media="(min-width: 321px) and (max-width: 599px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjQyMH0sInRvRm9ybWF0IjoiYXZpZiJ9fQ==" type="image/avif"><source media="(min-width: 600px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjgyOH0sInRvRm9ybWF0IjoiYXZpZiJ9fQ==" type="image/avif">
				<source media="(max-width: 320px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjMyMH19fQ=="><source media="(min-width: 321px) and (max-width: 599px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjQyMH19fQ=="><source media="(min-width: 600px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjgyOH19fQ==">
				<img src="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjgyOH19fQ==" alt="OK word" width="828" height="465">
			</picture>
			

		
	

					
	
	<figcaption>
											The spread of "OK" shows how important an all-purpose word can be. <span>Foxys Graphic/Shutterstock</span>
								</figcaption>

	</figure>
	</div>
	





	
		<p>"OK" is probably the <a href="https://www.npr.org/2010/11/20/131390650/ok-how-two-letters-made-america-s-greatest-word">most spoken word</a> in the world — besides English, people say "OK" in a dozen languages, including Spanish, Italian and Russian — and yet almost nobody can tell you what those two letters stand for or where the word came from.</p>

		
	
					
				
				


	
		<p>Was it borrowed from the <a href="https://www.google.com/books/edition/OK/mPhj9DIXCWAC?hl=en&amp;gbpv=1&amp;bsq=okeh">indigenous Choctaw word</a> "okeh," meaning, roughly, "OK"? Did it originate with a Boston baker named Otto Kimmel who liked to <a href="https://www.google.com/books/edition/OK/mPhj9DIXCWAC?hl=en&amp;gbpv=1">frost his initials</a> into his cookies? Does it have anything to do with the state of Oklahoma (OK) or the musical "<a href="https://youtu.be/kGXu0j6QDJ8">Oklahoma!</a>"?</p>

		
	
								

		
	
											

						
				
				


	
		<p>Nope, nope and nope. In fact, there's <a href="https://www.google.com/books/edition/OK/mPhj9DIXCWAC?hl=en&amp;gbpv=1&amp;bsq=okeh">no evidence</a> that "okeh" was part of the Chocktaw language.</p>

		
	
													
				
				


	
		<p>"OK is the greatest American word," says <a href="https://cla.umn.edu/about/directory/profile/aliber">Anatoly Liberman</a>, a linguist, translator and language professor at the University of Minnesota. "The history of OK is a history of incredible success, but nobody could have predicted that success."</p>

		
	
					
				
				


	
		<p>As you'll see, OK began as a piece of insider slang from the late 1830s and rode a (losing) presidential campaign to nationwide fame and eventually worldwide ubiquity.</p>

		
	
					
				
						
						

				
				

		
	




</div>		<div data-track-gtm="TOC">
			<p><strong>Contents</strong></p><ol>
						
					
																
										
					<li>
						<a data-target="pt1" href="#pt1">The Acronym Craze of the 1830s</a>
					</li>
							
					
																
										
					<li>
						<a data-target="pt2" href="#pt2">Misspelling Words Was Also a Thing</a>
					</li>
							
					
																
										
					<li>
						<a data-target="pt3" href="#pt3">The Very First Use of OK</a>
					</li>
							
					
																
										
					<li>
						<a data-target="pt4" href="#pt4">"Old Kinderhook" Takes "OK" National</a>
					</li>
										</ol>
		</div>



		
	<div id="page-wrap1" x-data="{ pageVisible : true }"><h2 data-page-nbr="1" data-logged="false" data-page-url="history-ok1.htm">
			


<span @click="pageVisible = !pageVisible" aria-expanded="true" aria-controls="page1" role="button">

			<svg :class="{ 'rotate-180' : !pageVisible, '' : pageVisible }" xmlns="http://www.w3.org/2000/svg" width="22" height="10" viewBox="0 0 28.396 13.211"><path d="M4398.8,5158.252l13.224,10.507,12.357-10.507" transform="translate(-4397.399 -5156.842)" fill="none" stroke-linecap="round" stroke-width="2"></path></svg>
	
		<span>The Acronym Craze of the 1830s</span>

	
</span>		</h2>

				

<div id="page1" data-slide="1" data-track-gtm="Content" x-show.transition="pageVisible === true">	
						
	





	
		<p>In the early 19th century, new printing technologies dramatically reduced the cost of publishing a daily newspaper, and there was a resulting explosion of inexpensive new dailies known collectively as the <a href="https://blogs.ubc.ca/etec540sept09/2009/10/19/the-rise-of-penny-newspapers-and-their-influence-on-mass-media/">penny press</a>. Competing for readers, penny papers in cities like New York, Philadelphia and Boston published not only straight news stories, but also witty takes on the latest political scandals, social scenes and popular trends.</p>

		
	
					
				
				


	
		<p>Think of it as the internet of the 1830s. And much like the internet, the lively back-and-forth chatter between penny paper editors gave birth to a new way of writing and eventually a new way of speaking.</p>

		
	
								

		
	
										
				
				


	
		<p>"Beginning in the summer of 1838, there developed in Boston a remarkable vogue of using abbreviations. It might well be called a craze," <a href="https://www.jstor.org/stable/453580">wrote</a> the famed etymologist <a href="https://www.nytimes.com/2002/10/18/nyregion/allen-read-96-the-ok-expert-is-dead.html">Allen Walker Read</a>, who was the first person to trace the full history of OK.</p>

		
	
					
				
				


	
		<p>Take these examples from Boston's Morning Post, whose editor, Charles Gordon Greene, sprinkled his columns with winking acronyms for everything and anything:</p>

		
	
					
				
				


	
		<div>
	<ul><li><span>O.F.M. ("our first men")</span></li><li><span>W.O.O.O.F.C. ("with one of our first citizens")</span></li><li><span>R.T.B.S. ("remains to be seen")</span></li><li><span>D.L.E.C. ("do let 'em come")</span></li><li><span>G.T.D.H.D. ("give the devil his due")</span></li><li><span>W.Y.G. ("will you go?")</span></li></ul>
</div>


		
	
		
				
				


	
		<p>By 1939, the "initial language," as it was sometimes called, had arrived in New York City and had already leapt from print to fashionable slang. "This is a species of spoken shorthand, which is getting into very general use among loafers and gentlemen of the fancy," <a href="https://www.jstor.org/stable/453580">wrote the editors</a> of New York's Evening Tattler.</p>

		
	
					
				
				


	
		<p>The editors even claimed to have overheard a conversation between two young sweethearts, where the girl turned to her beau and said, "O.K.K.B.W.P." "What could she have meant," wrote the Evening Tattler, "but 'One Kind Kiss Before We Part'?"</p>

		
	
					
				
				
	
									
				

		
	




</div>

			</div>


		
	<div id="page-wrap2" x-data="{ pageVisible : true }"><h2 data-page-nbr="2" data-logged="false" data-page-url="history-ok2.htm">
			


<span @click="pageVisible = !pageVisible" aria-expanded="true" aria-controls="page2" role="button">

			<svg :class="{ 'rotate-180' : !pageVisible, '' : pageVisible }" xmlns="http://www.w3.org/2000/svg" width="22" height="10" viewBox="0 0 28.396 13.211"><path d="M4398.8,5158.252l13.224,10.507,12.357-10.507" transform="translate(-4397.399 -5156.842)" fill="none" stroke-linecap="round" stroke-width="2"></path></svg>
	
		<span>Misspelling Words Was Also a Thing</span>

	
</span>		</h2>

				

<div id="page2" data-slide="2" data-track-gtm="Content" x-show.transition="pageVisible === true">	





	
		<p>In addition to the abbreviation craze, 19th-century Americans thought it was really funny to purposely misspell stuff. Read, the etymologist, cited the example of the comic writer George W. Arnold, who used the pen name "Joe Strickland" to write mangled letters to his fictional family, like this one from a trip abroad: "when I got here tha axt me if I was evver in Turky before. no ses I. but i've had a darn menny turkeys in me."</p>

		
	
					
				
				


	
		<p>By the late 1830s, the (hilarious) misspelling trend had combined with the acronym craze to produce punchy abbreviations like:</p>

		
	
								

		
	
										
				
				


	
		<div>
	<ul><li><span>K.G. for "no go" (as if spelled "know go")</span></li><li><span>K.Y. for "no use" (as if spelled "know yuse")</span></li><li><span>O.W. for "all right" (as if spelled "oll wright")</span></li></ul>
</div>


		
	
		
				
				


	
		<p>Absolutely no one says K.G. or O.W. anymore, but believe it or not, that witty wordplay laid the groundwork for the arrival of a two-letter abbreviation that would conquer the world.</p>

		
	
					
				
				
	
				
				

		
	




</div>

			</div>


		
	<div id="page-wrap3" x-data="{ pageVisible : true }"><h2 data-page-nbr="3" data-logged="false" data-page-url="history-ok3.htm">
			


<span @click="pageVisible = !pageVisible" aria-expanded="true" aria-controls="page3" role="button">

			<svg :class="{ 'rotate-180' : !pageVisible, '' : pageVisible }" xmlns="http://www.w3.org/2000/svg" width="22" height="10" viewBox="0 0 28.396 13.211"><path d="M4398.8,5158.252l13.224,10.507,12.357-10.507" transform="translate(-4397.399 -5156.842)" fill="none" stroke-linecap="round" stroke-width="2"></path></svg>
	
		<span>The Very First Use of OK</span>

	
</span>		</h2>

				

<div id="page3" data-slide="3" data-track-gtm="Content" x-show.transition="pageVisible === true">	





	
		<p>Before we get to the fateful date of March 21, 1839, let's tip our hats one more time to Allen Walker Read, the man who solved the mystery of OK's origins. Keep in mind that Read was working in the 1960s, decades before searchable digital newspaper archives.</p>

		
	
					
				
				


	
		<p>"Read must have spent hundreds of hours digging through tons and tons of physical newspapers, journals, private letters and other documents," says Liberman, who writes the weekly <a href="https://blog.oup.com/category/series-columns/oxford_etymologist/">Oxford Etymologist</a> blog and knows firsthand how hard it is to track down the history of words. "What that man did was absolutely astounding."</p>

		
	
								

		
	
										
				
				


	
		<p>OK, back to our story.</p>

		
	
					
				
				


	
		<p>In the spring of 1839, the editor of Boston's Morning Post, Charles Gordon Greene, was engaged in some good-natured trash talk with the editors of the Providence Journal in Rhode Island<i>.</i> It had to do with a semi-satirical citizens group in Boston called the Anti-Bell-Ringing Society (or A.B.R.S.), of which Greene was a member.</p>

		
	
					
				
				


	
		<p>The Providence paper poked fun at Greene and the A.B.R.S. and Greene had to set the record straight. So it was that on March 21, 1839, at the end of a short paragraph defending the A.B.R.S., Greene printed the following words: "<i>o.k.</i> — all correct."</p>

		
	
					
				
				


	
		<p>See what he did there? Similar to using O.W. for "oll wright," Greene had coined a new misspelled acronym: O.K. for "oll korrect." Three days after Greene introduced OK to the world, the Providence Journal editors responded with an "O.K." of their own.</p>

		
	
					
				
				


	
		<p>Like other offbeat acronyms of the day, O.K. was an inside joke randomly thrust into general circulation. But unlike O.W. or K.G., which enjoyed brief popularity in the 1830s, O.K. didn't die out.</p>

		
	
					
				
				


	
		<p>"Nobody knew that this facetious abbreviation would have such a long and happy life," says Liberman.</p>

		
	
					
				
				
	
				
				

		
	




</div>

			</div>


		
	<div id="page-wrap4" x-data="{ pageVisible : true }"><h2 data-page-nbr="4" data-logged="false" data-page-url="history-ok4.htm">
			


<span @click="pageVisible = !pageVisible" aria-expanded="true" aria-controls="page4" role="button">

			<svg :class="{ 'rotate-180' : !pageVisible, '' : pageVisible }" xmlns="http://www.w3.org/2000/svg" width="22" height="10" viewBox="0 0 28.396 13.211"><path d="M4398.8,5158.252l13.224,10.507,12.357-10.507" transform="translate(-4397.399 -5156.842)" fill="none" stroke-linecap="round" stroke-width="2"></path></svg>
	
		<span>"Old Kinderhook" Takes "OK" National</span>

	
</span>		</h2>

				

<div id="page4" data-slide="4" data-track-gtm="Content" x-show.transition="pageVisible === true">	
					<div>
<figure>
			
		
	
								
		
		
																																																								
			<picture>
				<source media="(max-width: 320px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjozMjB9LCJ0b0Zvcm1hdCI6ImF2aWYifX0=" type="image/avif"><source media="(min-width: 321px) and (max-width: 599px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjo0MjB9LCJ0b0Zvcm1hdCI6ImF2aWYifX0=" type="image/avif"><source media="(min-width: 600px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjo4Mjh9LCJ0b0Zvcm1hdCI6ImF2aWYifX0=" type="image/avif">
				<source media="(max-width: 320px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjozMjB9fX0="><source media="(min-width: 321px) and (max-width: 599px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjo0MjB9fX0="><source media="(min-width: 600px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjo4Mjh9fX0=">
				<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAJCAQAAACRI2S5AAAAEElEQVR42mNkIAAYRxWAAQAG9gAKqv6+AwAAAABJRU5ErkJggg==" data-src="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjo4Mjh9fX0=" alt="Martin Van Buren 1840 campaign" width="828" height="648">
			</picture>
			

		
	

					
	
	<figcaption>
											This political cartoon shows Martin Van Buren, who ran unsuccessfully against William Henry Harrison, known as the "log cabin and hard cider" candidate, during the 1840 presidential campaign. Van Buren's "OK" nickname is prominent. <span>Bettman/Getty Images</span>
								</figcaption>

	</figure>
	</div>
	





	
		<p>If you thought that the word OK <a href="https://www.npr.org/templates/story/story.php?storyId=5170008">originated</a> with Martin Van Buren, you'd be half right. The eighth president of the United States <a href="https://www.nps.gov/mava/index.htm">hailed from the small town</a> of Kinderhook, New York. Like his mentor and fellow Democrat Andrew Jackson, who was known as "Old Hickory," Van Buren's nickname was "Old Kinderhook."</p>

		
	
					
				
				


	
		<p>In the 1840 presidential election, William Henry Harrison and the Whig party challenged the incumbent Van Buren. Harrison's supporters came up with the catchy (for its time) campaign slogan (<a href="https://potus-geeks.livejournal.com/425230.html">and song</a>), "Tippecanoe and Tyler Too." The Democrats swung back with a slogan of their own: "O.K." <a href="https://www.history.com/news/the-birth-of-ok-175-years-ago">As in</a>, "Old Kinderhook is OK!"</p>

		
	
								

		
	
										
				
				


	
		<p>"[Van Buren] got the nickname Old Kinderhook, and early in 1840, OK clubs sprung up with the slogan, 'OK is OK.' So taking that funny little word and making it a mainstay of the political conversation in 1840, suddenly OK was <i>way</i> OK," said the late linguist Allan Metcalf <a href="https://www.npr.org/2010/11/20/131390650/ok-how-two-letters-made-america-s-greatest-word">in a 2010 NPR interview</a>. Metcalf was the author of "<a href="https://www.amazon.com/OK-Improbable-Story-Americas-Greatest/dp/0199892539">OK: The Improbable Story of America's Greatest Word</a>." Van Buren lost badly, but OK definitely won.</p>

		
	
					
				
				


	
		<p>After 1840, the word spread like wildfire and never looked back. Originally, OK appeared in telegraph messages (which may account for its international spread) and documents but not in everyday speech as it was "slangy." But that changed over time. </p>

		
	
					
				
				


	
		<p><a href="https://www.bbc.com/news/magazine-12503686">In an article for BBC Magazine</a>, Metcalf speculated as to why OK was popular all over the world: "It's not that it was needed to 'fill a gap' in any language. Before 1839, English speakers had 'yes,' 'good,' 'fine,' 'excellent,' 'satisfactory' and 'all right.' What OK provided that the others did not was neutrality, a way to affirm or to express agreement without having to offer an opinion. ... OK allows us to view a situation in simplest terms, just OK or not."</p>

		
	
					
				
				
	
					

				

		
	




</div>

			</div>





				</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It's always TCP_NODELAY (753 pts)]]></title>
            <link>https://brooker.co.za/blog/2024/05/09/nagle.html</link>
            <guid>40310896</guid>
            <pubDate>Thu, 09 May 2024 17:54:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brooker.co.za/blog/2024/05/09/nagle.html">https://brooker.co.za/blog/2024/05/09/nagle.html</a>, See on <a href="https://news.ycombinator.com/item?id=40310896">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post">


<p>It's not the 1980s anymore, thankfully.</p>

<p>The first thing I check when debugging latency issues in distributed systems is whether <a href="https://linux.die.net/man/7/tcp">TCP_NODELAY</a> is enabled. And it’s not just me. Every distributed system builder I know has lost hours to latency issues quickly fixed by enabling this simple socket option, suggesting that the default behavior is wrong, and perhaps that the whole concept is outmoded.</p>

<p>First, let’s be clear about what we’re talking about. There’s no better source than John Nagle’s <a href="https://datatracker.ietf.org/doc/html/rfc896">RFC896</a> from 1984<sup><a href="#foot1">1</a></sup>. First, the problem statement:</p>

<blockquote>
  <p>There is a special problem associated with small  packets.   When TCP  is  used  for  the transmission of single-character messages originating at a keyboard, the typical result  is  that  41  byte packets (one  byte  of data, 40 bytes of header) are transmitted for each byte of useful data.  This 4000%  overhead  is  annoying but tolerable on lightly loaded networks.</p>
</blockquote>

<p>In short, Nagle was interested in better amortizing the cost of TCP headers, to get better throughput out of the network. Up to 40x better throughput! These tiny packets had two main causes: human-interactive applications like shells, where folks were typing a byte at a time, and poorly implemented programs that dribbled messages out to the kernel through many <code>write</code> calls. Nagle’s proposal for fixing this was simple and smart:</p>

<blockquote>
  <p>A  simple and elegant solution has been discovered.</p>
</blockquote>

<blockquote>
  <p>The solution is to inhibit the sending of new TCP  segments  when new  outgoing  data  arrives  from  the  user  if  any previously transmitted data on the connection remains unacknowledged.</p>
</blockquote>

<p>When many people talk about Nagle’s algorithm, they talk about timers, but RFC896 doesn’t use any kind of timer other than the round-trip time on the network.</p>

<p><em>Nagle’s Algorithm and Delayed Acks</em></p>

<p>Nagle’s nice, clean, proposal interacted poorly with another TCP feature: delayed <code>ACK</code>. The idea behind delayed <code>ACK</code> is to delay sending the acknowledgement of a packet at least until there’s some data to send back (e.g. a <code>telnet</code> session echoing back the user’s typing), or until a timer expires. <a href="https://datatracker.ietf.org/doc/html/rfc813">RFC813</a> from 1982 is that first that seems to propose delaying <code>ACKs</code>:</p>

<blockquote>
  <p>The receiver of data will   refrain   from   sending   an   acknowledgement   under   certain circumstances, in which case it must set a timer which  will  cause  the acknowledgement  to be sent later.  However, the receiver should do this only where it is a reasonable guess that some other event will intervene and prevent the necessity of the timer  interrupt.</p>
</blockquote>

<p>which is then formalized further in <a href="https://datatracker.ietf.org/doc/html/rfc1122">RFC1122</a> from 1989. The interaction between these two features causes a problem: Nagle’s algorithm is blocking sending more data until an <code>ACK</code> is received, but delayed ack is delaying that <code>ack</code> until a response is ready. Great for keeping packets full, not so great for latency-sensitive pipelined applications.</p>

<p>This is a point Nagle has made himself several times. For example in this <a href="https://news.ycombinator.com/item?id=10608356">Hacker News comment</a>:</p>

<blockquote>
  <p>That still irks me. The real problem is not tinygram prevention. It’s ACK delays, and that stupid fixed timer. They both went into TCP around the same time, but independently. I did tinygram prevention (the Nagle algorithm) and Berkeley did delayed ACKs, both in the early 1980s. The combination of the two is awful.</p>
</blockquote>

<p>As systems builders this is should be a familiar situation: two reasonable features of the system that interact to create an undesirable behavior. This kind of interaction is one of the things that makes protocol design so hard.</p>

<p><em>Is Nagle blameless?</em></p>

<p>Unfortunately, it’s not just delayed ACK. Even without delayed ack and that <em>stupid fixed timer</em>, the behavior of Nagle’s algorithm probably isn’t what we want in distributed systems. A single in-datacenter RTT is typically around 500μs, then a couple of milliseconds between datacenters in the same region, and up to hundreds of milliseconds going around the globe. Given the vast amount of work a modern server can do in even a few hundred microseconds, delaying sending data for even one RTT isn’t clearly a win.</p>

<p>To make a clearer case, let’s turn back to the justification behind Nagle’s algorithm: amortizing the cost of headers and avoiding that 40x overhead on single-byte packets. But does anybody send single byte packets anymore? Most distributed databases and systems don’t. Partially that’s because they simply have more to say, partially its because of additional overhead of protocols like TLS, and partially its because of encoding and serialization overhead. But mostly, they have more to say.</p>

<p>The core concern of not sending tiny messages is still a very real one, but we’ve very effectively pushed that into the application layer. Sending a byte at a time wrapped in JSON isn’t going to be very efficient, no matter what Nagle’s algorithm does.</p>

<p><em>Is Nagle needed?</em></p>

<p>First, the uncontroversial take: if you’re building a latency-sensitive distributed system running on modern datacenter-class hardware, enable <code>TCP_NODELAY</code> (disable Nagle’s algorithm) without worries. You don’t need to feel bad. It’s not a sin. It’s OK. Just go ahead.</p>

<p>More controversially, I suspect that Nagle’s algorithm just isn’t needed on modern systems, given the traffic and application mix, and the capabilities of the hardware we have today. In other words, <code>TCP_NODELAY</code> should be the default. That’s going to make some “<code>write</code> every byte” code slower than it would otherwise be, but those applications should be fixed anyway if we care about efficiency.</p>

<p><em>Footnotes</em></p>

<ol>
  <li><a name="foot1"></a> I won’t got into it here, but RFC896 is also one of the earliest statements I can find of metastable behavior in computer networks. In it, Nagle says: “This condition is stable. Once the  saturation point has been reached, if the algorithm for selecting packets to be dropped is fair, the network will continue to operate in a degraded condition.”</li>
</ol>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leaked deck reveals how OpenAI is pitching publisher partnerships (298 pts)]]></title>
            <link>https://www.adweek.com/media/openai-preferred-publisher-program-deck/</link>
            <guid>40310228</guid>
            <pubDate>Thu, 09 May 2024 16:56:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.adweek.com/media/openai-preferred-publisher-program-deck/">https://www.adweek.com/media/openai-preferred-publisher-program-deck/</a>, See on <a href="https://news.ycombinator.com/item?id=40310228">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-continue-reading-wrapper="">
                                    



<div>
		
		<p>Mark your calendar for Mediaweek, October 29-30 in New York City. We’ll unpack the biggest shifts shaping the future of media—from tv to retail media to tech—and how marketers can prep to stay ahead. <a href="https://event.adweek.com/mediaweek-2024/4442894?ref=nativep1&amp;itm_source=ROS&amp;itm_medium=display&amp;itm_campaign=nativeMediaweek24&amp;itm_content=p1"><strong>Register</strong></a> with early-bird rates before sale ends!</p>
	</div>


<div><p>The generative artificial intelligence firm OpenAI has been pitching partnership opportunities to news publishers through an initiative called the Preferred Publishers Program, according to a deck obtained by ADWEEK and interviews with four industry executives.</p><p>OpenAI has been courting premium publishers dating back to July 2023, when it struck a licensing agreement with the Associated Press. It has since inked public partnerships with Axel Springer, The Financial Times, <a href="https://www.adweek.com/media/le-monde-english-subscribers-olympics/" target="_blank" rel="noreferrer noopener">Le Monde</a>, Prisa and Dotdash Meredith, although it has declined to share the specifics of any of its deals.</p><p>A representative for OpenAI disputed the accuracy of the information in the deck, which is more than three months old. The <a href="https://www.adweek.com/category/artificial-intelligence/" target="_blank">gen AI</a> firm also negotiates deals on a per-publisher basis, rather than structuring all of its deals uniformly, the representative said.</p><p>“We are engaging in productive conversations and partnerships with many news publishers around the world,” said a representative for OpenAI. “Our confidential documents are for discussion purposes only and ADWEEK’s reporting contains a number of mischaracterizations and outdated information.”</p><p>Nonetheless, the leaked deck reveals the basic structure of the partnerships OpenAI is proposing to media companies, as well as the incentives it is offering for their collaboration.</p><section> <p><a href="https://www.adweek.com/media/publishers-ai-licensing-negotiations-mark-an-inflection-point/" target="_blank"><img decoding="async" src="https://static-prod.adweek.com/wp-content/uploads/2023/11/publisher-ai-licensing-2023-640x360.jpg" alt="Copyright law favors creators, but commercial compromise offers a hedge against uncertainty."></a></p>  </section><h4><strong>Details from the pitch deck</strong></h4><p>The Preferred Publisher Program has five primary components, according to the deck.</p><p>First, it is available only to “select, high-quality editorial partners,” and its purpose is to help ChatGPT users more easily discover and engage with publishers’ brands and content.</p><p>Additionally, members of the program receive priority placement and “richer brand expression” in chat conversations, and their content benefits from more prominent link treatments. Finally, through PPP, OpenAI also offers licensed financial terms to publishers.</p><p>The financial incentives participating publishers can expect to receive are grouped into two buckets: guaranteed value and variable value.</p><p>Guaranteed value is a licensing payment that compensates the publisher for allowing OpenAI to access its backlog of data, while variable value is contingent on display success, a metric based on the number of users engaging with linked or displayed content.</p><p>The resulting financial offer would combine the guaranteed and variable values into one payment, which would be structured on an annual basis.&nbsp;</p><p>“The PPP program is more about scraping than training,” said one executive. “OpenAI has presumably already ingested and trained on these publishers’ archival data, but it needs access to contemporary content to answer contemporary queries.”</p><!--nextpage--><p>In return for these payments, OpenAI would gain two benefits. </p><p>It would have the ability to train on a publisher’s content and the license to display that information in ChatGPT products, complete with attribution and links. It would also get to announce the publisher as a preferred partner and work with them to build out these experiences.</p><h4><strong>Participation boosts publisher payouts</strong></h4><p>According to the deck, publisher participation in PPP creates a better experience for OpenAI users, which will help shift engagement toward browsing, i.e. queries that result in responses with links.</p><p>Roughly 25% of ChatGPT users already use the browse function, but the company expects that a majority of users will do so once the feature is broadly rolled out. If more users engage with publishers’ links, the <a href="https://www.adweek.com/category/media-news/" target="_blank" rel="noreferrer noopener">media companies</a> could earn larger payments for their variable value.&nbsp;</p><p>PPP members will see their content receive its “richer brand expression” through a series of content display products: the branded hover link, the anchored link and the in-line treatment.</p><p>In the hover treatment, which is available today, OpenAI will hyperlink keywords in its responses to search queries. The links appear as blue text and reveal a clickable tab when moused over. </p><p>In the anchor treatment, branded, clickable buttons appear below ChatGPT’s response to a user query. And the in-line product inserts a pullquote into the text of ChatGPT’s response, whose font is larger and includes a clickable, branded link.&nbsp;</p><p>All three content display products seek to cite the publishers whose writing is being used to answer the search query, although the setup will likely lead fewer users to visit publishers’ websites.&nbsp;</p><p>A recent model from The Atlantic found that if a search engine like Google were to integrate AI into search, it would answer a user’s query 75% of the time without requiring a clickthrough to its website.</p><h4><strong>Where publishers go from here</strong></h4><p>The details of the program add further color to the complicated relationship between digital publishers and OpenAI. The uncertain legal standing of the data-scraping methodology that OpenAI uses to power its large-language models has made licensing negotiations between the two parties complex.</p><p>While some publishers have opted to partner with OpenAI, others, <a href="https://www.adweek.com/media/open-ai-response-new-york-times-lawsuit/" target="_blank">including recent NewFronts participant The New York Times</a> and eight Alden Global Capital titles, have sued the tech firm on the grounds that it has used copyrighted articles without permission.</p><p>The vast majority of news publishers, as well as independent websites, have neither partnered with OpenAI nor taken legal action. According to one media executive, through programs such as Preferred Publisher, OpenAI is looking to change that.</p><p>“At the recent Aspen Conference in New York on AI and the news,” the person said, “OpenAI was very open about their need to attract publishers into their partnership program.”&nbsp;</p><!--nextpage--><p><em>This story has updated to include a response from OpenAI.</em></p></div>
<div id="meter-count">
  
  
    
    
  
  <a href="#" onclick="ShowAndHide()">
    
  </a>
  
  
  <div>
        <p>
          <h3>Enjoying Adweek's Content? Register for More Access!</h3>
        </p>
      </div>
</div>                                                                    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leaked FBI Email Reportedly Shows Desperation to Justify Warrantless Wiretaps (162 pts)]]></title>
            <link>https://gizmodo.com/leaked-fbi-email-warrantless-wiretaps-section-702-1851464520</link>
            <guid>40309957</guid>
            <pubDate>Thu, 09 May 2024 16:34:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/leaked-fbi-email-warrantless-wiretaps-section-702-1851464520">https://gizmodo.com/leaked-fbi-email-warrantless-wiretaps-section-702-1851464520</a>, See on <a href="https://news.ycombinator.com/item?id=40309957">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Congress reauthorized America’s warrantless wiretapping program last month after some <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/national-security-threat-likely-nukes-in-space-1851257693&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/national-security-threat-likely-nukes-in-space-1851257693">successful fearmongering</a></span> by national security hawks on Capitol Hill. But an internal FBI email, leaked to <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.wired.com/story/fbi-section-702-us-person-queries-email/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.wired.com/story/fbi-section-702-us-person-queries-email/" target="_blank" rel="noopener noreferrer">Wired</a></span> on Wednesday, may accidentally reveal how the federal law enforcement agency plans to overstep the spirit of the law, while technically maintaining the letter of the law.<br></p><div data-video-id="196937" data-monetizable="false" data-position="sidebar" data-video-title="Approaching Queerness in Doctor Who" data-video-blog-id="4" data-video-network="gizmodo" data-video-duration="72" data-playlist="196937,196931,196911" data-current="196937"><div><p>Approaching Queerness in Doctor Who</p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/196937/196937_240p.mp4" label="240p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/196937/196937_480p.mp4" label="480p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/196937/196937_720p.mp4" label="720p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/196937/196937_1080p.mp4" label="1080p" type="video/mp4"><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/22499.vtt" srclang="en"></video><div><ul><li data-label="">Off</li><li data-label="English">English</li></ul></div></div><p>The controversial spying program is Section 702 in the Foreign Intelligence Surveillance Act (FISA) and allows the interception of foreign communications that sometimes include American citizens. The program ostensibly includes safeguards to ensure the law isn’t being used to unnecessarily spy on Americans, but it’s pretty clear from this new email that the FBI likes being able to get communications from Americans.<br></p><p>The email obtained by <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.wired.com/story/fbi-section-702-us-person-queries-email/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.wired.com/story/fbi-section-702-us-person-queries-email/" target="_blank" rel="noopener noreferrer">Wired</a></span> dated April 20 was written by FBI Deputy Director Paul Abbate and sent out to employees internally.<br></p><p>“To continue to demonstrate why tools like this are essential to our mission, we need to <em>use</em> them, while also holding ourselves accountable for doing so properly and in compliance with legal requirements,” the email reads, according to Wired, which notes that the italicization on the word “use” was in the original email.</p><p>The FBI email made things even more explicit by encouraging searches for Americans when looking through intercepted communications.<br></p><p>“I urge everyone to continue to look for ways to appropriately use US  person queries to advance the mission, with the added confidence that  this new pre-approval requirement will help ensure that those queries  are fully compliant with the law,” the email reads.</p><p>The FBI’s response to Wired is particularly interesting, making it worth quoting at length. From Wired:<br></p><blockquote data-type="BlockQuote"><p>Following publication, FBI spokesperson Susan McKee provided a statement  from the bureau that mischaracterized WIRED’s reporting, inaccurately  claiming it “alleged that that the FBI instructed its employees to  violate the law or FBI policies.” The statement added that Abbate’s  email “emphasized Congress’ recognition of the vital importance of FISA  Section 702 to protect the American people and was sent to ensure that  FBI personnel were immediately aware of, and in compliance with, the  privacy enhancing changes the law has put in place.”</p></blockquote><p>Obviously, the FBI is going to say everyone at the agency follows the law since they quite literally are the law. But Wired spoke with Rep. Zoe Lofgren, a Democrat from California who notes this newly leaked email “directly contradicts earlier assertions” by the FBI when the agency was trying to get the law reauthorized.</p><p>It’s all a mess. The FBI got exactly what it wanted with the reauthorization of Section 702, something that was never really in doubt, even with pressure from a handful of politicians who opposed it. To paraphrase former president Richard Nixon, it’s <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.nixonlibrary.gov/media/video/excerpt-frost-interview-final&quot;,{&quot;metric25&quot;:1}]]" href="https://www.nixonlibrary.gov/media/video/excerpt-frost-interview-final" target="_blank" rel="noopener noreferrer">not illegal</a></span> when the FBI does it. But what are you going to do in such a <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.vox.com/2014/4/18/5624310/martin-gilens-testing-theories-of-american-politics-explained&quot;,{&quot;metric25&quot;:1}]]" href="https://www.vox.com/2014/4/18/5624310/martin-gilens-testing-theories-of-american-politics-explained" target="_blank" rel="noopener noreferrer">ridiculously rigged system</a></span>? </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Familial Transmission of Personality Is Higher Than Shown in Typical Studies (127 pts)]]></title>
            <link>https://osf.io/preprints/psyarxiv/7ygp6</link>
            <guid>40309840</guid>
            <pubDate>Thu, 09 May 2024 16:24:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://osf.io/preprints/psyarxiv/7ygp6">https://osf.io/preprints/psyarxiv/7ygp6</a>, See on <a href="https://news.ycombinator.com/item?id=40309840">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[ESP32 Drum Synth Machine (219 pts)]]></title>
            <link>https://github.com/zircothc/DRUM_2004_V1</link>
            <guid>40309759</guid>
            <pubDate>Thu, 09 May 2024 16:17:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/zircothc/DRUM_2004_V1">https://github.com/zircothc/DRUM_2004_V1</a>, See on <a href="https://news.ycombinator.com/item?id=40309759">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      


    <div>
      <p><a href="#start-of-content" data-skip-target-assigned="false">Skip to content</a>
      
      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p><react-partial partial-name="keyboard-shortcuts-dialog" data-ssr="false">
  
  
  
</react-partial>




      

        

            


<header role="banner" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  <h2>Navigation Menu</h2>

  

  <div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:zircothc/DRUM_2004_V1" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="Xls1zmfjVRFg9r4Wl7imLC4d7t70EC8VFGEzJ3l8PkSTdyhMRwvLvr6FuGcBXE97nR8Ke0WUSEFhP2Op2rYh0Q" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="zircothc/DRUM_2004_V1" data-current-org="" data-current-owner="zircothc" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=%2BiiCrl8XCZuAbyfAa9SDOHr3qvpevm5qEbQhyJ5E7dgHAE61KDk5rdJcw3r8O2y5EM1VVRGGeCAcnvPZAMPdBvItIhbHV0XYmg%2Fv8CP8RyrNEfe%2FO5azXeP83%2F0HfBzzThpa0pxtmgMeyOyb4XMSLxiL230rkV5xdveUyYtTqNEBGgJlJY5TshB80I2tgtZ6Os3NZL2gmjgXM9vDNSDwHD25huy6KhKPg0NpkNddr0SY6FIkrm2fjLlC3oqfuwpNkEgayvjoI7eDRnt67C1s%2Fz2aEhb9BHhYTqxbEYCpXRNdYPNAFBbjCQPJN9yoDnoEubkElq7AvFFM3NylFzAzSvtZjzOE1yO0JplkVZvZaeOlAUOj2K0dk%2F%2FSOwgbMQHPFoQesHKli8w8R2C3332APP4N2Vnm2xWkH6ctA5FLcp3EBJKNBNVzl4MatYE58pi4SwNAbI21ugU4PfXxFLyWGciBcMvFNnQhpAcYNENsQzIi5ugAkpKIzJQnqtEHdidObAsVK3H6mhqBCyrZrDPYIjfu--YhueA04nZryfZRfM--cAd3gG%2Fp%2FjkDIvZzaBkuUA%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=zircothc%2FDRUM_2004_V1" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/zircothc/DRUM_2004_V1&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="81dd76527504e666ffcf483d0fe30a1f780d08432723a3a9b88f6d261439a7e9" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div>
</header>

      
    </div>

  








    


    
    <include-fragment data-base-src="https://github.com/notifications/beta/shelf"></include-fragment>





  <div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  





    






  
  

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div id="repo-content-pjax-container">

  

  <include-fragment src="/zircothc/DRUM_2004_V1/spoofed_commit_check/a9e446620f1cd0590298ad68cbce2fedecb34a5c" data-test-selector="spoofed-commit-check"></include-fragment>

  <div data-view-component="true">        


















<react-partial partial-name="repos-overview" data-ssr="true">
  
  
  <div data-target="react-partial.reactRoot"><div><h2>Repository files navigation</h2><nav aria-label="Repository files"><ul role="list"><li><a href="#" aria-current="page"><span data-component="icon"></span><span data-component="text" data-content="README">README</span></a></li></ul></nav></div><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DRUM_2004_V1</h2><a id="user-content-drum_2004_v1" aria-label="Permalink: DRUM_2004_V1" href="#drum_2004_v1"></a></p>
<p dir="auto">ESP32 DRUM SYNTH MACHINE</p>
<p dir="auto">This is my DRUM SYNTH LOFI MACHINE.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/17828930/326164677-c8327dc2-a3f7-4d81-8d82-ebfe2a7c45c3.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ2NzctYzgzMjdkYzItYTNmNy00ZDgxLThkODItZWJmZTJhN2M0NWMzLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU1MjAxMzAzNmNiNGJlZWY0Y2RkNDgzZTMxZGU2NGMxYTJiOTYzYjRmMDJiYzgyN2JmZjE3NjExNjQ3ZGFhNDcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Uyua3Yza2N0QuZGlt3lN_wHPan8EM8u4O6Llrz74cEc"><img src="https://private-user-images.githubusercontent.com/17828930/326164677-c8327dc2-a3f7-4d81-8d82-ebfe2a7c45c3.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ2NzctYzgzMjdkYzItYTNmNy00ZDgxLThkODItZWJmZTJhN2M0NWMzLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU1MjAxMzAzNmNiNGJlZWY0Y2RkNDgzZTMxZGU2NGMxYTJiOTYzYjRmMDJiYzgyN2JmZjE3NjExNjQ3ZGFhNDcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Uyua3Yza2N0QuZGlt3lN_wHPan8EM8u4O6Llrz74cEc" alt="IMG_20240406_150440"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Synth engine:</h2><a id="user-content-synth-engine" aria-label="Permalink: Synth engine:" href="#synth-engine"></a></p>
<ul dir="auto">
<li>Wavetable synthesizer based on DZL Arduino library "The Synth" (<a href="https://github.com/dzlonline/the_synth">https://github.com/dzlonline/the_synth</a>)</li>
<li>16 sound polyphony</li>
<li>Sound parameters: Table, Length, Envelope, Pitch, Modulation, + Volume, Pan and Filter.</li>
<li>Filter (LowPassFilter) comes from Mozzi library (<a href="https://github.com/sensorium/Mozzi">https://github.com/sensorium/Mozzi</a>)</li>
</ul>
<p dir="auto">SEQUENCER:</p>
<ul dir="auto">
<li>16 step/pattern editor and random generators (pattern, sound parameters and notes)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hardware:</h2><a id="user-content-hardware" aria-label="Permalink: Hardware:" href="#hardware"></a></p>
<ul dir="auto">
<li>Lolin S2 Mini (ESP32 S2)</li>
<li>PCM5102A I2s dac</li>
<li>24 push buttons (8x3)</li>
<li>Rotary encoder</li>
<li>OLED display I2c</li>
<li>32 LED WS2812B</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Software:</h2><a id="user-content-software" aria-label="Permalink: Software:" href="#software"></a></p>
<p dir="auto">IDE:
Arduino 1.8.19</p>
<p dir="auto">Boards:
Expressif Systems 2.0.14</p>
<p dir="auto">Board: Lolin S2 Mini</p>
<p dir="auto">Libraries:</p>
<ul dir="auto">
<li>Sequencer Timer - uClock: <a href="https://github.com/midilab/uClock">https://github.com/midilab/uClock</a></li>
<li>RGB Leds - Adafruit Neopixel: <a href="https://github.com/adafruit/Adafruit_NeoPixel">https://github.com/adafruit/Adafruit_NeoPixel</a></li>
<li>OLED - u8g2: <a href="https://github.com/olikraus/u8g2">https://github.com/olikraus/u8g2</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Notes:</h2><a id="user-content-notes" aria-label="Permalink: Notes:" href="#notes"></a></p>
<p dir="auto">Schematics uploaded.</p>
<p dir="auto">Join solder pads near SCK pin in PCM5102A module.</p>
<p dir="auto">Video demo of the prototype:</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=rXl1gpWJp-g" rel="nofollow"><img src="https://camo.githubusercontent.com/6859cb54d06b51e0e0690b1ff5f288e2ec91981a74f66c0be1331ed00486f80d/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f72586c316770574a702d672f302e6a7067" alt="IMG_20240406_150231" data-canonical-src="https://img.youtube.com/vi/rXl1gpWJp-g/0.jpg"></a></p>
<p dir="auto">Waiting PCBs to build the first one :)
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/17828930/326164735-feb9b928-f76a-4b51-93ea-a7afbd6a5c28.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ3MzUtZmViOWI5MjgtZjc2YS00YjUxLTkzZWEtYTdhZmJkNmE1YzI4LmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNlZTFjNjg4YzljOWQyMTUwMjE1NjQ1YmI3MDExYjk2ZDgxZmY5MmJlZDYyNDVmMzEyMDc0YzkyMjI2NDAxZjUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Zq6yD07v-buQ7WaQxTmfWfWiBKceY6Unmnnss2J2fDs"><img src="https://private-user-images.githubusercontent.com/17828930/326164735-feb9b928-f76a-4b51-93ea-a7afbd6a5c28.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ3MzUtZmViOWI5MjgtZjc2YS00YjUxLTkzZWEtYTdhZmJkNmE1YzI4LmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNlZTFjNjg4YzljOWQyMTUwMjE1NjQ1YmI3MDExYjk2ZDgxZmY5MmJlZDYyNDVmMzEyMDc0YzkyMjI2NDAxZjUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Zq6yD07v-buQ7WaQxTmfWfWiBKceY6Unmnnss2J2fDs" alt="IMG_20240406_150231"></a>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/17828930/326164951-e1001f26-0993-4221-90d1-e9a2f710af0f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ5NTEtZTEwMDFmMjYtMDk5My00MjIxLTkwZDEtZTlhMmY3MTBhZjBmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA2ZmM0NTAzMDkyM2VmZjBiYzFmODI5MzcyMzBhMTZiNGU4YjFkODk0NzNhM2UwZGY5NjQ2ZGVmNmZmYzc3OTQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.DAz14RYXzspi3g35ofVXaOrf49ivpYcGhtBmBx8F5ZA"><img src="https://private-user-images.githubusercontent.com/17828930/326164951-e1001f26-0993-4221-90d1-e9a2f710af0f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ5NTEtZTEwMDFmMjYtMDk5My00MjIxLTkwZDEtZTlhMmY3MTBhZjBmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA2ZmM0NTAzMDkyM2VmZjBiYzFmODI5MzcyMzBhMTZiNGU4YjFkODk0NzNhM2UwZGY5NjQ2ZGVmNmZmYzc3OTQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.DAz14RYXzspi3g35ofVXaOrf49ivpYcGhtBmBx8F5ZA" alt="board"></a></p>
</article></div></div>
</react-partial>

        </div></div>

</turbo-frame>


    </main>
  </div>

          




    <ghcc-consent id="ghcc" data-initial-cookie-consent-allowed="" data-cookie-consent-required="true"></ghcc-consent>


  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Ellipsis – Automated PR reviews and bug fixes (108 pts)]]></title>
            <link>https://www.ellipsis.dev/</link>
            <guid>40309719</guid>
            <pubDate>Thu, 09 May 2024 16:14:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ellipsis.dev/">https://www.ellipsis.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=40309719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" data-framer-hydrate-v2="{&quot;routeId&quot;:&quot;augiA20Il&quot;,&quot;localeId&quot;:&quot;default&quot;,&quot;breakpoints&quot;:[{&quot;hash&quot;:&quot;72rtr7&quot;,&quot;mediaQuery&quot;:&quot;(min-width: 1440px)&quot;},{&quot;hash&quot;:&quot;103465x&quot;,&quot;mediaQuery&quot;:&quot;(min-width: 1280px) and (max-width: 1439px)&quot;},{&quot;hash&quot;:&quot;142h2bu&quot;,&quot;mediaQuery&quot;:&quot;(min-width: 810px) and (max-width: 1279px)&quot;},{&quot;hash&quot;:&quot;1pkud1z&quot;,&quot;mediaQuery&quot;:&quot;(max-width: 809px)&quot;}]}" data-framer-ssr-released-at="2024-05-06T12:43:44.396Z" data-framer-page-optimized-at="2024-05-09T17:51:33.965Z"><figure data-framer-name="Background" name="Background"></figure><div data-framer-name="cta" name="cta"><div data-framer-name="Content Wrapper" name="Content Wrapper"><a data-border="true" data-framer-name="Main" name="Main" href="https://docs.ellipsis.dev/code#from-a-pr" target="_blank" rel="noopener"></a><div><p data-framer-name="Enhance the way you" data-framer-component-type="RichTextContainer"><h2 data-styles-preset="LdKDZ3RUB"><span data-text-fill="true">Ship faster with AI</span></h2></p></div><p data-styles-preset="ppPqjGXDa">Ellipsis is an AI devtool that reviews pull requests and converts GitHub comments into working, tested code.</p></div><div data-framer-name="Content Wrapper" name="Content Wrapper"><p data-styles-preset="skV12T4aE">Backed by</p><div data-framer-name="YC_logo" name="YC_logo"><p><img decoding="async" sizes="(min-width: 1440px) 150px, (min-width: 1280px) and (max-width: 1439px) 150px, (min-width: 810px) and (max-width: 1279px) 150px, (max-width: 809px) 150px" srcset="https://framerusercontent.com/images/3EGsNY7lyfSWhMVz634fBMRDR4.png?scale-down-to=512 512w,https://framerusercontent.com/images/3EGsNY7lyfSWhMVz634fBMRDR4.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/3EGsNY7lyfSWhMVz634fBMRDR4.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/3EGsNY7lyfSWhMVz634fBMRDR4.png 4096w" src="https://framerusercontent.com/images/3EGsNY7lyfSWhMVz634fBMRDR4.png?scale-down-to=2048" alt="" data-framer-original-sizes="150px"></p></div></div></div><div data-framer-name="FEATURES" name="FEATURES"><div data-framer-name="Headings Wrapper" name="Headings Wrapper"><p><span><strong>But that's only the beginning</strong></span>. Ellipsis is capable of reviewing, writing, and answering questions about your source code.</p></div><div data-framer-name="Features Wrapper" name="Features Wrapper"><div data-framer-name="Feature Block" name="Feature Block" data-border="true"><p data-framer-name="100+ Languages Support" data-framer-component-type="RichTextContainer"><h4>20+ Languages Supported</h4></p></div><div data-framer-name="Feature Block" name="Feature Block"><div><p><img decoding="async" loading="lazy" src="https://framerusercontent.com/images/HlAwD5YbZZw5JczQDnm3WQhEw.png" alt=""></p></div><div data-framer-name="Content Wrapepr" name="Content Wrapepr"><p data-styles-preset="skV12T4aE">Ship faster by converting requirements into working, tested code.</p></div></div><div data-framer-name="Feature Block" name="Feature Block" data-border="true"><p data-framer-name="Modern and easy UI" data-framer-component-type="RichTextContainer"><h4>Free 7 day trial</h4></p><p>Includes unlimited code reviews, summaries, and generations.</p></div></div><div data-framer-name="Content Wrapper" name="Content Wrapper"><p>Ellipsis doesn't store or train on your source code. It will never commit to your default branch, and will only add new commits or open new pull requests when you explicitly request it. </p></div><div data-framer-name="Pricing" name="Pricing"><div><p>License per seat, reassign seats at any time.</p></div><div><div data-border="true"><div data-framer-name="Variant 1" tabindex="0"><p>Install in your repository</p></div><div data-framer-name="Variant 1" tabindex="0"><p>Email us at team@ellipsis.dev</p></div><div data-framer-name="Variant 1" tabindex="0"><p>If approved, get unlimited code reviews</p></div></div><div data-border="true"><div><p data-framer-component-type="RichTextContainer"><h5>per developer-month</h5></p></div><div><div data-framer-name="Variant 1" tabindex="0"><p>Automatic code reviews on every commit</p></div><div data-framer-name="Variant 1" tabindex="0"><p>Bug fixes &amp; code generations</p></div><div data-framer-name="Variant 1" tabindex="0"><p>Question &amp; answer functionality</p></div></div></div><div data-border="true"><div><p data-framer-component-type="RichTextContainer"><h2>Contact us</h2></p></div><div><div data-framer-name="Variant 1" tabindex="0"><p>Weekly code change summaries</p></div><div data-framer-name="Variant 1" tabindex="0"><p>Priority support, with SLA</p></div></div></div></div></div></div></div></div>]]></description>
        </item>
    </channel>
</rss>