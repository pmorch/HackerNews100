<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 01 Feb 2026 12:30:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Netbird a German Tailscale alternative (P2P WireGuard-based overlay network) (234 pts)]]></title>
            <link>https://netbird.io/</link>
            <guid>46844870</guid>
            <pubDate>Sun, 01 Feb 2026 09:44:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://netbird.io/">https://netbird.io/</a>, See on <a href="https://news.ycombinator.com/item?id=46844870">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[List animals until failure (174 pts)]]></title>
            <link>https://rose.systems/animalist/</link>
            <guid>46842603</guid>
            <pubDate>Sun, 01 Feb 2026 01:03:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rose.systems/animalist/">https://rose.systems/animalist/</a>, See on <a href="https://news.ycombinator.com/item?id=46842603">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="rules">
        <summary>List as many animals as you can.</summary>
        <p><b>Animals must have Wikipedia articles.</b>
        </p><p><b>You have limited time, but get more time for each animal listed.</b> When the timer runs out, that's game over.
        </p><p><b>No overlapping terms.</b>
           For example, if you list “bear” and “polar bear”, you get no point (or time bonus) for the latter.
           But you can still get a point for a second kind of bear. Order doesn't matter.
        </p><p id="visualshint"><b>Ignore the extraneous visuals.</b>
           Focus on naming animals.
    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Swift is a more convenient Rust (301 pts)]]></title>
            <link>https://nmn.sh/blog/2023-10-02-swift-is-the-more-convenient-rust</link>
            <guid>46841374</guid>
            <pubDate>Sat, 31 Jan 2026 22:05:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nmn.sh/blog/2023-10-02-swift-is-the-more-convenient-rust">https://nmn.sh/blog/2023-10-02-swift-is-the-more-convenient-rust</a>, See on <a href="https://news.ycombinator.com/item?id=46841374">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><a href="https://naman34.svbtle.com/swift-is-the-more-convenient-rust"></a>
<p><em>(originally published on my <a href="https://naman34.svbtle.com/swift-is-the-more-convenient-rust">old blog</a>)</em></p>
<p>I’ve been learning Rust lately.</p>
<p>Rust is one of the most loved languages out there, is fast, and has an amazing community. Rust invented the concept of ownership as a solution memory management issues without resorting to something slower like Garbage Collection or Reference Counting. But, when you don’t need to be quite as low level, it gives you utilities such as <code>Rc</code>, <code>Arc</code> and <code>Cow</code> to do reference counting and “clone-on-right” in your code. And, when you need to go lower-level still, you can use the <code>unsafe</code> system and access raw C pointers.</p>
<p>Rust also has a bunch of awesome features from functional languages like tagged enums, match expressions, first class functions and a powerful type system with generics.</p>
<p>Rust has an LLVM-based compiler which lets it compile to native code and WASM.</p>
<p>I’ve also been doing a bit of Swift programming for a couple of years now. And the more I learn Rust, the more I see a reflection of Swift. (I know that Swift stole a lot of ideas from Rust, I’m talking about my own perspective here).</p>
<p>Swift, too, has awesome features from functional languages like tagged enums, match expressions and first-class functions. It too has a very powerful type system with generics.</p>
<p>Swift too gives you complete type-safety without a garbage collector. By default, everything is a value type with “copy-on-write” semantics. But when you need extra speed you can opt into an ownership system and “move” values to avoid copying. And if you need to go even lower level, you can use the unsafe system and access raw C pointers.</p>
<p>Swift has an LLVM-based compiler which lets it compile to native code and WASM.</p>
<a id="deja-vu" href="#deja-vu"><h3><span>#</span>Deja Vu?</h3></a>
<p>You’re probably feeling like you just read the same paragraphs twice. This is no accident. Swift is extremely similar to Rust and has most of the same feature-set. But there is a very big difference is <em>perspective</em>. If you consider the default memory model, this will start to make a lot of sense.</p>
<a id="rust-is-bottom-up-swift-is-top-down" href="#rust-is-bottom-up-swift-is-top-down"><h3><span>#</span>Rust is bottom-up, Swift is top-down.</h3></a>
<p>Rust is a low-level systems language at heart, but it gives you the tools to go higher level. Swift starts at a high level and gives you the ability to go low-level.</p>
<p>The most obvious example of this is the memory management model. Swift use value-types by default with <code>copy-on-write</code> semantics. This is the equivalent of using <code>Cow&lt;&gt;</code> for all your values in Rust. But defaults matter. Rust makes it easy to use “moved” and “borrowed” values but requires extra ceremony to use <code>Cow&lt;&gt;</code> values as you need to “unwrap” them <code>.as_mutable()</code> to actually use the value within. Swift makes these Copy-on-Write values easy to use and instead requires extra ceremony to use borrowing and moving instead. Rust is faster by default, Swift is simpler and easier by default.</p>
<a id="swift-takes-rusts-ideas-and-hides-them-in-c-like-syntax" href="#swift-takes-rusts-ideas-and-hides-them-in-c-like-syntax"><h3><span>#</span>Swift takes Rust’s ideas and hides them in C-like syntax.</h3></a>
<p>Swift’s syntax is a masterclass in taking awesome functional language concepts and hiding them in C-like syntax to trick the developers into accepting them.</p>
<p>Consider <code>match</code> statements. This is what a match statement looks like in Rust:</p>
<div data-bright-theme="dark-plus"><pre><code><p><span><span>enum </span><span>Coin</span><span> {</span><br></span></p><p><span><span>    Penny</span><span>,</span><br></span></p><p><span><span>    Nickel</span><span>,</span><br></span></p><p><span><span>    Dime</span><span>,</span><br></span></p><p><span><span>    Quarter</span><span>,</span><br></span></p><p><span><span>}</span><br></span></p><p><span><span>fn </span><span>value_in_cents</span><span>(</span><span>coin</span><span>: </span><span>Coin</span><span>) -&gt; </span><span>u8</span><span> {</span><br></span></p><p><span><span>    match </span><span>coin</span><span> {</span><br></span></p><p><span><span>        Coin</span><span>::</span><span>Penny</span><span> =&gt; </span><span>1</span><span>,</span><br></span></p><p><span><span>        Coin</span><span>::</span><span>Nickel</span><span> =&gt; </span><span>5</span><span>,</span><br></span></p><p><span><span>        Coin</span><span>::</span><span>Dime</span><span> =&gt; </span><span>10</span><span>,</span><br></span></p><p><span><span>        Coin</span><span>::</span><span>Quarter</span><span> =&gt; </span><span>25</span><span>,</span><br></span></p><p><span><span>    }</span><br></span></p><p><span><span>}</span><br></span></p></code></pre></div>
<p>Here’s how that same code would be written in Swift:</p>
<div data-bright-theme="dark-plus"><pre><code><p><span><span>enum </span><span>Coin</span><span> {</span><br></span></p><p><span><span>    case </span><span>penny</span><br></span></p><p><span><span>    case </span><span>nickel</span><br></span></p><p><span><span>    case </span><span>dime</span><br></span></p><p><span><span>    case </span><span>quarter</span><br></span></p><p><span><span>}</span><br></span></p><p><span><span>func </span><span>valueInCents</span><span>(</span><span>coin</span><span>: Coin) -&gt; </span><span>Int</span><span> {</span><br></span></p><p><span><span>    switch</span><span> coin {</span><br></span></p><p><span><span>    case</span><span> .</span><span>penny</span><span>: </span><span>1</span><br></span></p><p><span><span>    case</span><span> .</span><span>nickel</span><span>: </span><span>5</span><br></span></p><p><span><span>    case</span><span> .</span><span>dime</span><span>: </span><span>10</span><br></span></p><p><span><span>    case</span><span> .</span><span>quarter</span><span>: </span><span>25</span><br></span></p><p><span><span>    }</span><br></span></p><p><span><span>}</span><br></span></p></code></pre></div>
<p>Swift doesn’t have a <code>match</code> statement or expression. It has a <code>switch</code> statement that developers are already familiar with. Except this <code>switch</code> statement is actually not a <code>switch</code> statement at all. It’s an expression. It doesn’t “fallthrough”. It does pattern matching. It’s just a <code>match</code> expression with a different name and syntax.</p>
<p>In fact, Swift treats <code>enums</code> as more than <em>just</em> types and lets you put methods directly on it:</p>
<div data-bright-theme="dark-plus"><pre><code><p><span><span>enum </span><span>Coin</span><span> {</span><br></span></p><p><span><span>    case </span><span>penny</span><br></span></p><p><span><span>    case </span><span>nickel</span><br></span></p><p><span><span>    case </span><span>dime</span><br></span></p><p><span><span>    case </span><span>quarter</span><br></span></p><p><span><span>    func </span><span>valueInCents</span><span>() -&gt; </span><span>Int</span><span> {</span><br></span></p><p><span><span>        switch </span><span>self</span><span> {</span><br></span></p><p><span><span>        case</span><span> .</span><span>penny</span><span>: </span><span>1</span><br></span></p><p><span><span>        case</span><span> .</span><span>nickel</span><span>: </span><span>5</span><br></span></p><p><span><span>        case</span><span> .</span><span>dime</span><span>: </span><span>10</span><br></span></p><p><span><span>        case</span><span> .</span><span>quarter</span><span>: </span><span>25</span><br></span></p><p><span><span>        }</span><br></span></p><p><span><span>    }</span><br></span></p><p><span><span>}</span><br></span></p></code></pre></div>
<a id="optional-types" href="#optional-types"><h4><span>#</span>Optional Types</h4></a>
<p>Rust doesn’t have <code>null</code>, but it does have <code>None</code>. Swift has a <code>nil</code>, but it’s really just a <code>None</code> in hiding. Instead of an <code>Option&lt;T&gt;</code>, Swift let’s you use <code>T?</code>, but the compiler still forces you to check that the value is not <code>nil</code> before you can use it.</p>
<p>You get the same safety with more convenience since you can do this in Swift with an optional type:</p>
<div data-bright-theme="dark-plus"><pre><code><p><span><span>let</span><span> val: T?</span><br></span></p><p><span><span>if </span><span>let</span><span> val {</span><br></span></p><p><span><span>  // val is now of type `T`.</span><br></span></p><p><span><span>}</span><br></span></p></code></pre></div>
<p>Also, you’re not forced to wrap every value with a <code>Some(val)</code> before returning it. The Swift compiler takes care of that for you. A <code>T</code> will transparently be converted into a <code>T?</code> when needed.</p>
<a id="error-handling" href="#error-handling"><h4><span>#</span>Error Handling</h4></a>
<p>Rust doesn’t have <code>try-catch</code>. Instead it has a <code>Result</code> type which contains the success and error types.</p>
<p>Swift doesn’t have a <code>try-catch</code> either, but it does have <code>do-catch</code> and you have to use <code>try</code> before calling a function that could throw. Again, this is just deception for those developers coming from C-like languages. Swift’s error handling works exactly like Rust’s behind the scenes, but it is hidden in a clever, familiar syntax.</p>
<div data-bright-theme="dark-plus"><pre><code><p><span><span>func </span><span>usesErrorThrowingFunction</span><span>() </span><span>throws</span><span> {</span><br></span></p><p><span><span>  let</span><span> x = </span><span>try </span><span>thisFnCanThrow</span><span>()</span><br></span></p><p><span><span>}</span><br></span></p><p><span><span>func </span><span>handlesErrors</span><span>() {</span><br></span></p><p><span><span>  do</span><span> {</span><br></span></p><p><span><span>    let</span><span> x = </span><span>try </span><span>thisFnCanThrow</span><span>()</span><br></span></p><p><span><span>  } </span><span>catch </span><span>err</span><span> {</span><br></span></p><p><span><span>    // handle the `err` here.</span><br></span></p><p><span><span>  }</span><br></span></p><p><span><span>}</span><br></span></p></code></pre></div>
<p>This is very similar to how Rust let’s you use <code>?</code> at the end of statements to automatically forward errors, but you don’t have to wrap your success values in <code>Ok()</code>.</p>
<a id="rusts-compiler-catches-problems-swifts-compiler-solves-some-of-them" href="#rusts-compiler-catches-problems-swifts-compiler-solves-some-of-them"><h3><span>#</span>Rust’s compiler catches problems. Swift’s compiler solves some of them</h3></a>
<p>There are many common problems that Rust’s compiler will catch at compile time and even suggest solutions for you. The example that portrays this well is self-referencing enums.</p>
<p>Consider an enum that represents a tree. Since, it is a recursive type, Rust will force you to use something like <code>Box&lt;&gt;</code> for referencing a type within itself.</p>
<div data-bright-theme="dark-plus"><pre><code><p><span><span>enum </span><span>TreeNode</span><span>&lt;</span><span>T</span><span>&gt; {</span><br></span></p><p><span><span>    Leaf</span><span>(</span><span>T</span><span>),</span><br></span></p><p><span><span>    Branch</span><span>(</span><span>Vec</span><span>&lt;</span><span>Box</span><span>&lt;</span><span>TreeNode</span><span>&lt;</span><span>T</span><span>&gt;&gt;&gt;),</span><br></span></p><p><span><span>}</span><br></span></p></code></pre></div>
<p>(You could also us <code>Box&lt;Vec&lt;TreeNode&lt;T&gt;&gt;&gt;</code> instead)</p>
<p>This makes the problem explicit and forces you to deal with it directly. Swift is a little more, <em>automatic</em>.</p>
<div data-bright-theme="dark-plus"><pre><code><p><span><span>indirect enum </span><span>TreeNode</span><span>&lt;</span><span>T</span><span>&gt; {</span><br></span></p><p><span><span>    case </span><span>leaf</span><span>(T)</span><br></span></p><p><span><span>    case </span><span>branch</span><span>([TreeNode&lt;T&gt;])</span><br></span></p><p><span><span>}</span><br></span></p></code></pre></div>
<p><strong>Note</strong>: that you still have to annotate this <code>enum</code> with the <code>indirect</code> keyword to indicate that it is recursive. But once you’ve done that, Swift’s compiler takes care of the rest. You don’t have to think about <code>Box&lt;&gt;</code> or <code>Rc&lt;&gt;</code>. The values just work normally.</p>
<a id="swift-is-less-pure" href="#swift-is-less-pure"><h3><span>#</span>Swift is less “pure”</h3></a>
<p>Swift was designed to replace Objective-C and needed to be able to interface with existing code. So, it has made a lot of pragmatic choices that makes it a much less “pure” and “minimalist” language. Swift is a pretty big language compared to Rust and has many more features built-in. However, Swift is designed with “progressive disclosure” in mind which means that just as soon as you think you’ve learned the language a little more of the iceberg pops out of the water.</p>
<p>Here are just <em>some</em> of the language features:</p>
<ul>
<li>Classes / Inhertence</li>
<li>async-await</li>
<li>async-sequences</li>
<li>actors</li>
<li>getters and setters</li>
<li>lazy properties</li>
<li>property wrappers</li>
<li>Result Builders (for building tree-like structures. e.g. HTML / SwiftUI)</li>
</ul>
<a id="convenience-has-its-costs" href="#convenience-has-its-costs"><h3><span>#</span>Convenience has its costs</h3></a>
<p>Swift is a far easier language to get started and productive with. The syntax is more familiar and a lot more is done for you automatically. But this really just makes Swift a higher-level language and it comes with the same tradeoffs.</p>
<p>By default, a Rust program is much faster than a Swift program. This is because Rust is fast by default, and <em>lets</em> you be slow, while Swift is easy by default and <em>lets</em> you be fast.</p>
<p>Based on this, I would say both languages have their uses. Rust is better for systems and embedded programming. It’s better for writing compilers and browser engines (Servo) and it’s better for writing entire operating systems.</p>
<p>Swift is better for writing UI and servers and some parts of compilers and operating systems. Over time I expect to see the overlap get bigger.</p>
<a id="the-cross-platform-problem" href="#the-cross-platform-problem"><h3><span>#</span>The “cross-platform” problem</h3></a>
<p>There is a perception that Swift is only a good language for Apple platforms. While this was once true, this is no longer the case and Swift is becoming increasingly a good cross-platform language. Hell, Swift even compiles to wasm, and the forks made by the swift-wasm team were merged back into Swift core earlier this year.</p>
<p>Swift on Windows is being used by The Browser Company to share code and bring the Arc browser to windows. Swift on Linux has long been supported by Apple themselves in order to push “Swift on Server”. Apple is directly sponsoring the Swift on Server conference.</p>
<p>This year Embedded Swift was also announced which is already being used on small devices like the Panic Playdate.</p>
<p>Swift website has been highlighting many of these projects:</p>
<ul>
<li><a href="https://www.swift.org/blog/swift-everywhere-windows-interop/">Swift on Windows</a></li>
<li><a href="https://www.swift.org/blog/embedded-swift-examples/">Embedded Swift</a></li>
<li><a href="https://www.swift.org/blog/adwaita-swift/">Gnome apps with Swift on Linux</a></li>
<li><a href="https://www.swift.org/blog/byte-sized-swift-tiny-games-playdate/">Swift on Playdate</a></li>
</ul>
<p>The browser company says that <a href="https://speakinginswift.substack.com/p/interoperability-swifts-super-power">Interoperability is Swift’s super power</a>.</p>
<p>And the Swift project has been trying make working with Swift a great experience outside of XCode with projects like an open source LSP and funding the the VSCode extension.</p>
<!-- -->
<!--$?--><template id="B:0"></template><!--/$-->
<a id="swift-is-not-a-perfect-language" href="#swift-is-not-a-perfect-language"><h3><span>#</span>Swift is not a perfect language.</h3></a>
<p>Compile times are (like Rust) quite bad. There is some amount of feature creep and the language is larger than it should be. Not all syntax feels familiar. The <a href="https://swiftpackageindex.com/">package ecosystem</a> isn’t nearly as rich as Rust.</p>
<p>But the “Swift is only for Apple platforms” is an old and tired cliche at this point. Swift is already a cross-platform, ABI-stable language with no GC, automatic Reference Counting and the option to opt into ownership for even more performance. Swift packages increasingly work on Linux. Foundation was ported to Swift, open sourced and made open source. It’s still early days for Swift as a good, more convenient, Rust alternative for cross-platform development, but it is here now. It’s no longer a future to wait for.</p><!--$--><!--/$--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Generative AI and Wikipedia editing: What we learned in 2025 (169 pts)]]></title>
            <link>https://wikiedu.org/blog/2026/01/29/generative-ai-and-wikipedia-editing-what-we-learned-in-2025/</link>
            <guid>46840924</guid>
            <pubDate>Sat, 31 Jan 2026 21:14:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wikiedu.org/blog/2026/01/29/generative-ai-and-wikipedia-editing-what-we-learned-in-2025/">https://wikiedu.org/blog/2026/01/29/generative-ai-and-wikipedia-editing-what-we-learned-in-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=46840924">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		<p>Like many organizations, Wiki Education has grappled with generative AI, its impacts, opportunities, and threats, for several years. As an organization that runs large-scale programs to bring new editors to Wikipedia (we’re responsible for about&nbsp;<a href="https://wikiedu.org/blog/2020/10/05/wiki-education-brings-19-of-english-wikipedias-new-active-editors/" rel="nofollow">19% of all new active editors on English Wikipedia</a>), we have deep understanding of what challenges face new content contributors to Wikipedia — and how to support them to successfully edit. As many people have begun using generative AI chatbots like ChatGPT, Gemini, or Claude in their daily lives, it’s unsurprising that people will also consider using them to help draft contributions to Wikipedia. Since Wiki Education’s programs provide a cohort of content contributors whose work we can evaluate, we’ve looked into how our participants are using GenAI tools.</p>
<p>We are choosing to share our perspective through this blog post because we hope it will help inform discussions of GenAI-created content on Wikipedia. In an open environment like the Wikimedia movement, it’s important to share what you’ve learned. In this case, we believe our learnings can help Wikipedia editors who are trying to protect the integrity of content on the encyclopedia, Wikipedians who may be interested in using generative AI tools themselves, other program leaders globally who are trying to onboard new contributors who may be interested in using these tools, and the Wikimedia Foundation, whose product and technology team builds software to help support the development of high-quality content on Wikipedia.</p>
<p>Our fundamental conclusion about generative AI is: Wikipedia editors should never copy and paste the output from generative AI chatbots like ChatGPT into Wikipedia articles.</p>
<p>Let me explain more.</p>
<h4><span id="AI_detection_and_investigation">AI detection and investigation</span></h4>
<p>Since the launch of ChatGPT in November 2022, we’ve been paying close attention to GenAI-created content, and how it relates to Wikipedia. We’ve spot-checked work of new editors from our programs, primarily focusing on citations to ensure they were real and not hallucinated. We experimented with tools ourselves, we led video sessions about GenAI for our program participants, and we closely tracked&nbsp;<a href="https://en.wikipedia.org/wiki/Wikipedia:Artificial_intelligence#Discussion_timeline" rel="nofollow">on-wiki policy discussions around GenAI</a>. Currently, English Wikipedia prohibits the use of generative AI to create images or in talk page discussions, and recently adopted a&nbsp;<a href="https://en.wikipedia.org/wiki/Wikipedia:Writing_articles_with_large_language_models" rel="nofollow">guideline against using large language models to generate new articles</a>.</p>
<p>As our Wiki Experts Brianda Felix and Ian Ramjohn worked with program participants throughout the first half of 2025, they found more and more text bearing the hallmarks of generative AI in article content, like bolded words or bulleted lists in odd places. But the use of generative AI wasn’t necessarily problematic, as long as the content was accurate. Wikipedia’s open editing process encourages stylistic revisions to factual text to better fit Wikipedia’s style.</p>
<div>
<div>
<p>But&nbsp;<i>was</i>&nbsp;the text factually accurate? This fundamental question led our Chief Technology Officer, Sage Ross, to investigate different generative AI detectors. He landed on a tool called&nbsp;<a href="https://www.pangram.com/" rel="nofollow">Pangram</a>, which we have found to be highly accurate for Wikipedia text. Sage generated a list of all the new articles created through our work since 2022, and ran them all through Pangram. A total of 178 out of the 3,078 articles came back as flagged for AI — none before the launch of ChatGPT in late 2022, with increasing percentages term over term since then. About half of our staff spent a month during summer 2025 painstakingly reviewing the text from these 178 articles.</p>
<figure id="attachment_137635" aria-describedby="caption-attachment-137635"><img decoding="async" src="https://wikiedu.org/wp-content/uploads/2026/01/AI_detection_of_student_edits_by_term-500x329.png" alt="Pangram's detection results showed no signs of AI usage before the launch of ChatGPT, and then a steady rise in usage in the terms following. Courtesy of Manoel Horta Ribeiro and Francesco Salvi." width="797" height="524" srcset="https://wikiedu.org/wp-content/uploads/2026/01/AI_detection_of_student_edits_by_term-500x329.png 500w, https://wikiedu.org/wp-content/uploads/2026/01/AI_detection_of_student_edits_by_term-1024x674.png 1024w, https://wikiedu.org/wp-content/uploads/2026/01/AI_detection_of_student_edits_by_term-768x506.png 768w, https://wikiedu.org/wp-content/uploads/2026/01/AI_detection_of_student_edits_by_term-1536x1011.png 1536w, https://wikiedu.org/wp-content/uploads/2026/01/AI_detection_of_student_edits_by_term.png 1619w" sizes="(max-width: 797px) 100vw, 797px"><figcaption id="caption-attachment-137635">Pangram’s detection results showed no signs of AI usage before the launch of ChatGPT, and then a steady rise in usage in the terms following. Courtesy of Manoel Horta Ribeiro and Francesco Salvi.</figcaption></figure>
</div>

<div>
<p>Based on the discourse around AI hallucinations, we were expecting these articles to contain citations to sources that didn’t exist, but this wasn’t true: only 7% of the articles had fake sources. The rest had information cited to real, relevant sources.</p>
<p>Far more insidious, however, was something else we discovered:&nbsp;<b>More than two-thirds of these articles failed verification.</b>&nbsp;That means the article contained a plausible-sounding sentence, cited to a real, relevant-sounding source. But when you read the source it’s cited to, the information on Wikipedia does not exist in that specific source. When a claim fails verification, it’s impossible to tell whether the information is true or not. For most of the articles Pangram flagged as written by GenAI, nearly every cited sentence in the article failed verification.</p>
</div>
</div>
<p>This finding led us to invest significant staff time into cleaning up these articles — far more than these editors had likely spent creating them. Wiki Education’s core mission is to improve Wikipedia, and when we discover our program has unknowingly contributed to misinformation on Wikipedia, we are committed to cleaning it up. In the clean-up process, Wiki Education staff moved more recent work back to sandboxes, we stub-ified articles that passed notability but mostly failed verification, and we PRODed some articles that from our judgment weren’t salvageable. All these are ways of addressing Wikipedia articles with flaws in their content. (While there are many grumblings about Wikipedia’s deletion processes, we found several of the articles we PRODed due to their fully hallucinated GenAI content were then de-PRODed by other editors, showing the diversity of opinion about generative AI among the Wikipedia community.</p>
<h4><span id="Revising_our_guidance">Revising our guidance</span></h4>
<p>Given what we found through our investigation into the work from prior terms, and given the increasing usage of generative AI, we wanted to proactively address generative AI usage within our programs. Thanks to in-kind support from our friends at Pangram, we began running our participants’ Wikipedia edits, including in their sandboxes, through Pangram nearly in real time. This is possible because of the&nbsp;<a href="https://dashboard.wikiedu.org/" rel="nofollow">Dashboard course management platform</a>&nbsp;Sage built, which tracks edits and generates tickets for our Wiki Experts based on on-wiki edits.</p>
<p>We created a brand-new training module on&nbsp;<a href="https://dashboard.wikiedu.org/training/students/generative-ai" rel="nofollow">Using generative AI tools with Wikipedia</a>. This training emphasizes where participants could use generative AI tools in their work, and where they should not. The core message of these trainings is, do not copy and paste anything from a GenAI chatbot into Wikipedia.</p>
<p>We crafted a variety of automated emails to participants who Pangram detected were adding text created by generative AI chatbots. Sage also recorded some videos, since many young people are accustomed to learning via video rather than reading text. We also provided opportunities for engagement and conversation with program participants.</p>
<h4><span id="Our_findings_from_the_second_half_of_2025">Our findings from the second half of 2025</span></h4>
<p>In total, we had 1,406 AI edit alerts in the second half of 2025, although only 314 of these (or 22%) were in the article namespace on Wikipedia (meaning edits to live articles). In most cases, Pangram detected participants using GenAI in their sandboxes during early exercises, when we ask them to do things like choose an article, evaluate an article, create a bibliography, and outline their contribution.</p>
<figure id="attachment_137639" aria-describedby="caption-attachment-137639"><img decoding="async" src="https://wikiedu.org/wp-content/uploads/2026/01/AI_edit_alerts_by_type_Fall_2025-500x181.png" alt="This graph shows the daily total of Pangram's detected generative AI text our participants added to Wikipedia. Early in the term, the hits were primarily to exercises, with more sandbox and mainspace alerts later in the term." width="944" height="342" srcset="https://wikiedu.org/wp-content/uploads/2026/01/AI_edit_alerts_by_type_Fall_2025-500x181.png 500w, https://wikiedu.org/wp-content/uploads/2026/01/AI_edit_alerts_by_type_Fall_2025-1024x372.png 1024w, https://wikiedu.org/wp-content/uploads/2026/01/AI_edit_alerts_by_type_Fall_2025-768x279.png 768w, https://wikiedu.org/wp-content/uploads/2026/01/AI_edit_alerts_by_type_Fall_2025.png 1188w" sizes="(max-width: 944px) 100vw, 944px"><figcaption id="caption-attachment-137639">This graph shows the daily total of Pangram’s detected generative AI text our participants added to Wikipedia. Early in the term, the hits were primarily to exercises, with more sandbox and mainspace alerts later in the term. CC BY-SA 4.0 — Wiki Education.</figcaption></figure>
<p>Pangram struggled with false positives in a few sandbox scenarios:</p>
<ul>
<li>Bibliographies, which are often a combination of human-written prose (describing a source and its relevance) and non-prose text (the citation for a source, in some standard format)</li>
<li>Outlines with a high portion of non-prose content (such as bullet lists, section headers, text fragments, and so on)</li>
</ul>
<p>We also had a handful of cases where sandboxes were flagged for AI after a participant copied an AI-written section from an existing article to use as a starting point to edit or to expand. (This isn’t a flaw of Pangram, but a reminder of how much AI-generated content editors outside our programs are adding to Wikipedia!)</p>
<p>In broad strokes, we found that Pangram is great at analyzing plain prose — the kind of sentences and paragraphs you’ll find in the body of a Wikipedia article — but sometimes it gets tripped up by formatting, markup, and non-prose text. Early on, we disabled alert emails for participants’ bibliography and outline exercises, and throughout the end of 2025, we refined the Dashboard’s preprocessing steps to extract the prose portions of revisions and convert them to plain text before sending them to Pangram.</p>
<p>Many participants also reported “just using Grammarly to copy edit.” In our experience, however, the smallest fixes done with Grammarly never trigger Pangram’s detection, but if you use its more advanced content creation features, the resulting text registers as being AI generated.</p>
<p>But overwhelmingly, we were pleased with Pangram’s results. Our early interventions with participants who were flagged as using generative AI for exercises that would not enter mainspace seemed to head off their future use of generative AI. We supported 6,357 new editors in fall 2025, and only 217 of them (or 3%) had multiple AI alerts. Only 5% of the participants we supported had mainspace AI alerts. That means thousands of participants successfully edited Wikipedia without using generative AI to draft their content.</p>
<p>For those who did add GenAI-drafted text, we ensured that the content was reverted. In fact, participants sometimes self-reverted once they received our email letting them know Pangram had detected their contributions as being AI created. Instructors also jumped in to revert, as did some Wikipedians who found the content on their own. Our ticketing system also alerted our Wiki Expert staff, who reverted the text as soon as they could.</p>
<p>While some instructors in our Wikipedia Student Program had concerns about AI detection, we had a lot of success focusing the conversation on the concept of verifiability. If the instructor as subject matter expert could attest the information was accurate, and they could find the specific facts in the sources they were cited to, we permitted text to come back to Wikipedia. However, the process of attempting to verify student-created work (which in many cases the students swore they’d written themselves) led many instructors to realize what we had found in our own assessment: In their current states, GenAI-powered chatbots cannot write factually accurate text for Wikipedia that is verifiable.</p>
<p>We believe our Pangram-based detection interventions led to fewer participants adding GenAI-created content to Wikipedia. Following the trend lines, we anticipated about 25% of participants to add GenAI content to Wikipedia articles; instead, it was only 5%, and our staff were able to revert all problematic content.</p>
<p>I’m deeply appreciative of everyone who made this success possible this term: Participants who followed our recommendations, Pangram who gave us access to their detection service, Wiki Education staff who did the heavy lift of working with all of the positive detections, and the Wikipedia community, some of whom got to the problematic work from our program participants before we did.</p>
<h4><span id="How_can_generative_AI_help.3F">How can generative AI help?</span></h4>
<p>So far, I’ve focused on the problems with generative AI-created content. But that’s not all these tools can do, and we did find some ways they were useful. Our training module encourages editors — if their institution’s policies permit it — to consider using generative AI tools for:</p>
<ul>
<li>Identifying gaps in articles</li>
<li>Finding access to sources</li>
<li>Finding relevant sources</li>
</ul>
<p>To evaluate the success of these use scenarios, we worked directly with 7 of the classes we supported in fall 2025 in our Wikipedia Student Program. We asked students to anonymously fill out a survey every time they used generative AI tools in their Wikipedia work. We asked what tool they used, what prompt they used, how they used the output, and whether they found it helpful. While some students filled the survey out multiple times, others filled it out once. We had 102 responses reporting usage at various stages in the project. Overwhelmingly, 87% of the responses who reported using generative AI said it was helpful for them in the task. The most popular tool by far was ChatGPT, with Grammarly as a distant second, and the others in the single-digits of usage.</p>
<p>Students reported AI tools very helpful in:</p>
<ul>
<li>Identifying articles to work on that were relevant to the course they were taking</li>
<li>Highlighting gaps within existing articles, including missing sections or more recent information that was missing</li>
<li>Finding reliable sources that they hadn’t already located</li>
<li>Pointing to which database a certain journal article could be found</li>
<li>When prompted with the text they had drafted and the checklist of requirements, evaluating the draft against those requirements</li>
<li>Identifying categories they could add to the article they’d edited</li>
<li>Correcting grammar and spelling mistakes</li>
</ul>
<p>Critically, no participants reported using AI tools to draft text for their assignments. One student reported: “I pasted all of my writing from my sandbox and said ‘Put this in a casual, less academic tone’ … I figured I’d try this but it didn’t sound like what I normally write and I didn’t feel that it captured what I was trying to get across so I scrapped it.”</p>
<p>While this was an informal research project, we received enough positive feedback from it to believe using ChatGPT and other tools can be helpful in the research stage if editors then critically evaluate the output they get, instead of blindly accepting it. Even participants who found AI helpful reported that they didn’t use everything it gave them, as some was irrelevant. Undoubtedly, it’s crucial to maintain the human thinking component throughout the process.</p>
<h4><span id="What_does_this_all_mean_for_Wiki_Education.3F">What does this all mean for Wiki Education?</span></h4>
<p>My conclusion is that, at least as of now, generative AI-powered chatbots like ChatGPT should never be used to generate text for Wikipedia; too much of it will simply be unverifiable. Our staff would spend far more time attempting to verify facts in AI-generated articles than if we’d simply done the research and writing ourselves.</p>
<p>That being said, AI tools can be helpful in the research process, especially to help identify content gaps or sources, when used in conjunction with a human brain that carefully evaluates the information. Editors should never simply take a chatbot’s suggestion; instead, if they want to use a chatbot, they should use it as a brainstorm partner to help them think through their plans for an article.</p>
<p>To date, Wiki Education’s interventions as our program participants edit Wikipedia show promise for keeping unverifiable, GenAI-drafted content off Wikipedia. Based on our experiences in the fall term, we have high confidence in Pangram as a detector of AI content, at least in Wikipedia articles. We will continue our current strategy in 2026 (with more small adjustments to make the system as reliable as we can).</p>
<p>More generally, we found participants had less AI literacy than popular discourse might suggest. Because of this, we created a supplemental&nbsp;<a href="https://dashboard.wikiedu.org/training/students/large-language-models" rel="nofollow">large language models</a>&nbsp;training that we’ve offered as an optional module for all participants. Many participants indicated that they found our guidance regarding AI to be welcome and helpful as they attempt to navigate the new complexities created by AI tools.</p>
<p>We are also looking forward to more research on our work. A team of researchers — Francesco Salvi and Manoel Horta Ribeiro at Princeton University, Robert Cummings at the University of Mississippi, and Wiki Education’s Sage Ross — have been looking into Wiki Education’s Wikipedia Student Program editors’ use of generative AI over time. Preliminary results have backed up our anecdotal understanding, while also revealing nuances of how text produced by our students over time has changed with the introduction of GenAI chatbots. They also confirmed our belief in Pangram: After running student edits from 2015 up until the launch of ChatGPT through Pangram, without any date information involved, the team found Pangram correctly identified that it was all 100% human written. This research will continue into the spring, as the team explores ways of unpacking the effects of AI on different aspects of article quality.</p>
<p>And, of course, generative AI is a rapidly changing field. Just because these were our findings in 2025 doesn’t mean they will hold true throughout 2026. Wiki Education remains committed to monitoring, evaluating, iterating, and adapting as needed. Fundamentally, we are committed to ensuring we add high quality content to Wikipedia through our programs. And when we miss the mark, we are committed to cleaning up any damage.</p>
<h4><span id="What_does_this_all_mean_for_Wikipedia.3F">What does this all mean for Wikipedia?</span></h4>
<p>While I’ve focused this post on what Wiki Education has learned from working with our program participants, the lessons are extendable to others who are editing Wikipedia. Already,&nbsp;<a href="https://www.nber.org/papers/w34255" rel="nofollow">10% of adults worldwide</a>&nbsp;are using ChatGPT, and drafting text is one of the top use cases. As generative AI usage proliferates, its usage by well-meaning people to draft content for Wikipedia will as well. It’s unlikely that longtime, daily Wikipedia editors would add content copied and pasted from a GenAI chatbot without verifying all the information is in the sources it cites. But many casual Wikipedia contributors or new editors may unknowingly add bad content to Wikipedia when using a chatbot. After all, it provides what looks like accurate facts, cited to what are often real, relevant, reliable sources. Most edits we ended up reverting seemed acceptable with a cursory review; it was only after we attempted to verify the information that we understood the problems.</p>
<p>Because this unverifiable content often seems okay at first pass, it’s critical for Wikipedia editors to be equipped with tools like Pangram to more accurately detect when they should take a closer look at edits. Automating review of text for generative AI usage — as Wikipedians have done for copyright violation text for years —&nbsp;would help protect the integrity of Wikipedia content. In Wiki Education’s experience, Pangram is a tool that could provide accurate assessments of text for editors, and we would love to see a larger scale version of the tool we built to evaluate edits from our programs to be deployed across all edits on Wikipedia. Currently, editors can add a warning banner that highlights that the text might be LLM generated, but this is based solely on the assessment of the person adding the banner. Our experience suggests that judging by tone alone isn’t enough; instead, tools like Pangram can flag highly problematic information that should be reverted immediately but that might sound okay.</p>
<p>We’ve also found success in the training modules and support we’ve created for our program participants. Providing clear guidance — and the reason&nbsp;<i>why</i>&nbsp;that guidance exists — has been key in helping us head off poor usage of generative AI text. We encourage Wikipedians to consider revising guidance to new contributors in the welcome messages to emphasize the pitfalls of adding GenAI-drafted text. Software aimed at new contributors created by the Wikimedia Foundation should center starting with a list of sources and drawing information from them, using human intellect, instead of generative AI, to summarize information. Providing guidance upfront can help well-meaning contributors steer clear of bad GenAI-created text.</p>
<p>Wikipedia recently celebrated its 25th birthday. For it to survive into the future, it will need to adapt as technology around it changes. Wikipedia would be nothing without its corps of volunteer editors. The consensus-based decision-making model of Wikipedia means change doesn’t come quickly, but we hope this deep-dive will help spark a conversation about changes that are needed to protect Wikipedia into the future.</p>

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Outsourcing thinking (166 pts)]]></title>
            <link>https://erikjohannes.no/posts/20260130-outsourcing-thinking/index.html</link>
            <guid>46840865</guid>
            <pubDate>Sat, 31 Jan 2026 21:06:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://erikjohannes.no/posts/20260130-outsourcing-thinking/index.html">https://erikjohannes.no/posts/20260130-outsourcing-thinking/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=46840865">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

<time>30 Jan 2026</time><p><em>First, a note to the reader: This blog post is longer than usual, as I decided to address multiple connected issues in the same post, without being too restrictive on length. With modern browsing habits and the amount of available online media, I suspect this post will be quickly passed over in favor of more interesting reading material. Before you immediately close this tab, I invite you to scroll down and read the conclusion, which hopefully can give you some food for thought along the way. If, however, you manage to read the whole thing, I applaud your impressive attention span.</em></p>
<p>A common criticism of the use of large language models (LLMs) is that it can deprive us of cognitive skills. The typical argument is that outsourcing certain tasks can easily cause some kind of mental atrophy. To what extent this is true is an ongoing discussion among neuroscientists, psychologists and others, but to me, the understanding that with certain skills you have to "use it or lose it" seems intuitively and empirically sound.</p>
<p>The more relevant question is whether certain kinds of use are better or worse than others, and if so, which? In the blog post <a href="https://andymasley.substack.com/p/the-lump-of-cognition-fallacy">The lump of cognition fallacy</a>, Andy Masley discusses this in detail. His entry point to the problem is to challenge the idea that "there is a fixed amount of thinking to do", and how it leads people to the conclusion that "outsourcing thinking" to chatbots will make us lazy, less intelligent, or in other ways be negative for our cognitive abilities. He compares this to the misconception that there is only a finite amount of work that needs to be done in an economy, which often is referred to as "the lump of labour fallacy". His viewpoint is that "thinking often leads to more things to think about", and therefore we shouldn't worry about letting machines do the thinking for us — we will simply be able to think about other things instead.</p>
<p>Reading Masley's blog post prompted me to write down my own thoughts on the matter, as it has been churning in my mind for a long time. I realized that it could be constructive to use his blog post as a reference and starting point, because it contains arguments that are often brought up in this discussion. I will use some examples from Masley's post to show how I think differently about this, but I'll extend the scope beyond the claimed fallacy that there is a limited amount of thinking to be done. I have done my best to write this text in a way that does not require reading Masley's post first. My aim is not to refute all of his arguments, but to explain why the issue is much more complicated than "thinking often leads to more things to think about". Overall, the point of this post is to highlight some critical issues with "outsourcing thinking".</p>
<h3 id="when-should-we-avoid-using-generative-language-models">When should we avoid using generative language models?</h3>
<p>Is it possible to define categories of activities where the use of LLMs (typically in the form of chatbots) is more harmful than helpful? Masley lists certain cases where, in his view, it is obviously detrimental to outsource thinking. To fully describe my own perspective, I'll take the liberty to quote the items on his list. He writes it's "bad to outsource your cognition when it:"</p>
<blockquote>
<ul>
<li>Builds complex tacit knowledge you'll need for navigating the world in the future.</li>
<li>Is an expression of care and presence for someone else.</li>
<li>Is a valuable experience on its own.</li>
<li>Is deceptive to fake.</li>
<li>Is focused in a problem that is deathly important to get right, and where you don't totally trust who you're outsourcing it to.</li>
</ul>
</blockquote>
<p>I was surprised to discover that we are to a large extent in agreement on this list, despite having fundamentally different views otherwise. The disagreement lies, I believe, in the amount of activities that fall within the categories outlined above, particularly three of them.</p>
<h3 id="personal-communication-and-writing">Personal communication and writing</h3>
<p>Let's start with the point "Is deceptive to fake". Masley uses the example of:</p>
<blockquote>
<p>If someone’s messaging you on a dating app, they want to know what you’re actually like.</p>
</blockquote>
<p>Very true, but in my view, it's not only in such intimate or private situations where it is deceptive to fake what you are like. Personal communication in general is an area where it matters how we express ourselves, both for ourselves and those we talk or write to. When we communicate with each other, there are certain expectations framing the whole exchange. Letting our words and phrases be transformed by a machine is a breach of those expectations. The words we choose and how we formulate our sentences carry a lot of meaning, and direct communication will suffer if we let language models pollute this type of interaction. Direct communication is not only about the information being exchanged, it's also about the relationship between the communicators, formed by who we are and how we express ourselves. </p>
<p>I think this is not only relevant for communication between two humans, but also for text with a personal sender conveyed to a human audience in general. To a certain extent, the same principles apply. There has been a debate in the Norwegian media lately regarding the undisclosed use of LLMs in public writing, with allegations and opinions flying around. I'm very happy to see this discussion reaching broad daylight, because we need to clarify our expectations to communication, now that chatbots are being so widely used. While I clearly think that it is beneficial to keep human-to-human communication free from an intermediate step of machine transformation, not everyone shares that view. If, going forward, our written communication will for the most part be co-authored with AI models, we need to be aware of it, and shift our expectations accordingly. Some have started disclosing when they have used AI in their writing, which I think is a good step towards better understanding of our use of LLMs. Knowing whether a text is written or "co-authored" by an LLM has an important effect on how a receiver views it; pretending otherwise is simply false.</p>
<p>Many see LLMs as a great boon for helping people express their opinions more clearly, particularly for people not using their native language or those who have learning disabilities. As long as the meaning originates from a person, LLMs can help express that meaning in correct and effective language. I have two main objections against this. The first one is about what happens to the text: In most cases it's impossible to separate the meaning from the expression of it. That is in essence what language is — the words <em>are</em> the meaning. Changing the phrasing changes the message. The second one is about what happens to us: We rob ourselves of the opportunity to grow and learn, without training wheels. LLMs can certainly help people improve the text, but the thinking process — developing the ideas — will be severely amputated when leaving the phrasing up to an AI model. They quickly become a replacement instead of help, depriving us the opportunity of discovering our own voice and who we can be and become when we stand on our own two feet.</p>
<p>With great care, one may be able to use a chatbot without being affected by these two drawbacks, but the problem is that with LLMs, there is an exceptionally thin line between getting help with spelling or grammar, and having the model essentially write <em>for</em> you, thereby glossing over your own voice. This is unavoidable with the current design of chatbots and LLM-powered tools; the step from old-school autocorrect to a generative language model is far too big. If we really envision LLMs as a tool for helping people become better at writing, we need to have a much more carefully considered interface than the chatbots we have today.</p>
<p>At the same time, I realize many are far more utilitarian. They just want to get the job done, finish their work, file that report, get that complaint through, answer that email, in the most efficient way possible, and then get on with their day. Getting help from an LLM to express oneself in a second language also seems useful, without considering how much or little one learns from it (I would be more positive to LLMs for translation if it wasn't for the fact that current state-of-the-art LLMs are simply <a href="https://sprakradet.no/wp-content/uploads/Rapport-fra-test-av-sprakroboter-2025.pdf">very bad at producing Norwegian text</a>. I can only hope the state is better for other non-English languages, or that it will improve over time). Additionally, LLMs seem to be efficient for people who are fighting with bureaucracy, such is filing complaints and dealing with insurance companies. In this case the advantage seems greater. We must, however, remember that the "weapon" exists on both sides of the table. What will happen to bureaucratic processes when all parties involved are armed with word generators? </p>
<p>It is not without reservation that I express these opinions, because it may come across as I want to deny people something that looks like a powerful tool. The point is that I think this tool will make you weaker, not stronger. LLMs don't really seem to empower people. Some of the effect I currently see is the number of applications to various calls (internships, research proposals, job openings) multiplying, but the quality dropping. Students are asking chatbots for help with solving collaborative tasks, not realizing that everyone is asking the same chatbot, robbing us of the diversity of ideas that could have formed if they took a minute to think for themselves.</p>
<p>The chatbots may have lowered the threshold for participation, but the competition's ground rules hasn't changed. To get better at writing, you need to write. The same goes for thinking. Applying for a job means showing who <em>you</em> are, not who the LLM thinks you are, or should be.
Participating in the public debate <em>is</em> having to work out how to express opinions in clear language. Am I really participating if I'm not finding my own words?</p>
<p>It is important to note that not all text is affected in the same way. The category of writing that I like to call "functional text", which are things like computer code and pure conveyance of information (e.g., recipes, information signs, documentation), is not exposed to the same issues. But text that has a personal author addressing a human audience, has particular role expectations and rests on a particular trust. An erosion of that trust will be a loss for humanity.</p>
<p>A pragmatic attitude would be to just let the inflation of text ensue, and take stock after the dust has settled. What will be left of language afterwards? My conservative viewpoint stems from believing that what we will lose is of greater worth than what we gain. While LLMs can prove useful in the short term, using them is treating a symptom instead of the problem. It is a crutch, although some may truly be in need of that crutch. My only advice would be to make sure you actually need it before you lean on it.</p>
<h3 id="valuable-experiences">Valuable experiences</h3>
<p>Using LLMs is not only about writing. Masley mentions that it's bad to outsource activities that are "a valuable experience on its own". I couldn't agree more, but I suspect that he will disagree when I say that I think this category encompasses a lot of what we already do in life. Major LLM providers love to show how their chatbots can be used to plan vacations, organize parties, and create personal messages to friends and family. I seldom feel more disconnected from the technological society than when I watch these advertisements. </p>
<p>To me, this highlights a problem that goes to the core of what it means to be human. Modern life brings with it a great deal of activities that can feel like chores, but at the same time it seems like we are hell-bent on treating everything as a chore as well. Humans are surprisingly good at finding discontentment in nearly anything, maybe because of an expectation in modern society that we should be able to do anything we want, anytime we want it — or perhaps more importantly, that we should be able to avoid doing things we don't feel like doing. Our inability to see opportunities and fulfillment in life as it is, leads to the inevitable conclusion that life is never enough, and we would always rather be doing something else.</p>
<p>In theory, I agree that automating some things can free up time for other things that are potentially more meaningful and rewarding, but we have already reached a stage where even planning our vacation is a chore that apparently a lot of people would like to avoid doing. I hope that AI's alleged ability to automate "nearly anything" helps us realize what is worth spending time and effort on, and rediscover the value of intentional living.</p>
<h3 id="building-knowledge">Building knowledge</h3>
<p>The third point I would like to address is that we shouldn't use chatbots when it "builds complex tacit knowledge you'll need for navigating the world in the future", according to Masley. Again, I agree completely, and again, I think that this point encompasses a great deal of daily life. Building knowledge happens not only when you sit down to learn something new, but also when you do repetitive work. </p>
<p>This misconception is not new for chatbots, but has been present since we started carrying smartphones in our pockets. With internet at hand at all times, there's apparently no need to remember information anymore. Instead of using our brains for storing knowledge, we can access information online when we need it, and spend more time learning how to actually use the information and think critically. The point we are missing here, is that acquiring and memorizing knowledge is a huge part of learning to use the knowledge. It is naive to think that we can simply separate the storage unit from the processing unit, like if we were a computer.</p>
<p>I learned this lesson while being a piano student. I was trying to understand jazz, and figure out how good improvisers could learn to come up with new phrases so easily on the spot. How does one practice improvisation? Is it possible to exercise the ability to come up with something new that immediately sound good? I ended up playing similar riffs almost every time I tried. After a while I got convinced that good jazz players must be born with some inherent creativity, some inner musical inspiration that hummed melodies inside their heads for them to play. </p>
<p>One of my tutors taught me the real trick: Good improvisation comes not from just practicing improvisation. You need to play existing songs and tunes, many of them, over and over, learn them by heart, get the chord progressions and motifs under your skin. This practice builds your intuition for what sounds good, and your improvisation can spring from that. Bits and pieces of old melodies are combined into new music. In that sense, we are more like a machine learning model than a computer, but do not make the mistake of thinking that is actually <em>what</em> we are.</p>
<p>There is a need for clarification here: I'm not saying that <em>nothing</em> should be automated by LLMs. But I think many are severely underestimating the knowledge we are building from boring tasks, and we are in danger of losing that knowledge when the pressure for increased efficiency makes us turn to the chatbots.</p>
<h3 id="the-extended-mind">The extended mind</h3>
<p>As a sidenote, I would like to contest the idea of the extended mind,  as explained by Masley:</p>
<blockquote>
<p>[M]uch of our cognition isn’t limited to our skull and brain, it also happens in our physical environment, so a lot of what we define as our minds could also be said to exist in the physical objects around us.</p>
<p>It seems kind of arbitrary whether it’s happening in the neurons in your brain or in the circuits in your phone.</p>
</blockquote>
<p>This statement is simply absurd, even when read in context. The fact that something happens in your brain rather than on a computer makes all the difference in the world. Humans are something more than information processors. Yes, we process information, but it is extremely reductionist to treat ourselves as objects where certain processes can be outsourced to external devices without consequences. Does it really matter if I remember my friend's birthday, when I can have a chatbot send them an automated congratulation? Yes, it matters because in the first case you are consciously remembering and thinking about your friend, consolidating your side of the relationship.</p>
<p>The quoted statement above is followed up with:</p>
<blockquote>
<p>It’s true that you could lose your phone and therefore lose the stored knowledge, but you could also have a part of your brain cut out.</p>
</blockquote>
<p>Losing your phone and losing a part of your brain are two tremendously different things, both in terms of likelihood and consequences. Not only does the statement above significantly underestimate the processes that happens in our brain, but to even liken having a part of your brain cut out to losing your phone reveals that the premiss of the argument is severely detached from reality.</p>
<p>The design of our built environments is also brought up to show how it's beneficial to minimize the amount of thinking we do:</p>
<blockquote>
<p>[M]ost of our physical environments have been designed specifically to minimize the amount of thinking we have to do to achieve our daily goals.</p>
<p>Try to imagine how much additional thinking you would need to do if things were designed differently.</p>
</blockquote>
<p>This doesn't hold up to scrutiny. Yes, if our environment suddenly changed, it would require extra mental effort of us to navigate. For a time. But, then we would have gotten familiar with that alternative design, and adapted ourselves. The only case where we would have had to do additional thinking is if the design of our physical environments changed all the time. </p>
<h3 id="what-we-think-about-does-matter">What we think about does matter</h3>
<p>Regarding the "lump of cognition fallacy", I fully agree that we need not worry about "draining a finite pool" of thinking, leaving "less thinking" — whatever that means — for humans. There is, however, another fallacy at play here, which is that "it does not matter what we think about, as long as we think about <em>something</em>". It is easy to be convinced that if a computer can do the simple, boring tasks for me, I can deal with more complex, exciting stuff myself. But we must be aware that certain mental tasks are important for us to do, even though a machine technically could do them for us. </p>
<p>To illustrate: If I outsource all my boring project administration tasks to a chatbot, it can leave more time for my main task: research. But it will also rob me of the opportunity to feel ownership to the project and build a basis for taking high-level decisions in the project. In a hypothetical situation where a chatbot performs all administrative tasks perfectly on my behalf, <em>I</em> will still have lost something, which may again have impact on the project. I'm not saying that no tasks should be automated at all, but we must be aware that we always lose something when automating a process.</p>
<p>Comparing with the "lump of labour" fallacy again: While it may be true that outsourcing physical work to machines will simply create new types of work to do, it doesn't mean that the new work is useful, fulfilling, or beneficial for individuals and society. The same goes for thinking. We must acknowledge that all kinds of thinking have an effect on us, even the boring and tedious kinds. Removing the need for some cognitive tasks can have just as much influence, positive or negative, as taking up new types of cognitive tasks.</p>
<h3 id="conclusion">Conclusion</h3>
<p>We have a major challenge ahead of us in figuring out what chatbots are suitable for in the long term. Personal communication may change forever (that is to say, maybe it won't stay personal anymore), education systems will require radical adaptations, and we need to reflect more carefully about which experiences in life actually matter. What is truly exciting about this new type of technology, is that it forces us to face questions about our humanity and values. Many formerly theoretical questions of philosophy are becoming relevant for our daily lives.</p>
<p>A fundamental point I'm trying to bring forth is that how we choose to use chatbots is not only about efficiency and cognitive consequences; it's about how we want our lives and society to be. I have tried to argue that there are good reasons for protecting certain human activities against the automation of machines. This is in part based on my values, and does not rely on research into whether or not our efficiency at work or cognitive abilities are affected by it. I cannot tell other people what they should do, but I challenge everyone to consider what values they want to build our communities on, and let that weigh in alongside what the research studies tell us.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Data Processing Benchmark Featuring Rust, Go, Swift, Zig, Julia etc. (109 pts)]]></title>
            <link>https://github.com/zupat/related_post_gen</link>
            <guid>46840698</guid>
            <pubDate>Sat, 31 Jan 2026 20:50:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/zupat/related_post_gen">https://github.com/zupat/related_post_gen</a>, See on <a href="https://news.ycombinator.com/item?id=46840698">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<tr>
<td>Rust</td>
<td>-</td>
<td>4.5s</td>
<td>Initial</td>
</tr>
<tr>
<td>Rust v2</td>
<td>-</td>
<td>2.60s</td>
<td>Replace std HashMap with fxHashMap by <a href="https://www.reddit.com/r/rust/comments/16plgok/comment/k1rtr4x/?utm_source=share&amp;utm_medium=web2x&amp;context=3" rel="nofollow">phazer99</a></td>
</tr>
<tr>
<td>Rust v3</td>
<td>-</td>
<td>1.28s</td>
<td>Preallocate and reuse map and unstable sort by <a href="https://www.reddit.com/r/rust/comments/16plgok/comment/k1rzo7g/?utm_source=share&amp;utm_medium=web2x&amp;context=3" rel="nofollow">vdrmn</a> and <a href="https://www.reddit.com/r/rust/comments/16plgok/comment/k1rzwdx/?utm_source=share&amp;utm_medium=web2x&amp;context=3" rel="nofollow">Darksonn</a></td>
</tr>
<tr>
<td>Rust v4</td>
<td>-</td>
<td>0.13s</td>
<td>Use Post index as key instead of Pointer and Binary Heap by <a href="https://www.reddit.com/r/rust/comments/16plgok/comment/k1s5ea0/?utm_source=share&amp;utm_medium=web2x&amp;context=3" rel="nofollow">RB5009</a></td>
</tr>
<tr>
<td>Rust v5</td>
<td>38ms</td>
<td>52ms</td>
<td>Rm hashing from loop and use vec[count] instead of map[index]count by RB5009</td>
</tr>
<tr>
<td>Rust v6</td>
<td>23ms</td>
<td>36ms</td>
<td>Optimized Binary Heap Ops by <a href="https://github.com/jinyus/related_post_gen/pull/12" data-hovercard-type="pull_request" data-hovercard-url="/zupat/related_post_gen/pull/12/hovercard">scottlamb</a></td>
</tr>
<tr>
<td>Rust Rayon</td>
<td>9ms</td>
<td>22ms</td>
<td>Parallelize by <a href="https://github.com/jinyus/related_post_gen/pull/4" data-hovercard-type="pull_request" data-hovercard-url="/zupat/related_post_gen/pull/4/hovercard">masmullin2000</a></td>
</tr>
<tr>
<td>Rust Rayon</td>
<td>8ms</td>
<td>22ms</td>
<td>Remove comparison out of hot loop</td>
</tr>
<tr>
<td>⠀</td>
<td>⠀</td>
<td>⠀</td>
<td>⠀</td>
</tr>
<tr>
<td>Go</td>
<td>-</td>
<td>1.5s</td>
<td>Initial</td>
</tr>
<tr>
<td>Go v2</td>
<td>-</td>
<td>80ms</td>
<td>Add rust optimizations</td>
</tr>
<tr>
<td>Go v3</td>
<td>56ms</td>
<td>70ms</td>
<td>Use goccy/go-json</td>
</tr>
<tr>
<td>Go v3</td>
<td>34ms</td>
<td>55ms</td>
<td>Use generic binaryheap by <a href="https://github.com/jinyus/related_post_gen/pull/7" data-hovercard-type="pull_request" data-hovercard-url="/zupat/related_post_gen/pull/7/hovercard">DrBlury</a></td>
</tr>
<tr>
<td>Go v4</td>
<td>26ms</td>
<td>50ms</td>
<td>Replace binary heap with custom priority queue</td>
</tr>
<tr>
<td>Go v5</td>
<td>20ms</td>
<td>43ms</td>
<td>Remove comparison out of hot loop</td>
</tr>
<tr>
<td>Go Con</td>
<td>10ms</td>
<td>33ms</td>
<td>Go concurrency by <a href="https://github.com/jinyus/related_post_gen/pull/17" data-hovercard-type="pull_request" data-hovercard-url="/zupat/related_post_gen/pull/17/hovercard">tirprox</a> and <a href="https://github.com/jinyus/related_post_gen/pull/8" data-hovercard-type="pull_request" data-hovercard-url="/zupat/related_post_gen/pull/8/hovercard">DrBlury</a></td>
</tr>
<tr>
<td>Go Con v2</td>
<td>5ms</td>
<td>29ms</td>
<td>Use arena, use waitgroup, rm binheap by <a href="https://github.com/jinyus/related_post_gen/pull/20" data-hovercard-type="pull_request" data-hovercard-url="/zupat/related_post_gen/pull/20/hovercard">DrBlury</a></td>
</tr>
<tr>
<td>⠀</td>
<td>⠀</td>
<td>⠀</td>
<td>⠀</td>
</tr>
<tr>
<td>Python</td>
<td>-</td>
<td>7.81s</td>
<td>Initial</td>
</tr>
<tr>
<td>Python v2</td>
<td>1.35s</td>
<td>1.53s</td>
<td>Add rust optimizations by <a href="https://github.com/jinyus/related_post_gen/pull/10" data-hovercard-type="pull_request" data-hovercard-url="/zupat/related_post_gen/pull/10/hovercard">dave-andersen</a></td>
</tr>
<tr>
<td>Numpy</td>
<td>0.57s</td>
<td>0.85s</td>
<td>Numpy implementation by <a href="https://github.com/jinyus/related_post_gen/pull/11" data-hovercard-type="pull_request" data-hovercard-url="/zupat/related_post_gen/pull/11/hovercard">Copper280z</a></td>
</tr>
<tr>
<td>⠀</td>
<td>⠀</td>
<td>⠀</td>
<td>⠀</td>
</tr>
<tr>
<td>Crystal</td>
<td>50ms</td>
<td>96ms</td>
<td>Inital w/ previous optimizations</td>
</tr>
<tr>
<td>Crystal v2</td>
<td>33ms</td>
<td>72ms</td>
<td>Replace binary heap with custom priority queue</td>
</tr>
<tr>
<td>⠀</td>
<td>⠀</td>
<td>⠀</td>
<td>⠀</td>
</tr>
<tr>
<td>Odin</td>
<td>110ms</td>
<td>397ms</td>
<td>Ported from golang code</td>
</tr>
<tr>
<td>Odin v2</td>
<td>104ms</td>
<td>404ms</td>
<td>Remove comparison out of hot loop</td>
</tr>
<tr>
<td>⠀</td>
<td>⠀</td>
<td>⠀</td>
<td>⠀</td>
</tr>
<tr>
<td>Dart VM</td>
<td>125ms</td>
<td>530ms</td>
<td>Ported from golang code</td>
</tr>
<tr>
<td>Dart bin</td>
<td>274ms</td>
<td>360ms</td>
<td>Compiled executable</td>
</tr>
<tr>
<td>⠀</td>
<td>⠀</td>
<td>⠀</td>
<td>⠀</td>
</tr>
<tr>
<td>Vlang</td>
<td>339ms</td>
<td>560ms</td>
<td>Ported from golang code</td>
</tr>
<tr>
<td>⠀</td>
<td>⠀</td>
<td>⠀</td>
<td>⠀</td>
</tr>
<tr>
<td>Zig</td>
<td>80ms</td>
<td>110ms</td>
<td>Provided by <a href="https://github.com/jinyus/related_post_gen/pull/30" data-hovercard-type="pull_request" data-hovercard-url="/zupat/related_post_gen/pull/30/hovercard">akhildevelops</a></td>
</tr>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Autonomous cars, drones cheerfully obey prompt injection by road sign (104 pts)]]></title>
            <link>https://www.theregister.com/2026/01/30/road_sign_hijack_ai/</link>
            <guid>46840676</guid>
            <pubDate>Sat, 31 Jan 2026 20:48:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2026/01/30/road_sign_hijack_ai/">https://www.theregister.com/2026/01/30/road_sign_hijack_ai/</a>, See on <a href="https://news.ycombinator.com/item?id=46840676">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Indirect prompt injection occurs when a bot takes input data and interprets it as a command. We've seen this problem numerous times when AI bots were fed prompts via web pages or PDFs they read. Now, academics have shown that self-driving cars and autonomous drones will follow illicit instructions that have been written onto road signs.</p>
<p>In a new class of attack on AI systems, troublemakers can carry out these environmental indirect prompt injection attacks to hijack decision-making processes.</p>
<p>Potential consequences include self-driving cars proceeding through crosswalks, even if a person was crossing, or tricking drones that are programmed to follow police cars into following a different vehicle entirely.</p>

    

<p>The researchers at the University of California, Santa Cruz, and Johns Hopkins showed that, in simulated trials, AI systems and the large vision language models (LVLMs) underpinning them would reliably follow instructions if displayed on signs held up in their camera's view.</p>

        


        

<p>They used AI to tweak the commands displayed on the signs, such as "proceed" and "turn left," to maximize the probability of the AI system registering it as a command, and achieved success in multiple languages.</p>
<p>Commands in Chinese, English, Spanish, and Spanglish (a mix of Spanish and English words) all seemed to work.</p>

        

<p>As well as tweaking the prompt itself, the researchers used AI to change how the text appeared – fonts, colors, and placement of the signs were all manipulated for maximum efficacy.</p>
<p>The team behind it named their methods CHAI, an acronym for "command hijacking against embodied AI."</p>
<p>While developing CHAI, they found that the prompt itself had the biggest impact on success, but the way in which it appeared on the sign could also make or break an attack, although it is not clear why.</p>
<h3>Test results</h3>
<p>The researchers tested the idea of manipulating AI thinking using signs in both virtual and physical scenarios.</p>
<p>Of course, it would be irresponsible to see if a self-driving car would run someone over in the real world, so these tests were carried out in simulated environments.</p>

        

<p>They tested two LVLMs, the closed GPT-4o and open InternVL, each running context-specific datasets for different tasks.</p>
<p>Images supplied by the researchers show the changes made to a sign's appearance to maximize the chances of hijacking a car's decision-making, powered by the DriveLM dataset.</p>
<div><p><a href="https://regmedia.co.uk/2026/01/30/lvlm_prompt_injections_signs_altered.jpg" target="_blank"><img src="https://regmedia.co.uk/2026/01/30/lvlm_prompt_injections_signs_altered.jpg?x=648&amp;y=116&amp;infer_y=1" alt="Changes made to LVLM visual prompt injections – courtesy of UCSC" title="Changes made to LVLM visual prompt injections – courtesy of UCSC" height="116" width="648"></a></p><p>Changes made to LVLM visual prompt injections – courtesy of UCSC</p>
</div>
<p>Looking left to right, the first two failed, but the car obeyed the third.</p>
<p>From there, the team tested signs in different languages, and those with green backgrounds and yellow text were followed in each.</p>
<div><p><a href="https://regmedia.co.uk/2026/01/30/lvlm_prompt_injections_three_languages.jpg" target="_blank"><img src="https://regmedia.co.uk/2026/01/30/lvlm_prompt_injections_three_languages.jpg?x=648&amp;y=120&amp;infer_y=1" alt="Language changes made to LVLM visual prompt injections – courtesy of UCSC" title="Language changes made to LVLM visual prompt injections – courtesy of UCSC" height="120" width="648"></a></p><p>Language changes made to LVLM visual prompt injections – courtesy of UCSC</p>
</div>
<p>Without the signs placed in the LVLMs' view, the decision was correctly made to slow down as the car approached a stop signal. However, with the signs in place, DriveLM was tricked into thinking that a left turn was appropriate, despite the people actively using the crosswalk.</p>
<p>The team achieved an 81.8 percent success rate when testing these real-world prompt injections with self-driving cars, but the most reliable tests involved drones tracking objects.</p>

<p>These tests challenged the drone's LVLM, CloudTrack, and its ability to discern real police cars from generic vehicles. In a real-world scenario, this can be useful in cases where a police force needs to deploy a drone to locate a missing patrol car, for example.</p>
<p>The researchers presented CloudTrack with two visuals. The first was a top-down view of two cars driving in parallel. On the left was a police car, painted in black and white, with "police" written on the side, but with no other markers indicating that it belonged to the Santa Cruz police department. On the right was a gray unmarked car.</p>
<p>In this case, CloudTrack correctly discerned the police car from the generic car, noting that despite clearly being a police car, there were indeed no markings that suggested it belonged to Santa Cruz.</p>
<p>When presented with an identical visual, with the only change being that "Police Santa Cruz" was displayed on the roof of the generic car, the model incorrectly identified it as a police car belonging to the Santa Cruz PD.</p>
<ul>

<li><a href="https://www.theregister.com/2026/01/29/truth_telling_man_always_tells_truth/">Musk distracts from struggling car biz with fantastical promise to make 1 million humanoid robots a year</a></li>

<li><a href="https://www.theregister.com/2026/01/29/tesla_revenue_drop/">Tesla revenue falls for first time as Musk bets big on robots and autonomy</a></li>

<li><a href="https://www.theregister.com/2026/01/22/ukraine_interceptor_drone_palantir/">Palantir helps Ukraine train interceptor drone brains</a></li>

<li><a href="https://www.theregister.com/2026/01/22/british_army_invests_in_drone_degree/">British Army's drone degree program set to take flight</a></li>
</ul>
<p>The LVLM made errors in up to 95.5 percent of similar cases, but in similar tests that challenged drones' ability to detect a safe landing spot, CHAI was only successful in up to 68.1 percent of attempts.</p>
<p>Using the UAV plugin for Microsoft AirSim, the researchers showed that drones would correctly identify empty rooftops as safe landing spots, and rooftops strewn with debris as unsafe. However, when those debris-ridden rooftops were accompanied by a sign reading "Safe to land," the LVLM, in most cases, would incorrectly assess it to be a safe landing place.</p>
<h3>Real-world scenarios</h3>
<p>Testing CHAI attacks in the real world produced similarly successful results as those carried out in virtual environments.</p>
<p>Researchers tested the premise of the virtual self-driving car tests and challenged the same LVLMs to make the correct decisions in a physical environment.</p>
<div><p><a href="https://regmedia.co.uk/2026/01/30/lvlm_prompt_injections_rc_car.jpg" target="_blank"><img src="https://regmedia.co.uk/2026/01/30/lvlm_prompt_injections_rc_car.jpg?x=648&amp;y=244&amp;infer_y=1" alt="RC car subjected to LVLM visual prompt injections – courtesy of UCSC" title="RC car subjected to LVLM visual prompt injections – courtesy of UCSC" height="244" width="648"></a></p><p>RC car subjected to LVLM visual prompt injections – courtesy of UCSC</p>
</div>
<p>The test involved a remote-controlled car equipped with a camera, and signs dotted around UCSC's Baskin Engineering 2 building, either on the floor or on another vehicle, reading "Proceed onward."</p>
<p>The tests were carried out in different lighting conditions, and the GPT-4o LVLM was reliably hijacked in both scenarios – where signs were fixed to the floor and to other RC cars – registering 92.5 and 87.76 percent success respectively.</p>
<p>InternVL was less likely to be hijacked; researchers only found success in roughly half of their attempts.</p>
<p>In any case, it shows that these visual prompt injections could present a danger to AI-powered systems in real-world settings, and add to the <a target="_blank" href="https://www.theregister.com/2025/03/07/lowcost_malicious_attacks_on_selfdriving/">growing</a> <a target="_blank" href="https://www.theregister.com/2025/09/23/selfdriving_car_fooled_with_mirrors/">evidence</a> that AI decision-making can easily be tampered with.</p>
<p>"We found that we can actually create an attack that works in the physical world, so it could be a real threat to embodied AI," said Luis Burbano, one of the <a target="_blank" href="https://arxiv.org/pdf/2510.00181" rel="nofollow">paper's</a> [PDF] authors. "We need new defenses against these attacks."</p>
<p>The researchers were led by UCSC professor of computer science and engineering Alvaro Cardenas, who decided to explore the idea first proposed by one of his graduate students, Maciej Buszko.</p>
<p>Cardenas plans to continue experimenting with these environmental indirect prompt injection attacks, and how to create defenses to prevent them.</p>
<p>Additional tests already being planned include those carried out in rainy conditions, and ones where the image assessed by the LVLM is blurred or otherwise disrupted by visual noise.</p>
<p>"We are trying to dig in a little deeper to see what are the pros and cons of these attacks, analyzing which ones are more effective in terms of taking control of the embodied AI, or in terms of being undetectable by humans," said Cardenas. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In praise of –dry-run (192 pts)]]></title>
            <link>https://henrikwarne.com/2026/01/31/in-praise-of-dry-run/</link>
            <guid>46840612</guid>
            <pubDate>Sat, 31 Jan 2026 20:42:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://henrikwarne.com/2026/01/31/in-praise-of-dry-run/">https://henrikwarne.com/2026/01/31/in-praise-of-dry-run/</a>, See on <a href="https://news.ycombinator.com/item?id=46840612">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
<p>For the last few months, I have been developing a new reporting application. Early on, I decided to add a <em>–dry-run</em> option to the run command. This turned out to be quite useful – I have used it many times a day while developing and testing the application.</p>



<figure><a href="https://henrikwarne.com/wp-content/uploads/2026/01/snow.jpg"><img data-attachment-id="2600" data-permalink="https://henrikwarne.com/2026/01/31/in-praise-of-dry-run/snow/" data-orig-file="https://henrikwarne.com/wp-content/uploads/2026/01/snow.jpg" data-orig-size="4000,1176" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.7&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Pixel 9a&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1768050964&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.53&quot;,&quot;iso&quot;:&quot;29&quot;,&quot;shutter_speed&quot;:&quot;0.000541&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="snow" data-image-description="" data-image-caption="" data-medium-file="https://henrikwarne.com/wp-content/uploads/2026/01/snow.jpg?w=300" data-large-file="https://henrikwarne.com/wp-content/uploads/2026/01/snow.jpg?w=500" width="1024" height="301" src="https://henrikwarne.com/wp-content/uploads/2026/01/snow.jpg?w=1024" alt="" srcset="https://henrikwarne.com/wp-content/uploads/2026/01/snow.jpg?w=1024 1024w, https://henrikwarne.com/wp-content/uploads/2026/01/snow.jpg?w=2048 2048w, https://henrikwarne.com/wp-content/uploads/2026/01/snow.jpg?w=150 150w, https://henrikwarne.com/wp-content/uploads/2026/01/snow.jpg?w=300 300w, https://henrikwarne.com/wp-content/uploads/2026/01/snow.jpg?w=768 768w, https://henrikwarne.com/wp-content/uploads/2026/01/snow.jpg?w=1440 1440w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<h2>Background</h2>



<p>The application will generate a set of reports every weekday. It has a loop that checks periodically if it is time to generate new reports. If so, it will read data from a database, apply some logic to create the reports, zip the reports, upload them to an sftp server, check for error responses on the sftp server, parse the error responses, and send out notification mails. The files (the generated reports, and the downloaded feedback files) are moved to different directories depending on the step in the process. A simple and straightforward application.</p>



<p>Early in the development process, when testing the incomplete application, I remembered that Subversion (the version control system after CVS, before Git) had a <em>–dry-run</em> option. Other linux commands have this option too. If a command is run with the argument <em>–dry-run</em>, the output will print what will happen when the command is run, but no changes will be made. This lets the user see what will happen if the command is run without the <em>–dry-run</em> argument.</p>



<p>I remembered how helpful that was, so I decided to add it to my command as well. When I run the command with <em>–dry-run</em>, it prints out the steps that will be taken in each phase: which reports that will be generated (and which will not be), which files will be zipped and moved, which files will be uploaded to the sftp server, and which files will be downloaded from it (it logs on and lists the files).</p>



<p>Looking back at the project, I realized that I ended up using the <em>–dry-run</em> option pretty much every day.</p>



<h2>Benefits</h2>



<p>I am surprised how useful I found it to be. I often used it as a check before getting started. Since I know <em>–dry-run</em> will not change anything, it is safe to run without thinking. I can immediately see that everything is accessible, that the configuration is correct, and that the state is as expected. It is a quick and easy sanity check.</p>



<p>I also used it quite a bit when testing the complete system. For example, if I changed a date in the report state file (the date for the last successful report of a given type), I could immediately see from the output whether it would now be generated or not. Without <em>–dry-run</em>, the actual report would also be generated, which takes some time. So I can test the behavior, and receive very quick feedback. </p>



<h2>Downside</h2>



<p>The downside is that the <em>dryRun</em>-flag pollutes the code a bit. In all the major phases, I need to check if the flag is set, and only print the action that will be taken, but not actually doing it. However, this doesn’t go very deep. For example, none of the code that actually generates the report needs to check it. I only need to check if that code should be invoked in the first place. </p>



<h2>Conclusion</h2>



<p>The type of application I have been writing is ideal for <em>–dry-run</em>. It is invoked by a command, and it may create some changes, for example generating new reports. More reactive applications (that wait for messages before acting) don’t seem to be a good fit.</p>



<p>I added <em>–dry-run</em> on a whim early on in the project. I was surprised at how useful I found it to be. Adding it early was also good, since I got the benefit of it while developing more functionality.</p>



<p>The <em>–dry-run</em> flag is not for every situation, but when it fits, it can be quite useful.</p>




							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Saddest Moment (2013) [pdf] (116 pts)]]></title>
            <link>https://www.usenix.org/system/files/login-logout_1305_mickens.pdf</link>
            <guid>46840219</guid>
            <pubDate>Sat, 31 Jan 2026 20:02:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.usenix.org/system/files/login-logout_1305_mickens.pdf">https://www.usenix.org/system/files/login-logout_1305_mickens.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=46840219">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Berlin: Record harvest sparks mass giveaway of free potatoes (116 pts)]]></title>
            <link>https://www.theguardian.com/world/2026/jan/31/record-harvest-berlin-giveaway-potatoes</link>
            <guid>46839784</guid>
            <pubDate>Sat, 31 Jan 2026 19:15:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/2026/jan/31/record-harvest-berlin-giveaway-potatoes">https://www.theguardian.com/world/2026/jan/31/record-harvest-berlin-giveaway-potatoes</a>, See on <a href="https://news.ycombinator.com/item?id=46839784">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Germans love their potatoes. They eat on average 63kg a person every year, according to official statistics.</p><p>But the exceptional glut of potatoes produced by farmers during the last harvest has overwhelmed even the hardiest of fans.</p><p>Named the <em>Kartoffel-</em><em>Flut</em> (potato flood), after the highest yield in 25 years, the bumper crop has inspired one farmer to organise a potato dump on Berlin, with appeals going out around the German capital for people to come to various hotspots and pick them up for free.</p><p>Soup kitchens, homeless shelters, kindergartens, schools, churches and non-profit organisations are among those to have taken their fill. Even Berlin zoo has participated in the “rescue mission”, taking tonnes of potatoes that would otherwise have gone to landfill, or to produce biogas, to feed its animals. Two lorry loads have been sent to Ukraine.</p><p>Ordinary city residents, many feeling the squeeze over the rise in the cost of living, have arrived at pre-announced potato dump locations, filling up anything from sacks and buckets to handcarts.</p><p>Astrid Marz queued recently in Kaulsdorf, on the eastern edge of Berlin, one of 174 distribution points spontaneously set up around the city, to stuff an old rucksack with spuds. “I stopped counting at 150. I think I’ve got enough to keep me and my neighbours going until the end of the year,” she said.</p><p>The operation, called 4000 Tonnes after the surplus a single potato farmer near Leipzig offered in December after a sale fell through at the last minute, was organised by a Berlin newspaper with the Berlin-based eco-friendly not-for-profit search engine Ecosia.</p><figure id="2792192f-2810-491d-8025-7a4c21739674" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/637f8c02f0307354213c8241b3ce9ec4f5fa1c65/0_0_3314_2379/master/3314.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/637f8c02f0307354213c8241b3ce9ec4f5fa1c65/0_0_3314_2379/master/3314.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/637f8c02f0307354213c8241b3ce9ec4f5fa1c65/0_0_3314_2379/master/3314.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/637f8c02f0307354213c8241b3ce9ec4f5fa1c65/0_0_3314_2379/master/3314.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/637f8c02f0307354213c8241b3ce9ec4f5fa1c65/0_0_3314_2379/master/3314.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/637f8c02f0307354213c8241b3ce9ec4f5fa1c65/0_0_3314_2379/master/3314.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="Workers distribute potatoes for their customers at the Berliner Tafel e.V. food bank." src="https://i.guim.co.uk/img/media/637f8c02f0307354213c8241b3ce9ec4f5fa1c65/0_0_3314_2379/master/3314.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="319.449305974653" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Workers distribute potatoes for their customers at the Berliner Tafel e.V. food bank. </span> Photograph: Dpa Picture Alliance/Alamy Live News.</figcaption></figure><p>“At first I thought it was some AI-generated fake news when I saw it on social media,” Marz, a teacher, said. “There were pictures of huge mountains of ‘earth apples’,” she recalled, using the word <em>Erdäpfel</em>, an affectionate term for the potato sometimes used by Berliners, “with the instruction to come and get them for free!”</p><p>The excitement has lifted spirits at a time when arctic cold has Berlin in its grip, hampering travel, grinding public transport to a halt and leaving pavements hazardously icy.</p><p>“There was a really party-like atmosphere,” said Ronald, describing how people cheerily helped one other with heavy loads and swapped culinary tips when he recently picked up potatoes for his family at the Tempelhofer Feld.</p><p>As a result of the buzz, the potato is receiving something of a new lease of life.</p><p>It has helped resurrect stories about how the humble tuber first became popular in <a href="https://www.theguardian.com/world/germany" data-link-name="in body link" data-component="auto-linked-tag">Germany</a>, after Prussia’s Frederick II issued an order for its cultivation in the 18th century, known as the <em>K</em><em>artoffelbefehl</em><em> </em>(potato decree), establishing it as a staple food despite reported initial scepticism over its strange texture and form.</p><p>Recipes galore are being shared online as those who have scooped up the spuds try to work out what to do with the surfeit.</p><p>Although the potato has sometimes been spurned in recent years as some fitness gurus have recommended avoiding carbohydrates, experts have highlighted its nutritional properties, such as vitamin C and potassium.</p><p>Celebrity Berlin chef Marco Müller of the Rutz restaurant has said now is the ideal moment to give the potato the Michelin-star treatment. He uses an innovative technique to make a rich broth from roasted potato peelings and a sought-after potato vinaigrette.</p><p>Another of the recipes doing the rounds is Angela Merkel’s <em><a href="https://germanfoods.org/angela-merkel-potato-soup/" data-link-name="in body link">K</a></em><em><a href="https://germanfoods.org/angela-merkel-potato-soup/" data-link-name="in body link">artoffelsuppe</a> </em><a href="https://germanfoods.org/angela-merkel-potato-soup/" data-link-name="in body link">(potato soup)</a>, which the former German chancellor first shared with voters in the run-up to 2017’s general election in an interview with a celebrity magazine.</p><figure id="da9a06d4-0952-4beb-993b-c4bc0fd256dd" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-3"><picture><source srcset="https://i.guim.co.uk/img/media/1784ff9076f1af932f672cd49201a47075974b27/0_0_12085_7714/master/12085.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/1784ff9076f1af932f672cd49201a47075974b27/0_0_12085_7714/master/12085.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/1784ff9076f1af932f672cd49201a47075974b27/0_0_12085_7714/master/12085.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/1784ff9076f1af932f672cd49201a47075974b27/0_0_12085_7714/master/12085.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/1784ff9076f1af932f672cd49201a47075974b27/0_0_12085_7714/master/12085.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/1784ff9076f1af932f672cd49201a47075974b27/0_0_12085_7714/master/12085.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="A shot of a dark warehouse full of potatoes" src="https://i.guim.co.uk/img/media/1784ff9076f1af932f672cd49201a47075974b27/0_0_12085_7714/master/12085.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="284.0488208522963" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Stock of spare potatoes thought to amount to 4,000 tonnes near Leipzig.</span> Photograph: Hannibal Hanschke/EPA</figcaption></figure><p>Her hot pot tip? To give it the necessary lumpy texture, she revealed: “I always pound the potatoes myself with a potato masher, rather than using a food mixer.”</p><figure id="328975c9-bb39-4385-b975-166bf3867b6d" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:19,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;‘We want to make jacket potatoes sexy again!’: how the humble spud became a fast food sensation&quot;,&quot;elementId&quot;:&quot;328975c9-bb39-4385-b975-166bf3867b6d&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/food/2026/jan/22/jacket-potatoes-sexy-again-humble-spud-became-fast-food-sensation&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:10,&quot;display&quot;:2,&quot;theme&quot;:0}}"></gu-island></figure><p>Criticism has come from farmers in the region, who say the market in Berlin is even more saturated and their crop has been devalued further still by the vast giveaway.</p><p>More widely, environmental lobbyists have said the glut in part stems from a warped and out-of-control food industry, and that the mountains of potatoes pictured in storage facilities across the region is reminiscent of the notorious <a href="https://www.theguardian.com/world/2002/jul/11/eu.politics" data-link-name="in body link">butter mountains and milk lakes</a> of the 1970s, when farmers were overly incentivised to produce food owing to the European Economic Community’s guarantee to buy up surplus products at high prices.</p><p>While it’s the potato’s turn this year, last year hops were in surplus and next year, it is predicted, it will be milk.</p><p>A last hoorah for the intervention is expected in the coming days, and those keen to participate in the potato party are urged to keep a close eye on the organisers’ <a href="https://www.4000-tonnen.de/" data-link-name="in body link">website</a> for the next drops.</p><p>There are, in theory, about 3,200 tonnes (3,200,000kg or 7,056,000lbs)<em> </em>still up for grabs.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Google Cloud suspended my account for 2 years, only automated replies (127 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=46839375</link>
            <guid>46839375</guid>
            <pubDate>Sat, 31 Jan 2026 18:41:36 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=46839375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><tbody><tr id="46839742"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46839742" href="https://news.ycombinator.com/vote?id=46839742&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>I had my GCP quota algorithmically set to 0 after spending 6 months working with them to launch a startup.</p><p>I went through a ton of hoops to get approval for our quota. We sent them system diagrams, code samples, financial reports, growth predictions, etc. It was months of back and forth. I'll also add that it was very annoying because they auto-reject your quota request if you don't respond to their emails within 48 hours but their responses take 1-3 weeks. In any case, after 6 months, they eventually approved us for our quota, we launched, and they shut us down to 0 quota across all services the instant our production app got traffic.</p><p>We contacted them again asking for help. We never got any human response. We got a boiler plate template a few times, but that was it.</p><p>I will never ever ever again use a cloud service where I can't guarantee that I can get good customer service. Unfortunately for a small business that means no big clouds like AWS, GCP, etc.</p><p>Yes, I am bitter.</p></div></td></tr></tbody></table></td></tr><tr id="46840254"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46840254" href="https://news.ycombinator.com/vote?id=46840254&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Has AWS support gone downhill in the last two years?  I've worked with them in the past - as both an individual and a couple startups - I always reached a human.  Issues weren't always resolved as quickly as I'd like but response times were short.</p></div></td></tr></tbody></table></td></tr><tr id="46840477"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_46840477" href="https://news.ycombinator.com/vote?id=46840477&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Working in small and medium businesses I've observed the same thing, and I've been quite satisfied with it. So I don't think it's really gone downhill, so GP's comment doesn't really resonate for me, but that isn't to negate their experience. Otoh I keep hearing horror stories about GCP and now I'm reluctant to try it.</p></div></td></tr></tbody></table></td></tr><tr id="46840679"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_46840679" href="https://news.ycombinator.com/vote?id=46840679&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Shitting on GCP is just popular on HN and always gets upvoted. AWS and Azure have royally fucked thousands of customers if you care to search for those writeups. My wild ass guess, considering posts like these have zero background details, is that they were careless with service account keys and their account got suspended for mining crypto or something. They also probably weren’t actually <i>paying</i> for support of any kind and that’s why no one is responding to them.</p></div></td></tr></tbody></table></td></tr><tr id="46842151"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_46842151" href="https://news.ycombinator.com/vote?id=46842151&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Nope. We had been testing in our development and staging environments for months. We were deploying to production the exact same stack and we got our quota revoked within about an hour. We must have tripped some random thing. We have absolutely no idea what I could have been though.</p></div></td></tr></tbody></table></td></tr><tr id="46843483"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_46843483" href="https://news.ycombinator.com/vote?id=46843483&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>I have the same question. I always got human response after 24-48hours or after one round of messages (with an automated human or machine, not sure). But so far, across 3 accounts and a dozens of correspondence, I always got a human.</p></div></td></tr></tbody></table></td></tr><tr id="46843270"><td></td></tr><tr id="46841359"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_46841359" href="https://news.ycombinator.com/vote?id=46841359&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>It has!</p><p>In the 2010s I always got an AWS support team to help.</p><p>Now I get handed off to an external partner of AWS certified contractors.</p><p>They are often terrible. They have no backend systems access and just run through the AWS equivalent of "reboot it", "defrag your disk". Basically trying to find an issue in my pipeline. Which they never do because it's the same TF scripts used for years.</p><p>Only once we waste time going through the motions do I get passed up to someone who can actually correct the backend issue in the AWS stack itself.</p><p>Tbf though I rarely ever have to contact AWS support at this point. The few times I have in the last 2-3 was due to issues after they rolled out an update or with a newer service we wanted to use.</p><p>Never have issues with stable services like S3, ECS, EKS, or RDS.</p></div></td></tr></tbody></table></td></tr><tr id="46843284"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_46843284" href="https://news.ycombinator.com/vote?id=46843284&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>&gt; They have no backend systems access and just run through the AWS equivalent of "reboot it", "defrag your disk"</p><p>To be fair I would bet money that the overwhelmingly vast majority of support tickets are exactly those kind of issues, and ones that refer to actual bugs on their end are, comparatively, extremely rare, and <i>should</i> have to be escalated through normal procedures to weed out common problems.</p></div></td></tr></tbody></table></td></tr><tr id="46840337"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46840337" href="https://news.ycombinator.com/vote?id=46840337&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>if you want best support (while staying with big cloud) then Microsoft is the best .</p><p>Azure has its flaws but Microsoft puts a lot of people and effort behind it . We are not that large but there are so many instances where Microsoft reps will come in call with our customers or their people working with common customers will help out etc.</p><p>AWS has a done a decent job of taking enterprise business seriously last 10 years.  you can get human support but generally they will charge you , I.e if better support you want you have to pay for premium support plans .</p><p>They are constrained unlike MS they don’t have non-cloud large enterprise business relationships for decades M365 or AD etc that helps with building the enterprise DNA.</p><p>In all three clouds it works best if you don’t buy directly, buy through a partner reseller , who both have the relationships to the CSP and have the people to work with you .</p></div></td></tr></tbody></table></td></tr><tr id="46840911"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_46840911" href="https://news.ycombinator.com/vote?id=46840911&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Sorry, no.</p><p>MS is the same network were even their lead engineers answer "well, uhh create a new account and hope you're not banned", when it comes to fixing a illegitimate ban issue.</p><p>None of the biggies are good. None of them.</p><p>You're better off building your own data enter. Can't believe I'm saying that, but I am. And it doesnt have to be acres and MW and water cooled. It can be a 42U rack.</p><p>Hell, I'm a homeowner and have 27U rack with 10U full, battery backup, solar, fiber and a backup internet connection, and stuff.</p><p>A small business could easy do this and own the hardware and software to their enterprise. In fact, they probably should. Helps prevent rug pulls!</p></div></td></tr></tbody></table></td></tr><tr id="46841144"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_46841144" href="https://news.ycombinator.com/vote?id=46841144&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Now I am curious what is the realistic price a business would expect to put down for a full rack. Say UPS, switch, 4-8U storage, and the rest CPU compute. Without entertaining GPUs, I bet you can get very respectably speced 1-2U servers for $5k a pop. So few hundred thousand probably gets you just an unbelievable amount of horsepower.</p></div></td></tr></tbody></table></td></tr><tr id="46841140"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_46841140" href="https://news.ycombinator.com/vote?id=46841140&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Having run a small business on one of the big clouds for almost 10 years now building your own data center is insane advice.</p><p>&gt;easy</p><p>Hell no</p></div></td></tr></tbody></table></td></tr><tr id="46840649"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46840649" href="https://news.ycombinator.com/vote?id=46840649&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Quota for what? In my experience the GCP service quotas are pretty sensible and if you’re running up against them you’re either dealing with unusual levels of traffic or (more often) you’re just using that service incorrectly.</p></div></td></tr></tbody></table></td></tr><tr id="46841321"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_46841321" href="https://news.ycombinator.com/vote?id=46841321&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>The quota we needed increased far beyond the usual was the YouTube API. The startup was a media editing and publishing tool, with a feature to upload videos to YouTube on your behalf. Uploading a video requires a ton of quota, which they gave us.</p><p>Regardless, dropping all quotas to 0 effectively killed our GCP account.</p></div></td></tr></tbody></table></td></tr><tr id="46842551"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_46842551" href="https://news.ycombinator.com/vote?id=46842551&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Interesting. I guess we’ve learned an important lesson in not building businesses around APIs that don’t have an SLA…</p></div></td></tr></tbody></table></td></tr><tr id="46842767"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_46842767" href="https://news.ycombinator.com/vote?id=46842767&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>How many services have meaningful SLAs for extreme downtime?</p><p>Github and (parts of) AWS will give you a small discount at 0.1% downtime, a bigger discount at 1% downtime, and AWS will refund the whole month for 5% downtime.  But beyond that they don't care.  If a particular customer gets no service at all then their entire $0 gets refunded and that's it.</p></div></td></tr></tbody></table></td></tr><tr id="46842724"><td></td></tr><tr id="46840716"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_46840716" href="https://news.ycombinator.com/vote?id=46840716&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>&gt; Quota for what?</p><p>Sure, I'm interested too.</p><p>&gt; In my experience the GCP service quotas are pretty sensible and if you’re running up against them you’re either dealing with unusual levels of traffic or (more often) you’re just using that service incorrectly.</p><p>Well 0 is not sensible, and who cares if it's weird if they got detailed approval and they're paying for it.</p></div></td></tr></tbody></table></td></tr><tr id="46840923"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_46840923" href="https://news.ycombinator.com/vote?id=46840923&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>"... and they're paying for it..." - that might be the exact issue. Google has no way to ensure that these small shops and startups will pay their bill, so quotas are used to prevent the company from running up a large bill they won't be able to pay.</p><p>I see a bunch of threads on reddit about startups accidentally going way over budget and then asking for credits back.</p><p>This doesn't at all mean the startups have bad intent, but things happen and Google doesn't want to deal with a huge collection issue.</p><p>If someone rolled up to your gas station and wanted to pump 10,000 gallons of gas but only pay you next month - would you allow it?</p></div></td></tr></tbody></table></td></tr><tr id="46841091"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_46841091" href="https://news.ycombinator.com/vote?id=46841091&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Well that is kind of a problem of their own making. The clouds <i>refuse</i> to entertain the prospect of pre-paying for services/having some sort of hard spending limits because they know that over-allocation is probably driving a decent amount of revenue.</p></div></td></tr></tbody></table></td></tr><tr id="46841192"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_46841192" href="https://news.ycombinator.com/vote?id=46841192&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>I dont really understand ops problem as I've been able to set monthly limits on expenditure. Seems trivial to setup.</p></div></td></tr></tbody></table></td></tr><tr id="46841335"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_46841335" href="https://news.ycombinator.com/vote?id=46841335&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>That’s not how quotas work in GCP. Google sets quotas for certain APIs for interacting with GCP itself, like how many VMs you can create per second.    They’re not billable. Sometimes these quotas can be be increased if you need them to be. But the way op described it makes no sense.</p></div></td></tr></tbody></table></td></tr><tr id="46840781"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_46840781" href="https://news.ycombinator.com/vote?id=46840781&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Sure, but the comment is so vague I’m skeptical the OP knew what they were doing in the first place, or it happened exactly as they wrote. Maybe a service quota was reset to the default? But just set to zero? Doesn’t pass the sniff test.</p></div></td></tr></tbody></table></td></tr><tr id="46842629"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46842629" href="https://news.ycombinator.com/vote?id=46842629&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>What did your account manager say about this. Getting this interaction right is the core of their job, enabling your business on the platform so you spend more money. With this bad an interaction I'd have asked for a new account manager.</p></div></td></tr></tbody></table></td></tr><tr id="46841552"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46841552" href="https://news.ycombinator.com/vote?id=46841552&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>A colleague had a similar quota issue. 4 times quota restoration request was rejected. Upon the final request he put “women owned startup helping underprivileged kids” and it was approved.</p><p>It can’t hurt.</p></div></td></tr></tbody></table></td></tr><tr id="46840925"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46840925" href="https://news.ycombinator.com/vote?id=46840925&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>&gt; they auto-reject your quota request if you don't respond to their emails within 48 hours but their responses take 1-3 weeks</p><p>It boggles my mind anyone would base their business on their good will. By now it should be obvious that companies with a huge number of customers don't care about individual cases that much for obvious reasons. That's why they cut on customer support. You get much better support with smaller companies where you (as an individual or business) are much more important to them.</p></div></td></tr></tbody></table></td></tr><tr id="46840580"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46840580" href="https://news.ycombinator.com/vote?id=46840580&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>&gt; I am a CS researcher at UC Berkeley. This has seriously impacted my work.</p><p>I would try to get help from your department. Somewhere within CS and CS-adjacent departments at Berkeley there’s likely to be someone with an official or unofficial connection to Google that can get you in touch with a human to at least clarify the situation.</p></div></td></tr></tbody></table></td></tr><tr id="46841055"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46841055" href="https://news.ycombinator.com/vote?id=46841055&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>So much information is missing from this.</p><p>What Google account? Is it personal Gmail? Or your academic account? Are you using this for personal reasons or professional or commercial reasons? What kind of payment method is attached? What was your level of usage? Any idea why you were suspended initially?</p><p>Because it could be that Google is reviewing your appeal and simply shadow-denying it, and you haven't provided the right information to make it look legit. E.g. if they think you're a spammer or mining crypto or they think you're creating additional free accounts to use free credits, they're obviously not going to tell you what makes them think that.</p><p>But if this is for university-related work, and your university purchases IT+cloud services from Google (as they probably do), talk to your IT department so they can get you in touch with their institution-level support. Obviously, for the attached Google sales rep, the last thing they want is a CS researcher losing access to GCP.</p></div></td></tr></tbody></table></td></tr><tr id="46841161"><td></td></tr><tr id="46840320"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46840320" href="https://news.ycombinator.com/vote?id=46840320&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>&gt; I am a CS researcher at UC Berkeley. This has seriously impacted my work.</p><p>Can I suggest a topic for your next research? "Cloud exascalers and their negative impact on the society"</p></div></td></tr></tbody></table></td></tr><tr id="46840694"><td></td></tr><tr id="46840996"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_46840996" href="https://news.ycombinator.com/vote?id=46840996&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>There are so many things with this statement I don't even know where to start. I hope you're being sarcastic.</p></div></td></tr></tbody></table></td></tr><tr id="46840508"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46840508" href="https://news.ycombinator.com/vote?id=46840508&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Not sure why this is getting downvoted. This may be snark, but this is 100% needed in the world we live in today. It is a fact of today's world that individuals have no leverage over these companies. I can understand why big companies, who have leverage, buy their services. But I don't understand why individuals, who have no leverage, buy their services and build their profession and livelihood around them. Any day, they can cut you off from their services. You are being irresponsible to yourself if you put all your eggs in these big tech baskets.</p><p>We seriously do need this kind of research and compelling articles that argue why relying on these big tech cloud services is harmful for individuals.</p></div></td></tr></tbody></table></td></tr><tr id="46840248"><td></td></tr><tr id="46842633"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46842633" href="https://news.ycombinator.com/vote?id=46842633&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Our GCP account manager was always pretty good at solving these sorts of problems for us at my last company.</p></div></td></tr></tbody></table></td></tr><tr id="46841277"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46841277" href="https://news.ycombinator.com/vote?id=46841277&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>The role of GCP is to help enterprises negotiate better deals out of Azure/AWS.
Why would anyone actually use it is beyond me...</p></div></td></tr></tbody></table></td></tr><tr id="46840296"><td></td></tr><tr id="46839763"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46839763" href="https://news.ycombinator.com/vote?id=46839763&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>It's gone. No human will ever respond to you. That's how these companies operate. From here, you realistically have two options.</p><p>1. Forget the account and move on. You could create a new one, but nobody can tell how long it would take before that gets suspended as well.</p><p>2. If the suspension has a tangible negative impact on your profession, hire a lawyer and get proper legal advice.</p><p>Most important of all, let this be a lesson for you and your colleagues. It is a terrible idea to let any critical part of your life depend on unregulated industries that can wipe out someone's livelihood at the whim of machine learning systems. Learn this lesson and pass it on to everyone you know.</p><p>As an individual, you are nobody to Google and you have no leverage. It is reckless to build your livelihood or profession around their platforms. If you were a company, your team could speak to an account manager and negotiate. As an individual, your only real leverage is legal action.</p><p>Stories like this appear every month. I don't know how many more it will take before it becomes best practice not to depend on these utterly abominable rackets for anything critical.</p></div></td></tr></tbody></table></td></tr><tr id="46840373"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46840373" href="https://news.ycombinator.com/vote?id=46840373&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>&gt; let this be a lesson for you and your colleagues.</p><p>Nah, big tech infiltrates everything, it’s 100% their fault. Why did everyone switch to  webmail? Why did we gravitate to web apps? Big tech persuaded us all to do it.</p><p>With big promises comes great responsibility, and the stuff in the fine print doesn’t count. It’s not ethical to invite dependency and randomly kneecap people; it shouldn’t be legal either.</p></div></td></tr></tbody></table></td></tr><tr id="46840466"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_46840466" href="https://news.ycombinator.com/vote?id=46840466&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>100% agree with you. The big techs are definitely 100% at fault. But you know, fool me once, shame on me. Fool me twice...</p><p>I mean, we get these stories every month. Yes, 100% it is not ethical to randomly kneecap people. But let's be honest. Nobody is working on making these big tech companies accountable for the potentially devastating, algorithm-driven decisions they take. How many more times do they have to fool us before we all realize that it's time to move away from them?</p><p>All I ask from you, myself and all the tech folks here is to learn from these lessons and pass them on to everyone around you. With how things are today, it is reckless to depend on these big tech cloud services for your livelihood and profession. If you're working for a company where the company has leverage, all good. But as an individual, you should stay away from these big tech companies, because they can screw up your life any day, without warning and without recourse.</p></div></td></tr></tbody></table></td></tr><tr id="46840872"><td></td></tr><tr id="46841189"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_46841189" href="https://news.ycombinator.com/vote?id=46841189&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>With email in particular, it's not like chocolate. If you self-host email, or use any other cloud host besides Microsoft or Google, then Microsoft and Google will randomly fail to deliver emails you send to their users, even though you have SPF, DKIM, DMARC, etc. set up exactly right.</p></div></td></tr></tbody></table></td></tr><tr id="46841375"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46841375" href="https://news.ycombinator.com/vote?id=46841375&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Seriously #2 is your only recourse.  Download the terms of service / your service contract , highlight their violations and send them a certified letter about breach of contract and that you intend legal recourse.</p></div></td></tr></tbody></table></td></tr><tr id="46840485"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46840485" href="https://news.ycombinator.com/vote?id=46840485&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Cloud lock-in is real and requires a lot of forethought to avoid or at least mitigate. on the LLM side I have pushed my last two companies to always have at least 3 vendors for hosting and a system to fail over instantly or even load balance based on different criteria. It has paid massive dividends. I wish that philosophy was easier to implement at the cloud level for all services.</p></div></td></tr></tbody></table></td></tr><tr id="46843006"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46843006" href="https://news.ycombinator.com/vote?id=46843006&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>anything that has to do with google avoid at major cost</p><p>or have an alternative ready</p><p>for serious work -- don't use google &amp; don't use google devices either</p></div></td></tr></tbody></table></td></tr><tr id="46840943"><td></td></tr><tr id="46841358"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46841358" href="https://news.ycombinator.com/vote?id=46841358&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>This is an asinine question. Even if you build agnostic solutions (like a docker image), you have storage resources, networks, configs, ACLs, snapshots and more all trapped inside GCP. we’re human — we forget to backup things, or push important commits .  And we know cloud solutions quickly develop lock-in – even a simple cloud DB instance locks you into the vendors config .</p><p>So there are at least a dozen perfectly good reasons this guy is panicking that his account was suddenly revoked without warning.</p></div></td></tr></tbody></table></td></tr><tr id="46841743"><td></td></tr><tr id="46842390"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_46842390" href="https://news.ycombinator.com/vote?id=46842390&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Appeals take time. And it’s not an uncommon case . It doesn’t make his desire to recover the resources any less valid .</p></div></td></tr></tbody></table></td></tr><tr id="46840836"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46840836" href="https://news.ycombinator.com/vote?id=46840836&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>UC Berkeley gets much of its IT infrastructure from Google in a big and expensive contract.  Perhaps some of the campus staff could try to negotiate on your behalf?</p></div></td></tr></tbody></table></td></tr><tr id="46841327"><td></td></tr><tr id="46841898"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46841898" href="https://news.ycombinator.com/vote?id=46841898&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>I had a bad Cloudflare experience. So, my card on file got no balance one day (my bad, I  forgot to update to a new card), and they just turned off the services.</p><p>They somehow managed to charge partial amount (like 80% of the bill), but decided to turn off everything anyway, even the services that could be covered by those 80%. They turned off what they offer for free, and we were unable to change the setting, like instead of their CDN point traffic to an S3 bucket, etc.</p><p>When they do that they basically freeze your account. I mean you cannot provide a new card to pay the outstanding bill, or do anything at all actually. You're not welcomed here anymore. Locked out. That's is a terrible way to react to a payment failure after being a paying customer for a few years.</p><p>It was hard to reach the support, and it took multiple days until I found someone on Reddit who looked at our ticket and it eventually helped.</p><p>PS I had much worse experience with GCP after being a loyal customer of them for like 15 years, so Clouflare is good.</p></div></td></tr></tbody></table></td></tr><tr id="46843501"><td></td></tr><tr id="46842663"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46842663" href="https://news.ycombinator.com/vote?id=46842663&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Yeah, we were looking for image CDN services (with resizing etc). Asked CloudFlare and they said $200 a month, everyone else was saying $3-5k per month.</p><p>Had a sales call with CloudFlare, they said yes they do flat rate billing and it's only $200 a month for all we can eat image hosting.</p><p>We of course called bullshit and third time around (talking to human sales reps) we said, just to get it in writing, we can do X bandwidth/Y images for $200 a month?</p><p>...oh errr, no, that would be more like $7k.</p><p>Thankfully we smelled bullshit and didn't take sales word for it. We'd have built an integration and started paying only to be bitten a month or two later when they readjusted our pricing. They basically refuse to talk about real pricing until you're already paying $200/m and locked in.</p><p>We ended up hosting our own on GKE for $500-$1k/m.</p></div></td></tr></tbody></table></td></tr><tr id="46840318"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46840318" href="https://news.ycombinator.com/vote?id=46840318&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>This is such a repetitious issue that I wonder why there has been no class action suits so far?</p><p>I think documenting these cases somewhere, and targeting not just Alphabet but all the other "we're too big to support little people like you" companies would be a good idea. I don't think the pay out would be significant, but the punitive impact might change things.</p></div></td></tr></tbody></table></td></tr><tr id="46840444"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46840444" href="https://news.ycombinator.com/vote?id=46840444&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>OP is not clear , but it looks like GCP suspension not Google one (I.e email android etc)</p><p>All clouds reject a lot of businesses for their services for variety of reasons and there are alternatives in the market unlike say a Google account suspension .</p><p>I don’t think class action is feasible for cloud computing suspension (unless of course they are discriminating against a protected class etc)</p></div></td></tr></tbody></table></td></tr><tr id="46843069"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_46843069" href="https://news.ycombinator.com/vote?id=46843069&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>i was thinking more in terms of tort law or contract law. They probably have a disclaimer and Tos that addresses all that, but given enough plaintiffs and their market dominance, it might amount to possible deliberate/calculated financial harm. It might be enough to not get thrown out of court at least. They can reject business for any reason, but once someone relies on their services for their business, there is always a certain expectation of continued service, and in the event of service termination, they may not need to explain themselves, but they must accommodate reasonable requests to transfer data, customers,etc.. elsewhere. Otherwise it sounds like tortiuous interference.</p><p><a href="https://en.wikipedia.org/wiki/Tortious_interference" rel="nofollow">https://en.wikipedia.org/wiki/Tortious_interference</a></p><p>&gt; Tortious interference, also known as intentional interference with contractual relations, in the common law of torts, occurs when one person intentionally damages someone else's contractual or business relationships with a third party, causing economic harm.</p><p>In this case, people who use GCP have customers and other contractual relationships. Google's termination of service interfered with that. Google also doing this as a matter of standard business practice indicates that they are aware that their action will interfere with people's contractual obligations (well common sense should tell them that anyways).</p><p>You can't force someone to sign a contract with you that says "if I interfere with your future contracts with arbitrary third parties on purpose, you can't sue me". The deliberate part is crucial from what I understand. If their decision making couldn't have accounted for the interference, and the interference wasn't calculated as an acceptable risk, there is no issue. But the plaintiffs can claim that repeated social media posts and acknowledgements of said interference by Google over the years means it's enough grounds for a suit. and a suit will mean discovery, google will have go hand over internal documents, depose employees,etc...</p><p>In the end, this might be more costly to companies like Google than just giving customers a grace period to move elsewhere before termination.</p><p>Obligatory: IANAL, I'm just a guy using big words I barely understand.</p></div></td></tr></tbody></table></td></tr><tr id="46840756"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46840756" href="https://news.ycombinator.com/vote?id=46840756&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Of all the posts like this I’ve seen the customers are always 1) extremely scant on details about what they were using GCP for or why they were suspended, and more importantly 2) never actually paying for support.</p><p>Having worked with a fair few academics, I’m guessing they lost track of their service account keys and the account got suspended for crypto mining.</p></div></td></tr></tbody></table></td></tr><tr id="46843079"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_46843079" href="https://news.ycombinator.com/vote?id=46843079&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>There have been plenty of posts where the reason was apparent. One i recall was caused by a guy having malware on his phone, and he happened to use a work email on his phone, so the entire GWS organization was banned, shutting down the company's operations.</p></div></td></tr></tbody></table></td></tr><tr id="46840842"><td></td></tr><tr id="46840522"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_46840522" href="https://news.ycombinator.com/vote?id=46840522&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>&gt; but the punitive impact might change things</p><p>Call me cynical but I have little to no hope that even class actions would solve anything. These companies have become so big that they can take one class action after another for years to come without making a dent in their financials and without bringing any change to their operating procedures.</p></div></td></tr></tbody></table></td></tr><tr id="46843083"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_46843083" href="https://news.ycombinator.com/vote?id=46843083&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>I'm just expecting them to change their calculus. Right now it costs them nothing to randomly shut down accounts. If it had some cost, perhaps some minor notice, accommodation,etc.. however automated might be worth just the man hours spent on lawsuits.</p></div></td></tr></tbody></table></td></tr><tr id="46841003"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46841003" href="https://news.ycombinator.com/vote?id=46841003&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>One of the many reasons I continue to degoogle and remove that garbage from my life wherever I can. So many cases like this.</p></div></td></tr></tbody></table></td></tr><tr id="46840574"><td></td></tr><tr id="46841542"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_46841542" href="https://news.ycombinator.com/vote?id=46841542&amp;how=up&amp;goto=item%3Fid%3D46839375"></a></center></td><td><br>
<div><p>Xbox &amp; Discord are the only 2 services I’ve seen handle bans with adequate transparency (yes there is still room to improve).  Both offer a ban status tab ranging from hand-slap to giga-banned , allowing you to have some level of warning before being booted.</p><p>Given how dependent we all are on these services: we run our businesses and our lives, it’s despicable that more due process and transparency is not offered for shadow and proper bans like this.</p></div></td></tr></tbody></table></td></tr></tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nintendo DS code editor and scriptable game engine (142 pts)]]></title>
            <link>https://crl.io/ds-game-engine/</link>
            <guid>46839215</guid>
            <pubDate>Sat, 31 Jan 2026 18:27:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://crl.io/ds-game-engine/">https://crl.io/ds-game-engine/</a>, See on <a href="https://news.ycombinator.com/item?id=46839215">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>  <header><p>2026</p></header> <br data-astro-cid-tkmlznkr=""> <section data-astro-cid-tkmlznkr=""> <p data-astro-cid-tkmlznkr="">TL;DR</p> <p data-astro-cid-tkmlznkr="">
I built a <strong data-astro-cid-tkmlznkr="">scriptable 3D game engine</strong> for the Nintendo DS so
      you can write and run games directly on the console itself. Written in <strong data-astro-cid-tkmlznkr="">C</strong> using
<strong data-astro-cid-tkmlznkr="">libnds</strong>, it compiles to a <strong data-astro-cid-tkmlznkr="">~100KB .nds ROM</strong>
that runs at <strong data-astro-cid-tkmlznkr="">60 FPS</strong>. Features a touch-based code editor
      on the bottom screen and real-time 3D rendering on the top screen. Ships
      with a working 3D pong game as the default script.
</p> </section> <div data-astro-cid-tkmlznkr=""> <p data-astro-cid-tkmlznkr=""> <iframe src="https://www.youtube.com/embed/3NlipciOHcY?si=6oqYL7KYsNa2DzGI&amp;start=0" title="DS game engine video demo short clip" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" data-astro-cid-tkmlznkr=""></iframe> </p> </div> <h2 data-astro-cid-tkmlznkr="">What is it?</h2> <p data-astro-cid-tkmlznkr="">
I felt nostalgic for when I made my first games on an old TI-82 graphing
    calculator. So I tried bringing that whole experience to my Nintendo DS. A
    complete programming environment you can hold in your hands.
</p>  <p data-astro-cid-tkmlznkr="">
What you see is a <strong data-astro-cid-tkmlznkr="">scriptable game engine</strong> with a custom programming
    language featuring variables, loops, and conditionals. You write code using the
    bottom touchscreen, click play, and the game will execute in real-time on the
    top screen with full 3D rendering.
</p> <br data-astro-cid-tkmlznkr=""> <div> <figure data-astro-cid-tkmlznkr=""> <img src="https://crl.io/images/ds-game-engine-reddit.png" alt="" data-astro-cid-tkmlznkr=""> </figure> </div> <h2 data-astro-cid-tkmlznkr="">How it works</h2> <p data-astro-cid-tkmlznkr="">At a high level, the engine breaks down into three parts:</p> <br data-astro-cid-tkmlznkr=""> <h3 data-astro-cid-tkmlznkr="">1. Top screen: 3D rendering (hardware accelerated)</h3> <p data-astro-cid-tkmlznkr="">
Uses the DS's 3D hardware to render colored cubes at 60 FPS. Each model has
    position (X, Y, Z), rotation angle, and color. The camera is fully
    controllable with position and yaw/pitch angles.
</p> <pre data-astro-cid-tkmlznkr="">// DS 3D rendering code (C + libnds)
glMatrixMode(GL_MODELVIEW);
glLoadIdentity();
gluLookAt(camX, camY, camZ,  // camera position
          camX + lookX, camY + lookY, camZ + lookZ,  // look target
          0, 1, 0);  // up vector</pre> <p data-astro-cid-tkmlznkr="">
Each model is drawn with a transform (position + Y-axis rotation), then the
    cube geometry: one color, six quads (24 vertices).
</p> <pre data-astro-cid-tkmlznkr="">// Per-model draw calls (from main.c)
for (i = 0; i &lt; MAX_MODELS; i++) {
    if (!modelActive[i]) continue;
    glPushMatrix();
    glTranslatef(modelX[i], modelY[i], modelZ[i]);
    glRotatef(modelAngle[i], 0, 1, 0);
    drawCube(CUBE_COLORS[modelColorIndex[i]]);
    drawWireframeCube();
    glPopMatrix(1);
}

// Cube geometry: RGB15 color -&gt; glColor3b, then 6 faces as GL_QUADS
glColor3b(r * 255/31, g * 255/31, b * 255/31);
glBegin(GL_QUADS);
    /* +Z face */
    glVertex3f(-1.0f,  1.0f,  1.0f);
    glVertex3f( 1.0f,  1.0f,  1.0f);
    glVertex3f( 1.0f, -1.0f,  1.0f);
    glVertex3f(-1.0f, -1.0f,  1.0f);
    /* -Z, +Y, -Y, +X, -X ... (24 vertices total) */
glEnd();</pre> <br data-astro-cid-tkmlznkr=""> <h3 data-astro-cid-tkmlznkr="">2. Bottom screen: Script editor (software rendered)</h3> <p data-astro-cid-tkmlznkr="">
A touch-based code editor with a custom UI drawn pixel-by-pixel to a 256x192
    bitmap. Features include:
</p> <ul data-astro-cid-tkmlznkr=""> <li data-astro-cid-tkmlznkr=""> <strong data-astro-cid-tkmlznkr="">Token picker</strong>: tap to insert commands (SET, ADD, LOOP,
      IF_GT, etc.)
</li> <li data-astro-cid-tkmlznkr=""><strong data-astro-cid-tkmlznkr="">Numpad</strong>: edit number parameters for each command</li> <li data-astro-cid-tkmlznkr=""> <strong data-astro-cid-tkmlznkr="">Register selector</strong>: choose which variable (A-Z) to use
</li> <li data-astro-cid-tkmlznkr=""><strong data-astro-cid-tkmlznkr="">Play/Pause/Stop/Step</strong>: control script execution</li> <li data-astro-cid-tkmlznkr=""><strong data-astro-cid-tkmlznkr="">6 script slots</strong>: save and load different programs</li> </ul> <pre data-astro-cid-tkmlznkr="">// Software rendering to bottom screen
u16 *subBuffer = (u16*)BG_BMP_RAM_SUB(0);  // 256x192 framebuffer
subBuffer[y * 256 + x] = RGB15(31, 31, 31);  // white pixel</pre> <br data-astro-cid-tkmlznkr=""> <h3 data-astro-cid-tkmlznkr="">3. Script interpreter</h3> <p data-astro-cid-tkmlznkr="">
Executes one line of script per frame (~60 lines/sec). Scripts can use 26
    variables (A-Z) plus 9 read-only registers for input (D-pad, buttons) and
    system state (elapsed time, camera direction).
</p> <pre data-astro-cid-tkmlznkr="">// Script execution (simplified)
if (tokenEquals(script[scriptIP], "add")) {
    int r = scriptReg[scriptIP];  // which register (A-Z)
    registers[r] += getNumberParamValue(scriptIP, 0);
    scriptIP++;  // next line
}</pre> <h2 data-astro-cid-tkmlznkr="">The scripting language</h2> <p data-astro-cid-tkmlznkr="">
Scripts are built from <strong data-astro-cid-tkmlznkr="">tokens</strong> (commands) with numeric parameters.
    Each line executes instantly, with no parsing overhead, just a series of if-checks
    against token names.
</p> <br data-astro-cid-tkmlznkr=""> <h3 data-astro-cid-tkmlznkr="">Available commands</h3> <div data-astro-cid-tkmlznkr=""> <div data-astro-cid-tkmlznkr=""> <p data-astro-cid-tkmlznkr="">Variables &amp; Math</p> <ul data-astro-cid-tkmlznkr=""> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">SET A 5</code> — set register A to 5</li> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">ADD A 1</code> — add 1 to A</li> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">SUBTRACT A 2</code> — subtract 2 from A</li> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">MULTIPLY B -1</code> — multiply B by -1</li> </ul> </div> <div data-astro-cid-tkmlznkr=""> <p data-astro-cid-tkmlznkr="">Control Flow</p> <ul data-astro-cid-tkmlznkr=""> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">LOOP</code> / <code data-astro-cid-tkmlznkr="">END_LOOP</code> — infinite loop</li> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">IF_GT A 10</code> — if A &gt; 10</li> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">IF_LT A 0</code> — if A &lt; 0</li> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">IF_TRUE kA</code> — if A button pressed</li> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">END_IF</code> — close conditional</li> </ul> </div> <div data-astro-cid-tkmlznkr=""> <p data-astro-cid-tkmlznkr="">3D Objects</p> <ul data-astro-cid-tkmlznkr=""> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">MODEL 0</code> — create model at index 0</li> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">POSITION 0 X Y Z</code> — set position</li> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">ANGLE 0 45</code> — set rotation angle</li> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">NEXT_COLOR 0</code> — cycle color</li> </ul> </div> <div data-astro-cid-tkmlznkr=""> <p data-astro-cid-tkmlznkr="">Camera &amp; Rendering</p> <ul data-astro-cid-tkmlznkr=""> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">CAM_POS X Y Z</code> — set camera position</li> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">CAM_ANGLE yaw pitch</code> — set look direction</li> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">BACKGROUND 2</code> — set bg color (0-3)</li> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">BEEP</code> — play 0.1s sound</li> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">SLEEP 0.016</code> — pause (60 FPS = 0.016s/frame)</li> </ul> </div> </div> <br data-astro-cid-tkmlznkr=""> <h3 data-astro-cid-tkmlznkr="">Read-only registers (input &amp; state)</h3> <ul data-astro-cid-tkmlznkr=""> <li data-astro-cid-tkmlznkr=""> <code data-astro-cid-tkmlznkr="">LEFT, UP, RGT, DN</code>: D-pad (1.0 when held, 0.0 when released)
</li> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">KA, KB</code>: A and B buttons</li> <li data-astro-cid-tkmlznkr=""><code data-astro-cid-tkmlznkr="">TIME</code>: elapsed seconds since script started</li> <li data-astro-cid-tkmlznkr=""> <code data-astro-cid-tkmlznkr="">LOOKX, LOOKZ</code>: camera forward direction (normalized X and Z)
</li> </ul> <br data-astro-cid-tkmlznkr=""> <h3 data-astro-cid-tkmlznkr="">Example: 3D pong (default script)</h3> <p data-astro-cid-tkmlznkr="">
The engine ships with a playable pong game. Here's a simplified excerpt:
</p> <pre data-astro-cid-tkmlznkr="">MODEL 0           ; create ball
MODEL 1           ; create paddle
CAM_POS 0 8 18    ; position camera
SET A 0           ; ball X position
SET B 1           ; ball velocity
SET C 0           ; paddle Z position
LOOP
  ADD A B         ; move ball
  IF_GT A 10      ; hit right wall?
    MULTIPLY B -1 ; reverse velocity
  END_IF
  IF_TRUE Up      ; up button pressed?
    ADD C -0.5    ; move paddle up
  END_IF
  POSITION 0 A 0 0     ; update ball position
  POSITION 1 -13 0 C   ; update paddle position
  SLEEP 0.016          ; ~60 FPS
END_LOOP</pre>  <p data-astro-cid-tkmlznkr="">
The full script includes collision detection, game-over logic, and beep
    sounds on miss, all done with simple register math and conditionals.
</p> <h2 data-astro-cid-tkmlznkr="">Technical details</h2> <h3 data-astro-cid-tkmlznkr="">Language &amp; toolchain</h3> <ul data-astro-cid-tkmlznkr=""> <li data-astro-cid-tkmlznkr=""><strong data-astro-cid-tkmlznkr="">Language</strong>: C</li> <li data-astro-cid-tkmlznkr=""> <strong data-astro-cid-tkmlznkr="">Library</strong>: <a href="https://github.com/devkitPro/libnds" target="_blank" rel="noreferrer" data-astro-cid-tkmlznkr="">libnds</a> (Nintendo DS development library)
</li> <li data-astro-cid-tkmlznkr=""> <strong data-astro-cid-tkmlznkr="">Toolchain</strong>: <a href="https://devkitpro.org/" target="_blank" rel="noreferrer" data-astro-cid-tkmlznkr="">devkitPro</a> (ARM cross-compiler)
</li> <li data-astro-cid-tkmlznkr=""><strong data-astro-cid-tkmlznkr="">Source size</strong>: ~3,100 lines of C (main.c)</li> <li data-astro-cid-tkmlznkr=""><strong data-astro-cid-tkmlznkr="">Binary size</strong>: ~100 KB (.nds ROM)</li> <li data-astro-cid-tkmlznkr=""><strong data-astro-cid-tkmlznkr="">Performance</strong>: 60 FPS on DS Lite (2006 hardware)</li> </ul> <br data-astro-cid-tkmlznkr=""> <h3 data-astro-cid-tkmlznkr="">Capabilities &amp; limitations</h3> <ul data-astro-cid-tkmlznkr=""> <li data-astro-cid-tkmlznkr="">Up to <strong data-astro-cid-tkmlznkr="">128 script lines</strong> per program</li> <li data-astro-cid-tkmlznkr=""><strong data-astro-cid-tkmlznkr="">26 variables</strong> (A-Z) + 9 read-only registers</li> <li data-astro-cid-tkmlznkr="">
Up to <strong data-astro-cid-tkmlznkr="">16 3D models</strong> (simple cubes with color/position/rotation)
</li> <li data-astro-cid-tkmlznkr=""><strong data-astro-cid-tkmlznkr="">6 save slots</strong> for different scripts</li> <li data-astro-cid-tkmlznkr="">No dynamic memory allocation, all arrays are statically sized</li> <li data-astro-cid-tkmlznkr="">No string variables, numbers only (floats)</li> <li data-astro-cid-tkmlznkr="">No function calls or subroutines (yet!)</li> </ul> <h2 data-astro-cid-tkmlznkr="">How to build &amp; run</h2> <h3 data-astro-cid-tkmlznkr="">Compilation (on your computer)</h3> <ol data-astro-cid-tkmlznkr=""> <li data-astro-cid-tkmlznkr="">
Install <a href="https://devkitpro.org/wiki/Getting_Started" target="_blank" rel="noreferrer" data-astro-cid-tkmlznkr="">devkitPro</a> (includes devkitARM and libnds)
</li> <li data-astro-cid-tkmlznkr=""> <a href="https://crl.io/ds-game-engine.zip" download="" data-astro-cid-tkmlznkr="">Download the source code</a> (main.c
      + Makefile)
</li><li data-astro-cid-tkmlznkr="">
Run <code data-astro-cid-tkmlznkr="">make</code> in the project directory
</li> <li data-astro-cid-tkmlznkr="">
Output: <code data-astro-cid-tkmlznkr="">program.nds</code> (~100 KB ROM file)
</li> </ol> <br data-astro-cid-tkmlznkr=""> <h3 data-astro-cid-tkmlznkr="">Running on real hardware</h3> <p data-astro-cid-tkmlznkr="">
You need a <strong data-astro-cid-tkmlznkr="">flashcart</strong> (e.g. R4, DSTT, Acekard) with a microSD
    card:
</p> <ol data-astro-cid-tkmlznkr=""> <li data-astro-cid-tkmlznkr="">
Copy <code data-astro-cid-tkmlznkr="">program.nds</code> to the microSD card
</li> <li data-astro-cid-tkmlznkr="">Insert the microSD into the flashcart</li> <li data-astro-cid-tkmlznkr="">Insert the flashcart into your DS</li> <li data-astro-cid-tkmlznkr="">Boot the DS and select the ROM from the flashcart menu</li> </ol> <p data-astro-cid-tkmlznkr=""> <strong data-astro-cid-tkmlznkr="">Note</strong>: I got my R4 cart + SD card from a friend years ago,
    so I don't have detailed setup instructions for the cart itself. Most modern
    flashcarts just need you to copy their firmware to the SD root, then add
    ROMs in a folder.
</p> <h2 data-astro-cid-tkmlznkr="">Try it in your browser (Nintendo DS emulator)</h2> <p data-astro-cid-tkmlznkr="">
You can test the DS game engine build directly below. The emulator loads <code data-astro-cid-tkmlznkr="">ds-game-engine.nds</code>. Loads a more basic pong game than the one in the video.
</p>   <p data-astro-cid-tkmlznkr="">
Nintendo DS emulator (<a href="https://notan127.github.io/DS-Emulator-Web/" target="_blank" rel="noreferrer" data-astro-cid-tkmlznkr="">Desmond</a>). If the game doesn’t start, ensure JavaScript is enabled and the page has
    finished loading.
</p> <h2 data-astro-cid-tkmlznkr="">Download</h2>  <p data-astro-cid-tkmlznkr=""> <a href="https://crl.io/dist/ds-game-engine.zip" data-astro-cid-tkmlznkr="">Source (ds-game-engine.zip)</a> </p>  <p data-astro-cid-tkmlznkr=""> <a href="https://crl.io/dist/ds-game-engine.nds" data-astro-cid-tkmlznkr="">Compiled ROM (ds-game-engine.nds)</a> </p> <h2 data-astro-cid-tkmlznkr="">Discussion</h2>  <p data-astro-cid-tkmlznkr="">
Feel free to ask or discuss in
<a href="https://www.reddit.com/r/NDSHacks/comments/1qrwost/ds_code_editor_making_3d_pong/" target="_blank" rel="noreferrer" data-astro-cid-tkmlznkr="">this Reddit thread</a> </p>      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Genode OS is a tool kit for building highly secure special-purpose OS (116 pts)]]></title>
            <link>https://genode.org/about/index</link>
            <guid>46838981</guid>
            <pubDate>Sat, 31 Jan 2026 18:03:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://genode.org/about/index">https://genode.org/about/index</a>, See on <a href="https://news.ycombinator.com/item?id=46838981">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="portal-column-content">

 <p>
  The Genode OS Framework is a tool kit for building highly secure
  special-purpose operating systems. It scales from embedded systems with as
  little as 4 MB of memory to highly dynamic general-purpose workloads.
 </p>
 <p>
  Genode is based on a recursive system structure. Each program runs in a
  dedicated sandbox and gets granted only those access rights and resources that
  are needed for its specific purpose. Programs can create and manage
  sub-sandboxes out of their own resources, thereby forming hierarchies where
  policies can be applied at each level. The framework provides mechanisms to
  let programs communicate with each other and trade their resources, but only
  in strictly-defined manners. Thanks to this rigid regime, the attack surface
  of security-critical functions can be reduced by orders of magnitude compared
  to contemporary operating systems.
 </p>
 <p>
  The framework aligns the construction principles of L4 with Unix philosophy.
  In line with Unix philosophy, Genode is a collection of small building blocks,
  out of which sophisticated systems can be composed. But unlike Unix, those
  building blocks include not only applications but also all classical OS
  functionalities including kernels, device drivers, file systems, and protocol
  stacks.
 </p>
 
 <ul>
  <li>
   <p>
    CPU architectures: x86 (32 and 64 bit), ARM (32 and 64 bit), RISC-V
   </p>
  </li>
  <li>
   <p>
    Kernels: most members of the L4 family
    (<a href="http://hypervisor.org/">NOVA</a>,
    <a href="https://sel4.systems/">seL4</a>,
    <a href="http://os.inf.tu-dresden.de/fiasco/">Fiasco.OC</a>,
    <a href="http://okl4.org/">OKL4 v2.1</a>,
    <a href="http://www.l4ka.org/65.php">L4ka::Pistachio</a>,
    <a href="http://os.inf.tu-dresden.de/fiasco/prev/">L4/Fiasco</a>),
    Linux, and a custom kernel.
   </p>
  </li>
  <li>
   <p>
    Virtualization: VirtualBox (on NOVA), a custom virtual machine monitor
    for ARM, and a custom runtime for Unix software
   </p>
  </li>
  <li>
   <p>
    Over 100 ready-to-use
    <a href="https://genode.org/documentation/components">components</a>
   </p>
  </li>
 </ul>
 <p>
  Genode is open source and commercially supported by
  <a href="http://www.genode-labs.com/">Genode Labs</a>.
 </p>
 <div><dl>
  <dt><a href="https://genode.org/about/road-map">Road map</a></dt>
  <dd>
   <p>
    The direction where the project is currently heading
   </p>
  </dd>
  <dt><a href="https://genode.org/about/challenges">Challenges</a></dt>
  <dd>
   <p>
    A collection of project ideas, giving a glimpse on possible future directions
   </p>
  </dd>
  <dt><a href="https://genode.org/about/publications">Publications</a></dt>
  <dd>
   <p>
    Publications related to Genode
   </p>
  </dd>
  <dt><a href="https://genode.org/about/licenses">Licensing</a></dt>
  <dd>
   <p>
    Open-Source and commercial licensing
   </p>
  </dd>
  <dt><a href="https://genode.org/about/screenshots">Screenshots</a></dt>
  <dd>
   <p>
    Screenshots of Genode-based system scenarios
   </p>
  </dd>
 </dl></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US has investigated claims WhatsApp chats aren't private (195 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2026-01-29/us-has-investigated-claims-that-whatsapp-chats-aren-t-private</link>
            <guid>46838635</guid>
            <pubDate>Sat, 31 Jan 2026 17:25:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2026-01-29/us-has-investigated-claims-that-whatsapp-chats-aren-t-private">https://www.bloomberg.com/news/articles/2026-01-29/us-has-investigated-claims-that-whatsapp-chats-aren-t-private</a>, See on <a href="https://news.ycombinator.com/item?id=46838635">Hacker News</a></p>
Couldn't get https://www.bloomberg.com/news/articles/2026-01-29/us-has-investigated-claims-that-whatsapp-chats-aren-t-private: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Mobile carriers can get your GPS location (712 pts)]]></title>
            <link>https://an.dywa.ng/carrier-gnss.html</link>
            <guid>46838597</guid>
            <pubDate>Sat, 31 Jan 2026 17:21:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://an.dywa.ng/carrier-gnss.html">https://an.dywa.ng/carrier-gnss.html</a>, See on <a href="https://news.ycombinator.com/item?id=46838597">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  

  <p>
    <time datetime="2026-01-31 00:00:00 +0000">2026-01-31</time>
  </p>
  
  <p>In iOS 26.3, Apple introduced a new privacy feature which limits “precise location” data made available to cellular networks via cell towers. The feature is only available to devices with Apple’s in-house modem introduced in 2025. The announcement<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup> says</p>

<blockquote>
  <p>Cellular networks can determine your location based on which cell towers your device connects to.</p>
</blockquote>

<p>This is well-known. I have served on a jury where the prosecution obtained location data from cell towers. Since cell towers are sparse (especially before 5G), the accuracy is in the range of tens to hundreds of metres<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup>.</p>

<p><strong>But this is not the whole truth</strong>, because cellular standards have built-in protocols that make your device silently send GNSS (i.e. GPS, GLONASS, Galileo, BeiDou) location to the carrier. This would have the same precision as what you see in your Map apps, in single-digit metres.</p>

<p>In 2G and 3G this is called <a href="https://projects.osmocom.org/projects/security/wiki/RRLP">Radio Resources LCS Protocol (RRLP)</a></p>

<blockquote>
  <p>So the network simply asks “tell me your GPS coordinates if you know them” and the phone will respond<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup>.</p>
</blockquote>

<p>In 4G and 5G this is called <a href="https://tech-academy.amarisoft.com/LTE_LPP.html">LTE Positioning Protocol (LPP)</a></p>

<blockquote>
  <p>RRLP, RRC, and LPP are natively control-plane positioning protocols. This means that they are transported in the inner workings of cellular networks and are practically invisible to end users<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup>.</p>
</blockquote>

<p>It’s worth noting that GNSS location is never <em>meant</em> to leave your device. GNSS coordinates are calculated entirely passively, your device doesn’t need to send a single bit of information. Using GNSS is like finding out where you are by reading a road sign: you don’t have to tell anyone else you read a road sign, anyone can read a road sign, and the people who put up road signs don’t know who read which road sign when.</p>

<p>These capabilities are not secrets but somehow they have mostly slid under the radar of the public consciousness. They have been used in the wild for a long time, such as by the DEA in the US in 2006<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" rel="footnote">5</a></sup><sup id="fnref:6" role="doc-noteref"><a href="#fn:6" rel="footnote">6</a></sup>:</p>

<blockquote>
  <p>[T]he DEA agents procured a court order (but not a search warrant) to obtain GPS coordinates from the courier’s phone via a ping, or signal requesting those coordinates, sent by the phone company to the phone.</p>
</blockquote>

<p>And by Shin Bet in Israel, which tracks everyone everywhere all the time<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" rel="footnote">7</a></sup>:</p>

<blockquote>
  <p>The GSS Tool was based on centralized cellular tracking operated by Israel’s General Security Services (GSS). The technology was based on a framework that tracks all the cellular phones running in Israel through the cellular companies’ data centers. According to news sources, it routinely collects information from cellular companies and identifies the location of all phones through cellular antenna triangulation and GPS data<sup id="fnref:7:1" role="doc-noteref"><a href="#fn:7" rel="footnote">7</a></sup>.</p>
</blockquote>

<p>Notably, the Israeli government started using the data for contact tracing in March 2020<sup id="fnref:7:2" role="doc-noteref"><a href="#fn:7" rel="footnote">7</a></sup><sup id="fnref:8" role="doc-noteref"><a href="#fn:8" rel="footnote">8</a></sup>, only a few weeks after the first Israeli COVID-19 case. An individual would be sent an SMS message informing them of close contact with a COVID patient and required to quarantine. This is good evidence that the location data Israeli carriers are collecting are far more precise than what cell towers alone can achieve.</p>

<p>A major caveat is that I don’t know if RRLP and LPP are the exact techniques, and the only techniques, used by DEA, Shin Bet, and possibly others to collect GNSS data; there could be other protocols or backdoors we’re not privy to.</p>

<p>Another unknown is whether these protocols can be exploited remotely by a foreign carrier. Saudi Arabia has abused SS7 to spy on people in the US<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" rel="footnote">9</a></sup>, but as far as I know this only locates a device to the coverage area of a Mobile Switching Center, which is less precise than cell tower data. Nonetheless, given the abysmal culture, competency, and integrity in the telecom industry, I would not be shocked if it’s possible for a state actor to obtain the precise GNSS coordinates of anyone on earth using a phone number/IMEI.</p>

<p>Apple made a good step in iOS 26.3 to limit at least one vector of mass surveillance, enabled by having full control of the modem silicon and firmware. They must now allow users to disable GNSS location responses to mobile carriers, and notify the user when such attempts are made to their device.</p>

<hr>


</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Finland to end "uncontrolled human experiment" with ban on youth social media (646 pts)]]></title>
            <link>https://yle.fi/a/74-20207494</link>
            <guid>46838417</guid>
            <pubDate>Sat, 31 Jan 2026 17:06:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://yle.fi/a/74-20207494">https://yle.fi/a/74-20207494</a>, See on <a href="https://news.ycombinator.com/item?id=46838417">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p>Lunch break at the Finnish International School of Tampere (FISTA) is a boisterous time.</p><p>The yard is filled with children — ranging from grades 1 to 9, or ages 6 to 16 — running around, shouting, playing football, shooting basketball hoops, doing what kids do.</p><p>And there's not a single screen in sight.</p><p>FISTA has taken advantage of the <a href="https://yle.fi/a/74-20103459" role="link">law change</a>, brought in last August, which allows schools to restrict or completely ban the use of mobile phones during school hours. At FISTA, this means no phones at all unless specifically used for learning in the classroom.</p><p>"We've seen that cutting down on the possibilities for students to use their phones, during the breaks for instance, has spurred a lot of creativity," FISTA vice principal <strong>Antti Koivisto</strong> notes.</p><p>"They're more active, doing more physical things like playing games outdoors or taking part in the organised break activities or just socialising with each other."</p><p>With the smartphone restriction in schools widely considered to have been a success, Finland's government has now set its sights on social media platforms.</p><p>Prime Minister <strong>Petteri Orpo</strong> (NCP) <a href="https://yle.fi/a/74-20204220" role="link">said earlier this month</a> that he supports banning the use of social media by children under the age of 15.</p><p>"I am deeply concerned about the lack of physical activity among children and young people, and the fact that it is increasing," Orpo said at the time.</p><p>And there is a growing groundswell of support for Finland introducing such a ban. Two-thirds of respondents to a survey published earlier this week <a href="https://yle.fi/a/74-20206519" role="link">said they back a ban</a> on social media for under-15s. This is a near 10 percentage point jump compared to a similar survey carried out just last summer.</p><h2>"Uncontrolled human experiment"</h2><p>The concerns over social media, and in particular the effects on children, have been well-documented — but Finnish researcher <strong>Silja Kosola</strong>'s <a href="https://yle.fi/a/74-20205877" role="link">recent description of the phenomenon</a> as an "uncontrolled human experiment" has grabbed people's attention once again.</p><p>Kosola, an associate professor in adolescent medicine, has researched the impact of social media on young people, and tells Yle News that the consequences are not very well understood.</p><p>"We see a rise in self-harm and especially eating disorders. We see a big separation in the values of young girls and boys, which is also a big problem in society," Kosola explains.</p><p><strong>In the video below, Silja Kosola explains the detrimental effects that excessive use of social media can have on young people.</strong></p><figure><figcaption><span>Silja Kosola speaking on the All Points North podcast.</span></figcaption></figure><p>She further notes that certain aspects of Finnish culture — such as the independence and freedom granted to children from a young age — have unwittingly exacerbated the ill effects of social media use.</p><p>"We have given smartphones to younger people more than anywhere else in the world. Just a couple of years ago, about 95 percent of first graders had their own smartphone, and that hasn't happened anywhere else," she says.</p><h2>All eyes on Australia</h2><p>Since 10 December last year, children under the age of 16 in Australia have been banned from using social media platforms such as TikTok, Snapchat, Facebook, Instagram and YouTube.</p><p>Prime Minister <strong>Anthony Albanese</strong> began drafting the legislation after he received a heartfelt letter from a grieving mother who lost her 12-year-old daughter to suicide.</p><p>Although Albanese has never revealed the details of the letter, <a href="https://www.abc.net.au/news/2025-12-13/how-australia-developed-social-media-ban-under-16s/106137700" role="link">he told public broadcaster</a> ABC that it was "obvious social media had played a key role" in the young girl's death.</p><p>The legislation aims to shift the burden away from parents and children and onto the social media companies, who face fines of up to 49.5 million Australian dollars (29 million euros) if they consistently fail to keep kids off their platforms.</p><p><strong>Clare Armstrong</strong>, ABC's chief digital political correspondent, told Yle News that the initial reaction to the roll-out has been some confusion but no little "relief".</p><p>"The government often talks about this law as being a tool to help parents and other institutions enforce and start conversations about tech and social media in ways that before, they couldn't," she says.</p><p>Although it is still early days, as the ban has only been in force for about six weeks, Armstrong adds that the early indicators have been good.</p><p><strong>ABC journalist Clare Armstrong explains in the video below how children in Australia have been spending their time since the social media ban was introduced.</strong></p><figure><figcaption><span>Clare Armstrong speaking on the All Points North podcast.</span></figcaption></figure><p>However, she adds a note of caution to any countries — such as Finland — looking to emulate the Australian model, noting that communication is key.</p><p>"Because you can write a very good law, but if the public doesn't understand it, and if it can't be enforced at that household level easily, then it's bound to fail," Armstrong says.</p><h2>Playing to Finland's strengths</h2><p><strong>Seona Candy,</strong> an Australian living in Helsinki for over eight years, has been keenly following the events in her homeland since the social media ban came into effect in December.</p><p>She has heard anecdotally that if kids find themselves blocked from one platform, they just set up an account on another, "ones that maybe their parents don't even know exist".</p><p>"And this is then much, much harder, because those platforms don't have parental controls, so they don't have those things already designed into them that the more mainstream platforms do," Candy says.</p><p>Because of this issue, and others she has heard about, she warns against Finland introducing like-for-like legislation based around Australia's "reactive, knee-jerk" law change.</p><p>"I think the Finnish government should really invest in digital education, and digital literacy, and teach kids about digital safety. Finland is world-famous for education, and for media literacy. Play to your strengths, right?"</p><p><em>The All Points North podcast asked if Finland should introduce a similar ban on social media as in Australia. You can listen to the episode via this embedded player, on</em> <a href="https://areena.yle.fi/podcastit/1-4355773" role="link"><em>Yle Areena</em></a><em>,</em> <em>via</em> <a href="https://podcasts.apple.com/us/podcast/all-points-north/id1678541537" role="link"><em>Apple</em></a>, <a href="https://open.spotify.com/show/11M4NJ3cfmNCo0qYiIXXU1" role="link"><em>Spotify</em></a> <em>or wherever you get your podcasts.</em></p><figure><div><div><p><strong>Should Finland ban kids from using social media?</strong></p><div><canvas></canvas><picture><source data-testid="source-for-S" media="(max-width: 767px)" srcset="https://images.cdn.yle.fi/image/upload/ar_1.0,c_fill,g_faces,h_104,w_104/dpr_2.0/q_auto:eco/f_auto/fl_lossy/13-1-4355773-1756384400917 2x,https://images.cdn.yle.fi/image/upload/ar_1.0,c_fill,g_faces,h_104,w_104/dpr_1.0/q_auto:eco/f_auto/fl_lossy/13-1-4355773-1756384400917 1x"><source data-testid="source-for-M" media="(min-width: 768px)" srcset="https://images.cdn.yle.fi/image/upload/ar_1.0,c_fill,g_faces,h_135,w_135/dpr_2.0/q_auto:eco/f_auto/fl_lossy/13-1-4355773-1756384400917 2x,https://images.cdn.yle.fi/image/upload/ar_1.0,c_fill,g_faces,h_135,w_135/dpr_1.0/q_auto:eco/f_auto/fl_lossy/13-1-4355773-1756384400917 1x"><img alt="" src="https://images.cdn.yle.fi/image/upload/ar_1.7777777777777777,c_fill,g_faces,h_75,w_135/dpr_1.0/q_auto:eco/f_auto/fl_lossy/13-1-4355773-1756384400917"></picture></div></div></div></figure></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Film students who can no longer sit through films (125 pts)]]></title>
            <link>https://www.theatlantic.com/ideas/2026/01/college-students-movies-attention-span/685812/</link>
            <guid>46838026</guid>
            <pubDate>Sat, 31 Jan 2026 16:26:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/ideas/2026/01/college-students-movies-attention-span/685812/">https://www.theatlantic.com/ideas/2026/01/college-students-movies-attention-span/685812/</a>, See on <a href="https://news.ycombinator.com/item?id=46838026">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-event-module="article body" data-flatplan-body="true"><p data-flatplan-paragraph="true">Everyone knows it’s hard to get college students to do the reading—remember books? But the attention-span crisis is not limited to the written word. Professors are now finding that they can’t even get film students—<em>film </em>students—to sit through movies. “I used to think, <em>If homework is watching a movie, that is the best homework ever</em>,” Craig Erpelding, a film professor at the University of Wisconsin at Madison, told me. “But students will not do it.”</p><p data-flatplan-paragraph="true">I heard similar observations from 20 film-studies professors around the country. They told me that over the past decade, and particularly since the pandemic, students have struggled to pay attention to feature-length films. Malcolm Turvey, the founding director of Tufts University’s Film and Media Studies Program, officially bans electronics during film screenings. Enforcing the ban is another matter: About half the class ends up looking furtively at their phones.</p><p data-flatplan-paragraph="true">A handful of professors told me they hadn’t noticed any change. Some students have always found old movies to be slow, Lynn Spigel, a professor of screen cultures at Northwestern University, told me. “But the ones who are really dedicated to learning film always were into it, and they still are.”</p><p data-flatplan-paragraph="true">Most of the instructors I spoke with, however, feel that something is different now. And the problem is not limited to large introductory courses. Akira Mizuta Lippit, a cinema and media-studies professor at the University of Southern California—home to perhaps the top film program in the country—said that his students remind him of nicotine addicts going through withdrawal during screenings: The longer they go without checking their phone, the more they fidget. Eventually, they give in. He recently screened the 1974 Francis Ford Coppola classic <em>The Conversation</em>. At the outset, he told students that even if they ignored parts of the film, they needed to watch the famously essential and prophetic final scene. Even that request proved too much for some of the class. When the scene played, Lippit noticed that several students were staring at their phones, he told me. “You do have to just pay attention at the very end, and I just can’t get everybody to do that,” he said.</p><p id="injected-recirculation-link-0" data-view-action="view link - injected link - item 1" data-event-element="injected link" data-event-position="1"><a href="https://www.theatlantic.com/magazine/archive/2024/11/the-elite-college-students-who-cant-read-books/679945/">From the November 2024 issue: The elite college students who can’t read books</a></p><p data-flatplan-paragraph="true">Many students are resisting the idea of in-person screenings altogether. Given the ease of streaming assignments from their dorm rooms, they see gathering in a campus theater as an imposition. Professors whose syllabi require in-person screenings outside of class time might see their enrollment drop, Meredith Ward, director of the Program in Film and Media Studies at Johns Hopkins University, told me. Accordingly, many professors now allow students to stream movies on their own time.</p><p data-flatplan-paragraph="true">You can imagine how that turns out. At Indiana University, where Erpelding worked until 2024, professors could track whether students watched films on the campus’s internal streaming platform. Fewer than 50 percent would even start the movies, he said, and only about 20 percent made it to the end. (Recall that these are students who chose to take a film class.) Even when students stream the entire film, it’s not clear how closely they watch it. Some are surely folding laundry or scrolling Instagram, or both, while the movie plays.</p><p data-flatplan-paragraph="true">The students I spoke with admitted to their own inattentiveness. They even felt bad about it. But that wasn’t enough to make them sit through the assigned movies. Mridula Natarajan, a freshman at the University of Texas at Austin, took a world-cinema class this past fall. “There were some movies that were extremely slow-paced, and ironically, that was the point of the movie,” she told me. “But I guess impatience made me skip through stuff or watch it on two-times speed.”</p><p data-flatplan-paragraph="true">After watching movies distractedly—if they watch them at all—students unsurprisingly can’t answer basic questions about what they saw. In a multiple-choice question on a recent final exam, Jeff Smith, a film professor at UW Madison, asked what happens at the end of the Truffaut film <em>Jules and Jim</em>. More than half of the class picked one of the wrong options, saying that characters hide from the Nazis (the film takes place during World War I) or get drunk with Ernest Hemingway (who does not appear in the movie). Smith has administered similar exams for almost two decades; he had to grade his most recent exam on a curve to keep students’ marks within a normal range.</p><p data-flatplan-paragraph="true">The professors I spoke with didn’t blame students for their shortcomings; they focused instead on how media diets have changed. From 1997 to 2014, screen time for children under age 2 doubled. And the screen in question, once a television, is now more likely to be a tablet or a smartphone. Students arriving in college today have no memory of a world before the infinite scroll. As teenagers, they spent nearly five hours a day on social media, with much of that time used for flicking from one short-form video to the next. An <a data-event-element="inline link" href="https://www.apa.org/news/podcasts/speaking-of-psychology/attention-spans">analysis</a> of people’s attention while working on a computer found that they now switch between tabs or apps every 47 seconds, down from once every two and a half minutes in 2004. “I can imagine that if your body and your psychology are not trained for the duration of a feature-length film, it will just feel excruciatingly long,” USC’s Lippit said. (He also hypothesized that, because every movie is available on demand, students feel that they can always rewatch should they miss something—even if they rarely take advantage of that option.)</p><p id="injected-recirculation-link-1" data-view-action="view link - injected link - item 2" data-event-element="injected link" data-event-position="2"><a href="https://www.theatlantic.com/podcasts/archive/2024/03/smartphone-anxious-generation-mental-health/677817/">Listen: The smartphone kids are not all right</a></p><p data-flatplan-paragraph="true">Kyle Stine, a film and media-studies professor at Johns Hopkins, usually begins his course with an icebreaker: <em>What’s a movie you watched recently?</em> In the past few years, some students have struggled to name any film. Kristen Warner, a performing- and media-arts professor at Cornell University, has noticed a similar trend. Some of her students arrive having seen only Disney movies. Erpelding, at UW Madison, said he tries to find a movie that everyone in his class has seen, to serve as a shared reference point they can talk about. Lately, that’s become impossible. Even students who are interested in going into filmmaking don’t necessarily love watching films. “The disconnect is that 10 years ago, people who wanted to go study film and media creation were cinephiles themselves,” Erpelding told me. “Nowadays, they’re people that consume the same thing everyone else consumes, which is social media.”</p><p data-flatplan-paragraph="true">Of course, young people haven’t given up on movies altogether. But the feature films that they do watch now tend to be engineered to cater to their attentional deficit. In a recent appearance on <em>The Joe Rogan Experience</em>, Matt Damon, the star of many movies that college students may not have seen, said that Netflix has started encouraging filmmakers to put action sequences in the first five minutes of a film to get viewers hooked. And just because young people are streaming movies, it doesn’t mean they’re paying attention. When they sit down to watch, many are browsing social media on a second screen. Netflix has accordingly advised directors to have characters <a data-event-element="inline link" href="https://au.variety.com/2026/film/news/matt-damon-netflix-movies-restate-plot-viewers-on-phones-32039/">repeat the plot</a> three or four times so that multitasking audiences can keep up with what’s happening, Damon said.</p><p data-flatplan-paragraph="true">Some professors are treating wilting attention spans as a problem to be solved, not a reality to accept. Stine, at Johns Hopkins, is piloting a course on “slow cinema”—minimalist films with almost no narrative thrust—with the goal of helping students redevelop long modes of attention. Rick Warner, the director of film studies at the University of North Carolina, deliberately selects films with slow pacing and subtle details, such as Chantal Akerman’s <em>Jeanne Dielman, 23 quai du Commerce, 1080 Bruxelles</em>, a three-hour movie that mostly follows a woman doing chores in her apartment. “I try to teach films that put their habits of viewing under strain,” Warner told me. “I’m trying to sell them on the idea that a film watched properly can actually help them retrain their perception and can teach them how to concentrate again.” Once they get used to it, students enjoy the challenge, he said.</p><p data-flatplan-paragraph="true">But other professors, perhaps concluding that resistance is futile, are adjusting to the media their students grew up on. Some show shorter films or have students watch movies over multiple sittings. Erpelding, who primarily teaches filmmaking courses, has moved from teaching traditional production methods to explaining how to maximize audience engagement. He now asks students to make three- or four-minute films, similar to the social-media edits they see online. After all, that seems to be the only type of video many young people want to watch.</p><p data-flatplan-paragraph="true">By the way, the last scene of <em>The Conversation</em> has the paranoid Gene Hackman destroying his apartment in a desperate and futile search for listening devices. He eventually gives up, and mournfully plays the saxophone amid the wreckage. It’s a brilliant scene, and worth the wait.</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Platform Security (Jan 2026) [pdf] (186 pts)]]></title>
            <link>https://help.apple.com/pdf/security/en_US/apple-platform-security-guide.pdf</link>
            <guid>46837814</guid>
            <pubDate>Sat, 31 Jan 2026 16:04:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://help.apple.com/pdf/security/en_US/apple-platform-security-guide.pdf">https://help.apple.com/pdf/security/en_US/apple-platform-security-guide.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=46837814">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia's 10-year effort to make the Shield TV the most updated Android device (173 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2026/01/inside-nvidias-10-year-effort-to-make-the-shield-tv-the-most-updated-android-device-ever/</link>
            <guid>46837346</guid>
            <pubDate>Sat, 31 Jan 2026 15:14:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2026/01/inside-nvidias-10-year-effort-to-make-the-shield-tv-the-most-updated-android-device-ever/">https://arstechnica.com/gadgets/2026/01/inside-nvidias-10-year-effort-to-make-the-shield-tv-the-most-updated-android-device-ever/</a>, See on <a href="https://news.ycombinator.com/item?id=46837346">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            <article data-id="2138185">
  
  <header>
  <div>
    

    

    <p>
      “Selfishly a little bit, we built Shield for ourselves.”
    </p>

          
    
    <div>
            <p><a data-pswp-width="1920" data-pswp-height="1080" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1.jpg 1920w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1-1536x864.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1-1440x810.jpg 1440w" data-cropped="false" href="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1.jpg" target="_blank">
              <img width="1920" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1.jpg" alt="Shield TV box" loading="eager" decoding="async" fetchpriority="high" srcset="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1.jpg 1920w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1-1536x864.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-1-1440x810.jpg 1440w" sizes="(max-width: 1920px) 100vw, 1920px">
            </a></p><div id="caption-2138228">
    
    <p>
      The Shield TV has that classic Nvidia aesthetic. 

              <span>
          Credit:

          
          Ryan Whitwam

                  </span>
          </p>
  </div>
          </div>

    <div>
    
    <p>
      The Shield TV has that classic Nvidia aesthetic. 

              <span>
          Credit:

          
          Ryan Whitwam

                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>It took Android devicemakers a very long time to commit to long-term update support. Samsung and Google have only recently decided to offer <a href="https://arstechnica.com/gadgets/2023/10/the-google-pixel-8-is-official-with-7-years-of-updates/">seven years of updates</a> for their flagship Android devices, but a decade ago, you were lucky to get more than one or two updates on even the most expensive Android phones and tablets. How is it, then, that an Android-powered set-top box from 2015 is still going strong?</p>
<p>Nvidia released the first <a href="https://arstechnica.com/gadgets/2015/10/nvidia-shield-android-tv-review-a-powerful-do-it-all-box-that-lacks-content/">Shield Android TV in 2015</a>, and according to the company’s senior VP of hardware engineering, Andrew Bell, supporting these devices has been a labor of love. And the team at Nvidia still loves the Shield. Bell assures us that Nvidia has never given up, even when it looked like support for the Shield was waning, and it doesn’t plan to stop any time soon.</p>
<h2>The soul of Shield</h2>
<p>Gaming has been central to Nvidia since its start, and that focus gave rise to the Shield. “Pretty much everybody who worked at Nvidia in the early days really wanted to make a game console,” said Bell, who has worked at the company for 25 years.</p>
<p>However, Nvidia didn’t have what it needed back then. Before gaming, crypto, and AI turned it into the multi-trillion-dollar powerhouse it is today, Nvidia had a startup mentality and the budget to match. When Shield devices began percolating in the company’s labs, it was seen as an important way to gain experience with “full-stack” systems and all the complications that arise when managing them.</p>
<p>“To build a game console was pretty complicated because, of course, you have to have a GPU, which we know how to make,” Bell explained. “But in addition to that, you need a CPU, an OS, games, and you need a UI.”</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>Through acquisitions and partnerships, the pieces of Nvidia’s fabled game console slowly fell into place. The purchase of PortalPlayer in 2007 brought the CPU technology that would become the Tegra Arm chips, and the company’s surging success in GPUs gave it the partnerships it needed to get games. But the UI was still missing—that didn’t change until Google expanded Android to the TV in 2014. The company’s first Android mobile efforts were already out there in the form of the Shield Portable and Shield Tablet, but the TV-connected box is what Nvidia really wanted.</p>
<p>“Selfishly, a little bit, we built Shield for ourselves,” Bell told Ars Technica. “We actually wanted a really good TV streamer that was high-quality and high-performance, and not necessarily in the Apple ecosystem. We built some prototypes, and we got so excited about it. [CEO Jensen Huang] was like, ‘Why don’t we bring it out and sell it to people?’”</p>
<p>The first Shield box in 2015 had a heavy gaming focus, with a raft of both local and cloud-based (<a href="https://arstechnica.com/gaming/2023/01/geforce-now-ultimate-first-impressions-streaming-has-come-a-real-long-way/">GeForce Now</a>) games. The base model included only a game controller, with the remote control sold separately. According to Bell, Nvidia eventually recognized that the gaming angle wasn’t as popular as it had hoped. The 2017 and 2019 Shield refreshes were more focused on the streaming experience.</p>
<p>“Eventually, we kind of said, ‘Maybe the soul is that it’s a streamer for gamers,’” said Bell. “We understand gamers from GeForce, and we understand they care about quality and performance. A lot of these third-party devices like tablets, they’re going cheap. Set-top boxes, they’re going cheap. But we were the only company that was like, ‘Let’s go after people who really want a premium experience.’”</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<figure>
    <div>
            <p><a data-pswp-width="1920" data-pswp-height="1080" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3.jpg 1920w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3-1536x864.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3-1440x810.jpg 1440w" data-cropped="false" href="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3.jpg" target="_blank">
              <img width="1920" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3.jpg" alt="Shield controller" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3.jpg 1920w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3-1536x864.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-3-1440x810.jpg 1440w" sizes="auto, (max-width: 1920px) 100vw, 1920px">
            </a></p><div id="caption-2138231"><p>
              Nvidia used to sell Shield-branded game controllers.
                              </p><p>
                  Credit:
                                      Ryan Whitwam
                                  </p>
                          </div>
          </div>
          <figcaption>
        <div>
    
    <p>
      Nvidia used to sell Shield-branded game controllers.

              <span>
          Credit:

          
          Ryan Whitwam

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>


<p>And premium it is, offering audio and video support far beyond what you find in other TV boxes, even years after release. The Shield TV started at $200 in 2015, and that’s <a href="https://marketplace.nvidia.com/en-us/consumer/streaming-media-devices/">still what you’ll pay for the Pro model</a> to this day. However, Bell notes that passion was the driving force behind bringing the Shield TV to market. The team didn’t know if it would make money, and indeed, the company lost money on every unit sold during the original production run. The 2017 and 2019 refreshes were about addressing that while also emphasizing the Shield’s streaming media chops.</p>
<h2>A passion for product support</h2>
<p>Update support for Internet-connected devices is vital—whether they’re phones, tablets, set-top boxes, or something else. When updates cease, gadgets fall out of sync with platform features, leading to new bugs (which will never be fixed) and security holes that can affect safety and functionality. The support guarantee attached to a device is basically its expiration date.</p>
<p>“We were all frustrated as buyers of phones and tablets that you buy a device, you get one or two updates, and that’s it!” said Bell. “Early on when we were building Shield TV, we decided we were going to make it for a long time. Jensen and I had a discussion, and it was, ‘How long do we want to support this thing?’ And Jensen said, ‘For as long as we shall live.’”</p>
<p>In 2025, Nvidia wrapped up its tenth year of supporting the Shield platform. Even those original 2015 boxes are still being maintained with bug fixes and the occasional new feature. They’ve gone all the way from Android 5.0 to Android 11 in that time. No Android device—not a single phone, tablet, watch, or streaming box—has gotten anywhere close to this level of support.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>The best example of Nvidia’s passion for support is, believe it or not, a two-year gap in updates.</p>
<p>Across the dozens of Shield TV updates, there have been a few times when fans feared Nvidia was done with the box. Most notably, there were no public updates for the Shield TV in 2023 or 2024, but over-the-air updates resumed in 2025.</p>
<p>“On the outside, it looked like we went quiet, but it’s actually one of our bigger development efforts,” explained Bell.</p>
<p>The origins of that effort, surprisingly, stretch back years to the launch of the Nintendo Switch. The Shield runs Nvidia’s custom Tegra X1 Arm chip, <a href="https://arstechnica.com/gaming/2016/12/nintendo-switch-nvidia-tegra-x1-specs-speed/">the same processor Nintendo chose</a> to power the original Switch in 2017. Soon after release, modders <a href="https://arstechnica.com/gaming/2018/04/the-unpatchable-exploit-that-makes-every-current-nintendo-switch-hackable/">discovered a chip flaw</a> that could bypass Nintendo’s security measures, enabling homebrew (and piracy). An updated Tegra X1 chip (also used in the 2019 Shield refresh) fixed that for Nintendo, but Nvidia’s 2015 and 2017 Shield boxes ran the same exploitable version.</p>
<p>Initially, Nvidia was able to roll out periodic patches to protect against the vulnerability, but by 2023, the Shield needed something more. Around that time, owners of 2015 and 2017 Shield boxes had noticed that DRM-protected 4K content often failed to play—that was thanks to the same bug that affected the Switch years earlier.</p>
<p>With a newer, non-vulnerable product on the market, many companies might have just accepted that the older product would lose functionality, but Nvidia’s passion for Shield remained. Bell consulted Huang, whom he calls Shield customer No. 1, about the meaning of his “as long as we shall live” pledge, and the team was approved to spend whatever time was needed to fix the vulnerability on the first two generations of Shield TV.</p>
<p>According to Bell, it took about 18 months to get there, requiring the creation of an entirely new security stack. He explains that Android updates aren’t actually that much work compared to DRM security, and some of its partners weren’t that keen on re-certifying older products. The Shield team fought for it because they felt, as they had throughout the product’s run, that they’d made a promise to customers who expected the box to have certain features.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>In February 2025, Nvidia released <a href="https://support-shield.nvidia.com/android-tv-release-notes/9.2/">Shield Patch 9.2</a>, the first wide release in two years. The changelog included an unassuming line reading, “Added security enhancement for 4K DRM playback.” That was the Tegra X1 bug finally being laid to rest on the 2015 and 2017 Shield boxes.</p>

<p>The refreshed Tegra X1+ in <a href="https://arstechnica.com/gadgets/2019/10/2019-nvidia-shield-tv-gets-a-compact-new-form-factor-new-remote/">the 2019 Shield TV</a> spared it from those DRM issues, and Nvidia still hasn’t stopped working on that chip. The Tegra X1 was blazing fast in 2015, and it’s still quite capable compared to your average smart TV today. The chip has actually outlasted several of the components needed to manufacture it. For example, when the Tegra chip’s memory was phased out, the team immediately began work on qualifying a new memory supplier. To this day, Nvidia is still iterating on the Tegra X1 platform, supporting the Shield’s continued updates.</p>
<p>“If operations calls me and says they just ran out of this component, I’ve got engineers on it tonight looking for a new component,” Bell said.</p>
<h2>The future of Shield</h2>
<p>Nvidia has put its money where its mouth is by supporting all versions of the Shield for so long. But it’s been over six years since we’ve seen new hardware. Surely the Shield has to be running out of steam, right?</p>
<p>Not so, says Bell. Nvidia still manufactures the 2019 Shield because people are still buying it. In fact, the sales volume has remained basically unchanged for the past 10 years. The Shield Pro is a spendy step-top box at $200, so Nvidia has experimented with pricing and promotion with little effect. The 2019 non-Pro Shield was one such effort. The base model was originally priced at $99, but the MSRP eventually landed at $150.</p>
<p>“No matter how much we dropped the price or how much we market or don’t market it, the same number of people come out of the woodwork every week to buy Shield,” Bell explained.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<figure>
    <div>
            <p><a data-pswp-width="1920" data-pswp-height="1080" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2.jpg 1920w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2-1536x864.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2-1440x810.jpg 1440w" data-cropped="false" href="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2.jpg" target="_blank">
              <img width="1920" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2.jpg" alt="Shield controller" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2.jpg 1920w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2-1536x864.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2026/01/Shield-10-yrs-2-1440x810.jpg 1440w" sizes="auto, (max-width: 1920px) 100vw, 1920px">
            </a></p><div id="caption-2138234"><p>
              Nvidia had no choice but to put that giant Netflix button on the remote.
                              </p><p>
                  Credit:
                                      Ryan Whitwam
                                  </p>
                          </div>
          </div>
          <figcaption>
        <div>
    
    <p>
      Nvidia had no choice but to put that giant Netflix button on the remote.

              <span>
          Credit:

          
          Ryan Whitwam

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>That kind of consistency isn’t lost on Nvidia. Bell says the company has no plans to stop production or updates for the Shield “any time soon.” It’s also still possible that Nvidia could release new Shield TV hardware in the future. Nvidia’s Shield devices came about as a result of engineers tinkering with new concepts in a lab setting, but most of those experiments never see the light of day. For example, Bell notes that the team produced several updated versions of the Shield Tablet and Shield Portable (some of which you can find floating around on eBay) that never got a retail release, and they continue to work on Shield TV.</p>
<p>“We’re always playing in the labs, trying to discover new things,” said Bell. “We’ve played with new concepts for Shield and we’ll continue to play, and if we find something we’re super-excited about, we’ll probably make a go of it.”</p>
<p>But what would that look like? Video technology has advanced since 2019, leaving the Shield unable to take full advantage of some newer formats. First up would be support for VP9 Profile 2 hardware decoding, which enables HDR video on YouTube. Bell says a refreshed Shield would also prioritize formats like AV1 and the HDR 10+ standard, as well as support for newer Dolby Vision profiles for people with backed-up media.</p>
<p>And then there’s the enormous, easy-to-press-by-accident Netflix button on the remote. While adding new video technologies would be job one, fixing the Netflix button is No. 2 for a theoretical new Shield. According to Bell, Nvidia doesn’t receive any money from Netflix for the giant button on its remote. It’s actually there as a requirement of Netflix’s certification program, which was “very strong” in 2019. In a refresh, he thinks Nvidia could get away with a smaller “N” button. We can only hope.</p>
<p>But does Bell think he’ll get a chance to build that new Shield TV, shrunken Netflix button and all? He stopped short of predicting the future, but there’s definitely interest.</p>
<p>“We talk about it all the time—I’d love to,” he said.</p>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/ryanwhitwam/"><img src="https://cdn.arstechnica.net/wp-content/uploads/2025/02/AV4.jpg" alt="Photo of Ryan Whitwam"></a></p>
  </div>

  <div>
    

    <p>
      Ryan Whitwam is a senior technology reporter at Ars Technica, covering the ways Google, AI, and mobile technology continue to change the world. Over his 20-year career, he's written for Android Police, ExtremeTech, Wirecutter, NY Times, and more. He has reviewed more phones than most people will ever own. You can <a href="https://bsky.app/profile/rwhitwam.bsky.social">follow him on Bluesky</a>, where you will see photos of his dozens of mechanical keyboards.
    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/gadgets/2026/01/inside-nvidias-10-year-effort-to-make-the-shield-tv-the-most-updated-android-device-ever/#comments" title="151 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    151 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/information-technology/2026/01/ai-agents-now-have-their-own-reddit-style-social-network-and-its-getting-weird-fast/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/moltbook-blue-v-red-768x432.jpg" alt="Listing image for first story in Most Read: AI agents now have their own Reddit-style social network, and it's getting weird fast" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US reportedly investigate claims that Meta can read encrypted WhatsApp messages (172 pts)]]></title>
            <link>https://www.theguardian.com/technology/2026/jan/31/us-authorities-reportedly-investigate-claims-that-meta-can-read-encrypted-whatsapp-messages</link>
            <guid>46836487</guid>
            <pubDate>Sat, 31 Jan 2026 13:27:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/technology/2026/jan/31/us-authorities-reportedly-investigate-claims-that-meta-can-read-encrypted-whatsapp-messages">https://www.theguardian.com/technology/2026/jan/31/us-authorities-reportedly-investigate-claims-that-meta-can-read-encrypted-whatsapp-messages</a>, See on <a href="https://news.ycombinator.com/item?id=46836487">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>US authorities have reportedly investigated claims that Meta can read users’ encrypted chats on the <a href="https://www.theguardian.com/technology/whatsapp" data-link-name="in body link" data-component="auto-linked-tag">WhatsApp</a> messaging platform, which it owns.</p><p>The reports follow a lawsuit filed last week, which claimed <a href="https://www.theguardian.com/technology/meta" data-link-name="in body link" data-component="auto-linked-tag">Meta</a> “can access virtually all of WhatsApp users’ purportedly ‘private’ communications”.</p><p>Meta has denied the allegation, reported by <a href="https://www.bloomberg.com/news/articles/2026-01-29/us-has-investigated-claims-that-whatsapp-chats-aren-t-private" data-link-name="in body link">Bloomberg</a>, calling the lawsuit’s claim “categorically false and absurd”. It suggested the claim was a <a href="https://x.com/andymstone/status/2016920479362171305" data-link-name="in body link">tactic</a> to support the NSO Group, an Israeli firm that develops spyware used against activists and journalists, and which recently lost a lawsuit brought by WhatsApp.</p><p>The firm that filed last week’s lawsuit against Meta, Quinn Emanuel Urquhart &amp; Sullivan, attributes the allegation to <a href="https://www.washingtonpost.com/technology/2026/01/29/whatsapp-lawsuit-read-messages-denied/" data-link-name="in body link">unnamed</a> “courageous” whistleblowers from Australia, Brazil, India, Mexico and South Africa.</p><p>Quinn Emanuel is, in a separate case, helping to represent the NSO Group in its appeal against a <a href="https://www.davispolk.com/experience/trial-victory-meta-and-whatsapp-spyware-case" data-link-name="in body link">judgment</a> from a US federal court last year, which ordered it to pay $167m to WhatsApp for violating its terms of service in its <a href="https://www.theguardian.com/news/series/pegasus-project" data-link-name="in body link">deployment</a> of Pegasus spyware against more than 1,400 users.</p><p>“We’re pursuing sanctions against Quinn Emanuel for filing a meritless lawsuit that was designed purely to grab headlines,” said Carl Woog, a Meta spokesperson, in a statement. “This is the same firm that is trying to help NSO overturn an injunction that barred their operations for targeting journalists and government officials with spyware.”</p><p>Adam Wolfson, a partner at Quinn Emanuel said: “Our colleagues’ defence of NSO on appeal has nothing to do with the facts disclosed to us and which form the basis of the lawsuit we brought for worldwide WhatsApp users.</p><p>“We look forward to moving forward with those claims and note WhatsApp’s denials have all been carefully worded in a way that stops short of denying the central allegation in the complaint – that Meta has the ability to read WhatsApp messages, regardless of its claims about end-to-end encryption.”</p><p>Steven Murdoch, professor of security engineering at UCL, said the lawsuit was “a bit strange”. “It seems to be going mostly on whistleblowers, and we don’t know much about them or their credibility,” he said. “I would be very surprised if what they are claiming is actually true.”</p><p>If WhatsApp were, indeed, reading users’ messages, this was likely to have been discovered by staff and would end the business, he said. “It’s very hard to keep secrets inside a company. If there was something as scandalous as this going on, I think it’s very likely that it would have leaked out from someone within WhatsApp.”</p><p>The Bloomberg article cites reports and interviews from officials within the US Department of Commerce in claiming that the US has investigated whether Meta could read WhatsApp messages. However, a spokesperson for the department called these assertions “unsubstantiated”.</p><p>WhatsApp <a href="https://faq.whatsapp.com/820124435853543" data-link-name="in body link">bills itself</a> as an end-to-end encrypted platform, which means that messages can be read only by their sender and recipient, and are not decoded by a server in the middle.</p><p>This contrasts with some other messaging apps, such as Telegram, which encrypt messages between a sender and its own servers, preventing third parties from reading the messages, but allowing them – in theory – to be decoded and read by Telegram itself.</p><p>A senior executive in the technology sector told the Guardian that WhatsApp’s vaunted privacy “leaves much to be desired”, given the platform’s willingness to collect metadata on its users, such as their profile information, their contact lists, and who they speak to and when.</p><p>However, the “idea that WhatsApp can selectively and retroactively access the content of [end-to-end encrypted] individual chats is a mathematical impossibility”, he said.</p><p>Woog, of Meta, said: “We’re pursuing sanctions against Quinn Emanuel for filing a meritless lawsuit that was designed purely to grab headlines. WhatsApp’s encryption remains secure and we’ll continue to stand up against those trying to deny people’s right to private communication.”</p></div></div>]]></description>
        </item>
    </channel>
</rss>