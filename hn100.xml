<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 28 Jul 2024 02:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: Semantic Grep – A Word2Vec-powered search tool (160 pts)]]></title>
            <link>https://github.com/arunsupe/semantic-grep</link>
            <guid>41088273</guid>
            <pubDate>Sat, 27 Jul 2024 18:02:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/arunsupe/semantic-grep">https://github.com/arunsupe/semantic-grep</a>, See on <a href="https://news.ycombinator.com/item?id=41088273">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">sgrep - Semantic Grep</h2><a id="user-content-sgrep---semantic-grep" aria-label="Permalink: sgrep - Semantic Grep" href="#sgrep---semantic-grep"></a></p>
<p dir="auto">sgrep is a command-line tool that performs semantic searches on text input using word embeddings. It's designed to find semantically similar matches to the query, going beyond simple string matching. The experience is designed to be similar to grep.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Example Usage</h2><a id="user-content-example-usage" aria-label="Permalink: Example Usage" href="#example-usage"></a></p>
<p dir="auto">Search for words similar to "death" in Hemingway's "The Old Man and the Sea" with context and line numbers:</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -s 'https://gutenberg.ca/ebooks/hemingwaye-oldmanandthesea/hemingwaye-oldmanandthesea-00-t.txt' \
    | sgrep -C 2 -n -threshold 0.55 death"><pre>curl -s <span><span>'</span>https://gutenberg.ca/ebooks/hemingwaye-oldmanandthesea/hemingwaye-oldmanandthesea-00-t.txt<span>'</span></span> \
    <span>|</span> sgrep -C 2 -n -threshold 0.55 death</pre></div>
<p dir="auto">Output:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/arunsupe/semantic-grep/blob/main/demo/image.png"><img src="https://github.com/arunsupe/semantic-grep/raw/main/demo/image.png" alt="alt text"></a></p>
<p dir="auto">This command:</p>
<div data-snippet-clipboard-copy-content="- Fetches the text of &quot;The Old Man and the Sea&quot; from Project Gutenberg Canada
- Pipes the text to sgrep
- Searches for words semantically similar to &quot;death&quot;
- Uses a similarity threshold of 0.55 (-threshold 0.55)
- Displays 2 lines of context before and after each match (-C 2)
- Shows line numbers (-n)"><pre><code>- Fetches the text of "The Old Man and the Sea" from Project Gutenberg Canada
- Pipes the text to sgrep
- Searches for words semantically similar to "death"
- Uses a similarity threshold of 0.55 (-threshold 0.55)
- Displays 2 lines of context before and after each match (-C 2)
- Shows line numbers (-n)
</code></pre></div>
<p dir="auto">The output will show matches with their similarity scores, highlighted words, context, and line numbers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Semantic search using Word2Vec embeddings</li>
<li>Configurable similarity threshold</li>
<li>Context display (before and after matching lines)</li>
<li>Color-coded output for matched words and line numbers</li>
<li>Support for reading from files or standard input</li>
<li>Configurable via JSON file and command-line arguments</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><strong>Binary</strong>:</p>
<ol dir="auto">
<li>Download the latest binary release</li>
<li>Download a word2vec model (see below)</li>
<li>Optionally, download the config.json to configure model location there (or do this from the command line)</li>
</ol>
<p dir="auto"><strong>From source (linux/osx)</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# clone
git clone https://github.com/arunsupe/semantic-grep.git
cd semantic-grep

# build
go build -o sgrep

# download a word2vec model using this helper script (see &quot;Word2Vec Model&quot; below)
bash download-model.sh"><pre><span><span>#</span> clone</span>
git clone https://github.com/arunsupe/semantic-grep.git
<span>cd</span> semantic-grep

<span><span>#</span> build</span>
go build -o sgrep

<span><span>#</span> download a word2vec model using this helper script (see "Word2Vec Model" below)</span>
bash download-model.sh</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Basic usage:</p>
<p dir="auto">./sgrep [options]  [file]</p>
<p dir="auto">If no file is specified, sgrep reads from standard input.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Command-line Options</h3><a id="user-content-command-line-options" aria-label="Permalink: Command-line Options" href="#command-line-options"></a></p>
<ul dir="auto">
<li><code>-model_path</code>: Path to the Word2Vec model file (overrides config file)</li>
<li><code>-threshold</code>: Similarity threshold for matching (default: 0.7)</li>
<li><code>-A</code>: Number of lines to display after a match</li>
<li><code>-B</code>: Number of lines to display before a match</li>
<li><code>-C</code>: Number of lines to display before and after a match</li>
<li><code>-n</code>: Print line numbers</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<ul dir="auto">
<li>sgrep can be configured using a JSON file. By default, it looks for <code>config.json</code> in the current directory.</li>
</ul>
<p dir="auto">Example <code>config.json</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    &quot;model_path&quot;: &quot;path/to/your/word2vec/model.bin&quot;
}"><pre>{
    <span>"model_path"</span>: <span><span>"</span>path/to/your/word2vec/model.bin<span>"</span></span>
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Word2Vec Model</h2><a id="user-content-word2vec-model" aria-label="Permalink: Word2Vec Model" href="#word2vec-model"></a></p>
<ul dir="auto">
<li>sgrep requires a Word2Vec model in binary format. You can use pre-trained models like Google's Word2Vec model or train your own using tools like gensim.</li>
<li>Download and unzip the .bin file locally and update the config.json.
<ul dir="auto">
<li>Google's Word2Vec: from <a href="https://github.com/mmihaltz/word2vec-GoogleNews-vectors">https://github.com/mmihaltz/word2vec-GoogleNews-vectors</a></li>
<li>A slim version: GoogleNews-vectors-negative300-SLIM.bin.gz model from <a href="https://github.com/eyaler/word2vec-slim/">https://github.com/eyaler/word2vec-slim/</a> (thanks to eyaler)</li>
</ul>
</li>
<li>download-model.sh is a simple helper script to download the small word2vec model hosted by eyaler and save it in models/googlenews-slim/ directory</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Please feel free to submit a Pull Request.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License - see the LICENSE file for details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Aphex Twin gave us a peek inside a 90s classic (2017) (135 pts)]]></title>
            <link>https://cdm.link/2017/07/aphex-twin-gave-us-peek-inside-90s-classic-heres-learned/</link>
            <guid>41088224</guid>
            <pubDate>Sat, 27 Jul 2024 17:54:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cdm.link/2017/07/aphex-twin-gave-us-peek-inside-90s-classic-heres-learned/">https://cdm.link/2017/07/aphex-twin-gave-us-peek-inside-90s-classic-heres-learned/</a>, See on <a href="https://news.ycombinator.com/item?id=41088224">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    	  <p>Aphex Twin’s “Vordhosbn” just got a surprising video reveal, showing how the track was made. So let’s revisit trackers and 90s underground music culture.<span id="more-80168"></span></p>
<p>You’re probably familiar with the term “white label,” but where did that term originate?”? Back in the early days of DJing, DJs were very territorial about their crate digging. Sometimes, in order to avoid rival DJs looking at their decks to ID their selections (this is way before the days of Shazam, remember), DJs would rip off the labels of a particularly rare record, leaving the white label residue with no identifying information.</p>
<p>Similarly, the 90s were an interesting time for music production. With the advent of computer sequencers, music became more complex – and in the wild west days before YouTube tutorials, concert phone vids, and everyone using Ableton Live, there was legitimate mystery behind how some of the most complex electronic music was made. Max? SuperCollider? Some homebrew software unavailable to the plebs? </p>
<p>If mystery in electronic music production was a game in the 90s, then Richard D. James was its undisputed winner. As Aphex Twin and a host of other pseudonyms, he created mind-bending sequences. As an interview subject, he was equal parts prankster and cagey. Sure, there was an idea of what the IDM greats were up to – Autechre and Plaid used Max, Squarepusher used Reaktor, Aphex used…something? The mystery has always been part of James’ appeal – here is a man who has claimed to sleep only four hours a night, or to have built or heavily modified all of his hardware, or to be sitting on hundreds if not thousands of unreleased tracks, among other tall tales.</p>
<p>Around 2014, something flipped with Richard D. James. After releasing Syro, his first album in 13 years as Aphex Twin, he unleashed the floodgates with a massive hard drive dump onto SoundCloud – seems he wasn’t lying about all those tracks after all. Following up with this, today you can see the <a href="https://aphextwin.warp.net/">debut of a custom Bleep store for Aphex Twin</a>, including loads of unreleased bonus tracks to go with his albums.</p>
<p>Of most interest to the nerds, however, has got to be this seemingly innocuous video, in which we get a trollingly-effected screencast video of Drukqs track “Vordhosbn”, playing out in the vintage tracker PlayerPro. James had previously identified PlayerPro as his main environment for making Drukqs – now we have video of it in action:</p>
<p><iframe src="https://player.vimeo.com/video/223378825" width="500" height="313" frameborder="0" title="pp" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe></p>
<p>So, there we have it. A classic Aphex Twin track with the curtain drawn up. What can we learn from this video? A few things:</p>
<ul>
<li>PlayerPro’s tracks were all monophonic, so the chords in “Vordhosbn” had to be made using multiple tracks</li>
<li>As expected with a tracker, it’s largely built from samples – likely from James’ substantial hardware collection</li>
<li>Hey, those oscilloscopes and spectral displays are fun</li>
</ul>
<p>Perhaps what’s best about this video is that it shows an Aphex classic for what it is – a track, composed in much the same way as any other electronic musician might do it. It doesn’t detract from the special qualities of Aphex’s music, but it does show us what was really going on behind all the mystery – music-making.</p>
<h3>Keep Track of It</h3>
<p>It’s worth spending a moment to celebrate trackers. Long before the days of piano rolls, trackers were the best way to make intricate sequences using a computer. YouTube is riddled with classic jungle tracks from the mid-90s using software like <a href="https://en.wikipedia.org/wiki/OctaMED">OctaMed</a>:</p>
<p><iframe loading="lazy" width="500" height="375" src="https://www.youtube.com/embed/gSj92M0N65M?feature=oembed" frameborder="0" allowfullscreen=""></iframe></p>
<p>For a dedicated community, trackers are still the way to go. And there’s no better tracker around now than Renoise – whose developers have done a fantastic job bringing the tracker workflow into the 21st century. Check out this video of Venetian Snares’ “Vache” done in Renoise:</p>
<p><iframe loading="lazy" width="500" height="375" src="https://www.youtube.com/embed/zGK-EzEa45U?feature=oembed" frameborder="0" allowfullscreen=""></iframe></p>
<p>Like most trackers, Renoise has something of a steep learning curve to get all the key commands right; once you’re there, however, you’ll find it to be a very nimble environment for wild micro-edits and crazy sequences. There’s definitely a reason why it remains a tool of choice for breakcore producers!</p>
<p><iframe loading="lazy" width="500" height="281" src="https://www.youtube.com/embed/I1M9kzfieK4?feature=oembed" frameborder="0" allowfullscreen=""></iframe></p>
<p>Do you use a tracker? What do you think of the workflow? What’s the best way for someone to get started with a tracker? Let us know in the comments!</p>
<p><em>Ed.: PlayerPro is available as free software for Mac, Windows, Linux … and yes, even FreeBSD.</em></p>
<p><strong><a href="https://sourceforge.net/projects/playerpro/">https://sourceforge.net/projects/playerpro/</a></strong></p>
<p><em>Returning CDM contributor David Abravanel is a marketer, musician, and technologist living in New York. He loves that shiny digital crunch. Follow him at <a href="http://dhla.me/">http://dhla.me</a></em></p>

        
    	  
    	  <div><p>Tags: <a href="https://cdm.link/tag/90s/" rel="tag">90s</a>, <a href="https://cdm.link/tag/aphex-twin/" rel="tag">aphex-twin</a>, <a href="https://cdm.link/tag/artists/" rel="tag">artists</a>, <a href="https://cdm.link/tag/awesomeness/" rel="tag">awesomeness</a>, <a href="https://cdm.link/tag/free/" rel="tag">free</a>, <a href="https://cdm.link/tag/free-as-in-freedom/" rel="tag">free as in freedom</a>, <a href="https://cdm.link/tag/history/" rel="tag">history</a>, <a href="https://cdm.link/tag/i-love-the-90s/" rel="tag">i-love-the-90s</a>, <a href="https://cdm.link/tag/linux/" rel="tag">Linux</a>, <a href="https://cdm.link/tag/mac/" rel="tag">Mac</a>, <a href="https://cdm.link/tag/open-source/" rel="tag">open-source</a>, <a href="https://cdm.link/tag/renoise/" rel="tag">Renoise</a>, <a href="https://cdm.link/tag/richard-d-james/" rel="tag">Richard D. James</a>, <a href="https://cdm.link/tag/software/" rel="tag">Software</a>, <a href="https://cdm.link/tag/trackers/" rel="tag">trackers</a>, <a href="https://cdm.link/tag/unix/" rel="tag">unix</a>, <a href="https://cdm.link/tag/venetian-snares/" rel="tag">Venetian Snares</a>, <a href="https://cdm.link/tag/warp-records/" rel="tag">warp-records</a>, <a href="https://cdm.link/tag/windows/" rel="tag">Windows</a></p></div>
    	  
    		      		
    						
				
				
				        
                          
                  	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An experiment in UI density created with Svelte (468 pts)]]></title>
            <link>https://cybernetic.dev/grid</link>
            <guid>41088013</guid>
            <pubDate>Sat, 27 Jul 2024 17:23:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cybernetic.dev/grid">https://cybernetic.dev/grid</a>, See on <a href="https://news.ycombinator.com/item?id=41088013">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><!-- HTML_TAG_START -->Fits as much data on a screen as possible, while allowing in-depth control of data density to ensure legibility. Pagination is used only when absolutely necessary.<!-- HTML_TAG_END --></p> <!-- HTML_TAG_START --><p>Changing the density-relevant settings <code>Decimals</code>, <code>Group Digits</code>, <code>Min Font Size</code>, <code>Min x-Padding</code>, <code>Min y-Padding</code> does not always lead to a change in UI density, since it is determined by all parameters jointly or, as in the case of <code>Min Font Size</code>, <code>Min x-Padding</code>, <code>Min y-Padding</code>, is already realized since these are minimum values.</p><!-- HTML_TAG_END --></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Göttingen was one of the most productive centers of mathematics (132 pts)]]></title>
            <link>https://theconversation.com/how-one-german-city-developed-and-then-lost-generations-of-math-geniuses-106750</link>
            <guid>41087429</guid>
            <pubDate>Sat, 27 Jul 2024 16:08:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theconversation.com/how-one-german-city-developed-and-then-lost-generations-of-math-geniuses-106750">https://theconversation.com/how-one-german-city-developed-and-then-lost-generations-of-math-geniuses-106750</a>, See on <a href="https://news.ycombinator.com/item?id=41087429">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>There are two things that connect the names Gauss, Riemann, Hilbert and Noether. One is their outstanding breadth of contributions to the field of mathematics. The other is that each was a professor at the same university in Göttingen, Germany. </p>

<p>Although relatively unknown today, Göttingen, a small German university town, was for a time one of the <a href="https://www.britannica.com/topic/University-of-Gottingen">most productive centers of mathematics</a> in history. </p>

<p>Göttingen’s rise to mathematical primacy occurred over generations, but its fall took less than a decade when its stars were pushed abroad by the advent of National Socialism, the ideology of the Nazi Party. The university’s best minds <a href="https://physicstoday.scitation.org/do/10.1063/PT.6.4.20180926a/full/">left Germany in the early 1930s</a>, transferring its substantial mathematical legacy to Princeton, New York University, and other British and American universities. By 1943, <a href="https://assets.rockefellerfoundation.org/app/uploads/20150530122143/Annual-Report-1942.pdf">16 former Göttingen</a> faculty members were in the U.S.</p>

<p>The story of the rise and fall of mathematics in Göttingen has largely been forgotten, but names associated with the place still appear frequently in the world of mathematics. Its legacy survives today in other mathematical research powerhouses around the world.</p>

<h2>Founding of the university</h2>

<p>In 1734, King George II, who ruled both the United Kingdom and a large area of land in Northern Europe, founded a <a href="https://www.uni-goettingen.de/en/history-of-the-university-%E2%80%93-an-overview/90607.html">university in Göttingen</a>, Germany. </p>

<p>The Enlightenment was by this point in full swing in northern Germany. For example, <a href="https://www.storyofmathematics.com/17th_leibniz.html">mathematician Gottfried Leibniz</a> developed calculus less than 100 miles north of the new university, just 50 years before its founding. </p>



<p>Finding themselves in the midst of the Enlightenment, scientific researchers at the new University of Göttingen had more academic freedom than <a href="https://brill.com/view/title/15703">generations past</a>. <a href="http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780198206606.001.0001/acprof-9780198206606-chapter-2">They were promised</a> intellectual autonomy and freedom from close religious supervision. Instead, they were recruited solely to advance knowledge and carry out original research. The education of students was also more <a href="https://books.google.com/books/about/Challenges_to_the_Enlightenment.html?id=k7baAAAAMAAJ">egalitarian</a> than it had been previously in Europe, as both rich and poor were admitted and trained.</p>

<h2>Great mathematicians</h2>

<figure>
            <a href="https://images.theconversation.com/files/249479/original/file-20181207-128217-10pk4ht.jpeg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="" data-src="https://images.theconversation.com/files/249479/original/file-20181207-128217-10pk4ht.jpeg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip" data-srcset="https://images.theconversation.com/files/249479/original/file-20181207-128217-10pk4ht.jpeg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=800&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/249479/original/file-20181207-128217-10pk4ht.jpeg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=800&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/249479/original/file-20181207-128217-10pk4ht.jpeg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=800&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/249479/original/file-20181207-128217-10pk4ht.jpeg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=1005&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/249479/original/file-20181207-128217-10pk4ht.jpeg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=1005&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/249479/original/file-20181207-128217-10pk4ht.jpeg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=1005&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/249479/original/file-20181207-128217-10pk4ht.jpeg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip"></p></a>
            <figcaption>
              <span>A monument in Göttingen depicting Gauss with physicist Wilhelm Weber.</span>
              <span><a href="https://commons.wikimedia.org/wiki/File:Goe.Gauss.Weber.Denkmal.Wall.B%C3%BCrgerstr.detail.03.Seite.JPG#/media/File:Goe.Gauss.Weber.Denkmal.Wall.B%C3%BCrgerstr.detail.03.Seite.JPG">Longbow4u/Wikimedia</a></span>
            </figcaption>
          </figure>

<p>By the late 18th century, the university in Göttingen was a well-known center of scientific learning in Germany. Its enduring mathematical prowess, however, originated in <a href="http://www.math.wichita.edu/history/men/gauss.html">Carl Friedrich Gauss</a>. Often referred to as the prince of mathematics, his research at Göttingen between 1795 and 1855 spanned from algebra to magnetism to astronomy.</p>

<p>Gauss’s discoveries were groundbreaking, but the reputation that he started in Göttingen only grew as <a href="https://www.uni-math.gwdg.de/en/burmann.xhtml">mathematicians from across Europe</a> flocked to the town. Bernhard Riemann, the head of mathematics at Göttingen from 1859 to 1866, invented Riemannian geometry, which paved the way for Einstein’s future work on relativity. Felix Klein, the chair of mathematics from 1886 to 1913, was the first to describe the <a href="https://en.wikipedia.org/wiki/Klein_bottle">Klein bottle</a>, a 3-dimensional object with just one side, similar to the <a href="https://theconversation.com/the-weird-world-of-one-sided-objects-101936">Mobius strip</a>. </p>

<figure>
            <p><img alt="" data-src="https://images.theconversation.com/files/249410/original/file-20181207-128193-7pa65p.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip" data-srcset="https://images.theconversation.com/files/249410/original/file-20181207-128193-7pa65p.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=1152&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/249410/original/file-20181207-128193-7pa65p.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=1152&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/249410/original/file-20181207-128193-7pa65p.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=1152&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/249410/original/file-20181207-128193-7pa65p.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=1447&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/249410/original/file-20181207-128193-7pa65p.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=1447&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/249410/original/file-20181207-128193-7pa65p.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=1447&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/249410/original/file-20181207-128193-7pa65p.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip"></p>
            <figcaption>
              <span>A Klein bottle is a two-dimensional surface – its inside is its outside.</span>
              <span><a href="https://commons.wikimedia.org/wiki/File:Klein_bottle.svg">Tttrung/Wikimedia</a>, <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a></span>
            </figcaption>
          </figure>

<p>Klein was instrumental in hiring the next generation of mathematicians to Göttingen. This generation included <a href="http://www-groups.dcs.st-and.ac.uk/history/Biographies/Runge.html">Carl Runge</a>, who helped invent a key part of today’s most accurate weather prediction software, the <a href="https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods">Runge-Kutta time stepper</a>; Hermann <a href="http://www-history.mcs.st-andrews.ac.uk/Biographies/Minkowski.html">Minkowski</a>, who is perhaps best known for his work on relativity; and David Hilbert. </p>

<p>Hilbert’s <a href="https://www.cmi.ac.in/%7Esmahanta/hilbert.html">famous 23 problems</a>, presented at the International Congress of Mathematicians in 1900, guided mathematical research for the entire 20th century. During his career as professor and head of the math department at Göttingen, he mentored an astounding<a href="https://genealogy.math.ndsu.nodak.edu/id.php?id=7298"> 76 Ph.D. students</a>, many of whom went on to make seminal discoveries of their own. </p>

<h2>The exodus</h2>

<p>After Gauss’s appointment at the university until the early 1930s, Göttingen’s mathematical prowess survived in an environment of constant political turmoil, including the Napoleonic Wars, the Franco-Prussian War and World War I.</p>

<p>But the wave of nationalism that accompanied the Nazis’ rise to power in the early 1930s transformed Göttingen. The 1933 <a href="https://en.wikipedia.org/wiki/Law_for_the_Restoration_of_the_Professional_Civil_Service">Law for the Restoration of the Professional Civil Service</a> made it illegal for any non-Aryan, specifically Jews, to serve as a professor or teacher in Germany. In response to this and other <a href="https://encyclopedia.ushmm.org/content/en/article/antisemitic-legislation-1933-1939">anti-Semitic legislation</a>, Jewish scholars, professors with Jewish connections and anyone who opposed Nazism fled Germany. </p>

<figure>
            <p><img alt="" data-src="https://images.theconversation.com/files/249478/original/file-20181207-128196-1py9aoe.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip" data-srcset="https://images.theconversation.com/files/249478/original/file-20181207-128196-1py9aoe.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=915&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/249478/original/file-20181207-128196-1py9aoe.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=915&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/249478/original/file-20181207-128196-1py9aoe.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=915&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/249478/original/file-20181207-128196-1py9aoe.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=1149&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/249478/original/file-20181207-128196-1py9aoe.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=1149&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/249478/original/file-20181207-128196-1py9aoe.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=1149&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/249478/original/file-20181207-128196-1py9aoe.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip"></p>
            <figcaption>
              <span>Emmy Noether.</span>
              <span><a href="https://commons.wikimedia.org/wiki/File:Noether.jpg">Wikimedia</a></span>
            </figcaption>
          </figure>

<p><a href="https://www.sdsc.edu/ScienceWomen/noether.html">Emmy Noether</a>, who had been the first female professor of mathematics at Göttingen and was described by Einstein as <a href="https://www.nytimes.com/1935/05/04/archives/the-late-emmy-noether-professor-einstein-writes-in-appreciation-of.html">the most important woman in the history of mathematics</a>, left in 1933 to teach at Bryn Mawr College. <a href="https://www.britannica.com/biography/Richard-Courant">Richard Courant</a> left in 1933 to help found the top U.S. applied mathematics institute at New York University. <a href="https://www.ias.edu/hermann-weyl-life">Hermann Weyl</a>, who had been appointed Hilbert’s successor as chair of mathematics in Göttingen, moved to Princeton, where he helped to transform the Institute for Advanced Studies into a research powerhouse.</p>

<p>Hilbert was asked in 1934 by the minister of science under the Nazi regime whether mathematics in Göttingen had suffered from the departure of the Jews and friends of the Jews. <a href="https://www.atomicheritage.org/history/scientific-exodus">He replied</a>: “Suffered? It hasn’t suffered, Mr. Minister. It doesn’t exist anymore!” Hilbert was right. Only one of the pre-Nazi full professors stayed past 1934. </p>

<p>The center of mathematics shifted quickly during the Nazi era and in the wake of World War II. Courant, Weyl and others helped move it to the U.K. and the U.S., where most of the <a href="https://www.usnews.com/education/best-global-universities/mathematics">top-ranked mathematics programs</a> are located today. </p>

<p>These countries’ mathematical heritage is in Göttingen. Its story is their story.</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Oscar Zariski  was one of the founders of modern algebraic geometry (158 pts)]]></title>
            <link>https://boogiemath.org/meta/meta-9.html</link>
            <guid>41086060</guid>
            <pubDate>Sat, 27 Jul 2024 12:12:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://boogiemath.org/meta/meta-9.html">https://boogiemath.org/meta/meta-9.html</a>, See on <a href="https://news.ycombinator.com/item?id=41086060">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <center><img src="https://boogiemath.org/assets/img/zariski.283fce0c.png" width="80%"></center> <p>I am always excited when I stumble upon a biography of a mathematician I wasn’t previously aware of. It’s strange (or perhaps not, given we’re talking about mathematics) how many brilliant minds in the field lack the biographical treatment they deserve. Consider just a few luminaries from the first half of the 20th century:
<a href="https://en.wikipedia.org/wiki/Andr%C3%A9_Weil" target="_blank" rel="noopener noreferrer">André Weil<span> <span>(opens new window)</span></span></a>,
<a href="https://en.wikipedia.org/wiki/Hermann_Weyl" target="_blank" rel="noopener noreferrer">Hermann Weyl<span> <span>(opens new window)</span></span></a>,
<a href="https://en.wikipedia.org/wiki/Carl_Ludwig_Siegel" target="_blank" rel="noopener noreferrer">Carl Ludwig Siegel<span> <span>(opens new window)</span></span></a>,
<a href="https://en.wikipedia.org/wiki/Emil_Artin" target="_blank" rel="noopener noreferrer">Emil Artin<span> <span>(opens new window)</span></span></a>,
<a href="https://en.wikipedia.org/wiki/Edmund_Landau" target="_blank" rel="noopener noreferrer">Edmund Landau<span> <span>(opens new window)</span></span></a>,
<a href="https://en.wikipedia.org/wiki/Helmut_Hasse" target="_blank" rel="noopener noreferrer">Helmut Hasse<span> <span>(opens new window)</span></span></a>.</p> <p>For physicists, it’s a bit different. I think they are much better covered.
The reason might be that their work is more directly connected to everyday life. Just think of nuclear energy
or the theory of relativity, which is crucial for
<a href="https://en.wikipedia.org/wiki/Global_Positioning_System" target="_blank" rel="noopener noreferrer">GPS<span> <span>(opens new window)</span></span></a> to work properly.</p> <p>So, recently I came across the
<a href="https://amzn.to/3HX6kUq" target="_blank" rel="noopener noreferrer">biography of Oscar Zariski<span> <span>(opens new window)</span></span></a>.
Zariski (1899-1986) was one of the founders of modern algebraic geometry.</p> <p>My wish is that one day I will be able to read Zariski’s book Algebraic Surfaces from 1935, as well as the 1971 version, which includes notes from his students—remarkable mathematicians like
<a href="https://en.wikipedia.org/wiki/Robin_Hartshorne" target="_blank" rel="noopener noreferrer">Robin Hartshorne<span> <span>(opens new window)</span></span></a>
and <a href="https://en.wikipedia.org/wiki/David_Mumford" target="_blank" rel="noopener noreferrer">David Mumford<span> <span>(opens new window)</span></span></a>.
Zariski mentioned that with this book, he began doing real mathematics, real algebraic geometry. In it, he reconstructed the algebraic geometry developed by the Italian school using the modern algebra introduced by
<a href="https://en.wikipedia.org/wiki/Emmy_Noether" target="_blank" rel="noopener noreferrer">Emmy Noether<span> <span>(opens new window)</span></span></a> and
<a href="https://en.wikipedia.org/wiki/Wolfgang_Krull" target="_blank" rel="noopener noreferrer">Wolfgang Krull<span> <span>(opens new window)</span></span></a>.</p> <p>But what about his life?
Let’s consider the following paragraph from the book:
“I spent hours and hours doing math problems without a teacher forcing me. Whole books of algebra
problems – I did them one after another. I was only seven or eight, but I always wanted mathematics.”</p> <p>That might be something you’d find in many biographies of world-renowned mathematicians, right? However, Carol Parikh notes: “Although Zariski was by all accounts an exceptionally quick and eager math student, the full extent of his gifts became apparent relatively late in life. He was almost twenty-five before he published his first paper and almost fifty when he did his great work on holomorphic functions...”</p> <p>In fact, Zariski remained active and productive even into his eighties.</p> <p>But how did Zariski’s story begin? It seems his older brother Moses played a significant role in shaping young Oscar. He taught him elementary algebra, and even when Oscar began surpassing him, he never resented it. While Oscar was in gymnasium, Moses would buy math and philosophy books for him during his business trips to Moscow and Petrograd. Oscar was especially interested in Hegel and Marx.</p> <p>I think it’s difficult for us today to fully grasp the hope that the Russian Revolution brought to the working people. There’s the following paragraph in the biography that illustrates this well: “He crossed the border into Italy in the late winter of 1921, having decided to enroll at the University of Pisa. On the platform at Udine, where he had stopped to change trains, he was recognized as Russian. Before he could even step into the waiting room, he found himself surrounded by a crowd of railway workers eager to hear about the revolution.”</p> <p>Of course, Italy was a turbulent country at that time, and Parikh continues: “Had there been a Fascist among the workers, the warmth of his welcome might have been quite otherwise...”</p> <p>But back to mathematics.
I was quite surprised when I read the following paragraph: “In the fall of 1921 the University of Rome was the most important center
of algebraic geometry in the world. What is now known as 'the Italian School' had been started by
<a href="https://en.wikipedia.org/wiki/Luigi_Cremona" target="_blank" rel="noopener noreferrer">Luigi Cremona<span> <span>(opens new window)</span></span></a>...
It was only after 1900, however, as a result of the combined efforts of three great
Italian mathematicians –
<a href="https://en.wikipedia.org/wiki/Guido_Castelnuovo" target="_blank" rel="noopener noreferrer">Guido Castelnuovo<span> <span>(opens new window)</span></span></a>,
<a href="https://en.wikipedia.org/wiki/Federigo_Enriques" target="_blank" rel="noopener noreferrer">Federigo Enriques<span> <span>(opens new window)</span></span></a>,
and <a href="https://en.wikipedia.org/wiki/Francesco_Severi" target="_blank" rel="noopener noreferrer">Francesco Severi<span> <span>(opens new window)</span></span></a>
– that the Italians
had carried algebraic geometry off in a startling new direction.”</p> <p>In many history books on mathematics, you read so much about Göttingen mathematicians that you might easily get the impression that most of the important work was done there. Of course, that was not the case. I was surprised to learn that algebraic geometry actually began in Rome.</p> <p>However, Zariski later regretted not engaging with Göttingen mathematics earlier: “It was a pity that my Italian teachers never told me there was such a tremendous development of algebra connected with algebraic geometry.”</p> <p>The leader of the algebra revolution in Göttingen was, of course, Emmy Noether. Emmy Noether is one of my favorite mathematicians, and I’m always happy when I discover something new about her. Her lectures were legendary, but there are two anecdotes from Zariski’s biography that I hadn’t heard before: “Once, for example, when she was lecturing, her slip came down. She bent down, pulled off the slip, threw it into the corridor, and kept on lecturing.” And another: “Noether would be so eager to get her thoughts down that she would write across a wet blackboard, leaving her students to wait patiently for it to dry so they could read it.”</p> <p>There was at least some interaction with Göttingen, though. It is mentioned that Edmund Landau visited Rome at some point, and when he heard that the young Zariski liked to play chess, he invited him to a game. “But how can we play with all these people around?” asked Zariski, as they were in the middle of a party. “Easily,” Landau replied. “Blank. You know, without a board.”</p> <p>Zariski and other mathematicians and physicists regularly gathered at Caffè Greco to gossip and play chess. It’s interesting how important cafés were at that time. A recurring theme in the biographies of European mathematicians from the first half of the 20th century is their role in cafés. Just think of the
<a href="https://en.wikipedia.org/wiki/Vienna_Circle" target="_blank" rel="noopener noreferrer">Vienna circle<span> <span>(opens new window)</span></span></a>
or the
<a href="https://en.wikipedia.org/wiki/Scottish_Caf%C3%A9" target="_blank" rel="noopener noreferrer">Scottish Café<span> <span>(opens new window)</span></span></a>.</p> <p>What also surprised me in the biography was the striking difference between Jews in Italy and in Poland. Last year, I read Leopold Infeld’s autobiography, where he describes the Jewish ghettos in Poland as being almost completely isolated from the general population. In contrast, Zariski’s wife, Yole, who was an Italian Jew, was raised as an Italian and was largely unfamiliar with Jewish traditions. She was utterly surprised when she first saw the Jewish quarter in Warsaw, remarking: “The Jews in side curls and kaftans made me feel that I was living in two different nations.”</p> <p>But the story in the book that I liked the most is this one: Zariski was, of course, very much obsessed with mathematics. On the day he and his fiancée Yole were getting married, with Yole already dressed in white and veiled and the rabbi standing by, the bridegroom was nowhere to be found. It turned out he was working on a mathematical problem. Luckily, Yole was neither angry nor surprised; she was amused. Ha! I need to tell this to my wife.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sqlitefs: SQLite as a Filesystem (132 pts)]]></title>
            <link>https://github.com/narumatt/sqlitefs</link>
            <guid>41085856</guid>
            <pubDate>Sat, 27 Jul 2024 11:29:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/narumatt/sqlitefs">https://github.com/narumatt/sqlitefs</a>, See on <a href="https://news.ycombinator.com/item?id=41085856">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">sqlite-fs</h2><a id="user-content-sqlite-fs" aria-label="Permalink: sqlite-fs" href="#sqlite-fs"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">About</h2><a id="user-content-about" aria-label="Permalink: About" href="#about"></a></p>
<p dir="auto">sqlite-fs allows Linux and MacOS to mount a sqlite database file as a normal filesystem.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Latest Rust Programming Language (≥ 1.38)</li>
<li>libfuse(Linux) or osxfuse(MacOS) is requied by <a href="https://github.com/zargony/fuse-rs">fuse-rs</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mount a filesystem</h3><a id="user-content-mount-a-filesystem" aria-label="Permalink: Mount a filesystem" href="#mount-a-filesystem"></a></p>
<div data-snippet-clipboard-copy-content="$ sqlite-fs <mount_point> [<db_path>]"><pre><code>$ sqlite-fs &lt;mount_point&gt; [&lt;db_path&gt;]
</code></pre></div>
<p dir="auto">If a database file doesn't exist, sqlite-fs create db file and tables.</p>
<p dir="auto">If a database file name isn't specified, sqlite-fs use in-memory-db instead of a file.
All data will be deleted when the filesystem is closed.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Unmount a filesystem</h3><a id="user-content-unmount-a-filesystem" aria-label="Permalink: Unmount a filesystem" href="#unmount-a-filesystem"></a></p>
<ul dir="auto">
<li>Linux</li>
</ul>
<div data-snippet-clipboard-copy-content="$ fusermount -u <mount_point>"><pre><code>$ fusermount -u &lt;mount_point&gt;
</code></pre></div>
<ul dir="auto">
<li>Mac</li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">example</h2><a id="user-content-example" aria-label="Permalink: example" href="#example"></a></p>
<div data-snippet-clipboard-copy-content="$ sqlite-fs ~/mount ~/filesystem.sqlite &amp;
$ echo &quot;Hello world\!&quot; > ~/mount/hello.txt
$ cat ~/mount/hello.txt
Hello world!"><pre><code>$ sqlite-fs ~/mount ~/filesystem.sqlite &amp;
$ echo "Hello world\!" &gt; ~/mount/hello.txt
$ cat ~/mount/hello.txt
Hello world!
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">functions</h2><a id="user-content-functions" aria-label="Permalink: functions" href="#functions"></a></p>
<ul>
<li> Create/Read/Delete directories</li>
<li> Create/Read/Write/Delete files</li>
<li> Change attributions</li>
<li> Copy/Move files</li>
<li> Create Hard Link and Symbolic Link</li>
<li> Read/Write extended attributes</li>
<li>[] File lock operations</li>
<li>[] Strict error handling</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Learning about PCI-e: Driver and DMA (152 pts)]]></title>
            <link>https://blog.davidv.dev/posts/pcie-driver-dma/</link>
            <guid>41085713</guid>
            <pubDate>Sat, 27 Jul 2024 10:55:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.davidv.dev/posts/pcie-driver-dma/">https://blog.davidv.dev/posts/pcie-driver-dma/</a>, See on <a href="https://news.ycombinator.com/item?id=41085713">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="blogpost">
                
                <section id="blogpost-content">
                    <p>In the <a href="https://blog.davidv.dev/learning-pcie.html">previous entry</a> we covered the implementation of a trivial PCI-e device, which allowed us to read and write to it, 32 bits at a time, 
by relying on manual peek/poke with a hardcoded address (<code>0xfe000000</code>) which came from copy-pasting the address of BAR0 from <code>lspci</code>.</p>

<p>To get this address programmatically, we need to ask the PCI subsystem for the details of the memory mapping for this device.</p>

<p>First, we need to make a <a href="https://elixir.bootlin.com/linux/v6.9/source/include/linux/pci.h#L887">struct pci_driver</a>, which only requires two fields: a table of supported devices, and a <code>probe</code> function.</p>

<p>The table of supported devices is an array of the pairs of device/vendor IDs which this driver supports:</p>

<div>
<pre><span></span><code><span>static</span><span> </span><span>struct</span><span> </span><span>pci_device_id</span><span> </span><span>gpu_id_tbl</span><span>[]</span><span> </span><span>=</span><span> </span><span>{</span>
<span>    </span><span>{</span><span> </span><span>PCI_DEVICE</span><span>(</span><span>0x1234</span><span>,</span><span> </span><span>0x1337</span><span>)</span><span> </span><span>},</span>
<span>    </span><span>{</span><span> </span><span>0</span><span>,</span><span> </span><span>},</span>
<span>};</span>
</code></pre>
</div>

<p>The <code>probe</code> function (which is only called if the device/vendor IDs match), needs to return <code>0</code> if it takes ownership of the device.</p>

<p>We need to update the driver's state to hold a reference to the device's memory region</p>

<div>
<pre><span></span><code><span> </span>typedef struct GpuState {
<span> </span>   struct pci_dev *pdev;
<span>+   u8 __iomem *hwmem;</span>
<span> </span>} GpuState;
</code></pre>
</div>

<p>Within the <code>probe</code> function, we can enable the device and store a reference to the <code>pci_dev</code>:</p>

<div>
<pre><span></span><code><span>static</span><span> </span><span>int</span><span> </span><span>gpu_probe</span><span>(</span><span>struct</span><span> </span><span>pci_dev</span><span> </span><span>*</span><span>pdev</span><span>,</span><span> </span><span>const</span><span> </span><span>struct</span><span> </span><span>pci_device_id</span><span> </span><span>*</span><span>id</span><span>)</span><span> </span><span>{</span>
<span>    </span><span>int</span><span> </span><span>bars</span><span>;</span>
<span>    </span><span>int</span><span> </span><span>err</span><span>;</span>
<span>    </span><span>unsigned</span><span> </span><span>long</span><span> </span><span>mmio_start</span><span>,</span><span> </span><span>mmio_len</span><span>;</span>
<span>    </span><span>GpuState</span><span>*</span><span> </span><span>gpu</span><span> </span><span>=</span><span> </span><span>kmalloc</span><span>(</span><span>sizeof</span><span>(</span><span>struct</span><span> </span><span>GpuState</span><span>),</span><span> </span><span>GFP_KERNEL</span><span>);</span>
<span>    </span><span>gpu</span><span>-&gt;</span><span>pdev</span><span> </span><span>=</span><span> </span><span>pdev</span><span>;</span>
<span>    </span><span>pr_info</span><span>(</span><span>"called probe"</span><span>);</span>

<span>    </span><span>pci_enable_device_mem</span><span>(</span><span>pdev</span><span>);</span>

<span>    </span><span>// create a bitfield of the available BARs, eg: 0b1 for 'BAR #0'</span>
<span>    </span><span>bars</span><span> </span><span>=</span><span> </span><span>pci_select_bars</span><span>(</span><span>pdev</span><span>,</span><span> </span><span>IORESOURCE_MEM</span><span>);</span>

<span>    </span><span>// claim ownership of the address space for each BAR in the bitfield</span>
<span>    </span><span>pci_request_region</span><span>(</span><span>pdev</span><span>,</span><span> </span><span>bars</span><span>,</span><span> </span><span>"gpu-pci"</span><span>);</span>

<span>    </span><span>mmio_start</span><span> </span><span>=</span><span> </span><span>pci_resource_start</span><span>(</span><span>pdev</span><span>,</span><span> </span><span>0</span><span>);</span>
<span>    </span><span>mmio_len</span><span> </span><span>=</span><span> </span><span>pci_resource_len</span><span>(</span><span>pdev</span><span>,</span><span> </span><span>0</span><span>);</span>

<span>    </span><span>// map physical address to virtual</span>
<span>    </span><span>gpu</span><span>-&gt;</span><span>hwmem</span><span> </span><span>=</span><span> </span><span>ioremap</span><span>(</span><span>mmio_start</span><span>,</span><span> </span><span>mmio_len</span><span>);</span>
<span>    </span><span>pr_info</span><span>(</span><span>"mmio starts at 0x%lx; hwmem 0x%px"</span><span>,</span><span> </span><span>mmio_start</span><span>,</span><span> </span><span>gpu</span><span>-&gt;</span><span>hwmem</span><span>);</span>
<span>    </span><span>return</span><span> </span><span>0</span><span>;</span>
<span>}</span>
</code></pre>
</div>

<p>Now, if we call <code>pci_register_driver</code> during <code>module_init</code>, we can see the card is initialized and we get back the BAR0 address:</p>

<div>
<pre><span></span><code><span>[</span><span>    </span><span>0</span>.488699<span>]</span><span> </span>called<span> </span>probe
<span>[</span><span>    </span><span>0</span>.488705<span>]</span><span> </span>mmio<span> </span>starts<span> </span>at<span> </span>0xfe000000<span>;</span><span> </span>hwmem<span> </span>0xffffbf5200600000
<span>[</span><span>    </span><span>0</span>.488817<span>]</span><span> </span>gpu_module_init<span> </span><span>done</span>
</code></pre>
</div>

<a data-header="1" href="#exposing-the-card-to-userspace"><h2 id="exposing-the-card-to-userspace">Exposing the card to userspace</h2></a>

<p>Now that we have mapped the BAR0 address space in our kernel driver, we can create a character device to allow user-space applications to interact with the PCIe device through file operations: <code>read(2)</code> and <code>write(2)</code>.</p>

<p>For this driver, we only need to implement <code>open</code>, <code>read</code> and <code>write</code>, which have these signatures:</p>

<div>
<pre><span></span><code><span>static</span><span> </span><span>int</span><span> </span><span>gpu_open</span><span>(</span><span>struct</span><span> </span><span>inode</span><span> </span><span>*</span><span>inode</span><span>,</span><span> </span><span>struct</span><span> </span><span>file</span><span> </span><span>*</span><span>file</span><span>);</span>
<span>static</span><span> </span><span>ssize_t</span><span> </span><span>gpu_read</span><span>(</span><span>struct</span><span> </span><span>file</span><span> </span><span>*</span><span>file</span><span>,</span><span> </span><span>char</span><span> </span><span>__user</span><span> </span><span>*</span><span>buf</span><span>,</span><span> </span><span>size_t</span><span> </span><span>count</span><span>,</span><span> </span><span>loff_t</span><span> </span><span>*</span><span>offset</span><span>);</span>
<span>static</span><span> </span><span>ssize_t</span><span> </span><span>gpu_write</span><span>(</span><span>struct</span><span> </span><span>file</span><span> </span><span>*</span><span>file</span><span>,</span><span> </span><span>const</span><span> </span><span>char</span><span> </span><span>__user</span><span> </span><span>*</span><span>buf</span><span>,</span><span> </span><span>size_t</span><span> </span><span>count</span><span>,</span><span> </span><span>loff_t</span><span> </span><span>*</span><span>offset</span><span>);</span>
</code></pre>
</div>

<p>First, add a reference to the cdev in the driver's state</p>

<div>
<pre><span></span><code><span> </span>typedef struct GpuState {
<span> </span>   struct pci_dev *pdev;
<span> </span>   u8 __iomem *hwmem;
<span>+   struct cdev cdev;</span>
<span> </span>} GpuState;
</code></pre>
</div>

<p>Then we define a set of file operations and a <code>setup</code> function:</p>

<div>
<pre><span></span><code><span>static</span><span> </span><span>const</span><span> </span><span>struct</span><span> </span><span>file_operations</span><span> </span><span>fileops</span><span> </span><span>=</span><span> </span><span>{</span>
<span>    </span><span>.</span><span>owner</span><span>      </span><span>=</span><span> </span><span>THIS_MODULE</span><span>,</span>
<span>    </span><span>.</span><span>open</span><span>       </span><span>=</span><span> </span><span>NULL</span><span>,</span>
<span>    </span><span>.</span><span>read</span><span>       </span><span>=</span><span> </span><span>NULL</span><span>,</span>
<span>    </span><span>.</span><span>write</span><span>      </span><span>=</span><span> </span><span>NULL</span>
<span>};</span>

<span>int</span><span> </span><span>setup_chardev</span><span>(</span><span>GpuState</span><span>*</span><span> </span><span>gpu</span><span>,</span><span> </span><span>struct</span><span> </span><span>class</span><span>*</span><span> </span><span>class</span><span>,</span><span> </span><span>struct</span><span> </span><span>pci_dev</span><span> </span><span>*</span><span>pdev</span><span>)</span><span> </span><span>{</span>
<span>    </span><span>dev_t</span><span> </span><span>dev_num</span><span>,</span><span> </span><span>major</span><span>;</span>
<span>    </span><span>alloc_chrdev_region</span><span>(</span><span>&amp;</span><span>dev_num</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>MAX_CHAR_DEVICES</span><span>,</span><span> </span><span>"gpu-chardev"</span><span>);</span>
<span>    </span><span>major</span><span> </span><span>=</span><span> </span><span>MAJOR</span><span>(</span><span>dev_num</span><span>);</span>

<span>    </span><span>cdev_init</span><span>(</span><span>&amp;</span><span>gpu</span><span>-&gt;</span><span>cdev</span><span>,</span><span> </span><span>&amp;</span><span>fileops</span><span>);</span>
<span>    </span><span>cdev_add</span><span>(</span><span>&amp;</span><span>gpu</span><span>-&gt;</span><span>cdev</span><span>,</span><span> </span><span>MKDEV</span><span>(</span><span>major</span><span>,</span><span> </span><span>0</span><span>),</span><span> </span><span>1</span><span>);</span>
<span>    </span><span>device_create</span><span>(</span><span>class</span><span>,</span><span> </span><span>NULL</span><span>,</span><span> </span><span>MKDEV</span><span>(</span><span>major</span><span>,</span><span> </span><span>0</span><span>),</span><span> </span><span>NULL</span><span>,</span><span> </span><span>"gpu-io"</span><span>);</span>
<span>    </span><span>return</span><span> </span><span>0</span><span>;</span>
<span>}</span>
</code></pre>
</div>

<p>At this point, the character device will be visible in the filesystem:</p>

<div>
<pre><span></span><code>/<span> </span><span># ls -l /dev/gpu-io </span>
crw-rw----<span>    </span><span>1</span><span> </span><span>0</span><span>        </span><span>0</span><span>         </span><span>241</span>,<span>   </span><span>0</span><span> </span>May<span> </span><span>25</span><span> </span><span>15</span>:58<span> </span>/dev/gpu-io
</code></pre>
</div>

<p>At this point, I tried to <code>write</code> and got a bit stuck, as <code>write</code> receives a <code>void* private_data</code> via the <code>struct file*</code> but it must be populated during <code>open</code>, which does <em>not</em> receive a <code>private_data</code>/<code>user_data</code> argument.</p>

<p>When reading the definition of <a href="https://elixir.bootlin.com/linux/latest/source/include/linux/fs.h#L721">struct inode</a>, I saw a pointer back to the character device (<code>struct cdev *i_cdev</code>), which gave me an idea:</p>

<p>As <code>struct GpuState</code> <em>embeds</em> <code>struct cdev</code>, having a pointer to <code>struct cdev</code> allows us to get a reference back to <code>GpuState</code> with <code>offsetof</code>:</p>

<p><img src="https://blog.davidv.dev/images/pcie-device/container_of.svg"></p>

<p>The kernel provides a <code>container_of</code> macro which is built for this specific purpose, so we can now implement open/read/write:</p>

<div>
<pre><span></span><code><span>static</span><span> </span><span>int</span><span> </span><span>gpu_open</span><span>(</span><span>struct</span><span> </span><span>inode</span><span> </span><span>*</span><span>inode</span><span>,</span><span> </span><span>struct</span><span> </span><span>file</span><span> </span><span>*</span><span>file</span><span>)</span><span> </span><span>{</span>
<span>    </span><span>GpuState</span><span> </span><span>*</span><span>gpu</span><span> </span><span>=</span><span> </span><span>container_of</span><span>(</span><span>inode</span><span>-&gt;</span><span>i_cdev</span><span>,</span><span> </span><span>struct</span><span> </span><span>GpuState</span><span>,</span><span> </span><span>cdev</span><span>);</span>
<span>    </span><span>file</span><span>-&gt;</span><span>private_data</span><span> </span><span>=</span><span> </span><span>gpu</span><span>;</span>
<span>    </span><span>return</span><span> </span><span>0</span><span>;</span>
<span>}</span>
</code></pre>
</div>

<p>and read/write are simple "one <span data-tooltip="32 bits">DWORD</span> at a time" implementations:</p>

<div>
<pre><span></span><code><span>static</span><span> </span><span>ssize_t</span><span> </span><span>gpu_read</span><span>(</span><span>struct</span><span> </span><span>file</span><span> </span><span>*</span><span>file</span><span>,</span><span> </span><span>char</span><span> </span><span>__user</span><span> </span><span>*</span><span>buf</span><span>,</span><span> </span><span>size_t</span><span> </span><span>count</span><span>,</span><span> </span><span>loff_t</span><span> </span><span>*</span><span>offset</span><span>)</span><span> </span><span>{</span>
<span>    </span><span>GpuState</span><span> </span><span>*</span><span>gpu</span><span> </span><span>=</span><span> </span><span>(</span><span>GpuState</span><span>*</span><span>)</span><span> </span><span>file</span><span>-&gt;</span><span>private_data</span><span>;</span>
<span>    </span><span>uint32_t</span><span> </span><span>read_val</span><span> </span><span>=</span><span> </span><span>ioread32</span><span>(</span><span>gpu</span><span>-&gt;</span><span>hwmem</span><span> </span><span>+</span><span> </span><span>*</span><span>offset</span><span>);</span>
<span>    </span><span>copy_to_user</span><span>(</span><span>buf</span><span>,</span><span> </span><span>&amp;</span><span>read_val</span><span>,</span><span> </span><span>4</span><span>);</span>
<span>    </span><span>*</span><span>offset</span><span> </span><span>+=</span><span> </span><span>4</span><span>;</span>
<span>    </span><span>return</span><span> </span><span>4</span><span>;</span>
<span>}</span>
<span>static</span><span> </span><span>ssize_t</span><span> </span><span>gpu_write</span><span>(</span><span>struct</span><span> </span><span>file</span><span> </span><span>*</span><span>file</span><span>,</span><span> </span><span>const</span><span> </span><span>char</span><span> </span><span>__user</span><span> </span><span>*</span><span>buf</span><span>,</span><span> </span><span>size_t</span><span> </span><span>count</span><span>,</span><span> </span><span>loff_t</span><span> </span><span>*</span><span>offset</span><span>)</span><span> </span><span>{</span>
<span>    </span><span>GpuState</span><span> </span><span>*</span><span>gpu</span><span> </span><span>=</span><span> </span><span>(</span><span>GpuState</span><span>*</span><span>)</span><span> </span><span>file</span><span>-&gt;</span><span>private_data</span><span>;</span>
<span>    </span><span>u32</span><span> </span><span>n</span><span>;</span>
<span>    </span><span>copy_from_user</span><span>(</span><span>&amp;</span><span>n</span><span>,</span><span> </span><span>buf</span><span> </span><span>+</span><span> </span><span>*</span><span>offset</span><span> </span><span>+</span><span> </span><span>written</span><span>,</span><span> </span><span>4</span><span>);</span>
<span>    </span><span>*</span><span>offset</span><span> </span><span>+=</span><span> </span><span>4</span><span>;</span>
<span>    </span><span>return</span><span> </span><span>4</span><span>;</span>
<span>}</span>
</code></pre>
</div>

<p>This method works well for small data transfers, but it's not practical for larger ones. Sending 1 packet a time keeps the CPU busy and is quite slow; for example, transferring 1.2MiB (640x480 at 32bpp) took ~800ms!</p>

<a data-header="1" href="#letting-the-&quot;hardware&quot;-do-the-hard-work"><h2 id="letting-the-&quot;hardware&quot;-do-the-hard-work">Letting the "hardware" do the hard work</h2></a>

<p>Instead of having the CPU copy one <span data-tooltip="32 bits">DWORD</span> worth of data at a time, we can ask the card to take care of copying the data itself, by using <span data-tooltip="Direct Memory Access">DMA</span>.</p>

<p>To send work requests to the card, we can use <a href="https://en.wikipedia.org/wiki/Memory-mapped_I/O_and_port-mapped_I/O">memory-mapped IO</a>: we treat certain memory addresses as registers, which will be the 'parameters' to our 'function call', and we treat other memory addresses as the execution of a 'function call'.</p>

<p>When defining the interface for this DMA "function call":</p>

<ol>
<li>The CPU has to tell the card:
<ul>
<li>What data to copy (source address, length)</li>
<li>The destination address</li>
<li>Whether the data flows <em>towards</em> main memory or <em>from</em> main memory (read or write)</li>
</ul></li>
<li>The CPU has to tell the card when it is ready for the copy to start</li>
<li>The card has to tell the CPU when it has finished the transfer</li>
</ol>

<p>As an example, we can map these addresses as registers:</p>

<div>
<pre><span></span><code><span>#define REG_DMA_DIR         0</span>
<span>#define REG_DMA_ADDR_SRC    1</span>
<span>#define REG_DMA_ADDR_DST    2</span>
<span>#define REG_DMA_LEN         3</span>
</code></pre>
</div>

<p>and we can define a set of "commands" to imply a call, as being different from just filling in some registers</p>

<div>
<pre><span></span><code><span>#define CMD_ADDR_BASE       0xf00</span>
<span>#define CMD_DMA_START       (CMD_ADDR_BASE + 0)</span>
</code></pre>
</div>

<p>and implement a function to execute DMA:</p>

<div>
<pre><span></span><code><span>static</span><span> </span><span>void</span><span> </span><span>write_reg</span><span>(</span><span>GpuState</span><span>*</span><span> </span><span>gpu</span><span>,</span><span> </span><span>u32</span><span> </span><span>val</span><span>,</span><span> </span><span>u32</span><span> </span><span>reg</span><span>)</span><span> </span><span>{</span>
<span>    </span><span>iowrite32</span><span>(</span><span>val</span><span>,</span><span>  </span><span>gpu</span><span>-&gt;</span><span>hwmem</span><span> </span><span>+</span><span> </span><span>(</span><span>reg</span><span> </span><span>*</span><span> </span><span>sizeof</span><span>(</span><span>u32</span><span>)));</span>
<span>}</span>
<span>void</span><span> </span><span>execute_dma</span><span>(</span><span>GpuState</span><span>*</span><span> </span><span>gpu</span><span>,</span><span> </span><span>u8</span><span> </span><span>dir</span><span>,</span><span> </span><span>u32</span><span> </span><span>src</span><span>,</span><span> </span><span>u32</span><span> </span><span>dst</span><span>,</span><span> </span><span>u32</span><span> </span><span>len</span><span>)</span><span> </span><span>{</span>
<span>    </span><span>write_reg</span><span>(</span><span>gpu</span><span>,</span><span> </span><span>dir</span><span>,</span><span> </span><span>REG_DMA_DIR</span><span>);</span>
<span>    </span><span>write_reg</span><span>(</span><span>gpu</span><span>,</span><span> </span><span>src</span><span>,</span><span> </span><span>REG_DMA_ADDR_SRC</span><span>);</span>
<span>    </span><span>write_reg</span><span>(</span><span>gpu</span><span>,</span><span> </span><span>dst</span><span>,</span><span> </span><span>REG_DMA_ADDR_DST</span><span>);</span>
<span>    </span><span>write_reg</span><span>(</span><span>gpu</span><span>,</span><span> </span><span>len</span><span>,</span><span> </span><span>REG_DMA_LEN</span><span>);</span>
<span>    </span><span>write_reg</span><span>(</span><span>gpu</span><span>,</span><span> </span><span>1</span><span>,</span><span>   </span><span>CMD_DMA_START</span><span>);</span>
<span>}</span>
</code></pre>
</div>

<p>We also need to implement the MMIO side in the adapter, by replacing the previous <code>gpu_write</code> function:</p>

<div>
<pre><span></span><code><span>static</span><span> </span><span>void</span><span> </span><span>gpu_write</span><span>(</span><span>void</span><span> </span><span>*</span><span>opaque</span><span>,</span><span> </span><span>hwaddr</span><span> </span><span>addr</span><span>,</span><span> </span><span>uint64_t</span><span> </span><span>val</span><span>,</span><span> </span><span>unsigned</span><span> </span><span>size</span><span>)</span><span> </span><span>{</span>
<span>    </span><span>GpuState</span><span> </span><span>*</span><span>gpu</span><span> </span><span>=</span><span> </span><span>opaque</span><span>;</span>
<span>    </span><span>val</span><span> </span><span>=</span><span> </span><span>lower_n_bytes</span><span>(</span><span>val</span><span>,</span><span> </span><span>size</span><span>);</span>
<span>    </span><span>uint32_t</span><span> </span><span>reg</span><span> </span><span>=</span><span> </span><span>addr</span><span> </span><span>/</span><span> </span><span>4</span><span>;</span>
<span>    </span><span>if</span><span> </span><span>(</span><span>reg</span><span> </span><span>&lt;</span><span> </span><span>CMD_ADDR_BASE</span><span>)</span><span> </span><span>{</span><span> </span><span>// register</span>
<span>        </span><span>gpu</span><span>-&gt;</span><span>registers</span><span>[</span><span>reg</span><span>]</span><span> </span><span>=</span><span> </span><span>(</span><span>uint32_t</span><span>)</span><span>val</span><span>;</span>
<span>        </span><span>return</span><span>;</span>
<span>    </span><span>}</span>
<span>    </span><span>switch</span><span> </span><span>(</span><span>reg</span><span>)</span><span> </span><span>{</span>
<span>        </span><span>case</span><span> </span><span>REG_DMA_START</span><span>:</span>
<span>            </span><span>if</span><span> </span><span>(</span><span>gpu</span><span>-&gt;</span><span>registers</span><span>[</span><span>REG_DMA_DIR</span><span>]</span><span> </span><span>==</span><span> </span><span>DIR_HOST_TO_GPU</span><span>)</span><span> </span><span>{</span>
<span>                </span><span>pci_dma_read</span><span>(</span><span>&amp;</span><span>gpu</span><span>-&gt;</span><span>pdev</span><span>,</span>
<span>                             </span><span>gpu</span><span>-&gt;</span><span>registers</span><span>[</span><span>REG_DMA_ADDR_SRC</span><span>],</span><span> </span><span>// host addr</span>
<span>                             </span><span>(</span><span>gpu</span><span>-&gt;</span><span>framebuffer</span><span> </span><span>+</span><span> </span><span>gpu</span><span>-&gt;</span><span>registers</span><span>[</span><span>REG_DMA_ADDR_DST</span><span>]),</span><span> </span><span>// dev addr</span>
<span>                             </span><span>gpu</span><span>-&gt;</span><span>registers</span><span>[</span><span>REG_DMA_LEN</span><span>]);</span>
<span>            </span><span>}</span><span> </span><span>else</span><span> </span><span>{</span>
<span>                </span><span>printf</span><span>(</span><span>"Unimplemented DMA direction</span><span>\n</span><span>"</span><span>);</span>
<span>            </span><span>}</span>
<span>            </span><span>break</span><span>;</span>
<span>    </span><span>}</span>
<span>}</span>
</code></pre>
</div>

<p>in which we only store the 'arguments' to the DMA 'function call', and execute it when a token value is written.</p>

<p>Then, we can go back to the kernel driver and implement <code>write</code>:</p>

<div>
<pre><span></span><code><span>static</span><span> </span><span>ssize_t</span><span> </span><span>gpu_fb_write</span><span>(</span><span>struct</span><span> </span><span>file</span><span> </span><span>*</span><span>file</span><span>,</span><span> </span><span>const</span><span> </span><span>char</span><span> </span><span>__user</span><span> </span><span>*</span><span>buf</span><span>,</span><span> </span><span>size_t</span><span> </span><span>count</span><span>,</span><span> </span><span>loff_t</span><span> </span><span>*</span><span>offset</span><span>)</span><span> </span><span>{</span>
<span>    </span><span>GpuState</span><span> </span><span>*</span><span>gpu</span><span> </span><span>=</span><span> </span><span>(</span><span>GpuState</span><span>*</span><span>)</span><span> </span><span>file</span><span>-&gt;</span><span>private_data</span><span>;</span>
<span>    </span><span>dma_addr_t</span><span> </span><span>dma_addr</span><span>;</span>
<span>    </span><span>u8</span><span>*</span><span> </span><span>kbuf</span><span> </span><span>=</span><span> </span><span>kmalloc</span><span>(</span><span>count</span><span>,</span><span> </span><span>GFP_KERNEL</span><span>);</span>
<span>    </span><span>copy_from_user</span><span>(</span><span>kbuf</span><span>,</span><span> </span><span>buf</span><span>,</span><span> </span><span>count</span><span>);</span>

<span>    </span><span>dma_addr</span><span> </span><span>=</span><span> </span><span>dma_map_single</span><span>(</span><span>&amp;</span><span>gpu</span><span>-&gt;</span><span>pdev</span><span>-&gt;</span><span>dev</span><span>,</span><span> </span><span>kbuf</span><span>,</span><span> </span><span>count</span><span>,</span><span> </span><span>DMA_TO_DEVICE</span><span>);</span>
<span>    </span><span>execute_dma</span><span>(</span><span>gpu</span><span>,</span><span> </span><span>DIR_HOST_TO_GPU</span><span>,</span><span> </span><span>dma_addr</span><span>,</span><span> </span><span>*</span><span>offset</span><span>,</span><span> </span><span>count</span><span>);</span>
<span>    </span><span>kfree</span><span>(</span><span>kbuf</span><span>);</span>
<span>    </span><span>return</span><span> </span><span>count</span><span>;</span>
<span>}</span>
</code></pre>
</div>

<p>Which now is fast enough to report as ~300us on my system.</p>

<a data-header="1" href="#blocking-writes"><h2 id="blocking-writes">Blocking writes</h2></a>

<p>There's a problem though; the DMA execution is asynchronous, and it would be a lot nicer if <code>write</code> would block until the write has finished.</p>

<p><em>Conveniently</em>, PCI-e cards can arbitrarily send signals to the CPU as <span data-tooltip="MSI">Message Signalled Interrupts</span> -- we can send an MSI to notify the CPU that the DMA transfer has completed.</p>

<p>We are only focusing on MSIs here which, as the name implies, they communicate the interrupt by sending a normal message (packet) on the bus, this is in contrast with classic interrupts which had a dedicated electrical connection to send out-of-band signals.</p>

<p>To configure MSI-X, we need to set aside some space to store some configuration for each interrupt (the MSI-X table) and some extra space for a bitmap of pending interrupts (the <span data-tooltip="Pending Bit Array">PBA</span>, but we won't use it).</p>

<p>First, we define some shared constants:</p>

<div>
<pre><span></span><code><span>#define IRQ_COUNT           1</span>
<span>#define IRQ_DMA_DONE_NR     0</span>
<span>#define MSIX_ADDR_BASE      0x1000</span>
<span>#define PBA_ADDR_BASE       0x3000</span>
</code></pre>
</div>

<p>In QEMU, in <code>pci_gpu_realize</code> we need to add</p>

<div>
<pre><span></span><code><span>msix_init</span><span>(</span><span>pdev</span><span>,</span><span> </span><span>IRQ_COUNT</span><span>,</span>
<span>          </span><span>&amp;</span><span>gpu</span><span>-&gt;</span><span>mem</span><span>,</span><span> </span><span>0</span><span> </span><span>/* table_bar_nr = bar id */</span><span>,</span><span> </span><span>MSIX_ADDR_BASE</span><span>,</span>
<span>          </span><span>&amp;</span><span>gpu</span><span>-&gt;</span><span>mem</span><span>,</span><span> </span><span>0</span><span> </span><span>/* pba_bar_nr = bar id */</span><span>,</span><span> </span><span>PBA_ADDR_BASE</span><span>,</span>
<span>          </span><span>0x0</span><span> </span><span>/* capabilities */</span><span>,</span><span> </span><span>errp</span><span>);</span>
<span>for</span><span>(</span><span>int</span><span> </span><span>i</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span> </span><span>&lt;</span><span> </span><span>IRQ_COUNT</span><span>;</span><span> </span><span>i</span><span>++</span><span>)</span>
<span>    </span><span>msix_vector_use</span><span>(</span><span>pdev</span><span>,</span><span> </span><span>i</span><span>);</span>
</code></pre>
</div>

<p>which will reserve an 8KiB space for MSIs (at the 4K offset) and another 8KiB space for PBAs at the 12KiB offset, or, in stack form:</p>

<p><img src="https://blog.davidv.dev/images/pcie-device/ioregion.svg"></p>

<p>At this point, we can see that MSI-X are enabled, and the offsets of the vector table &amp; PBA:</p>

<div>
<pre><span></span><code>/<span> </span><span># lspci -vv</span>
...
Region<span> </span><span>0</span>:<span> </span>Memory<span> </span>at<span> </span>fc000000<span> </span><span>(</span><span>32</span>-bit,<span> </span>non-prefetchable<span>)</span><span> </span><span>[</span><span>size</span><span>=</span>16M<span>]</span>
Capabilities:<span> </span><span>[</span><span>40</span><span>]</span><span> </span>MSI-X:<span> </span>Enable-<span> </span><span>Count</span><span>=</span><span>1</span><span> </span>Masked-
<span>        </span>Vector<span> </span>table:<span> </span><span>BAR</span><span>=</span><span>0</span><span> </span><span>offset</span><span>=</span><span>00001000</span>
<span>        </span>PBA:<span> </span><span>BAR</span><span>=</span><span>0</span><span> </span><span>offset</span><span>=</span><span>00003000</span>
</code></pre>
</div>

<p>Then to send an interrupt when <code>pci_dma_read</code> finishes, we can call</p>

<div>
<pre><span></span><code><span>msix_notify</span><span>(</span><span>&amp;</span><span>gpu</span><span>-&gt;</span><span>pdev</span><span>,</span><span> </span><span>IRQ_DMA_DONE_NR</span><span>);</span>
</code></pre>
</div>

<p>The kernel needs to hook a handler for the interrupt, which can be done with</p>

<div>
<pre><span></span><code><span>static</span><span> </span><span>irqreturn_t</span><span> </span><span>irq_handler</span><span>(</span><span>int</span><span> </span><span>irq</span><span>,</span><span> </span><span>void</span><span> </span><span>*</span><span>data</span><span>)</span><span> </span><span>{</span>
<span>    </span><span>pr_info</span><span>(</span><span>"IRQ %d received</span><span>\n</span><span>"</span><span>,</span><span> </span><span>irq</span><span>);</span>
<span>    </span><span>return</span><span> </span><span>IRQ_HANDLED</span><span>;</span>
<span>}</span>
<span>static</span><span> </span><span>int</span><span> </span><span>setup_msi</span><span>(</span><span>GpuState</span><span>*</span><span> </span><span>gpu</span><span>)</span><span> </span><span>{</span>
<span>    </span><span>int</span><span> </span><span>msi_vecs</span><span>;</span>
<span>    </span><span>int</span><span> </span><span>irq_num</span><span>;</span>

<span>    </span><span>msi_vecs</span><span> </span><span>=</span><span> </span><span>pci_alloc_irq_vectors</span><span>(</span><span>gpu</span><span>-&gt;</span><span>pdev</span><span>,</span><span> </span><span>IRQ_COUNT</span><span>,</span><span> </span><span>IRQ_COUNT</span><span>,</span><span> </span><span>PCI_IRQ_MSIX</span><span> </span><span>|</span><span> </span><span>PCI_IRQ_MSI</span><span>);</span>
<span>    </span><span>irq_num</span><span> </span><span>=</span><span> </span><span>pci_irq_vector</span><span>(</span><span>gpu</span><span>-&gt;</span><span>pdev</span><span>,</span><span> </span><span>IRQ_DMA_DONE_NR</span><span>);</span>
<span>    </span><span>pr_info</span><span>(</span><span>"Got MSI vec %d, IRQ num %d"</span><span>,</span><span> </span><span>msi_vecs</span><span>,</span><span> </span><span>irq_num</span><span>);</span>
<span>    </span><span>request_threaded_irq</span><span>(</span><span>irq_num</span><span>,</span><span> </span><span>irq_handler</span><span>,</span><span> </span><span>NULL</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>"GPU-Dma0"</span><span>,</span><span> </span><span>gpu</span><span>);</span>
<span>    </span><span>return</span><span> </span><span>0</span><span>;</span>
<span>}</span>
</code></pre>
</div>

<p>and we can call <code>setup_msi</code> in the <code>gpu_probe</code> (PCI probe) function.</p>

<p>On boot, we can see that the kernel assigned an IRQ number for us:</p>

<div>
<pre><span></span><code>/<span> </span><span># grep Dma /proc/interrupts </span>
<span>             </span>CPU<span> </span><span>0</span>
<span>  </span><span>24</span>:<span>          </span><span>0</span><span>          </span>PCI-MSIX-0000:00:02.0<span>   </span><span>0</span>-edge<span>      </span>GPU-Dma0
</code></pre>
</div>

<p>However, this does not work, because the card does not yet have permissions to independently send messages to the CPU. To be able to do this, the card has to be granted the 'bus master' capability.</p>

<p>Bus mastering is a feature which allows devices to directly manipulate system memory without involving the CPU.</p>

<p>We can grant the card bus master capabilities by calling <code>pci_set_master(pdev);</code> in the kernel's <code>gpu_probe</code> function, after which, if we call <code>write</code> twice we can see:</p>

<div>
<pre><span></span><code><span>[</span><span>    </span><span>7</span>.086591<span>]</span><span> </span>IRQ<span> </span><span>24</span><span> </span>received
<span>[</span><span>   </span><span>11</span>.540884<span>]</span><span> </span>IRQ<span> </span><span>24</span><span> </span>received
</code></pre>
</div>

<a data-header="1" href="#actually-blocking-writes"><h3 id="actually-blocking-writes">Actually blocking writes</h3></a>

<p>With the interrupt machinery in place we can use a <a href="https://stackoverflow.com/a/20065881/3530257">wait queue</a> to convert <code>write</code> to blocking.</p>

<div>
<pre><span></span><code><span>wait_queue_head_t</span><span> </span><span>wq</span><span>;</span>
<span>volatile</span><span> </span><span>int</span><span> </span><span>irq_fired</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span>
<span>static</span><span> </span><span>irqreturn_t</span><span> </span><span>irq_handler</span><span>(</span><span>int</span><span> </span><span>irq</span><span>,</span><span> </span><span>void</span><span> </span><span>*</span><span>data</span><span>)</span><span> </span><span>{</span>
<span>    </span><span>irq_fired</span><span> </span><span>=</span><span> </span><span>1</span><span>;</span>
<span>    </span><span>wake_up_interruptible</span><span>(</span><span>&amp;</span><span>wq</span><span>);</span>
<span>    </span><span>return</span><span> </span><span>IRQ_HANDLED</span><span>;</span>
<span>}</span>
</code></pre>
</div>

<p>add <code>init_waitqueue_head(&amp;wq)</code> to <code>setup_msi</code>, and add the blocking condition in <code>write</code>:</p>

<div>
<pre><span></span><code><span> </span>static ssize_t gpu_fb_write(struct file *file, const char __user *buf, size_t count, loff_t *offset) {
<span> </span>   ...
<span> </span>   execute_dma(gpu, DIR_HOST_TO_GPU, dma_addr, *offset, count);
<span>+   if (wait_event_interruptible(wq, irq_fired != 0)) {</span>
<span>+       pr_info("interrupted");</span>
<span>+       return -ERESTARTSYS;</span>
<span>+   }</span>
<span> </span>   kfree(kbuf);
<span> </span>   return count;
<span> </span>}
</code></pre>
</div>

<a data-header="1" href="#displaying-something"><h2 id="displaying-something">Displaying something</h2></a>

<p>We now have a 'framebuffer' that can receive <code>write(2)</code> from userspace, and will forward the data as-is to a PCI-e device using DMA; we can cheat a little bit to pretend we have a working GPU, by hooking the card's buffer to QEMU's console output:</p>

<p>In QEMU's source:</p>

<div>
<pre><span></span><code><span> </span>struct GpuState {
<span> </span>   PCIDevice pdev;
<span> </span>   MemoryRegion mem;
<span>+   QemuConsole* con;</span>
<span> </span>   uint32_t registers[0x100000 / 32]; // 1 MiB = 32k, 32 bit registers
<span> </span>   uint32_t framebuffer[0x200000]; // barely enough for 1920x1080 at 32bpp
<span> </span>};
</code></pre>
</div>

<div>
<pre><span></span><code><span> </span>static void pci_gpu_realize(PCIDevice *pdev, Error **errp) {
<span> </span>   ...
<span>+    gpu-&gt;con = graphic_console_init(DEVICE(pdev), 0, &amp;ghwops, gpu);</span>
<span>+    DisplaySurface *surface = qemu_console_surface(gpu-&gt;con);</span>
<span>+   // Display a test pattern</span>
<span>+   for(int i = 0; i&lt;640*480; i++) {</span>
<span>+       ((uint32_t*)surface_data(surface))[i] = i;</span>
<span>+   }</span>
}
</code></pre>
</div>

<p>and add a "passthrough" implementation for the display surface</p>

<div>
<pre><span></span><code><span>static</span><span> </span><span>void</span><span> </span><span>vga_update_display</span><span>(</span><span>void</span><span> </span><span>*</span><span>opaque</span><span>)</span><span> </span><span>{</span>
<span>    </span><span>GpuState</span><span>*</span><span> </span><span>gpu</span><span> </span><span>=</span><span> </span><span>opaque</span><span>;</span>
<span>    </span><span>DisplaySurface</span><span> </span><span>*</span><span>surface</span><span> </span><span>=</span><span> </span><span>qemu_console_surface</span><span>(</span><span>gpu</span><span>-&gt;</span><span>con</span><span>);</span>
<span>    </span><span>for</span><span>(</span><span>int</span><span> </span><span>i</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span> </span><span>i</span><span>&lt;</span><span>640</span><span>*</span><span>480</span><span>;</span><span> </span><span>i</span><span>++</span><span>)</span><span> </span><span>{</span>
<span>        </span><span>((</span><span>uint32_t</span><span>*</span><span>)</span><span>surface_data</span><span>(</span><span>surface</span><span>))[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>gpu</span><span>-&gt;</span><span>framebuffer</span><span>[</span><span>i</span><span> </span><span>%</span><span> </span><span>0x200000</span><span> </span><span>];</span>
<span>    </span><span>}</span>

<span>    </span><span>dpy_gfx_update</span><span>(</span><span>gpu</span><span>-&gt;</span><span>con</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>640</span><span>,</span><span> </span><span>480</span><span>);</span>
<span>}</span>
<span>static</span><span> </span><span>const</span><span> </span><span>GraphicHwOps</span><span> </span><span>ghwops</span><span> </span><span>=</span><span> </span><span>{</span>
<span>   </span><span>.</span><span>gfx_update</span><span>  </span><span>=</span><span> </span><span>vga_update_display</span><span>,</span>
<span>};</span>
</code></pre>
</div>

<p>when launching QEMU, we can now see the test pattern:</p>

<center><img alt="" src="https://blog.davidv.dev/images/pcie-device/qemu_test_pattern.png"></center>

<p>And whenever writing patterns to the underlying device, we can see the display change!</p>

<center><video controls=""><source src="https://blog.davidv.dev/videos/pcie-device/qemu_test_pattern_writes.mp4"></video></center>

<hr>

<p>That's it for now, next time, we <em>may</em> look at multiple DMA engines, zero-copy DMA and/or becoming a <em>real</em> GPU.</p>

<p>You can find the source <a href="https://github.com/DavidVentura/pci-device">here</a>.</p>

<a data-header="1" href="#references:"><h2 id="references:">References:</h2></a>

<ol>
<li><a href="https://www.kernel.org/doc/Documentation/PCI/pci.txt">Kernel's docs on PCI</a></li>
<li><a href="https://www.kernel.org/doc/Documentation/PCI/MSI-HOWTO.txt">Kernel's docs on MSI</a></li>
<li><a href="https://linux-kernel-labs.github.io/refs/heads/master/labs/device_drivers.html">Linux Kernel Labs - Device Drivers</a></li>
<li><a href="https://olegkutkov.me/2021/01/07/writing-a-pci-device-driver-for-linux/">Writing a PCI device driver for Linux</a></li>
<li><a href="https://olegkutkov.me/2018/03/14/simple-linux-character-device-driver/">Simple character device driver for Linux</a></li>
<li><a href="https://github.com/luizinhosuraty/pciemu">pciemu</a></li>
<li><a href="https://www.intel.com/content/www/us/en/docs/programmable/683686/20-4/msi-x-capability-structure.html">MSI-X Capability structure</a></li>
</ol>

                </section>
            </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SQLite: 35% Faster Than the Filesystem (432 pts)]]></title>
            <link>https://sqlite.org/fasterthanfs.html</link>
            <guid>41085376</guid>
            <pubDate>Sat, 27 Jul 2024 09:10:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sqlite.org/fasterthanfs.html">https://sqlite.org/fasterthanfs.html</a>, See on <a href="https://news.ycombinator.com/item?id=41085376">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<p>
35% Faster Than The Filesystem
</p>


</div>





<h2 id="summary"><span>1. </span>Summary</h2>

<p>SQLite reads and writes small blobs (for example, thumbnail images)
<a href="#approx">35% faster¹</a> than the same blobs
can be read from or written to individual files on disk using
fread() or fwrite().

</p><p>Furthermore, a single SQLite database holding
10-kilobyte blobs uses about 20% less disk space than
storing the blobs in individual files.

</p><p>The performance difference arises (we believe) because when
working from an SQLite database, the open() and close() system calls
are invoked only once, whereas
open() and close() are invoked once for each blob
when using blobs stored in individual files.  It appears that the
overhead of calling open() and close() is greater than the overhead
of using the database.  The size reduction arises from the fact that
individual files are padded out to the next multiple of the filesystem
block size, whereas the blobs are packed more tightly into an SQLite
database.

</p><p>
The measurements in this article were made during the week of 2017-06-05
using a version of SQLite in between 3.19.2 and 3.20.0.  You may expect
future versions of SQLite to perform even better.

</p><h2 id="caveats"><span>1.1. </span>Caveats</h2>

<p>
¹The 35% figure above is approximate.  Actual timings vary
depending on hardware, operating system, and the
details of the experiment, and due to random performance fluctuations
on real-world hardware.  See the text below for more detail.
Try the experiments yourself.  Report significant deviations on
the <a href="https://sqlite.org/support.html#fx">SQLite forum</a>.
</p>

<p>
The 35% figure is based on running tests on every machine
that the author has easily at hand.
Some reviewers of this article report that SQLite has higher 
latency than direct I/O on their systems.  We do not yet understand
the difference.  We also see indications that SQLite does not
perform as well as direct I/O when experiments are run using
a cold filesystem cache.

</p><p>
So let your take-away be this: read/write latency for
SQLite is competitive with read/write latency of individual files on
disk.  Often SQLite is faster.  Sometimes SQLite is almost
as fast.  Either way, this article disproves the common
assumption that a relational database must be slower than direct
filesystem I/O.

</p>

<p>
A <a href="https://golangexample.com/an-unscientific-benchmark-of-sqlite-vs-the-file-system-btrfs/">2022 study</a>
(<a href="https://github.com/chrisdavies/dbench">alternative link on GitHub</a>) found that
SQLite is <i>roughly</i> twice as fast at real-world workloads compared to Btrfs and Ext4 on Linux.

</p><p>
<a href="https://www.microsoft.com/en-us/research/people/gray/">Jim Gray</a>
and others studied the read performance of BLOBs
versus file I/O for Microsoft SQL Server and found that reading BLOBs 
out of the 
database was faster for BLOB sizes less than between 250KiB and 1MiB.
(<a href="https://www.microsoft.com/en-us/research/publication/to-blob-or-not-to-blob-large-object-storage-in-a-database-or-a-filesystem/">Paper</a>).
In that study, the database still stores the filename of the content even
if the content is held in a separate file.  So the database is consulted
for every BLOB, even if it is only to extract the filename.  In this
article, the key for the BLOB is the filename, so no preliminary database
access is required.  Because the database is never used at all when
reading content from individual files in this article, the threshold
at which direct file I/O becomes faster is smaller than it is in Gray's
paper.

</p><p>
The <a href="https://sqlite.org/intern-v-extern-blob.html">Internal Versus External BLOBs</a> article on this website is an
earlier investigation (circa 2011) that uses the same approach as the
Jim Gray paper — storing the blob filenames as entries in the
database — but for SQLite instead of SQL Server.



</p><h2 id="how_these_measurements_are_made"><span>2. </span>How These Measurements Are Made</h2>

<p>I/O performance is measured using the
<a href="https://www.sqlite.org/src/file/test/kvtest.c">kvtest.c</a> program
from the SQLite source tree.
To compile this test program, first gather the kvtest.c source file
into a directory with the <a href="https://sqlite.org/amalgamation.html">SQLite amalgamation</a> source
files "sqlite3.c" and "sqlite3.h".  Then on unix, run a command like
the following:

</p><div><pre>gcc -Os -I. -DSQLITE_DIRECT_OVERFLOW_READ \
  kvtest.c sqlite3.c -o kvtest -ldl -lpthread
</pre></div>

<p>Or on Windows with MSVC:

</p><div><pre>cl -I. -DSQLITE_DIRECT_OVERFLOW_READ kvtest.c sqlite3.c
</pre></div>

<p>Instructions for compiling for Android
are <a href="#compile-android">shown below</a>.

</p><p>
Use the resulting "kvtest" program to
generate a test database with 100,000 random uncompressible
blobs, each with a random
size between 8,000 and 12,000 bytes
using a command like this:

</p><div><pre>./kvtest init test1.db --count 100k --size 10k --variance 2k
</pre></div>

<p>
If desired, you can verify the new database by running this command:

</p>

<p>
Next, make copies of all the blobs into individual files in a directory
using a command like this:

</p><div><pre>./kvtest export test1.db test1.dir
</pre></div>

<p>
At this point, you can measure the amount of disk space used by
the test1.db database and the space used by the test1.dir directory
and all of its content.  On a standard Ubuntu Linux desktop, the
database file will be 1,024,512,000 bytes in size and the test1.dir
directory will use 1,228,800,000 bytes of space (according to "du -k"),
about 20% more than the database.

</p><p>
The "test1.dir" directory created above puts all the blobs into a single
folder.  It was conjectured that some operating systems would perform 
poorly when a single directory contains 100,000 objects.  To test this,
the kvtest program can also store the blobs in a hierarchy of folders with no
more than 100 files and/or subdirectories per folder.  The alternative
on-disk representation of the blobs can be created using the --tree
command-line option to the "export" command, like this:

</p><div><pre>./kvtest export test1.db test1.tree --tree
</pre></div>

<p>
The test1.dir directory will contain 100,000 files
with names like "000000", "000001", "000002" and so forth but the
test1.tree directory will contain the same files in subdirectories like
"00/00/00", "00/00/01", and so on.  The test1.dir and test1.test
directories take up approximately the same amount of space, though
test1.test is very slightly larger due to the extra directory entries.

</p><p>
All of the experiments that follow operate the same with either 
"test1.dir" or "test1.tree".  Very little performance difference is
measured in either case, regardless of operating system.

</p><p>
Measure the performance for reading blobs from the database and from
individual files using these commands:

</p><div><pre>./kvtest run test1.db --count 100k --blob-api
./kvtest run test1.dir --count 100k --blob-api
./kvtest run test1.tree --count 100k --blob-api
</pre></div>

<p>
Depending on your hardware and operating system, you should see that reads 
from the test1.db database file are about 35% faster than reads from 
individual files in the test1.dir or test1.tree folders.  Results can vary
significantly from one run to the next due to caching, so it is advisable
to run tests multiple times and take an average or a worst case or a best
case, depending on your requirements.

</p><p>The --blob-api option on the database read test causes kvtest to use
the <a href="https://sqlite.org/c3ref/blob_read.html">sqlite3_blob_read()</a> feature of SQLite to load the content of the
blobs, rather than running pure SQL statements.  This helps SQLite to run
a little faster on read tests.  You can omit that option to compare the
performance of SQLite running SQL statements.
In that case, the SQLite still out-performs direct reads, though
by not as much as when using <a href="https://sqlite.org/c3ref/blob_read.html">sqlite3_blob_read()</a>.
The --blob-api option is ignored for tests that read from individual disk
files.

</p><p>
Measure write performance by adding the --update option.  This causes
the blobs are overwritten in place with another random blob of
exactly the same size.

</p><div><pre>./kvtest run test1.db --count 100k --update
./kvtest run test1.dir --count 100k --update
./kvtest run test1.tree --count 100k --update
</pre></div>

<p>
The writing test above is not completely fair, since SQLite is doing
<a href="https://sqlite.org/transactional.html">power-safe transactions</a> whereas the direct-to-disk writing is not.
To put the tests on a more equal footing, add either the --nosync
option to the SQLite writes to disable calling fsync() or
FlushFileBuffers() to force content to disk, or using the --fsync option
for the direct-to-disk tests to force them to invoke fsync() or
FlushFileBuffers() when updating disk files.

</p><p>
By default, kvtest runs the database I/O measurements all within
a single transaction.  Use the --multitrans option to run each blob
read or write in a separate transaction.  The --multitrans option makes
SQLite much slower, and uncompetitive with direct disk I/O.  This
option proves, yet again, that to get the most performance out of
SQLite, you should group as much database interaction as possible within
a single transaction.

</p><p>
There are many other testing options, which can be seen by running
the command:

</p>

<h2 id="read_performance_measurements"><span>2.1. </span>Read Performance Measurements</h2>

<p>The chart below shows data collected using 
<a href="https://www.sqlite.org/src/file/test/kvtest.c">kvtest.c</a> on five different
systems:

</p><ul>
<li><b>Win7</b>: A circa-2009 Dell Inspiron laptop, Pentium dual-core
    at 2.30GHz, 4GiB RAM, Windows7.
</li><li><b>Win10</b>: A 2016 Lenovo YOGA 910, Intel i7-7500 at 2.70GHz,
    16GiB RAM, Windows10.
</li><li><b>Mac</b>: A 2015 MacBook Pro, 3.1GHz intel Core i7, 16GiB RAM,
    MacOS 10.12.5
</li><li><b>Ubuntu</b>: Desktop built from Intel i7-4770K at 3.50GHz, 32GiB RAM,
    Ubuntu 16.04.2 LTS
</li><li><b>Android</b>: Galaxy S3, ARMv7, 2GiB RAM
</li></ul>

<p>All machines use SSD except Win7 which has a
hard-drive. The test database is 100K blobs with sizes uniformly
distributed between 8K and 12K, for a total of about 1 gigabyte
of content.  The database page size
is 4KiB.  The -DSQLITE_DIRECT_OVERFLOW_READ compile-time option was
used for all of these tests.
Tests were run multiple times.
The first run was used to warm up the cache and its timings were discarded.

</p><p>
The chart below shows average time to read a blob directly from the
filesystem versus the time needed to read the same blob from the SQLite 
database.
The actual timings vary considerably from one system to another 
(the Ubuntu desktop is much
faster than the Galaxy S3 phone, for example).  
This chart shows the ratio of the
times needed to read blobs from a file divided by the time needed to
from the database.  The left-most column in the chart is the normalized
time to read from the database, for reference.

</p><p>
In this chart, an SQL statement ("SELECT v FROM kv WHERE k=?1") 
is prepared once.  Then for each blob, the blob key value is bound 
to the ?1 parameter and the statement is evaluated to extract the
blob content.

</p><p>
The chart shows that on Windows10, content can be read from the SQLite
database about 5 times faster than it can be read directly from disk.
On Android, SQLite is only about 35% faster than reading from disk.

</p><center>
<p><img src="https://sqlite.org/images/faster-read-sql.jpg">
</p>
<br>
Chart 1:  SQLite read latency relative to direct filesystem reads.<br>
100K blobs, avg 10KB each, random order using SQL
</center>

<p>
The performance can be improved slightly by bypassing the SQL layer
and reading the blob content directly using the
<a href="https://sqlite.org/c3ref/blob_read.html">sqlite3_blob_read()</a> interface, as shown in the next chart:

</p><center>
<p><img src="https://sqlite.org/images/faster-read-blobapi.jpg">
</p>
<br>
Chart 2:  SQLite read latency relative to direct filesystem reads.<br>
100K blobs, avg size 10KB, random order<br>
using sqlite3_blob_read().
</center>

<p>
Further performance improves can be made by using the
<a href="https://sqlite.org/mmap.html">memory-mapped I/O</a> feature of SQLite.  In the next chart, the
entire 1GB database file is memory mapped and blobs are read
(in random order) using the <a href="https://sqlite.org/c3ref/blob_read.html">sqlite3_blob_read()</a> interface.
With these optimizations, SQLite is twice as fast as Android
or MacOS-X and over 10 times faster than Windows.

</p><center>
<p><img src="https://sqlite.org/images/faster-read-mmap.jpg">
</p>
<br>
Chart 3:  SQLite read latency relative to direct filesystem reads.<br>
100K blobs, avg size 10KB, random order<br>
using sqlite3_blob_read() from a memory-mapped database.
</center>

<p>
The third chart shows that reading blob content out of SQLite can be
twice as fast as reading from individual files on disk for Mac and
Android, and an amazing ten times faster for Windows.

</p><h2 id="write_performance_measurements"><span>2.2. </span>Write Performance Measurements</h2>

<p>
Writes are slower.
On all systems, using both direct I/O and SQLite, write performance is
between 5 and 15 times slower than reads.

</p><p>
Write performance measurements were made by replacing (overwriting)
an entire blob with a different blob.  All of the blobs in these
experiment are random and incompressible.  Because writes are so much
slower than reads, only 10,000 of the 100,000 blobs in the database
are replaced.  The blobs to be replaced are selected at random and
are in no particular order.

</p><p>
The direct-to-disk writes are accomplished using fopen()/fwrite()/fclose().
By default, and in all the results shown below, the OS filesystem buffers are
never flushed to persistent storage using fsync() or
FlushFileBuffers().  In other words, there is no attempt to make the
direct-to-disk writes transactional or power-safe.
We found that invoking fsync() or FlushFileBuffers() on each file
written causes direct-to-disk storage
to be about 10 times or more slower than writes to SQLite.

</p><p>
The next chart compares SQLite database updates in <a href="https://sqlite.org/wal.html">WAL mode</a>
against raw direct-to-disk overwrites of separate files on disk.
The <a href="https://sqlite.org/pragma.html#pragma_synchronous">PRAGMA synchronous</a> setting is NORMAL.
All database writes are in a single transaction.
The timer for the database writes is stopped after the transaction
commits, but before a <a href="https://sqlite.org/wal.html#ckpt">checkpoint</a> is run.
Note that the SQLite writes, unlike the direct-to-disk writes,
are <a href="https://sqlite.org/transactional.html">transactional</a> and <a href="https://sqlite.org/transactional.html">power-safe</a>, though because the synchronous
setting is NORMAL instead of FULL, the transactions are not durable.

</p><center>
<p><img src="https://sqlite.org/images/faster-write-safe.jpg">
</p>
<br>
Chart 4:  SQLite write latency relative to direct filesystem writes.<br>
10K blobs, avg size 10KB, random order,<br>
WAL mode with synchronous NORMAL,<br>
exclusive of checkpoint time
</center>

<p>
The android performance numbers for the write experiments are omitted
because the performance tests on the Galaxy S3 are so random.  Two
consecutive runs of the exact same experiment would give wildly different
times.  And, to be fair, the performance of SQLite on android is slightly
slower than writing directly to disk.

</p><p>
The next chart shows the performance of SQLite versus direct-to-disk
when transactions are disabled (<a href="https://sqlite.org/pragma.html#pragma_journal_mode">PRAGMA journal_mode=OFF</a>)
and <a href="https://sqlite.org/pragma.html#pragma_synchronous">PRAGMA synchronous</a> is set to OFF.  These settings put SQLite on an
equal footing with direct-to-disk writes, which is to say they make the
data prone to corruption due to system crashes and power failures.

</p><center>
<p><img src="https://sqlite.org/images/faster-write-unsafe.jpg">
</p>
<br>
Chart 5:  SQLite write latency relative to direct filesystem writes.<br>
10K blobs, avg size 10KB, random order,<br>
journaling disabled, synchronous OFF.
</center>

<p>
In all of the write tests, it is important to disable anti-virus software
prior to running the direct-to-disk performance tests.  We found that
anti-virus software slows down direct-to-disk by an order of magnitude
whereas it impacts SQLite writes very little.  This is probably due to the
fact that direct-to-disk changes thousands of separate files which all need
to be checked by anti-virus, whereas SQLite writes only changes the single
database file.

</p><h2 id="variations"><span>2.3. </span>Variations</h2>

<p>The <a href="https://sqlite.org/compile.html#direct_overflow_read">-DSQLITE_DIRECT_OVERFLOW_READ</a> compile-time option causes SQLite
to bypass its page cache when reading content from overflow pages.  This
helps database reads of 10K blobs run a little faster, but not all that much
faster.  SQLite still holds a speed advantage over direct filesystem reads
without the SQLITE_DIRECT_OVERFLOW_READ compile-time option.

</p><p>Other compile-time options such as using -O3 instead of -Os or
using <a href="https://sqlite.org/compile.html#threadsafe">-DSQLITE_THREADSAFE=0</a> and/or some of the other
<a href="https://sqlite.org/compile.html#rcmd">recommended compile-time options</a> might help SQLite to run even faster
relative to direct filesystem reads.

</p><p>The size of the blobs in the test data affects performance.
The filesystem will generally be faster for larger blobs, since
the overhead of open() and close() is amortized over more bytes of I/O,
whereas the database will be more efficient in both speed and space
as the average blob size decreases.


</p><h2 id="general_findings"><span>3. </span>General Findings</h2>

<ol type="A">
<li>
<p>SQLite is competitive with, and usually faster than, blobs stored in
separate files on disk, for both reading and writing.

</p></li><li>
<p>SQLite is much faster than direct writes to disk on Windows
when anti-virus protection is turned on.  Since anti-virus software
is and should be on by default in Windows, that means that SQLite
is generally much faster than direct disk writes on Windows.

</p></li><li>
<p>Reading is about an order of magnitude faster than writing, for all
systems and for both SQLite and direct-to-disk I/O.

</p></li><li>
<p>I/O performance varies widely depending on operating system and hardware.
Make your own measurements before drawing conclusions.

</p></li><li>
<p>Some other SQL database engines advise developers to store blobs in separate
files and then store the filename in the database.  In that case, where
the database must first be consulted to find the filename before opening
and reading the file, simply storing the entire blob in the database
gives much faster read and write performance with SQLite.
See the <a href="https://sqlite.org/intern-v-extern-blob.html">Internal Versus External BLOBs</a> article for more information.
</p></li></ol>


<h2 id="additional_notes"><span>4. </span>Additional Notes</h2>

<h2 id="compiling_and_testing_on_android"><span>4.1. </span>Compiling And Testing on Android</h2>

<p>
The kvtest program is compiled and run on Android as follows.
First install the Android SDK and NDK.  Then prepare a script
named "android-gcc" that looks approximately like this:

</p><div><pre>#!/bin/sh
#
NDK=/home/drh/Android/Sdk/ndk-bundle
SYSROOT=$NDK/platforms/android-16/arch-arm
ABIN=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin
GCC=$ABIN/arm-linux-androideabi-gcc
$GCC --sysroot=$SYSROOT -fPIC -pie $*
</pre></div>

<p>Make that script executable and put it on your $PATH.  Then
compile the kvtest program as follows:

</p><div><pre>android-gcc -Os -I. kvtest.c sqlite3.c -o kvtest-android
</pre></div>

<p>Next, move the resulting kvtest-android executable to the Android
device:

</p><div><pre>adb push kvtest-android /data/local/tmp
</pre></div>

<p>Finally use "adb shell" to get a shell prompt on the Android device,
cd into the /data/local/tmp directory, and begin running the tests
as with any other unix host.
</p><p><small><i>This page last modified on  <a href="https://sqlite.org/docsrc/honeypot" id="mtimelink" data-href="https://sqlite.org/docsrc/finfo/pages/fasterthanfs.in?m=0f7552234f">2023-12-05 14:43:20</a> UTC </i></small></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In the Beginning Was the Command Line (297 pts)]]></title>
            <link>https://web.stanford.edu/class/cs81n/command.txt</link>
            <guid>41084795</guid>
            <pubDate>Sat, 27 Jul 2024 06:25:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://web.stanford.edu/class/cs81n/command.txt">https://web.stanford.edu/class/cs81n/command.txt</a>, See on <a href="https://news.ycombinator.com/item?id=41084795">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Bril: An Intermediate Language for Teaching Compilers (128 pts)]]></title>
            <link>https://www.cs.cornell.edu/~asampson/blog/bril.html</link>
            <guid>41084318</guid>
            <pubDate>Sat, 27 Jul 2024 03:50:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cs.cornell.edu/~asampson/blog/bril.html">https://www.cs.cornell.edu/~asampson/blog/bril.html</a>, See on <a href="https://news.ycombinator.com/item?id=41084318">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<p>When I started a new <a href="https://www.cs.cornell.edu/courses/cs6120/2023fa/">PhD-level compilers course</a> a few years ago,
I thought it was important to use a “hands-on” structure.
There is a big difference between understanding an algorithm on a whiteboard and implementing it, inevitably running into bugs when your implementation encounters real programs.
At the same time, I wanted students to get started quickly, without learning the overwhelming APIs that come with industrial-strength compilers.</p>

<p>I created <a href="https://capra.cs.cornell.edu/bril/">Bril</a>, the Big Red Intermediate Language, to support the class’s implementation projects.
Bril isn’t very interesting from a compiler engineering perspective, but
I think it’s pretty good for the specific use case of teaching compilers classes.
Here’s a factorial program:</p>

<div><pre><code><span>@main</span><span>(</span><span>input</span><span>:</span><span> </span><span>int</span><span>)</span><span> </span><span>{</span><span>
  </span><span>res</span><span>:</span><span> </span><span>int</span><span> </span><span>=</span><span> </span><span>call</span><span> </span><span>@fact</span><span> </span><span>input</span><span>;</span><span>
  </span><span>print</span><span> </span><span>res</span><span>;</span><span>
</span><span>}</span><span>

</span><span>@fact</span><span>(</span><span>n</span><span>:</span><span> </span><span>int</span><span>):</span><span> </span><span>int</span><span> </span><span>{</span><span>
  </span><span>one</span><span>:</span><span> </span><span>int</span><span> </span><span>=</span><span> </span><span>const</span><span> </span><span>1</span><span>;</span><span>
  </span><span>cond</span><span>:</span><span> </span><span>bool</span><span> </span><span>=</span><span> </span><span>le</span><span> </span><span>n</span><span> </span><span>one</span><span>;</span><span>
  </span><span>br</span><span> </span><span>cond</span><span> </span><span>.then</span><span> </span><span>.else</span><span>;</span><span>
</span><span>.then</span><span>:</span><span>
  </span><span>ret</span><span> </span><span>one</span><span>;</span><span>
</span><span>.else</span><span>:</span><span>
  </span><span>decr</span><span>:</span><span> </span><span>int</span><span> </span><span>=</span><span> </span><span>sub</span><span> </span><span>n</span><span> </span><span>one</span><span>;</span><span>
  </span><span>rec</span><span>:</span><span> </span><span>int</span><span> </span><span>=</span><span> </span><span>call</span><span> </span><span>@fact</span><span> </span><span>decr</span><span>;</span><span>
  </span><span>prod</span><span>:</span><span> </span><span>int</span><span> </span><span>=</span><span> </span><span>mul</span><span> </span><span>n</span><span> </span><span>rec</span><span>;</span><span>
  </span><span>ret</span><span> </span><span>prod</span><span>;</span><span>
</span><span>}</span><span>
</span></code></pre></div>

<p>Bril is the only compiler IL I know of that is specifically designed for education.
Focusing on teaching means that Bril prioritizes these goals:</p>

<ul>
  <li>It is fast to get started working with the IL.</li>
  <li>It is easy to mix and match components that work with the IL, including things that fellow students write.</li>
  <li>The semantics are simple, without too many distractions.</li>
  <li>The syntax is ruthlessly regular.</li>
</ul>

<p>Bril is different from other ILs because it prioritizes those goals above other, more typical ones:
code size, compiler speed, and performance of the generated code.</p>

<p>Aside from that inversion of priorities, Bril looks a lot like any other modern compiler IL.
It’s an instruction-based, assembly-like, typed, <a href="https://en.wikipedia.org/wiki/A-normal_form">ANF</a> language.
There’s a quote from <a href="https://en.wikipedia.org/wiki/Why_the_lucky_stiff">why the lucky stiff</a> where he introduces <a href="https://camping.github.io/camping.io/">Camping</a>, the original web microframework, as “a little white blood cell in the vein of Rails.”
If LLVM is an entire circulatory system, Bril is a single blood cell.</p>

<h2 id="bril-is-json">Bril is JSON</h2>

<p>Bril programs are JSON documents.
Here’s how students get started working with Bril code using Python:</p>

<div><pre><code><span>import</span> <span>json</span>
<span>import</span> <span>sys</span>
<span>prog</span> <span>=</span> <span>json</span><span>.</span><span>load</span><span>(</span><span>sys</span><span>.</span><span>stdin</span><span>)</span>
</code></pre></div>

<p>I’m obviously being a little silly here.
But seriously, the JSON-as-syntax idea is in service of the <em>fast to get started</em> and <em>easy to mix and match components</em> goals above.
I wanted Bril to do these things:</p>

<ul>
  <li><strong>Let students use any programming language they want.</strong>
I wanted my compilers course to be accessible to lots of PhD students, including people with only tangential interest in compilers.
Letting them use the languages they’re comfortable with is a great way to avoid any ramp-up phase where students must learn some specific “realistic” compiler implementation language if they don’t already know it.</li>
  <li><strong>No framework is required to get started.</strong>
For the first offering of CS 6120, no libraries existed, and I needed to run the course somehow.
Beyond that practical matter, this constraint is valuable as a complexity limiter:
students can get started with simple stuff without learning any APIs.
These days, Bril does come with libraries that are great for avoiding JSON-handling frustrations when you scale up:
for <a href="https://github.com/sampsyo/bril/tree/main/bril-rs">Rust</a>, <a href="https://github.com/sampsyo/bril/tree/main/bril-ocaml">OCaml</a>, <a href="https://github.com/sampsyo/bril/tree/main/bril-swift">Swift</a>, and <a href="https://github.com/sampsyo/bril/tree/main/bril-ts">TypeScript</a>.
But the fact that they’re not really <em>required</em> keeps the onramps gentle.</li>
  <li><strong>Compose small pieces with Unix pipelines.</strong>
You can wire up Bril workflows with shell pipelines, like <code>cat code.json | my_opt | my_friends_opt | brilck</code>.
I want students in CS 6120 to freely share code with each other and to borrow bits of functionality I wrote.
For a PhD-level class, this trust-based “open-source” course setup makes way more sense to me than a typical undergrad-style class where collaboration is forbidden.
Piping JSON from one tool to the next is a great vehicle for sharing.</li>
</ul>

<p>So, JSON is the canonical form for Bril code.
Here’s a complete Bril program:</p>

<div><pre><code><span>{</span><span>
  </span><span>"functions"</span><span>:</span><span> </span><span>[{</span><span>
    </span><span>"name"</span><span>:</span><span> </span><span>"main"</span><span>,</span><span>
    </span><span>"args"</span><span>:</span><span> </span><span>[],</span><span>
    </span><span>"instrs"</span><span>:</span><span> </span><span>[</span><span>
      </span><span>{</span><span> </span><span>"op"</span><span>:</span><span> </span><span>"const"</span><span>,</span><span> </span><span>"type"</span><span>:</span><span> </span><span>"int"</span><span>,</span><span> </span><span>"dest"</span><span>:</span><span> </span><span>"v0"</span><span>,</span><span> </span><span>"value"</span><span>:</span><span> </span><span>1</span><span> </span><span>},</span><span>
      </span><span>{</span><span> </span><span>"op"</span><span>:</span><span> </span><span>"const"</span><span>,</span><span> </span><span>"type"</span><span>:</span><span> </span><span>"int"</span><span>,</span><span> </span><span>"dest"</span><span>:</span><span> </span><span>"v1"</span><span>,</span><span> </span><span>"value"</span><span>:</span><span> </span><span>2</span><span> </span><span>},</span><span>
      </span><span>{</span><span> </span><span>"op"</span><span>:</span><span> </span><span>"add"</span><span>,</span><span> </span><span>"type"</span><span>:</span><span> </span><span>"int"</span><span>,</span><span> </span><span>"dest"</span><span>:</span><span> </span><span>"v2"</span><span>,</span><span> </span><span>"args"</span><span>:</span><span> </span><span>[</span><span>"v0"</span><span>,</span><span> </span><span>"v1"</span><span>]</span><span> </span><span>},</span><span>
      </span><span>{</span><span> </span><span>"op"</span><span>:</span><span> </span><span>"print"</span><span>,</span><span> </span><span>"args"</span><span>:</span><span> </span><span>[</span><span>"v2"</span><span>]</span><span> </span><span>}</span><span>
    </span><span>]</span><span>
  </span><span>}]</span><span>
</span><span>}</span><span>
</span></code></pre></div>

<p>This program has one function, <code>main</code>, with no arguments and 4 instructions:
two <code>const</code> instructions, an <code>add</code>, and a <code>print</code>.</p>

<p>Even though Bril is JSON, it also has a text form.
I will, however, die on the following hill:
<em>the text form is only a second-class convenience</em>, and it is not the language itself.
The text syntax exists solely to cater to our foibles as humans for whom reading JSON directly is annoying.
Bril itself is the JSON format you see above.
But as a concession to our foibles, among Bril’s many tools are a <a href="https://github.com/sampsyo/bril/blob/main/bril-txt/briltxt.py">parser and pretty-printer</a>.
Here’s the text form of the program above:</p>

<div><pre><code><span>@main</span><span> </span><span>{</span><span>
  </span><span>v0</span><span>:</span><span> </span><span>int</span><span> </span><span>=</span><span> </span><span>const</span><span> </span><span>1</span><span>;</span><span>
  </span><span>v1</span><span>:</span><span> </span><span>int</span><span> </span><span>=</span><span> </span><span>const</span><span> </span><span>2</span><span>;</span><span>
  </span><span>v2</span><span>:</span><span> </span><span>int</span><span> </span><span>=</span><span> </span><span>add</span><span> </span><span>v0</span><span> </span><span>v1</span><span>;</span><span>
  </span><span>print</span><span> </span><span>v2</span><span>;</span><span>
</span><span>}</span><span>
</span></code></pre></div>

<p>As a consequence, working with Bril means typing commands like this a lot:</p>

<div><pre><code>$ bril2json &lt; program.bril | do_something | bril2txt
</code></pre></div>

<p>It can get tedious to constantly convert to and from JSON,
and it’s wasteful to serialize and deserialize programs at each stage in a long pipeline.
But the trade-off is that the Bril ecosystem comprises a large number of small pieces, loosely joined and infinitely remixable on the command line.</p>

<h2 id="language-design-good-bad-and-ugly">Language Design: Good, Bad, and Ugly</h2>

<p>There are a few design decisions in the language itself that reflect Bril’s education-over-practicality priorities.
For instance, <code>print</code> is a <a href="https://capra.cs.cornell.edu/bril/lang/core.html">core opcode</a> in Bril; I don’t think this would be a good idea in most compilers, but it makes it easy to write small examples.</p>

<p>Another quirk is that Bril is <em>extremely</em> <a href="https://en.wikipedia.org/wiki/A-normal_form">A-normal form</a>, to the point that constants always have to go in their own instructions and get their own names.
To increment an integer, for example, you can’t do this:</p>



<p>Instead, Bril code is full of one-off constant variables, like this:</p>

<div><pre><code><span>one</span><span>:</span><span> </span><span>int</span><span> </span><span>=</span><span> </span><span>const</span><span> </span><span>1</span><span>;</span><span>
</span><span>incr</span><span>:</span><span> </span><span>int</span><span> </span><span>=</span><span> </span><span>add</span><span> </span><span>n</span><span> </span><span>one</span><span>;</span><span>
</span></code></pre></div>

<p>This more-ANF-than-ANF approach to constants is verbose to the point of silliness.
But it simplifies the way you write some basic IL traversals because you don’t have to worry about whether operands come from variables or constants.
For many use cases, you get to handle constants the same way you do any other instruction.
For teaching, I think the regularity is worth the silliness.</p>

<p>Bril is extensible, in a loosey-goosey way.
The string-heavy JSON syntax means it’s trivial to add new opcodes and data types.
Beyond the <a href="https://capra.cs.cornell.edu/bril/lang/core.html">core language</a>, there are “official” extensions for <a href="https://capra.cs.cornell.edu/bril/lang/memory.html">manually managed memory</a>, <a href="https://capra.cs.cornell.edu/bril/lang/float.html">floating-point numbers</a>, a funky form of <a href="https://capra.cs.cornell.edu/bril/lang/spec.html">speculation</a> I use for teaching JIT principles, <a href="https://capra.cs.cornell.edu/bril/lang/import.html">module imports</a>, and <a href="https://capra.cs.cornell.edu/bril/lang/char.html">characters</a>.
While a <em>laissez faire</em> approach to extensions has worked so far, it’s also a mess:
there’s no systematic way to tell which extensions a given program uses or which language features a given tool supports.
<a href="https://github.com/sampsyo/bril/issues/38">A more explicit approach to extensibility</a> would make the growing ecosystem easier to manage.</p>

<p>Finally, Bril does not require SSA.
There is <a href="https://capra.cs.cornell.edu/bril/lang/ssa.html">an SSA form</a> that includes a <code>phi</code> instruction, but the language itself has mutable variables.
I wouldn’t recommend this strategy for any other IL, but it’s helpful for teaching for three big reasons:</p>

<ol>
  <li>I want students to feel the pain of working with non-SSA programs before the course introduces SSA. This frustration can help motivate why SSA is the modern consensus.</li>
  <li>The course includes a task where students <a href="https://www.cs.cornell.edu/courses/cs6120/2023fa/lesson/6/#tasks">implement into-SSA and out-of-SSA transformations</a>.</li>
  <li>It’s really easy to generate Bril code from frontend languages that have mutable variables. The alternative would be LLVM’s <a href="https://llvm.org/doxygen/Mem2Reg_8cpp_source.html">mem2reg</a>/”just put all the frontend variables in memory” trick, but Bril avoids building memory into the core language for simplicity.</li>
</ol>

<p>Unfortunately, this aftermarket SSA retrofit has been a huge headache.
It has caused <a href="https://github.com/sampsyo/bril/issues/108">persistent problems with undefinedness</a> and <a href="https://github.com/sampsyo/bril/issues/330">classic correctness problems when translating out of SSA</a>.
I think my original design is fundamentally flawed;
it was a mistake to treat <code>phi</code> semantically as “just another instruction” instead of a more invasive change to the language.
Bril’s SSA form needs a full rework, probably including an actual language extension along the lines of <a href="https://mlir.llvm.org/docs/Rationale/Rationale/#block-arguments-vs-phi-nodes">MLIR’s basic block arguments</a>.
It has been an interesting lesson for me that SSA comes with subtle design implications that are difficult to retrofit onto an existing mutation-oriented IL.</p>

<h2 id="the-bril-ecosystem">The Bril Ecosystem</h2>

<p><img src="https://www.cs.cornell.edu/~asampson/media/bril/ecosystem.svg"></p>

<p>I cobbled together the first version of Bril in a hurry in the weeks before the fall 2019 semester began.
Since then, via the “open-source class” nature of <a href="https://www.cs.cornell.edu/courses/cs6120/2023fa/">CS 6120</a>, students have contributed a host of tools for working with the language.
The diagram above shows a sampling of what is in <a href="https://github.com/sampsyo/bril">the monorepo</a>;
empty boxes are things I made and shaded boxes are things students contributed.
One <a href="https://agentcooper.io/">satisfied CS 6120 customer</a>
built a snazzy <a href="https://agentcooper.github.io/bril-playground/">web playground</a> that I find super impressive.
You can find many more random tools by <a href="https://github.com/search?q=bril+compiler&amp;type=repositories">searching on GitHub</a>.</p>

<p>Most of the language extensions I mentioned were contributed by CS 6120 students.
In the run-up to the first semester, I was low on time and left memory, function calls, and floating-point numbers as “exercises for the reader.”
You can read 2019 blog posts <a href="https://www.cs.cornell.edu/courses/cs6120/2019fa/blog/manually-managed-memory/">by Drew Zagieboylo &amp; Ryan Doenges about the memory extension</a>,
<a href="https://www.cs.cornell.edu/courses/cs6120/2019fa/blog/function-calls/">by Alexa VanHattum &amp; Gregory Yauney about designing function calls</a>,
and <a href="https://www.cs.cornell.edu/courses/cs6120/2019fa/blog/floats-static-arrays/">by Dietrich Geisler about floats</a>.
Laziness can pay off.</p>

<p>Please <a href="https://discuss.systems/@adrian">get in touch</a> if you’re using Bril for something unconventional!
I love learning about the weird places it has gone.</p>


</article></div>]]></description>
        </item>
    </channel>
</rss>