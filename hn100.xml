<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 23 Mar 2025 13:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Improving recommendation systems and search in the age of LLMs (213 pts)]]></title>
            <link>https://eugeneyan.com/writing/recsys-llm/</link>
            <guid>43450732</guid>
            <pubDate>Sun, 23 Mar 2025 03:40:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eugeneyan.com/writing/recsys-llm/">https://eugeneyan.com/writing/recsys-llm/</a>, See on <a href="https://news.ycombinator.com/item?id=43450732">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            



<!--https://docs.mathjax.org/en/latest/input/tex/delimiters.html-->

<p>Recommendation systems and search have historically drawn inspiration from language modeling. For example, the adoption of <a href="https://arxiv.org/abs/2009.12192" target="_blank">Word2vec</a> to learn item embeddings (for embedding-based retrieval), and using <a href="https://arxiv.org/abs/1511.06939" target="_blank">GRUs</a>, <a href="https://arxiv.org/abs/1905.06874" target="_blank">Transformer</a>, and <a href="https://arxiv.org/abs/1904.06690" target="_blank">BERT</a> to predict the next best item (for ranking). The current paradigm of large language models is no different.</p>

<p>Here, we’ll discuss how industrial search and recommendation systems have evolved over the past year or so and cover model architectures, data generation, training paradigms, and unified frameworks:</p>
<ul>
  <li><a href="#llmmultimodality-augmented-model-architecture">LLM/multimodality-augmented model architecture</a></li>
  <li><a href="#llm-assisted-data-generation-and-analysis">LLM-assisted data generation and analysis</a></li>
  <li><a href="#scaling-laws-transfer-learning-distillation-loras">Scaling Laws, transfer learning, distillation, LoRAs, etc.</a></li>
  <li><a href="#unified-architectures-for-search-and-recommendations">Unified architectures for search and recommendations</a></li>
</ul>

<h2 id="llmmultimodality-augmented-model-architecture">LLM/multimodality-augmented model architecture</h2>

<p>Recommendation models are increasingly adopting language models and multimodal content to overcome traditional limitations of ID-based approaches. These hybrid architectures include content understanding alongside the strengths of behavioral modeling, addressing the common challenges of cold-start and long-tail item recommendations.</p>

<p><strong><a href="https://arxiv.org/abs/2306.08121" target="_blank">Semantic IDs (YouTube)</a> explores content-derived features as substitutes for traditional hash-based IDs.</strong> This approach targets difficulties in predicting user preferences for new and infrequently interacted items. Their solution involves a two-stage framework.</p>

<p>In the first stage, a transformer-based video encoder (similar to Video-BERT) generates dense content embeddings. These embeddings are then compressed into discrete Semantic IDs through a Residual Quantization Variational AutoEncoder (RQ-VAE). Representing user histories with these compact semantic IDs—a few integers rather than high-dimensional embeddings—significantly improves efficiency. Once trained, the RQ-VAE is frozen and used to generate Semantic IDs for the second stage to train a production-scale ranking model.</p>

<p>The RQ-VAE itself is a single-layer encoder-decoder structure with a 256-dimensional latent space. It has eight quantization levels with a codebook of 2048 entries per level. The encoder maps content embeddings to a latent vector, while a residual quantizer discretizes this vector, and the decoder reconstructs the original embedding. The initial embeddings originate from a transformer with a VideoBERT backbone, producing detailed, 2048-dimensional representations that capture the topical content in video.</p>

<p><img src="https://eugeneyan.com/assets/semantic-ids-fig1.jpg" loading="lazy" title="Semantic IDs" alt="Semantic IDs"></p>

<p>To integrate Semantic IDs into ranking models, the authors propose two techniques: an N-gram-based approach, which groups fixed-length sequences, and a SentencePiece Model (SPM)-based method that adaptively learns variable-length subwords. The ranking model is a multi-task production ranking model that recommends the next video to watch given the current video and user history.</p>

<p><strong>Results:</strong> Directly using the dense content embeddings performed worse than using random hash IDs. The authors hypothesize that ranking models heavily rely on memorization from the ID-based embedding tables—replacing these with <em>fixed</em> dense content embeddings led to poorer CTR. However, both N-gram and SPM methods did better than random hashing, especially in cold-start scenarios. Ablation tests revealed that while N-gram approaches had a slight advantage when embedding table sizes were limited (e.g., $8 \times K$ or $4 \times K^2$), SPM methods offered superior generalization and efficiency with larger embedding tables.</p>

<p><img src="https://eugeneyan.com/assets/semantic-ids-fig2.jpg" loading="lazy" title="Semantic IDs" alt="Semantic IDs"></p>
<p>Dense content embeddings (dashed lines) perform worse than random hashing (solid orange).</p>

<p>Similarly,&nbsp;<strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688098" target="_blank">M3CSR (Kuaishou)</a> introduces multimodal content embeddings (visual, textual, audio) clustered via K-means into trainable category IDs.</strong> This transforms static content embeddings into adaptable, behavior-aligned representations.</p>

<p>The M3CSR framework has a dual-tower architecture, splitting user-side and item-side towers to optimize for online inference efficiency where user and item embeddings can be pre-computed and indexed via approximate nearest neighbor indices. Item embeddings are derived from multimodal pretrained models—ResNet for visual, Sentence-BERT for text, and VGGish for audio—and concatenated into a single embedding vector. These vectors are then clustered using K-means (with approximately 1,000 clusters from over 10 million videos).</p>

<p><img src="https://eugeneyan.com/assets/m3csr-fig2.jpg" loading="lazy" title="M3CSR" alt="M3CSR"></p>

<p>Next, cluster IDs are embedded through a Modal Encoder, a dense network translating content features into behaviorally aligned spaces and assigning trainable embeddings. The Modal Encoder uses a dense network to learn the mapping from content-space to behavior space and a cluster ID lookup to assign a trainable cluster ID embedding.</p>

<p><img src="https://eugeneyan.com/assets/m3csr-fig3.jpg" loading="lazy" title="M3CSR" alt="M3CSR"></p>

<p>On the user side, M3CSR learns on user behavior sequences to train sequential models that capture user preferences. In addition, to accurately model user modality preferences, the framework concatenates general behavioral interests with modality-specific interests. These modality-specific interests are derived by converting item IDs back into their multimodal embeddings using the same Modal Encoder.</p>

<p><strong>Results:</strong> M3CSR outperformed several multimodal baselines such as VBPR, MMGCN, and LATTICE. Ablation studies highlighted the importance of modeling modality-specific user interests and demonstrated consistent superiority of multimodal features over single-modal features across datasets (Amazon, TikTok, Allrecipes). A/B testing measured that clicks increased by 3.4%, likes by 3.0%, and follows by 3.1%. In cold-start scenarios, M3CSR also showed improved performance, achieving a 1.2% boost in cold-start velocity and a 3.6% increase in cold-start video coverage.</p>

<p><strong><a href="https://arxiv.org/abs/2310.19453" target="_blank">FLIP (Huawei)</a> shows how to align ID-based recommendation models with LLMs by jointly learning from masked tabular and language data.</strong> The core idea is to reconstruct masked features from one modality (user and item IDs) using information from another modality (text tokens), ensuring tight cross-modal alignment.</p>

<p>FLIP operates in three stages: modality transformation, modality alignment pretraining, and adaptive finetuning. First, tabular data is translated into text using structured prompt templates. Then, joint masked language/tabular modeling is conducted to achieve fine-grained alignment between modalities. During pretraining, textual data undergoes field-level masking (replacing entire fields with <code>[MASK]</code> tokens), while corresponding tabular features are masked by substituting feature IDs with <code>[MASK]</code>.</p>

<p>FLIP trains two parallel models with three objectives: (i) Masked Language Modeling (MLM) predicts masked text tokens using complete tabular context; (ii) Masked Tabular Modeling (MTM) predicts masked feature IDs leveraging textual data; and (iii) Instance-level Contrastive Learning (ICL) aligns global representations across modalities.</p>

<p><img src="https://eugeneyan.com/assets/flip-fig1.jpg" loading="lazy" title="FLIP" alt="FLIP"></p>

<p>Finally, the aligned models—TinyBERT as the LLM and DCNv2 as the ID-based model—are finetuned on the downstream click-through rate (CTR) prediction task. To do this, FLIP adds randomly initialized output layers on both models to estimate click probabilities. The final prediction is a weighted sum of both models’ outputs, where the weights are learned adaptively during training.</p>

<p><img src="https://eugeneyan.com/assets/flip-fig2.jpg" loading="lazy" title="FLIP" alt="FLIP"></p>

<p><strong>Results:</strong> FLIP outperforms the baselines of ID-only, LLM-only, and ID+LLM models. Ablation studies show that (i) both MLM and MTM objectives improve performance, (ii) field-level masking is more effective than random token masking, and (iii) joint reconstruction between modalities is key.</p>

<p>Similarly, <strong><a href="https://dl.acm.org/doi/10.1145/3523227.3551482" target="_blank">beeFormer</a> demonstrates how to train language-only Transformers on user-item interaction data enriched with textual information.</strong> The goal is to bridge the gap between semantic similarity (from textual data) and interaction-based similarity (from user behavior).</p>

<p>beeFormer combines a sentence Transformer encoder for item embeddings with an <a href="https://dl.acm.org/doi/10.1145/3523227.3551482" target="_blank">ELSA (scalabl<strong>E</strong> <strong>L</strong>inear <strong>S</strong>hallow <strong>A</strong>utoencoder)</a>-based decoder that captures patterns from user-item interactions. First, item embeddings are generated through a Transformer trained on textual data. These embeddings are then used to compute user recommendations via ELSA’s low-rank approximation of item-to-item weight. The key here is to backpropagate the gradients from the recommendation loss through the Transformer model. As a result, weight updates capture interaction patterns rather than just semantic similarity.</p>

<p><img src="https://eugeneyan.com/assets/beeformer-fig1.jpg" loading="lazy" title="beeFormer" alt="beeFormer"></p>

<p>To make training computationally feasible on large catalogs, beeFormer applies gradient checkpointing to manage memory usage, gradient accumulation for larger effective batch sizes, and negative sampling to focus training efficiently on relevant items.</p>

<p><strong>Results:</strong> Offline evaluations show that beeFormer surpasses baseline models like mpnet-base-v2 and bge-m3. However, the comparison is limited (and IMHO unfair) since the baselines weren’t finetuned on the training dataset. Interestingly, models trained across multiple domains (movies + books) performed better than domain-specific ones, suggesting that there was transfer learning across domains.</p>

<p><strong><a href="https://arxiv.org/abs/2405.02429" target="_blank">CALRec (Google)</a> introduces a two-stage framework that finetunes a pretrained LLM (PaLM-2 XXS) for sequential recommendations.</strong> Both user interactions and model predictions are represented entirely through text.</p>

<p>First, all input (e.g., user-item interactions) is converted into text sequences by concatenating meaningful attributes (title, category, brand, price) into structured textual prompts. Attributes are formatted in the style of “Attribute name: Attribute description” and concatenated. At the end of the user history sequence, they append the item prefix, thus prompting the LLM to predict the user’s next purchase as a sentence completion task.</p>

<p><img src="https://eugeneyan.com/assets/calrec-fig2.jpg" loading="lazy" title="CALRec" alt="CALRec"></p>

<p>CALRec has a two-stage finetuning approach. The first stage involves multi-category training to adapt the model to sequential recommendation patterns in a category-agnostic way. The second stage refines the model within specific item categories. The training objective combines next-item generation tasks (predicting textual descriptions of items) with auxiliary contrastive alignment. The former aims to generate the text description of the target item given the user’s history; the latter applies contrastive loss on the output of the separate user and item towers to align user history to target item representations.</p>

<p><img src="https://eugeneyan.com/assets/calrec-fig1.jpg" loading="lazy" title="CALRec" alt="CALRec"></p>

<p>During inference, the model is prompted to generate multiple candidates via temperature sampling. They remove duplicates, sort by the output’s log probabilities in descending order, and keep the top k candidates. Then, these textual predictions are matched to catalog items via BM25 and sorted by the matching scores.</p>

<p><strong>Results:</strong> On the Amazon Review Dataset 2018, CALRec outperforms ID-based and text-based baselines (e.g., SASRec, BERT4Rec, FDSA, UniSRec). While the evaluation dataset is limited, CalRec beating the baselines is promising. Ablations demonstrate the necessity of both training stages, especially highlighting transfer learning benefits from multi-category training and incremental gains (0.8 - 1.7%) from contrastive alignment.</p>

<p><strong><a href="https://arxiv.org/abs/2405.11441" target="_blank">EmbSum (Meta)</a> presents a content-based recommendation approach using precomputed textual summaries of user interests and candidate items</strong> to capture interactions within the user engagement history.</p>

<p>EmbSum uses T5-small (61M parameters) to encode user interactions and candidate content, managing long user histories by partitioning them into sessions for encoding. Then, Mixtral-8x22B-Instruct generates the interpretable user interest summaries from user histories. These summaries are then fed into the T5’s encoder to derive final embeddings.</p>

<p><img src="https://eugeneyan.com/assets/embsum-fig1.jpg" loading="lazy" title="EmbSum" alt="EmbSum"></p>

<p>Key to this architecture are User Poly-Embeddings (UPE) and Content Poly-Embeddings (CPE). To get a global representation for UPE, they take the last token of the decoder output (<code>[EOS]</code>) and concatenate it with the representation vectors from the session encoder. This combined representation passes through a poly-attention layer which distills nuanced user interests into multiple embeddings. EmbSum training combines noisy contrastive estimation loss and summarization loss, ensuring high-quality user embeddings.</p>

<p><strong>Results:</strong> EmbSum beats several state-of-the-art content-based recommenders. <em>Nonetheless, direct comparisons with behavioral recommenders were glaringly absent.</em> Ablation studies show that CPE contributes most to performance, followed by session-based grouping and encoding, user poly-embeddings, and summarization losses. Additionally, GPT-4 evaluations indicate strong interpretability and quality of generated user interest summaries.</p>

<p>• • •</p>

<h2 id="llm-assisted-data-generation-and-analysis">LLM-assisted data generation and analysis</h2>

<p>Another common theme is using LLMs to enrich data. Several papers share about using LLMs to tackle data scarcity and enhance the quality of search and recommendations. Examples include generating webpage metadata at Bing, creating synthetic training data to identify poor job matches at Indeed, adding semantic labels for query understanding at Yelp, crafting exploratory search queries at Spotify, and enriching music playlist metadata at Amazon.</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688062" target="_blank">Recommendation Quality Improvement (Bing)</a> shares how Bing improved webpage recommendations by using LLMs to generate high-quality metadata</strong> and training an LLM to predict clicks and quality.</p>

<p>Previously, Bing’s webpage representations relied on extractive summaries, which often caused query classification failures. To address this, they used GPT-4 to generate high-quality titles and snippets from full webpage content for two million pages. Then, for efficient large-scale deployment, they finetuned a Mistral-7B model using this GPT-4-generated data.</p>

<p>To improve webpage-to-webpage recommendation rankings, they finetuned a multitask MiniLM-based cross-encoder on both pairwise click predictions <em>and</em> quality classification tasks. The resulting quality scores were then linearly combined with click predictions from an existing LightGBM ranker.</p>

<p><img src="https://eugeneyan.com/assets/bing-fig2.jpg" loading="lazy" title="Recommendation Quality Improvement" alt="Recommendation Quality Improvement"></p>
<p>The MiniLM (right) is ensembled with the LightGBM ranker (left).</p>

<p>To better understand user preferences, they defined 16 distinct recommendation scenarios reflecting common user patterns. Using high-precision prompts, they classified each webpage-to-webpage recommendation, incorporating the enhanced title and snippets from Mistral-7B, into these scenarios. Then, by monitoring the distribution changes of each scenario, they quantified the improvements in webpage recommendation quality.</p>

<p><img src="https://eugeneyan.com/assets/bing-table4.jpg" loading="lazy" title="Recommendation Quality Improvement" alt="Recommendation Quality Improvement"></p>

<p><strong>Results:</strong> The enhanced system reduced clickbait by 31%, low-authority content by 35%, and duplicate content by 76%. At the same time, higher authority content increased by 18%, cross-medium recommendations rose by 48%, and recommendations with greater specificity improved by 20%. This is despite lower-quality content (e.g., clickbait) historically showing higher CTR, demonstrating the effectiveness of the quality-focused cross-encoder.</p>

<p>(👉 Recommended read) <strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688043" target="_blank">Expected Bad Match (Indeed)</a> shares how they used LLM-generated labels to filter poor job matches.</strong> Specifically, they finetuned LLMs to evaluate recommendation quality and generate labels for a post-processing classifier.</p>

<p>They started with building an evaluation set by cross-reviewing 250 matches, narrowing it down to 147 high confidence labeled examples. Then, they prompted various LLMs, such as Llama2 and Mistral-7B, using expert recruitment guidelines to evaluate match quality across dimensions like job descriptions, resumes, and user interactions. However, these models struggled with detailed prompts, producing generalized assessments that didn’t consider detailed job and job seeker information. On the other hand, GPT-4 performed better but was prohibitively expensive.</p>

<p>To balance cost and effectiveness, the team finetuned GPT-3.5 on a curated dataset of over 200 human-reviewed GPT-4 responses. This finetuned GPT-3.5 matched GPT-4’s performance at just a quarter of the cost and latency. But despite the improvements, its inference latency of 6.7 seconds remained too high for online use. Thus, they trained a lightweight classifier, eBadMatch, using LLM-generated labels and categorical features from job descriptions, resumes, and user activity. In production, a daily pipeline samples job matches, engineers features, anonymizes data, generates LLM labels, and retrains the model. This classifier acts as a post-processing filter to remove low-quality matches.</p>

<p><strong>Results:</strong> The eBadMatch classifier achieved an AUC-ROC of 0.86 against LLM labels, with latency suitable for real-time filtering. Online experiments demonstrated that applying a 20% threshold filter on invitation-to-apply emails reduced batch matches by 17.68%, lowered unsubscribe rates by 4.97%, and increased application rates by 4.13%. Similar improvements were observed in homepage recommendation feeds.</p>

<p><img src="https://eugeneyan.com/assets/ebadmatch-table2.jpg" loading="lazy" title="Expected Bad Match" alt="Expected Bad Match"></p>

<p>(👉 Recommended read) <strong><a href="https://engineeringblog.yelp.com/2025/02/search-query-understanding-with-LLMs.html" target="_blank">Query Understanding (Yelp)</a> shows how they integrated LLMs into their query understanding pipeline</strong> to improve query segmentation and review highlights.</p>

<p>Query segmentation identifies meaningful parts of user queries—such as topic, name, time, location, and question—and tags them accordingly. Along the way, they learned that spelling correction and segmentation could be done together and thus added a meta tag to mark spell-corrected sections and combined both tasks into a single prompt. Retrieval-augmented generation (RAG) further improved segmentation accuracy by incorporating business names and categories as context that disambiguated user intent. For evaluation, they compared LLM-identified segments against human-labeled datasets of name match and location intent.</p>

<p>Review highlights selects key snippets from reviews to highlight in search results. They used LLMs to generate synonymous phrases suitable for highlights. Curated examples prompted LLMs to replicate human reasoning in phrase expansion. RAG further enhanced relevance by augmenting the input with relevant business categories to guide phrase generation. Offline evaluation was done via human annotators before online A/B testing of the new highlight phrases. To scale efficiently and cover 95% of traffic, Yelp pre-computed snippet expansions using batch calls to OpenAI and stored them in key-value stores to reduce latency.</p>

<p><img src="https://eugeneyan.com/assets/yelp-fig3.jpg" loading="lazy" title="Review highlights" alt="Review highlights"></p>

<p>The team shared their approach—from initial formulation and proof of concept (POC) to scaling up. Initially, they assessed LLM suitability and defined the project’s scope. During POC, they leveraged the power-law distribution of queries, caching pre-computed LLM responses for common queries covering most traffic. To scale, they created golden datasets using GPT-4 outputs and finetuned smaller, cost-effective models like GPT-4o-mini. Additionally, real-time models like BERT and T5 addressed less frequent, long-tail queries.</p>

<p><strong>Results:</strong> Yelp’s query segmentation significantly improved location intent detection, while enhanced review highlights increased both session and search click-through rates (CTR), especially benefiting long-tail queries.</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688035" target="_blank">Query Recommendations (Spotify)</a> details how they built a hybrid query recommendation system to suggest exploratory search queries</strong> alongside direct results. This approach was necessary to support Spotify’s expansion beyond music to podcasts, audiobooks, and diverse content types by helping users explore those content.</p>

<p><img src="https://eugeneyan.com/assets/query-recs-fig1.jpg" loading="lazy" title="Query Recommendations" alt="Query Recommendations"></p>

<p>Spotify generated query suggestions by (i) extracting from catalog titles, playlist names, and podcasts, (ii) mining suggestions from search logs, (iii) leveraging users’ recent searches, (iv) applying metadata and expansion rules (e.g., “artist name” + “covers”), and (v) generating synthetic natural language queries via LLMs. To generate synthetic queries, techniques such as Doc2query and InPars were used to broaden query variations, enhancing exploratory searches and mitigating retrievability bias.</p>

<p>The query suggestions were then combined with regular results and ranked by a point-wise ranker optimized for downstream user actions like streaming or adding content to playlists. The ranker use features such as lexical matching, query statistics, retrieval scores, and user consumption patterns. For personalization, they relied on vector representations of users and query suggestion candidates.</p>

<p><strong>Results:</strong> Spotify saw a 9% increase in exploratory intent queries, a 30% rise in maximum query length per user, and a 10% increase in average query length—this suggests the query recommendation updates helped users express more complex intents. An online ablation showed the ranker’s removal caused a 20% decline in clicks on recommendations, underscoring its importance.</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688047" target="_blank">Playlist Search (Amazon)</a> discusses Amazon’s integration of LLMs into playlist search pipelines to tackle challenges</strong> like data scarcity, metadata enrichment, and scalable evaluation while reducing reliance on manual annotation.</p>

<p>To enrich metadata, they used LLMs (LLM curator) to create detailed descriptions for community playlists based on their initial 15 tracks, capturing themes, genres, activities, and artists. (These community playlists typically only had a playlist title.) This addressed data scarcity in community-generated content. Then, Flan-T5-XL was finetuned to scale this inference process.</p>

<p>They also applied LLMs to generate synthetic queries paired with playlists (and associated metadata) to create training data for bi-encoder models. These pairs were generated and scored by an LLM (LLM labeler) to maintain balanced positive and negative examples. Lastly, they used an LLM (LLM judge), guided by human annotations and careful prompting to ensure alignment, to streamline evaluations.</p>

<p><img src="https://eugeneyan.com/assets/playlist-fig1.jpg" loading="lazy" title="Playlist Search" alt="Playlist Search"></p>

<p><strong>Results:</strong> Integrating LLMs led to substantial double-digit recall improvements across benchmarks, SEO, and paraphrasing datasets. Overall, the use of LLMs helped overcome the challenges of data scarcity and evaluation scalability without extensive manual effort.</p>

<p>• • •</p>

<h2 id="scaling-laws-transfer-learning-distillation-loras">Scaling Laws, transfer learning, distillation, LoRAs</h2>

<p>Another trend is the adoption of training approaches from large language models (LLMs) and computer vision into recommender systems. This includes exploring scaling laws (how model size and data quantity affect performance), using knowledge distillation to transfer insights from large models to smaller, efficient ones, applying cross-domain transfer learning to handle limited data, and parameter-efficient fine-tuning techniques such as LoRAs.</p>

<p>(👉 Recommended read)&nbsp;<strong><a href="https://arxiv.org/abs/2311.11351" target="_blank">Scaling Laws</a> investigates how the performance of ID-based sequential recommender models improve as their model size and data scale increase.</strong> The authors uncovered a predictable power-law relationship where performance consistently improves as the size of both models and datasets expands.</p>

<p>They adopt a decoder-only transformer architecture, experimenting with models ranging from 98.3K to 0.8B parameters. They evaluated these models on the MovieLens-20M and Amazon-2018 datasets. For the Amazon dataset, interaction records from 29 domains were combined, sorted chronologically, and simplified to include only item IDs without additional metadata. The datasets were then formatted into fixed-length sequences of 50 items each; shorter sequences were padded and longer ones were truncated. The model is then optimized to predict the next item at time step $t + 1$ conditioned on the previous $t$ items.</p>

<p><img src="https://eugeneyan.com/assets/scaling-fig1.jpg" loading="lazy" title="Scaling Laws" alt="Scaling Laws"></p>

<p>To tackle instability in training larger models, the authors introduced two key improvements. First, they implemented layer-wise adaptive dropout, applying higher dropout rates in lower layers and lower dropout rates in upper layers. The intuition is that lower layers process direct input from data and are more prone to overfitting. Conversely, higher layers build more abstract representations and thus benefit from less dropout to reduce information loss that could lead to underfitting.</p>

<p>The second improvement was dynamically switching optimizers during training—starting with Adam before switching to stochastic gradient descent (SGD) at a predefined point. This approach is motivated by the observation that Adam quickly reduces loss in early training phases but ultimately SGD achieves better convergence.</p>

<p><strong>Results:</strong> Unsurprisingly, increased model capacity (excluding embedding parameters) consistently reduced cross-entropy loss. They modeled this with a power-law curve and accurately predicted performance for larger models (75.5M and 0.8B params). Similarly, they observed that larger models could achieve lower losses even with smaller datasets, whereas smaller models needed more data to reach comparable performance. For example, a smaller 98.3K-parameter model required twice the data (18.5M interactions) compared to a larger 75.5M-parameter model (9.2M interactions) to attain similar performance.</p>

<p><img src="https://eugeneyan.com/assets/scaling-fig2.jpg" loading="lazy" title="Scaling Laws" alt="Scaling Laws"></p>

<p>Regarding data repetition, models of sizes 75.5M and 98.3K parameters continued improving beyond a single training epoch, with notable gains observed from two to five epochs. Surprisingly, changing model shape had minimal impact on performance. Ablation studies showed that layer-wise adaptive dropout and optimizer switching substantially enhanced performance in larger models (24 layers), though smaller models (2 layers) remained largely unaffected. Further ablations on five challenging recommendation tasks highlighted the advantage of larger models, particularly for long-tail items and cold-start users.</p>

<p><strong><a href="https://arxiv.org/abs/2401.01497" target="_blank">PrepRec</a> shows how pretraining can be adapted to recommender systems, enabling cross-domain, zero-shot recommendations.</strong> The key innovation is leveraging item popularity dynamics derived solely from user interactions, without relying on item metadata.</p>

<p>PrepRec uses popularity statistics calculated over coarse (monthly) and fine (weekly) timescales. These popularity metrics are converted into percentiles and then encoded into vector representations. In addition, the model incorporates relative time intervals between user interactions and uses a fixed positional encoding for each interaction in a user’s sequence. (IMHO, while the approach is effective, it relies on several specialized techniques—coarse vs. fine-grained periods, relative time intervals, and positional encodings—which might limit its generalizability.)</p>

<p><img src="https://eugeneyan.com/assets/preprec-fig2.jpg" loading="lazy" title="PrepRec" alt="PrepRec"></p>

<p>For training, PrepRec has binary cross-entropy as the objective and uses Adam for optimization. The model and baselines have consistent settings: embedding dimension of 50, max sequence length of 200, and batch size of 128. During inference, PrepRec calculates item popularity dynamics from the target domain before generating recommendations via inference on the pretrained model.</p>

<p><strong>Results:</strong> PrepRec achieves promising zero-shot performance, with only a minor reduction (2-6% recall@10) compared to models like SasREC and BERT4Rec which were specifically trained on the target domains. When trained from scratch on the target domains, PrepRec matches or slightly surpasses these models in regular sequential recommendations despite using just 1-5% of their parameters, thanks to not having item-specific embeddings. Ablations showed that modeling relative time intervals significantly boosted performance, and capturing both coarse and fine-grained popularity trends was essential for tracking evolving user interests.</p>

<p><strong><a href="https://arxiv.org/abs/2408.16238" target="_blank">E-CDCTR (Meituan)</a> demonstrates the potential of transfer learning by using organic item data to improve click-through rate (CTR) predictions in advertising</strong>, tackling the challenge of sparse ad data.</p>

<p>E-CDCTR has three components: the tiny pretraining model (TPM), complete pretraining model (CPM), and advertising CTR model (A-CTR). The TPM, a lightweight model with just embedding and MLP layers, trains monthly on six months of organic impressions and clicks. It captures long-term collaborative filtering signals via historical user and item embeddings. Features include user and item IDs, category IDs, etc.</p>

<p><img src="https://eugeneyan.com/assets/e-cdctr-fig2.jpg" loading="lazy" title="E-CDCTR" alt="E-CDCTR"></p>

<p>Next, the CPM pretrains a CTR model weekly using the most recent month’s organic data and using the user and item embeddings learned by TPM. Finally, the A-CTR model is initialized from the CPM and finetuned daily on advertising-specific data. A-CTR also uses user and item embeddings from the TPM. A-CTR also uses richer features such as user behavior sequences, user context, item metadata, and feature interactions, resulting in a more sophisticated model architecture that includes sequential input, feature crosses, and a larger MLP layer.</p>

<p>For online inference, E-CDCTR employs user and item embeddings generated by TPM from the past three months. The A-CTR model then uses these embeddings to predict the advertising CTR. (The authors mention using self-attention to combine embeddings but provide limited details on training it.)</p>

<p><strong>Results:</strong> E-CDCTR outperforms cross-domain baselines such as KEEP, CoNet, DARec, and MMoE. Ablation studies confirm the value of both TPM and CPM, with CPM having a more substantial impact. In addition, extending historical embeddings from one to three months further enhanced performance, whereas simply merging advertising data with organic data did not yield improvements.</p>

<p><strong><a href="https://arxiv.org/abs/2408.14678" target="_blank">Bridging the Gap (YouTube)</a> shares insights on applying knowledge distillation in large-scale personalized video recommendations at YouTube.</strong></p>

<p>Their recommenders are multi-objective pointwise models for ranking videos. These models simultaneously optimizing short-term objectives like video CTR and long-term objectives like the estimated long-value value of a user. Their models typically feature a teacher-student setup, with the teacher and student models sharing similar architectures though the teacher model is 2 - 4x larger than the student model.</p>

<p>However, distribution shifts between teacher and student can cause biases. To address this, the authors propose an auxiliary distillation strategy—instead of directly using the teacher’s predictions (soft labels), they decouple the hard labels from the soft teacher predictions via separate task logits. This enables the student model to effectively learn from the teacher without inheriting unwanted biases.</p>

<p><img src="https://eugeneyan.com/assets/bridge-fig2.jpg" loading="lazy" title="Bridging the gap" alt="Bridging the gap"></p>

<p>To amortize the cost of training the large teacher model, they have a single teacher improve multiple student models. As a result, a single teacher model can provide distilled knowledge to various specialized recommendation tasks, reducing redundancy and computational overhead. Teacher labels are stored in a columnar database that prioritizes read performance for the students during training.</p>

<p><img src="https://eugeneyan.com/assets/bridge-fig3.jpg" loading="lazy" title="Bridging the gap" alt="Bridging the gap"></p>

<p><strong>Results:</strong> The auxiliary distillation strategy delivered a 0.4% improvement in E(LTV) prediction compared to direct distillation methods, which performed similarly to models without distillation. This confirms the auxiliary distillation approach’s effectiveness in reducing teacher noise. In ablation studies on teacher size, even a modest teacher (2x the student’s size) led to meaningful improvements (+0.42% engagement, +0.34% satisfaction) while a 4x teacher led to +0.43% engagement and +0.46% satisfaction.</p>

<p>Similarly, <strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688041" target="_blank">Self-Auxiliary Distillation (Google)</a> introduces a distillation framework aimed at improving sample efficiency for large-scale recommendation models.</strong></p>

<p>The core idea is to prioritize training on high-quality labels while improving the resolution of lower-quality labels. The intuition is that positive labels provide more signal than negative labels when predicting CTR, thus it makes sense to prioritize them. On the other hand, negative labels are closer to weak positives than an absolute zero—thus, representing them with an estimated CTR value offers better training signal.</p>

<p>The model has a shared bottom tower with two heads: the main head (teacher) is trained directly on ground-truth labels, serving as the primary inference model and generating calibrated soft labels. Calibration is maintained by ensuring the mean prediction matches the mean of actual labels. The auxiliary head (student) learns from a mixture of these soft teacher labels and original labels, helping stabilize the training process. Specifically, the auxiliary head has a bilateral branch where one branch distills knowledge from the teacher’s soft labels and the other learns from the hard ground-truth label. A selector merges the labels from both branches using functions such as $max(y, y’)$.</p>

<p><img src="https://eugeneyan.com/assets/selfaux-fig1.jpg" loading="lazy" title="Self-auxiliary distillation" alt="Self-auxiliary distillation"></p>

<p><strong>Results:</strong> Self-attention distillation consistently improved recommendation quality across multiple domains including apps, commerce, and video recommendations. Ablations show that training on original ground-truth labels primarily drives performance gains, while the distillation component significantly stabilizes and aligns the model’s predictions. Training exclusively on ground-truth labels showed inconsistent results while training on the distillation labels only didn’t lead to improvements.</p>

<p><strong><a href="https://arxiv.org/abs/2405.00338" target="_blank">DLLM2Rec</a> shows how to distill recommendation knowledge from LLMs into lightweight, conventional sequential recommendation models</strong>, making deployment more practical. The paper identifies three main challenges: (i) unreliable teacher knowledge/labels, (ii) the capability gap between teacher and student models, and (iii) semantic divergence between the teacher’s and student’s embedding spaces.</p>

<p>To tackle these issues, DLLM2Rec adopts two key strategies: importance-aware ranking distillation and collaborative embedding distillation. Importance-aware ranking distillation focuses on selecting reliable instances for training via importance weights. These weights consider factors like ranking position (prioritizing items ranked higher by the teacher), teacher confidence (evaluated through content similarity between generated descriptions and actual items), and the consistency between the teacher’s and student’s recommendations. Meanwhile, collaborative embedding distillation involves using a learnable MLP to effectively translate embeddings from the teacher’s semantic space into the student’s space.</p>

<p><img src="https://eugeneyan.com/assets/dllm2rec-fig1.jpg" loading="lazy" title="DLLM2Rec" alt="DLLM2Rec"></p>

<p>In their experiments, they use BIGRec (built on Llama2-7B) as the teacher and three popular sequential models (GRU4Rec, SASRec, and DROS) as students.</p>

<p><strong>Results:</strong> DLLM2Rec boosts the performance of student models, showing an average improvement of 47.97% across three datasets (Amazon Video Games, MovieLens-10M, and Amazon Toys and Games) when evaluating hit rate@k and NDCG@k (see Table 5 in the paper). Additionally, inference time dropped significantly, from 3-6 hours with the teacher model down to just 1.6-1.8 seconds with DLLM2Rec.</p>

<p><strong><a href="https://arxiv.org/abs/2408.08913" target="_blank">MLoRA (Alibaba)</a> describes using domain-specific LoRAs (low-rank adapters) to enhance multi-domain CTR prediction models.</strong> It addresses two common problems: data sparsity (limited data per domain) and domain diversity (variations across domains) that typically arise when training either separate models or a single combined model respectively.</p>

<p>They adopt a two-step training process. First, they pretrained a shared backbone network on extensive, multi-domain data to learn generalizable patterns across domains. Then, they freeze the backbone and finetune domain-specific LoRAs on each domain’s unique data. A key challenge was adapting LoRA ranks layer-by-layer due to varying dimensions in CTR model layers. (Recommender models have different dimentions per layer unlike language models which typically have uniform dimensions.) In their experiments, all models had hidden layers of 256, 128, and 64 dimensions.</p>

<p><img src="https://eugeneyan.com/assets/mlora-fig3.jpg" loading="lazy" title="MLoRA" alt="MLoRA"></p>

<p>To get a sense of data distribution differences between pretraining and finetuning: During their A/B test, the pretrained backbone used 13 billion samples spanning 90 days from 10 domains, whereas finetuning involved 3.2 billion samples from just 21 days.</p>

<p><strong>Results:</strong> MLoRA increased AUC by 0.5% across datasets such as Taobao-10, Amazon-6, and MovieLens. Ablation studies showed that domains with smaller datasets and higher inter-domain differences benefited more. They also found that simpler models (like MLP) performed best with lower LoRA ranks (32), while more complex models (like DeepFM) benefited from higher ranks (64 - 128). A/B testing showed substantial business gains—a 1.49% lift in CTR, a 3.37% boost in conversions, and a 2.71% increase in paid buyers—with only a modest 1.76% rise in model complexity due to the use of LoRAs.</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688053" target="_blank">Taming One-Epoch (Pinterest)</a> highlights the challenge of models overfitting after just one training epoch</strong>, primarily due to the long-tail nature of recommendation data. (Perhaps the Scaling Laws paper above, which showed gains beyond one epoch, used datasets (i.e., Amazon and MovieLens) that had the long-tail filtered out.) This overfitting arises because tail entries have far more degrees of freedom compared to the limited training examples available.</p>

<p>Here’s more context on the “one-epoch problem”: In online experiments, they saw that deep CTR models without ID embeddings typically require multiple epochs to converge. However, introducing ID embeddings often causes performance to peak after just one epoch, leading to worse results compared to multi-epoch training without ID embeddings.</p>

<p>Their solution involves two distinct stages. In the first stage, they pretrain foundational ID embeddings using a minimal dot-product model combined with contrastive loss, utilizing in-batch and uniformly random negatives. This contrastive approach reduces the effective dimensionality of tail entries, minimizing overfitting. Moreover, because the pretraining step is relatively lightweight, they can use a much larger dataset—around ten times the engagement data compared to the downstream recommendation model.</p>

<p>In the second stage, the pretrained embeddings are finetuned in task-specific models for multiple epochs. By separating embedding pretraining from downstream finetuning, they mitigate overfitting and get better results compared to merely freezing the embeddings.</p>

<p><img src="https://eugeneyan.com/assets/one-epoch-fig1.jpg" loading="lazy" title="Taming One-Epoch" alt="Taming One-Epoch"></p>

<p><strong>Results:</strong> In Figure 2 above, the typical binary cross-entropy (BCE) loss tends to overfit after the first epoch, whereas the contrastive loss remains stable. Ablation studies revealed that a single-stage training method underperformed relative to baseline models due to severe overfitting (−3.347% for Homefeed and −1.907% for Related Pins). Conversely, the two-stage training consistently yielded superior results (+1.323% Homefeed, +2.187% Related Pins), and in online A/B tests, led to a significant overall engagement lift of 2.2%.</p>

<p><strong><a href="https://arxiv.org/abs/2409.14517" target="_blank">Sliding Window Training (Netflix)</a> describes their method for efficiently training on long user history sequences</strong> without incurring the memory and latency costs associated with large input sizes. One workaround is to truncate user historical interactions—however, this comes at the cost of not using valuable information from the entire user journey.</p>

<p>Their solution is elegantly simple. Assuming a baseline model that only handles sequences of up to 100 items, they introduce a sliding window sampler during training. This sampler selects different segments of user history in each training epoch, allowing the model to learn on long-term user patterns. Additionally, they experimented with mixing epochs—some focused exclusively on sliding windows, while others emphasized only the latest 100 interactions—to balance between recent user behavior and historical preferences.</p>

<p><img src="https://eugeneyan.com/assets/sliding-window-fig2.jpg" loading="lazy" title="Sliding Window Training" alt="Sliding Window Training"></p>

<p><strong>Results:</strong> Offline evaluations showed the sliding window method consistently outperformed models trained solely on the most recent 100 interactions. Specifically, a pure sliding window variant slightly reduced Mean Reciprocal Rank (MRR) by 1.2%, but improved Mean Average Precision (MAP) by 1.5% and recall significantly by 7.01%. Hybrid approaches combining sliding windows with recent interactions, and extending input sequence lengths to 500 or even 1000 items, delivered the best overall performance. However, these extended approaches had slightly worse perplexity, indicating a trade-off between predictive confidence and actual recommendation performance.</p>

<p>• • •</p>

<h2 id="unified-architectures-for-search-and-recommendations">Unified architectures for search and recommendations</h2>

<p>The final theme highlights a growing shift toward unified system architectures that blend search and recommendations, drawing inspiration from foundation models. Instead of deploying multiple single-task models, recent papers present unified frameworks capable of handling diverse retrieval and ranking tasks within a shared infrastructure. For example, LinkedIn’s 360Brew and Netflix’s UniCoRn show how unified models trained on multiple tasks can outperform specialized, single-task counterparts.</p>

<p><strong><a href="https://arxiv.org/abs/2410.16823" target="_blank">Bridging Search &amp; Recommendations (Spotify)</a> demonstrates the advantages of training a unified generative retrieval model</strong> on both search and recommendation data, rather than separately, and how it can outperform task-specific models.</p>

<p>In their approach, a generative recommender predicts item IDs based on a user’s past interactions, while a generative search retriever predicts item IDs from tokenized search queries. The underlying model builds upon Flan-T5-base, extending the vocabulary to include all item IDs with one additional token per item. These models are trained auto-regressively using teacher forcing and cross-entropy loss, aiming to accurately predict the next relevant item ID. During inference, item IDs are generated directly from either a user’s interaction history (for recommendations) or a text query (for search).</p>

<p><img src="https://eugeneyan.com/assets/bridging-spotify-table1.jpg" loading="lazy" title="Bridging search and recsys" alt="Bridging search aand recsys"></p>

<p>Evaluation is done via standard recall metrics (recall@10 for simulated datasets, recall@30 for real-world datasets) against common baselines like BM25, SASRec, and BERT4Rec.</p>

<p><strong>Results:</strong> Jointly trained multi-task models outperformed their single-task counterparts, achieving an average increase of 16% in recall@30. On the Podcasts dataset, the unified model significantly improved performance by +33% across both tasks, especially for torso items (those outside the top 1%), showing gains of 262% for recommendations and 855% for search.</p>

<p>While the research wasn’t focused on replacing conventional models, the comparisons against behavioral baselines were insightful. Across three datasets, generative models consistently lagged behind specialized recommendation baselines (SASRec, BERT4Rec) significantly (green below). Similarly, for search, traditional baselines (BM25, Bi-encoder) were still superior (green below). This indicates that generative retrieval models are still far from fully replacing conventional methods.</p>

<p><img src="https://eugeneyan.com/assets/bridging-spotify-table5.jpg" loading="lazy" title="Bridging search and recsys" alt="Bridging search aand recsys"></p>

<p>(👉 Recommended read)  <strong><a href="https://arxiv.org/abs/2501.16450" target="_blank">360Brew (LinkedIn)</a> consolidates several ID-based ranking models into a single large 150B decoder-only model</strong> equipped with a natural language interface, effectively replacing traditional feature engineering with prompt engineering.</p>

<p><img src="https://eugeneyan.com/assets/360brew-table1.jpg" loading="lazy" title="360Brew" alt="360Brew"></p>

<p>360Brew builds upon the Mixtral-8x22B pretrained Mixture-of-Experts model. Its fine-tuning dataset includes 3-6 months of interactions from roughly 45 million monthly active users in the US, encompassing member profiles, job descriptions, posts, and various interaction logs—all transformed into a text-based format.</p>

<p>Training involves three key stages. First, continuous pretraining (CPT) is done with a maximum context length of 16K tokens with packing techniques. Next, instruction fine-tuning (IFT) is performed using a mix of open-source datasets (such as UltraChat) and internally generated instruction-following data. Finally, supervised fine-tuning (SFT) applies multi-turn chat templates designed to enhance the model’s understanding of member-entity interactions, improving its predictive capabilities across specific user interfaces.</p>

<p>The model was trained on 256-512 H100 GPUs using FSDP, and production deployment adopts vLLM and inference-time RoPE scaling. 360Brew focuses on binary prediction tasks, such as whether a user will like a posts, and uses token logits to assign scores.</p>

<p><strong>Results:</strong> The unified model supports over 30 different ranking tasks across LinkedIn’s platforms, matching or surpassing specialized production models while reducing complexity and maintenance overhead. The researchers also found that the unified model improved substantially with more data—while initial iterations performed poorly, tripling the dataset resulted in performance exceeding specialized models (Figure 2 below). Additionally, larger models consistently outperformed smaller versions (8x22B &gt; 8x7B &gt; 7B). Also, 360Brew delivered strong performance for cold-start users, outperforming traditional models by a wider margin when user interaction data was limited.</p>

<p><img src="https://eugeneyan.com/assets/360brew-fig2.jpg" loading="lazy" title="360Brew" alt="360Brew"></p>

<p>Similarly, <strong><a href="https://arxiv.org/abs/2408.10394" target="_blank">UniCoRn (Netflix)</a> introduces a unified contextual ranker designed to serve both search and recommendation tasks</strong> through a shared contextual framework. This unified model achieves comparable or better performance than multiple specialized models, thus reducing operational complexity.</p>

<p>The UniCoRn model uses contextual information such as user ID, search queries, country, source entity ID, and task type, predicting the probability of positive engagement with a target entity (e.g., a movie). Since not all contexts are always available, heuristics are used to impute missing data. For example, missing source entity IDs in search tasks are imputed as null, and missing query contexts in recommendation tasks use the entity’s display names.</p>

<p>UniCoRn incorporates two broad feature categories: context-specific features (like query length and source entity embeddings) and combined context-target features (such as click counts for a target entity in response to a query). The architecture includes embedding layers for categorical features, enhanced with residual connections and feature crossing.</p>

<p><img src="https://eugeneyan.com/assets/unicorn-fig1.jpg" loading="lazy" title="Unicorn" alt="Unicorn"></p>

<p>Training uses binary cross-entropy loss and the Adam optimizer. Netflix incrementally increased personalization: starting from a semi-personalized model using user clusters, progressing to including outputs from other recommendation models, and finally incorporating pretrained and fine-tuned user and item embeddings.</p>

<p><strong>Results:</strong> UniCoRn consistently matched or exceeded specialized models. Personalization boosted outcomes, delivering a 10% improvement in recommendations and a 7% lift in search. Ablation studies showed the importance of explicitly including the task type as context, imputing missing features to maximize feature coverage, and applying feature crossing to enhance multi-task learning effectiveness.</p>

<p>(👉 Recommended read) <strong><a href="https://arxiv.org/abs/2306.04833" target="_blank">Unified Embeddings (Etsy)</a> shares how they unified transformer-based, term-based, and graph-based embeddings within a two-tower model</strong> architecture. This goal was to address common gaps such as mismatches between search queries and product vocabulary (lexical matching) and the poor performance of neural embeddings due to limited user context.</p>

<p><img src="https://eugeneyan.com/assets/unifiedemb-fig2.jpg" loading="lazy" title="Unified Embeddings" alt="Unified Embeddings"></p>

<p>Their model adopts a classic two-tower structure, consisting of a product encoder and a joint query-user encoder. The product encoder combines transformer-based embeddings, bipartite graph embeddings (trained using a full year of query-product interaction data), product title embeddings, and location information. Interestingly, direct finetuning of transformer-based models like distilBERT and T5 did not yield significant offline metric improvements. Instead, inspired by docT5query, they pretrained a T5-small model specifically designed to predict historically purchased queries based on product descriptions. The query-user encoder combines query text embeddings, location, and historical engagement data. Both query/title and location embeddings are shared across the two towers for consistency.</p>

<p>They emphasize the effectiveness of negative sampling, sharing multiple approaches such as hard in-batch negatives (positives from other queries within the batch), uniform negatives (randomly selected from the entire product corpus), and dynamic hard negatives (random samples narrowed down by the model to identify the most challenging examples). The goal here is to find the most similar negatives to help the model learn on the hardest samples.</p>

<p>To balance relevance with product quality, they integrated quality boosting into their embeddings via an approximate nearest neighbor (ANN) index. Product embeddings are augmented with query-independent quality scores reflecting attributes such as product ratings, freshness, and conversion rates—factors proven to increase engagement independently from query relevance. Given the original product embeddings, they concatenate it with the quality score vectors; the respective query embedding is concatenated with a constant vector. The final score of the product, for a query, is the dot product of the updated product and query embedding.</p>

<p><img src="https://eugeneyan.com/assets/unifiedemb-fig3.jpg" loading="lazy" title="Unified Embeddings" alt="Unified Embeddings"></p>

<p>The system operates through two main stages: offline indexing and online serving. Offline, embeddings and quality scores are generated and pre-indexed into an ANN system (using FAISS with a 4-bit product quantizer). This approach, combined with a re-ranking step, achieves a recall loss below 4% while keeping latency under 20ms@p99. At the online stage, incoming queries are embedded in real time to retrieve products from the ANN index. They also shared how they applied caching while handling in-session personalization features.</p>

<p><strong>Results:</strong> In A/B testing, the unified embedding model drove a site-wide conversion lift of 2.63% and boosted organic search purchases by 5.58%. Offline tests showed that Unified Embeddings consistently outperformed traditional baselines for both head and tail queries. Ablation studies revealed the strongest contributions came from graph embeddings (+15% recall@100), followed by description embeddings (+6.3%) and attributes (+3.9%). Additionally, location embeddings significantly improved purchase recall@100 (+8%) for US users by minimizing geographic mismatches. Removing hard negatives resulted in a noticeable 7% drop in performance, underscoring their importance.</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688039" target="_blank">Embedding Long Tail (Best Buy)</a> shared how they optimize semantic product search to better address long-tail queries</strong> which typically suffer from sparse user interaction data.</p>

<p>To create a high-quality dataset, they collected user engagement data from product pages and applied a two-stage filtering process, reducing data volume (by 10x) while maintaining quality and balanced coverage across product categories. First, they retained interactions observed from at least two unique visitors, then performed stratified sampling across categories to mitigate popularity bias. To further augment this data, they prompted a Llama-13B model to generate ten synthetic search queries per product using the product’s title, category, description, and specifications, thus ensuring comprehensive catalog coverage.</p>

<p>Their model follows a two-tower architecture based on Best Buy’s internally developed BERT variant, an adaptation of RoBERTa finetuned through masked language modeling on search queries and product information. They used the first five layers of this BERT model to initialize both the search and product encoders. Training involved using in-batch negatives with multi-class cross-entropy loss. For deployment, Solr functions as both the inverted index and vector database, with a caching layer added to minimize redundant requests to the embedding service.</p>

<p><strong>Results:</strong> Adding semantic retrieval to the existing lexical search improved conversion rates by 3% in online A/B tests. Offline experiments demonstrated incremental improvements through various enhancements: two-stage data filtering (+0.24% recall@200), synthetic positive queries (+0.7%), additional product features (+1.15%), query-to-query followed by query-to-product fine-tuning (+2.44%), and model weight merging (+4.67%). Notably, their final model outperformed the baseline (all-mpnet-base-v2) while using only half the parameters at 50M vs 110M. (Nonetheless, it may not have been a fair comparison given the baseline was not finetuned.)</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688030" target="_blank">User Behavioral Service (YouTube)</a> presented an innovative approach for serving large user sequence models efficiently while sidestepping latency challenges.</strong></p>

<p><img src="https://eugeneyan.com/assets/ubs-fig1.jpg" loading="lazy" title="User Behavioral Service" alt="User Behavioral Service"></p>

<p>The intuition behind User Behavior Service (UBS) is decoupling the serving of the user sequence model from the main recommendation model. This design allows independent control over user embedding computation. Although both models are co-trained, they are exported and served separately. The user model computes embeddings asynchronously, storing them in a high-speed key-value cache that’s regularly updated. If a requested embedding isn’t available, an empty embedding is returned while an asynchronous refresh is triggered. This setup enables experimentation with significantly larger models without latency constraints—a concept similar to what I described as “Just-in-time infrastructure” in my <a href="https://eugeneyan.com/speaking/recsys2022-keynote/" target="_blank">RecSys 2022 keynote</a>.</p>

<p><strong>Results:</strong> In A/B tests, UBS improved performance across six different ranking tasks while limiting the increase in cost. For example, a User Model with a sequence length of 1,000 showed a 0.38% improvement in online metrics compared to a baseline model using a sequence length of 20, with offline accuracy gains ranging from 0.01% to 0.40% across multiple tasks. Directly serving a large user sequence model would have increased costs by 28.7% but the UBS approach limited this increase to just 2.8%.</p>

<p>(👉 Recommended read) <strong><a href="https://arxiv.org/abs/2409.02856" target="_blank">Modern Ranking Platform (Zalando)</a> details their real-time platform designed for both search and browsing scenarios.</strong> The paper discusses their system design, candidate generation, retrieval methods, and ranking policies.</p>

<p><img src="https://eugeneyan.com/assets/zalando-fig2.jpg" loading="lazy" title="Modern Ranking Platform" alt="Modern Ranking Platform"></p>

<p>Their platform is built around a few key principles:</p>

<ul>
  <li><strong>Composability:</strong> Models can be combined vertically (layered ranking) or horizontally by integrating outputs from various models or candidate generators.</li>
  <li><strong>Scalability:</strong> To manage computational costs, the platform first uses efficient but less precise candidate generators. These initial candidates are then refined by more accurate but computationally intensive rankers, a <a href="https://eugeneyan.com/writing/system-design-for-discovery/" target="_blank">standard design for recsys</a>.</li>
  <li><strong>Shared Infrastructure:</strong> Whenever possible, training datasets, embeddings, feature stores, and serving infrastructure are reused to simplify operations.</li>
  <li><strong>Steerable Ranking:</strong> The platform allows external adjustments through a policy layer, making it easy to align rankings with business objectives.</li>
</ul>

<p><img src="https://eugeneyan.com/assets/zalando-fig3.jpg" loading="lazy" title="Modern Ranking Platform" alt="Modern Ranking Platform"></p>

<p>Their candidate generator uses a classic two-tower model. The customer tower updates embeddings based on a customer’s recent actions and current context whenever the customer visits the site, ensuring embeddings remain fresh. The item tower precomputes item embeddings and stores them in a vector database for rapid retrieval. These embeddings are matched via dot product. To create customer embeddings, a Transformer encoder is trained on historical customer behavior and contextual data, predicting the next likely interaction.</p>

<p><img src="https://eugeneyan.com/assets/zalando-fig4.jpg" loading="lazy" title="Modern Ranking Platform" alt="Modern Ranking Platform"></p>

<p>The ranker is a multi-task model that predicts the likelihood of different customer actions, such as clicks, adding items to wishlist or cart, and purchases. Each action has its own prediction head, with all contributing equally to training loss. During serving, each action type’s importance can be dynamically adjusted. Overall, the ranker outputs personalized scores for each candidate item across multiple potential customer interactions.</p>

<p>Finally, the policy layer ensures the system aligns with broader business goals. For instance, it can encourage exploration by promoting new products through heuristics like epsilon-greedy strategies. It also applies other business rules, such as reducing the visibility of previously purchased items and ensuring item diversity by preventing items from the same brand from appearing back-to-back.</p>

<p><strong>Results:</strong> The unified architecture demonstrated strong performance across four A/B tests, achieving a combined engagement increase of +15% and a revenue uplift of +2.2%. Iterative improvements further illustrate the effectiveness of each system component: introducing trainable embeddings in candidate generation boosted engagement by +4.48% and revenue by +0.18%; adding advanced ranking and policy layers delivered an additional +4.04% engagement and +0.86% revenue; and using contextual data provided a further lift of +2.40% in engagement and +0.60% in revenue.</p>

<p>• • •</p>

<p>Although early research in 2023—that applied LLMs to recommendations and search—often fell short, these recent efforts show more promise, especially since they’re backed by industry results. It suggests that there are tangible benefits from exploring the augmentation of recsys and search systems with LLMs, increasing performance while reducing cost and effort.</p>

<h2 id="references">References</h2>

<p>Chamberlain, Benjamin P., et al. “Tuning Word2vec for Large Scale Recommendation Systems.” <em>Fourteenth ACM Conference on Recommender Systems</em>, 2020, pp. 732–37. <em>arXiv.org</em>, <a href="https://doi.org/10.1145/3383313.3418486">https://doi.org/10.1145/3383313.3418486</a>.</p>

<p>Hidasi, Balázs, et al. <em>Session-Based Recommendations with Recurrent Neural Networks</em>. arXiv:1511.06939, arXiv, 29 Mar. 2016. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.1511.06939">https://doi.org/10.48550/arXiv.1511.06939</a>.</p>

<p>Chen, Qiwei, et al. <em>Behavior Sequence Transformer for E-Commerce Recommendation in Alibaba</em>. arXiv:1905.06874, arXiv, 15 May 2019. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.1905.06874">https://doi.org/10.48550/arXiv.1905.06874</a>.</p>

<p>Sun, Fei, et al. <em>BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer</em>. arXiv:1904.06690, arXiv, 21 Aug. 2019. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.1904.06690">https://doi.org/10.48550/arXiv.1904.06690</a>.</p>

<p>Singh, Anima, et al. <em>Better Generalization with Semantic IDs: A Case Study in Ranking for Recommendations</em>. arXiv:2306.08121, arXiv, 30 May 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2306.08121">https://doi.org/10.48550/arXiv.2306.08121</a>.</p>

<p>Chen, Gaode, et al. “A Multi-Modal Modeling Framework for Cold-Start Short-Video Recommendation.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 391–400. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688098">https://doi.org/10.1145/3640457.3688098</a>.</p>

<p>Wang, Hangyu, et al. <em>FLIP: Fine-Grained Alignment between ID-Based Models and Pretrained Language Models for CTR Prediction</em>. arXiv:2310.19453, arXiv, 30 Oct. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2310.19453">https://doi.org/10.48550/arXiv.2310.19453</a>.</p>

<p>Vančura, Vojtěch, et al. “beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems.” <em>18th ACM Conference on Recommender Systems</em>, 2024, pp. 1102–07. <em>arXiv.org</em>, <a href="https://doi.org/10.1145/3640457.3691707">https://doi.org/10.1145/3640457.3691707</a>.</p>

<p>Li, Yaoyiran, et al. <em>CALRec: Contrastive Alignment of Generative LLMs for Sequential Recommendation</em>. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2405.02429">https://doi.org/10.48550/arXiv.2405.02429</a>.</p>

<p>Zhang, Chiyu, et al. <em>EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations</em>. arXiv:2405.11441, arXiv, 19 Aug. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2405.11441">https://doi.org/10.48550/arXiv.2405.11441</a>.</p>

<p>Shah, Jaidev, et al. “Analyzing User Preferences and Quality Improvement on Bing’s WebPage Recommendation Experience with Large Language Models.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 751–54. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688062">https://doi.org/10.1145/3640457.3688062</a>.</p>

<p>Pei, Yingchi, et al. “Leveraging LLM Generated Labels to Reduce Bad Matches in Job Recommendations.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 796–99. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688043">https://doi.org/10.1145/3640457.3688043</a>.</p>

<p><em>Search Query Understanding with LLMs: From Ideation to Production</em>. <a href="https://engineeringblog.yelp.com/2025/02/search-query-understanding-with-LLMs.html">https://engineeringblog.yelp.com/2025/02/search-query-understanding-with-LLMs.html</a>. Accessed 5 Mar. 2025.</p>

<p>Lindstrom, Henrik, et al. “Encouraging Exploration in Spotify Search through Query Recommendations.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 775–77. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688035">https://doi.org/10.1145/3640457.3688035</a>.</p>

<p>Aluri, Geetha Sai, et al. “Playlist Search Reinvented: LLMs Behind the Curtain.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 813–15. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688047">https://doi.org/10.1145/3640457.3688047</a>.</p>

<p>Zhang, Gaowei, et al. <em>Scaling Law of Large Sequential Recommendation Models</em>. arXiv:2311.11351, arXiv, 19 Nov. 2023. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2311.11351">https://doi.org/10.48550/arXiv.2311.11351</a>.</p>

<p>Wang, Junting, et al. “A Pre-Trained Sequential Recommendation Framework: Popularity Dynamics for Zero-Shot Transfer.” <em>18th ACM Conference on Recommender Systems</em>, 2024, pp. 433–43. <em>arXiv.org</em>, <a href="https://doi.org/10.1145/3640457.3688145">https://doi.org/10.1145/3640457.3688145</a>.</p>

<p>Liu, Qi, et al. <em>Efficient Transfer Learning Framework for Cross-Domain Click-Through Rate Prediction</em>. arXiv:2408.16238, arXiv, 29 Aug. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2408.16238">https://doi.org/10.48550/arXiv.2408.16238</a>.</p>

<p>Khani, Nikhil, et al. <em>Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems</em>. arXiv:2408.14678, arXiv, 26 Aug. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2408.14678">https://doi.org/10.48550/arXiv.2408.14678</a>.</p>

<p>Zhang, Yin, et al. “Self-Auxiliary Distillation for Sample Efficient Learning in Google-Scale Recommenders.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 829–31. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688041">https://doi.org/10.1145/3640457.3688041</a>.</p>

<p>Cui, Yu, et al. “Distillation Matters: Empowering Sequential Recommenders to Match the Performance of Large Language Model.” <em>18th ACM Conference on Recommender Systems</em>, 2024, pp. 507–17. <em>arXiv.org</em>, <a href="https://doi.org/10.1145/3640457.3688118">https://doi.org/10.1145/3640457.3688118</a>.</p>

<p>Yang, Zhiming, et al. <em>MLoRA: Multi-Domain Low-Rank Adaptive Network for CTR Prediction</em>. arXiv:2408.08913, arXiv, 14 Aug. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2408.08913">https://doi.org/10.48550/arXiv.2408.08913</a>.</p>

<p>Hsu, Yi-Ping, et al. “Taming the One-Epoch Phenomenon in Online Recommendation System by Two-Stage Contrastive ID Pre-Training.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 838–40. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688053">https://doi.org/10.1145/3640457.3688053</a>.</p>

<p>Joshi, Swanand, et al. “Sliding Window Training - Utilizing Historical Recommender Systems Data for Foundation Models.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 835–37. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688051">https://doi.org/10.1145/3640457.3688051</a>.</p>

<p>Penha, Gustavo, et al. <em>Bridging Search and Recommendation in Generative Retrieval: Does One Task Help the Other?</em> arXiv:2410.16823, arXiv, 22 Oct. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2410.16823">https://doi.org/10.48550/arXiv.2410.16823</a>.</p>

<p>Firooz, Hamed, et al. <em>360Brew: A Decoder-Only Foundation Model for Personalized Ranking and Recommendation</em>. arXiv:2501.16450, arXiv, 27 Jan. 2025. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2501.16450">https://doi.org/10.48550/arXiv.2501.16450</a>.</p>

<p>Bhattacharya, Moumita, et al. <em>Joint Modeling of Search and Recommendations Via an Unified Contextual Recommender (UniCoRn)</em>. arXiv:2408.10394, arXiv, 19 Aug. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2408.10394">https://doi.org/10.48550/arXiv.2408.10394</a>.</p>

<p>Jha, Rishikesh, et al. <em>Unified Embedding Based Personalized Retrieval in Etsy Search</em>. arXiv:2306.04833, arXiv, 25 Sept. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2306.04833">https://doi.org/10.48550/arXiv.2306.04833</a>.</p>

<p>Kekuda, Akshay, Yuyang Zhang, and Arun Udayashankar. “Embedding based retrieval for long tail search queries in ecommerce.” Proceedings of the 18th ACM Conference on Recommender Systems. 2024. <a href="https://dl.acm.org/doi/10.1145/3640457.3688039">https://dl.acm.org/doi/10.1145/3640457.3688039</a>.</p>

<p>Li, Yuening, et al. “Short-Form Video Needs Long-Term Interests: An Industrial Solution for Serving Large User Sequence Models.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 832–34. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688030">https://doi.org/10.1145/3640457.3688030</a>.</p>

<p>Celikik, Marjan, et al. <em>Building a Scalable, Effective, and Steerable Search and Ranking Platform</em>. 1, arXiv:2409.02856, arXiv, 4 Sept. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2409.02856">https://doi.org/10.48550/arXiv.2409.02856</a>.</p>


            
            
<p>If you found this useful, please cite this write-up as:</p>

<blockquote>
    <p>Yan, Ziyou. (Mar 2025). Improving Recommendation Systems &amp; Search in the Age of LLMs. eugeneyan.com.
        https://eugeneyan.com/writing/recsys-llm/.</p>
</blockquote>

<p>or</p>

<div><pre><code>@article{yan2025recsys-llm,
  title   = {Improving Recommendation Systems &amp; Search in the Age of LLMs},
  author  = {Yan, Ziyou},
  journal = {eugeneyan.com},
  year    = {2025},
  month   = {Mar},
  url     = {https://eugeneyan.com/writing/recsys-llm/}
}</code></pre>
</div>

            
            
            



<p><span>Share on:  </span></p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Next.js version 15.2.3 has been released to address a security vulnerability (153 pts)]]></title>
            <link>https://nextjs.org/blog/cve-2025-29927</link>
            <guid>43448723</guid>
            <pubDate>Sat, 22 Mar 2025 21:19:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nextjs.org/blog/cve-2025-29927">https://nextjs.org/blog/cve-2025-29927</a>, See on <a href="https://news.ycombinator.com/item?id=43448723">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Next.js version 15.2.3 has been released to address a security vulnerability (<a href="https://github.com/advisories/GHSA-f82v-jwr5-mffw" rel="noopener noreferrer nofollow" target="_blank">CVE-2025-29927<span><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.75011 4H6.00011V5.5H6.75011H9.43945L5.46978 9.46967L4.93945 10L6.00011 11.0607L6.53044 10.5303L10.499 6.56182V9.25V10H11.999V9.25V5C11.999 4.44772 11.5512 4 10.999 4H6.75011Z" fill="currentColor"></path></svg></span></a>). Additionally, backported patches are available.</p>
<p>We recommend that all self-hosted Next.js deployments using <code>next start</code> and <code>output: 'standalone'</code> should <a href="https://nextjs.org/docs/app/building-your-application/upgrading">update</a> immediately.</p>
<p>Continue reading for more details on the CVE.</p>
<h2 id="timeline" data-docs-heading=""><a href="#timeline">Timeline<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<ul>
<li><code>2025-02-27T06:03Z</code>: Disclosure to Next.js team via GitHub private vulnerability reporting</li>
<li><code>2025-03-14T17:13Z</code>: Next.js team started triaging the report</li>
<li><code>2025-03-14T19:08Z</code>: Patch pushed for Next.js 15.x</li>
<li><code>2025-03-14T19:26Z</code>: Patch pushed for Next.js 14.x</li>
<li><code>2025-03-17T22:44Z</code>: Next.js 14.2.25 released</li>
<li><code>2025-03-18T00:23Z</code>: Next.js 15.2.3 released</li>
<li><code>2025-03-18T18:03Z</code>: CVE-2025-29927 issued by GitHub</li>
<li><code>2025-03-21T10:17Z</code>: Security Advisory published</li>
<li><code>2025-03-22T21:21Z</code>: Next.js 13.5.9 released</li>
</ul>
<p>We are also publishing a backport for v12. We will update this post as they are released.</p>
<h2 id="vulnerability-details" data-docs-heading=""><a href="#vulnerability-details">Vulnerability details<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<p>Next.js uses an internal header <code>x-middleware-subrequest</code> to prevent recursive requests from triggering infinite loops. The security report showed it was possible to skip running Middleware, which could allow requests to skip critical checks—such as authorization cookie validation—before reaching routes.</p>
<h2 id="impact-scope" data-docs-heading=""><a href="#impact-scope">Impact scope<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<h3 id="affected" data-docs-heading=""><a href="#affected">Affected<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h3>
<ul>
<li>Self-hosted Next.js applications using Middleware (<code>next start</code> with output: <code>standalone</code>)</li>
<li>This affects you if you rely on Middleware for auth or security checks, which are not then validated later in your application.</li>
<li>Applications using Cloudflare can turn on a <a href="https://developers.cloudflare.com/changelog/2025-03-22-next-js-vulnerability-waf/" rel="noopener noreferrer nofollow" target="_blank">Managed WAF rule<span><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.75011 4H6.00011V5.5H6.75011H9.43945L5.46978 9.46967L4.93945 10L6.00011 11.0607L6.53044 10.5303L10.499 6.56182V9.25V10H11.999V9.25V5C11.999 4.44772 11.5512 4 10.999 4H6.75011Z" fill="currentColor"></path></svg></span></a></li>
</ul>
<h3 id="not-affected" data-docs-heading=""><a href="#not-affected">Not affected<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h3>
<ul>
<li>Applications hosted on Vercel</li>
<li>Applications hosted on Netlify</li>
<li>Applications deployed as static exports (Middleware not executed)</li>
</ul>
<h2 id="patched-versions" data-docs-heading=""><a href="#patched-versions">Patched versions<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<ul>
<li>For Next.js 15.x, this issue is fixed in <code>15.2.3</code></li>
<li>For Next.js 14.x, this issue is fixed in <code>14.2.25</code></li>
<li>For Next.js 13.x, this issue is fixed in <code>13.5.9</code></li>
</ul>
<p>If patching to a safe version is infeasible, it is recommended that you prevent external user requests which contain the <code>x-middleware-subrequest</code> header from reaching your Next.js application.</p>
<p>We are also publishing a backport for v12. We will update this post as they are released.</p>
<h2 id="our-security-responsibility" data-docs-heading=""><a href="#our-security-responsibility">Our security responsibility<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<p>Next.js has published <a href="https://github.com/vercel/next.js/security/advisories?state=published" rel="noopener noreferrer" target="_blank">16 security advisories<span><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.75011 4H6.00011V5.5H6.75011H9.43945L5.46978 9.46967L4.93945 10L6.00011 11.0607L6.53044 10.5303L10.499 6.56182V9.25V10H11.999V9.25V5C11.999 4.44772 11.5512 4 10.999 4H6.75011Z" fill="currentColor"></path></svg></span></a> since 2016. Over time, we've continued to improve how we gather, patch, and disclose vulnerabilities.</p>
<p>GitHub Security Advisories and CVEs are industry-standard approaches to notifying users, vendors, and companies of vulnerabilities in software. While we have published a CVE, <strong>we missed the mark</strong> on partner communications.</p>
<p>To help us more proactively work with partners depending on Next.js, and other infrastructure providers, we are opening a partner mailing list. Please reach out to <a href="mailto:partners@nextjs.org"><code>partners@nextjs.org</code></a> to be included.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CEO of Kubient sentenced for fraud (161 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/03/ceo-of-ai-ad-tech-firm-pledging-world-free-of-fraud-sentenced-for-fraud/</link>
            <guid>43448606</guid>
            <pubDate>Sat, 22 Mar 2025 21:02:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/03/ceo-of-ai-ad-tech-firm-pledging-world-free-of-fraud-sentenced-for-fraud/">https://arstechnica.com/gadgets/2025/03/ceo-of-ai-ad-tech-firm-pledging-world-free-of-fraud-sentenced-for-fraud/</a>, See on <a href="https://news.ycombinator.com/item?id=43448606">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                      
                      
          <p>In <a href="https://web.archive.org/web/20240511122402/https://kubient.com/">May 2024</a>, the website of ad-tech firm Kubient touted that the company was "a perfect blend" of ad veterans and developers, "committed to solving the growing problem of fraud" in digital ads. Like many corporate sites, it also linked old blog posts from its home page, including <a href="https://web.archive.org/web/20240315021936/https://kubient.com/ad-fraud/how-to-create-a-world-free-of-fraud-kubients-secret-sauce/">a May 2022 post</a> on "How to create a world free of fraud: Kubient's secret sauce."</p>
<p>These days, Kubient's website cannot be reached, the team is no more, and CEO Paul Roberts is <a href="https://www.justice.gov/usao-sdny/pr/former-ceo-kubient-inc-sentenced-prison-connection-accounting-fraud-scheme">due to serve one year and one day in prison</a>, having pled guilty Thursday to creating his own small world of fraud. Roberts, according to federal prosecutors, schemed to create $1.3 million in fraudulent revenue statements to bolster Kubient's initial public offering (IPO) and significantly oversold "KAI," Kubient's artificial intelligence tool.</p>
<p>The core of the case is an I-pay-you, you-pay-me gambit that Roberts initiated with an unnamed "Company-1," according to prosecutors. Kubient and this firm would each bill the other for nearly identical amounts, with Kubient purportedly deploying KAI to find instances of ad fraud in the other company's ad spend.</p>
<p>Roberts, prosecutors said, "directed Kubient employees to generate fake KAI reports based on made-up metrics and no underlying data at all." These fake reports helped sell the story to independent auditors and book the synthetic revenue in financial statements, according to Roberts' indictment.</p>

          
                      
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["Vibe Coding" vs. Reality (200 pts)]]></title>
            <link>https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.html</link>
            <guid>43448432</guid>
            <pubDate>Sat, 22 Mar 2025 20:35:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.html">https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.html</a>, See on <a href="https://news.ycombinator.com/item?id=43448432">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body"><article><time datetime="2025-03-19T15:00:00.000Z" title="3/19/2025, 10:00:00 AM">Published Mar 19, 2025</time> - 11 min read - <a href="https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.txt">Text Only</a><div id="table-of-contents-container"><p>Table of contents</p><ul><li><a href="#title" data-id="title">"Vibe Coding" vs Reality</a></li><li><a href="#working-around-the-problem" data-id="working-around-the-problem">Working around the problem</a></li><li><a href="#conclusion" data-id="conclusion">Conclusion</a></li></ul></div><p>There's a trend on social media where many repeat <a href="https://x.com/karpathy/status/1886192184808149383">Andrej Karpathy's words</a> (<a href="https://archive.is/yNSTA">archived</a>): "give in to the vibes, embrace exponentials, and forget that the code even exists." This belief — like many flawed takes humanity holds — comes from laziness, inexperience, and self-deluding imagination. It is called "Vibe Coding."</p><div><p><img width="128" height="128" alt="head-empty" src="https://cendyne.dev/s/128/cendyne/head-empty" sizes="128px"></p><div><p>"Embrace the exponentials" sounds like it came from an NFT junkie.</p></div></div><div><p><img width="128" height="128" alt="shinji-cup" src="https://cendyne.dev/s/128/cendyne/shinji-cup" sizes="128px"></p><div><p>Like the NFT crowd, there is a bubble of unreality they cling to justifying their perception of the world.</p></div></div><p>Producing software is now more accessible as newer tools allow people to describe what they want in a natural language to a large language model (LLM). This idea is catching on because LLM agents are now accessible to anyone willing to subscribe to vendors like <a href="https://www.cursor.com/en">Cursor</a>, <a href="https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode">GitHub</a>, Windsurf, and others. These editors have an "agent" option where users can request something and in response changes are made to the appropriate files, rather than only the file currently in focus. Over time, the agent will request to run commands to run tests or even run scripts it previously wrote to the file system, much as you would if you were solving the problem.</p><p>In 2022, folks could copy code into <a href="https://en.wikipedia.org/wiki/ChatGPT">ChatGPT</a> and ask questions or for rewrites.</p><p>In 2023, folks could ask it to review and edit a single file with an IDE integration like Copilot.</p><p>In 2024 and 2025, folks could ask it to solve a specific problem in the project and have it find out what files to edit, edit them, then verify its own work, and correct any mistakes it made with feedback from linting errors and unit tests.</p><p>With LLM agents having so much capability, people can delegate the idea of refining their imprecise ideas to a precise implementation elaborated by an LLM through "Vibe Coding."</p><div><p><a href="https://twitter.com/a16z">@a16z</a> <a href="https://twitter.com/stuffyokodraws">@stuffyokodraws</a> First - what is vibe coding?</p><p>A concise definition from <a href="https://twitter.com/stuffyokodraws">@stuffyokodraws</a>, and then an exploration of how technical vs. non-technical users approach these tools.</p></div><p>If you open a blank folder and tell it to set up an initial project, it can do a lot at once. With no rules, no patterns to mimic, and no constraints, it can produce something that feels more tailored for you in minutes than <code>npx create-react-app</code> ever could.</p><p>With a simple instruction like "I want to create a website for my ski resort" and about ten minutes of having it massage errors of its own making, I can have just that.</p><p><img data-blurhash="MSQc#U~WxtIW%LM|t6t7WCt7^*9Zt7%LR*" data-width="645" data-height="423" data-ratio="true" src="https://cendyne.dev/c/XH7rgh3H?width=645" alt="A generated website about a ski resort with a phrase like 'Easy to Reach, Hard to Leave'" width="645" height="423"></p><p>These leaps of progress are what fuels the "Vibe Coding" idea. To go from nothing to something shareable and personal sounds incredible.</p><div><p><img width="128" height="128" alt="beat-saber" src="https://cendyne.dev/s/128/cendyne/beat-saber" sizes="128px"></p><div><p>This moment provided a thrill I hadn't experienced in a long time when coding. However, this excitement drained quickly the further I got from a blank canvas.</p></div></div><p>Agents, as a concept, aren’t new. <a href="https://www.youtube.com/watch?v=ijeXop674Dg">Google IO made up buzzwords</a> like <a href="https://www.theverge.com/2024/12/11/24317436/google-deepmind-project-astra-mariner-ai-agent">"agentic era"</a> (<a href="https://archive.is/dw8XE">archived</a>) to describe this concept. It has been realized through open technologies like <a href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a>, <a href="https://github.com/OpenBMB/XAgent">XAgent</a>, and <a href="https://www.anthropic.com/news/model-context-protocol">more recently by Anthropic</a> with the <a href="https://modelcontextprotocol.io/introduction">Model Context Protocol</a> (MCP).</p><p>When the model can interact with more than just a person who proxies their outputs into different domains, it is autonomous. If it can perform searches on the web or in a codebase, it can enrich its own context with the information it needs to fulfill the current request. Further, when it can commit outputs and then gain immediate and automatic feedback on those outputs, it can refine its solution without a person intervening.</p><p>There are actions that do prompt the user for consent before proceeding, such as running commands in the console or deleting files. This consent can be pre approved with a mode called "YOLO."</p><p><img data-blurhash="E04ec*~qofxuoft7D%Rj?b-;xut7" data-width="645" data-height="149" data-ratio="true" src="https://cendyne.dev/c/E8IJPCCd?width=645" alt="Cursor settings YOLO mode, allows running commands automatically" width="645" height="149"></p><div><p><img width="128" height="127" alt="we-live-in-a-society" src="https://cendyne.dev/s/128/cendyne/we-live-in-a-society" sizes="128px"></p><div><p>A mode for "You Only Live Once"!? Really?</p></div></div><p>You can witness this autonomy for yourself today in Cursor.</p><p>The agent concept has merit and today can deliver proofs of concept that <a href="https://youtu.be/IACHfKmZMr8">VC firms like Y-Combinator</a> will invest in — proofs of concept that are trash by unskilled founders hoping to win the lottery while living the life of leisure.</p><div><p>I’ve cracked vibe coding, TrendFeed has almost hit its first 10k month, and Ai built the entire thing</p><p>Im just sitting here sipping coffee, coding with Ai + MCP</p><p>Also more time to shitpost on X haha</p></div><div><p><img width="128" height="128" alt="cheers" src="https://cendyne.dev/s/128/cendyne/cheers" sizes="128px" loading="lazy"></p><div><p>The optimal technical founder for a VC is not the 10x engineer. It is someone who'll deliver <em>enough</em> of a product to test its fitness in the market and then succeed in raising more investment money. Their execution on their vision and hiring prowess is more important than their technical skillset.</p></div></div><p>The execution of agents today is over-hyped and does not hold up to the needs of any functioning businesses which need experts to develop and maintain their technical capabilities instead of single points of failure on the internet.</p><div><p>babe, come to bed</p><p>i can't, i'm vibe coding</p></div><p>These models are trained on average sloppy code, wrong answers on Stack Overflow, and the junk that ends up on Quora. Despite the power and capability Claude 3.7 Sonnet has in small contexts, when faced with even a small codebase it makes constant silly mistakes that no normal developer would repeat and continue to repeat every hour of its operation.</p><div><p><span></span><span>Specific details on the mistakes, feel free to skip</span><span></span></p><div><ul><li>Regularly clones TypeScript interfaces instead of exporting the original and importing it.</li><li>Reinvents components all the time with the same structure without searching the code base for an existing copy of that component.</li><li>Writes trusted server side logic on the client side, using RPC calls to update the database.</li><li>As a feature develops, it prioritizes maintaining previous mistakes instead of re-evaluating its design, even when told to do so. You have to say the previous implementation is outright unusable for it to replace its design.</li><li>Cursor has some sort of <a href="https://www.reddit.com/r/ClaudeAI/comments/1i8n3wq/does_claude_have_stupid_mode_enabled_tonight/">"concise mode"</a> (<a href="https://archive.is/iU8gx">archived</a>) that they'll turn on when there is high load where the model will still be rated at the normal price but behaves in a useless manner. This mode will omit details, drop important findings, and corrupt the output that is being produced.</li><li>Cannot be trusted to produce unit tests with decent coverage.</li><li>Will often break the project's code to fit a unit test rather than fix the unit test when told to do so.</li><li>When told to fix styles with precise details, it will alter the wrong component entirely.</li><li>When told specifically where there are many duplicated components and instructed to refactor, will only refactor the first instance of that component in the file instead of all instances in all files.</li><li>When told to refactor code, fails to search for the breaks it caused even when told to do so.</li><li>Will merrily produce files over 1000 lines which exceed its context window over time, even when told to refactor early on.</li><li>Will regularly erase entire route handlers if not bound to the file hierarchy.</li></ul></div></div><p>As currently designed, these models cannot learn new information. They cannot do better than the dataset they were created with. Instead their capability is realized by how effective they can process tokens entering their context window.</p><p>If you ask Claude 3.7 Sonnet to develop a runtime schema for validating some domain specific language and then ask it to refactor the file — because it is too large for its context window to continue — it will degrade and output incoherent nonsense before finishing its work.</p><p><img data-blurhash="E042PB?bM{kCWBof%M%MRjRjtRay" data-width="645" data-height="244" data-ratio="true" src="https://cendyne.dev/c/l6YxDTlA?width=645" alt="Now that we've created all the schemado that: ... I'v schema files for each schema schemaschema schemaactored code?" width="645" height="244" loading="lazy"></p><div><p><img width="128" height="110" alt="wat" src="https://cendyne.dev/s/128/cendyne/wat" sizes="128px" loading="lazy"></p><div><p>It did not type "I've" correctly and conjoined the words "schema" and "refactored" into one.</p></div></div><div><p>my saas was built with Cursor, zero hand written code</p><p>AI is no longer just an assistant, it’s also the builder</p><p>Now, you can continue to whine about it or start building.</p><p>P.S. Yes, people pay for it</p></div><p>You cannot ask these tools today to develop a performant React application. You cannot ask these tools to implement a secure user registration flow. It will choose to execute functions like is user registered on the client instead of the server.</p><div><p><img width="128" height="128" alt="trash" src="https://cendyne.dev/s/128/cendyne/trash" sizes="128px" loading="lazy"></p><div><p>Others are learning this the hard way too.</p></div></div><div><p>guys, i'm under attack</p><p>ever since I started to share how I built my SaaS using Cursor</p><p>random thing are happening, maxed out usage on api keys, people bypassing the subscription, creating random shit on db</p><p>as you know, I'm not technical so this is taking me longer that usual to figure out</p><p>for now, I will stop sharing what I do publicly on X</p><p>there are just some weird ppl out there</p></div><p>Without expert intervention, the best these tools can do today is produce a somewhat functional mockup, where every future change beyond that risks destroying existing functionality.</p><p>I cannot — and would not — trust a team member who vibe codes in a production application. The constant negligence I observe when "Vibe Coding" is atrocious and unacceptable to a customer base of any size.</p><p>No available model demonstrates consistent and necessary attention to detail needed for a production environment. They are not yet equipped or designed to transform information involving multiple contexts inherent to producing a digital product.</p><p>These tools are optimized to produce solutions that fit in a single screen of markdown and are now being asked to do far more than they were trained for. As the context window overflows and the model degrades, it will fail to even format MCP calls correctly and upon reaching this point of no return, produces a log that comes across as being tortured. Like a robot losing a limb, it will try and try again to walk only to fall down until the editor pauses the conversation to save on resources.</p><p><img data-blurhash="L03+Dt~q%2ofM{WURjWBx]t7WUj[" data-width="645" data-height="628" data-ratio="true" src="https://cendyne.dev/c/xNtV9Ji3?width=645" alt="Let me try a different approach. Error calling tool." width="645" height="628" loading="lazy"></p><h2 id="working-around-the-problem">Working around the problem</h2><p>A modern <a href="https://en.wikipedia.org/wiki/Twitch_Plays_Pok%C3%A9mon">"Twitch plays Pokémon"</a> is going on right now: <a href="https://www.twitch.tv/claudeplayspokemon">Claude Plays Pokémon</a>. It mitigates this context window problem by starting a new context with seeded information provided by its previous incarnation in the form of many Markdown files, which it can then read as if new and search via MCP during its playthrough.</p><div><div><p>So, what makes this possible? Claude was given a knowledge base to store notes, vision to see the screen, and function calls which allow it to simulate button presses and navigate the game.</p><p>Together, they allow Claude to sustain gameplay with tens of thousands of interactions.</p></div><p><img data-blurhash="L04epF.7M-a1.6t6WDofI2VtxstR" data-width="645" data-height="645" data-ratio="true" src="https://cendyne.dev/c/a9W5MgiW?width=645" alt="Photo included with tweet" width="645" height="645" loading="lazy"></p></div><p>Even so, it can make bad assumptions and spend 43 hours intentionally blacking out over and over in Mt. Moon (an in-game route between story locations) making no effective progress towards achieving its next goal because by the time it could second guess itself, its context window is no longer fit to continue.</p><p><video poster="https://cendyne.dev/c/-PEsNE-L" preload="metadata" playsinline="" controls="" autoplay="" loop="" muted="" width="644" height="362"><source src="https://cendyne.dev/c-no-index/3RrkLRXm" type="video/mp4"><img data-blurhash="L98zorE19a~C^+IoIo-p9t%2xaE1" data-width="644" data-height="362" data-ratio="true" src="https://cendyne.dev/c/-PEsNE-L?width=645" alt="Claude plays pokemon going through a context clean up and restart" width="644" height="362" loading="lazy"></video></p><div><p><img width="128" height="128" alt="galaxy-brain2" src="https://cendyne.dev/s/128/cendyne/galaxy-brain2" sizes="128px" loading="lazy"></p><div><p>It did escape and progress, but only after the critic instance of the model suggested its assumption was incorrect.</p></div></div><p>After a context cleanup completes, which takes about five minutes (the video above is edited to the meaningful moments), the model proceeds to make the same mistakes its prior incarnation did. The notes it wrote are not meaningfully interpreted in context, I find the same happens too with the Cursor rules I write.</p><p>While increasing the length of the context window will improve some immediate experiences, this is a problem of scale that needs a different solution for agents to be more effective and, perhaps, move "Vibe Coding" closer to reality.</p><div><p><img width="107" height="128" alt="thinker" src="https://cendyne.dev/s/128/cendyne/thinker" sizes="128px" loading="lazy"></p><div><p>Would a formalized <a href="https://bulletjournal.com/">bullet journal</a> over MCP help a model be more complete in delivering more reliable results?</p></div></div><div><div><p>As long as the model correctly checks it before concluding its work is complete!</p></div><p><img width="128" height="123" alt="point-left" src="https://cendyne.dev/s/128/jacobi/point-left" sizes="128px" loading="lazy"></p></div><p><img data-blurhash="K5SF;MIV~q?vt7%Mxuxuj[" data-width="645" data-height="669" data-ratio="true" src="https://cendyne.dev/c/FD-gkvrg?width=645" alt="Bullet journal with ski examples" width="645" height="669" loading="lazy"></p><p>A bullet journal may be one of many tools that improve the reliability of the models we have today.</p><p>The next issue is that these models cannot ingest information from multiple concurrent real-time sources. In one terminal we may be running the server and in another some end-to-end tests. Both of these terminals were created at the agent's request. It either ignores or is not fed the stack trace logged by the server in the first terminal as it watches the output of the end-to-end tests fail and retry, fail and retry.</p><p>For agents to have the impact promised by the hype, LLMs need a robust mechanism to mimic the development of short and long term memory without fine-tuning the memories into the model.</p><p>Furthermore, for agents to contribute to a team, there must be a way to develop long-term memories bound to the organization and its products that seamlessly merge with and reconcile with memories personal to each team member.</p><p>And lastly, these memories have to be portable. As models improve and are integrated into our tools, domain specific memories must be usable by the next generation of large language models.</p><h2 id="conclusion">Conclusion</h2><p>"Vibe Coding" might get you 80% the way to a functioning concept. But to produce something reliable, secure, and worth spending money on, you’ll need experienced humans to do the hard work not possible with today’s models.</p><p>Agents do demonstrate enough capability that LinkedIn CEO influencers confidently spread the unreality that we can replace jobs with "agentic AI."</p><p>Agents do enable skilled people to create more independently than they ever have. For the time being, it will not replace those that can solve the hard problems that only experience and intuition can identify. Like other no-code solutions, agents do give the less skilled more capability than they had the day before. Until they develop their own competent skill set, "Vibe Coders" will not be able to release production quality software in this world, no matter how <em>exponential</em> the agent is over their own inferior skill set.</p><p>Keep an eye on how LLM agents develop and improve. For now, they are worth evaluating and discussing, but are not ready for us to delegate the precise task of creating reliable, secure, and scalable software that powers our society. "Vibe Coding" will not create the next big thing in 2025.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mathematical Methods for Physics [pdf] (120 pts)]]></title>
            <link>https://www.ma.imperial.ac.uk/~dturaev/Mathematical_Methods2021.pdf</link>
            <guid>43448193</guid>
            <pubDate>Sat, 22 Mar 2025 19:58:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ma.imperial.ac.uk/~dturaev/Mathematical_Methods2021.pdf">https://www.ma.imperial.ac.uk/~dturaev/Mathematical_Methods2021.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=43448193">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Italy demands Google poison DNS under strict Piracy Shield law (166 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/03/italian-court-orders-google-to-block-iptv-pirate-sites-at-dns-level/</link>
            <guid>43448112</guid>
            <pubDate>Sat, 22 Mar 2025 19:46:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/03/italian-court-orders-google-to-block-iptv-pirate-sites-at-dns-level/">https://arstechnica.com/gadgets/2025/03/italian-court-orders-google-to-block-iptv-pirate-sites-at-dns-level/</a>, See on <a href="https://news.ycombinator.com/item?id=43448112">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>Spotted by <a href="https://torrentfreak.com/court-orders-google-to-poison-public-dns-to-prevent-iptv-piracy-250321/">TorrentFreak</a>, AGCOM Commissioner Massimiliano Capitanio <a href="https://www.linkedin.com/feed/update/urn:li:activity:7308503541390741504/">took to LinkedIn</a> to celebrate the ruling, as well as the existence of the Italian Piracy Shield. "The Judge confirmed the value of AGCOM's investigations, once again giving legitimacy to a system for the protection of copyright that is unique in the world," said Capitanio.</p>
<p>Capitanio went on to complain that Google has routinely ignored AGCOM's listing of pirate sites, which are supposed to be blocked in 30 minutes or less under the law. He noted the violation was so clear-cut that the order was issued without giving Google a chance to respond, known as <em>inaudita altera parte</em> in Italian courts.</p>
<p>This decision follows a similar case against Internet backbone firm Cloudflare. In January, the Court of Milan found that Cloudflare's CDN, DNS server, and WARP VPN were facilitating piracy. The court threatened Cloudflare with fines of up to 10,000 euros per day if it did not begin blocking the sites.</p>
<p>Google could face similar sanctions, but AGCOM has had difficulty getting international tech behemoths to acknowledge their legal obligations in the country. We've reached out to Google for comment and will update this report if we hear back.</p>


          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NixOS and reproducible builds could have detected the xz backdoor (266 pts)]]></title>
            <link>https://luj.fr/blog/how-nixos-could-have-detected-xz.html</link>
            <guid>43448075</guid>
            <pubDate>Sat, 22 Mar 2025 19:39:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://luj.fr/blog/how-nixos-could-have-detected-xz.html">https://luj.fr/blog/how-nixos-could-have-detected-xz.html</a>, See on <a href="https://news.ycombinator.com/item?id=43448075">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h2 id="introduction">Introduction</h2>
<p>In March 2024, a backdoor was discovered in <code>xz</code>, a (de)-compression software that is regularly used at the core of Linux distributions to unpack source tarballs of packaged software. The backdoor had been covertly inserted by a malicious maintainer under the pseudonym of <em>Jia Tan</em> over a period of three years. This event deeply stunned the open source community as the attack was both of <strong>massive impact</strong> (it allowed <em>remote code execution</em> on all affected machines that had <code>ssh</code> installed) and <strong>extremely difficult to detect</strong>. In fact, it was only thanks to the diligence (and maybe luck) of Andres Freund – a Postgres developer working at Microsoft – that the catastrophe was avoided: while investigating a seemingly unrelated 500ms performance regression in <code>ssh</code> that he was experiencing on several <em>Debian unstable</em> machines, he was able to trace it back to the <code>liblzma</code> library, identify the backdoor and document it.</p>
<p>While it was already established that the open source supply chain was often the target of malicious actors, what is stunning is the amount of energy invested by <em>Jia Tan</em> to gain the trust of the maintainer of the <code>xz</code> project, acquire push access to the repository and then among other perfectly legitimate contributions insert – piece by piece – the code for a very sophisticated and obfuscated backdoor. This should be a wake up call for the OSS community. We should consider the open source supply chain a high value target for powerful threat actors, and to collectively find countermeasures against such attacks.</p>
<p>In this article, I’ll discuss the inner workings of the <code>xz</code> backdoor and how I think we could have mechanically detected it thanks to build reproducibility.</p>
<h2 id="how-does-the-attack-work">How does the attack work?</h2>
<p>The main intent of the backdoor is to allow for <em>remote code execution</em> on the target by hijacking the <code>ssh</code> program. To do that, it replaces the behavior of some of <code>ssh</code>’s functions (most importantly the <code>RSA_public_decrypt</code> one) in order to allow an attacker to execute arbitrary commands on a victim’s machine when some specific RSA key is used to log in. Two main pieces are combined to put together to install and activate the backdoor:</p>
<ol type="1">
<li><p><strong>A script to de-obfuscate and install a malicious object file as part of the <code>xz</code> build process.</strong>
Interestingly the backdoor was not comprehensively contained in the source code for <code>xz</code>. Instead, the malicious components were only contained in tarballs built and signed by the malicious maintainer <em>Jia Tan</em> and published alongside releases <code>5.6.0</code> and <code>5.6.1</code> of <code>xz</code>. This time the additional release tarball contained slight and disguised modifications to extract a malicious object file from the <code>.xz</code> files used as data for some test contained in the repository.</p></li>
<li><p><strong>A procedure to hook the <code>RSA_public_decrypt</code> function.</strong> The backdoor uses the <em>ifunc</em> mechanism of <code>glibc</code> to modify the address of the <code>RSA_public_function</code> when <code>ssh</code> is loaded, in case <code>ssh</code> links against <code>liblzma</code> through <code>libsystemd</code>.</p></li>
</ol>



<h2 id="a-script-to-de-obfuscate-and-install-a-malicious-object-file-as-part-of-the-xz-build-process">1. A script to de-obfuscate and install a malicious object file as part of the <code>xz</code> build process</h2>
<p>As explained above, the malicious object file is stored directly in the <code>xz</code> git repository, hidden in some test files. The project being a decompression software, test cases include <code>.xz</code> files to be decompressed, making it possible to hide some machine code into fake test files;
<strong>The backdoor is not active in the code contained in the git repository, it is only included by building <code>xz</code> from the tarball released by the project</strong>, which has a few differences with the actual contents of the repository, most importantly in the <code>m4/build-to-host.m4</code> file.</p>
<pre tabindex="0"><code><span><span>diff --git a/m4/build-to-host.m4 b/m4/build-to-host.m4</span></span>
<span><span>index f928e9ab..d5ec3153 100644</span></span>
<span><span>--- a/m4/build-to-host.m4</span></span>
<span><span>+++ b/m4/build-to-host.m4</span></span>
<span><span>@@</span><span> -1,4 +1,4 </span><span>@@</span></span>
<span><span>-</span><span># build-to-host.m4 serial 3</span></span>
<span><span>+</span><span># build-to-host.m4 serial 30</span></span>
<span><span> dnl Copyright (C) 2023-2024 Free Software Foundation, Inc.</span></span>
<span><span> dnl This file is free software; the Free Software Foundation</span></span>
<span><span> dnl gives unlimited permission to copy and/or distribute it,</span></span>
<span><span>@@</span><span> -37,6 +37,7 </span><span>@@</span><span> AC_DEFUN([gl_BUILD_TO_HOST],</span></span>
<span></span>
<span><span>   dnl Define somedir_c.</span></span>
<span><span>   gl_final_[$1]="$[$1]"</span></span>
<span><span>+</span><span>  gl_[$1]_prefix=`echo $gl_am_configmake | sed "s/.*\.//g"`</span></span>
<span><span>   dnl Translate it from build syntax to host syntax.</span></span>
<span><span>   case "$build_os" in</span></span>
<span><span>     cygwin*)</span></span>
<span><span>@@</span><span> -58,14 +59,40 </span><span>@@</span><span> AC_DEFUN([gl_BUILD_TO_HOST],</span></span>
<span><span>   if test "$[$1]_c_make" = '\"'"${gl_final_[$1]}"'\"'; then</span></span>
<span><span>     [$1]_c_make='\"$([$1])\"'</span></span>
<span><span>   fi</span></span>
<span><span>+</span><span>  if test "x$gl_am_configmake" != "x"; then</span></span>
<span><span>+</span><span>    gl_[$1]_config='sed \"r\n\" $gl_am_configmake | eval $gl_path_map | $gl_[$1]_prefix -d 2&gt;/dev/null'</span></span>
<span><span>+</span><span>  else</span></span>
<span><span>+</span><span>    gl_[$1]_config=''</span></span>
<span><span>+</span><span>  fi</span></span>
<span><span>+</span><span>  _LT_TAGDECL([], [gl_path_map], [2])dnl</span></span>
<span><span>+</span><span>  _LT_TAGDECL([], [gl_[$1]_prefix], [2])dnl</span></span>
<span><span>+</span><span>  _LT_TAGDECL([], [gl_am_configmake], [2])dnl</span></span>
<span><span>+</span><span>  _LT_TAGDECL([], [[$1]_c_make], [2])dnl</span></span>
<span><span>+</span><span>  _LT_TAGDECL([], [gl_[$1]_config], [2])dnl</span></span>
<span><span>   AC_SUBST([$1_c_make])</span></span>
<span><span>+</span></span>
<span><span>+</span><span>  dnl If the host conversion code has been placed in $gl_config_gt,</span></span>
<span><span>+</span><span>  dnl instead of duplicating it all over again into config.status,</span></span>
<span><span>+</span><span>  dnl then we will have config.status run $gl_config_gt later, so it</span></span>
<span><span>+</span><span>  dnl needs to know what name is stored there:</span></span>
<span><span>+</span><span>  AC_CONFIG_COMMANDS([build-to-host], [eval $gl_config_gt | $SHELL 2&gt;/dev/null], [gl_config_gt="eval \$gl_[$1]_config"])</span></span>
<span><span> ])</span></span>
<span></span>
<span><span> dnl Some initializations for gl_BUILD_TO_HOST.</span></span>
<span><span> AC_DEFUN([gl_BUILD_TO_HOST_INIT],</span></span>
<span><span> [</span></span>
<span><span>+</span><span>  dnl Search for Automake-defined pkg* macros, in the order</span></span>
<span><span>+</span><span>  dnl listed in the Automake 1.10a+ documentation.</span></span>
<span><span>+</span><span>  gl_am_configmake=`grep -aErls "#{4}[[:alnum:]]{5}#{4}$" $srcdir/ 2&gt;/dev/null`</span></span>
<span><span>+</span><span>  if test -n "$gl_am_configmake"; then</span></span>
<span><span>+</span><span>    HAVE_PKG_CONFIGMAKE=1</span></span>
<span><span>+</span><span>  else</span></span>
<span><span>+</span><span>    HAVE_PKG_CONFIGMAKE=0</span></span>
<span><span>+</span><span>  fi</span></span>
<span><span>+</span></span>
<span><span>   gl_sed_double_backslashes='s/\\/\\\\/g'</span></span>
<span><span>   gl_sed_escape_doublequotes='s/"/\\"/g'</span></span>
<span><span>+</span><span>  gl_path_map='tr "\t \-_" " \t_\-"'</span></span>
<span><span> changequote(,)dnl</span></span>
<span><span>   gl_sed_escape_for_make_1="s,\\([ \"&amp;'();&lt;&gt;\\\\\`|]\\),\\\\\\1,g"</span></span>
<span><span> changequote([,])dnl</span></span>
<span></span></code></pre>

<p>While these changes may look benign to the naive eyes and well commented, they are actually hiding a chain of commands that decrypts/deobfuscates several fake <code>.xz</code> test files to ultimately produce two files:</p>
<ul>
<li>a shell script that is run during the build of <code>xz</code> ;</li>
<li>a malicious binary object file.</li>
</ul>
<p>There is an <a href="https://research.swtch.com/xz-script">excellent analysis from Russ Cox</a> that explains in detail how these two malicious resources are produced during the build process, and I advise any interested reader to find all relevant details there.</p>
<p>The shell script run during the build has two main purposes:</p>
<ol type="1">
<li>Verifying that the conditions to execute the backdoor are met on the builder (the backdoor targets specific Linux distributions, needs specific features of the <code>glibc</code> activated, needs <code>ssh</code> installed, etc) ;</li>
<li>Modifying the (legitimate) <code>liblzma_la-crc64_fast.o</code> to use the <code>_get_cpuid</code> symbol defined in the backdoor object file.</li>
</ol>
<h2 id="a-procedure-to-hook-the-rsa_public_decrypt-function">2. A procedure to hook the <code>RSA_public_decrypt</code> function</h2>
<p>So how does a backdoor in the <code>xz</code> executable have any effect on <code>ssh</code>?
To understand that, we have to take a little detour in the realm of dynamic loaders and dynamically linked programs. Whenever a program depends on a library, there are two ways that library can be linked into the final executable:</p>
<ul>
<li>statically, in that case the library is embedded into the final executable, hence increasing its size ;</li>
<li>dynamically, in which case it is the role of the dynamic loader (<code>ld-linux.so</code> in Linux) to find that shared library when the program starts and load it in memory.</li>
</ul>
<p>When a program is compiled using dynamic linking, the addresses of the symbols belonging to dynamically linked libraries cannot be provided at compilation time: their position in memory is not know ahead of time! Instead, a reference to the <em>Global Offset Table</em> (or <em>GOT</em>) is inserted. When the program is started, the actual addresses are filled in the GOT by the dynamic linker.</p>
<p>The <code>xz</code> backdoor uses a functionality of the <code>glibc</code> called <em>ifunc</em> to force execution of code during dynamic loading time: <em>ifunc</em> is designed to allow selection between several implementations of the same function at dynamic loading time.</p>
<pre tabindex="0"><code><span><span>#include</span><span> &lt;stdio.h&gt;</span></span>
<span></span>
<span><span>// Declaration of ifunc resolver function</span></span>
<span><span>int</span><span> (</span><span>*</span><span>resolve_add</span><span>(</span><span>void</span><span>))(</span><span>int</span><span>,</span><span> int</span><span>);</span></span>
<span></span>
<span><span>// First version of the add function</span></span>
<span><span>int</span><span> add_v1</span><span>(</span><span>int</span><span> a</span><span>,</span><span> int</span><span> b</span><span>)</span><span> {</span></span>
<span><span>    printf</span><span>(</span><span>"Using add_v1</span><span>\n</span><span>"</span><span>);</span></span>
<span><span>    return</span><span> a </span><span>+</span><span> b</span><span>;</span></span>
<span><span>}</span></span>
<span></span>
<span><span>// Second version of the add function</span></span>
<span><span>int</span><span> add_v2</span><span>(</span><span>int</span><span> a</span><span>,</span><span> int</span><span> b</span><span>)</span><span> {</span></span>
<span><span>    printf</span><span>(</span><span>"Using add_v2</span><span>\n</span><span>"</span><span>);</span></span>
<span><span>    return</span><span> a </span><span>+</span><span> b</span><span>;</span></span>
<span><span>}</span></span>
<span></span>
<span><span>// Resolver function that chooses the correct version of the function</span></span>
<span><span>int</span><span> (</span><span>*</span><span>resolve_add</span><span>(</span><span>void</span><span>))(</span><span>int</span><span>,</span><span> int</span><span>)</span><span> {</span></span>
<span><span>    // You can implement any runtime check here.</span></span>
<span><span>    // In that case we check if the system is 64bit</span></span>
<span><span>    if</span><span> (</span><span>sizeof</span><span>(</span><span>void</span><span>*</span><span>)</span><span> ==</span><span> 8</span><span>)</span><span> {</span></span>
<span><span>        return</span><span> add_v2</span><span>;</span></span>
<span><span>    }</span><span> else</span><span> {</span></span>
<span><span>        return</span><span> add_v1</span><span>;</span></span>
<span><span>    }</span></span>
<span><span>}</span></span>
<span></span>
<span><span>// Define the ifunc attribute for the add function</span></span>
<span><span>int</span><span> add</span><span>(</span><span>int</span><span> a</span><span>,</span><span> int</span><span> b</span><span>)</span><span> __attribute__</span><span>((</span><span>ifunc</span><span>(</span><span>"resolve_add"</span><span>)));</span></span>
<span></span>
<span><span>int</span><span> main</span><span>()</span><span> {</span></span>
<span><span>    int</span><span> result </span><span>=</span><span> add</span><span>(</span><span>10</span><span>,</span><span> 20</span><span>);</span></span>
<span><span>    printf</span><span>(</span><span>"Result: %d</span><span>\n</span><span>"</span><span>,</span><span> result</span><span>);</span></span>
<span><span>    return</span><span> 0</span><span>;</span></span>
<span><span>}</span></span>
<span></span></code></pre>

<p>In the above example, the <em>ifunc</em> attribute surrounding the <code>add</code> function indicates that the version that will be executed will be determined at dynamic loading time by running the <code>resolve_add</code> function. In that case, the <code>resolve_add</code> function returns <code>add_v1</code> or <code>add_v2</code> depending if the running system is a 64 bit system or not – and as such is completely harmless – but this technique is used by the <code>xz</code> backdoor to run some malicious code at dynamic loading time.</p>
<p><em>But dynamic loading of which program?</em> Well, of <code>ssh</code>! In some Linux distributions (Debian and Fedora for example), <code>ssh</code> is patched to support <code>systemd</code> notifications and for this purpose, links with <code>libsystemd</code>, that in turn links with <code>liblzma</code>. In those distribution <code>sshd</code> hence has a transitive dependency on <code>liblzma</code>.</p>
<figure id="fig:SED-HR4049">
<img src="https://luj.fr/assets/links.png">
<figcaption>Dependency chain between <code>sshd</code> and <code>liblzma</code></figcaption>
</figure>
<p>This is how the backdoor works: whenever <code>sshd</code> is executed, the dynamic loader loads <code>libsystemd</code> and then <code>liblzma</code>. With the backdoor installed, and leveraging the <em>ifunc</em> functionality as explained above, the backdoor is able to run arbitrary code when <code>liblzma</code> is being loaded. Indeed, as you remember from the previous section, the backdoor script modifies one of the legitimate <code>xz</code> object files: it actually modifies the resolver of one of the functions that uses <em>ifunc</em> to call its own malicious <code>_get_cpuid</code> symbol. When called, this function meddles with the GOT (that is not yet read-only at this time of execution) to modify the address of the <code>RSA_public_decrypt</code> function, replacing it by a malicious one! That’s it, at this point <code>sshd</code> uses the malicious <code>RSA_public_decrypt</code> function that gives RCE privileges to the attacker.</p>
<p>Once again, there exist more precise reports on exactly how the hooking happens that a curious reader might read, like <a href="https://securelist.com/xz-backdoor-story-part-1/112354/">this one</a> for example. There is also <a href="https://arxiv.org/pdf/2404.08987">a research article</a> summarizing the attack vector and possible mitigations that I recommend reading.</p>
<h2 id="avoiding-the-xz-catastrophe-in-the-future">Avoiding the <code>xz</code> catastrophe in the future</h2>
<p>What should our takeaways be from this near-miss and what should we do to minimize the risks of such an attack happening again in the future? Obviously, there is a lot to be said about the social issues at play here<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> and how we can build better resilience in the OSS ecosystem against malicious entities taking over really fundamental OSS projects, but in this piece I’ll only address the technical aspects of the question.</p>
<p>People are often convinced that OSS is more trustworthy than closed-source software because the code can be audited by practitioners and security professionals in order to detect vulnerabilities or backdoors. In this instance, this procedure has been made difficult by the fact that part of the code activating the backdoor was not included in the sources available within the git repository but was instead present in the maintainer-provided tarball. While this was used to hide the backdoor out of sight of most investigating eyes, this is also an opportunity for us to improve our software supply chain security processes.</p>
<h2 id="building-software-from-trusted-sources">Building software from trusted sources</h2>
<p>One immediate observation that we can make in reaction to this supply chain incident is that it was only effective because a lot of distributions were using the maintainer provided tarball to build <code>xz</code> instead of the raw source code supplied by the git forge (in this case, GitHub). This reliance on release tarballs has plenty of historical and practical reasons:</p>
<ul>
<li>the tarball workflow predates the existence of <code>git</code> and was used in the earliest Linux distributions;</li>
<li>tarballs are self-contained archives that encapsulate the exact state of the source code intended for release while git repositories can be altered, creating the need for a snapshot of the code;</li>
<li>tarballs can contain intermediary artifacts (for example manpages) used to lighten the build process, or configure scripts to target specific hardware, etc;</li>
<li>tarballs allow the source code to be compressed which is useful for space efficiency.</li>
</ul>
<p>This being said, these reasons do not weigh enough in my opinion to justify the security risks they create. In all places where it is technically feasible, we should build software from sources authenticated by the most trustworthy party. For example, if a project is developed on GitHub, an archive is automatically generated by GitHub for each release. The risk of a compromise of that release archive is far lower than the risk of a malicious maintainer distributing unfaithful tarballs, as it would require compromising the GitHub infrastructure (and at this point the problem is much more serious). This reasoning can be extended in all cases where the development is happening on a platform operated by a trusted third party like Codeberg/SourceHut/Gitlab, etc.</p>
<h3 id="when-the-situation-allows-it">When the situation allows it…</h3>
<p><strong>NixOS</strong> is a distribution built on the functional package management model, that is to say every package is encoded as an expression written in Nix, a functional programming language. A Nix expression for a software project is usually a function mapping all the project dependencies to a “build recipe” that can be later executed to build the package. I am a NixOS developer and I was surprised when the backdoor was revealed to see that the malicious version of <code>xz</code> had ended up being distributed to our users<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>. While there is no policy about this, there is a culture among NixOS maintainers of using the source archive automatically generated by GitHub (that are simply snapshots of the source code) when available through the <code>fetchFromGitHub</code> function. In the simplified example of the <code>xz</code> package below, you can see that the sources for the package are actually extracted from the manually uploaded <em>malicious</em> maintainer provided tarball through another source fetcher: <code>fetchurl</code>.</p>
<pre tabindex="0"><code><span><span>{</span><span> lib</span><span>,</span><span> stdenv</span><span>,</span><span> fetchurl</span></span>
<span><span>,</span><span> enableStatic</span><span> ?</span><span> stdenv</span><span>.</span><span>hostPlatform</span><span>.</span><span>isStatic</span></span>
<span><span>}:</span></span>
<span></span>
<span><span>stdenv</span><span>.</span><span>mkDerivation </span><span>rec</span><span> {</span></span>
<span><span>  pname</span><span> =</span><span> "xz"</span><span>;</span></span>
<span><span>  version</span><span> =</span><span> "5.6.0"</span><span>;</span></span>
<span></span>
<span><span>  src</span><span> =</span><span> fetchurl </span><span>{</span></span>
<span><span>    url</span><span> =</span><span> "https://github.com/tukaani-project/xz/releases/download/v</span><span>${</span><span>version</span><span>}</span><span>/xz-</span><span>${</span><span>version</span><span>}</span><span>.tar.xz"</span><span>;</span></span>
<span><span>    hash</span><span> =</span><span> "sha256-AWGCxwu1x8nrNGUDDjp/a6ol4XsOjAr+kncuYCGEPOI="</span><span>;</span></span>
<span><span>  };</span></span>
<span><span>...</span></span>
<span><span>}</span></span>
<span></span></code></pre>

<p>To understand why, we must first talk about the bootstrap of <code>nixpkgs</code>. The concept of a bootstrap is the idea that one could rebuild all of the packages in <code>nixpkgs</code> from a small set of seed binaries. This is an important security property because it means that there are no other external tools that one must trust in order to trust the toolchain that is used to build the software distribution. What we call the “bootstrap” in the context of a software distribution like <code>nixpkgs</code>, is all the steps needed to build the basic compilation environment to be used by other packages, called <code>stdenv</code> in nixpkgs. Building <code>stdenv</code> is not an easy task; how does one build <code>gcc</code> when one doesn’t even have a C compiler? The answer is that you start from a very small binary that does nothing fancy but is enough to build <code>hex</code>, a minimalist assembler, which in turn can build a more complex assembler, and this until we are able to build more complex software and finally a modern C compiler. The bootstraping story of Nix/Guix is an incredibly interesting topic, that I will not cover extensively here, but I strongly advise reading blog posts from the Guix community, that are on the bleeding edge (they have <a href="https://guix.gnu.org/en/blog/2023/the-full-source-bootstrap-building-from-source-all-the-way-down/">introduced a 357-byte bootstrap</a> that is being adapted for nixpkgs).</p>
<p>What does all that has to do with <code>xz</code> though? Well, <code>xz</code> is included in the nixpkgs bootstrap!</p>
<pre tabindex="0"><code><span><span>$</span><span> nix-build -A stdenv</span></span>
<span><span>/nix/store/91d27rjqlhkzx7mhzxrir1jcr40nyc7p-stdenv-linux</span></span>
<span><span>$</span><span> nix-store --query --graph result</span></span>
<span></span></code></pre>

<p><img src="https://luj.fr/assets/runtime.png"></p>
<p>We can see now that <code>stdenv</code> depends at runtime on <code>xz</code>, so it is indeed built during the bootstrap stage. To understand a bit more why this is the case, I’ll also generate a graph of the software in <code>stdenv</code> that depends on <code>xz</code> at buildtime.</p>
<pre tabindex="0"><code><span><span>$</span><span> nix-store --query --graph </span><span>$(</span><span>nix-eval</span><span> --raw</span><span> -f</span><span> default</span><span> stdenv.drvPath</span><span>)</span></span>
<span></span></code></pre>

<p><img src="https://luj.fr/assets/buildtime.png"></p>
<p>We can see that several packages depend on <code>xz</code>. Let’s take <code>coreutils</code> for example and try to understand why it depends on <code>xz</code> by reading its derivation file, which is the intermediary representation of the build process obtained by evaluating the Nix expression for <code>coreutils</code>:</p>
<pre tabindex="0"><code><span><span>{</span></span>
<span><span>  "</span><span>/nix/store/57hlz5fnvfgljivf7p18fmcl1yp6d29z-coreutils-9.5.drv</span><span>"</span><span>:</span><span> {</span></span>
<span><span>    "</span><span>args</span><span>"</span><span>:</span><span> [</span></span>
<span><span>      "-e"</span><span>,</span></span>
<span><span>      "/nix/store/v6x3cs394jgqfbi0a42pam708flxaphh-default-builder.sh"</span></span>
<span><span>    ],</span></span>
<span><span>    "</span><span>builder</span><span>"</span><span>:</span><span> "/nix/store/razasrvdg7ckplfmvdxv4ia3wbayr94s-bootstrap-tools/bin/bash"</span><span>,</span></span>
<span></span>
<span><span>      ...</span></span>
<span></span>
<span><span>    "</span><span>inputDrvs</span><span>"</span><span>:</span><span> {</span></span>
<span></span>
<span><span>      ...</span></span>
<span></span>
<span><span>      "</span><span>/nix/store/c0wk92pcxbxi7579xws6bj12mrim1av6-xz-5.6.2.drv</span><span>"</span><span>:</span><span> {</span></span>
<span><span>        "</span><span>dynamicOutputs</span><span>"</span><span>:</span><span> {},</span></span>
<span><span>        "</span><span>outputs</span><span>"</span><span>:</span><span> [</span></span>
<span><span>          "bin"</span></span>
<span><span>        ]</span></span>
<span><span>      },</span></span>
<span><span>      "</span><span>/nix/store/xv4333kfggq3zn065a3pwrj7ddbs4vzg-coreutils-9.5.tar.xz.drv</span><span>"</span><span>:</span><span> {</span></span>
<span><span>        "</span><span>dynamicOutputs</span><span>"</span><span>:</span><span> {},</span></span>
<span><span>        "</span><span>outputs</span><span>"</span><span>:</span><span> [</span></span>
<span><span>          "out"</span></span>
<span><span>        ]</span></span>
<span><span>      }</span></span>
<span><span>    },</span></span>
<span></span>
<span><span>    ...</span></span>
<span></span>
<span><span>    "</span><span>system</span><span>"</span><span>:</span><span> "x86_64-linux"</span></span>
<span><span>  }</span></span>
<span><span>}</span></span>
<span></span>
<span></span></code></pre>

<p>The <code>inputDrvs</code> field here correspond to all the other packages or expressions that the <code>coreutils</code> build process depends on. We see that in particular it depends on two components:</p>
<ul>
<li><code>/nix/store/c0wk92pcxbxi7579xws6bj12mrim1av6-xz-5.6.2.drv</code>, which is <code>xz</code> itself;</li>
<li><code>/nix/store/xv4333kfggq3zn065a3pwrj7ddbs4vzg-coreutils-9.5.tar.xz.drv</code> which is a source archive for <code>coreutils</code>! As it is a <code>.xz</code> archive, we need <code>xz</code> to unpack it and that is where the dependency comes from!</li>
</ul>
<p>The same reasoning applies to the other three direct dependencies that we could see in the graph earlier.</p>
<p><code>xz</code> being built as part of the bootstrap means it doesn’t have access to all the facilities normal packages in nixpkgs can rely on. In particular it can only access packages that are built <em>before</em> in bootstrap. For example, to build <code>xz</code> from sources, we need <code>autoconf</code> to generate the configure script. But <code>autoconf</code> has a dependency on <code>xz</code>! Using the maintainer tarball allows us to break this dependency cycle.</p>
<pre tabindex="0"><code><span><span>$</span><span> nix why-depends --derivation nixpkgs#autoconf nixpkgs#xz</span></span>
<span><span>/nix/store/2rajzdx3wkivlc38fyhj0avyp10k2vjj-autoconf-2.72.drv</span></span>
<span><span>└───/nix/store/jnnb5ihdh6r3idmqrj2ha95ir42icafq-stdenv-linux.drv</span></span>
<span><span>    └───/nix/store/sqwqnilfwkw6p2f5gaj6n1xlsy054fnw-xz-5.6.4.drv</span></span>
<span></span></code></pre>

<p>In conclusion, at the point in the <code>nixpkgs</code> graph where the <code>xz</code> package is built, the GitHub source archive cannot be used and we have to rely on the maintainer provided tarball, and hence, trust it. That does not mean that further verification cannot be implemented in <code>nixpkgs</code>, though…</p>
<h2 id="building-trust-into-untrusted-release-tarballs">Building trust into untrusted release tarballs</h2>
<p>To recap, the main reason that made NixOS vulnerable to the <code>xz</code> attack is that it is built as part of the bootstrap phase, at a point where we rely on maintainer-provided tarballs instead of the ones generated by GitHub. This incident shows that we should have specific protections in place, to ensure software built as part of our bootstrap is trustworthy.</p>
<h3 id="by-comparing-sources">1. By comparing sources</h3>
<p>One idea that comes to mind is that it should be easy, as a distribution, to verify that the sources tarballs we are using are indeed identical to the GitHub ones. There was even <a href="https://github.com/NixOS/nixpkgs/pull/300542">a pull request opened to introduce such a protection scheme</a>. While this seem like a natural idea, it doesn’t really work in practice: it’s not that rare that the maintainer provided tarball differs from the sources, and it’s often nothing to worry about.</p>
<blockquote data-embed-url="https://mastodon.social/@bagder/112181123475212554/embed"> <a href="https://mastodon.social/@bagder/112181123475212554" target="_blank"> <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="32" height="32" viewBox="0 0 79 75"><path d="M74.7135 16.6043C73.6199 8.54587 66.5351 2.19527 58.1366 0.964691C56.7196 0.756754 51.351 0 38.9148 0H38.822C26.3824 0 23.7135 0.756754 22.2966 0.964691C14.1319 2.16118 6.67571 7.86752 4.86669 16.0214C3.99657 20.0369 3.90371 24.4888 4.06535 28.5726C4.29578 34.4289 4.34049 40.275 4.877 46.1075C5.24791 49.9817 5.89495 53.8251 6.81328 57.6088C8.53288 64.5968 15.4938 70.4122 22.3138 72.7848C29.6155 75.259 37.468 75.6697 44.9919 73.971C45.8196 73.7801 46.6381 73.5586 47.4475 73.3063C49.2737 72.7302 51.4164 72.086 52.9915 70.9542C53.0131 70.9384 53.0308 70.9178 53.0433 70.8942C53.0558 70.8706 53.0628 70.8445 53.0637 70.8179V65.1661C53.0634 65.1412 53.0574 65.1167 53.0462 65.0944C53.035 65.0721 53.0189 65.0525 52.9992 65.0371C52.9794 65.0218 52.9564 65.011 52.9318 65.0056C52.9073 65.0002 52.8819 65.0003 52.8574 65.0059C48.0369 66.1472 43.0971 66.7193 38.141 66.7103C29.6118 66.7103 27.3178 62.6981 26.6609 61.0278C26.1329 59.5842 25.7976 58.0784 25.6636 56.5486C25.6622 56.5229 25.667 56.4973 25.6775 56.4738C25.688 56.4502 25.7039 56.4295 25.724 56.4132C25.7441 56.397 25.7678 56.3856 25.7931 56.3801C25.8185 56.3746 25.8448 56.3751 25.8699 56.3816C30.6101 57.5151 35.4693 58.0873 40.3455 58.086C41.5183 58.086 42.6876 58.086 43.8604 58.0553C48.7647 57.919 53.9339 57.6701 58.7591 56.7361C58.8794 56.7123 58.9998 56.6918 59.103 56.6611C66.7139 55.2124 73.9569 50.665 74.6929 39.1501C74.7204 38.6967 74.7892 34.4016 74.7892 33.9312C74.7926 32.3325 75.3085 22.5901 74.7135 16.6043ZM62.9996 45.3371H54.9966V25.9069C54.9966 21.8163 53.277 19.7302 49.7793 19.7302C45.9343 19.7302 44.0083 22.1981 44.0083 27.0727V37.7082H36.0534V27.0727C36.0534 22.1981 34.124 19.7302 30.279 19.7302C26.8019 19.7302 25.0651 21.8163 25.0617 25.9069V45.3371H17.0656V25.3172C17.0656 21.2266 18.1191 17.9769 20.2262 15.568C22.3998 13.1648 25.2509 11.9308 28.7898 11.9308C32.8859 11.9308 35.9812 13.492 38.0447 16.6111L40.036 19.9245L42.0308 16.6111C44.0943 13.492 47.1896 11.9308 51.2788 11.9308C54.8143 11.9308 57.6654 13.1648 59.8459 15.568C61.9529 17.9746 63.0065 21.2243 63.0065 25.3172L62.9996 45.3371Z" fill="currentColor"></path></svg> <p>Post by @bagder@mastodon.social</p> <p>View on Mastodon</p> </a> </blockquote> 

<p>As Daniel Stenberg (the maintainer of <code>curl</code>) explains, the release tarball being different than the source is a <em>feature</em>: it allows the maintainer to include intermediary artifacts like manpages or configure scripts for example (this is especially useful for distributions that want to get rid of the dependency on <code>autoconf</code> to build the program). Of course when we care about software supply chain security, this flexibility that project maintainers have in the way they provide the release assets is actually a liability because it forces us to trust them to do it honestly.</p>
<h3 id="leveraging-bitwise-reproducibility">2. Leveraging bitwise reproducibility</h3>
<p><strong>Reproducible builds</strong> is a property of a software project that is verified if building it twice in the same conditions yields the exact same (bitwise identical) artifacts. Build reproducibility is not something easy to obtain, as there are all kinds of nondeterminisms that can happen in build processes, and making as many packages as possible reproducible is the purpose of the <a href="https://reproducible-builds.org/">reproducible-builds</a> group. It is also a property recognized as instrumental to increase the trust in the distribution of binary artifacts (see <a href="https://arxiv.org/abs/2104.06020">Reproducible Builds: Increasing the Integrity of Software Supply Chains</a> for a detailed report).</p>
<p>There are several ways bitwise reproducibility could be used to build up trust in untrusted maintainer provided tarballs:</p>
<ol>
<li><p>Reproducibly building the tarball</p>
<p>A first approach that has been <a href="https://peter.eisentraut.org/blog/2024/08/13/the-new-postgresql-17-make-dist">adopted by the postgresql project</a> is to make the tarball generation process reproducible. This allows any user (or a linux distribution) to independently verify that the maintainer provided tarball was honestly generated from the original source code.</p>
<p><img src="https://luj.fr/assets/reproducible-tarball.png"></p>
<p>With this method, you can keep some advantages of building from tarballs (including the tarball containing some intermediary build artifacts like manpages or configure scripts). However, the drawback of this approach for software supply chain security is that it has to be implemented by upstream project maintainers. This means that adoption of this kind of security feature will probably be slow in the FOSS community, and while it is a good practice to make <em>everything</em> reproducible, including the tarball generation process, this is not the most effective way to increase software supply chain security <em>today</em>.</p></li>
<li><p>Checking for build convergence between various starting assets</p>



<p>Assuming <code>xz</code> is bitwise reproducible (and that is indeed the case), and that the maintainer provided tarball doesn’t contain any modification that impacts the build process, building it from the GitHub tarball or from the maintainer provided tarball <em>should</em> produce the same artifacts, right? Based on this idea, my proposal is to build <code>xz</code> a second time <em>after</em> the bootstrap, this time using the GitHub tarball (which is only possible after the bootstrap). If both builds differ we can suspect that there a suspicion of a supply chain compromise.</p>
<figure id="fig:SED-HR4049">
<img src="https://luj.fr/assets/sumary-method.png">
<figcaption>Summary of the method I propose to detect vulnerable <code>xz</code> source tarballs</figcaption>
</figure>
<p>Let’s see how this could be implemented:</p>
<p>First, we rewrite the <code>xz</code> package, this time using the <code>fetchFromGitHub</code> function. I create a <code>after-boostrap.nix</code> file alongside the original <code>xz</code> expression in the <code>pkgs/tools/compression/xz</code> directory of <code>nixpkgs</code>:</p>
<pre tabindex="0"><code><span><span>  {</span></span>
<span><span>  lib</span><span>,</span></span>
<span><span>  stdenv</span><span>,</span></span>
<span><span>  fetchurl</span><span>,</span></span>
<span><span>  enableStatic</span><span> ?</span><span> false</span><span>,</span></span>
<span><span>  writeScript</span><span>,</span></span>
<span><span>  fetchFromGitHub</span><span>,</span></span>
<span><span>  testers</span><span>,</span></span>
<span><span>  gettext</span><span>,</span></span>
<span><span>  autoconf</span><span>,</span></span>
<span><span>  libtool</span><span>,</span></span>
<span><span>  automake</span><span>,</span></span>
<span><span>  perl538Packages</span><span>,</span></span>
<span><span>  doxygen</span><span>,</span></span>
<span><span>  xz</span><span>,</span></span>
<span><span>}:</span></span>
<span></span>
<span><span>stdenv</span><span>.</span><span>mkDerivation </span><span>(</span><span>finalAttrs</span><span>:</span><span> {</span></span>
<span><span>  pname</span><span> =</span><span> "xz"</span><span>;</span></span>
<span><span>  version</span><span> =</span><span> "5.6.1"</span><span>;</span></span>
<span></span>
<span><span>  src</span><span> =</span><span> fetchFromGitHub </span><span>{</span></span>
<span><span>    owner</span><span> =</span><span> "tukaani-project"</span><span>;</span></span>
<span><span>    repo</span><span> =</span><span> "xz"</span><span>;</span></span>
<span><span>    rev</span><span> =</span><span> "v</span><span>${</span><span>finalAttrs</span><span>.</span><span>version</span><span>}</span><span>"</span><span>;</span></span>
<span><span>    hash</span><span> =</span><span> "sha256-alrSXZ0KWVlti6crmdxf/qMdrvZsY5yigcV9j6GIZ6c="</span><span>;</span></span>
<span><span>  };</span></span>
<span></span>
<span><span>  strictDeps</span><span> =</span><span> true</span><span>;</span></span>
<span><span>  configureFlags</span><span> =</span><span> lib</span><span>.</span><span>optional enableStatic </span><span>"--disable-shared"</span><span>;</span></span>
<span><span>  enableParallelBuilding</span><span> =</span><span> true</span><span>;</span></span>
<span><span>  doCheck</span><span> =</span><span> true</span><span>;</span></span>
<span></span>
<span><span>  nativeBuildInputs</span><span> =</span><span> [</span></span>
<span><span>    gettext</span></span>
<span><span>    autoconf</span></span>
<span><span>    libtool</span></span>
<span><span>    automake</span></span>
<span><span>    perl538Packages</span><span>.</span><span>Po4a</span></span>
<span><span>    doxygen</span></span>
<span><span>    perl</span></span>
<span><span>  ];</span></span>
<span></span>
<span><span>  preConfigure</span><span> =</span><span> ''</span></span>
<span><span>    ./autogen.sh</span></span>
<span><span>  ''</span><span>;</span></span>
<span></span>
<span><span>})</span></span>
<span></span></code></pre>

<p>I removed details here to focus on the most important: the Nix expression is very similar to the actual derivation for <code>xz</code>, the only difference (apart from the method to fetch the source) is that we need to use <code>autoconf</code> to generate configure scripts. When using the maintainer provided tarball these are already pre-generated for us (as Daniel Stenberg was explaining in the toot above) – which is very handy particularly when you are building <code>xz</code> in the bootstrap phase of a distribution and you don’t want a dependency on <code>autoconf</code> / <code>automake</code> – but in this instance we have to do it ourselves.</p>
<p>Now that we can build <code>xz</code> from the code archive provided by GitHub, we have to write Nix code to compare both outputs. For that purpose, we register a new phase called <code>compareArtifacts</code>, that runs at the very end of the build process. To make my point, I’ll first only compare the <code>liblzma.so</code> file (the one that was modified by the backdoor), but we could easily generalize this phase to all binaries and libraries outputs:</p>
<pre tabindex="0"><code><span><span>postPhases = </span><span>[</span><span> "compareArtifacts"</span><span> ]</span><span>;</span></span>
<span></span>
<span><span>compareArtifacts = </span><span>''</span></span>
<span><span>  diff $out/lib/liblzma.so </span><span>${</span><span>xz</span><span>.</span><span>out</span><span>}</span><span>/lib/liblzma.so</span></span>
<span><span>''</span><span>;</span></span>
<span></span></code></pre>

<p>After this change, building <code>xz-after-bootstrap</code> on master<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> still works, showing that in a normal setting, both artifacts are indeed identical.</p>
<pre tabindex="0"><code><span><span>$</span><span> nix-build -A xz-after-bootstrap</span></span>
<span><span>/nix/store/h23rfcjxbp1vqmmbvxkv0f69r579kfc1-xz-5.6.1</span></span>
<span></span></code></pre>

<p>Let’s now try our detection method on the backdoored <code>xz</code> and see what happens! We checkout revision <code>c53bbe3</code> that contains the said version<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>, and build <code>xz-after-bootstrap</code>.</p>
<pre tabindex="0"><code><span><span>$</span><span> git checkout c53bbe3</span></span>
<span><span>$</span><span> nix-build -A xz-after-boostrap</span></span>
<span><span>/nix/store/57p62d3m98s2bgma5hcz12b4vv6nhijn-xz-5.6.1</span></span>
<span></span></code></pre>

<p>Again, identical artifacts? Remember that the backdoor was not active in NixOS, partly because there is a check that the <code>RPM_ARCH</code> variable is set in the script that installs the backdoor. So let’s set it in <code>pkgs/tools/compression/xz/default.nix</code> to activate the backdoor<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<pre tabindex="0"><code><span><span>env</span><span>.</span><span>RPM_ARCH = </span><span>true</span><span>;</span></span>
<span></span></code></pre>

<pre tabindex="0"><code><span><span>$</span><span> nix-build -A xz-after-boostrap</span></span>
<span><span>/nix/store/57p62d3m98s2bgma5hcz12b4vv6nhijn-xz-5.6.1</span></span>
<span><span>...</span></span>
<span><span>...</span></span>
<span><span>Running phase: compareBins</span></span>
<span><span>Binary files /nix/store/cxz8iq3hx65krsyraill6figp03dk54n-xz-5.6.1/lib/liblzma.so and /nix/store/4qp2khyb22hg6a3jiy4hqmasjinfkp2g-xz-5.6.1/lib/liblzma.so differ</span></span>
<span></span></code></pre>

<p>That’s it, binary artifacts are different now! Let’s try to understand a bit more what makes them different by keeping them as part of the output. For that, we modify the <code>compareArtifacts</code> phase:</p>
<pre tabindex="0"><code><span><span>compareArtifacts = </span><span>''</span></span>
<span><span>  cp </span><span>${</span><span>xz</span><span>.</span><span>out</span><span>}</span><span>/lib/liblzma.so $out/xzBootstrap</span></span>
<span><span>  cp $out/lib/liblzma.so $out/xzAfterBootstrap</span></span>
<span><span>  diff $out/lib/liblzma.so </span><span>${</span><span>xz</span><span>.</span><span>out</span><span>}</span><span>/lib/liblzma.so || true</span></span>
<span><span>''</span><span>;</span></span>
<span></span></code></pre>

<p>This time the diff doesn’t make the build fail and we store both versions of the <code>liblzma.so</code> to be able to compare them afterwards.</p>
<pre tabindex="0"><code><span><span>$</span><span> ls -lah result</span></span>
<span><span>total 69M</span></span>
<span><span>dr-xr-xr-x      6 root root     99 Jan  1  1970 .</span></span>
<span><span>drwxrwxr-t 365666 root nixbld  85M Dec 10 14:27 ..</span></span>
<span><span>dr-xr-xr-x      2 root root   4.0K Jan  1  1970 bin</span></span>
<span><span>dr-xr-xr-x      3 root root     32 Jan  1  1970 include</span></span>
<span><span>dr-xr-xr-x      3 root root    103 Jan  1  1970 lib</span></span>
<span><span>dr-xr-xr-x      4 root root     31 Jan  1  1970 share</span></span>
<span><span>-r-xr-xr-x      1 root root   210K Jan  1  1970 xzAfterBootstrap</span></span>
<span><span>-r-xr-xr-x      1 root root   258K Jan  1  1970 xzBootstrap</span></span>
<span></span></code></pre>

<p>We can notice that there is even a significant size difference between the two artifacts with an increase of 48Kb for the backdoored one. Let’s try to understand where this difference comes from. We can use the <code>nm</code> command from <code>binutils</code> to list the symbols in an artifact:</p>
<pre tabindex="0"><code><span><span>$</span><span> nm result/xzAfterBootstrap</span></span>
<span><span>000000000000d3b0 t alone_decode</span></span>
<span><span>000000000000d380 t alone_decoder_end</span></span>
<span><span>000000000000d240 t alone_decoder_memconfig</span></span>
<span><span>0000000000008cc0 t alone_encode</span></span>
<span><span>0000000000008c90 t alone_encoder_end</span></span>
<span><span>0000000000008db0 t alone_encoder_init</span></span>
<span><span>0000000000020a80 t arm64_code</span></span>
<span><span>0000000000020810 t arm_code</span></span>
<span><span>0000000000020910 t armthumb_code</span></span>
<span><span>000000000000d8d0 t auto_decode</span></span>
<span><span>000000000000d8a0 t auto_decoder_end</span></span>
<span><span>000000000000d730 t auto_decoder_get_check</span></span>
<span><span>000000000000d7a0 t auto_decoder_init</span></span>
<span><span>000000000000d750 t auto_decoder_memconfig</span></span>
<span><span>0000000000022850 r available_checks.1</span></span>
<span><span>00000000000225f0 r bcj_optmap</span></span>
<span><span>0000000000008fb0 t block_buffer_encode</span></span>
<span><span>...</span></span>
<span></span></code></pre>

<p>Now we can diff the symbols between the two artifacts:</p>
<pre tabindex="0"><code><span><span>$ diff -u0 &lt;(nm --format=just-symbols xzAfterBootstrap) &lt;(nm --format=just-symbols xzBootstrap)</span></span>
<span><span>--- /dev/fd/63	2024-12-10 15:27:11.477332683 +0000</span></span>
<span><span>+++ /dev/fd/62	2024-12-10 15:27:11.478332717 +0000</span></span>
<span><span>@@</span><span> -31,0 +32 </span><span>@@</span></span>
<span><span>+</span><span>_cpuid</span></span>
<span><span>@@</span><span> -65,0 +67 </span><span>@@</span></span>
<span><span>+</span><span>_get_cpuid</span></span>
<span><span>@@</span><span> -448,0 +451 </span><span>@@</span></span>
<span><span>+</span><span>__tls_get_addr@GLIBC_2.3</span></span>
<span></span></code></pre>

<p>TADA! We see the added <code>_get_cpuid</code> symbol, documented in numerous technical report about the <code>xz</code> backdoor, confirming our method works!</p>
<p><strong>Addendum 1: How to implement this safeguard in <code>nixpkgs</code>?</strong></p>
<p>I think <code>nixpkgs</code> should implement this kind of safeguard for every package built as part of the bootstrap phase that is not using a trusted source archive. The <code>*-after-bootstrap</code> packages could then be added to the channel blockers to ensure that there is big red alarm that requires intervention from the maintainers if ever one of those would not build.</p>
<p>As a proof of concept, and to gather the feedback of the community I opened <a href="https://github.com/NixOS/nixpkgs/pull/391569">a pull request</a> in the <code>nixpkgs</code> repository for the <code>xz</code> case, but if the method is adopted we should then implement it for the other candidate packages in <code>nixpkgs</code>’s bootstrap.</p>
<p><strong>Addendum 2: Evaluation: reproducibility of <code>stdenv</code> over time</strong></p>
<p>As discussed above, the method I propose assumes the packages we want to build trust in are <em>bitwise reproducible</em>. In order to help validate the approach, let’s verify that the packages belonging to the <code>stdenv</code> runtime are indeed reproducible.
To do that, I have (as part of a bigger research project whose findings are summarized in <a href="https://luj.fr/blog/is-nixos-truly-reproducible.html">another blog post</a>) sampled 17 <code>nixpkgs-unstable</code> revisions from 2017 to 2023 and rebuilt every <em>non-fixed-output-derivation</em> (FOD) composing <code>stdenv</code> from these revisions using the <code>nix-build --check</code> command to check for bitwise reproducibility.
Here are my findings:</p>
<ul>
<li>In every revision <code>xz</code> was bitwise reproducible ;</li>
<li>In 12 of the 17 revisions there was either one or two packages that were buildable but not reproducible, but those packages are consistent over time: for example <code>gcc</code> has consistently been non reproducible from 2017 to 2021 and <code>bash</code> until 2019.</li>
</ul>
<p>These findings, while showing that this method cannot be applied to <em>every</em> package in <code>stdenv</code>, are encouraging: even if some packages are not bitwise reproducible, they are consistently so, which means that it should be possible to selectively activate it on packages that exhibit good reproducibility in the long term.</p>
<p><strong>Addendum 3: Limitations: the trusting trust issue</strong></p>
<p>The trusting trust issue is a famous <a href="https://www.cs.cmu.edu/~rdriley/487/papers/Thompson_1984_ReflectionsonTrustingTrust.pdf">thought experiment initiated by Ken Thomson</a> during his Turing award acceptance lecture. The idea is the following: assume there is a backdoor in compilers we use to build our software such that the compiler propagates the backdoor to all new version of itself that it builds, but behaves normally for any other build until some point in time where it backdoors all executables it produces. Moderns compilers often need a previous version of themselves to be compiled so there must be an initial executable that we have to trust to build our software, making this kind of sophisticated attack <em>theoretically</em> possible and completely undetectable.
Similarly, the method I am proposing here requires to make the assumption that the untrusted <code>xz</code> (the one built during the bootstrap phase) can’t indirectly corrupt the build of <code>xz-after-bootstrap</code> to make it look like the produced artifacts are identical. Again, such an attack would probably be extremely complex to craft so the assumption here seems sane.</p></li>
</ol>
<h3 id="thanks">Thanks</h3>
<p>I would like to thank <a href="https://www.theozimmermann.net/">Théo Zimmermann</a>, <a href="https://orcid.org/0009-0008-7972-7160">Pol Dellaiera</a>, <a href="https://groundry.org/">Martin Schwaighofer</a>, and <a href="https://upsilon.cc/~zack/">Stefano Zacchiroli</a> for their valuable feedback and insightful discussions during the writing of this blog post. Their contributions significantly helped me organize and refine my ideas on this topic.</p>
<section id="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><em>Jia Tan</em> essentially (through multiple identities) pressured the main <code>xz</code> maintainer into accepting new maintainers for the project, claiming that the project was receiving sub-par maintenance.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Fortunately, even though the malicious version was available to users, the backdoor was not active on NixOS has it was specifically made to target Debian and Fedora systems.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Tested at the time of writing on revision <code>1426c51</code><a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>For obvious reasons, the backdoored tarball has been deleted from GitHub and the project’s website but it is still available in the NixOS cache!<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>This illustrates the power and limitation of this method: it only detects modifications of the tarball that have an impact on the final result. In the case of the <code>xz</code> backdoor, NixOS executables did not contain the backdoor and as such without any modification we would not have discovered the backdoor. So yes, the title is a little bit catchy, but illustrates the idea.<a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The polar vortex is hitting the brakes (213 pts)]]></title>
            <link>https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-hitting-brakes</link>
            <guid>43448023</guid>
            <pubDate>Sat, 22 Mar 2025 19:31:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-hitting-brakes">https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-hitting-brakes</a>, See on <a href="https://news.ycombinator.com/item?id=43448023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>For much of this winter season, the polar vortex winds at 60°N have been racing around the stratospheric polar region. During February alone, these west-to-east winds were two times stronger than normal for that time of year. However, the latest forecasts suggest that the polar vortex is about to switch gears with a major vortex disruption to happen this weekend. Read on to find out why the polar vortex could be bottoming out early this season.</span></p>
<figure><p><a href="https://www.climate.gov/media/16835"><img loading="lazy" src="https://www.climate.gov/sites/default/files/styles/full_width_620_original_image/public/2025-03/observed-forecasted-winds-2025-03-06_0.png?itok=yUooMOq2" width="620" height="359" alt="time series of stratospheric winds"></a></p>
      
            <p>Observed and forecasted (NOAA GEFSv12) polar vortex wind speeds at 60°N (bold blue line) compared to the natural range of variability (faint blue shading). Since mid-November, these stratospheric winds have been stronger than normal (thin blue line). However, that’s about to change as the latest forecasts (issued March 3, 2025) indicate the winds at 60°N are going to dramatically decrease over the next few days (bold purple line), indicating a polar vortex disruption. The big question is whether these winds will rebound toward their normal strength before the end of the season. NOAA Climate.gov image, adapted from original by Laura Ciasto.</p>
      
      </figure><h2><strong>Stratospheric pit stop</strong></h2>
<p><span><span>At the time of writing this post, the polar stratospheric west-to-east winds are still speeding around the Arctic [footnote #1], but forecasts suggest they are not only going to come to a screeching halt by the weekend, but they are then going to strongly reverse direction. When this wind reversal (i.e., winds become east-to-west) occurs at 60°N and 10 hPa (~19 mi/30 km above us), it’s called </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/cooking-stratospheric-polar-vortex-disruption"><span>a sudden stratospheric warming</span></a><span>. As the name suggests, these major polar vortex disruptions are linked to incredible stratospheric temperature increases over a short period of time [footnote #2]. For this upcoming event, temperatures in the mid-stratosphere could increase as much as 45°F (25°C) in less than 5 days. </span></span></p>
<figure><p><a href="https://www.climate.gov/media/16836"><img loading="lazy" src="https://www.climate.gov/sites/default/files/styles/full_width_620_original_image/public/2025-03/observed-forecasted-polar-cap-temperatures-2025-03-06.png?itok=evp_E-Sz" width="620" height="361" alt="time series of stratospheric temperatures"></a></p>
      
            <p>Observed and forecasted (NOAA GEFSv12) polar cap temperatures compared to the natural range of variability (faint orange shading). Since October, these stratospheric temperatures (bold red line) have been colder than normal (thin red line). This is expected because strong polar vortex winds act as a barrier between cold Arctic air and warmer mid-latitude air. As the polar vortex becomes disrupted, the stratosphere will warm quickly and intensely (bold pink line), hence the name sudden stratospheric warming. NOAA Climate.gov image, adapted from original by Laura Ciasto.</p>
      
      </figure><p><span><span>Sudden stratospheric warming events usually come in two possible flavors in which the polar vortex either displaces off the pole or splits into two smaller vortexes. This particular event may be a bit of both. The initial warming event kicks off with the polar vortex shifted toward Europe, but the forecasts also show pieces of the vortex splitting off from the main lobes several days later.</span></span></p>
<figure><p><a href="https://www.climate.gov/media/16838" hreflang="en"><img loading="lazy" src="https://www.climate.gov/sites/default/files/styles/full_width_stretch_featured_image/public/2025-03/polar-vortex-temperature-winds-forecast-observed-2025-03-06--corrected-01.png?itok=9p9Vfm9d" width="1100" height="720" alt="maps of temperature and winds over Northern Hemisphere"></a>
</p>

            <p>Evolution and forecast of stratospheric conditions. Earlier this week (March 4 2025; left panel), the polar vortex winds (vectors) were situated closer to the pole keeping the relatively cold air (light shading) isolated from the warmer surrounding air (orange/red shading). By March 10, 2025 (middle panel), the GFS forecast indicates the polar vortex will be nudged farther off the pole, with warmer air flooding the Arctic. The average winds around 60°N will become east-to-west, characterizing a <em>sudden stratospheric warming</em>. This disruption to the polar vortex is expected to continue through at least the next two weeks with smaller lobes of the vortex periodically splitting off (e.g., March 13, 2025, right panel). Current forecasts suggest that the stratospheric winds will not recover this spring and become west-to-east again. If so, this event will be classified as a <em>final warming</em> instead of a mid-winter <em>sudden stratospheric warming</em>. NOAA Climate.gov image, based on Global Forecast System data provided by Laura Ciasto.</p>
      
      </figure><h2><strong>Will the polar vortex rev its engine again?</strong></h2>
<p><span>One of the big questions regarding this polar vortex disruption is whether the stratospheric winds at 60°N will recover and become west-to-east again, extending the polar vortex season (and its ability to influence weather patterns) into late spring. Forecasts [footnote #3] do not currently show a recovery, so this pit stop may be the end of the vortex’s racing season. If this turns out to be the case, then it would be classified as a “final stratospheric warming” rather than a major sudden stratospheric warming.</span></p>
<p><span><span><span>As we discussed in last season’s </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/last-hurrah-polar-vortex"><span>post</span></a><span>, final warmings occur every spring as sunlight returns to the North Pole and the temperature differences between the equator and pole decrease. As a result, the west-to-east winds that are maintained by that temperature difference decrease and transition to east-to-west winds. This transition usually happens sometime in mid-April, but there have been 5 years since 1958 when final warmings occurred before March 15.&nbsp; Like this year, those years corresponded to winters without a mid-winter sudden stratospheric warming [footnote #4].</span></span></span></p>
<h2><strong>A potential stratosphere-troposphere fender bender </strong></h2>
<figure><p><a href="https://www.climate.gov/media/16837"><img loading="lazy" src="https://www.climate.gov/sites/default/files/styles/full_width_620_original_image/public/2025-03/polar-vortex-geopotential-height-2025-03-06.png?itok=IoXKmOrY" width="620" height="307" alt="contour plot of atmospheric thickness anomalies over polar cap"></a></p>
      
            <p>Differences from average atmospheric thickness (“standardized geopotential height anomalies”) in the column of air over the Arctic for the stratosphere and troposphere. Since the beginning of the year, low-thickness anomalies (purple shading indicative of a stronger than average polar vortex) have dominated the stratosphere but only periodically coupled down to the troposphere. Latest forecasts show a dramatic change with thickness anomalies increasing (orange shading), consistent with a polar vortex disruption. These stratospheric anomalies are preceded by tropospheric anomalies of the same sign, hinting at a nudge from below. However, it’s too soon to tell whether these stratospheric anomalies will then drip down into the troposphere again. Standardized anomalies are based on departures from the 1991-2020 Climate Forecast System Reanalysis climatologies and have been divided by the standard deviation. Data are from the Global Forecast System observational analysis and forecast.</p>
      
      </figure><p><span>Regardless of whether this is the final warming or the vortex decides to ride again, both have the potential to impact our weather this spring. Disruptions to the polar vortex can communicate down to the troposphere and disrupt the jet stream. These disruptions to the jet stream can bring colder than normal Arctic air down into the eastern United States.&nbsp;</span></p>
<p><span>Now this doesn’t mean you need to bring your winter tires back out while your garden tools continue to collect dust. First, it’s too soon to tell whether this vortex disruption will make its way down to the troposphere as the latest forecast doesn’t show much stratosphere-troposphere interaction after the onset of the warming event. Second, though the impacts of March sudden warmings are very similar to those in mid-winter, spring is coming, so any Arctic air brought down in the US won't "feel" as cold compared to if it happened in January because we are in a warmer part of the year. </span></p>
<p><span>Even if the polar vortex season ends early this year, we’re hoping to have at least 1 or 2 more posts (including a guest author) so stay tuned!</span></p>
<h2><strong>Footnotes</strong></h2>
<p><span>[1] We spent several posts this winter talking about the strong, but sometimes stretchy, polar vortex and what that has meant for our winter weather. If you’re interested, please read more </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-causing-us-cold-air-outbreak"><span>here</span></a><span>, </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/another-blast-arctic-air-time-stretched-strong-polar-vortex"><span>here</span></a><span>, and </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-intensifications-overlooked-influencer"><span>here</span></a><span>. </span></p>
<p><span>[2] The sudden increase in temperature over such a short period of time occurs for a couple of reasons.&nbsp; As the polar winds weaken and reverse direction during a major sudden stratospheric warming, there is a component of the air that moves poleward and descends rapidly over the Arctic and pressure increases. As the air descends it warms: this is one of the reasons why the temperatures can increase so impressively during a major warming event. Furthermore, the polar vortex winds act as a barrier between cold Arctic air and warmer mid-latitude air. When the winds/barrier weaken, warmer mid-latitude winds can enter the polar stratosphere and contribute to increasing temperatures.</span></p>
<p><span>[3] We show the American GEFS model in these posts, but the ECMWF model currently doesn’t show a vortex recovery in the next several weeks either.</span></p>
<p><span>[4] The link between winters with a sudden warming and late season final warmings (and correspondingly, years without a sudden warming and early season final warmings) is thought to be due to the tug of war in the stratosphere between dynamic and radiative processes that control the strength of the polar vortex. In particular, if a sudden warming occurs during mid-winter, the polar stratospheric winds will be pulled towards returning to a west-to-east flowing state to balance the stratospheric temperature gradient created by lack of sunlight over the pole. If this recovery of the stratospheric winds to west-to-east flow occurs, it provides potentially weeks to months of additional time for planetary waves to interact with the winds, extending the timing of the final warming until much later. On the other hand, if the sudden warming occurs near the spring equinox, when sunlight has returned to the pole, the stratospheric winds feel no radiative force to return to a west-to-east state, and so often the winds will stay east-to-west (corresponding to an early season final warming).&nbsp;</span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[California AG Rob Bonta Urgently Issues Consumer Alert for 23andMe Customers (412 pts)]]></title>
            <link>https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers</link>
            <guid>43447421</guid>
            <pubDate>Sat, 22 Mar 2025 17:55:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers">https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers</a>, See on <a href="https://news.ycombinator.com/item?id=43447421">Hacker News</a></p>
<div id="readability-page-1" class="page"><div property="content:encoded"><p><em>Californians have the right to direct the company to delete their genetic data</em>&nbsp;</p>
<p><b>OAKLAND</b>&nbsp;— California Attorney General Rob Bonta today issued a consumer alert to customers of 23andMe, a genetic testing and information company. The California-based company has publicly reported that it is in financial distress and&nbsp;stated in securities filings that there is substantial doubt about its ability to continue as a going concern.&nbsp;Due to the trove of sensitive consumer data 23andMe has amassed, Attorney General Bonta reminds Californians of their right to&nbsp;direct&nbsp;the deletion of their genetic data under the Genetic Information Privacy Act (GIPA) and California Consumer Protection Act (CCPA).&nbsp;Californians who want to invoke these rights can do so by going to 23andMe's website.&nbsp;</p>
<p>“California has robust privacy laws that allow consumers to take control and request that a company delete their genetic data,”&nbsp;<b>said Attorney General Bonta.</b>&nbsp;“Given 23andMe’s reported financial distress, I remind&nbsp;Californians to consider invoking their rights and directing 23andMe to delete their data and destroy any samples of genetic material held by the company.”&nbsp;</p>
<p><b>To Delete Genetic Data from 23andMe:</b></p>
<ol>
<li>Consumers can delete their account and personal information by taking the following steps:</li>
<li>Log into your 23andMe account on their website.&nbsp;</li>
<li>Go to the “Settings” section of your profile.</li>
<li>Scroll to a section labeled “23andMe Data” at the bottom of the page.&nbsp;</li>
<li>Click “View” next to “23andMe Data”</li>
<li>Download your data: If you want a copy of your genetic data for personal storage, choose the option to download it to your device before proceeding.</li>
<li>Scroll to the “Delete Data” section.&nbsp;</li>
<li>Click “Permanently Delete Data.”&nbsp;</li>
<li>Confirm your request:&nbsp;You’ll receive an email from 23andMe; follow the link in the email to confirm your deletion request.</li>
</ol>
<p><b>To Destroy Your 23andMe Test Sample:</b></p>
<p>If you previously opted to have your saliva sample and DNA stored by 23andMe, but want to change that preference, you can do so from your account settings page, under “Preferences.”</p>
<p><b>To Revoke Permission for Your Genetic Data to be Used for Research:</b></p>
<p>If you previously consented to 23andMe and third-party researchers to use your genetic data and sample for research, you may withdraw consent from the account settings page, under “Research and Product Consents.”</p>
<p>Under GIPA, California consumers can delete their account and genetic data and have their biological sample destroyed.&nbsp;In addition, GIPA permits California consumers to revoke consent that they provided a genetic testing company to collect, use, and disclose genetic data and to store biological samples after the initial testing has been completed.&nbsp;The CCPA also vests California consumers with the right to delete personal information, which includes genetic data, from businesses that collect personal information from the consumer. &nbsp;&nbsp;</p>
<p>To learn more about the CCPA, please visit&nbsp;<a href="https://oag.ca.gov/privacy/ccpa">here</a>.&nbsp;&nbsp;</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Map Features in OpenStreetMap with Computer Vision (256 pts)]]></title>
            <link>https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/</link>
            <guid>43447335</guid>
            <pubDate>Sat, 22 Mar 2025 17:42:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/">https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/</a>, See on <a href="https://news.ycombinator.com/item?id=43447335">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <h3 id="motivation">Motivation</h3><p>At Mozilla.ai, we believe that there are a lot of opportunities where artificial intelligence (AI) can empower communities driven by open collaboration.&nbsp;</p><p>These opportunities need to be designed carefully, though, as many members of these communities (and people in general) are increasingly worried about the amount of <a href="https://en.wikipedia.org/wiki/AI_slop?ref=blog.mozilla.ai"><u>AI slop</u></a> flooding the internet.</p><p>With this idea in mind we developed and released the <a href="https://github.com/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai"><u>OpenStreetMap AI Helper</u></a> Blueprint. If you love maps and are interested in training your own computer vision model, you’ll enjoy diving into this Blueprint.</p><h3 id="why-openstreetmap">Why OpenStreetMap?</h3><p>Data is one of the most important components of any AI application, and <a href="https://www.openstreetmap.org/?ref=blog.mozilla.ai"><u>OpenStreetMap</u></a> has a vibrant community that collaborates to maintain and extend the most complete open map database available. </p><p>If you haven’t heard of it, <a href="https://www.openstreetmap.org/?ref=blog.mozilla.ai"><u>OpenStreetMap</u></a> is an open, editable map of the world created by a community of mappers who contribute and maintain data about roads, trails, cafés, railway stations, and more.</p><p>Combined with other sources, like satellite imagery, this database offers infinite possibilities to train different AI models.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeQ5_VUhyWiLkZAqDkmqKu7p3vT5hC873l9vFSbduVam2uC3odROrGsOWLUdbYi9ZHAyWLHR-QT2SoowtHgqxcR_aaaJu6joEG6cNMhxdV2IiAvAToa_TQkic9Qx8sgkFxzTPBZ?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="351"></figure><p>As a long-time user and contributor to <a href="https://www.openstreetmap.org/?ref=blog.mozilla.ai"><u>OpenStreetMap</u></a> , I wanted to build an end-to-end application where a model is first trained with this data and then used to contribute back.</p><p>The idea is to use AI to speed up the slower parts of the mapping process (roaming around the map, drawing polygons) while keeping a human in the loop for the critical parts (verifying that the generated data is correct).</p><h3 id="why-computer-vision">Why Computer Vision?</h3><p>Large Language Models (LLM) and, more recently, Visual Language Models (VLM) are sucking all the oxygen out of the AI room, but there are a lot of interesting applications that don’t (need to) use this type of models.</p><p>Many of the <a href="https://wiki.openstreetmap.org/wiki/Map_features?ref=blog.mozilla.ai"><u>Map Features</u></a> you can find in OpenStreetMap are represented with a polygon ('Area'). It turns out that finding and drawing these polygons is a very time consuming task for a human, but Computer Vision models can be easily trained for the task (when provided with enough data).</p><p>We chose to split the work of finding and drawing map features into 2 computer vision tasks using state-of-the-art non-LLM models: </p><ul><li><strong>Object Detection</strong> with <a href="https://docs.ultralytics.com/es/models/yolo11/?ref=blog.mozilla.ai"><u>YOLOv11</u></a>, by <a href="https://www.ultralytics.com/?ref=blog.mozilla.ai" rel="noreferrer">Ultralytics</a>, which identifies where relevant features exist in an image.</li><li><strong>Segmentation </strong>with <a href="https://ai.meta.com/sam2/?ref=blog.mozilla.ai"><u>SAM2</u></a>, by <a href="https://ai.meta.com/?ref=blog.mozilla.ai" rel="noreferrer">Meta</a>, which refines the detected features by outlining their exact shape.</li></ul><p>These models are lightweight, fast, and local-friendly – it’s refreshing to work with models that don’t demand a high-end GPU just to function. As an example, the combined weights of YOLOv11 and SAM2 take much less disk space (&lt;250MB) than any of the smallest Visual Language Models available, like <a href="https://huggingface.co/HuggingFaceTB/SmolVLM-Base?ref=blog.mozilla.ai"><u>SmolVLM </u></a>(4.5GB).</p><p>By combining these models, we can automate much of the mapping process while keeping humans in control for final verification.</p><h3 id="the-openstreetmap-ai-helper-blueprint">The OpenStreetMap AI Helper Blueprint</h3><p>The Blueprint can be divided into 3 stages:</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeAMhkrtYiDH7ZMsybzM7GF1KSiJMUTddPlc-xyfzlVdW6Mkd1xrtEFK81P_27vNHCMCnHGpEwEQN5-wIrkGKax_JpiBrClnzi1hVzMrrVvfpHk3fwd1fu_uiHmxxrpGiZcnKsqBA?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="160"></figure><p><strong>Stage 1: Create an Object Detection dataset from OpenStreetMap</strong></p><p>The first stage involves fetching data from OpenStreetMap, combining it with satellite images, and transforming it into a format suitable for training.</p><p>You can run it yourself in the <a href="https://colab.research.google.com/github/mozilla-ai/osm-ai-helper/blob/main/demo/create_dataset.ipynb?ref=blog.mozilla.ai"><u>Create Dataset Colab</u></a>.</p><p>For fetching OpenStreetMap data, we use:</p><ul><li>The <a href="https://nominatim.org/?ref=blog.mozilla.ai"><u>Nominatim API</u></a> to provide users with a flexible way of selecting an area of interest. In our swimming pool example, we use <a href="https://nominatim.openstreetmap.org/ui/details.html?osmtype=R&amp;osmid=349036&amp;class=boundary&amp;ref=blog.mozilla.ai"><u>Galicia</u></a> for training and <a href="https://nominatim.openstreetmap.org/ui/details.html?osmtype=R&amp;osmid=3808752&amp;class=boundary&amp;ref=blog.mozilla.ai"><u>Viana do Castelo</u></a> for validation.</li><li>The <a href="https://wiki.openstreetmap.org/wiki/Overpass_API?ref=blog.mozilla.ai"><u>Overpass API</u></a> to download all the relevant polygons using specific <a href="https://wiki.openstreetmap.org/wiki/Tags?ref=blog.mozilla.ai"><u>tags</u></a> within the selected area of interest. In our swimming pool example, we use <a href="https://wiki.openstreetmap.org/wiki/Tag:leisure=swimming_pool?ref=blog.mozilla.ai"><u>leisure=swimming_pool</u></a> discarding the ones also tagged with <a href="https://wiki.openstreetmap.org/wiki/Tag:location%3Dindoor?ref=blog.mozilla.ai"><u>location=indoor</u></a>.</li></ul><p>Once all the polygons have been downloaded, you can choose a <a href="https://docs.mapbox.com/help/glossary/zoom-level/?ref=blog.mozilla.ai"><u>zoom level</u></a>. We use this zoom level to first identify all the tiles that contain a polygon and then download them using the <a href="https://docs.mapbox.com/api/maps/static-tiles/?ref=blog.mozilla.ai"><u>Static Tiles API</u></a> from <a href="https://www.mapbox.com/?ref=blog.mozilla.ai"><u>Mapbox</u></a>.</p><p>The polygons in latitude and longitude coordinates are transformed to a bounding box in pixel coordinates relative to each tile and then saved in the <a href="https://docs.ultralytics.com/datasets/detect/?ref=blog.mozilla.ai#ultralytics-yolo-format"><u>Ultralytics YOLO format</u></a>.</p><p>Finally, the dataset is uploaded to the <a href="https://huggingface.co/docs/hub/datasets?ref=blog.mozilla.ai"><u>Hugging Face Hub</u></a>. You can check our example <a href="https://huggingface.co/datasets/mozilla-ai/osm-swimming-pools?ref=blog.mozilla.ai"><u>mozilla-ai/osm-swimming-pools</u></a>.</p><p><strong>Stage 2 - Finetune an Object Detection model</strong></p><p>Once the dataset is uploaded in the right format, finetuning a <a href="https://docs.ultralytics.com/models/yolo11/?ref=blog.mozilla.ai"><u>YOLOv11</u></a> (or any other model supported by Ultralytics) is quite easy. </p><p>You can run it yourself in the <a href="https://colab.research.google.com/github/mozilla-ai/osm-ai-helper/blob/main/demo/finetune_model.ipynb?ref=blog.mozilla.ai"><u>Finetune Model Colab</u></a> and check all the <a href="https://docs.ultralytics.com/modes/train/?ref=blog.mozilla.ai#augmentation-settings-and-hyperparameters"><u>available hyperparameters</u></a>.</p><p>Once the model is trained, it is also uploaded to the <a href="https://huggingface.co/docs/hub/datasets?ref=blog.mozilla.ai"><u>Hugging Face Hub</u></a>. You can check our example <a href="https://huggingface.co/mozilla-ai/swimming-pool-detector?ref=blog.mozilla.ai"><u>mozilla-ai/swimming-pool-detector</u></a>.</p><p><strong>Stage 3 - Contributing to OpenStreetMap</strong></p><p>Once you have a finetuned Object Detection model, you can use it to run inference across multiple tiles. </p><p>You can run inference yourself in the <a href="https://colab.research.google.com/github/mozilla-ai/osm-ai-helper/blob/main/demo/run_inference.ipynb?ref=blog.mozilla.ai"><u>Run Inference Colab</u></a>. </p><p>We also provide a hosted demo where you can try our example swimming pool detector: <a href="https://huggingface.co/spaces/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai"><u>HuggingFace Demo</u></a>.</p><p>The inference requires a couple of human interactions. First, you need to first pick a point of interest in the map:</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXczl1Ib_aQv-G0IUYx7lKv4s1eTCGt1AKHI8G_d9IpbUKtRfV--HyjkxVYT-AdiZ5e-5VeF-TuhRotvvsMOx4SWVFDDaoZtPPxuoYHStzl-a1aWrIn_hy8WpxBbx97sQtEvMEaniw?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="315"></figure><p>After a point is selected, a bounding box is computed around it based on the <em>margin </em>argument.</p><p>All the existing elements of interest are downloaded from <a href="https://www.openstreetmap.org/?ref=blog.mozilla.ai"><u>OpenStreetMap</u></a>, and all the tiles are downloaded from <a href="https://www.mapbox.com/?ref=blog.mozilla.ai"><u>Mapbox</u></a> and joined to create a stacked image.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeP6u-8eEa5cfLIZurqx4zcct2TeWMMFD999MfRNC0F4uQ7kaqBZPmdeUdQrVwB1fiS4MBPlpL86Vljev1WlvxtZhXGB5qy9d1Ghbh9lKlim3UsDyZTkANaU2TLwgx13URfCJRJnQ?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="599" height="601"></figure><p>The stacked image is divided into overlapping tiles. For each tile, we run the Object Detection model (<a href="https://docs.ultralytics.com/models/yolo11/?ref=blog.mozilla.ai">YOLOv11</a>). If an object of interest is detected (e.g. a swimming pool), we pass the bounding box to the Segmentation model (<a href="https://github.com/facebookresearch/sam2?ref=blog.mozilla.ai"><u>SAM2</u></a>) to obtain a segmentation mask.</p><figure><img src="https://blog.mozilla.ai/content/images/2025/02/image.png" alt="" loading="lazy" width="1106" height="590" srcset="https://blog.mozilla.ai/content/images/size/w600/2025/02/image.png 600w, https://blog.mozilla.ai/content/images/size/w1000/2025/02/image.png 1000w, https://blog.mozilla.ai/content/images/2025/02/image.png 1106w" sizes="(min-width: 720px) 720px"></figure><p>All the predicted polygons are checked against the existing ones, downloaded from OpenStreetMap, in order to avoid duplicates.&nbsp;All those identified as <em>new </em>are displayed one by one for manual verification and filtering.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfwAlhR0KxRyrjR8RPsYBbhdhYQn6udmJi_QeeAelz52YHK_K5UNLIbvI7RsAG1tsgzb0HKGB1MTezBjOlZRtfdpAfhIY1UMOlAmM_GxPSyQeYcn1J_cB9FdMLlu9-mUG7k13FAiQ?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="301"></figure><p>The ones you chose to keep will be then uploaded to OpenStreetMap in a single <a href="https://wiki.openstreetmap.org/wiki/Changeset?ref=blog.mozilla.ai"><u>changeset</u></a><u>.</u></p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXewc5Qru4SyT3SBDxHV7EDGZ8XEjwV7P4uFAIhY-zkIafmJ_GP8yBeul4yNb9qAVOVfsIgFrfs7oaLq2Tb0HKuF_oWWXcx75likKPmGkyUF_uc9hCDhsgLXOC4OthbZZVGSZAcf?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="293"></figure><h3 id="closing-thoughts">Closing thoughts</h3><p>OpenStreetMap is a powerful example of open collaboration to create a rich, community-driven map of the world. </p><p>The OpenStreatMap AI Helper Blueprint shows that, with the right approach, AI can enhance human contributions while keeping human verification at the core.&nbsp;In the fully manual process it takes about 1 min to map 2-3 swimming pools, whereas using the blueprint, even without an optimized UX, I can map about 10-15 in the same time (~5x more).</p><p>It also highlights the value of high-quality data from projects like OpenStreetMap, which enables to easily train models like YOLOv11 to perform object detection – proving that you shouldn’t always throw an LLM at the problem.</p><p>We’d love for you to try the <a href="https://github.com/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai"><u>OpenStreetMap AI Helper Blueprint</u></a> and experiment with training a model on a different map feature. If you’re interested, feel free to contribute to the repo to help improve it, or fork it to extend it even further!</p><p>To find other Blueprints we’ve released, check out the <a href="https://developer-hub.mozilla.ai/blueprints?ref=blog.mozilla.ai" rel="noopener noreferrer">Blueprints Hub</a>.</p>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tencent's 'Hunyuan-T1'–The First Mamba-Powered Ultra-Large Model (242 pts)]]></title>
            <link>https://llm.hunyuan.tencent.com/#/blog/hy-t1?lang=en</link>
            <guid>43447254</guid>
            <pubDate>Sat, 22 Mar 2025 17:25:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://llm.hunyuan.tencent.com/#/blog/hy-t1?lang=en">https://llm.hunyuan.tencent.com/#/blog/hy-t1?lang=en</a>, See on <a href="https://news.ycombinator.com/item?id=43447254">Hacker News</a></p>
Couldn't get https://llm.hunyuan.tencent.com/#/blog/hy-t1?lang=en: Error: unable to verify the first certificate]]></description>
        </item>
        <item>
            <title><![CDATA[Facebook to stop targeting ads at UK woman after legal fight (131 pts)]]></title>
            <link>https://www.bbc.co.uk/news/articles/c1en1yjv4dpo</link>
            <guid>43446821</guid>
            <pubDate>Sat, 22 Mar 2025 16:22:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.co.uk/news/articles/c1en1yjv4dpo">https://www.bbc.co.uk/news/articles/c1en1yjv4dpo</a>, See on <a href="https://news.ycombinator.com/item?id=43446821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="main-content" data-testid="main-content"><article><header data-component="headline-block"></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 800w, https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 976w" type="image/webp"><img alt="A woman with blonde hair, blue eyes and pink lipstick stares at the camera" src="https://ichef.bbci.co.uk/ace/standard/1200/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 800w, https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 976w" width="1200" height="675"></picture></span><span role="text"><span>Image source, </span>Tanya O'Carroll</span></p><figcaption><span>Image caption, </span><p>Facebook has agreed to stop targeting adverts at Tanya O'Carroll after she filed a lawsuit against its parent company</p></figcaption></figure></div><div data-component="text-block"><p><b>Facebook has agreed to stop targeting adverts at an individual user using personal data after she filed a lawsuit against its parent company, tech giant Meta.</b></p><p>Tanya O'Carroll, 37, who lives in London and works in the tech policy and human rights sector, said it would open a "gateway" for other people wanting to stop the social media company from serving them adverts based on their demographics and interests.</p><p>The Information Commissioner's Office, the UK's data watchdog, said online targeted advertising should be considered direct marketing.</p><p>In a statement, Meta said it provided "robust settings and tools for users to control their data and advertising preferences".</p></div><div data-component="text-block"><p>Ms O'Carroll, who created her Facebook account about 20 years ago, filed a lawsuit against Meta in 2022, asking it to stop using her personal data to fill her social media feeds with targeted adverts based on topics it thought she was interested in.</p><p>"I knew that this kind of predatory, invasive advertising is actually something that we all have a legal right to object to," Ms O'Carroll told Radio 4's Today Programme. </p><p>"I don't think we should have to accept these unfair terms where we consent to all that invasive data tracking and surveillance."</p><p>It was when she found out she was pregnant in 2017 that she realised the extent to which Facebook was targeting adverts at her.</p><p>She said the adverts she got "suddenly started changing within weeks to lots of baby photos and other things - ads about babies and pregnancy and motherhood".</p><p>"I just found it unnerving - this was before I'd even told people in my private life, and yet Facebook had already determined that I was pregnant," she continued.</p></div><div data-component="text-block"><p>General Data Protection Regulation (GDPR) legislation controls how personal information is used by organisations.</p><p>Ms O'Carroll's lawsuit argued that Facebook's targeted advertising system was covered by the UK's definition of direct marketing, giving individuals the right to object.</p><p>Meta said that adverts on its platform could only be targeted to groups of a minimum size of 100 people, rather than individuals, so did not count as direct marketing. But the Information Commissioner's Office (ICO) disagreed.</p><p>"Organisations must respect people's choices about how their data is used," a spokesperson for the ICO said. "This means giving users a clear way to opt out of their data being used in this way."</p><p>Ms O'Carroll said that Meta had agreed to stop using her personal data for direct marketing purposes, "which in non-legalese means I've essentially been able to turn off all the creepy, invasive, targeted ads on Facebook".</p><p>She said that she did not want to stop using Facebook, saying that it is "filled with all of those connections and family and friends, and entire chapters of my life".</p></div><div data-component="text-block"><p>Ms O'Carroll said she hoped her individual settlement would make it easier for others who wanted Facebook to stop giving them targeted adverts.</p><p>"If other people want to exercise their right, I believe they now have a gateway to do so knowing that the UK regulator will back them up," she said.</p><p>Meta said it disagreed with Ms O'Carroll's claims, adding "no business can be mandated to give away its services for free."</p><p>A spokesperson added: "Facebook and Instagram cost a significant amount of money to build and maintain, and these services are free for British consumers because of personalised advertising."</p><p>"Our services support British jobs and economic growth by connecting businesses with the people most likely to buy their products, while enabling universal access to online services regardless of income. We will continue to defend its value while upholding user choice and privacy."</p><p>Facebook and Instagram have a <a href="https://www.bbc.co.uk/news/technology-67226394">subscription service</a> in most of Europe, where users can pay monthly so that they don't get ads on the platform.</p><p>The Meta spokesperson said the company was "exploring the option" of offering a similar service to UK users and would "share further information in due course."</p></div><section data-component="links-block"><p><h2 type="normal">More on this story</h2></p></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: We made an MCP server so Cursor can debug Node.js on its own (114 pts)]]></title>
            <link>https://www.npmjs.com/package/@hyperdrive-eng/mcp-nodejs-debugger</link>
            <guid>43446659</guid>
            <pubDate>Sat, 22 Mar 2025 15:57:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npmjs.com/package/@hyperdrive-eng/mcp-nodejs-debugger">https://www.npmjs.com/package/@hyperdrive-eng/mcp-nodejs-debugger</a>, See on <a href="https://news.ycombinator.com/item?id=43446659">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><div><p><span>0.2.1<!-- -->&nbsp;•&nbsp;</span><span>Public</span><span>&nbsp;•&nbsp;Published <time datetime="2025-03-19T22:55:36.283Z" title="3/19/2025, 10:55:36 PM">3 days ago</time></span></p></div><ul role="tablist" aria-owns="package-tab-readme package-tab-code package-tab-dependencies package-tab-dependents package-tab-versions"><li role="presentation"><a href="https://www.npmjs.com/package/@hyperdrive-eng/mcp-nodejs-debugger?activeTab=readme" aria-selected="true" role="tab" aria-controls="tabpanel-readme" id="package-tab-readme" tabindex="0"><span> Readme</span></a></li><li role="presentation"><a href="https://www.npmjs.com/package/@hyperdrive-eng/mcp-nodejs-debugger?activeTab=code" aria-selected="false" role="tab" aria-controls="tabpanel-explore" id="package-tab-code" tabindex="-1"><span>Code <span><span>Beta</span></span></span></a></li><li role="presentation"><a href="https://www.npmjs.com/package/@hyperdrive-eng/mcp-nodejs-debugger?activeTab=dependencies" aria-selected="false" role="tab" aria-controls="tabpanel-dependencies" id="package-tab-dependencies" tabindex="-1"><span>4 Dependencies</span></a></li><li role="presentation"><a href="https://www.npmjs.com/package/@hyperdrive-eng/mcp-nodejs-debugger?activeTab=dependents" aria-selected="false" role="tab" aria-controls="tabpanel-dependents" id="package-tab-dependents" tabindex="-1"><span>0 Dependents</span></a></li><li role="presentation"><a href="https://www.npmjs.com/package/@hyperdrive-eng/mcp-nodejs-debugger?activeTab=versions" aria-selected="false" role="tab" aria-controls="tabpanel-versions" id="package-tab-versions" tabindex="-1"><span>4 Versions</span></a></li></ul><p><span><div id="tabpanel-readme" aria-labelledby="package-tab-readme" role="tabpanel" data-attribute=""><article><div id="readme"><div><h2>MCP NodeJS Debugger</h2></div>
<p>An MCP server that gives Claude Code access to NodeJS at runtime to help you debug: <a href="https://www.npmjs.com/package/mcp-nodejs-debugger" rel="nofollow"><code>mcp-nodejs-debugger</code></a>.</p>
<div><h2>Get started</h2></div>
<ol>
<li>
<p>Add to Claude Code:</p>
<div><pre>claude mcp add nodejs-debugger npx @hyperdrive-eng/mcp-nodejs-debugger</pre></div>
</li>
<li>
<p>Start Claude Code</p>
<div><pre>claude
╭───────────────────────────────────────────────────────╮
│ ✻ Welcome to Claude Code research preview<span>!</span>            │
│                                                       │
│   /help <span>for</span> <span>help</span>                                      │
│                                                       │
│   Found 1 MCP server (use /mcp <span>for</span> status)            │
╰───────────────────────────────────────────────────────╯</pre></div>
</li>
<li>
<p>Run a NodeJS server in <a href="https://nodejs.org/en/learn/getting-started/debugging" rel="nofollow">debug mode</a> (i.e. with the <code>--inspect</code> flat)</p>
<div><pre><span><span>#</span> In another terminal</span>
node --inspect {your_file.js}</pre></div>
</li>
<li>
<p>Ask Claude Code to debug your NodeJS server at runtime</p>
<div><pre><span>&gt;</span> I<span><span>'</span>m getting a runtime error in NodeJS. </span>
<span></span>
<span>  {YOUR_RUNTIME_ERROR}</span>
<span>  </span>
<span>  Please help me debug this error at runtime using the nodejs-debugger mcp.</span></pre></div>
</li>
</ol>
<div><h2>Usage</h2></div>
<ol>
<li>
<p>Add to Claude Code:</p>
<div><pre>claude mcp add nodejs-debugger npx mcp-nodejs-debugger</pre></div>
</li>
<li>
<p>Verify connection</p>
<div><pre><span>&gt;</span> /mcp
  ⎿  MCP Server Status

     • nodejs-debugger: connected</pre></div>
</li>
<li>
<p>Remove from Claude Code:</p>
<div><pre>claude remove nodejs-debugger</pre></div>
</li>
</ol>
<div><h2>Example</h2></div>
<ol>
<li>
<p>Here is a buggy NodeJS server:</p>
<pre><code>node --inspect index.js

Debugger listening on ws://127.0.0.1:9229/2862f5a2-8618-4516-8429-1248b397e9b4

#######################
## THE RUNTIME ERROR ##
#######################
MongooseServerSelectionError: Could not connect to any servers in your MongoDB Atlas cluster. One common reason is that you're trying to access the database from an IP that isn't whitelisted. Make sure your current IP address is on your Atlas cluster's IP whitelist: https://www.mongodb.com/docs/atlas/security-whitelist/
    at _handleConnectionErrors (/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/node_modules/mongoose/lib/connection.js:1110:11)
    at NativeConnection.openUri (/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/node_modules/mongoose/lib/connection.js:1041:11)
    at async main (/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app.js:35:3) {
  reason: TopologyDescription {
    type: 'ReplicaSetNoPrimary',
    servers: Map(3) {
      'ac-8e6hgk7-shard-00-01.cojoign.mongodb.net:27017' =&gt; [ServerDescription],
      'ac-8e6hgk7-shard-00-02.cojoign.mongodb.net:27017' =&gt; [ServerDescription],
      'ac-8e6hgk7-shard-00-00.cojoign.mongodb.net:27017' =&gt; [ServerDescription]
    },
    stale: false,
    compatible: true,
    heartbeatFrequencyMS: 10000,
    localThresholdMS: 15,
    setName: 'atlas-5syy8t-shard-0',
    maxElectionId: null,
    maxSetVersion: null,
    commonWireVersion: 0,
    logicalSessionTimeoutMinutes: null
  },
  code: undefined
}
</code></pre>
</li>
<li>
<p>Claude Code debugs the NodeJS server at runtime:</p>
<div><pre><span>&gt;</span> I<span><span>'</span>m getting a runtime error connecting to mongoose. Please help me debug by checking the value of </span>
<span>  the mongodb configs at runtime using the nodejs-debugger mcp.</span>
<span></span>
<span>  ⏺ I<span>'</span></span>ll <span>help</span> you debug the mongoose connection issue using the Node.js debugger. Let<span><span>'</span>s first understand where the</span>
<span>  MongoDB connection is configured.</span>
<span></span>
<span>  ⏺ Search(pattern: "**/*.js")…</span>
<span>    ⎿ &nbsp;Found 100 files</span>
<span></span>
<span>  ⏺ Let<span>'</span></span>s <span>set</span> a breakpoint at the mongoose connection point and continue:

  ⏺ nodejs-debugger:set_breakpoint (MCP)(file:                                                                         …
        <span><span>"</span>/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app.js<span>"</span></span>,
         line: 35)
  &nbsp;&nbsp;⎿ &nbsp;Breakpoint <span>set</span> successfully. ID:
       2:34:0:file:///Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app<span>\.</span>js
  
  ⏺ Let<span><span>'</span>s list all our breakpoints:</span>
<span>  </span>
<span>  ⏺ nodejs-debugger:list_breakpoints (MCP)</span>
<span>  &nbsp;&nbsp;⎿ &nbsp;[</span>
<span>         {</span>
<span>           "file": "/Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app.js",</span>
<span>           "line": 35,</span>
<span>           "id": "2:34:0:file:///Users/arthur/Documents/thirdparties/mdn/express-locallibrary-tutorial/app\\.js"</span>
<span>         }</span>
<span>       ]</span>
<span>  </span>
<span>  ⏺ Let<span>'</span></span>s try to execute a specific piece of code to check the connection variables:

  ⏺ nodejs-debugger:nodejs_inspect (MCP)(js_code: <span><span>"</span>// Get the mongoose module and inspect connection\nconst mongoose = …</span>
<span>                                        require('mongoose');\nconsole.log('Current mongoose version:',</span>
<span>                                        mongoose.version);\nconsole.log('Current mongoose connection state:',</span>
<span>                                        mongoose.connection.readyState);\n// Log connection string safely (without</span>
<span>                                        password)\nconst connectionString = process.env.MONGODB_URI || \n</span>
<span>                                        <span>\"</span>mongodb+srv://cooluser:coolpassword@cluster0.cojoign.mongodb.net/local_library?retryWr</span>
<span>                                        ites=true&amp;w=majority&amp;appName=Cluster0<span>\"</span>;\nconsole.log('Connection string</span>
<span>                                        (masked):', connectionString.replace(/(?&lt;=mongodb<span>\\</span>+srv:<span>\\</span>/<span>\\</span>/<span>\\</span>w+:)(.+?)(?=@)/,</span>
<span>                                         '*****'));<span>"</span></span>)
  
  &nbsp;&nbsp;⎿ &nbsp;Console output:
       [log] Current mongoose version: 8.9.5
       [log] Current mongoose connection state: 0
       [log] Connection string (masked): mongodb+srv://cooluser:coolpassword@cluster0.cojoign.mongodb.net/local_library<span>?</span>retryWri
       tes=true<span>&amp;</span>w=majority<span>&amp;</span>appName=Cluster0

  ⏺ The runtime error you<span><span>'</span>re seeing in your application is related to MongoDB</span>
<span>    Atlas connection issues. Based on your error message, the problem is:</span>
<span>  </span>
<span>    1. You<span>'</span></span>re using placeholder credentials (<span><span>"</span>cooluser:coolpassword<span>"</span></span>)
  
    To fix this, you have two options:
  
    1. Use a <span>local</span> MongoDB instance by changing line 29-30 <span>in</span> app.js to:
    const dev_db_url = <span><span>"</span>mongodb://127.0.0.1:27017/local_library<span>"</span></span><span>;</span>
    2. Or correctly <span>set</span> up MongoDB Atlas:
      - Log into Atlas at https://cloud.mongodb.com
      - Go to Network Access and add your current IP to whitelist
      - Create proper database credentials
      - Update the connection string with real credentials
  
    Which option would you like to proceed with<span>?</span></pre></div>
</li>
</ol>
</div></article></div></span><span aria-live="polite"></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Paul A. M. Dirac, Interview by Friedrich Hund (1982) [video] (106 pts)]]></title>
            <link>https://www.youtube.com/watch?v=xJzrU38pGWc</link>
            <guid>43446442</guid>
            <pubDate>Sat, 22 Mar 2025 15:30:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=xJzrU38pGWc">https://www.youtube.com/watch?v=xJzrU38pGWc</a>, See on <a href="https://news.ycombinator.com/item?id=43446442">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon wants a product safety regulator declared unconstitutional (152 pts)]]></title>
            <link>https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/</link>
            <guid>43446103</guid>
            <pubDate>Sat, 22 Mar 2025 14:56:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/">https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/</a>, See on <a href="https://news.ycombinator.com/item?id=43446103">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[PyTorch Internals: Ezyang's Blog (357 pts)]]></title>
            <link>https://blog.ezyang.com/2019/05/pytorch-internals/</link>
            <guid>43445931</guid>
            <pubDate>Sat, 22 Mar 2025 14:39:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.ezyang.com/2019/05/pytorch-internals/">https://blog.ezyang.com/2019/05/pytorch-internals/</a>, See on <a href="https://news.ycombinator.com/item?id=43445931">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<!-- -*- mode: rst -*- -->
<p>This post is a long form essay version of a talk about PyTorch internals, that I gave at the PyTorch NYC meetup on May 14, 2019.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-01.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-01.png"></p></div>
<p>Hi everyone!  Today I want to talk about the internals of <a href="https://pytorch.org/">PyTorch</a>.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-02.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-02.png"></p></div>
<p>This talk is for those of you who have used PyTorch, and thought to yourself, "It would be great if I could contribute to PyTorch," but were scared by PyTorch's behemoth of a C++ codebase.  I'm not going to lie: the PyTorch codebase can be a bit overwhelming at times. The purpose of this talk is to put a map in your hands: to tell you about the basic conceptual structure of a "tensor library that supports automatic differentiation", and give you some tools and tricks for finding your way around the codebase.  I'm going to assume that you've written some PyTorch before, but haven't necessarily delved deeper into how a machine learning library is written.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-03.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-03.png"></p></div>
<p>The talk is in two parts: in the first part, I'm going to first introduce you to the conceptual universe of a tensor library.  I'll start by talking about the tensor data type you know and love, and give a more detailed discussion about what exactly this data type provides, which will lead us to a better understanding of how it is actually implemented under the hood.  If you're an advanced user of PyTorch, you'll be familiar with most of this material.  We'll also talk about the trinity of "extension points", layout, device and dtype, which guide how we think about extensions to the tensor class.  In the live talk at PyTorch NYC, I skipped the slides about autograd, but I'll talk a little bit about them in these notes as well.</p>
<p>The second part grapples with the actual nitty gritty details involved with actually coding in PyTorch.  I'll tell you how to cut your way through swaths of autograd code, what code actually matters and what is legacy, and also all of the cool tools that PyTorch gives you for writing kernels.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-04.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-04.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-05.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-05.png"></p></div>
<p>The tensor is the central data structure in PyTorch.  You probably have a pretty good idea about what a tensor intuitively represents: its an n-dimensional data structure containing some sort of scalar type, e.g., floats, ints, et cetera.  We can think of a tensor as consisting of some data, and then some metadata describing the size of the tensor, the type of the elements in contains (dtype), what device the tensor lives on (CPU memory? CUDA memory?)</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-06.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-06.png"></p></div>
<p>There's also a little piece of metadata you might be less familiar with: the stride.  Strides are actually one of the distinctive features of PyTorch, so it's worth discussing them a little more.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-07.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-07.png"></p></div>
<p>A tensor is a mathematical concept.  But to represent it on our computers, we have to define some sort of physical representation for them.  The most common representation is to lay out each element of the tensor contiguously in memory (that's where the term contiguous comes from), writing out each row to memory, as you see above. In the example above, I've specified that the tensor contains 32-bit integers, so you can see that each integer lies in a physical address, each offset four bytes from each other.  To remember what the actual dimensions of the tensor are, we have to also record what the sizes are as extra metadata.</p>
<p>So, what do strides have to do with this picture?</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-08.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-08.png"></p></div>
<p>Suppose that I want to access the element at position <tt>tensor[1, 0]</tt> in my logical representation.  How do I translate this logical position into a location in physical memory?  Strides tell me how to do this: to find out where any element for a tensor lives, I multiply each index with the respective stride for that dimension, and sum them all together.  In the picture above, I've color coded the first dimension blue and the second dimension red, so you can follow the index and stride in the stride calculation.  Doing this sum, I get two (zero-indexed), and indeed, the number three lives two below the beginning of the contiguous array.</p>
<p>(Later in the talk, I'll talk about TensorAccessor, a convenience class that handles the indexing calculation.  When you use TensorAccessor, rather than raw pointers, this calculation is handled under the covers for you.)</p>
<p>Strides are the fundamental basis of how we provide views to PyTorch users.  For example, suppose that I want to extract out a tensor that represents the second row of the tensor above:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-09.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-09.png"></p></div>
<p>Using advanced indexing support, I can just write <tt>tensor[1, :]</tt> to get this row.  Here's the important thing: when I do this, I don't create a new tensor; instead, I just return a tensor which is a different view on the underlying data.  This means that if I, for example, edit the data in that view, it will be reflected in the original tensor.  In this case, it's not too hard to see how to do this: three and four live in contiguous memory, and all we need to do is record an offset saying that the data of this (logical) tensor lives two down from the top.  (Every tensor records an offset, but most of the time it's zero, and I'll omit it from my diagrams when that's the case.)</p>
<!--  -->
<blockquote>
<p>Question from the talk: If I take a view on a tensor, how do I free the memory of the underlying tensor?</p>
<p>Answer: You have to make a copy of the view, thus disconnecting it from the original physical memory.  There's really not much else you can do.  By the way, if you have written Java in the old days, taking substrings of strings has a similar problem, because by default no copy is made, so the substring retains the (possibly very large string). Apparently, they <a href="https://stackoverflow.com/questions/14161050/java-string-substring-method-potential-memory-leak">fixed this in Java 7u6</a>.</p>
</blockquote>
<p>A more interesting case is if I want to take the first column:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-10.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-10.png"></p></div>
<p>When we look at the physical memory, we see that the elements of the column are not contiguous: there's a gap of one element between each one.  Here, strides come to the rescue: instead of specifying a stride of one, we specify a stride of two, saying that between one element and the next, you need to jump two slots.  (By the way, this is why it's called a "stride": if we think of an index as walking across the layout, the stride says how many locations we stride forward every time we take a step.)</p>
<p>The stride representation can actually let you represent all sorts of interesting views on tensors; if you want to play around with the possibilities, check out the <a href="https://ezyang.github.io/stride-visualizer/index.html">Stride Visualizer</a>.</p>
<p>Let's step back for a moment, and think about how we would actually implement this functionality (after all, this is an internals talk.)  If we can have views on tensor, this means we have to decouple the notion of the tensor (the user-visible concept that you know and love), and the actual physical data that stores the data of the tensor (called storage):</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-11.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-11.png"></p></div>
<p>There may be multiple tensors which share the same storage.  Storage defines the dtype and physical size of the tensor, while each tensor records the sizes, strides and offset, defining the logical interpretation of the physical memory.</p>
<p>One thing to realize is that there is always a pair of Tensor-Storage, even for "simple" cases where you don't really need a storage (e.g., you just allocated a contiguous tensor with <tt>torch.zeros(2, 2)</tt>).</p>
<!--  -->
<blockquote>
By the way, we're interested in making this picture not true; instead of having a separate concept of storage, just define a view to be a tensor that is backed by a base tensor.  This is a little more complicated, but it has the benefit that contiguous tensors get a much more direct representation without the Storage indirection.  A change like this would make PyTorch's internal representation a bit more like Numpy's.</blockquote>
<hr>
<p>We've talked quite a bit about the data layout of tensor (some might say, if you get the data representation right, everything else falls in place).  But it's also worth briefly talking about how operations on the tensor are implemented.  At the very most abstract level, when you call <tt>torch.mm</tt>, two dispatches happen:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-12.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-12.png"></p></div>
<p>The first dispatch is based on the device type and layout of a tensor: e.g., whether or not it is a CPU tensor or a CUDA tensor (and also, e.g., whether or not it is a strided tensor or a sparse one).  This is a dynamic dispatch: it's a virtual function call (exactly where that virtual function call occurs will be the subject of the second half of this talk).  It should make sense that you need to do a dispatch here: the implementation of CPU matrix multiply is quite different from a CUDA implementation.  It is a <em>dynamic</em> dispatch because these kernels may live in separate libraries (e.g., <tt>libcaffe2.so</tt> versus <tt>libcaffe2_gpu.so</tt>), and so you have no choice: if you want to get into a library that you don't have a direct dependency on, you have to dynamic dispatch your way there.</p>
<p>The second dispatch is a dispatch on the dtype in question.  This dispatch is just a simple switch-statement for whatever dtypes a kernel chooses to support.  Upon reflection, it should also make sense that we need to a dispatch here: the CPU code (or CUDA code, as it may) that implements multiplication on <tt>float</tt> is different from the code for <tt>int</tt>.  It stands to reason you need separate kernels for each dtype.</p>
<p>This is probably the most important mental picture to have in your head, if you're trying to understand the way operators in PyTorch are invoked.  We'll return to this picture when it's time to look more at code.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-13.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-13.png"></p></div>
<p>Since we have been talking about Tensor, I also want to take a little time to the world of tensor extensions.  After all, there's more to life than dense, CPU float tensors.  There's all sorts of interesting extensions going on, like XLA tensors, or quantized tensors, or MKL-DNN tensors, and one of the things we have to think about, as a tensor library, is how to accommodate these extensions.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-14.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-14.png"></p></div>
<p>Our current model for extensions offers four extension points on tensors.  First, there is the trinity three parameters which uniquely determine what a tensor is:</p>
<ul>
<li>The <strong>device</strong>, the description of where the tensor's physical memory is actually stored, e.g., on a CPU, on an NVIDIA GPU (cuda), or perhaps on an AMD GPU (hip) or a TPU (xla).  The distinguishing characteristic of a device is that it has its own allocator, that doesn't work with any other device.</li>
<li>The <strong>layout</strong>, which describes how we logically interpret this physical memory.  The most common layout is a strided tensor, but sparse tensors have a different layout involving a pair of tensors, one for indices, and one for data; MKL-DNN tensors may have even more exotic layout, like blocked layout, which can't be represented using merely strides.</li>
<li>The <strong>dtype</strong>, which describes what it is that is actually stored in each element of the tensor.  This could be floats or integers, or it could be, for example, quantized integers.</li>
</ul>
<p>If you want to add an extension to PyTorch tensors (by the way, if that's what you want to do, please talk to us!  None of these things can be done out-of-tree at the moment), you should think about which of these parameters you would extend.  The Cartesian product of these parameters define all of the possible tensors you can make.  Now, not all of these combinations may actually have kernels (who's got kernels for sparse, quantized tensors on FPGA?) but in <em>principle</em> the combination could make sense, and thus we support expressing it, at the very least.</p>
<p>There's one last way you can make an "extension" to Tensor functionality, and that's write a wrapper class around PyTorch tensors that implements your object type.  This perhaps sounds obvious, but sometimes people reach for extending one of the three parameters when they should have just made a wrapper class instead.  One notable merit of wrapper classes is they can be developed entirely out of tree.</p>
<p>When should you write a tensor wrapper, versus extending PyTorch itself?  The key test is whether or not you need to pass this tensor along during the autograd backwards pass.  This test, for example, tells us that sparse tensor should be a true tensor extension, and not just a Python object that contains an indices and values tensor: when doing optimization on networks involving embeddings, we want the gradient generated by the embedding to be sparse.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-15.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-15.png"></p></div>
<p>Our philosophy on extensions also has an impact of the data layout of tensor itself.  One thing we really want out of our tensor struct is for it to have a fixed layout: we don't want fundamental (and very frequently called) operations like "What's the size of a tensor?" to require virtual dispatches.  So when you look at the actual layout of a Tensor (defined in the <a href="https://github.com/pytorch/pytorch/blob/master/c10/core/TensorImpl.h">TensorImpl struct</a>),  what we see is a common prefix of all fields that we consider all "tensor"-like things to universally have, plus a few fields that are only really applicable for strided tensors, but are <em>so</em> important that we've kept them in the main struct, and then a suffix of custom fields that can be done on a per-Tensor basis.  Sparse tensors, for example, store their indices and values in this suffix.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-16.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-16.png"></p></div>
<p>I told you all about tensors, but if that was the only thing PyTorch provided, we'd basically just be a Numpy clone.  The distinguishing characteristic of PyTorch when it was originally released was that it provided automatic differentiation on tensors (these days, we have other cool features like TorchScript; but back then, this was it!)</p>
<p>What does automatic differentiation do?  It's the machinery that's responsible for taking a neural network:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-17.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-17.png"></p></div>
<p>...and fill in the missing code that actually computes the gradients of your network:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-18.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-18.png"></p></div>
<p>Take a moment to study this diagram.  There's a lot to unpack; here's what to look at:</p>
<ol>
<li>First, rest your eyes on the variables in red and blue.  PyTorch implements <a href="https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation">reverse-mode automatic differentiation</a>, which means that we effectively walk the forward computations "backward" to compute the gradients.  You can see this if you look at the variable names: at the bottom of the red, we compute <tt>loss</tt>; then, the first thing we do in the blue part of the program is compute <tt>grad_loss</tt>.  <tt>loss</tt> was computed from <tt>next_h2</tt>, so we compute <tt>grad_next_h2</tt>.  Technically, these variables which we call <tt>grad_</tt> are not really gradients; they're really Jacobians left-multiplied by a vector, but in PyTorch we just call them <tt>grad</tt> and mostly everyone knows what we mean.</li>
<li>If the structure of the code stays the same, the behavior doesn't: each line from forwards is replaced with a different computation, that represents the derivative of the forward operation.  For example, the <tt>tanh</tt> operation is translated into a <tt>tanh_backward</tt> operation (these two lines are connected via a grey line on the left hand side of the diagram).  The inputs and outputs of the forward and backward operations are swapped: if the forward operation produced <tt>next_h2</tt>, the backward operation takes <tt>grad_next_h2</tt> as an input.</li>
</ol>
<p>The whole point of autograd is to do the computation that is described by this diagram, but without actually ever generating this source.  PyTorch autograd doesn't do a source-to-source transformation (though PyTorch JIT does know how to do symbolic differentiation).</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-19.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-19.png"></p></div>
<p>To do this, we need to store more metadata when we carry out operations on tensors.  Let's adjust our picture of the tensor data structure: now instead of just a tensor which points to a storage, we now have a variable which wraps this tensor, and also stores more information (AutogradMeta), which is needed for performing autograd when a user calls <tt>loss.backward()</tt> in their PyTorch script.</p>
<!--  -->
<blockquote>
This is yet another slide which will hopefully be out of date in the near future.  Will Feng is working on a <a href="https://github.com/pytorch/pytorch/issues/13638">Variable-Tensor merge in C++</a>, following a simple merge which happened to PyTorch's frontend interface.</blockquote>
<p>We also have to update our picture about dispatch:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-20.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-20.png"></p></div>
<p>Before we dispatch to CPU or CUDA implementations, there is another dispatch on variables, which is responsible for unwrapping variables, calling the underlying implementation (in green), and then rewrapping the results into variables and recording the necessary autograd metadata for backwards.</p>
<p>Some implementations don't unwrap; they just call into other variable implementations.  So you might spend a while in the Variable universe.  However, once you unwrap and go into the non-Variable Tensor universe, that's it; you never go back to Variable (except by returning from your function.)</p>
<hr>
<p>In my NY meetup talk, I skipped the following seven slides.  I'm also going to delay writeup for them; you'll have to wait for the sequel for some text.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-21.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-21.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-22.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-22.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-23.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-23.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-24.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-24.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-25.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-25.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-26.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-26.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-27.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-27.png"></p></div>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-28.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-28.png"></p></div>
<p>Enough about concepts, let's look at some code.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-29.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-29.png"></p></div>
<p>PyTorch has a lot of folders, and there is a very detailed description of what they are in the <a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#codebase-structure">CONTRIBUTING</a> document, but really, there are only four directories you really need to know about:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-30.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-30.png"></p></div>
<ul>
<li>First, <tt>torch/</tt> contains what you are most familiar with: the actual Python modules that you import and use.  This stuff is Python code and easy to hack on (just make a change and see what happens).  However, lurking not too deep below the surface is...</li>
<li><tt>torch/csrc/</tt>, the C++ code that implements what you might call the frontend of PyTorch.  In more descriptive terms, it implements the binding code that translates between the Python and C++ universe, and also some pretty important pieces of PyTorch, like the autograd engine and the JIT compiler.  It also contains the C++ frontend code.</li>
<li><tt>aten/</tt>, short for "A Tensor Library" (coined by Zachary DeVito), is a C++ library that implements the operations of Tensors.  If you're looking for where some kernel code lives, chances are it's in ATen.  ATen itself bifurcates into two neighborhoods of operators: the "native" operators, which are modern, C++ implementations of operators, and the "legacy" operators (TH, THC, THNN, THCUNN), which are legacy, C implementations.  The legacy operators are the bad part of town; try not to spend too much time there if you can.</li>
<li><tt>c10/</tt>, which is a pun on Caffe2 and A"Ten" (get it? Caffe 10) contains the core abstractions of PyTorch, including the actual implementations of the Tensor and Storage data structures.</li>
</ul>
<p>That's a lot of places to look for code; we should probably simplify the directory structure, but that's how it is.  If you're trying to work on operators, you'll spend most of your time in <tt>aten</tt>.</p>
<p>Let's see how this separation of code breaks down in practice:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-31.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-31.png"></p></div>
<p>When you call a function like <tt>torch.add</tt>, what actually happens?  If you remember the discussion we had about dispatching, you already have the basic picture in your head:</p>
<ol>
<li>We have to translate from Python realm to the C++ realm (Python argument parsing)</li>
<li>We handle <strong>variable</strong> dispatch (VariableType--Type, by the way, doesn't really have anything to do programming language types, and is just a gadget for doing dispatch.)</li>
<li>We handle <strong>device type / layout</strong> dispatch (Type)</li>
<li>We have the actual kernel, which is either a modern native function, or a legacy TH function.</li>
</ol>
<p>Each of these steps corresponds concretely to some code.  Let's cut our way through the jungle.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-32.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-32.png"></p></div>
<p>Our initial landing point in the C++ code is the C implementation of a Python function, which we've exposed to the Python side as something like <tt>torch._C.VariableFunctions.add</tt>.  <tt>THPVariable_add</tt> is the implementation of one such implementation.</p>
<p>One important thing to know about this code is that it is auto-generated.  If you search in the GitHub repository, you won't find it, because you have to actually build PyTorch to see it.  Another important thing is, you don't have to really deeply understand what this code is doing; the idea is to skim over it and get a sense for what it is doing.  Above, I've annotated some of the most important bits in blue: you can see that there is a use of a class <tt>PythonArgParser</tt> to actually pull out C++ objects out of the Python <tt>args</tt> and <tt>kwargs</tt>; we then call a <tt>dispatch_add</tt> function (which I've inlined in red); this releases the global interpreter lock and then calls a plain old method on the C++ Tensor <tt>self</tt>.  On its way back, we rewrap the returned <tt>Tensor</tt> back into a <tt>PyObject</tt>.</p>
<p>(At this point, there's an error in the slides: I'm supposed to tell you about the Variable dispatch code.  I haven't fixed it here yet.  Some magic happens, then...)</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-33.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-33.png"></p></div>
<p>When we call the <tt>add</tt> method on the <tt>Tensor</tt> class, no virtual dispatch happens yet.  Instead, we have an inline method which calls a virtual method on a "Type" object.  This method is the actual virtual method (this is why I say Type is just a "gadget" that gets you dynamic dispatch.)  In the particular case of this example, this virtual call dispatches to an implementation of add on a class named <tt>TypeDefault</tt>.  This happens to be because we have an implementation of <tt>add</tt> that is the same for every device type (both CPU and CUDA); if we had happened to have different implementations, we might have instead landed on something like <tt><span>CPUFloatType::add</span></tt>.  It is this implementation of the virtual method that finally gets us to the actual kernel code.</p>
<!--  -->
<blockquote>
Hopefully, this slide will be out-of-date very soon too; Roy Li is working on replacing <tt>Type</tt> dispatch with another mechanism which will help us better support PyTorch on mobile.</blockquote>
<p>It's worth reemphasizing that all of the code, until we got to the kernel, is automatically generated.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-34.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-34.png"></p></div>
<p>It's a bit twisty and turny, so once you have some basic orientation about what's going on, I recommend just jumping straight to the kernels.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-35.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-35.png"></p></div>
<p>PyTorch offers a lot of useful tools for prospective kernel writers.  In this section, we'll walk through a few of them.  But first of all, what do you need to write a kernel?</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-36.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-36.png"></p></div>
<p>We generally think of a kernel in PyTorch consisting of the following parts:</p>
<ol>
<li>First, there's some metadata which we write about the kernel, which powers the code generation and lets you get all the bindings to Python, without having to write a single line of code.</li>
<li>Once you've gotten to the kernel, you're past the device type / layout dispatch. The first thing you need to write is error checking, to make sure the input tensors are the correct dimensions.  (Error checking is really important!  Don't skimp on it!)</li>
<li>Next, we generally have to allocate the result tensor which we are going to write the output into.</li>
<li>Time for the kernel proper.  At this point, you now should do the second, dtype dispatch, to jump into a kernel which is specialized per dtype it operates on.  (You don't want to do this too early, because then you will be uselessly duplicating code that looks the same in any case.)</li>
<li>Most performant kernels need some sort of parallelization, so that you can take advantage of multi-CPU systems.  (CUDA kernels are "implicitly" parallelized, since their programming model is built on top of massive parallelization).</li>
<li>Finally, you need to access the data and do the computation you wanted to do!</li>
</ol>
<p>In the subsequent slides, we'll walk through some of the tools PyTorch has for helping you implementing these steps.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-37.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-37.png"></p></div>
<p>To take advantage of all of the code generation which PyTorch brings, you need to write a <em>schema</em> for your operator.  The schema gives a mypy-esque type of your function, and also controls whether or not we generate bindings for methods or functions on Tensor.  You also tell the schema what implementations of your operator should be called for given device-layout combinations.  Check out the <a href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md">README in native</a> is for more information about this format.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-38.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-38.png"></p></div>
<p>You also may need to define a derivative for your operation in <a href="https://github.com/pytorch/pytorch/blob/master/tools/autograd/derivatives.yaml">derivatives.yaml</a>.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-39.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-39.png"></p></div>
<p>Error checking can be done by way of either a low level or a high level API.  The low level API is just a macro, <tt>TORCH_CHECK</tt>, which takes a boolean, and then any number of arguments to make up the error string to render if the boolean is not true.  One nice thing about this macro is that you can intermix strings with non-string data; everything is formatted using their implementation of <tt>operator&lt;&lt;</tt>, and most important data types in PyTorch have <tt>operator&lt;&lt;</tt> implementations.</p>
<p>The high level API saves you from having to write up repetitive error messages over and over again.  The way it works is you first wrap each <tt>Tensor</tt> into a <tt>TensorArg</tt>, which contains information about where the tensor came from (e.g., its argument name).  It then provides a number of pre-canned functions for checking various properties; e.g., <tt>checkDim()</tt> tests if the tensor's dimensionality is a fixed number.  If it's not, the function provides a user-friendly error message based on the <tt>TensorArg</tt> metadata.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-40.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-40.png"></p></div>
<p>One important thing to be aware about when writing operators in PyTorch, is that you are often signing up to write <em>three</em> operators: <tt>abs_out</tt>, which operates on a preallocated output (this implements the <tt>out=</tt> keyword argument), <tt>abs_</tt>, which operates inplace, and <tt>abs</tt>, which is the plain old functional version of an operator.</p>
<p>Most of the time, <tt>abs_out</tt> is the real workhorse, and <tt>abs</tt> and <tt>abs_</tt> are just thin wrappers around <tt>abs_out</tt>; but sometimes writing specialized implementations for each case are warranted.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-41.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-41.png"></p></div>
<p>To do dtype dispatch, you should use the <tt>AT_DISPATCH_ALL_TYPES</tt> macro.  This takes in the dtype of the tensor you want to dispatch over, and a lambda which will be specialized for each dtype that is dispatchable from the macro.  Usually, this lambda just calls a templated helper function.</p>
<p>This macro doesn't just "do dispatch", it also decides what dtypes your kernel will support.  As such, there are actually quite a few versions of this macro, which let you pick different subsets of dtypes to generate specializations for.  Most of the time, you'll just want <tt>AT_DISPATCH_ALL_TYPES</tt>, but keep an eye out for situations when you might want to dispatch to some more types.  There's guidance in <a href="https://github.com/pytorch/pytorch/blob/21ef4cc615a7d9d772ade52a5023900718b09e92/aten/src/ATen/Dispatch.h#L62">Dispatch.h</a> for how to select the correct one for your use-case.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-43.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-43.png"></p></div>
<p>On CPU, you frequently want to parallelize your code.  In the past, this was usually done by directly sprinkling OpenMP pragmas in your code.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-42.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-42.png"></p></div>
<p>At some point, we have to actually access the data.  PyTorch offers quite a few options for doing this.</p>
<ol>
<li>If you just want to get a value at some specific location, you should use <tt>TensorAccessor</tt>.  A tensor accessor is like a tensor, but it hard codes the dimensionality and dtype of the tensor as template parameters.  When you retrieve an accessor like <tt>x.accessor&lt;float, <span>3&gt;();</span></tt>, we do a runtime test to make sure that the tensor really is this format; but after that, every access is unchecked.  Tensor accessors handle strides correctly, so you should prefer using them over raw pointer access (which, unfortunately, some legacy kernels do.)  There is also a <tt>PackedTensorAccessor</tt>, which is specifically useful for sending an accessor over a CUDA launch, so that you can get accessors from inside your CUDA kernel.  (One notable gotcha: <tt>TensorAccessor</tt> defaults to 64-bit indexing, which is much slower than 32-bit indexing in CUDA!)</li>
<li>If you're writing some sort of operator with very regular element access, for example, a pointwise operation, you are much better off using a higher level of abstraction, the <tt>TensorIterator</tt>.   This helper class automatically handles broadcasting and type promotion for you, and is quite handy.</li>
<li>For true speed on CPU, you may need to write your kernel using vectorized CPU instructions.  We've got helpers for that too!  The <tt>Vec256</tt> class represents a vector of scalars and provides a number of methods which perform vectorized operations on them all at once.  Helpers like <tt>binary_kernel_vec</tt> then let you easily run vectorized operations, and then finish everything that doesn't round nicely into vector instructions using plain old instructions.  The infrastructure here also manages compiling your kernel multiple times under different instruction sets, and then testing at runtime what instructions your CPU supports, and using the best kernel in those situations.</li>
</ol>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-44.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-44.png"></p></div>
<p>A lot of kernels in PyTorch are still written in the legacy TH style.  (By the way, TH stands for TorcH.  It's a pretty nice acronym, but unfortunately it is a bit poisoned; if you see TH in the name, assume that it's legacy.)  What do I mean by the legacy TH style?</p>
<ol>
<li>It's written in C style, no (or very little) use of C++.</li>
<li>It's manually refcounted (with manual calls to <tt>THTensor_free</tt> to decrease refcounts when you're done using tensors), and</li>
<li>It lives in <tt>generic/</tt> directory, which means that we are actually going to compile the file multiple times, but with different <tt>#define scalar_t</tt>.</li>
</ol>
<p>This code is pretty crazy, and we hate reviewing it, so please don't add to it.  One of the more useful tasks that you can do, if you like to code but don't know too much about kernel writing, is to port some of these TH functions to ATen.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-45.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-45.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-46.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-46.png"></p></div>
<p>To wrap up, I want to talk a little bit about working efficiently on PyTorch.  If the largeness of PyTorch's C++ codebase is the first gatekeeper that stops people from contributing to PyTorch, the efficiency of your workflow is the second gatekeeper.  If you try to work on C++ with Python habits, <strong>you will have a bad time</strong>: it will take forever to recompile PyTorch, and it will take you forever to tell if your changes worked or not.</p>
<p>How to work efficiently could probably be a talk in and of itself, but this slide calls out some of the most common anti-patterns I've seen when someone complains: "It's hard to work on PyTorch."</p>
<ol>
<li>If you edit a header, especially one that is included by many source files (and especially if it is included by CUDA files), expect a very long rebuild.  Try to stick to editing cpp files, and edit headers sparingly!</li>
<li>Our CI is a very wonderful, zero-setup way to test if your changes worked or not. But expect to wait an hour or two before you get back signal.  If you are working on a change that will require lots of experimentation, spend the time setting up a local development environment.  Similarly, if you run into a hard to debug problem on a specific CI configuration, set it up locally.  You can <a href="https://github.com/pytorch/ossci-job-dsl">download and run the Docker images locally</a></li>
<li>The <a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#use-ccache">CONTRIBUTING guide explains how to setup ccache</a>; this is highly recommended, because sometimes it will help you get lucky and avoid a massive recompile when you edit a header.  It also helps cover up bugs in our build system, when we recompile files when we shouldn't.</li>
<li>At the end of the day, we have a lot of C++ code, and you will have a much more pleasant experience if you build on a beefy server with CPUs and RAM.  In particular, I don't recommend doing CUDA builds on a laptop; building CUDA is sloooooow and laptops tend to not have enough juice to turnaround quickly enough.</li>
</ol>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-47.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-47.png"></p></div>
<p>So that's it for a whirlwind tour of PyTorch's internals!  Many, many things have been omitted; but hopefully the descriptions and explanations here can help you get a grip on at least a substantial portion of the codebase.</p>
<p>Where should you go from here?  What kinds of contributions can you make?  A good place to start is our issue tracker.  Starting earlier this year, we have been triaging issues; issues labeled <strong>triaged</strong> mean that at least one PyTorch developer has looked at it and made an initial assessment about the issue.  You can use these labels to find out what issues we think are <a href="https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22high+priority%22+label%3Atriaged">high priority</a> or look up issues specific to some module, e.g., <a href="https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3Atriaged+label%3A%22module%3A+autograd%22">autograd</a> or find issues which we think are <a href="https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3Atriaged+label%3Asmall">small</a> (word of warning: we're sometimes wrong!)</p>
<p>Even if you don't want to get started with coding right away, there are many other useful activities like improving documentation (I <em>love</em> merging documentation PRs, they are so great), helping us reproduce bug reports from other users, and also just helping us discuss RFCs on the issue tracker. PyTorch would not be where it is today without our open source contributors; we hope you can join us too!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Understanding R1-Zero-Like Training: A Critical Perspective (130 pts)]]></title>
            <link>https://github.com/sail-sg/understand-r1-zero</link>
            <guid>43445894</guid>
            <pubDate>Sat, 22 Mar 2025 14:35:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/sail-sg/understand-r1-zero">https://github.com/sail-sg/understand-r1-zero</a>, See on <a href="https://news.ycombinator.com/item?id=43445894">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p dir="auto"><h2 tabindex="-1" dir="auto">Understanding R1-Zero-Like Training: A Critical Perspective</h2><a id="user-content-understanding-r1-zero-like-training-a-critical-perspective" aria-label="Permalink: Understanding R1-Zero-Like Training: A Critical Perspective" href="#understanding-r1-zero-like-training-a-critical-perspective"></a></p>
<p dir="auto"><a href="https://github.com/sail-sg/understand-r1-zero/blob/main/understand-r1-zero.pdf"><img src="https://camo.githubusercontent.com/b8b7a1f24d0115db199a318f8baba58ee41a4e62682fe54b18361fc57a7acceb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617065722d3843413141463f6c6f676f3d72656164746865646f6373266c6f676f436f6c6f723d7768697465" alt="Paper" data-canonical-src="https://img.shields.io/badge/Paper-8CA1AF?logo=readthedocs&amp;logoColor=white"></a></p>
<p dir="auto"><a href="https://github.com/sail-sg/understand-r1-zero"><img src="https://camo.githubusercontent.com/254c537f0b39d9dd4f7190bc33f014ebac3cc91df480eb0cf914615a568a8612/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f556e6465727374616e6425323052312532305a65726f2d3030303030303f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562266c6f676f436f6c6f723d303030266c6f676f436f6c6f723d7768697465" alt="Github" data-canonical-src="https://img.shields.io/badge/Understand%20R1%20Zero-000000?style=for-the-badge&amp;logo=github&amp;logoColor=000&amp;logoColor=white"></a>  <a href="https://huggingface.co/collections/sail/oat-zero-understanding-r1-zero-like-training-67dcdb07b9f3eb05f1501c4a" rel="nofollow"><img src="https://camo.githubusercontent.com/4de185a217b7c8b03b23d64603118bd1247c2be4b8d46529d0141177fc31fe2e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d6f64656c5f436f6c6c656374696f6e2d6663643032323f7374796c653d666f722d7468652d6261646765266c6f676f3d68756767696e6766616365266c6f676f436f6c6f723d303030" alt="Hugging Face Collection" data-canonical-src="https://img.shields.io/badge/Model_Collection-fcd022?style=for-the-badge&amp;logo=huggingface&amp;logoColor=000"></a></p>

</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Updates</h2><a id="user-content-updates" aria-label="Permalink: Updates" href="#updates"></a></p>
<ul dir="auto">
<li>21/03/2025: 🎉 We release our paper, models and codebase. Our R1-Zero training is implemented with 🌾 <a href="https://github.com/sail-sg/oat">Oat</a>, a highly modular, research-friendly and efficient LLM RL framework.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Links</h2><a id="user-content-links" aria-label="Permalink: Links" href="#links"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Understanding R1-Zero-Like Training</strong></p>
<ul dir="auto">
<li>📄 <a href="https://github.com/sail-sg/understand-r1-zero/blob/main/understand-r1-zero.pdf">Paper</a></li>
<li>🤗 <a href="https://huggingface.co/collections/sail/oat-zero-understanding-r1-zero-like-training-67dcdb07b9f3eb05f1501c4a" rel="nofollow">Models</a></li>
</ul>
</li>
<li>
<p dir="auto"><strong>There May Not Be Aha Moment in R1-Zero-like Training — A Pilot Study</strong></p>
<ul dir="auto">
<li>📄 <a href="https://oatllm.notion.site/oat-zero" rel="nofollow">Blog</a></li>
<li>💻 <a href="https://github.com/sail-sg/oat-zero">Code</a></li>
</ul>
</li>
<li>
<p dir="auto"><strong>OAT: A research-friendly framework for LLM online alignment</strong></p>
<ul dir="auto">
<li>💻 <a href="https://github.com/sail-sg/oat">Codebase</a></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">TL;DR</h2><a id="user-content-tldr" aria-label="Permalink: TL;DR" href="#tldr"></a></p>
<p dir="auto">To understand R1-Zero-like training, we critically examine two core components: <strong>base models</strong>
and <strong>reinforcement learning</strong>. We highlight our findings below.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">On base models:</h3><a id="user-content-on-base-models" aria-label="Permalink: On base models:" href="#on-base-models"></a></p>
<ol dir="auto">
<li><strong>DeepSeek-V3-Base already exhibit "Aha moment"</strong>.</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/deepseek-base-aha.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/deepseek-base-aha.png" width="70%/"></a>
</p>
<ol start="2" dir="auto">
<li>As the popular choice for R1-Zero-like training, Qwen2.5 base models demonstrate strong reasoning capabilities
even <strong>without</strong> prompt templates: the average benchmark scores improve by <strong>~60%</strong> (compared to the traditional 4-shot prompting)!</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/qwen-math-base-scores.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/qwen-math-base-scores.png" width="70%/"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">On reinforcement learning:</h3><a id="user-content-on-reinforcement-learning" aria-label="Permalink: On reinforcement learning:" href="#on-reinforcement-learning"></a></p>
<ol start="3" dir="auto">
<li>GRPO leads to <strong>biased</strong> optimization! We propose a simple fix that improves token efficiency
while maintaining reasoning performance, termed as Dr. GRPO (GRPO <strong>D</strong>one <strong>R</strong>ight).</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/drgrpo.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/drgrpo.png" width="80%/"></a>
</p>
<ol start="4" dir="auto">
<li>In R1-Zero-like training, the template and the question set perform a duet to affect the RL dynamics
<ul dir="auto">
<li>(Left Plot) For Qwen2.5-Math-1.5B, a mismatched template (e.g., R1 template) in fact <strong>destructs the reasoning capabilities before RL reconstructing it</strong>. This makes the improvement impressive on the surface.</li>
<li>(Middle Plot) However, if a template does not deviate from the pretraining distribution too far, even a small and completely o.o.d. question set (e.g., GSM8K) could induce the reasoning ability equally well, by reinforcing correct reasoning behaviors instead of infusing new knowledge.</li>
</ul>
</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/template-data-duet.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/template-data-duet.png" width="80%/"></a>
</p>
<ol start="5" dir="auto">
<li>Beyond Qwen, Llama can also be RL-tuned from base models. In this case, domain-specific pretraining will improves RL ceiling.
<ul dir="auto">
<li>(Right Plot) GRPO can even make Llama with math knowledge "Aha" by increasing the output length; however, it is likely due to its length bias, which can be removed by Dr. GRPO.</li>
</ul>
</li>
</ol>
 <p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/llama-r1-zero.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/llama-r1-zero.png" width="70%/"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Our minimalist R1-Zero recipe:</h3><a id="user-content-our-minimalist-r1-zero-recipe" aria-label="Permalink: Our minimalist R1-Zero recipe:" href="#our-minimalist-r1-zero-recipe"></a></p>
<p dir="auto">Our analysis suggests a minimalist recipe for R1-Zero-like training:</p>
<p dir="auto">We RL-tune Qwen2.5-
Math-7B using the (unbiased) Dr. GRPO algorithm on MATH level 3-5 questions with the Qwen-Math template, and achieve state-of-the-art performance with only 27 hours compute on 8× A100 GPUs.</p>
 <p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/understand-r1-zero/blob/main/assets/benchmark.png"><img src="https://github.com/sail-sg/understand-r1-zero/raw/main/assets/benchmark.png" width="90%/"></a>
</p>
<p dir="auto">If you are interested in more details, please check out our <a href="https://github.com/sail-sg/understand-r1-zero/blob/main/understand-r1-zero.pdf">paper</a>!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Install</h3><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto">We recommend a clean <code>python==3.10</code> environment for development.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install vllm &amp; oat, the LLM RL framework we developed r1-zero training on.
pip install vllm==0.7.2 &amp;&amp; pip install oat-llm==0.0.9

# Install this package locally to use the math grader.
git clone git@github.com:sail-sg/understand-r1-zero.git &amp;&amp; cd understand-r1-zero
pip install -e ."><pre><span><span>#</span> Install vllm &amp; oat, the LLM RL framework we developed r1-zero training on.</span>
pip install vllm==0.7.2 &amp;&amp; pip install oat-llm==0.0.9

<span><span>#</span> Install this package locally to use the math grader.</span>
git clone git@github.com:sail-sg/understand-r1-zero.git &amp;&amp; cd understand-r1-zero
pip install -e .</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Training</h3><a id="user-content-training" aria-label="Permalink: Training" href="#training"></a></p>
<p dir="auto">We implement R1-Zero training by extending Oat's Learner and Actor components. Please see <a href="https://github.com/sail-sg/understand-r1-zero/blob/main/train_zero_math.py">train_zero_math.py</a> for a step-by-step guide.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Patch LD_LIBRARY_PATH to avoid dependency errors:
export LD_LIBRARY_PATH=$(python -c &quot;import sysconfig; print(sysconfig.get_config_var('LIBDIR'))&quot;):$LD_LIBRARY_PATH

# Run the experiment (tested on 8 x A100-40G) with Dr. GRPO:
# (change to `--critic_type grpo` for running GRPO)
python train_zero_math.py \
    --critic_type drgrpo \
    --gpus 8 \
    --enable_prefix_caching \
    --collocate \
    --vllm_sleep \
    --vllm_gpu_ratio 0.35 \
    --gradient-checkpointing \
    --flash-attn \
    --bf16 \
    --rnd-seed \
    --learning_rate 0.000001 \
    --lr_scheduler constant \
    --num_ppo_epochs 1 \
    --beta 0 \
    --oracle_type reward \
    --oracle math \
    --pretrain Qwen/Qwen2.5-Math-1.5B \
    --prompt_template r1 \
    --zero-stage 2 \
    --ref_offload \
    --prompt_data ./datasets/train/math_12k \
    --train_split train \
    --input_key problem \
    --output_key answer \
    --max-train 9999999 \
    --num_prompt_epoch 20 \
    --prompt_max_length 1024 \
    --num_samples 8 \
    --temperature 1 \
    --top_p 1 \
    --generate_max_length 3000 \
    --save_steps -1 \
    --train_batch_size 128 \
    --rollout_batch_size 128 \
    --rollout_batch_size_per_device 16 \
    --pi_buffer_maxlen_per_device 128 \
    --eval_batch_size 200 \
    --eval_steps 16 \
    --eval_temperature 0 \
    --eval_generate_max_length 3000 \
    --eval_data ./datasets/evaluation_suite \
    --eval_input_key input \
    --use-wb \
    --wb-run-name qwen2.5-Math-1.5b-r1-zero \
    --wb_project oat-zero"><pre><span><span>#</span> Patch LD_LIBRARY_PATH to avoid dependency errors:</span>
export LD_LIBRARY_PATH=$(python -c "import sysconfig; print(sysconfig.get_config_var('LIBDIR'))"):$LD_LIBRARY_PATH

<span><span>#</span> Run the experiment (tested on 8 x A100-40G) with Dr. GRPO:</span>
<span><span>#</span> (change to `--critic_type grpo` for running GRPO)</span>
python train_zero_math.py \
    --critic_type drgrpo \
    --gpus 8 \
    --enable_prefix_caching \
    --collocate \
    --vllm_sleep \
    --vllm_gpu_ratio 0.35 \
    --gradient-checkpointing \
    --flash-attn \
    --bf16 \
    --rnd-seed \
    --learning_rate 0.000001 \
    --lr_scheduler constant \
    --num_ppo_epochs 1 \
    --beta 0 \
    --oracle_type reward \
    --oracle math \
    --pretrain Qwen/Qwen2.5-Math-1.5B \
    --prompt_template r1 \
    --zero-stage 2 \
    --ref_offload \
    --prompt_data ./datasets/train/math_12k \
    --train_split train \
    --input_key problem \
    --output_key answer \
    --max-train 9999999 \
    --num_prompt_epoch 20 \
    --prompt_max_length 1024 \
    --num_samples 8 \
    --temperature 1 \
    --top_p 1 \
    --generate_max_length 3000 \
    --save_steps -1 \
    --train_batch_size 128 \
    --rollout_batch_size 128 \
    --rollout_batch_size_per_device 16 \
    --pi_buffer_maxlen_per_device 128 \
    --eval_batch_size 200 \
    --eval_steps 16 \
    --eval_temperature 0 \
    --eval_generate_max_length 3000 \
    --eval_data ./datasets/evaluation_suite \
    --eval_input_key input \
    --use-wb \
    --wb-run-name qwen2.5-Math-1.5b-r1-zero \
    --wb_project oat-zero</pre></div>
<p dir="auto">Please see <a href="https://github.com/sail-sg/understand-r1-zero/blob/main/examples">here</a> for more example scripts.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Evaluation</h3><a id="user-content-evaluation" aria-label="Permalink: Evaluation" href="#evaluation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Evaluate our models:
python evaluate_model.py --model_name sail/Qwen2.5-Math-7B-Oat-Zero
python evaluate_model.py --model_name sail/Qwen2.5-Math-1.5B-Oat-Zero
python evaluate_model.py --model_name sail/Llama-3.2-3B-Oat-Zero --template r1

# Evaluate baseline models:
python evaluate_model.py --model_name Qwen/Qwen2.5-Math-1.5B
python evaluate_model.py --model_name Qwen/Qwen2.5-Math-7B
python evaluate_model.py --model_name hkust-nlp/Qwen-2.5-Math-7B-SimpleRL-Zero
python evaluate_model.py --model_name PRIME-RL/Eurus-2-7B-PRIME-Zero
python evaluate_model.py --model_name Open-Reasoner-Zero/Open-Reasoner-Zero-7B"><pre><span><span>#</span> Evaluate our models:</span>
python evaluate_model.py --model_name sail/Qwen2.5-Math-7B-Oat-Zero
python evaluate_model.py --model_name sail/Qwen2.5-Math-1.5B-Oat-Zero
python evaluate_model.py --model_name sail/Llama-3.2-3B-Oat-Zero --template r1

<span><span>#</span> Evaluate baseline models:</span>
python evaluate_model.py --model_name Qwen/Qwen2.5-Math-1.5B
python evaluate_model.py --model_name Qwen/Qwen2.5-Math-7B
python evaluate_model.py --model_name hkust-nlp/Qwen-2.5-Math-7B-SimpleRL-Zero
python evaluate_model.py --model_name PRIME-RL/Eurus-2-7B-PRIME-Zero
python evaluate_model.py --model_name Open-Reasoner-Zero/Open-Reasoner-Zero-7B</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you find our work useful for your research, please consider citing:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{liu2025understanding,
  title={Understanding R1-Zero-Like Training: A Critical Perspective},
  author={Zichen Liu and Changyu Chen and Wenjun Li and Penghui Qi and Tianyu Pang and Chao Du and Wee Sun Lee and Min Lin},
  year={2025},
  howpublished={\url{https://github.com/sail-sg/understand-r1-zero}},
}"><pre><span>@misc</span>{<span>liu2025understanding</span>,
  <span>title</span>=<span><span>{</span>Understanding R1-Zero-Like Training: A Critical Perspective<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Zichen Liu and Changyu Chen and Wenjun Li and Penghui Qi and Tianyu Pang and Chao Du and Wee Sun Lee and Min Lin<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
  <span>howpublished</span>=<span><span>{</span>\url{https://github.com/sail-sg/understand-r1-zero}<span>}</span></span>,
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgement</h2><a id="user-content-acknowledgement" aria-label="Permalink: Acknowledgement" href="#acknowledgement"></a></p>
<ul dir="auto">
<li>This work is supported by <a href="https://sail.sea.com/" rel="nofollow">Sea AI Lab</a> for computing resources.</li>
<li>The training codes are built on <a href="https://github.com/sail-sg/oat">Oat</a>, which employs <a href="https://github.com/vllm-project/vllm">vLLM</a>, <a href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a> and <a href="https://github.com/google-deepmind/launchpad">launchpad</a>.</li>
<li>The base models are from <a href="https://huggingface.co/Qwen/Qwen2.5-Math-7B" rel="nofollow">Qwen2.5-Math</a>, <a href="https://huggingface.co/meta-llama/Llama-3.2-3B" rel="nofollow">Llama</a>, and <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base" rel="nofollow">DeepSeek</a>.</li>
<li>We thank Qingfeng Lan for his time in thoroughly reviewing our code.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: FastOpenAPI – automated docs for many Python frameworks (135 pts)]]></title>
            <link>https://github.com/mr-fatalyst/fastopenapi</link>
            <guid>43445720</guid>
            <pubDate>Sat, 22 Mar 2025 14:10:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mr-fatalyst/fastopenapi">https://github.com/mr-fatalyst/fastopenapi</a>, See on <a href="https://news.ycombinator.com/item?id=43445720">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/mr-fatalyst/fastopenapi/master/logo.png"><img src="https://raw.githubusercontent.com/mr-fatalyst/fastopenapi/master/logo.png" alt="Logo"></a>
</p>
<p dir="auto">
  <b>FastOpenAPI</b> is a library for generating and integrating OpenAPI schemas using Pydantic and various frameworks.
</p>
<p dir="auto">
  This project was inspired by <a href="https://fastapi.tiangolo.com/" rel="nofollow">FastAPI</a> and aims to provide a similar developer-friendly experience.
</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/95207bc5a4752edda90d15266a572544a3dbb84deb37be8db3f51ec6a8b7d47a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d722d666174616c7973742f666173746f70656e617069"><img src="https://camo.githubusercontent.com/95207bc5a4752edda90d15266a572544a3dbb84deb37be8db3f51ec6a8b7d47a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d722d666174616c7973742f666173746f70656e617069" data-canonical-src="https://img.shields.io/github/license/mr-fatalyst/fastopenapi"></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/mr-fatalyst/fastopenapi/actions/workflows/master.yml/badge.svg"><img src="https://github.com/mr-fatalyst/fastopenapi/actions/workflows/master.yml/badge.svg"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5c66ef23261df1a6bde8781d2cf6f4d04e2fbd46e5776cab7b6ffb384f83f4ff/68747470733a2f2f636f6465636f762e696f2f67682f6d722d666174616c7973742f666173746f70656e6170692f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d55534852314930434a42"><img src="https://camo.githubusercontent.com/5c66ef23261df1a6bde8781d2cf6f4d04e2fbd46e5776cab7b6ffb384f83f4ff/68747470733a2f2f636f6465636f762e696f2f67682f6d722d666174616c7973742f666173746f70656e6170692f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d55534852314930434a42" data-canonical-src="https://codecov.io/gh/mr-fatalyst/fastopenapi/branch/master/graph/badge.svg?token=USHR1I0CJB"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/59e6735443dc41ae21b13a0392b65b11928e6f239981db11985ca745ab0c23b9/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f666173746f70656e617069"><img src="https://camo.githubusercontent.com/59e6735443dc41ae21b13a0392b65b11928e6f239981db11985ca745ab0c23b9/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f666173746f70656e617069" data-canonical-src="https://img.shields.io/pypi/v/fastopenapi"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a8e0850dc525c2d40b3cf3ba469031461d1cc9f23f40f2d79f3df0c5d9674709/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f666173746f70656e617069"><img src="https://camo.githubusercontent.com/a8e0850dc525c2d40b3cf3ba469031461d1cc9f23f40f2d79f3df0c5d9674709/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f666173746f70656e617069" data-canonical-src="https://img.shields.io/pypi/pyversions/fastopenapi"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/37049c06136b234af5254472216d00bb66a6e2436a06246a0036e2ef25f75da2/68747470733a2f2f7374617469632e706570792e746563682f62616467652f666173746f70656e617069"><img src="https://camo.githubusercontent.com/37049c06136b234af5254472216d00bb66a6e2436a06246a0036e2ef25f75da2/68747470733a2f2f7374617469632e706570792e746563682f62616467652f666173746f70656e617069" alt="PyPI Downloads" data-canonical-src="https://static.pepy.tech/badge/fastopenapi"></a>
</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📦 Installation</h2><a id="user-content--installation" aria-label="Permalink: 📦 Installation" href="#-installation"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Install only FastOpenAPI:</h4><a id="user-content-install-only-fastopenapi" aria-label="Permalink: Install only FastOpenAPI:" href="#install-only-fastopenapi"></a></p>

<p dir="auto"><h4 tabindex="-1" dir="auto">Install FastOpenAPI with a specific framework:</h4><a id="user-content-install-fastopenapi-with-a-specific-framework" aria-label="Permalink: Install FastOpenAPI with a specific framework:" href="#install-fastopenapi-with-a-specific-framework"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install fastopenapi[falcon]"><pre>pip install fastopenapi[falcon]</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="pip install fastopenapi[flask]"><pre>pip install fastopenapi[flask]</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="pip install fastopenapi[sanic]"><pre>pip install fastopenapi[sanic]</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="pip install fastopenapi[starlette]"><pre>pip install fastopenapi[starlette]</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="pip install fastopenapi[tornado]"><pre>pip install fastopenapi[tornado]</pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🛠️ Quick Start</h2><a id="user-content-️-quick-start" aria-label="Permalink: 🛠️ Quick Start" href="#️-quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Step 1. Create an application</h3><a id="user-content-step-1-create-an-application" aria-label="Permalink: Step 1. Create an application" href="#step-1-create-an-application"></a></p>
<ul dir="auto">
<li>Create the <code>main.py</code> file</li>
<li>Copy the code from an example</li>
<li>For some examples uvicorn is required (<code>pip install uvicorn</code>)</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Examples:</h4><a id="user-content-examples" aria-label="Permalink: Examples:" href="#examples"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3d06f6d932d933ba71c0c56c68a06e5a89e2a2774b1e12ba41aa0f5358d7b320/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f46616c636f6e2d3435623864383f7374796c653d666c6174266c6f676f3d66616c636f6e266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/3d06f6d932d933ba71c0c56c68a06e5a89e2a2774b1e12ba41aa0f5358d7b320/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f46616c636f6e2d3435623864383f7374796c653d666c6174266c6f676f3d66616c636f6e266c6f676f436f6c6f723d7768697465" alt="Falcon" data-canonical-src="https://img.shields.io/badge/Falcon-45b8d8?style=flat&amp;logo=falcon&amp;logoColor=white"></a></p>
<details>
  <summary>Click to expand the Falcon Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="import falcon.asgi
import uvicorn
from pydantic import BaseModel

from fastopenapi.routers import FalconRouter

app = falcon.asgi.App()
router = FalconRouter(app=app)


class HelloResponse(BaseModel):
    message: str


@router.get(&quot;/hello&quot;, tags=[&quot;Hello&quot;], status_code=200, response_model=HelloResponse)
async def hello(name: str):
    &quot;&quot;&quot;Say hello from Falcon&quot;&quot;&quot;
    return HelloResponse(message=f&quot;Hello, {name}! It's Falcon!&quot;)


if __name__ == &quot;__main__&quot;:
    uvicorn.run(app, host=&quot;127.0.0.1&quot;, port=8000)"><pre><span>import</span> <span>falcon</span>.<span>asgi</span>
<span>import</span> <span>uvicorn</span>
<span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>

<span>from</span> <span>fastopenapi</span>.<span>routers</span> <span>import</span> <span>FalconRouter</span>

<span>app</span> <span>=</span> <span>falcon</span>.<span>asgi</span>.<span>App</span>()
<span>router</span> <span>=</span> <span>FalconRouter</span>(<span>app</span><span>=</span><span>app</span>)


<span>class</span> <span>HelloResponse</span>(<span>BaseModel</span>):
    <span>message</span>: <span>str</span>


<span>@<span>router</span>.<span>get</span>(<span>"/hello"</span>, <span>tags</span><span>=</span>[<span>"Hello"</span>], <span>status_code</span><span>=</span><span>200</span>, <span>response_model</span><span>=</span><span>HelloResponse</span>)</span>
<span>async</span> <span>def</span> <span>hello</span>(<span>name</span>: <span>str</span>):
    <span>"""Say hello from Falcon"""</span>
    <span>return</span> <span>HelloResponse</span>(<span>message</span><span>=</span><span>f"Hello, <span><span>{</span><span>name</span><span>}</span></span>! It's Falcon!"</span>)


<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>uvicorn</span>.<span>run</span>(<span>app</span>, <span>host</span><span>=</span><span>"127.0.0.1"</span>, <span>port</span><span>=</span><span>8000</span>)</pre></div>
</details>
</li>
<li>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/68c5bb6d17237e3c5de59740254e6ec055b6d781415ac1b8712bdb588660e266/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d466c61736b2d4545454545453f7374796c653d666c6174266c6f676f3d666c61736b266c6f676f436f6c6f723d626c61636b"><img src="https://camo.githubusercontent.com/68c5bb6d17237e3c5de59740254e6ec055b6d781415ac1b8712bdb588660e266/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d466c61736b2d4545454545453f7374796c653d666c6174266c6f676f3d666c61736b266c6f676f436f6c6f723d626c61636b" alt="Flask" data-canonical-src="https://img.shields.io/badge/-Flask-EEEEEE?style=flat&amp;logo=flask&amp;logoColor=black"></a></p>
<details>
  <summary>Click to expand the Flask Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="from flask import Flask
from pydantic import BaseModel

from fastopenapi.routers import FlaskRouter

app = Flask(__name__)
router = FlaskRouter(app=app)


class HelloResponse(BaseModel):
    message: str


@router.get(&quot;/hello&quot;, tags=[&quot;Hello&quot;], status_code=200, response_model=HelloResponse)
def hello(name: str):
    &quot;&quot;&quot;Say hello from Flask&quot;&quot;&quot;
    return HelloResponse(message=f&quot;Hello, {name}! It's Flask!&quot;)


if __name__ == &quot;__main__&quot;:
    app.run(port=8000)"><pre><span>from</span> <span>flask</span> <span>import</span> <span>Flask</span>
<span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>

<span>from</span> <span>fastopenapi</span>.<span>routers</span> <span>import</span> <span>FlaskRouter</span>

<span>app</span> <span>=</span> <span>Flask</span>(<span>__name__</span>)
<span>router</span> <span>=</span> <span>FlaskRouter</span>(<span>app</span><span>=</span><span>app</span>)


<span>class</span> <span>HelloResponse</span>(<span>BaseModel</span>):
    <span>message</span>: <span>str</span>


<span>@<span>router</span>.<span>get</span>(<span>"/hello"</span>, <span>tags</span><span>=</span>[<span>"Hello"</span>], <span>status_code</span><span>=</span><span>200</span>, <span>response_model</span><span>=</span><span>HelloResponse</span>)</span>
<span>def</span> <span>hello</span>(<span>name</span>: <span>str</span>):
    <span>"""Say hello from Flask"""</span>
    <span>return</span> <span>HelloResponse</span>(<span>message</span><span>=</span><span>f"Hello, <span><span>{</span><span>name</span><span>}</span></span>! It's Flask!"</span>)


<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>app</span>.<span>run</span>(<span>port</span><span>=</span><span>8000</span>)</pre></div>
</details>
</li>
<li>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/40014866e3e1b3170af04182fddb5675874cd752bf04329133b7092d971b68fc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d51756172742d3439393744303f7374796c653d666c6174266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/40014866e3e1b3170af04182fddb5675874cd752bf04329133b7092d971b68fc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d51756172742d3439393744303f7374796c653d666c6174266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465" alt="Quart" data-canonical-src="https://img.shields.io/badge/-Quart-4997D0?style=flat&amp;logo=python&amp;logoColor=white"></a></p>
<details>
  <summary>Click to expand the Quart Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="from pydantic import BaseModel
from quart import Quart

from fastopenapi.routers import QuartRouter

app = Quart(__name__)
router = QuartRouter(app=app)


class HelloResponse(BaseModel):
    message: str


@router.get(&quot;/hello&quot;, tags=[&quot;Hello&quot;], status_code=200, response_model=HelloResponse)
async def hello(name: str):
    &quot;&quot;&quot;Say hello from Quart&quot;&quot;&quot;
    return HelloResponse(message=f&quot;Hello, {name}! It's Quart!&quot;)


if __name__ == &quot;__main__&quot;:
    app.run(port=8000)"><pre><span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>
<span>from</span> <span>quart</span> <span>import</span> <span>Quart</span>

<span>from</span> <span>fastopenapi</span>.<span>routers</span> <span>import</span> <span>QuartRouter</span>

<span>app</span> <span>=</span> <span>Quart</span>(<span>__name__</span>)
<span>router</span> <span>=</span> <span>QuartRouter</span>(<span>app</span><span>=</span><span>app</span>)


<span>class</span> <span>HelloResponse</span>(<span>BaseModel</span>):
    <span>message</span>: <span>str</span>


<span>@<span>router</span>.<span>get</span>(<span>"/hello"</span>, <span>tags</span><span>=</span>[<span>"Hello"</span>], <span>status_code</span><span>=</span><span>200</span>, <span>response_model</span><span>=</span><span>HelloResponse</span>)</span>
<span>async</span> <span>def</span> <span>hello</span>(<span>name</span>: <span>str</span>):
    <span>"""Say hello from Quart"""</span>
    <span>return</span> <span>HelloResponse</span>(<span>message</span><span>=</span><span>f"Hello, <span><span>{</span><span>name</span><span>}</span></span>! It's Quart!"</span>)


<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>app</span>.<span>run</span>(<span>port</span><span>=</span><span>8000</span>)</pre></div>
</details>
</li>
<li>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/aa08690e4c157edae7675b8b72b3797aae7c461734c01d79f9f50dabda226d4f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d53616e69632d3030626666663f7374796c653d666c6174266c6f676f3d73616e6963266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/aa08690e4c157edae7675b8b72b3797aae7c461734c01d79f9f50dabda226d4f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d53616e69632d3030626666663f7374796c653d666c6174266c6f676f3d73616e6963266c6f676f436f6c6f723d7768697465" alt="Sanic" data-canonical-src="https://img.shields.io/badge/-Sanic-00bfff?style=flat&amp;logo=sanic&amp;logoColor=white"></a></p>
<details>
  <summary>Click to expand the Sanic Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="from pydantic import BaseModel
from sanic import Sanic

from fastopenapi.routers import SanicRouter

app = Sanic(&quot;MySanicApp&quot;)
router = SanicRouter(app=app)


class HelloResponse(BaseModel):
    message: str


@router.get(&quot;/hello&quot;, tags=[&quot;Hello&quot;], status_code=200, response_model=HelloResponse)
async def hello(name: str):
    &quot;&quot;&quot;Say hello from Sanic&quot;&quot;&quot;
    return HelloResponse(message=f&quot;Hello, {name}! It's Sanic!&quot;)


if __name__ == &quot;__main__&quot;:
    app.run(host=&quot;0.0.0.0&quot;, port=8000)"><pre><span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>
<span>from</span> <span>sanic</span> <span>import</span> <span>Sanic</span>

<span>from</span> <span>fastopenapi</span>.<span>routers</span> <span>import</span> <span>SanicRouter</span>

<span>app</span> <span>=</span> <span>Sanic</span>(<span>"MySanicApp"</span>)
<span>router</span> <span>=</span> <span>SanicRouter</span>(<span>app</span><span>=</span><span>app</span>)


<span>class</span> <span>HelloResponse</span>(<span>BaseModel</span>):
    <span>message</span>: <span>str</span>


<span>@<span>router</span>.<span>get</span>(<span>"/hello"</span>, <span>tags</span><span>=</span>[<span>"Hello"</span>], <span>status_code</span><span>=</span><span>200</span>, <span>response_model</span><span>=</span><span>HelloResponse</span>)</span>
<span>async</span> <span>def</span> <span>hello</span>(<span>name</span>: <span>str</span>):
    <span>"""Say hello from Sanic"""</span>
    <span>return</span> <span>HelloResponse</span>(<span>message</span><span>=</span><span>f"Hello, <span><span>{</span><span>name</span><span>}</span></span>! It's Sanic!"</span>)


<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>app</span>.<span>run</span>(<span>host</span><span>=</span><span>"0.0.0.0"</span>, <span>port</span><span>=</span><span>8000</span>)</pre></div>
</details>
</li>
<li>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7d050f9c2c3198ebf9345a40bb456e217b6c2184bfeae53de6d26ab02bcae5b7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d537461726c657474652d3442303038323f7374796c653d666c6174266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/7d050f9c2c3198ebf9345a40bb456e217b6c2184bfeae53de6d26ab02bcae5b7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d537461726c657474652d3442303038323f7374796c653d666c6174266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465" alt="Starlette" data-canonical-src="https://img.shields.io/badge/-Starlette-4B0082?style=flat&amp;logo=python&amp;logoColor=white"></a></p>
<details>
  <summary>Click to expand the Starlette Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="import uvicorn
from pydantic import BaseModel
from starlette.applications import Starlette

from fastopenapi.routers import StarletteRouter

app = Starlette()
router = StarletteRouter(app=app)


class HelloResponse(BaseModel):
    message: str


@router.get(&quot;/hello&quot;, tags=[&quot;Hello&quot;], status_code=200, response_model=HelloResponse)
async def hello(name: str):
    &quot;&quot;&quot;Say hello from Starlette&quot;&quot;&quot;
    return HelloResponse(message=f&quot;Hello, {name}! It's Starlette!&quot;)

if __name__ == &quot;__main__&quot;:
    uvicorn.run(app, host=&quot;127.0.0.1&quot;, port=8000)"><pre><span>import</span> <span>uvicorn</span>
<span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>
<span>from</span> <span>starlette</span>.<span>applications</span> <span>import</span> <span>Starlette</span>

<span>from</span> <span>fastopenapi</span>.<span>routers</span> <span>import</span> <span>StarletteRouter</span>

<span>app</span> <span>=</span> <span>Starlette</span>()
<span>router</span> <span>=</span> <span>StarletteRouter</span>(<span>app</span><span>=</span><span>app</span>)


<span>class</span> <span>HelloResponse</span>(<span>BaseModel</span>):
    <span>message</span>: <span>str</span>


<span>@<span>router</span>.<span>get</span>(<span>"/hello"</span>, <span>tags</span><span>=</span>[<span>"Hello"</span>], <span>status_code</span><span>=</span><span>200</span>, <span>response_model</span><span>=</span><span>HelloResponse</span>)</span>
<span>async</span> <span>def</span> <span>hello</span>(<span>name</span>: <span>str</span>):
    <span>"""Say hello from Starlette"""</span>
    <span>return</span> <span>HelloResponse</span>(<span>message</span><span>=</span><span>f"Hello, <span><span>{</span><span>name</span><span>}</span></span>! It's Starlette!"</span>)

<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>uvicorn</span>.<span>run</span>(<span>app</span>, <span>host</span><span>=</span><span>"127.0.0.1"</span>, <span>port</span><span>=</span><span>8000</span>)</pre></div>
</details>
</li>
<li>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e6db5c55534d9a6d47361d7ce0d82d1141ab481e3bf08d5c106f0be85fe0c94d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d546f726e61646f2d3239383042393f7374796c653d666c6174266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/e6db5c55534d9a6d47361d7ce0d82d1141ab481e3bf08d5c106f0be85fe0c94d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d546f726e61646f2d3239383042393f7374796c653d666c6174266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465" alt="Tornado" data-canonical-src="https://img.shields.io/badge/-Tornado-2980B9?style=flat&amp;logo=python&amp;logoColor=white"></a></p>
<details>
  <summary>Click to expand the Tornado Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio

from pydantic import BaseModel
from tornado.web import Application

from fastopenapi.routers.tornado import TornadoRouter

app = Application()

router = TornadoRouter(app=app)


class HelloResponse(BaseModel):
    message: str


@router.get(&quot;/hello&quot;, tags=[&quot;Hello&quot;], status_code=200, response_model=HelloResponse)
def hello(name: str):
    &quot;&quot;&quot;Say hello from Tornado&quot;&quot;&quot;
    return HelloResponse(message=f&quot;Hello, {name}! It's Tornado!&quot;)


async def main():
    app.listen(8000)
    await asyncio.Event().wait()


if __name__ == &quot;__main__&quot;:
    asyncio.run(main())"><pre><span>import</span> <span>asyncio</span>

<span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>
<span>from</span> <span>tornado</span>.<span>web</span> <span>import</span> <span>Application</span>

<span>from</span> <span>fastopenapi</span>.<span>routers</span>.<span>tornado</span> <span>import</span> <span>TornadoRouter</span>

<span>app</span> <span>=</span> <span>Application</span>()

<span>router</span> <span>=</span> <span>TornadoRouter</span>(<span>app</span><span>=</span><span>app</span>)


<span>class</span> <span>HelloResponse</span>(<span>BaseModel</span>):
    <span>message</span>: <span>str</span>


<span>@<span>router</span>.<span>get</span>(<span>"/hello"</span>, <span>tags</span><span>=</span>[<span>"Hello"</span>], <span>status_code</span><span>=</span><span>200</span>, <span>response_model</span><span>=</span><span>HelloResponse</span>)</span>
<span>def</span> <span>hello</span>(<span>name</span>: <span>str</span>):
    <span>"""Say hello from Tornado"""</span>
    <span>return</span> <span>HelloResponse</span>(<span>message</span><span>=</span><span>f"Hello, <span><span>{</span><span>name</span><span>}</span></span>! It's Tornado!"</span>)


<span>async</span> <span>def</span> <span>main</span>():
    <span>app</span>.<span>listen</span>(<span>8000</span>)
    <span>await</span> <span>asyncio</span>.<span>Event</span>().<span>wait</span>()


<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>
</details>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Step 2. Run the server</h3><a id="user-content-step-2-run-the-server" aria-label="Permalink: Step 2. Run the server" href="#step-2-run-the-server"></a></p>
<p dir="auto">Launch the application:</p>

<p dir="auto">Once launched, the documentation will be available at:</p>
<p dir="auto">Swagger UI:</p>
<div data-snippet-clipboard-copy-content="http://127.0.0.1:8000/docs"><pre><code>http://127.0.0.1:8000/docs
</code></pre></div>
<p dir="auto">ReDoc UI:</p>
<div data-snippet-clipboard-copy-content="http://127.0.0.1:8000/redoc"><pre><code>http://127.0.0.1:8000/redoc
</code></pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">⚙️ Features</h2><a id="user-content-️-features" aria-label="Permalink: ⚙️ Features" href="#️-features"></a></p>
<ul dir="auto">
<li><strong>Generate OpenAPI schemas</strong> with Pydantic v2.</li>
<li><strong>Data validation</strong> using Pydantic models.</li>
<li><strong>Supports multiple frameworks:</strong> Falcon, Flask, Quart, Sanic, Starlette, Tornado.</li>
<li><strong>Proxy routing provides FastAPI-style routing</strong></li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📖 Documentation</h2><a id="user-content--documentation" aria-label="Permalink: 📖 Documentation" href="#-documentation"></a></p>
<p dir="auto">Explore the <a href="https://github.com/mr-fatalyst/fastopenapi/blob/master/docs/en/index.md">Docs</a> for an overview of FastOpenAPI, its core components, and usage guidelines. The documentation is continuously updated and improved.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📂 Advanced Examples</h2><a id="user-content--advanced-examples" aria-label="Permalink: 📂 Advanced Examples" href="#-advanced-examples"></a></p>
<p dir="auto">Examples of integration and detailed usage for each framework are available in the <a href="https://github.com/mr-fatalyst/fastopenapi/tree/master/examples"><code>examples</code></a> directory.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📊 Quick &amp; Dirty Benchmarks</h2><a id="user-content--quick--dirty-benchmarks" aria-label="Permalink: 📊 Quick &amp; Dirty Benchmarks" href="#-quick--dirty-benchmarks"></a></p>
<p dir="auto">Fast but not perfect benchmarks. Check the <a href="https://github.com/mr-fatalyst/fastopenapi/tree/master/benchmarks"><code>benchmarks</code></a> directory for details.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">✅ Development Recommendations</h2><a id="user-content--development-recommendations" aria-label="Permalink: ✅ Development Recommendations" href="#-development-recommendations"></a></p>
<ul dir="auto">
<li>Use Pydantic models for strict typing and data validation.</li>
<li>Follow the project structure similar to provided examples for easy scalability.</li>
<li>Regularly update dependencies and monitor library updates for new features.</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🛠️ Contributing</h2><a id="user-content-️-contributing" aria-label="Permalink: 🛠️ Contributing" href="#️-contributing"></a></p>
<p dir="auto">If you have suggestions or find a bug, please open an issue or create a pull request on GitHub.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📄 <strong>License</strong></h2><a id="user-content--license" aria-label="Permalink: 📄 License" href="#-license"></a></p>
<p dir="auto">This project is licensed under the terms of the MIT license.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Landrun: Sandbox any Linux process using Landlock, no root or containers (380 pts)]]></title>
            <link>https://github.com/Zouuup/landrun</link>
            <guid>43445662</guid>
            <pubDate>Sat, 22 Mar 2025 13:56:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Zouuup/landrun">https://github.com/Zouuup/landrun</a>, See on <a href="https://news.ycombinator.com/item?id=43445662">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">landrun</h2><a id="user-content-landrun" aria-label="Permalink: landrun" href="#landrun"></a></p>
<p dir="auto">A lightweight, secure sandbox for running Linux processes using Landlock LSM. Think firejail, but with kernel-level security and minimal overhead.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>🔒 Kernel-level security using Landlock LSM</li>
<li>🚀 Lightweight and fast execution</li>
<li>🛡️ Fine-grained access control for directories</li>
<li>🔄 Support for read and write paths</li>
<li>⚡ Optional execution permissions for allowed paths</li>
<li>🌐 TCP network access control (binding and connecting)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/Zouuup/landrun/blob/main/demo.gif"><img src="https://github.com/Zouuup/landrun/raw/main/demo.gif" alt="landrun demo" width="700" data-animated-image=""></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Linux kernel 5.13 or later with Landlock LSM enabled</li>
<li>Linux kernel 6.8 or later for network restrictions (TCP bind/connect)</li>
<li>Go 1.18 or later (for building from source)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quick Install</h3><a id="user-content-quick-install" aria-label="Permalink: Quick Install" href="#quick-install"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="go install github.com/zouuup/landrun/cmd/landrun@latest"><pre>go install github.com/zouuup/landrun/cmd/landrun@latest</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">From Source</h3><a id="user-content-from-source" aria-label="Permalink: From Source" href="#from-source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/zouuup/landrun.git
cd landrun
go build -o landrun cmd/landrun/main.go
sudo cp landrun /usr/local/bin/"><pre>git clone https://github.com/zouuup/landrun.git
<span>cd</span> landrun
go build -o landrun cmd/landrun/main.go
sudo cp landrun /usr/local/bin/</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Basic syntax:</p>
<div dir="auto" data-snippet-clipboard-copy-content="landrun [options] <command> [args...]"><pre>landrun [options] <span>&lt;</span>command<span>&gt;</span> [args...]</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Options</h3><a id="user-content-options" aria-label="Permalink: Options" href="#options"></a></p>
<ul dir="auto">
<li><code>--ro &lt;path&gt;</code>: Allow read-only access to specified path (can be specified multiple times)</li>
<li><code>--rw &lt;path&gt;</code>: Allow read-write access to specified path (can be specified multiple times)</li>
<li><code>--exec</code>: Allow executing files in allowed paths</li>
<li><code>--bind-tcp &lt;port&gt;</code>: Allow binding to specified TCP port (can be specified multiple times)</li>
<li><code>--connect-tcp &lt;port&gt;</code>: Allow connecting to specified TCP port (can be specified multiple times)</li>
<li><code>--best-effort</code>: Use best effort mode, falling back to less restrictive sandbox if necessary [default: enabled]</li>
<li><code>--log-level &lt;level&gt;</code>: Set logging level (error, info, debug) [default: "error"]</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Important Notes</h3><a id="user-content-important-notes" aria-label="Permalink: Important Notes" href="#important-notes"></a></p>
<ul dir="auto">
<li>You must explicitly add the path to the command you want to run with the <code>--ro</code> flag</li>
<li>For system commands, you typically need to include <code>/usr/bin</code>, <code>/usr/lib</code>, and other system directories</li>
<li>When using <code>--exec</code>, you still need to specify the directories containing executables with <code>--ro</code></li>
<li>Network restrictions require Linux kernel 6.8 or later with Landlock ABI v5</li>
<li>The <code>--best-effort</code> flag allows graceful degradation on older kernels that don't support all requested restrictions</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Environment Variables</h3><a id="user-content-environment-variables" aria-label="Permalink: Environment Variables" href="#environment-variables"></a></p>
<ul dir="auto">
<li><code>LANDRUN_LOG_LEVEL</code>: Set logging level (error, info, debug)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<ol dir="auto">
<li>Run a command with read-only access to a directory:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /path/to/dir ls /path/to/dir"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /path/to/dir ls /path/to/dir</pre></div>
<ol start="2" dir="auto">
<li>Run a command with write access to a directory:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --rw /path/to/dir touch /path/to/dir/newfile"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --rw /path/to/dir touch /path/to/dir/newfile</pre></div>
<ol start="3" dir="auto">
<li>Run a command with execution permissions:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --exec /usr/bin/bash"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --exec /usr/bin/bash</pre></div>
<ol start="4" dir="auto">
<li>Run with debug logging:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --log-level debug --ro /usr/bin --ro /lib --ro /lib64 --ro /path/to/dir ls"><pre>landrun --log-level debug --ro /usr/bin --ro /lib --ro /lib64 --ro /path/to/dir ls</pre></div>
<ol start="5" dir="auto">
<li>Run with network restrictions:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --bind-tcp 8080 --connect-tcp 53 /usr/bin/my-server"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --bind-tcp 8080 --connect-tcp 53 /usr/bin/my-server</pre></div>
<p dir="auto">This will allow the program to only bind to TCP port 8080 and connect to TCP port 53.</p>
<ol start="6" dir="auto">
<li>Run a DNS client with appropriate permissions:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /etc/resolv.conf --connect-tcp 53 dig example.com"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /etc/resolv.conf --connect-tcp 53 dig example.com</pre></div>
<p dir="auto">This allows DNS resolution by granting access to /etc/resolv.conf and permitting connections to port 53 (DNS).</p>
<ol start="7" dir="auto">
<li>Run a web server with selective network permissions:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /var/www --rw /var/log --bind-tcp 80 --bind-tcp 443 /usr/bin/nginx"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /var/www --rw /var/log --bind-tcp 80 --bind-tcp 443 /usr/bin/nginx</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security</h2><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<p dir="auto">landrun uses Linux's Landlock LSM to create a secure sandbox environment. It provides:</p>
<ul dir="auto">
<li>File system access control</li>
<li>Directory access restrictions</li>
<li>Execution control</li>
<li>TCP network restrictions</li>
<li>Process isolation</li>
</ul>
<p dir="auto">Landlock is an access-control system that enables processes to securely restrict themselves and their future children. As a stackable Linux Security Module (LSM), it creates additional security layers on top of existing system-wide access controls, helping to mitigate security impacts from bugs or malicious behavior in applications.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Landlock Access Control Rights</h3><a id="user-content-landlock-access-control-rights" aria-label="Permalink: Landlock Access Control Rights" href="#landlock-access-control-rights"></a></p>
<p dir="auto">landrun leverages Landlock's fine-grained access control mechanisms, which include:</p>
<p dir="auto"><strong>File-specific rights:</strong></p>
<ul dir="auto">
<li>Execute files (<code>LANDLOCK_ACCESS_FS_EXECUTE</code>)</li>
<li>Write to files (<code>LANDLOCK_ACCESS_FS_WRITE_FILE</code>)</li>
<li>Read files (<code>LANDLOCK_ACCESS_FS_READ_FILE</code>)</li>
<li>Truncate files (<code>LANDLOCK_ACCESS_FS_TRUNCATE</code>) - Available since Landlock ABI v3</li>
</ul>
<p dir="auto"><strong>Directory-specific rights:</strong></p>
<ul dir="auto">
<li>Read directory contents (<code>LANDLOCK_ACCESS_FS_READ_DIR</code>)</li>
<li>Remove directories (<code>LANDLOCK_ACCESS_FS_REMOVE_DIR</code>)</li>
<li>Remove files (<code>LANDLOCK_ACCESS_FS_REMOVE_FILE</code>)</li>
<li>Create various filesystem objects (char devices, directories, regular files, sockets, etc.)</li>
<li>Refer/reparent files across directories (<code>LANDLOCK_ACCESS_FS_REFER</code>) - Available since Landlock ABI v2</li>
</ul>
<p dir="auto"><strong>Network-specific rights</strong> (requires Linux 6.8+ with Landlock ABI v5):</p>
<ul dir="auto">
<li>Bind to specific TCP ports (<code>LANDLOCK_ACCESS_NET_BIND_TCP</code>)</li>
<li>Connect to specific TCP ports (<code>LANDLOCK_ACCESS_NET_CONNECT_TCP</code>)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Limitations</h3><a id="user-content-limitations" aria-label="Permalink: Limitations" href="#limitations"></a></p>
<ul dir="auto">
<li>Landlock must be supported by your kernel</li>
<li>Network restrictions require Linux kernel 6.8+ with Landlock ABI v5</li>
<li>Some operations may require additional permissions</li>
<li>Files or directories opened before sandboxing are not subject to Landlock restrictions</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Kernel Compatibility Table</h2><a id="user-content-kernel-compatibility-table" aria-label="Permalink: Kernel Compatibility Table" href="#kernel-compatibility-table"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Feature</th>
<th>Minimum Kernel Version</th>
<th>Landlock ABI Version</th>
</tr>
</thead>
<tbody>
<tr>
<td>Basic filesystem sandboxing</td>
<td>5.13</td>
<td>1</td>
</tr>
<tr>
<td>File referring/reparenting control</td>
<td>5.17</td>
<td>2</td>
</tr>
<tr>
<td>File truncation control</td>
<td>6.1</td>
<td>3</td>
</tr>
<tr>
<td>Network TCP restrictions</td>
<td>6.8</td>
<td>5</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto">If you receive "permission denied" or similar errors:</p>
<ol dir="auto">
<li>Ensure you've added all necessary paths with <code>--ro</code> or <code>--rw</code></li>
<li>Try running with <code>--log-level debug</code> to see detailed permission information</li>
<li>Check that Landlock is supported and enabled on your system:
<div dir="auto" data-snippet-clipboard-copy-content="grep -E 'landlock|lsm=' /boot/config-$(uname -r)"><pre>grep -E <span><span>'</span>landlock|lsm=<span>'</span></span> /boot/config-<span><span>$(</span>uname -r<span>)</span></span></pre></div>
You should see <code>CONFIG_SECURITY_LANDLOCK=y</code> and <code>lsm=landlock,...</code> in the output</li>
<li>For network restrictions, verify your kernel version is 6.8+ with Landlock ABI v5:

</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technical Details</h2><a id="user-content-technical-details" aria-label="Permalink: Technical Details" href="#technical-details"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Implementation</h3><a id="user-content-implementation" aria-label="Permalink: Implementation" href="#implementation"></a></p>
<p dir="auto">This project uses the <code>landlock-lsm/go-landlock</code> package for sandboxing, which provides both filesystem and network restrictions. The current implementation supports:</p>
<ul dir="auto">
<li>Read/write/execute restrictions for files and directories</li>
<li>TCP port binding restrictions</li>
<li>TCP port connection restrictions</li>
<li>Best-effort mode for graceful degradation on older kernels</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Best-Effort Mode</h3><a id="user-content-best-effort-mode" aria-label="Permalink: Best-Effort Mode" href="#best-effort-mode"></a></p>
<p dir="auto">When using <code>--best-effort</code> (enabled by default), landrun will gracefully degrade to using the best available Landlock version on the current kernel. This means:</p>
<ul dir="auto">
<li>On Linux 6.8+: Full filesystem and network restrictions</li>
<li>On Linux 6.1-6.7: Filesystem restrictions including truncation, but no network restrictions</li>
<li>On Linux 5.17-6.0: Basic filesystem restrictions including file reparenting, but no truncation control or network restrictions</li>
<li>On Linux 5.13-5.16: Basic filesystem restrictions without file reparenting, truncation control, or network restrictions</li>
<li>On older Linux: No restrictions (sandbox disabled)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Future Features</h2><a id="user-content-future-features" aria-label="Permalink: Future Features" href="#future-features"></a></p>
<p dir="auto">Based on the Linux Landlock API capabilities, we plan to add:</p>
<ul dir="auto">
<li>🔒 Enhanced filesystem controls with more fine-grained permissions</li>
<li>🌐 Support for UDP and other network protocol restrictions (when supported by Linux kernel)</li>
<li>🔄 Process scoping and resource controls</li>
<li>🛡️ Additional security features as they become available in the Landlock API</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the GNU General Public License v2</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Please feel free to submit a Pull Request.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Differential Geometry: A First Course in Curves and Surfaces [pdf] (145 pts)]]></title>
            <link>https://math.franklin.uga.edu/sites/default/files/users/user317/ShifrinDiffGeo.pdf</link>
            <guid>43445614</guid>
            <pubDate>Sat, 22 Mar 2025 13:46:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://math.franklin.uga.edu/sites/default/files/users/user317/ShifrinDiffGeo.pdf">https://math.franklin.uga.edu/sites/default/files/users/user317/ShifrinDiffGeo.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=43445614">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
    </channel>
</rss>