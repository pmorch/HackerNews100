<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 25 Mar 2025 19:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[4o Image Generation (154 pts)]]></title>
            <link>https://openai.com/index/introducing-4o-image-generation/</link>
            <guid>43474112</guid>
            <pubDate>Tue, 25 Mar 2025 18:06:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/introducing-4o-image-generation/">https://openai.com/index/introducing-4o-image-generation/</a>, See on <a href="https://news.ycombinator.com/item?id=43474112">Hacker News</a></p>
Couldn't get https://openai.com/index/introducing-4o-image-generation/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Stoop Coffee: How a Simple Idea Transformed My Neighborhood (296 pts)]]></title>
            <link>https://supernuclear.substack.com/p/stoop-coffee-how-a-simple-idea-transformed</link>
            <guid>43473618</guid>
            <pubDate>Tue, 25 Mar 2025 17:16:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://supernuclear.substack.com/p/stoop-coffee-how-a-simple-idea-transformed">https://supernuclear.substack.com/p/stoop-coffee-how-a-simple-idea-transformed</a>, See on <a href="https://news.ycombinator.com/item?id=43473618">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em>Editor’s note: This is a guest post from Patty Smith. Patty and her husband enjoy the kind of neighborhood so many of us would like - connected, helpful, fun - but it wasn’t that way two years ago. A simple tradition changed their neighborhood and is a good reminder of how small, consistent actions can have outsize results. It also shows you don’t have to share a kitchen or a roof to live in community. </em></p><p>18 months ago, I wasn’t planning on spending more time hanging out with my neighbors than with friends I’d known for decades. It started with a simple goal: my husband Tyler and I wanted that sense of community that feels like it’s only possible in the suburbs, but we believed we could achieve this while living in San Francisco. We brainstormed: should we make cookies and knock on doors? Should we invite neighbors over for dinner? Ultimately, we landed on sipping coffee on our “stoop”.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg" width="1024" height="768" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67eb8c21-aa8c-4ef1-94b7-60434d27763e_1024x768.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><em>Tyler (left) and Patty (me! right) on one of our first stoop coffees</em></figcaption></figure></div><p>Hanging out on a stoop is not a novel concept. Unfortunately, an increasing trend of isolation has resulted in fewer and fewer neighbors gathering to connect with one another. Stooping has provided benefits to so many communities. Why not bring this concept to my own neighborhood?</p><p>Tyler and I were already having leisurely weekend morning coffees in our house, so it was an easy pivot to sit outside with our coffees and enjoy the sunshine. And thus our tradition began. Every weekend, we would bring our folding chairs out onto the street – we had to make do since our house doesn’t have a stoop – and enjoy our caffeine. As we saw people entering or exiting their homes, we'd enthusiastically wave them down, introduce ourselves, and write down their names in our shared spreadsheet. I wore a goofy tie-dyed Six Flags hat so people would remember us as “those people” and we started calling this our brand awareness campaign (but of course, we live in SF).</p><p>We met Luke a month or two after we’d been “stooping” on a regular basis. He came by to introduce himself and asked to exchange numbers so we could let him know if we’d be out there in the future, he’d love to join. At the time we didn’t realize how important this moment was for us. We’d been meeting many neighbors in passing but Luke was the first person to offer to sit with us and he wanted to know how to coordinate. In retrospect we should have been trying to get peoples’ numbers all along but hey, we were new to this!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg" width="1024" height="768" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:768,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd83466aa-47d3-4aef-94f9-9e2110540c40_1024x768.jpeg 1456w" sizes="100vw"></picture></div></a><figcaption><em>A typical weekend stoop hang</em></figcaption></figure></div><p><span>As soon as Luke started coming to stoop we actually started to resemble a group. It was validating to see a few neighbors getting together and this quickly attracted more! We learned to bring extra folding chairs for people who wanted to drop in for “just a minute” (or ninety) and Luke started bringing homebrew coffee to share. After a while, we realized it was starting to become unwieldy texting everyone when we were going to be outside. Thus, the WhatsApp group was born. At first this was just a place to announce when we’d be out having stoop coffee, but we soon realized people wanted to connect over more things than just coffee. So we ended up converting the group into a </span><a href="https://www.cooby.co/en/post/whatsapp-communities" rel="">WhatsApp Community</a><span> where we could have chats dedicated to certain topics or groups and plan other types of events together. Things were starting to get fun!</span></p><p>The first larger event that our “stoopers” wanted to host together was a block party that soon got scoped down to a pancake party. We made a spreadsheet, assigned tasks, and acquired obscene amounts of pancake mix. We decided to host the party on the sidewalk in front of our neighbor’s garage to keep things easy and so we didn’t have to apply for permits. Gathering tables, chairs and an electric griddle was quick work with so many neighbors invested in making the party a success. The most important thing we did in preparation for this party was to print out 100 door drops to deliver to the nearest set of neighbors and post party fliers up on telephone poles. It was old school but it worked! Most of the new faces we saw were people who found out about the event through our paper invitations.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png" width="1172" height="658" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:658,&quot;width&quot;:1172,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47fa6693-47e9-4bc5-b777-f94c5c484689_1172x658.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The Pancake Party in full swing</figcaption></figure></div><p>The event was a resounding success. 70+ people came by, and we added over 50 new numbers to our WhatsApp Community. From that point forward, each stoop coffee started seeing at least 10-15 faces and new people were volunteering to host. A few neighbor gatherings later – including an epic “Dipsgiving” sidewalk potluck where everyone brought a dip to share – the momentum has continued and we now have multiple events every week. During the span of a recent week in December, we had a neighborhood trash pickup, a cookie swap, a TV show watch party, a parent hang at a neighborhood brewery, and—of course—a stoop coffee.</p><p>As I write this post, I realize that the “we” from earlier in the post has grown from just Tyler and me to a “we” that represents many more people who are invested in our community. It feels like our neighborhood is thriving. The in-person gatherings continue even without much intervention from the original few who got the ball rolling. The daily chatter on our WhatsApp is so gosh darn wholesome. Someone even sold a car in our ‘classifieds’ chat! Tyler and I have made many new friends in the neighborhood, and our neighbors who never knew each other before are becoming friends with each other. Our neighborhood community is now a group of people that we rely on and who rely on us for emotional support, last-minute childcare, home-cooked meals, general comradery, and much more. The best part is that I can tell we are still early in our growth, there are still many people to meet, and I feel a palpable sense of awe when I learn about a new skill or talent that exists right next door.</p><ul><li><p><strong>Keeping it simple</strong><span>: we’ve realized that some of our best events require the smallest amount of effort. To avoid burnout, we’ve intentionally kept our community building as low-lift as possible, which has the added benefit of creating space for other people to step up.</span></p></li><li><p><strong>Broadening vs. deepening</strong><span>: we bucket our events into “broadening” events which have the purpose of meeting new neighbors and “deepening” events which allow us to get to know our existing neighbors better. Being aware of that classification has helped us be strategic depending on what feels needed for the time and season.</span></p></li><li><p><strong>Seasonal events</strong><span>: Naturally, the colder months have become a better time for deepening events that often occur in someone’s home (e.g. TV show watch parties, cookie swaps, potlucks), while warmer events are better for getting together outside and broadening our community (e.g. sidewalk chalk murals, pancake parties, bonus evening stoop beers).</span></p></li><li><p><strong>The street as a third space</strong><span>: most of our stoop coffees are held in the street in front of someone’s driveway. This has the benefit of being visible and inviting to other neighbors, while making use of a previously underutilized space. It’s also got us thinking of other unused spaces that we can turn into community-gathering spots, such as turning a nearby parking spot into a parklet or a transit stop into a community gathering space.</span></p></li><li><p><strong>Relying on the community:</strong><span> It can often feel overwhelming to take on planning a big event. We started using the phrase “the universe provides” because the real magic is found in asking and giving freely within the community. It’s a daily treat to see neighbors stepping up for one another in unexpected ways now that more of us are connected!</span></p></li></ul><p>Our biggest goal in the coming year is to help more people organize in-person events and build towards a future where the community is sustainable even if we ever (god forbid!) move away. We’ve also been looking to connect with local businesses and influence policy matters that impact our local footprint. A few of us met recently to ideate on how we can keep vibrant commerce happening at our local businesses and how to best connect with the city decision-makers responsible for issues that impact our neighborhood community. To inform where we spend our energy, we’ve been starting with the issues that folks in the community care about and want to change (yes, we asked people at a recent block party!). We are also trying to find quick wins in collaborating with the city government to show that our voices can be heard and have a positive impact.</p><p>I cherish the neighbors I’ve met and am so grateful for the many people who put time and effort into building our community (Luke and Tyler, in particular, deserve a special shoutout). As we continue to grow, I’m excited to keep learning from others – please reach out if you want to brainstorm about neighborhood community building!</p><p><em><span>Thanks Patty for sharing your story! For more inspiration on how to do things like this in your neighborhood, please also see Savannah’s post on </span><a href="https://supernuclear.substack.com/p/building-neighborhood-communities" rel="">Building Neighborhood Communities</a><span>.</span></em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini 2.5: Our most intelligent AI model (301 pts)]]></title>
            <link>https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/</link>
            <guid>43473489</guid>
            <pubDate>Tue, 25 Mar 2025 17:01:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/">https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=43473489">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="jump-content" tabindex="-1">
            

    
    

    <article>

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Gemini 2.5: Our most intelligent AI model&quot;
  }">
      <div>
          
            <p>Mar 25, 2025</p>
          
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
      
        <p>
          Gemini 2.5 is a thinking model, designed to tackle increasingly complex problems. Our first 2.5 model, Gemini 2.5 Pro Experimental, leads common benchmarks by meaningful margins and showcases strong reasoning and code capabilities.
        </p>
      
    </div>

    

    
      










<div>
    <figure>
      <div>
        <p><img alt="Five glowing blue rectangles, decreasing in size, angled diagonally across a dark background, suggesting depth and layers." data-component="uni-progressive-image" fetchpriority="high" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_keyword_header_no_text.width-200.format-webp.webp" width="360px" data-sizes="(max-width: 1023px) 100vw,(min-width: 1024px and max-width: 1259) 80vw, 1046px" data-srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_keyword_header_no_text.width-800.format-webp.webp 800w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_keyword_header_no_text.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_keyword_header_no_text.width-1600.format-webp.webp 1600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_keyword_header_no_text.width-2200.format-webp.webp 2200w">
        </p>
      </div>
      
    </figure>
  </div>






    

    
    <div>
        
          
            <div data-component="uni-article-jumplinks" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Jumplinks&quot;,
    &quot;section_header&quot;: &quot;Gemini 2.5: Our most intelligent AI model&quot;
  }">
  <nav aria-label="Article Jumplinks">
    <p><span>In this story</span>
    </p>
    
    
    <div>
      <ul id="article-jumplinks__list">
        
        <li>
          <a aria-label="link to Introducing Gemini 2.5" href="#gemini-2-5-thinking" id="gemini-2-5-thinking-anchor">Introducing Gemini 2.5</a>
        </li>
        
        <li>
          <a aria-label="link to Gemini 2.5 Pro" href="#gemini-2-5-pro" id="gemini-2-5-pro-anchor">Gemini 2.5 Pro</a>
        </li>
        
        <li>
          <a aria-label="link to Enhanced reasoning" href="#enhanced-reasoning" id="enhanced-reasoning-anchor">Enhanced reasoning</a>
        </li>
        
        <li>
          <a aria-label="link to Advanced coding" href="#advanced-coding" id="advanced-coding-anchor">Advanced coding</a>
        </li>
        
        <li>
          <a aria-label="link to The best of Gemini" href="#building-on-best-gemini" id="building-on-best-gemini-anchor">The best of Gemini</a>
        </li>
        
      </ul>
    </div>
    
  </nav>
</div>
          
          
          <div data-reading-time="true" data-component="uni-article-body">

            
              





<uni-article-speakable page-title="Gemini 2.5: Our most intelligent AI model" listen-to-article="Listen to article" data-date-modified="2025-03-25T17:00:17.781204+00:00" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-script-src="https://www.gstatic.com/readaloud/player/web/api/js/api.js"></uni-article-speakable>

            

            
            
<!--article text-->

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Gemini 2.5: Our most intelligent AI model&quot;
         }"><p data-block-key="rwvoz">Today we’re introducing Gemini 2.5, our most intelligent AI model. Our first 2.5 release is an experimental version of 2.5 Pro, which is state-of-the-art on a wide range of benchmarks and debuts at #1 on <a href="https://lmarena.ai/?leaderboard">LMArena</a> by a significant margin.</p><p data-block-key="4r4db"><a href="https://deepmind.google/technologies/gemini">Gemini 2.5 models</a> are thinking models, capable of reasoning through their thoughts before responding, resulting in enhanced performance and improved accuracy.</p><p data-block-key="f0oom">In the field of AI, a system’s capacity for “reasoning” refers to more than just classification and prediction. It refers to its ability to analyze information, draw logical conclusions, incorporate context and nuance, and make informed decisions.</p><p data-block-key="2bv7r">For a long time, we’ve explored ways of making AI smarter and more capable of reasoning through techniques like <a href="https://www.nature.com/articles/nature16961">reinforcement learning</a> and <a href="https://arxiv.org/abs/2201.11903">chain-of-thought prompting</a>. Building on this, we recently introduced our first thinking model, <a href="https://deepmind.google/technologies/gemini/flash-thinking/">Gemini 2.0 Flash Thinking</a>.</p><p data-block-key="b6ali">Now, with Gemini 2.5, we've achieved a new level of performance by combining a significantly enhanced base model with improved post-training. Going forward, we’re building these thinking capabilities directly into all of our models, so they can handle more complex problems and support even more capable, context-aware agents.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Gemini 2.5: Our most intelligent AI model&quot;
         }"><h2 data-block-key="w1q20">Introducing Gemini 2.5 Pro</h2><p data-block-key="fapl9">Gemini 2.5 Pro Experimental is our most advanced model for complex tasks. It tops the <a href="https://lmarena.ai/?leaderboard">LMArena</a> leaderboard — which measures human preferences — by a significant margin, indicating a highly capable model equipped with high-quality style. 2.5 Pro also shows strong reasoning and code capabilities, leading on common coding, math and science benchmarks.</p><p data-block-key="5tlko">Gemini 2.5 Pro is available now in <a href="http://aistudio.google.com/app/prompts/new_chat?model=gemini-2.5-pro-exp-03-25">Google AI Studio</a> and in the <a href="https://gemini.google.com/">Gemini app</a> for Gemini Advanced users, and will be coming to <a href="https://console.cloud.google.com/freetrial?redirectPath=/vertex-ai/studio">Vertex AI</a> soon. We’ll also introduce pricing in the coming weeks, enabling people to use 2.5 Pro with higher rate limits for scaled production use.</p></div>
  

  
    






<uni-image-full-width alignment="full" alt-text="Detailed table displays performance of multiple large language models on tests like math, coding, and reasoning. Gemini 2.5 Pro shows top results in several categories, indicated by highlighted cells. Fine print at the bottom provides context for the data." external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="Gemini 2.5: Our most intelligent AI model" external-link="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini_benchmarks_cropped_light2x.original.png" custom-class="image-full-width--constrained-width uni-component-spacing">
  
  
    <p><img alt="Detailed table displays performance of multiple large language models on tests like math, coding, and reasoning. Gemini 2.5 Pro shows top results in several categories, indicated by highlighted cells. Fine print at the bottom provides context for the data." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_benchmarks_cropped_light2x.gif">
    </p>
  
</uni-image-full-width>


  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Gemini 2.5: Our most intelligent AI model&quot;
         }"><h2 data-block-key="rwvoz">Enhanced reasoning</h2><p data-block-key="def61">Gemini 2.5 Pro is state-of-the-art across a range of benchmarks requiring advanced reasoning. Without test-time techniques that increase cost, like majority voting, 2.5 Pro leads in math and science benchmarks like GPQA and AIME 2025.</p><p data-block-key="7n9sj">It also scores a state-of-the-art 18.8% across models without tool use on Humanity’s Last Exam, a dataset designed by hundreds of subject matter experts to capture the human frontier of knowledge and reasoning.</p></div>
  

  
    






<uni-image-full-width alignment="full" alt-text="Bar charts comparing the performance of Gemini 2.5 Pro with other AI models like OpenAI GPT-4.5 and Claude 3.7 Sonnet across three categories: Reasoning, Science, and Mathematics. Gemini 2.5 Pro shows strong results in all categories." external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="Gemini 2.5: Our most intelligent AI model" external-link="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_2.5_blog_1.original.jpg" custom-class="image-full-width--constrained-width uni-component-spacing">
  
  
    <p><img alt="Bar charts comparing the performance of Gemini 2.5 Pro with other AI models like OpenAI GPT-4.5 and Claude 3.7 Sonnet across three categories: Reasoning, Science, and Mathematics. Gemini 2.5 Pro shows strong results in all categories." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_2.5_blog_1.width-100.format-webp.webp" loading="lazy" data-loading="{
            &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_2.5_blog_1.width-500.format-webp.webp&quot;,
            &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_2.5_blog_1.width-1000.format-webp.webp&quot;
          }">
    </p>
  
</uni-image-full-width>


  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Gemini 2.5: Our most intelligent AI model&quot;
         }"><h2 data-block-key="rwvoz">Advanced coding</h2><p data-block-key="cvan3">We’ve been focused on coding performance, and with Gemini 2.5 we’ve achieved a big leap over 2.0 — with more improvements to come. 2.5 Pro excels at creating visually compelling web apps and agentic code applications, along with code transformation and editing. On SWE-Bench Verified, the industry standard for agentic code evals, Gemini 2.5 Pro scores 63.8% with a custom agent setup.</p><p data-block-key="4vj36">Here’s an example of how 2.5 Pro can use its reasoning capabilities to create a video game by producing the executable code from a single line prompt.</p></div>
  

  
    
  
    


  <uni-youtube-player-article index="11" thumbnail-alt="Animation of dinosaur game made with Gemini" video-id="RLCBSpgos6s" video-type="video">
  </uni-youtube-player-article>


  


  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Gemini 2.5: Our most intelligent AI model&quot;
         }"><h2 data-block-key="rwvoz">Building on the best of Gemini</h2><p data-block-key="o2v1">Gemini 2.5 builds on what makes Gemini models great — native multimodality and a long context window. 2.5 Pro ships today with a 1 million token context window (2 million coming soon), with strong performance that improves over previous generations. It can comprehend vast datasets and handle complex problems from different information sources, including text, audio, images, video and even entire code repositories.</p><p data-block-key="1jf6n">Developers and enterprises can start experimenting with Gemini 2.5 Pro in <a href="http://aistudio.google.com/app/prompts/new_chat?model=gemini-2.5-pro-exp-03-25">Google AI Studio</a> now, and <a href="https://gemini.google.com/">Gemini Advanced</a> users can select it in the model dropdown on desktop and mobile. It will be available on <a href="https://console.cloud.google.com/freetrial?redirectPath=/vertex-ai/studio">Vertex AI</a> in the coming weeks.</p><p data-block-key="2hak6">As always, we welcome feedback so we can continue to improve Gemini’s impressive new abilities at a rapid pace, all with the goal of making our AI more helpful.</p></div>
  


            
            

            
              




            
          </div>
        
      </div>
  </article>
  





  

  


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An AI bubble threatens Silicon Valley, and all of us (131 pts)]]></title>
            <link>https://prospect.org/power/2025-03-25-bubble-trouble-ai-threat/</link>
            <guid>43470786</guid>
            <pubDate>Tue, 25 Mar 2025 13:13:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://prospect.org/power/2025-03-25-bubble-trouble-ai-threat/">https://prospect.org/power/2025-03-25-bubble-trouble-ai-threat/</a>, See on <a href="https://news.ycombinator.com/item?id=43470786">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content" data-transition="content" data-loop="false" itemprop="articleBody">

    <p><em>This article appears in the</em><em> <a href="https://prospect.org/topics/apr-2025-issue/">April 2025</a> issue of</em> The American Prospect <em>magazine. <a href="https://simplecirc.com/subscribe/the-american-prospect" target="_blank" aria-label="Link opens in new window (Subscribe here)">Subscribe here</a>.</em></p>

<p>The week of Donald Trump’s inauguration, Sam Altman, the CEO of OpenAI, stood tall next to the president as he made a <a href="https://www.cnn.com/2025/01/21/tech/openai-oracle-softbank-trump-ai-investment/index.html" target="_blank" aria-label="Link opens in new window (dramatic announcement)">dramatic announcement</a>: the launch of Project Stargate, a $500 billion supercluster in the rolling plains of Texas that would run OpenAI’s massive artificial-intelligence models. Befitting its name, Stargate would dwarf most megaprojects in human history. Even the $100 billion that Altman promised would be deployed “immediately” would be much more expensive than the Manhattan Project ($30 billion in current dollars) and the COVID vaccine’s Operation Warp Speed ($18 billion), rivaling the multiyear construction of the Interstate Highway System ($114 billion). OpenAI would have all the computing infrastructure it needed to complete its ultimate goal of building humanity’s last invention: artificial general intelligence (AGI).</p>

<hr>
<p><strong>Art for this story was created with Midjourney 6.1, an AI image generator.</strong></p>

<hr>
<p>But the reaction to Stargate was muted as Silicon Valley had turned its attention west. A new generative AI model called DeepSeek R1, released by the Chinese hedge fund High-Flyer, sent a threatening tremor through the balance sheets and investment portfolios of the tech industry. DeepSeek’s latest version, allegedly trained for just $6 million (though <a href="https://www.techspot.com/news/106612-deepseek-ai-costs-far-exceed-55-million-claim.html" target="_blank" aria-label="Link opens in new window (this has been contested)">this has been contested</a>), <a href="https://arxiv.org/pdf/2501.12948" target="_blank" aria-label="Link opens in new window (matched)">matched</a> the performance of OpenAI’s flagship reasoning model o1 <a href="https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/" target="_blank" aria-label="Link opens in new window (at 95 percent lower cost)">at 95 percent lower cost</a>. R1 even learned o1 reasoning techniques, OpenAI’s much-hyped “secret sauce” to allow it to maintain a wide technical lead over other models. Best of all, R1 is open-source down to the model weights, so anyone can download and modify the details of the model themselves for free.</p>

<p>It’s an existential threat to OpenAI’s business model, which depends on using its technical lead to sell the most expensive subscriptions in the industry. It also threatens to pop a speculative bubble around generative AI inflated by the Silicon Valley hype machine, with hundreds of billions at stake.</p>

<p>Venture capital (VC) funds, drunk on a decade of “growth at all costs,” have poured <a href="https://www.cbinsights.com/research/report/ai-trends-q2-2024/" target="_blank" aria-label="Link opens in new window (about $200 billion into generative AI)">about $200 billion into generative AI</a>. Making matters worse, the stock market’s bull run is deeply dependent on the growth of the Big Tech companies fueling the AI bubble. In 2023, <a href="https://finance.yahoo.com/news/one-chart-shows-how-the-magnificent-7-have-dominated-the-stock-market-in-2023-203250125.html" target="_blank" aria-label="Link opens in new window (71 percent of the total gains)">71 percent of the total gains</a> in the S&amp;P 500 were attributable to the “Magnificent Seven”—Apple, Nvidia, Tesla, Alphabet, Meta, Amazon, and Microsoft—all of which are among the biggest spenders on AI. Just four—Microsoft, Alphabet, Amazon, and Meta—<a href="https://www.ft.com/content/634b7ec5-10c3-44d3-ae49-2a5b9ad566fa" target="_blank" aria-label="Link opens in new window (combined)">combined</a> for $246 billion of capital expenditure in 2024 to support the AI build-out. Goldman Sachs <a href="https://www.goldmansachs.com/images/migrated/insights/pages/gs-research/gen-ai--too-much-spend%2C-too-little-benefit-/TOM_AI%202.0_ForRedaction.pdf" target="_blank" aria-label="Link opens in new window (expects)">expects</a> Big Tech to spend over $1 trillion on chips and data centers to power AI over the next five years. Yet OpenAI, the current market leader, <a href="https://www.theinformation.com/briefings/openai-reportedly-in-talks-to-raise-6-5-billion-at-150-billion-valuation" target="_blank" aria-label="Link opens in new window (expects)">expects</a> to lose $5 billion this year, and its annual losses to swell to $11 billion by 2026. If the AI bubble bursts, it not only threatens to wipe out VC firms in the Valley but also blow a gaping hole in the public markets and cause an economy-wide meltdown.</p>

<h3><b>OpenAI’s Ever-Increasing Costs</b></h3>

<p>The basic problem facing Silicon Valley today is, ironically, one of growth. There are no more digital frontiers to conquer. The young, pioneering upstarts—Facebook, Google, Amazon—that struck out toward the digital wilderness are now the monopolists, constraining growth with onerous rentier fees they can charge because of their market-making size. The software industry’s spectacular returns from the launch of the internet in the ’90s to the end of the 2010s would never come back, but venture capitalists still chased the chance to invest in the next Facebook or Google. This has led to what AI critic Ed Zitron calls the “<a href="https://www.wheresyoured.at/the-rot-economy/" target="_blank" aria-label="Link opens in new window (rot economy)">rot economy</a>,” in which VCs overhype a series of digital technologies—the blockchain, then cryptocurrencies, then NFTs, and then the metaverse—promising the limitless growth of the early internet companies. According to Zitron, each of these innovations failed to either transform existing industries or become sustainable industries themselves, because the business case at the heart of these technologies was rotten, pushed forward by wasteful, bloated venture investments still selling an endless digital frontier of growth that no longer existed. Enter AGI, the proposed creation of an AI with an intelligence that dwarfs any single person’s and possibly the collective intelligence of humanity. Once AGI is built, we can easily solve many of the toughest challenges <a href="https://www.forbes.com/sites/moorinsights/2025/01/30/the-stargate-project-trump-touts-500-billion-bid-for-ai-dominance/" target="_blank" aria-label="Link opens in new window (facing humanity)">facing humanity</a>: climate change, cancer, new net-zero energy sources.</p>

<p>And no company has pushed the coming of AGI more than OpenAI, which has ridden the hype to incredible heights since its release of generative chatbot ChatGPT. Last year, OpenAI <a href="https://www.nytimes.com/2024/09/27/technology/openai-chatgpt-investors-funding.html" target="_blank" aria-label="Link opens in new window (completed)">completed</a> a blockbuster funding round, raising $6.6 billion at a valuation of $157 billion, making it the third most valuable startup in the world at the time after SpaceX and ByteDance, TikTok’s parent company. OpenAI, which released ChatGPT in November 2022, now <a href="https://www.wsj.com/tech/ai/openai-nearly-doubles-valuation-to-157-billion-in-funding-round-ee220607" target="_blank" aria-label="Link opens in new window (sees)">sees</a> 250 million weekly active users and about 11 million paying subscribers for its AI tools. The startup’s monthly revenue hit $300 million in August, up more than 1,700 percent since the start of 2023, and it expects to clear $3.7 billion for the year. By all accounts, this is another world-changing startup on a meteoric rise. Yet take a deeper look at OpenAI’s financial situation and expected future growth, and cracks begin to show.</p>

<p>To start, OpenAI is burning money at an impressive but unsustainable pace. The latest funding round is its third in the last two years, atypical for a startup, that also <a href="https://www.cnbc.com/2024/10/03/openai-gets-4-billion-revolving-credit-line-on-top-of-latest-funding.html" target="_blank" aria-label="Link opens in new window (included)">included</a> a $4 billion revolving line of credit—a loan on tap, essentially—on top of the $6.6 billion of equity, revealing an insatiable need for investor cash to survive. Despite $3.7 billion in sales this year, OpenAI expects to lose $5 billion due to the stratospheric costs of building and running generative AI models, which includes $4 billion in cloud computing to run their AI models, $3 billion in computing to train the next generation of models, and $1.5 billion for its staff. According to its own numbers, OpenAI loses $2 for every $1 it makes, a red flag for the sustainability of any business. Worse, these costs are expected to increase as ChatGPT gains users and OpenAI seeks to upgrade its foundation model from GPT-4 to GPT-5 sometime in the next six months.</p>

<p>Financial documents reviewed by <a href="https://www.theinformation.com/briefings/openai-reportedly-in-talks-to-raise-6-5-billion-at-150-billion-valuation" target="_blank" aria-label="Link opens in new window (The Information)">The Information</a> confirm this trajectory as the startup predicts its annual losses will hit $14 billion by 2026. Further, OpenAI sees $100 billion in annual revenue—a number that would rival Nestlé and Target’s returns—as the point at which it will finally break even. For comparison, Google’s parent company, Alphabet, only cleared $100 billion in sales in 2021, 23 years after its founding, yet boasted a portfolio of money-making products, including Google Search, the Android operating system, Gmail, and cloud computing.</p>

<p>OpenAI is deeply dependent on hypothetical breakthroughs from future models that unlock more capabilities to boost its subscription price and grow its user base. Its GPT-5 class models and beyond must pull godlike capacities for AI out of the algorithmic ether to create a user base of hundreds of millions of paid subscribers. Yet, with the release of the open-source DeepSeek R1 model earlier this month, OpenAI has no moat for its increasingly expensive products. The R1 matched its performance across math, chemistry, and coding tasks, independently learned OpenAI’s reasoning techniques, and can be downloaded, modified, and deployed for free. Why would people continue to pay $20, let alone the $200 tier OpenAI reserves for its latest, greatest models, rather than use something that can deliver the same performance at a 95 percent lower price?</p>

<blockquote><p>Venture capital funds, drunk on a decade of “growth at all costs,” have poured about $200 billion into generative AI.</p>
</blockquote>

<h3><b>Silicon Valley Is All In on AI</b></h3>

<p>Wall Street asked itself the same question after the release of DeepSeek R1 and panicked, wiping more than 15 percent ($600 billion) off Nvidia’s stock price, the largest single-day loss for a company ever. And that’s not the only bad sign Altman received about OpenAI’s future. OpenAI <a href="https://www.wsj.com/tech/ai/openaiin-talks-for-huge-investment-round-valuing-it-up-to-300-billion-2a2d4327" target="_blank" aria-label="Link opens in new window (is in talks again)">is in talks again</a> to raise more money (less than a year after raising $10 billion) at a proposed $340 billion valuation. In most cases, a startup doubling its valuation would be great news. But for OpenAI, the money may make things worse. It signals a desperate need for cash and puts more pressure on a company that today loses $2 for every dollar it makes. As Zitron pointed out, at $340 billion, few companies have the liquidity to acquire OpenAI, and public investors expect strong returns and profitability to justify an IPO anywhere near that price. Plus, the latest round of funding is being led by Masayoshi Son, a billionaire investor known more for losing money than making it. Given Son’s Vision Fund’s disastrous investing record, Zitron said, it’s as bearish a signal as you could find. Hanging over all this for OpenAI <a href="https://www.wheresyoured.at/subprimeai/" target="_blank" aria-label="Link opens in new window (is the fact that)">is the fact that</a> Microsoft’s investments in the company, which run north of $10 billion, are not standard equity investments but “profit participation units” that will convert to debt in a year and a half.</p>

<p>It’s not just OpenAI that’s burning through billions. Silicon Valley has hyped AI as the next internet or iPhone, and has invested like it cannot afford to miss out on the next big tech revolution. In 2021, with the last gasp of zero-interest-rate loans paired with trillions in COVID relief spending, venture capitalists <a href="https://www.ccn.com/analysis/ai-companies-raised-50b-2023-vc/" target="_blank" aria-label="Link opens in new window (poured a record $78.5 billion)">poured a record $78.5 billion</a> into the AI space. And, despite a broader slowdown in venture activity, the second quarter of 2024 set the record for quarterly venture investing in AI <a href="https://www.cbinsights.com/research/report/ai-trends-q2-2024/" target="_blank" aria-label="Link opens in new window (at $23.3 billion)">at $23.3 billion</a>. In fact, 33 percent of VC portfolios <a href="https://www.statista.com/chart/33346/ai-share-of-vc-investments-in-the-us/" target="_blank" aria-label="Link opens in new window (are committed)">are committed</a> to AI, another worrying sign of concentration.</p>

<p>Even so, Big Tech companies are the biggest spenders on AI. While VCs dropped approximately $200 billion into AI between 2021 and 2024, Big Tech is on pace to surpass that amount this year alone. According to Goldman Sachs research, cloud computing giants are expected to plow over $1 trillion over the next five years into graphics processing units (GPUs) and to build data centers to power generative AI. AI is an expensive technology like few before it.</p>

<p>All those racks of GPUs and supercluster data centers need power, and the power industry is also embarking on a once-in-a-generation investment spree to keep up. The scale of expected data centers to power generative AI is difficult to wrap your head around. Oracle recently <a href="https://www.cnbc.com/2024/09/10/oracle-is-designing-a-data-center-that-would-be-powered-by-three-small-nuclear-reactors.html" target="_blank" aria-label="Link opens in new window (announced)">announced</a> plans to build a gigawatt-scale data center just for AI, powered by a trio of nuclear reactors, while OpenAI <a href="https://arstechnica.com/tech-policy/2024/09/openai-asked-us-to-approve-energy-guzzling-5gw-data-centers-report-says/" target="_blank" aria-label="Link opens in new window (pitched)">pitched</a> the White House on the necessity of five-gigawatt data centers, which would be enough power for about three million homes consumed by a single AI data center. A recent report from McKinsey <a href="https://www.mckinsey.com/industries/private-capital/our-insights/how-data-centers-and-the-energy-sector-can-sate-ais-hunger-for-power" target="_blank" aria-label="Link opens in new window (expects)">expects</a> the electricity going to fuel AI data centers will triple from 3 to 4 percent of the country’s electricity to 11 to 12 percent by 2030. The power industry typically grows 2 to 3 percent a year, far too little to meet the predicted jump in demand. McKinsey estimates that power utilities would have to spend $500 billion on top of their planned capital expenditure to keep up with AI needs. If true, this presents a serious bottleneck for not just OpenAI but the expected growth of the AI industry.</p>

<h3><b>Where’s the Money, Lebowski?</b></h3>

<p>Between VCs, Big Tech, and power utilities, the bill for generative AI comes out to close to $2 trillion in spending over the next five years alone. Adding all this up, some are starting to question the economic fundamentals of generative AI. Jim Covello, head of global equity research at Goldman Sachs, <a href="https://www.goldmansachs.com/images/migrated/insights/pages/gs-research/gen-ai--too-much-spend%2C-too-little-benefit-/TOM_AI%202.0_ForRedaction.pdf" target="_blank" aria-label="Link opens in new window (doubts)">doubts</a> the technology can recoup what’s been invested as, unlike the internet, it fails to solve complex business problems at a lower cost than what’s available today. Plus, he argues, the most expensive inputs for generative AI, GPUs and energy, are unlikely to decline meaningfully for the tech industry over time, given how far demand outstrips supply for both. While AI-fueled coding could definitely boost productivity, it’s hard to see how it could become a multitrillion-dollar industry.</p>

<p>Surveys confirm that for many workers, AI tools like ChatGPT <i>reduce</i> their productivity by increasing the volume of content and steps needed to complete a given task, and by frequently introducing errors that have to be checked and corrected. A <a href="https://www.cio.com/article/3540579/devs-gaining-little-if-anything-from-ai-coding-assistants.html" target="_blank" aria-label="Link opens in new window (study by Uplevel Data Labs)">study by Uplevel Data Labs</a> tracked 800 software engineers using Copilot on GitHub and found no measurable increase in coding productivity, despite this exact use case being the one pointed to the most by AI companies. And even productivity gains may come at a cost: Microsoft researchers <a href="https://www.microsoft.com/en-us/research/uploads/prod/2025/01/lee_2025_ai_critical_thinking_survey.pdf" target="_blank" aria-label="Link opens in new window (concluded)">concluded</a> that workers became more productive using generative AI tools but their critical thinking skills declined, presumably because they were offloading the thinking to AI. Looking past the hype, the business case for generative AI two years after the stunning success of ChatGPT appears weaker by the day.</p>

<p>Even worse, as AI expert Gary Marcus <a href="https://garymarcus.substack.com/p/five-things-most-people-dont-seem" target="_blank" aria-label="Link opens in new window (pointed out)">pointed out</a>, DeepSeek’s R1 model <a href="https://www.msn.com/en-us/money/other/openai-is-highly-overvalued-and-deepseek-just-blew-up-their-business-model-says-nyu-s-gary-marcus/vi-AA1y0qgI" target="_blank" aria-label="Link opens in new window (spells serious trouble)">spells serious trouble</a> for OpenAI and the cloud giants. The only way OpenAI could hope to recoup the billions it was spending on GPUs to train bigger and bigger models was to maintain a large enough technical lead over other AI companies to justify charging up to $200 for paid subscriptions to its models. That lead <a href="https://garymarcus.substack.com/p/the-race-for-ai-supremacy-is-over" target="_blank" aria-label="Link opens in new window (just vaporized)">just vaporized</a> and was given to the entire industry for free. In response, Altman has already twice cut the prices of his subscriptions in an effort to stay competitive. But without millions of paid subscriptions, it’s difficult to see the pathway to profitability for a company that loses $2 for every $1 it brings in and expects costs to continue to grow approximately tenfold in five years. OpenAI has set $100 billion as its break-even point, which would require it to increase its revenue by a factor of 25 in just five years, an incredible feat of scale that its current business model does not justify.</p>

<p>OpenAI is, however, the perfect kind of growth-at-all-costs story investors need to think still exists—capable of not only achieving Meta- or Amazon-like growth again, but becoming an indispensable part of growth and innovation in every industry in the future, too. No industry could escape, and software would close its jaws around the world, finally, as Marc Andreessen <a href="https://a16z.com/why-software-is-eating-the-world/" target="_blank" aria-label="Link opens in new window (predicted)">predicted</a> in 2011.</p>

<p>For his part, Gary Marcus has taken to calling OpenAI the WeWork of AI—WeWork, of course, is the poster boy for wasteful, nearly fraudulent, growth-at-all-costs investing that led to a spectacular downfall. Marcus is so confident current approaches cannot take us to the promised land of AGI that he bet Anthropic CEO Dario Amodei $100,000 that AGI would not be achieved by the end of 2027. Without AGI, the valuations of leading AI startups like OpenAI ($340 billion) and Anthropic (<a href="https://www.cnbc.com/2025/02/24/anthropic-closes-in-on-3point5-billion-funding-round-.html" target="_blank" aria-label="Link opens in new window ($61.5 billion))">$61.5 billion)</a> stop making sense. If GPUs are no longer the most capital-efficient or effective way to build better AI models, then the expected AI computing “supercycle” that the hundreds of billions in capital expenditure is premised on never arrives. Instead, the underlying asset bubble of a multitrillion-dollar bet on GPUs as the necessary component to an internet-like era of growth vanishes into thin air.</p>

<blockquote><p>For many workers, AI tools reduce their productivity by increasing the volume of steps needed to complete a given task.</p>
</blockquote>

<h3><b>Just How Big Will the Blast Be?</b></h3>

<p>OpenAI’s incredible burn rate, the trillions in capital expenditure by cloud giants and utilities to build out the infrastructure necessary to support AI, the supply bottlenecks ahead from the power and semiconductor industries, and the questionable economic gains from these tools all point to a generative AI bubble. Should the bubble burst, startups and venture funds alike face possible extinction, and a big enough drop from the Magnificent Seven could spark skittish markets to panic, leading to wider economic contagion, given how dependent on the growth of the top technology companies the public markets have become.</p>

<p>In 2024, the Magnificent Seven <a href="https://www.cnbc.com/2024/07/01/how-magnificent-7-affects-sp-500-stock-market-concentration.html" target="_blank" aria-label="Link opens in new window (were responsible)">were responsible</a> for the lion’s share of the growth of the S&amp;P 500, with the returns of the other 493 companies flat. When Nvidia hit its peak valuation of $3 trillion over the summer, just five of the seven—Microsoft, Apple, Nvidia, Alphabet, and Amazon—accounted for 29 percent of the total index’s value, <a href="https://www.inc.com/phil-rosen/stock-market-outlook-nvidia-apple-microsoft-amazon-recession-fed-rates.html" target="_blank" aria-label="Link opens in new window (surpassing)">surpassing</a> the concentration of the five top technology companies just before the dot-com crash. <a href="https://www.reuters.com/markets/echoes-dotcom-bubble-haunt-ai-driven-us-stock-market-2024-07-02/" target="_blank" aria-label="Link opens in new window (Nvidia)">Nvidia</a> has been on an incredible bull run over the last five years, its shares <a href="https://www.reuters.com/markets/echoes-dotcom-bubble-haunt-ai-driven-us-stock-market-2024-07-02/" target="_blank" aria-label="Link opens in new window (gaining a dizzying 4,300 percent)">gaining a dizzying 4,300 percent</a>, reminiscent of how network equipment maker Cisco grew about 4,500 percent in the five years leading up to its peak just before the dot-com crash in 2000.</p>

<p>Nvidia and the other Magnificent Seven members are in a codependent relationship when it comes to AI hype. They are Nvidia’s biggest customers, feeding the bull run by pushing demand for GPUs beyond even what chipmaker TSMC can supply. At the moment, Nvidia can pass those prices on to their customers, the only clusters big enough for AI computing. But should demand for AI fall, all seven will tumble with it.</p>

<p>For the tech industry, DeepSeek is a threat to its incredible bull run because it proved three things. First, frontier AI models could be trained much more cheaply and efficiently than the current Silicon Valley approach of building massive models requiring hundreds of thousands of GPUs to train. From a capital perspective, the U.S. strategy is wasteful, relying on at least ten times the investment to make similar model progress. Second, DeepSeek showed you could train a state-of-the-art model without the latest GPUs, calling into question the current demand for the latest GPUs that is so hot customers have been facing delays of six months to a year to get their hands on them. Finally, the high valuations of leading AI startups depend on a technical lead in their models to charge prices anywhere near what they need to recoup their computing costs, but that technical lead, enabled by a combination of closed-source models, billions in capital expenditure, and export controls blocking Chinese companies like DeepSeek from accessing the latest GPUs, is gone. Should demand for GPUs fall or even not hit the exponential increases the billions invested are betting on, the bubble will pop.</p>

<p>Given the stock market’s dependence on tech companies for growth, the trigger may not come from the AI industry itself, but any pullback in spending will crater the current trajectory of the AI industry. Many potential triggers abound: a crypto crash; President Trump’s trade wars with Canada, Mexico, and China; the stated goal to cut more than $1 trillion of government spending by Elon Musk’s Department of Government Efficiency; or a Chinese invasion of Taiwan, where <a href="https://www.wired.com/story/taiwan-makes-the-majority-of-the-worlds-computer-chips-now-its-running-out-of-electricity/" target="_blank" aria-label="Link opens in new window (nearly 70 percent)">nearly 70 percent</a> of the world’s advanced computer chips are manufactured. You can tell Wall Street is worried about a bubble, because Nvidia is hit the hardest by any bearish AI news, and even when the market panic has nothing to do with the tech industry, like when the Japan currency trade happened last summer, the Magnificent Seven suffer punishing losses.</p>

<p>The AI bubble wobbles more precariously by the day. Some bubbles, like that of the dot-com crash, end up being positive in the long run, despite the short-term economic pain of it bursting. But some, like the 2008 housing bubble, leave permanent scars on the economy and can knock an entire industry off its growth trajectory for years. To date, the U.S. housing industry <a href="https://www.barrons.com/articles/home-building-in-the-u-s-still-hasnt-recovered-from-the-last-recession-heres-why-51568282700" target="_blank" aria-label="Link opens in new window (has not recovered)">has not recovered</a> to pre-2008 growth trend lines, a major contributor to the housing crisis gripping the U.S. That is the fire that the tech industry is playing with today.</p>

<p>This is not the Silicon Valley of lore. Venture investors, for all their tech manifestos celebrating “<a href="https://a16z.com/the-little-tech-agenda/" target="_blank" aria-label="Link opens in new window (little tech)">little tech</a>” and entrepreneurship, have come to <a href="https://www.irmagazine.com/shareholder-targeting-id/how-sovereign-wealth-funds-are-inflating-silicon-valley-bubble" target="_blank" aria-label="Link opens in new window (resemble)">resemble</a> more traditional financial firms, raising money from pension funds, hedge funds, and sovereign wealth funds. Silicon Valley has gone corporate and managerial; even private equity invests in the Valley today. The fusion of venture capital and Wall Street threatens to bring the unbridled speculation of unregulated finance and the breathless tech industry hype together in a single, massive bubble. Inimical to the old ethos of the Valley and emblematic of a bloated, rotten investing strategy, in Silicon Valley now the money chases founders rather than founders chasing money. Maybe, after the fallout of the AI bubble is felt and the sun sets on Silicon Valley for a bit, the tech world can do a hard reset and return to its more innovative days again.</p>


    
    

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla deliveries down 43% in Europe while EVs are up 31% (163 pts)]]></title>
            <link>https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/</link>
            <guid>43470763</guid>
            <pubDate>Tue, 25 Mar 2025 13:11:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/">https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/</a>, See on <a href="https://news.ycombinator.com/item?id=43470763">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1600" height="818" src="https://electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?quality=82&amp;strip=all&amp;w=1600" alt="Tesla all cars hero" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/12/Tesla-all-casrs-hero.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high"></figure>

<p>The official numbers from ACEA are out, and they confirmed that Tesla deliveries have crashed by 43% in Europe so far this year.</p>



<p>It should be concerning for Tesla as electric vehicle sales are up 31% during the period.</p>



<p>Based on the main European auto markets already having reported vehicle registrations earlier this month, we already had <a href="https://electrek.co/2025/03/05/tesla-tsla-sales-crash-continues-in-europe-with-germany-down-70/" target="_blank" rel="noreferrer noopener">a good idea of Tesla’s performance in the market</a>, but now the European Automobile Manufacturers Association (ACEA) has made it official.</p>



<p>ACEA has released February registration numbers confirming that Tesla only delivered 16,888 units in the EU, EFTA, and UK markets in February 2025, compared to 28,182 units in 2024.</p>	
	



<p>For the first two months of the year, it totals 26,619 units, or 42.6% less than the 46,343 units delivered during the same period in 2024.</p>



<p>This is amid a 3.4% decline in automotive sales in Europe, but Tesla can hardly use that as an excuse since ACEA is reporting a 28.4% (31% in EU, EFTA, and UK) increase in battery-electric vehicle (BEV) sales this year despite Tesla’s erosion in the market:</p>



<blockquote>
<p><strong>Across the first two months of 2025</strong>, new battery-electric car sales grew by 28.4%, to 255,489 units, capturing 15.2% of total EU market share. Three of the four largest markets in the EU, accounting for 64% of all battery-electric car registrations, recorded robust double-digit gains: Germany (+41%), Belgium (+38%), and the Netherlands (+25%). This contrasted with France, which saw a slight decline of 1.3%.</p>
</blockquote>



<p>In the EU, EFTA, and UK markets, BEVs account for 17% of the entire auto market, with PHEVs adding another 7%.</p>



<p>Tesla had the worst performance of all automakers in the market:</p>



<figure><img decoding="async" width="1440" height="1612" src="https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09%E2%80%AFAM.png?w=915" alt="" srcset="https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png 1440w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=134,150 134w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=268,300 268w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=768,860 768w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=915,1024 915w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=1372,1536 1372w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=313,350 313w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=140,157 140w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=893,1000 893w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-25-at-8.02.09 https://electrek.co/2025/03/25/tesla-tsla-deliveries-down-43-in-europe-while-evs-are-up-28/AM.png?resize=150,168 150w" sizes="(max-width: 1440px) 100vw, 1440px"></figure>



<h2 id="h-electrek-s-take">Electrek’s Take</h2>



<p>Tesla fans are holding on to the idea that this is not a real problem because it is mostly due to the Model Y changeover, but that’s simply not true.</p>



<p>Model 3 registrations are also down in most European markets, despite Tesla having a similar situation as Model Y for Model 3 during this time last year.</p>



<p>In fact, Model 3 is down 29.4% in Europe so far this year despite plenty of inventory.</p>



<p>The shift to the new Model Y design is certainly having an effect, but it cannot account for the 43% drop in deliveries.</p>



<p>With deliveries of the new Model Y having started this month in Europe, we can see Tesla is still suffering in markets that report registration daily.</p>



<p>Tesla has only delivered 655 cars so far in March in Sweden compared to 2,524 for the whole month of March in 2024.</p>



<p>In Norway, Tesla is at 1,444 deliveries compared to 2,334 units for the whole month of March in 2024.</p>



<p>Tesla is already down 20,000 units in Europe for the first two months of the year compared to its 2024 performance, and that number could grow to 30,000 units by the end of the quarter.</p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<div><p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p><p><a href="https://bit.ly/Beatbot_SpringCleaning"><img src="https://electrek.co/wp-content/uploads/sites/3/2025/03/750x150-Beatbot-Native-Ad-Banners-2.png" alt="" width="750" height="150"></a></p></div>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Samsung CEO Jong-hee Han has died (233 pts)]]></title>
            <link>https://www.engadget.com/big-tech/samsung-ceo-jong-hee-han-has-died-120029286.html</link>
            <guid>43470699</guid>
            <pubDate>Tue, 25 Mar 2025 13:04:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/big-tech/samsung-ceo-jong-hee-han-has-died-120029286.html">https://www.engadget.com/big-tech/samsung-ceo-jong-hee-han-has-died-120029286.html</a>, See on <a href="https://news.ycombinator.com/item?id=43470699">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>One of Samsung's CEOs, Jong-hee Han, has died due to a heart attack, according to <a data-i13n="cpos:1;pos:1" href="https://www.reuters.com/world/asia-pacific/samsung-electronics-says-co-ceo-han-jong-hee-has-died-cardiac-arrest-2025-03-25/" rel="nofollow noopener" target="_blank" data-ylk="slk:Reuters;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas"><em>Reuters</em></a> and <a data-i13n="cpos:2;pos:1" href="https://www.cnbc.com/2025/03/25/samsung-electronics-says-co-ceo-han-jong-hee-has-passed-away.html" rel="nofollow noopener" target="_blank" data-ylk="slk:CNBC;cpos:2;pos:1;elm:context_link;itc:0;sec:content-canvas"><em>CNBC</em></a>. He was 63. Han joined the company in 1988 and became the head of product research and development for visual display in 2011. He then led Samsung's TV business before he was named as the head of Samsung DX, which is what the company calls its <a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/samsung-combines-mobile-consumer-electronics-055424678.html" data-ylk="slk:merged mobile and consumer electronics;cpos:3;pos:1;elm:context_link;itc:0;sec:content-canvas">merged mobile and consumer electronics</a> divisions, in 2021. In 2022, he officially became the company's Vice Chairman and CEO. Han had no experience in mobile before he started leading the company's DX group, but Samsung gave him credit for helping it get to the top of global TV sales for 15 years.</p><p><em>CNBC</em> says Han was one of the executives who hosted Samsung's annual general shareholders meeting just a week ago and answered questions about the company's poor stock performance. During the meeting, Han <a data-i13n="cpos:4;pos:1" href="https://www.cnbc.com/2025/03/19/samsung-ceo-says-company-will-pursue-deals-as-it-struggles-for-growth.html" rel="nofollow noopener" target="_blank" data-ylk="slk:apologized;cpos:4;pos:1;elm:context_link;itc:0;sec:content-canvas">apologized</a> to the shareholders, telling them that Samsung "failed to adequately respond to the rapidly evolving AI semiconductor market." He also told the shareholders that Samsung was having difficulties when it came to semiconductor-related mergers and acquisitions due to regulatory issues, but that the company was "determined to produce some tangible results this year."</p><p>Based on the notice Samsung <a data-i13n="cpos:5;pos:1" href="https://www.samsung.com/global/ir/reports-disclosures/public-disclosure-view.84335/" rel="nofollow noopener" target="_blank" data-ylk="slk:published;cpos:5;pos:1;elm:context_link;itc:0;sec:content-canvas">published</a>, Han's co-CEO Young-Hyun Jun is now the sole CEO of the company. Jun, who also heads Samsung's semiconductor business, was appointed as Han's co-CEO in November 2024. It's not clear whether Samsung is planning to appoint another CEO in the future.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VGGT: Visual Geometry Grounded Transformer (102 pts)]]></title>
            <link>https://github.com/facebookresearch/vggt</link>
            <guid>43470651</guid>
            <pubDate>Tue, 25 Mar 2025 12:59:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/facebookresearch/vggt">https://github.com/facebookresearch/vggt</a>, See on <a href="https://news.ycombinator.com/item?id=43470651">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{wang2025vggt,
  title={VGGT: Visual Geometry Grounded Transformer},
  author={Wang, Jianyuan and Chen, Minghao and Karaev, Nikita and Vedaldi, Andrea and Rupprecht, Christian and Novotny, David},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}"><pre><span>@inproceedings</span>{<span>wang2025vggt</span>,
  <span>title</span>=<span><span>{</span>VGGT: Visual Geometry Grounded Transformer<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Wang, Jianyuan and Chen, Minghao and Karaev, Nikita and Vedaldi, Andrea and Rupprecht, Christian and Novotny, David<span>}</span></span>,
  <span>booktitle</span>=<span><span>{</span>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2025<span>}</span></span>
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">Visual Geometry Grounded Transformer (VGGT, CVPR 2025) is a feed-forward neural network that directly infers all key 3D attributes of a scene, including extrinsic and intrinsic camera parameters, point maps, depth maps, and 3D point tracks, <strong>from one, a few, or hundreds of its views, within seconds</strong>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto">First, clone this repository to your local machine, and install the dependencies (torch, torchvision, numpy, Pillow, and huggingface_hub).</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:facebookresearch/vggt.git 
cd vggt
pip install -r requirements.txt"><pre>git clone git@github.com:facebookresearch/vggt.git 
<span>cd</span> vggt
pip install -r requirements.txt</pre></div>
<p dir="auto">Alternatively, you can install VGGT as a package (<a href="https://github.com/facebookresearch/vggt/blob/main/docs/package.md">click here</a> for details).</p>
<p dir="auto">Now, try the model with just a few lines of code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from vggt.models.vggt import VGGT
from vggt.utils.load_fn import load_and_preprocess_images

device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) 
dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16

# Initialize the model and load the pretrained weights.
# This will automatically download the model weights the first time it's run, which may take a while.
model = VGGT.from_pretrained(&quot;facebook/VGGT-1B&quot;).to(device)

# Load and preprocess example images (replace with your own image paths)
image_names = [&quot;path/to/imageA.png&quot;, &quot;path/to/imageB.png&quot;, &quot;path/to/imageC.png&quot;]  
images = load_and_preprocess_images(image_names).to(device)

with torch.no_grad():
    with torch.cuda.amp.autocast(dtype=dtype):
        # Predict attributes including cameras, depth maps, and point maps.
        predictions = model(images)"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>vggt</span>.<span>models</span>.<span>vggt</span> <span>import</span> <span>VGGT</span>
<span>from</span> <span>vggt</span>.<span>utils</span>.<span>load_fn</span> <span>import</span> <span>load_and_preprocess_images</span>

<span>device</span> <span>=</span> <span>"cuda"</span> <span>if</span> <span>torch</span>.<span>cuda</span>.<span>is_available</span>() <span>else</span> <span>"cpu"</span>
<span># bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) </span>
<span>dtype</span> <span>=</span> <span>torch</span>.<span>bfloat16</span> <span>if</span> <span>torch</span>.<span>cuda</span>.<span>get_device_capability</span>()[<span>0</span>] <span>&gt;=</span> <span>8</span> <span>else</span> <span>torch</span>.<span>float16</span>

<span># Initialize the model and load the pretrained weights.</span>
<span># This will automatically download the model weights the first time it's run, which may take a while.</span>
<span>model</span> <span>=</span> <span>VGGT</span>.<span>from_pretrained</span>(<span>"facebook/VGGT-1B"</span>).<span>to</span>(<span>device</span>)

<span># Load and preprocess example images (replace with your own image paths)</span>
<span>image_names</span> <span>=</span> [<span>"path/to/imageA.png"</span>, <span>"path/to/imageB.png"</span>, <span>"path/to/imageC.png"</span>]  
<span>images</span> <span>=</span> <span>load_and_preprocess_images</span>(<span>image_names</span>).<span>to</span>(<span>device</span>)

<span>with</span> <span>torch</span>.<span>no_grad</span>():
    <span>with</span> <span>torch</span>.<span>cuda</span>.<span>amp</span>.<span>autocast</span>(<span>dtype</span><span>=</span><span>dtype</span>):
        <span># Predict attributes including cameras, depth maps, and point maps.</span>
        <span>predictions</span> <span>=</span> <span>model</span>(<span>images</span>)</pre></div>
<p dir="auto">The model weights will be automatically downloaded from Hugging Face. If you encounter issues such as slow loading, you can manually download them <a href="https://huggingface.co/facebook/VGGT-1B/blob/main/model.pt" rel="nofollow">here</a> and load, or:</p>
<div dir="auto" data-snippet-clipboard-copy-content="model = VGGT()
_URL = &quot;https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt&quot;
model.load_state_dict(torch.hub.load_state_dict_from_url(_URL))"><pre><span>model</span> <span>=</span> <span>VGGT</span>()
<span>_URL</span> <span>=</span> <span>"https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt"</span>
<span>model</span>.<span>load_state_dict</span>(<span>torch</span>.<span>hub</span>.<span>load_state_dict_from_url</span>(<span>_URL</span>))</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Detailed Usage</h2><a id="user-content-detailed-usage" aria-label="Permalink: Detailed Usage" href="#detailed-usage"></a></p>
<p dir="auto">You can also optionally choose which attributes (branches) to predict, as shown below. This achieves the same result as the example above. This example uses a batch size of 1 (processing a single scene), but it naturally works for multiple scenes.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from vggt.utils.pose_enc import pose_encoding_to_extri_intri
from vggt.utils.geometry import unproject_depth_map_to_point_map

with torch.no_grad():
    with torch.cuda.amp.autocast(dtype=dtype):
        images = images[None]  # add batch dimension
        aggregated_tokens_list, ps_idx = model.aggregator(images)
                
    # Predict Cameras
    pose_enc = model.camera_head(aggregated_tokens_list)[-1]
    # Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)
    extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images.shape[-2:])

    # Predict Depth Maps
    depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images, ps_idx)

    # Predict Point Maps
    point_map, point_conf = model.point_head(aggregated_tokens_list, images, ps_idx)
        
    # Construct 3D Points from Depth Maps and Cameras
    # which usually leads to more accurate 3D points than point map branch
    point_map_by_unprojection = unproject_depth_map_to_point_map(depth_map.squeeze(0), 
                                                                extrinsic.squeeze(0), 
                                                                intrinsic.squeeze(0))

    # Predict Tracks
    # choose your own points to track, with shape (N, 2) for one scene
    query_points = torch.FloatTensor([[100.0, 200.0], 
                                        [60.72, 259.94]]).to(device)
    track_list, vis_score, conf_score = model.track_head(aggregated_tokens_list, images, ps_idx, query_points=query_points[None])"><pre><span>from</span> <span>vggt</span>.<span>utils</span>.<span>pose_enc</span> <span>import</span> <span>pose_encoding_to_extri_intri</span>
<span>from</span> <span>vggt</span>.<span>utils</span>.<span>geometry</span> <span>import</span> <span>unproject_depth_map_to_point_map</span>

<span>with</span> <span>torch</span>.<span>no_grad</span>():
    <span>with</span> <span>torch</span>.<span>cuda</span>.<span>amp</span>.<span>autocast</span>(<span>dtype</span><span>=</span><span>dtype</span>):
        <span>images</span> <span>=</span> <span>images</span>[<span>None</span>]  <span># add batch dimension</span>
        <span>aggregated_tokens_list</span>, <span>ps_idx</span> <span>=</span> <span>model</span>.<span>aggregator</span>(<span>images</span>)
                
    <span># Predict Cameras</span>
    <span>pose_enc</span> <span>=</span> <span>model</span>.<span>camera_head</span>(<span>aggregated_tokens_list</span>)[<span>-</span><span>1</span>]
    <span># Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)</span>
    <span>extrinsic</span>, <span>intrinsic</span> <span>=</span> <span>pose_encoding_to_extri_intri</span>(<span>pose_enc</span>, <span>images</span>.<span>shape</span>[<span>-</span><span>2</span>:])

    <span># Predict Depth Maps</span>
    <span>depth_map</span>, <span>depth_conf</span> <span>=</span> <span>model</span>.<span>depth_head</span>(<span>aggregated_tokens_list</span>, <span>images</span>, <span>ps_idx</span>)

    <span># Predict Point Maps</span>
    <span>point_map</span>, <span>point_conf</span> <span>=</span> <span>model</span>.<span>point_head</span>(<span>aggregated_tokens_list</span>, <span>images</span>, <span>ps_idx</span>)
        
    <span># Construct 3D Points from Depth Maps and Cameras</span>
    <span># which usually leads to more accurate 3D points than point map branch</span>
    <span>point_map_by_unprojection</span> <span>=</span> <span>unproject_depth_map_to_point_map</span>(<span>depth_map</span>.<span>squeeze</span>(<span>0</span>), 
                                                                <span>extrinsic</span>.<span>squeeze</span>(<span>0</span>), 
                                                                <span>intrinsic</span>.<span>squeeze</span>(<span>0</span>))

    <span># Predict Tracks</span>
    <span># choose your own points to track, with shape (N, 2) for one scene</span>
    <span>query_points</span> <span>=</span> <span>torch</span>.<span>FloatTensor</span>([[<span>100.0</span>, <span>200.0</span>], 
                                        [<span>60.72</span>, <span>259.94</span>]]).<span>to</span>(<span>device</span>)
    <span>track_list</span>, <span>vis_score</span>, <span>conf_score</span> <span>=</span> <span>model</span>.<span>track_head</span>(<span>aggregated_tokens_list</span>, <span>images</span>, <span>ps_idx</span>, <span>query_points</span><span>=</span><span>query_points</span>[<span>None</span>])</pre></div>
<p dir="auto">Furthermore, if certain pixels in the input frames are unwanted (e.g., reflective surfaces, sky, or water), you can simply mask them by setting the corresponding pixel values to 0 or 1. Precise segmentation masks aren't necessary - simple bounding box masks work effectively (check this <a href="https://github.com/facebookresearch/vggt/issues/47" data-hovercard-type="issue" data-hovercard-url="/facebookresearch/vggt/issues/47/hovercard">issue</a> for an example).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Visualization</h2><a id="user-content-visualization" aria-label="Permalink: Visualization" href="#visualization"></a></p>
<p dir="auto">We provide multiple ways to visualize your 3D reconstructions and tracking results. Before using these visualization tools, install the required dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements_demo.txt"><pre>pip install -r requirements_demo.txt</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Interactive 3D Visualization</h3><a id="user-content-interactive-3d-visualization" aria-label="Permalink: Interactive 3D Visualization" href="#interactive-3d-visualization"></a></p>
<p dir="auto"><strong>Please note:</strong> VGGT typically reconstructs a scene in less than 1 second. However, visualizing 3D points may take tens of seconds due to third-party rendering, independent of VGGT's processing time. The visualization is slow especially when the number of images is large.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Gradio Web Interface</h4><a id="user-content-gradio-web-interface" aria-label="Permalink: Gradio Web Interface" href="#gradio-web-interface"></a></p>
<p dir="auto">Our Gradio-based interface allows you to upload images/videos, run reconstruction, and interactively explore the 3D scene in your browser. You can launch this in your local machine or try it on <a href="https://huggingface.co/spaces/facebook/vggt" rel="nofollow">Hugging Face</a>.</p>

<details>
<summary>Click to preview the Gradio interactive interface</summary>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/45f94454b15337d5f7b7b39344673976a1266a32e2096355f49311c396068d8c/68747470733a2f2f6a7974696d652e6769746875622e696f2f646174612f766767745f68665f64656d6f5f73637265656e2e706e67"><img src="https://camo.githubusercontent.com/45f94454b15337d5f7b7b39344673976a1266a32e2096355f49311c396068d8c/68747470733a2f2f6a7974696d652e6769746875622e696f2f646174612f766767745f68665f64656d6f5f73637265656e2e706e67" alt="Gradio Web Interface Preview" data-canonical-src="https://jytime.github.io/data/vggt_hf_demo_screen.png"></a></p>
</details>
<p dir="auto"><h4 tabindex="-1" dir="auto">Viser 3D Viewer</h4><a id="user-content-viser-3d-viewer" aria-label="Permalink: Viser 3D Viewer" href="#viser-3d-viewer"></a></p>
<p dir="auto">Run the following command to run reconstruction and visualize the point clouds in viser. Note this script requires a path to a folder containing images. It assumes only image files under the folder. You can set <code>--use_point_map</code> to use the point cloud from the point map branch, instead of the depth-based point cloud.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python demo_viser.py --image_folder path/to/your/images/folder"><pre>python demo_viser.py --image_folder path/to/your/images/folder</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Track Visualization</h3><a id="user-content-track-visualization" aria-label="Permalink: Track Visualization" href="#track-visualization"></a></p>
<p dir="auto">To visualize point tracks across multiple images:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from vggt.utils.visual_track import visualize_tracks_on_images
track = track_list[-1]
visualize_tracks_on_images(images, track, (conf_score>0.2) &amp; (vis_score>0.2), out_dir=&quot;track_visuals&quot;)"><pre><span>from</span> <span>vggt</span>.<span>utils</span>.<span>visual_track</span> <span>import</span> <span>visualize_tracks_on_images</span>
<span>track</span> <span>=</span> <span>track_list</span>[<span>-</span><span>1</span>]
<span>visualize_tracks_on_images</span>(<span>images</span>, <span>track</span>, (<span>conf_score</span><span>&gt;</span><span>0.2</span>) <span>&amp;</span> (<span>vis_score</span><span>&gt;</span><span>0.2</span>), <span>out_dir</span><span>=</span><span>"track_visuals"</span>)</pre></div>
<p dir="auto">This plots the tracks on the images and saves them to the specified output directory.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Single-view Reconstruction</h2><a id="user-content-single-view-reconstruction" aria-label="Permalink: Single-view Reconstruction" href="#single-view-reconstruction"></a></p>
<p dir="auto">Our model shows surprisingly good performance on single-view reconstruction, although it was never trained for this task. The model does not need to duplicate the single-view image to a pair, instead, it can directly infer the 3D structure from the tokens of the single view image. Feel free to try it with our demos above, which naturally works for single-view reconstruction.</p>
<p dir="auto">We did not quantitatively test monocular depth estimation performance ourselves, but <a href="https://github.com/kabouzeid">@kabouzeid</a> generously provided a comparison of VGGT to recent methods <a href="https://github.com/facebookresearch/vggt/issues/36" data-hovercard-type="issue" data-hovercard-url="/facebookresearch/vggt/issues/36/hovercard">here</a>. VGGT shows competitive or better results compared to state-of-the-art monocular approaches such as DepthAnything v2 or MoGe, despite never being explicitly trained for single-view tasks.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Runtime and GPU Memory</h2><a id="user-content-runtime-and-gpu-memory" aria-label="Permalink: Runtime and GPU Memory" href="#runtime-and-gpu-memory"></a></p>
<p dir="auto">We benchmark the runtime and GPU memory usage of VGGT's aggregator on a single NVIDIA H100 GPU across various input sizes.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><strong>Input Frames</strong></th>
<th>1</th>
<th>2</th>
<th>4</th>
<th>8</th>
<th>10</th>
<th>20</th>
<th>50</th>
<th>100</th>
<th>200</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Time (s)</strong></td>
<td>0.04</td>
<td>0.05</td>
<td>0.07</td>
<td>0.11</td>
<td>0.14</td>
<td>0.31</td>
<td>1.04</td>
<td>3.12</td>
<td>8.75</td>
</tr>
<tr>
<td><strong>Memory (GB)</strong></td>
<td>1.88</td>
<td>2.07</td>
<td>2.45</td>
<td>3.23</td>
<td>3.63</td>
<td>5.58</td>
<td>11.41</td>
<td>21.15</td>
<td>40.63</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Note that these results were obtained using Flash Attention 3, which is faster than the default Flash Attention 2 implementation while maintaining almost the same memory usage. Feel free to compile Flash Attention 3 from source to get better performance.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Research Progression</h2><a id="user-content-research-progression" aria-label="Permalink: Research Progression" href="#research-progression"></a></p>
<p dir="auto">Our work builds upon a series of previous research projects. If you're interested in understanding how our research evolved, check out our previous works:</p>
<markdown-accessiblity-table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<p dir="auto">Thanks to these great repositories: <a href="https://github.com/facebookresearch/PoseDiffusion">PoseDiffusion</a>, <a href="https://github.com/facebookresearch/vggsfm">VGGSfM</a>, <a href="https://github.com/facebookresearch/co-tracker">CoTracker</a>, <a href="https://github.com/facebookresearch/dinov2">DINOv2</a>, <a href="https://github.com/naver/dust3r">Dust3r</a>, <a href="https://github.com/microsoft/moge">Moge</a>, <a href="https://github.com/facebookresearch/pytorch3d">PyTorch3D</a>, <a href="https://github.com/xiongzhu666/Sky-Segmentation-and-Post-processing">Sky Segmentation</a>, <a href="https://github.com/DepthAnything/Depth-Anything-V2">Depth Anything V2</a>, <a href="https://github.com/YvanYin/Metric3D">Metric3D</a> and many other inspiring works in the community.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Checklist</h2><a id="user-content-checklist" aria-label="Permalink: Checklist" href="#checklist"></a></p>
<ul>
<li> Release the training code</li>
<li> Release VGGT-500M and VGGT-200M</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">See the <a href="https://github.com/facebookresearch/vggt/blob/main/LICENSE.txt">LICENSE</a> file for details about the license under which this code is made available.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[X’s director of engineering, Haofei Wang, has left the company (105 pts)]]></title>
            <link>https://www.theverge.com/twitter/634833/x-head-engineering-leaves-elon-musk</link>
            <guid>43470613</guid>
            <pubDate>Tue, 25 Mar 2025 12:54:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/twitter/634833/x-head-engineering-leaves-elon-musk">https://www.theverge.com/twitter/634833/x-head-engineering-leaves-elon-musk</a>, See on <a href="https://news.ycombinator.com/item?id=43470613">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><img alt="Kylie Robison" data-chromatic="ignore" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/197393/GMbiBVSaAAABxU1.0.jpeg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=48 1x, https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/197393/GMbiBVSaAAABxU1.0.jpeg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96 2x" src="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/197393/GMbiBVSaAAABxU1.0.jpeg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96"></p><p><a href="https://www.theverge.com/authors/kylie-robison">Kylie Robison</a> <span>is a senior AI reporter working with The Verge’s policy and tech teams. She previously worked at Fortune Magazine and Business Insider.</span></p></div><div id="zephr-anchor"><p>X’s director of engineering, Haofei Wang, has suddenly left the company, according to sources with knowledge of the matter.</p><p>Wang first joined Elon Musk’s X in July 2023 and has been an integral part of the company’s leadership, often serving as a conduit between Musk and the rest of the company’s engineers. More recently, he was seen internally as X’s defacto head of engineering and product, especially with Musk recently focusing more of his time on xAI and DOGE. It’s unclear why Wang is departing now<em>. </em>Neither he nor a company spokesperson responded to a request for comment in time for publication.</p><p>X recently added other engineering leadership: Mike Dalton and Uday Ruddaraju, both previously technical leads at Robinhood, joined in January. Their LinkedIn profiles show that they also work at xAI, which has become <a href="https://www.theverge.com/2025/1/10/24339249/elon-musk-xai-x-twitter">increasingly intertwined</a> with X over the last year.</p><div><div><p id="do-you-work-at-x-or-xai"><h2><strong>Do you work at X or xAI?</strong></h2></p></div><p> I’d love to chat. You can reach me securely on Signal @kylie.01 or via email at kylie@theverge.com.</p></div><p>Thanks to the growing profile of xAI and Musk’s newfound political influence, X’s business appears to be turning around. The company reportedly just <a href="https://www.ft.com/content/d4616dec-c4c7-417f-8549-134710bbc5b1">obtained a $44 billion valuation</a> from investors — the same price Musk paid for Twitter in 2022. While Musk remains an avid poster on X, his attention and leadership at X has become increasingly split since he started campaigning for President Donald Trump last summer.</p><p>When Musk first purchased Twitter, he vowed to transform it into an “everything app” akin to China’s WeChat. <em>The Verge</em> <a href="https://www.theverge.com/2023/10/26/23934216/x-twitter-bank-elon-musk-2024">previously reported an internal X meeting</a> where Musk said that it “would blow my mind” if the company couldn’t handle “someone’s entire financial life” by the end of 2024. Those plans haven’t come to fruition, though my sources say that work is still underway to launch the X Money payments platform (<a href="https://www.theverge.com/news/599137/x-money-payments-service-2025-launch">it’s slated to come</a> “later this year”).</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Coding Isn't Programming (143 pts)]]></title>
            <link>https://www.socallinuxexpo.org/scale/22x/presentations/closing-keynote-leslie-lamport</link>
            <guid>43469711</guid>
            <pubDate>Tue, 25 Mar 2025 10:34:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.socallinuxexpo.org/scale/22x/presentations/closing-keynote-leslie-lamport">https://www.socallinuxexpo.org/scale/22x/presentations/closing-keynote-leslie-lamport</a>, See on <a href="https://news.ycombinator.com/item?id=43469711">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
      <a id="main-content"></a>
                    
                                                        
<div>
      
  <p><span><div>
        <p><span>        <span><img typeof="foaf:Image" src="https://www.socallinuxexpo.org/sites/default/files/styles/square_thumbnail/public/speakers/Leslie_Lamport.jpg?itok=1UPhMyDa" width="180" height="180" alt=""></span>  </span>  </p>
    </div></span>  </p>  
  <div><p><a href="https://www.socallinuxexpo.org/scale/22x/speakers/leslie-lamport">Leslie Lamport</a></p></div>  
  <div>    <p><span>Audience: </span></p><div><p><a href="https://www.socallinuxexpo.org/scale/22x/audience-level/everyone">Everyone</a></p></div>  </div>  
  <div>    <p><span>Topic: </span></p><div><p><a href="https://www.socallinuxexpo.org/scale/22x/track/keynote">Keynote</a></p></div>  </div>  
  <div>        <p>Join us for a captivating closing keynote with the legendary Leslie Lamport, Turing Award winner and pioneer in the field of distributed computing. We'll discuss computing history, open source and distributed systems.&nbsp;</p>  </div>  
  <div>    <p><span>Presentation: </span></p><div><p><span><img alt="PDF icon" title="application/pdf" src="https://www.socallinuxexpo.org/modules/file/icons/application-pdf.png"> <a href="https://www.socallinuxexpo.org/sites/default/files/presentations/linux-expo%20%281%29.pdf" type="application/pdf; length=1401215">linux-expo (1).pdf</a></span></p></div>  </div>  
  <div>    <p><span>Room: </span></p><p>Ballroom DE</p>  </div>  
  <div>    <p><span>Time: </span></p><p><span>Sunday, March 9, 2025 - <span><span property="dc:date" datatype="xsd:dateTime" content="2025-03-09T15:00:00-07:00">15:00</span> to <span property="dc:date" datatype="xsd:dateTime" content="2025-03-09T16:00:00-07:00">16:00</span></span></span></p>  </div>  
  <div>    <p><span>Audio/Video: </span></p><p><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/tsSDvflzJbc?si=2LSe2NTfzfKwXNWO" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>  </div>  </div>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Great Barefoot Running Hysteria of 2010 (171 pts)]]></title>
            <link>https://runningshoescore.com/blog/barefoot-running-hysteria-of-2010</link>
            <guid>43469690</guid>
            <pubDate>Tue, 25 Mar 2025 10:28:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://runningshoescore.com/blog/barefoot-running-hysteria-of-2010">https://runningshoescore.com/blog/barefoot-running-hysteria-of-2010</a>, See on <a href="https://news.ycombinator.com/item?id=43469690">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <p>
03/25/25 • 9 minute read
    </p>
<p>The year was 2010.  “Ke$ha's Tik Tok”  was topping the Billboard charts.  Steve Jobs has just introduced a goofy new oversized iPhone called an “iPad”.  And in running forums across the internet far and wide, hoards of enthusiasts preached the gospel of a new way of running: without shoes.</p><figure><img src="https://api.needto.run/content/images/2024/01/barefoot.png" alt="A graph of Google search interest in barefoot running showing a huge spike around 2010, which trails off by 2013" loading="lazy" width="1136" height="580" srcset="https://api.needto.run/content/images/size/w600/2024/01/barefoot.png 600w, https://api.needto.run/content/images/size/w1000/2024/01/barefoot.png 1000w, https://api.needto.run/content/images/2024/01/barefoot.png 1136w" sizes="(min-width: 720px) 720px"><figcaption><span>Google Trends interest in "Barefoot Running" from 2004 to present</span></figcaption></figure><p>The “Great Barefoot Running Hysteria of 2010”, as I call it, took the amateur running world by storm.  Propelled by dramatic claims of performance improvements and injury prevention, barefoot running gave rise to a vocal (and often militant) contingent of enthusiasts and entirely new classes of footwear.  And then over the course of a several years, it faded away almost as quickly as it came, leaving behind changes in running shoes and culture forever.  In this post, we'll explore the history and legacy of the barefoot running movement.</p><h3 id="the-barefoot-running-movement-how-it-started">The Barefoot Running Movement &amp; How It Started</h3><p>Barefoot running is–of course–as old as humanity itself. In fact, people have run barefoot throughout most of human history, with the practice continuing today in several cultures, such as Kenya and indigenous peoples in Mexico.  </p><p>In this sense, before addressing the modern origins of <em>barefoot </em>running, we need to talk about the origins of <em>shod </em>running<em>.</em>  The concept of running shoes as we understand them today, specifically designed to improve running efficiency and comfort, did not emerge until the late 19th and early 20th centuries. This evolution coincided with the rise of organized sports and recreational running, which spurred the development of footwear tailored to the specific needs of runners. </p><figure><img src="https://api.needto.run/content/images/2024/05/lord-spencers-shoes-the-first-specialized-running-shoes-ever-made-from-1865-2.jpg" alt="" loading="lazy" width="460" height="347"></figure><p>The first breakthrough came in 1865 when an English shoemaker brilliantly suggested adding spikes to otherwise normal-looking dress shoes to make them suitable for cross-country running. Later, in the early 20th century, shoes with rubber soles were introduced, offering improved grip and shock absorption, marking a significant advancement in the design of running shoes.</p><blockquote>The result of these running shoe trends was running shoe stores filled with bulky, over-engineered clown shoes that promised to prevent running injuries, but probably didn't.</blockquote><p>Into more modern times, running shoes continued to evolve past their humble beginnings into heavier, more complex footwear with exotic materials like plastics &amp; EVA foam.  In the 80s and 90s, many shoemakers became fixated on <em>stability </em>and the notion that pronation, the natural roll of the foot after it lands, was a cause for running injuries.  Running shoes were increasingly designed to try to prevent this movement with wedges of foam that support and stabilize the arch of the foot, a high heel-to-toe drop, and other questionable features like plastic air bubbles in the heel to cushion the foot with each step.</p><p>The result of these running shoe trends was running shoe stores filled with bulky, over-engineered clown shoes that promised to prevent running injuries, but probably didn't, and were mostly just uncomfortable to use. You can see an example of this kind of design in action in the Nike Air Max 90, which is now also available in a more tasteful re-issue.  While the original was marketed as a running shoe, the modern re-issue is a nostalgic fashion shoe that you should definitely <em>not </em>run in.</p><figure><img src="https://api.needto.run/content/images/2024/03/geqhdlqounif1d4bvgz2.jpg" alt="A bulky black leather running shoe from the 90s with an air bubble in the heel" loading="lazy" width="670" height="464" srcset="https://api.needto.run/content/images/size/w600/2024/03/geqhdlqounif1d4bvgz2.jpg 600w, https://api.needto.run/content/images/2024/03/geqhdlqounif1d4bvgz2.jpg 670w"><figcaption><span>The original Nike Air Max 90</span></figcaption></figure><figure><img src="https://api.needto.run/content/images/2024/03/air-max-90-mens-shoes-Bd2qnn.png-copy.png" alt="A blue &amp; whit modern re-issue of the Air Max 90 running shoe with an air bubble in the heel" loading="lazy" width="1580" height="877" srcset="https://api.needto.run/content/images/size/w600/2024/03/air-max-90-mens-shoes-Bd2qnn.png-copy.png 600w, https://api.needto.run/content/images/size/w1000/2024/03/air-max-90-mens-shoes-Bd2qnn.png-copy.png 1000w, https://api.needto.run/content/images/2024/03/air-max-90-mens-shoes-Bd2qnn.png-copy.png 1580w" sizes="(min-width: 720px) 720px"><figcaption><span>A modern re-issue of the Nike Air Max 90 (from </span><a href="https://nike.com/?ref=api.needto.run" rel="noreferrer"><span>Nike.com</span></a><span>)</span></figcaption></figure><p>The barefoot running renaissance can thus be understood as a backlash to the dominant running shoe trends at the time.  The revival was driven by a growing dissatisfaction with traditional running shoes, which many believed contributed to injuries and impeded natural foot movement. Proponents of barefoot running argued that it encouraged a more natural gait, reducing the impact on the legs and back and enhancing the overall running experience.</p><p>In 2004, inspired by Stanford athletes training barefoot, Nike introduced the <em>Nike Free</em>, a running shoe that bucked the bulky running shoe trends of the time with a super flexible sole and a minimal heel-to-toe offset.</p><figure><img src="https://api.needto.run/content/images/2025/03/Nike_Free-_3_running_shoe.jpg" alt="A picture of blue Nike running shoes with a flexible sole and a small height difference between the toe and heel" loading="lazy" width="400" height="280"><figcaption><span>The original Nike Free (via Wikipedia)</span></figcaption></figure><p>Then, in 2006, Vibram launched the 5-Fingers: a shoe intended to be as close to barefoot as possible, with a thin rubber sole and glove-like fit for the toes.  This was the shoe that truly ushered in the <em>minimalist </em>mindset: running shoes should enable a "natural" gait and that <em>less is more</em>. </p><figure><img src="https://api.needto.run/content/images/2024/03/vibram.jpg" alt="" loading="lazy" width="1101" height="695" srcset="https://api.needto.run/content/images/size/w600/2024/03/vibram.jpg 600w, https://api.needto.run/content/images/size/w1000/2024/03/vibram.jpg 1000w, https://api.needto.run/content/images/2024/03/vibram.jpg 1101w" sizes="(min-width: 720px) 720px"><figcaption><span>Vibram 5 Fingers (via Amazon)</span></figcaption></figure><p>The barefoot-inspired <em>Nike Free </em>and <em>Vibram 5 Fingers</em> set the stage, but the real barefoot running revolution began not with a pair of shoes, but with a book about an indigenous tribe of Native Mexicans.</p><h3 id="christopher-mcdougalls-born-to-run">Christopher McDougall's "Born to Run"</h3><p>While the over-engineered running shoe backlash had already started with shoes like the Nike Free and the Vibram 5 Fingers, the spark that lit the barefoot running powder keg was Christopher McDougall's 2009 bestseller, "Born to Run". The book, which explores the running habits of the Tarahumara Native Mexican tribe, known for their long-distance running ability, captivated the imagination of runners–and non-runners–everywhere. </p><figure><img src="https://api.needto.run/content/images/2024/01/51-Ry-ireXL._SL500_.jpg" alt="A book cover for &quot;Born to Run&quot; featuring an illustration of a barefoot and a man running" loading="lazy" width="500" height="500"><figcaption><span>Book cover for "Born to Run" by Christopher McDougall</span></figcaption></figure><p>The Tarahumara, or Rarámuri, as they refer to themselves, are an indigenous people who reside in the rugged and remote Copper Canyon region of Northwestern Mexico. Renowned for their extraordinary long-distance running abilities, the Tarahumara have garnered international attention and admiration from runners and researchers alike. Their running habits, deeply embedded in their culture and lifestyle, are not merely for sport but serve practical and ceremonial purposes as well.</p><p>McDougall's narrative suggested that modern running injuries were virtually non-existent among the Tarahumara.  They often ran barefoot or in minimal footwear, sandals crafted from leather and tire strips called <em>huaraches.  </em>The sandals provide minimal cushioning and protection, promoting a natural running form that many attribute to their low incidence of running-related injuries according to McDougall.  </p><p>The popularity of McDougall's book sparked widespread curiosity and enthusiasm for barefoot running, and many readers happily ditched their bulky running shoes for new, minimalist alternatives.  As the barefoot running movement evolved, so did the market for running footwear, leading to the development of minimalist shoes designed to mimic the barefoot running experience while providing some protection from the hazards of rough terrain. </p><p>Bolstered by the popularity of "Born to Run", barefoot running was suddenly <em>everywhere.  </em>This period saw a surge in barefoot running clinics, forums, and social media groups where enthusiasts shared tips and experiences.  </p><p>And as often happens, what started as a perfectly reasonable idea took on a life of its own and became dogmatic: barefoot running was <em>the way to run</em>.  The movement gave rise to a series of increasingly lofty and strongly worded claims: barefoot running prevented injuries; barefoot running was more efficient; heel striking was evil; barefoot running was the natural and therefore "correct" way to run.  The idea transcended running communities and seeped into popular culture prompting lofty headlines like this New York Times article, <a href="https://www.nytimes.com/2011/11/06/magazine/running-christopher-mcdougall.html?pagewanted=1&amp;_r=1&amp;ref=api.needto.run">The Once and Future Way to Run</a>.</p><p>The idea even transcended the sport of running itself.  To many, the barefoot running movement was not merely about the act of running without shoes; it represented a broader philosophy seeking to embrace simplicity, natural form, and mindfulness in the pursuit of physical fitness and well-being.  The fervor around barefoot running bordered on religious.</p><p>Despite the smug sense of superiority that some barefoot proponents projected, many of the most enthusiastic adopters of barefoot running were, in fact, novice and inexperienced runners.  While barefoot running enthusiasts were eager to point out elite runners training or racing barefoot, like <a href="https://en.wikipedia.org/wiki/Zola_Budd?ref=api.needto.run">Zola Budd</a>, these barefoot elites are the exception rather than the rule.  Most serious and elite runners generally sat out the barefoot trend, or at least took a more nuanced approach.  Many advanced runners already gravitated towards less bulky shoes, and many that did adopt barefoot running did so as part of isolated workouts on soft surfaces.</p><p>It's difficult to quantify this claim, but my personal memory of this period was that online discourse around running form and footwear was dominated by an aggressive mob mentality around barefoot running.   If you were running with shoes on, you were–according to the online mob–"doin' it wrong", as the internet was fond of saying at the time.  </p><p>Though it generally didn't represent the opinions of more experienced runners, the barefoot running crowd was certainly the loudest. Opinions that didn't completely jive with the all-minimalist approach were often brutally and violently downvoted.</p><p>Despite the enthusiasm and claims of the benefits of barefoot running, research into the practice was thin.  While some of the claims seemed logical, most of the evidence around the benefits of barefoot running was anecdotal.  Much of the argument hinged around barefoot running being the <em>natural </em>and thus correct way to run, and that running with over-engineered running shoes was <em>unnatural</em> and thus incorrect.</p><p>Some will recognize this line of reasoning as the <a href="https://en.wikipedia.org/wiki/Appeal_to_nature?ref=api.needto.run#Examples"><em>appeal to nature fallacy</em></a>: a logical fallacy in which a subject is claimed to be <em>good </em>simply because it is <em>natural</em>.  The fallacy pops up frequently in health &amp; medical settings when people extol the virtues of "all-natural" products or alternative medicines.  Sure, some natural products are healthy and beneficial; but so are a lot of deadly poisons.  Similarly, diet, where the appeal to nature is used to justify all sorts of sometimes conflicting food choices (throughout history humans have eaten wildly versatile diets).  This doesn't mean that natural is <em>bad, </em>it simply means that it's not a valid argument in and of itself for the benefits of barefoot running.</p><p>As time went on and the research caught up with the trend, the results were mixed at best: some studies showed potential benefits, while others highlighted increased risks. Critics of barefoot running point to evidence showing an increased incidence of certain types of injuries, such as Achilles tendinitis and metatarsal stress fractures, among runners transitioning to barefoot or minimalist running without proper adaptation. Additionally, there's an obvious concern about the lack of protection from environmental hazards (e.g., sharp objects, rough terrain) when running without traditional footwear, which can lead to acute injuries. </p><p>At the same time, many fledgling barefoot runners soon faced a reality check. Reports of injuries began to surface, casting doubt on the benefits of running without shoes. Podiatrists and sports medicine professionals started warning about the potential risks, especially for those with pre-existing foot conditions, those who transitioned too quickly, or those who tried to run too much. </p><p>Many who stuck with the sport and aspired to run longer distances such as the marathon soon learned that it's simply difficult to put in the mileage required to excel at these distances without proper footwear.  Though you're sure to find counterexamples (I still see a small handful of barefoot runners at major running events like the Boston Marathon), most runners who are putting in 50, 60, 70, or more miles a week to race a marathon are doing so with shoes designed to help cushion the impact.</p><h2 id="long-lasting-changes-to-running-shoes">Long-Lasting Changes to Running Shoes</h2><p>Despite the decline in barefoot running's popularity, its impact on the running shoe industry was undeniable. Recognizing the demand for a more natural running experience, shoe manufacturers began developing more lightweight, minimalist running shoes with less cushioning and a lower heel-to-toe drop. </p><p>More importantly, the minimalist movement helped end the dominance of needlessly overbuilt running shoes.  Of course, highly supportive &amp; motion control running shoes are still available, and some runners prefer to run in them, but it's no longer the dominant paradigm.</p><p>Ironically, the minimalist shoe movement triggered its own backlash with the <em>maximalist </em>shoe movement of mega-cushioned shoes, ushered in by brands like HOKA.  But the maximalist trend is not a return to the <em>overbuilt </em>shoes of the 90s–in fact, it still incorporates minimalist concepts like lower heel-to-toe drop, lightweight materials, and placing less emphasis on the support and motion control that earlier running shoes relied on.  </p><p>These innovations aimed to combine the benefits of barefoot running with the protection and support of traditional running shoes. Today, many of these features remain integral in modern running shoe design, marking a lasting legacy of the barefoot running movement.</p><p>So in the end, while the barefoot running hysteria of 2010 may have been a short-lived trend, it sparked crucial conversations about running health and led to significant advancements in running footwear. Though no longer in the limelight, its impact continues to influence how we run and think about our running gear.</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spammers are better at SPF, DKIM, and DMARC than everyone else (350 pts)]]></title>
            <link>https://toad.social/@grumpybozo/114213600922816869</link>
            <guid>43468995</guid>
            <pubDate>Tue, 25 Mar 2025 08:14:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://toad.social/@grumpybozo/114213600922816869">https://toad.social/@grumpybozo/114213600922816869</a>, See on <a href="https://news.ycombinator.com/item?id=43468995">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Writing your own C++ standard library from scratch (133 pts)]]></title>
            <link>https://nibblestew.blogspot.com/2025/03/writing-your-own-c-standard-library.html</link>
            <guid>43468976</guid>
            <pubDate>Tue, 25 Mar 2025 08:10:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nibblestew.blogspot.com/2025/03/writing-your-own-c-standard-library.html">https://nibblestew.blogspot.com/2025/03/writing-your-own-c-standard-library.html</a>, See on <a href="https://news.ycombinator.com/item?id=43468976">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-7528406150771042207" itemprop="description articleBody">
<p>The C++ standard library (also know as the STL) is, without a doubt, an astounding piece of work. Its scope, performance and incredible backwards compatibility have taken decades of work by many of the world's best programmers. My hat's off to all those people who have contributed to it.</p><p>All of that is not to say that it is not without its problems. The biggest one being the absolutely abysmal compile times but unreadability, and certain unoptimalities caused by strict backwards compatibility are also at the top of the list. In fact, it could be argued that most of the things people really dislike about C++ are features of the STL rather than the language itself. Fortunately, using the STL is not mandatory. If you are crazy enough, you can disable it completely and build your own standard library in the best Bender style.</p><p>One of the main advantages of being an unemployed-by-choice open source developer is that you can do all of that if you wish. There are no incompetent middle damagers hovering over your shoulder to ensure you are "producing immediate customer value" rather than "wasting time on useless polishing that does not produce immediate customer value".</p><p>It's <i>my</i> time, and I'll waste it if I want to!</p><h2>What's in it?</h2><p>The biggest design questions of a standard library are scope and the "feel" of the API. Rather than spending time on design, we steal it. Thus, when in doubt, read the Python stdlib documentation and replicate it. Thus the name of the library is <span>pystd</span>.</p><h2>The test app</h2><p>To keep the scope meaningful, we start by writing only enough of stdlib to build an app that reads a text file, validates it as UTF-8, splits the contents into words, counts how many time each word appears in the file and prints all words and how many times it appears sorted by decreasing count.</p><p>This requires, at least:</p><ul><li>File handling</li><li>Strings</li><li>UTF8 validation</li><li>A hash map</li><li>A vector</li><li>Sorting</li></ul><h2>The training wheels come off</h2><p>The code is available in <a href="https://github.com/jpakkane/pystd">this Github repo</a> for those who want to follow along at home.</p><p>Disabling the STL is fairly easy (with Linux+GCC at least) and requires only these two Meson statements:</p><blockquote><p><span><span>add_global_arguments('-nostdinc++', language: 'cpp')
</span><br>add_global_link_arguments('-nostdlib++', '-lsupc++', language: 'cpp')<br></span></p></blockquote><p>The <span>supc++</span> library is (according to stackoverflow) a support library GCC needs to implement core language features. Now the stdlib is off and it is time to implement everything with sticks, stones and duct tape.</p><h2>The outcome</h2><p>Once you have implemented everything discussed above and auxiliary stuff like a hashing framework the main application looks like this.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiEpKmdPLSQOlvqiROHQIOEFf96ejCzNHnIReu06sLvUVAtlIcenIWOKCBcT0smhJHsxktrgR63nzQsU3ZrW_-JNnwd3b-rOw-nn6pEilsioc-G2LuMKR4LHYnbUOAo_uVfRRBpFgyEMv_z7vEHbAkFwovyH_RWWPd6-A6sk9qlZeLOKl6Dgczt8ORMBeo/s728/pystd_main.png"><img data-original-height="529" data-original-width="728" height="233" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiEpKmdPLSQOlvqiROHQIOEFf96ejCzNHnIReu06sLvUVAtlIcenIWOKCBcT0smhJHsxktrgR63nzQsU3ZrW_-JNnwd3b-rOw-nn6pEilsioc-G2LuMKR4LHYnbUOAo_uVfRRBpFgyEMv_z7vEHbAkFwovyH_RWWPd6-A6sk9qlZeLOKl6Dgczt8ORMBeo/s320/pystd_main.png" width="320"></a></p><p>The end result is both Valgrind and Asan clean. There is one chunk of unreleased memory, but that comes from <span>supc++</span>. There is probably UB in the implementation. But it should be the good kind of UB that, if it would actually not work, would break the entire Linux userspace because <i>everything</i> depends on it working "as expected".</p><p>All of this took fewer than 1000 lines of code in the library itself (including a regex implementation that is not actually used). For comparison merely including <span>vector</span> from the STL brings in 27 thousand lines of code.</p><h2>Comparison to an STL version</h2><p>Converting this code to use the STL is fairly simple and only requires changing some types and fine tuning the API.&nbsp; The main difference is that the STL version does not validate that the input is UTF-8 as there is no builtin function for that. Now we can compare the two.</p><p>Runtime for both is 0.001 to 0.002 seconds on the small test file I used. Pystd is not noticeably slower than the STL version, which is enough for our purposes. It almost certainly scales worse because there has been zero performance work on it.</p><p>Compiling the pystd version with <span>-O2</span> takes 0.3 seconds whereas the STL version takes 1.2 seconds. The measurements were done on a Ryzen 7 3700X processor.&nbsp;</p><p>The executable's unstripped size is 349k for STL and 309k for pystd. The stripped sizes are 23k and 135k. Approximately 100 k of the pystd executable comes from <span>supc++</span>. In the STL version that probably comes dynamically from <span>libstdc++</span>&nbsp;(which, on this machine, takes 2.5 MB).</p><h2>Perfect ABI stability</h2><p>Designing a standard library is exceedingly difficult because you can't ever really change it. Someone, somewhere, is depending on every misfeature in it so they can never be changed.</p><p>Pystd has been designed to both support perfect ABI stability <i>and</i> make it possible to change it in arbitrary ways in the future. If you start from scratch this turned out to be fairly simple.</p><p>The sample code above used the <span>pystd</span> namespace. It does not actually exist. Instead it is defined like this in the cpp file:</p><blockquote><p><span>#include &lt;pystd2025.hpp&gt;</span>&nbsp;</p></blockquote><blockquote><p><span>namespace pystd = pystd2025;</span></p></blockquote><p>In pystd all code is in a namespace with a year and is stored in a header file with the same year. The idea is, then, that every year you create a new release. This involves copying all stdlib header files to a file with the new year and regexping the namespace declarations to match. The old code is now frozen forever (except for bug fixes) whereas the new code can be changed at will because there are&nbsp;<i>zero existing lines of code that depend on it</i>.</p><p>End users now have the choice of when to update their code to use newer pystd versions. Even better, if there is an old library that can not be updated, any of the old versions can be used in parallel. For example:</p><p><span>pystd2030::SomeType foo;<br>pystd2025::SomeType bar(foo.something(), foo.something_else());</span></p><p>Thus if no code is ever updated, everything keeps working. If all code is updated at once, everything works. If only parts of the code are updated, things can still be made to work with some glue code. This puts the maintenance burden on the people whose projects can not be updated as opposed to every other developer in the world. This is as it should be, and also would motivate people with broken deps to spend some more effort to get them fixed.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reflecting on WikiTok (137 pts)]]></title>
            <link>https://www.aizk.sh/posts/reflecting-on-wikitok</link>
            <guid>43468491</guid>
            <pubDate>Tue, 25 Mar 2025 06:09:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.aizk.sh/posts/reflecting-on-wikitok">https://www.aizk.sh/posts/reflecting-on-wikitok</a>, See on <a href="https://news.ycombinator.com/item?id=43468491">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Search My Site – open-source search engine for personal and independent websites (236 pts)]]></title>
            <link>https://searchmysite.net</link>
            <guid>43467541</guid>
            <pubDate>Tue, 25 Mar 2025 02:19:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://searchmysite.net">https://searchmysite.net</a>, See on <a href="https://news.ycombinator.com/item?id=43467541">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<h2>About searchmysite.net</h2>
					<p>
						What is the search engine and why should I use it?
					</p>
					<blockquote>
						The searchmysite.net search engine is a niche search, focussing on the "indieweb" or "small web" or "digital gardens", i.e. non-commercial content, primarily personal and independent websites.
					</blockquote>
					<p>
						If you want to research people's personal experiences of or deep-dives into certain topics, hobbies or interests, then you may find the searchmysite.net public search useful to avoid having to wade through all the marketing websites and blog spam that fill the big search engines. It differs from the commercial search engines because it:
					</p>

					<ul>
						<li>Indexes only user-submitted and moderated sites, rather than indexing the entire internet with all of its spam, "search engine optimisation" and "click-bait" content.</li>
						<li>Does not show adverts and promotes advert-free results pages, to remove the incentives for spam and surveillance capitalism.</li>
						<li>Aims to have a sustainable and user-aligned operating model, planning to pay running costs via the "search as a service" features, rather than relying on advertising which could put it in conflict with user needs.</li>
						<li>Offers an unusually high level of privacy for a search engine, thanks to the operating model which is not based on advertising or collecting personal information for sale to advertisers (see the Privacy Policy for full details).</li>
						<li>Is fully open source, for even greater transparency of the searching, ranking and indexing processes, and for greater community involvement in improving the service.</li>
					</ul>

				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jeffrey Goldberg on being added to the group chat by Trump Administration (121 pts)]]></title>
            <link>https://www.theatlantic.com/newsletters/archive/2025/03/jeffrey-goldberg-group-chat-military-houthi-yemen/682160/</link>
            <guid>43466983</guid>
            <pubDate>Tue, 25 Mar 2025 00:45:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/newsletters/archive/2025/03/jeffrey-goldberg-group-chat-military-houthi-yemen/682160/">https://www.theatlantic.com/newsletters/archive/2025/03/jeffrey-goldberg-group-chat-military-houthi-yemen/682160/</a>, See on <a href="https://news.ycombinator.com/item?id=43466983">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-event-module="article body" data-flatplan-body="true"><p data-flatplan-paragraph="true"><small><i>This is an edition of </i>The Atlantic<i> Daily, a newsletter that guides you through the biggest stories of the day, helps you discover new ideas, and recommends the best in culture. </i><a data-event-element="inline link" data-gtm-vis-first-on-screen31117857_899="351" data-gtm-vis-has-fired31117857_899="1" data-gtm-vis-recent-on-screen31117857_899="351" data-gtm-vis-total-visible-time31117857_899="100" href="https://www.theatlantic.com/newsletters/sign-up/atlantic-daily/"><i>Sign up for it here.</i></a></small></p><p data-flatplan-paragraph="true">The Trump administration has provided many jaw-dropping moments, but few have been as shocking as editor in chief <a data-event-element="inline link" href="https://www.theatlantic.com/politics/archive/2025/03/trump-administration-accidentally-texted-me-its-war-plans/682151/">Jeffrey Goldberg’s scoop</a> published today. Goldberg reported on how he was inadvertently added to a discussion of a military strike on Houthi militias in Yemen, conducted over the encrypted messaging app Signal. In essence, a reporter was invited to listen while the nation’s top security officials weighed and debated a military action, and was sent detailed information about the strike.</p><p data-flatplan-paragraph="true">Even President Donald Trump seemed unaware of the breach. “You’re saying that they had what?” he replied when asked about the news. Trump added that he is “not a big fan of <i>The Atlantic</i>,” something he’s <a data-event-element="inline link" href="https://www.theatlantic.com/politics/archive/2024/10/trump-attacks-atlantics-jeffrey-goldberg-over-hitler/680422/">previously made clear</a>, and which makes the unintentional leak all the more remarkable. A spokesperson for the National Security Council said, “This appears to be an authentic message chain, and we are reviewing how an inadvertent number was added to the chain.”</p><p data-flatplan-paragraph="true">I called Goldberg this afternoon to learn more about how the story came about and what the disclosure reveals about the Trump administration. This interview has been condensed and edited.</p><hr><p data-flatplan-paragraph="true"><b>David A. Graham: </b>Has anything like this ever happened to you before?</p><p data-flatplan-paragraph="true"><b>Jeffrey Goldberg:</b> I think that on one level, this is very relatable. Everyone has sent a text or an email to an unintended recipient, and sometimes they’ve embarrassed themselves by doing that. This is, I would say, at a different level—but it kind of proves a point, which is that there’s a reason people who work on sensitive issues in the government aren’t supposed to use Signal, even though it is end-to-end encrypted. Anyone can use Signal, so if you’re not careful, you might pull into your conversation a Houthi sympathizer or a magazine editor.</p><p data-flatplan-paragraph="true">Our colleague <a data-event-element="inline link" href="https://www.theatlantic.com/author/shane-harris/">Shane Harris</a> points out that the phones of top senior defense and national-security and intelligence officials are targets of intelligence operations. Imagine what you can do if you saw everything that the CIA director may be texting, even on a secure phone—especially on a secure phone. I’m mindful of the fact that the Trump team has already dealt with a <a data-event-element="inline link" href="https://www.theatlantic.com/politics/archive/2025/01/classified-documents-trump-case/681327/">serious issue</a> in the securing of sensitive documents in Mar-a-Lago. If you’re going to make a big deal about <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2017/07/but-his-emails/533274/">Hillary Clinton’s emails</a>, you may want to have excellent communication hygiene.</p><p data-flatplan-paragraph="true"><b>David:</b> Tell me how you came to conclude that this group was real.</p><p data-flatplan-paragraph="true"><b>Jeffrey: </b>It was actually chilling: 11:44 a.m., Saturday the 15th, eastern time, in my car in a parking lot, just checking my phone, and I see a text from Pete Hegseth, or somebody who’s hoaxing me as Pete Hegseth. It provides information about upcoming military operations with timings attached: <em>T</em><i>his is going to happen. Then this happens, then this happens, and this happens.</i> I’m sitting there in my car and thinking I’m about to find out if this was an elaborate scam or not. So I thought to myself,<i> I’m sitting here for the next two hours with my hands around my phone.</i> I check on X around 1:55, and yes, Sanaa is being bombed, so then I have the realization that this is almost certainly a real channel and not just an elaborate fakery of some sort. And that’s when I began to realize that I had to write about this massive security breach.</p><p data-flatplan-paragraph="true"><b>David: </b>You have done a lot of sensitive national-security reporting. Have you ever received any information like this?</p><p data-flatplan-paragraph="true"><b>Jeffrey: </b>No, nothing like this. This was like an intravenous drip of information that no one in the government thinks journalists should have. Until almost the very last minute, I could not believe that this was actually happening, that there could be a Mack-truck-size breach, that somehow, the editor in chief of <i>The Atlantic</i> was invited into a conversation with the intelligence agencies, secretaries, the national security adviser. Like most reporters, I’ve been a recipient of leaks. A leak is a totally different thing. That’s a whistleblower trying to make complaints. This is just reckless.</p><p data-flatplan-paragraph="true"><b>David: </b>There’s the horror that something like this would happen on an operational level, but in terms of what we learn from the substance of the conversation, what are the most important things that people should take away?</p><p data-flatplan-paragraph="true"><b>Jeffrey: </b>The actual conversation that they have is fascinating, and in a certain way impressive. It’s nice to see that they’re disagreeing with one another. It’s very useful for the public to know that the vice president has a more hands-off approach than other members of the administration. One of the things that I found interesting was that when a person named “S M” in the chat, who I took to be Stephen Miller, comes in and says, “As I heard it, the president was clear,” this kind of shuts down the conversation. It suggests that Stephen Miller can be in a conversation with, among others, the vice president of the United States and still can get his way. (Miller did not reply to a request for comment or confirm that he is “S M.”)</p><p data-flatplan-paragraph="true"><b>David: </b>In addition to the question of how secure Signal is, this is also notable, because without this report, none of these conversations might be preserved for posterity. You note that National Security Adviser Michael Waltz set some messages to disappear after a week or so.</p><p data-flatplan-paragraph="true"><b>Jeffrey: </b>This is an interesting question: Are they using Signal because it’s convenient? Are they using Signal because it disappears? According to the experts Shane interviewed, the administration should not have established a Signal thread for such conversations in the first place, but once it did, what one is supposed to do legally is copy an official government account, and that government account will then send these threads to the National Archives for posterity, for research, for accountability. But if you’re using a disappearing-text app, I don’t know. That’s one of the questions that I’ve asked and have not gotten answered yet.</p><p data-flatplan-paragraph="true"><b>David:</b> It is remarkable that this would happen with any reporter. It’s even more remarkable that this would happen with someone like you, and with a publication that has been specifically singled out by the president. Do you have any sense of how this happened?</p><p data-flatplan-paragraph="true"><b>Jeffrey: </b>I literally have no idea. The remarkable thing is that no one in the group asked, <i>Who’s JG?</i>, and when I removed myself from the group, seemingly nobody said, <i>Hey, why did JG leave?</i></p><p data-flatplan-paragraph="true"><b>David:</b> Are you concerned about retaliation from the Trump administration because of this story?</p><p data-flatplan-paragraph="true"><b>Jeffrey: </b>It’s not my role to care about the possibility of threats or retaliation. We just have to come to work and do our jobs to the best of our ability. Unfortunately, in our society today—we see this across corporate journalism and law firms and other industries—there’s too much preemptive obeying for my taste. All we can do is just go do our jobs.</p><p data-flatplan-paragraph="true"><a data-event-element="inline link" href="https://www.theatlantic.com/politics/archive/2025/03/trump-administration-accidentally-texted-me-its-war-plans/682151/"><b>Read the full article he</b></a><b><a data-event-element="inline link" href="https://www.theatlantic.com/politics/archive/2025/03/trump-administration-accidentally-texted-me-its-war-plans/682151/">re</a></b><a data-event-element="inline link" href="https://www.theatlantic.com/politics/archive/2025/03/trump-administration-accidentally-texted-me-its-war-plans/682151/"><b>.</b></a></p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[German parliament votes as a Git contribution graph (560 pts)]]></title>
            <link>https://abstimmung.eu/git/2024</link>
            <guid>43466509</guid>
            <pubDate>Mon, 24 Mar 2025 23:29:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abstimmung.eu/git/2024">https://abstimmung.eu/git/2024</a>, See on <a href="https://news.ycombinator.com/item?id=43466509">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><h2>17. Januar 2024</h2><hr></p><a href="https://abstimmung.eu/votes/53"><div data-slot="card"><div><h3 lang="de">Jahresbericht 2022 der Wehrbeauftragten (Entschließungsantrag)</h3><p><span data-slot="badge">Verteidigung</span><span data-slot="badge">Außenpolitik</span><span data-slot="badge">Ukraine</span><span data-slot="badge">Bundeswehr</span><span data-slot="badge">Militär</span></p><p>Die Abstimmung betrifft den Entschließungsantrag der CDU/CSU-Fraktion zum Jahresbericht 2022 der Wehrbeauftragten, der die Bundesregierung auffordert, einsatzbereite TAURUS-Marschflugkörper an die Ukraine zu liefern und diese unverzüglich nachzubeschaffen.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>18. Januar 2024</h2><hr></p><a href="https://abstimmung.eu/votes/54"><div data-slot="card"><div><h3 lang="de">Entlastung der Landwirtschaft</h3><p><span data-slot="badge">Landwirtschaft</span><span data-slot="badge">Wirtschaft</span><span data-slot="badge">Steuerpolitik</span><span data-slot="badge">Agrarpolitik</span><span data-slot="badge">Innovation</span><span data-slot="badge">Bürokratie</span></p><p>Der Antrag der CDU/CSU-Fraktion betrifft die Unterstützung der Landwirtschaft und zielt darauf ab, die Wettbewerbsfähigkeit der Landwirtschaft zu erhalten und zu verbessern, anstatt sie durch politische Maßnahmen zu schwächen. Der Antrag fordert die Bundesregierung auf, Maßnahmen zu ergreifen, um die Landwirtschaft zu fördern und zu entlasten.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/55"><div data-slot="card"><div><h3 lang="de">Friedensinitiative für die Ukraine und Russland (Beschlussempfehlung)</h3><p><span data-slot="badge">Außenpolitik</span><span data-slot="badge">Sicherheitspolitik</span><span data-slot="badge">Friedenspolitik</span><span data-slot="badge">Ukraine</span><span data-slot="badge">Russland</span></p><p>Die Abstimmung betrifft einen Antrag der AfD-Fraktion, der eine Friedensinitiative mit Sicherheitsgarantien für die Ukraine und Russland fordert, um Deutschlands Verantwortung für den Frieden in Europa gerecht zu werden.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>19. Januar 2024</h2><hr></p><a href="https://abstimmung.eu/votes/56"><div data-slot="card"><div><h3 lang="de">Modernisierung des Staatsangehörigkeitsrechts (GesEntw BReg)</h3><p><span data-slot="badge">Staatsangehörigkeit</span><span data-slot="badge">Integration</span><span data-slot="badge">Recht</span><span data-slot="badge">Gesellschaft</span><span data-slot="badge">Migration</span></p><p>Die Abstimmung betrifft die Modernisierung des Staatsangehörigkeitsrechts (StARModG), um den Zugang zur deutschen Staatsangehörigkeit zu erleichtern und die Integration von Ausländern zu fördern.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>01. Februar 2024</h2><hr></p><a href="https://abstimmung.eu/votes/57"><div data-slot="card"><div><h3 lang="de">Änderung des Bundeswahlgesetzes</h3><p><span data-slot="badge">Wahlrecht</span><span data-slot="badge">Innenpolitik</span><span data-slot="badge">Bevölkerungsentwicklung</span><span data-slot="badge">Kommunalpolitik</span><span data-slot="badge">Gesetzgebung</span></p><p>Die Abstimmung betrifft die Änderung des Bundeswahlgesetzes, um die Wahlkreiseinteilung an die aktuelle Bevölkerungsentwicklung anzupassen und kommunale Gebietsänderungen zu berücksichtigen. Ziel ist es, sicherzustellen, dass die Wahl zum Deutschen Bundestag auf einer fairen und gleichmäßigen Grundlage stattfindet.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>22. Februar 2024</h2><hr></p><a href="https://abstimmung.eu/votes/58"><div data-slot="card"><div><h3 lang="de">Zehn Jahre russischer Krieg gegen die Ukraine</h3><p><span data-slot="badge">Ukraine</span><span data-slot="badge">Russland</span><span data-slot="badge">Sicherheitspolitik</span><span data-slot="badge">Außenpolitik</span><span data-slot="badge">Militär</span><span data-slot="badge">Völkerrecht</span></p><p>Der Antrag betrifft die Unterstützung der Ukraine angesichts des russischen Krieges und zielt darauf ab, die deutsche und europäische Sicherheit zu stärken sowie die Verantwortlichen für Kriegsverbrechen zur Rechenschaft zu ziehen.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/60"><div data-slot="card"><div><h3 lang="de">Deutsche Außen- und Sicherheitspolitik (Antrag CDU/CSU)</h3><p><span data-slot="badge">Außenpolitik</span><span data-slot="badge">Sicherheitspolitik</span><span data-slot="badge">Verteidigung</span><span data-slot="badge">Internationale Beziehungen</span><span data-slot="badge">Bevölkerungsschutz</span></p><p>Der Antrag der CDU/CSU betrifft eine Neuausrichtung der deutschen Außen- und Sicherheitspolitik, um auf die veränderte Bedrohungslage durch Russland zu reagieren und Deutschlands Rolle in Europa und der Welt zu stärken. Er fordert eine "echte Zeitenwende" mit konkreten Maßnahmen zur Stärkung der Verteidigungsfähigkeit, des Bevölkerungsschutzes und der internationalen Zusammenarbeit.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>23. Februar 2024</h2><hr></p><a href="https://abstimmung.eu/votes/59"><div data-slot="card"><div><h3 lang="de">Wachstumschancengesetz;Kfz-Haftpflichtversicherung (BeschlEmpf Vermittlungsausschuss)</h3><p><span data-slot="badge">Wirtschaft</span><span data-slot="badge">Steuerrecht</span><span data-slot="badge">Investition</span><span data-slot="badge">Innovation</span><span data-slot="badge">Finanzen</span></p><p>Die Beschlussempfehlung des Vermittlungsausschusses betrifft das Gesetz zur Stärkung von Wachstumschancen, Investitionen und Innovation sowie Steuervereinfachung und Steuerfairness (Wachstumschancengesetz). Sie zielt darauf ab, Änderungen an diesem Gesetz vorzunehmen, um Wachstum, Investitionen und Innovation zu fördern sowie das Steuersystem zu vereinfachen und fairer zu gestalten.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/61"><div data-slot="card"><div><h3 lang="de">Cannabisgesetz (GesEntwurf BReg)</h3><p><span data-slot="badge">Gesundheit</span><span data-slot="badge">Drogenpolitik</span><span data-slot="badge">Jugendschutz</span><span data-slot="badge">Justiz</span><span data-slot="badge">Sicherheit</span></p><p>Das Cannabisgesetz regelt den kontrollierten Umgang mit Cannabis und ändert weitere Vorschriften. Es zielt darauf ab, den Gesundheitsschutz zu verbessern, den illegalen Markt einzudämmen und den Kinder- und Jugendschutz zu stärken.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/62"><div data-slot="card"><div><h3 lang="de">Bundeswehreinsatz EUNAVFOR ASPIDES (BeschlEmpf)</h3><p><span data-slot="badge">Außenpolitik</span><span data-slot="badge">Sicherheitspolitik</span><span data-slot="badge">Europäische Union</span><span data-slot="badge">Militär</span><span data-slot="badge">Internationaler Handel</span></p><p>Die Abstimmung betrifft die Beteiligung bewaffneter deutscher Streitkräfte an der von der Europäischen Union geführten Operation EUNAVFOR ASPIDES, die zum Schutz der Schifffahrt im Roten Meer und angrenzenden Gebieten beitragen soll. Der Bundestag stimmt über den Antrag der Bundesregierung zur Entsendung von bis zu 700 Soldaten bis maximal zum 28. Februar 2025 ab.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>14. März 2024</h2><hr></p><a href="https://abstimmung.eu/votes/63"><div data-slot="card"><div><h3 lang="de">Ablehnung von Lieferung des Taurus-Marschflugkörpers (Beschlussempfehlung)</h3><p><span data-slot="badge">Außenpolitik</span><span data-slot="badge">Verteidigung</span><span data-slot="badge">Sicherheitspolitik</span><span data-slot="badge">Ukraine</span><span data-slot="badge">Militär</span></p><p>Die Abstimmung betrifft den Antrag der CDU/CSU, die Bundesregierung aufzufordern, der Ukraine Taurus-Marschflugkörper aus Bundeswehrbeständen zu liefern, die Integration des Waffensystems in ukrainische Flugzeuge zu unterstützen, ukrainische Soldaten auszubilden, Hemmnisse bei der Weitergabe sensibler Informationen zu beseitigen, Ausrüstungslücken bei der Bundeswehr durch Nachbeschaffung zu schließen, die Produktionskapazitäten der Industrie zu erhöhen und weitere Taurus-Marschflugkörper zu beschaffen. Der Antrag wird abgelehnt.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>20. März 2024</h2><hr></p><a href="https://abstimmung.eu/votes/64"><div data-slot="card"><div><h3 lang="de">Steuervergünstigung für Agrardiesel (Antrag AfD)</h3><p><span data-slot="badge">Landwirtschaft</span><span data-slot="badge">Steuerpolitik</span><span data-slot="badge">Subventionen</span><span data-slot="badge">Haushaltspolitik</span><span data-slot="badge">Agrardiesel</span></p><p>Die Abstimmung betrifft den Antrag der AfD-Fraktion, die Steuervergünstigung für Agrardiesel beizubehalten, um die deutsche Landwirtschaft nicht zusätzlich zu belasten und die Lebensmittelpreise nicht künstlich zu verteuern. Der Antrag sieht vor, Kürzungen in anderen Bereichen des Haushalts vorzunehmen, falls Einsparungen notwendig sind, und die Bundesregierung aufzufordern, Beschlüsse der Parlamentsgremien zu respektieren.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>21. März 2024</h2><hr></p><a href="https://abstimmung.eu/votes/65"><div data-slot="card"><div><h3 lang="de">Bundeswehreinsatz SEA GUARDIAN im Mittelmeer (Antrag BReg)</h3><p><span data-slot="badge">Sicherheitspolitik</span><span data-slot="badge">Militär</span><span data-slot="badge">NATO</span><span data-slot="badge">Mittelmeer</span><span data-slot="badge">Terrorismusbekämpfung</span><span data-slot="badge">Internationale Beziehungen</span></p><p>Die Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der NATO-geführten maritimen Sicherheitsoperation SEA GUARDIAN (MSO SG) im Mittelmeer, um zur Sicherheit und Stabilität in der Region beizutragen. Der Deutsche Bundestag stimmt über den Antrag der Bundesregierung zur Verlängerung dieses Einsatzes ab.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/66"><div data-slot="card"><div><h3 lang="de">Bundeswehreinsatz in der Republik Südsudan (UNMISS)</h3><p><span data-slot="badge">Außenpolitik</span><span data-slot="badge">Sicherheitspolitik</span><span data-slot="badge">Verteidigung</span><span data-slot="badge">Humanitäre Hilfe</span><span data-slot="badge">Friedensmission</span><span data-slot="badge">UN</span></p><p>Die Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der UN-Friedensmission UNMISS (United Nations Mission in South Sudan) in der Republik Südsudan. Es geht darum, ob der Bundestag der von der Bundesregierung beschlossenen Verlängerung des Einsatzes zustimmt.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>12. April 2024</h2><hr></p><a href="https://abstimmung.eu/votes/67"><div data-slot="card"><div><h3 lang="de">Selbstbestimmung in Bezug auf den Geschlechtseintrag</h3><p><span data-slot="badge">Gesellschaftspolitik</span><span data-slot="badge">Gleichstellung</span><span data-slot="badge">Selbstbestimmung</span><span data-slot="badge">Transgender</span><span data-slot="badge">Recht</span></p><p>Die Abstimmung betrifft den Entwurf eines Gesetzes über die Selbstbestimmung in Bezug auf den Geschlechtseintrag und zur Änderung weiterer Vorschriften. Das Gesetz zielt darauf ab, die Regelungen zur Änderung des Geschlechtseintrags zu vereinfachen und zu vereinheitlichen.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/68"><div data-slot="card"><div><h3 lang="de">Änderung des Energiewirtschaftsgesetzes</h3><p><span data-slot="badge">Energiewirtschaft</span><span data-slot="badge">Wasserstoff</span><span data-slot="badge">Infrastruktur</span><span data-slot="badge">Netzentwicklung</span><span data-slot="badge">Finanzierung</span></p><p>Die Abstimmung betrifft ein Gesetz zur Änderung des Energiewirtschaftsgesetzes (EnWG), um den Aufbau einer nationalen Wasserstoffinfrastruktur zu fördern und die Finanzierung des Wasserstoff-Kernnetzes zu regeln.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>25. April 2024</h2><hr></p><a href="https://abstimmung.eu/votes/141"><div data-slot="card"><div><h3 lang="de">Operation EUNAVFOR MED IRINI (Beschlussempfehlung)</h3><p><span data-slot="badge">Außenpolitik</span><span data-slot="badge">Sicherheitspolitik</span><span data-slot="badge">Europäische Union</span><span data-slot="badge">Militär</span><span data-slot="badge">Libyen</span></p><p>Die Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der Operation EUNAVFOR MED IRINI der Europäischen Union. Ziel ist es, das Waffenembargo gegen Libyen weiterhin durchzusetzen und zur Stabilisierung des Landes beizutragen.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>26. April 2024</h2><hr></p><a href="https://abstimmung.eu/votes/69"><div data-slot="card"><div><h3 lang="de">Erneuerbare-Energien-Gesetz (Solarpaket)</h3><p><span data-slot="badge">Erneuerbare Energien</span><span data-slot="badge">Photovoltaik</span><span data-slot="badge">Klimaschutz</span><span data-slot="badge">Energiewende</span><span data-slot="badge">Bürokratieabbau</span><span data-slot="badge">Netzausbau</span></p><p>Das Gesetz zur Änderung des Erneuerbare-Energien-Gesetzes (EEG) zielt darauf ab, den Ausbau der Photovoltaik (PV) in Deutschland zu beschleunigen und zu steigern, um die Klimaziele zu erreichen. Es werden Anreize für Solaranlagen geschaffen und Bürokratie abgebaut.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>16. Mai 2024</h2><hr></p><a href="https://abstimmung.eu/votes/70"><div data-slot="card"><div><h3 lang="de">Ablehnung eines Antrags gegen den WHO-Pandemievertrag</h3><p><span data-slot="badge">Gesundheitspolitik</span><span data-slot="badge">Internationale Beziehungen</span><span data-slot="badge">Souveränität</span><span data-slot="badge">WHO</span><span data-slot="badge">Pandemie</span><span data-slot="badge">Demokratie</span></p><p>Die Abstimmung betrifft den Antrag der AfD-Fraktion, den WHO-Pandemievertrag und die überarbeiteten Internationalen Gesundheitsvorschriften abzulehnen. Der Antrag zielt darauf ab, die Souveränität Deutschlands in Gesundheitsfragen zu wahren und eine vermeintliche Machtausweitung der WHO zu verhindern.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>05. Juni 2024</h2><hr></p><a href="https://abstimmung.eu/votes/71"><div data-slot="card"><div><h3 lang="de">Ablehnung eines Antrags zur Abschaffung des Solidaritätszuschlags</h3><p><span data-slot="badge">Steuerpolitik</span><span data-slot="badge">Finanzen</span><span data-slot="badge">Wirtschaft</span><span data-slot="badge">Solidaritätszuschlag</span><span data-slot="badge">Abgabenlast</span><span data-slot="badge">Steuerreform</span></p><p>Die Abstimmung betrifft den Antrag der AfD-Fraktion zur Abschaffung des Solidaritätszuschlags als ersten Schritt einer umfassenden Steuerreform zur Entlastung von Mittelstand, Unternehmen und Arbeitnehmern. Der Finanzausschuss empfiehlt die Ablehnung des Antrags.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>06. Juni 2024</h2><hr></p><a href="https://abstimmung.eu/votes/72"><div data-slot="card"><div><h3 lang="de">Ablehnung eines Antrags zur Bekämpfung des politischen Islam</h3><p><span data-slot="badge">Inneres und Heimat</span><span data-slot="badge">Politischer Islam</span><span data-slot="badge">Demokratie</span><span data-slot="badge">Recht</span><span data-slot="badge">Sicherheit</span><span data-slot="badge">Migration</span></p><p>Die Abstimmung betrifft den Antrag der CDU/CSU-Fraktion, den politischen Islam als Gefahr für die freiheitliche Demokratie wirksam zu bekämpfen. Der Ausschuss für Inneres und Heimat empfiehlt die Ablehnung des Antrags.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/73"><div data-slot="card"><div><h3 lang="de">Beschleunigung immissionsschutzrechtlicher Genehmigungsverfahren</h3><p><span data-slot="badge">Umwelt</span><span data-slot="badge">Klimaschutz</span><span data-slot="badge">Energie</span><span data-slot="badge">Wirtschaft</span><span data-slot="badge">Genehmigungsverfahren</span><span data-slot="badge">Erneuerbare Energien</span></p><p>Die Abstimmung betrifft ein Gesetz zur Verbesserung des Klimaschutzes, zur Beschleunigung von Genehmigungsverfahren im Immissionsschutz und zur Umsetzung von EU-Recht, mit dem Ziel, Verfahren zu vereinfachen und den Ausbau erneuerbarer Energien zu fördern.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>13. Juni 2024</h2><hr></p><a href="https://abstimmung.eu/votes/74"><div data-slot="card"><div><h3 lang="de">Antrag zu Konsequenzen aus dem Terror von Mannheim</h3><p><span data-slot="badge">Innere Sicherheit</span><span data-slot="badge">Migration</span><span data-slot="badge">Integration</span><span data-slot="badge">Terrorismusbekämpfung</span><span data-slot="badge">Rechtspolitik</span></p><p>Der Antrag der CDU/CSU-Fraktion betrifft die Forderung nach klaren Konsequenzen aus dem Terroranschlag von Mannheim und zielt darauf ab, die Migrations- und Integrationspolitik zu verschärfen sowie die Sicherheitsmaßnahmen zu erhöhen.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/75"><div data-slot="card"><div><h3 lang="de">Ablehnung eines Antrags zum Verbot des Vereins Muslim Interaktiv</h3><p><span data-slot="badge">Inneres und Heimat</span><span data-slot="badge">Vereinsrecht</span><span data-slot="badge">Extremismus</span><span data-slot="badge">Islamismus</span><span data-slot="badge">Sicherheitspolitik</span></p><p>Die Abstimmung betrifft den Antrag der AfD-Fraktion, den Verein Muslim Interaktiv zu verbieten, da dieser als extremistisch und gegen die freiheitliche demokratische Grundordnung gerichtet angesehen wird. Der Bundestag lehnt den Antrag ab.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/76"><div data-slot="card"><div><h3 lang="de">Antrag zum Eintritt in die zweite Beratung zur Aufhebung des Lieferkettengesetzes</h3><p><span data-slot="badge">Wirtschaft</span><span data-slot="badge">Recht</span><span data-slot="badge">Soziales</span><span data-slot="badge">Europäische Union</span></p><p>Die Abstimmung betrifft einen Antrag der CDU/CSU-Fraktion, direkt in die zweite Beratung eines Gesetzentwurfs zur Aufhebung des Lieferkettensorgfaltspflichtengesetzes (LkSG) einzutreten.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>27. Juni 2024</h2><hr></p><a href="https://abstimmung.eu/votes/77"><div data-slot="card"><div><h3 lang="de">Bundeswehreinsatz EUFOR ALTHEA</h3><p><span data-slot="badge">Außenpolitik</span><span data-slot="badge">Sicherheitspolitik</span><span data-slot="badge">Europäische Union</span><span data-slot="badge">Bundeswehr</span><span data-slot="badge">Bosnien und Herzegowina</span></p><p>Die Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der EU-geführten Sicherheitsoperation EUFOR ALTHEA in Bosnien und Herzegowina bis zum 30. Juni 2025.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/78"><div data-slot="card"><div><h3 lang="de">Bundeswehreinsatz im Kosovo (KFOR)</h3><p><span data-slot="badge">Außenpolitik</span><span data-slot="badge">Sicherheitspolitik</span><span data-slot="badge">Verteidigung</span><span data-slot="badge">Kosovo</span><span data-slot="badge">Bundeswehr</span><span data-slot="badge">Internationale Beziehungen</span></p><p>Die Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der internationalen Sicherheitspräsenz im Kosovo (KFOR). Es geht darum, das Mandat für den Einsatz der Bundeswehr im Rahmen von KFOR zu verlängern.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/79"><div data-slot="card"><div><h3 lang="de">Bundeswehreinsatz im Libanon (UNIFIL)</h3><p><span data-slot="badge">Außenpolitik</span><span data-slot="badge">Sicherheitspolitik</span><span data-slot="badge">Bundeswehr</span><span data-slot="badge">Libanon</span><span data-slot="badge">UNIFIL</span><span data-slot="badge">Friedensmission</span></p><p>Die Abstimmung betrifft die Fortsetzung der Beteiligung bewaffneter deutscher Streitkräfte an der "United Nations Interim Force in Lebanon" (UNIFIL), einer Friedensmission der Vereinten Nationen im Libanon. Es geht um die Verlängerung des Mandats und die damit verbundenen Aufgaben und Befugnisse der deutschen Soldaten.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>05. Juli 2024</h2><hr></p><a href="https://abstimmung.eu/votes/80"><div data-slot="card"><div><h3 lang="de">Änderung des Schwangerschaftskonfliktgesetzes</h3><p><span data-slot="badge">Soziales</span><span data-slot="badge">Familie</span><span data-slot="badge">Frauenrechte</span><span data-slot="badge">Gesundheit</span><span data-slot="badge">Recht</span></p><p>Die Abstimmung betrifft die Änderung des Schwangerschaftskonfliktgesetzes (SchKG) und zielt darauf ab, einen bundeseinheitlichen und rechtssicheren Umgang mit sogenannten Gehsteigbelästigungen vor Schwangerschaftsberatungsstellen und Einrichtungen zur Vornahme von Schwangerschaftsabbrüchen sicherzustellen.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>17. Oktober 2024</h2><hr></p><a href="https://abstimmung.eu/votes/81"><div data-slot="card"><div><h3 lang="de">Entwurf zur Aufhebung des Lieferkettengesetzes</h3><p><span data-slot="badge">Wirtschaft</span><span data-slot="badge">Menschenrechte</span><span data-slot="badge">Umwelt</span><span data-slot="badge">Lieferketten</span><span data-slot="badge">Gesetzgebung</span><span data-slot="badge">Europäische Union</span></p><p>Die Abstimmung betrifft den Gesetzentwurf der CDU/CSU zur Aufhebung des Lieferkettensorgfaltspflichtengesetzes (LkSG), das Unternehmen zur Einhaltung von Menschenrechten in ihren Lieferketten verpflichtet.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/82"><div data-slot="card"><div><h3 lang="de">Gesetzentwurf zur Krankenhausreform</h3><p><span data-slot="badge">Gesundheit</span><span data-slot="badge">Krankenhaus</span><span data-slot="badge">Versorgung</span><span data-slot="badge">Finanzierung</span><span data-slot="badge">Qualitätssicherung</span></p><p>Die Abstimmung betrifft den Entwurf eines Gesetzes zur Verbesserung der Versorgungsqualität im Krankenhaus und zur Reform der Vergütungsstrukturen (KHVVG), das darauf abzielt, die Krankenhausversorgung zu verbessern und die Finanzierung zu reformieren.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/83"><div data-slot="card"><div><h3 lang="de">Bundeswehreinsatz im Irak</h3><p><span data-slot="badge">Außenpolitik</span><span data-slot="badge">Sicherheitspolitik</span><span data-slot="badge">Militär</span><span data-slot="badge">Irak</span><span data-slot="badge">Terrorismusbekämpfung</span></p><p>Die Abstimmung betrifft die Fortsetzung des Einsatzes bewaffneter deutscher Streitkräfte im Irak, um die Stabilisierung des Landes zu sichern, ein Wiedererstarken des IS (Islamischer Staat) zu verhindern und die Versöhnung im Irak zu fördern. Der Bundestag stimmt über den Antrag der Bundesregierung ab, diesen Einsatz bis zum 31. Januar 2026 mit bis zu 500 Soldatinnen und Soldaten zu verlängern.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>18. Oktober 2024</h2><hr></p><a href="https://abstimmung.eu/votes/84"><div data-slot="card"><div><h3 lang="de">Änderungsantrag zur „Verbesserung der inneren Sicherheit“</h3><p><span data-slot="badge">Innere Sicherheit</span><span data-slot="badge">Asylrecht</span><span data-slot="badge">Migrationspolitik</span><span data-slot="badge">Grenzschutz</span><span data-slot="badge">Terrorismusbekämpfung</span><span data-slot="badge">Waffenrecht</span></p><p>Die Abstimmung betrifft einen Änderungsantrag der CDU/CSU-Fraktion zu einem Gesetzentwurf, der die innere Sicherheit und das Asylsystem verbessern soll, insbesondere durch Änderungen im Asylgesetz und im Ausweisungsrecht.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/85"><div data-slot="card"><div><h3 lang="de">Artikel 5 des Entwurfs „Verbesserung der inneren Sicherheit“ (2. Beratung)</h3><p><span data-slot="badge">Inneres</span><span data-slot="badge">Sicherheit</span><span data-slot="badge">Waffenrecht</span><span data-slot="badge">Extremismus</span><span data-slot="badge">Terrorismus</span><span data-slot="badge">Öffentliche Sicherheit</span></p><p>Die Abstimmung betrifft Artikel 5 eines Gesetzentwurfs, der darauf abzielt, das Waffenrecht zu verschärfen, um Extremisten und Terroristen den Zugang zu Waffen zu erschweren und die öffentliche Sicherheit zu erhöhen.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/86"><div data-slot="card"><div><h3 lang="de">Gesetzentwurf „Verbesserung der Terrorismusbekämpfung“</h3><p><span data-slot="badge">Innere Sicherheit</span><span data-slot="badge">Terrorismusbekämpfung</span><span data-slot="badge">Extremismusprävention</span><span data-slot="badge">Datenschutz</span><span data-slot="badge">Bürgerrechte</span><span data-slot="badge">Waffenrecht</span></p><p>Der Gesetzentwurf zur Verbesserung der Terrorismusbekämpfung zielt darauf ab, den Sicherheitsbehörden im digitalen Raum zusätzliche Befugnisse zu geben und die Zusammenarbeit zwischen Bund, Ländern und zivilgesellschaftlichen Akteuren zu stärken, um extremistischen und islamistischen Tendenzen entgegenzuwirken.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/87"><div data-slot="card"><div><h3 lang="de">Gesetzentwurf „Verbesserung der inneren Sicherheit“</h3><p><span data-slot="badge">Innere Sicherheit</span><span data-slot="badge">Asylrecht</span><span data-slot="badge">Waffenrecht</span><span data-slot="badge">Extremismus</span><span data-slot="badge">Terrorismusbekämpfung</span></p><p>Der Gesetzentwurf zur Verbesserung der inneren Sicherheit und des Asylsystems betrifft die Anpassung von Gesetzen, um die innere Sicherheit Deutschlands zu stärken und das Asylsystem effektiver zu gestalten. Er reagiert auf aktuelle Bedrohungen und soll den Behörden mehr Handlungsspielraum geben.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/88"><div data-slot="card"><div><h3 lang="de">Erster Entschließungsantrag zur „Verbesserung der inneren Sicherheit“</h3><p><span data-slot="badge">Innere Sicherheit</span><span data-slot="badge">Asylpolitik</span><span data-slot="badge">Migrationspolitik</span><span data-slot="badge">Waffenrecht</span><span data-slot="badge">Islamismus</span><span data-slot="badge">Extremismusbekämpfung</span></p><p>Die Abstimmung betrifft den Entschließungsantrag der CDU/CSU zum Gesetzentwurf zur Verbesserung der inneren Sicherheit und des Asylsystems, der sich mit der Migrationspolitik, dem Waffenrecht und der Bekämpfung des Islamismus auseinandersetzt.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/89"><div data-slot="card"><div><h3 lang="de">Zweiter Entschließungsantrag zur „Verbesserung der inneren Sicherheit“</h3><p><span data-slot="badge">Innere Sicherheit</span><span data-slot="badge">Asylpolitik</span><span data-slot="badge">Waffenrecht</span><span data-slot="badge">Migrationspolitik</span><span data-slot="badge">Extremismusprävention</span></p><p>Die Abstimmung betrifft einen Entschließungsantrag der CDU/CSU zum Gesetzentwurf zur Verbesserung der inneren Sicherheit und des Asylsystems, der sich gegen die Schwerpunkte der Regierungsfraktionen im Bereich Waffenrecht richtet und stattdessen eine andere Migrations- und Sicherheitspolitik fordert.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/90"><div data-slot="card"><div><h3 lang="de">Dritter Entschließungsantrag zur „Verbesserung der inneren Sicherheit“</h3><p><span data-slot="badge">Innere Sicherheit</span><span data-slot="badge">Asylrecht</span><span data-slot="badge">Migration</span><span data-slot="badge">Grenzschutz</span><span data-slot="badge">EU-Recht</span></p><p>Die Abstimmung betrifft einen Entschließungsantrag der CDU/CSU-Fraktion zu einem Gesetzentwurf, der die innere Sicherheit und das Asylsystem verbessern soll. Der Antrag zielt darauf ab, die Bundesregierung zu umfassenden Zurückweisungen von Personen ohne Einreiserecht an den deutschen Grenzen aufzufordern.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/91"><div data-slot="card"><div><h3 lang="de">Ablehnung eines Antrags zu Messerangriffen</h3><p><span data-slot="badge">Inneres</span><span data-slot="badge">Recht</span><span data-slot="badge">Kriminalität</span><span data-slot="badge">Waffenrecht</span><span data-slot="badge">Migration</span><span data-slot="badge">Sicherheit</span></p><p>Die Beschlussempfehlung des Ausschusses für Inneres und Heimat betrifft die Ablehnung eines Antrags der AfD-Fraktion, der eine gezielte Sanktionierung von Messerangriffen anstelle von Verschärfungen im Waffenrecht fordert.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><a href="https://abstimmung.eu/votes/92"><div data-slot="card"><div><h3 lang="de">Ablehnung eines Antrags zu Drittstaatenangehörigen</h3><p><span data-slot="badge">Asylrecht</span><span data-slot="badge">Migrationspolitik</span><span data-slot="badge">Grenzkontrolle</span><span data-slot="badge">Rücknahmeabkommen</span></p><p>Die Abstimmung betrifft den Antrag der AfD-Fraktion, die Bundesregierung zu verpflichten, Drittstaatsangehörige an den Außengrenzen Deutschlands zurückzuweisen und bilaterale Rücknahmeabkommen mit Nachbarstaaten anzuwenden.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>16. Dezember 2024</h2><hr></p><a href="https://abstimmung.eu/votes/93"><div data-slot="card"><div><h3 lang="de">Antrag gemäß Artikel 68 des Grundgesetzes (Vertrauensfrage)</h3><p><span data-slot="badge">Politik</span><span data-slot="badge">Regierung</span><span data-slot="badge">Grundgesetz</span><span data-slot="badge">Vertrauensfrage</span></p><p>Die Abstimmung betrifft die Vertrauensfrage gemäß Artikel 68 des Grundgesetzes, bei der der Bundeskanzler das Vertrauen des Bundestages ausspricht. Der Bundeskanzler beabsichtigt, vor der Abstimmung eine Erklärung abzugeben.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a><p><h2>19. Dezember 2024</h2><hr></p><a href="https://abstimmung.eu/votes/94"><div data-slot="card"><div><h3 lang="de">Gesetzentwurf zur Änderung von Artikel 93 und 94 des Grundgesetzes (Bundesverfassungsgericht)</h3><p><span data-slot="badge">Verfassungsrecht</span><span data-slot="badge">Bundesverfassungsgericht</span><span data-slot="badge">Rechtsstaat</span><span data-slot="badge">Politik</span><span data-slot="badge">Justiz</span></p><p>Der Gesetzentwurf betrifft die Änderung des Grundgesetzes in Bezug auf die Stellung und Struktur des Bundesverfassungsgerichts, indem Artikel 93 und 94 punktuell ergänzt und deren Inhalte systematisch neu geordnet werden. Ziel ist es, die Organqualität des Bundesverfassungsgerichts sowie die unmittelbare Bindung der öffentlichen Gewalt an seine Entscheidungen stärker hervorzuheben.</p></div><div><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 3v16a2 2 0 0 0 2 2h16"></path><path d="M18 17V9"></path><path d="M13 17V5"></path><path d="M8 17v-3"></path></svg><h4>Ergebnisse des Bundestags</h4></p></div></div></a></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Three Hundred Years Later, a Tool from Isaac Newton Gets an Update (144 pts)]]></title>
            <link>https://www.quantamagazine.org/three-hundred-years-later-a-tool-from-isaac-newton-gets-an-update-20250324/</link>
            <guid>43465971</guid>
            <pubDate>Mon, 24 Mar 2025 22:16:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/three-hundred-years-later-a-tool-from-isaac-newton-gets-an-update-20250324/">https://www.quantamagazine.org/three-hundred-years-later-a-tool-from-isaac-newton-gets-an-update-20250324/</a>, See on <a href="https://news.ycombinator.com/item?id=43465971">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-role="selectable">
    <p>Every day, researchers search for optimal solutions. They might want to figure out where to build a major airline hub. Or to determine how to maximize return while minimizing risk in an investment portfolio. Or to develop self-driving cars that can distinguish between traffic lights and stop signs.</p>
<p>Mathematically, these problems get translated into a search for the minimum values of functions. But in all these scenarios, the functions are too complicated to assess directly. Researchers have to approximate the minimal values instead.</p>
<p>It turns out that one of the best ways to do this is by using an algorithm that Isaac Newton developed over 300 years ago. This algorithm is fairly simple. It’s a little like searching, blindfolded, for the lowest point in an unfamiliar landscape. As you put one foot in front of the other, the only information you need is whether you’re going uphill or downhill, and whether the grade is increasing or decreasing. Using that information, you can get a good approximation of the minimum relatively quickly.</p>
<p>Although enormously powerful — centuries later, Newton’s method is still crucial for solving present-day problems in logistics, finance, computer vision and even pure math — it also has a significant shortcoming. It doesn’t work well on all functions. So mathematicians have continued to study the technique, figuring out different ways to broaden its scope without sacrificing efficiency.</p>
<p>Last summer, three researchers <a href="https://arxiv.org/abs/2311.06374">announced the latest improvement</a> to Newton’s method. <a href="https://aaa.princeton.edu/">Amir Ali Ahmadi</a> of Princeton University, along with his former students <a href="https://chaudhrya.github.io/">Abraar Chaudhry</a> (now at the Georgia Institute of Technology) and <a href="https://medicine.yale.edu/profile/jeffrey-zhang/">Jeffrey Zhang</a> (now at Yale University), extended Newton’s method to work efficiently on the broadest class of functions yet.</p>
<p>“Newton’s method has 1,000 different applications in optimization,” Ahmadi said. “Potentially our algorithm can replace it.”</p>
<figure>
    <p><img width="2126" height="2560" src="https://www.quantamagazine.org/wp-content/uploads/2025/03/IsaacNewton_crGodfreyKneller_Public-Domain-scaled.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/03/IsaacNewton_crGodfreyKneller_Public-Domain-scaled.webp 2126w, https://www.quantamagazine.org/wp-content/uploads/2025/03/IsaacNewton_crGodfreyKneller_Public-Domain-1428x1720.webp 1428w, https://www.quantamagazine.org/wp-content/uploads/2025/03/IsaacNewton_crGodfreyKneller_Public-Domain-432x520.webp 432w, https://www.quantamagazine.org/wp-content/uploads/2025/03/IsaacNewton_crGodfreyKneller_Public-Domain-768x925.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/03/IsaacNewton_crGodfreyKneller_Public-Domain-1275x1536.webp 1275w, https://www.quantamagazine.org/wp-content/uploads/2025/03/IsaacNewton_crGodfreyKneller_Public-Domain-1701x2048.webp 1701w" sizes="(max-width: 2126px) 100vw, 2126px">    </p>
            <figcaption>
                            <p>In the 1680s, Isaac Newton developed an algorithm for finding optimal solutions. Three centuries later, mathematicians are still using and honing his method.</p>
            <p>Godfrey Kneller/Public Domain</p>
        </figcaption>
    </figure>

<h2><strong>A Centuries-Old Technique</strong></h2>
<p>Mathematical functions transform inputs into outputs. Often, the most important feature of a function is its minimum value — the combination of inputs that produces the smallest possible output.</p>
<p>But finding the minimum is hard. Functions can have dozens of variables raised to high powers, defying formulaic analysis; graphs of their solutions form high-dimensional landscapes that are impossible to explore from a bird’s-eye view. In those higher-dimensional landscapes, said <a href="https://www.maths.ox.ac.uk/people/coralia.cartis">Coralia Cartis</a> of the University of Oxford, “We want to find a valley. Some are local valleys; others are the lowest point. You’re trying to find these things, and the question is: What info do you have to guide you to that?”</p>
<p>In the 1680s, Newton recognized that even when you’re dealing with a very complicated function, you’ll still always have access to at least two pieces of information to help you find its deepest valley. First, you can calculate the function’s so-called first derivative, or slope: the steepness of the function at a given point. Second, you can compute the rate at which the slope itself is changing (the function’s second derivative).</p>
<figure>
    <p><img width="1429" height="897" src="https://www.quantamagazine.org/wp-content/uploads/2025/03/Amir-Ali-Ahmadi_cr-Archives-of-the-Mathematisches-Forschungsinstitut-Oberwolfach.jpg" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/03/Amir-Ali-Ahmadi_cr-Archives-of-the-Mathematisches-Forschungsinstitut-Oberwolfach.jpg 1429w, https://www.quantamagazine.org/wp-content/uploads/2025/03/Amir-Ali-Ahmadi_cr-Archives-of-the-Mathematisches-Forschungsinstitut-Oberwolfach-520x326.jpg 520w, https://www.quantamagazine.org/wp-content/uploads/2025/03/Amir-Ali-Ahmadi_cr-Archives-of-the-Mathematisches-Forschungsinstitut-Oberwolfach-768x482.jpg 768w" sizes="(max-width: 1429px) 100vw, 1429px">    </p>
            <figcaption>
                            <p>Amir Ali Ahmadi sees optimization problems everywhere he looks.</p>
            <p>Archives of the Mathematisches Forschungsinstitut Oberwolfach</p>
        </figcaption>
    </figure>

<p>Say you’re trying to find the minimum of some complicated function. First, choose a point on the function that you think might be close to the true minimum. Compute the function’s first and second derivatives at that point. These derivatives can be used to construct a special quadratic equation — a parabola if your function lives in a 2D plane, and a cuplike shape called a paraboloid if your function is higher dimensional. This quadratic equation, which is called a Taylor approximation, roughly resembles your function at the point you chose.</p>
<p>Now calculate the minimum of the quadratic equation instead of the original — something you can do easily, using a well-known formula. (That’s because quadratic equations are simple; it’s when equations get more complicated that calculating the minimum becomes prohibitive.) You’ll get a point. Then plug the coordinates of that point back into your original function, and you’ll get a new point on the function that is, hopefully, closer to its true minimum. Start the entire process again.</p>
<p>Newton proved that if you keep on repeating this process, you’ll eventually home in on the minimum value of the original, more complicated function. The method doesn’t always work, especially if you start at a point that’s too far away from the true minimum. But for the most part, it does. And it has some desirable attributes.</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/03/Improving_Newtons_Method_crMarkBelan-Desktopv1.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2025/03/Improving_Newtons_Method_crMarkBelan-Mobilev1.svg" alt="" decoding="async">    </p>
            <figcaption>
            <div>
    <p>Mark Belan/<i data-stringify-type="italic">Quanta Magazine;&nbsp;</i>Source:&nbsp;<a href="https://arxiv.org/abs/2305.07512" target="_blank" rel="noopener noreferrer" data-stringify-link="https://arxiv.org/abs/2305.07512" data-sk="tooltip_parent">arxiv:2305.07512</a></p>
</div>
        </figcaption>
    </figure>

<p>Other iterative methods, like gradient descent — the algorithm used in today’s machine learning models — converge toward the true minimum at a linear rate. Newton’s method converges toward it much faster: at a “quadratic” rate. In other words, it can identify the minimum value in fewer iterations than gradient descent. (Each iteration of Newton’s method is more computationally expensive than an iteration of gradient descent, which is why researchers prefer gradient descent for certain applications, like training neural networks. But Newton’s method is still enormously efficient, making it useful in all sorts of contexts.)</p>
<p>Newton could have written his method to converge toward the true minimum value even faster if, instead of taking just the first and second derivatives at each point, he had also taken, say, the third and fourth derivatives. That would have given him more complicated Taylor approximations, with exponents greater than 2. But the whole crux of his strategy was to transform a complicated function into a simpler one. These more complicated Taylor equations were more than Newton could handle mathematically.</p>

<p>“Newton did it for degree 2. He did that because nobody knew how to minimize higher-order polynomials,” Ahmadi said.</p>
<p>In the centuries since, mathematicians have worked to extend his method, to probe how much information they can squeeze out of more complicated Taylor approximations of their functions.</p>
<p>In the 19th century, for instance, the Russian mathematician Pafnuty Chebyshev proposed a version of Newton’s method that approximated functions with cubic equations (which have an exponent of 3). But his algorithm didn’t work when the original function involved multiple variables. Much more recently, in 2021, Yurii Nesterov (now at Corvinus University of Budapest) demonstrated <a href="https://link.springer.com/article/10.1007/s10107-019-01449-1">how to approximate functions</a> of any number of variables efficiently with cubic equations. But his method couldn’t be extended to approximate functions using quartic equations, quintics and so on without losing its efficiency. Nevertheless, the proof was a major breakthrough in the field.</p>
<p>Now Ahmadi, Chaudhry and Zhang have taken Nesterov’s result another step further. Their algorithm works for any number of variables and arbitrarily many derivatives. Moreover, it remains efficient for all these cases — something that until now wasn’t possible.</p>
<p>But first, they had to find a way to make a hard math problem a lot easier.</p>
<h2><strong>Finding Wiggle Room</strong></h2>
<p>There is no fast, general purpose method for finding the minima of functions raised to high exponents. That’s always been the main limitation of Newton’s method. But there are certain types of functions that have characteristics that make them easy to minimize. In the new work, Ahmadi, Chaudhry and Zhang prove that it’s always possible to find approximating equations that have these characteristics. They then show how to adapt these equations to run Newton’s method efficiently.</p>
<p>What properties make an equation easy to minimize? Two things: The first is that the equation should be bowl-shaped, or “convex.” Rather than having many valleys, it has just one — meaning that when you try to minimize it, you don’t have to worry about mistaking an arbitrary valley for the lowest one.</p>
<figure>
    <p><img width="1600" height="1059" src="https://www.quantamagazine.org/wp-content/uploads/2025/03/Abraar-Chaudhry_crCamille-Carpenter-Henriquez.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/03/Abraar-Chaudhry_crCamille-Carpenter-Henriquez.webp 1600w, https://www.quantamagazine.org/wp-content/uploads/2025/03/Abraar-Chaudhry_crCamille-Carpenter-Henriquez-520x344.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/03/Abraar-Chaudhry_crCamille-Carpenter-Henriquez-768x508.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/03/Abraar-Chaudhry_crCamille-Carpenter-Henriquez-1536x1017.webp 1536w" sizes="(max-width: 1600px) 100vw, 1600px">    </p>
            <figcaption>
                            <p>Abraar Chaudhry and two colleagues recently found a way to improve a centuries-old method for finding the minima of functions.</p>
            <p>Camille Carpenter Henriquez</p>
        </figcaption>
    </figure>

<p>The second property is that the equation can be written as a sum of squares. For example, 5<em>x</em><sup>2</sup> + 16<em>x</em> + 13 can be written as the sum (<em>x</em> + 2)<sup>2</sup> + (2<em>x</em> + 3)<sup>2</sup>. In recent years, mathematicians have developed techniques for minimizing equations with arbitrarily large exponents so long as they are both convex and a sum of squares. However, those techniques were of little help when it came to Newton’s method. Most of the time, the Taylor approximation you use won’t have these nice properties.</p>
<p>But Ahmadi, Chaudhry and Zhang figured out how to use a technique called semidefinite programming to wiggle the Taylor approximation just enough to make it both a sum of squares and convex, though not so much that it became unmoored from the original function it was supposed to resemble.</p>
<p>They essentially added a fudge factor to the Taylor expansion, turning it into an equation that had the two desired properties. “We can change the Taylor expansion a bit to make it simpler to minimize. Think of the Taylor expansion, but modified a little bit,” Ahmadi said. He and his colleagues then showed that, using this modified version of the Taylor expansion — which involved arbitrarily many derivatives — their algorithm would still converge on the true minimum of the original function. Moreover, the rate of convergence would scale with the number of derivatives used: Just as using two derivatives allowed Newton to approach the true minimum at a quadratic rate, using three derivatives enabled the researchers to approach it at a cubic rate, and so on.</p>
<p>Ahmadi, Chaudhry and Zhang had created a more powerful version of Newton’s method that could reach the true minimum value of a function in fewer iterations than previous techniques.</p>
        
        
<p>Like the original version of Newton’s method, each iteration of this new algorithm is still computationally more expensive than methods such as gradient descent. As a result, for the moment, the new work won’t change the way self-driving cars, machine learning algorithms or air traffic control systems work. The best bet in these cases is still gradient descent.</p>
<p>“Many ideas in optimization take years before they are made fully practical,” said <a href="https://jasonaltschuler.github.io/">Jason Altschuler</a> of the University of Pennsylvania. “But this seems like a fresh perspective.”</p>
<p>If, over time, the underlying computational technology needed to run Newton’s method becomes more efficient — making each iteration less computationally expensive — then the algorithm developed by Ahmadi, Chaudhry and Zhang could eventually surpass gradient descent for all sorts of applications, including machine learning.</p>
<p>“Our algorithm right now is provably faster, in theory,” Ahmadi said. He’s hopeful, he added, that in 10 to 20 years, it will also be so in practice.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Arc-AGI-2 and ARC Prize 2025 (180 pts)]]></title>
            <link>https://arcprize.org/blog/announcing-arc-agi-2-and-arc-prize-2025</link>
            <guid>43465147</guid>
            <pubDate>Mon, 24 Mar 2025 20:35:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arcprize.org/blog/announcing-arc-agi-2-and-arc-prize-2025">https://arcprize.org/blog/announcing-arc-agi-2-and-arc-prize-2025</a>, See on <a href="https://news.ycombinator.com/item?id=43465147">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		

<div>
    <p><a href="https://arcprize.org/arc-agi#arc-agi-2"><img src="https://arcprize.org/media/images/blog/arc-agi-2-years.jpg" alt="ARC-AGI-2 challenges test-time reasoning"></a>
    </p>
    <p><a href="https://arcprize.org/competition"><img src="https://arcprize.org/media/images/blog/arc-prize-2025-card.png" alt="Launching ARC Prize 2025"></a>
    </p>
</div>

<h3 id="level-up-to-reach-agi">Level Up to Reach AGI</h3>

<p>Good AGI benchmarks act as useful progress indicators. Better AGI benchmarks clearly discern capabilities. The best AGI benchmarks do all this and actively inspire research and guide innovation.</p>

<p>At ARC Prize, our mission is to serve as a North Star towards AGI through enduring benchmarks, directing efforts towards systems capable of general intelligence and significantly compressing the timeline for scientific breakthroughs.</p>

<p>ARC-AGI-1 has measured progress towards AGI since 2019 and was the only benchmark to <a href="https://arcprize.org/oai-o3-pub-breakthrough">pinpoint the exact moment in late 2024</a> when AI moved beyond pure memorization. OpenAI used ARC-AGI-1 to demonstrate this progress with their o3 system which combines deep learning-based LLMs with reasoning synthesis engines.</p>

<p>ARC Prize 2024 inspired thousands of independent <a href="https://arcprize.org/blog/2024-university-tour-recap">students</a> and <a href="https://arcprize.org/2024-results">researchers</a> to work alongside frontier labs on new test-time adaption ideas.</p>

<p>But there is more work to do to reach AGI. AGI still needs new ideas.</p>

<p>We can characterize systems like o3 as going from "zero to one" on the fluid intelligence spectrum. But these systems are highly inefficient and currently require significant human supervision during the training process to adapt to new domains.</p>

<h4 id="announcements">Announcements</h4>

<p>Today we’re excited to launch ARC-AGI-2 to challenge the new frontier. ARC-AGI-2 is even harder for AI (in particular, AI reasoning systems), while maintaining the same relative ease for humans. Pure LLMs score 0% on ARC-AGI-2, and public AI reasoning systems achieve only single-digit percentage scores. In contrast, every task in ARC-AGI-2 has been solved by at least 2 humans in under 2 attempts.</p>

<p>Alongside it, today we're announcing <a href="https://arcprize.org/competition">ARC Prize 2025</a> (going live on Kaggle this week), designed to drive open-source progress on highly efficient, general systems capable of beating ARC-AGI-2.</p>

<hr>

<h3 id="easy-for-humans-hard-for-ai">Easy for Humans, Hard for AI</h3>

<p>All other AI benchmarks focus on superhuman capabilities or specialized knowledge by testing "PhD++" skills. ARC-AGI is the only benchmark that takes the opposite design choice – by focusing on tasks that are relatively easy for humans, yet hard, or impossible, for AI, we shine a spotlight on capability gaps that do not spontaneously emerge from "scaling up".</p>

<p>The ARC Prize Foundation adapts this into our definition for measuring AGI: the gap between the set of tasks that are easy for humans and hard for AI. When this gap is zero, when there are no remaining tasks, we can find that challenge AI, we will have achieved AGI.</p>

<p>Addressing these capability gaps requires novel insight and new ideas. Importantly, ARC-AGI does not exist purely to measure AGI progress. It also exists to inspire researchers to work on new ideas.</p>

<p>Intelligence requires the ability to generalize from limited experience and apply knowledge in new, unexpected situations. AI systems are already superhuman in many specific domains (e.g., playing Go and image recognition.) However, these are narrow, specialized capabilities. The "human-ai gap" reveals what's missing for general intelligence - highly efficiently acquiring new skills.</p>

<hr>

<h3 id="introducing-arc-agi-2">Introducing ARC-AGI-2</h3>

<p>The ARC-AGI-2 benchmark launches today. This second edition in the ARC-AGI series raises the bar for difficulty for AI while maintaining the same relative ease for humans.</p>

<p>Every ARC-AGI-2 task was solved by at least 2 humans in 2 attempts or less in a controlled study with hundreds of human participants. This matches the rules we hold for AI, which gets two attempts per task.</p>

<p>ARC-AGI-1, introduced in 2019, was designed to challenge deep learning. Specifically, it was designed to resist the ability to simply "memorize" the training dataset. ARC-AGI comprises a training dataset and several evaluation sets, including a private eval set used for the ARC Prize 2024 contest. The training set is intended to teach the Core Knowledge Priors required to solve tasks in the evaluation sets. To solve the evaluation tasks, AI systems must demonstrate basic fluid intelligence or the ability to adapt to novel, never-before-seen tasks.</p>

<p>As an analogy, think of the training set as a way to learn grade school math symbols, and the evaluation set requires you to solve algebra equations using your knowledge of those symbols. You cannot simply memorize your way to the answer, you must apply existing knowledge to new problems.</p>

<p>Any AI system capable of beating ARC-AGI-1 demonstrates a binary level of fluid intelligence. In contrast, ARC-AGI-2 significantly raises the bar for AI. To beat it, you must demonstrate <em>both</em> a high level of adaptability and high efficiency.</p>

<p>While designing ARC-AGI-2, we studied these properties of frontier AI reasoning systems. Below are example tasks to illustrate some of what we discovered. All of the following tasks are part of ARC-AGI-2 and were (1) solved by at least 2 humans in under 2 attempts and (2) unsolved by any frontier AI reasoning system.</p>

<p><em>Note: special thank you to the ARC Prize community for contributing to this analysis, including <a href="https://substack.com/home/post/p-153619775">Mace</a>, <a href="https://anokas.substack.com/p/llms-struggle-with-perception-not-reasoning-arcagi">Mikel</a>, and many members in the <a href="https://discord.com/channels/1237180803335720961/1319729162466099290">ARC Prize Discord</a>.</em></p>

<hr>

<h3 id="still-hard-for-ai">Still Hard for AI</h3>

<h4 id="symbolic-interpretation">Symbolic Interpretation</h4>

<p>We found that frontier AI reasoning systems struggle with tasks requiring symbols to be interpreted as having meaning beyond their visual patterns. Systems attempted symmetry checking, mirroring, transformations, and even recognized connecting elements, but failed to assign semantic significance to the symbols themselves.</p>

<figure>
  <img src="https://arcprize.org/media/images/blog/arc-agi-2-unsolved-1.png" alt="ARC-AGI-2 Pubic Eval Task: #e3721c99">
  <figcaption>Example of symbolic interpretation, ARC-AGI-2 Pubic Eval Task #e3721c99. <a href="https://arcprize.org/play?id=e3721c99" target="_blank">Try this task.</a></figcaption>
</figure>

<h4 id="compositional-reasoning">Compositional Reasoning</h4>

<p>We found AI reasoning systems struggle with tasks requiring simultaneous application of rules, or application of multiple rules that interact with each other. In contrast, if a task only has one, or very few, global rules, we found these systems can consistently discover and apply them.</p>

<figure>
  <img src="https://arcprize.org/media/images/blog/arc-agi-2-unsolved-2.png" alt="ARC-AGI-2 Pubic Eval Task: #cbebaa4b">
  <figcaption>Example of compositional reasoning, ARC-AGI-2 Pubic Eval Task #cbebaa4b. <a href="https://arcprize.org/play?id=cbebaa4b" target="_blank">Try this task.</a></figcaption>
</figure>

<h4 id="contextual-rule-application">Contextual Rule Application</h4>

<p>We found AI reasoning systems struggle with tasks where rules must be applied differently based on context. Systems tend to fixate on superficial patterns rather than understanding the underlying selection principles.</p>

<figure>
  <img src="https://arcprize.org/media/images/blog/arc-agi-2-unsolved-3.png" alt="ARC-AGI-2 Pubic Eval Task: #b5ca7ac4">
  <figcaption>Example of contextual rule application, ARC-AGI-2 Pubic Eval Task #b5ca7ac4. <a href="https://arcprize.org/play?id=b5ca7ac4" target="_blank">Try this task.</a></figcaption>
</figure>

<hr>

<h3 id="datasets">Datasets</h3>

<p>ARC-AGI-2 is comprised of the following datasets:</p>

<table>
    <tbody><tr>
        <th>Dataset</th>
        <th>Task Quantity</th>
        <th>Calibration</th>
        <th>Visibilty</th>
        <th>Description</th>
    </tr>
    <tr>
        <td><a href="https://github.com/arcprize/ARC-AGI-2/tree/main/data/training">Training</a></td>
        <td>1000</td>
        <td>Uncalibrated</td>
        <td>Public</td>
        <td>A spectrum of difficulty ranging from very easy to very difficult for both humans and AI, designed to expose and teach Core Knowledge Priors; use to train your system.</td>
    </tr>
    <tr>
        <td><a href="https://github.com/arcprize/ARC-AGI-2/tree/main/data/evaluation">Public Eval</a></td>
        <td>120</td>
        <td>Calibrated</td>
        <td>Public</td>
        <td>All tasks solved pass@2 by at least two humans; use to test your system.</td>
    </tr>
    <tr>
        <td><a href="https://arcprize.org/guide#semi-private">Semi-Private Eval</a></td>
        <td>120</td>
        <td>Calibrated</td>
        <td>Private</td>
        <td>All tasks solved pass@2 by at least two humans, used for the Kaggle live contest leaderboard and ARC Prize leaderboard. "Semi" means these tasks may have been exposed to limited third-parties (e.g., via API.)</td>
    </tr>
    <tr>
        <td><a href="https://arcprize.org/guide#private">Private Eval</a></td>
        <td>120</td>
        <td>Calibrated</td>
        <td>Private</td>
        <td>All tasks solved pass@2 by at least two humans, used for Kaggle final contest leaderboard. "Private" means these tasks have not been exposed to third-parties.</td>
    </tr>
</tbody></table>

<p><code>Calibrated</code> means that these tasks are IDD (independent and identically distributed). In principle, non-overfit scores across the public, semi-private, and private eval sets should be directly comparable. To gather these data, we tested with over 400 humans live in a controlled setting. Human solvability data for public tasks will be open-sourced alongside the ARC-AGI-2 paper in the coming weeks.</p>

<p>Like ARC-AGI-1, ARC-AGI-2 uses a <code>pass@2</code> measurement system to account for the fact that certain tasks have explicit ambiguity and require two guesses to disambiguate. As well as to catch any unintentional ambiguity or mistakes in the dataset. Given controlled human testing with ARC-AGI-2, we are more confident in the task quality compared to ARC-AGI-1.</p>

<p>Here's the official <code>changelog</code> for ARC-AGI-2:</p>

<ol>
    <li>All eval sets (public, semi-private, private) now contain 120 tasks (up from 100).</li>
    <li>Removed tasks from eval sets that were susceptible to brute force search (all solved tasks from the original <a href="https://www.kaggle.com/competitions/abstraction-and-reasoning-challenge">2020 Kaggle contest</a>).</li>
    <li>Performed controlled human testing to calibrate eval set difficulty to ensure IDD and verify pass@2 solvability by at least 2 humans (to match AI rules).</li>
    <li>Designed new tasks to challenge AI reasoning systems based on study (symbolic interpretation, compositional reasoning, contextual rules, and more).</li>
</ol>

<p>We've also re-evaluated ARC-AGI-2 against all public AI systems. Here are the starting scores.</p>

<table>
    <tbody><tr>
        <td><b>System</b></td>
        <td><b>ARC-AGI-1</b></td>
        <td><b>ARC-AGI-2</b></td>
        <td><b>Efficiency (cost/task)</b></td>
    </tr>
    <tr>
        <td>Human panel (at least 2 humans)</td>
        <td>98%</td>
        <td>100%</td>
        <td>$17</td>
    </tr>
    <tr>
        <td>Human panel (average)</td>
        <td>64.2%</td>
        <td>60%</td>
        <td>$17</td>
    </tr>
    <tr>
        <td>o3-low (CoT + Search/Synthesis)</td>
        <td>75.7%</td>
        <td>4%*</td>
        <td>$200</td>
    </tr>
    <tr>
        <td>o1-pro (CoT + Search/Synthesis)</td>
        <td>~50%</td>
        <td>1%*</td>
        <td>$200*</td>
    </tr>
    <tr>
        <td>ARChitects (Kaggle 2024 Winner)</td>
        <td>53.5%</td>
        <td>3%</td>
        <td>$0.25</td>
    </tr>
    <tr>
        <td>o3-mini-high (Single CoT)</td>
        <td>35%</td>
        <td>0.0%</td>
        <td>$0.41</td>
    </tr>
    <tr>
        <td>r1 and r1-zero (Single CoT)</td>
        <td>15.8%</td>
        <td>0.3%</td>
        <td>$0.08</td>
    </tr>
    <tr>
        <td>gpt-4.5 (Pure LLM)</td>
        <td>10.3%</td>
        <td>0.0%</td>
        <td>$0.29</td>
    </tr>
</tbody></table>

<p><em>Scores labeled with * are in-progress estimates based on partial results we've been able to aggregate so far and o1-pro pricing. Full results will be published once available.</em></p>

<p>All scores reported pass@2 and on the semi-private eval set (except ARC-AGI-1 human panel and ARChitects, which used public and private eval, respectively).</p>

<p>Human panel efficiency is based upon a $115-150 show-up fee, plus a $5/task solve incentive. We optimized costs to produce show-ups (only 70% of registrations actually showed up). Even though we believe the true limit to human intelligence cost efficiency is likely in the $2-5/task range, we report $17/task to reflect the actual data we collected.</p>

<p>We are looking forward to testing the production version of OpenAI o3 low/high once API access is available. Using ARC-AGI-1 tasks that transferred to ARC-AGI-2, we estimate that o3-low would score ~4% and o3-high would potentially score up to 15-20% with very high compute (thousands of dollars per task).</p>

<hr>

<h3 id="intelligence-is-not-just-capability">Intelligence Is Not Just Capability</h3>

<p>Going forward, all ARC-AGI reporting will come along with an efficiency metric. We are starting with cost because it is the most directly comparable between human and AI performance.</p>

<p>Intelligence is not solely defined by the ability to solve problems or achieve high scores. The efficiency with which those capabilities are acquired and deployed is a crucial, defining component. The core question being asked is not just, "Can AI acquire skill to solve a task?" but also, "At what efficiency or cost?"</p>

<figure>
  <img src="https://arcprize.org/media/images/arc-agi-2-efficiency.jpg" alt="ARC-AGI-2: Scale is Not Enough">
  <figcaption>Frontier AI system scores on ARC-AGI-1 vs. ARC-AGI-2</figcaption>
</figure>

<p>We know that brute-force search could eventually solve ARC-AGI (given unlimited resources and time to search). This would not represent true intelligence. Intelligence is about finding the solution efficiently, not exhaustively.</p>

<p>This focus on efficiency is a core principle behind the ARC-AGI. We will now explicitly quantify the cost of intelligence, requiring solutions to demonstrate not just capability, but also the efficient use of resources that defines general intelligence.</p>

<p>Our <a href="https://arcprize.org/leaderboard">new leaderboard page</a> reports progress along both a score and cost axis.</p>

<figure>
  <img src="https://arcprize.org/media/images/leaderboard-20250323.png" alt="ARC-AGI new leaderboard">
  <figcaption>ARC-AGI's new leaderboard showing both score and efficiency as of March 24, 2025</figcaption>
</figure>

<hr>

<h3 id="competition-goes-live-this-week-1000000-in-prizes">Competition Goes Live this Week! $1,000,000 in Prizes</h3>

<p>
<iframe src="https://www.youtube.com/embed/z6cTTkVqAyg?si=ndNcx1LavLvp2sQ8&amp;controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p>

<!--Alongside ARC-AGI-2, we're excited to announce the <a href="https://www.kaggle.com/competitions/arc-prize-2025">ARC Prize 2025 Kaggle Competition</a>.-->
<p>Alongside ARC-AGI-2, we're excited to announce <a href="https://arcprize.org/competition">ARC Prize 2025 is back</a>! The contest will be again hosted on Kaggle between March and November. The competition goes live later this week, sign up <a href="#" data-modal-id="signup">here</a> to get notified.</p>

<p>There is $125K in guaranteed progress prizes, with an additional $700K Grand Prize (up $100k from last year!) waiting to be unlocked by a solution that scores greater than 85%, as well as $175K to-be-announced prizes.</p>

<p>Unlike the <a href="https://arcprize.org/leaderboard">public leaderboard</a> on arcprize.org, Kaggle rules restrict you from using internet APIs, and you only get ~$50 worth of compute per submission. In order to be eligible for prizes, contestants must open source and share their solution and work into the public domain at the end of the competition.</p>

<p>Last year's competition saw incredible success — over 1,500 teams participated, generating 40 influential research papers. ARC Prize-winning researchers introduced innovations now adopted across the AI industry.</p>

<p>What progress can you unlock and share with the world this year?</p>

<h4 id="prize-categories">Prize Categories</h4>

<p><strong>Grand Prize ($700K)</strong><br>
Unlocked once first team reaches 85% within Kaggles efficiency limits.</p>

<p><strong>Top Score Prize ($75K)</strong><br>
Awarded to the highest-scoring submissions.</p>

<p><strong>Paper Prize ($50K)</strong><br>
Awarded to the submission demonstrating the most significant conceptual progress toward solving ARC-AGI. Papers must be tied to a scored submission, but innovative ideas matter more than achieving a high score.</p>

<p><strong>To-be-announced Prizes ($175k)</strong><br>
More details on these coming during the 2025 competition.</p>

<p>You can find more details on our <a href="https://arcprize.org/competition">Competition page</a>.</p>

<h4 id="changelog">Changelog</h4>

<p>Here's a quick look at the competition <code>changelog</code> from 2024 to 2025.</p>

<ol>
    <li><strong>ARC-AGI-2:</strong> we've replaced the ARC-AGI-1 dataset going forward.</li>
    <li><strong>New leaderboard reporting:</strong> The Kaggle live contest leaderboard will report scores on semi-private, with final prize results reported one time on the private eval after content closes.</li>
    <li><strong>Stronger open-source provisions:</strong> teams will be required to open source their solutions before receiving official private eval set scores.</li>
    <li><strong>More compute:</strong> 2x compute vs. 2024 (L4x4s), about $50 worth now.</li>
    <li><strong>More overfit prevention:</strong> we've made additional changes to score reporting on Kaggle to reduce data mining and overfiting and to incentivize conceptual progress.</li>
</ol>

<hr>

<h3 id="progress-depends-on-new-ideas--maybe-yours">Progress Depends On New Ideas — Maybe Yours?</h3>

<p>The current state of AI operates in an idea-constrained environment. The next breakthrough might not come from a major AI lab but from a new, creative approach from someone like you.</p>

<p>If you're driven by curiosity, inspired by complexity, and committed to the rigorous pursuit of genuine general intelligence, ARC Prize 2025 is your opportunity.</p>

<p>Join us in the pursuit of open science.</p>

<h4 id="ready-to-get-started">Ready to get started?</h4>

<p>Good luck! We can't wait to see what you build.</p>



	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cottagecore Programmers (111 pts)]]></title>
            <link>https://tjmorley.com/blogposts/cottagecoreprogrammers.html</link>
            <guid>43464914</guid>
            <pubDate>Mon, 24 Mar 2025 20:08:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tjmorley.com/blogposts/cottagecoreprogrammers.html">https://tjmorley.com/blogposts/cottagecoreprogrammers.html</a>, See on <a href="https://news.ycombinator.com/item?id=43464914">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="Essay">
    <p>
    Every programmer I know has at some point longingly expressed how “[they] want to work with [their] hands”. Some say they wish they were a carpenter, some want to be in the fields on a farm, some want to raise chickens, or milk cows. It is however inevitable that I either hear someone say it or agree with it at some point. Often, the sentiment is accompanied by a general dissatisfaction with the impact of the speaker’s work or profession. Maybe someone just got a particularly mind-numbing feature request, or spent several weeks arguing in meetings about several pixels of padding for text on a button on a support page in a single app.
    </p>
    <p>
    One of the first examples that got me to notice this ongoing trend was back around the end of 2020, when a number of my friends shared and commented on <a href="https://news.ycombinator.com/item?id=24541964">a hackernews thread</a> about someone who had responded to a request to add an RSS feed to a DBMS with the news that he had quit programming and switched to carpentry, in a particularly pithy manner (as an aside, I took a look at his <a href="https://www.longwalkwoodworking.com/">woodworking shop</a> while writing this up, and it is quite beautiful, you should definitely check it out).
    </p>
    <p><img src="https://tjmorley.com/assets/rss_dbms_github.png" alt="Screenshot of a githhub post saying that the author missed a comment of many months ago because they no longer build software, but instead make furniture out of wood."></p><p>
    Now the relevant part here isn’t necessarily this particular post or person (It seems to me that he really prefers and loves woodworking over software which is of course completely valid, and I honestly understand it in many ways), but the reactions to it, many of which were wistful among people I knew as well as among the comments online. Though there was certainly debate between commenters(see the thread linked earlier if curious). If I had to sum up the reactions, they fell into two camps. It was split between a group who mainly expressed admiration and a desire to be doing similar work, interspersed with condemnations leveled towards modern software practices and businesses, and a second group who mainly saw the flaws in modern development as being similar to those in any other job, or who thought that other jobs were even worse off. My real interest in writing this post is exploring why people – especially tech workers – seem to resonate so strongly with stories such as this one, where other tech workers leave behind their positions to return to work based in crafts, agriculture, or homesteading. This is particularly interesting to me given the seeming stability of this fascination regardless of the economic situation for tech workers (in 2020, the market was quite strong, though there’s been noted layoffs in recent years). In fact, <a href="https://www.reddit.com/r/cscareerquestions/comments/1bjsan7/i_think_i_get_the_whole_drop_out_of_tech_and_do/">here</a> is another thread on reddit from a year ago referencing the same phenomenon. Another one <a href="https://www.reddit.com/r/sysadmin/comments/p85p6b/this_guy_actually_did_leave_the_tech_life_to_be_a/?rdt=43971">here</a> I've only looked through the comments of, but many discuss their desire to leave IT for farming. If you aren't a tech worker, trust me in that you will hear people say how much they want this if you are a tech worker.
    </p>
    <p>
    ebd2’s post was just one particularly salient example, but if you’ve been online enough over the last decade or so, you’ve probably seen plenty of these kinds of posts in the same way I have, and not just from software developers or general tech workers. Across tech, it appears that many people are tired of working in offices, with computers, with meetings or emails, and want to abandon this life and start anew in more meaningful labor. In fact, over the last few years, there’s been a noted increase(see <a href="https://fox8.com/news/homesteading-taking-off-in-popularity-particularly-among-younger-generations/">here</a> or <a href="https://www.bbc.com/culture/article/20201208-cottagecore-and-the-rise-of-the-modern-rural-fantasy">here</a>) in a style of content that portrays idyllic lifestyles in times perceived as simpler, especially in short form video content. I can’t help but propose this “cottagecore” type content as deriving from the same zeitgeist.  For popular examples, take a look at <a href="https://en.wikipedia.org/wiki/Nara_Smith">Nara Smith</a> making all sorts of odd things from scratch, or at the popular but ominous <a href="https://www.youtube.com/@TheBallerinaFarm/shorts">ballerina farm</a>, which inspired significant discussion over it’s formation (discussed <a href="https://www.thetimes.com/magazines/the-sunday-times-magazine/article/meet-the-queen-of-the-trad-wives-and-her-eight-children-plfr50cgk">here</a> among other places). If this content is so wildly popular, there’s certainly some form of backlash against modernity and a desire to turn back the clock.
    </p>
    <p>
    After hearing this desire to return to a more hands-on type of labor once again recently, I began to ask myself why it was so common. Why does everyone who sits behind a computer long to be out in the fields or workshops? Is this specific to some subgroup in tech that I happen to cross paths with regularly, or is it a broader modern ennui? To understand why I find the topic so interesting though, you’ll probably require a bit of background on me. I currently work as a researcher in the application of machine learning to problems in healthcare and genetics, but I was raised on a farm in a very rural area of Vermont. My mother’s side of the family have been farmers in the area for generations, generally growing and selling hay, as well as sheep and some amount of dairy farming. As a child on a farm, I of course helped work on the farm – throwing hay bales, carrying water buckets, shoveling massive piles of manure. I also worked in the associated side industries, like basic construction. I appreciate what I learned from this work, and the lessons were deeply valuable ones, but at the same time I would be lying if I didn’t admit that the intensity of it was a serious motivator for me to study harder than I otherwise would have and focus on technical skills, precisely so that I wouldn’t have to do physical labor anymore. I dreamed about the idea of getting paid to sit in an air conditioned office, free of clouds of dust, never having to deal with the constant little cuts and pricks from the blades of dried grass in the air of a hayloft slicing my arms over and over. Personally, I still like gardening, and there are parts of farming I enjoy quite a lot. If I lived in an area with more space, I would likely still farm as a hobby, but having your income or food supply come from it is a very different concept in terms of stress.
    </p>
    <p>
    Considering that, I hope it makes sense as to why I find this trend puzzling. Why would people who are so comfortable, whose job was to me a lifelong goal, want to do exactly what I worked so hard to move away from? I suspect the answer is tied up in the nature of our work in the modern world, as well as in the inspection of who has been mythologized in American history. So we’ll be taking a short departure from the modern day to look into this, if you’ll excuse the seeming change in subject. In the following sections, I’ll largely focus on this from the perspective of farming in terms of verbiage and discussion, but I believe many of these psychological yearnings and trends in the American populace in relation to manual craftsperson or homesteading type work have some similarities.
    </p>
    <h3>The Homesteader within the American Mythos</h3>
    <p>
    Farmers and small landowners in general have been largely lionized in American history, where examples abound of political figures extolling the virtues, necessity of, and unique individualism belonging to farmers. I’ll admit it’s a little flattering when people talk about the sort of work I did for so long as uniquely virtuous, but I’m left feeling unsure as to how healthy it is for a society to degrade some types of work in order to raise up others. For an example, think of William Jennings Bryan, an American politician popular among farmers in the late 19th century and after its turn. He had a large effect on the views of many rural Americans and regularly espoused the idea that agrarian work was inherently superior to other forms of labor. The classic example of his views is his famous “Cross of Gold” speech, where he said the following:
    </p>
    <blockquote cite="https://www.historymatters.gmu.edu/d/5354/">
        “You come to us and tell us that the great cities are in favor of the gold standard. I tell you that the great cities rest upon these broad and fertile prairies. Burn down your cities and leave our farms, and your cities will spring up again as if by magic. But destroy our farms and the grass will grow in the streets of every city in the country.” 
    </blockquote>
    <p><cite>William Jennings Bryan: “<a href="https://www.historymatters.gmu.edu/d/5354/">Cross of Gold” July 9, 1896, Chicago</a></cite></p>
    <p>The idea expressed here is clear – farms power the existence of the cities, so they should be shown more respect.</p>
    <p>
    Stepping back another century, think of Thomas Jefferson. He was arguably the primary supporter for the United States to be an agrarian republic during its founding, through his writings such as “Notes on the State of Virginia.” In mentioning this book, I must note that many viewpoints he expresses in it are clearly and deeply reprehensible, primarily his comments on race, and those on the law. Still, the views of Jefferson were advanced by many in the early days of the United States and he influenced much of the national consciousness, regardless of my opinion on him. Richard Hofstadter, an American historian has written thoroughly on the subject of this phenomenon and its impact on American values. His work includes much discussion of the social establishment of virtues in the United States around agrarianism, with one example being his essay “The Myth of the Happy Yeoman.” In it, he gives an excellent and concise description of the manner in which people such as Jefferson or Bryan argued about the superiority of rural life:
    </p>
    <blockquote cite="">
        “Like any complex of ideas, the agrarian myth cannot be defined in a phrase, but its component themes form a clear pattern. Its hero was the yeoman farmer, its central conception the notion that he is the ideal man and the ideal citizen. Unstinted praise of the special virtues of the farmer and the special values of rural life was coupled with the assertion that agriculture, as a calling uniquely productive and uniquely important to society, had a special right to the concern and protection of government. The yeoman, who owned a small farm and worked it with the aid of his family, was the incarnation of the simple, honest, independent, healthy, happy human being. Because he lived in close communion with beneficent nature, his life was believed to have a wholesomeness and integrity impossible for the depraved populations of cities.”
    </blockquote>
    <p><cite>Richard Hofstadter: “<a href="https://www.americanheritage.com/myth-happy-yeoman">The Myth of the Happy Yeoman</a>”, 1956</cite></p>
    <p>
    To me, this feels almost as if it could have been written today, in description aimed towards the varying videos and posts on social media waxing poetically on the forgotten idyllic lifestyles of life on the farm. Yet this was written almost seventy years ago, about topics which were in heavy discussion more than fifty years prior. Hofstadter even seems to imply in the essay that this ideal of the yeoman had declined, and says that “agrarian myth” was subsumed and replaced by an “even stronger ideal, the notion of opportunity, of career, of the self-made man.” If the agrarian myth declined and was subsumed, why does it seem to be re-energized today? The key here is the ideal of the self-made man, which brings our focus back to the present day. Though I think the feelings of ennui expressed by many tech workers that inspired me to write this are most pronounced in the field of programming, I don’t think they are limited solely to them.
    </p>
    <h3>Psychological Shifts in American Ideals</h3>
    <p>
    My general opinion is that the ideal of the “self-made man” has made a transition from a widely accepted goal to one viewed as contentious, and to many, unrealistic. The average American citizen no longer views stability in this way as a plausible concept, disheartened by <a href="https://www.pewresearch.org/social-trends/2020/01/09/trends-in-income-and-wealth-inequality/">rising income inequality</a> and disillusioned by the proliferation of easily accessed get-rich-quick schemes (with obviously low rates of success) such as cryptocurrency, app-based stock trading within meme stocks, and exploitation of audiences by sudden social media stars. These feelings of despair often center around the idea of homeownership – central to the ideal of the yeoman and of any independent citizen is the idea of owning a residence, and of setting roots in an area. Whether or not owning housing is as achievable as it was in the past in the US is a complicated question – the rate of increase in housing costs has been significantly higher than the rate of increase in average wage over the last <a href="https://www.thezebra.com/resources/home/housing-trends-visualized/">60 years</a>, so housing is unarguably taking up a larger percentage of our income. Though data prior to 1980 on homeownership by age group is harder to come by, it appears that it has not changed dramatically for most groups (the main difference being a slight downward trend and a significant drop during and after the 2008 subprime mortgage crisis for most groups which are not older than age 65, while the 65+ group has over time increased their homeownership rates by about 5%).
    </p>
    <p><img src="https://tjmorley.com/assets/censushousing.png" alt="U.S. Census Bureau Housing and Vacancy survery showing 5 age categories percent homeownership rate remaining relatively stable from 1980s to today, with the only exceptions being those mentioned above."></p><p>
    One could argue, as many have, that some of this increase in housing cost is offset by decreased costs in other expenditures such as food or technology. However, the important point here is not about whether the material reality has effectively changed, but in how individuals currently perceive both the current reality and the past. As recently as two years ago, Americans polled on whether they believe that life for people like now compared to 50 years ago was better or worse generally said that they <a href="https://www.pewresearch.org/short-reads/2023/04/24/americans-take-a-dim-view-of-the-nations-future-look-more-positively-at-the-past/">believed life was now worse.</a> There is a pervasive belief that life was better in the past and I believe that individuals who feel this way tend to move themselves towards social values that they see as indicative of the past, such as agrarian lifestyles. They retreat to an earlier part of the American psyche, to that of the independent farmer, escaping from the confusion and fear they see in a modern economy and impersonality of technology.
    </p>
    <p>
    One counterpoint to this is as follows – if this is prevalent in individuals employed in tech, could it truly be linked to income inequality? Many tech workers enjoy high incomes relative to the average worker in this country. This is true, but it draws us to our next point, that of alienation. This is a topic which is less grounded in surveys or statistics, but one which is more readily understood simply through discussion of how we view the nature of work. Many individuals find satisfaction from being appreciated for what they contribute in their work, a simple enough proposition. However, direct service and interaction with the individuals who benefit from your work is generally not an efficiency within a market. One baker making bread for his neighbors may find more satisfaction in that, but wouldn’t it be more efficient to have a centralized bread distribution factory, staffed by hundreds of bakers, who have their pooled effort in the form of hearty loaves of glutenous manna distributed efficiently across multiple communities? As we create more and more efficient economies across a variety of fields of work, we continually disentangle and alienate individuals from the effects and recipients of their labor. When you become detached from the meaning behind your work in this way though, it becomes difficult to assign meaning to your work.
    </p>
    <p>
    It is difficult to think of any field more forcibly disentangled from any sort of understanding of the impact of your labor than the majority of positions in tech. Through a series of entirely digital processes you are completing tasks which often, upon reflection, feel almost imaginary. Programs pushed through layers of committees work well to distribute responsibility for potential bugs or issues, but they have a secondary effect of  stripping much of the individual ownership of a given product. If you were some engineer working on a large corporate website, it’s less that you made the whole experience, and more that you contributed to it. You likely never see any indication of whether people appreciate or like the things that you made beyond abstract visit rates and click-through statistics, as opposed to a person acknowledging that you created something that they appreciate. Of course people in this position would feel deeply alienated from the work that they do over time, even when they logically understand the economic or personal value of what they create, the emotional connection they seek to find value through work is lacking. This is exactly where it ties back into the rising income inequality though – say you’re a programmer working at a company that you feel is draining your soul. You feel no connection to your work. But what else can you do? You’re fully aware that other careers pay far less, so why would you work in a different position, especially when you’ve already worked your way up the ladder within this one? This choice could feel even worse if you had a family depending on you. So instead many people detach, they start to feel more apathetic and they simply step through the motions. Some others instead start to yearn to turn back the clock to an almost mythical time period that they see depicted in popular media as better. It is exactly this combination of alienation from the meaning of one’s labor, a lack of confidence in the viability of a stable career-driven path to “success” or becoming “self-made” without luck, and a view of craft-based or agricultural labor as existing in a different time period possessing greater intrinsic meaning and inherent nobility which combine together within the American psyche to form a fascination with these types of work across social strata. Idealization of farming seems much more likely now that it almost feels mythical as compared to when most individuals likely either were engaged in it or knew someone who was.
    </p>
    <p><img src="https://tjmorley.com/assets/percent_labor_force_agricultural.png" alt="A chart with percent and year, with percent dropping from ~74% to ~2%-3% from the years 1800 to 2000."></p><p><cite>Image: Percent of the Labor Force Employed in Agriculture, United States, 1800 to 2000. Source: Ruggles, Steven. (2007). The Decline of Intergenerational Coresidence in the United States, 1850 to 2000. American sociological review. 72. 964-989. 10.1177/000312240707200606.</cite></p>
    <p>
    In the above chart, Dr. Ruggles demonstrates as part of his paper on the decline of intergenerational coresidence in the US the astronomical change in the amount of agricultural workers in the country, dropping from it being the occupation of the vast majority of the labor force to only a tiny percent over the last few decades. Importantly, there is now no American alive who remembers the country when more than a 20%-30% of its workers were employed in agriculture. The median American by age, in their late 30s currently, has only ever known a world where a single-digit proportion of the US workforce was working agriculturally.
    </p>
    <p>
    In fact, given these statistics I would take the point of these sort of agrarian type jobs seeming mythical a step further, given that so few Americans are familiar with the actual work behind jobs in the agricultural sector or related ones, I would argue that for many Americans the reality of this type of work is almost indistinguishable from fables or myth in their psychological context, due to lack of exposure. Due to that seeming separation from their mental context of what work is, it enhances the feeling of escapism which this work-fantasy provides, though the conception many hold of it is even more accurately described as a fable due to the significant difference between reality and the projection which many seem to aim for in their cottagecore fantasy. Certainly on Instagram or TikTok you can find depictions of women waltzing through their gardens, baby in their arms, cheery-faced while their husband walks around with flannel and chops wood and their goats bleat in the distance and a chicken pecks in the yard. This idyllic view certainly leaves out the part where she has to take a pick and scrape dirt and feces out of those goats hooves and trim them on a regular basis to prevent foot-rot (They certainly didn’t evolve to walk in dirt all day, but to jump around on rocks). It doesn’t touch on when they hadn’t realized that they had too high a rooster hen ratio, and woke up to find one rooster pulling the eyes of another one out of its skull. It certainly doesn’t display the economic difficulties inherent in these agricultural endeavors: checking your fruit trees and seeing half your apples have one large bite out of them from deer, looking in your coop and seeing that several of your hens got dragged off by foxes or coyotes, having one of your prime dairy cows die due to complications after a stillbirth, going to water your large tomato crop and finding that blister beetles have started stripping all the foliage from your plants. The possibilities are endless and the situations are often dirty and difficult, and while for a hobby farm these are just unfortunate setbacks, if your ability to put food on the table for your family is determined by the outcome of these sort of situations, then they’re less of a setback and more of a devastating blow to whether or not you can remain economically solvent.
    </p>
    <p>
    The idealization of farmers and craftsmen is not unique to the USA either(see Tolstoy’s opinions on agrarianism as well), but I can primarily speak to how people feel about the current climate of work within our country. I will say that I think many people yearning to turn back many changes in society are largely misguided in this – though there is absolutely value in hands on work, in serving one’s community, turning back the clock in all aspects risks erasing so many true gains we’ve made as a society, especially if the core of the issue is in our relationship with work as a society. Personally, I can’t deny that seeing someone doing work with their own two hands feels more authentic and true to me. I find the idea of others in my field who have never experienced physical labor very strange to consider. There is a clear flaw with some of this idealization though. The ideal of finding our value in work and making something of ourselves through the sweat of our brow is a noble one in many ways but contains a corrosive core as well. If you center your value entirely in how appreciative others are of the contributions you make, and in what you can provide, you ignore the actual concept of community, and you potentially build a worldview that denigrates those who you view as not contributing “enough” in your manner. This is purely my opinion, but I think the healthier way to approach this concept is one of sacrifice and service. If you have the capability to contribute work in some way – physical strength, knowledge, or mental acuity, then you have a moral responsibility to do so – yet not having as much capability as some other person is not something that should negate your self-worth. Instead, the act of doing what you can is what should be seen as virtuous, rather than the absolute economic output of your personal capability. Do what you can, build a community through simple acts of socialization and giving to others, and improve the lives of everyone around you. That is how we find fulfillment in a world such as this one.
    </p>
    <p><cite> Theodore Morley</cite></p>
    </div></div>]]></description>
        </item>
    </channel>
</rss>