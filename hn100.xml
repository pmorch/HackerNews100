<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 09 Dec 2025 07:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Horses: AI progress is steady. Human equivalence is sudden (316 pts)]]></title>
            <link>https://andyljones.com/posts/horses.html</link>
            <guid>46199723</guid>
            <pubDate>Tue, 09 Dec 2025 00:26:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://andyljones.com/posts/horses.html">https://andyljones.com/posts/horses.html</a>, See on <a href="https://news.ycombinator.com/item?id=46199723">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
        
<p>So after all these hours talking about AI, in these last five minutes I am going to talk about: horses.</p>
<p><img src="https://andyljones.com/source/horses/horse_efficiency.png" alt="Engine efficiency over time, showing steady improvement"></p>
<p>Engines, steam engines, were invented in 1700.</p>
<p>And what followed was 200 years of steady improvement, with engines getting 20% better a decade.</p>
<p>For the first 120 years of that steady improvement, horses didn't notice at all.</p>
<p>Then, between 1930 and 1950, 90% of the horses in the US disappeared.</p>
<p>Progress in engines was steady. Equivalence to horses was sudden.</p>
<hr>

<p>But enough about horses. Let's talk about chess!</p>
<p><img src="https://andyljones.com/source/horses/chess.png" alt="Computer chess Elo over time, showing steady 50 point per year improvement"></p>
<p>Folks started tracking computer chess in 1985.</p>
<p>And for the next 40 years, computer chess would improve by 50 Elo per year.</p>
<p>That meant in 2000, a human grandmaster could expect to win 90% of their games against a computer.</p>
<p>But ten years later, the same human grandmaster would lose 90% of their games against a computer.</p>
<p>Progress in chess was steady. Equivalence to humans was sudden.</p>
<hr>

<p>Enough about chess! Let's talk about AI.</p>
<p><img src="https://andyljones.com/source/horses/project_cost.png" alt="AI datacenter capital expenditure over time"></p>
<p>Capital expenditure on AI has been pretty steady.</p>
<p>Right now we're - globally - spending the equivalent of 2% of US GDP on AI datacenters each year.</p>
<p>That number seems to have steadily been doubling over the past few years.</p>
<p>And it seems - according to the deals signed - likely to carry on doubling for the next few years.</p>
<hr>

<p>But from my perspective, from equivalence to me, it hasn't been steady at all.</p>
<p><img src="https://andyljones.com/source/horses/ask_claude.png" alt="Questions answered by humans vs Claude over time"></p>
<p>I was one of the first researchers hired at Anthropic.</p>
<p>This pink line, back in 2024, was a large part of my job. Answer technical questions for new hires.</p>
<p>Back then, me and other old-timers were answering about 4,000 new-hire questions a month.</p>
<p>Then in December, Claude finally got good enough to answer some of those questions for us.</p>
<p>In December, it was some of those questions. Six months later, 80% of the questions I'd been being asked had disappeared.</p>
<p>Claude, meanwhile, was now answering 30,000 questions a month; eight times as many questions as me &amp; mine ever did.</p>
<hr>

<p>Now. Answering those questions was only part of my job.</p>
<p>But while it took horses decades to be overcome, and chess masters years, it took me all of six months to be surpassed.</p>
<p><img src="https://andyljones.com/source/horses/per_token_cost.png" alt="Cost per million words: AI researcher vs subsistence farmer vs Sonnet"></p>
<p>Surpassed by a system that costs one thousand times less than I do.</p>
<p>A system that costs less, per word thought or written, than it'd cost to hire the cheapest human labor on the face of the planet.</p>
<hr>

<p>And so I find myself thinking a lot about horses, nowadays.</p>
<p><img src="https://andyljones.com/source/horses/horse_car.png" alt="Horses vs cars in the United States, with 'me' marked at 1920"></p>
<p>In 1920, there were 25 million horses in the United States, 25 million horses totally ambivalent to two hundred years of progress in mechanical engines.</p>
<p>And not very long after, 93 per cent of those horses had disappeared.</p>
<p>I very much hope we'll get the two decades that horses did.</p>
<p>But looking at how fast Claude is automating my job, I think we're getting a lot less.</p>
<hr>

<p><em>This was a five-minute lightning talk given over the summer of 2025 to round out a small workshop.</em></p>
<p><em>All opinions are my own and not those of my employer.</em></p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The universal weight subspace hypothesis (228 pts)]]></title>
            <link>https://arxiv.org/abs/2512.05117</link>
            <guid>46199623</guid>
            <pubDate>Tue, 09 Dec 2025 00:16:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2512.05117">https://arxiv.org/abs/2512.05117</a>, See on <a href="https://news.ycombinator.com/item?id=46199623">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2512.05117">View PDF</a>
    <a href="https://arxiv.org/html/2512.05117v2">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Prakhar Kaushik [<a href="https://arxiv.org/show-email/784dfcb8/2512.05117" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2512.05117v1" rel="nofollow">[v1]</a></strong>
        Thu, 4 Dec 2025 18:59:58 UTC (14,316 KB)<br>
    <strong>[v2]</strong>
        Sat, 6 Dec 2025 04:42:07 UTC (14,321 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kroger acknowledges that its bet on robotics went too far (133 pts)]]></title>
            <link>https://www.grocerydive.com/news/kroger-ocado-close-automated-fulfillment-centers-robotics-grocery-ecommerce/805931/</link>
            <guid>46199411</guid>
            <pubDate>Mon, 08 Dec 2025 23:53:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.grocerydive.com/news/kroger-ocado-close-automated-fulfillment-centers-robotics-grocery-ecommerce/805931/">https://www.grocerydive.com/news/kroger-ocado-close-automated-fulfillment-centers-robotics-grocery-ecommerce/805931/</a>, See on <a href="https://news.ycombinator.com/item?id=46199411">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                        


<div>
        <p>
            This audio is auto-generated. Please let us know if you have <a href="https://www.grocerydive.com/contact/">feedback</a>.
        </p>
    </div>


                        

<p>Kroger’s <a href="https://www.grocerydive.com/news/kroger-ecommerce-profitability-400M-ocado-automated-fulfillmnet-centers-delivery/805781/">announcement on Tuesday</a> that it will shutter three of its robotic e-commerce fulfillment facilities represents a sharp turnabout for the grocery company, which until recently had expressed confidence in its ability to leverage automation to run a profitable online grocery business.</p>
<p>Less than a year ago, Kroger said it <a href="https://www.grocerydive.com/news/kroger-and-ocado-plan-to-open-2-more-automated-fulfillment-centers/741254/">planned to expand</a> the fleet of high-tech fulfillment centers it has been developing in partnership with U.K.-based warehouse automation company Ocado. And in mid-2024, Kroger revealed that it would <a href="https://www.grocerydive.com/news/kroger-ocado-technology-automated-fulfillment-centers/721986/">install new technology</a> from Ocado to improve the efficiency of the warehouses.</p>



<p>When Kroger <a href="https://www.grocerydive.com/news/grocery--kroger-partners-with-ocado-in-a-bet-on-the-future-of-online-grocery/534008/">launched its partnership with Ocado</a>, the company “believed in the relentless drive to innovate way ahead of the market in order to delight our customers and advance our position as one of America’s leading e-commerce companies,” former Kroger CEO Rodney McMullen <a href="https://www.youtube.com/watch?v=E4kH-atEpd4">said in a video</a> about improvements to its equipment that the automation company announced last year.</p>
<p>However, Kroger’s projected confidence came even as it was questioning whether the Ocado network was living up to expectations.</p>
<p>Kroger revealed in September 2023 that it had decided to <a href="https://www.grocerydive.com/news/kroger-ocado-grocery-cfc-delivery-pickup/693287/">pause development of the Ocado project</a> as it waited to see if sites it had already started operating would meet performance benchmarks.</p>
<p>In a further sign that its strategy was faltering, Kroger announced last March it would <a href="https://www.grocerydive.com/news/kroger-ocado-closing-3-e-commerce-fulfillment-facilities/711583/">close three spoke facilities</a> that worked in tandem with several of its robotic centers, with a spokesperson noting that the facilities “did not meet the benchmarks we set for success.”</p>
<p>By September 2025, it was clear that depending on automation as the foundation of a money-making grocery delivery business was probably not going to pan out for Kroger. Speaking during an earnings call, interim Kroger CEO Ron Sargent — who <a href="https://www.grocerydive.com/news/kroger-ceo-rodney-mcmullen-resigns-ethics-probe/741345/">took over in March</a> after McMullen’s sudden departure following an ethics probe — said the company would <a href="https://www.supplychaindive.com/news/kroger-is-reviewing-its-automated-e-commerce-fulfillment-network/759926/">conduct a “full site-by-site analysis</a>” of the Ocado network.</p>
<p>Sargent also said Kroger would refocus its e-commerce efforts on its fleet of more than 2,700 grocery supermarkets because it believed that its stores gave it a way to “reach new customer segments and expand rapid delivery capabilities without significant capital investments.”</p>


<p>Kroger said on Tuesday that its decision to close the three robotic facilities, along with other adjustments to its e-commerce operations, would provide a $400 million boost as it looks to improve e-commerce profitability. But the course-correction will be expensive, forcing Kroger to incur charges of about $2.6 billion.</p>
<p>Ken Fenyo, a former Kroger executive who now advises retailers on technology as managing partner of Pine Street Advisors, said the changes Kroger is making reflect the broader reality that grocery e-commerce has not reached the levels the industry had predicted when the COVID-19 pandemic supercharged digital sales five years ago.</p>
<p>Fenyo added that Kroger’s decision to locate the Ocado centers outside of cities turned out to be a key flaw.</p>
<p>“Ultimately those were hard places to make this model work,” said Fenyo. “You didn’t have enough people ordering, and you had a fair amount of distance to drive to get the orders to them. And so ultimately, these large centers were just not processing enough orders to pay for all that technology investment you had to make.”</p>
<p>With its automated fulfillment network, Kroger bet that consumers would be willing to trade delivery speed for sensible prices on grocery orders. That model has been highly successful for Ocado in the U.K., but U.S. consumers <a href="https://www.grocerydive.com/news/delivery-speed-is-king-for-online-shoppers-survey-shows/599282/">have shown they value speed of delivery</a>, with companies like Instacart and DoorDash expanding rapidly in recent years and rolling out services like 30-minute delivery.</p>
<p>Acknowledging this reality, Kroger noted on Tuesday that it’s <a href="https://www.grocerydive.com/news/kroger-doordash-grocery-delivery-ecommerce/761315/">deepening partnerships with third-party delivery companies</a>. The grocer also said it will pilot “capital-light, store-based automation in high-volume markets” — a seeming nod to the type of micro-fulfillment technology that grocers have tested in recent years, and that Amazon is <a href="https://www.grocerydive.com/news/amazon-whole-foods-store-within-store-automated-micro-fulfillment/804749/">currently piloting in a Whole Foods Market</a> store in Pennsylvania.&nbsp;</p>
<p>Fenyo pointed out that micro-fulfillment technology has also run into significant headwinds, adding that he thinks that outside of areas with large numbers of shoppers and high online ordering volume, putting automated order-assembly systems in stores probably doesn’t justify the cost.</p>
<p>Kroger’s decision to reduce its commitment to automation also poses a significant setback to Ocado, which has positioned its relationship with Kroger as a key endorsement of its warehouse automation technology. Shares in the U.K.-based robotics company have fallen dramatically and are now <a href="https://www.theguardian.com/business/nils-pratley-on-finance/2025/nov/18/ocado-share-price-back-where-it-started-fancy-robots">back to their level 15 years ago</a>, when the company <a href="https://multiples.vc/public-comps/ocado-group-valuation-multiples">went public</a>.</p>

                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Icons in Menus Everywhere – Send Help (403 pts)]]></title>
            <link>https://blog.jim-nielsen.com/2025/icons-in-menus/</link>
            <guid>46196688</guid>
            <pubDate>Mon, 08 Dec 2025 19:44:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.jim-nielsen.com/2025/icons-in-menus/">https://blog.jim-nielsen.com/2025/icons-in-menus/</a>, See on <a href="https://news.ycombinator.com/item?id=46196688">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://mastodon.social/@jimniels/115556046706814962">I complained about this on the socials</a>, but I didn’t get it all out of my system. So now I write a blog post.</p><p>I’ve never liked the philosophy of “put an icon in every menu item by default”.</p><p>Google Sheets, for example, does this. Go to “File” or “Edit” or “View” and you’ll see a menu with a list of options, every single one having an icon (same thing with the right-click context menu).</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-sheets.png" width="725" height="848" alt="Screenshot of menus with icons in Google Sheets"></p><p>It’s extra noise to me. It’s not that I think menu items should <em>never</em> have icons. I think they can be incredibly useful (more on that below). It’s more that I don’t like the idea of “give each menu item an icon” being the default approach.</p><p>This posture lends itself to a practice where designers have an attitude of “I need an icon to fill up this space” instead of an attitude of “Does the addition of a icon here, and the cognitive load of parsing and understanding it, help or hurt how someone would use this menu system?”</p><p>The former doesn’t require thinking. It’s just templating — they all have icons, so we need to put <em>something</em> there. The latter requires care and thoughtfulness for each use case and its context.</p><p>To defend my point, one of the examples I always pointed to was macOS. For the longest time, Apple’s OS-level menus seemed to avoid this default approach of sticking icons in every menu item.</p><p>That is, until macOS Tahoe shipped.</p><p>Tahoe now has icons in menus everywhere. For example, here’s the Apple menu:</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-mac.png" width="312" height="351" alt="Screenshot of the Apple menu in macOS tahoe where every menu item is prefixed with an icon."></p><p>Let’s look at others. As I’m writing this I have Safari open. Let’s look at the “Safari” menu:</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-safari-about.png" width="333" height="445" alt="Screenshot of the Safari menu in macOS Tahoe where about half of the menu items are prefixed with an icon."></p><p>Hmm. Interesting. Ok so we’ve got an icon for like half the menu items. I wonder why some get icons and others don’t?</p><p>For example, the “Settings” menu item (third from the top) has an icon. But the other item in its grouping “Privacy Report” does not. I wonder why? Especially when Safari has an icon for Privacy report, like if you go to customize the toolbar you’ll see it:</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-macos-safari-privacy-report.png" width="723" height="259" alt="Screenshot of the Customize Toolbar UI in Safari and the Privacy Report button has a red highlight around indicating its icon."></p><p>Hmm. Who knows? Let’s keep going.</p><p>Let’s look at the "File" menu in Safari:</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-safari-file.png" width="422" height="567" alt="Screenshot of the File menu Safari in macOS Tahoe where only a few menu items are prefixed with an icon. Some are indented, others not."></p><p>Some groupings have icons and get inset, while other groupings don’t have icons and don’t get inset. Interesting…again I wonder what the rationale is here? How do you choose? It’s not clear to me.</p><p>Let’s keep going. Let’s go to the "View" menu:</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-safari-view.png" width="373" height="648" alt="Screenshot of the View menu in Safari on macOS Tahoe where some menu items are prefixed with an icon and two also have a checkmark."></p><p>Oh boy, now we’re really in it. Some of these menu items have the notion of a toggle (indicated by the checkmark) so now you’ve got all kinds of alignment things to deal with. The visual symbols are doubling-up when there’s a toggle <em>and</em> an icon.</p><p>The “View” menu in Mail is a similar mix of:</p><ul><li>Text</li><li>Text + toggles</li><li>Text + icons</li><li>Text + icons + toggles</li></ul><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-mail-view.png" width="343" height="770" alt="Screenshot of the View menu in Mail on macOS Tahoe showing how menu items can be indented and have icons, not have icons, and have toggles with checkmarks."></p><p>You know what would be a fun game? Get a bunch of people in a room, show them menus where the textual labels are gone, and see who can get the most right.</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-app-edit.png" width="188" height="541" alt="Screenshot of a menu in macOS Tahoe where every menu item is prefixed with an icon but the labels are blurred out so you don’t know for sure what each menu item is."></p><p>But I digress.</p><p>In so many of these cases, I honestly can’t intuit why some menus have icons and others do not. What are so many of these icons affording me at the cost of extra visual and cognitive parsing? I don’t know.</p><p>To be fair, there are <em>some</em> menus where these visual symbols are incredibly useful. Take this menu from Finder:</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-finder-window.png" width="614" height="604" alt="Screenshot of a Finder menu in macOS Tahoe where every menu item is prefixed with a useful icon."></p><p>The visual depiction of how those are going to align is actually incredibly useful because it’s way easier for my brain to parse the symbol and understand where the window is going to go than it is to read the text and imagine in my head what “Top Left” or “Bottom &amp; Top” or “Quarters” will mean. But a visual symbol? I instantly get it!</p><p>Those are good icons in menus. I like those.</p><h2 id="apple-abandons-its-own-guidance">Apple Abandons Its Own Guidance</h2><p>What I find really interesting about this change on Apple’s part is how it seemingly goes against their own previous human interface guidelines (as <a href="https://mastodon.gassner.io/@peter/115559008588925643">pointed out to me by Peter Gassner</a>).</p><p>They have an entire section in their 2005 guidelines titled “Using Symbols in Menus”:</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-hig.png" width="679" height="359" alt="Screenshot from Apple’s Human Interface Guidelines"></p><p>See what it says?</p><blockquote><p>There are a few standard symbols you can use to indicate additional information in menus…Don’t use other, arbitrary symbols in menus, because they add visual clutter and may confuse people.</p></blockquote><p>Confused people. That’s me.</p><p>They even have an example of what <em>not</em> to do and guess what it looks like? A menu in macOS Tahoe.</p><p><img src="https://cdn.jim-nielsen.com/blog/2025/context-menu-hig-donts.png" width="343" height="224" alt="Screenshot from the HIG denoting how you shouldn’t use arbitrary symbols in menus." data-og-image=""></p><h2 id="conclusion">Conclusion</h2><p>It’s pretty obvious how I feel. I’m tired of all this visual noise in my menus.</p><p>And now that Apple has seemingly thrown in with the “stick an icon in every menu by default” crowd, it’s harder than ever for me to convince people otherwise. To persuade, “Hey, unless you can articulate a really good reason to add this, maybe our default posture should be no icons in menus?”</p><p>So I guess this is the world I live in now. Icons in menus. Icons in menus everywhere.</p><p>Send help.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Trials avoid high risk patients and underestimate drug harms (102 pts)]]></title>
            <link>https://www.nber.org/papers/w34534</link>
            <guid>46196308</guid>
            <pubDate>Mon, 08 Dec 2025 19:07:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nber.org/papers/w34534">https://www.nber.org/papers/w34534</a>, See on <a href="https://news.ycombinator.com/item?id=46196308">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <a id="main-content" tabindex="-1"></a>
        <div id="block-nber-breadcrumbs">
  
    
        
  
      <nav aria-label="You are here:">
      
      <ul>
                  <li>
                          <a href="https://www.nber.org/">Home</a>
                      </li>
                  <li>
                          <a href="https://www.nber.org/research">Research</a>
                      </li>
                  <li>
                          <a href="https://www.nber.org/papers">Working Papers</a>
                      </li>
                  <li>
                          Trials Avoid High Risk Patients and…
                      </li>
              </ul>
    </nav>
  
  </div>

  
  
        

<div>
  <div>
        <p><span>Working Paper</span> 34534
  </p>

        <p><span>DOI</span> 10.3386/w34534
  </p>

        <p><span>Issue Date</span> <time datetime="2025-12-04T12:00:00Z">December 2025</time>

  </p>

          </div>
  <div>
    <p>
The FDA does not formally regulate representativeness, but if trials under-enroll vulnerable patients, the resulting evidence may understate harm from drugs. We study the relationship between trial participation and the risk of drug-induced adverse events for cancer medications using data from the Surveillance, Epidemiology, and End Results Program linked to Medicare claims. Initiating treatment with a cancer drug increases the risk of hospitalization due to serious adverse events (SAE) by 2 percentage points per month (a 250% increase). Heterogeneity in SAE treatment effects can be predicted by patient's comorbidities, frailty, and demographic characteristics. Patients at the 90th percentile of the risk distribution experience a 2.5 times greater increase in SAEs after treatment initiation compared to patients at the 10th percentile of the risk distribution yet are 4 times less likely to enroll in trials. The predicted SAE treatment effects for the drug's target population are 15% larger than the predicted SAE treatment effects for trial enrollees, corresponding to 1 additional induced SAE hospitalization for every 25 patients per year of treatment. We formalize conditions under which regulating representativeness of SAE risk will lead to more externally valid trials, and we discuss how our results could inform regulatory requirements.
</p>
  </div>
  
</div>

  

  

<div>
  <ul>
          <li>
        
      </li>
    
    <li>
      <div id="accordion-body-guid2" aria-labelledby="accordion-button-guid2">
        <p>Copy Citation</p>
        <div>
            <p>
                Jason Abaluck, Leila Agha, and Sachin Shah, "Trials Avoid High Risk Patients and Underestimate Drug Harms," NBER Working Paper 34534 (2025), https://doi.org/10.3386/w34534.
            </p>

            </div>
                    <p>Download Citation</p>
            

            </div>    </li>

    
      </ul>
</div>


<div>
    <h2>Related</h2>
    <div>
    
      
                <div>
    <h3>Topics</h3>
    
  </div>

      
                <div>
    <h3>Programs</h3>
    
  </div>

      
                <div>
    <h3>Working Groups</h3>
    
  </div>

      
                        <div>
            <h3>Projects</h3>
            
          </div>
              
      
      
      
      
      
      
      
      
          </div>
  </div>


  
  
        <section id="block-morefromthenber">
    <h2>More from the NBER</h2>
    
    <div>
    

<div data-href="/research/videos/2025-17th-annual-feldstein-lecture-n-gregory-mankiw-fiscal-future">
      <p><img loading="lazy" src="https://www.nber.org/sites/default/files/styles/promo/public/2025-07/MF%20Lecture%202025%20updated.png?itok=ij7zY5fj" width="736" height="414" alt=" 2025, 17th Annual Feldstein Lecture, N. Gregory Mankiw,&quot; The Fiscal Future&quot;" typeof="foaf:Image">



    </p>
  
  

      <ul>
      <li>Feldstein Lecture</li>
    </ul>
  
      <ul>
      <li>
        Presenter:
        

              <span>
      <a href="https://www.nber.org/people/gregory_mankiw">N. Gregory Mankiw</a>    </span>
      
      </li>
    </ul>
  
  
  
      <p>N. Gregory Mankiw, Robert M. Beren Professor of Economics at Harvard University, presented the 2025 Martin Feldstein...</p>
  </div>

    

<div data-href="/research/videos/2025-methods-lecture-raj-chetty-and-kosuke-imai-uncovering-causal-mechanisms-mediation-analysis-and">
      <p><img loading="lazy" src="https://www.nber.org/sites/default/files/styles/promo/public/2025-07/Methods%20Lecture%20SI%202025_0.png?itok=hsgorA8D" width="736" height="414" alt=" 2025 Methods Lecture, Raj Chetty, &quot;Uncovering Causal Mechanisms: Mediation Analysis and Surrogate Indices&quot;" typeof="foaf:Image">



    </p>
  
  

      <ul>
      <li>Methods Lectures</li>
    </ul>
  
      <ul>
      <li>
        Presenters:
        

              <span>
      <a href="https://www.nber.org/people/raj_chetty">Raj Chetty</a>    </span>
                  <span>
       &amp; <a href="https://www.nber.org/people/kosuke_imai">Kosuke Imai</a>    </span>
      
      </li>
    </ul>
  
  
  
      <p>SlidesBackground materials on mediationImai, Kosuke, Dustin Tingley, and Teppei Yamamoto. (2013). “Experimental Designs...</p>
  </div>

    

<div data-href="/research/videos/2025-international-trade-and-macroeconomics-panel-future-global-economy">
      <p><img loading="lazy" src="https://www.nber.org/sites/default/files/styles/promo/public/2025-08/SI%20International%20trade%20Panel%202025.png?itok=2DuJTJDm" width="736" height="414" alt="2025 International Trade and Macroeconomics, &quot;Panel on The Future of the Global Economy&quot;" typeof="foaf:Image">



    </p>
  
  

      <ul>
      <li>Panel Discussion</li>
    </ul>
  
      <ul>
      <li>
        Presenters:
        

              <span>
      <a href="https://www.nber.org/people/oleg_itskhoki">Oleg Itskhoki</a>,     </span>
                  <span>
      <a href="https://www.nber.org/people/paul_krugman">Paul R. Krugman</a>    </span>
                  <span>
       &amp; <a href="https://www.nber.org/people/linda_tesar">Linda Tesar</a>    </span>
      
      </li>
    </ul>
  
  
  
      <p>Supported by the Alfred P. Sloan Foundation grant #G-2023-19633, the Lynde and Harry Bradley Foundation grant #20251294...</p>
  </div>
</div>
  </section>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Has the cost of building software dropped 90%? (247 pts)]]></title>
            <link>https://martinalderson.com/posts/has-the-cost-of-software-just-dropped-90-percent/</link>
            <guid>46196228</guid>
            <pubDate>Mon, 08 Dec 2025 19:00:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://martinalderson.com/posts/has-the-cost-of-software-just-dropped-90-percent/">https://martinalderson.com/posts/has-the-cost-of-software-just-dropped-90-percent/</a>, See on <a href="https://news.ycombinator.com/item?id=46196228">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    
    
    <div>
        <p>I've been building software professionally for nearly 20 years. I've been through a lot of changes - the 'birth' of SaaS, the mass shift towards mobile apps, the outrageous hype around blockchain, and the perennial promise that low-code would make developers obsolete.</p>
<p>The economics have changed <em>dramatically</em> now with agentic coding, and it is going to totally transform the software development industry (and the wider economy). 2026 is going to catch a lot of people off guard.</p>
<p>In my previous post I delved into why I think <a href="https://martinalderson.com/posts/are-we-in-a-gpt4-style-leap-that-evals-cant-see/">evals are missing</a> some of the big leaps, but thinking this over since then (and recent experience) has made me confident we're in the early stages of a once-in-a-generation shift.</p>
<h2>The cost of shipping</h2>
<p>I started developing just around the time open source started to really explode - but it was clear this was one of the first big shifts in cost of building custom software. I can remember eye watering costs for SQL Server or Oracle - and as such started out really with MySQL, which did allow you to build custom networked applications without incurring five or six figures of annual database licensing costs.</p>
<p>Since then we've had cloud (which I would debate is a cost saving at all, but let's be generous and assume it has some initial capex savings) and lately what I feel has been the era of complexity. Software engineering has got - in my opinion, often needlessly - complicated, with people rushing to very labour intensive patterns such as TDD, microservices, super complex React frontends and Kubernetes. I definitely don't think we've seen much of a cost decrease in the past few years.</p>
<p><img src="https://martinalderson.com/img/cost_of_shipping@2x.png" alt="Cost of shipping software over time"></p>
<p>AI Agents however in my mind <em>massively</em> reduce the labour cost of developing software.</p>
<h2>So where do the 90% savings actually come from?</h2>
<p>At the start of 2025 I was incredibly sceptical of a lot of the AI coding tools - and a lot of them I still am. Many of the platforms felt like glorified low code tooling (Loveable, Bolt, etc), or VS Code forks with some semi-useful (but often annoying) autocomplete improvements.</p>
<p>Take an average project for an internal tool in a company. Let's assume the data modelling is already done to some degree, and you need to implement a web app to manage widgets.</p>
<p>Previously, you'd have a small team of people working on setting up CI/CD, building out data access patterns and building out the core services. Then usually a whole load of CRUD-style pages and maybe some dashboards and graphs for the user to make. Finally you'd (hopefully) add some automated unit/integration/e2e tests to make sure it was fairly solid and ship it, maybe a month later.</p>
<p>And that's just the direct labour. Every person on the project adds coordination overhead. Standups, ticket management, code reviews, handoffs between frontend and backend, waiting for someone to unblock you. The actual coding is often a fraction of where the time goes.</p>
<p><em>Nearly all of this</em> can be done in a few hours with an agentic coding CLI. I've had Claude Code write an entire unit/integration test suite in a few hours (300+ tests) for a fairly complex internal tool. This would take me, or many developers I know and respect, days to write by hand.</p>
<p>The agentic coding tools have got <em>extremely</em> good at converting business logic specifications into pretty well written APIs and services.</p>
<p>A project that would have taken a month now takes a week. The thinking time is roughly the same  - the implementation time collapsed. And with smaller teams, you get the inverse of Brooks's Law: instead of communication overhead scaling with headcount, it disappears. A handful of people can suddenly achieve an order of magnitude more.</p>
<h2>Latent demand</h2>
<p>On the face of it, this seems like incredibly bad news for the software development industry - but economics tells us otherwise.</p>
<p><a href="https://en.wikipedia.org/wiki/Jevons_paradox">Jevons Paradox</a> says that when something becomes cheaper to produce, we don't just do the same amount for less money. Take electric lighting for example; while sales of candles and gas lamps fell, overall <em>far</em> more artificial light was generated.</p>
<p>If we apply this to software engineering, think of supply and demand. There is <em>so much</em> latent demand for software. I'm sure every organisation has hundreds if not thousands of Excel sheets tracking important business processes that would be far better off as a SaaS app. Let's say they get a quote from an agency to build one into an app for $50k - only essential ones meet the grade. At $5k (for a decent developer + AI tooling) - suddenly there is far more demand.</p>
<p><img src="https://martinalderson.com/img/latent_demand@2x.png" alt="Latent demand for software"></p>
<h2>Domain knowledge is the only moat</h2>
<p>So where does that leave us? Right now there is still enormous value in having a human 'babysit' the agent - checking its work, suggesting the approach and shortcutting bad approaches. Pure YOLO vibe coding ends up in a total mess very quickly, but with a human in the loop I think you can build incredibly good quality software, <em>very</em> quickly.</p>
<p>This then allows developers who really master this technology to be hugely effective at solving business problems. Their domain and industry knowledge becomes a huge lever - knowing the best architectural decisions for a project, knowing which framework to use and which libraries work best.</p>
<p>Layer on understanding of the business domain and it does genuinely feel like the mythical 10x engineer is here. Equally, the pairing of a business domain expert with a motivated developer and these tools becomes an incredibly powerful combination, and something I think we'll see becoming quite common - instead of a 'squad' of a business specialist and a set of developers, we'll see a far tighter pairing of a couple of people.</p>
<p>This combination allows you to iterate incredibly quickly, and software becomes almost disposable - if the direction is bad, then throw it away and start again, using those learnings. This takes a fairly large mindset shift, but the hard work is the <em>conceptual thinking</em>, not the typing.</p>
<h2>Don't get caught off guard</h2>
<p>The agents and models are still improving rapidly, which I don't think is really being captured in the benchmarks. Opus 4.5 seems to be able to follow long 10-20 minute sessions without going completely off piste. We're just starting to see the results of the hundreds of billions of dollars of capex that has gone into GB200 GPUs now, and I'm sure newer models will quickly make these look completely obsolete.</p>
<p>However, I've spoken to so many software engineers that are really fighting this change. I've heard the same objections too many times - LLMs make too many mistakes, it can't understand <code>[framework]</code>, or it doesn't really save any time.</p>
<p>These assertions are rapidly becoming completely false, and remind me a lot of the desktop engineers who dismissed the iPhone in 2007. I think we all know how that turned out - networking got better, the phones got way faster and the mobile operating systems became very capable.</p>
<p>Engineers need to really lean in to the change in my opinion. This won't change overnight - large corporates are still very much behind the curve in general, lost in a web of bureaucracy of vendor approvals and management structures that leave them incredibly vulnerable to smaller competitors.</p>
<p>But if you're working for a smaller company or team and have the power to use these tools, you should. Your job is going to change - but software has always changed. Just perhaps this time it's going to change faster than anyone anticipates. 2026 is coming.</p>
<p>One objection I hear a lot is that LLMs are only good at greenfield projects. I'd push back hard on this. I've spent plenty of time trying to understand 3-year-old+ codebases where everyone who wrote it has left. Agents make this dramatically easier - explaining what the code does, finding the bug(s), suggesting the fix. I'd rather inherit a repo written with an agent and a good engineer in the loop than one written by a questionable quality contractor who left three years ago, with no tests, and a spaghetti mess of classes and methods.</p>

    </div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jepsen: NATS 2.12.1 (340 pts)]]></title>
            <link>https://jepsen.io/analyses/nats-2.12.1</link>
            <guid>46196105</guid>
            <pubDate>Mon, 08 Dec 2025 18:51:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jepsen.io/analyses/nats-2.12.1">https://jepsen.io/analyses/nats-2.12.1</a>, See on <a href="https://news.ycombinator.com/item?id=46196105">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><a href="https://nats.io/">NATS</a> is a distributed streaming system. Regular NATS streams offer only best-effort delivery, but a subsystem, called JetStream, guarantees messages are delivered at least once. We tested NATS JetStream, version 2.12.1, and found that it lost writes if data files were truncated or corrupted on a minority of nodes. We also found that coordinated power failures, or an OS crash on a single node combined with network delays or process pauses, can cause the loss of committed writes and persistent split-brain. This data loss was caused (at least in part) by choosing to flush writes to disk every two minutes, rather than before acknowledging them. We also include a belated note on data loss due to process crashes in version 2.10.22, which was fixed in 2.10.23. NATS has now documented the risk of its default <code>fsync</code> policy, and the remaining issues remain under investigation. This research was performed independently by Jepsen, without compensation, and conducted in accordance with the <a href="https://jepsen.io/analyses/ethics">Jepsen ethics policy</a>.</p><article>
  <div>
<h2 data-number="1" id="background"> Background</h2>
<p><a href="https://nats.io/">NATS</a> is a popular streaming system. Producers <a href="https://docs.nats.io/nats-concepts/core-nats/pubsub">publish messages to streams</a>, and consumers subscribe to those streams, fetching messages from them. Regular NATS streams are allowed to drop messages. However, NATS has a subsystem called <a href="https://docs.nats.io/nats-concepts/jetstream">JetStream</a>, which <a href="https://docs.nats.io/running-a-nats-service/configuration/clustering/jetstream_clustering">uses</a> the <a href="https://raft.github.io/">Raft consensus algorithm</a> to replicate data among nodes. JetStream promises <a href="https://docs.nats.io/nats-concepts/jetstream#exactly-once-semantics">“at least once”</a> delivery: messages may be duplicated, but acknowledged messages<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> should not be lost.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> Moreover, JetStream streams are <a href="https://docs.nats.io/nats-concepts/jetstream#persistent-and-consistent-distributed-storage">totally ordered logs</a>.</p>
<p>JetStream is intended to <a href="https://docs.nats.io/nats-concepts/jetstream">“self-heal and always be available”</a>. The documentation also states that <a href="https://docs.nats.io/nats-concepts/jetstream#persistent-and-consistent-distributed-storage">“the formal consistency model of NATS JetStream is Linearizable”</a>. At most one of these claims can be true: the <a href="https://www.cs.princeton.edu/courses/archive/spr22/cos418/papers/cap.pdf">CAP theorem</a> tells us that <a href="https://jepsen.io/consistency/models/linearizable">Linearizable</a> systems can not be totally available.<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> In practice, they tend to be available so long as a majority of nodes are non-faulty and communicating. If, say, a single node loses network connectivity, operations must fail on that node. If three out of five nodes crash, all operations must fail.</p>
<p>Indeed, a <a href="https://docs.nats.io/nats-concepts/jetstream#persistent-and-consistent-distributed-storage">later section</a> of the JetStream docs acknowledges this fact, saying that streams with three replicas can tolerate the loss of one server, and those with five can tolerate the simultaneous loss of two.</p>
<blockquote>
<p>Replicas=5 - Can tolerate simultaneous loss of two servers servicing the stream. Mitigates risk at the expense of performance.</p>
</blockquote>
<p>When does NATS guarantee a message will be durable? The <a href="https://docs.nats.io/using-nats/developer/develop_jetstream?q=sync#publish-to-a-stream">JetStream developer docs</a> say that once a JetStream client’s <code>publish</code> request is acknowledged by the server, that message has “been successfully persisted”. The <a href="https://docs.nats.io/running-a-nats-service/configuration/clustering/jetstream_clustering#the-quorum">clustering configuration documentation</a> says:</p>
<blockquote>
<p>In order to ensure data consistency across complete restarts, a quorum of servers is required. A quorum is ½ cluster size + 1. This is the minimum number of nodes to ensure at least one node has the most recent data and state after a catastrophic failure. So for a cluster size of 3, you’ll need at least two JetStream enabled NATS servers available to store new messages. For a cluster size of 5, you’ll need at least 3 NATS servers, and so forth.</p>
</blockquote>
<p>With these guarantees in mind, we set out to test NATS JetStream behavior under a variety of simulated faults.</p>
<h2 data-number="2" id="test-design"> Test Design</h2>
<p>We designed a <a href="https://github.com/jepsen-io/nats">test suite</a> for NATS JetStream using the <a href="https://github.com/jepsen-io/jepsen">Jepsen testing library</a>, using <a href="https://github.com/nats-io/nats.java">JNATS</a> (the official Java client) at version 2.24.0. Most of our tests ran in Debian 12 containers under LXC; <a href="https://github.com/jepsen-io/nats/tree/4760f97f86350c5c9983478656dbcbcdade33817/antithesis">some tests</a> ran in <a href="https://antithesis.com/">Antithesis</a>, using the official NATS Docker images. In all our tests we created a single JetStream stream with a target replication factor of five. Per NATS’ recommendations, our clusters generally contained three or five nodes. We tested a variety of versions, but the bulk of this work focused on NATS 2.12.1.</p>
<p>The test harness <a href="https://github.com/jepsen-io/nats/blob/4760f97f86350c5c9983478656dbcbcdade33817/src/jepsen/nats/nemesis.clj">injected a variety of faults</a>, including process pauses, crashes, network partitions, and packet loss, as well as single-bit errors and truncation of data files. We limited file corruption to a minority of nodes. We also simulated power failure—a crash with partial amnesia—using the <a href="https://github.com/dsrhaslab/lazyfs">LazyFS</a> filesystem. LazyFS allows Jepsen to drop any writes which have not yet been flushed using a call to (e.g.) <code>fsync</code>.</p>
<p>Our tests did not measure Linearizability or <a href="https://jepsen.io/consistency/models/serializable">Serializability</a>. Instead we ran <a href="https://github.com/jepsen-io/nats/blob/4760f97f86350c5c9983478656dbcbcdade33817/src/jepsen/nats/queue.clj#L238-L266">several producer processes</a>, each bound to a single NATS client, which published globally unique values to a single JetStream stream. Each message included the process number and a sequence number within that process, so message <code>4-0</code> denoted the first <code>publish</code> attempted by process <code>4</code>, message <code>4-1</code> denoted the second, and so on. At the end of the test we ensured all nodes were running, resolved any network partitions or other faults, subscribed to the stream, and <a href="https://github.com/jepsen-io/nats/blob/4760f97f86350c5c9983478656dbcbcdade33817/src/jepsen/nats/queue.clj#L189-L201">attempted to read all acknowledged messages from the the stream</a>. Each reader called <code>fetch</code> until it had observed (at least) the last acknowledged message published by each process, or timed out.</p>
<p>We measured JetStream’s at-least-once semantics <a href="https://github.com/jepsen-io/nats/blob/4760f97f86350c5c9983478656dbcbcdade33817/src/jepsen/nats/queue.clj#L359-L424">based on the union of all published and read messages</a>. We considered a message <em>OK</em> if it was attempted and read. Messages were <em>lost</em> if they were acknowledged as published, but never read by any process. We divided lost messages into three epochs, based on the first and last OK messages written by the same process.<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a> We called those lost before the first OK message the <em>lost-prefix</em>, those lost after all the last OK message the <em>lost-postfix</em>, and all others the <em>lost-middle</em>. This helped to distinguish between lagging readers and true data loss.</p>
<p>In addition to verifying each acknowledged message was delivered to at least one consumer across all nodes, we also checked the set of messages read by all consumers connected to a specific node. We called it <em>divergence</em>, or <em>split-brain</em>, when an acknowledged message was missing from some nodes but not others.</p>
<h2 data-number="3" id="results"> Results</h2>
<p>We begin with a belated note on total data loss in version 2.10.22, then continue with four findings related to data loss and replica divergence in version 2.12.1: two with file corruption, and two with power failures.</p>
<h2 data-number="3.1" id="total-data-loss-on-crash-in-2.10.22-6888"> Total Data Loss on Crash in 2.10.22 (#6888)</h2>
<p>Before discussing version 2.12.1, we present a long-overdue finding from earlier work. In versions 2.10.20 through 2.10.22 (released 2024-10-17), we found that process crashes alone could cause the total loss of a JetStream stream and all its associated data. Subscription requests would return <code>"No matching streams for subject"</code>, and <code>getStreamNames()</code> would return an empty list. These conditions would persist for hours: <a href="https://github.com/user-attachments/files/20133999/20250509T191519.377-0500.zip">in this test run</a>, we waited 10,000 seconds for the cluster to recover, but the stream never returned.</p>
<p>Jepsen reported this issue to NATS as <a href="https://github.com/nats-io/nats-server/issues/6888">#6888</a>, but it appears that NATS had already identified several potential causes for this problem and resolved them. In <a href="https://github.com/nats-io/nats-server/pull/5946">#5946</a>, a cluster-wide crash occurring shortly after a stream was created could cause the loss of the stream. A new leader would be elected with a snapshot which preceded the creation of the stream, and replicate that empty snapshot to followers, causing everyone to delete their copy of the stream. In <a href="https://github.com/nats-io/nats-server/pull/5700">#5700</a>, tests running in <a href="https://antithesis.com/">Antithesis</a> found that out-of-order delivery of snapshot messages could cause streams to be deleted and re-created as well. In <a href="https://github.com/nats-io/nats-server/pull/6061">#6061</a>, process crashes could cause nodes to delete their local Raft state. All of these fixes were released as a part of 2.10.23, and we no longer observed the problem in that version.</p>
<h2 data-number="3.2" id="lost-writes-with-.blk-file-corruption-7549"> Lost Writes With <code>.blk</code> File Corruption (#7549)</h2>
<p>NATS has several checksum mechanisms meant to detect data corruption in on-disk files. However, we found that single-bit errors or truncation of JetStream’s <code>.blk</code> files could cause the cluster to lose large windows of writes. This occurred even when file corruption was limited to just one or two nodes out of five. For instance, <a href="https://s3.amazonaws.com/jepsen.io/analyses/nats-2.12.1/20251116T061217-blk-bitflip.zip">file corruption in this test run</a> caused NATS to lose 679,153 acknowledged writes out of 1,367,069 total, including 201,286 which were missing even though later values written by the same process were later read.</p>
<p><img src="https://jepsen.io/analyses/nats-2.12.1/blk-bitflip-loss.png" alt="A timeseries plot of write loss over time. A large block of writes is lost around sixty seconds, followed by a few which survive, and then the rest of the successfully acknowledged writes are lost as well."><br>
</p>
<p>In some cases, file corruption caused the quiet loss of <a href="https://s3.amazonaws.com/jepsen.io/analyses/nats-2.12.1/20251116T030143-blk-bitflip-single-loss.zip">just a single message</a>. In others, writes vanished in large blocks. Even worse, bitflips could cause split-brain, where different nodes returned different sets of messages. In <a href="https://s3.amazonaws.com/jepsen.io/analyses/nats-2.12.1/20251120T093731-blk-bitflip-split-brain.zip">this test</a>, NATS acknowledged a total of 1,479,661 messages. However, single-bit errors in <code>.blk</code> files on nodes <code>n1</code> and <code>n3</code> caused nodes <code>n1</code>, <code>n3</code>, and <code>n5</code> to lose up to 78% of those acknowledged messages. Node <code>n1</code> lost 852,413 messages, and nodes <code>n3</code> and <code>n5</code> lost 1,167,167 messages, despite <code>n5</code>’s data files remaining intact. Messages were lost in prefix, middle, and postfix: the stream, at least on those three nodes, resembled Swiss cheese.</p>
<p>NATS is investigating this issue (<a href="https://github.com/nats-io/nats-server/issues/7549">#7549</a>).</p>
<h2 data-number="3.3" id="total-data-loss-with-snapshot-file-corruption-7556"> Total Data Loss With Snapshot File Corruption (#7556)</h2>
<p>When we truncated or introduced single-bit errors into JetStream’s snapshot files in <code>data/jetstream/$SYS/_js_/</code>, we found that nodes would sometimes decide that a stream had been orphaned, and delete all its data files. This happened even when only a minority of nodes in the cluster experienced file corruption. The cluster would never recover quorum, and the stream remained unavailable for the remainder of the test.</p>
<p>In <a href="https://s3.amazonaws.com/jepsen.io/analyses/nats-2.12.1/20251115T142345-snap-bitflip-quorum-break.zip">this test run</a>, we introduced single-bit errors into snapshots on nodes <code>n3</code> and <code>n5</code>. During the final recovery period, node <code>n3</code> became the metadata leader for the cluster and decided to clean up <code>jepsen-stream</code>, which stored all the test’s messages.</p>
<pre><code>[1010859] 2025/11/15 20:27:02.947432 [INF]
Self is new JetStream cluster metadata leader
[1010859] 2025/11/15 20:27:14.996174 [WRN]
Detected orphaned stream 'jepsen &gt;
jepsen-stream', will cleanup</code></pre>
<p>Nodes <code>n3</code> and <code>n5</code> then deleted all files in the stream directory. This might seem defensible—after all, some of <code>n3</code>’s data files <em>were</em> corrupted. However, <code>n3</code> managed to become the leader of the cluster despite its corrupt state! In general, leader-based consensus systems must be careful to ensure that any node which becomes a leader is aware of majority committed state. Becoming a leader, then opting to delete a stream full of committed data, is particularly troubling.</p>
<p>Although nodes <code>n1</code>, <code>n2</code>, and <code>n4</code> retained their data files, <code>n1</code> struggled to apply snapshots; <code>n4</code> declared that <code>jepsen-stream</code> had no quorum and stalled. Every attempt to subscribe to the stream threw <code>[SUB-90007] No matching streams for subject</code>. Jepsen filed issue <a href="https://github.com/nats-io/nats-server/issues/7556">#7556</a> for this, and the NATS team is looking into it.</p>
<h2 data-number="3.4" id="lazy-fsync-by-default-7564"> Lazy <code>fsync</code> by Default (#7564)</h2>
<p>NATS JetStream promises that once a <code>publish</code> call has been acknowledged, it is “successfully persisted”. This is not exactly true. By default, NATS calls <code>fsync</code> to flush data to disk only once every two minutes, but acknowledges messages immediately. Consequently, recently acknowledged writes are generally <em>not</em> persisted, and could be lost to coordinated power failure, kernel crashes, etc. For instance, simulated power failures in <a href="https://github.com/user-attachments/files/23631053/20251119T075152.133-0600.zip">this test run</a> caused NATS to lose roughly thirty seconds of writes: 131,418 out of 930,005 messages.</p>
<p><img src="https://jepsen.io/analyses/nats-2.12.1/power-loss.png" alt="A timeseries plot of data loss over time. Acknowledged writes are fine for the first 125 seconds, then all acknowledged writes are lost for the remainder of the test."><br>
</p>
<p>Because the default flush interval is quite large, even killing a single node at a time is sufficient to cause data loss, so long as nodes fail within a few seconds of each other. In <a href="https://github.com/user-attachments/files/23631363/20251119T085347.396-0600.zip">this run</a>, a series of single-node failures in the first two minutes of the test caused NATS to delete the entire stream, along with all of its messages.</p>
<p>There are only two mentions of this behavior in the NATS documentation. The first is in the <a href="https://docs.nats.io/release-notes/whats_new/whats_new_210">2.10 release notes</a>. The second, <a href="https://docs.nats.io/running-a-nats-service/configuration">buried in the configuration docs</a>, describes the <code>sync_interval</code> option:</p>
<blockquote>
<p>Change the default fsync/sync interval for page cache in the filestore. By default JetStream relies on stream replication in the cluster to guarantee data is available after an OS crash. If you run JetStream without replication or with a replication of just 2 you may want to shorten the fsync/sync interval. You can force an fsync after each messsage [sic] with <code>always</code>, this will slow down the throughput to a few hundred msg/s.</p>
</blockquote>
<p>Consensus protocols often require that nodes sync to disk before acknowledging an operation. For example, the famous 2007 paper <a href="https://www.cs.utexas.edu/~lorenzo/corsi/cs380d/papers/paper2-1.pdf">Paxos Made Live</a> remarks:</p>
<blockquote>
<p>Note that all writes have to be flushed to disk immediately before the system can proceed any further.</p>
</blockquote>
<p>The <a href="https://web.stanford.edu/~ouster/cgi-bin/papers/OngaroPhD.pdf">Raft thesis</a> on which NATS is based is clear that nodes must “flush [new log entries] to their disks” before acknowledging. Section 11.7.3 discusses the possibility of instead writing data to disk asynchronously, and concludes:</p>
<blockquote>
<p>The trade-off is that data loss is possible in catastrophic events. For example, if a majority of the cluster were to restart simultaneously, the cluster would have potentially lost entries and would not be able to form a new view. Raft could be extended in similar ways to support disk-less operation, but we think the risk of availability or data loss usually outweighs the benefits.</p>
</blockquote>
<p>For similar reasons, replicated systems like <a href="https://www.mongodb.com/docs/manual/reference/replica-configuration/#mongodb-rsconf-rsconf.writeConcernMajorityJournalDefault">MongoDB</a>, <a href="https://www.redhat.com/en/blog/a-guide-to-etcd">etcd</a>, <a href="https://docs.tigerbeetle.com/single-page/">TigerBeetle</a>, <a href="https://thinkingaboutdistributedsystems.blogspot.com/2017/09/what-we-can-learn-from-zookeepers.html">Zookeeper</a>, <a href="https://www.redpanda.com/blog/top-performance-considerations-redpanda">Redpanda</a>, and <a href="https://docs.pingcap.com/tidb/stable/release-5.0.0/#configuration-file-parameters">TiDB</a> sync data to disk before acknowledging an operation as committed.</p>
<p>However, some systems do choose to <code>fsync</code> asynchronously. YugabyteDB’s default is <a href="https://docs.yugabyte.com/stable/reference/configuration/yb-master/#durable-wal-write">to acknowledge un-fsynced writes</a>. Liskov and Cowling’s <a href="http://pmg.csail.mit.edu/papers/vr-revisited.pdf">Viewstamped Replication Revisited</a> assumes replicas are “highly unlikely to fail at the same time”—but acknowledges that if they were to fail simultaneously, state would be lost. Apache Kafka <a href="https://jack-vanlightly.com/blog/2023/4/24/why-apache-kafka-doesnt-need-fsync-to-be-safe">makes a similar choice</a>, but claims that it is not vulnerable to coordinated failure because Kafka “doesn’t store unflushed data in its own memory, but in the page cache”. This offers resilience to the Kafka process itself crashing, but not power failure.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a> Jepsen remains skeptical of this approach: as <a href="https://www.usenix.org/system/files/conference/osdi16/osdi16-alagappan.pdf">Alagappan et al.</a> argue, <a href="https://www.usenix.org/system/files/conference/atc13/atc13-cidon.pdf">extensive</a> <a href="https://pages.cs.wisc.edu/~akella/CS838/F15/838-CloudPapers/hdfs.pdf">literature</a> <a href="https://static.googleusercontent.com/media/research.google.com/en//people/jeff/SOCC2010-keynote-slides.pdf">on</a> <a href="https://aws.amazon.com/message/2329B7/">correlated</a> <a href="https://www.datacenterknowledge.com/cloud/lightning-in-belgium-disrupts-google-cloud-services-updated-">failures</a> <a href="https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Ford.pdf">suggests</a> <a href="https://haeberlen.cis.upenn.edu/papers/glacier-nsdi2005.pdf">we</a> <a href="http://issg.cs.duke.edu/publications/disasters-fast04.pdf">should</a> <a href="https://web.archive.org/web/20080108203642/https://radar.oreilly.com/archives/2007/07/365_main_datace.html">continue</a> <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45499.pdf">to</a> <a href="https://www.energy.gov/oe/august-2003-blackout">take</a> <a href="https://www.webofscience.com/wos/woscc/full-record/WOS:000245079400017?&amp;SID=USW2EC0C58xDjRHQcDMVJJIWW5emx">this</a> <a href="https://web.archive.org/web/20170519044830/https://www.joyent.com/blog/postmortem-for-outage-of-us-east-1-may-27-2014">risk</a> <a href="https://pace.gatech.edu/2025/04/02/cooling-failure-in-coda-datacenter/">seriously</a>. Heat waves, grid instability, fires, lightning, tornadoes, and floods are not necessarily constrained to a single availability zone.</p>
<p>Jepsen suggests that NATS change the default value for <code>fsync</code> to <code>always</code>, rather than every two minutes. Alternatively, NATS documentation should prominently disclose that JetStream may lose data when nodes experience correlated power failure, or fail in rapid succession (<a href="https://github.com/nats-io/nats-server/issues/7564">#7564</a>).</p>
<h2 data-number="3.5" id="a-single-os-crash-can-cause-split-brain-7567"> A Single OS Crash Can Cause Split-Brain (#7567)</h2>
<p>In response to #7564, NATS engineers <a href="https://github.com/nats-io/nats-server/issues/7564">noted</a> that most production deployments run with each node in a separate availability zone, which reduces the probability of correlated failure. This raises the question: how many power failures (or hardware faults, kernel crashes, etc.) are required to cause data loss? Perhaps surprisingly, in an asynchronous network the answer is “just one”.</p>
<p>To understand why, consider that a system which remains partly available when a minority of nodes are unavailable must allow states in which a committed operation is present—solely in memory—on a bare majority of nodes. For example, in a leader-follower protocol the leader of a three-node cluster may consider a write committed as soon as a single follower has responded: it has two acknowledgements, counting itself. Under normal operation there will usually be some window of committed operations in this state.<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p>

<p>Now imagine that one of those two nodes loses power and restarts. Because the write was stored only in memory, rather than on disk, the acknowledged write is no longer present on that node. There now exist two out of three nodes which do <em>not</em> have the write. Since the system is fault-tolerant, these two nodes must be able to form a quorum and continue processing requests—creating new states of the system in which the acknowledged write never happened.</p>
<p>Strictly speaking, this fault requires nothing more than a single power failure (or HW fault, kernel crash, etc.) and an asynchronous network—one which is allowed to deliver messages arbitrarily late. Whether it occurs in practice depends on the specific messages exchanged by the replication system, which node fails, how long it remains offline, the order of message delivery, and so on. However, one can reliably induce data loss by killing, pausing, or partitioning away a minority of nodes before and after a simulated OS crash.</p>
<p>For example, process pauses and a single simulated power failure in <a href="https://github.com/user-attachments/files/23638240/20251119T155654.398-0600.zip">this test run</a> caused JetStream to lose acknowledged writes for windows roughly on par with <code>sync_interval</code>. Stranger still, the cluster entered a persistent split-brain which continued after all nodes were restarted and the network healed. Consider these two plots of lost writes, based on final reads performed against nodes <code>n1</code> and <code>n5</code> respectively:</p>
<p><img src="https://jepsen.io/analyses/nats-2.12.1/single-kill-split-brain-n1.png" alt="A plot of data loss on n1. A few seconds of writes are lost around 42 seconds."><br>
</p>
<p><img src="https://jepsen.io/analyses/nats-2.12.1/single-kill-split-brain-n5.png" alt="A plot of data loss on n5. About six seconds of writes are lost at 58 seconds."><br>
</p>
<p>Consumers talking to <code>n1</code> failed to observe a short window of acknowledged messages written around 42 seconds into the test. Meanwhile, consumers talking to <code>n5</code> would miss acknowledged messages written around 58 seconds. Both windows of write loss were on the order of our choice of <code>sync_interval = 10s</code> for this run. In repeated testing, we found that any node in the cluster could lose committed writes, including the node which failed, those which received writes before the failure, and those which received writes afterwards.</p>
<p>The fact that a single power failure can cause data loss is not new. In 2023, RedPanda wrote <a href="https://www.redpanda.com/blog/why-fsync-is-needed-for-data-safety-in-kafka-or-non-byzantine-protocols">a detailed blog post</a> showing that Kafka’s default lazy <code>fsync</code> could lead to data loss under exactly this scenario. However, it is especially concerning that this scenario led to persistent replica divergence, not just data loss! We filed <a href="https://github.com/nats-io/nats-server/issues/7567">#7567</a> for this issue, and the NATS team is investigating.</p>
<table>
<thead>
<tr>
<th>№</th>
<th>Summary</th>
<th>Event Required</th>
<th>Fixed in</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/nats-io/nats-server/issues/6888">#6888</a></td>
<td>Stream deleted on crash in 2.10.22</td>
<td>Crashes</td>
<td>2.10.23</td>
</tr>
<tr>
<td><a href="https://github.com/nats-io/nats-server/issues/7549">#7549</a></td>
<td>Lost writes due to <code>.blk</code> file corruption</td>
<td>Minority truncation or bitflip</td>
<td>Unresolved</td>
</tr>
<tr>
<td><a href="https://github.com/nats-io/nats-server/issues/7556">#7556</a></td>
<td>Stream deleted due to snapshot file corruption</td>
<td>Minority truncation or bitflip</td>
<td>Unresolved</td>
</tr>
<tr>
<td><a href="https://github.com/nats-io/nats-server/issues/7564">#7564</a></td>
<td>Write loss due to lazy <code>fsync</code> policy</td>
<td>Coordinated OS crash</td>
<td>Documented</td>
</tr>
<tr>
<td><a href="https://github.com/nats-io/nats-server/issues/7567">#7567</a></td>
<td>Write loss and split-brain</td>
<td>Single OS crash and pause</td>
<td>Unresolved</td>
</tr>
</tbody>
</table>
<h2 data-number="4" id="discussion"> Discussion</h2>
<p>In NATS 2.10.22, process crashes could cause JetStream to forget a stream ever existed (#6888). This issue was identified independently by NATS and resolved in version 2.10.23, released on 2024-12-10. We did not observe data loss with simple network partitions, process pauses, or crashes in version 2.12.1.</p>
<p>However, we found that in NATS 2.12.1, file corruption and simulated OS crashes could both lead to data loss and persistent split-brain. Bitflips or truncation of either <code>.blk</code> (#7549) or snapshot (#7556) files, even on a minority of nodes, could cause the loss of single messages, large windows of messages, or even cause some nodes to delete their stream data altogether. Messages could be missing on some nodes and present on others. NATS has multiple checksum mechanisms designed to limit the impact of file corruption; more thorough testing of these mechanisms seems warranted.</p>
<p>By default, NATS only flushes data to disk every two minutes, but acknowledges operations immediately. This approach can lead to the loss of committed writes when several nodes experience a power failure, kernel crash, or hardware fault concurrently—or in rapid succession (#7564). In addition, a single OS crash combined with process crashes, pauses, or network partitions can cause the loss of acknowledged messages and persistent split-brain (#7567). We recommended NATS change the default value of <code>fsync</code> to <code>always</code>, or clearly document these hazards. NATS has <a href="https://github.com/nats-io/nats.docs/pull/896">added new documentation</a> to the <a href="https://docs.nats.io/nats-concepts/jetstream#persistent-and-consistent-distributed-storage">JetStream Concepts page</a>.</p>
<p>This documentation <a href="https://docs.nats.io/nats-concepts/jetstream#goals">also describes</a> several goals for JetStream, including that “[t]he system must self-heal and always be available.” This is impossible: the CAP theorem states that Linearizable systems cannot be totally available in an asynchronous network. In our three and five-node clusters JetStream generally behaved like a typical Raft implementation. Operations proceeded on a majority of connected nodes but isolated nodes were unavailable, and if a majority failed, the system as a whole became unavailable. Jepsen suggests clarifying this part of the documentation.</p>
<p>As always, Jepsen takes an experimental approach to safety verification: we can prove the presence of bugs, but not their absence. While we make extensive efforts to find problems, we cannot prove correctness.</p>
<h2 data-number="4.1" id="lazyfs"> LazyFS</h2>
<p>This work demonstrates that systems which do not exhibit data loss under normal process crashes (e.g.&nbsp;<code>kill -9 &lt;PID&gt;</code>) may lose data or enter split-brain under simulated OS-level crashes. Our tests relied heavily on <a href="https://github.com/dsrhaslab/lazyfs">LazyFS</a>, a project of <a href="https://www.inesctec.pt/en">INESC TEC</a> at the University of Porto.<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a> After killing a process, we used LazyFS to simulate the effects of a power failure by dropping writes to the filesystem which had not yet been <code>fsync</code>ed to disk.</p>
<p>While this work focused purely on the loss of unflushed writes, LazyFS can also simulate linear and non-linear torn writes: an anomaly where a storage device persists part, but not all, of written data thanks to (e.g.) IO cache reordering. Our 2024 paper <a href="https://www.vldb.org/pvldb/vol17/p3017-ramos.pdf">When Amnesia Strikes</a> discusses these faults in more detail, highlighting bugs in PostgreSQL, Redis, ZooKeeper, etcd, LevelDB, PebblesDB, and the Lightning Network.</p>
<h2 data-number="4.2" id="future-work"> Future Work</h2>
<p>We designed only a simple workload for NATS which checked for lost records either across all consumers, or across all consumers bound to a single node. We did not check whether single consumers could miss messages, or the order in which they were delivered. We did not check NATS’ claims of Linearizable writes or Serializable operations in general. We also did not evaluate JetStream’s “exactly-once semantics”. All of these could prove fruitful avenues for further tests.</p>
<p>In some tests, we <a href="https://github.com/jepsen-io/nats/blob/4760f97f86350c5c9983478656dbcbcdade33817/src/jepsen/nats/nemesis.clj#L67-L263">added and removed</a> nodes from the cluster. This work <a href="https://github.com/nats-io/nats-server/issues/7545">generated some preliminary results</a>. However, the NATS documentation for membership changes was incorrect and incomplete: it gave <a href="https://github.com/nats-io/nats.docs/pull/893">the wrong command</a> for removing peers, and there appears to be an undocumented but mandatory <a href="https://github.com/nats-io/nats-server/issues/7545#issuecomment-3528168499">health check step</a> for newly-added nodes. As of this writing, Jepsen is unsure how to safely add or remove nodes to a NATS cluster. Consequently, we leave membership changes for future research.</p>
<p><em>Our thanks to <a href="https://www.inesctec.pt/en">INESC TEC</a> and everyone on the LazyFS team, including Maria Ramos, João Azevedo, José Pereira, Tânia Esteves, Ricardo Macedo, and João Paulo. Jepsen is also grateful to Silvia Botros, Kellan Elliott-McCrea, Carla Geisser, Coda Hale, and Marc Hedlund for their expertise regarding datacenter power failures, correlated kernel panics, disk faults, and other causes of OS-level crashes. Finally, our thanks to <a href="https://www.irenekannyo.com/">Irene Kannyo</a> for her editorial support. This research was performed independently by Jepsen, without compensation, and conducted in accordance with the <a href="https://jepsen.io/analyses/ethics">Jepsen ethics policy</a>.</em></p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>Throughout this report we use “acknowledged message” to describe a message whose <code>publish</code> request was acknowledged successfully by some server. NATS also offers a separate notion of acknowledgement, which indicates when a message has been processed and need not be delivered again.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>JetStream also promises “exactly once semantics” in some scenarios. We leave this for later research.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>The CAP theorem’s definition of “availability” requires that all operations on non-faulty nodes must succeed.<a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>This is overly conservative: in a system with Linearizable writes, we should never observe a lost message which was acknowledged prior to the invocation of the <code>publish</code> call for an OK message, regardless of process. However, early testing with NATS suggested that it might be better to test a weaker property, and come to stronger conclusions about data loss.<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p><a href="https://www.redpanda.com/blog/why-fsync-is-needed-for-data-safety-in-kafka-or-non-byzantine-protocols">Redpanda argues</a> that the situation is actually worse: a single power failure, combined with network partitions or process pauses, can cause Kafka to lose committed data.<a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>Some protocols, like Raft, consider an operation committed as soon as it is acknowledged by a majority of nodes. These systems offer lower latencies, but at any given time there are likely a few committed operations which are missing from a minority of nodes due to normal network latency. Other systems, like Kafka, require acknowledgement from <em>all</em> “online” nodes before considering an operation committed. These systems offer worse latency in healthy clusters (since they must wait for the slowest node) but in exchange, committed operations can only be missing from some node when the fault detector decides that node is no longer online (e.g.&nbsp;due to elevated latency).<a href="#fnref6" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>Jepsen contributed some funds, testing, and integration assistance to LazyFS, but most credit belongs to the LazyFS team.<a href="#fnref7" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
  </div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deep dive on Nvidia circular funding (292 pts)]]></title>
            <link>https://philippeoger.com/pages/deep-dive-into-nvidias-virtuous-cycle</link>
            <guid>46196076</guid>
            <pubDate>Mon, 08 Dec 2025 18:48:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://philippeoger.com/pages/deep-dive-into-nvidias-virtuous-cycle">https://philippeoger.com/pages/deep-dive-into-nvidias-virtuous-cycle</a>, See on <a href="https://news.ycombinator.com/item?id=46196076">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        
    <h2 id="nvidia-frenemy-relation-with-openai-and-oracle">NVIDIA frenemy relation with OpenAI and Oracle</h2>
<p>I’ve spent the last 48 hours completely falling down the rabbit hole of
<a href="https://nvidianews.nvidia.com/">NVIDIA’s Q3 Fiscal 2026 earnings report</a>. If
you just skim the headlines, everything looks perfect: Revenue is up 62% to $57
billion, and Jensen Huang is talking about a "virtuous cycle of AI."</p>
<p>But I wanted to understand what was <em>really</em> happening under the hood, so I dug
into the balance sheet and cross-referenced it with all the news swirling
around OpenAI and Oracle. I’m not a professional Wall Street analyst, but even
just connecting the dots myself (with the help of Gemini), I’m seeing some cracks in the "AI Alliance."
While NVIDIA posts record numbers, it feels like their biggest customers are
quietly arming themselves for a breakout.</p>
<p>Here is my take on the hardware market, the "frenemy" dynamics between OpenAI
and NVIDIA, and the "circular financing" theories that everyone—including
Michael Burry, has been talking about.</p>
<p>Here is a quick summary of the points I'll discuss below:</p>
<ul>
<li><a href="#nvidias-earnings-perfection-with-a-side-of-stress">NVIDIA’s Earnings: Perfection with a side of stress</a></li>
<li><a href="#making-sense-of-the-round-tripping-news">Making Sense of the Round-Tripping News</a></li>
<li><a href="#openai-making-moves-to-reduce-dependency-on-nvidia">OpenAI making moves to reduce dependency on NVIDIA</a></li>
<li><a href="#an-interesting-idea-for-oracle-groq-acquisition">An interesting idea for Oracle: Groq acquisition</a></li>
<li><a href="#final-thoughts">Final Thoughts</a></li>
</ul>
<h2 id="nvidias-earnings-perfection-with-a-side-of-stress">NVIDIA’s Earnings: Perfection with a side of stress</h2>
<p>On the surface, NVIDIA is the absolute monarch of the AI era. You can’t argue
with a Data Center segment that now makes up nearly 90% of the company's
business. However, when I looked closer at the financials, I found three
specific things that stood out to me as "red flags."</p>
<ul>
<li><strong>The Cash Flow Mystery:</strong> NVIDIA reported a massive <strong>$31.9 billion in Net
  Income</strong>, but when I checked the cash flow statement, they only generated
  <strong>$23.8 billion in Operating Cash Flow</strong>. That is an $8 billion gap where
  profits aren't converting to cash immediately.</li>
<li><strong>The Inventory Balloon:</strong> I noticed that inventory has nearly doubled this
  year, hitting <strong>$19.8 billion</strong>. Management says this is to prep for the
  "Blackwell" launch, but holding ~120 days of inventory seems like a huge
  capital drag to me.</li>
<li><strong>The "Paper" Chase:</strong> I calculated their Days Sales Outstanding (DSO), and
  it has crept up to about <strong>53 days</strong>. As revenue skyrockets, NVIDIA is
  waiting nearly two months to get paid, which suggests they might be extending
  massive credit terms to enterprise clients to keep the flywheel spinning.</li>
</ul>
<p>My personal read? NVIDIA is "burning the furniture" to build inventory, betting
everything that the <a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/">Blackwell architecture</a>
will sell out instantly in Q4.</p>
<h2 id="making-sense-of-the-round-tripping-news">Making Sense of the Round-Tripping News</h2>
<p>I want to be clear: I didn't discover this next part. It’s been all over the
financial news lately, and if you follow <strong>Michael Burry</strong> (the "Big Short"
guy), you’ve probably seen his tweets warning about "circular financing" and
<a href="https://www.investing.com/news/stock-market-news/michael-burry-warns-of-suspicious-revenue-recognition-after-nvidia-earnings-4369197">suspicious revenue recognition</a>.</p>
<p>I wanted to map it out for myself to see what the fuss was about. Burry shared
a chart recently that visualizes a "web" of deals, and it looks something like
this:</p>
<ol>
<li><strong>Leg 1:</strong> NVIDIA pledges billions (part of a widely reported $100B
    investment roadmap) to <strong>OpenAI</strong>.</li>
<li><strong>Leg 2:</strong> OpenAI signs a massive <strong>$300 billion</strong> cloud contract with
    <strong>Oracle</strong> (Project Stargate) to host its models.</li>
<li><strong>Leg 3:</strong> To fulfill that contract, Oracle turns around and places a <strong>$40
    billion</strong> order for NVIDIA’s GB200 GPUs.</li>
</ol>
<p>Here is the Nano Banana Pro generation I just did for the visual people out there:</p>
<p><img alt="NVIDIA-OpenAI-Oracle Circular Financing" src="https://philippeoger.com/static/img/circular-funding.png"></p>
<p>Burry’s argument, and the reason <a href="https://m.economictimes.com/news/international/us/nvidia-rejects-circular-financing-claims-as-top-short-sellers-push-back/articleshow/125589622.cms">regulators like the DOJ are reportedly looking
into this</a>—is
that this mimics "Round-Tripping." It raises a tough question: If NVIDIA
stopped investing in OpenAI, would OpenAI still have the cash to sign that deal
with Oracle? And would Oracle still buy those chips? If the answer is "no,"
then some of that revenue might be more fragile than it looks.</p>
<h2 id="openai-making-moves-to-reduce-dependency-on-nvidia">OpenAI making moves to reduce dependency on NVIDIA</h2>
<p>The other big shift I’ve been tracking is OpenAI’s pivot. They used to be
NVIDIA’s star pupil, but now they look more like a future rival.
On one hand, they are hugging NVIDIA tight—deploying 10 gigawatts of infrastructure to train GPT-6. But on the
other, they seem to be building a supply chain to kill their dependency on
Jensen Huang.</p>
<p>The evidence is pretty loud if you look for it. "Project Stargate" isn't just a
data center; it's a huge infrastructure plan that includes custom hardware.
OpenAI made some news buying DRAM wafers directly from Samsung and SK Hynix (the 2 main HBM
world provider), bypassing NVIDIA’s supply chain, and many others, as reported 
<a href="https://openai.com/index/samsung-and-sk-join-stargate/">here</a>, 
<a href="https://www.asiafinancial.com/samsung-sk-hynix-building-stargate-korea-using-open-ai">here</a>, or 
<a href="https://www.kedglobal.com/artificial-intelligence/newsView/ked202510010013">here</a>, and widely debated <a href="https://news.ycombinator.com/item?id=46169224#46170844">on Hacker News here</a>.</p>
<p>Plus, the talent migration is telling: OpenAI has poached
key silicon talent, including Richard Ho (Google’s former TPU
lead) back in 2023, and more recently many hardware engineers from Apple (around 40
apparently).</p>
<p>With the <a href="https://openai.com/index/openai-and-broadcom-announce-strategic-collaboration/">Broadcom partnership</a>, 
my guess is OpenAI plans to use NVIDIA GPUs to <em>create</em> intelligence, but run that
intelligence on their own custom silicon to stop bleeding cash, or by betting on 
Edge TPU-like chips for inference, similar to what Google does with its NPU chip.</p>
<p>The big question is, which money is Openai planning on using to fund this? 
and how much influence does NVIDIA has over OpenAI’s future plans?</p>
<p>The $100 billions that NVIDIA is "investing" in OpenAI is not yet confirmed neither,
as reported <a href="https://fortune.com/2025/12/02/nvidia-openai-deal-not-signed-yet-100-billion-rally-colette-kress/">here</a>,</p>
<h2 id="an-interesting-idea-for-oracle-groq-acquisition">An interesting idea for Oracle: Groq acquisition</h2>
<p>Everyone is talking about <strong>Inference</strong> costs right now, basically, how
expensive it is to actually <em>run</em> ChatGPT or any other LLMs versus training it.
Now I'm looking at <strong>Groq</strong>, a startup claiming specifically to be faster and cheaper
than NVIDIA for this task. The founder is <a href="https://www.linkedin.com/in/ross-jonathan/">Jonathan Ross</a>, 
a former Google TPU lead and literally the person that basically had the idea of TPU.</p>
<p>There is another layer to this that I think is getting overlooked as well: <strong>The
HBM Shortage</strong> created by Openai’s direct wafer purchases.</p>
<p>From what I understand, one of the biggest bottlenecks for NVIDIA right now is
HBM (High Bandwidth Memory), which is manufactured in specialized memory fabs
that are completely overwhelmed. However, Groq’s architecture relies on SRAM
(Static RAM). Since SRAM is typically built in logic fabs (like TSMC) alongside
the processors themselves, it theoretically shouldn't face the same supply
chain crunch as HBM.</p>
<p>Looking at all those pieces, I feel Oracle should seriously look into buying Groq.
Buying Groq wouldn't just give Oracle a faster chip, it could give them a chip that is
actually <em>available</em> when everything else is sold out. It’s a supply chain hedge.</p>
<p>It's also a massive edge for its main client, OpenAI, to get faster and cheaper inference.</p>
<p>Combine that with the fact that <a href="https://www.fool.com/investing/2025/12/02/michael-burry-just-sent-a-warning-to-artificial-in/">Oracle’s margins on renting NVIDIA chips are
brutal</a>, reportedly
as low as 14%, then the deal just makes sense. By owning Groq, Oracle could stop
paying the "NVIDIA Tax," fix their margins, and bypass the HBM shortage
entirely.</p>
<p>Groq currently has a valuation of around $6.9 billions, <a href="https://groq.com/newsroom/groq-raises-750-million-as-inference-demand-surges">according to its last funding round
in september 2025</a>.
Even with a premium, Oracle has financial firepower to make that acquisition happen.</p>
<p><strong>But would NVIDIA let that happen? and if the answer is no, then what does that tell us
about the circular funding in place? Is there a Quid pro quo where Nvidia agrees to invest 
100 billions in OpenAI in exchange of Oracle being exclusive to Nvidia?</strong> </p>
<h2 id="final-thoughts">Final Thoughts</h2>
<p>As we head into 2026, when looking at Nvidia, openai and Oracle dynamics, it looks like they are squeezing each 
other balls. I do not know if Nvidia knew about the Openai deal about the wafer memory supply, or was there any collusion?
Does NVIDIA is fighting to maintain exclusivity for both training and inference at Stargate? What kind of chips is Openai
planning on building ? TPU/LPU like? Or more Edge TPU? </p>
<p><a href="https://www.techradar.com/pro/security/could-the-ai-bubble-be-real-this-sage-of-the-2008-market-crash-the-central-character-of-the-big-short-certainly-thinks-so">Michael Burry is betting against the whole
thing</a>. </p>
<p>Me, I’m just a guy reading the reports, I have no way to speculate on this market. But I do know one thing: The AI hardware market 
is hotter than ever, and the next few quarters are going to be fascinating to watch.</p>
<p>I have not discussed much about TPU from Google in this article, but I cover some thoughts <a href="https://philippeoger.com/pages/why-googles-tpu-could-beat-nvidias-gpu-in-the-long-run">about the TPU vs GPU 
in a previous post recently.</a>.
It seems Google responded quickly to the current situation about the memory wafer shortage by securing a <a href="https://www.kedglobal.com/korean-chipmakers/newsView/ked202512010003">major deal 
with Samsung in 2026</a>.</p>
<p><em>Disclaimer: I say very smart things sometimes, but say stupid things a lot more. Take this in consideration when reading this blog post</em></p>

    
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Quanta to publish popular math and physics books by Terence Tao and David Tong (113 pts)]]></title>
            <link>https://www.simonsfoundation.org/2025/12/08/quanta-books-to-publish-popular-math-and-physics-titles-by-terence-tao-and-david-tong/</link>
            <guid>46195225</guid>
            <pubDate>Mon, 08 Dec 2025 17:39:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.simonsfoundation.org/2025/12/08/quanta-books-to-publish-popular-math-and-physics-titles-by-terence-tao-and-david-tong/">https://www.simonsfoundation.org/2025/12/08/quanta-books-to-publish-popular-math-and-physics-titles-by-terence-tao-and-david-tong/</a>, See on <a href="https://news.ycombinator.com/item?id=46195225">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p><a href="https://www.quantabooks.org/">Quanta Books</a> is delighted to announce two new upcoming books by mathematician Terence Tao and theoretical physicist David Tong.</p>
<p><em>Six Math Essentials</em> will be Tao’s first math book written for a popular audience. In the book, Tao — a recipient of the Fields Medal and one of the world’s top mathematicians — will explore six ideas that have guided mathematicians throughout history. This short and friendly volume is for all readers, Tao says, because he believes that “mathematics has become unnecessarily intimidating and abstruse to the general public while being more essential than ever in the modern world.” <em>Six Math Essentials</em> will be available internationally, with translated editions in Chinese, French, Greek, Italian, Polish and other languages. It will arrive in U.S. bookstores in November 2026.</p>
<p>Tong’s book, <em>Everything Is Fields</em>, will illuminate quantum field theory — the physics that explains the fundamental makeup of the universe — drawing from Tong’s distinguished track record as a quantum field theorist and public communicator. “This book reveals the hidden unity that ties together particles and forces,” says Tong. “Everything — matter, light, even you — are just waves on a restless sea known as a quantum field.”</p>
<p>“Terry Tao and David Tong are intellectual powerhouses and seasoned communicators,” says Thomas Lin, publisher of Quanta Books and founding editor of the Pulitzer Prize­–winning <a href="https://www.quantamagazine.org/"><em>Quanta Magazine</em></a>. “Their&nbsp;books embody the curiosity and ambition that animate our imprint, and I can’t wait to share them with readers everywhere.”</p>
<p>Quanta Books is an editorially independent subsidiary of the Simons Foundation and a partner imprint of <a href="https://us.macmillan.com/fsg/">Farrar, Straus and Giroux</a>. The imprint publishes books that illuminate and elucidate the central questions and fundamental ideas of modern science for readers, inviting a deeper understanding of the universe through artful storytelling. Quanta Books’ first title, <em>The Proof in the Code</em> by math journalist Kevin Hartnett, will be published in June 2026 and is available for <a href="https://us.macmillan.com/books/9780374620066/theproofinthecode/">preorder</a> now.</p>
<p>For more information, visit <a href="https://www.quantabooks.org/">QuantaBooks.org</a>.</p>
<h2><em>Six Math Essentials</em></h2>
<p>In <em>Six Math Essentials</em>, Tao, the world’s most renowned mathematician, introduces readers to six core ideas that have guided mathematicians from antiquity to the frontiers of what we know today. This elegant volume explores: numbers as the gateway to quantitative thinking, algebra as the gateway to abstraction, geometry as a way to go beyond what we can see, probability as a tool to navigate uncertainty with rigorous thinking, analysis as a means to tame the very large or very small, and dynamics as the mathematics of change. <em>Six Math Essentials</em> — Tao’s first popular math book — offers a glimpse into the workings of an incomparable mind and how he thinks about the creativity, beauty, and interconnectedness of the mathematical enterprise. Math, Tao insists, isn’t magic — it’s a powerful way of thinking that anyone can learn.</p>
<h2><em>Everything Is Fields</em></h2>
<p>In <em>Everything Is Fields</em>, Tong leads readers on a lively tour through quantum field theory. Tong, a leading theoretical physicist and University of Cambridge professor, explores Quantum field theory, or QFT. The theory forms the underlying mathematical framework of the Standard Model, the deepest description we have of the fundamental laws of physics. And, as Tong shows, it reveals a startling truth: that, at our most basic level, we are made not of particles or forces, but fields, fluid-like substances stretched throughout the entire universe. With his infectious sense of wonder and characteristic wit, Tong buoys our journey through the most difficult topic in theoretical physics. He revels in all that we’ve learned about our world and illuminates the questions we’re still trying to answer about the stuff that makes up you, me, and everything else.</p>
<h2><em>The Proof in the Code</em></h2>
<p><em>The Proof in the Code</em> is the definitive account of the birth and rise of Lean, a proof assistant developed at Microsoft that is transforming the enterprise of mathematics and ushering in a new era of human-computer collaboration. Although Lean was originally conceived of as a code-checking program, a small group of mathematicians recognized its potential to become something far more powerful: the “truth oracle” that thinkers have sought for centuries, a tool to definitively verify or refute any mathematical or logical assertion, no matter how complex. This is the story of the grassroots effort to make that dream a reality. Filled with insights about the future of math, computers, and AI, <em>The Proof in the Code</em> is a brilliant work of journalism by Hartnett, a leading math writer whose research and reporting offer a profound answer to a longstanding mystery: Can computers reveal universal truths?</p>
</div><div>
  <p>For more information, please contact <a href="https://www.simonsfoundation.org/cdn-cgi/l/email-protection#86efe8e0e9c6f7f3e7e8f2e7e4e9e9edf5a8e9f4e1"><span data-cfemail="4821262e2708393d29263c292a2727233b66273a2f">[email&nbsp;protected]</span></a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI should only run as fast as we can catch up (143 pts)]]></title>
            <link>https://higashi.blog/2025/12/07/ai-verification/</link>
            <guid>46195198</guid>
            <pubDate>Mon, 08 Dec 2025 17:38:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://higashi.blog/2025/12/07/ai-verification/">https://higashi.blog/2025/12/07/ai-verification/</a>, See on <a href="https://news.ycombinator.com/item?id=46195198">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
  <p><span>07 Dec 2025</span></p><h2 id="ai-should-only-run-as-fast-as-we-can-catch-up">AI should only run as fast as we can catch up.</h2>

<h3 id="the-story-of-daniel-and-eric">The story of Daniel and Eric</h3>

<p>Recently I have spoke with two of my friends who all had fun playing with AI.</p>

<p>Last month, I met with Eric, a fearless PM at a medium size startup who recently got into vibe coding with Gemini.
<!--more--></p>

<blockquote>
  <p>After getting familiarized with Gemini, Eric was genuinely amazed by how AI quickly turns prompt into playable web applications. It served great purpose as a first prototype to communicate ideas to designers and engineers. But Eric really wanted to skip those steps and directly ship it to prod. But he couldn’t really understand that Gemini actually built a single-page HTML file that merely looks like a working app. Sadly, one cannot build a reliable enterprise product out of this. And there is really no effective way for Eric to catch up on these technical details and outpace the engineering team himself.</p>
</blockquote>

<p>Last week, I had coffee with Daniel, a senior staff engineer who recently grew fond of AI coding and found it to be the true force multiplier.</p>

<blockquote>
  <p>Daniel was skeptical of AI at first, but lately he hasn’t wrote a single line of code for months already. What he does is just precisely prompt the AI to create new components in an existing framework (involving Kafka, postgres, AuthN/Z, and k8s infra stuff) and adhering to certain preexisting paradigms. He would just spot-check the correctness of AI’s work and quickly spin up local deployments to verify it’s indeed working. Later, he pushes the changes through code review process and lands those features. All without writing a single line of code and it’s production ready just as if he wrote them himself. To Daniel, building and shipping things fast and scalable is simpler than ever.</p>
</blockquote>

<h3 id="interpolating-between-the-two-stories">Interpolating between the two stories</h3>

<p>After speaking with Eric and Daniel, I suddenly feel that there is an overarching theme around the use of AI that we can probably interpolate out of the stories here. And after pondering for a weekend, I think I can attempt to describe it now: it’s the problem of <strong>reliable engineering - how can we make AI work reliably</strong>.</p>

<p>With the AI superpower, one can task it to do all crazy things on the internet with just typing a few lines of prompt. AI always thinks and learns faster than us, this is undeniable now. However, to make AI work actually useful (not only works, but reliable and trustworthy), we also need to catch up with what the AI does as quickly as possible.</p>

<p>It’s almost like - we need to send the AI off to learn and think as fast as possible, but we also need to catch up as soon as possible to make it all relevant. And the speed we catch up things is critical to whether AI can help us effectively do these tasks. For the case of Daniel, he can spot-check and basically just skim through AI’s work and know for sure it’s doing the right thing with a few simple tests steps to verify, hence his results are more reliable. Whereas for Eric, he needs to basically learn software development from the bottom up to comprehend what the AI has done, and that really doesn’t give him the edge to outpace engineering teams to ship features reliably by himself.</p>

<h3 id="where-ai-exploded-fast-verification-slow-learning-and-creation">Where AI exploded: fast verification, slow learning and creation</h3>

<p>To generalize the problem again, I think for all the tasks we do, we can break them down into two parts: learning/creation and verification. Basically doing the task and checking if the task is done right. Interestingly, this gives us a good perspective to our relationship with AI on performing such tasks.</p>

<p>Effort wise, if <strong>verification «&nbsp;learning/creation</strong>, one can very effectively check AI’s work and be confident about its reliability.</p>

<p>If <strong>verification ~= learning/creation</strong>, one spends equal amount of time checking AI’s work. It’s not a big win, maybe AI becomes a good automation script to cut down some boilerplate.</p>

<p>If <strong>verification&nbsp;» learning/creation</strong>, one cannot be sure about AI’s work that easily, and we are in the vibe-land.</p>

<p>A very good example of the first category is image (and video) generation. Drawing/rendering a realistic looking image is a crazily hard task. Have you tried to make a slide look nicer? It will take me literally hours to center the text boxes to make it look “good”. However, you really just need to take a look at the output of Nano Banana and you can tell if it’s a good render or a bad one based on how you feel. The verification is literally <strong>instantaneous</strong> and <strong>effortless</strong> because it’s all encoded as feeling or vibes in your brain. “Does this look right?” probably can be answered in the span of milliseconds by your vision cortex. There is also no special knowledge required - <strong>human beings have been evaluating visual images since birth</strong>, hardwired into our instincts.</p>

<p>The significant cost asymmetry can greatly explain why AI image generation exploded. If we can look for similar scenarios, we can probably identify other “killer” use cases of AI as well.</p>

<h3 id="verification-debt-scarier-than-tech-debt">Verification debt: scarier than tech debt</h3>

<p>However, if we go down into the bottom of the spectrum where verification becomes more intense - requiring domain knowledge, technical expertise, industry know-hows to tell if the AI is producing slop or not, we will enter this dark age of piling verification debt. More things are being created, but we are lagging behind to check if any of it actually works to our satisfaction.</p>

<p>If an organization keeps vibe-coding without catching up with verification, those tasks can quickly end up as “debts” that needs to be verified. When verification becomes the bottleneck, dangerous things can happen if we still want to move fast - we will risk ourselves running unverified code and having unexpected side effects that are yet to be validated. It can also apply to other fields - imagine asking AI to craft a new vaccine and you don’t want to wait for FDA to use it.</p>

<p>I’ve come across a few blog posts that talks about Verification Debt already. I think it’s genuinely a good problem for technical leaders to have in their mind in this era.</p>

<h3 id="verification-engineering-is-the-next-context-engineering">Verification Engineering is the next Context Engineering</h3>

<p>AI can only reliably run as fast as we check their work. It’s almost like a complexity theory claim. But I believe it needs to be the case to ensure we can harvest the exponential warp speed of AI but also remain robust and competent, as these technologies ultimate serve human beings, and us human beings need technology to be reliable and accountable, as we humans are already flaky enough ;)</p>

<p>This brings out the topic of Verification Engineering. I believe this can be a big thing after Context Engineering (which is the big thing after Prompt Engineering). By cleverly rearranging tasks and using nice abstractions and frameworks, we can make verification of AI performed tasks easier and use AI to ship more solid products the world. No more slop.</p>

<p>I can think of a few ideas to kickoff verification engineering:</p>

<ul>
  <li>How to craft more technicall precise prompts to guide AI to surgically do things, rather than vibing it.</li>
  <li>How to train more capable technical stakeholders who can effectively verify and approve what AI has done.</li>
  <li>How to find more tasks that are relatively easy to verify but rather hard to create.</li>
  <li>How to push our theoretical boundaries of what things we can succinctly verify (complexity theory strikes again).</li>
</ul>

<h3 id="where-next">Where next</h3>

<p>I believe whoever figures out ways to effectively verify more complex tasks using human brains, can gain the most benefit out of the AI boom. Maybe we need to discard traditional programming languages and start programming in abstract graph-like dataflow representations where one can easily tell if a thing is done right or wrong despite its language or implementation details.</p>

<p>Maybe our future is like the one depicted in Severance - we look at computer screens with wiggly numbers and whatever “feels right” is the right thing to do. We can harvest these effortless low latency “feelings” that nature gives us to make AI do more powerful work.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We collected 10k hours of neuro-language data in our basement (104 pts)]]></title>
            <link>https://condu.it/thought/10k-hours</link>
            <guid>46195109</guid>
            <pubDate>Mon, 08 Dec 2025 17:33:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://condu.it/thought/10k-hours">https://condu.it/thought/10k-hours</a>, See on <a href="https://news.ycombinator.com/item?id=46195109">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mobile-blog-content">
                        <p>
                            Over the last 6 months, we collected ~10k hours of data across thousands of unique individuals. As far as we know, this is the largest neuro-language dataset in the world.<sup><a href="#fn1-mobile">[1]</a></sup><span id="fn1-mobile" data-footnote-number="[1]">See <a href="https://openneuro.org/datasets/ds002345/versions/1.1.4">here</a>, <a href="https://openneuro.org/datasets/ds003643/versions/2.0.7">here</a>, <a href="https://arxiv.org/abs/2504.21214">here</a>, <a href="https://www.nature.com/articles/sdata2018291">here</a>, and <a href="https://arxiv.org/abs/2502.17480">here</a> (discussion only, no data available) for some of the larger datasets. See recent papers discussing the problem of small datasets <a href="https://www.sciencedirect.com/science/article/pii/S1878929324001312">here</a>, <a href="https://arxiv.org/abs/2407.07595">here</a>, and <a href="https://thesai.org/Downloads/Volume16No4/Paper_85-Speech_Decoding_from_EEG_Signals.pdf">here</a>.</span> Why did we do this? We train thought-to-text models. That is, we train models to decode semantic content from noninvasive neural data. Here are some entirely zero-shot examples:
                        </p>

                        <p><span id="fn2" data-footnote-number="[2]">The neural data is taken from the seconds leading up to but not including the time when the subject typed or spoke, meaning that the model detects an idea before the subject even compiles that idea down into words.</span>
                        </p>
                        <table>
                            <thead>
                                <tr>
                                    <th>Ground truth</th>
                                    <th>Model prediction (based ONLY on neural data)<sup><a href="#fn2">[2]</a></sup></th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>the room seemed colder</td>
                                    <td>there was a breeze even a gentle gust</td>
                                </tr>
                                <tr>
                                    <td>do you have a favorite app or website</td>
                                    <td>do you have any favorite robot</td>
                                </tr>
                                <tr>
                                    <td>then she smiled faintly and nodded</td>
                                    <td>she shrugged, hoping to look indifferent.</td>
                                </tr>
                                <tr>
                                    <td colspan="2">All examples are zero-shot to new subjects, whom the model has never seen before.</td>
                                </tr>
                            </tbody>
                        </table>

                        <p>
                            We'll write about the model in a future post. But before you can train a model that generalizes to new people, you need to get many thousands of hours of data. When we started, the existing datasets were either inapplicable or tiny. Most were in the low hundreds of hours (if that), and most had tens or, at a stretch, hundreds of subjects.
                        </p>

                        <p><img src="https://condu.it/thought/10k-hours/images/image4.png" alt="Data collection setup"></p><p>
                            So we got thousands of people to come wear headsets in our basement. This post is about how we collected our dataset—what participants do, the hardware and software involved, and what we learned about operations and ML when we scaled it up.
                        </p>
<nav id="toc">
                            <p>Contents</p>
                            <ul>
                                <li><a href="#introduction">Introduction</a></li>
                                <li><a href="#what-participants-do">What participants actually do</a></li>
                                <li><a href="#headsets">Headsets</a></li>
                                <li><a href="#modalities">Modalities</a></li>
                                <li><a href="#training-vs-inference">Training vs. inference</a></li>
                                <li><a href="#noise-reduction">Noise Reduction</a></li>
                                <li><a href="#why-noise-matters-less">Why noise matters much less at scale</a></li>
                                <li><a href="#scaling-the-operation">Scaling the operation</a></li>
                                <li><a href="#people-and-bookings">People and bookings</a></li>
                                <li><a href="#marginal-cost">Marginal cost per usable hour of data</a></li>
                                <li><a href="#now-what">Now What</a></li>
                                <li><a href="#appendix">Appendix: Booths</a></li>
                            </ul>
                        </nav>

                        <h2 id="what-participants-do">What participants actually do</h2>

                        <p>
                            A participant comes in, signs a consent form, and sits down in a booth. A session manager fits a headset onto them and starts the session. Then, the participant has a freeform conversation with an LLM for two hours.
                        </p>

                        <p>
                            Sessions vary. Some are listening and speaking with an LLM, and some are reading and typing.<sup><a href="#fn3">[3]</a></sup><span id="fn3" data-footnote-number="[3]">We use Deepgram for audio transcription, OSS120B on Cerebras for the LLM responses, and ElevenLabs for voicing certain replies. In the past, we used various Gemma and Llama models on Groq.</span> The goal is to maximize the amount that subjects type or say during the two-hour period, without constraining the topics they discuss.<sup><a href="#fn4">[4]</a></sup><span id="fn4" data-footnote-number="[4]">In the beginning, we included tasks like 'retype this sentence', or 'paraphrase this but use this different tone'. Over time, we eliminated these and replaced them with more freeform conversation. We still include a few baseline tasks for calibration and easy model evals.</span> Each session produces multimodal neural data time-aligned with text and audio.
                        </p>

                        <p>
                            Participants have to touch-type without looking at the keyboard. In the beginning, participants would occasionally press a crazy key combination that crashed or closed the software. We could have fixed this in the code, but that would've taken time—so instead we 'simplified' the keyboards.
                        </p>

                        <p><img src="https://condu.it/thought/10k-hours/images/image6.jpg" alt="Simplified keyboard"></p><p>
                            What your participants type—and whether it's remotely coherent—is a more difficult problem. We implemented a token quantity/quality scoring system that determines if we invite a participant back for future sessions, and we make sure participants know about this so they're incentivized to engage.
                        </p>

                        <p>
                            Below are passages typed by participants in May vs. October:
                        </p>

                        <table>
                            <thead>
                                <tr>
                                    <th>May:</th>
                                    <th>October:</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>SO, AI NEEDS THIS CODE: 1, THOSE WHO BELONG TO THE CHURCH CAN NEVER BE FOUND GUILTY WHEN SINNED 2. HIDE THE SINS! CRIMES! WHICH IS A FEDERAL CRIME BUT THOSE ARE THE OLDEST TEACHINGS OR LAWS OF CHRISTIANITY! AND WE ARE ALL LIVING IN THIS HELL IN THE WEST. CHRISTIANS ARE DEEMED CRIMINALLY INSANE, PER A JEWISH THERAPIST, AND THE TEACHINGS ARE SUGGEST VERY GROTESQUE CRIMES AND SHE SHOWED ME THE PASSAGES IN THE FAKE VATICAN BIBLE. NO WONDER IS WAS NOT WRITTEN BY JESUS! DUH!</td>
                                    <td>I guess the way I am thinking about it is that since the amygdala is the irrational fight or flight part of the brain it would activate/be used with a higher frequency when a human being finds themselves under threat. Humans tend not to find themselves under threat when experiencing loving and therefore safe interactions. Therefore,when engaging in positive social interaction, the amygdala is less reactive. I don't know exactly what has sparked this interest other than a curiosity to understant the human brain and how we make decisions and funtion as social beings. I guess it all could stem from my interest in improving well being/ reducing suffering.</td>
                                </tr>
                                <tr>
                                    <td>l''''''''''''''''''''''''''''xcccccccccccccccccccccccccccccccccccccccccccczzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzccccckkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkllllllllllllllllllllllllllllllllllllllll,llll</td>
                                    <td>I would travel to local elementary schools and teach kids how to ride bikes as well as teach them bike safety stuff. That was the most enjoyable part and stuck with me the most. I think it was seeing their excitement when they would get riding on their own. And watching their independence and confidence flourish. It was a super rewarding experience. This is so funny, it feels like a job interview. I think its the beginning of a newfound independence and selfhood for a lot of the kids.They get to move on their own accord and get to experience the world in a new way, its the first taste of freedom.</td>
                                </tr>
                            </tbody>
                        </table>

                        <p>
                            You'll also get much better engagement if the LLM personalizes the sessions. For the first few months of data collection, participants chatted with the LLM about generic, banal topics. Now, participants introduce themselves to the LLM very early in the session, and the LLM uses that context to tailor back-and-forth conversation to the particular person it's talking to. As a result, participants engage more with the LLM—and therefore provide better data.
                        </p>

                        <p><img src="https://condu.it/thought/10k-hours/images/image11.png" alt="Participant session interface">

                        <img src="https://condu.it/thought/10k-hours/images/image5.png" alt="Ventilation setup"></p><p>
                            Participants often raised discomfort as a distraction from the sessions. Ventilation was a common complaint. So, we bought <a href="https://www.amazon.com/dp/B07FPFVZTZ">these fans</a> and <a href="https://www.amazon.com/dp/B0791V19H7">these pipes</a>. These can't be plugged in next to the data collection booths (because of electrical interference), so we snake an ~8m ventilation pipe along the ceiling from a central hub into each booth.
                        </p>

                        <p>
                            Making the headsets comfortable to wear is difficult, since you need to press a 4-pound helmet into participants' scalps. To address this, we cut polygonal sections of padding that compress inwards so as to not cover any sensors.
                        </p>

                        <div>
                            <p><img src="https://condu.it/thought/10k-hours/images/image1.png" alt="% of participants by # of sessions completed"></p><p>% of participants by # of sessions completed</p>
                        </div>

                        <p>
                            At first, &lt;20% of participants even finished their first session. Now, &gt;97% complete their first session, and almost half sign up for more.
                        </p>

                        <h2 id="headsets">Headsets</h2>

                        <p>
                            There were two main things we thought about when we designed the headsets. The first was what modalities the headsets should have, and the second was how training headsets should compare to inference ones.
                        </p>

                        <h4 id="modalities">Modalities</h4>

                        <p>
                            There are many ways of measuring brain data: common modalities include EEG, fMRI, fNIRS, transcranial ultrasound, and MEG. We tried various modalities, but the main takeaway we found is that you need multiple. You can't practically make it work with just one, even if you get the best possible headset of that modality.
                        </p>

                        <p>
                            None of the available multimodal headsets were good enough (far worse than the best single modality versions of each). So we bought some of the best single-modality headsets, took them apart, 3D printed parts to make them fit together, and combined them into our own optimized multimodal headsets.<sup><a href="#fn5">[5]</a></sup><span id="fn5" data-footnote-number="[5]">We have a 3D printer at our office that we use for prototyping and designing pieces. For the ones we put in production in data collection, we send them out to a professional printer and have them printed in bulk. We usually have them printed in Pa-F Nylon, which is stiffer and holds up longer before needing replacement.</span>
                        </p>

                        <p>
                            If you want your model to perform well across various neural modalities and across sensors from different providers, you should design and train on a range of headsets. We buy sensors from several providers, combine them into different multimodal headsets, and then use those headsets essentially interchangeably. We also designed our data format such that data from many kinds of sensors fit nicely into a single, standard framework that our model can parse.
                        </p>

                        <h4 id="training-vs-inference">Training vs. inference</h4>

                        <p>
                            Designing headsets for training is very different from designing headsets for inference—what we'll eventually sell as a product. Training headsets should be maximally sensor-dense, can afford to be expensive, and don't need to be as comfortable. In inference, though, few people are willing to wear a 4-pound helmet as they go about their day—even if it can read their minds. So, we did ablation studies. The take-away here is that you should only think about the inference headset once you've trained a model on your data, because that lets you figure out the exact minimal inference headset.
                        </p>

                        <div>
                            <div>
                                <p><img src="https://condu.it/thought/10k-hours/images/image10.jpg" alt="Inference headset concept"></p><p>(inference headset concept)</p>
                            </div>
                            <div>
                                <p><img src="https://condu.it/thought/10k-hours/images/image3.jpg" alt="Training headset concept"></p><p>(training headset concept)</p>
                            </div>
                        </div>

                        <p>
                            What should be shared across both training and inference is your data format. Initially, we got this wrong: we used HDF5 for data collection and storage and processed it into MDS for model training. Eventually, we switched to using Zarr 3 for everything. Zarr 3 gives us chunked, cloud-native storage with the same format for training and inference.
                        </p>

                        <p>
                            You might think a crucial consideration for training (and for inference) is noise. At first, so did we.
                        </p>

                        <details open="">
                            <summary><h2 id="noise-reduction">Noise Reduction</h2></summary>
                            <div>
                                    <p>
                                        The sources of noise you'll notice are very different depending on which modality you use. That said, all modalities of noninvasive neural data are noisy. We're not disclosing all the modalities or headset configurations we use here, but we'll use EEG as an example. The important lessons, which apply to any modality, are that (1) noise-reduction is only worth it if it doesn't cripple the amount of data you can collect, and (2) you should always keep in mind the logistics of running sessions and recruiting participants.
                                    </p>

                                    <h4 id="gel">Gel</h4>

                                    <p>
                                        The classic wisdom is that gel makes EEG data much better, and without it, your data will be substantially noisier. But if you care about data quantity, you probably shouldn't use gel.
                                    </p>

                                    <p>
                                        It takes up to 30 minutes to apply, and we allocate ~3 minutes for the time between one participant finishing a session and the next one starting.<sup><a href="#fn6">[6]</a></sup><span id="fn6" data-footnote-number="[6]">Most kinds of gel also dry out over time, meaning that we likely would've had to make sessions shorter—and fewer participants would have signed up if they had to let us put gel in their hair.</span> Using gel would've &gt;2xed the marginal cost of an hour of data.
                                    </p>

                                    <p>
                                        Instead, we got the highest quality dry electrodes we could, and we spring-loaded the 3D printed pieces so that a spring presses the electrode against the head. We had to try <a href="https://www.acxesspring.com/english/catalogsearch/advanced/result/?category=cs&amp;unit_measure=en&amp;cs_od%5Bfrom%5D=0.4&amp;cs_od%5Bto%5D=0.6&amp;cs_fl%5Bfrom%5D=0.9&amp;cs_fl%5Bto%5D=1.1&amp;material_type=0&amp;cs_rt%5Bfrom%5D=0.3&amp;cs_rt%5Bto%5D=2&amp;form_key=uJgOhWqe9j9Ipypv">various strengths of spring</a> because we wanted to maximize contact without causing discomfort. Generally, stronger springs work well at the front and back of the head; and weaker ones on the top of the head and above the ears.
                                    </p>

                                    <p>
                                        The essential take-away here is that the fast switching time (2-3 mins) is super important. If you care about data quantity, you should operate with some fixed switching time as a constraint, and limit yourself only to interventions that improve quality without violating that constraint.
                                    </p>

                                    <h4 id="electrical-noise">Electrical noise</h4>

                                    <p>
                                        Most buildings have a lot of background electrical noise, which shows up on any EEG power spectrum—in particular, a spike at 60Hz, the U.S. power line frequency. Here is what that spike looks like with no filtering:
                                    </p>

                                    <p><img src="https://condu.it/thought/10k-hours/images/image2.png" alt="EEG power spectrum showing 60Hz spike"></p><p>(not from our dataset—example from <a href="https://mne.discourse.group/t/psd-power-peaks-at-25-and-0-hz/6148">MNE</a>.<sup><a href="#fn7">[7]</a></sup><span id="fn7" data-footnote-number="[7]">Worth noting that this data is from outside of the United States, where the power line frequency is 50hz rather than 60hz.</span>)</p>

                                    <p>
                                        At first, we tried to get around this by triple-layering <a href="https://www.uline.com/Product/Detail/H-3086/Anti-Fatigue-Mats/Cadillac-Mat-3-8-thick-2-x-2-Black?pricode=WA9154&amp;gadtype=pla&amp;id=H-3086&amp;gad_source=1&amp;gad_campaignid=10688098445&amp;gbraid=0AAAAAD_uetMrP89IMnou9MvnpRfxBK41U&amp;gclid=CjwKCAiAraXJBhBJEiwAjz7MZb_nvjmv-n2pAcf2_9lEM78xU_45GjlWEVIgyg9rE71AHd_dCU8fHBoCGfAQAvD_BwE">rubber mats</a> around the equipment.
                                        But the fundamental issue was that some of the headset components weren't wireless, so we had to plug them into the wall (meaning that the rubber didn't help that much, though it does help a bit and we still use it).
                                    </p>

                                    <p>
                                        We then tried getting <a href="https://www.amazon.com/Furman-Conditioner-Protector-Electrical-Extension/dp/B008A85LL2">adapters that plug into the wall and output clean power</a>. This didn't really help.
                                        Eventually, we used <a href="https://www.ankersolix.com/products/c1000?variant=49702371524938">Anker batteries</a> and only plugged stuff into the DC adapters (we got extra batteries so we could switch them out to charge). This helped a lot, but the thing that really helped was turning off all the power to that side of the building.
                                    </p>

                                    <p>
                                        Turning the power off had a lot of downsides. It meant we had to drag ~30 lb batteries back and forth an average of once an hour to charge, and it was difficult to power some of the headsets with only DC power, which made us drop ~10% of frames.
                                    </p>

                                    <p>
                                        Luckily, after a few thousand hours, noise stopped mattering as much.
                                    </p>

                                    
                                </div>
                        </details>

                        <h2 id="why-noise-matters-less">Why noise matters much less at scale</h2>

                        <p>
                            The key observation: data quantity swamps every noise-reduction technique once you cross ~4k-5k hours.
                        </p>

                        <p>
                            When we only had a few hundred hours, denoising was mandatory. Every extra source of variation—different booths, power setups, posture changes—meant the same neural pattern showed up in fewer comparable examples, so the encoder had less to learn from. Keeping the environment stable and electrically boring was the easiest way to keep the problem manageable.
                        </p>

                        <p>
                            At ~4-5 thousand hours, that constraint changes. The model now sees the same patterns across many people and setups, and has enough capacity to represent both the mess and the neural signal.<sup><a href="#fn8">[8]</a></sup><span id="fn8" data-footnote-number="[8]">Similar effects appear in other modalities. Speech models like Whisper, trained on hundreds of thousands of hours of diverse, weakly supervised web audio, show that trading label quality for sheer quantity improves robustness and generalization (see <a href="https://arxiv.org/abs/2212.04356">here</a>). Video-language models trained on uncurated instructional videos learn strong representations even though a large fraction of clip-caption pairs are misaligned or noisy (see <a href="https://arxiv.org/pdf/1912.06430">here</a>). In each of these cases, once the dataset is sufficiently large and diverse, total volume of data outweighs strict curation and noiselessness for downstream robustness.</span> The decoder gets enough examples to tell apart "this changes with the text" from "this is just the room". At that point, data quantity overwhelms noise, and most of the extreme noise-reduction work stops buying much—so we turned the power back on.
                        </p>

                        <h2 id="scaling-the-operation">Scaling the operation</h2>

                        <p>
                            After a few thousand hours, noise stops being the thing to worry about in data collection. The things that matter most are
                        </p>

                        <ol>
                            <li>The raw number of people you can put in headsets; and</li>
                            <li>The marginal cost per usable hour of data.</li>
                        </ol>

                        <h4 id="people-and-bookings">People and bookings</h4>

                        <p><img src="https://condu.it/thought/10k-hours/images/image12.png" alt="Participant recruitment poster"></p><p>
                            Since we run sessions 20 hours/day, 7 days/week, we get a lot of bookings and see a lot of people. An Uber driver once started telling us about 'this great new way to earn money in SF'—and it turned out to be our data collection.
                        </p>

                        <p>
                            Surprisingly central to getting headset occupancy high enough was building a custom booking suite.<sup><a href="#fn9">[9]</a></sup><span id="fn9" data-footnote-number="[9]">We tried Calendly, You Can Book Me, and various other things before making our own. In the end, all the available booking systems had different issues, e.g. not allowing us to blacklist certain people, not allowing dynamic pricing or overbooking, and limited visibility for participants and bookings.</span> There are two main tenets: dynamic pricing and dynamic overbooking. Because few people book at 7am on a Sunday, dynamic pricing means participants are paid more for that slot. Because many people book at 7pm on a Friday, but few of them actually show up, dynamic overbooking allows more people to sign up. The overbooking algorithm can also access information about particular participants.<sup><a href="#fn10">[10]</a></sup><span id="fn10" data-footnote-number="[10]">E.g. if Alice has reliably shown up for sessions before, the algorithm lowers the expected total no-show rate during future times when Alice has booked.</span>
                        </p>

                        <p><img src="https://condu.it/thought/10k-hours/images/image7.png" alt="Booking system dashboard"></p><p>
                            In order to get your model to generalize, it's important to get a dataset of thousands of unique individuals. That is *not* just thousands of hours from dozens or hundreds of individuals. In an ideal world, most participants would only come in for one or two sessions, but that trades off hard against total hours. We cap the number of sessions that any one participant is allowed to do at 10 sessions. Before we introduced the cap, our schedule was fantastically full, but we weren't getting enough unique participants because long-term returners were filling all the slots.
                        </p>

                        <p>
                            Even so, participant recruitment gets easier with scale. We now have participant-ambassadors, whom we pay to recruit more participants for us even after they've completed their 10 sessions.<sup><a href="#fn11">[11]</a></sup><span id="fn11" data-footnote-number="[11]">Since the start, we've tried dozens of ways to directly recruit first-time participants. By far the most effective has been Craigslist. Almost every day since April, we've posted a listing—<a href="https://sfbay.craigslist.org/sfc/lbg/d/san-francisco-chat-with-ai-get-paid-up/7896688506.html">in</a> <a href="https://sfbay.craigslist.org/sfc/crg/d/san-francisco-think-fast-type-fast-earn/7897143450.html">sections</a> <a href="https://sfbay.craigslist.org/sfc/wrg/d/san-francisco-two-hours-one-headset/7893366908.html">from</a> '<a href="https://sfbay.craigslist.org/sfc/cpg/d/san-francisco-earn-550-testing-new/7896689107.html">computer</a>' <a href="https://sfbay.craigslist.org/sfc/wrg/d/san-francisco-up-to-550-to-chat-with-ai/7894838033.html">to</a> '<a href="https://sfbay.craigslist.org/sfc/crg/d/san-francisco-up-to-550-to-chat-with-ai/7895293226.html">creative</a>' <a href="https://sfbay.craigslist.org/sfc/lbg/d/san-francisco-join-the-research-thats/7893367163.html">to</a> '<a href="https://sfbay.craigslist.org/sfc/lbg/d/san-francisco-we-strap-device-to-your/7897143946.html">labor gigs</a>'—that advertises a $50 payout for wearing a helmet and typing for two hours.</span>
                        </p>

                        <h4 id="marginal-cost">Marginal cost per usable hour of data</h4>

                        <p>
                            Between May and October, we cut the marginal cost per usable hour of data by ~40%. Here are the highest-impact things we did.
                        </p>

                        <p>
                            In August, we entirely rewrote the data format and data collection backend to catch issues in the data live, before participants complete two potentially useless hours of data collection. The sessions stream to the cloud, and we automatically sanity-check each session in real time for modality dropout, token quality, timestamp drift, and alignment jitter. Any session that falls outside the tolerance bands gets flagged for session managers to restart or debug.<sup><a href="#fn12">[12]</a></sup><span id="fn12" data-footnote-number="[12]">This is only possible because we changed our data format to use Zarr 3 and optimized it for fast quality checks.</span>
                        </p>

                        <p>
                            This change alone cut the marginal cost of data by ~30% and ~1.5xed the amount of usable data we collect.
                        </p>

                        <p>
                            Second, we enable session managers to run more sessions in parallel without sacrificing supervision. We put <a href="https://www.amazon.com/dp/B0F2HCHS52">EVERSECU cameras</a> in the booths, so session managers can monitor and speak directly to participants without leaving the main supervision station. We also made a unified booking -&gt; intake -&gt; data collection backend, which massively simplifies the participant intake process and improves security.<sup><a href="#fn13">[13]</a></sup><span id="fn13" data-footnote-number="[13]">As one example of how the unified system helps, it detects how much support a given participant is likely to need (based on, e.g., whether they've attended sessions before, their answers to questions on the booking form, etc.) and how many concurrent bookings are already scheduled for that participant's sign-up time. If needed, it can also stagger booking start-times by 5-10 minutes so session managers don't struggle with an onslaught of arrivals all at once.</span>
                        </p>

                        <h2 id="now-what">Now What</h2>

                        <p>
                            The steps to building thought-to-text have always been clear: (1) collect a dataset; (2) train a model; (3) close the loop. We're now well into step two—we spend &gt;95% of our time training models and very little time actively thinking about data collection.
                        </p>

                        <p>
                            But you can't have a model without a dataset, so you do need to get this part right.
                        </p>

                        <p>
                            If you're collecting a similar kind of data, training multi-modal models, or want to give us cheap GPUs, we'd love to hear from you. Please reach out to us at <a href="mailto:contact@condu.it">contact@condu.it</a>.
                        </p>

                        <p>
                            And if this dataset sounds cool to you and you want to train models with it, we're hiring engineers and researchers. Reach out to us at <a href="mailto:jobs@condu.it">jobs@condu.it</a>.
                        </p>

                        <hr>

                        <details>
                            <summary><h2 id="appendix">Appendix: Booths</h2></summary>
                            <div>
                                    <p>
                                        We started out putting each participant in a separate room at a normal work station. We saw huge noise spikes in the data from participants moving their heads, and sometimes they'd get up and walk around with the headset on or take the headset off without telling us.
                                    </p>

                                    <p>
                                        The solution to this was putting multiple booths in one shared room for easier supervision. We also installed chinrests that hold participants' heads still, which help reduce motion artifacts in the data.<sup><a href="#fn14">[14]</a></sup><span id="fn14" data-footnote-number="[14]">We initially wanted to get something like an optician's chinrest, but the bar across the forehead got in the way of the headset. We ended up buying <a href="https://www.amazon.com/Vondynote-Desktop-Speaker-Monitor-Adjustable/dp/B0C1NHTNN6/ref=sr_1_5?crid=1EMUTNK27HD3R&amp;dib=eyJ2IjoiMSJ9.oKWo-hoGxOY41YRBZ1z6jBnI3UaB-8lUM7TE5fQyokQL8pxhLH2H0eBShU8HGHo_hzWWmfClRVX3PUbFWmaAEeC6ECWAUmlxEogINUhyreaSAV2TmKUtbGVKEuexz4CFNNIOe1E50kSNIr6HneQca7p5eK0AQ3w3_8dV9--G0RmcXOFZed1t6tXNSNxZ8mBjo3BhejOb1wQJ1X2UiaTnZ4KWuPZTgIKyO7rR_CpAV9DywLeCwPJbXKXqNXfR_N5M43K_3mCoM2uEtLK68-cjEtT7nPo6e7VjvuC_-kib3i4.HhLWkX9pJEmlPk5gG6qVEWdN8dywr0f-9_JSJVH0PsA&amp;dib_tag=se&amp;keywords=speaker%2Bstand&amp;qid=1764383726&amp;s=musical-instruments&amp;sprefix=speaker%2Bstan,mi,170&amp;sr=1-5">speaker stands</a> and sawing pieces of wood to screw onto them. This works pretty well, although participants don't always use them. You should ensure that any desks, chairs, and chinrests that you buy are height-adjustable.</span>
                                    </p>

                                    <p>
                                        Now, we use <a href="https://zenbooth.net/products/zenbooth-duo?srsltid=AfmBOopJCM2lQH9yLpdLhC7OVeurtDVG1i-mPuu-NiCQ_Hh39xfmQS3g&amp;variant=32767488819271">these nice phone booths</a> (~$10k each, though you can sometimes get them used). We initially picked them because they were the best option for turning into safe Faraday Cages.
                                    </p>

                                    <p>
                                        We've stopped worrying so much about electrical noise, so we only ever bothered turning one booth into a Faraday Cage. But professional phone booths save a lot of hassle and set participants at ease, so you should use them if you can.
                                    </p>

                                    <p>
                                        If you don't have two weeks to wait for booths to arrive or if you want a cheaper option, we also used <a href="https://www.amazon.com/dp/B0DJ3CNQFP?ref=cm_sw_r_cso_cp_apin_dp_1KJDHPCHRDQ55WCFM532&amp;ref_=cm_sw_r_cso_cp_apin_dp_1KJDHPCHRDQ55WCFM532&amp;social_share=cm_sw_r_cso_cp_apin_dp_1KJDHPCHRDQ55WCFM532&amp;previewDoh=1&amp;th=1">these vocal recording booths</a>. The downside of using these is that they aren't remotely soundproof, so the participants could hear each other talking—which interfered with speaking and listening tasks.
                                    </p>

                                    <p>
                                        We added three layers of <a href="https://www.amazon.com/dp/B0DM5V3ZMJ?ref=ppx_pop_mob_ap_share">soundproof curtains</a>.<sup><a href="#fn15">[15]</a></sup><span id="fn15" data-footnote-number="[15]">This still wasn't enough, so we got dozens of <a href="https://www.amazon.com/dp/B0CR15SX4S?ref=ppx_pop_mob_ap_share&amp;th=1">sound panels</a> and used rope to hang them wall to wall in the booths.</span> Unfortunately, the weight of the curtains caused the booths to collapse. The solution to this is a lot of rope, which we used to tie the poles of the booth together and then nailed into a hook in the wall.
                                    </p>

                                    <p><img src="https://condu.it/thought/10k-hours/images/image8.png" alt="DIY booths soundproofed">
                                        <img src="https://condu.it/thought/10k-hours/images/image13.jpg" alt="Stock vocal booths">
                                        <img src="https://condu.it/thought/10k-hours/images/image9.png" alt="Zenbooth booth">
                                    </p>
                                    <p>(our first DIY booths, soundproofed)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(stock vocal booths)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(our Zenbooth booth)</p>

                                    <p>
                                        It costs ~$2,000 to set up these booths: $600 for the booth itself, $1,300 for soundproofing, and $100 for miscellaneous construction (rope, screws, etc). They look less professional, and you can't make them into a safe Faraday Cage, but otherwise this setup actually does work pretty well. We have a couple that we still use in our current data collection center, and they've been running flawlessly 20 hours/day for months.
                                    </p>

                                    
                                </div>
                        </details>
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft has a problem: nobody wants to buy or use its shoddy AI products (393 pts)]]></title>
            <link>https://www.windowscentral.com/artificial-intelligence/microsoft-has-a-problem-nobody-wants-to-buy-or-use-its-shoddy-ai</link>
            <guid>46194615</guid>
            <pubDate>Mon, 08 Dec 2025 16:54:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.windowscentral.com/artificial-intelligence/microsoft-has-a-problem-nobody-wants-to-buy-or-use-its-shoddy-ai">https://www.windowscentral.com/artificial-intelligence/microsoft-has-a-problem-nobody-wants-to-buy-or-use-its-shoddy-ai</a>, See on <a href="https://news.ycombinator.com/item?id=46194615">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8.jpg" alt="Microsoft Chief Executicve (CEO) Satya Nadella takes part in the Partnership for Global Infrastructure and Investment Event during the G7 Summit at the Borgo Egnazia resort in Savelletri, Italy, on June 13, 2024." srcset="https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>Satya Nadella is burning decades of customer good will chasing the latest tech fad.</span>
<span>(Image credit: Getty Images | MANDEL NGAN)</span>
</figcaption>
</div>
<div id="article-body">
<p id="1e8f441c-4bde-4884-b8c2-b13351aaf279">If there's one thing that typifies Microsoft under CEO <a data-analytics-id="inline-link" href="https://www.windowscentral.com/tag/satya-nadella" data-auto-tag-linker="true" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/tag/satya-nadella">Satya Nadella</a>'s tenure: it's a general inability to connect with customers.</p><p id="1e8f441c-4bde-4884-b8c2-b13351aaf279-2">A recent report from The Information <a data-analytics-id="inline-link" href="https://futurism.com/artificial-intelligence/microsoft-sell-ai-agents-disaster" data-url="https://futurism.com/artificial-intelligence/microsoft-sell-ai-agents-disaster" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link">detailed</a> how Microsoft's internal AI efforts are going awry, with cut forecasts and sales goals for its Azure AI products across the board. The Information said that Microsoft's sales people are "struggling" to meet goals, owing to a complete lack of demand. Microsoft denied the reports, but it can't deny market share growth trends — all of which point to Google Gemini surging ahead.</p><p>Last week we wrote about how <a data-analytics-id="inline-link" href="https://www.windowscentral.com/artificial-intelligence/microsofts-advantages-in-artificial-intelligence-evaporate-google-gemini-surges-ahead-and-openai-declares-code-red-situation" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/artificial-intelligence/microsofts-advantages-in-artificial-intelligence-evaporate-google-gemini-surges-ahead-and-openai-declares-code-red-situation">Microsoft Copilot's backend partner OpenAI issued a "code red" situation</a>. <a data-analytics-id="inline-link" href="https://www.windowscentral.com/artificial-intelligence/openai-chatgpt" data-auto-tag-linker="true" data-mrf-recirculation="inline-link" data-before-rewrite-redirect="https://www.windowscentral.com/tag/chatgpt" data-before-rewrite-localise="https://www.windowscentral.com/artificial-intelligence/openai-chatgpt">ChatGPT</a> has fallen behind Google Gemini in problem solving, and Nano Banana image generation has outpaced <a data-analytics-id="inline-link" href="https://www.windowscentral.com/artificial-intelligence/openai-chatgpt" data-auto-tag-linker="true" data-mrf-recirculation="inline-link" data-before-rewrite-redirect="https://www.windowscentral.com/tag/openai" data-before-rewrite-localise="https://www.windowscentral.com/artificial-intelligence/openai-chatgpt">OpenAI</a>'s own DALLE by leaps and bounds.</p><p>With OpenAI's business model under constant scrutiny and racking up genuinely dangerous levels of debt, it's become a cascading problem for Microsoft to have tied up layer upon layer of its business in what might end up being something of a lame duck.</p><div id="slice-container-table-tyDfwpUzK9bsP5cDNpttgK-JCD6FwCDIVvwgcT8AXZb69Aits1NWxnM"><div><p>Swipe to scroll horizontally</p><svg viewBox="0 0 23 30" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M21.554 15.726a2.878 2.878 0 0 0-1.705-.374 2.881 2.881 0 0 0-1.388-3.068 2.877 2.877 0 0 0-1.992-.333 2.884 2.884 0 0 0-.1-.766 2.865 2.865 0 0 0-1.346-1.75c-.47-.27-.996-.4-1.527-.385l2.742-4.73a2.87 2.87 0 0 0 .323-.83h2.612V2.084h-2.661A2.861 2.861 0 0 0 15.18.385a2.903 2.903 0 0 0-3.952 1.055l-.373.644H2.983l1.003-1L2.99.09 1.28 1.793l-.999.995L2.99 5.484l.998-.994-1.003-.999h7.054L6.505 9.586c-.34.066-.905.186-1.523.366-1.405.41-2.321.895-2.8 1.483-.742.911-1.159 2.513-1.277 4.898l-.001.01c-.067 1.816.946 6.943.99 7.16a.688.688 0 0 0 1.35-.266c-.01-.051-1.023-5.177-.963-6.84.127-2.556.598-3.64.97-4.098.133-.163.602-.587 2.104-1.027l.206-.058-1.425 2.458a.685.685 0 0 0 .252.937c.33.19.75.077.94-.251L12.42 2.126a1.52 1.52 0 0 1 2.07-.552c.35.2.6.527.705.916.105.39.051.797-.15 1.145l-4.767 8.222a.685.685 0 0 0 .252.937c.33.19.75.077.94-.25l.794-1.368c.201-.348.529-.597.92-.702a1.508 1.508 0 0 1 1.854 1.066c.105.39.052.796-.15 1.144l-.377.652-.002.002-.898 1.55a.685.685 0 0 0 .252.938c.329.189.75.077.94-.251l.9-1.551c.201-.348.528-.597.92-.702a1.512 1.512 0 0 1 1.703 2.21l-1.223 2.11a.685.685 0 0 0 .252.938c.33.189.75.076.941-.252l.5-.862c.202-.348.529-.597.92-.702.392-.104.8-.051 1.15.15.723.416.972 1.34.554 2.06l-3.525 6.08c-.517.892-1.57 1.795-3.044 2.611-1.156.64-2.163.998-2.173 1.002a.685.685 0 0 0 .23 1.333.688.688 0 0 0 .229-.04c.18-.062 4.419-1.575 5.952-4.22l3.524-6.08a2.878 2.878 0 0 0-1.059-3.934Z" fill="#333"></path></svg></div><div><table tabindex="0"><caption>FirstPageSage AI Chatbot Usage Chart (December 3, 2025)</caption><thead><tr><th colspan="1"><p>#</p></th><th colspan="1"><p>Generative AI Chatbot</p></th><th colspan="1"><p>AI Search Market Share</p></th><th colspan="1"><p>Estimated Quarterly User Growth</p></th></tr></thead><tbody><tr><td colspan="1"><p>1</p></td><td colspan="1"><p>ChatGPT (excluding Copilot)</p></td><td colspan="1"><p>61.30%</p></td><td colspan="1"><p>7% ▲</p></td></tr><tr><td colspan="1"><p>2</p></td><td colspan="1"><p>Microsoft Copilot</p></td><td colspan="1"><p>14.10%</p></td><td colspan="1"><p>2% ▲</p></td></tr><tr><td colspan="1"><p>3</p></td><td colspan="1"><p>Google Gemini</p></td><td colspan="1"><p>13.40%</p></td><td colspan="1"><p>12% ▲</p></td></tr><tr><td colspan="1"><p>4</p></td><td colspan="1"><p>Perplexity</p></td><td colspan="1"><p>6.40%</p></td><td colspan="1"><p>4% ▲</p></td></tr><tr><td colspan="1"><p>5</p></td><td colspan="1"><p>Claude AI</p></td><td colspan="1"><p>3.80%</p></td><td colspan="1"><p>14% ▲</p></td></tr><tr><td colspan="1"><p>6</p></td><td colspan="1"><p>Grok</p></td><td colspan="1"><p>0.60%</p></td><td colspan="1"><p>6% ▲</p></td></tr><tr><td colspan="1"><p>7</p></td><td colspan="1"><p>Deepseek</p></td><td colspan="1"><p>0.20%</p></td><td colspan="1"><p>10% ▲</p></td></tr></tbody></table></div></div><p id="56a99c4f-3541-4d6e-9abd-84496f630185">There are reams of <a data-analytics-id="inline-link" href="https://futurism.com/professors-company-ai-agents" data-url="https://futurism.com/professors-company-ai-agents" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link">research</a> that suggest agentic AI tools require human intervention at a frequency ratio that makes them cost ineffective, but Microsoft seems unbothered that its tools are poorly conceived.</p><p>In any case, <a data-analytics-id="inline-link" href="https://www.windowscentral.com/artificial-intelligence/openai-chatgpt/openai-is-racing-to-give-chatgpt-a-flashy-upgrade" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/artificial-intelligence/openai-chatgpt/openai-is-racing-to-give-chatgpt-a-flashy-upgrade">OpenAI is supposedly going to launch future models of ChatGPT early</a> in attempts to combat the rise of Google Gemini. I suspect the issues are deeper for Microsoft, who have worked tirelessly under Satya Nadella to create doubt around its products.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-tyDfwpUzK9bsP5cDNpttgK"><section><p>All the latest news, reviews, and guides for Windows and Xbox diehards.</p></section></div><p>SEO and analytics firm FirstPageSage has <a data-analytics-id="inline-link" href="https://firstpagesage.com/reports/top-generative-ai-chatbots/" data-url="https://firstpagesage.com/reports/top-generative-ai-chatbots/" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link">released</a> its AI market share report for the start of December, and it shows Google Gemini actively poised to supplant <a data-analytics-id="inline-link" href="https://www.windowscentral.com/artificial-intelligence/microsoft-copilot" data-auto-tag-linker="true" data-mrf-recirculation="inline-link" data-before-rewrite-redirect="https://www.windowscentral.com/tag/microsoft-copilot" data-before-rewrite-localise="https://www.windowscentral.com/artificial-intelligence/microsoft-copilot">Microsoft Copilot</a>. Based on reports that Google Gemini is now actively beating ChatGPT's best models, FirstPageSage has Google Gemini sprinting past Microsoft Copilot quarter over quarter, although ChatGPT itself will remain the front runner.</p><h2 id="google-s-ai-advantages-are-accumulating-as-microsoft-s-disadvantages-snowball-3">Google's AI advantages are accumulating, as Microsoft's disadvantages snowball</h2><figure data-bordeaux-image-check="" id="0b22eb90-2613-4868-bb5c-37f46ab7d8ed"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD.jpg" alt="Cloud servers" srcset="https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-320-80.jpg 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD.jpg">
</picture></p></div><figcaption itemprop="caption description"><span>Microsoft's destiny under Satya Nadella seems to increasingly point towards being a server broker for NVIDIA, rather than tech leader and innovator. </span><span itemprop="copyrightHolder">(Image credit: Microsoft)</span></figcaption></figure><p id="1a5d5863-697f-4059-b8e8-07196f40b762">Whether it's Google's Tensor server tech or dominating position with Google Play-bound Android, Microsoft's lack of forethought and attention paid to their actual customers is starting to catch up with the firm. <a data-analytics-id="inline-link" href="https://www.windowscentral.com/microsoft/satya-nadella-calls-microsofts-size-a-massive-disadvantage-in-ai" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/microsoft/satya-nadella-calls-microsofts-size-a-massive-disadvantage-in-ai">Nadella has sought to blame the company's unwieldy size</a> for the lack of innovation, but it reads like an excuse to me. It's all about priorities — and Nadella has chased shareholder sentiment over delivering for its customers or employees, and that short-termism is going to put Microsoft on the backfoot if AI actually does deliver another computing paradigm shift.</p><p>Microsoft depends almost entirely on pricy NVIDIA technology for its data centers, whereas Google is actively investing to own the entire stack. Microsoft has also worked incredibly hard to cram half-baked AI features into its products, whereas Google has arguably been a lot more thoughtful in its approach. Microsoft sprinted out of the gate like a bull in a China shop, and investors rewarded them for it — but fast forward to 2025, and Google's AI products simply work better, and are more in-tune with how people might actually use them.</p><p>I am someone who is actively using the AI features across Google Android and Microsoft Windows on a day to day basis, and the delta between the two companies is growing ever wider. Basic stuff like the photo editing features on Google Pixel phones are <em>lightyears </em>beyond the abysmal tools found in the Microsoft Photos app on Windows. Google Gemini in Google Apps is also far smarter and far more intuitive than Copilot on <a data-analytics-id="inline-link" href="https://www.windowscentral.com/tag/microsoft-365" data-auto-tag-linker="true" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/tag/microsoft-365">Microsoft 365</a>, as someone actively using both across the two businesses I work in.</p><figure id="4c687d83-84a4-4a8c-8b93-53ce94f3717c"><blockquote><p>Microsoft's "ship it now fix it later" attitude risks giving its AI products an Internet Explorer-like reputation for poor quality.</p></blockquote></figure><p id="48567dfa-f0e2-4a60-8b61-235ee6f45a7d">Dare I say it, Gemini is actually helpful, and can usually execute tasks you might actually need in a day to day job. "Find me a meeting slot on this date to accommodate these timezones" — Gemini will actually do it. Copilot 365 doesn't even have the capability to schedule a calendar event with natural language in the Outlook mobile app, or even provide something as basic as clickable links in some cases. At least Xbox's Gaming Copilot has a beta tag to explain why it fails half of the time. It's truly absurd how half-baked a lot of these features are, and it's odd that Microsoft sought to ship them in this state. And <a data-analytics-id="inline-link" href="https://www.windowscentral.com/microsoft/windows-11/microsoft-ai-ceo-pushes-back-against-critics-after-recent-windows-ai-backlash-the-fact-that-people-are-unimpressed-is-mindblowing-to-me" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/microsoft/windows-11/microsoft-ai-ceo-pushes-back-against-critics-after-recent-windows-ai-backlash-the-fact-that-people-are-unimpressed-is-mindblowing-to-me">Microsoft wants to make Windows 12 AI first</a>? <em>Please</em>.</p><p>Microsoft's "ship it now fix it later" attitude risks giving its AI products an Internet Explorer-like reputation for poor quality, sacrificing the future to more patient, thoughtful companies who spend a little more time polishing first. Microsoft's strategy for AI seems to revolve around offering cheaper, lower quality products at lower costs (<em><a data-analytics-id="inline-link" href="https://www.windowscentral.com/microsoft/microsoft-teams" data-auto-tag-linker="true" data-mrf-recirculation="inline-link" data-before-rewrite-redirect="https://www.windowscentral.com/tag/microsoft-teams" data-before-rewrite-localise="https://www.windowscentral.com/microsoft/microsoft-teams">Microsoft Teams</a>, hi</em>), over more expensive higher-quality options its competitors are offering. Whether or not that strategy will work for artificial intelligence, which is exorbitantly expensive to run, remains to be seen.</p><p>Microsoft's savvy early investment in OpenAI gave it an incredibly strong position early on, but as we get deeper into the cycle, some cracks are starting to show. Many of Microsoft's AI products to date simply scream of a total lack of direction and utter chaos, but it's not all hopeless. Some of Microsoft's enterprise solutions for AI are seeing strong growth. <a data-analytics-id="inline-link" href="https://www.windowscentral.com/software-apps/over-15-million-developers-now-use-this-ai-coding-tool-from-microsoft" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/software-apps/over-15-million-developers-now-use-this-ai-coding-tool-from-microsoft">Github Copilot</a> has been something of a success story for Redmond, and Microsoft is exploring its own <a data-analytics-id="inline-link" href="https://www.windowscentral.com/microsoft/microsoft-enters-the-chip-game-with-its-own-arm-processors-for-ai-and-computing-workloads" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/microsoft/microsoft-enters-the-chip-game-with-its-own-arm-processors-for-ai-and-computing-workloads">Maia and Cobalt chips</a> and even language models, in attempts to decouple itself from NVIDIA and OpenAI respectively. But Satya Nadella's Microsoft has an uncanny knack for failing to deliver on promising initiatives like those.</p><p>Without a stronger emphasis on quality, Microsoft's future in AI could simply end up revolving around re-selling NVIDIA server tech and jacking up local electricity prices, rather than providing any real home-grown innovation in the space. Shareholders will be more than happy for Microsoft to simply be a server reseller, but it would be a ignoble legacy for what was previously one of tech's most innovative companies.</p><hr id="5a919395-5c1e-4929-941e-3a2f3f0b04b3"><a href="https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE" id="f5f9035e-e569-452e-9804-21314e4a6271" data-url="https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><figure data-bordeaux-image-check=""><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png.webp 1200w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-320-80.png.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX.png" alt="Click to follow Windows Central on Google News" srcset="https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png 1200w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png 1024w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png 970w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-320-80.png 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX.png">
</picture></p></div></figure></a><p id="c04c3c18-271c-4279-8d9e-972f8d84ee71"><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE" target="_blank" data-url="https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link"><em>Windows Central on Google News</em></a><em> to keep our latest news, insights, and features at the top of your feeds!</em></p><hr id="876d2032-2e29-42b2-b13e-5aa4a2af9e89">
</div>



<div id="slice-container-authorBio-tyDfwpUzK9bsP5cDNpttgK"><p>Jez Corden is the Executive Editor at Windows Central, focusing primarily on all things Xbox and gaming. Jez is known for breaking exclusive news and analysis as relates to the Microsoft ecosystem while being powered by tea. Follow on <a href="http://www.twitter.com/jezcorden">Twitter (X)</a> and tune in to the <a href="https://anchor.fm/thexboxtwo">XB2 Podcast</a>, all about, you guessed it, Xbox!</p></div>
</section>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A series of tricks and techniques I learned doing tiny GLSL demos (162 pts)]]></title>
            <link>https://blog.pkh.me/p/48-a-series-of-tricks-and-techniques-i-learned-doing-tiny-glsl-demos.html</link>
            <guid>46194477</guid>
            <pubDate>Mon, 08 Dec 2025 16:44:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.pkh.me/p/48-a-series-of-tricks-and-techniques-i-learned-doing-tiny-glsl-demos.html">https://blog.pkh.me/p/48-a-series-of-tricks-and-techniques-i-learned-doing-tiny-glsl-demos.html</a>, See on <a href="https://news.ycombinator.com/item?id=46194477">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>In the past two months or so, I spent some time making tiny GLSL demos. I wrote
an article about the first one, <a href="https://blog.pkh.me/p/45-code-golfing-a-tiny-demo-using-maths-and-a-pinch-of-insanity.html">Red Alp</a>. There, I went into details about the
whole process, so I recommend to check it out first if you're not familiar with
the field.</p>
<p><img src="https://blog.pkh.me/img/demo-tricks/thumb.jpg" alt="preview of the 4 demos"></p>
<p>We will look at 4 demos: <a href="#Moonlight">Moonlight</a>, <a href="#Entrance3">Entrance 3</a>,
<a href="#Archipelago">Archipelago</a>, and <a href="#Cutie">Cutie</a>. But this time, for each
demo, we're going to cover one or two things I learned from it. It won't be a
deep dive into every aspect because it would be extremely redundant. Instead,
I'll take you along a journey of learning experiences.</p>

<h2>Moonlight</h2>
<figure>
  <canvas width="480" height="340" data-fragment="/frag/demo-tricks/moonlight.frag"></canvas>
  <figcaption>Moonlight demo in 460 characters</figcaption>
</figure>
<pre><code>// Moonlight [460] by bµg
// License: CC BY-NC-SA 4.0
void main(){vec3 o,p,u=vec3((P+P-R)/R.y,1),Q;Q++;for(float d,a,m,i,t;i++&lt;1e2;p=t&lt;7.2?Q:vec3(2,1,0),d=abs(d)*.15+.1,o+=p/m+(t&gt;9.?d=9.,Q:p/d),t+=min(m,d))for(p=normalize(u)*t,p.z-=5e1,m=max(length(p)-1e1,.01),p.z+=T,d=5.-length(p.xy*=mat2(cos(t*.2+vec4(0,33,11,0)))),a=.01;a&lt;1.;a+=a)p.xz*=mat2(8,6,-6,8)*.1,d-=abs(dot(sin(p/a*.6-T*.3),p-p+a)),m+=abs(dot(sin(p/a/5.),p-p+a/5.));o/=4e2;O=vec4(tanh(mix(vec3(-35,-15,8),vec3(118,95,60),o-o*length(u.xy*.5))*.01),1);}
</code></pre>

<p>In Red Alp, I used volumetric raymarching to go through the clouds and fog, and
it took quite a significant part of the code to make the absorption and emission
convincing. But there is an alternative technique that is surprisingly simpler.</p>
<p>In the raymarching loop, the color contribution at each iteration becomes <span>1/d</span>
or <span>c/d</span> where <span>d</span> is the density of the material at the current ray position,
and <span>c</span> an optional color tint if you don't want to work in grayscale level.
Some variants exist, for example <span>1/d^2</span>, but we'll focus on <span>1/d</span>.</p>
<h3>1/d explanation</h3>
<p>Let's see how it looks in practice with a simple cube raymarch where we use this
peculiar contribution:</p>
<figure>
  <canvas width="480" height="340" data-fragment="/frag/demo-tricks/onecube.frag"></canvas>
  <figcaption>One glowing and rotating cube</figcaption>
</figure>
<pre><code>void main() {
    float d, t;
    vec3 o, p,
         u = normalize(vec3(P+P-R,R.y)); // screen to world coordinate

    for (int i = 0; i &lt; 30; i++) {
        p = u * t; // ray position

        p.z -= 3.; // take a step back

        // Rodriguez rotation with an arbitrary angle of π/2
        // and unaligned axis
        vec3 a = normalize(cos(T+vec3(0,2,4)));
        p = a*dot(a,p)-cross(a,p);

        // Signed distance function of a cube of size 1
        p = abs(p)-1.;
        d = length(max(p,0.)) + min(max(p.x,max(p.y,p.z)),0.);

        // Maxed out to not enter the solid
        d = max(d,.001);

        t += d; // stepping forward by that distance

        // Our mysterious contribution to the output
        o += 1./d;
    }

    // Arbitrary scale within visible range
    O = vec4(o/200., 1);
}
</code></pre>
<div>
<p>Note</p>
<p>The signed function of the cube is from the <a href="https://iquilezles.org/articles/distfunctions/">classic Inigo Quilez
page</a>. For the rotation you can refer to <a href="https://mini.gmshaders.com/p/3d-rotation">Xor</a> or
<a href="https://suricrasia.online/blog/shader-functions/">Blackle</a> article. For the general understanding of
the code, see my previous article on <a href="https://blog.pkh.me/p/45-code-golfing-a-tiny-demo-using-maths-and-a-pinch-of-insanity.html">Red Alp</a>.</p>
</div>
<p>The first time I saw it, I wondered whether it was a creative take, or if it was
backed by physical properties.</p>
<p>Let's simplify the problem with the following figure:</p>
<figure>
  <img src="https://blog.pkh.me/img/demo-tricks/ray.png" alt="">
  <figcaption>A ray passing by a radiating object</figcaption>
</figure>
<p>The glowing object sends photons that spread all around it. The further we go
from the object, the more spread these photons are, basically following the
<a href="https://en.wikipedia.org/wiki/Inverse-square_law">inverse square law</a> <span>1/r^2</span>, which gives the photons density,
where <span>r</span> is the distance to the target object.</p>
<p>Let's say we send a ray and want to know how many photons are present along the
whole path. We have to "sum", or rather integrate, all these photons density
measures along the ray. Since we are doing a discrete sampling (the dots on the
figure), we need to interpolate the photons density <em>between</em> each sampling
point as well.</p>
<p>Given two arbitrary sampling points and their corresponding distance <span>d_n</span>
and <span>d_{n+1}</span>, any intermediate distance can be linearly interpolated with
<span>r=\mathrm{mix}(d_n,d_{n+1},t)</span> where <span>t</span> is within <span>[0,1]</span>. Applying the
inverse square law from before (<span>1/r^2</span>), the integrated photons density between
these 2 points can be expressed with this formula (in <span>t</span> range):</p>
<p>
v = \Delta t \int \frac{1}{\mathrm{mix}(d_n,d_{n+1},t)^2} dt
</p>
<p><span>t</span> being normalized, the <span>\Delta t</span> is here to covers the actual segment
distance. With the help of Sympy we can do the integration:</p>
<pre><code>&gt;&gt;&gt; a, b, D, t = symbols('a b D t', real=True)
&gt;&gt;&gt; mix = a*(1-t) + b*t
&gt;&gt;&gt; D * integrate(1/mix**2, (t,0,1)).simplify()
 D
───
a⋅b
</code></pre>
<p>So the result of this integration is:</p>
<p>
v = \frac{\Delta t}{d_{n}d_{n+1}}.
</p>
<p>Now the key is that in the loop, <span>\Delta t</span> stepping is actually <span>d_{n+1}</span>, so
we end up with:</p>
<p>
v = \frac{\Delta t}{d_{n}\Delta t} = \frac{1}{d_n}
</p>
<p>And we find back our mysterious <span>1/d</span>. It's "physically correct", assuming
vacuum space. Of course, reality is more complex, and we don't even need to
stick to that formula, but it was nice figuring out that this simple fraction is
a fairly good model of reality.</p>
<h3>Going through the object</h3>
<p>In the cube example we didn't go through the object, using <code>max(d, .001)</code>. But
if we were to add some transparency, we could have used <code>d = A*abs(d)+B</code>
instead, where <code>A</code> could be interpreted as absorption and <code>B</code> the pass-through,
or transparency.</p>
<figure>
  <canvas width="480" height="340" data-fragment="/frag/demo-tricks/onecube-alpha.frag"></canvas>
  <figcaption>One glowing, transparent, and rotating cube; A=0.4, B=0.1</figcaption>
</figure>
<p>I first saw this formula mentioned in <a href="https://mini.gmshaders.com/p/volumetric">Xor article on volumetric</a>.
To understand it a bit better, here is my intuitive take: the <code>+B</code> causes a
potential penetration into the solid at the next iteration, which wouldn't
happen otherwise (or only very marginally). When inside the solid, the <code>abs(d)</code>
causes the ray to continue further (by the amount of the distance to the closest
edge). Then the multiplication by <code>A</code> makes sure we don't penetrate too fast
into it; it's the absorption, or "damping".</p>
<p>This is basically the technique I used in Moonlight to avoid the complex
absorption/emission code.</p>

<h2>Entrance 3</h2>
<figure>
  <canvas width="480" height="340" data-fragment="/frag/demo-tricks/entrance3.frag"></canvas>
  <figcaption>Entrance 3 demo in 465 characters</figcaption>
</figure>
<pre><code>// Entrance 3 [465] by bµg
// License: CC BY-NC-SA 4.0
#define V for(s++;d&lt;l&amp;&amp;s&gt;.001;q=abs(p+=v*s)-45.,b=abs(p+vec3(mod(T*5.,80.)-7.,45.+sin(T*10.)*.2,12))-vec3(1,7,1),d+=s=min(max(p.y,-min(max(abs(p.y+28.)-17.,abs(p.z+12.)-4.),max(q.x,max(q.y,q.z)))),max(b.x,max(b.y,b.z))))
void main(){float d,s,r=1.7,l=2e2;vec3 b,v=b-.58,q,p=mat3(r,0,-r,-1,2,-1,b+1.4)*vec3((P+P-R)/R.y*20.4,30);V;r=exp(-d*d/1e4)*.2;l=length(v=-vec3(90,30,10)-p);v/=l;d=1.;V;r+=50.*d/l/l;O=vec4(pow(mix(vec3(0,4,9),vec3(80,7,2),r*r)*.01,p-p+.45),1);}
</code></pre>

<p>This demo was probably one of the most challenging, but I'm pretty happy with its
atmospheric vibe. It's kind of different than the usual demos for this size.</p>
<p>I initially tried with some voxels, but I couldn't make it work with the light
under 512 characters (the initialization code was too large, not the branchless
<a href="https://en.wikipedia.org/wiki/Digital_differential_analyzer_(graphics_algorithm)">DDA</a> stepping). It also had annoying limitations (typically the animation was
unit bound), so I fell back to a classic raymarching.</p>
<p>The first thing I did differently was to use an <a href="https://iquilezles.org/articles/distfunctions2dlinf/">L-∞ norm</a> instead of an
euclidean norm for the distance function: every solid is a cube so it's
appropriate to use simpler formulas.</p>
<p>For the light, it's not an illusion, it's an actual light: after the first
raymarch to a solid, the ray direction is reoriented toward the light and the
march runs again (it's the <code>V</code> macro). Hitting a solid or not defines if the
fragment should be lighten up or not.</p>
<h3>Mobile bugs</h3>
<p>A bad surprise of this demo was uncovering two driver bugs on mobile:</p>
<ul>
<li>One with tricky <a href="https://crbug.com/462233638">for-loop compounds on Snapdragon/Adreno</a> because I was trying
hard to avoid the macros and functions.</li>
<li>One with <a href="https://crbug.com/462288594">chained assignments on Imagination/PowerVR</a> (typically affect
Google Pixel Pro 10).</li>
</ul>
<p>The first was worked around with the <code>V</code> macro (actually saved 3 characters in
the process), but the 2nd one had to be unpacked and made me lose 2 characters.</p>
<h3>Isometry</h3>
<p>Another thing I studied was how to set up the camera in a non-perspective
<a href="https://en.wikipedia.org/wiki/Isometric_projection">isometric or dimetric view</a>. I couldn't make sense of the maths from
the Wikipedia page (it just didn't work), but Sympy rescued me again:</p>
<pre><code># Counter-clockwise rotation
a, ax0, ax1, ax2 = symbols('a ax0:3')
c, s = cos(a), sin(a)
k = 1-c
rot = Matrix(3,3, [
    # col 1            col 2              # col 3
    ax0*ax0*k + c,     ax0*ax1*k + ax2*s, ax0*ax2*k - ax1*s, # row 1
    ax1*ax0*k - ax2*s, ax1*ax1*k + c,     ax1*ax2*k + ax0*s, # row 2
    ax2*ax0*k + ax1*s, ax2*ax1*k - ax0*s, ax2*ax2*k + c      # row 3
])

# Rotation by 45° on the y-axis
m45 = rot.subs({a:rad(-45), ax0:0, ax1:1, ax2:0})

# Apply the 2nd rotation on the x-axis to get the transform matrices for two
# classic projections
# Note: asin(tan(rad(30))) is the same as atan(sin(rad(45)))
isometric = m45 * rot.subs({a:asin(tan(rad(30))), ax0:1, ax1:0, ax2:0})
dimetric  = m45 * rot.subs({a:         rad(30),   ax0:1, ax1:0, ax2:0})
</code></pre>
<p>Inspecting the matrices and factoring out the common terms, we obtain the
following transform matrices:</p>
<p>
M_{iso} = \sqrt{2}\sqrt{3}\begin{bmatrix}
   \sqrt{3} &amp; -1 &amp; \sqrt{2} \\
          0 &amp;  2 &amp; \sqrt{2} \\
  -\sqrt{3} &amp; -1 &amp; \sqrt{2}
\end{bmatrix} \text{ and } M_{dim} = \frac{4}{\sqrt{2}}\begin{bmatrix}
     2 &amp;       -1 &amp; \sqrt{3} \\
     0 &amp; \sqrt{6} &amp; \sqrt{2} \\
    -2 &amp;       -1 &amp; \sqrt{3}
\end{bmatrix}
</p>
<p>The ray direction is common to all fragments, so we use the central UV
coordinate (0,0) as reference point. We push it forward for convenience: (0,0,1),
and transform it with our matrix. This gives the central screen coordinate in
world space. Since the obtained point coordinate is relative to the world
origin, to go from that point to the origin, we just have to flip its sign. The
ray direction formula is then:</p>
<p>
d_{iso} = -M_{iso} \begin{bmatrix}0 \\ 0 \\ 1\end{bmatrix} = -\frac{\sqrt{3}}{3}\begin{bmatrix}1 \\ 1 \\ 1\end{bmatrix}
\text{ and } d_{dim} = -M_{dim} \begin{bmatrix}0 \\ 0 \\ 1\end{bmatrix} = -\frac{1}{4} \begin{bmatrix}\sqrt{6} \\ 2 \\ \sqrt{6}\end{bmatrix}
</p>
<p>To get the ray origin of every other pixel, the remaining question is: what is
the smallest distance we need to step back the screen coordinates such that,
when applying the transformation, the view wouldn't clip into the ground at
<span>y=0</span>.</p>
<p>This requirement can be modeled with the following expression:</p>
<p>
M \begin{bmatrix}x \\ -1 \\ z\end{bmatrix} &gt; 0
</p>
<p>The -1 being the lowest y-screen coordinate (which we don't want into the
ground). The lazy bum in me just asks Sympy to solve it for me:</p>
<pre><code>x, z = symbols("x z", real=True)
u = m * Matrix([x, -1, z])
uz = solve(u[1] &gt; 0, z)
</code></pre>
<p>We get <span>z&gt;\sqrt{2}</span> for isometric, and <span>z&gt;\sqrt{3}</span> for dimetric.</p>
<p>With an arbitrary scale <code>S</code> of the coordinate we end up with the following:</p>
<pre><code>const float S = 50.;
vec2 u = (P+P-R)/R.y * S; // scaled screen coordinates

float A=sqrt(2.), B=sqrt(3.);

// Isometric
vec3 rd = -vec3(1)/B,
     ro = mat3(B,0,-B,-1,2,-1,A,A,A)/A/B * vec3(u, A*S + eps);

// Dimetric
vec3 rd = -vec3(B,A,B)/A/2.,
     ro = mat3(2,0,-2,-1,A*B,-1,B,A,B)/A/2. * vec3(u, B*S + eps);
</code></pre>
<p>The <code>eps</code> is an arbitrary small value to make sure the y-coordinate ends up
above 0.</p>
<p>In Entrance 3, I used a rough approximation of the isometric setup.</p>

<h2>Archipelago</h2>
<figure>
  <canvas width="480" height="340" data-fragment="/frag/demo-tricks/archipelago.frag"></canvas>
  <figcaption>Archipelago demo in 472 characters</figcaption>
</figure>
<pre><code>// Archipelago [472] by bµg
// License: CC BY-NC-SA 4.0
#define r(a)*=mat2(cos(a+vec4(0,11,33,0))),
void main(){vec3 p,q,k;for(float w,x,a,b,i,t,h,e=.1,d=e,z=.001;i++&lt;50.&amp;&amp;d&gt;z;h+=k.y,w=h-d,t+=d=min(d,h)*.8,O=vec4((w&gt;z?k.zxx*e:k.zyz/20.)+i/1e2+max(1.-abs(w/e),z),1))for(p=normalize(vec3(P+P-R,R.y))*t,p.zy r(1.)p.z+=T+T,p.x+=sin(w=T*.4)*2.,p.xy r(cos(w)*e)d=p.y+=4.,h=d-2.3+abs(p.x*.2),q=p,k-=k,a=e,b=.8;a&gt;z;a*=.8,b*=.5)q.xz r(.6)p.xz r(.6)k.y+=abs(dot(sin(q.xz*.4/b),R-R+b)),k.x+=w=a*exp(sin(x=p.x/a*e+T+T)),p.x-=w*cos(x),d-=w;}
</code></pre>

<p>For this infinite procedurally generated Japan, I wanted to mark a rupture with
my red/orange obsession. Technically speaking, it's actually fairly basic if
you're familiar with Red Alp. I used the same noise for the mountains/islands,
but the water uses a different noise.</p>
<p>The per octave noise curve is <code>w=exp(sin(x))</code>, with the particularity of
shifting the <code>x</code> coordinate with its derivative: <code>x-=w*cos(x)</code>. This is some
form of <a href="https://iquilezles.org/articles/warp/">domain warping</a> that gives the nice effect here. When I say <code>x</code>, I'm
really referring to the x-axis position. It is not needed to work with the
z-component (xz forms the flat plane) because each octave of the fbm has a
rotation that "mixes" both axis, so <code>z</code> is actually backed in <code>x</code>.</p>
<figure>
  <img src="https://blog.pkh.me/img/demo-tricks/waves.png" alt="">
  <figcaption>w=exp(sin(x))</figcaption>
</figure>
<div>
<p>Note</p>
<p>I didn't come up with the formula, but found it first one <a href="https://youtu.be/PH9q0HNBjT4&amp;t=1025s">this video by
Acerola</a>. I don't know if he's the original author, but I've
seen the formula being replicated in various places.</p>
</div>

<h2>Cutie</h2>
<figure>
  <canvas width="480" height="340" data-fragment="/frag/demo-tricks/cutie.frag"></canvas>
  <figcaption>Cutie demo in 602 characters</figcaption>
</figure>
<pre><code>// Cutie [602] by bµg
// License: CC BY-NC-SA 4.0
#define V vec3
#define L length(p
#define C(A,B,X,Y)d=min(d,-.2*log2(exp2(X-L-A)/.2)+exp2(Y-L-B)/.2)))
#define H(Z)S,k=fract(T*1.5+s),a=V(1.3,.2,Z),b=V(1,.3*max(1.-abs(3.*k-1.),z),Z*.75+3.*max(-k*S,k-1.)),q=b*S,q+=a+sqrt(1.-dot(q,q))*normalize(V(-b.y,b.x,0)),C(a,q,3.5,2.5),C(q,a-b,2.5,2.)
void main(){float i,t,k,z,s,S=.5,d=S;for(V p,q,a,b;i++&lt;5e1&amp;&amp;d&gt;.001;t+=d=min(d,s=L+V(S-2.*p.x,-1,S))-S))p=normalize(V(P+P-R,R.y))*t,p.z-=5.,p.zy*=mat2(cos(vec4(1,12,34,1))),p.xz*=mat2(cos(sin(T)+vec4(0,11,33,0))),d=1.+p.y,C(z,V(z,z,1.2),7.5,6.),s=p.x&lt;z?p.x=-p.x,z:H(z),s+=H(1.);O=vec4(V(exp(-i/(s&gt;d?1e2:9.))),1);}
</code></pre>

<p>Here I got cocky and thought I could manage to fit it in 512 chars. I failed,
by 90 characters. I did use the <a href="https://iquilezles.org/articles/smin/">smoothmin</a> operator for the first time: every
limb of the body of Cutie is composed of two spheres creating a rounded cone
(two sphere of different size smoothly merged like metaballs).</p>
<figure>
  <canvas width="480" height="340" data-fragment="/frag/demo-tricks/metaballs.frag"></canvas>
  <figcaption>2 spheres merging using the smin operator</figcaption>
</figure>
<p>Then I used <a href="https://iquilezles.org/articles/simpleik/">simple IK kinetics</a> for the animation. Using leg parts
with a size of 1 helped simplifying the formula and make it shorter.</p>
<p>You may be wondering about the smooth visuals itself: I didn't use the depth
map but simply the number of iterations. Due to the nature of the raymarching
algorithm, when a ray passes close to a shape, it slows down significantly,
increasing the number of iterations. This is super useful because it exaggerate
the contour of the shapes naturally. It's wrapped into an exponential, but <code>i</code>
defines the output color directly.</p>
<h2>What's next</h2>
<p>I will continue making more of those, keeping my artistic ambition low because
of the 512 characters constraint I'm imposing on myself.</p>
<p><img src="https://blog.pkh.me/img/demo-tricks/512.jpg" alt="meme about the 512 chars limit"></p>
<p>You may be wondering why I keep this obsession about 512 characters, and many
people called me out on this one. There are actually many arguments:</p>
<ul>
<li>A tiny demo has to focus on one or two very scoped aspects of computer
graphics, which makes it perfect as a <strong>learning support</strong>.</li>
<li>It's part of the <strong>artistic performance</strong>: it's not just techniques and
visuals, the wizardry of the code is part of why it's so impressive. We're in
an era of visuals, people have been fed with the craziest VFX ever. But have
they seen them with a few hundreds bytes of code?</li>
<li>The constraint helps me <strong>finish the work</strong>: when making art, there is always
this question of when to stop. Here there is an intractable point where I just
cannot do more and I have to move on.</li>
<li>Similarly, it <strong>prevents my ambition</strong> from tricking me into some colossal
project I will never finish or even start. That format has a ton of
limitations, and that's its strength.</li>
<li>Working on such a tiny piece of code for days/weeks just <strong>brings me joy</strong>. I
do feel like a craftsperson, spending an unreasonable amount of time
perfecting it, for the beauty of it.</li>
<li>I'm trying to build a portfolio, and it's important for me to keep it
<strong>consistent</strong>. If the size limit was different, I would have done things
differently, so I can't change it now. If I had hundreds more characters,
Red Alp might have had birds, the sky opening to lit a beam of light on the
mountains, etc.</li>
</ul>
<p>Why 512 in particular? It happens to be the size of a toot on <a href="https://fosstodon.org/@bug">my Mastodon
instance</a> so I can fit the code there, and I found it to be a good
balance.</p>
</article><p>For updates and more frequent content you can follow me on
<a href="https://fosstodon.org/@bug">Mastodon</a>. Feel also free to subscribe to the
<a href="https://blog.pkh.me/rss.xml">RSS</a> in order to be notified of new write-ups. It is also usually
possible to reach me through other means (check the footer below). Finally,
discussions on some of the articles can sometimes be found on HackerNews,
Lobste.rs and Reddit.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hunting for North Korean Fiber Optic Cables (246 pts)]]></title>
            <link>https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/</link>
            <guid>46194384</guid>
            <pubDate>Mon, 08 Dec 2025 16:38:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/">https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/</a>, See on <a href="https://news.ycombinator.com/item?id=46194384">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>Before we go any further, one thing that I want to make clear is that the word <em>assume</em> is going to be doing some heavy lifting throughout this post. This was a rabbit hole that I recently went down and I probably have more questions than answers, but I still wanted to document what I had found so far. If you have additional information or findings you want to share, as always feel free to reach out: <a>contact@nkinternet.com</a>.</p>



<p>It all started with a PowerPoint that I came across a few weeks ago. It was presented by the DPRK to the ICAO on the state of their aviation industry and their ADS-B deployment inside North Korea. However, one slide in particular caught my eye because it showed a fiber optic cable running across the country</p>



<figure><a href="https://nkinternet.com/wp-content/uploads/2025/12/d3e27ea5-25f1-49ed-9392-e38cb466b732.png"><img data-attachment-id="1012" data-permalink="https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/d3e27ea5-25f1-49ed-9392-e38cb466b732/" data-orig-file="https://nkinternet.com/wp-content/uploads/2025/12/d3e27ea5-25f1-49ed-9392-e38cb466b732.png" data-orig-size="1224,742" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="d3e27ea5-25f1-49ed-9392-e38cb466b732" data-image-description="" data-image-caption="" data-medium-file="https://nkinternet.com/wp-content/uploads/2025/12/d3e27ea5-25f1-49ed-9392-e38cb466b732.png?w=300" data-large-file="https://nkinternet.com/wp-content/uploads/2025/12/d3e27ea5-25f1-49ed-9392-e38cb466b732.png?w=676" width="1024" height="620" src="https://nkinternet.com/wp-content/uploads/2025/12/d3e27ea5-25f1-49ed-9392-e38cb466b732.png?w=1024" alt="" srcset="https://nkinternet.com/wp-content/uploads/2025/12/d3e27ea5-25f1-49ed-9392-e38cb466b732.png?w=1024 1024w, https://nkinternet.com/wp-content/uploads/2025/12/d3e27ea5-25f1-49ed-9392-e38cb466b732.png?w=150 150w, https://nkinternet.com/wp-content/uploads/2025/12/d3e27ea5-25f1-49ed-9392-e38cb466b732.png?w=300 300w, https://nkinternet.com/wp-content/uploads/2025/12/d3e27ea5-25f1-49ed-9392-e38cb466b732.png?w=768 768w, https://nkinternet.com/wp-content/uploads/2025/12/d3e27ea5-25f1-49ed-9392-e38cb466b732.png 1224w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption><em>You can find a full link to the presentation <a href="https://nkinternet.com/wp-content/uploads/2025/11/gacadprk-presentation2019.4.pdf">here</a></em>.</figcaption></figure>



<p>This got me wondering more about the physical layout of the network inside North Korea. From the map we know that there’s a connection between Pyongyang and Odaejin, although given the mountains in the middle of the country it probably isn’t a direct link. There isn’t a lot of information on fiber in North Korea, but there are a few outside sources that help provide clues about how things might be laid out.</p>



<p><strong>Historic Fiber Information</strong></p>



<p><a href="https://www.38north.org/2017/10/mwilliams100117/">38North</a> first reported the connection from Russia’s TTK to the DPRK over the Korea–Russia Friendship Bridge back in 2017. Additionally, a picture found on Flickr looking toward Tumangang after the bridge doesn’t show any utility poles and instead seems to display some kind of infrastructure in the grass to the side of the tracks. Assuming this interpretation is correct, the fiber is likely buried underground as it enters the country and passes through the vicinity of Tumangang Station.</p>



<figure><a href="https://nkinternet.com/wp-content/uploads/2025/12/image.png"><img data-attachment-id="1015" data-permalink="https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/image-18/" data-orig-file="https://nkinternet.com/wp-content/uploads/2025/12/image.png" data-orig-size="1507,947" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://nkinternet.com/wp-content/uploads/2025/12/image.png?w=300" data-large-file="https://nkinternet.com/wp-content/uploads/2025/12/image.png?w=676" width="1024" height="643" src="https://nkinternet.com/wp-content/uploads/2025/12/image.png?w=1024" alt="" srcset="https://nkinternet.com/wp-content/uploads/2025/12/image.png?w=1024 1024w, https://nkinternet.com/wp-content/uploads/2025/12/image.png?w=150 150w, https://nkinternet.com/wp-content/uploads/2025/12/image.png?w=300 300w, https://nkinternet.com/wp-content/uploads/2025/12/image.png?w=768 768w, https://nkinternet.com/wp-content/uploads/2025/12/image.png?w=1440 1440w, https://nkinternet.com/wp-content/uploads/2025/12/image.png 1507w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption><em>From user Moravius on Flickr</em> <em>which appears to show possible infrastructure in the grass. The white pole on the right side of the tracks are used as distance markers.</em></figcaption></figure>



<p>According to a report from <a href="https://www.nautilus.org/wp-content/uploads/2011/12/DPRK_Digital_Transformation.pdf">The Nautilus Institute</a> we can gather a few additional details about the internet inside North Korea</p>



<ul>
<li>One of the first lines was installed in September 1995 between Pyongyang and Hamhung</li>



<li>In February 1998 a link between Pyongyang and Sinuiju was completed</li>



<li>As of 2000, DPRK’s operational optical fiber telecom lines included: Pyongyang – Hamhung; Pyongyang – Sinuiju including all cities and counties in North Pyongan Province; Hamhung Rajin-Sonbong; Rajin-Songbong – Hunchun (China), Pyongyang – Nampo.</li>



<li>In 2003 the original domestic cell phone network was built for North Korean citizens in Pyongyang, Namp’o, reportedly in all provincial capitals, on the Pyongyang-Myohyangsan tourist highway, and the Pyongyang-Kaesong and Wonsan-Hamhung highways</li>



<li>The Kwangmyong network’s data is transmitted via fiber optic cable with a backbone capacity of 2.5 GB per second between all the provinces.</li>
</ul>



<p>Based on these notes, it starts to paint a picture that the fiber link coming from Russia likely travels down the east coast of the DPRK before connecting to Pyongyang. Several city pairs—Pyongyang–Hamhung and Rajin–Sonbong—line up with earlier deployments of east-coast fiber infrastructure.</p>



<p><strong>Kwangmyong Internal Topology</strong></p>



<p>The report also notes that all of the provinces in North Korea were connected to the Kwangmyong via fiber. The Kwangmyong for those not familiar is the intranet that most citizens in the DPRK can access as they do not have access to the outside internet. While not much information is available about the Kwangmyong, these <a href="https://www.koreaittimes.com/news/articleView.html?idxno=57500">notes</a> from Choi Sung, Professor of Computer Science at Namseoul University provides some additional details on how the network is laid how, as well as information on the regional networks that are connected. A map provided in his notes shows some of the main points of the Kwangmyong with three of them located along the northeast of North Korea.</p>



<figure><a href="https://nkinternet.com/wp-content/uploads/2025/12/57500_23201_1225.jpg"><img data-attachment-id="1020" data-permalink="https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/57500_23201_1225/" data-orig-file="https://nkinternet.com/wp-content/uploads/2025/12/57500_23201_1225.jpg" data-orig-size="600,494" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="57500_23201_1225" data-image-description="" data-image-caption="" data-medium-file="https://nkinternet.com/wp-content/uploads/2025/12/57500_23201_1225.jpg?w=300" data-large-file="https://nkinternet.com/wp-content/uploads/2025/12/57500_23201_1225.jpg?w=600" width="600" height="494" src="https://nkinternet.com/wp-content/uploads/2025/12/57500_23201_1225.jpg?w=600" alt="" srcset="https://nkinternet.com/wp-content/uploads/2025/12/57500_23201_1225.jpg 600w, https://nkinternet.com/wp-content/uploads/2025/12/57500_23201_1225.jpg?w=150 150w, https://nkinternet.com/wp-content/uploads/2025/12/57500_23201_1225.jpg?w=300 300w" sizes="(max-width: 600px) 100vw, 600px"></a></figure>



<p><strong>Railways, Roads, and Practical Fiber Routing</strong></p>



<p>This starts to paint a rough picture of how the network is physically deployed in North Korea but we can also look to some outside sources to get some confirmation. <a href="https://www.38north.org/wp-content/uploads/2022/11/02_38-North_North-Korean-Cell-Coverage-Map.png">38North </a>once again provides some great detail on cell phone towers in North Korea. The interesting thing being an apparent line down the east coast which follows major roads and highways but would also in theory have easier access to the fiber back haul to support the cell network.</p>



<figure><a href="https://nkinternet.com/wp-content/uploads/2025/12/02_38-north_north-korean-cell-coverage-map.png"><img data-attachment-id="1022" data-permalink="https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/02_38-north_north-korean-cell-coverage-map/" data-orig-file="https://nkinternet.com/wp-content/uploads/2025/12/02_38-north_north-korean-cell-coverage-map.png" data-orig-size="3319,3305" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="02_38-North_North-Korean-Cell-Coverage-Map" data-image-description="" data-image-caption="" data-medium-file="https://nkinternet.com/wp-content/uploads/2025/12/02_38-north_north-korean-cell-coverage-map.png?w=300" data-large-file="https://nkinternet.com/wp-content/uploads/2025/12/02_38-north_north-korean-cell-coverage-map.png?w=676" loading="lazy" width="1024" height="1019" src="https://nkinternet.com/wp-content/uploads/2025/12/02_38-north_north-korean-cell-coverage-map.png?w=1024" alt="" srcset="https://nkinternet.com/wp-content/uploads/2025/12/02_38-north_north-korean-cell-coverage-map.png?w=1024 1024w, https://nkinternet.com/wp-content/uploads/2025/12/02_38-north_north-korean-cell-coverage-map.png?w=2048 2048w, https://nkinternet.com/wp-content/uploads/2025/12/02_38-north_north-korean-cell-coverage-map.png?w=150 150w, https://nkinternet.com/wp-content/uploads/2025/12/02_38-north_north-korean-cell-coverage-map.png?w=300 300w, https://nkinternet.com/wp-content/uploads/2025/12/02_38-north_north-korean-cell-coverage-map.png?w=768 768w, https://nkinternet.com/wp-content/uploads/2025/12/02_38-north_north-korean-cell-coverage-map.png?w=1440 1440w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>All of this seems to suggest that the fiber lines were run along major roads and railways up the east coast. A map from Beyond Parallel shows the major rail lines, which has the Pyongra line up the east coast.</p>



<figure><a href="https://nkinternet.com/wp-content/uploads/2025/12/railway-map_crossings_update_map-e1543948581630.jpg"><img data-attachment-id="1024" data-permalink="https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/railway-map_crossings_update_map-e1543948581630/" data-orig-file="https://nkinternet.com/wp-content/uploads/2025/12/railway-map_crossings_update_map-e1543948581630.jpg" data-orig-size="1000,716" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Railway-Map_crossings_update_map-e1543948581630" data-image-description="" data-image-caption="" data-medium-file="https://nkinternet.com/wp-content/uploads/2025/12/railway-map_crossings_update_map-e1543948581630.jpg?w=300" data-large-file="https://nkinternet.com/wp-content/uploads/2025/12/railway-map_crossings_update_map-e1543948581630.jpg?w=676" loading="lazy" width="1000" height="716" src="https://nkinternet.com/wp-content/uploads/2025/12/railway-map_crossings_update_map-e1543948581630.jpg?w=1000" alt="" srcset="https://nkinternet.com/wp-content/uploads/2025/12/railway-map_crossings_update_map-e1543948581630.jpg 1000w, https://nkinternet.com/wp-content/uploads/2025/12/railway-map_crossings_update_map-e1543948581630.jpg?w=150 150w, https://nkinternet.com/wp-content/uploads/2025/12/railway-map_crossings_update_map-e1543948581630.jpg?w=300 300w, https://nkinternet.com/wp-content/uploads/2025/12/railway-map_crossings_update_map-e1543948581630.jpg?w=768 768w" sizes="(max-width: 1000px) 100vw, 1000px"></a></figure>



<p><strong>Looking For Clues Along the Railway</strong></p>



<p>Some additional digging for pictures from along the line suggest that there is infrastructure deployed along the tracks, although it’s difficult to confirm from pictures exactly what is buried. The following shows what appears to be a junction box at the base of a pole along the line.</p>



<figure><a href="https://www.flickr.com/photos/josephferris76/6993153876/"><img data-attachment-id="1026" data-permalink="https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/image-19/" data-orig-file="https://nkinternet.com/wp-content/uploads/2025/12/image-1.png" data-orig-size="600,443" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://nkinternet.com/wp-content/uploads/2025/12/image-1.png?w=300" data-large-file="https://nkinternet.com/wp-content/uploads/2025/12/image-1.png?w=600" loading="lazy" width="600" height="443" src="https://nkinternet.com/wp-content/uploads/2025/12/image-1.png?w=600" alt="" srcset="https://nkinternet.com/wp-content/uploads/2025/12/image-1.png 600w, https://nkinternet.com/wp-content/uploads/2025/12/image-1.png?w=150 150w, https://nkinternet.com/wp-content/uploads/2025/12/image-1.png?w=300 300w" sizes="(max-width: 600px) 100vw, 600px"></a><figcaption><em>Picture from Flickr user josephferris</em></figcaption></figure>



<p>The line does have a path along it as well with mile markers. While it is used by bikes and pedestrians, it provides a nice path for supporting fiber and other communications runs along the tracks.</p>



<figure><a href="https://www.flickr.com/photos/21005841@N04/15644749122/in/album-72157648984981562"><img data-attachment-id="1028" data-permalink="https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/image-20/" data-orig-file="https://nkinternet.com/wp-content/uploads/2025/12/image-2.png" data-orig-size="835,820" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://nkinternet.com/wp-content/uploads/2025/12/image-2.png?w=300" data-large-file="https://nkinternet.com/wp-content/uploads/2025/12/image-2.png?w=676" loading="lazy" width="835" height="820" src="https://nkinternet.com/wp-content/uploads/2025/12/image-2.png?w=835" alt="" srcset="https://nkinternet.com/wp-content/uploads/2025/12/image-2.png 835w, https://nkinternet.com/wp-content/uploads/2025/12/image-2.png?w=150 150w, https://nkinternet.com/wp-content/uploads/2025/12/image-2.png?w=300 300w, https://nkinternet.com/wp-content/uploads/2025/12/image-2.png?w=768 768w" sizes="(max-width: 835px) 100vw, 835px"></a><figcaption><em>Picture from Flickr user Andrew M. showing paths along the line.</em></figcaption></figure>



<p>The Pyongra line also crosses through the mountains at points but it is assumed at certain junctions the fiber was laid along the AH 6/National Highway 7 up the coast as there are parts of the line discovered that do not have a path along the tracks. In these places it is assumed they follow the road, although finding pictures of the highway to further examine is challenging.</p>



<figure><a href="https://nkinternet.com/wp-content/uploads/2025/12/image-6.png"><img data-attachment-id="1037" data-permalink="https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/image-24/" data-orig-file="https://nkinternet.com/wp-content/uploads/2025/12/image-6.png" data-orig-size="1321,1049" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://nkinternet.com/wp-content/uploads/2025/12/image-6.png?w=300" data-large-file="https://nkinternet.com/wp-content/uploads/2025/12/image-6.png?w=676" loading="lazy" width="1024" height="813" src="https://nkinternet.com/wp-content/uploads/2025/12/image-6.png?w=1024" alt="" srcset="https://nkinternet.com/wp-content/uploads/2025/12/image-6.png?w=1024 1024w, https://nkinternet.com/wp-content/uploads/2025/12/image-6.png?w=150 150w, https://nkinternet.com/wp-content/uploads/2025/12/image-6.png?w=300 300w, https://nkinternet.com/wp-content/uploads/2025/12/image-6.png?w=768 768w, https://nkinternet.com/wp-content/uploads/2025/12/image-6.png 1321w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption><em>Pyongra line through the mountains. At these points it’s assumed that the fiber optic cables are laid along roads/highways instead of the right of way along the railroad.</em></figcaption></figure>



<p>Lastly at certain stations we can see utility boxes along the side of the track suggesting buried conduits/cables are laid along the tracks.</p>



<figure><a href="https://www.flickr.com/photos/mytripsmypics/26020131208/in/gallery-194433501@N07-72157720168302177/"><img data-attachment-id="1030" data-permalink="https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/image-21/" data-orig-file="https://nkinternet.com/wp-content/uploads/2025/12/image-3.png" data-orig-size="360,304" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://nkinternet.com/wp-content/uploads/2025/12/image-3.png?w=300" data-large-file="https://nkinternet.com/wp-content/uploads/2025/12/image-3.png?w=360" loading="lazy" width="360" height="304" src="https://nkinternet.com/wp-content/uploads/2025/12/image-3.png?w=360" alt="" srcset="https://nkinternet.com/wp-content/uploads/2025/12/image-3.png 360w, https://nkinternet.com/wp-content/uploads/2025/12/image-3.png?w=150 150w, https://nkinternet.com/wp-content/uploads/2025/12/image-3.png?w=300 300w" sizes="(max-width: 360px) 100vw, 360px"></a></figure>



<p>From a video taken in 2012 there does appear to be some signs of objects along the tracks, although difficult to confirm due to the video quality. The screenshot below is the clearest I could find of a rectangular box buried in a clearing along the line.</p>



<figure><a href="https://www.flickr.com/photos/21005841@N04/15599502031/in/pool-dprk_rail/"><img data-attachment-id="1032" data-permalink="https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/image-22/" data-orig-file="https://nkinternet.com/wp-content/uploads/2025/12/image-4.png" data-orig-size="997,609" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://nkinternet.com/wp-content/uploads/2025/12/image-4.png?w=300" data-large-file="https://nkinternet.com/wp-content/uploads/2025/12/image-4.png?w=676" loading="lazy" width="997" height="609" src="https://nkinternet.com/wp-content/uploads/2025/12/image-4.png?w=997" alt="" srcset="https://nkinternet.com/wp-content/uploads/2025/12/image-4.png 997w, https://nkinternet.com/wp-content/uploads/2025/12/image-4.png?w=150 150w, https://nkinternet.com/wp-content/uploads/2025/12/image-4.png?w=300 300w, https://nkinternet.com/wp-content/uploads/2025/12/image-4.png?w=768 768w" sizes="(max-width: 997px) 100vw, 997px"></a><figcaption><em>From Flickr user Andrew M. Screenshot is from ~21 seconds in the linked video</em></figcaption></figure>



<p>Based on this information of what is confirmed and looking at major cities, it appears there is a route that follows Pyongyang → Wonsan → Hamhung → Chongjin → Rajin → Tumangang which follows the Pyongra line as well as the AH 6/National Highway 7 up the coast. The following map highlights a rough path.</p>



<figure><a href="https://nkinternet.com/wp-content/uploads/2025/12/image-5.png"><img data-attachment-id="1035" data-permalink="https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/image-23/" data-orig-file="https://nkinternet.com/wp-content/uploads/2025/12/image-5.png" data-orig-size="1524,1080" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://nkinternet.com/wp-content/uploads/2025/12/image-5.png?w=300" data-large-file="https://nkinternet.com/wp-content/uploads/2025/12/image-5.png?w=676" loading="lazy" width="1024" height="725" src="https://nkinternet.com/wp-content/uploads/2025/12/image-5.png?w=1024" alt="" srcset="https://nkinternet.com/wp-content/uploads/2025/12/image-5.png?w=1024 1024w, https://nkinternet.com/wp-content/uploads/2025/12/image-5.png?w=150 150w, https://nkinternet.com/wp-content/uploads/2025/12/image-5.png?w=300 300w, https://nkinternet.com/wp-content/uploads/2025/12/image-5.png?w=768 768w, https://nkinternet.com/wp-content/uploads/2025/12/image-5.png?w=1440 1440w, https://nkinternet.com/wp-content/uploads/2025/12/image-5.png 1524w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>Interestingly by mapping out the possible fiber locations we can start to draw conclusions based on other sources. According to a video by <a href="https://www.youtube.com/watch?v=AcjXPae3Mhw">Cappy’s Army</a> he proposes that when the US Navy Seals landed in NOrth Korea in 2019 the most likely place this would have occurred is Sinpo. As the goal was to depoy a covert listening device this could also line up with supporting the idea that a fiber backbone runs down the east coast of North Korea as Sinpo would be relatively close. </p>



<p><strong>What Does This Mean For the Network?</strong></p>



<p>In addition to the fiber link via Russia, the other fiber optic cable into North Korea comes in via China by way of Sinuiju and Dandong. Although we don’t know for sure where servers are deployed inside North Korea, based on the map of Kwangmyong the first assumption is that things are mainly centralized in Pyongyang.</p>



<p>Out of the 1,024 IPs assigned to North Korea we observe the following behavior based on the CIDR block:</p>



<ul>
<li>175.45.176.0/24 is exclusively routed via China Unicom</li>



<li>175.45.177.0/24 is exclusively routed via Russia TransTelekom</li>



<li>175.45.178.0/24 is dual-homed and can take either path before crossing into North Korea</li>
</ul>



<p>With this information in mind, running a traceroute with the TCP flag set gives us a slightly better look at how traffic behaves once it reaches the country. For the following tests we’re going to assume there is a fiber path on the west coming in from China toward Pyongyang, as well as a path on the east side coming from Russia.</p>



<p>From the US east coast to 175.45.176.71, the final hop in China before entering North Korea shows roughly 50 ms of additional latency before reaching the DPRK host. This suggests there may be extra devices, distance, or internal routing inside the country before the packet reaches its final destination.</p>



<pre>10 103.35.255.254 (103.35.255.254) 234.306 ms 234.082 ms 234.329 ms<br>11 * * * <br>12 * * * <br>13 * * * <br>14 175.45.176.71 (175.45.176.71) 296.081 ms 294.795 ms 294.605 ms <br>15 175.45.176.71 (175.45.176.71) 282.938 ms 284.446 ms 282.227 ms</pre>



<p>Interestingly, running a traceroute to 175.45.177.10 shows a similar pattern in terms of missing hops, but with much lower internal latency. In fact, the ~4 ms difference between the last Russian router and the DPRK host suggests the handoff between Russia and North Korea happens very close—network-wise—to where this device is located. This contrasts with the China path, which appears to take a longer or more complex route before reaching its final destination.</p>



<pre>10	188.43.225.153	185.192 ms	183.649 ms	189.089 ms<br>11	 * 	 * 		 <br>12	 * 	 * 		 <br>13	 * 	 * 		 <br>14	175.45.177.10	195.996 ms	186.801 ms	186.353 ms<br>15	175.45.177.10	188.886 ms	201.103 ms	193.334 </pre>



<p>If everything is centralized in Pyongyang this would mean the handoff from Russia is completed in Pyongyang as well. However, it could also indicate that 175.45.177.0/24 is not hosted in Pyongyang at all and is instead located closer to the Russia–North Korea border. More testing is definitely required however before any conclusions can be drawn about where these devices physically reside.</p>



<p><strong>What can we learn from all of this? </strong></p>



<p>Making some assumptions we can get a better idea of how the internet works and is laid out inside North Korea. While not much is officially confirmed using some other sources we can get a possible idea of how things work. As mentioned at the start, the word assume does a lot of heavy lifting. However if you do have other information or ideas feel free to reach out at contact@nkinternet.com</p>





<div>
	
	<hr>
	

	
	<h3>Discover more from North Korean Internet</h3>
	

	
	<p>Subscribe to get the latest posts sent to your email.</p>
	

	
	
	
</div>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Let's put Tailscale on a jailbroken Kindle (264 pts)]]></title>
            <link>https://tailscale.com/blog/tailscale-jailbroken-kindle</link>
            <guid>46194337</guid>
            <pubDate>Mon, 08 Dec 2025 16:34:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tailscale.com/blog/tailscale-jailbroken-kindle">https://tailscale.com/blog/tailscale-jailbroken-kindle</a>, See on <a href="https://news.ycombinator.com/item?id=46194337">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>“It’s a rite of passage to run Tailscale on weird devices.”</p><p>So writes Mitanshu Sukhwani <a target="" rel="noreferrer" href="https://blog.papermatch.me/html/Tailscale_on_Kindle">on his blog</a>, detailing the steps for getting Tailscale onto a <a target="" rel="noreferrer" href="https://kindlemodding.org/">jailbroken Kindle</a>. Getting there, and seeing a kindle entry with a satisfying green dot in your <a target="" rel="noreferrer" href="https://login.tailscale.com/start/?utm_source=blog&amp;utm_medium=content&amp;utm_campaign=jailbreak-kindle">Tailscale admin console</a>, takes some doing. But take the trip, and you’ll end up with an e-reader that can run some neat unofficial apps, and is more open to third-party and DRM-free ebooks. And with a Tailscale connection, it’s easier to connect to files and a command line on your underpowered little Linux slab.</p><p>“For me, it's the freedom of being able to do anything with the device I own,” Sukhwani writes by email. “What I can do with the freedom is a different story.”</p><figure id=""><img _type="asset" video="[object Object]" alt="Close-up of a Kindle, with a Tailscale logo across its screen, and quote from Fredric Jameson on top: &quot;If everything means something else, then so does technology.&quot;" loading="lazy" width="1000" height="750" decoding="async" data-nimg="1" srcset="https://cdn.sanity.io/images/w77i7m8x/production/7bbe7afbd47ed73302e8e97c51311c31406e10d5-1000x750.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1x, https://cdn.sanity.io/images/w77i7m8x/production/7bbe7afbd47ed73302e8e97c51311c31406e10d5-1000x750.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2x" src="https://cdn.sanity.io/images/w77i7m8x/production/7bbe7afbd47ed73302e8e97c51311c31406e10d5-1000x750.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format"><figcaption>A jailbroken Kindle allows you to set a custom screensaver inside KOReader—even transparent, if you like. Corporate logos are optional.</figcaption></figure><h2 id="what-is-a-jailbroken-kindle-exactly"><a href="#what-is-a-jailbroken-kindle-exactly">What is a jailbroken Kindle, exactly?<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2><p><a target="" rel="noreferrer" href="https://en.wikipedia.org/wiki/Privilege_escalation#Jailbreaking">Jailbreaking</a> refers to removing the software restrictions on a device put there by its maker. Getting around these restrictions, typically by gaining “root” or administrative access, allows for accessing operating system internals, running unapproved software, and generally doing more things than a manufacturer intended. With the Kindle, you still get the standard Kindle reading experience, including Amazon's store and the ability to send the Kindle books from apps like Libby. You just add many more options, too.</p><p>The term gained purchase after the first iPhone’s debut in mid-2007; since then, nearly every device with a restricted environment has gained its own jailbreaking scene, including Kindles (debuting five months after the iPhone).</p><p>Kindle jailbreaks come along every so often. Right now, an unlocking scheme based on Amazon’s own lockscreen ads, “<a target="" rel="noreferrer" href="https://kindlemodding.org/jailbreaking/AdBreak/">AdBreak</a>,” is available for all but the most up-to-date Kindles (earlier than firmware version 5.18.5.0.2). I know this because I wrote this paragraph and the next on my 11th-generation Kindle, using the open-source Textadept editor, a <a target="" rel="noreferrer" href="https://www.mobileread.com/forums/showthread.php?t=369712&amp;highlight=bluetooth+keyboard">Bluetooth keyboard</a>, and <a target="" rel="noreferrer" href="https://login.tailscale.com/start/?utm_source=blog&amp;utm_medium=content&amp;utm_campaign=jailbreak-kindle">Tailscale</a> to move this draft file around.</p><p>One paragraph doesn’t seem that impressive until you consider that on a standard Kindle, you cannot do any of that. Transferring files by SSH, or Taildrop, is certainly not allowed. And that’s in addition to other upgrades you can get by jailbreaking a Kindle, including the feature-rich, customizable e-reader <a target="" rel="noreferrer" href="https://github.com/koreader/koreader">KOReader</a>, and lots of little apps available in repositories like <a target="" rel="noreferrer" href="https://github.com/KindleTweaks/KindleForge">KindleForge</a>.</p><p>If your Kindle has been connected to Wi-Fi all this time (as of early December 2025), it may have automatically updated itself and no longer be ready for jailbreaking. If you think it still has a chance, immediately put it into airplane mode and follow along.</p><p>Obligatory notice here: You’re running a risk of bricking your device (having it become unresponsive and unrecoverable) and voiding your warranty when you do this. That having been noted, let's dig further.</p><figure id=""><img _type="asset" video="[object Object]" alt="Close-up of a Kindle screen, showing the &quot;/Tailscale&quot; menu in large buttons: &quot;Start Tailscaled,&quot; &quot;Start Tailscale,&quot; &quot;Stop Tailscaled,&quot; &quot;Stop Tailscale,&quot; &quot;Receive Taildrop Files,&quot; and &quot;/&quot; (which is end or &quot;go back&quot;)." loading="lazy" width="1000" height="1000" decoding="async" data-nimg="1" srcset="https://cdn.sanity.io/images/w77i7m8x/production/34241c0f5bf6796f4d1da3189858d70826a4e01a-1000x1000.png?w=1080&amp;q=75&amp;fit=clip&amp;auto=format 1x, https://cdn.sanity.io/images/w77i7m8x/production/34241c0f5bf6796f4d1da3189858d70826a4e01a-1000x1000.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format 2x" src="https://cdn.sanity.io/images/w77i7m8x/production/34241c0f5bf6796f4d1da3189858d70826a4e01a-1000x1000.png?w=2048&amp;q=75&amp;fit=clip&amp;auto=format"><figcaption>It's not exactly a Liquid Glass interface, but it enables some neat tricks.</figcaption></figure><h2 id="what-tailscale-adds-to-a-jailbroken-kindle"><a href="#what-tailscale-adds-to-a-jailbroken-kindle">What Tailscale adds to a jailbroken Kindle<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2><p>Tailscale isn’t necessary on a jailbroken Kindle, but it really helps. Here are some of the ways Tailscale makes messing about with an opened-up Kindle more fun:</p><ul><li>A persistent IP address (<a target="" rel="noreferrer" href="http://100.xx/">100.xx</a>.yyy.zzz), just like any other Tailscale device, instead of having to remember yet another 192.168.random.number</li><li>Easier SSH access with <a target="" rel="noreferrer" href="https://tailscale.com/kb/1081/magicdns/?utm_source=blog&amp;utm_medium=content&amp;utm_campaign=jailbreak-kindle">magicDNS</a>: ssh root@kindle and you’re in</li><li><a target="" rel="noreferrer" href="https://tailscale.com/kb/1106/taildrop/?utm_source=blog&amp;utm_medium=content&amp;utm_campaign=jailbreak-kindle">Taildrop</a> for sending files to whatever Kindle directory you want</li><li>Setting up a self-hosted Calibre Web library with Tailscale, then securely grabbing books from it anywhere with KOReader.</li></ul><p>Key to the Kindle-plus-Tailscale experience is an easier way (SSH and Taildrop) to get epub, mobi, and other e-book and document formats into the /documents folder, ready for your KOReader sessions. Tailscale also helps with setting up some of the key jailbreak apps, saving you from plugging and unplugging the Kindle into a computer via USB cord (and then finding a second USB cord, because the first one never works, for some reason).</p><h2 id="getting-your-kindle-ready"><a href="#getting-your-kindle-ready">Getting your Kindle ready<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2><p>What follows is by no means a comprehensive guide to jailbreaking and accessing your Kindle. You will want to read the documentation for each tool and app closely. Pay particular attention to which Kindle you have, which version number of the Kindle firmware it’s running, and how much space you have left on that device.</p><p>The first step is to check your Kindle’s version number (Settings &gt; Device info) and see if there is a jailbreak method available for it. The Kindle Modding Wiki is the jailbreaking community’s go-to resource. As of this writing, there is a “WinterBreak” process available for Kindles running firmware below 15.18.1, and AdBreak is available for firmwares from 15.18.1 through 5.18.5.0.1.</p><p>If your Kindle’s version number fits one of those ranges, put it in Airplane mode and move on. If not, you’re going to have to wait until the next jailbreak method comes along.</p><figure id=""><div><p><iframe width="100%" height="100%" src="https://www.youtube.com/embed/l4ZliC82RtA?si=fWOkN9Cnf40TozhW" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p></div><figcaption>Dammit Jeff's video on the latest (as of late October) jailbreak provides both a good overview and detailed tips on setting up a jailbroken Kindle.</figcaption></figure><h2 id="the-actual-jailbreaking-part"><a href="#the-actual-jailbreaking-part">The actual jailbreaking part<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2><p>Before you dive in, have a computer (PC, Mac, or Linux) and USB cable that works with your Kindle handy. Have your Kindle on reliable Wi-Fi, like your home network—but don’t take your Kindle off airplane mode if you’ve been keeping it that way.</p><ul><li><a target="" rel="noreferrer" href="https://kindlemodding.org/jailbreaking/index.html">Follow these steps to jailbreak your Kindle</a>. The techniques are different, but you may need to do some other tasks, like enable advertisements, or <a target="" rel="noreferrer" href="https://kindlemodding.org/jailbreaking/prevent-auto-update/">fill your Kindle with junk files</a> to prevent automatic updates midway through the process.</li><li><a target="" rel="noreferrer" href="https://kindlemodding.org/jailbreaking/post-jailbreak/setting-up-a-hotfix/">Install a hotfix</a> and <a target="" rel="noreferrer" href="https://kindlemodding.org/jailbreaking/post-jailbreak/disable-ota.html">disable over-the-air updates</a> so that you can keep your Kindle on Wi-Fi and not have its jailbreak undone</li><li><a target="" rel="noreferrer" href="https://kindlemodding.org/jailbreaking/post-jailbreak/installing-kual-mrpi/">Install</a> the Kindle Unified Application Launcher (KUAL) and MRPI (MobileRead Package Installer). KUAL is vital to installing most jailbroken apps, including Tailscale.</li><li>You will almost certainly want to <a target="" rel="noreferrer" href="https://kindlemodding.org/jailbreaking/post-jailbreak/koreader.html">install KOReader</a>, too.</li></ul><p>Those bits above are standard jailbreaking procedures. If you want Tailscale on your Kindle, you’ll go a bit further.</p><h2 id="adding-tailscale-to-a-jailbroken-kindle"><a href="#adding-tailscale-to-a-jailbroken-kindle">Adding Tailscale to a jailbroken Kindle<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2><p>Make sure you have KUAL and MRPI installed and working. Next up: install this <a target="" rel="noreferrer" href="https://github.com/notmarek/kindle-usbnetlite">“simple” version of USBNetworking for Kindle</a><strong>.</strong></p><p>Before you go further, you’ll want to choose between <a target="" rel="noreferrer" href="https://github.com/mitanshu7/tailscale_kual">Mitanshu’s “standard” Tailscale repository</a>, or the fork of it that <a target="" rel="noreferrer" href="https://github.com/jonaolden/tailscale_kual">enables Taildrop</a>. I recommend the Taildrop-enabled fork; if it goes wrong, or stops being updated, it’s fairly easy (relative to doing this kind of project) to wipe it and go back to Mitanshu’s “vanilla” version.Either way, you’ll want to get USB access to your Kindle for this next part. If you toggled on USBNetworking to try it out, toggle it off; you can’t get USB access while it’s running, as its name somewhat implies.</p><ol><li>Download the Tailscale/KUAL repository of your choice using git clone or download a ZIP from the <strong>Code </strong>button on GitHub</li><li>Head to Tailscale’s page of static Linux binaries and grab the latest arm (not arm64) release</li><li>Copy the tailscale and tailscaled binaries from the Tailscale download and place them into the /extensions/tailscale/bin directory of the KUAL/Kindle repository you’ll be copying over</li><li>Head to your Tailscale admin console and <a target="" rel="noreferrer" href="https://login.tailscale.com/admin/settings/keys">generate an authentication key</a>. Name it something like kindle; you’ll want to enable the “Reusable” and “Pre-approved” options. Copy the key that is generated.</li><li>Open the file extensions/tailscale/config/auth_key.txt for editing while it is on your (non-Kindle) computer. Paste in the key text you generated.</li><li>If you’re using the variant with Taildrop, you can set a custom directory in which to deliver Taildrop files by editing extensions/tailscale/config/taildrop_dir.txt; setting /mnt/us/documents makes sense if you’re mostly sending yourself things to read in KOReader.</li><li>Head into the extensions folder on your computer and copy the tailscale folder you’ve set up into the extensions folder on your Kindle.</li></ol><p>With all that done, open up KUAL on your Kindle. Go into USBNetLite and click <strong>USBNetwork Status </strong>to ensure it is enabled (tap the <strong>Toggle</strong> button if not). Go back (with the <strong>“/”</strong> button at the bottom), tap <strong>Tailscale</strong>, and first tap <strong>Start Tailscaled</strong> (note the “d” at the end). Wait about 10 seconds to give <a target="" rel="noreferrer" href="https://tailscale.com/kb/1278/tailscaled/?utm_source=blog&amp;utm_medium=content&amp;utm_campaign=jailbreak-kindle">the Tailscaled daemon</a> time to start, then tap <strong>Start Tailscale</strong>.</p><p>If everything is settled, you should be able to see your Kindle as connected on your Tailscale admin console. Once you’ve finished smiling to yourself, click the three dots on the right-hand side of the Kindle row and select “Disable key expiry.” In most situations, you’re better off not having to patch a new key value into a Kindle text file every few months.</p><figure id=""><img _type="asset" video="[object Object]" alt="Screenshot of a sharing intent on an iPhone, titled &quot;Send via Tailscale,&quot; with a My Devices list showing &quot;Kindle&quot; and &quot;pi3b&quot; (as linux devices) with green dots, signifying availability." loading="lazy" width="1179" height="1179" decoding="async" data-nimg="1" srcset="https://cdn.sanity.io/images/w77i7m8x/production/a978381cca9548c99b5a7eedf9ec51d00fe6c8af-1179x1179.png?w=1200&amp;q=75&amp;fit=clip&amp;auto=format 1x, https://cdn.sanity.io/images/w77i7m8x/production/a978381cca9548c99b5a7eedf9ec51d00fe6c8af-1179x1179.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format 2x" src="https://cdn.sanity.io/images/w77i7m8x/production/a978381cca9548c99b5a7eedf9ec51d00fe6c8af-1179x1179.png?w=3840&amp;q=75&amp;fit=clip&amp;auto=format"><figcaption>Turn on your Kindle and send it books from any device.</figcaption></figure><h2 id="enjoy-your-slightly-less-wonky-kindle"><a href="#enjoy-your-slightly-less-wonky-kindle">Enjoy your (slightly) less wonky Kindle<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2><p>With Tailscale installed, it’s easier to get into your Kindle via SSH for file management and installing and configuring other apps. Getting a Bluetooth keyboard to work via the Kindle’s quirky command-line Bluetooth interface would not have been fun using a touchscreen keyboard.</p><p>Because the Kindle is on your tailnet, it can access anything else you have hosted there. Kindles set up this way can use tools like the <a target="" rel="noreferrer" href="https://github.com/mitchellurgero/kindle-shortcut-browser">Shortcut Browser</a> to become dashboards for <a target="" rel="noreferrer" href="https://tailscale.com/blog/remotely-access-home-assistant">Home Assistant</a>, or access a self-hosted <a target="" rel="noreferrer" href="https://github.com/janeczku/calibre-web">Calibre-Web</a> e-book server (with <a target="" rel="noreferrer" href="https://github.com/mitanshu7/tailscale_kual/issues/2#issuecomment-2710486540">some tweaking</a>).</p><p>Having Taildrop handy, and having it drop files directly into the documents folder, is probably my favorite upgrade. I was on my phone, at a train station, when I came across Annalee Newitz’s <a target="" rel="noreferrer" href="https://bookshop.org/p/books/automatic-noodle-annalee-newitz/625018d0518991aa"><em>Automatic Noodle</em> at </a><a target="" rel="noreferrer" href="http://bookshop.org/">Bookshop.org</a>. I bought it on my phone and downloaded the DRM-free epub file. When I got home, I opened and unlocked my Kindle, sent the epub to the Kindle via Taildrop, then tapped <strong>Receive Taildrop Files</strong> in the Tailscale app inside KUAL. Epubs, PDFs, comic book archives, DjVu files—they’re all ready to be dropped in.</p><p>If you’ve gotten Tailscale to run on weird (or just uncommon) devices, we’d more than love to hear about it. Let us know on <a target="" rel="noreferrer" href="https://www.reddit.com/r/Tailscale/">Reddit</a>, <a target="" rel="noreferrer" href="https://discord.com/invite/tailscale">Discord</a>, <a target="" rel="noreferrer" href="https://bsky.app/profile/tailscale.com">Bluesky</a>, <a target="" rel="noreferrer" href="https://hachyderm.io/@tailscale">Mastodon</a>, or <a target="" rel="noreferrer" href="https://www.linkedin.com/company/tailscale/product/">LinkedIn</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google confirms Android attacks; no fix for most Samsung users (183 pts)]]></title>
            <link>https://www.forbes.com/sites/zakdoffman/2025/12/08/google-confirms-android-attacks-no-fix-for-most-samsung-users/</link>
            <guid>46194315</guid>
            <pubDate>Mon, 08 Dec 2025 16:32:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.forbes.com/sites/zakdoffman/2025/12/08/google-confirms-android-attacks-no-fix-for-most-samsung-users/">https://www.forbes.com/sites/zakdoffman/2025/12/08/google-confirms-android-attacks-no-fix-for-most-samsung-users/</a>, See on <a href="https://news.ycombinator.com/item?id=46194315">Hacker News</a></p>
Couldn't get https://www.forbes.com/sites/zakdoffman/2025/12/08/google-confirms-android-attacks-no-fix-for-most-samsung-users/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[No more O'Reilly subscriptions for me (144 pts)]]></title>
            <link>https://zerokspot.com/weblog/2025/12/05/no-more-oreilly-subscriptions-for-me/</link>
            <guid>46194063</guid>
            <pubDate>Mon, 08 Dec 2025 16:14:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zerokspot.com/weblog/2025/12/05/no-more-oreilly-subscriptions-for-me/">https://zerokspot.com/weblog/2025/12/05/no-more-oreilly-subscriptions-for-me/</a>, See on <a href="https://news.ycombinator.com/item?id=46194063">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                        <p>For the last two years I’ve had an <a href="https://www.oreilly.com/online-learning/pricing.html">O’Reilly subscription</a>. Their offer is quite attractive with unlimited access to books not only by O’Reilly but also Manning and others. The catalog is just enourmous and covers pretty much every technical book around software engineering et al. that I might ever want to read. There are also tons of other learning resources in there like conference recordings and webinars, but I’m mostly there for the books. Unfortunately, I cannot read technical books fast and definitely not fast enough to make the subscription be worth $500 per year.</p>
<p>Another problem for me is the usability of the mobile client. I mostly read books on my tablet but also like to use some spare time during commutes to make some progress on my phone. The synchronization there is extremely spotty and the app, when being evicted and reloaded by the operating system, throws me more often than not back to the start screen instead of reopening the previously open book at the right page. I also haven’t found a theme that I enjoy as much as the ones offered by Apple Books or the Kindle app and so reading hasn’t been all that enjoyable for me.</p>
<p>All of this together will most likely not make me renew my subscription for the new year. Given the price, it will be probably cheaper for me to buy only the books that I want from Kobo et al. where I can get O’Reilly books without DRM and keep them beyond any subscription limit. I also just noticed that I still have some credits left from the time I’ve had a <a href="https://zerokspot.com/weblog/2023/05/22/manning-also-has-a-subscriptions/">Manning subscription</a> 😂</p>

                        
                        

                        
                    </div><div>
                            <h2>This post was inspired by...</h2>
                            
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD GPU Debugger (234 pts)]]></title>
            <link>https://thegeeko.me/blog/amd-gpu-debugging/</link>
            <guid>46193931</guid>
            <pubDate>Mon, 08 Dec 2025 16:06:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thegeeko.me/blog/amd-gpu-debugging/">https://thegeeko.me/blog/amd-gpu-debugging/</a>, See on <a href="https://news.ycombinator.com/item?id=46193931">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-gjtny2mx="">    <a href="https://thegeeko.me/" data-astro-cid-cjjlykpo=""> <svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg" data-astro-cid-cjjlykpo=""> <path d="M2.5 6.5H9.5C11.1569 6.5 12.5 7.84315 12.5 9.5V9.5C12.5 11.1569 11.1569 12.5 9.5 12.5H7.5M2.5 6.5L5.5 9.5M2.5 6.5L5.5 3.5" stroke="currentColor" stroke-width="1.25" stroke-linecap="round" stroke-linejoin="round" data-astro-cid-cjjlykpo=""></path> </svg>
index
</a>   <div id="toc"> <nav> <ul id="toc-list"> <!-- Back to top link --> <li> <a href="#" title="Back to top" data-text="Back to top">
Back to top
</a> </li> <!-- TOC items --> <li> <a href="#tbatma" title="TBA/TMA" data-text="TBA/TMA"> TBA/TMA </a> </li><li> <a href="#amdgpu-debugfs" title="AMDGPU Debugfs" data-text="AMDGPU Debugfs"> AMDGPU Debugfs </a> </li><li> <a href="#the-trap-handler" title="The Trap Handler" data-text="The Trap Handler"> The Trap Handler </a> </li><li> <a href="#spir-v" title="SPIR-V" data-text="SPIR-V"> SPIR-V </a> </li><li> <a href="#an-actual-debugger" title="An Actual Debugger" data-text="An Actual Debugger"> An Actual Debugger </a> </li><li> <a href="#breakpoints-and-stepping" title="Breakpoints and Stepping" data-text="Breakpoints and Stepping"> Breakpoints and Stepping </a> </li><li> <a href="#source-code-line-mapping" title="Source Code Line Mapping" data-text="Source Code Line Mapping"> Source Code Line Mapping </a> </li><li> <a href="#address-watching-aka-watchpoints" title="Address Watching aka Watchpoints" data-text="Address Watching aka Watchpoints"> Address Watching aka Watchpoints </a> </li><li> <a href="#variables-types-and-names" title="Variables Types and Names" data-text="Variables Types and Names"> Variables Types and Names </a> </li><li> <a href="#vulkan-integration" title="Vulkan Integration" data-text="Vulkan Integration"> Vulkan Integration </a> </li><li> <a href="#bonus-round" title="Bonus Round" data-text="Bonus Round"> Bonus Round </a> </li> </ul> </nav> </div>     <p>I’ve always wondered why we don’t have a GPU debugger similar to the one used for CPUs. A tool that allows pausing execution and examining the current state. This capability feels essential, especially since the GPU’s concurrent execution model is much harder to reason about. After searching for solutions, I came across rocgdb, a debugger for AMD’s ROCm environment. Unfortunately, its scope is limited to that environment. Still, this shows it’s technically possible. I then found a helpful <a href="https://martty.github.io/posts/radbg_part_1/">series of blog posts</a> by <a href="https://martty.github.io/about/">Marcell Kiss</a>, detailing how he achieved this, which inspired me to try to recreate the process myself.</p>
<h2 id="lets-try-to-talk-to-the-gpu-directly">Let’s Try To Talk To The GPU Directly</h2>
<p>The best place to start learning about this is <a href="https://docs.mesa3d.org/drivers/radv.html">RADV</a>. By tracing what it does, we can find how to do it. Our goal here is to run the most basic shader <code>nop 0</code> without using Vulkan, aka RADV in our case.</p>
<p>First of all, we need to open the DRM file to establish a connection with the KMD, using a simple open(“/dev/dri/cardX”), then we find that it’s calling <code>amdgpu_device_initialize</code>, which is a function defined in <code>libdrm</code>, which is a library that acts as middleware between user mode drivers(UMD) like <code>RADV</code> and and kernel mode drivers(KMD) like amdgpu driver, and then when we try to do some actual work we have to create a context which can be achieved by calling <code>amdgpu_cs_ctx_create</code> from <code>libdrm</code> again, next up we need to allocate 2 buffers one of them for our code and the other for writing our commands into, we do this by calling a couple of functions, here’s how I do it:</p>
<pre tabindex="0" data-language="c"><code><span><span>void</span><span> bo_alloc</span><span>(</span><span>amdgpu_t</span><span>*</span><span> dev</span><span>,</span><span> size_t</span><span> size</span><span>,</span><span> u32 domain</span><span>,</span><span> bool</span><span> uncached</span><span>,</span><span> amdgpubo_t</span><span>*</span><span> bo) {</span></span>
<span><span> s32    ret         </span><span>=</span><span> -</span><span>1</span><span>;</span></span>
<span><span> u32    alignment   </span><span>=</span><span> 0</span><span>;</span></span>
<span><span> u32    flags       </span><span>=</span><span> 0</span><span>;</span></span>
<span><span> size_t</span><span> actual_size </span><span>=</span><span> 0</span><span>;</span></span>
<span></span>
<span><span> amdgpu_bo_handle bo_handle </span><span>=</span><span> NULL</span><span>;</span></span>
<span><span> amdgpu_va_handle va_handle </span><span>=</span><span> NULL</span><span>;</span></span>
<span><span> u64              va_addr   </span><span>=</span><span> 0</span><span>;</span></span>
<span><span> void*</span><span>            host_addr </span><span>=</span><span> NULL</span><span>;</span></span></code></pre>
<p>Here we’re choosing the domain and assigning flags based on the params, some buffers we will need uncached, as we will see:</p>
<pre tabindex="0" data-language="c"><code><span><span> if</span><span> (</span></span>
<span><span>   domain </span><span>!=</span><span> AMDGPU_GEM_DOMAIN_GWS </span><span>&amp;&amp;</span><span> domain </span><span>!=</span><span> AMDGPU_GEM_DOMAIN_GDS </span><span>&amp;&amp;</span></span>
<span><span>   domain </span><span>!=</span><span> AMDGPU_GEM_DOMAIN_OA) {</span></span>
<span><span>  actual_size </span><span>=</span><span> (size </span><span>+</span><span> 4096</span><span> -</span><span> 1</span><span>) </span><span>&amp;</span><span> 0x</span><span>FFFFFFFFFFFFF000</span><span>ULL</span><span>;</span></span>
<span><span>  alignment   </span><span>=</span><span> 4096</span><span>;</span></span>
<span><span>  flags       </span><span>=</span><span> AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED </span><span>|</span><span> AMDGPU_GEM_CREATE_VRAM_CLEARED </span><span>|</span></span>
<span><span>          AMDGPU_GEM_CREATE_VM_ALWAYS_VALID;</span></span>
<span><span>  flags </span><span>|=</span></span>
<span><span>    uncached </span><span>?</span><span> (domain </span><span>==</span><span> AMDGPU_GEM_DOMAIN_GTT) </span><span>*</span><span> AMDGPU_GEM_CREATE_CPU_GTT_USWC </span><span>:</span><span> 0</span><span>;</span></span>
<span><span> } </span><span>else</span><span> {</span></span>
<span><span>  actual_size </span><span>=</span><span> size;</span></span>
<span><span>  alignment   </span><span>=</span><span> 1</span><span>;</span></span>
<span><span>  flags       </span><span>=</span><span> AMDGPU_GEM_CREATE_NO_CPU_ACCESS;</span></span>
<span><span> }</span></span>
<span></span>
<span><span> struct</span><span> amdgpu_bo_alloc_request req </span><span>=</span><span> {</span></span>
<span><span>  .alloc_size     </span><span>=</span><span> actual_size</span><span>,</span></span>
<span><span>  .phys_alignment </span><span>=</span><span> alignment</span><span>,</span></span>
<span><span>  .preferred_heap </span><span>=</span><span> domain</span><span>,</span></span>
<span><span>  .flags          </span><span>=</span><span> flags</span><span>,</span></span>
<span><span> };</span></span>
<span></span>
<span><span> // memory aquired!!</span></span>
<span><span> ret </span><span>=</span><span> amdgpu_bo_alloc</span><span>(dev</span><span>-&gt;</span><span>dev_handle</span><span>,</span><span> &amp;</span><span>req</span><span>,</span><span> &amp;</span><span>bo_handle);</span></span>
<span><span> HDB_ASSERT</span><span>(</span><span>!</span><span>ret</span><span>,</span><span> "can't allocate bo"</span><span>);</span></span></code></pre>
<p>Now we have the memory, we need to map it. I opt to map anything that can be CPU-mapped for ease of use. We have to map the memory to both the GPU and the CPU virtual space. The KMD creates the page table when we open the DRM file, as shown <a href="https://elixir.bootlin.com/linux/v6.18/source/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c#L1425">here</a>.</p>
<p>So map it to the GPU VM and, if possible, to the CPU VM as well. Here, at this point, there’s a libdrm function that does all of this setup for us and maps the memory, but I found that even when specifying <code>AMDGPU_VM_MTYPE_UC</code>, it doesn’t always tag the page as uncached, not quite sure if it’s a
bug in my code or something in <code>libdrm</code> anyways, the function is <code>amdgpu_bo_va_op</code>, I opted to do it manually here and issue the IOCTL call myself:</p>
<pre tabindex="0" data-language="c"><code><span><span> u32 kms_handle </span><span>=</span><span> 0</span><span>;</span></span>
<span><span> amdgpu_bo_export</span><span>(bo_handle</span><span>,</span><span> amdgpu_bo_handle_type_kms</span><span>,</span><span> &amp;</span><span>kms_handle);</span></span>
<span></span>
<span><span> ret </span><span>=</span><span> amdgpu_va_range_alloc</span><span>(</span></span>
<span><span>   dev</span><span>-&gt;</span><span>dev_handle</span><span>,</span></span>
<span><span>   amdgpu_gpu_va_range_general</span><span>,</span></span>
<span><span>   actual_size</span><span>,</span></span>
<span><span>   4096</span><span>,</span></span>
<span><span>   0</span><span>,</span></span>
<span><span>   &amp;</span><span>va_addr</span><span>,</span></span>
<span><span>   &amp;</span><span>va_handle</span><span>,</span></span>
<span><span>   0</span><span>);</span></span>
<span><span> HDB_ASSERT</span><span>(</span><span>!</span><span>ret</span><span>,</span><span> "can't allocate VA"</span><span>);</span></span>
<span></span>
<span><span> u64 map_flags </span><span>=</span></span>
<span><span>   AMDGPU_VM_PAGE_EXECUTABLE </span><span>|</span><span> AMDGPU_VM_PAGE_READABLE </span><span>|</span><span> AMDGPU_VM_PAGE_WRITEABLE;</span></span>
<span><span> map_flags </span><span>|=</span><span> uncached </span><span>?</span><span> AMDGPU_VM_MTYPE_UC </span><span>|</span><span> AMDGPU_VM_PAGE_NOALLOC </span><span>:</span><span> 0</span><span>;</span></span>
<span></span>
<span><span> struct</span><span> drm_amdgpu_gem_va va </span><span>=</span><span> {</span></span>
<span><span>  .handle       </span><span>=</span><span> kms_handle</span><span>,</span></span>
<span><span>  .operation    </span><span>=</span><span> AMDGPU_VA_OP_MAP</span><span>,</span></span>
<span><span>  .flags        </span><span>=</span><span> map_flags</span><span>,</span></span>
<span><span>  .va_address   </span><span>=</span><span> va_addr</span><span>,</span></span>
<span><span>  .offset_in_bo </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>  .map_size     </span><span>=</span><span> actual_size</span><span>,</span></span>
<span></span>
<span><span> };</span></span>
<span></span>
<span><span> ret </span><span>=</span><span> drm_ioctl_write_read</span><span>(dev</span><span>-&gt;</span><span>drm_fd</span><span>,</span><span> DRM_AMDGPU_GEM_VA</span><span>,</span><span> &amp;</span><span>va</span><span>,</span><span> sizeof</span><span>(va));</span></span>
<span><span> HDB_ASSERT</span><span>(</span><span>!</span><span>ret</span><span>,</span><span> "can't map bo in GPU space"</span><span>);</span></span>
<span><span> // ret = amdgpu_bo_va_op(bo_handle, 0, actual_size, va_addr, map_flags,</span></span>
<span><span> // AMDGPU_VA_OP_MAP);</span></span>
<span></span>
<span><span> if</span><span> (flags </span><span>&amp;</span><span> AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {</span></span>
<span><span>  ret </span><span>=</span><span> amdgpu_bo_cpu_map(bo_handle</span><span>,</span><span> &amp;</span><span>host_addr)</span><span>;</span></span>
<span><span>  HDB_ASSERT(</span><span>!</span><span>ret</span><span>,</span><span> "can't map bo in CPU space"</span><span>)</span><span>;</span></span>
<span></span>
<span><span>  // AMDGPU_GEM_CREATE_VRAM_CLEARED doesn't really memset the memory to 0 anyways for</span></span>
<span><span>  // debug I'll just do it manually for now</span></span>
<span><span>  memset(host_addr</span><span>,</span><span> 0x</span><span>0</span><span>,</span><span> actual_size)</span><span>;</span></span>
<span><span> }</span></span>
<span></span>
<span><span> *</span><span>bo </span><span>=</span><span> (</span><span>amdgpubo_t</span><span>){</span></span>
<span><span>  .bo_handle </span><span>=</span><span> bo_handle</span><span>,</span></span>
<span><span>  .va_handle </span><span>=</span><span> va_handle</span><span>,</span></span>
<span><span>  .va_addr   </span><span>=</span><span> va_addr</span><span>,</span></span>
<span><span>  .size      </span><span>=</span><span> actual_size</span><span>,</span></span>
<span><span>  .host_addr </span><span>=</span><span> host_addr</span><span>,</span></span>
<span><span> };</span></span>
<span><span>}</span></span></code></pre>
<p>Now we have the context and 2 buffers. Next, fill those buffers and send our commands to the KMD, which will then forward them to the Command Processor (CP) in the GPU for processing.</p>
<p>Let’s compile our code. We can use clang assembler for that, like this:</p>
<pre tabindex="0" data-language="c"><code><span><span># https:</span><span>//gitlab.freedesktop.org/martty/radbg-poc/-/blob/master/ll-as.sh</span></span>
<span><span>clang </span><span>-</span><span>c </span><span>-</span><span>x assembler </span><span>-</span><span>target amdgcn</span><span>-</span><span>amd</span><span>-</span><span>amdhsa </span><span>-</span><span>mcpu</span><span>=</span><span>gfx1100 </span><span>-</span><span>o </span><span>asm</span><span>.o </span><span>"$1"</span></span>
<span><span>objdump </span><span>-</span><span>h </span><span>asm</span><span>.o </span><span>|</span><span> grep .text </span><span>|</span><span> awk </span><span>'{print "dd if='</span><span>asm</span><span>.o</span><span>' of='</span><span>asmc.bin</span><span>' bs=1 count=$[0x" $3 "] skip=$[0x" $6 "] status=none"}'</span><span> |</span><span> bash</span></span>
<span><span>#rm </span><span>asm</span><span>.o</span></span></code></pre>
<p>The bash script compiles the code, and then we’re only interested in the actual machine code, so we use objdump to figure out the offset and the size of the section and copy it to a new file called asmc.bin, then we can just load the file and write its bytes to the CPU-mapped address of the code buffer.</p>
<p>Next up, filling in the commands. This was extremely confusing for me because it’s not well documented.
It was mostly learning how <code>RADV</code> does things and trying to do similar things. Also, shout-out to the folks on the Graphics Programming Discord server for helping me, especially Picoduck. The commands are encoded in a special format called <code>PM4 Packets</code>, which has multiple types. We only care about <code>Type 3</code>: each packet has an opcode and the number of bytes it contains.</p>
<p>The first thing we need to do is program the GPU registers, then dispatch the shader. Some of those registers are <code>rsrc[1-3]</code>; those registers are responsible for a number of configurations, pgm_[lo/hi], which hold the pointer to the code buffer and <code>num_thread_[x/y/z]</code>; those are responsible for the number of threads inside a work group. All of those are set using the <code>set shader register</code> packets, and here is how to encode them:</p>
<p><span>It’s worth mentioning that we can set multiple registers in 1 packet if they’re consecutive.</span></p>
<pre tabindex="0" data-language="c"><code><span><span>void</span><span> pkt3_set_sh_reg</span><span>(</span><span>pkt3_packets_t</span><span>*</span><span> packets</span><span>,</span><span> u32 reg</span><span>,</span><span> u32 value) {</span></span>
<span><span> HDB_ASSERT(</span></span>
<span><span>   reg </span><span>&gt;=</span><span> SI_SH_REG_OFFSET </span><span>&amp;&amp;</span><span> reg </span><span>&lt;</span><span> SI_SH_REG_END</span><span>,</span></span>
<span><span>   "can't set register outside sh registers span"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> // packet header</span></span>
<span><span> da_append(packets</span><span>,</span><span> PKT3(PKT3_SET_SH_REG</span><span>,</span><span> 1</span><span>,</span><span> 0</span><span>))</span><span>;</span></span>
<span><span> // offset of the register</span></span>
<span><span> da_append(packets</span><span>,</span><span> (reg </span><span>-</span><span> SI_SH_REG_OFFSET) </span><span>/</span><span> 4</span><span>)</span><span>;</span></span>
<span><span> da_append(packets</span><span>,</span><span> value)</span><span>;</span></span>
<span><span>}</span></span></code></pre>
<p>Then we append the dispatch command:</p>
<pre tabindex="0" data-language="c"><code><span><span>// we're going for 1 thread since we want the simplest case here.</span></span>
<span></span>
<span><span>da_append</span><span>(</span><span>&amp;</span><span>pkt3_packets</span><span>,</span><span> PKT3</span><span>(PKT3_DISPATCH_DIRECT</span><span>,</span><span> 3</span><span>,</span><span> 0</span><span>) </span><span>|</span><span> PKT3_SHADER_TYPE_S</span><span>(</span><span>1</span><span>));</span></span>
<span><span>da_append</span><span>(</span><span>&amp;</span><span>pkt3_packets</span><span>,</span><span> 1</span><span>u</span><span>);</span></span>
<span><span>da_append</span><span>(</span><span>&amp;</span><span>pkt3_packets</span><span>,</span><span> 1</span><span>u</span><span>);</span></span>
<span><span>da_append</span><span>(</span><span>&amp;</span><span>pkt3_packets</span><span>,</span><span> 1</span><span>u</span><span>);</span></span>
<span><span>da_append</span><span>(</span><span>&amp;</span><span>pkt3_packets</span><span>,</span><span> dispatch_initiator);</span></span></code></pre>
<p>Now we want to write those commands into our buffer and send them to the KMD:</p>
<pre tabindex="0" data-language="c"><code><span><span>void</span><span> dev_submit</span><span>(</span></span>
<span><span>  amdgpu_t</span><span>*</span><span>         dev</span><span>,</span></span>
<span><span>  pkt3_packets_t</span><span>*</span><span>   packets</span><span>,</span></span>
<span><span>  amdgpu_bo_handle</span><span>*</span><span> buffers</span><span>,</span></span>
<span><span>  u32               buffers_count</span><span>,</span></span>
<span><span>  amdgpu_submit_t</span><span>*</span><span>  submit</span></span>
<span><span>) {</span></span>
<span><span> s32        ret </span><span>=</span><span> -</span><span>1</span><span>;</span></span>
<span><span> amdgpubo_t</span><span> ib  </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span></span>
<span><span> bo_alloc(dev</span><span>,</span><span> pkt3_size(packets)</span><span>,</span><span> AMDGPU_GEM_DOMAIN_GTT</span><span>,</span><span> false</span><span>,</span><span> &amp;</span><span>ib)</span><span>;</span></span>
<span><span> bo_upload(</span><span>&amp;</span><span>ib</span><span>,</span><span> packets</span><span>-&gt;</span><span>data</span><span>,</span><span> pkt3_size(packets))</span><span>;</span></span>
<span></span>
<span><span> amdgpu_bo_handle</span><span>*</span><span> bo_handles </span><span>=</span><span> // +1 for the indirect buffer</span></span>
<span><span>   (amdgpu_bo_handle</span><span>*</span><span>)</span><span>malloc(</span><span>sizeof</span><span>(amdgpu_bo_handle) </span><span>*</span><span> (buffers_count </span><span>+</span><span> 1</span><span>))</span><span>;</span></span>
<span></span>
<span><span> bo_handles[</span><span>0</span><span>] </span><span>=</span><span> ib</span><span>.</span><span>bo_handle;</span></span>
<span><span> for_range(i</span><span>,</span><span> 0</span><span>,</span><span> buffers_count)</span><span> {</span></span>
<span><span>  bo_handles[i </span><span>+</span><span> 1</span><span>] </span><span>=</span><span> buffers[i];</span></span>
<span><span> }</span></span>
<span></span>
<span><span> amdgpu_bo_list_handle bo_list </span><span>=</span><span> NULL</span><span>;</span></span>
<span><span> ret </span><span>=</span></span>
<span><span>   amdgpu_bo_list_create(</span><span>dev</span><span>-&gt;</span><span>dev_handle</span><span>,</span><span> buffers_count </span><span>+</span><span> 1</span><span>,</span><span> bo_handles</span><span>,</span><span> NULL</span><span>,</span><span> &amp;</span><span>bo_list)</span><span>;</span></span>
<span><span> HDB_ASSERT(</span><span>!</span><span>ret</span><span>,</span><span> "can't create a bo list"</span><span>)</span><span>;</span></span>
<span><span> free(bo_handles)</span><span>;</span></span>
<span></span>
<span><span> struct</span><span> amdgpu_cs_ib_info ib_info </span><span>=</span><span> {</span></span>
<span><span>  .flags         </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>  .ib_mc_address </span><span>=</span><span> ib</span><span>.</span><span>va_addr</span><span>,</span></span>
<span><span>  .size          </span><span>=</span><span> packets</span><span>-&gt;</span><span>count</span><span>,</span></span>
<span><span> };</span></span>
<span></span>
<span><span> struct</span><span> amdgpu_cs_request req </span><span>=</span><span> {</span></span>
<span><span>  .flags                  </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>  .ip_type                </span><span>=</span><span> AMDGPU_HW_IP_COMPUTE</span><span>,</span></span>
<span><span>  .ip_instance            </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>  .ring                   </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>  .resources              </span><span>=</span><span> bo_list</span><span>,</span></span>
<span><span>  .number_of_dependencies </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>  .dependencies           </span><span>=</span><span> NULL</span><span>,</span></span>
<span><span>  .number_of_ibs          </span><span>=</span><span> 1</span><span>,</span></span>
<span><span>  .ibs                    </span><span>=</span><span> &amp;</span><span>ib_info</span><span>,</span></span>
<span><span>  .seq_no                 </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>  .fence_info             </span><span>=</span><span> { </span><span>0</span><span> }</span><span>,</span></span>
<span><span> };</span></span>
<span></span>
<span><span> ret </span><span>=</span><span> amdgpu_cs_submit(</span><span>dev</span><span>-&gt;</span><span>ctx_handle</span><span>,</span><span> 0</span><span>,</span><span> &amp;</span><span>req</span><span>,</span><span> 1</span><span>)</span><span>;</span></span>
<span><span> HDB_ASSERT(</span><span>!</span><span>ret</span><span>,</span><span> "can't submit indirect buffer request"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> *</span><span>submit </span><span>=</span><span> (</span><span>amdgpu_submit_t</span><span>){</span></span>
<span><span>    .ib </span><span>=</span><span> ib</span><span>,</span></span>
<span><span>    .bo_list </span><span>=</span><span> bo_list</span><span>,</span></span>
<span><span>    .fence </span><span>=</span><span> {</span></span>
<span><span>      .context </span><span>=</span><span> dev</span><span>-&gt;</span><span>ctx_handle</span><span>,</span></span>
<span><span>      .ip_type </span><span>=</span><span> AMDGPU_HW_IP_COMPUTE</span><span>,</span></span>
<span><span>      .ip_instance </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>      .ring </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>      .fence </span><span>=</span><span> req</span><span>.</span><span>seq_no</span><span>,</span></span>
<span><span>    }</span><span>,</span></span>
<span><span>  };</span></span>
<span><span>}</span></span></code></pre>
<p><span>Here is a good point to make a more complex shader that outputs something. For example, writing 1 to a buffer.</span></p>
<p>No GPU hangs ?! nothing happened ?! cool, cool, now we have a shader that runs on the GPU, what’s next? Let’s try to hang the GPU by pausing the execution, aka make the GPU trap.</p>
<h2 id="tbatma">TBA/TMA</h2>
<p>The RDNA3’s ISA manual does mention 2 registers, <code>TBA, TMA</code>; here’s how they describe them respectively:</p>
<blockquote>
<p>Holds the pointer to the current trap handler program address. Per-VMID register. Bit [63] indicates if the trap
handler is present (1) or not (0) and is not considered part of the address
(bit[62] is replicated into address bit[63]).  Accessed via S_SENDMSG_RTN.</p>
</blockquote>
<blockquote>
<p>Temporary register for shader operations. For example, it can hold a pointer to memory used by the trap handler.</p>
</blockquote>
<p><span>You can configure the GPU to enter the trap handler when encountering certain exceptions listed in the RDNA3 ISA manual.</span></p>
<p>We know from <a href="https://martty.github.io/about/">Marcell Kiss’s</a> blog posts that we need to compile a trap handler, which is a normal shader the GPU switches to when encountering a <code>s_trap</code>. The TBA register has a special bit that indicates whether the trap handler is enabled.</p>
<p>Since these are privileged registers, we cannot write to them from user space. To bridge this gap for debugging, we can utilize the debugfs interface. Luckily, we have <a href="https://umr.readthedocs.io/en/main/intro.html">UMR</a>, which uses that debugfs interface, and it’s open source; we copy AMD’s homework here which is great.</p>
<h2 id="amdgpu-debugfs">AMDGPU Debugfs</h2>
<p>The amdgpu KMD has a couple of files in debugfs under <code>/sys/kernel/debug/dri/{PCI address}</code>; one of them is <code>regs2</code>, which is an interface to a <a href="https://elixir.bootlin.com/linux/v6.18/source/drivers/gpu/drm/amd/amdgpu/amdgpu_debugfs.c#L369"><code>amdgpu_debugfs_regs2_write</code></a> in the kernel that writes to the registers. It works by simply opening the file, seeking the register’s offset, and then writing; it also performs some synchronisation and writes the value correctly. We need to provide more parameters about the register before writing to the file, tho and do that by using an ioctl call. Here are the ioctl arguments:</p>
<pre tabindex="0" data-language="c"><code><span><span>typedef</span><span> struct</span><span> amdgpu_debugfs_regs2_iocdata_v2 {</span></span>
<span><span> __u32 use_srbm</span><span>,</span><span> use_grbm</span><span>,</span><span> pg_lock;</span></span>
<span><span> struct</span><span> {</span></span>
<span><span>  __u32 se</span><span>,</span><span> sh</span><span>,</span><span> instance;</span></span>
<span><span> } grbm;</span></span>
<span><span> struct</span><span> {</span></span>
<span><span>  __u32 me</span><span>,</span><span> pipe</span><span>,</span><span> queue</span><span>,</span><span> vmid;</span></span>
<span><span> } srbm;</span></span>
<span><span> __u32 xcc_id;</span></span>
<span><span>} </span><span>regs2_ioc_data_t</span><span>;</span></span></code></pre>
<p>The 2 structs are because there are 2 types of registers, GRBM and SRBM, each of which is banked by different constructs; you can learn more about some of them here in <a href="https://docs.kernel.org/gpu/amdgpu/driver-core.html#gfx-compute-and-sdma-overall-behaviour">the Linux kernel documentation</a>.</p>
<p>Turns out our registers here are SBRM registers and banked by VMIDs, meaning each VMID has its own TBA and TMA registers. Cool, now we need to figure out the VMID of our process. As far as I understand, VMIDs are a way for the GPU to identify a specific process context, including the page table base address, so the address translation unit can translate a virtual memory address. The context is created when we open the DRM file. They get assigned dynamically at dispatch time, which is a problem for us; we want to write to those registers before dispatch.</p>
<p>We can obtain the VMID of the dispatched process by querying the <code>HW_ID2</code> register with s_getreg_b32. I do a hack here, by enabling the trap handler in every VMID, and there are 16 of them, the first being special, and used by the KMD and the last 8 allocated to the amdkfd driver. We loop over the remaining VMIDs and write to those registers. This can cause issues to other processes using other VMIDs by enabling trap handlers in them and writing the virtual address of our trap handler, which is only valid within our virtual memory address space. It’s relatively safe tho since most other processes won’t cause a trap<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>.</p>
<p>Now we can write to TMA and TBA, here’s the code:</p>
<pre tabindex="0" data-language="c"><code><span><span>void</span><span> dev_op_reg32</span><span>(</span></span>
<span><span>  amdgpu_t</span><span>*</span><span> dev</span><span>,</span><span> gc_11_reg_t</span><span> reg</span><span>,</span><span> regs2_ioc_data_t</span><span> ioc_data</span><span>,</span><span> reg_32_op_t</span><span> op</span><span>,</span><span> u32</span><span>*</span><span> value) {</span></span>
<span><span> s32 ret </span><span>=</span><span> 0</span><span>;</span></span>
<span></span>
<span><span> reg_info_t</span><span> reg_info     </span><span>=</span><span> gc_11_regs_infos[reg];</span></span>
<span><span> uint64_t</span><span>   reg_offset   </span><span>=</span><span> gc_11_regs_offsets[reg];</span></span>
<span><span> uint64_t</span><span>   base_offset  </span><span>=</span><span> dev</span><span>-&gt;</span><span>gc_regs_base_addr[</span><span>reg_info</span><span>.</span><span>soc_index];</span></span>
<span><span> uint64_t</span><span>   total_offset </span><span>=</span><span> (reg_offset </span><span>+</span><span> base_offset);</span></span>
<span></span>
<span><span> // seems like we're multiplying by 4 here because the registers database in UMRs</span></span>
<span><span> // source has them in indexes rather than bytes.</span></span>
<span><span> total_offset </span><span>*=</span><span> (</span><span>reg_info</span><span>.</span><span>type </span><span>==</span><span> REG_MMIO) </span><span>?</span><span> 4</span><span> :</span><span> 1</span><span>;</span></span>
<span></span>
<span><span> ret </span><span>=</span><span> hdb_ioctl(</span><span>dev</span><span>-&gt;</span><span>regs2_fd</span><span>,</span><span> AMDGPU_DEBUGFS_REGS2_IOC_SET_STATE_V2</span><span>,</span><span> &amp;</span><span>ioc_data)</span><span>;</span></span>
<span><span> HDB_ASSERT(</span><span>!</span><span>ret</span><span>,</span><span> "Failed to set registers state"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> size_t</span><span> size </span><span>=</span><span> lseek(</span><span>dev</span><span>-&gt;</span><span>regs2_fd</span><span>,</span><span> total_offset</span><span>,</span><span> SEEK_SET)</span><span>;</span></span>
<span><span> HDB_ASSERT(size </span><span>==</span><span> total_offset</span><span>,</span><span> "Failed to seek register address"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> switch</span><span> (op) {</span></span>
<span><span> case</span><span> REG_OP_READ </span><span>:</span><span> size </span><span>=</span><span> read</span><span>(dev</span><span>-&gt;</span><span>regs2_fd</span><span>,</span><span> value</span><span>,</span><span> 4</span><span>); </span><span>break</span><span>;</span></span>
<span><span> case</span><span> REG_OP_WRITE</span><span>:</span><span> size </span><span>=</span><span> write</span><span>(dev</span><span>-&gt;</span><span>regs2_fd</span><span>,</span><span> value</span><span>,</span><span> 4</span><span>); </span><span>break</span><span>;</span></span>
<span><span> default</span><span>          :</span><span> HDB_ASSERT</span><span>(</span><span>false</span><span>,</span><span> "unsupported op"</span><span>);</span></span>
<span><span> }</span></span>
<span></span>
<span><span> HDB_ASSERT(size </span><span>==</span><span> 4</span><span>,</span><span> "Failed to write/read the values to/from the register"</span><span>)</span><span>;</span></span>
<span><span>}</span></span></code></pre>
<p>And here’s how we write to <code>TMA</code> and <code>TBA</code>:
<span>If you noticed, I’m using bitfields. I use them because working with them is much easier than macros, and while the byte order is not guaranteed by the C spec, it’s guaranteed by System V ABI, which Linux adheres to.</span></p>
<pre tabindex="0" data-language="c"><code><span><span>void</span><span> dev_setup_trap_handler</span><span>(</span><span>amdgpu_t</span><span>*</span><span> dev</span><span>,</span><span> u64 tba</span><span>,</span><span> u64 tma) {</span></span>
<span><span> reg_sq_shader_tma_lo_t</span><span> tma_lo </span><span>=</span><span> { .raw </span><span>=</span><span> (u32)(tma) };</span></span>
<span><span> reg_sq_shader_tma_hi_t</span><span> tma_hi </span><span>=</span><span> { .raw </span><span>=</span><span> (u32)(tma </span><span>&gt;&gt;</span><span> 32</span><span>) };</span></span>
<span></span>
<span><span> reg_sq_shader_tba_lo_t</span><span> tba_lo </span><span>=</span><span> { .raw </span><span>=</span><span> (u32)(tba </span><span>&gt;&gt;</span><span> 8</span><span>) };</span></span>
<span><span> reg_sq_shader_tba_hi_t</span><span> tba_hi </span><span>=</span><span> { .raw </span><span>=</span><span> (u32)(tba </span><span>&gt;&gt;</span><span> 40</span><span>) };</span></span>
<span></span>
<span><span> tba_hi</span><span>.</span><span>trap_en </span><span>=</span><span> 1</span><span>;</span></span>
<span></span>
<span><span> regs2_ioc_data_t</span><span> ioc_data </span><span>=</span><span> {</span></span>
<span><span>  .use_srbm </span><span>=</span><span> 1</span><span>,</span></span>
<span><span>  .xcc_id   </span><span>=</span><span> -</span><span>1</span><span>,</span></span>
<span><span> };</span></span>
<span></span>
<span><span> // NOTE(hadi):</span></span>
<span><span> // vmid's get assigned when code starts executing before hand we don't know which vmid</span></span>
<span><span> // will get assigned to our process so we just set all of them</span></span>
<span><span> for_range(i</span><span>,</span><span> 1</span><span>,</span><span> 9</span><span>)</span><span> {</span></span>
<span><span>  ioc_data</span><span>.</span><span>srbm</span><span>.</span><span>vmid </span><span>=</span><span> i;</span></span>
<span><span>  dev_op_reg32(dev</span><span>,</span><span> REG_SQ_SHADER_TBA_LO</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_WRITE</span><span>,</span><span> &amp;</span><span>tba_lo</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span>  dev_op_reg32(dev</span><span>,</span><span> REG_SQ_SHADER_TBA_HI</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_WRITE</span><span>,</span><span> &amp;</span><span>tba_hi</span><span>.</span><span>raw)</span><span>;</span></span>
<span></span>
<span><span>  dev_op_reg32(dev</span><span>,</span><span> REG_SQ_SHADER_TMA_LO</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_WRITE</span><span>,</span><span> &amp;</span><span>tma_lo</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span>  dev_op_reg32(dev</span><span>,</span><span> REG_SQ_SHADER_TMA_HI</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_WRITE</span><span>,</span><span> &amp;</span><span>tma_hi</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> }</span></span>
<span><span>}</span></span></code></pre>
<p>Anyway, now that we can write to those registers, if we enable the trap handler correctly, the GPU should hang when we launch our shader if we added <code>s_trap</code> instruction to it, or we enabled the <code>TRAP_ON_START</code> bit in rsrc3<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup> register.</p>
<p>Now, let’s try to write a trap handler.</p>
<h2 id="the-trap-handler">The Trap Handler</h2>
<p><span>If you wrote a different shader that outputs to a buffer, u can try writing to that shader from the trap handler, which is nice to make sure it’s actually being run.</span></p>
<p>We need 2 things: our trap handler and some scratch memory to use when needed, which we will store the address of in the TMA register.</p>
<p>The trap handler is just a normal program running in privileged state, meaning we have access to special registers like TTMP[0-15]. When we enter a trap handler, we need to first ensure that the state of the GPU registers is saved, just as the kernel does for CPU processes when context-switching, by saving a copy of the stable registers and the program counter, etc. The problem, tho, is that we don’t have a stable ABI for GPUs, or at least not one I’m aware of, and compilers use all the registers they can, so we need to save everything.</p>
<p>AMD GPUs’ Command Processors (CPs) have context-switching functionality, and the amdkfd driver does implement some <a href="https://elixir.bootlin.com/linux/v6.18/source/drivers/gpu/drm/amd/amdkfd/cwsr_trap_handler_gfx10.asm">context-switching shaders</a>. The problem is they’re not documented, and we have to figure them out from the amdkfd driver source and from other parts of the driver stack that interact with it, which is a pain in the ass. I kinda did a workaround here since I didn’t find luck understanding how it works, and some other reasons I’ll discuss later in the post.</p>
<p>The workaround here is to use only TTMP registers and a combination of specific instructions to copy the values of some registers, allowing us to use more instructions to copy the remaining registers. The main idea is to make use of the <code>global_store_addtid_b32</code> instruction, which adds the index of the current thread within the wave to the writing address, aka</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>I</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>h</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>d</mi></mrow></msub><mo>∗</mo><mn>4</mn><mo>+</mo><mi>a</mi><mi>d</mi><mi>d</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">ID_{thread} * 4 + address</annotation></semantics></math></span></span></span></p><p>This allows us to write a unique value per thread using only TTMP registers, which are unique per wave, not per thread<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup>, so we can save the context of a single wave.</p>
<p>The problem is that if we have more than 1 wave, they will overlap, and we will have a race condition.</p>
<p>Here is the code:</p>
<pre tabindex="0" data-language="asm"><code><span><span>start</span><span>:</span></span>
<span><span> ;; save the STATUS word into ttmp8</span></span>
<span><span> s_getreg_b32 ttmp8, hwreg(HW_REG_STATUS)</span></span>
<span></span>
<span><span> ;; save exec into ttmp[2:3]</span></span>
<span><span> s_mov_b64 ttmp[</span><span>2</span><span>:</span><span>3</span><span>], exec</span></span>
<span></span>
<span><span> ;; getting the address of our tma buffer</span></span>
<span><span> s_sendmsg_rtn_b64 ttmp[</span><span>4</span><span>:</span><span>5</span><span>], sendmsg(MSG_RTN_GET_TMA)</span></span>
<span><span> s_waitcnt lgkmcnt(</span><span>0</span><span>)</span></span>
<span></span>
<span><span> ;; save vcc</span></span>
<span><span> s_mov_b64 ttmp[</span><span>6</span><span>:</span><span>7</span><span>], vcc</span></span>
<span></span>
<span><span> ;; enable all threads so they can write their vgpr registers</span></span>
<span><span> s_mov_b64 exec, -</span><span>1</span></span>
<span></span>
<span><span> ;; FIXME(hadi): this assumes only 1 wave is running</span></span>
<span><span> global_store_addtid_b32 v0, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET        glc slc dlc</span></span>
<span><span> global_store_addtid_b32 v1, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>256</span><span>  glc slc dlc</span></span>
<span><span> global_store_addtid_b32 v2, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>512</span><span>  glc slc dlc</span></span>
<span><span> global_store_addtid_b32 v3, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>768</span><span>  glc slc dlc</span></span>
<span><span> global_store_addtid_b32 v4, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>1024</span><span> glc slc dlc</span></span>
<span><span> global_store_addtid_b32 v5, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>1280</span><span> glc slc dlc</span></span>
<span><span> global_store_addtid_b32 v6, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>1536</span><span> glc slc dlc</span></span>
<span><span> s_waitcnt vmcnt(</span><span>0</span><span>)</span></span>
<span></span>
<span><span> ;; only first thread is supposed to write sgprs of the wave</span></span>
<span><span> s_mov_b64 exec, </span><span>1</span></span>
<span><span> v_mov_b32 v1, s0</span></span>
<span><span> v_mov_b32 v2, s1</span></span>
<span><span> v_mov_b32 v3, s2</span></span>
<span><span> v_mov_b32 v4, s3</span></span>
<span><span> v_mov_b32 v5, s4</span></span>
<span><span> v_mov_b32 v0, </span><span>0</span></span>
<span><span> global_store_b32 v0, v1, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_SREG_OFFSET glc slc dlc</span></span>
<span><span> global_store_b32 v0, v2, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_SREG_OFFSET + </span><span>4</span><span> glc slc dlc</span></span>
<span><span> global_store_b32 v0, v3, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_SREG_OFFSET + </span><span>8</span><span> glc slc dlc</span></span>
<span><span> global_store_b32 v0, v4, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_SREG_OFFSET + </span><span>12</span><span> glc slc dlc</span></span>
<span><span> global_store_b32 v0, v5, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_SREG_OFFSET + </span><span>16</span><span> glc slc dlc</span></span>
<span><span> s_waitcnt vmcnt(</span><span>0</span><span>)</span></span>
<span></span>
<span><span> ;; enable all threads</span></span>
<span><span> s_mov_b64 exec, -</span><span>1</span></span></code></pre>
<p>Now that we have those values in memory, we need to tell the CPU: Hey, we got the data, and pause the GPU’s execution until the CPU issues a command. Also, notice we can just modify those from the CPU.</p>
<p>Before we tell the CPU, we need to write some values that might help the CPU. Here are they:</p>
<pre tabindex="0" data-language="asm"><code><span><span> ;; IDs to identify which parts of the hardware we are running on exactly</span></span>
<span><span> s_getreg_b32 ttmp10, hwreg(HW_REG_HW_ID1)</span></span>
<span><span> s_getreg_b32 ttmp11, hwreg(HW_REG_HW_ID2)</span></span>
<span><span> v_mov_b32 v3, ttmp10</span></span>
<span><span> v_mov_b32 v4, ttmp11</span></span>
<span><span> global_store_dwordx2 v1, v[</span><span>3</span><span>:</span><span>4</span><span>], ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_DATA_OFFSET glc slc dlc</span></span>
<span></span>
<span><span> ;; the original vcc mask</span></span>
<span><span> v_mov_b32 v3, ttmp6</span></span>
<span><span> v_mov_b32 v4, ttmp7</span></span>
<span><span> global_store_dwordx2 v1, v[</span><span>3</span><span>:</span><span>4</span><span>], ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>2048</span><span> glc slc dlc</span></span>
<span><span> s_waitcnt vmcnt(</span><span>0</span><span>)</span></span>
<span></span>
<span><span> ;; the original exec mask</span></span>
<span><span> v_mov_b32 v3, ttmp2</span></span>
<span><span> v_mov_b32 v4, ttmp3</span></span>
<span><span> global_store_dwordx2 v1, v[</span><span>3</span><span>:</span><span>4</span><span>], ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>2056</span><span> glc slc dlc</span></span>
<span><span> s_waitcnt vmcnt(</span><span>0</span><span>)</span></span>
<span></span>
<span><span> ;; the program counter</span></span>
<span><span> v_mov_b32 v3, ttmp0</span></span>
<span><span> v_mov_b32 v4, ttmp1</span></span>
<span><span> v_and_b32 v4, v4, </span><span>0xffff</span></span>
<span><span> global_store_dwordx2 v1, v[</span><span>3</span><span>:</span><span>4</span><span>], ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>16</span><span> glc slc dlc</span></span>
<span></span>
<span><span> s_waitcnt vmcnt(</span><span>0</span><span>)</span></span></code></pre>
<p>Now the GPU should just wait for the CPU, and here’s the spin code it’s implemented as described by Marcell Kiss <a href="https://martty.github.io/posts/radbg_part_4/#busier-waiting">here</a>:</p>
<pre tabindex="0" data-language="asm"><code><span><span>SPIN</span><span>:</span></span>
<span><span> global_load_dword v1, v2, ttmp[</span><span>4</span><span>:</span><span>5</span><span>] glc slc dlc</span></span>
<span></span>
<span><span>SPIN1</span><span>:</span></span>
<span><span> // I found the bit range of </span><span>10</span><span> to </span><span>15</span><span> using trial </span><span>and</span><span> error </span><span>in</span><span> the</span></span>
<span><span> // isa manual specifies that it's a </span><span>6</span><span>-bit number but the offset </span><span>10</span></span>
<span><span> // is just trial </span><span>and</span><span> error</span></span>
<span><span>  s_getreg_b32 ttmp13, hwreg(HW_REG_IB_STS, </span><span>10</span><span>, </span><span>15</span><span>)</span></span>
<span><span> s_and_b32 ttmp13, ttmp13, ttmp13</span></span>
<span><span> s_cbranch_scc1 SPIN1</span></span>
<span></span>
<span><span> v_readfirstlane_b32 ttmp13, v1</span></span>
<span><span> s_and_b32 ttmp13, ttmp13, ttmp13</span></span>
<span><span> s_cbranch_scc0 SPIN</span></span>
<span></span>
<span><span>CLEAR</span><span>:</span></span>
<span><span> v_mov_b32 v2, </span><span>0</span></span>
<span><span> v_mov_b32 v1, </span><span>0</span></span>
<span><span> global_store_dword v1, v2, ttmp[</span><span>4</span><span>:</span><span>5</span><span>] glc slc dlc</span></span>
<span><span> s_waitcnt vmcnt(</span><span>0</span><span>)</span></span></code></pre>
<p>The main loop in the CPU is like enable trap handler, then dispatch shader, then wait for the GPU to write some specific value in a specific address to signal all data is there, then examine and display, and tell the GPU all clear, go ahead.</p>
<p>Now that our uncached buffers are in play, we just keep looping and checking whether the GPU has written the register values. When it does, the first thing we do is halt the wave by writing into the <code>SQ_CMD</code> register to allow us to do whatever with the wave without causing any issues, tho if we halt for too long, the GPU CP will reset the command queue and kill the process, but we can change that behaviour by adjusting <a href="https://www.kernel.org/doc/html/v4.20/gpu/amdgpu.html#module-parameters">lockup_timeout</a> parameter of the amdgpu kernel module:</p>
<pre tabindex="0" data-language="c"><code><span><span>reg_sq_wave_hw_id1_t</span><span> hw1 </span><span>=</span><span> { .raw </span><span>=</span><span> tma[</span><span>2</span><span>] };</span></span>
<span><span>reg_sq_wave_hw_id2_t</span><span> hw2 </span><span>=</span><span> { .raw </span><span>=</span><span> tma[</span><span>3</span><span>] };</span></span>
<span></span>
<span><span>reg_sq_cmd_t</span><span> halt_cmd </span><span>=</span><span> {</span></span>
<span><span> .cmd  </span><span>=</span><span> 1</span><span>,</span></span>
<span><span> .mode </span><span>=</span><span> 1</span><span>,</span></span>
<span><span> .data </span><span>=</span><span> 1</span><span>,</span></span>
<span><span>};</span></span>
<span></span>
<span><span>regs2_ioc_data_t</span><span> ioc_data </span><span>=</span><span> {</span></span>
<span><span> .use_srbm </span><span>=</span><span> false</span><span>,</span></span>
<span><span> .use_grbm </span><span>=</span><span> true</span><span>,</span></span>
<span><span>};</span></span>
<span></span>
<span><span>dev_op_reg32</span><span>(</span><span>&amp;</span><span>amdgpu</span><span>,</span><span> REG_SQ_CMD</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_WRITE</span><span>,</span><span> &amp;</span><span>halt_cmd.raw);</span></span>
<span><span>gpu_is_halted </span><span>=</span><span> true</span><span>;</span></span></code></pre>
<p>From here on, we can do whatever with the data we have. All the data we need to build a proper debugger. We will come back to what to do with the data in a bit; let’s assume we did what was needed for now.</p>
<p>Now that we’re done with the CPU, we need to write to the first byte in our TMA buffer, since the trap handler checks for that, then resume the wave, and the trap handler should pick it up. We can resume by writing to the <code>SQ_CMD</code> register again:</p>
<pre tabindex="0" data-language="c"><code><span><span>halt_cmd.mode </span><span>=</span><span> 0</span><span>;</span></span>
<span><span>dev_op_reg32</span><span>(</span><span>&amp;</span><span>amdgpu</span><span>,</span><span> REG_SQ_CMD</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_WRITE</span><span>,</span><span> &amp;</span><span>halt_cmd.raw);</span></span>
<span><span>gpu_is_halted </span><span>=</span><span> false</span><span>;</span></span></code></pre>
<p>Then the GPU should continue. We need to restore everything and return the program counter to the original address. Based on whether it’s a hardware trap or not, the program counter may point to the instruction before or the instruction itself. The ISA manual and Marcell Kiss’s posts explain that well, so refer to them.</p>
<pre tabindex="0" data-language="asm"><code><span><span>RETURN</span><span>:</span></span>
<span><span> ;; extract the trap ID from ttmp1</span></span>
<span><span> s_and_b32 ttmp9, ttmp1, PC_HI_TRAP_ID_MASK</span></span>
<span><span> s_lshr_b32 ttmp9, ttmp9, PC_HI_TRAP_ID_SHIFT</span></span>
<span></span>
<span><span> ;; if the trapID == 0, then this is a hardware trap,</span></span>
<span><span> ;; we don't need to fix up the return address</span></span>
<span><span> s_cmpk_eq_u32 ttmp9, </span><span>0</span></span>
<span><span> s_cbranch_scc1 RETURN_FROM_NON_S_TRAP</span></span>
<span></span>
<span><span> ;; restore PC</span></span>
<span><span> ;; add 4 to the faulting address, with carry</span></span>
<span><span> s_add_u32 ttmp0, ttmp0, </span><span>4</span></span>
<span><span> s_addc_u32 ttmp1, ttmp1, </span><span>0</span></span>
<span></span>
<span><span>RETURN_FROM_NON_S_TRAP</span><span>:</span></span>
<span><span> s_load_dwordx4 s[</span><span>0</span><span>:</span><span>3</span><span>], ttmp[</span><span>4</span><span>:</span><span>5</span><span>], TMA_SREG_OFFSET glc dlc</span></span>
<span><span> s_load_dword s4, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], TMA_SREG_OFFSET + </span><span>16</span><span> glc dlc</span></span>
<span><span> s_waitcnt lgkmcnt(</span><span>0</span><span>)</span></span>
<span></span>
<span><span> s_mov_b64 exec, -</span><span>1</span></span>
<span><span> global_load_addtid_b32 v0, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET        glc slc dlc</span></span>
<span><span> global_load_addtid_b32 v1, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>256</span><span>  glc slc dlc</span></span>
<span><span> global_load_addtid_b32 v2, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>512</span><span>  glc slc dlc</span></span>
<span><span> global_load_addtid_b32 v3, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>768</span><span>  glc slc dlc</span></span>
<span><span> global_load_addtid_b32 v4, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>1024</span><span> glc slc dlc</span></span>
<span><span> global_load_addtid_b32 v5, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>1280</span><span> glc slc dlc</span></span>
<span><span> global_load_addtid_b32 v6, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>1536</span><span> glc slc dlc</span></span>
<span><span> s_waitcnt vmcnt(</span><span>0</span><span>)</span></span>
<span></span>
<span><span> ;; mask off non-address high bits from ttmp1</span></span>
<span><span> s_and_b32 ttmp1, ttmp1, </span><span>0xffff</span></span>
<span></span>
<span><span> ;; restore exec</span></span>
<span><span> s_load_b64 vcc, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>2048</span><span> glc dlc</span></span>
<span><span> s_load_b64 ttmp[</span><span>2</span><span>:</span><span>3</span><span>], ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>2056</span><span> glc dlc</span></span>
<span><span> s_waitcnt lgkmcnt(</span><span>0</span><span>)</span></span>
<span><span> s_mov_b64 exec, ttmp[</span><span>2</span><span>:</span><span>3</span><span>]</span></span>
<span></span>
<span><span> ;; restore STATUS.EXECZ, not writable by s_setreg_b32</span></span>
<span><span> s_and_b64 exec, exec, exec</span></span>
<span></span>
<span><span> ;; restore STATUS.VCCZ, not writable by s_setreg_b32</span></span>
<span><span> s_and_b64 vcc, vcc, vcc</span></span>
<span></span>
<span><span> ;; restore STATUS.SCC</span></span>
<span><span> s_setreg_b32 hwreg(HW_REG_STATUS, </span><span>0</span><span>, </span><span>1</span><span>), ttmp8</span></span>
<span></span>
<span><span> s_waitcnt vmcnt(</span><span>0</span><span>) lgkmcnt(</span><span>0</span><span>) expcnt(</span><span>0</span><span>)  </span><span>; Full pipeline flush</span></span>
<span><span> ;; return from trap handler and restore STATUS.PRIV</span></span>
<span><span> s_rfe_b64 [ttmp0, ttmp1]</span></span></code></pre>
<h2 id="spir-v">SPIR-V</h2>
<p>Now we can run compiled code directly, but we don’t want people to compile their code manually, then extract the text section, and give it to us. The plan is to take SPIR-V code, compile it correctly, then run it, or, even better, integrate with RADV and let RADV give us more information to work with.</p>
<p>My main plan was making like fork RADV and then add then make report for us the vulkan calls and then we can have a better view on the GPU work know the buffers/textures it’s using etc, This seems like a lot more work tho so I’ll keep it in mind but not doing that for now unless someone is willing to pay me for that ;).</p>
<p>For now, let’s just use RADV’s compiler <code>ACO</code>. Luckily, RADV has a <code>null_winsys</code> mode, aka it will not do actual work or open DRM files, just a fake Vulkan device, which is perfect for our case here, since we care about nothing other than just compiling code. We can enable it by setting the env var <code>RADV_FORCE_FAMILY</code>, then we just call what we need like this:</p>
<pre tabindex="0" data-language="c"><code><span><span>int32_t</span><span> hdb_compile_spirv_to_bin</span><span>(</span></span>
<span><span>  const</span><span> void*</span><span> spirv_binary</span><span>,</span></span>
<span><span>  size_t</span><span> size</span><span>,</span></span>
<span><span>  hdb_shader_stage_t</span><span> stage</span><span>,</span></span>
<span><span>  hdb_shader_t</span><span>*</span><span> shader</span></span>
<span><span>) {</span></span>
<span><span> setenv(</span><span>"RADV_FORCE_FAMILY"</span><span>,</span><span> "navi31"</span><span>,</span><span> 1</span><span>)</span><span>;</span></span>
<span><span> //  setenv("RADV_DEBUG", "nocache,noopt", 1);</span></span>
<span><span> setenv(</span><span>"ACO_DEBUG"</span><span>,</span><span> "nocache,noopt"</span><span>,</span><span> 1</span><span>)</span><span>;</span></span>
<span></span>
<span><span> VkInstanceCreateInfo i_cinfo </span><span>=</span><span> {</span></span>
<span><span>  .sType </span><span>=</span><span> VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO</span><span>,</span></span>
<span><span>  .pApplicationInfo </span><span>=</span></span>
<span><span>    &amp;</span><span>(VkApplicationInfo){</span></span>
<span><span>      .sType              </span><span>=</span><span> VK_STRUCTURE_TYPE_APPLICATION_INFO</span><span>,</span></span>
<span><span>      .pApplicationName   </span><span>=</span><span> "HDB Shader Compiler"</span><span>,</span></span>
<span><span>      .applicationVersion </span><span>=</span><span> 1</span><span>,</span></span>
<span><span>      .pEngineName        </span><span>=</span><span> "HDB"</span><span>,</span></span>
<span><span>      .engineVersion      </span><span>=</span><span> 1</span><span>,</span></span>
<span><span>      .apiVersion         </span><span>=</span><span> VK_API_VERSION_1_4</span><span>,</span></span>
<span><span>    }</span><span>,</span></span>
<span><span> };</span></span>
<span></span>
<span><span> VkInstance vk_instance </span><span>=</span><span> {};</span></span>
<span><span> radv_CreateInstance(</span><span>&amp;</span><span>i_cinfo</span><span>,</span><span> NULL</span><span>,</span><span> &amp;</span><span>vk_instance)</span><span>;</span></span>
<span></span>
<span><span> struct</span><span> radv_instance</span><span>*</span><span> instance </span><span>=</span><span> radv_instance_from_handle(vk_instance)</span><span>;</span></span>
<span><span> instance</span><span>-&gt;</span><span>debug_flags </span><span>|=</span></span>
<span><span>   RADV_DEBUG_NIR_DEBUG_INFO </span><span>|</span><span> RADV_DEBUG_NO_CACHE </span><span>|</span><span> RADV_DEBUG_INFO;</span></span>
<span></span>
<span><span> uint32_t</span><span>         n       </span><span>=</span><span> 1</span><span>;</span></span>
<span><span> VkPhysicalDevice vk_pdev </span><span>=</span><span> {};</span></span>
<span><span> instance</span><span>-&gt;</span><span>vk</span><span>.</span><span>dispatch_table.</span><span>EnumeratePhysicalDevices</span><span>(</span><span>vk_instance</span><span>,</span><span> &amp;</span><span>n</span><span>,</span><span> &amp;</span><span>vk_pdev</span><span>);</span></span>
<span></span>
<span><span> struct</span><span> radv_physical_device</span><span>*</span><span> pdev </span><span>=</span><span> radv_physical_device_from_handle(vk_pdev)</span><span>;</span></span>
<span><span> pdev</span><span>-&gt;</span><span>use_llvm                    </span><span>=</span><span> false</span><span>;</span></span>
<span></span>
<span><span> VkDeviceCreateInfo d_cinfo </span><span>=</span><span> { VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO };</span></span>
<span><span> VkDevice vk_dev </span><span>=</span><span> {};</span></span>
<span><span> pdev</span><span>-&gt;</span><span>vk</span><span>.</span><span>dispatch_table.</span><span>CreateDevice</span><span>(</span><span>vk_pdev</span><span>,</span><span> &amp;</span><span>d_cinfo</span><span>,</span><span> NULL</span><span>,</span><span> &amp;</span><span>vk_dev</span><span>);</span></span>
<span></span>
<span><span> struct</span><span> radv_device</span><span>*</span><span> dev </span><span>=</span><span> radv_device_from_handle(vk_dev)</span><span>;</span></span>
<span></span>
<span><span> struct</span><span> radv_shader_stage radv_stage </span><span>=</span><span> {</span></span>
<span><span>  .</span><span>spirv</span><span>.</span><span>data </span><span>=</span><span> spirv_binary</span><span>,</span></span>
<span><span>  .</span><span>spirv</span><span>.</span><span>size </span><span>=</span><span> size</span><span>,</span></span>
<span><span>  .entrypoint </span><span>=</span><span> "main"</span><span>,</span></span>
<span><span>  .stage      </span><span>=</span><span> MESA_SHADER_COMPUTE</span><span>,</span></span>
<span><span>  .layout </span><span>=</span><span> {</span></span>
<span><span>   .push_constant_size </span><span>=</span><span> 16</span><span>,</span></span>
<span><span>  }</span><span>,</span></span>
<span><span>  .key </span><span>=</span><span> {</span></span>
<span><span>   .optimisations_disabled </span><span>=</span><span> true</span><span>,</span></span>
<span><span>  }</span><span>,</span></span>
<span><span> };</span></span>
<span></span>
<span><span> struct</span><span> radv_shader_binary</span><span>*</span><span> cs_bin </span><span>=</span><span> NULL</span><span>;</span></span>
<span><span> struct</span><span> radv_shader</span><span>*</span><span>        cs_shader </span><span>=</span></span>
<span><span>   radv_compile_cs(dev</span><span>,</span><span> NULL</span><span>,</span><span> &amp;</span><span>radv_stage</span><span>,</span><span> true</span><span>,</span><span> true</span><span>,</span><span> false</span><span>,</span><span> true</span><span>,</span><span> &amp;</span><span>cs_bin)</span><span>;</span></span>
<span></span>
<span><span> *</span><span>shader </span><span>=</span><span> (</span><span>hdb_shader_t</span><span>){</span></span>
<span><span>  .bin              </span><span>=</span><span> cs_shader</span><span>-&gt;</span><span>code</span><span>,</span></span>
<span><span>  .bin_size         </span><span>=</span><span> cs_shader</span><span>-&gt;</span><span>code_size</span><span>,</span></span>
<span><span>  .rsrc1            </span><span>=</span><span> cs_shader</span><span>-&gt;</span><span>config</span><span>.</span><span>rsrc1</span><span>,</span></span>
<span><span>  .rsrc2            </span><span>=</span><span> cs_shader</span><span>-&gt;</span><span>config</span><span>.</span><span>rsrc2</span><span>,</span></span>
<span><span>  .rsrc3            </span><span>=</span><span> cs_shader</span><span>-&gt;</span><span>config</span><span>.</span><span>rsrc3</span><span>,</span></span>
<span><span>  .debug_info       </span><span>=</span><span> cs_shader</span><span>-&gt;</span><span>debug_info</span><span>,</span></span>
<span><span>  .debug_info_count </span><span>=</span><span> cs_shader</span><span>-&gt;</span><span>debug_info_count</span><span>,</span></span>
<span><span> };</span></span>
<span></span>
<span><span> return</span><span> 0</span><span>;</span></span>
<span><span>}</span></span></code></pre>
<p>Now that we have a well-structured loop and communication between the GPU and the CPU, we can run SPIR-V binaries to some extent. Let’s see how we can make it an actual debugger.</p>
<h2 id="an-actual-debugger">An Actual Debugger</h2>
<p>We talked earlier about CPs natively supporting context-switching, this appears to be compute spcific feature,
which prevents from implementing it for other types of shaders, tho, it appears that mesh shaders and raytracing
shaders are just compute shaders under the hood, which will allow us to use that functionality. For now debugging
one wave feels enough, also we can moify the wave parameters to debug some specific indices.</p>
<p>Here’s some of the features</p>
<h2 id="breakpoints-and-stepping">Breakpoints and Stepping</h2>
<p>For stepping, we can use 2 bits: one in <code>RSRC1</code> and the other in <code>RSRC3</code>. They’re <code>DEBUG_MODE</code> and <code>TRAP_ON_START</code>, respectively. The former enters the trap handler after each instruction, and the latter enters before the first instruction. This means we can automatically enable instruction-level stepping.</p>
<p>Regarding breakpoints, I haven’t implemented them, but they’re rather simple to implement here by us having the base address of the code buffer and knowing the size of each instruction; we can calculate the program counter location ahead and have a list of them available to the GPU, and we can do a binary search on the trap handler.</p>
<h2 id="source-code-line-mapping">Source Code Line Mapping</h2>
<p>The ACO shader compiler does generate instruction-level source code mapping, which is good enough for our purposes here. By taking the offset<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup> of the current program counter and indexing into the code buffer, we can retrieve the current instruction and disassemble it, as well as find the source code mapping from the debug info.</p>
<h2 id="address-watching-aka-watchpoints">Address Watching aka Watchpoints</h2>
<p>We can implement this by marking the GPU page as protected. On a GPU fault, we enter the trap handler, check whether it’s within the range of our buffers and textures, and then act accordingly. Also, looking at the registers, we can find these:</p>
<pre tabindex="0" data-language="c"><code><span><span>typedef</span><span> union</span><span> {</span></span>
<span><span> struct</span><span> {</span></span>
<span><span>  uint32_t</span><span> addr: </span><span>16</span><span>;</span></span>
<span><span> };</span></span>
<span><span> uint32_t</span><span> raw;</span></span>
<span><span>} </span><span>reg_sq_watch0_addr_h_t</span><span>;</span></span>
<span></span>
<span><span>typedef</span><span> union</span><span> {</span></span>
<span><span> struct</span><span> {</span></span>
<span><span>  uint32_t</span><span> __reserved_0 : </span><span>6</span><span>;</span></span>
<span><span>  uint32_t</span><span> addr: </span><span>26</span><span>;</span></span>
<span><span> };</span></span>
<span><span> uint32_t</span><span> raw;</span></span>
<span><span>} </span><span>reg_sq_watch0_addr_l_t</span><span>;</span></span></code></pre>
<p>which suggests that the hardware already supports this natively, so we don’t even need to do that dance. It needs more investigation on my part, tho, since I didn’t implement this.</p>
<h2 id="variables-types-and-names">Variables Types and Names</h2>
<p>This needs some serious plumbing, since we need to make NIR(Mesa’s intermediate representation) optimisation passes propagate debug info correctly. I already started on this <a href="https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/37705">here</a>. Then we need to make ACO track variables and store the information.</p>
<h2 id="vulkan-integration">Vulkan Integration</h2>
<p>This requires ditching our simple UMD we made earlier and using RADV, which is what should happen eventually, then we have our custom driver maybe pause on before a specific frame, or get triggered by a key, and then ask before each dispatch if to attach to it or not, or something similar, since we have a full proper Vulkan implementation we already have all the information we would need like buffers, textures, push constants, types, variable names, .. etc, that would be a much better and more pleasant debugger to use.</p>
<hr>
<p>Finally, here’s some live footage:</p>

    <figure>
      <iframe src="https://www.youtube.com/embed/HDMC9GhaLyc" title="YouTube video player" loading="lazy" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
    </figure>
    
<h2 id="bonus-round">Bonus Round</h2>
<p>Here is an incomplete user-mode page walking code for gfx11, aka rx7900xtx</p>
<pre tabindex="0" data-language="c"><code><span><span>typedef</span><span> struct</span><span> {</span></span>
<span><span> u64 valid         : </span><span>1</span><span>;</span><span>  // 0</span></span>
<span><span> u64 system        : </span><span>1</span><span>;</span><span>  // 1</span></span>
<span><span> u64 coherent      : </span><span>1</span><span>;</span><span>  // 2</span></span>
<span><span> u64 __reserved_0  : </span><span>3</span><span>;</span><span>  // 5</span></span>
<span><span> u64 pte_base_addr : </span><span>42</span><span>;</span><span> // 47</span></span>
<span><span> u64 pa_rsvd       : </span><span>4</span><span>;</span><span>  // 51</span></span>
<span><span> u64 __reserved_1  : </span><span>2</span><span>;</span><span>  // 53</span></span>
<span><span> u64 mall_reuse    : </span><span>2</span><span>;</span><span>  // 55</span></span>
<span><span> u64 tfs_addr      : </span><span>1</span><span>;</span><span>  // 56</span></span>
<span><span> u64 __reserved_2  : </span><span>1</span><span>;</span><span>  // 57</span></span>
<span><span> u64 frag_size     : </span><span>5</span><span>;</span><span>  // 62</span></span>
<span><span> u64 pte           : </span><span>1</span><span>;</span><span>  // 63</span></span>
<span><span>} </span><span>pde_t</span><span>;</span></span>
<span></span>
<span><span>typedef</span><span> struct</span><span> {</span></span>
<span><span> u64 valid          : </span><span>1</span><span>;</span><span> // = pte_entry &amp; 1;</span></span>
<span><span> u64 system         : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 1) &amp; 1;</span></span>
<span><span> u64 coherent       : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 2) &amp; 1;</span></span>
<span><span> u64 tmz            : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 3) &amp; 1;</span></span>
<span><span> u64 execute        : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 4) &amp; 1;</span></span>
<span><span> u64 read           : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 5) &amp; 1;</span></span>
<span><span> u64 write          : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 6) &amp; 1;</span></span>
<span><span> u64 fragment       : </span><span>5</span><span>;</span><span> // = (pte_entry &gt;&gt; 7) &amp; 0x1F;</span></span>
<span><span> u64 page_base_addr : </span><span>36</span><span>;</span></span>
<span><span> u64 mtype          : </span><span>2</span><span>;</span><span> // = (pte_entry &gt;&gt; 48) &amp; 3;</span></span>
<span><span> u64 prt            : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 51) &amp; 1;</span></span>
<span><span> u64 software       : </span><span>2</span><span>;</span><span> // = (pte_entry &gt;&gt; 52) &amp; 3;</span></span>
<span><span> u64 pde            : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 54) &amp; 1;</span></span>
<span><span> u64 __reserved_0   : </span><span>1</span><span>;</span></span>
<span><span> u64 further        : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 56) &amp; 1;</span></span>
<span><span> u64 gcr            : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 57) &amp; 1;</span></span>
<span><span> u64 llc_noalloc    : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 58) &amp; 1;</span></span>
<span><span>} </span><span>pte_t</span><span>;</span></span>
<span></span>
<span><span>static</span><span> inline</span><span> pde_t</span><span> decode_pde</span><span>(u64 pde_raw) {</span></span>
<span><span> pde_t</span><span> pde         </span><span>=</span><span> *</span><span>((</span><span>pde_t</span><span>*</span><span>)(</span><span>&amp;</span><span>pde_raw));</span></span>
<span><span> pde</span><span>.</span><span>pte_base_addr </span><span>=</span><span> (u64)</span><span>pde</span><span>.</span><span>pte_base_addr </span><span>&lt;&lt;</span><span> 6</span><span>;</span></span>
<span><span> return</span><span> pde;</span></span>
<span><span>}</span></span>
<span></span>
<span><span>static</span><span> inline</span><span> pte_t</span><span> decode_pte</span><span>(u64 pde_raw) {</span></span>
<span><span> pte_t</span><span> pte          </span><span>=</span><span> *</span><span>((</span><span>pte_t</span><span>*</span><span>)(</span><span>&amp;</span><span>pde_raw));</span></span>
<span><span> pte</span><span>.</span><span>page_base_addr </span><span>=</span><span> (u64)</span><span>pte</span><span>.</span><span>page_base_addr </span><span>&lt;&lt;</span><span> 12</span><span>;</span></span>
<span><span> return</span><span> pte;</span></span>
<span><span>}</span></span>
<span></span>
<span><span>static</span><span> inline</span><span> u64 </span><span>log2_range_round_up</span><span>(u64 s</span><span>,</span><span> u64 e) {</span></span>
<span><span> u64 x </span><span>=</span><span> e </span><span>-</span><span> s </span><span>-</span><span> 1</span><span>;</span></span>
<span><span> return</span><span> (x </span><span>==</span><span> 0</span><span> ||</span><span> x </span><span>==</span><span> 1</span><span>) </span><span>?</span><span> 1</span><span> :</span><span> 64</span><span> -</span><span> __builtin_clzll(x)</span><span>;</span></span>
<span><span>}</span></span>
<span></span>
<span><span>void</span><span> dev_linear_vram</span><span>(</span><span>amdgpu_t</span><span>*</span><span> dev</span><span>,</span><span> u64 phy_addr</span><span>,</span><span> size_t</span><span> size</span><span>,</span><span> void*</span><span> buf) {</span></span>
<span><span> HDB_ASSERT(</span><span>!</span><span>((phy_addr </span><span>&amp;</span><span> 3</span><span>) </span><span>||</span><span> (size </span><span>&amp;</span><span> 3</span><span>))</span><span>,</span><span> "Must be page aligned address and size"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> size_t</span><span> offset </span><span>=</span><span> lseek(</span><span>dev</span><span>-&gt;</span><span>vram_fd</span><span>,</span><span> phy_addr</span><span>,</span><span> SEEK_SET)</span><span>;</span></span>
<span><span> HDB_ASSERT(offset </span><span>==</span><span> phy_addr</span><span>,</span><span> "Couldn't seek to the requested addr"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> offset </span><span>=</span><span> read(</span><span>dev</span><span>-&gt;</span><span>vram_fd</span><span>,</span><span> buf</span><span>,</span><span> size)</span><span>;</span></span>
<span><span> HDB_ASSERT(offset </span><span>==</span><span> size</span><span>,</span><span> "Couldn't read the full requested size"</span><span>)</span><span>;</span></span>
<span><span>}</span></span>
<span></span>
<span><span>void</span><span> dev_decode</span><span>(</span><span>amdgpu_t</span><span>*</span><span> dev</span><span>,</span><span> u32 vmid</span><span>,</span><span> u64 va_addr) {</span></span>
<span><span> reg_gcmc_vm_fb_location_base_t</span><span> fb_base_reg   </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> reg_gcmc_vm_fb_location_top_t</span><span>  fb_top_reg    </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> reg_gcmc_vm_fb_offset_t</span><span>        fb_offset_reg </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span></span>
<span><span> regs2_ioc_data_t</span><span> ioc_data </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> dev_op_reg32(</span></span>
<span><span>   dev</span><span>,</span><span> REG_GCMC_VM_FB_LOCATION_BASE</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>fb_base_reg</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> REG_GCMC_VM_FB_LOCATION_TOP</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>fb_top_reg</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> REG_GCMC_VM_FB_OFFSET</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>fb_offset_reg</span><span>.</span><span>raw)</span><span>;</span></span>
<span></span>
<span><span> u64 fb_offset </span><span>=</span><span> (u64)</span><span>fb_offset_reg</span><span>.</span><span>fb_offset;</span></span>
<span></span>
<span><span> // TODO(hadi): add zfb mode support</span></span>
<span><span> bool</span><span> zfb </span><span>=</span><span> fb_top_reg</span><span>.</span><span>fb_top </span><span>+</span><span> 1</span><span> &lt;</span><span> fb_base_reg</span><span>.</span><span>fb_base;</span></span>
<span><span> HDB_ASSERT(</span><span>!</span><span>zfb</span><span>,</span><span> "ZFB mode is not implemented yet!"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> // printf(</span></span>
<span><span> //   "fb base: 0x%x\nfb_top: 0x%x\nfb_offset: 0x%x\n",</span></span>
<span><span> //   fb_base_reg.raw,</span></span>
<span><span> //   fb_top_reg.raw,</span></span>
<span><span> //   fb_offset_reg.raw);</span></span>
<span></span>
<span><span> gc_11_reg_t</span><span> pt_start_lo_id </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> gc_11_reg_t</span><span> pt_start_hi_id </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> gc_11_reg_t</span><span> pt_end_lo_id   </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> gc_11_reg_t</span><span> pt_end_hi_id   </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> gc_11_reg_t</span><span> pt_base_hi_id  </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> gc_11_reg_t</span><span> pt_base_lo_id  </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> gc_11_reg_t</span><span> ctx_cntl_id    </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span></span>
<span><span> switch</span><span> (vmid) {</span></span>
<span><span> case</span><span> 0</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT0_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT0_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT0_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT0_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT0_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT0_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT0_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 1</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT1_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT1_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT1_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT1_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT1_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT1_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT1_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 2</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT2_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT2_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT2_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT2_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT2_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT2_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT2_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 3</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT3_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT3_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT3_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT3_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT3_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT3_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT3_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 4</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT4_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT4_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT4_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT4_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT4_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT4_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT4_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 5</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT5_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT5_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT5_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT5_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT5_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT5_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT5_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 6</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT6_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT6_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT6_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT6_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT6_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT6_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT6_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 7</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT7_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 8</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT8_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT8_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT8_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT8_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT7_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 9</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT9_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT9_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT9_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT9_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT7_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 10</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT10_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT10_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT10_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT10_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT10_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT10_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT10_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 11</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT11_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT11_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT11_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT11_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT11_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT11_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT11_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 12</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT12_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT12_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT12_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT12_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT12_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT12_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT12_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 13</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT13_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT13_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT13_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT13_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT13_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT13_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT13_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 14</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT14_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT14_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT14_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT14_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT14_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT14_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT14_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 15</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT15_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT15_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT15_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT15_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT15_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT15_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT15_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> default</span><span>:</span><span> HDB_ASSERT</span><span>(</span><span>false</span><span>,</span><span> "Out of range VMID 0-15 trying to access </span><span>%u</span><span>"</span><span>,</span><span> vmid);</span></span>
<span><span> }</span></span>
<span></span>
<span><span> // all the types of the contexts are the same so will just use 0 but pass the correct</span></span>
<span><span> // register enum to the read function</span></span>
<span><span> reg_gcvm_context0_page_table_start_addr_lo32_t</span><span> pt_start_lo </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> reg_gcvm_context0_page_table_start_addr_hi32_t</span><span> pt_start_hi </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> reg_gcvm_context0_page_table_end_addr_lo32_t</span><span>   pt_end_lo   </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> reg_gcvm_context0_page_table_end_addr_hi32_t</span><span>   pt_end_hi   </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> reg_gcvm_context0_page_table_base_addr_lo32_t</span><span>  pt_base_lo  </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> reg_gcvm_context0_page_table_base_addr_hi32_t</span><span>  pt_base_hi  </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> reg_gcvm_context0_cntl_t</span><span>                       ctx_cntl    </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> pt_start_lo_id</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>pt_start_lo</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> pt_start_hi_id</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>pt_start_hi</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> pt_end_lo_id</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>pt_end_lo</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> pt_end_hi_id</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>pt_end_hi</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> pt_base_lo_id</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>pt_base_lo</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> pt_base_hi_id</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>pt_base_hi</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> ctx_cntl_id</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>ctx_cntl</span><span>.</span><span>raw)</span><span>;</span></span>
<span></span>
<span><span> u64 pt_start_addr </span><span>=</span><span> ((u64)</span><span>pt_start_lo</span><span>.</span><span>raw </span><span>&lt;&lt;</span><span> 12</span><span>) </span><span>|</span><span> ((u64)</span><span>pt_start_hi</span><span>.</span><span>raw </span><span>&lt;&lt;</span><span> 44</span><span>);</span></span>
<span><span> u64 pt_end_addr   </span><span>=</span><span> ((u64)</span><span>pt_end_lo</span><span>.</span><span>raw </span><span>&lt;&lt;</span><span> 12</span><span>) </span><span>|</span><span> ((u64)</span><span>pt_end_hi</span><span>.</span><span>raw </span><span>&lt;&lt;</span><span> 44</span><span>);</span></span>
<span><span> u64 pt_base_addr  </span><span>=</span><span> ((u64)</span><span>pt_base_lo</span><span>.</span><span>raw </span><span>&lt;&lt;</span><span> 0</span><span>) </span><span>|</span><span> ((u64)</span><span>pt_base_hi</span><span>.</span><span>raw </span><span>&lt;&lt;</span><span> 32</span><span>);</span></span>
<span><span> u32 pt_depth      </span><span>=</span><span> ctx_cntl</span><span>.</span><span>page_table_depth;</span></span>
<span><span> u32 ptb_size      </span><span>=</span><span> ctx_cntl</span><span>.</span><span>page_table_block_size;</span></span>
<span></span>
<span><span> HDB_ASSERT(pt_base_addr </span><span>!=</span><span> 0x</span><span>ffffffffffffffff</span><span>ull</span><span>,</span><span> "Invalid page table base addr"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> printf(</span></span>
<span><span>   "\tPage Table Start: 0x</span><span>%lx</span><span>\n\tPage Table End: 0x</span><span>%lx</span><span>\n\tPage Table Base: "</span></span>
<span><span>   "0x</span><span>%lx</span><span>\n\tPage Table Depth: </span><span>%u</span><span>\n\tBlock Size: </span><span>%u</span><span>\n"</span><span>,</span></span>
<span><span>   pt_start_addr</span><span>,</span></span>
<span><span>   pt_end_addr</span><span>,</span></span>
<span><span>   pt_base_addr</span><span>,</span></span>
<span><span>   pt_depth</span><span>,</span></span>
<span><span>   ptb_size)</span><span>;</span></span>
<span></span>
<span><span> // decode base PDB</span></span>
<span><span> pde_t</span><span> pde </span><span>=</span><span> decode_pde(pt_base_addr)</span><span>;</span></span>
<span><span> pt_base_addr </span><span>-=</span><span> fb_offset </span><span>*</span><span> !</span><span>pde</span><span>.</span><span>system;</span><span> // substract only on vram</span></span>
<span></span>
<span><span> u64 pt_last_byte_addr </span><span>=</span><span> pt_end_addr </span><span>+</span><span> 0x</span><span>fff</span><span>;</span><span> // 0xfff is 1 page</span></span>
<span><span> HDB_ASSERT(</span></span>
<span><span>   pt_start_addr </span><span>&lt;=</span><span> va_addr </span><span>||</span><span> va_addr </span><span>&lt;</span><span> pt_last_byte_addr</span><span>,</span></span>
<span><span>   "Invalid virtual address outside the range of the root page table of this vm"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> va_addr </span><span>-=</span><span> pt_start_addr;</span></span>
<span><span> //</span></span>
<span><span> // Size of the first PDB depends on the total coverage of the</span></span>
<span><span> // page table and the PAGE_TABLE_BLOCK_SIZE.</span></span>
<span><span> // Entire table takes ceil(log2(total_vm_size)) bits</span></span>
<span><span> // All PDBs except the first one take 9 bits each</span></span>
<span><span> // The PTB covers at least 2 MiB (21 bits)</span></span>
<span><span> // And PAGE_TABLE_BLOCK_SIZE is log2(num 2MiB ranges PTB covers)</span></span>
<span><span> // As such, the formula for the size of the first PDB is:</span></span>
<span><span> //                       PDB1, PDB0, etc.      PTB covers at least 2 MiB</span></span>
<span><span> //                                        Block size can make it cover more</span></span>
<span><span> //   total_vm_bits - (9 * num_middle_pdbs) - (page_table_block_size + 21)</span></span>
<span><span> //</span></span>
<span><span> // we need the total range range here not the last byte addr like above</span></span>
<span><span> u32 total_vaddr_bits </span><span>=</span><span> log2_range_round_up(pt_start_addr</span><span>,</span><span> pt_end_addr </span><span>+</span><span> 0x</span><span>1000</span><span>)</span><span>;</span></span>
<span></span>
<span><span> u32 total_pdb_bits </span><span>=</span><span> total_vaddr_bits;</span></span>
<span><span> // substract everything from the va_addr to leave just the pdb bits</span></span>
<span><span> total_pdb_bits </span><span>-=</span><span> 9</span><span> *</span><span> (pt_depth </span><span>-</span><span> 1</span><span>);</span><span> // middle PDBs each is 9 bits</span></span>
<span><span> total_pdb_bits </span><span>-=</span><span> (ptb_size </span><span>+</span><span> 21</span><span>);</span><span>    // at least 2mb(21) bits + ptb_size</span></span>
<span></span>
<span><span> // u64 va_mask = (1ull &lt;&lt; total_pdb_bits) - 1;</span></span>
<span><span> // va_mask &lt;&lt;= (total_vaddr_bits - total_pdb_bits);</span></span>
<span></span>
<span><span> // pde_t pdes[8]  = { 0 };</span></span>
<span><span> // u32   curr_pde = 0;</span></span>
<span><span> // u64   pde_addr = 0;</span></span>
<span><span> // u64  loop_pde = pt_base_addr;</span></span>
<span></span>
<span><span> if</span><span> (pt_depth </span><span>==</span><span> 0</span><span>) { </span><span>HDB_ASSERT(</span><span>false</span><span>,</span><span> "DEPTH = 0 is not implemented yet"</span><span>)</span><span>; }</span></span>
<span></span>
<span><span> pde_t</span><span> curr_pde    </span><span>=</span><span> pde;</span></span>
<span><span> u64   entry_bits  </span><span>=</span><span> 0</span><span>;</span></span>
<span><span> s32   curr_depth  </span><span>=</span><span> pt_depth;</span></span>
<span><span> bool</span><span>  pde0_is_pte </span><span>=</span><span> false</span><span>;</span></span>
<span><span> // walk all middle PDEs</span></span>
<span><span> while</span><span> (curr_depth </span><span>&gt;</span><span> 0</span><span>) {</span></span>
<span><span>  // printf("pde(%u):0x%lx \n", curr_depth, curr_pde.pte_base_addr);</span></span>
<span><span>  u64 next_entry_addr </span><span>=</span><span> 0</span><span>;</span></span>
<span></span>
<span><span>  u32 shift_amount </span><span>=</span><span> total_vaddr_bits;</span></span>
<span><span>  shift_amount </span><span>-=</span><span> total_pdb_bits;</span></span>
<span><span>  // for each pdb shift 9 more</span></span>
<span><span>  shift_amount </span><span>-=</span><span> ((pt_depth </span><span>-</span><span> curr_depth) </span><span>*</span><span> 9</span><span>);</span></span>
<span></span>
<span><span>  // shift address and mask out unused bits</span></span>
<span><span>  u64 next_pde_idx </span><span>=</span><span> va_addr </span><span>&gt;&gt;</span><span> shift_amount;</span></span>
<span><span>  next_pde_idx </span><span>&amp;=</span><span> 0x</span><span>1ff</span><span>;</span></span>
<span></span>
<span><span>  // if on vram we need to apply this offset</span></span>
<span><span>  if</span><span> (</span><span>!</span><span>curr_pde</span><span>.</span><span>system) </span><span>curr_pde</span><span>.</span><span>pte_base_addr </span><span>-=</span><span> fb_offset;</span></span>
<span></span>
<span><span>  next_entry_addr </span><span>=</span><span> curr_pde</span><span>.</span><span>pte_base_addr </span><span>+</span><span> next_pde_idx </span><span>*</span><span> 8</span><span>;</span></span>
<span><span>  curr_depth</span><span>--</span><span>;</span></span>
<span></span>
<span><span>  if</span><span> (</span><span>!</span><span>curr_pde</span><span>.</span><span>system) {</span></span>
<span><span>   dev_linear_vram(dev</span><span>,</span><span> next_entry_addr</span><span>,</span><span> 8</span><span>,</span><span> &amp;</span><span>entry_bits)</span><span>;</span></span>
<span><span>   curr_pde </span><span>=</span><span> decode_pde(entry_bits)</span><span>;</span></span>
<span><span>   printf(</span></span>
<span><span>     "\tPage Dir Entry(</span><span>%u</span><span>):\n\t  Addr:0x</span><span>%lx</span><span>\n\t  Base: 0x</span><span>%lx</span><span>\n\n\t        ↓\n\n"</span><span>,</span></span>
<span><span>     curr_depth</span><span>,</span></span>
<span><span>     next_entry_addr</span><span>,</span></span>
<span><span>     curr_pde</span><span>.</span><span>pte_base_addr)</span><span>;</span></span>
<span><span>  } </span><span>else</span><span> {</span></span>
<span><span>   HDB_ASSERT(</span><span>false</span><span>,</span><span> "GTT physical memory access is not implemented yet"</span><span>)</span><span>;</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  if</span><span> (</span><span>!</span><span>curr_pde</span><span>.</span><span>valid) { </span><span>break</span><span>; }</span></span>
<span></span>
<span><span>  if</span><span> (</span><span>curr_pde</span><span>.</span><span>pte) {</span></span>
<span><span>   // PDB0 can act as a pte</span></span>
<span><span>   // also I'm making an assumption here that UMRs code doesn't make</span></span>
<span><span>   // that the the PDB0 as PTE path can't have the further bit set</span></span>
<span><span>   pde0_is_pte </span><span>=</span><span> true</span><span>;</span></span>
<span><span>   break</span><span>;</span></span>
<span><span>  }</span></span>
<span><span> }</span></span>
<span></span>
<span><span> if</span><span> (pde0_is_pte) { </span><span>HDB_ASSERT(</span><span>false</span><span>,</span><span> "PDE0 as PTE is not implemented yet"</span><span>)</span><span>; }</span></span>
<span></span>
<span><span> // page_table_block_size is the number of 2MiB regions covered by a PTB</span></span>
<span><span> // If we set it to 0, then PTB cover 2 MiB</span></span>
<span><span> // If it's 9 PTB cover 1024 MiB</span></span>
<span><span> // pde0_block_fragment_size tells us how many 4 KiB regions each PTE covers</span></span>
<span><span> // If it's 0 PTEs cover 4 KiB</span></span>
<span><span> // If it's 9 PTEs cover 2 MiB</span></span>
<span><span> // So the number of PTEs in a PTB is 2^(9+ptbs-pbfs)</span></span>
<span><span> //</span></span>
<span><span> // size here is actually the log_2 of the size</span></span>
<span><span> u32 pte_page_size  </span><span>=</span><span> curr_pde</span><span>.</span><span>frag_size;</span></span>
<span><span> u32 ptes_per_ptb   </span><span>=</span><span> 9</span><span> +</span><span> ptb_size </span><span>-</span><span> pte_page_size;</span></span>
<span><span> u64 pte_index_mask </span><span>=</span><span> (</span><span>1</span><span>ul</span><span> &lt;&lt;</span><span> ptes_per_ptb) </span><span>-</span><span> 1</span><span>;</span></span>
<span></span>
<span><span> u32 pte_bits_count   </span><span>=</span><span> pte_page_size </span><span>+</span><span> 12</span><span>;</span></span>
<span><span> u64 page_offset_mask </span><span>=</span><span> (</span><span>1</span><span>ul</span><span> &lt;&lt;</span><span> pte_bits_count) </span><span>-</span><span> 1</span><span>;</span><span> // minimum of 12</span></span>
<span></span>
<span><span> u64 pte_index </span><span>=</span><span> (va_addr </span><span>&gt;&gt;</span><span> pte_bits_count) </span><span>&amp;</span><span> pte_index_mask;</span></span>
<span><span> u64 pte_addr  </span><span>=</span><span> curr_pde</span><span>.</span><span>pte_base_addr </span><span>+</span><span> pte_index </span><span>*</span><span> 8</span><span>;</span></span>
<span></span>
<span><span> pte_t</span><span> pte </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> if</span><span> (</span><span>!</span><span>curr_pde</span><span>.</span><span>system) {</span></span>
<span><span>  dev_linear_vram(dev</span><span>,</span><span> pte_addr</span><span>,</span><span> 8</span><span>,</span><span> &amp;</span><span>entry_bits)</span><span>;</span></span>
<span><span>  pte </span><span>=</span><span> decode_pte(entry_bits)</span><span>;</span></span>
<span></span>
<span><span>  printf(</span><span>"\tPage Table Entry: 0x</span><span>%lx</span><span>\n"</span><span>,</span><span> pte</span><span>.</span><span>page_base_addr)</span><span>;</span></span>
<span><span> } </span><span>else</span><span> {</span></span>
<span><span>  HDB_ASSERT(</span><span>false</span><span>,</span><span> "GTT physical memory access is not implemented yet"</span><span>)</span><span>;</span></span>
<span><span> }</span></span>
<span></span>
<span><span> if</span><span> (</span><span>pte</span><span>.</span><span>further) { </span><span>HDB_ASSERT(</span><span>false</span><span>,</span><span> "PTE as PDE walking is not implemented yet"</span><span>)</span><span>; }</span></span>
<span><span> if</span><span> (</span><span>!</span><span>pte</span><span>.</span><span>system) </span><span>pte</span><span>.</span><span>page_base_addr </span><span>-=</span><span> fb_offset;</span></span>
<span></span>
<span><span> u64 offset_in_page </span><span>=</span><span> va_addr </span><span>&amp;</span><span> page_offset_mask;</span></span>
<span><span> u64 physical_addr  </span><span>=</span><span> pte</span><span>.</span><span>page_base_addr </span><span>+</span><span> offset_in_page;</span></span>
<span><span> printf(</span><span>"\tFinal Physical Address: 0x</span><span>%lx</span><span>\n"</span><span>,</span><span> physical_addr)</span><span>;</span></span>
<span><span>}</span></span></code></pre>
<section data-footnotes="">
<ol>
<li id="user-content-fn-1">
<p>Other processes need to have a s_trap instruction or have trap on exception flags set, which is not true for most normal GPU processes. <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-2">
<p>Available since RDNA3, if I’m not mistaken. <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
<li id="user-content-fn-3">
<p>VGPRs are unique per thread, and SGPRs are unique per wave <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3">↩</a></p>
</li>
<li id="user-content-fn-4">
<p>We can get that by subtracting the current program counter from the address of the code buffer. <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 4">↩</a></p>
</li>
</ol>
</section>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tsunami warning issued after 7.6-magnitude earthquake strikes Japan (133 pts)]]></title>
            <link>https://earthquake.usgs.gov/earthquakes/map/?currentFeatureId=us6000rtdt&amp;extent=-5.61599,111.26953&amp;extent=70.40735,173.14453</link>
            <guid>46193413</guid>
            <pubDate>Mon, 08 Dec 2025 15:33:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://earthquake.usgs.gov/earthquakes/map/?currentFeatureId=us6000rtdt&#x26;extent=-5.61599,111.26953&#x26;extent=70.40735,173.14453">https://earthquake.usgs.gov/earthquakes/map/?currentFeatureId=us6000rtdt&#x26;extent=-5.61599,111.26953&#x26;extent=70.40735,173.14453</a>, See on <a href="https://news.ycombinator.com/item?id=46193413">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>
          The Latest Earthquakes application supports most recent browsers,
          <a href="https://angular.io/guide/browser-support" target="_blank">view supported browsers</a>.
        </p>
        <p>
          If the application does not load, try our
          <a href="https://earthquake.usgs.gov/legacy/map/" target="_blank"> legacy Latest Earthquakes application</a>.
        </p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I successfully recreated the 1996 Space Jam website with Claude (106 pts)]]></title>
            <link>https://theahura.substack.com/p/i-successfully-recreated-the-1996</link>
            <guid>46193412</guid>
            <pubDate>Mon, 08 Dec 2025 15:33:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theahura.substack.com/p/i-successfully-recreated-the-1996">https://theahura.substack.com/p/i-successfully-recreated-the-1996</a>, See on <a href="https://news.ycombinator.com/item?id=46193412">Hacker News</a></p>
Couldn't get https://theahura.substack.com/p/i-successfully-recreated-the-1996: Error: getaddrinfo ENOTFOUND 12gramsofcarbon.com]]></description>
        </item>
        <item>
            <title><![CDATA[7.6 earthquake off the coast of Japan (136 pts)]]></title>
            <link>https://www.data.jma.go.jp/multi/quake/quake_detail.html?eventID=20251208232600&amp;lang=en</link>
            <guid>46193035</guid>
            <pubDate>Mon, 08 Dec 2025 15:05:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.data.jma.go.jp/multi/quake/quake_detail.html?eventID=20251208232600&#x26;lang=en">https://www.data.jma.go.jp/multi/quake/quake_detail.html?eventID=20251208232600&#x26;lang=en</a>, See on <a href="https://news.ycombinator.com/item?id=46193035">Hacker News</a></p>
<div id="readability-page-1" class="page">
<div>

<header>
<div>
<h2><img id="icon_jma" src="https://www.data.jma.go.jp/multi/images/common/logo.gif"></h2>
<div>

<form>
  
</form>
</div>
</div>
</header>

<div id="globalMenuS">

<nav id="global-nav">
<ul>
<li><a id="tab_home" href="https://www.data.jma.go.jp/multi/index.html"></a></li>
<li><a id="tab_warn" href="https://www.data.jma.go.jp/multi/warn/index.html"></a></li>
<li><a id="tab_yoho" href="https://www.data.jma.go.jp/multi/yoho/index.html"></a></li>
<li><a id="tab_cyclon" href="https://www.data.jma.go.jp/multi/cyclone/index.html"></a></li>
<li><a id="tab_cloud" target="_blank" href="#"></a></li>
<li><a id="tab_kotan" target="_blank" href="#"></a></li>
<li><a id="tab_sub_landslide" target="_blank" href="#"></a>
</li><li><a id="tab_sub_inundation" target="_blank" href="#"></a>
</li><li><a id="tab_sub_flood" target="_blank" href="#"></a>
</li><li><a id="tab_tsunami" href="https://www.data.jma.go.jp/multi/tsunami/index.html"></a></li>
<li><a id="tab_quake" href="https://www.data.jma.go.jp/multi/quake/index.html"></a></li>
<li><a id="tab_volcano" href="https://www.data.jma.go.jp/multi/volcano/index.html"></a></li>
</ul>
</nav>
</div><!-- globalMenuS end-->

<main>
<div>

<div>
<ul>
<li><a id="breadcrumb_home" href="https://www.data.jma.go.jp/multi/index.html"></a></li>
<li><a id="breadcrumb_list" href="https://www.data.jma.go.jp/multi/quake/index.html"></a></li>
<li id="breadcrumb_detail"></li>
</ul>
</div>

<section>



<div>
<ul id="list_legend">
<li id="list_si"></li>
<li id="list_si7"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level07.png" alt="震度7"></li>
<li id="list_si6p"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level06_p.png" alt="震度6強"></li>
<li id="list_si6m"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level06_m.png" alt="震度6弱"></li>
<li id="list_si5p"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level05_p.png" alt="震度5強"></li>
<li id="list_si5m"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level05_m.png" alt="震度5弱"></li>
<li id="list_si4"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level04.png" alt="震度4"></li>
<li id="list_si3"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level03.png" alt="震度3"></li>
<li id="list_si2"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level02.png" alt="震度2"></li>
<li id="list_si1"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level01.png" alt="震度1"></li>
<li id="list_epicenter"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_center.png" alt="震央"></li>
</ul>
</div>

<article>

<table id="quakeindex_table">
	<tbody>
		<tr>
			<th id="quakeinfotable_col01" scope="col"></th>
			<th id="quakeinfotable_col02" scope="col"></th>
			<th id="quakeinfotable_col03" scope="col"></th>
			<th id="quakeinfotable_col04" scope="col"></th>
			<th id="quakeinfotable_col05" scope="col"></th>
			<th id="quakeinfotable_col06" scope="col"></th>
		</tr>
		</tbody>
</table>



<table id="quakedetail_index">
	<tbody>
		<tr>
			<th id="quakedetailtable_col01" scope="col">都道府県</th>
			<th id="quakedetailtable_col02" scope="col">震度</th>
			<th id="quakedetailtable_col03" scope="col">市町村名</th>
		</tr>
	</tbody>
</table>

</article>




</section>

</div>
</main>

<div id="pagetop"><p><a href="#top">topへ</a></p></div>
<center>この地図は、国土地理院長の承認を得て、同院発行の電子地図（タイル）を複製したものである。（承認番号　令元情複、第462号）</center>


<!-- /footer -->








</div>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Uber starts selling ride/eats data to marketers (179 pts)]]></title>
            <link>https://www.businessinsider.com/uber-ads-launches-intelligence-insights-trips-takeout-data-marketers-2025-12</link>
            <guid>46192962</guid>
            <pubDate>Mon, 08 Dec 2025 15:00:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/uber-ads-launches-intelligence-insights-trips-takeout-data-marketers-2025-12">https://www.businessinsider.com/uber-ads-launches-intelligence-insights-trips-takeout-data-marketers-2025-12</a>, See on <a href="https://news.ycombinator.com/item?id=46192962">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-content-container="">

  
    
  
    

  <section>
    
    
    
    
      <section id="post-body" data-component-type="post-body" data-load-strategy="exclude" data-lock-content="">
            
            
            
            <div data-component-type="post-hero" data-load-strategy="exclude">
                
                <figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                    <div>
                      <meta itemprop="contentUrl" content="https://i.insider.com/6936854671107c9f34577f3f?width=700">
                      <p><img src="https://i.insider.com/6936854671107c9f34577f3f?width=700" srcset="https://i.insider.com/6936854671107c9f34577f3f?width=400&amp;format=jpeg&amp;auto=webp 400w, https://i.insider.com/6936854671107c9f34577f3f?width=500&amp;format=jpeg&amp;auto=webp 500w, https://i.insider.com/6936854671107c9f34577f3f?width=700&amp;format=jpeg&amp;auto=webp 700w, https://i.insider.com/6936854671107c9f34577f3f?width=1000&amp;format=jpeg&amp;auto=webp 1000w, https://i.insider.com/6936854671107c9f34577f3f?width=1300&amp;format=jpeg&amp;auto=webp 1300w, https://i.insider.com/6936854671107c9f34577f3f?width=2000&amp;format=jpeg&amp;auto=webp 2000w" sizes="(min-width: 1280px) 900px" alt="uber" decoding="sync">
                    </p></div>
                
                  <span>
                        <span>
                          
                          <label for="caption-drawer-btn">
                            <svg role="img" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 24 24">
                              <path fill="currentColor" fill-rule="evenodd" d="m4.56 18.5 7.486-7.72 7.394 7.626 2.56-2.64L12.046 5.5 2 15.86l2.56 2.64Z"></path>
                            </svg>        </label>
                  
                          <figcaption data-e2e-name="image-caption">
                            <span>Uber offers ads in its app, on in-car screens, and on vehicles.</span>
                            <span>
                              <span data-e2e-name="image-source" itemprop="creditText">Artur Widak/NurPhoto via Getty Images</span>          </span>
                          </figcaption>
                        </span>
                  </span></figure>
            </div>
    
    
    
              
      
            
      
              
              
              
              <div data-component-type="post-summary-bullets" data-load-strategy="exclude" data-track-marfeel="post-summary-bullets">
                <ul>
                    <li>Uber Advertising is launching an insights platform for marketers called Uber Intelligence.</li>
                    <li>It has partnered with LiveRamp to aggregate users' data without revealing their identities.</li>
                    <li>Uber has said its ad business is on track to generate $1.5 billion in revenue this year.</li>
                </ul>
              </div>
      
            
            
            
            
            <section data-component-type="post-body-content" data-load-strategy="exclude" data-track-content="" data-post-type="story" data-track-marfeel="post-body-content">
            
                <p>Uber wants advertisers to level up their marketing by tapping into data on the millions of rides and deliveries its users order every day.</p><p>The ride-hailing giant's ad division is announcing the launch of a new insights platform called Uber Intelligence on Monday, the company exclusively told Business Insider.</p><p>Launched in partnership with the data-connectivity platform LiveRamp, Uber Intelligence will let advertisers securely combine their customer data with Uber's to help surface insights about their audiences, based on what they eat and where they travel.</p><p>It uses LiveRamp's <a target="_self" href="https://www.businessinsider.com/the-adtech-firms-leading-the-charge-in-how-advertising-data-is-used-2021-7#liveramp-6" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">clean room technology</a>, which lets companies aggregate their data in a privacy-safe environment, without sharing or seeing each other's raw or personally identifiable customer information.</p><p>A hotel brand could use Uber Intelligence to help identify which restaurants or entertainment venues it might want to partner with for its loyalty program, for example.</p><p>Uber also hopes the platform can act as a flywheel for its broader ad business. Marketers can use the data clean room for segmentation, such as identifying customers who are heavy business travelers, then targeting them with ads on their next trip to the airport in the Uber app or on screens inside Uber cars.</p><p>"That seamlessness is why we're so excited," Edwin Wong, global head of measurement at Uber Advertising, told Business Insider in an interview. He added that the aim is for marketers to begin saying, "'Oh, I'm not just understanding Uber, I'm understanding Uber in my marketing context.'"</p><h2 id="42ba9d9a-79c8-4251-aa36-0bf02422f1ea" data-toc-id="42ba9d9a-79c8-4251-aa36-0bf02422f1ea">Uber's other route to revenue</h2><p>Uber Intelligence is the latest step in the evolution of <a target="_self" href="https://www.businessinsider.com/uber-ads-plans-1-billion-revenue-2024-investor-presentation-2022-2" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">Uber's ad business</a>. Uber officially launched its dedicated advertising division in 2022. It offers an array of <a target="_self" href="https://www.businessinsider.com/uber-ads-plans-1-billion-revenue-2024-investor-presentation-2022-2" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">ad formats</a> in the Uber and Uber Eats apps, on in-car tablets, in emails to its users, and on car tops.</p><p>The company said in May that its ad business had reached a $1.5 billion revenue run rate — the figure it has projected to hit by the end of 2025 — which would represent a 60% increase on last year. The company doesn't break out a more specific ad-revenue figure and hasn't provided an update on the run-rate number since May.</p><p>Uber Intelligence forms part of a bespoke set of services it offers its top advertisers. Earlier this year, it launched a <a target="_self" href="https://www.businessinsider.com/uber-offers-ads-let-brands-pay-for-users-rides-2025-6" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">creative studio </a>where brands can partner with Uber to deliver more bespoke campaigns, such as offering rides to <a target="_self" rel="" href="https://www.businessinsider.com/miami-grand-prix-distinct-features-f1-race-marina-2024-5" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}"><u>Miami F1 Grand Prix attendees</u></a> in a luxury vehicle sponsored by La Mer, packed with freebie skincare products.</p><p>Andrew Frank, analyst at the research firm Gartner, said the launch of Uber Intelligence is another signal that Uber's ad business is maturing.</p><p>"Early-stage ad businesses tend to focus exclusively on selling inventory while more mature ones focus more on delivering differentiated value through targeting and measurement solutions that help brands understand and optimize the impact of their spend," Frank told Business Insider.</p><p>Uber's unique source of "terrestrial data" put it in good standing against the likes of Amazon, Google, and other retail media networks that emphasize the value of their data-driven insights, Frank added. However, he said Uber may need to address privacy concerns related to aggregating highly sensitive data in order to maintain consumer trust and to comply with evolving global regulators as a collector of first-party data.</p><p>Vihan Sharma, chief revenue officer of LiveRamp, said its platform provides technical guarantees to ensure "zero movement of data."</p><p>"The whole objective of a clean room technology is to build trust between data owners and consumers and the advertising ecosystem," Sharma said.</p>
            
            
            </section>
            
            
            
            
            
            
    
    
    
    
      </section>

    
    <!-- Included desktop "post-aside" -->  

    
      
      <section data-component-type="post-bottom" data-load-strategy="exclude" data-track-marfeel="post-bottom">
        <section>
    
    
    
          
          
          
          <div data-component-type="post-category-tags" data-load-strategy="lazy" data-track-marfeel="post-category-tags">
            <ul data-track-click-shared="{&quot;product_field&quot;:&quot;bi_value_unassigned&quot;,&quot;event&quot;:&quot;navigation&quot;,&quot;element_name&quot;:&quot;category_link&quot;}">
                
                <li>
                  <a data-track-click="" href="https://www.businessinsider.com/category/uber" title="Uber">Uber</a>
                </li>      
                <li>
                  <a data-track-click="" href="https://www.businessinsider.com/category/exclusive" title="Exclusive">Exclusive</a>
                </li>
          
            </ul>
          </div>
    
            
              
              
              <section data-component-type="dad-related-posts" data-delay-third-party-scripts="true" data-size="4" data-min-size="3" data-container-index="" data-included-verticals="advertising" data-placement="post-bottom" data-track-marfeel="dad-related-posts-post-bottom" data-excluded-verticals="bi-video" data-root-margin="250px 0px" data-track-view="{&quot;element_name&quot;:&quot;end_of_article_recirc&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;,&quot;subscription_experience&quot;:&quot;bi_value_unassigned&quot;}">
                  <p>
                    <h2>
                      Read next
                    </h2>
                  </p>
            
                
              </section>
        </section>
    
        
    
          <section data-track-page-area="Post Bottom">
          <!-- Included desktop "taboola" -->    <vendor-taboola data-component-type="vendor-taboola" data-root-margin="0px 0px 100% 0px" data-consent="MARKETING" config="{&quot;providerName&quot;:&quot;taboola&quot;,&quot;providerPageType&quot;:{&quot;article&quot;:&quot;auto&quot;},&quot;providerUrl&quot;:&quot;//cdn.taboola.com/libtrc/businessinsider/loader.js&quot;,&quot;providerFlushValue&quot;:{&quot;flush&quot;:true},&quot;providerData&quot;:{&quot;mode&quot;:&quot;thumbs-1r&quot;,&quot;container&quot;:&quot;taboola-below-main-column&quot;,&quot;placement&quot;:&quot;below-main-column&quot;,&quot;onlyOn&quot;:&quot;desktop&quot;,&quot;target_type&quot;:&quot;mix&quot;}}" data-load-strategy="defer">
                
              </vendor-taboola>
          
          <!-- Excluded mobile "taboola" --></section>
            
            
      </section>
  </section>

  
  


  <back-to-home data-component-type="back-to-home" data-load-strategy="defer" data-only-on="mobile">
  
    
  
    
  </back-to-home></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Strong earthquake hits northern Japan, tsunami warning issued (125 pts)]]></title>
            <link>https://www3.nhk.or.jp/nhkworld/en/news/20251209_02/</link>
            <guid>46192846</guid>
            <pubDate>Mon, 08 Dec 2025 14:50:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www3.nhk.or.jp/nhkworld/en/news/20251209_02/">https://www3.nhk.or.jp/nhkworld/en/news/20251209_02/</a>, See on <a href="https://news.ycombinator.com/item?id=46192846">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

      
                    
                  <p>A strong earthquake has struck northern Japan. The quake struck off the eastern coast of Aomori Prefecture at 11:15 p.m. on Monday. Its focus was 54 kilometers deep, and the magnitude is estimated at 7.5.</p>
      
                    
                <h3>Strong tremors felt across the region</h3>
      
                    
                  <p>The Japan Meteorological Agency has downgraded the magnitude of the quake centered off the Pacific coast in Aomori Prefecture to 7.5 from 7.6.</p>
        
                    
                  <p>The tremor struck at 11:15 p.m. on Monday. The depth has also been adjusted to 54 kilometers, from an initial estimate of 50 kilometers.</p>
        
                    
                  <p>Tremors with an intensity of upper 6 on the Japanese intensity scale of 0 to 7 were observed in the city of Hachinohe in Aomori Prefecture.</p>
                                              
                                          
                  <p><em>A hotel employee in Hachinohe City said: </em>It seems there are multiple injured people. Everyone appears to be conscious.</p>
        
                    
                  <p>If you are in these areas, try to remain in a safe place and protect yourself. Use extreme caution if you must move.  Damage around you could be heavy. Stay alert. More tremors are possible</p>
      
                    
                <h3>Tsunami warnings downgraded to advisories</h3>
      
                    
                  <p>The Meteorological Agency issued a tsunami warning for the Pacific coastline in northern Japan, including Iwate Prefecture, and parts of Hokkaido and Aomori prefectures late Monday night. But the agency has downgraded tsunami warnings to advisories for coastal areas in the Hokkaido and Tohoku regions. Officials say people should still stay away from bodies of water.</p>
                                              
                                          
                  <p><em>The agency says: </em>it is the first time the agency has issued a tsunami warning since July, when a powerful quake off Kamchatka, Russia, prompted it to issue one for Japan's Pacific coastal areas.</p>
        
                    
                  <p>Tsunami advisories are in place for parts of Hokkaido and Aomori Prefecture, Iwate, Miyagi and Fukushima prefectures.</p>
        
                    
                  <p>The agency is calling on people to stay away from the coastline, as well as the mouths of rivers.</p>
        
                    
                  <p>According to authorities, long-period ground motions were recorded during the Monday earthquake.</p>
        
                    
                  <p>Long-period ground motions are slow, large-amplitude seismic waves with frequencies of 2 seconds or longer that occur during a large earthquake. Such motions are known to have a significant impact on high-rise buildings.</p>
        
                    
                  <p>Strong long-period motions, classified class-3, the second highest in the 4-level scale were observed in the village of Rokkasho in Aomori Prefecture. Such class-3 waves are strong enough to make it difficult for people in a high-rise building to stand up.</p>
      
                    
                <h3>Residents ordered to evacuate</h3>
      
                    
                  <p>After tsunami warnings were issued, some municipalities in Hokkaido, and the Tohoku region issued evacuation orders to residents.</p>
      
                    
                <h3>Traffic disrupted</h3>
      
                    
                  <p>East Japan Railway Company says that as of Tuesday, outbound trains on the Tohoku Shinkansen have been suspended between Fukushima and Shin-Aomori stations due to the earthquake. The company says three trains stopped in this section.</p>
        
                    
                  <p>The company says that it is checking for any damage to railway tracks and that it remains unclear when services will resume.</p>
        
                    
                  <p>The Morioka branch of East Japan Railway says that as of midnight on Tuesday, services on the Tohoku Main Line were suspended in Iwate Prefecture.</p>
        
                    
                  <p>It says two trains made emergency stops. It remains unclear when services will resume. There are no reports of injuries.</p>
        
                    
                  <p>As for Hokkaido, the operator of its busiest airport, New Chitose Airport near Sapporo, says that as of 11:40 p.m. on Monday, it was checking whether there are any abnormalities on two runways.</p>
        
                    
                  <p>Highways have been affected. East Nippon Expressway Company says that as of 11:45 p.m. on Monday, traffic was completely stopped between the Shiraoi and Shinchitose Airport Interchanges and between the Tomakomai Higashi and Numanohata Nishi Interchanges.</p>
      
                    
                <h3>Power Companies: No abnormalities at nuclear plants</h3>
      
                    
                  <p>Tokyo Electric Power Company says it has confirmed that there are no abnormalities at the Fukushima Daiichi and Daini nuclear plants.</p>
        
                    
                  <p>The company says it halted the release of treated and diluted water from the Fukushima Daiichi nuclear power plant at 11:42 pm on Monday, as per predetermined procedures.</p>
        
                    
                  <p>The facility suffered a triple meltdown during the March 2011 earthquake and tsunami.  The water used to cool molten fuel has been mixing with rain and groundwater.</p>
        
                    
                  <p>That has been treated to remove most radioactive substances, except tritium. It's then diluted, reducing levels of tritium to well below the World Health Organization's guidance for drinking water, before it is released into the ocean.</p>
        
                    
                  <p>TEPCO also ordered some employees at the facility to evacuate. There have been no reports so far of injuries at the nuclear power plant.</p>
        
                    
                  <p>Tohoku Electric Power Company says no abnormalities have been detected at the Higashidori nuclear power plant in Aomori Prefecture and the Onagawa plant in Miyagi Prefecture.</p>
        
                    
                  <p>Hokkaido Electric Power Company says no problems have been found at the Tomari nuclear power plant in the prefecture.</p>
      
                    
                <h3>Government bracing for damages</h3>
      
                    
                  <p>The Japanese government set up a task force at the crisis management center in the prime minister's office at 11:16 p.m. on Monday in response to the earthquake.</p>
        
                    
                  <p>Prime Minister Takaichi Sanae entered the prime minister's office shortly after 11:50 p.m.</p>
        
                    
                  <p>She instructed the government to immediately provide information on any tsunami and evacuation orders to the people in an appropriate manner, take thorough measures to prevent harm, such as evacuating residents, and get a grasp of the extent of damage as soon as possible.</p>
                                              
                                          
                  <p><em>Takaichi: </em>The central government will work closely with local governments and make the utmost effort to carry out measures, such as emergency response, including rescue for the affected people.</p>
        
                    
                  <p>Chief Cabinet Secretary Kihara Minoru held a news conference on Tuesday. Kihara said the government continues to assess the extent of the damage.</p>
        
                    
                  <p>He added that the government is devoting all its efforts to disaster prevention measures, with rescue and relief efforts as its top priority, led by the police, fire departments, Self-Defense Forces, and Japan Coast Guard.</p>
        
                    
                  <p>The Japan Meteorological Agency will soon hold a news conference on the earthquake. It is expected to explain what precautions should be taken in quake-hit areas.</p>
      
                    
                <h3>Expert view on the quake</h3>
                                            
                                          
                  <p><em>Sakai Shinichi, professor at the Earthquake Research Institute of the University of Tokyo, says: </em>If this was a shallow earthquake centered in the sea, there is a high possibility that a tsunami has already occurred. People should stay away from the coast. It is important to evacuate and to take measures to stay warm.</p>
                                              
                                          
                  <p><em>Sakai says: </em>The epicenter may be north of the epicenter area of the 2011 Great East Japan Earthquake. This time, the earthquake is believed to have occurred at the plate boundary, so I think it was a slightly larger earthquake. The magnitude could be revised in the future.</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Increases Office 365 and Microsoft 365 License Prices (118 pts)]]></title>
            <link>https://office365itpros.com/2025/12/08/microsoft-365-pricing-increase/</link>
            <guid>46192186</guid>
            <pubDate>Mon, 08 Dec 2025 13:49:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://office365itpros.com/2025/12/08/microsoft-365-pricing-increase/">https://office365itpros.com/2025/12/08/microsoft-365-pricing-increase/</a>, See on <a href="https://news.ycombinator.com/item?id=46192186">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

			<main id="main" role="main">
				
<article id="post-71757">

	<!-- .entry-header -->
	
		
	<div>
		
<div id="ez-toc-container">
<p>Table of Contents</p>
<nav><ul><li><a href="#New_Microsoft_365_Pricing_Goes_into_Effect_on_July_1_2026">New Microsoft 365 Pricing Goes into Effect on July 1, 2026</a></li><li><a href="#Last_Microsoft_365_License_Increase_in_2022">Last Microsoft 365 License Increase in 2022</a></li><li><a href="#Justifying_the_Additional_Cost">Justifying the Additional Cost</a></li><li><a href="#So_Many_New_Features">So Many New Features</a></li><li><a href="#A_Question_of_Value">A Question of Value</a></li></ul></nav></div>
<h2><span id="New_Microsoft_365_Pricing_Goes_into_Effect_on_July_1_2026"></span>New Microsoft 365 Pricing Goes into Effect on July 1, 2026<span></span></h2>



<p>On December 4, 2025, Microsoft <a href="https://www.microsoft.com/en-us/microsoft-365/blog/2025/12/04/advancing-microsoft-365-new-capabilities-and-pricing-update/?WT.mc_id=M365-MVP-9501" target="_blank" rel="noreferrer noopener">announced a range of price increases</a> for Microsoft 365 monthly licenses. The new pricing (Figure 1) goes into effect from July 1, 2026, the start of Microsoft’s FY27 fiscal year.</p>






<div>
<figure><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="732" height="831" data-attachment-id="71758" data-permalink="https://office365itpros.com/2025/12/08/microsoft-365-pricing-increase/microsoft-365-prices-july-1-2026/" data-orig-file="https://i0.wp.com/office365itpros.com/wp-content/uploads/2025/12/Microsoft-365-Prices-July-1-2026.jpg?fit=732%2C831&amp;ssl=1" data-orig-size="732,831" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Microsoft 365 Prices July 1 2026" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/office365itpros.com/wp-content/uploads/2025/12/Microsoft-365-Prices-July-1-2026.jpg?fit=264%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/office365itpros.com/wp-content/uploads/2025/12/Microsoft-365-Prices-July-1-2026.jpg?fit=732%2C831&amp;ssl=1" src="https://i0.wp.com/office365itpros.com/wp-content/uploads/2025/12/Microsoft-365-Prices-July-1-2026.jpg?resize=732%2C831&amp;ssl=1" alt="Microsoft 365 License pricing from July 1, 2026 (source: Microsoft)." srcset="https://i0.wp.com/office365itpros.com/wp-content/uploads/2025/12/Microsoft-365-Prices-July-1-2026.jpg?w=732&amp;ssl=1 732w, https://i0.wp.com/office365itpros.com/wp-content/uploads/2025/12/Microsoft-365-Prices-July-1-2026.jpg?resize=264%2C300&amp;ssl=1 264w" sizes="(max-width: 732px) 100vw, 732px"><figcaption>Figure 1: Microsoft 365 License pricing from July 1, 2026 (source: Microsoft)</figcaption></figure>
</div>


<p>According to Microsoft, they want to “<em>give customers ample time to plan</em>.” However, there’s not much choice for tenants if their operations are embedded in the Microsoft 365 ecosystem, so this is a case of “<em>getting used to new pricing</em>” rather than “<em>having time to consider migrating away from Microsoft 365.</em>” Once you’re embedded in the Microsoft 365 ecosystem, it’s hard to leave.</p>



<p>Some organizations do consider going back to on-premises servers. It’s certainly an option, even to the now available and oddly named <a href="https://learn.microsoft.com/en-us/azure/azure-local/concepts/microsoft-365-local-overview?view=azloc-2511&amp;WT.mc_id=M365-MVP-9501" target="_blank" rel="noreferrer noopener">Microsoft 365 Local</a>, a product that shares precisely nothing but its name with the rest of the Microsoft 365 ecosystem.</p>



<h2><span id="Last_Microsoft_365_License_Increase_in_2022"></span>Last Microsoft 365 License Increase in 2022<span></span></h2>



<p>Microsoft last increased <a href="https://practical365.com/microsoft-increase-prices-for-office-365-microsoft-365-march-2022/" target="_blank" rel="noopener">Microsoft 365 license prices in March 2022</a>. At the time, Microsoft added $3/monthly to Office 365 E3m and E5, and $4/monthly to Microsoft 365 E3. The Microsoft 365 E5 price was left unchanged.</p>



<p>This time round, the monthly increases range from zero (Office 365 E1) to $3 (the big plans used by large enterprises like Office 365 E3 and Microsoft 365 E5). At $2/average across the Microsoft 365 base (around 446 million paid seats based on data provided at <a href="https://office365itpros.com/2025/11/03/office-365-for-it-pros-125/" target="_blank" rel="noreferrer noopener">Microsoft’s FY26 Q1 earnings</a>), the increase could bring in an extra $10.7 billion. The price changes shown in Figure 1 apply to the commercial cloud. Equivalent increases apply to other sectors, such as education and government.</p>



<p>In FY26 Q1, the Microsoft Cloud operated at a <a href="https://view.officeapps.live.com/op/view.aspx?src=https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/TranscriptFY26Q1" target="_blank" rel="noreferrer noopener">healthy 68% operating margin</a>, so it’s not as if Microsoft does not achieve an adequate return from Microsoft 365. However, as noted in the earnings transcript, the operating margin for the Microsoft Cloud is down year-over-year due to “investments in AI.” One interpretation is that the extra $10 billion from the price increases will offset some of the red ink Microsoft is bleeding because of the investments they’re making in datacenter capacity, hardware, and software needed to make Copilot useful,</p>



<h2><span id="Justifying_the_Additional_Cost"></span>Justifying the Additional Cost<span></span></h2>



<p>Just like last time around, Microsoft justifies the increase by pointing to an array of new features and functionality that they’ve delivered. Microsoft 365 E5 customers recently received news that they will soon get Security Copilot, and another announcement revealed that the Microsoft 365 E3 and E5 plans will both <a href="https://techcommunity.microsoft.com/blog/microsoftintuneblog/microsoft-365-adds-advanced-microsoft-intune-solutions-at-scale/4474272?WT.mc_id=M365-MVP-9501" target="_blank" rel="noreferrer noopener">gain functionality from the Microsoft Intune Suite in the coming months</a>.</p>



<p>Plans that don’t get Security Copilot or the Intune Suite must do with new apps like Microsoft Loop, Clipchamp, and Places, all introduced since the 2022 price increase. Good as these apps are, a tenant has to use them to extract value to justify the additional cost,. A welcome change is the addition of Microsoft 365 Defender for Office 365 P1 to Office 365 E3 and Microsoft 365 E3, even if this might provoke further worry about i<a href="https://office365itpros.com/2025/11/25/microsoft-defender-for-office-365-3/" target="_blank" rel="noreferrer noopener">ncurring cost to license shared mailboxes that benefit from Defender functionality</a>.</p>



<h2><span id="So_Many_New_Features"></span>So Many New Features<span></span></h2>



<p>Curiously, the blog highlights the release of 1,100 new features in the last year across “<em>Microsoft 365, Copilot, and SharePoint</em>.” I thought SharePoint was a core part of Microsoft 365, but apparently, it’s so important that SharePoint deserves its own mention. Teams just doesn’t get a mention these days. I also wonder how many of the new features are related to Copilot and are therefore useless to tenants that don’t use Copilot.</p>



<p>By comparison, in 2022, Microsoft claimed <a href="https://www.microsoft.com/en-us/microsoft-365/blog/2021/08/19/new-pricing-for-microsoft-365/" target="_blank" rel="noreferrer noopener">the release of 1,400 new features</a> in communication and collaboration (aka Teams), security and compliance, and AI and automation (not Copilot!). At the time, I asked how many of the updates were useful. The same could be asked now. Quantity of updates pushed out in a never-ending stream is no substitute for usefulness or quality.</p>



<h2><span id="A_Question_of_Value"></span>A Question of Value<span></span></h2>



<p>I’m unsure if any organization can use all the functionality bundled into Microsoft 365. It’s a feature-rich environment with lots to recommend it. I worry about quality of software, the pace of change, the way that Microsoft relentlessly pushes AI at every opportunity, and poor communication about the value of changes at times.</p>



<p>Overall, Microsoft 365 remains very a competitive offering, even if the basic enterprise license is now $312/user/year and the headline E5 license a whopping $720/user/year. Then again, it wasn’t too long ago since a shrink-wrapped copy of Office cost over $300, so perhaps the cost isn’t so bad after all. Either way, I’m sure the increases will cause tenants to devote some time to study their current license mix and allocation to see if any savings are possible (the <a href="https://office365itpros.com/2024/09/12/microsoft-365-licensing-report-194/" target="_blank" rel="noreferrer noopener">Microsoft 365 licensing report script</a> might be useful here).</p>







<hr>







<p>Support the work of the Office 365 for IT Pros team by subscribing to the <a href="https://o365itpros.gumroad.com/l/O365IT/" target="_blank" rel="noreferrer noopener">Office 365 for IT Pros</a> eBook. Your support pays for the time we need to track, analyze, and document the changing world of Microsoft 365 and Office 365. Only humans contribute to our work!</p>
			
	</div><!-- .entry-content -->
	
		
	<!-- .entry-footer -->
</article><!-- #post-## -->
				
				
				
				
<!-- #comments -->

			</main><!-- #main -->
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IBM to Acquire Confluent (215 pts)]]></title>
            <link>https://www.confluent.io/blog/ibm-to-acquire-confluent/</link>
            <guid>46192130</guid>
            <pubDate>Mon, 08 Dec 2025 13:43:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.confluent.io/blog/ibm-to-acquire-confluent/">https://www.confluent.io/blog/ibm-to-acquire-confluent/</a>, See on <a href="https://news.ycombinator.com/item?id=46192130">Hacker News</a></p>
Couldn't get https://www.confluent.io/blog/ibm-to-acquire-confluent/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Flow: Actor-based language for C++, used by FoundationDB (118 pts)]]></title>
            <link>https://github.com/apple/foundationdb/tree/main/flow</link>
            <guid>46191763</guid>
            <pubDate>Mon, 08 Dec 2025 13:08:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/apple/foundationdb/tree/main/flow">https://github.com/apple/foundationdb/tree/main/flow</a>, See on <a href="https://news.ycombinator.com/item?id=46191763">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <nav aria-label="Global">
              <ul>
                  <li>
      

      <div>
        <div>
            <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_platform_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>
          GitHub Copilot

        </p><p>

        Write better code with AI
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_spark&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_spark_link_platform_navbar&quot;}" href="https://github.com/features/spark">
      
      <div>
        <p>
          GitHub Spark

            <span>
              New
            </span>
        </p><p>

        Build and deploy intelligent apps
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_platform_navbar&quot;}" href="https://github.com/features/models">
      
      <div>
        <p>
          GitHub Models

            <span>
              New
            </span>
        </p><p>

        Manage and compare prompts
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_platform_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
        <p>
          GitHub Advanced Security

        </p><p>

        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_platform_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>
          Actions

        </p><p>

        Automate any workflow
      </p></div>

    
</a></li>

                  </ul>
                </div>
            <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_platform_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>
          Codespaces

        </p><p>

        Instant dev environments
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_platform_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>
          Issues

        </p><p>

        Plan and track work
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_platform_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>
          Code Review

        </p><p>

        Manage code changes
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_platform_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>
          Discussions

        </p><p>

        Collaborate outside of code
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_platform_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
        <p>
          Code Search

        </p><p>

        Find more, search less
      </p></div>

    
</a></li>

                  </ul>
                </div>
            
        </div>

          <p>
            <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;view_all_features&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;view_all_features_link_platform_navbar&quot;}" href="https://github.com/features">
              View all features
              
</a>          </p>
      </div>
</li>


                  <li>
      

      
</li>


                  <li>
      

      <div>

                      <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                      <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://github.com/resources/events">
      Events &amp; Webinars

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://github.com/partners">
      Partners

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                  </ul>
                </div>
</li>


                  <li>
      

      <div>
                <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>
          GitHub Sponsors

        </p><p>

        Fund open source developers
      </p></div>

    
</a></li>

                  </ul>
                </div>
                <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
        <p>
          The ReadME Project

        </p><p>

        GitHub community articles
      </p></div>

    
</a></li>

                  </ul>
                </div>
                
            </div>
</li>


                  <li>
      

      <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>
          Enterprise platform

        </p><p>

        AI-powered developer platform
      </p></div>

    
</a></li>

                  </ul>
                </div>
</li>


                  <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;platform&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;platform_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

              </ul>
            </nav>

        <div>
                


<qbsearch-input data-scope="repo:apple/foundationdb" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="VzGtbcoHR5NVfM3UEW4NeMWaGPCfTmFALHa-mZNBibPV9WIuNfySNvIrjewVzaPvAENZYtp5GPvO0CyO8Iqddg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="apple/foundationdb" data-current-org="apple" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Ffiles%2Fdisambiguate&amp;source=header-repo&amp;source_repo=apple%2Ffoundationdb" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/apple/foundationdb/tree/main/flow&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="adc6c520096aba4b9c312d1bbdb738f44da328c673bf7d9b59e937ae8e6f4a20" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/files/disambiguate;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-b53e156c-6526-4250-a31d-3ddcc5fe46e3" for="icon-button-bc02474f-9988-48de-b047-100fc93266f0" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.bda088c061b17a984ea2.module.css">
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.753d458774a2f782559b.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>


      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bad Dye Job (176 pts)]]></title>
            <link>https://daringfireball.net/2025/12/bad_dye_job</link>
            <guid>46191194</guid>
            <pubDate>Mon, 08 Dec 2025 11:47:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daringfireball.net/2025/12/bad_dye_job">https://daringfireball.net/2025/12/bad_dye_job</a>, See on <a href="https://news.ycombinator.com/item?id=46191194">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="Box">



<p>In <a href="https://daringfireball.net/linked/2025/12/03/alan-dye-leaves-apple-for-meta">my post earlier today</a> on the then-breaking news that Alan Dye has left Apple to join Meta as chief design officer (<a href="https://www.meta.com/media-gallery/executives/">a new title at the company</a><sup id="fnr1-2025-12-03"><a href="#fn1-2025-12-03">1</a></sup>), I wrote:</p>

<blockquote>
  <p>It sounds like Dye chose to jump ship, and wasn’t squeezed out (as
it seems <a href="https://daringfireball.net/linked/2025/12/01/giannandrea-out">with former AI chief John Giannandrea</a> earlier this
week). Gurman/Bloomberg are spinning this like a coup for Meta
(headline: “<a href="https://www.bloomberg.com/news/articles/2025-12-03/apple-design-executive-alan-dye-poached-by-meta-in-major-coup">Apple Design Executive Alan Dye Poached by Meta in
Major Coup</a>”), but I think this is the best personnel news at Apple
in decades. Dye’s decade-long stint running Apple’s software
design team has been, on the whole, terrible — and rather than
getting better, the problems have been getting worse.</p>
</blockquote>

<p>Dye’s replacement at Apple is longtime Apple designer Stephen Lemay. I’ve never met Lemay (or at least can’t recall meeting him), and prior to today never heard much about him. But that’s typical for Apple employees. Part of the job working for Apple is remaining under the radar and out of the public eye. What I’ve learned today is that Lemay, very much unlike Dye, is a career interface/interaction designer. Sources I’ve spoken to who’ve worked with Lemay at Apple speak highly of him, particularly his attention to detail and craftsmanship. Those things have been sorely lacking in the Dye era. Not everyone loves everything Lemay has worked on, but nobody bats 1.000 and designers love to critique each other’s work. I’ve chatted with people with criticisms of specific things Lemay has worked on or led at Apple (e.g. aspects of iPadOS multitasking that struck many of us as deliberately limiting, rather than empowering), but <em>everyone</em> I’ve spoken to is happy — if not downright giddy — at the news that Lemay is replacing Dye. Lemay is well-liked personally and deeply respected talent-wise. Said one source, in a position to know the choices, “I don’t think there was a better choice than Lemay.”</p>

<p>The sentiment within the ranks at Apple is that today’s news is almost too good to be true. People had given up hope that Dye would ever get squeezed out, and no one expected that he’d just up and leave on his own. (If you care about design, there’s nowhere to go but down after leaving Apple. What people overlooked is the obvious: Alan Dye doesn’t actually care about design.)</p>

<p>What I struggled with in the wake of today’s news is how to square the following contradiction:</p>

<ul>
<li><p>Dye apparently left for Meta on his own; he wasn’t squeezed out.</p></li>
<li><p>Apple replacing Dye with Lemay seemingly signals a significant shift in direction, replacing a guy whose approach was almost entirely superficial/visual with a guy who’s spent his entire career sweating actual interaction details.</p></li>
</ul>

<p>If Apple’s senior leadership would have been happy to have Dye remain as leader of Apple’s software design teams, why didn’t they replace him with a Dye acolyte? Conversely, if the decision makers at Apple saw the need for a directional change, why wasn’t Dye pushed out?<sup id="fnr2-2025-12-03"><a href="#fn2-2025-12-03">2</a></sup></p>

<p>The answer, I think, is that the decision to elevate Lemay wasn’t about direction, but loyalty. Why risk putting in a Dye-aligned replacement when that person might immediately get poached too? We know, from this year’s AI recruitment battles, that Zuckerberg is willing to throw <a href="https://fortune.com/2025/06/18/metas-100-million-signing-bonuses-openai-staff-extreme-ai-talent-war/">almost unfathomable sums of money</a> to poach talent he wants to hire from competitors. Gurman reported that Billy Sorrentino, a Dye deputy who has served as a senior director of design at Apple since 2016, is leaving for Meta with Dye.<sup id="fnr3-2025-12-03"><a href="#fn3-2025-12-03">3</a></sup> I don’t have any other names, but word on the street is that other members of Dye’s inner circle are leaving Apple for Meta with him. But those who remain — or who might remain, if they’d have been offered the promotion to replace Dye — simply can’t be trusted from the perspective of senior leadership, who were apparently blindsided by Dye’s departure for Meta. They wouldn’t have given Dye a prime spot in the WWDC keynote if they thought he might be leaving within months.</p>

<p>So the change in direction we may see — that many of us desperately <em>hope</em> to see — under Lemay’s leadership might be happenstance. More a factor of Lemay being politically safe, as someone predating Dye and outside Dye’s inner circle at Apple, than from Tim Cook or anyone else in senior leadership seeing a <em>need</em> for a directional change in UI design. But happenstance or not, it could be the best thing to happen to Apple’s HI design in the entire stretch since Steve Jobs’s passing and Scott Forstall’s ouster.</p>

<p>Putting Alan Dye in charge of user interface design was the one big mistake Jony Ive made as Apple’s Chief Design Officer.<sup id="fnr4-2025-12-03"><a href="#fn4-2025-12-03">4</a></sup> Dye had no background in user interface design — he came from a brand and print advertising background. Before joining Apple, <a href="https://thenextweb.com/news/how-alan-dye-went-from-iphone-box-designer-to-apples-head-of-ui">he was design director for the fashion brand Kate Spade</a>, and before that worked on branding for the ad agency Ogilvy. His promotion to lead Apple’s software interface design team under Ive happened in 2015, when Apple was launching Apple Watch, their closest foray into the world of fashion. It might have made some sense to bring someone from the fashion/brand world to lead software design for Apple Watch, but it sure didn’t seem to make sense for the rest of Apple’s platforms. And the decade of Dye’s HI leadership has proven it.</p>

<p>The most galling moment in Dye’s entire tenure was <a href="https://www.youtube.com/watch?v=H3KnMyojEQU">the opening of this year’s iPhone event keynote in September</a>, which began with a title card showing the <a href="https://daringfireball.net/linked/2007/01/23/how-it-works">oft-cited Jobs quote</a> “Design is not just what it looks like and feels like. Design is how it works.” The whole problem with the Dye era of HI design at Apple is that it has so largely — not entirely, but largely — been driven purely by how things look. There are a lot of things in Apple’s software — <a href="https://daringfireball.net/linked/2025/11/07/tahoes-terrible-icons">like app icons</a> — that don’t even look good any more. But it’s the “how it works” part that has gone so horribly off the rails. Alan Dye seems like <em>exactly</em> the sort of person Jobs was describing in the first part of that quote: “People think it’s this veneer — that the designers are handed this box and told, ‘Make it look good!’”</p>

<p>I am not a Liquid Glass hater. I actually think, on the whole, iOS 26 is a better and more usable UI than iOS 18. But MacOS 26 Tahoe is a mess, visually, and I’m not sure there’s a single thing about its UI that is better than MacOS 15 Sequoia. There are <a href="https://sixcolors.com/post/2025/09/macos-26-tahoe-review-power-under-glass/">new software features in Tahoe</a> that are excellent and serve as legitimate enticements to upgrade. But I’m talking about the user interface — the work from Alan Dye’s HI team, not Craig Federighi’s teams. I think the fact that Liquid Glass is worse on MacOS than it is on iOS is not just a factor of iOS being Apple’s most popular, most profitable, most important platform — and thus garnering more of Apple’s internal attention. I think it’s also about the fact that the Mac interface, with multiple windows, bigger displays, and more complexity, demands more nuanced, more expert, interaction design skills. Things like depth, layering, and unambiguous indications of input focus are important aspects of any platform. But they’re more important on the platform which, by design, shoulders more complexity. Back in 2010, predicting a bright future for the Mac at a time when many pundits were thinking Apple would soon put the entire platform out to pasture, I wrote, “<a href="https://daringfireball.net/2010/12/future_of_the_mac_in_an_ios_world">It’s the heaviness of the Mac that allows iOS to remain light</a>.” That remains as true today as it was 15 years ago. But Liquid Glass, especially as expressed on MacOS, is a lightweight poorly considered design system as a whole, and its conceptual thinness is not sufficient to properly allow the Mac to carry the weight it needs to bear.</p>

<p>Perhaps more tellingly, there should have been no need for the “<a href="https://daringfireball.net/linked/2025/10/21/ios-26-1-beta-4-liquid-glass-tinted-option">clear/tinted</a>” Liquid Glass preference setting that Apple added in the 26.1 OS releases. Alan Dye wasn’t fired, by all accounts, but that preference setting was as good a sign as any that he should have been. And it’s very much a sign that inside Apple, there’s a strong enough contingent of people who prioritize how things work — like, you know, <a href="https://x.com/Namelongnumbers/status/1996303867735375978">whether you can read text against the background of an alert</a> — to get a setting like this shipped, outside the Accessibility section of Settings.</p>

<p>It remains worrisome that Apple needed to luck into Dye leaving the company. But fortune favors the prepared, and Apple remains prepared by having an inordinate number of longtime talented HI designers at the company. The oddest thing about Alan Dye’s stint leading software design is that there are, effectively, zero design critics who’ve been on his side. The debate regarding Apple’s software design over the last decade isn’t between those on Dye’s side and those against. It’s only a matter of debating how bad it’s been, and how far it’s fallen from its previous remarkable heights. It’s rather extraordinary in today’s hyper-partisan world that there’s nearly universal agreement amongst actual practitioners of user-interface design that Alan Dye is a fraud who led the company deeply astray. It was a big problem inside the company too. I’m aware of dozens of designers who’ve left Apple, out of frustration over the company’s direction, to work at places like LoveFrom, OpenAI, and their secretive joint venture <a href="https://daringfireball.net/linked/2025/05/21/sam-and-jony-io">io</a>. I’m not sure there are any interaction designers at io who aren’t ex-Apple, and if there are, it’s only a handful. From the stories I’m aware of, the theme is identical: these are designers driven to do great work, and under Alan Dye, “doing great work” was no longer the guiding principle at Apple. If reaching the most users is your goal, go work on design at Google, or Microsoft, or Meta. (Design, of course, isn’t even a thing at Amazon.) Designers choose to work at Apple to do the best work in the industry. That has stopped being true under Alan Dye. The most talented designers I know are the harshest critics of Dye’s body of work, and the direction in which it’s been heading.</p>

<p>Back in June, after WWDC, I quoted from Alan Dye’s introduction of Liquid Glass during the keynote, and then quoted from <a href="https://youtu.be/dHrVGk0WwYM?t=381">Steve Jobs’s introduction of Aqua</a> when he unveiled the Mac OS X Public Beta in January 2000. <a href="https://daringfireball.net/2025/06/some_brief_thoughts_and_observations_on_wwdc_2025">I wrote</a>:</p>

<blockquote>
  <p>Re-watching Jobs’s introduction of Aqua for the umpteenth time, I
still find it enthralling. I found Alan Dye’s introduction of
Liquid Glass to be soporific, if not downright horseshitty.</p>
</blockquote>

<p>One of the bits from Jobs’s Aqua introduction I quoted was this:</p>

<blockquote>
  <p>This is what the top of windows look like. These three buttons
look like a traffic signal, don’t they? Red means close the
window. Yellow means minimize the window. And green means maximize
the window. Pretty simple. And tremendous fit and finish in this
operating system. When you roll over these things, you get those.
You see them? And when you are no longer the key window, they go
transparent. So a lot of fit and finish in this.</p>
</blockquote>

<p>After I published that post, I got a note from a designer friend who left Apple, in frustration, a few years ago. After watching Jobs’s Aqua introduction for the first time in years, he told me, “I’m really struck by Steve directly speaking to ‘radio buttons’ and ‘the key window’.” He had the feeling that Dye and his team looked down on interface designers who used terms like Jobs himself once used — in a public keynote, no less. That to Dye’s circle, such terms felt too much like “programmer talk”. But the history of Apple (and NeXT) user interface design is the opposite. Designers and programmers used to — and still should — speak the exact same language about such concepts. Steve Jobs certainly did, and something feels profoundly broken about that disconnect under Alan Dye’s leadership. It’s like the head of cinematography for a movie telling the camera team to stop talking about nerdy shit like “f-stops”. The head of cinematography shouldn’t just abide talking about f-stops and focal lengths, but love it. Said my friend to me, regarding his interactions with Dye and his team at Apple, “I swear I had conversations in which I mentioned ‘key window’ and no one knew what I meant.”</p>

<p>That won’t be a problem with Stephen Lemay. Understanding of fundamental principles will no longer be lacking. Lemay has been at Apple spanning the gamut between the <a href="https://daringfireball.net/search/greg+christie">Greg Christie</a>/<a href="https://daringfireball.net/search/bas+ording">Bas Ording</a> glory days and the current era. At the very least, Lemay running HI should stop the bleeding — both in terms of work quality and talent retention. I sincerely believe things might measurably improve, but I’m more sure that things will stop getting worse. That alone will be a win for everyone — even though the change was seemingly driven by Mark Zuckerberg’s desire to poach Dye, not Tim Cook and Apple’s senior leadership realizing they should have shitcanned him long ago.</p>

<p>Alan Dye is not untalented. But his talents at Apple were in politics. His political skill was so profound that it was <em>his</em> decision to leave, despite the fact that his tenure is considered a disaster by actual designers inside and outside the company. He obviously figured out how to please Apple’s senior leadership. His departure today landed as a total surprise because his stature within the company seemed so secure. And so I think he might do very well at Meta. Not because he can bring world-class interaction design expertise — because he obviously can’t — but because the path to success at Meta has never been driven by design. It’s about getting done what Zuck wants done. Dye might excel at that. Dye was an anchor holding Apple back, but might elevate design at Meta.<sup id="fnr5-2025-12-03"><a href="#fn5-2025-12-03">5</a></sup></p>

<p>My favorite reaction to today’s news is <a href="https://x.com/8hipulin/status/1996318006335401997">this one-liner from a guy on Twitter/X</a>: “The average IQ of both companies has increased.”</p>





 <!-- PreviousNext -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[12 Days of Shell (204 pts)]]></title>
            <link>https://12days.cmdchallenge.com</link>
            <guid>46190577</guid>
            <pubDate>Mon, 08 Dec 2025 10:13:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://12days.cmdchallenge.com">https://12days.cmdchallenge.com</a>, See on <a href="https://news.ycombinator.com/item?id=46190577">Hacker News</a></p>
Couldn't get https://12days.cmdchallenge.com: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The fuck off contact page (411 pts)]]></title>
            <link>https://www.nicchan.me/blog/the-f-off-contact-page/</link>
            <guid>46189994</guid>
            <pubDate>Mon, 08 Dec 2025 08:57:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nicchan.me/blog/the-f-off-contact-page/">https://www.nicchan.me/blog/the-f-off-contact-page/</a>, See on <a href="https://news.ycombinator.com/item?id=46189994">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-7jjqptxk=""> 
<p>Many years ago, I had a client that sold a service. They weren’t a design agency, but for the sake of anonymity, we’ll just call them a design agency. Let us say that their core offering was a full-service design package, but they also made a substantial part of their income from doing smaller tasks related to their primary offering. These kind of services included smaller tasks like one-off campaigns or newsletter designs; tasks that their customers may very well be able to do on their own, but the prospect of freeing up some time by by offloading it to an expert was a tempting offer for many of their customers, and made up a significant chunk of their revenue.</p>
<p>We were hired to do a complete redesign of their site from the ground up. The process went smoothly at first, all the wireframes were approved without issue, but when it came to the design phase, we began to hit walls. For example, they would stumble across sites that they liked and wanted to depart from the agreed-upon wireframes in order to implement a similar design.</p>
<p>The problem was, they were thinking about their inspiration sites from an aesthetic point of view, not from a user experience perspective. Their decisions were coming from a place of ‘we like the balance of imagery and text  in this page’ and not ‘we think this design will achieve the intended goal of the page.’ Now, you know me, I love a good <a href="https://www.smashingmagazine.com/2025/04/gild-just-one-lily/">singular gilded lily</a>, but the client had unwittingly stumbled across a trap, they had fallen in love with what I call a “Fuck off contact page.”</p>
<section><h2 id="what-the-fuck-is-a-fuck-off-contact-page">What the fuck is a ‘fuck off contact page?’</h2><p>A “fuck off contact page” is what a company throws together when they actually don’t want anyone to contact them at all. They are usually found on the websites of million or billion dollar companies, likely Software-as-a-service (SaaS) companies that are trying to reduce the amount of money they spend on support by carefully hiding the real support channels behind login walls. These companies tend to offer multiple tiers of support, with enterprise customers having a customer success manager who they can call on this ancient device we call phones, whereas the lower-paying customers may have to wrangle various in-app ticket mechanisms. If you solve your own problem by reading the knowledge base, then this is a win for the company. They don’t want to hear from you, they want you to fuck off.</p><figure><img alt="Two mobile wireframes. On the left, the wireframe has a large heading that says Contact, and a contact form with two fields, 'Name' and 'How can we help you?' below it. On the right, the mockup has a large heading that says Contact, and three icons with text underneath. In order, they are 'Check out our knowledge base', 'Visit us in person' and 'Reach out to our sales team.'" loading="lazy" decoding="async" sizes="(min-width: 752px) 752px, 100vw" srcset="https://res.cloudinary.com/nicchan/image/upload/w_640,h_611,c_lfill,f_auto/v1765177043/contact 640w,
https://res.cloudinary.com/nicchan/image/upload/w_750,h_716,c_lfill,f_auto/v1765177043/contact 750w,
https://res.cloudinary.com/nicchan/image/upload/w_752,h_718,c_lfill,f_auto/v1765177043/contact 752w,
https://res.cloudinary.com/nicchan/image/upload/w_828,h_791,c_lfill,f_auto/v1765177043/contact 828w,
https://res.cloudinary.com/nicchan/image/upload/w_960,h_917,c_lfill,f_auto/v1765177043/contact 960w,
https://res.cloudinary.com/nicchan/image/upload/w_1080,h_1031,c_lfill,f_auto/v1765177043/contact 1080w,
https://res.cloudinary.com/nicchan/image/upload/w_1280,h_1222,c_lfill,f_auto/v1765177043/contact 1280w,
https://res.cloudinary.com/nicchan/image/upload/w_1504,h_1436,c_lfill,f_auto/v1765177043/contact 1504w" src="https://res.cloudinary.com/nicchan/image/upload/w_752,h_718,c_lfill,f_auto/v1765177043/contact"><figcaption>These are recreated versions of the wireframes that we did for the site, the original contact form version of the page is on the left, and the ‘fuck off contact page’ is on the right. In actuality, the ‘fuck off contact page’ was even more ‘fuck off’ due to the whitespace and a large hero image. This meant the only option that ‘talk to the sales team’, the only option that would put you in touch with a human anytime soon, was at the very bottom of the page, long after some people would stop scrolling.</figcaption></figure><p>In other words, this is entirely inappropriate for the kind of service-based agency that our client was. The billion dollar SaaS company wants to reduce the number of incoming inquiries, and is hoping to weed out anyone who is not determined to contact them by giving them unsatisfying options. The service company wants to show how helpful they are and cultivate leads. These are fundamentally opposing goals.</p><p>Let me explain further. I’m not sure about you, but as a user, when I see a button that says ‘talk to our sales team’, I treat the entire region of the page with the same trepidation as nuclear waste. The page is now a no-go zone, and I try to exit as quickly as possible, knowing that whatever my original query was, I’m going to have to solve it unassisted. Seeing as this is a company who makes money off of convincing people to let them handle the easy stuff, adding friction to this key part of their sales funnel just doesn’t feel like a winning strategy.</p></section>
<section><h2 id="how-the-fuck-did-you-convince-them-to-change-their-minds">How the fuck did you convince them to change their minds?</h2><p>Try as we might, we couldn’t. In all honesty, we probably could have done more in order to talk them out of it, but the project had gone in such a way where we were focused on trying to talk the client out of changing other things that would drastically increase design or development time beyond the initial scope. In other words, we were too busy putting out other fires. This re-designed contact page, as certain as we were of how bad of an idea it was, wasn’t a fire, so we let it through.</p><p>The project finished on time, everyone got paid, and the client was happy with the end result, but I still felt very disappointed in the whole thing. While I personally believe in the value of good design, I also believe there are a lot of smoke-and-mirrors in the industry, and I hated the thought that I might have inadvertently contributed to it. Even if the client is happy, it didn’t meet my internal bar for a quality product worth sticking my name on, and I feel like I’ve let down both the client and the end-users.</p></section>
<section><h2 id="how-the-fuck-do-i-avoid-being-in-a-position-where-im-asked-to-implement-a-fuck-off-contact-page">How the fuck do I avoid being in a position where I’m asked to implement a ‘fuck off contact page’?</h2><p>I think our problems started from before we even began to touch a single design tool. As a favor to one of the folks involved, we had discounted our rates for this client, and I think that set us off on the wrong foot. Instead of seeing us as people who brought valuable knowledge and expertise to the project, they saw us as the hands that would execute their vision.</p><p>Especially for those not familiar with the process of design, it can be tempting to see things like discovery and wireframing as obstacles to be cleared before you get to the fun part, designing the visual identity. Unfortunately, many designers are also guilty of this!</p><p>As service providers, I believe we need to do a better job on educating clients on the design process and why each step is so important. This is radical idea in some circles, but knowing why you’re building something is a necessary part of doing a good job at it! That’s why we do things like determining the architecture before we start thinking about the brand. Flow charts and diagrams are not as fun as interactive prototypes, but they’re much more important to get right.</p><p>Also, the discounted pricing probably didn’t help — instead of signaling that we were doing a favor out of respect for them, it just signaled that we were easily exploitable. There was a lack of trust throughout the process, on both sides. While I really want to believe that I can have the kind of relationships with clients where constructive disagreement is welcomed and valued, how I get there is still something I’m figuring out, even many years later.</p><p>I think that’s part of the reason why I blog. By blogging, I’m putting a body of work out there that communicates my values and ethos. While much of the details of my client work has to remain private, these posts can be public, and hopefully they can help me find people who resonate with what I have to offer. Or you know, just be bold enough to communicate ‘Fuck off’ to those who don’t!</p><p>(Feel free to <a href="https://www.nicchan.me/contact">reach out</a> if you’re interested in working with folks who care, maybe a little too much, about doing right by your users.)</p></section>
<section><h2 id="links-and-resources">Links and Resources</h2><ul>
<li>I hit publish on this as I listened to <a href="https://www.youtube.com/watch?v=a7sh-5UYnJc">Andy Bell’s recent talk at Beyond Tellerand</a>. It has SO many gems of wisdom about the core skills required to deliver successful client projects. It’s also why I love working with <a href="https://set.studio/">Set.Studio</a>, check ‘em out.</li>
</ul></section> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GitHub Actions Has a Package Manager, and It Might Be the Worst (309 pts)]]></title>
            <link>https://nesbitt.io/2025/12/06/github-actions-package-manager.html</link>
            <guid>46189692</guid>
            <pubDate>Mon, 08 Dec 2025 08:15:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nesbitt.io/2025/12/06/github-actions-package-manager.html">https://nesbitt.io/2025/12/06/github-actions-package-manager.html</a>, See on <a href="https://news.ycombinator.com/item?id=46189692">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>After putting together <a href="https://github.com/ecosyste-ms/package-manager-resolvers">ecosyste-ms/package-manager-resolvers</a>, I started wondering what dependency resolution algorithm GitHub Actions uses. When you write <code>uses: actions/checkout@v4</code> in a workflow file, you’re declaring a dependency. GitHub resolves it, downloads it, and executes it. That’s package management. So I went spelunking into the runner codebase to see how it works. What I found was concerning.</p>

<p>Package managers are a critical part of software supply chain security. The industry has spent years hardening them after incidents like left-pad, event-stream, and countless others. Lockfiles, integrity hashes, and dependency visibility aren’t optional extras. They’re the baseline. GitHub Actions ignores all of it.</p>

<p>Compared to mature package ecosystems:</p>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>npm</th>
      <th>Cargo</th>
      <th>NuGet</th>
      <th>Bundler</th>
      <th>Go</th>
      <th>Actions</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Lockfile</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✗</td>
    </tr>
    <tr>
      <td>Transitive pinning</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✗</td>
    </tr>
    <tr>
      <td>Integrity hashes</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✗</td>
    </tr>
    <tr>
      <td>Dependency tree visibility</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✗</td>
    </tr>
    <tr>
      <td>Resolution specification</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✗</td>
    </tr>
  </tbody>
</table>

<p>The core problem is the lack of a lockfile. Every other package manager figured this out decades ago: you declare loose constraints in a manifest, the resolver picks specific versions, and the lockfile records exactly what was chosen. GitHub Actions has no equivalent. Every run re-resolves from your workflow file, and the results can change without any modification to your code.</p>

<p><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/koishybayev">Research from USENIX Security 2022</a> analyzed over 200,000 repositories and found that 99.7% execute externally developed Actions, 97% use Actions from unverified creators, and 18% run Actions with missing security updates. The researchers identified four fundamental security properties that CI/CD systems need: admittance control, execution control, code control, and access to secrets. GitHub Actions fails to provide adequate tooling for any of them. A <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/muralee">follow-up study</a> using static taint analysis found code injection vulnerabilities in over 4,300 workflows across 2.7 million analyzed. Nearly every GitHub Actions user is running third-party code with no verification, no lockfile, and no visibility into what that code depends on.</p>

<p><strong>Mutable versions.</strong> When you pin to <code>actions/checkout@v4</code>, that tag can move. The maintainer can push a new commit and retag. Your workflow changes silently. A lockfile would record the SHA that <code>@v4</code> resolved to, giving you reproducibility while keeping version tags readable. Instead, you have to choose: readable tags with no stability, or unreadable SHAs with no automated update path.</p>

<p>GitHub has added mitigations. <a href="https://docs.github.com/en/code-security/supply-chain-security/understanding-your-software-supply-chain/immutable-releases">Immutable releases</a> lock a release’s git tag after publication. Organizations can enforce SHA pinning as a policy. You can limit workflows to actions from verified creators. These help, but they only address the top-level dependency. They do nothing for transitive dependencies, which is the primary attack vector.</p>

<p><strong>Invisible transitive dependencies.</strong> SHA pinning doesn’t solve this. Composite actions resolve their own dependencies, but you can’t see or control what they pull in. When you pin an action to a SHA, you only lock the outer file. If it internally pulls <code>some-helper@v1</code> with a mutable tag, your workflow is still vulnerable. You have zero visibility into this. A lockfile would record the entire resolved tree, making transitive dependencies visible and pinnable. <a href="https://doi.org/10.1145/3643991.3644899">Research on JavaScript Actions</a> found that 54% contain at least one security weakness, with most vulnerabilities coming from indirect dependencies. The <a href="https://unit42.paloaltonetworks.com/github-actions-supply-chain-attack/">tj-actions/changed-files incident</a> showed how this plays out in practice: a compromised action updated its transitive dependencies to exfiltrate secrets. With a lockfile, the unexpected transitive change would have been visible in a diff.</p>

<p><strong>No integrity verification.</strong> npm records <code>integrity</code> hashes in the lockfile. Cargo records checksums in <code>Cargo.lock</code>. When you install, the package manager verifies the download matches what was recorded. Actions has nothing. You trust GitHub to give you the right code for a SHA. A lockfile with integrity hashes would let you verify that what you’re running matches what you resolved.</p>

<p><strong>Re-runs aren’t reproducible.</strong> GitHub staff have <a href="https://github.com/orgs/community/discussions/27083">confirmed this explicitly</a>: “if the workflow uses some actions at a version, if that version was force pushed/updated, we will be fetching the latest version there.” A failed job re-run can silently get different code than the original run. Cache interaction makes it worse: caches only save on successful jobs, so a re-run after a force-push gets different code <em>and</em> has to rebuild the cache. Two sources of non-determinism compounding. A lockfile would make re-runs deterministic: same lockfile, same code, every time.</p>

<p><strong>No dependency tree visibility.</strong> npm has <code>npm ls</code>. Cargo has <code>cargo tree</code>. You can inspect your full dependency graph, find duplicates, trace how a transitive dependency got pulled in. Actions gives you nothing. You can’t see what your workflow actually depends on without manually reading every composite action’s source. A lockfile would be a complete manifest of your dependency tree.</p>

<p><strong>Undocumented resolution semantics.</strong> Every package manager documents how dependency resolution works. npm has a spec. Cargo has a spec. Actions resolution is undocumented. The <a href="https://github.com/actions/runner">runner source is public</a>, and the entire “resolution algorithm” is in <a href="https://github.com/actions/runner/blob/main/src/Runner.Worker/ActionManager.cs">ActionManager.cs</a>. Here’s a simplified version of what it does:</p>

<div><pre><code><span>// Simplified from actions/runner ActionManager.cs</span>
<span>async</span> <span>Task</span> <span>PrepareActionsAsync</span><span>(</span><span>steps</span><span>)</span> <span>{</span>
    <span>// Start fresh every time - no caching</span>
    <span>DeleteDirectory</span><span>(</span><span>"_work/_actions"</span><span>);</span>

    <span>await</span> <span>PrepareActionsRecursiveAsync</span><span>(</span><span>steps</span><span>,</span> <span>depth</span><span>:</span> <span>0</span><span>);</span>
<span>}</span>

<span>async</span> <span>Task</span> <span>PrepareActionsRecursiveAsync</span><span>(</span><span>actions</span><span>,</span> <span>depth</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>depth</span> <span>&gt;</span> <span>10</span><span>)</span>
        <span>throw</span> <span>new</span> <span>Exception</span><span>(</span><span>"Composite action depth exceeded max depth 10"</span><span>);</span>

    <span>foreach</span> <span>(</span><span>var</span> <span>action</span> <span>in</span> <span>actions</span><span>)</span> <span>{</span>
        <span>// Resolution happens on GitHub's server - opaque to us</span>
        <span>var</span> <span>downloadInfo</span> <span>=</span> <span>await</span> <span>GetDownloadInfoFromGitHub</span><span>(</span><span>action</span><span>.</span><span>Reference</span><span>);</span>

        <span>// Download and extract - no integrity verification</span>
        <span>var</span> <span>tarball</span> <span>=</span> <span>await</span> <span>Download</span><span>(</span><span>downloadInfo</span><span>.</span><span>TarballUrl</span><span>);</span>
        <span>Extract</span><span>(</span><span>tarball</span><span>,</span> <span>$"_actions/</span><span>{</span><span>action</span><span>.</span><span>Owner</span><span>}</span><span>/</span><span>{</span><span>action</span><span>.</span><span>Repo</span><span>}</span><span>/</span><span>{</span><span>downloadInfo</span><span>.</span><span>Sha</span><span>}</span><span>"</span><span>);</span>

        <span>// If composite, recurse into its dependencies</span>
        <span>var</span> <span>actionYml</span> <span>=</span> <span>Parse</span><span>(</span><span>$"_actions/</span><span>{</span><span>action</span><span>.</span><span>Owner</span><span>}</span><span>/</span><span>{</span><span>action</span><span>.</span><span>Repo</span><span>}</span><span>/</span><span>{</span><span>downloadInfo</span><span>.</span><span>Sha</span><span>}</span><span>/action.yml"</span><span>);</span>
        <span>if</span> <span>(</span><span>actionYml</span><span>.</span><span>Type</span> <span>==</span> <span>"composite"</span><span>)</span> <span>{</span>
            <span>// These nested actions may use mutable tags - we have no control</span>
            <span>await</span> <span>PrepareActionsRecursiveAsync</span><span>(</span><span>actionYml</span><span>.</span><span>Steps</span><span>,</span> <span>depth</span> <span>+</span> <span>1</span><span>);</span>
        <span>}</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>That’s it. No version constraints, no deduplication (the same action referenced twice gets downloaded twice), no integrity checks. The tarball URL comes from GitHub’s API, and you trust them to return the right content for the SHA. A lockfile wouldn’t fix the missing spec, but it would at least give you a concrete record of what resolution produced.</p>

<p>Even setting lockfiles aside, Actions has other issues that proper package managers solved long ago.</p>

<p><strong>No registry.</strong> Actions live in git repositories. There’s no central index, no security scanning, no malware detection, no typosquatting prevention. A real registry can flag malicious packages, store immutable copies independent of the source, and provide a single point for security response. The Marketplace exists but it’s a thin layer over repository search. Without a registry, there’s nowhere for immutable metadata to live. If an action’s source repository disappears or gets compromised, there’s no fallback.</p>

<p><strong>Shared mutable environment.</strong> Actions aren’t sandboxed from each other. Two actions calling <code>setup-node</code> with different versions mutate the same <code>$PATH</code>. The outcome depends on execution order, not any deterministic resolution.</p>

<p><strong>No offline support.</strong> Actions are pulled from GitHub on every run. There’s no offline installation mode, no vendoring mechanism, no way to run without network access. Other package managers let you vendor dependencies or set up private mirrors. With Actions, if GitHub is down, your CI is down.</p>

<p><strong>The namespace is GitHub usernames.</strong> Anyone who creates a GitHub account owns that namespace for actions. Account takeovers and typosquatting are possible. When a popular action maintainer’s account gets compromised, attackers can push malicious code and retag. A lockfile with integrity hashes wouldn’t prevent account takeovers, but it would detect when the code changes unexpectedly. The hash mismatch would fail the build instead of silently running attacker-controlled code. Another option would be something like Go’s checksum database, a transparent log of known-good hashes that catches when the same version suddenly has different contents.</p>

<h3 id="how-did-we-get-here">How Did We Get Here?</h3>

<p>The Actions runner is forked from Azure DevOps, designed for enterprises with controlled internal task libraries where you trust your pipeline tasks. GitHub bolted a public marketplace onto that foundation without rethinking the trust model. The addition of composite actions and reusable workflows created a dependency system, but the implementation ignored lessons from package management: lockfiles, integrity verification, transitive pinning, dependency visibility.</p>

<p>This matters beyond CI/CD. Trusted publishing is being rolled out across package registries: PyPI, npm, RubyGems, and others now let you publish packages directly from GitHub Actions using OIDC tokens instead of long-lived secrets. OIDC removes one class of attacks (stolen credentials) but amplifies another: the supply chain security of these registries now depends entirely on GitHub Actions, a system that lacks the lockfile and integrity controls these registries themselves require. A compromise in your workflow’s action dependencies can lead to malicious packages on registries with better security practices than the system they’re trusting to publish.</p>

<p>Other CI systems have done better. GitLab CI added an <code>integrity</code> keyword in version 17.9 that lets you specify a SHA256 hash for remote includes. If the hash doesn’t match, the pipeline fails. Their documentation explicitly warns that including remote configs “is similar to pulling a third-party dependency” and recommends pinning to full commit SHAs. GitLab recognized the problem and shipped integrity verification. GitHub closed the feature request.</p>

<p>GitHub’s design choices don’t just affect GitHub users. Forgejo Actions maintains compatibility with GitHub Actions, which means projects migrating to Codeberg for ethical reasons inherit the same broken CI architecture. The Forgejo maintainers <a href="https://codeberg.org/forgejo/discussions/issues/214">openly acknowledge the problems</a>, with contributors calling GitHub Actions’ ecosystem “terribly designed and executed.” But they’re stuck maintaining compatibility with it. Codeberg mirrors common actions to reduce GitHub dependency, but the fundamental issues are baked into the model itself. GitHub’s design flaws are spreading to the alternatives.</p>

<p><a href="https://github.com/actions/runner/issues/2195">GitHub issue #2195</a> requested lockfile support. It was closed as “not planned” in 2022. Palo Alto’s <a href="https://unit42.paloaltonetworks.com/github-actions-supply-chain-vulnerabilities/">“Unpinnable Actions” research</a> documented how even SHA-pinned actions can have unpinnable transitive dependencies.</p>

<p>Dependabot can update action versions, which helps. Some teams vendor actions into their own repos. <a href="https://zizmor.sh/">zizmor</a> is excellent at scanning workflows and finding security issues. But these are workarounds for a system that lacks the basics.</p>

<p>The fix is a lockfile. Record resolved SHAs for every action reference, including transitives. Add integrity hashes. Make the dependency tree inspectable. GitHub closed the request three years ago and hasn’t revisited it.</p>

<hr>

<p><strong>Further reading:</strong></p>

<ul>
  <li><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/koishybayev">Characterizing the Security of GitHub CI Workflows</a> - Koishybayev et al., USENIX Security 2022</li>
  <li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/muralee">ARGUS: A Framework for Staged Static Taint Analysis of GitHub Workflows and Actions</a> - Muralee et al., USENIX Security 2023</li>
  <li><a href="https://www.wiz.io/blog/new-github-action-supply-chain-attack-reviewdog-action-setup">New GitHub Action supply chain attack: reviewdog/action-setup</a> - Wiz Research, 2025</li>
  <li><a href="https://www.paloaltonetworks.com/blog/cloud-security/unpinnable-actions-github-security/">Unpinnable Actions: How Malicious Code Can Sneak into Your GitHub Actions Workflows</a></li>
  <li><a href="https://www.paloaltonetworks.com/blog/cloud-security/github-actions-worm-dependencies/">GitHub Actions Worm: Compromising GitHub Repositories Through the Actions Dependency Tree</a></li>
  <li><a href="https://github.com/actions/setup-python/issues/377">setup-python: Action can be compromised via mutable dependency</a></li>
</ul>

  </div>

  
</article>

      </div></div>]]></description>
        </item>
    </channel>
</rss>