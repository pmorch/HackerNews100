<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 15 Mar 2024 17:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Boeing Whistleblower: "If Anything Happens to Me, It's Not Suicide" (127 pts)]]></title>
            <link>https://twitter.com/WallStreetSilv/status/1768517997285482626</link>
            <guid>39715161</guid>
            <pubDate>Fri, 15 Mar 2024 13:20:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/WallStreetSilv/status/1768517997285482626">https://twitter.com/WallStreetSilv/status/1768517997285482626</a>, See on <a href="https://news.ycombinator.com/item?id=39715161">Hacker News</a></p>
Couldn't get https://twitter.com/WallStreetSilv/status/1768517997285482626: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[IAM Is the Worst (193 pts)]]></title>
            <link>https://matduggan.com/iam-is-the-worst/</link>
            <guid>39714155</guid>
            <pubDate>Fri, 15 Mar 2024 10:55:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matduggan.com/iam-is-the-worst/">https://matduggan.com/iam-is-the-worst/</a>, See on <a href="https://news.ycombinator.com/item?id=39714155">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
<p>Imagine your job was to clean a giant office building. You go from floor to floor, opening doors, collecting trash, getting a vacuum out of the cleaning closet and putting it back. It's a normal job and part of that job is someone gives you a key. The key opens every door everywhere. Everyone understands the key is powerful, but they also understand you need to do your job. </p><p>Then your management hears about someone stealing janitor keys. So they take away your universal key and they say "you need to tell Suzie, our security engineer, which keys you need at which time". But the keys don't just unlock one door, some unlock a lot of doors and some desk drawers, some open the vault (imagine this is the Die Hard building), some don't open any doors but instead turn on the coffee machine. Obviously the keys have titles, but the titles mean nothing. Do you need the "executive_floor/admin" key or the "executive_floor/viewer" key? </p><p>But you are a good employee and understand that security is a part of the job. So you dutifully request the keys you think you need, try to do your job, open a new ticket when the key doesn't open a door you want, try it again, it still doesn't open the door you want so then there's another key. Soon your keyring is massive, just a clanging sound as you walk down the hallway. It mostly works, but a lot of the keys open stuff you don't need, which makes you think maybe this entire thing was pointless. </p><p>The company is growing and we need new janitors, but they don't want to give all the new janitors your key ring. So they roll out a new system which says "now the keys can only open doors that we have written down that this key can open, even if it says "executive_floor/admin". The problem is people move offices all the time, so even if the list of what doors that key opened was true when it was issued, it's not true tomorrow. The Security team and HR share a list, but the list sometimes drifts or maybe someone moves offices without telling the right people. </p><p>Soon nobody is really 100% sure what you can or cannot open, including you. Sure someone can audit it and figure it out, but the risk of removing access means you cannot do your job and the office doesn't get cleaned. So practically speaking the longer someone works as a janitor the more doors they can open until eventually they have the same level of access as your original master key even if that wasn't the intent. </p><p>That's IAM (Identity and access management) in cloud providers today. </p><h3 id="stare-into-madness">Stare Into Madness</h3><figure><img src="https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/PolicyEvaluationHorizontal111621.png" alt="" loading="lazy"><figcaption><span>AWS IAM Approval Flow</span></figcaption></figure><figure><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image1_copy_3.max-2000x2000.jpg" alt="" loading="lazy"><figcaption><span>GCP IAM Approval Flow</span></figcaption></figure><figure><img src="https://images.fastcompany.com/upload/Simple.jpg" alt="It's Not Natural, It's Just Simple: Food Branding Co-Opts Another Mean" loading="lazy"></figure><p>Honestly I don't even know why I'm complaining. Of course it's entirely reasonable to expect anyone working in a cloud environment to understand the dozen+ ways that they may or may not have access to a particular resource. Maybe they have permissions at a folder level, or an org level, but that permission is gated by specific resources. </p><p>Maybe they don't even have access but the tool they're interacting with the resource with has permission to do it, so they can do it but only as long as they are SSH'd into host01, not if they try to do it through some cloud shell. Possibly they had access to it before, but now they don't since they moved teams. Perhaps the members of this team were previously part of some existing group but now new employees aren't added to that group so some parts of the team can access X but others cannot. Or they actually have the correct permissions to the resource but the resource is located in another account and they don't have the right permission to traverse the networking link between the two VPCs.</p><p>Meanwhile someone is staring at these flowcharts trying to figure out what in hell is even happening here. As someone who has had to do this multiple times in my life, let me tell you the real-world workflow that ends up happening. </p><ul><li>Developer wants to launch a new service using new cloud products. They put in a ticket for me to give them access to the correct "roles" to do this. </li><li>I need to look at two elements of it, both what are the permissions the person needs in order to see if the thing is working and then the permissions the service needs in order to complete the task it is trying to complete. </li><li>So I go through my giant list of roles and try to cobble together something that I think based on the names will do what I want. Do you feel like a <code>roles/datastore.viewer</code> or more of a <code>roles/datastore.keyVisualizerViewer</code>? To run backups is <code>roles/datastore.backupsAdmin</code> sufficient or do I need to add <code>roles/datastore.backupSchedulesAdmin</code> in there as well?</li><li>They try it and it doesn't work. Reopen the ticket with "I still get authorizationerror:foo". I switch that role with a different role, try it again. Run it through the simulator, it seems to work, but they report a new different error because actually in order to use service A you need to also have a role in service B. Go into bathroom, scream into the void and return to your terminal.</li><li>We end up cobbling together a custom role that includes all the permissions that this application needs and the remaining 90% of permissions are something it will never ever use but will just sit there as a possible security hole. </li><li>Because /* permissions are the work of Satan, I need to scope it to specific instances of that resource and just hope nobody ever adds a SQS queue without....checking the permissions I guess. In theory we should catch it in the non-prod environments but there's always the chance that someone messes up something at a higher level of permissions that does something in non-prod and doesn't exist in prod so we'll just kinda cross our fingers there. </li></ul><h3 id="gcp-makes-it-worse">GCP Makes It Worse</h3><p>So that's effectively the AWS story, which is terrible but at least it's possible to cobble together something that works and you can audit. Google looked at this and said "what if we could express how much we hate Infrastructure teams as a service?" Expensive coffee robots were engaged, colorful furniture was sat on and the brightest minds of our generation came up with a system so punishing you'd think you did something to offend them personally. </p><p>Google looked at AWS and said "this is a tire fire" as corporations put non-prod and prod environments in the same accounts and then tried to divide them by conditionals. So they came up with a folder structure:</p><figure><img src="https://infosec.rodeo/assets/img/blog/gcp_resource_hierarchy.png" alt="GCP Resource Hierarchy" loading="lazy"></figure><p>The problem is that this design encourages unsafe practices by promoting "groups should be set at the folder level with one of the default basic roles". It makes sense logically at first that you are a viewer, editor or owner. But as GCP adds more services this model breaks down quickly because each one of these encompasses thousands upon thousands of permissions. So additional IAM predefined roles were layered on. </p><p>People were encouraged to move away from the basic roles and towards the predefined roles. There are ServiceAgent roles that were designated for service accounts, aka the permissions you actual application has and then everything else. Then there are 1687 other roles for you to pick from to assign to your groups of users. </p><figure><img src="https://matduggan.com/content/images/2024/03/image-2.png" alt="" loading="lazy" width="860" height="334" srcset="https://matduggan.com/content/images/size/w600/2024/03/image-2.png 600w, https://matduggan.com/content/images/2024/03/image-2.png 860w" sizes="(min-width: 720px) 720px"></figure><p>The problem is none of this is actually best practice. Even when assigning users "small roles", we're still not following the principal of least privilege. Also the roles don't remain static. As new services come online permissions are added to roles. </p><figure><img src="https://matduggan.com/content/images/2024/03/image-4.png" alt="" loading="lazy" width="2000" height="1254" srcset="https://matduggan.com/content/images/size/w600/2024/03/image-4.png 600w, https://matduggan.com/content/images/size/w1000/2024/03/image-4.png 1000w, https://matduggan.com/content/images/size/w1600/2024/03/image-4.png 1600w, https://matduggan.com/content/images/size/w2400/2024/03/image-4.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>The above is an automated process that pulls down all the roles from the gcloud CLI tool and updates them for latest. It is a constant state of flux with roles with daily changes. It gets even more complicated though. </p><p>You also need to check the launch stage of a role. </p><blockquote>Custom roles include a launch stage as part of the role's metadata. The most common launch stages for custom roles are ALPHA, BETA, and GA. These launch stages are informational; they help you keep track of whether each role is ready for widespread use. Another common launch stage is DISABLED. This launch stage lets you disable a custom role.</blockquote><blockquote>We recommend that you use launch stages to convey the following information about the role:</blockquote><blockquote>EAP or ALPHA: The role is still being developed or tested, or it includes permissions for Google Cloud services or features that are not yet public. It is not ready for widespread use.<br>BETA: The role has been tested on a limited basis, or it includes permissions for Google Cloud services or features that are not generally available.<br>GA: The role has been widely tested, and all of its permissions are for Google Cloud services or features that are generally available.<br>DEPRECATED: The role is no longer in use.</blockquote><h3 id="who-cares">Who Cares?</h3><p>Why would anyone care if Google is constantly changing roles? Well it matters because with GCP to make a custom role, you cannot combine predefined roles. Instead you need to go down to the permission level to list out all of the things those roles can do, then feed that list of permissions into the definition of your custom role and push that up to GCP. </p><p>In order to follow best practices this is what you have to do. Otherwise you will always be left with users that have a ton of unused permissions along with the fear of a security breach allowing someone to execute commands in your GCP account through an applications service account that cause way more damage than the actual application justifies. </p><p>So you get to build automated tooling which either queries the predefined roles for change over time and roll those into your custom roles so that you can assign a user or group one specific role that lets them do everything they need. Or you can assign these same folks multiple of the 1600+ predefined roles, accept that they have permissions they don't need and also just internalize that day to day you don't know how much the scope of those permissions have changed. </p><h3 id="the-obvious-solution">The Obvious Solution</h3><p>Why am I ranting about this? Because the solution is so blindly obvious I don't understand why we're not already doing it. It's a solution I've had to build, myself, multiple times and at this point am furious that this keeps being my responsibility as I funnel hundreds of thousands of dollars to cloud providers. </p><p>What is this obvious solution? You, an application developer, need to launch a new service. I give you a service account that lets you do almost everything inside of that account along with a viewer account for your user that lets you go into the web console and see everything. You churn away happily, writing code that uses all those new great services. Meanwhile, we're tracking all the permissions your application and you are using. </p><p>At some time interval, 30 or 90 or whatever days, my tool looks at the permissions your application has used over the last 90 days and says "remove the global permissions and scope it to these". I don't need to ask you what you need, because I can see it. In the same vein I do the same thing with your user or group permissions. You don't need viewer everywhere because I can see what you've looked at. </p><p>Both GCP and AWS support this and have all this functionality baked in. GCP has the <a href="https://cloud.google.com/policy-intelligence/docs/role-recommendations-overview" rel="noreferrer">role recommendations</a> which tracks exactly what I'm talking about and recommends lowering the role. <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_access-advisor.html" rel="noreferrer">AWS tracks the exact same information</a> and can be used to do the exact same thing. </p><p><strong>What if the user needs different permissions in a hurry?</strong></p><p>This is not actually that hard to account for and <em>again</em> is something I and countless others have been forced to make over and over. You can issue expiring permissions in both situations where a user can request a role be temporarily granted to them and then it disappears in 4 hours. I've seen every version of these, from Slack bots to websites, but they're all the same thing. If user is in X group they're allowed to request Y temporary permissions. OR if the user is on-call as determined with an API call to the on-call provider they get more powers. Either design works fine. </p><p><strong>That seems like a giant security hole</strong></p><p>Compared to what? Team A guessing what Team B needs even though they don't ever do the work that Team B does? Some security team receiving a request for permissions and trying to figure out if the request "makes sense" or not? At least this approach is based on actual data and not throwing darts at a list of IAM roles and seeing what "feels right". </p><h3 id="conclusion">Conclusion</h3><p>IAM started out as an easy idea that as more and more services were launched, started to become nightmarish to organize. It's too hard to do the right thing now and it's even harder to do the right thing in GCP compared to AWS. The solution is not complicated. We have all the tools, all the data, we understand how they fit together. We just need one of the providers to be brave enough to say "obviously we messed up and this legacy system you all built your access control on is bad and broken". It'll be horrible, we'll all grumble and moan but in the end it'll be a better world for us all. </p><p>Feedback: <a href="https://c.im/@matdevdug">https://c.im/@matdevdug</a></p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Matrix Multiplication with Half the Multiplications (131 pts)]]></title>
            <link>https://github.com/trevorpogue/algebraic-nnhw</link>
            <guid>39714053</guid>
            <pubDate>Fri, 15 Mar 2024 10:36:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/trevorpogue/algebraic-nnhw">https://github.com/trevorpogue/algebraic-nnhw</a>, See on <a href="https://news.ycombinator.com/item?id=39714053">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">This repository contains the source code for ML hardware architectures that require nearly half the number of multiplier units to achieve the same performance, by executing alternative inner-product algorithms that trade nearly half the multiplications for cheap low-bitwidth additions, while still producing identical output as the conventional inner product. This increases the theoretical throughput and compute efficiency limits of ML accelerators. See the following journal publication for the full details:</p>
<p dir="auto">T. E. Pogue and N. Nicolici, "Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators," in IEEE Transactions on Computers, vol. 73, no. 2, pp. 495-509, Feb. 2024, doi: 10.1109/TC.2023.3334140.</p>

<p dir="auto">Article URL: <a href="https://ieeexplore.ieee.org/document/10323219" rel="nofollow">https://ieeexplore.ieee.org/document/10323219</a></p>
<p dir="auto">Open-access version: <a href="https://arxiv.org/abs/2311.12224" rel="nofollow">https://arxiv.org/abs/2311.12224</a></p>
<p dir="auto">Abstract: We introduce a new algorithm called the Free-pipeline Fast Inner Product (FFIP) and its hardware architecture that improve an under-explored fast inner-product algorithm (FIP) proposed by Winograd in 1968. Unlike the unrelated Winograd minimal filtering algorithms for convolutional layers, FIP is applicable to all machine learning (ML) model layers that can mainly decompose to matrix multiplication, including fully-connected, convolutional, recurrent, and attention/transformer layers. We implement FIP for the first time in an ML accelerator then present our FFIP algorithm and generalized architecture which inherently improve FIP's clock frequency and, as a consequence, throughput for a similar hardware cost. Finally, we contribute ML-specific optimizations for the FIP and FFIP algorithms and architectures. We show that FFIP can be seamlessly incorporated into traditional fixed-point systolic array ML accelerators to achieve the same throughput with half the number of multiply-accumulate (MAC) units, or it can double the maximum systolic array size that can fit onto devices with a fixed hardware budget. Our FFIP implementation for non-sparse ML models with 8 to 16-bit fixed-point inputs achieves higher throughput and compute efficiency than the best-in-class prior solutions on the same type of compute platform.</p>
<p dir="auto">The following diagram shows an overview of the ML accelerator system implemented in this source code:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/12535207/285502293-11a7d485-04a3-4e9d-b9fb-91c35c80086f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8yODU1MDIyOTMtMTFhN2Q0ODUtMDRhMy00ZTlkLWI5ZmItOTFjMzVjODAwODZmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjZTJkM2MyZDdkODdkNDZlODg3NTJkZDcyZmYzNjhhMzJiYmM1NzRlNDk3MjM3YTM3MzMwMjNkODllOTdiZGUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.9a6yVkzLncS7pXRGFoxzDOrjm01sfCz7jQEFtOGf5oE"><img src="https://private-user-images.githubusercontent.com/12535207/285502293-11a7d485-04a3-4e9d-b9fb-91c35c80086f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8yODU1MDIyOTMtMTFhN2Q0ODUtMDRhMy00ZTlkLWI5ZmItOTFjMzVjODAwODZmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjZTJkM2MyZDdkODdkNDZlODg3NTJkZDcyZmYzNjhhMzJiYmM1NzRlNDk3MjM3YTM3MzMwMjNkODllOTdiZGUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.9a6yVkzLncS7pXRGFoxzDOrjm01sfCz7jQEFtOGf5oE" width="450"></a></p>
<p dir="auto">The FIP and FFIP systolic array/MXU processing elements (PE)s shown below in (b) and (c) implement the FIP and FFIP inner-product algorithms and each individually provide the same effective computational power as the two baseline PEs shown in (a) combined which implement the baseline inner product as in previous systolic-array ML accelerators:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/12535207/300184475-d9b956a2-25fa-4173-8ba9-8fd27d02f0c1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8zMDAxODQ0NzUtZDliOTU2YTItMjVmYS00MTczLThiYTktOGZkMjdkMDJmMGMxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRkMWY4NjIyODBkNDAwNjExNzZkZTJiMmJiZDk0YTRlY2I1Zjk0MDE1YjZlM2UyMGNiMzIyYzA1NjQyZGY0OTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.433P7aTz05yurSzH_kAJDMyMGRZZAi1IUDxKuesO_zU"><img src="https://private-user-images.githubusercontent.com/12535207/300184475-d9b956a2-25fa-4173-8ba9-8fd27d02f0c1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8zMDAxODQ0NzUtZDliOTU2YTItMjVmYS00MTczLThiYTktOGZkMjdkMDJmMGMxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRkMWY4NjIyODBkNDAwNjExNzZkZTJiMmJiZDk0YTRlY2I1Zjk0MDE1YjZlM2UyMGNiMzIyYzA1NjQyZGY0OTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.433P7aTz05yurSzH_kAJDMyMGRZZAi1IUDxKuesO_zU" width="450"></a></p>
<p dir="auto">The following is a diagram of the MXU/systolic array and shows how the PEs are connected:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/12535207/300986120-baf3e2f7-1767-49ec-811e-7cb44fac8d92.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8zMDA5ODYxMjAtYmFmM2UyZjctMTc2Ny00OWVjLTgxMWUtN2NiNDRmYWM4ZDkyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTBlYWRkZWQ0MjM3YWJkNWRkZjYyOGNmZTc5ZWQ1M2RkNjEyYmFiOGZjOWFkYTg4ZGZhNjlhNmUwOThkOTViNzEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QnLh8a3ntuJUB_ICABvi0rG-Czg0Yt2kVquJBTFXLZ0"><img src="https://private-user-images.githubusercontent.com/12535207/300986120-baf3e2f7-1767-49ec-811e-7cb44fac8d92.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8zMDA5ODYxMjAtYmFmM2UyZjctMTc2Ny00OWVjLTgxMWUtN2NiNDRmYWM4ZDkyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTBlYWRkZWQ0MjM3YWJkNWRkZjYyOGNmZTc5ZWQ1M2RkNjEyYmFiOGZjOWFkYTg4ZGZhNjlhNmUwOThkOTViNzEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QnLh8a3ntuJUB_ICABvi0rG-Czg0Yt2kVquJBTFXLZ0" width="450"></a></p>
<p dir="auto">The source code organization is as follows:</p>
<ul dir="auto">
<li>compiler
<ul dir="auto">
<li>A compiler for parsing Python model descriptions into accelerator instructions that allow it to accelerate the model. This part also includes code for interfacing with a PCIe driver for initiating model execution on the accelerator, reading back results and performance counters, and testing the correctness of the results.</li>
</ul>
</li>
<li>rtl
<ul dir="auto">
<li>Synthesizable SystemVerilog RTL.</li>
</ul>
</li>
<li>sim
<ul dir="auto">
<li>Scripts for setting up simulation environments for testing.</li>
</ul>
</li>
<li>tests
<ul dir="auto">
<li>UVM-based testbench source code for verifying the accelerator in simulation using Cocotb.</li>
</ul>
</li>
<li>utils
<ul dir="auto">
<li>Additional Python packages and scripts used in this project that the author created for general development utilities and aids.</li>
</ul>
</li>
</ul>
<p dir="auto">The files rtl/top/define.svh and rtl/top/pkg.sv contain a number of configurable parameters such as FIP_METHOD in define.svh which defines the systolic array type (baseline, FIP, or FFIP), SZI and SZJ which define the systolic array height/width, and LAYERIO_WIDTH/WEIGHT_WIDTH which define the input bitwidths.</p>
<p dir="auto">The directory rtl/arith includes mxu.sv and mac_array.sv which contain the RTL for the baseline, FIP, and, FFIP systolic array architectures (depending on the value of the parameter FIP_METHOD).</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking (191 pts)]]></title>
            <link>https://arxiv.org/abs/2403.09629</link>
            <guid>39713634</guid>
            <pubDate>Fri, 15 Mar 2024 09:24:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.09629">https://arxiv.org/abs/2403.09629</a>, See on <a href="https://news.ycombinator.com/item?id=39713634">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2403.09629">Download PDF</a>
    <a href="https://arxiv.org/html/2403.09629v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM's ability to directly answer difficult questions. In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and observe a perplexity improvement of difficult tokens in natural text. Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Eric Zelikman [<a href="https://arxiv.org/show-email/094380da/2403.09629">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 14 Mar 2024 17:58:16 UTC (510 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Berlin's techno scene added to Unesco intangible cultural heritage list (227 pts)]]></title>
            <link>https://www.theguardian.com/world/2024/mar/15/berlins-techno-scene-added-to-unesco-intangible-cultural-heritage-list</link>
            <guid>39713323</guid>
            <pubDate>Fri, 15 Mar 2024 08:29:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/2024/mar/15/berlins-techno-scene-added-to-unesco-intangible-cultural-heritage-list">https://www.theguardian.com/world/2024/mar/15/berlins-techno-scene-added-to-unesco-intangible-cultural-heritage-list</a>, See on <a href="https://news.ycombinator.com/item?id=39713323">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Germany’s culture ministry and Unesco commission have added Berlin’s techno scene to the country’s list of intangible cultural heritage, in recognition of the scene’s contribution to the cultural identity of the city.</p><p>Berlin’s Clubcommission, a network for Berlin’s techno clubs and musicians, <a href="https://twitter.com/clubcommission/status/1767979957173580132" data-link-name="in body link">described</a> the move as “another milestone for Berlin techno producers, artists, club operators and event organisers”.</p><p>Lutz Leichsenring, an executive member of Clubcommission’s board, <a href="https://www.dw.com/en/berlin-techno-added-to-unesco-cultural-heritage-list/a-68515354" data-link-name="in body link">told the German broadcaster DW</a>: “The decision will help us ensure that club culture is recognised as a valuable sector worthy of protection and support.”</p><p>For more than a decade there has been a campaign to have techno culture and music added to Germany’s list, spearheaded by Rave the Planet, a non-profit supporting electronic music culture.</p><p>“Congratulations to all the cultural creators who have shaped and contributed to Berlin’s techno culture,” the group said in a statement on social media. “This is a major milestone for the entire culture, and our joy is beyond words.”</p><p>Rave the Planet submitted the application for techno to be included in the list in November 2022.</p><p>Intangible cultural heritage status is more commonly granted to more traditional cultural activities, such as Malawian Mwinoghe dancing or Slovakian bagpipe culture. The recent recognition on Unesco’s list of intangible cultural heritage of Jamaican reggae and India’s huge Kumbh Mela festival, however, prompted techno community leaders in Berlin to campaign for their scene to be included in Germany’s register, which is separate to the Unesco list.</p><p>Techno is a fundamental part of the city, according to Peter Kirn, a Berlin-based DJ and music producer. In 2021 he told the Observer: “In other cities, people wouldn’t accept music that’s really hard or weird and full of synthesisers and really brutal, distorted drum machines. You can’t play that at peak hour in a club, let alone over lunch. And here it’s totally acceptable to play that over lunch.</p><p>“Techno has become a refuge for people who are marginalised, and there’s a natural attraction to Berlin as a place which is more permissive when you come from places that are less permissive.”</p><p>The techno scene is one of six new entries on the intangible cultural heritage list in Germany; others include fruit wine and mountaineering. A parade in Bavaria known as the <em>Kirchseeoner Perchtenlauf</em>, where attenders dress as furry monsters, was also added to the list.</p></div><div><p><span data-dcr-style="bullet"></span> The headline and text of this article were amended on 15 March 2024. The techno scene has been added to the national intangible cultural heritage list compiled by the German commission for Unesco, not the global intangible cultural heritage list compiled by Unesco as an earlier version indicated.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[John Barnett before death "if anything happens, it's not suicide", claims friend (493 pts)]]></title>
            <link>https://abcnews4.com/news/local/if-anything-happens-its-not-suicide-boeing-whistleblowers-prediction-before-death-south-carolina-abc-news-4-2024</link>
            <guid>39712618</guid>
            <pubDate>Fri, 15 Mar 2024 06:32:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abcnews4.com/news/local/if-anything-happens-its-not-suicide-boeing-whistleblowers-prediction-before-death-south-carolina-abc-news-4-2024">https://abcnews4.com/news/local/if-anything-happens-its-not-suicide-boeing-whistleblowers-prediction-before-death-south-carolina-abc-news-4-2024</a>, See on <a href="https://news.ycombinator.com/item?id=39712618">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>John Barnett's family friend Jennifer doesn't think the Boeing whistleblower committed suicide in Charleston. In fact, she says he predicted what may happen to him days before he left for his deposition. March 14, 2024. (Provided-FILE, WCIV)</p><div id="js-Story-Content-0"><p><span>CHARLESTON COUNTY, S.C. (WCIV)  — </span>A close family friend of John Barnett said he predicted he might wind up dead and that a story could surface that he killed himself. </p><p>But at the time, he told her not to believe it. </p><p>"I know that he did not commit suicide," said Jennifer, a friend of Barnett's. "There's no way." </p><p>Jennifer said they talked about this exact scenario playing out. However, now, his words seem like a premonition he told her directly not to believe. </p><p>"I know John because his mom and my mom are best friends," Jennifer said. "Over the years, get-togethers, birthdays, celebrations and whatnot. We've all got together and talked." </p><p><strong><em>READ MORE:<a href="https://abcnews4.com/news/local/mystery-lingers-around-boeing-whistleblower-death-at-charleston-hotel-charleston-county-john-barnett-boeing-news-lawsuit-abc-news-4-wciv-2024" target="_blank" title="https://abcnews4.com/news/local/mystery-lingers-around-boeing-whistleblower-death-at-charleston-hotel-charleston-county-john-barnett-boeing-news-lawsuit-abc-news-4-wciv-2024"> "Mystery lingers around Boeing whistleblower's death at Charleston hotel."</a></em></strong></p><p>When Jennifer needed help one day, Barnett came by to see her. They talked about his upcoming deposition in Charleston. Jennifer knew Barnett filed an extremely damaging complaint against Boeing. He said the aerospace giant retaliated against him when he blew the whistle on unsafe practices. <br></p><p>For more than 30 years, he was a quality manager. He'd recently retired and moved back to Louisiana to look after his mom. </p><p>"He wasn't concerned about safety because I asked him," Jennifer said. "I said, 'Aren't you scared?' And he said, 'No, I ain't scared, but if anything happens to me, it's not suicide.'" </p><p>Jennifer added: "I know that he did not commit suicide. There's no way. He loved life too much. He loved his family too much. He loved his brothers too much to put them through what they're going through right now." </p><p>Jennifer said she thinks somebody "didn't like what he had to say" and wanted to "shut him up" without it coming back to anyone. </p><p><strong><em>READ MORE: <a href="https://abcnews4.com/news/local/john-was-a-brave-boeing-whistleblowers-lawyer-responds-to-news-of-his-death-john-barnett-boeing-news-abc-news-wciv-news-4-2024" target="_blank" title="https://abcnews4.com/news/local/john-was-a-brave-boeing-whistleblowers-lawyer-responds-to-news-of-his-death-john-barnett-boeing-news-abc-news-wciv-news-4-2024">"'John was brave': Boeing whistleblower's lawyer responds to news of his death."</a></em></strong></p><p>"That's why they made it look like a suicide," Jennifer said. </p><p>The last time Jennifer saw Barnett was at her father's funeral in late February. He was one of the pallbearers. Sometimes family and friends referred to him by his middle name – Mitch. </p><p>"I think everybody is in disbelief and can't believe it," Jennifer said. "I don't care what they say, I know that Mitch didn't do that." </p><p>Just because Barnett is dead doesn't mean the case won't move forward. </p><p>His attorney said they're still prepared to go to trial in June. </p><p>News 4 reached out to Boeing following Barnett's death. They provided the following statement: </p><p>"We are saddened by Mr. Barnett’s passing, and our thoughts are with his family and friends.”<br></p><p><strong><em>READ MORE: <a href="https://abcnews4.com/news/local/boeing-whistleblower-dies-in-charleston-charleston-county-coroners-office-confirms-south-carolina-boeing-news-abc-news-4" target="_blank" title="https://abcnews4.com/news/local/boeing-whistleblower-dies-in-charleston-charleston-county-coroners-office-confirms-south-carolina-boeing-news-abc-news-4">"Boeing whistleblower dies in Charleston, Charleston County Coroner's Office confirms."</a></em></strong></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Harvard concluded that a dishonesty expert committed misconduct (161 pts)]]></title>
            <link>https://www.chronicle.com/article/heres-the-unsealed-report-showing-how-harvard-concluded-that-a-dishonesty-expert-committed-misconduct</link>
            <guid>39712173</guid>
            <pubDate>Fri, 15 Mar 2024 04:53:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chronicle.com/article/heres-the-unsealed-report-showing-how-harvard-concluded-that-a-dishonesty-expert-committed-misconduct">https://www.chronicle.com/article/heres-the-unsealed-report-showing-how-harvard-concluded-that-a-dishonesty-expert-committed-misconduct</a>, See on <a href="https://news.ycombinator.com/item?id=39712173">Hacker News</a></p>
Couldn't get https://www.chronicle.com/article/heres-the-unsealed-report-showing-how-harvard-concluded-that-a-dishonesty-expert-committed-misconduct: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[RSS was released 25 years ago today (103 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/RSS</link>
            <guid>39712025</guid>
            <pubDate>Fri, 15 Mar 2024 04:16:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/RSS">https://en.wikipedia.org/wiki/RSS</a>, See on <a href="https://news.ycombinator.com/item?id=39712025">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en" dir="ltr" id="mw-content-text">




<table><caption>RSS</caption><tbody><tr><td colspan="2"><span typeof="mw:File"><a href="https://en.wikipedia.org/wiki/File:Feed-icon.svg" title="Feed Computer icon."><img alt="Feed Computer icon." src="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/128px-Feed-icon.svg.png" decoding="async" width="128" height="128" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/192px-Feed-icon.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/256px-Feed-icon.svg.png 2x" data-file-width="128" data-file-height="128"></a></span></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Filename_extension" title="Filename extension">Filename extension</a></th><td><p><kbd>.rss, .xml</kbd></p></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Media_type" title="Media type">Internet media&nbsp;type</a></th><td><code>application/rss+xml</code>&nbsp;(registration not finished)<sup id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup></td></tr><tr><th scope="row">Developed&nbsp;by</th><td><a href="https://en.wikipedia.org/wiki/RSS_Advisory_Board" title="RSS Advisory Board">RSS Advisory Board</a></td></tr><tr><th scope="row">Initial release</th><td>RSS 0.90 (Netscape), March&nbsp;15, 1999<span>; 25 years ago</span></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Software_release_life_cycle" title="Software release life cycle">Latest release</a></th><td><p>RSS 2.0 (version 2.0.11)<br>March&nbsp;30, 2009<span>; 14 years ago</span> </p></td></tr><tr><th scope="row">Type of format</th><td><a href="https://en.wikipedia.org/wiki/Web_syndication" title="Web syndication">Web syndication</a></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Digital_container_format" title="Digital container format">Container&nbsp;for</a></th><td>Updates of a website and its related metadata (<a href="https://en.wikipedia.org/wiki/Web_feed" title="Web feed">web feed</a>)</td></tr><tr><th scope="row">Extended&nbsp;from</th><td><a href="https://en.wikipedia.org/wiki/XML" title="XML">XML</a></td></tr><tr><th scope="row"><span><a href="https://en.wikipedia.org/wiki/Open_file_format" title="Open file format">Open format</a>?</span></th><td>Yes</td></tr><tr><th scope="row">Website</th><td><span><a rel="nofollow" href="http://rssboard.org/rss-specification">rssboard<wbr>.org<wbr>/rss-specification</a></span></td></tr></tbody></table>
<p><b>RSS</b> (<b><a href="https://en.wikipedia.org/wiki/Resource_Description_Framework" title="Resource Description Framework">RDF</a> Site Summary</b> or <b>Really Simple Syndication</b>)<sup id="cite_ref-powers-2003-1_2-0"><a href="#cite_note-powers-2003-1-2">[2]</a></sup> is a <a href="https://en.wikipedia.org/wiki/Web_feed" title="Web feed">web feed</a><sup id="cite_ref-Netsc99_3-0"><a href="#cite_note-Netsc99-3">[3]</a></sup> that allows users and applications to access updates to websites in a <a href="https://en.wikipedia.org/wiki/Standardization" title="Standardization">standardized</a>, computer-readable format. Subscribing to RSS feeds can allow a user to keep track of many different websites in a single <a href="https://en.wikipedia.org/wiki/News_aggregator" title="News aggregator">news aggregator</a>, which constantly monitor sites for new content, removing the need for the user to manually check them. News aggregators (or "RSS readers") can be built into a <a href="https://en.wikipedia.org/wiki/Web_application" title="Web application">browser</a>, installed on a <a href="https://en.wikipedia.org/wiki/Application_software" title="Application software">desktop computer</a>, or installed on a <a href="https://en.wikipedia.org/wiki/Mobile_app" title="Mobile app">mobile device</a>.
</p><p>Websites usually use RSS feeds to publish frequently updated information, such as <a href="https://en.wikipedia.org/wiki/Blog" title="Blog">blog</a> entries, news headlines, episodes of audio and video series, or for distributing <a href="https://en.wikipedia.org/wiki/Podcast" title="Podcast">podcasts</a>. An RSS document (called "feed", "web feed",<sup id="cite_ref-GuardWF_4-0"><a href="#cite_note-GuardWF-4">[4]</a></sup> or "channel") includes full or summarized text, and <a href="https://en.wikipedia.org/wiki/Metadata" title="Metadata">metadata</a>, like publishing date and author's name. RSS formats are specified using a generic <a href="https://en.wikipedia.org/wiki/XML" title="XML">XML</a> file.
</p><p>Although RSS formats have evolved from as early as March 1999,<sup id="cite_ref-Qstart_5-0"><a href="#cite_note-Qstart-5">[5]</a></sup> it was between 2005 and 2006 when RSS gained widespread use, and the ("<span typeof="mw:File"><a href="https://en.wikipedia.org/wiki/File:Feed-icon.svg"><img src="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/16px-Feed-icon.svg.png" decoding="async" width="16" height="16" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/24px-Feed-icon.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/32px-Feed-icon.svg.png 2x" data-file-width="128" data-file-height="128"></a></span>") icon was decided upon by several major web browsers. RSS feed data is presented to users using software called a news aggregator and the passing of content is called <a href="https://en.wikipedia.org/wiki/Web_syndication" title="Web syndication">web syndication</a>. Users subscribe to feeds either by entering a feed's <a href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier" title="Uniform Resource Identifier">URI</a> into the reader or by clicking on the browser's <a href="https://en.wikipedia.org/wiki/Web_feed#Feed_icon" title="Web feed">feed icon</a>. The RSS reader checks the user's feeds regularly for new information and can automatically download it, if that function is enabled.
</p>
<meta property="mw:PageProp/toc">
<h2><span id="History">History</span></h2>
<table role="presentation"><tbody><tr><td><p><span typeof="mw:File"><span><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/Ambox_current_red.svg/42px-Ambox_current_red.svg.png" decoding="async" width="42" height="34" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/Ambox_current_red.svg/63px-Ambox_current_red.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/Ambox_current_red.svg/84px-Ambox_current_red.svg.png 2x" data-file-width="360" data-file-height="290"></span></span></p></td><td><p>This section needs to be <b>updated</b>.<span> Please help update this article to reflect recent events or newly available information.</span>  <span><i>(<span>October 2013</span>)</i></span></p></td></tr></tbody></table>

<p>The RSS formats were preceded by several attempts at <a href="https://en.wikipedia.org/wiki/Web_syndication" title="Web syndication">web syndication</a> that did not achieve widespread popularity. The basic idea of restructuring information about websites goes back to as early as 1995, when <a href="https://en.wikipedia.org/wiki/Ramanathan_V._Guha" title="Ramanathan V. Guha">Ramanathan V. Guha</a> and others in <a href="https://en.wikipedia.org/wiki/Apple_Inc." title="Apple Inc.">Apple</a>'s <a href="https://en.wikipedia.org/wiki/Apple_Advanced_Technology_Group" title="Apple Advanced Technology Group">Advanced Technology Group</a> developed the <a href="https://en.wikipedia.org/wiki/Meta_Content_Framework" title="Meta Content Framework">Meta Content Framework</a>.<sup id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup>
</p><p><a href="https://en.wikipedia.org/wiki/Resource_Description_Framework" title="Resource Description Framework">RDF</a> Site Summary, the first version of RSS, was created by <a href="https://en.wikipedia.org/w/index.php?title=Dan_Libby&amp;action=edit&amp;redlink=1" title="Dan Libby (page does not exist)">Dan Libby</a> and Ramanathan V. Guha at <a href="https://en.wikipedia.org/wiki/Netscape" title="Netscape">Netscape</a>. It was released in March 1999 for use on the My.Netscape.Com portal.<sup id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup> This version became known as RSS 0.9.<sup id="cite_ref-Qstart_5-1"><a href="#cite_note-Qstart-5">[5]</a></sup> In July 1999, Dan Libby of Netscape produced a new version, RSS 0.91,<sup id="cite_ref-Netsc99_3-1"><a href="#cite_note-Netsc99-3">[3]</a></sup> which simplified the format by removing RDF elements and incorporating elements from <a href="https://en.wikipedia.org/wiki/Dave_Winer" title="Dave Winer">Dave Winer</a>'s news syndication format.<sup id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup> Libby also renamed the format from RDF to RSS <b>Rich Site Summary</b> and outlined further development of the format in a "futures document".<sup id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup>
</p><p>This would be Netscape's last participation in RSS development for eight years. As RSS was being embraced by web publishers who wanted their feeds to be used on My.Netscape.Com and other early RSS portals, Netscape dropped RSS support from My.Netscape.Com in April 2001 during new owner <a href="https://en.wikipedia.org/wiki/AOL" title="AOL">AOL</a>'s restructuring of the company, also removing documentation and tools that supported the format.<sup id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup>
</p><p>Two parties emerged to fill the void, with neither Netscape's help nor approval: The <a href="https://en.wikipedia.org/wiki/RSS-DEV_Working_Group" title="RSS-DEV Working Group">RSS-DEV Working Group</a> and Dave Winer, whose <a href="https://en.wikipedia.org/wiki/UserLand_Software" title="UserLand Software">UserLand Software</a> had published some of the first publishing tools outside Netscape that could read and write RSS.
</p><p>Winer published a modified version of the RSS 0.91 specification on the UserLand website, covering how it was being used in his company's products, and claimed copyright to the document.<sup id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup> A few months later, UserLand filed a U.S. trademark registration for RSS, but failed to respond to a <a href="https://en.wikipedia.org/wiki/United_States_Patent_and_Trademark_Office" title="United States Patent and Trademark Office">USPTO</a> trademark examiner's request and the request was rejected in December 2001.<sup id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup>
</p><p>The RSS-DEV Working Group, a project whose members included <a href="https://en.wikipedia.org/wiki/Aaron_Swartz" title="Aaron Swartz">Aaron Swartz</a>,<sup id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup> Guha and representatives of <a href="https://en.wikipedia.org/wiki/O%27Reilly_Media" title="O'Reilly Media">O'Reilly Media</a> and <a href="https://en.wikipedia.org/wiki/Moreover_Technologies" title="Moreover Technologies">Moreover</a>, produced RSS 1.0 in December 2000.<sup id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup> This new version, which reclaimed the name RDF Site Summary from RSS 0.9, reintroduced support for RDF and added <a href="https://en.wikipedia.org/wiki/XML_namespace" title="XML namespace">XML namespaces</a> support, adopting elements from standard metadata vocabularies such as <a href="https://en.wikipedia.org/wiki/Dublin_Core" title="Dublin Core">Dublin Core</a>.
</p><p>In December 2000, Winer released RSS 0.92<sup id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup>
a minor set of changes aside from the introduction of the enclosure element, which permitted audio files to be carried in RSS feeds and helped spark <a href="https://en.wikipedia.org/wiki/Podcast" title="Podcast">podcasting</a>. He also released drafts of RSS 0.93 and RSS 0.94 that were subsequently withdrawn.<sup id="cite_ref-17"><a href="#cite_note-17">[17]</a></sup>
</p><p>In September 2002, Winer released a major new version of the format, RSS 2.0, that redubbed its initials Really Simple Syndication. RSS 2.0 removed the <i>type</i> attribute added in the RSS 0.94 draft and added support for namespaces. To preserve backward compatibility with RSS 0.92, namespace support applies only to other content included within an RSS 2.0 feed, not the RSS 2.0 elements themselves.<sup id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup> (Although other standards such as <a href="https://en.wikipedia.org/wiki/Atom_(standard)" title="Atom (standard)">Atom</a> attempt to correct this limitation, RSS feeds are not aggregated with other content often enough to shift the popularity from RSS to other formats having full namespace support.)
</p><p>Because neither Winer nor the RSS-DEV Working Group had Netscape's involvement, they could not make an official claim on the RSS name or format. This has fueled ongoing controversy<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citing_sources" title="Wikipedia:Citing sources"><span title="Statement needs to be more specific about the content to which it refers. (September 2016)">specify</span></a></i>]</sup> in the syndication development community as to which entity was the proper publisher of RSS.
</p><p>One product of that contentious debate was the creation of an alternative syndication format, Atom, that began in June 2003.<sup id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup> The Atom syndication format, whose creation was in part motivated by a desire to get a clean start free of the issues surrounding RSS, has been adopted as <a href="https://en.wikipedia.org/wiki/IETF" title="IETF">IETF</a> Proposed Standard <a href="https://en.wikipedia.org/wiki/RFC_(identifier)" title="RFC (identifier)">RFC</a>&nbsp;<a rel="nofollow" href="https://datatracker.ietf.org/doc/html/rfc4287">4287</a>.
</p><p>In July 2003, Winer and UserLand Software assigned the copyright of the RSS 2.0 specification to Harvard's <a href="https://en.wikipedia.org/wiki/Berkman_Klein_Center_for_Internet_%26_Society" title="Berkman Klein Center for Internet &amp; Society">Berkman Klein Center for Internet &amp; Society</a>, where he had just begun a term as a visiting fellow.<sup id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup> At the same time, Winer launched the <a href="https://en.wikipedia.org/wiki/RSS_Advisory_Board" title="RSS Advisory Board">RSS Advisory Board</a> with <a href="https://en.wikipedia.org/wiki/NetNewsWire" title="NetNewsWire">Brent Simmons</a> and <a href="https://en.wikipedia.org/wiki/Jon_Udell" title="Jon Udell">Jon Udell</a>, a group whose purpose was to maintain and publish the specification and answer questions about the format.<sup id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup>
</p><p>In September 2004, Stephen Horlander created the now ubiquitous <a href="https://en.wikipedia.org/wiki/Web_feed#Feed_icon" title="Web feed">RSS icon</a> (<span typeof="mw:File"><a href="https://en.wikipedia.org/wiki/File:Feed-icon.svg"><img src="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/16px-Feed-icon.svg.png" decoding="async" width="16" height="16" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/24px-Feed-icon.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/32px-Feed-icon.svg.png 2x" data-file-width="128" data-file-height="128"></a></span>) for use in the <a href="https://en.wikipedia.org/wiki/Mozilla" title="Mozilla">Mozilla</a> <a href="https://en.wikipedia.org/wiki/Firefox" title="Firefox">Firefox</a> <a href="https://en.wikipedia.org/wiki/Web_Browser" title="Web Browser">browser</a>.<sup id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup>
</p><p>In December 2005, the Microsoft Internet Explorer team and
<a href="https://en.wikipedia.org/wiki/Microsoft_Outlook" title="Microsoft Outlook">Microsoft Outlook</a> team<sup id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup> announced on their blogs that they were adopting Firefox's RSS icon. In February 2006, <a href="https://en.wikipedia.org/wiki/Opera_Software" title="Opera Software">Opera Software</a> followed suit.<sup id="cite_ref-24"><a href="#cite_note-24">[24]</a></sup> This effectively made the orange square with white radio waves the industry standard for RSS and Atom feeds, replacing the large variety of icons and text that had been used previously to identify syndication data.
</p><p>In January 2006, <a href="https://en.wikipedia.org/wiki/Rogers_Cadenhead" title="Rogers Cadenhead">Rogers Cadenhead</a> relaunched the RSS Advisory Board without Dave Winer's participation, with a stated desire to continue the development of the RSS format and resolve ambiguities. In June 2007, the board revised their version of the specification to confirm that namespaces may extend core elements with namespace attributes, as Microsoft has done in Internet Explorer 7. According to their view, a difference of interpretation left publishers unsure of whether this was permitted or forbidden.
</p>
<h2><span id="Example">Example</span></h2>
<p>RSS is <a href="https://en.wikipedia.org/wiki/XML" title="XML">XML</a>-formatted plain text. The RSS format itself is relatively easy to read both by automated processes and by humans alike. An example feed could have contents such as the following:
</p>
<div dir="ltr"><pre><span></span><span>&lt;?xml version="1.0" encoding="UTF-8"&nbsp;?&gt;</span>
<span>&lt;rss</span><span> </span><span>version=</span><span>"2.0"</span><span>&gt;</span>
<span>&lt;channel&gt;</span>
<span> </span><span>&lt;title&gt;</span>RSS<span> </span>Title<span>&lt;/title&gt;</span>
<span> </span><span>&lt;description&gt;</span>This<span> </span>is<span> </span>an<span> </span>example<span> </span>of<span> </span>an<span> </span>RSS<span> </span>feed<span>&lt;/description&gt;</span>
<span> </span><span>&lt;link&gt;</span>http://www.example.com/main.html<span>&lt;/link&gt;</span>
<span> </span><span>&lt;copyright&gt;</span>2020<span> </span>Example.com<span> </span>All<span> </span>rights<span> </span>reserved<span>&lt;/copyright&gt;</span>
<span> </span><span>&lt;lastBuildDate&gt;</span>Mon,<span> </span>6<span> </span>Sep<span> </span>2010<span> </span>00:01:00<span> </span>+0000<span>&lt;/lastBuildDate&gt;</span>
<span> </span><span>&lt;pubDate&gt;</span>Sun,<span> </span>6<span> </span>Sep<span> </span>2009<span> </span>16:20:00<span> </span>+0000<span>&lt;/pubDate&gt;</span>
<span> </span><span>&lt;ttl&gt;</span>1800<span>&lt;/ttl&gt;</span>

<span> </span><span>&lt;item&gt;</span>
<span>  </span><span>&lt;title&gt;</span>Example<span> </span>entry<span>&lt;/title&gt;</span>
<span>  </span><span>&lt;description&gt;</span>Here<span> </span>is<span> </span>some<span> </span>text<span> </span>containing<span> </span>an<span> </span>interesting<span> </span>description.<span>&lt;/description&gt;</span>
<span>  </span><span>&lt;link&gt;</span>http://www.example.com/blog/post/1<span>&lt;/link&gt;</span>
<span>  </span><span>&lt;guid</span><span> </span><span>isPermaLink=</span><span>"false"</span><span>&gt;</span>7bd204c6-1655-4c27-aeee-53f933c5395f<span>&lt;/guid&gt;</span>
<span>  </span><span>&lt;pubDate&gt;</span>Sun,<span> </span>6<span> </span>Sep<span> </span>2009<span> </span>16:20:00<span> </span>+0000<span>&lt;/pubDate&gt;</span>
<span> </span><span>&lt;/item&gt;</span>

<span>&lt;/channel&gt;</span>
<span>&lt;/rss&gt;</span>
</pre></div>
<h3><span id="Aggregators">Aggregators</span></h3>

<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Tiny_Tiny_RSS_English_Interface.png"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Tiny_Tiny_RSS_English_Interface.png/330px-Tiny_Tiny_RSS_English_Interface.png" decoding="async" width="330" height="186" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Tiny_Tiny_RSS_English_Interface.png/495px-Tiny_Tiny_RSS_English_Interface.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Tiny_Tiny_RSS_English_Interface.png/660px-Tiny_Tiny_RSS_English_Interface.png 2x" data-file-width="1366" data-file-height="768"></a><figcaption>User interface of an RSS feed reader on a desktop computer</figcaption></figure>
<p>When retrieved, RSS reading software could use the XML structure to present a neat display to the end users. There are various news aggregator software for desktop and mobile devices, but RSS can also be built-in inside <a href="https://en.wikipedia.org/wiki/Web_browser" title="Web browser">web browsers</a> or <a href="https://en.wikipedia.org/wiki/Email_client" title="Email client">email clients</a> like <a href="https://en.wikipedia.org/wiki/Mozilla_Thunderbird" title="Mozilla Thunderbird">Mozilla Thunderbird</a>.
</p>
<h2><span id="Variants">Variants</span></h2>
<p>There are several different versions of RSS, falling into two major branches (RDF and 2.*).
</p><p>The RDF (or RSS 1.*) branch includes the following versions:
</p>
<ul><li>RSS 0.90 was the original Netscape RSS version. This RSS was called <i>RDF Site Summary</i>, but was based on an early working draft of the RDF standard, and was not compatible with the final RDF Recommendation.</li>
<li>RSS 1.0 is an open format by the RSS-DEV Working Group, again standing for <i>RDF Site Summary</i>. RSS 1.0 is an RDF format like RSS 0.90, but not fully compatible with it, since 1.0 is based on the final RDF 1.0 Recommendation.</li>
<li>RSS 1.1 is also an open format and is intended to update and replace RSS 1.0. The specification is an independent draft not supported or endorsed in any way by the RSS-Dev Working Group or any other organization.</li></ul>
<p>The RSS 2.* branch (initially UserLand, now Harvard) includes the following versions:
</p>
<ul><li>RSS 0.91 is the simplified RSS version released by Netscape, and also the version number of the simplified version originally championed by Dave Winer from Userland Software. The Netscape version was now called <i>Rich Site Summary</i>; this was no longer an RDF format, but was relatively easy to use.</li>
<li>RSS 0.92 through 0.94 are expansions of the RSS 0.91 format, which are mostly compatible with each other and with Winer's version of RSS 0.91, but are not compatible with RSS 0.90.</li>
<li>RSS 2.0.1 has the internal version number 2.0. RSS 2.0.1 was proclaimed to be "frozen", but still updated shortly after release without changing the version number.  RSS now stood for <i>Really Simple Syndication</i>.  The major change in this version is an explicit extension mechanism using XML namespaces.<sup id="cite_ref-W3C_REC_XML_Namespace_25-0"><a href="#cite_note-W3C_REC_XML_Namespace-25">[25]</a></sup></li></ul>
<p>Later versions in each branch are <a href="https://en.wikipedia.org/wiki/Backward_compatibility" title="Backward compatibility">backward-compatible</a> with earlier versions (aside from non-conformant RDF syntax in 0.90), and both versions include properly documented extension mechanisms using XML Namespaces, either directly (in the 2.* branch) or through RDF (in the 1.* branch).  Most syndication software supports both branches. "The Myth of RSS Compatibility", an article written in 2004 by RSS critic and <a href="https://en.wikipedia.org/wiki/Atom_(standard)" title="Atom (standard)">Atom</a> advocate <a href="https://en.wikipedia.org/wiki/Mark_Pilgrim" title="Mark Pilgrim">Mark Pilgrim</a>, discusses RSS version compatibility issues in more detail.
</p><p>The extension mechanisms make it possible for each branch to copy innovations in the other. For example, the RSS 2.* branch was the first to support <a href="https://en.wikipedia.org/wiki/RSS_enclosure" title="RSS enclosure">enclosures</a>, making it the current leading choice for podcasting, and as of 2005 is the format supported for that use by <a href="https://en.wikipedia.org/wiki/ITunes" title="ITunes">iTunes</a> and other podcasting software; however, an enclosure extension is now available for the RSS 1.* branch, mod_enclosure.  Likewise, the RSS 2.* core specification does not support providing full-text in addition to a synopsis, but the RSS 1.* markup can be (and often is) used as an extension.  There are also several common outside extension packages available, e.g. one  from <a href="https://en.wikipedia.org/wiki/Microsoft" title="Microsoft">Microsoft</a> for use in <a href="https://en.wikipedia.org/wiki/Internet_Explorer" title="Internet Explorer">Internet Explorer</a> 7.
</p><p>The most serious compatibility problem is with HTML markup. Userland's RSS reader—generally considered as the reference implementation—did not originally filter out <a href="https://en.wikipedia.org/wiki/HTML" title="HTML">HTML</a> markup from feeds. As a result, publishers began placing HTML markup into the titles and descriptions of items in their RSS feeds. This behavior has become expected of readers, to the point of becoming a <a href="https://en.wikipedia.org/wiki/De_facto" title="De facto">de facto</a> standard.<sup id="cite_ref-26"><a href="#cite_note-26">[26]</a></sup> Though there is still some inconsistency in how software handles this markup, particularly in titles. The RSS 2.0 specification was later updated to include examples of entity-encoded HTML; however, all prior plain text usages remain valid.
</p><p>As of January&nbsp;2007, tracking data from www.syndic8.com indicates that the three main versions of RSS in current use are 0.91, 1.0, and 2.0, constituting 13%, 17%, and 67% of worldwide RSS usage, respectively.<sup id="cite_ref-27"><a href="#cite_note-27">[27]</a></sup> These figures, however, do not include usage of the rival web feed format Atom. As of August&nbsp;2008, the syndic8.com website is indexing 546,069 total feeds, of which 86,496 (16%) were some dialect of Atom and 438,102 were some dialect of RSS.<sup id="cite_ref-28"><a href="#cite_note-28">[28]</a></sup>
</p>
<h2><span id="Modules">Modules</span></h2>
<p>The primary objective of all RSS modules is to extend the basic XML schema established for more robust syndication of content. This inherently allows for more diverse, yet standardized, transactions without modifying the core RSS specification.
</p><p>To accomplish this extension, a tightly controlled vocabulary (in the RSS world, "module"; in the XML world, "schema") is declared through an XML namespace to give names to concepts and relationships between those concepts.
</p><p>Some RSS 2.0 modules with established namespaces are:
</p>
<ul><li><a href="https://en.wikipedia.org/wiki/Media_RSS" title="Media RSS">Media RSS</a> (MRSS) 2.0 Module</li>
<li><a rel="nofollow" href="http://www.opensearch.org/Specifications/OpenSearch/1.1">OpenSearch RSS 2.0 Module</a></li></ul>
<h2><span id="Interoperability">Interoperability</span></h2>
<p>Although the number of items in an RSS channel is theoretically unlimited, some <a href="https://en.wikipedia.org/wiki/News_aggregators" title="News aggregators">news aggregators</a> do not support RSS files larger than 150KB. For example, applications that rely on the Common Feed List of <a href="https://en.wikipedia.org/wiki/Microsoft_Windows" title="Microsoft Windows">Windows</a> might handle such files as if they were corrupt, and not open them. <a href="https://en.wikipedia.org/wiki/Interoperability" title="Interoperability">Interoperability</a> can be maximized by keeping the file size under this limit.
</p><p><a href="https://en.wikipedia.org/wiki/Podcast" title="Podcast">Podcasts</a> are distributed using RSS. To listen to a podcast, a user adds the RSS feed to their podcast client, and the client can then list available episodes and download or stream them for listening or viewing. To be included in a podcast directory the feed must for each episode provide a title, description, artwork, category, language, and explicit rating. There are some services that specifically indexes and is a <a href="https://en.wikipedia.org/wiki/Search_engine" title="Search engine">search engine</a> for podcasts.<sup id="cite_ref-29"><a href="#cite_note-29">[29]</a></sup>
</p><p>Some <a href="https://en.wikipedia.org/wiki/BitTorrent" title="BitTorrent">BitTorrent</a> clients support RSS. RSS feeds which provide links to .torrent files allow users to <a href="https://en.wikipedia.org/wiki/Broadcatching" title="Broadcatching">subscribe and automatically download</a> content as soon as it is published.
</p>
<h3></h3>

<p>Some services deliver RSS to an email inbox, sending updates from user's personal selection and schedules. Examples of such services include <a href="https://en.wikipedia.org/wiki/IFTTT" title="IFTTT">IFTTT</a>, <a href="https://en.wikipedia.org/wiki/Zapier" title="Zapier">Zapier</a> and others.<sup id="cite_ref-30"><a href="#cite_note-30">[30]</a></sup> Conversely, some services deliver email to RSS readers.<sup id="cite_ref-31"><a href="#cite_note-31">[31]</a></sup> Further services like e. g. <a href="https://en.wikipedia.org/wiki/Gmane" title="Gmane">Gmane</a> allow to subscribe to feeds via <a href="https://en.wikipedia.org/wiki/NNTP" title="NNTP">NNTP</a>.
</p><p>It may be noted that <a href="https://en.wikipedia.org/wiki/Email_client" title="Email client">email clients</a> such as <a href="https://en.wikipedia.org/wiki/Mozilla_Thunderbird" title="Mozilla Thunderbird">Thunderbird</a> supports RSS natively.<sup id="cite_ref-32"><a href="#cite_note-32">[32]</a></sup>
</p>
<h2></h2>
<p>Both RSS and <a href="https://en.wikipedia.org/wiki/Atom_(web_standard)" title="Atom (web standard)">Atom</a> are widely supported and are compatible with all major consumer feed readers. RSS gained wider use because of early feed reader support. Technically, Atom has several advantages: less restrictive licensing, <a href="https://en.wikipedia.org/wiki/Internet_Assigned_Numbers_Authority" title="Internet Assigned Numbers Authority">IANA</a>-registered <a href="https://en.wikipedia.org/wiki/MIME_type" title="MIME type">MIME type</a>, XML namespace, <a href="https://en.wikipedia.org/wiki/URI" title="URI">URI</a> support, <a href="https://en.wikipedia.org/wiki/RELAX_NG" title="RELAX NG">RELAX NG</a> support.<sup id="cite_ref-33"><a href="#cite_note-33">[33]</a></sup>
</p><p>The following table shows RSS elements alongside Atom elements where they are equivalent.
</p><p>Note: the <a href="https://en.wikipedia.org/wiki/Asterisk" title="Asterisk">asterisk</a> character (*) indicates that an element must be provided (Atom elements "author" and "link" are only required under certain conditions).
</p>
<table>

<tbody><tr>
<th scope="col">RSS 2.0
</th>
<th scope="col">Atom 1.0
</th></tr>
<tr>
<td><code>author</code>
</td>
<td><code>author</code>*
</td></tr>
<tr>
<td><code>category</code>
</td>
<td><code>category</code>
</td></tr>
<tr>
<td><code>channel</code>
</td>
<td><code>feed</code>
</td></tr>
<tr>
<td><code>copyright</code>
</td>
<td><code>rights</code>
</td></tr>
<tr>
<td>—
</td>
<td><code>subtitle</code>
</td></tr>
<tr>
<td><code>description</code>*
</td>
<td><code>summary</code> and/or <code>content</code>
</td></tr>
<tr>
<td><code>generator</code>
</td>
<td><code>generator</code>
</td></tr>
<tr>
<td><code>guid</code>
</td>
<td><code>id</code>*
</td></tr>
<tr>
<td><code>image</code>
</td>
<td><code>logo</code>
</td></tr>
<tr>
<td><code>item</code>
</td>
<td><code>entry</code>
</td></tr>
<tr>
<td><code>lastBuildDate</code> (in <code>channel</code>)
</td>
<td><code>updated</code>*
</td></tr>
<tr>
<td><code>link</code>*
</td>
<td><code>link</code>*
</td></tr>
<tr>
<td><code>managingEditor</code>
</td>
<td><code>author</code> or <code>contributor</code>
</td></tr>
<tr>
<td><code>pubDate</code>
</td>
<td><code>published</code> (subelement of <code>entry</code>)
</td></tr>
<tr>
<td><code>title</code>*
</td>
<td><code>title</code>*
</td></tr>
<tr>
<td><code><a href="https://en.wikipedia.org/wiki/Time_to_live" title="Time to live">ttl</a></code>
</td>
<td>—
</td></tr></tbody></table>
<h2><span id="Current_usage">Current usage</span></h2>
<p>Several major sites such as <a href="https://en.wikipedia.org/wiki/Facebook" title="Facebook">Facebook</a> and <a href="https://en.wikipedia.org/wiki/Twitter" title="Twitter">Twitter</a> previously offered RSS feeds but have reduced or removed support. Additionally, widely used readers such as <a href="https://en.wikipedia.org/wiki/Shiira" title="Shiira">Shiira</a>, FeedDemon, and particularly <a href="https://en.wikipedia.org/wiki/Google_Reader" title="Google Reader">Google Reader</a>, have all been discontinued as of 2013, citing declining popularity in RSS.<sup id="cite_ref-ClosureAnnouncement_34-0"><a href="#cite_note-ClosureAnnouncement-34">[34]</a></sup> RSS support was removed in <a href="https://en.wikipedia.org/wiki/OS_X_Mountain_Lion" title="OS X Mountain Lion">OS X Mountain Lion</a>'s versions of <a href="https://en.wikipedia.org/wiki/Apple_Mail" title="Apple Mail">Mail</a> and <a href="https://en.wikipedia.org/wiki/Safari_(web_browser)" title="Safari (web browser)">Safari</a>, although the features were partially restored in Safari 8.<sup id="cite_ref-36"><a href="#cite_note-36">[36]</a></sup> Mozilla removed RSS support from <a href="https://en.wikipedia.org/wiki/Mozilla_Firefox" title="Mozilla Firefox">Mozilla Firefox</a> version 64.0, joining <a href="https://en.wikipedia.org/wiki/Google_Chrome" title="Google Chrome">Google Chrome</a> and <a href="https://en.wikipedia.org/wiki/Microsoft_Edge" title="Microsoft Edge">Microsoft Edge</a> which do not include RSS support, thus leaving <a href="https://en.wikipedia.org/wiki/Internet_Explorer" title="Internet Explorer">Internet Explorer</a> as the last major browser to include RSS support by default.<sup id="cite_ref-37"><a href="#cite_note-37">[37]</a></sup><sup id="cite_ref-38"><a href="#cite_note-38">[38]</a></sup>
</p><p>Since the late 2010s there has been an uptick in RSS interest again. In 2018, <i><a href="https://en.wikipedia.org/wiki/Wired_(magazine)" title="Wired (magazine)">Wired</a></i> published an article named "It's Time for an RSS Revival", citing that RSS gives more control over content compared to algorithms and trackers from social media sites. At that time, <a href="https://en.wikipedia.org/wiki/Feedly" title="Feedly">Feedly</a> was the most popular RSS reader.<sup id="cite_ref-39"><a href="#cite_note-39">[39]</a></sup> Chrome on <a href="https://en.wikipedia.org/wiki/Android_(operating_system)" title="Android (operating system)">Android</a> has added the ability to follow RSS feeds as of 2021.<sup id="cite_ref-40"><a href="#cite_note-40">[40]</a></sup>
</p>
<h2><span id="See_also">See also</span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/JSON_Feed" title="JSON Feed">JSON Feed</a></li>
<li><a href="https://en.wikipedia.org/wiki/Aaron_Swartz" title="Aaron Swartz">Aaron Swartz</a></li>
<li><a href="https://en.wikipedia.org/wiki/Comparison_of_feed_aggregators" title="Comparison of feed aggregators">Comparison of feed aggregators</a></li>
<li><a href="https://en.wikipedia.org/wiki/Data_portability" title="Data portability">Data portability</a></li>
<li><a href="https://en.wikipedia.org/wiki/FeedSync" title="FeedSync">FeedSync</a> previously Simple Sharing Extensions</li>
<li><a href="https://en.wikipedia.org/wiki/HAtom" title="HAtom">hAtom</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mashup_(web_application_hybrid)" title="Mashup (web application hybrid)">Mashup (web application hybrid)</a></li>
<li><a href="https://en.wikipedia.org/wiki/WebSub" title="WebSub">WebSub</a></li></ul>
<h2><span id="Notes">Notes</span></h2>
<div>
<ul><li><cite id="CITEREFPowers2003"><a href="https://en.wikipedia.org/wiki/Shelley_Powers" title="Shelley Powers">Powers, Shelley</a> (2003). <i>Practical RDF</i>. <a href="https://en.wikipedia.org/wiki/O%27Reilly_Media" title="O'Reilly Media">O'Reilly</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Practical+RDF&amp;rft.pub=O%27Reilly&amp;rft.date=2003&amp;rft.aulast=Powers&amp;rft.aufirst=Shelley&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></li></ul>
</div>
<h2><span id="References">References</span></h2>
<div>
<ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span><cite><a rel="nofollow" href="https://tools.ietf.org/id/draft-nottingham-rss-media-type-00">"The application/rss+xml Media Type"</a>. Network Working Group. May 22, 2006. <a rel="nofollow" href="https://web.archive.org/web/20220614140253/https://tools.ietf.org/id/draft-nottingham-rss-media-type-00.txt">Archived</a> from the original on June 14, 2022<span>. Retrieved <span>August 16,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+application%2Frss%2Bxml+Media+Type&amp;rft.pub=Network+Working+Group&amp;rft.date=2006-05-22&amp;rft_id=https%3A%2F%2Ftools.ietf.org%2Fid%2Fdraft-nottingham-rss-media-type-00&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-powers-2003-1-2"><span><b><a href="#cite_ref-powers-2003-1_2-0">^</a></b></span> <span><a href="#CITEREFPowers2003">Powers 2003</a>, p.&nbsp;10: "Another very common use of RDF/XML is in a version of RSS called RSS 1.0 or RDF/RSS. The meaning of the RSS abbreviation has changed over the years, but the basic premise behind it is to provide an XML-formatted feed consisting of an abstract of content and a link to a document containing the full content. When Netscape originally created the first implementation of an RSS specification, RSS stood for RDF Site Summary, and the plan was to use RDF/XML. When the company released, instead, a non-RDF XML version of the specification, RSS stood for Rich Site Summary. Recently, there has been increased activity with RSS, and two paths are emerging: one considers RSS to stand for Really Simple Syndication, a simple XML solution (promoted as RSS 2.0 by Dave Winer at Userland), and one returns RSS to its original roots of RDF Site Summary (RSS 1.0 by the RSS 1.0 Development group)."</span>
</li>
<li id="cite_note-Netsc99-3"><span>^ <a href="#cite_ref-Netsc99_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Netsc99_3-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFLibby,_Dan1999">Libby, Dan (July 10, 1999). <a rel="nofollow" href="https://web.archive.org/web/20001204093600/http://my.netscape.com/publish/formats/rss-spec-0.91.html">"RSS 0.91 Spec, revision 3"</a>. <a href="https://en.wikipedia.org/wiki/Netscape" title="Netscape">Netscape ttem</a>. Archived from <a rel="nofollow" href="http://my.netscape.com/publish/formats/rss-spec-0.91.html">the original</a> on December 4, 2000<span>. Retrieved <span>February 14,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+0.91+Spec%2C+revision+3&amp;rft.pub=Netscape+ttem&amp;rft.date=1999-07-10&amp;rft.au=Libby%2C+Dan&amp;rft_id=http%3A%2F%2Fmy.netscape.com%2Fpublish%2Fformats%2Frss-spec-0.91.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-GuardWF-4"><span><b><a href="#cite_ref-GuardWF_4-0">^</a></b></span> <span>"Web feeds | RSS | The Guardian | guardian.co.uk",
  <i>The Guardian</i>, London, 2008, webpage:
  <a rel="nofollow" href="https://www.theguardian.com/help/feeds">GuardianUK-webfeeds</a>. <a rel="nofollow" href="https://web.archive.org/web/20171215111443/https://www.theguardian.com/help/feeds">Archived</a> December 15, 2017, at the <a href="https://en.wikipedia.org/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a>.</span>
</li>
<li id="cite_note-Qstart-5"><span>^ <a href="#cite_ref-Qstart_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Qstart_5-1"><sup><i><b>b</b></i></sup></a></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20001208063100/http://my.netscape.com/publish/help/quickstart.html">"My Netscape Network: Quick Start"</a>. <a href="https://en.wikipedia.org/wiki/Netscape" title="Netscape">Netscape Communications</a>. Archived from <a rel="nofollow" href="http://my.netscape.com/publish/help/quickstart.html">the original</a> on December 8, 2000<span>. Retrieved <span>October 31,</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=My+Netscape+Network%3A+Quick+Start&amp;rft.pub=Netscape+Communications&amp;rft_id=http%3A%2F%2Fmy.netscape.com%2Fpublish%2Fhelp%2Fquickstart.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>

<li id="cite_note-7"><span><b><a href="#cite_ref-7">^</a></b></span> <span><cite id="CITEREFLash,_Alex1997">Lash, Alex (October 3, 1997). <a rel="nofollow" href="https://web.archive.org/web/20110809151456/http://news.cnet.com/2100-1001-203893.html">"W3C takes first step toward RDF spec"</a>. Archived from <a rel="nofollow" href="http://news.cnet.com/2100-1001-203893.html">the original</a> on August 9, 2011<span>. Retrieved <span>February 16,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=W3C+takes+first+step+toward+RDF+spec&amp;rft.date=1997-10-03&amp;rft.au=Lash%2C+Alex&amp;rft_id=http%3A%2F%2Fnews.cnet.com%2F2100-1001-203893.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-8"><span><b><a href="#cite_ref-8">^</a></b></span> <span><cite id="CITEREFHines1999">Hines, Matt (March 15, 1999). "Netscape Broadens Portal Content Strategy". <i>Newsbytes</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Newsbytes&amp;rft.atitle=Netscape+Broadens+Portal+Content+Strategy&amp;rft.date=1999-03-15&amp;rft.aulast=Hines&amp;rft.aufirst=Matt&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-9"><span><b><a href="#cite_ref-9">^</a></b></span> <span><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+History&amp;rft.date=2007-06-07&amp;rft.au=RSS+Advisory+Board&amp;rft_id=http%3A%2F%2Fwww.rssboard.org%2Frss-history&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-10"><span><b><a href="#cite_ref-10">^</a></b></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20001204123600/http://my.netscape.com/publish/help/futures.html">"MNN Future Directions"</a>. <a href="https://en.wikipedia.org/wiki/Netscape" title="Netscape">Netscape Communications</a>. Archived from <a rel="nofollow" href="http://my.netscape.com/publish/help/futures.html">the original</a> on December 4, 2000<span>. Retrieved <span>October 31,</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=MNN+Future+Directions&amp;rft.pub=Netscape+Communications&amp;rft_id=http%3A%2F%2Fmy.netscape.com%2Fpublish%2Fhelp%2Ffutures.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-11"><span><b><a href="#cite_ref-11">^</a></b></span> <span><cite id="CITEREFAndrew_King2003">Andrew King (April 13, 2003). <a rel="nofollow" href="https://web.archive.org/web/20070119031128/http://www.webreference.com/authoring/languages/xml/rss/1/">"The Evolution of RSS"</a>. Archived from <a rel="nofollow" href="http://www.webreference.com/authoring/languages/xml/rss/1/">the original</a> on January 19, 2007<span>. Retrieved <span>January 17,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+Evolution+of+RSS&amp;rft.date=2003-04-13&amp;rft.au=Andrew+King&amp;rft_id=http%3A%2F%2Fwww.webreference.com%2Fauthoring%2Flanguages%2Fxml%2Frss%2F1%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-12"><span><b><a href="#cite_ref-12">^</a></b></span> <span><cite id="CITEREFWiner,_Dave2000">Winer, Dave (June 4, 2000). <a rel="nofollow" href="https://web.archive.org/web/20061110001520/http://backend.userland.com/rss091#copyrightAndDisclaimer">"RSS 0.91: Copyright and Disclaimer"</a>. UserLand Software. Archived from <a rel="nofollow" href="http://backend.userland.com/rss091#copyrightAndDisclaimer">the original</a> on November 10, 2006<span>. Retrieved <span>October 31,</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+0.91%3A+Copyright+and+Disclaimer&amp;rft.pub=UserLand+Software&amp;rft.date=2000-06-04&amp;rft.au=Winer%2C+Dave&amp;rft_id=http%3A%2F%2Fbackend.userland.com%2Frss091%23copyrightAndDisclaimer&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-13"><span><b><a href="#cite_ref-13">^</a></b></span> <span><cite id="CITEREFU.S._Patent_&amp;_Trademark_Office">U.S. Patent &amp; Trademark Office. <a rel="nofollow" href="http://tarr.uspto.gov/servlet/tarr?regser=serial&amp;entry=78025336">"<span></span>'RSS' Trademark Latest Status Info"</a>. <a rel="nofollow" href="https://web.archive.org/web/20070816233807/http://tarr.uspto.gov/servlet/tarr?regser=serial&amp;entry=78025336">Archived</a> from the original on August 16, 2007<span>. Retrieved <span>September 4,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=%27RSS%27+Trademark+Latest+Status+Info&amp;rft.au=U.S.+Patent+%26+Trademark+Office&amp;rft_id=http%3A%2F%2Ftarr.uspto.gov%2Fservlet%2Ftarr%3Fregser%3Dserial%26entry%3D78025336&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-14"><span><b><a href="#cite_ref-14">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.harvardmagazine.com/2013/01/rss-creator-aaron-swartz-dead-at-26">"RSS Creator Aaron Swartz Dead at 26"</a>. <i>Harvard Magazine</i>. January 14, 2013. <a rel="nofollow" href="https://web.archive.org/web/20210629135531/https://www.harvardmagazine.com/2013/01/rss-creator-aaron-swartz-dead-at-26">Archived</a> from the original on June 29, 2021<span>. Retrieved <span>June 29,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Harvard+Magazine&amp;rft.atitle=RSS+Creator+Aaron+Swartz+Dead+at+26&amp;rft.date=2013-01-14&amp;rft_id=https%3A%2F%2Fwww.harvardmagazine.com%2F2013%2F01%2Frss-creator-aaron-swartz-dead-at-26&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-15"><span><b><a href="#cite_ref-15">^</a></b></span> <span><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RDF+Site+Summary+%28RSS%29+1.0&amp;rft.date=2000-12-09&amp;rft.au=RSS-DEV+Working+Group&amp;rft_id=http%3A%2F%2Fweb.resource.org%2Frss%2F1.0%2Fspec&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-16"><span><b><a href="#cite_ref-16">^</a></b></span> <span><cite id="CITEREFWiner,_Dave2000">Winer, Dave (December 25, 2000). <a rel="nofollow" href="https://web.archive.org/web/20110131184230/http://backend.userland.com/rss092">"RSS 0.92 Specification"</a>. UserLand Software. Archived from <a rel="nofollow" href="http://backend.userland.com/rss092">the original</a> on January 31, 2011<span>. Retrieved <span>October 31,</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+0.92+Specification&amp;rft.pub=UserLand+Software&amp;rft.date=2000-12-25&amp;rft.au=Winer%2C+Dave&amp;rft_id=http%3A%2F%2Fbackend.userland.com%2Frss092&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-17"><span><b><a href="#cite_ref-17">^</a></b></span> <span><cite id="CITEREFWiner,_Dave2001">Winer, Dave (April 20, 2001). <a rel="nofollow" href="https://web.archive.org/web/20061102171227/http://backend.userland.com/rss093">"RSS 0.93 Specification"</a>. UserLand Software. Archived from <a rel="nofollow" href="http://backend.userland.com/rss093">the original</a> on November 2, 2006<span>. Retrieved <span>October 31,</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+0.93+Specification&amp;rft.pub=UserLand+Software&amp;rft.date=2001-04-20&amp;rft.au=Winer%2C+Dave&amp;rft_id=http%3A%2F%2Fbackend.userland.com%2Frss093&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-18"><span><b><a href="#cite_ref-18">^</a></b></span> <span><cite id="CITEREFHarvard_Law2007">Harvard Law (April 14, 2007). <a rel="nofollow" href="http://cyber.law.harvard.edu/rss/toplevelNamespace.html">"Top-level namespaces"</a>. <a rel="nofollow" href="https://web.archive.org/web/20110605164517/http://cyber.law.harvard.edu/rss/toplevelNamespace.html">Archived</a> from the original on June 5, 2011<span>. Retrieved <span>August 3,</span> 2009</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Top-level+namespaces&amp;rft.date=2007-04-14&amp;rft.au=Harvard+Law&amp;rft_id=http%3A%2F%2Fcyber.law.harvard.edu%2Frss%2FtoplevelNamespace.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-19"><span><b><a href="#cite_ref-19">^</a></b></span> <span><cite id="CITEREFFesta2003">Festa, Paul (August 4, 2003). <a rel="nofollow" href="http://news.cnet.com/Battle-of-the-blog/2009-1032_3-5059006.html">"Dispute exposes bitter power struggle behind Web logs"</a>. news.cnet.com. <a rel="nofollow" href="https://web.archive.org/web/20090806234534/http://news.cnet.com/Battle-of-the-blog/2009-1032_3-5059006.html">Archived</a> from the original on August 6, 2009<span>. Retrieved <span>August 6,</span> 2008</span>. <q>The conflict centers on something called Really Simple Syndication (RSS), a technology widely used to syndicate blogs and other Web content. The dispute pits Harvard Law School fellow Dave Winer, the blogging pioneer who is the key gatekeeper of RSS, against advocates of a different format.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Dispute+exposes+bitter+power+struggle+behind+Web+logs&amp;rft.pub=news.cnet.com&amp;rft.date=2003-08-04&amp;rft.aulast=Festa&amp;rft.aufirst=Paul&amp;rft_id=http%3A%2F%2Fnews.cnet.com%2FBattle-of-the-blog%2F2009-1032_3-5059006.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-20"><span><b><a href="#cite_ref-20">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.rssboard.org/advisory-board-notes">"Advisory Board Notes"</a>. <a href="https://en.wikipedia.org/wiki/RSS_Advisory_Board" title="RSS Advisory Board">RSS Advisory Board</a>. July 18, 2003. <a rel="nofollow" href="https://web.archive.org/web/20070927051743/http://www.rssboard.org/advisory-board-notes">Archived</a> from the original on September 27, 2007<span>. Retrieved <span>September 4,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Advisory+Board+Notes&amp;rft.pub=RSS+Advisory+Board&amp;rft.date=2003-07-18&amp;rft_id=http%3A%2F%2Fwww.rssboard.org%2Fadvisory-board-notes&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-21"><span><b><a href="#cite_ref-21">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.scripting.com/2003/07/18.html#rss20News">"RSS 2.0 News"</a>. <i>Scripting News</i>. <a href="https://en.wikipedia.org/wiki/Dave_Winer" title="Dave Winer">Dave Winer</a>. July 18, 2003. <a rel="nofollow" href="https://web.archive.org/web/20070822014007/http://www.scripting.com/2003/07/18.html#rss20News">Archived</a> from the original on August 22, 2007<span>. Retrieved <span>September 4,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Scripting+News&amp;rft.atitle=RSS+2.0+News&amp;rft.date=2003-07-18&amp;rft_id=http%3A%2F%2Fwww.scripting.com%2F2003%2F07%2F18.html%23rss20News&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-22"><span><b><a href="#cite_ref-22">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.squarefree.com/burningedge/2004/09/26/2004-09-26-branch-builds/">"2004-09-26 Branch builds"</a>. <i>The Burning Edge</i>. September 26, 2004. <a rel="nofollow" href="https://web.archive.org/web/20141009071447/http://www.squarefree.com/burningedge/2004/09/26/2004-09-26-branch-builds/">Archived</a> from the original on October 9, 2014<span>. Retrieved <span>October 6,</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Burning+Edge&amp;rft.atitle=2004-09-26+Branch+builds&amp;rft.date=2004-09-26&amp;rft_id=http%3A%2F%2Fwww.squarefree.com%2Fburningedge%2F2004%2F09%2F26%2F2004-09-26-branch-builds%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-23"><span><b><a href="#cite_ref-23">^</a></b></span> <span>"<a rel="nofollow" href="https://web.archive.org/web/20051217102644/http://blogs.msdn.com/michael_affronti/archive/2005/12/15/504316.aspx">RSS icon goodness</a>", blog post by Michael A. Affronti of Microsoft (Outlook Program Manager), December 15, 2005</span>
</li>
<li id="cite_note-24"><span><b><a href="#cite_ref-24">^</a></b></span> <span><cite id="CITEREFtrond2006">trond (February 16, 2006). <a rel="nofollow" href="https://web.archive.org/web/20100417170259/http://my.opera.com/desktopteam/blog/show.dml/146296">"Making love to the new feed icon"</a>. Opera Desktop Team. Archived from <a rel="nofollow" href="http://my.opera.com/desktopteam/blog/show.dml/146296">the original</a> on April 17, 2010<span>. Retrieved <span>July 4,</span> 2010</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Making+love+to+the+new+feed+icon&amp;rft.pub=Opera+Desktop+Team&amp;rft.date=2006-02-16&amp;rft.au=trond&amp;rft_id=http%3A%2F%2Fmy.opera.com%2Fdesktopteam%2Fblog%2Fshow.dml%2F146296&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-W3C_REC_XML_Namespace-25"><span><b><a href="#cite_ref-W3C_REC_XML_Namespace_25-0">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.w3.org/TR/REC-xml-names/">"Namespaces in XML 1.0"</a> (2nd&nbsp;ed.). W3C. August 16, 2006. <a rel="nofollow" href="https://web.archive.org/web/20110316043909/http://www.w3.org/TR/REC-xml-names/">Archived</a> from the original on March 16, 2011<span>. Retrieved <span>May 22,</span> 2008</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Namespaces+in+XML+1.0&amp;rft.edition=2nd&amp;rft.pub=W3C&amp;rft.date=2006-08-16&amp;rft_id=http%3A%2F%2Fwww.w3.org%2FTR%2FREC-xml-names%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-26"><span><b><a href="#cite_ref-26">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.w3.org/2001/10/glance/doc/howto.html">"W3C RSS 1.0 News Feed Creation How-To"</a>. <i>www.w3.org</i>. <a rel="nofollow" href="https://web.archive.org/web/20220614140126/https://www.w3.org/2001/10/glance/doc/howto.html">Archived</a> from the original on June 14, 2022<span>. Retrieved <span>February 5,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.w3.org&amp;rft.atitle=W3C+RSS+1.0+News+Feed+Creation+How-To&amp;rft_id=https%3A%2F%2Fwww.w3.org%2F2001%2F10%2Fglance%2Fdoc%2Fhowto.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-27"><span><b><a href="#cite_ref-27">^</a></b></span> <span><cite id="CITEREFHolzner">Holzner, Steven. <a rel="nofollow" href="http://www.peachpit.com/articles/article.aspx?p=674690">"Peachpit article"</a>. Peachpit article. <a rel="nofollow" href="https://web.archive.org/web/20111109173320/http://www.peachpit.com/articles/article.aspx?p=674690">Archived</a> from the original on November 9, 2011<span>. Retrieved <span>December 11,</span> 2010</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Peachpit+article&amp;rft.pub=Peachpit+article&amp;rft.aulast=Holzner&amp;rft.aufirst=Steven&amp;rft_id=http%3A%2F%2Fwww.peachpit.com%2Farticles%2Farticle.aspx%3Fp%3D674690&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-28"><span><b><a href="#cite_ref-28">^</a></b></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20020803040757/http://www.syndic8.com/stats.php?Section=feeds#tabtable">"Syndic8 stats table"</a>. Syndic8.com. Archived from <a rel="nofollow" href="http://www.syndic8.com/stats.php?Section=feeds#tabtable">the original</a> on August 3, 2002<span>. Retrieved <span>August 12,</span> 2011</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Syndic8+stats+table&amp;rft.pub=Syndic8.com&amp;rft_id=http%3A%2F%2Fwww.syndic8.com%2Fstats.php%3FSection%3Dfeeds%23tabtable&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-29"><span><b><a href="#cite_ref-29">^</a></b></span> <span><cite><a rel="nofollow" href="https://lifehacker.com/the-best-podcast-search-engine-1818560337">"The Best Podcast Search Engine"</a>. <i>Lifehacker</i>. September 20, 2017. <a rel="nofollow" href="https://web.archive.org/web/20201129195032/https://lifehacker.com/the-best-podcast-search-engine-1818560337">Archived</a> from the original on November 29, 2020<span>. Retrieved <span>February 5,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Lifehacker&amp;rft.atitle=The+Best+Podcast+Search+Engine&amp;rft.date=2017-09-20&amp;rft_id=https%3A%2F%2Flifehacker.com%2Fthe-best-podcast-search-engine-1818560337&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-30"><span><b><a href="#cite_ref-30">^</a></b></span> <span><cite><a rel="nofollow" href="https://blogtrottr.com/">"Free realtime RSS and Atom feed to email service. Get your favourite blogs, feeds, and news delivered to your inbox"</a>. <a rel="nofollow" href="https://web.archive.org/web/20170128081150/http://blogtrottr.com/">Archived</a> from the original on January 28, 2017<span>. Retrieved <span>January 26,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Free+realtime+RSS+and+Atom+feed+to+email+service.+Get+your+favourite+blogs%2C+feeds%2C+and+news+delivered+to+your+inbox.&amp;rft_id=https%3A%2F%2Fblogtrottr.com%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-31"><span><b><a href="#cite_ref-31">^</a></b></span> <span><cite><a rel="nofollow" href="https://rss.com/">"RSS Feed Reader, your tool for saving time and money at RSS.com"</a>. <a rel="nofollow" href="https://web.archive.org/web/20170125224151/https://www.rss.com/">Archived</a> from the original on January 25, 2017<span>. Retrieved <span>January 26,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+Feed+Reader%2C+your+tool+for+saving+time+and+money+at+RSS.com&amp;rft_id=https%3A%2F%2Frss.com%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-32"><span><b><a href="#cite_ref-32">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.uslsoftware.com/how-to-use-thunderbird-to-get-rss-feeds/">"How to use Thunderbird to get RSS feeds! Here's How it Works"</a>. October 17, 2018. <a rel="nofollow" href="https://web.archive.org/web/20210413005112/https://www.uslsoftware.com/how-to-use-thunderbird-to-get-rss-feeds/">Archived</a> from the original on April 13, 2021<span>. Retrieved <span>February 5,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=How+to+use+Thunderbird+to+get+RSS+feeds%21+Here%27s+How+it+Works&amp;rft.date=2018-10-17&amp;rft_id=https%3A%2F%2Fwww.uslsoftware.com%2Fhow-to-use-thunderbird-to-get-rss-feeds%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-33"><span><b><a href="#cite_ref-33">^</a></b></span> <span><cite id="CITEREFLeslie_Sikos2011">Leslie Sikos (2011). <a rel="nofollow" href="https://web.archive.org/web/20150402152305/http://www.masteringhtml5css3.com/"><i>Web standards – Mastering HTML5, CSS3, and XML</i></a>. <a href="https://en.wikipedia.org/wiki/Apress" title="Apress">Apress</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-1-4302-4041-9" title="Special:BookSources/978-1-4302-4041-9"><bdi>978-1-4302-4041-9</bdi></a>. Archived from <a rel="nofollow" href="http://www.masteringhtml5css3.com/">the original</a> on April 2, 2015<span>. Retrieved <span>June 14,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Web+standards+%E2%80%93+Mastering+HTML5%2C+CSS3%2C+and+XML&amp;rft.pub=Apress&amp;rft.date=2011&amp;rft.isbn=978-1-4302-4041-9&amp;rft.au=Leslie+Sikos&amp;rft_id=http%3A%2F%2Fwww.masteringhtml5css3.com&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-ClosureAnnouncement-34"><span><b><a href="#cite_ref-ClosureAnnouncement_34-0">^</a></b></span> <span><cite id="CITEREFHölzle">Hölzle, Urs. <a rel="nofollow" href="http://googleblog.blogspot.com/2013/03/a-second-spring-of-cleaning.html">"A second spring of cleaning"</a>. googleblog.blogspot.com. <a rel="nofollow" href="https://web.archive.org/web/20130314045128/http://googleblog.blogspot.com/2013/03/a-second-spring-of-cleaning.html">Archived</a> from the original on March 14, 2013<span>. Retrieved <span>March 14,</span> 2013</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=A+second+spring+of+cleaning&amp;rft.pub=googleblog.blogspot.com&amp;rft.aulast=H%C3%B6lzle&amp;rft.aufirst=Urs&amp;rft_id=http%3A%2F%2Fgoogleblog.blogspot.com%2F2013%2F03%2Fa-second-spring-of-cleaning.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>

<li id="cite_note-36"><span><b><a href="#cite_ref-36">^</a></b></span> <span><cite><a rel="nofollow" href="http://osxdaily.com/2014/11/03/subscribe-rss-feeds-safari-os-x/">"Subscribe to RSS Feeds in Safari for OS X Yosemite"</a>. OSX Daily. November 3, 2014. <a rel="nofollow" href="https://web.archive.org/web/20150121185232/http://osxdaily.com/2014/11/03/subscribe-rss-feeds-safari-os-x/">Archived</a> from the original on January 21, 2015<span>. Retrieved <span>January 24,</span> 2015</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Subscribe+to+RSS+Feeds+in+Safari+for+OS+X+Yosemite&amp;rft.pub=OSX+Daily&amp;rft.date=2014-11-03&amp;rft_id=http%3A%2F%2Fosxdaily.com%2F2014%2F11%2F03%2Fsubscribe-rss-feeds-safari-os-x%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-37"><span><b><a href="#cite_ref-37">^</a></b></span> <span><cite id="CITEREFCimpanu2018">Cimpanu, Catalin (July 26, 2018). <a rel="nofollow" href="https://www.bleepingcomputer.com/news/software/mozilla-to-remove-support-for-built-in-feed-reader-from-firefox/">"Mozilla to Remove Support for Built-In Feed Reader From Firefox"</a>. <i>BleepingComputer</i>. <a rel="nofollow" href="https://web.archive.org/web/20180726144716/https://www.bleepingcomputer.com/news/software/mozilla-to-remove-support-for-built-in-feed-reader-from-firefox/">Archived</a> from the original on July 26, 2018<span>. Retrieved <span>July 26,</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=BleepingComputer&amp;rft.atitle=Mozilla+to+Remove+Support+for+Built-In+Feed+Reader+From+Firefox&amp;rft.date=2018-07-26&amp;rft.aulast=Cimpanu&amp;rft.aufirst=Catalin&amp;rft_id=https%3A%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fsoftware%2Fmozilla-to-remove-support-for-built-in-feed-reader-from-firefox%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-38"><span><b><a href="#cite_ref-38">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.mozilla.org/en-US/firefox/64.0/releasenotes/">"Firefox 64.0, See All New Features, Updates and Fixes"</a>. <i>Mozilla</i>. December 11, 2018. <a rel="nofollow" href="https://web.archive.org/web/20181211143259/https://www.mozilla.org/en-US/firefox/64.0/releasenotes/">Archived</a> from the original on December 11, 2018<span>. Retrieved <span>December 12,</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Mozilla&amp;rft.atitle=Firefox+64.0%2C+See+All+New+Features%2C+Updates+and+Fixes&amp;rft.date=2018-12-11&amp;rft_id=https%3A%2F%2Fwww.mozilla.org%2Fen-US%2Ffirefox%2F64.0%2Freleasenotes%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-39"><span><b><a href="#cite_ref-39">^</a></b></span> <span><cite id="CITEREFBarrett2018">Barrett, Brian (March 30, 2018). <a rel="nofollow" href="https://www.wired.com/story/rss-readers-feedly-inoreader-old-reader/">"It's Time for an RSS Revival"</a>. <i>Wired</i>. <a rel="nofollow" href="https://web.archive.org/web/20210812114050/https://www.wired.com/story/rss-readers-feedly-inoreader-old-reader/">Archived</a> from the original on August 12, 2021<span>. Retrieved <span>July 26,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wired&amp;rft.atitle=It%27s+Time+for+an+RSS+Revival&amp;rft.date=2018-03-30&amp;rft.aulast=Barrett&amp;rft.aufirst=Brian&amp;rft_id=https%3A%2F%2Fwww.wired.com%2Fstory%2Frss-readers-feedly-inoreader-old-reader%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-40"><span><b><a href="#cite_ref-40">^</a></b></span> <span><cite id="CITEREFCampbell2021">Campbell, Ian Carlos (October 8, 2021). <a rel="nofollow" href="https://www.theverge.com/2021/10/8/22716813/google-chrome-follow-button-rss-reader">"Google Reader is still defunct, but now you can 'follow' RSS feeds in Chrome on Android"</a>. <i>The Verge</i>. <a rel="nofollow" href="https://web.archive.org/web/20220605165318/https://www.theverge.com/2021/10/8/22716813/google-chrome-follow-button-rss-reader">Archived</a> from the original on June 5, 2022<span>. Retrieved <span>June 19,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Verge&amp;rft.atitle=Google+Reader+is+still+defunct%2C+but+now+you+can+%27follow%27+RSS+feeds+in+Chrome+on+Android&amp;rft.date=2021-10-08&amp;rft.aulast=Campbell&amp;rft.aufirst=Ian+Carlos&amp;rft_id=https%3A%2F%2Fwww.theverge.com%2F2021%2F10%2F8%2F22716813%2Fgoogle-chrome-follow-button-rss-reader&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
</ol></div>
<h2><span id="External_links">External links</span></h2>

<ul><li><a rel="nofollow" href="http://www.rssboard.org/rss-0-9-0">RSS 0.90 Specification</a></li>
<li><a rel="nofollow" href="http://www.rssboard.org/rss-0-9-1-netscape">RSS 0.91 Specification</a></li>
<li><a rel="nofollow" href="http://web.resource.org/rss/1.0/">RSS 1.0 Specifications</a></li>
<li><a rel="nofollow" href="http://www.rssboard.org/rss-specification">RSS 2.0 Specification</a></li>
<li><a rel="nofollow" href="https://web.archive.org/web/20110718034619/http://diveintomark.org/archives/2002/09/06/history_of_the_rss_fork">History of the RSS Fork</a> (Mark Pilgrim)</li>
<li><a rel="nofollow" href="https://www.xul.fr/en-xml-rss.html">Building an RSS feed</a> Tutorial with example</li></ul>





<!-- 
NewPP limit report
Parsed by mw‐web.eqiad.main‐7d644d6d99‐qsftd
Cached time: 20240315110540
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.817 seconds
Real time usage: 1.060 seconds
Preprocessor visited node count: 4966/1000000
Post‐expand include size: 166970/2097152 bytes
Template argument size: 5945/2097152 bytes
Highest expansion depth: 24/100
Expensive parser function count: 17/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 148753/5000000 bytes
Lua time usage: 0.512/10.000 seconds
Lua memory usage: 8675186/52428800 bytes
Number of Wikibase entities loaded: 1/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  915.847      1 -total
 27.41%  251.036      1 Template:Reflist
 16.24%  148.715     34 Template:Cite_web
 10.77%   98.630      9 Template:Navbox
  8.56%   78.396      1 Template:Web_syndication
  8.45%   77.375      1 Template:Infobox_file_format
  8.43%   77.162      1 Template:Short_description
  8.05%   73.749      1 Template:Infobox
  7.57%   69.316     13 Template:Main_other
  6.33%   58.011      2 Template:Cite_book
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:93489-0!canonical and timestamp 20240315110540 and revision id 1210081177. Rendering was triggered because: page-view
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Postgres is eating the database world (219 pts)]]></title>
            <link>https://medium.com/@fengruohang/postgres-is-eating-the-database-world-157c204dcfc4</link>
            <guid>39711863</guid>
            <pubDate>Fri, 15 Mar 2024 03:43:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/@fengruohang/postgres-is-eating-the-database-world-157c204dcfc4">https://medium.com/@fengruohang/postgres-is-eating-the-database-world-157c204dcfc4</a>, See on <a href="https://news.ycombinator.com/item?id=39711863">Hacker News</a></p>
Couldn't get https://medium.com/@fengruohang/postgres-is-eating-the-database-world-157c204dcfc4: Error: Request failed with status code 429]]></description>
        </item>
        <item>
            <title><![CDATA[Vision Pro: What we got wrong at Oculus that Apple got right (246 pts)]]></title>
            <link>https://hugo.blog/2024/03/11/vision-pro/</link>
            <guid>39711725</guid>
            <pubDate>Fri, 15 Mar 2024 03:15:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hugo.blog/2024/03/11/vision-pro/">https://hugo.blog/2024/03/11/vision-pro/</a>, See on <a href="https://news.ycombinator.com/item?id=39711725">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><em>by <a href="https://twitter.com/hbarra" target="_blank" rel="noreferrer noopener">Hugo Barra</a></em> (former Head of Oculus at Meta)</p>



<p>Friends and colleagues have been asking me to share my perspective on the Apple Vision Pro as a product. Inspired by my dear friend <a href="https://ma.tt/category/birthday/" target="_blank" rel="noreferrer noopener">Matt Mullenweg’s 40th post</a>, I decided to put pen to paper.</p>



<p>This started as blog post and became an essay before too long, so I’ve structured my writing in multiple sections each with a clear lead to make it a bit easier to digest — peppered with my own ‘takes’. I’ve tried to stick to original thoughts for the most part and link to what others have said where applicable.</p>



<p>Some of the topics I touch on:</p>



<ul>
<li>Why I believe Vision Pro may be an over-engineered “devkit”</li>



<li>The genius &amp; audacity behind some of Apple’s hardware decisions</li>



<li><em>Gaze &amp; pinch</em> is an incredible UI superpower and major industry ah-ha moment</li>



<li>Why the Vision Pro software/content story is so dull and unimaginative</li>



<li>Why most people won’t use Vision Pro for watching TV/movies</li>



<li>Apple’s bet in immersive video is a total game-changer for <em>Live Sports</em></li>



<li>Why I returned my Vision Pro… and my Top 10 wishlist to reconsider</li>



<li>Apple’s VR debut is the best thing that ever happened to Oculus/Meta</li>



<li>My unsolicited product advice to Meta for <em>Quest Pro 2</em> and beyond</li>
</ul>



<h2><strong>The Apple Vision Pro is the Northstar the VR industry needed, whether we admit it or not</strong></h2>



<p>I’ve been a VR enthusiast for most of my adult life, from working as an intern at <a href="https://www.roadtovr.com/end-of-an-era-disneyquest-first-vr-attraction-set-to-close/" target="_blank" rel="noreferrer noopener">Disney Quest VR</a> in the 1990s, to being an early backer of the <a href="https://www.kickstarter.com/projects/1523379957/oculus-rift-step-into-the-game">Oculus Rift DK1 on Kickstarter</a> in 2013, to leading the Oculus VR/AR team at Meta from 2017 to 2020 (and getting to work alongside VR legends like John Carmack, Brendan Iribe and Jason Rubin), and always testing every VR product or experience I can get my hands on.</p>



<p>Back in my Oculus days, I used to semi-seriously joke with our team (and usually got a lot of heat for it!) that the best thing that could ever happen to us was having Apple enter the VR industry and become a direct competitor to Oculus. I’ve always believed that strong competition pushes a team to do their best work in any industry. This became clear to me especially after living for nearly 10 years at the center of the iOS/Android battle of ecosystems where each side made the other infinitely better by constantly raising the bar on UX, features, performance, developer APIs etc, and seeing each side respond by not only fast following but usually also improving on what the other had released. (And this definitely went both ways: iOS copied Android as much as Android copied iOS).</p>



<p>But in the case of VR at Oculus, we also never really felt like the world had a Northstar that could truly capture human hearts and minds, and without that it would be impossible to transition VR from being a niche gamer tech to the incredible spatial computing paradigm that we always thought it potentially represented (which I still very much believe in). Apple could <em>really </em>help us if they cared about VR.</p>



<p>The Vision Pro launch has more or less done exactly what I had always hoped for, which is to build a huge wave of awareness and curiosity that elevates the spatial computing ecosystem and could ultimately lead to mass-market consumer demand and a lot more developer interest that VR has ever had. Now it’s up to the industry to create enough user value and demonstrate whether this is in fact the future of computing.</p>



<h2><strong>The Vision Pro’s <em>instant magic</em> comes down to just: (1) an unprecedented new level of presence in VR, and (2) a new UI superpower using <strong>gaze &amp; pinch</strong></strong></h2>



<p>Using Vision Pro is an instantly magical and intuitive experience — whether or not you’ve used other VR headsets — purely because of Apple’s unrelenting focus on delivering two specific capabilities that speak to our humanity:</p>



<p><strong>1) Feeling present and connected to your physical world</strong>: thanks to a high-fidelity passthrough (“mixed reality”) experience with very low latency, excellent distortion correction (<em>much</em> better than Quest 3), and sufficiently high resolution that allows you to even see your phone/computer screen through the passthrough cameras (i.e. without taking your headset off).</p>



<p>Even though there are major gaps left to be filled in future versions of the Vision Pro hardware (which I’ll get into later), this level of connection with the real world — or “presence” as VR folks like to call it — is something that no other VR headset has ever come even close to delivering and so far was only remotely possible with AR headsets (ex: HoloLens and Magic Leap) which feature physically transparent displays but have their own significant limitations in many other areas. Apple’s implementation of Optic ID as an overlay on top of live passthrough is a beautiful design decision that only enhances this sense of presence.</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong>The Vision Pro high-fidelity passthrough experience parallels <strong>Apple’s introduction of the iPhone’s original <em>retina display</em></strong>, which set a new experience bar and gold standard in mobile display fidelity. While much remains to be improved in the Vision Pro passthrough experience, Apple is unquestionably setting a new standard for all future headsets (by any vendor) that VR passthrough must be good enough to closely resemble reality.</mark></p>
</blockquote>



<p><strong>2) Having a new UI superpower with gaze &amp; pinch</strong>, thanks to a very precise eye tracking system (with 2 dedicated cameras per eye) embedded into the lenses, coupled with a wide-field-of-view hand tracking system that can “see” a finger pinch even with your hands are down or resting on your lap. Because it works so effortlessly for the user, it really feels like having a new “laser vision” superpower.</p>



<p>The hardware needed to track eyes and hands in VR has been around for over a decade, and it’s Apple unique ability to bring everything together in a magical way that makes this UI superpower the most important achievement of the entire Vision Pro product, without a shadow of doubt.</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong>The Vision Pro’s new “gaze + pinch” input modality is <strong>the VR equivalent of the iPhone’s capacitive multi-touch gestures</strong>. Introduced by Apple with the first iPhone launch nearly 17 years ago, multi-touch instantly became a new standard that changed computing forever. “Gaze + pinch” is so groundbreaking that it’s an instant defacto standard for VR interaction that future VR headsets be forced to adopt sooner or later. It’s also going to be a huge developer unlock that leads to gaze-based interaction ideas that will blow our minds. </mark></p>
</blockquote>



<h2>Hardware</h2>



<h2><strong>Vision Pro is a meticulously over-engineered “devkit” that is <em>far too heavy</em> <em>to have product-market fit</em> but good enough to seed curiosity into the world</strong></h2>



<p>The Oculus VR story began with the 2013 launch of <em><a href="https://en.wikipedia.org/wiki/Oculus_Rift#Development_Kit_1" target="_blank" rel="noreferrer noopener">Oculus Rift DK1</a></em> (short for “devkit v1” or “development kit v1”). This was a headset launched by the original Oculus startup team — years before it was acquired by Facebook — with the explicit goal of seeding developer interest well before a commercial release. Given that VR was a non-existing market then, releasing a devkit was the correct and necessary strategy <em>there and then</em> for a startup to start building a content library as well as momentum among enthusiasts ahead of launching a consumer product. The team released a <a href="https://en.wikipedia.org/wiki/Oculus_Rift#Development_Kit_2" target="_blank" rel="noreferrer noopener"><em>DK2</em></a> about a year later in 2014, and finally launched the first Oculus Rift consumer headset in 2015.</p>







<figure><img data-attachment-id="208" data-permalink="https://hugo.blog/2024/03/11/vision-pro/palmer-luckey-oculus-dk1/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png" data-orig-size="1500,844" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="palmer-luckey-oculus-dk1" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=1024" width="1024" height="576" src="https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=768 768w, https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png 1500w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Oculus co-founder Palmer Luckey wearing the original Oculus Rift DK1 released in 2013 </figcaption></figure>



<p>When I joined Facebook to lead the Oculus team in 2017 after the acquisition, one of the many battles I found myself in the middle of almost immediately was the “devkit war”. The Oculus team’s DK1 and DK2 legacy was so strong that it was not uncommon to hear arguments in product meetings pushing for us to launch VR headsets still in prototype stage as “devkits” to end users. Since Oculus was no longer a startup — and had the resources to both extensively test prototypes without launching them as products <em>and </em>run extensive pre-launch developer programs — it no longer made sense for Oculus devkits to exist. This stance often didn’t make me very popular amongst some of my Oculus OG colleagues.</p>



<p>Fast forward to 2024. After the Vision Pro launch, the VR hardware enthusiast community (including Oculus OG folks I’m still in touch with) quickly arrived at the conclusion that Apple really played it safe in the design of this first VR product by over-engineering it. For starters, Vision Pro ships with more sensors than what’s likely necessary to deliver Apple’s intended experience. This is typical in a first-generation product that’s been under development for so many years. It makes Vision Pro start to feel like a devkit.</p>







<figure><img data-attachment-id="148" data-permalink="https://hugo.blog/2024/03/11/vision-pro/visionpro_sensors/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png" data-orig-size="804,486" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="visionpro_sensors" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=804" width="804" height="486" src="https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=804" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png 804w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=768 768w" sizes="(max-width: 804px) 100vw, 804px"><figcaption>A sensor party: 6 tracking cameras, 2 passthrough cameras, 2 depth sensors<br>(plus 4 eye-tracking cameras not shown)</figcaption></figure>



<p>Here’s a quick comparison with existing VR headsets:</p>







<figure><table><thead><tr><th>Sensor</th><th>Vision Pro</th><th>Meta <br>Quest 3</th><th>Meta <br>Quest Pro</th></tr></thead><tbody><tr><td>Environment passthrough cameras</td><td>2</td><td>2</td><td>1</td></tr><tr><td>World tracking cameras</td><td>6</td><td>4</td><td>6</td></tr><tr><td>Depth sensors</td><td>2</td><td>1</td><td>–</td></tr><tr><td>Eye tracking cameras</td><td>4</td><td>–</td><td>2</td></tr><tr><td><strong>Total</strong></td><td><strong>14</strong></td><td><strong>7</strong></td><td><strong>9</strong></td></tr></tbody></table><figcaption>Side-by-side comparison with the sensor stacks of other VR headsets</figcaption></figure>



<p>This over-spec’ing is unsurprising and characteristic of a v1 product where its creator wants to ensure it survives the hardest tests early users will no doubt want to put the product through. It’s also a way for Apple to see how far developers will push the product’s capabilities, as Apple is no doubt relying on that community to produce the majority of software/content magic for this new type of computer, as they’ve previously done with every other device class.</p>



<p>Apple’s decision to over-spec the Vision Pro does, however, lead to the inevitable consequence of a headset weighing above 600g — heavier than most other VR headsets in the market to date — that <strong>makes it difficult for most people to wear it for more than 30-45 minutes at a time without suffering a lot of discomfort</strong>. Most of the discomfort comes in the form of pressure against the user’s face and the back of the person’s head.</p>







<blockquote>
<p><mark><strong>MY TAKE:</strong> Because of its heavy weight, Vision Pro has inevitably landed in the world as a high-quality “devkit” designed to capture everyone’s curiosity, hearts &amp; minds with its magic (especially through the voice of enthusiastic tech influencers) while being realistically focused on developers as its primary audience. In other words, the Vision Pro is a devkit that helps prepare the world to receive a more mainstream Apple VR headset that could have product-market fit in 1 or 2 generations.</mark></p>
</blockquote>



<p>All things considered, I do believe Apple’s calculus was correct in prioritizing launching a first-generation product with fewer experience and design compromises at the expense of user comfort. And while many people have argued Apple could have avoided this major comfort issue by redistributing weight or using lighter materials, those attempts would have come at the expense of beauty and design. (I’ll come back to the weight issue shortly.)</p>



<p>With this in mind, it’s easy to understand two particularly important decisions Apple made for the Vision Pro launch:</p>



<ul>
<li><strong>Designing an incredible in-store Vision Pro demo experience</strong>, with the primary goal of getting as many people as possible to experience the magic of VR through Apple’s lenses — most of whom have no intention to even consider a $4,000 purchase. The demo is only secondarily focused on actually selling Vision Pro headsets.</li>



<li><strong>Launching an iconic woven strap that photographs beautifully</strong> even though this strap simply isn’t comfortable enough for the vast majority of head shapes. It’s easy to conclude that this decision paid off because nearly every bit of media coverage (including and especially third-party reviews on YouTube) uses the woven strap despite the fact that it’s less comfortable than the dual loop strap that’s “hidden in the box”.</li>
</ul>



<h2><strong>The existence of Vision Pro in 2024 is entirely a function of Apple managing to ship a <em>first-of-its-kind</em> ultra <strong>high-resolution display</strong></strong></h2>



<p>One of our biggest product positioning struggles within the Oculus VR team from the very beginning — especially when trying to convince reviewers — was always related to having <em>underwhelming displays</em>. Every single Oculus headset that ever shipped (including the latest Quest 3) has suffered from resolution/pixelation issues varying from “terrible” to “pretty bad”. It’s like we’re living in the VR-equivalent world of VGA computer monitors.</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong>In order For Apple to make a huge splash entering the VR market — a category that’s been around the consumer world for nearly 10 years — they needed to launch a product that was unambiguously better than anything that had ever existed. The obvious way to do that was to <strong>attack the Achilles heel of all existing headsets and reinvent the VR display</strong>, and that’s exactly what Apple did with the Vision Pro.</mark></p>
</blockquote>



<p>Vision Pro is the first VR headset that offers good enough resolution and visual acuity with little semblance of a <a href="https://pimax.com/what-is-the-screen-door-effect-in-vr/" target="_blank" rel="noreferrer noopener"><em>screen door effect</em></a> or pixelation artifacts. This level of presence and fidelity could only be made possible with an ultra high-res display, and it’s 100% clear that achieving an first-of-its-kind level of display quality was the internal launch bar for Vision Pro at Apple.</p>



<p>Apple’s relentless and uncompromising hardware insanity is largely what made it possible for such a high-res display to exist in a VR headset, and it’s clear that this product couldn’t possibly have launched much sooner than 2024 for one simple limiting factor — the maturity of micro-OLED displays plus the existence of power-efficient chipsets that can deliver the heavy compute required to drive this kind of display (i.e. the M2).</p>



<p>Micro-OLED displays differ from any other previous consumer display technology because they are manufactured on top of a silicon substrate (similar to how semiconductor chips are made). To put the insanity of micro-OLED displays in perspective, the <strong>Vision Pro panel has a 7.4x higher pixel density than the latest iPhone and nearly 3x the Quest 3</strong>:</p>







<figure><table><thead><tr><th>Feature</th><th>Vision Pro</th><th>Bigscreen Beyond</th><th>Quest 3</th><th>iPhone 15 Pro Max</th></tr></thead><tbody><tr><td>Display Type</td><td>Micro-OLED</td><td>Micro-OLED</td><td>LCD</td><td>OLED</td></tr><tr><td>Resolution <br>(pixels per eye)</td><td>3660 x 3200</td><td>2560 x 2560</td><td>2064 x 2208</td><td>2796 x 1290</td></tr><tr><td>Total Pixels</td><td>23 million</td><td>13 million</td><td>9 million</td><td>3.6 million</td></tr><tr><td>Pixels Per Inch (PPI)</td><td>3386</td><td>(unknown)</td><td>1218</td><td>460</td></tr><tr><td>Pixels Per Degree (PPD)</td><td>34</td><td>32</td><td>25</td><td>94 <br>(at 1 foot distance)</td></tr></tbody></table></figure>



<p><em>(See the appendix of this essay for a quick explanation of <strong>Pixels Per Degree </strong>or <strong>PPD</strong>)</em></p>



<p>The folks at iFixit created this stunning GIF using a scientific microscope to compare the pixel size of the Vision Pro display — which measures 7.5 μm, the size of a human red blood cell — with the pixel size of the latest iPad and iPhone displays:</p>







<figure><img data-attachment-id="54" data-permalink="https://hugo.blog/2024/03/11/vision-pro/avp-display/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif" data-orig-size="940,520" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="avp-display" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=940" width="940" height="520" src="https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=940" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif 940w, https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=768 768w" sizes="(max-width: 940px) 100vw, 940px"><figcaption>Source: <a href="https://www.ifixit.com/News/90409/vision-pro-teardown-part-2-whats-the-display-resolution" target="_blank" rel="noreferrer noopener">iFixit</a></figcaption></figure>



<p>The Apple Vision Pro’s micro-OLED display has created a lot of chatter in my hardware supply chain world, with lots of companies — predominantly smartphone OEMs — quickly racing to try and build a product that can deliver a similar experience to Vision Pro. Apple has secured a 1-year exclusive with <a href="https://www.sony-semicon.com/en/products/microdisplay/oled.html" target="_blank" rel="noreferrer noopener">Sony Semiconductor Solutions Group</a> and its second supplier <a href="https://www.seeya-tech.com/en/" target="_blank" rel="noreferrer noopener">SeeYA Technology</a>. There are also rumors Apple is dropping Sony as a display supplier and replacing it with <a href="https://www.boe.com/en/Enterprise/VR_AR">BOE</a> (whose <a href="https://www.boe.com/en/Enterprise/VR_AR">website</a> says a panel equivalent to Vision Pro is at “sample” stage).</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong>I fully expect the <a href="https://www.prnewswire.com/news-releases/lg-and-meta-forge-collaboration-with-meta-to-accelerate-xr-business-302073794.html" target="_blank" rel="noreferrer noopener">recently announced Meta/LG partnership</a> to be all about creating a supply chain advantage for Meta so they can race a <em>Quest Pro 2 </em>product into market that can compete with Vision Pro with LG putting some skin in the game to lower the street price of the headset.</mark></p>
</blockquote>



<p>(P.S. For anyone who wants to dig into more details of the Vision Pro display and pass-through system, I highly recommend <a href="https://www.ifixit.com/News/90409/vision-pro-teardown-part-2-whats-the-display-resolution" target="_blank" rel="noreferrer noopener">this article from iFixit</a> and <a href="https://kguttag.com/about-karl-guttag/" target="_blank" rel="noreferrer noopener">this article from Karl Guttag</a>, who’s an amazingly talented expert in display devices).</p>



<h2><strong>Apple made the Vision Pro display <em>intentionally blurry</em> in order to hide pixelation artifacts and make graphics appear smoother</strong></h2>



<p>There is a very good reason Apple has not used the word <em>retina</em> anywhere in their marketing materials for Vision Pro. It’s the simple fact that Vision Pro’s display does not pass the retina test — which is a <em>resolution high enough that the human eye can no longer discern individual pixels</em>. The Vision Pro display is nowhere near retina quality for a VR headset (see appendix for details) and yet <strong>our eyes cannot see individual pixels when looking at it</strong>. What gives?</p>



<p>During the first few days using Vision Pro, there was something that kept calling my attention but which I struggled to get my arms (or eyes) around. Everything my eyes saw in the headset felt a bit softer than I expected, and I initially attributed this to the seemingly refreshing absence of any <em><a href="https://pimax.com/what-is-the-screen-door-effect-in-vr/" target="_blank" rel="noreferrer noopener">screen door effect</a></em> — a pixelation artifact that has essentially doomed all VR headsets created up until now.</p>



<p>Well, as it turns out, the incredible <a href="https://kguttag.com/about-karl-guttag/" target="_blank" rel="noreferrer noopener">Karl Guttag</a> ran a meticulous <a href="https://kguttag.com/2024/03/01/apple-vision-pros-optics-blurrier-lower-contrast-than-meta-quest-3/" target="_blank" rel="noreferrer noopener">photographic analysis of the Vision Pro display</a> and came to a curious and possibly disturbing conclusion: <strong>Apple <em>intentionally calibrated the Vision Pro display slightly out of focus</em> to make pixels a bit blurry and hide the screen door effect “in plain sight”</strong>.</p>



<p>This image from Karl’s blog explains this well by comparing Vision Pro and Quest 3 displays side by side at a close enough distance where it’s possible to see individual pixels and clearly see the intentional blur that was added to the Vision Pro display:</p>







<figure><img data-attachment-id="179" data-permalink="https://hugo.blog/2024/03/11/vision-pro/avp-vs-mq3-close-up-crop-copy/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp" data-orig-size="471,519" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="avp-vs-mq3-close-up-crop-copy" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp?w=272" data-large-file="https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp?w=471" loading="lazy" width="471" height="519" src="https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp?w=471" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp 471w, https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp?w=136 136w, https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp?w=272 272w" sizes="(max-width: 471px) 100vw, 471px"><figcaption>Extreme close-up comparison between Vision Pro (AVP) and Quest 3 (MQ3) displays (Source: <a href="https://kguttag.com/2024/03/01/apple-vision-pros-optics-blurrier-lower-contrast-than-meta-quest-3/">KGOnTech</a>)</figcaption></figure>



<p>What Karl concluded is that even though Quest 3 has a much lower display resolution than Vision Pro (1,218 PPI vs. 3,386 PPI), Quest 3 appears objectively crisper especially when showing high-contrast graphics. In other words, Quest 3 is squeezing the highest possible resolution out of its display at the expense of a “harsher look” while Apple is giving up some of the Vision Pro’s display resolution in order to achieve a “softer look”. Karl may disagree with my conclusion on this point:</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong> Intentionally making the Vision Pro optics blurry is a clever move by Apple because it results in way smoother graphics across the board by hiding the screen door effect (which in practice means that you won’t see pixelation artifacts). This is also where Apple’s “taste” comes in, essentially resulting in the Vision Pro display being tuned to have a unique, softer, and more refined aesthetic than Quest 3 (or any other VR headsets). This is certainly a refreshing approach to designing VR hardware.</mark></p>
</blockquote>



<p>With this design decision, Apple is no doubt giving up a bit of the Vision Pro display’s high pixel resolution in order to achieve overall smoother graphics. You are definitely losing some text crispness in order to gain a higher perception of quality for images, video and 3D animations. This is a big benefit of starting with an ultra high-resolution micro-OLED display — Apple had enough pixels to work with that they could afford to make this trade-off. <strong>This is the kind of thing that our hardcore VR engineers at Oculus would have fought against to the end of the world, and I doubt we could have ever shipped a “blurred headset”, LOL!</strong></p>



<h2><strong>Sadly, the Vision Pro display suffers from <em>significant motion blur &amp; image quality issues</em> that render passthrough mode unusable for longer periods</strong></h2>



<p>While Apple’s decision to make individual pixels blurry on the Vision Pro display was extremely clever, the headset unfortunately suffers from a completely different type of blur that’s extremely problematic for the overall experience.</p>



<p>From the very first time I put on my Vision Pro, I noticed <strong>a lot of motion blur in passthrough</strong> mode even in excellent ambient lighting conditions and a still noticeable amount even when viewing immersive content. While my immediate instinct was to think that all VR headsets have that kind of motion blur and it’s just more noticeable on Vision Pro, a side-by-side comparison with Quest 3 quickly proved it’s significantly more serious on Vision Pro. This is particularly surprising considering that the passthrough cameras and display are both running at 90 hertz.</p>



<p>Since none of the initial Vision Pro reviews pointed out this issue, I ended up even calling Apple support to find out if this might be a known problem or possibly even a hardware defect. But then more in-depth reviews began pointing out the same problem (I highly recommend <a href="https://youtu.be/eOH33sWgds8?si=TTWCd8-UqX-D9-Eg" target="_blank" rel="noreferrer noopener">this review by Snazzy Labs</a>).</p>



<p>Motion blur in passthrough mode ended up being one of the many reasons why I decided to return my Vision Pro, because it’s just uncomfortable, leads to unnecessary eye strain, and really gets in the way of anyone using the headset for longer periods of time in passthrough mode.</p>



<p>There are other noticeable issues as well which affect passthrough mode, including <strong>very little dynamic range, incorrect white balance in most indoor use cases, and signs of edge distortion and chromatic aberration</strong>. Some of these might be addressed by software updates, but I expect most will not as they probably are limitations of the hardware stack.</p>



<h2><strong>The Vision Pro packs <em>a lot more computing power </em>than most people might realize — the M2 + R1 combination puts it at the level of a MacBook Pro</strong></h2>



<p>Any standalone VR headset is basically a 2-in-1 system: a regular “computing” computer and a spatial computer bundled together.</p>



<ul>
<li><strong>A <em>regular computer</em> in charge of running applications and performing general computation</strong>: this is everything that happens on your smartphone, tablet or notebook, including running the OS, executing applications across CPU/GPU loads, and doing computation work in the background.</li>



<li><strong>A <em>spatial computer</em> in charge of the environment</strong>: it keeps track of the whole environment, tracks your hands &amp; eyes, and ensures that everything — your surroundings, the OS system UI, and your apps — gets rendered in the right physical place in space and updated at 90 to 120 times per second while your head and body are moving around.</li>
</ul>



<p>These two “computers” must operate together without missing a beat — any latency above 20 milliseconds becomes quickly noticeable in VR and will often translate very quickly into user perception of unresponsiveness or jankiness, which can cause discomfort, eye strain or even dizziness for many people.</p>



<p>Enter the Vision Pro dual-chip design:</p>







<blockquote>
<p>“<em>A unique dual‑chip design enables the spatial experiences on Apple Vision Pro. The powerful M2 chip simultaneously runs visionOS, executes advanced computer vision algorithms, and delivers stunning graphics, all with incredible efficiency. And the brand-new R1 chip is specifically dedicated to process input from the cameras, sensors, and microphones, streaming images to the displays within 12 milliseconds — for a virtually lag-free, real-time view of the world.</em>“</p>
<cite>Apple Vision Pro website</cite></blockquote>



<p>The Vision Pro ships with the same M2 chip as the 2022 iPad Pro (or 2022 MacBook Air) alongside the new R1 chip which handles the massive amount of data coming from the 20+ tracking cameras and depth sensors (sensor fusion). What’s interesting to note is that Vision Pro actually does perform largely like an iPad Pro in benchmark tests that push CPU and GPU to their limits in both single-core and multi-core scenarios (see chart below). </p>







<figure><img data-attachment-id="83" data-permalink="https://hugo.blog/2024/03/11/vision-pro/vision-pro_geekbench/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png" data-orig-size="1542,1416" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="vision-pro_geekbench" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=1024" loading="lazy" width="1024" height="940" src="https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png 1542w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Source: <a href="https://www.pcmag.com/reviews/apple-vision-pro" target="_blank" rel="noreferrer noopener">PC Magazine</a></figcaption></figure>



<p>This is more impressive than it seems, and demonstrates that the R1 chip is doing a very significant amount of heavy lifting — essentially the vast majority the spatial computing workload — leaving a lot of compute room for the M2 chip to deliver the same level of performance as if it was just running inside an iPad Pro. </p>



<p>By all accounts so far, the R1 chip appears to be of a fairly similar package size as the M2 chip (though built using a specialized architecture), which puts the Vision Pro well ahead of any current generation iPad or MacBook Air, and likely more on par with a MacBook Pro from a silicon performance perspective. Definitely an impressive achievement by the Apple Silicon team.</p>



<p>This also begs the question… what if you could completely offload the Vision Pro’s compute to another Apple device?</p>



<h2><strong>Apple’s decision to use a tethered pack <em>will</em> <em>enable future Vision headsets to be much lighter</em> by offloading compute to an iPhone, iPad or MacBook </strong></h2>



<p>One of the most controversial aspects of Vision Pro is the fact that it sports a tethered battery pack, differently from all other commercially available standalone VR headsets. Many people have heavily criticized Apple for this decision because of the inconvenience of the “hanging” external battery.</p>



<p>I agree with Palmer Luckey (<a href="https://youtu.be/S-sD2FTjSaw?si=AQUTyJria4TKmSbc&amp;t=1437" target="_blank" rel="noreferrer noopener">from his recent interview with Peter Diamandis</a>) that this was a necessary short-term decision on Apple’s part given the reality of the hardware shipping inside the Vision Pro, but more importantly it was a very intentional long-term decision, which I’ll explain.</p>



<p>As I mentioned earlier, the Vision is a meticulously over-engineered computer with a very significant collection of power-hungry components:</p>



<ul>
<li>2x laptop-class processors (the R1 chip is almost the same size as the M2, which is the same processor shipping on MacBooks)</li>



<li>2x very bright micro-OLED displays with high pixel density</li>



<li>1x auxiliary EyeSight display</li>



<li>12x cameras and other sensors</li>



<li>2x blower fans</li>



<li>2x speakers</li>
</ul>



<p>As Quin from Snazzy Labs carefully explains <a href="https://youtu.be/eOH33sWgds8?si=M_-SlaOYK5k-1SPP&amp;t=988" target="_blank" rel="noreferrer noopener">in his excellent review</a>, the Vision Pro likely draws as much as <strong>40 watts of power</strong>, which is more than most MacBook laptops. This also means it has a power supply with the potential of generating a lot of heat. So, in addition to transferring the battery weight out of the headset, the decision to move to a tethered pack also keeps a huge heat source safely away from your head.</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong>All that said, the long-term strategic reason for having an external battery pack is to set expectations with Vision Pro users that there will <em>always</em> be an external box connected to the headset. In future Vision headsets, Apple should be able to comfortably start moving a lot of electronics off the headset, possibly shaving off as much half of the weight over a few generations and <strong>target around 300g</strong>. This also opens an extremely interesting path for Apple in a few years to <strong>use an iPhone, iPad or MacBook as the tethered computer driving the headset</strong>, which would dramatically simplify the headset.</mark> </p>
</blockquote>



<p>Interestingly, there is a tethered VR headset in the market today that demonstrates this desirable end state. It’s the <a href="https://www.bigscreenvr.com/" target="_blank" rel="noreferrer noopener"><strong>Bigscreen Beyond</strong></a>, the world’s smallest PC VR headset (i.e. needs to be tethered to a computer) that is lighter than even most ski goggles at 127 grams. Bigscreen’s ability to build this product is in many ways a bit of cheating since the headset was stripped of all sensors (no external cameras or eye tracking), but its existence nonetheless plays an important role in letting us experience what the future holds and where Apple’s sights are focused.</p>







<figure><img data-attachment-id="152" data-permalink="https://hugo.blog/2024/03/11/vision-pro/carmack_bigscreen-beyond/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg" data-orig-size="1600,1183" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="carmack_bigscreen-beyond" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=1024" loading="lazy" width="1024" height="757" src="https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg 1600w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>John Carmack wearing the Bigscreen Beyond VR headset, <br>which weighs 127g (vs. the Vision Pro’s 600g)</figcaption></figure>



<h2>Software</h2>



<h2><strong>The Vision Pro software story is a <em>bold antithesis of VR</em> — and the lack of exciting AR apps at launch paints the product into an empty corner</strong></h2>



<p><em>“Welcome to the era of spatial computing”</em> is Apple’s leading slogan for Vision Pro and, and as expected by everyone in the VR industry, Apple is going all-in on AR (augmented reality) to deliver on this proposition. The company has gone out of their way to actively ignore everything that VR has been know for over the last decade.</p>



<p>At the center of Apple’s marketing for Vision Pro is <em>“keeping users connected to their surroundings and other people”</em>. Reading in-between these lines, it’s not hard to see that Apple is taking an anti-VR stance that borderline accuses Meta’s approach to VR of promoting human isolation while positioning Vision Pro as the antithesis of that.</p>







<blockquote>
<p><mark><strong>MY TAKE:</strong> Apple’s anti-VR stance is a risky move because it negates most of the traditional immersive content that has made the VR medium popular until now, and at least for now is painting Vision Pro into an empty corner. This reminds me of Apple’s broad </mark><mark>stance on privacy — built to be in complete opposition to Meta/Google — which has put them in a tight spot by severely limiting their options and restricting innovation in the age of Gen AI. </mark></p>
</blockquote>



<p><strong>There are no fully immersive games in the Vision Pro app store</strong>, whereas easily &gt;90% of the Oculus Quest catalog is made of immersive VR games. Instead of leveraging the existing community of high-quality immersive VR content developers, Apple is focusing all of its energy exclusively on AR use cases that play to the company’s ecosystem strengths — iOS apps and MacOS productivity — which I’ll dive into over the next few sections.</p>



<p>The launch roster of 3D AR apps &amp; games is a tremendous disappointment — in both quality and quantity — and mostly includes a few simple casual games, some of which are originally 2D games hastily converted into 3D art. The fact that <em>ARKit</em> has been available for so many years on iPad and iPhone (despite its limited success) should have made it possible for Apple to easily round up developers into building a sufficient number of exciting and impressive AR titles for Vision Pro. Instead, <strong>we’re seeing an initial lack of developer excitement for the category that should have been the <em>most defining and inspiring category </em>on Vision Pro.</strong></p>



<p>Ironically, Meta made almost exactly the same mistake launching Quest Pro in 2022. That headset shipped with close to zero AR apps despite their emphasis in the launch messaging on “full-color mixed reality”.</p>







<blockquote>
<p><mark><strong>MY TAKE</strong>: This may be the first device category where Apple’s “build it and they will come” approach to creating developer traction may simply not work as previously. It will be many years (and possibly even more than a decade) before there are tens of millions of active Vision Pro users willing to pay for spatial AR apps. Apple will need to take a page out of the Oculus playbook and actively motivate developers financially to develop for Vision Pro.</mark></p>
</blockquote>



<h2><strong>Vision Pro’s positioning as <strong>a </strong><em><strong>productivity &amp; movie watching “big scree</strong>n”</em></strong> <strong>is dull &amp; unimaginative <strong>but Apple is unashamedly owning it</strong> </strong></h2>



<p>With a weak and limited launch roster of AR apps that doesn’t include a single flagship 3D app or game, Apple had to focus the entire positioning for Vision Pro at launch almost entirely on how it plugs into the existing Apple ecosystem of 2D apps.</p>



<p>In the usual Apple-style product marketing, the launch messaging for Vision Pro is very explicitly codified in the <a href="https://www.apple.com/apple-vision-pro/" target="_blank" rel="noreferrer noopener">product webpage</a> and every single marketing asset is consistent with it. How Apple has chosen to sequence their product messaging matters just as much as the messages themselves. <strong>Vision Pro is 60% about 2D productivity and 40% about watching media/movies on a big screen</strong>:</p>







<figure><table><tbody><tr><td><strong>Use case</strong></td><td><strong>Apple Slogans</strong></td></tr><tr><td>Productivity</td><td><em>“Free your desktop. And your apps will follow.”</em><p><em>“How to work&nbsp;in all‑new ways.”</em></p></td></tr><tr><td>Media</td><td><em>“The ultimate theater. Wherever you are. </em><p>“<em>An immersive way to experience entertainment.”</em></p></td></tr></tbody></table></figure>



<p>On a side note, <em>FaceTime with Persona avatars</em> and <em>spatial photos &amp; videos</em> are also pushed as core pillars in the Vision Pro product messaging, but they’re clearly just ancillary use cases to support marketing. Though too small to matter for now, they may (and for Apple’s sake, hopefully will) end up playing a much bigger role in the future.</p>







<blockquote>
<p><mark><strong>MY TAKE:</strong> The Vision Pro launch is a significant missed opportunity, with Apple “welcoming us into the era of spatial computing” with a software and services stack that is practically only focused on 2D use cases. Though the in-store demos paint an exciting future, the experience delivered by Apple at launch is dull and unimaginative at best.</mark></p>
</blockquote>



<p>Putting aside my criticism to Apple’s focus for the Vision Pro at launch, the next few sections will offer a deep dive into my thoughts and opinions on the software and experience enabling <em>Productivity</em> and <em>Media</em> use cases. </p>



<div>
<h2><strong>The Vision Pro desperately wants to be <em>the “future of work</em>” and pick up where Meta Quest Pro completely dropped the ball, but…</strong></h2>



<p>One of our strongest thesis from the early Oculus days was always about VR playing a defining role in the “future of work”, from running 2D apps in massive virtual displays to having native 3D apps that would make it a lot easier to work and collaborate with others on a project.</p>



<p>When Meta announced the Quest Pro in 2022, much of its marketing hype was in fact around the <a href="https://www.youtube.com/watch?v=eYjU9mV7-6g" target="_blank" rel="noreferrer noopener"><strong>Workrooms app</strong></a> (at the time led by my incredibly talented friend Mike LeBeau). The app allows you use your Mac from within VR, with a lot of attention to details needed to make it truly possible to work in VR for several hours, including support for up to 3 virtual monitors and the ability to see your physical keyboard in passthrough or replace it with a fully 3D rendered tracked twin.</p>



<figure><img data-attachment-id="196" data-permalink="https://hugo.blog/2024/03/11/vision-pro/quest-pro_workrooms-2/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png" data-orig-size="719,479" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="quest-pro_workrooms" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png?w=719" loading="lazy" width="719" height="479" src="https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png?w=719" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png 719w, https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png?w=300 300w" sizes="(max-width: 719px) 100vw, 719px"><figcaption>Image from the Quest Pro launch marketing</figcaption></figure>



<p>Quest Pro was designed with the goal of being a lot more comfortable than other VR headsets so that people could wear it for longer periods of time. While this was a well-intended attempt, the product had a major flaw which made it less than a “minimum viable product” and simply did not justify a price tag well above $1,000. The display resolution — at 22 PPD (pixels per degree) — was too low and vastly insufficient to unlock “working in VR” because of poor text readability. This shortcoming (in addition to very poor quality passthrough) was so massive that it rendered the product practically irrelevant at launch — I ended up returning my unit within 24 hours of first use.</p>



<p><strong><em>Can Vision Pro deliver where Quest Pro (and Quest 3) have failed?</em></strong></p>



<p>In order to really put Vision Pro to the test in real-life scenarios, I spent well over 100 hours trying to deploy as many of my own productivity workflows as I could, including about 1/3 of the work in this essay. I’ll share my conclusions over the next couple of sections.</p>



<p>First off, before diving into the value proposition of Vision Pro as a work/productivity computer, I needed to clearly frame my “jobs to be done” as specifically as possible. When I’m in “work mode” — whether doing actual professional work or just life management stuff — I have three distinct workstations that I use back and forth (aside from my smartphone, which I won’t include here):</p>



<ul>
<li><strong>Office workstation</strong> <strong>| Mac Pro with 2x Apple XDR 6K displays:</strong> my highest productivity setup because it gives me access to everything I need in a single view and enables zero-hurdle multi-tasking; it’s basically my gold standard for any task or project no matter how complex with the highest speed &amp; quality</li>



<li><strong>Laptop | MacBook Pro 16-inch:</strong> medium-high productivity setup with a sufficiently large retina-quality display that still enables complex tasks with good enough multi-tasking though I do feel noticeably less productive; it requires a backpack to carry when going outside of home/office</li>



<li><strong>Tablet | iPad Pro 11-inch with keyboard:</strong> a low-medium productivity setup good for focused single-app work with extremely limited multi-tasking (ex. email, writing that doesn’t require research, some life planning) but still better than using my phone; one great advantage is that I can carry this “mini computer” more easily than a laptop without really needing a backpack</li>
</ul>



<p>Here’s a table summarizing these workstations:</p>



<figure><table><tbody><tr><td><strong>Device</strong></td><td><strong>Number of Pixels</strong></td><td><strong>Number of simult. windows</strong></td><td><strong>Ideal for</strong></td><td><strong>Ergonomics</strong></td><td><strong>Portability</strong></td><td><strong>Cost</strong></td></tr><tr><td>Mac Pro + 2x XDR displays</td><td>XXLarge<br><em>(2×20 million)</em></td><td>8+</td><td>Any creative project with lots of multitasking</td><td>Ideal</td><td>–</td><td>&gt;$10,000</td></tr><tr><td>MacBook Pro 16-inch</td><td>Large<br><em>(7.7 million)</em></td><td>2-4</td><td>Most creative projects with limited multitasking</td><td>Good</td><td>Medium</td><td>$3,000</td></tr><tr><td>iPad Pro 11-inch + keyboard case</td><td>Medium<br><em>(4 million)</em></td><td>1</td><td>Full email, simple editing</td><td>Not great</td><td>High</td><td>$1,200</td></tr></tbody></table></figure>



<p><strong>I then asked myself: </strong>Could I see myself using Vision Pro as a productivity device instead of (or in conjunction with) any of my existing workstations?</p>



<p>These are the specific questions I set off to answer (from the lowest to highest bar):</p>



<ul>
<li>Can Vision Pro be a complete alternative to my Tablet Workstation so that I could carry it around instead of an iPad Pro?</li>



<li>Can Vision Pro enhance my Laptop Workstation enough that it feels like having a “virtual XDR display” or two?</li>



<li>Could Vision Pro ever be better than ALL of my workstations at least for some productivity tasks? <strong><em>⇒ This is what excites me the most!</em></strong></li>
</ul>
</div>



<div>
<h2><strong><span>Productivity Thesis #1</span>: Vision Pro as an iPad Pro replacement </strong></h2>



<blockquote>
<p><strong>Status:</strong> ❌ NOT READY <em>(but it’s promising!)</em></p>



<p><mark><strong>MY TAKE: </strong>The Vision Pro aspires to become your “spatial iPad Pro” with really good potential for much better multi-tasking (than an iPad) and the ability to do focused work anywhere, but there’s simply too much usability friction and too many important apps missing for that to be a reality today (or likely in the next 1-2 years).</mark></p>
</blockquote>



<p>The Vision Pro is conveniently designed by Apple to immediately fit right into the existing Apple ecosystem as a (rather expensive) alternative to an iPad Pro. The headset has identical compute (same M2 chip) to an iPad Pro and conveniently supports iPad apps running natively. In fact, it’s easy to claim the Vision Pro should in principle be better than an iPad Pro because you can run multiple iPad apps side-by-side in full screen mode, which would overcome one of the biggest productivity limitations of iPads — poor multitasking.</p>



<p>However, in reality this claim really doesn’t hold true at all (at least not yet) for a few important limitations of Vision Pro at launch:</p>



<ul>
<li><strong>Many iPad apps don’t work well </strong>(or at all) on Vision Pro despite <a href="https://developer.apple.com/help/app-store-connect/manage-your-apps-availability/manage-availability-of-iphone-and-ipad-apps-on-apple-vision-pro/" target="_blank" rel="noreferrer noopener">Apple automatically opting in developers</a>. There is substantial friction and instability navigating inside productivity apps given that they’re designed for a multi-touch UI (ex: some iPad gestures don’t exist in Vision Pro, and some touch targets are too small). A lot of apps will require some effort by their developers to work well enough.</li>



<li><strong>Most productivity apps are still missing from the App Store</strong> (likely for the reason above), which would leave large holes in most people’s workflows. For example, the most important missing apps for my own workflows include Chrome, Gmail, GDocs/Sheets/Slides, Asana.</li>



<li><strong>Text input is still quite buggy</strong> which adds more friction to any productivity workflow. Cursor placement, text selection and editing are super error prone. Dictation doesn’t stream results as you speak.</li>



<li><strong>You must carry a keyboard and a trackpad</strong> (mice are <span>not</span> supported) for the vast majority of your iPad-class productivity workflows on Vision Pro, which could be an added inconvenience (compared to carrying an iPad with a keyboard case or even a laptop). Editing documents, spreadsheets or presentations without those is virtually impossible.</li>



<li><strong>There is no reliable workspace persistency</strong> which adds even more friction — you are forced to re-open apps, and then re-size and reposition windows almost every time. The capabilities we all want (which Apple should be able to ship soon if they want to) are (i) persistent workspaces, (ii) location-specific workspaces, and (iii) a spatial computing equivalent of Mission Control.</li>
</ul>



<p>All that said, these limitations can all be addressed by Apple and the potential of Vision Pro as an iPad Pro replacement really is there. Even though the iPad Pro has nearly twice the PPD (pixels per degree) as the Vision Pro, text readability of iPad apps on Vision Pro is good enough for you to run 3 or 4 side-by-side apps plus a number of ambient widgets.</p>



<p>I also really believe there’s a large enough white canvas for lots of Apple-style innovation and magic around letting users configure and manage their workspace with a combination of 2D panels and virtual 3D objects. The potential is really significant as long as Apple really empowers developers to innovate here (try <em>Nicholas Jitkoff’s <a href="https://www.widget.vision/" target="_blank" rel="noreferrer noopener">widget.vision</a></em> to see some great early examples — the NY Times front page widget is my favorite). </p>



<p><strong>Call me crazy, but I personally could get quite excited by the idea of a “spatial iPad Pro”</strong> if I was able to actually get all of my iPad apps on the Vision Pro, and if Apple addresses all of the issues causing friction in my workflows. The reason why is simply that of focus – to be able to really “dial down reality” and tune into the work wherever I am, without carrying my laptop with me but still having some degree of multitasking available.</p>



<h2><strong><span>Productivity Thesis #2</span>: Vision Pro as a MacBook virtual external monitor </strong></h2>



<blockquote>
<p><strong>Status:</strong> ✅ ALMOST READY <em>(needs some bug fixing!)</em></p>



<p><mark><strong>MY TAKE: </strong>The Vision Pro is a few software bug fixes away from being a suitable virtual-equivalent to an external monitor similar to a 27-inch Apple Studio Display that makes it easy to work immersively in VR using all your existing MacOS apps and workflows on a huge screen (but don’t expect an Apple XDR 6K experience!).</mark></p>
</blockquote>



<p>One of Vision Pro’s best pieces of pure software/experience magic is the ability to seamlessly connect to a MacBook by simply looking at the computer while wearing the headset. This is a simple improvement to the traditional AirPlay UI that creates a profound sense of seamlessness which VR has always lacked.</p>



<p>Before diving into this thesis, I’ll establish that the Vision Pro will <em>never</em> become a suitable alternative for my office workstation with dual Apple XDR 6K displays. At 32 inches per-monitor and a total of 40 million pixels (with pixel density of 218 PPI and angular resolution &gt;100 PPD) and without a weight around my head, it is simply not a bar I would hold a VR headset against today or at any point in the future.</p>



<p>The more interesting question to focus on is whether Vision Pro could even begin to look like a suitable replacement for one or more 27-inch Apple Studio Displays (or equivalent). The short answer today is that Vision Pro can indeed come close (or very close) to that, but there are some important limitations that Apple needs to address to make this a relatively frictionless use case:</p>



<ul>
<li><strong>Lack of dual (or triple) monitor support</strong> is a huge bummer despite the fact that there are decent reasons for it (largely related to requiring a lot of local Wi-Fi bandwidth). Even though Vision Pro has a relatively narrow field of view that still feels like looking through binoculars, if I could get two or three virtual monitors out of a MacBook Air for example, things would start to look more interesting.</li>



<li><strong>Inconsistent keyboard and trackpad behaviors</strong> makes it very hard to switch back and forth between iPad/Vision apps and the Mac virtual display. I constantly find myself looking for my cursor, seeing the virtual keyboard pop up onscreen when clearly I don’t need it (if I’m using a physical keyboard), not to mention that I cannot use my beloved Logitech MX mouse.</li>



<li><strong>There is no reliable workspace persistency</strong> which is exactly the same issue I discussed when talking about the iPad Pro use case in the previous section. This should be an easy fix.</li>



<li><strong>Eye tracking doesn’t work in MacOS</strong> which not only leads to inconsistent input modalities as I said above but also just feels like a huge missed opportunity to offer a magical capability that MacOS has never seen before. This is not a low-hanging bug fix, but I don’t see it as a huge technical leap for Apple if the MacOS team wants to address it.</li>



<li><strong>MacOS apps are “stuck” inside the virtual monitor</strong> instead of being allowed to move around the entire space. This is another missed opportunity to deliver a truly spatial/immersive experience with Vision Pro, though significantly more complicated for Apple to address and would require really careful engineering by both MacOS and visionOS teams.</li>
</ul>



<p>Many of the issues I highlighted above are straightforward software challenges that Apple is well equipped to address and would make a world of difference. I suspect it’s a matter of dealing with the usual internal politics/collaboration challenges getting the MacOS team to dedicate the necessary resources to address bugs and feature requests from the visionOS team.</p>



<p>The bottom line for me is that <strong>we can see a relatively near future where carrying a MacBook Air and a Vision Pro in your backpack could give you a reasonably good workstation</strong>, one that delivers enough benefits in the form of productivity gains that you might be willing to wear a headset for a few hours in a café, on an airplane, or even on your couch at home. (This perspective is of course made in complete absence of value-for-money considerations).</p>



<p>Unsurprisingly, this is Apple’s strongest hand with the Vision Pro launch as it’s 100% controlled by them and uniquely leverages the existing Apple ecosystem. Yes, it’s a very uninspiring and unimaginative use case, but it might be powerful enough for Apple to move a lot of headsets.</p>



<h2><strong>Watching movies in Vision Pro is great at first but most people will stop doing it after the initial novelty excitement wears off</strong></h2>



<p>Watching TV/movies in virtual reality seemed like such an incredibly compelling idea that we (the Oculus team at Meta/Facebook) built an entire product around that idea — <em>Oculus Go</em>. Launched in 2018, Oculus Go was the biggest product failure I’ve ever been associated with for the simple reason that it had extremely low retention despite strong partnerships with Netflix and YouTube. <strong>Most users who bought Oculus Go completely abandoned the headset after a few weeks.</strong> The full story is much more nuanced (including the fact that the Oculus Go failure got us on the path to Oculus Quest very quickly), but it taught us an important lesson.</p>



<p>The lesson we learned is that watching traditional (rectilinear) TV or movies in VR feels incredibly compelling at first, but the novelty wears off for most people after a few weeks. The reasons are:</p>



<ul>
<li><strong>It’s just not physically comfortable</strong> compared to watching TV or movies on an TV, tablet, or laptop, primarily because of the pressure on your head and face, plus the fact that you can’t comfortably sit in any position or lie down while wearing with the headset</li>



<li><strong>There’s a lot of friction </strong>to start watching a video in a VR headset if you’re not already in VR — frequently a lot more steps required (especially finding and putting on the headset) and a more cumbersome navigation UI compared to our other devices</li>



<li><strong>It’s socially isolating and lonely</strong> to watch videos in VR, which will be a deal breaker for many people (although definitely not all)</li>
</ul>



<p>Back in the Oculus Go days, we concluded rather quickly that media consumption in VR is simply not a core “daily driver” pillar but more an ancillary use case that adds some value to other core pillars (such as productivity or gaming).</p>



<p>Vision Pro does bring more to the table with a much better display than previous VR headsets which can create magical movie experiences on occasion. For instance, watching an animated Disney or Pixar movie in 3D is absolutely stunning. But the essential product-market fit challenge remains:  </p>



<blockquote>
<p><mark><strong>MY TAKE:</strong> VR is simply not a medium people will gravitate towards for watching 2D media on a regular basis. Adding to this all of the Vision Pro’s </mark><mark>comfort and friction issues, most people who get excited about watching media in the headset will eventually find themselves going back to their TV, tablet or laptop as their primary devices for video.</mark></p>
</blockquote>



<p>Watching 3D movies on Vision Pro is a fun entertainment experience, but these videos are “boxed” and don’t feel anything like witnessing real life. With the Vision Pro, Apple launched its new <em>Apple Immersive</em> video format, which opens the door for a new class of entertainment.</p>



<h2><strong>Apple Immersive Video opens a new world of possibilities for media in VR — but its <em>hyperrealism</em> may bring an unexpected <em>uncanny valley challenge</em></strong></h2>



<p>One of the big original bets we made at Facebook/Meta with the launch of <em>Oculus Go</em> in 2018 was that immersive 180-degree video would attract a massive amount of consumer interest and that this would somehow trigger a chain reaction in the world of entertainment. We were able to secure partnerships with a small number of media companies who had become specialized in capturing VR video early on, and we were off to the races.</p>



<p>Our initial excitement cooled off quickly. VR180 video quality on Oculus Go was decent but flat, washed out and far from amazing mostly due to low resolution. These videos didn’t create a true sense of presence, of feeling transported to another reality. And most of the content was of one-off nature, with no real franchises that would have people coming back for more (with the exception of sports, which failed initially for other reasons that I’ll come back to later).</p>



<p>Within a year, the Oculus team pivoted to VR gaming and stopped investing in immersive video almost completely.</p>



<p>In 2020, Apple acquired <a href="https://www.youtube.com/@Nextvr" target="_blank" rel="noreferrer noopener"><strong>NextVR</strong></a>, one of the small but highly respected companies we had been working with as they were edging into bankruptcy (we passed on the acquisition at Meta/Oculus). NextVR had spent over a decade building and perfecting VR 180 camera technology and production pipelines for broadcast-quality video. <a href="https://www.youtube.com/@Nextvr" target="_blank" rel="noreferrer noopener">The NextVR YouTube channel</a> is still live and provides amazing examples of what became possible with their technology <em>(make sure to pan around using your mouse/finger while watching videos in their YT channel)</em>.</p>



<figure><img data-attachment-id="102" data-permalink="https://hugo.blog/2024/03/11/vision-pro/nextvr_camera/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png" data-orig-size="1200,800" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="nextvr_camera" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=1024" loading="lazy" width="1024" height="682" src="https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>The latest publicly displayed NextVR 180-3D camera in 2018 (Source: <a href="https://www.theverge.com/2018/1/8/16862048/nextvr-six-degrees-of-freedom-augmented-reality-events-ces-2018" target="_blank" rel="noreferrer noopener">The Verge</a>)</figcaption></figure>



<p>The NextVR acquisition is what led to the incredibly <strong>Apple Immersive </strong>video format, which enables capture of <em>3D video in 180 degrees in 8K resolution at 90 frames per second</em>, an absolute juggernaut format with 8 times the number of pixels of a regular 4K video. <strong>The best way to think of the new <em>Apple Immersive</em> video format is kind of like a new IMAX-3D</strong>, but the real magic is the fact that it’s projected inside an imaginary 180-degree sphere (horizontally <em>and</em> vertically) that takes over your entire field of view.</p>



<p>Vision Pro is the first VR headset that enables playback of 180-degree 3D video at what feels to the eyes like 4K quality. At launch, there are four Apple TV short films on Vision Pro shot in Apple Immersive video format. My absolute favorite of these films — <em>Adventure</em> — is a jaw-dropping cinematic piece that is likely to win its share of movie awards. Experiencing the Norwegian fjords with this level of immersion is completely breathtaking, so much so that it might be my favorite experience so far in Vision Pro. I have never felt transported to another place in this manner in any experience I’ve ever had, anywhere, period.</p>



<figure><img data-attachment-id="112" data-permalink="https://hugo.blog/2024/03/11/vision-pro/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post-jpg-slideshow_large/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg" data-orig-size="1960,1104" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=1024" loading="lazy" width="1024" height="576" src="https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg 1960w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><strong>Adventure</strong> is one of the short films shot in Apple Immersive format released with the Vision Pro launch</figcaption></figure>



<p>My second favorite Apple Immersive video — <em><a href="https://www.youtube.com/watch?v=d555q5vaYns" target="_blank" rel="noreferrer noopener">Alicia Keys: Rehearsal Room</a></em> — is a super fun and intimate concert that really makes you feel what presence in VR could be like with another human. While it’ll be fun for nearly everyone to see Alicia Keys in a close-up VR performance, it may not be nearly as happy and inspiring to see a human in close proximity in other situations.</p>



<blockquote>
<p><mark><strong>MY TAKE:</strong> The super high-fidelity <mark>Apple Immersive </mark>video format will run into an unexpected and significant “uncanny valley” challenge as a consequence of its <em>hyperrealism</em>. Seeing someone right in such close proximity to you and in such high fidelity may feel cool to one person but will feel uncomfortable or overwhelming to others. Less so in a scene like this intimate music concert or sports game, but probably a lot more so in dramatic storytelling and other types of more realistic films.</mark></p>
</blockquote>



<p>Back in the Oculus days, we used to run experiments to try and really understand which lines could not be crossed in VR content to avoid people feeling overwhelmed or even unsafe. One of the findings in these experiments was that too much realism and fidelity could be one of the things that crosses a line. In other words, <em>hyperrealism</em> could quickly drag people into the <em>uncanny valley</em>, one of two places we always want to avoid in VR (the other place is motion sickness).</p>



<figure><img data-attachment-id="139" data-permalink="https://hugo.blog/2024/03/11/vision-pro/alicia/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg" data-orig-size="1280,720" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="alicia" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=1024" loading="lazy" width="1024" height="576" src="https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg 1280w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Many people will feel themselves crossing the uncanny valley while<br>watching <em><a href="https://www.youtube.com/watch?v=d555q5vaYns" target="_blank" rel="noreferrer noopener">Alicia Keys: Rehearsal Room</a></em> on Vision Pro</figcaption></figure>



<p>Navigating this creative challenge will take time and a lot of experimentation on Apple’s side, and they’re the one company in the world we can trust to have the level of sensitivity and artistry for this journey, not to mention the ability to hire the best of the best talent. Practically, it probably means we can expect to see beautiful experiential films in Apple Immersive format exploring topics such as beautiful landscapes, wildlife, travel and music, but are less likely to see deep human storytelling with people in close proximity to the camera (which is typical of nearly all traditional filmmaking).</p>



<p>Luckily for Apple, there is one category where hyperrealism is much less likely to be an issue especially for hardcore fans — <strong><em>Live Sports</em></strong>.</p>



<h2><b><em>Live sports</em> will be Apple’s secret weapon to sell a huge number of Vision Pro headsets to hardcore fans — but it’s going to be a long &amp; expensive journey</b></h2>



<p>One of the original Oculus Go 30-second TV commercials featured an <a href="https://www.youtube.com/watch?v=SUdJt_3i0Us" target="_blank" rel="noreferrer noopener">NBA courtside banter between Adam Levine and Jonah Hill</a> wearing the Oculus headset while watching a live game together in VR (each sitting in their own physical living room):</p>



<figure><img data-attachment-id="121" data-permalink="https://hugo.blog/2024/03/11/vision-pro/nextvr_nba/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png" data-orig-size="3476,1844" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="nextvr_nba" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=1024" loading="lazy" width="1024" height="543" src="https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=2048 2048w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Screenshot from an original <em>Oculus Go</em> TV commercial (<a href="https://www.youtube.com/watch?v=SUdJt_3i0Us" target="_blank" rel="noreferrer noopener">full video</a>)</figcaption></figure>



<p>This TV commercial did extremely well, drove a significant amount of Oculus Go sales (after all, that headset only cost $199) and definitely showed that we were on to something potentially quite powerful with hardcore sports fans. But as I explained in the previous section, we did not manage to bring it to reality in a way that would meet expectations.</p>



<p>In the end, our team at Oculus completely failed to realize the opportunity of redefining the sports audience experience through VR for a number of reasons, but primarily because we just didn’t have the patience to develop that market. We were unable to build the necessary industry support with sports leagues and broadcast right holders initially, so we stopped trying and the VR sports segment nearly died. There are small efforts on Quest today such as <a href="https://xtadiumvr.com/" target="_blank" rel="noreferrer noopener"><em>Xtadium</em></a> and <a href="https://www.oculus.com/vr/6525150070834263/" target="_blank" rel="noreferrer noopener"><em>Meta Horizons</em></a>, but the quality of the experience and the limited live content make it all too insignificant to matter. To date, nobody ever really tried hard enough to create this market.</p>



<p>Apple has the opportunity to completely change this, for a few reasons:</p>



<ul>
<li><strong>Apple Immersive on Vision Pro is a transformative experience</strong> in terms of video quality and its ability to deliver a real sense of presence. Watching a game in high-resolution VR has the <em>potential</em> to be legitimately better than a regular 4K TV broadcast by enabling hardcore fans to feel much closer to the action.</li>



<li><strong>Apple has VR broadcast expertise</strong> with its acquisition of NextVR, and could have been painstakingly building a robust production pipeline for live 8K video, which is a tall technical challenge requiring non-trivial investment and specialized talent.</li>



<li><strong>Apple is already active in the sports broadcast rights world</strong> through their existing MLS license and several other rumored conversations that may lead to Apple buying a lot more broadcast rights to continue to strengthen Apple TV (ex. English Premier League, Formula 1).</li>
</ul>



<p>The first place where Apple will likely explore using Apple Immersive and Vision Pro for a live broadcast is Major League Soccer in the US. Their recent announcement is a strong indicator this is likely coming in late 2024 or early 2025 (to continue building momentum for Vision Pro):</p>



<blockquote>
<p><em>Coming soon, all Apple Vision Pro users can experience the best of the 2023 MLS Cup Playoffs with <strong>the first-ever sports film captured in Apple Immersive Video</strong>. Viewers will feel every heart-pounding moment in 8K 3D with a 180-degree field of view and Spatial Audio that transports them to each match. </em></p>
<cite><a href="https://www.apple.com/newsroom/2024/02/2024-mls-season-kicks-off-today-exclusively-on-mls-season-pass-on-apple-tv/" target="_blank" rel="noreferrer noopener">Apple Press Release – February 2024</a></cite></blockquote>



<figure><img data-attachment-id="110" data-permalink="https://hugo.blog/2024/03/11/vision-pro/nextvr_mls/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp" data-orig-size="1500,750" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="nextvr_mls" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=1024" loading="lazy" width="1024" height="512" src="https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp 1500w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Apple will likely use MLS as a testing ground for developing Apple Immersive live broadcasts</figcaption></figure>



<p>Beyond MLS (where Apple already has a long-term agreement and the ability to basically do anything), it will take a significant amount of time and money for Apple to strike the necessary agreements with the main sports leagues (NBA, NFL, MLB, Premier League etc) to enable this kind of immersive broadcast experience. That said, this is likely only a matter of time, as the opportunity to rethink audience sports is large enough that it would matter a lot even to a multi-trillion dollar company like Apple.</p>



<blockquote>
<p><mark><strong>MY TAKE: </strong>Just to put things in perspective, prices of tickets for watching live sports (in the actual venue) have been going steadily up and are now in the $100s even for average to bad seats, with premium tickets easily going into the $1000s (<a href="https://www.cbsnews.com/news/how-much-super-bowl-2024-tickets-prices/" target="_blank" rel="noreferrer noopener"><strong>the cheapest SuperBowl ticket in 2024 was around $2,000 at face value</strong></a>). The business case for a high-quality immersive “courtside” experience on Vision Pro is almost unquestionably very strong.</mark></p>
</blockquote>



<p>There are two major aspects Apple will have to nail in order to successfully monetize this opportunity, both of which will require a lot of design, engineering and experimentation:</p>



<ul>
<li><strong>Live sports are very social</strong>, which means Apple will have to invest heavily in delivering a co-watching experience that works equally well for people who are physically in the same room or virtually co-located, and which feels as natural as casually watching a game sitting on the couch with your family or at a bar with your friends.</li>



<li><strong>The experience bar will be very high</strong>, which means Apple will have to really customize every aspect of the experience to the nature of each sports to make it better than watching a game on a large 4K television — including camera angles, special replays, birds-eye visualizations, analysis overlays, game stats etc.</li>
</ul>



<p>This is a massive canvas for innovation, and it will take several generations of Vision Pro to get there. I’m optimistic and, speaking from the position of having been part of a team that really tried to go after this opportunity, I really believe this is one of those things where “it takes an Apple” to change the game (pun very much intended!).</p>



<figure><img data-attachment-id="137" data-permalink="https://hugo.blog/2024/03/11/vision-pro/xtadium_tennis/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png" data-orig-size="1024,591" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="xtadium_tennis" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=1024" loading="lazy" width="1024" height="591" src="https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png 1024w, https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Watching live tennis on Xtadium app on Quest: <br>multiple cameras to choose from <em>plus</em> simultaneous TV broadcast on giant virtual floating screen</figcaption></figure>



<figure><img data-attachment-id="142" data-permalink="https://hugo.blog/2024/03/11/vision-pro/visionpro_golf/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp" data-orig-size="1280,720" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="visionpro_golf" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=1024" loading="lazy" width="1024" height="576" src="https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp 1280w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>PGA app on Vision Pro: <br>birds-eye view of a 3D model of the course and ability to track shots of a recorded prior match</figcaption></figure>



<figure><img data-attachment-id="174" data-permalink="https://hugo.blog/2024/03/11/vision-pro/visionpro_f1/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png" data-orig-size="3448,1910" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="visionpro_f1" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=1024" loading="lazy" width="1024" height="567" src="https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=2048 2048w, https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Concept of a Formula 1 mixed reality broadcast by viz artist John LePore (Source: <a href="https://www.youtube.com/watch?v=y9FpgxNeWJk" target="_blank" rel="noreferrer noopener">YouTube</a>):<br>birds-eye track view, main broadcast on giant floating screen, multi-camera access, live telemetry</figcaption></figure>



<h2>Conclusions<strong> </strong></h2>



<h2><strong>Why I returned my Vision Pro, and my wish list for what Apple could do to fix &amp; improve the product</strong> </h2>



<p>As a “product guy”, I usually force myself to behave like a real consumer making real trade-offs as much as I possibly can. I believe that always putting myself in the user’s shoes is an important part of what I do not just for my own products but also for products built by other people. I admit Vision Pro is the ultimate tech toy, but since I’m not an active developer I can’t justify the $4,049.78 price tag (512GB model + California sales tax) simply for keeping up with the VR market, so I returned my Vision Pro for a full refund inside the 14-day return window.</p>



<figure><img data-attachment-id="201" data-permalink="https://hugo.blog/2024/03/11/vision-pro/vision-pro_return/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png" data-orig-size="2018,1046" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="vision-pro_return" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=1024" loading="lazy" width="1024" height="530" src="https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=768 768w, https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png 2018w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>In Apple’s journey of <em>product-market fit</em> in VR, the Vision Pro has a long way to be able to deliver true retention. Apple’s high-risk decision to completely exclude immersive VR games from the Vision Pro app store — <em>plus</em> their inexplicable failure to create exciting momentum by not having high-quality AR apps at launch — don’t leave them with many options to deliver user value in the near term to non-developers.</p>



<p>The only low-hanging fruit is to make productivity really good, which despite being incredibly unimaginative and dull, should be one of Apple’s biggest focus in the next iterations of visionOS. I don’t discard the possibility of once again owning a 1st-gen Vision Pro in the future once Apple addresses all of the friction issues I uncovered and shared above. </p>



<p>During the 2 weeks of my Vision Pro experience, I accumulated a very long list of bug fixes and feature requests based on 2 weeks of Vision Pro usage. I’m going to share my Top 10 here:</p>



<ol>
<li><strong>Make productivity use cases frictionless </strong>first of all by closing the gap with developers to bring essential iPad apps to Vision Pro // fix text input &amp; editing and make it seamless // add support for 2 (and ideally 3) MacOS remote displays // add workspace window persistency // build “spatial Mission Control” and enforce a minimal recommended focal distance</li>



<li><strong>Have developers build amazing AR games</strong> and do everything possible to set a really high quality bar &amp; reward their creativity // add SharePlay support with <em>Personas</em> and really push for multi-player support, enabling people to be <em>and</em> play together.</li>



<li><strong>Improve passthrough mode</strong> to the extent that the hardware sensor stack allows, ideally reducing motion blur, improving white balance, and making seeing your hands more seamless (when viewing immersive content)</li>



<li><strong>Create workspace spatial persistency</strong> and allow me to configure different rooms in my home or office in such a way that Vision Pro always remembers my room-specific configurations</li>



<li><strong>Make 3D widgets &amp; objects first-class citizens </strong>in visionOS and enable people to decorate their homes &amp; offices persistently</li>



<li><strong>Let people bring their iPhone into VR</strong> by simply looking at the device (like the MacOS virtual display feature) and then getting a floating panel that they can place anywhere in their space — this will work wonders in reducing FOMO while in VR</li>



<li><strong>Add a <em>Guest mode</em> so anyone can give the Apple in-store demo</strong> and make it possible for Vision Pro users to “spread the love” — there’s nothing more magical than giving someone their first VR demo</li>



<li><strong>Add Persona support to SharePlay </strong>for watching video to so people can actually feel like they’re together — VR has a bad reputation of loneliness &amp; isolation, so making VR social must be a priority even though few people will use social features at first. There aren’t enough people with Vision Pro for this to be a practical use case today, but it’s important for Apple to set the right tone.</li>



<li><strong>Launch tons of beautiful environments</strong> ideally with a steady frequency, taking a page out of the Apple TV screensaver playbook — and include beautiful indoor environments as well (not just landscapes)</li>



<li><strong>(Lastly…) Find a way to let people play immersive VR games</strong> by implementing <a href="https://en.wikipedia.org/wiki/OpenXR" target="_blank" rel="noreferrer noopener">OpenXR</a> support, forming a partnership with <em>SteamVR</em> or simply opening up visionOS a bit to allow VR developers and enthusiasts to build compatibility themselves</li>
</ol>
</div>



<h2><strong>ONE MORE THING: (1) Why Meta’s Android moment is finally here, (2) My unsolicited product advice for <em>Quest Pro 2</em> and beyond</strong></h2>



<p>As I said at the beginning of this essay, while working at Meta/Oculus I used to semi-seriously joke that the best thing that could ever happen to us was having Apple enter the VR industry. One of the main reasons for me to say this was that I knew Apple would do the best job of any company making people really <em>want</em> VR through its unparalleled brand, design and marketing. Oculus co-founder Palmer Luckey puts it best:</p>







<blockquote>
<p><em>“VR will become something everyone wants before it becomes something everyone can afford.”</em></p>
<cite><a href="https://twitter.com/palmerluckey/status/679907802962214912" target="_blank" rel="noreferrer noopener">Palmer Luckey, 2015 tweet</a></cite></blockquote>



<p>For Meta, the Vision Pro launch is the best marketing tool for Quest VR that the company could have dreamed of but could have never achieved on its own, for a few reasons:</p>



<ul>
<li><strong>It elevates VR to a level of mainstream consumer curiosity</strong> and breaks away from gamer and VR enthusiast niches; in media coverage alone the Vision Pro probably had 1,000x more reach than any Oculus/Quest launch in history</li>



<li><strong>It sets a new experience gold standard for VR</strong> especially by pushing the existing boundaries in display resolution and creating a new paradigm of “UI magic” with gaze &amp; pinch which may be an instant defacto standard</li>



<li><strong>It establishes a pricing envelope</strong> that enables Meta to break away from the $500 price point that Quest has been stuck in, and specifically allows them to ship a Quest Pro 2 headset priced at a $1,000 to $1,500 (but likely not higher) without being completely rejected by consumers</li>



<li><strong>It creates a formidable competitor</strong> for Meta teams to maniacally chase and will almost certainly force the company to move with a much greater sense of urgency internally (which would be a great outcome as friends on the inside are constantly complaining Meta Reality Labs moves too slowly)</li>
</ul>



<p><strong><em>What should Meta do in response to the Vision Pro launch?</em></strong></p>



<p>In order to really seize this moment and opportunity created by the Vision Pro launch, Meta needs to ensure it ships a VR headset by mid-2025 that both builds on the new experience gold standard created by the Vision Pro <em>and </em>is objectively a better product across as many dimensions as possible. It is imperative for Meta not to repeat the inexplicable debacle that was the <em>Quest Pro</em> launch in 2022.</p>



<p>I put together my own <strong>Top 10</strong> <strong>wish list for <em>Quest Pro 2</em></strong>:</p>



<ol>
<li><strong>Double down investment in micro-OLED</strong> as it’s likely the only way to achieve display resolution at or near Vision Pro; I suspect this may be exactly what the <a href="https://www.roadtovr.com/meta-lg-xr-partnership/" target="_blank" rel="noreferrer noopener">recently announced LG partnership</a> is about</li>



<li><strong>Build an ergonomic headset</strong> <strong>that can be worn for 2-4 hours</strong> without causing any major discomfort issues; ideally offering two battery options: (1) a head-strap with a built-in battery in the back of the head, and (2) a wired pack (like the Vision Pro) that moves the battery off the head and reduces the headset weight to below 500 grams while increasing energy capacity. </li>



<li><strong>Deliver better passthrough than Vision Pro</strong> by dramatically improving Quest 3’s latency and distortion correction and improving upon all of the Vision Pro passthrough issues — ensure no perceivable motion blur, high dynamic range, accurate white balance</li>



<li><strong>Take Apple’s gaze+pinch UI to the next level</strong> by productizing all of the amazing research on hand tracking done at Meta (ex: Rob Wang’s super talented group) to enable fine-grained gestures such as scrolling and D-pad selection by detecting small finger movements solely via camera input (this is <em>not</em> the CTRL Labs stack… that’s for the future)</li>



<li><strong>Partner with Microsoft to make Windows computers 1st class citizens</strong> in Quest Pro 2 and enable advanced desktop productivity use cases that go well beyond virtual monitors (ex: make it possible to take any window and place it in space)</li>



<li><strong>Launch Android 2D tablet apps natively on Quest</strong> to match the Vision Pro iPad compatibility library either by partnering with Google to license <em>Play Store</em> (which <a href="https://www.theinformation.com/articles/meta-rebuffed-google-proposal-for-a-vr-and-ar-tie-up">seems unlikely these days</a> though I still believe Ash Jhaveri and Hiroshi could work together to pull it off) or just build a curated tablet app store directly (which we had considered in the past at Oculus but passed on)</li>



<li><strong>Launch human-like avatars with Quest Pro 2</strong> by productizing Meta’s mind-blowing <a href="https://www.uploadvr.com/meta-codec-avatars-might-be-coming-to-quest/" target="_blank" rel="noreferrer noopener">Codec Avatars technology</a>, likely one of the VR research areas that has received the most R&amp;D dollars for the last 7+ years, used by <a href="https://youtu.be/MVYrJJNdrEg?si=EIZJ_Op4xRY9PD-h&amp;t=605" target="_blank" rel="noreferrer noopener">Lex Friedman in his interview with Mark Zuckerberg in late 2023</a></li>



<li><strong>Launch high-definition room scanning and unlock teleportation</strong> using technology that has existed within Oculus Research for several years now; it is time for Meta to make this future a reality where people can be remote but <em>feel</em> truly present by visiting each other’s home, office or favorite place</li>
</ol>



<h2>Appendix: Other Fun Things</h2>



<h2><strong>Despite all of its hardware insanity the Vision Pro display is still a far cry from a VR retina display (and may never get there)</strong></h2>



<p>As noted above the Vision Pro display delivers insane pixel density at over 3,000 PPI (compared to 500 PPI for the highest resolution smartphones), but because the panel is so close to our eyes, it still doesn’t come even close to the resolution it would need to have in order to qualify as a <em><a href="https://en.wikipedia.org/wiki/Retina_display">retina display</a></em>.</p>



<p>A retina display, by <a href="https://www.kybervision.com/Blog/files/AppleRetinaDisplay.html" target="_blank" rel="noreferrer noopener">Apple’s definition</a>, is a display with a high enough resolution that the human eye cannot resolve individual pixels. Because different devices are used at varying distances from the eye, there is no single PPI (pixels per inch) standard for retina across all device categories. Instead, it’s useful to look at PPD (pixels per degree), which is a measure of angular resolution independent of viewing distance, or specifically the number of horizontal pixels per degree of <em>viewing angle</em>. See <a href="https://simulavr.com/blog/ppd-optics/" target="_blank" rel="noreferrer noopener">this article from SimulaVR</a> for an excellent explanation, and here’s an image from that article:</p>







<figure><img data-attachment-id="74" data-permalink="https://hugo.blog/2024/03/11/vision-pro/ppd/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/ppd.png" data-orig-size="460,286" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ppd" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/ppd.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/ppd.png?w=460" loading="lazy" width="460" height="286" src="https://hugobarracom.files.wordpress.com/2024/02/ppd.png?w=460" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/ppd.png 460w, https://hugobarracom.files.wordpress.com/2024/02/ppd.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/ppd.png?w=300 300w" sizes="(max-width: 460px) 100vw, 460px"><figcaption>Source: <a href="https://simulavr.com/blog/ppd-optics/" target="_blank" rel="noreferrer noopener">SimularVR</a></figcaption></figure>



<p>A human eye with 20/20 vision has a resolution of 60 PPD. This means very specifically that we can resolve 60 pixels per each 1 degree of viewing angle (or 1 pixel per arc minute, which is 1/60th of a degree). The Vision Pro has an angular resolution of 34, which is 1/3 more than the Meta Quest 3 but still very far away from the 60 PPD we’d need for a retina-quality display.</p>



<p>Achieving angular resolution anywhere near 60 PPD in a VR headset is likely not possible with any technology in the mid-term horizon.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TextSnatcher: Copy text from images, for the Linux Desktop (280 pts)]]></title>
            <link>https://github.com/RajSolai/TextSnatcher</link>
            <guid>39711621</guid>
            <pubDate>Fri, 15 Mar 2024 02:57:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/RajSolai/TextSnatcher">https://github.com/RajSolai/TextSnatcher</a>, See on <a href="https://news.ycombinator.com/item?id=39711621">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a href="https://wiki.gnome.org/Projects/Vala" rel="nofollow"><img src="https://camo.githubusercontent.com/b7b96bad0bedef8e14b37aa18f8559eb09fae075833a1a640ae2d3af86a6d49f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d616465253230576974682d56616c612532302d413536444532" alt="Vala Programming language" data-canonical-src="https://img.shields.io/badge/Made%20With-Vala%20-A56DE2"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/RajSolai/TextSnatcher/actions/workflows/flatpak-build.yml/badge.svg"><img src="https://github.com/RajSolai/TextSnatcher/actions/workflows/flatpak-build.yml/badge.svg" alt="Flatpak Build workflow"></a></p>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/RajSolai/TextSnatcher/blob/master/data/icons/com.github.rajsolai.textsnatcher.svg"><img src="https://github.com/RajSolai/TextSnatcher/raw/master/data/icons/com.github.rajsolai.textsnatcher.svg" height="110px"></a></p><p dir="auto"><h2 tabindex="-1" dir="auto">TextSnatcher</h2><a id="user-content-textsnatcher" aria-label="Permalink: TextSnatcher" href="#textsnatcher"></a></p>
<p dir="auto">Copy Text from Images with ease, Perform OCR operations in seconds.</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-default.png"><img alt="TextSnatcher OCR App for Linux" src="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-default.png"></a><br>
<a href="https://www.producthunt.com/posts/textsnatcher?utm_source=badge-featured&amp;utm_medium=badge&amp;utm_souce=badge-textsnatcher" rel="nofollow"><img src="https://camo.githubusercontent.com/461f8889a0b1219ad9051d64231614176a85e2eb03b4b06a8770180fd8831e2b/68747470733a2f2f6170692e70726f6475637468756e742e636f6d2f776964676574732f656d6265642d696d6167652f76312f66656174757265642e7376673f706f73745f69643d333434343031267468656d653d6c69676874" alt="TextSnatcher - How to copy text from images, answer is TextSnatcher | Product Hunt" width="250" height="54" data-canonical-src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=344401&amp;theme=light"></a>
</p></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Download</h2><a id="user-content-download" aria-label="Permalink: Download" href="#download"></a></p>
<div dir="auto">
  <p><a href="https://flathub.org/apps/details/com.github.rajsolai.textsnatcher" rel="nofollow"><img width="240" alt="Download on Flathub" src="https://camo.githubusercontent.com/1a343de10fc46d1a887a0640899212590da30f4d415ce2caf85e4462bd95a3b8/68747470733a2f2f666c61746875622e6f72672f6173736574732f6261646765732f666c61746875622d62616467652d692d656e2e706e67" data-canonical-src="https://flathub.org/assets/badges/flathub-badge-i-en.png"></a></p><p dir="auto"><a href="https://appcenter.elementary.io/com.github.rajsolai.textsnatcher" rel="nofollow"><img src="https://camo.githubusercontent.com/02fb9c2fbb1ea0024a489c44798c28f84ba7149a8d144a20a8a5a28debd1e293/68747470733a2f2f61707063656e7465722e656c656d656e746172792e696f2f62616467652e737667" alt="Get it on AppCenter" data-canonical-src="https://appcenter.elementary.io/badge.svg"></a></p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Multiple Language Support.</li>
<li>Copy Text from images with a Drag.</li>
<li>Drag over any Image and Paste.</li>
<li>Fast and Easy to Use.</li>
<li>This application uses the Tesseract OCR 4.x for the character
recognition.</li>
<li>Read more about <a href="https://tesseract-ocr.github.io/tessdoc/Home.html" rel="nofollow">Tesseract</a> and Star ⭐️ <a href="https://github.com/tesseract-ocr/tesseract">Tesseract-Project</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screencasts</h2><a id="user-content-screencasts" aria-label="Permalink: Screencasts" href="#screencasts"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description textsnatcher-eng.mp4">textsnatcher-eng.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/54436424/152921719-228485ba-0d37-4b01-864e-63a2792248b5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA0ODYzMDUsIm5iZiI6MTcxMDQ4NjAwNSwicGF0aCI6Ii81NDQzNjQyNC8xNTI5MjE3MTktMjI4NDg1YmEtMGQzNy00YjAxLTg2NGUtNjNhMjc5MjI0OGI1Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDA3MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ4MmZjZDQ2ZmVlODU3MDY5NTg2ZDI2YTkzOGNjZjY1OGFmZmE4MTA3ZTAzNDAwNTg3N2Y4ZTY3N2MyZGQ3YzcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.j0LbbGRiW-lVKdyCCb2HQo0TYTj_CfpHqm5TSWM-tqo" data-canonical-src="https://private-user-images.githubusercontent.com/54436424/152921719-228485ba-0d37-4b01-864e-63a2792248b5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA0ODYzMDUsIm5iZiI6MTcxMDQ4NjAwNSwicGF0aCI6Ii81NDQzNjQyNC8xNTI5MjE3MTktMjI4NDg1YmEtMGQzNy00YjAxLTg2NGUtNjNhMjc5MjI0OGI1Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDA3MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ4MmZjZDQ2ZmVlODU3MDY5NTg2ZDI2YTkzOGNjZjY1OGFmZmE4MTA3ZTAzNDAwNTg3N2Y4ZTY3N2MyZGQ3YzcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.j0LbbGRiW-lVKdyCCb2HQo0TYTj_CfpHqm5TSWM-tqo" controls="controls" muted="muted">

  </video>
</details>

<details open="">
  <summary>
    
    <span aria-label="Video description textsnatcher-tamil.mp4">textsnatcher-tamil.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/54436424/152921736-c9567c9d-0afa-4c09-8706-6b2a1b6b635a.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA0ODYzMDUsIm5iZiI6MTcxMDQ4NjAwNSwicGF0aCI6Ii81NDQzNjQyNC8xNTI5MjE3MzYtYzk1NjdjOWQtMGFmYS00YzA5LTg3MDYtNmIyYTFiNmI2MzVhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDA3MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA4ZTFhZTUyN2RkOGU4YTI4OWMzMTUxZDc2MjkxYzFjZjU4NTQ5NmNlMzU2ODU3YjIyMDc0MGNjNzA4OGE4NjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.eeFrGifG7N1bcXi_wZRrMhFlRHV9v-gTz0_INWoCg1A" data-canonical-src="https://private-user-images.githubusercontent.com/54436424/152921736-c9567c9d-0afa-4c09-8706-6b2a1b6b635a.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA0ODYzMDUsIm5iZiI6MTcxMDQ4NjAwNSwicGF0aCI6Ii81NDQzNjQyNC8xNTI5MjE3MzYtYzk1NjdjOWQtMGFmYS00YzA5LTg3MDYtNmIyYTFiNmI2MzVhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDA3MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA4ZTFhZTUyN2RkOGU4YTI4OWMzMTUxZDc2MjkxYzFjZjU4NTQ5NmNlMzU2ODU3YjIyMDc0MGNjNzA4OGE4NjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.eeFrGifG7N1bcXi_wZRrMhFlRHV9v-gTz0_INWoCg1A" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Screenshots</h2><a id="user-content-screenshots" aria-label="Permalink: Screenshots" href="#screenshots"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-default.png"><img src="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-default.png" alt="TextSnatcher OCR App for Linux"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-dark.png"><img src="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-dark.png" alt="TextSnatcher OCR App for Linux"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support Me</h2><a id="user-content-support-me" aria-label="Permalink: Support Me" href="#support-me"></a></p>
<p dir="auto"><a href="https://www.buymeacoffee.com/rajsolai" rel="nofollow"><img src="https://camo.githubusercontent.com/cace41b0afc90c68d0207e2bd809ee121f9ff4f72ac032e8ced972aee7adbb23/68747470733a2f2f63646e2e6275796d6561636f666665652e636f6d2f627574746f6e732f76322f64656661756c742d79656c6c6f772e706e67" alt="Buy Me A Coffee" data-canonical-src="https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png"></a></p>
<p dir="auto"><a href="https://ko-fi.com/R6R7ABG0F" rel="nofollow"><img src="https://camo.githubusercontent.com/ce32b4940b9ebf361cfd346ba0582815846406854cd2f701c11a85cb21eaa939/68747470733a2f2f6b6f2d66692e636f6d2f696d672f676974687562627574746f6e5f736d2e737667" alt="ko-fi" data-canonical-src="https://ko-fi.com/img/githubbutton_sm.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Social Media Posts</h2><a id="user-content-social-media-posts" aria-label="Permalink: Social Media Posts" href="#social-media-posts"></a></p>
<p dir="auto"><a href="https://www.linkedin.com/posts/solai085_linux-commentbelow-apple-activity-6826408004519374848-wxsw" rel="nofollow">LinkedIn Post on Why I created TextSnatcher</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dependencies</h2><a id="user-content-dependencies" aria-label="Permalink: Dependencies" href="#dependencies"></a></p>
<p dir="auto">Ensure you have these dependencies installed</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Runtime Dependency</h3><a id="user-content-runtime-dependency" aria-label="Permalink: Runtime Dependency" href="#runtime-dependency"></a></p>
<ul dir="auto">
<li>scrot</li>
<li>tesseract-ocr</li>
<li>tesseract language data
<a href="https://archlinux.org/packages/community/x86_64/tesseract" rel="nofollow">arch repos</a>
<a href="https://packages.debian.org/search?keywords=tesseract-ocr" rel="nofollow">debian repos</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Buildtime Dependency</h3><a id="user-content-buildtime-dependency" aria-label="Permalink: Buildtime Dependency" href="#buildtime-dependency"></a></p>
<ul dir="auto">
<li>granite</li>
<li>gtk+-3.0</li>
<li>gobject-2.0</li>
<li>gdk-pixbuf-2.0</li>
<li>libhandy-1</li>
<li>libportal-0.5</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install, build and run</h2><a id="user-content-install-build-and-run" aria-label="Permalink: Install, build and run" href="#install-build-and-run"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# clone repository
git clone https://github.com/RajSolai/TextSnatcher.git TextSnatcher
# cd to dir
cd TextSnatcher
# run meson
meson build --prefix=/usr
# cd to build, build and test
cd build
sudo ninja install &amp;&amp; com.github.rajsolai.textsnatcher"><pre><span><span>#</span> clone repository</span>
git clone https://github.com/RajSolai/TextSnatcher.git TextSnatcher
<span><span>#</span> cd to dir</span>
<span>cd</span> TextSnatcher
<span><span>#</span> run meson</span>
meson build --prefix=/usr
<span><span>#</span> cd to build, build and test</span>
<span>cd</span> build
sudo ninja install <span>&amp;&amp;</span> com.github.rajsolai.textsnatcher</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inspirations</h2><a id="user-content-inspirations" aria-label="Permalink: Inspirations" href="#inspirations"></a></p>
<ul dir="auto">
<li>ReadMe: <a href="https://github.com/alainm23/planner">https://github.com/alainm23/planner</a></li>
<li>Application Structure: <a href="https://github.com/alcadica/develop">https://github.com/alcadica/develop</a></li>
<li>TextSniper (MacOS Application)</li>
</ul>
<p dir="auto">Made with ❤️ for Linux</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New York Disbars Infamous Copyright Troll (381 pts)]]></title>
            <link>https://abovethelaw.com/2024/03/new-york-disbars-infamous-copyright-troll/</link>
            <guid>39710455</guid>
            <pubDate>Thu, 14 Mar 2024 23:53:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abovethelaw.com/2024/03/new-york-disbars-infamous-copyright-troll/">https://abovethelaw.com/2024/03/new-york-disbars-infamous-copyright-troll/</a>, See on <a href="https://news.ycombinator.com/item?id=39710455">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="single-post-content">
<p><img src="https://abovethelaw.com/uploads/2021/03/iStock-484137638-300x200.jpg" alt="Dictionary Series – Ethics" width="300" height="200">For years, Richard Liebowitz ran a very successful operation mostly sending threatening letters to companies claiming that they had infringed upon copyrights held by his photographer clients. Under the best of circumstances it’s a niche practice area that’s… <em>kinda shady</em>. But Liebowitz gained a degree of infamy across a number of matters for high-profile missteps in cases that sparked the ire of federal judges. Now, finally, <a href="https://nycourts.gov/reporter/3dseries/2024/2024_01309.htm" target="_blank" rel="noopener">New York has disbarred him</a>.</p>
<p>Liebowitz wasn’t alone in the copyright trolling practice. A number of entities scour the internet looking for photographs that they can claim are “unlicensed” and demanding thousands of dollars to settle the matter knowing that between statutory damages for copyright infringement and the cost of litigation, most companies will just pay it. Many times, the photo in question actually is legally licensed through an agency like Getty Images, but the plaintiff photographer has, for whatever reason, pulled the image since the license was granted.</p>
<p>This runs the risk that some plaintiff might do this on purpose hoping to catch some legal licenseholder unawares and bank on the target just settling to avoid bringing any lawyers into the situation. Which is why, for example,&nbsp;a judge in one case cited by the disbarment opinion ordered Liebowitz “produce to the defendant records sufficient to show the royalty paid the last three times that the picture at issue was licensed, and the number of times the picture was licensed in the last five years; if the picture was never licensed, the plaintiff was to certify that fact as part of the plaintiff’s production.” In this case, Liebowitz “did not timely produce the required royalty information to the defendant” per the disbarment opinion.</p>
<p>Though most of the opinion describes more fundamental case management problems. From a case brought in 2017:</p>
<blockquote><p>The respondent stated under penalty of perjury that he did not and had never made a settlement demand in this matter. In fact, the respondent had sent the defendant’s counsel an email in which the respondent proposed settling the matter for the sum of $25,000.</p></blockquote>
<p>And another case brought in 2017:</p>
<blockquote><p>On January 13, 2018, the respondent submitted a letter (hereinafter the January 13, 2018 letter) to the District Court, requesting an adjournment of the pretrial conference scheduled for January 19, 2018, and stating that the defendant “had yet to respond to the complaint” and that the plaintiff intended to file a motion for a default judgment. Judge Cote granted the request and ordered the motion for entry of default due on January 26, 2018.</p>
<p>The respondent’s statement in his January 13, 2018 letter that the defendant “had yet to respond to the complaint” was false and misleading, and the respondent knew that it was false and misleading when he made it. The January 13, 2018 letter failed to advise the court of the months-long history of communication between the parties, beginning in July 2017, as mentioned above.</p></blockquote>
<p>From yet another matter:</p>
<blockquote><p>The plaintiff admitted in a deposition and in other documents that the Photograph had been previously published on numerous occasions. To prevent the defendants from learning that the plaintiff did not hold a valid registration, the respondent stonewalled the defendants’ requests for documents and information. The respondent also failed to comply with an order by Magistrate Judge Debra Freeman to obtain and produce Copyright Office documents to demonstrate a valid registration. After it came to light that the Photograph was not registered, and despite the record stating otherwise, the respondent argued, without evidence, that the lack of registration was merely a mistake.</p></blockquote>
<p>If there’s a lesson to take away from these and the many, many more examples included in the opinion, it’s that copyright trolling outfits are largely unprepared for someone to push back on their demands. Firing off demand letters, memorializing boilerplate licensing agreements, and collecting cash is a tidy business model right up until a firm has to juggle hearings and discovery requests and experts and “not committing perjury.”</p>
<p>But perhaps the most bizarre story involves Liebowitz missing an April 12, 2019 hearing, explaining that his grandfather had passed. When Judge Seibel directed Liebowitz under penalty of contempt to furnish evidence or documentation regarding the date of his grandfather’s death, Liebowitz shot back that the order “likely constitutes a usurpation of judicial authority or a breach of judicial decorum.”</p>
<blockquote><p>On November 7, 2019, the respondent retained counsel to represent him in the contempt proceedings, and on November 11, 2019, the respondent sent a letter to Judge Seibel admitting that he failed to carry out his responsibilities to the District Court and to his adversary. The respondent also admitted that his grandfather died on April 9, 2019, and was buried that same day.</p></blockquote>
<p>Just. Wow. You know, “my grandfather died this week” is something you can tell a court <em>before</em> a hearing and they’ll probably grant it. <a href="https://abovethelaw.com/2023/05/pregnant-dont-plan-on-practicing-before-ohios-supreme-court-any-time-soon/" target="_blank" rel="noopener">It’s not like you’re asking to give birth or anything</a>. But to lie about it to the court and then keep doubling down is… a choice.</p>
<p>And a poor one as it turns out:</p>
<blockquote><p>ORDERED that pursuant to 22 NYCRR 1240.13, the respondent, Richard P. Liebowitz, a suspended attorney, is disbarred, effective immediately, and his name is stricken from the roll of attorneys and counselors-at-law….</p></blockquote>
<p><a href="https://nycourts.gov/reporter/3dseries/2024/2024_01309.htm" target="_blank" rel="noopener">Matter of Liebowitz</a> [New York Courts]</p>
<hr>
<p><img src="https://abovethelaw.com/uploads/2016/11/Headshot-300x200.jpg" alt="Headshot" width="189" height="126"><strong><em><a href="http://abovethelaw.com/author/joe-patrice/" target="_blank" rel="noopener">Joe Patrice</a>&nbsp;is a senior editor at Above the Law and co-host of <a href="http://legaltalknetwork.com/podcasts/thinking-like-a-lawyer/" target="_blank" rel="noopener">Thinking Like A Lawyer</a>. Feel free to&nbsp;<a href="mailto:joepatrice@abovethelaw.com">email</a> any tips, questions, or comments. Follow him on&nbsp;<a href="https://twitter.com/josephpatrice" target="_blank" rel="noopener">Twitter</a>&nbsp;if you’re interested in law, politics, and a healthy dose of college sports news. Joe also serves as a <a href="https://www.rpnexecsearch.com/josephpatrice" target="_blank" rel="noopener">Managing Director at RPN Executive Search</a>.</em></strong></p>
<p><a href="https://bit.ly/406FC2u" target="_blank">
<img src="https://abovethelaw.com/uploads/2023/06/Practice_Banner_600x250_D.png" alt="CRM Banner">
</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Getty Makes Nearly 88,000 Art Images Free to Use However You Like (383 pts)]]></title>
            <link>https://www.openculture.com/2024/03/the-getty-makes-nearly-88000-art-images-free-to-use-however-you-like.html</link>
            <guid>39710454</guid>
            <pubDate>Thu, 14 Mar 2024 23:53:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openculture.com/2024/03/the-getty-makes-nearly-88000-art-images-free-to-use-however-you-like.html">https://www.openculture.com/2024/03/the-getty-makes-nearly-88000-art-images-free-to-use-however-you-like.html</a>, See on <a href="https://news.ycombinator.com/item?id=39710454">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p><img loading="lazy" fetchpriority="high" decoding="async" src="https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024.jpg" alt="" width="1024" height="805" srcset="https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024.jpg 1024w, https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024-360x283.jpg 360w, https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024-240x189.jpg 240w, https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024-768x604.jpg 768w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024.jpg" data-srcset="https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024.jpg 1024w, https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024-360x283.jpg 360w, https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024-240x189.jpg 240w, https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024-768x604.jpg 768w"></p>
<p>Since the J. Paul Get­ty Muse­um <a href="https://www.openculture.com/2013/08/the-getty-puts-4600-art-images-into-the-public-domain.html">launched its Open Con­tent pro­gram back in 2013</a>, we’ve been fea­tur­ing their efforts to&nbsp;make their vast col­lec­tion of cul­tur­al arti­facts freely acces­si­ble online. They’ve released not just dig­i­tized works of art, but also a great many <a href="https://www.openculture.com/2016/06/enjoy-100000-free-art-art-history-texts-courtesy-of-the-getty-research-portal.html">art his­to­ry texts</a> and <a href="https://www.openculture.com/2016/06/enjoy-100000-free-art-art-history-texts-courtesy-of-the-getty-research-portal.html">art books in gen­er­al</a>. Just this week, they announced an expan­sion of access to their dig­i­tal archive, in that they’ve made near­ly 88,000 images free to down­load on their <a href="https://newsletters.getty.edu/t/t-l-eyhxty-tltjujtrih-r/" target="_blank" rel="noopener" data-saferedirecturl="https://www.google.com/url?q=https://newsletters.getty.edu/t/t-l-eyhxty-tltjujtrih-r/&amp;source=gmail&amp;ust=1709952228434000&amp;usg=AOvVaw3D3T98bAbjSN30Dt8L1t4o">Open Con­tent</a>&nbsp;data­base under&nbsp;<a href="https://newsletters.getty.edu/t/t-l-eyhxty-tltjujtrih-y/" target="_blank" rel="noopener" data-saferedirecturl="https://www.google.com/url?q=https://newsletters.getty.edu/t/t-l-eyhxty-tltjujtrih-y/&amp;source=gmail&amp;ust=1709952228434000&amp;usg=AOvVaw22gWg7B6gaFWK4wfpyrGsj">Cre­ative Com­mons Zero (CC0)</a>. That means “you can copy, mod­i­fy, dis­trib­ute and per­form the work, even for com­mer­cial pur­pos­es, all with­out ask­ing per­mis­sion.”</p>
<p><img loading="lazy" decoding="async" src="https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802.jpg" alt="" width="802" height="1023" srcset="https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802.jpg 802w, https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802-282x360.jpg 282w, https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802-188x240.jpg 188w, https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802-768x980.jpg 768w" sizes="(max-width: 802px) 100vw, 802px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802.jpg" data-srcset="https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802.jpg 802w, https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802-282x360.jpg 282w, https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802-188x240.jpg 188w, https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802-768x980.jpg 768w"></p>
<p>The Get­ty sug­gests that you “add a print of your favorite Dutch still life to your gallery wall or cre­ate a show­er cur­tain using the <a href="https://www.getty.edu/art/collection/object/103JNH"><em>Iris­es</em></a> by Van Gogh.” But if you <a href="https://www.getty.edu/art/collection/search?open_content=true">search the open con­tent in their archive your­self</a>, you can sure­ly get much more cre­ative than that.</p>


<p>The por­tal’s inter­face lets you search by cre­ation date (with a time­line graph stretch­ing back to the year 6000 BC), medi­um (from agate and alabaster to wood­cut and zinc), object type (includ­ing paint­ings, pho­tographs, and sculp­tures, of course, but also akro­te­ria, horse trap­pings, and tweez­ers), and cul­ture. The selec­tion reflects the wide man­date of the Get­ty’s col­lec­tion, which encom­pass­es as many of the civ­i­liza­tions of the world as it does the eras of human his­to­ry.</p>
<p><img loading="lazy" decoding="async" src="https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024.jpg" alt="" width="1024" height="534" srcset="https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024.jpg 1024w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-360x188.jpg 360w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-240x125.jpg 240w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-768x401.jpg 768w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-470x246.jpg 470w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-484x252.jpg 484w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024.jpg" data-srcset="https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024.jpg 1024w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-360x188.jpg 360w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-240x125.jpg 240w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-768x401.jpg 768w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-470x246.jpg 470w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-484x252.jpg 484w"></p>
<p>In the <a href="https://www.getty.edu/art/collection/search?open_content=true">Get­ty’s open-con­tent archive</a>, you’ll find&nbsp;ancient sculp­ture from <a href="https://www.getty.edu/art/collection/search?open_content=true&amp;q=greek+sculpture">Greece</a>, <a href="https://www.getty.edu/art/collection/search?open_content=true&amp;q=roman+sculpture">Rome</a> and many oth­er parts of the world besides; a frag­men­tary oinochoe (that is, a wine jug) from third-cen­tu­ry-BC Ptole­ma­ic Egypt; <span>lav­ish­ly illu­mi­nat­ed </span><a href="https://www.getty.edu/art/collection/search?open_content=true&amp;q=book+of+hours">medieval books of hours</a><span> (of the kind </span><a href="https://www.openculture.com/2023/04/discover-the-medieval-illuminated-manuscript-les-tres-riches-heures-du-duc-de-berry.html">pre­vi­ous­ly fea­tured here on Open Cul­ture</a><span>); works by such inno­v­a­tive French painters as <a href="https://www.getty.edu/art/collection/search?open_content=true&amp;q=manet">Édouard Manet</a> and <a href="https://www.getty.edu/art/collection/search?open_content=true&amp;q=degas">Edgar Degas</a></span>; the stereo­scop­ic pho­tog­ra­phy of <a href="https://www.getty.edu/art/collection/person/104VP5">Car­leton H. Graves</a>, who in the late nine­teenth and ear­ly twen­ti­eth cen­tu­ry cap­tured places from Den­mark and Pales­tine, to Japan and Korea; the dar­ing abstrac­tions of artists like <a href="https://www.getty.edu/art/collection/object/10411Q">Hannes Maria Flach</a>, <a href="https://www.getty.edu/art/collection/object/106F7N">Jaromír Funke</a>, and <a href="https://www.getty.edu/art/collection/object/106GE7">Fran­cis Bruguière</a>. But what you do with them is, of course, entire­ly up to you. Enter <a href="https://www.getty.edu/art/collection/search?open_content=true">the col­lec­tion here</a>.</p>
<p><strong>Relat­ed con­tent:</strong></p>
<p><a href="https://www.openculture.com/2019/01/the-getty-digital-archive-expands-to-135000-images.html#google_vignette">The Get­ty Dig­i­tal Archive Expands to 135,000 Free Images: Down­load High Res­o­lu­tion Scans of Paint­ings, Sculp­tures, Pho­tographs &amp; Much Much More</a></p>
<p><a href="https://www.openculture.com/2021/03/a-search-engine-for-finding-free-public-domain-images-from-world-class-museums.html">A Search Engine for Find­ing Free, Pub­lic Domain Images from World-Class Muse­ums</a></p>
<p><a href="https://www.openculture.com/2016/06/enjoy-100000-free-art-art-history-texts-courtesy-of-the-getty-research-portal.html">100,000 Free Art His­to­ry Texts Now Avail­able Online Thanks to the Get­ty Research Por­tal</a></p>
<p><a href="https://www.openculture.com/2021/06/download-great-works-of-art-from-40-museums-worldwide.html">Down­load Great Works of Art from 40+ Muse­ums World­wide: Explore Artvee, the New Art Search Engine</a></p>
<p><a href="https://www.openculture.com/2023/04/the-smithsonian-puts-4-5-million-high-res-images-online.html">The Smith­son­ian Puts 4.5 Mil­lion High-Res Images Online and Into the Pub­lic Domain, Mak­ing Them Free to Use</a></p>
<p><a href="https://www.openculture.com/2018/12/download-325-free-art-books-getty-museum.html">Down­load Over 325 Free Art Books From the Get­ty Muse­um</a></p>
<p><em>Based in Seoul,&nbsp;</em><em><a href="http://blog.colinmarshall.org/">Col­in</a></em><em><a href="http://blog.colinmarshall.org/">&nbsp;M</a></em><em><a href="http://blog.colinmarshall.org/">a</a></em><em><a href="http://blog.colinmarshall.org/">rshall</a>&nbsp;writes and broad­cas</em><em>ts on cities, lan­guage, and cul­ture. His projects include the Sub­stack newslet­ter</em>&nbsp;<a href="https://colinmarshall.substack.com/">Books on Cities</a>,<em>&nbsp;the book&nbsp;</em>The State­less City: a Walk through 21st-Cen­tu­ry Los Ange­les&nbsp;<em>and the video series&nbsp;</em><a href="https://vimeo.com/channels/thecityincinema" rel="nofollow">The City in Cin­e­ma</a><em>. Fol­low him on Twit­ter at&nbsp;<a href="https://twitter.com/#%21/colinmarshall">@colinm</a></em><em><a href="https://twitter.com/#%21/colinmarshall">a</a></em><em><a href="https://twitter.com/#%21/colinmarshall">rshall</a>&nbsp;or on&nbsp;<a href="https://www.facebook.com/colinmarshallessayist">Face­book</a>.</em></p>
<br>		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Class Action Against General Motors LLC, OnStar LLC, LexisNexis Risk Solutions [pdf] (460 pts)]]></title>
            <link>https://static01.nyt.com/newsgraphics/documenttools/0a813fc8e0ac1b6c/6c03d310-full.pdf</link>
            <guid>39709991</guid>
            <pubDate>Thu, 14 Mar 2024 23:00:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://static01.nyt.com/newsgraphics/documenttools/0a813fc8e0ac1b6c/6c03d310-full.pdf">https://static01.nyt.com/newsgraphics/documenttools/0a813fc8e0ac1b6c/6c03d310-full.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=39709991">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Oregon Outback is now the largest Dark Sky Sanctuary in the world (261 pts)]]></title>
            <link>https://www.hereisoregon.com/experiences/2024/03/oregon-outback-is-now-the-largest-dark-sky-sanctuary-in-the-world.html</link>
            <guid>39709981</guid>
            <pubDate>Thu, 14 Mar 2024 22:59:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hereisoregon.com/experiences/2024/03/oregon-outback-is-now-the-largest-dark-sky-sanctuary-in-the-world.html">https://www.hereisoregon.com/experiences/2024/03/oregon-outback-is-now-the-largest-dark-sky-sanctuary-in-the-world.html</a>, See on <a href="https://news.ycombinator.com/item?id=39709981">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><figure><picture><source srcset="https://www.hereisoregon.com/resizer/_HagYJqVYuY9wOMkujjALuG74VA=/1440x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/TQPJJ43OFNE5VNRSKKPGHDIBPM.jpeg" media="screen and (min-width: 992px)"><source srcset="https://www.hereisoregon.com/resizer/xjmp4FlBSk0CP8atOehfv-UyIJE=/1024x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/TQPJJ43OFNE5VNRSKKPGHDIBPM.jpeg" media="screen and (min-width: 768px)"><source srcset="https://www.hereisoregon.com/resizer/-25zd8lcRPlJpxKkTBgvGEVim2Q=/768x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/TQPJJ43OFNE5VNRSKKPGHDIBPM.jpeg" media="screen and (min-width: 0px)"><img alt="stars and milky way over a small wooden cabin at night" src="https://www.hereisoregon.com/resizer/_HagYJqVYuY9wOMkujjALuG74VA=/1440x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/TQPJJ43OFNE5VNRSKKPGHDIBPM.jpeg" width="1440" height="0" loading="lazy"></picture><figcaption><p><span>Night sky over Summer Lake Hot Springs, outside of Paisley, Oregon, a few days before the height of the Perseid meteor shower. </span>(Samantha Swindler/Samantha Swindler/ The Oregonian)</p></figcaption></figure><p>A 2.5 million-acre swath of southern Oregon has been named the largest Dark Sky Sanctuary in the world.</p><p>The region, which on Monday was officially named the Oregon Outback International Dark Sky Sanctuary, comprises the southeastern half of Lake County, including <a href="https://www.oregonlive.com/travel/2017/07/a_rugged_high_desert_adventure.html">Hart Mountain</a>, <a href="https://www.oregonlive.com/environment/2022/01/oregons-lake-abert-is-in-deep-trouble-the-state-shut-down-its-effort-to-figure-out-why.html">Lake Abert</a> and <a href="https://www.oregonlive.com/living/2023/09/at-summer-lake-hot-springs-soaking-out-under-the-stars-is-the-golden-time.html">Summer Lake</a>. Future plans include expanding the sanctuary to 11.4 million acres across Harney and Malheur counties.</p><p>The designation was given by <a href="https://darksky.org/news/outback-dark-international-dark-sky-sanctuary/">DarkSky International</a>, an organization dedicated to protecting the nighttime environment and preserving dark skies through environmentally responsible outdoor lighting. The project is the work of the <a href="https://www.southernoregon.org/industry/oregon-outback-dark-sky-network/">Oregon Dark Sky Network</a>, an ad-hoc group of state, local and federal officials, private individuals, business owners and tourism agencies.</p><p>Travel Southern Oregon, which is a member of the network, celebrated the designation in a news release Monday.</p><p>“This four-year collaboration brings together so many of the elements we try to achieve in regenerative tourism,” Bob Hackett, executive director of Travel Southern Oregon, said. “It not only elevates the destination experience for visitors to Lake County and opens up opportunities for local businesses, but it also helps agencies and residents steward their lands in ways that celebrate a legacy of starry night skies for generations to come.”</p><p>Oregon already has two destinations with official DarkSky International designations: Prineville Reservoir State Park, which in 2021 <a href="https://www.oregonlive.com/travel/2021/05/prineville-reservoir-certified-as-oregons-first-dark-sky-park.html">became a Dark Sky Park</a>, and Sunriver Nature Center &amp; Observatory, which was <a href="https://darksky.org/news/sunriver-first-idsp-in-oregon/">named a Dark Sky Place</a> in 2020.</p><figure><picture><source srcset="https://www.hereisoregon.com/resizer/J8hUmJNk1nmt058kbhEAJbymmtI=/1440x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/KJLYG2Y3EJAFNI6OUVSSVQXG3A.jpg" media="screen and (min-width: 992px)"><source srcset="https://www.hereisoregon.com/resizer/ms0PNDHrqjILVovr83settcHlTA=/1024x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/KJLYG2Y3EJAFNI6OUVSSVQXG3A.jpg" media="screen and (min-width: 768px)"><source srcset="https://www.hereisoregon.com/resizer/0gZWWnb5JgsA6m6qfP2bu19q5Oo=/768x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/KJLYG2Y3EJAFNI6OUVSSVQXG3A.jpg" media="screen and (min-width: 0px)"><img src="https://www.hereisoregon.com/resizer/J8hUmJNk1nmt058kbhEAJbymmtI=/1440x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/KJLYG2Y3EJAFNI6OUVSSVQXG3A.jpg" width="1440" height="0" loading="lazy"></picture><figcaption><p>(Mark Graves/The Oregonian)</p></figcaption></figure><p>The Oregon Outback International Dark Sky Sanctuary is now the largest of 19 Dark Sky Sanctuaries, which are spread out across five continents. At 2.5 million acres, the Oregon sanctuary is larger than Minnesota’s 1 million-acre Boundary Waters Canoe Area Wilderness, which was designated as a Dark Sky Sanctuary in 2020.</p><p>The expansion of the Oregon Outback International Dark Sky Sanctuary seems inevitable, with only a few local approvals and lighting changes needed to make it happen, DarkSky International said. Most land in the region is either privately property or public lands managed by the Bureau of Land Management. The largest city in the area is Lakeview, home to fewer than 2,500 people.</p><p>Stargazers know southern and southeast Oregon as home to some of the best places to watch meteor showers and other astronomical events. Dark, clear skies are ideal for anyone hoping to peer into the cosmos, whether with a telescope or the naked eye.</p><p>Amber Harrison, program manager for DarkSky International, said in a news release Monday that the organization is already looking forward to the second phase of the Oregon Outback project, the big expansion, which would be the first landscape-scale sanctuary of its kind.</p><p>“Congratulations to the Oregon Outback Dark Sky Network team for achieving a monumental milestone in our journey towards preserving the night,” Harrison said. “Your dedication and collaboration have made Phase 1 of the Oregon Outback International Dark Sky Sanctuary a reality, showcasing the power of collective action in safeguarding night sky protections.”</p><p>--<a href="https://www.oregonlive.com/user/jameshale/posts.html">Jamie Hale</a> covers travel and the outdoors and co-hosts the <a href="https://podcasts.apple.com/us/podcast/peak-northwest/id1486961693">Peak Northwest podcast</a>. Reach him at 503-294-4077, <a href="mailto:jhale@oregonian.com">jhale@oregonian.com</a> or <a href="http://www.twitter.com/halejamesb">@HaleJamesB</a>.</p><p>Our journalism needs your support. Subscribe today to <a href="https://www.oregonlive.com/subscribe/">OregonLive.com</a>.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What I learned from looking at 900 most popular open source AI tools (319 pts)]]></title>
            <link>https://huyenchip.com/2024/03/14/ai-oss.html</link>
            <guid>39709912</guid>
            <pubDate>Thu, 14 Mar 2024 22:51:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://huyenchip.com/2024/03/14/ai-oss.html">https://huyenchip.com/2024/03/14/ai-oss.html</a>, See on <a href="https://news.ycombinator.com/item?id=39709912">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>Four years ago, I did an analysis of the <a href="https://huyenchip.com/2020/06/22/mlops.html">open source ML ecosystem</a>. Since then, the landscape has changed, so I revisited the topic. This time, I focused exclusively on the stack around foundation models.</p>

<p>The full list of open source AI repos is hosted at <a href="https://huyenchip.com/llama-police">llama-police</a>. The list is updated every 6 hours.</p>

<hr>
<p><b>Table of contents</b><br>
<a href="#data">Data</a><br>
…. <a href="#add_missing_repos">How to add missing repos</a><br>
<a href="#the_new_ai_stack">The New AI Stack</a><br>
…. <a href="#ai_stack_over_time">AI stack over time</a><br>
…….. <a href="#applications">Applications</a><br>
…….. <a href="#ai_engineering">AI engineering</a><br>
…….. <a href="#model_development">Model development</a><br>
…….. <a href="#infrastructure">Infrastructure</a><br>
<a href="#open_source_ai_developers">Open source AI developers</a><br>
…. <a href="#the_rise_of_one_person_companies">One-person billion-dollar companies</a><br>
…. <a href="#1_million_commits">1 million commits</a><br>
<a href="#the_growing_china_open_source_ecosystem">The growing China’s open source ecosystem</a><br>
<a href="#live_fast_die_young">Live fast, die young</a><br>
<a href="#my_personal_favorite_ideas">My personal favorite ideas</a><br>
<a href="#conclusion">Conclusion</a><br></p>

<hr>


<h2 id="data">Data</h2>

<p>I searched GitHub using the keywords <code>gpt</code>, <code>llm</code>, and <code>generative ai</code>. If AI feels so overwhelming right now, it’s because it is. There are 118K results for <code>gpt</code> alone.</p>

<p>To make my life easier, I limited my search to the repos with at least 500 stars. There were 590 results for <code>llm</code>, 531 for <code>gpt</code>, and 38 for <code>generative ai</code>. I also occasionally checked GitHub trending and social media for new repos.</p>

<p>After MANY hours, I found 896 repos. Of these, 51 are tutorials (e.g. <a href="https://github.com/dair-ai/Prompt-Engineering-Guide">dair-ai/Prompt-Engineering-Guide</a>) and aggregated lists (e.g. <a href="https://github.com/f/awesome-chatgpt-prompts">f/awesome-chatgpt-prompts</a>). While these tutorials and lists are helpful, I’m more interested in software. I still include them in the final list, but the analysis is done with the 845 software repositories.</p>

<p>It was a painful but rewarding process. It gave me a much better understanding of what people are working on, how incredibly collaborative the open source community is, and just how much China’s open source ecosystem diverges from the Western one.</p>

<h3 id="add_missing_repos">Add missing repos</h3>

<p>I undoubtedly missed a ton of repos. You can submit the missing repos <a href="https://forms.gle/1ijNSnizgWQaVYK16">here</a>. The list will be automatically updated every day.</p>

<p>Feel free to submit the repos with less than 500 stars. I’ll continue tracking them and add them to the list when they reach 500 stars!</p>

<h2 id="the_new_ai_stack">The New AI Stack</h2>

<p>I think of the AI stack as consisting of 4 layers: infrastructure, model development, application development, and applications.</p>

<center>
    <figure>
    <img alt="Generative AI Stack" src="https://huyenchip.com/assets/pics/ai-oss/1-ai-stack.png">
    </figure>
</center>


<ol>
  <li>
    <p><strong>Infrastructure</strong></p>

    <p>At the bottom is the stack is infrastructure, which includes toolings for serving (<a href="https://github.com/vllm-project/vllm">vllm</a>, <a href="https://github.com/triton-inference-server/server">NVIDIA’s Triton</a>), compute management (<a href="https://github.com/skypilot-org/skypilot">skypilot</a>), vector search and database (<a href="https://github.com/facebookresearch/faiss">faiss</a>, <a href="https://milvus.io/">milvus</a>, <a href="https://github.com/qdrant/qdrant">qdrant</a>, <a href="https://github.com/lancedb/lancedb">lancedb</a>), ….</p>
  </li>
  <li>
    <p><strong>Model development</strong></p>

    <p>This layer provides toolings for developing models, including frameworks for modeling &amp; training (transformers, pytorch, DeepSpeed), inference optimization (ggml, openai/triton), dataset engineering, evaluation, ….. Anything that involves changing a model’s weights happens in this layer, including finetuning.</p>
  </li>
  <li>
    <p><strong>Application development</strong>
 With readily available models, anyone can develop applications on top of them. This is the layer that has seen the most actions in the last 2 years and is still rapidly evolving. This layer is also known as AI engineering.</p>

    <p>Application development involves prompt engineering, RAG, AI interface, …</p>
  </li>
  <li>
    <p><strong>Applications</strong></p>

    <p>There are many open sourced applications built on top of existing models. The most popular types of applications are coding, workflow automation, information aggregation, …</p>
  </li>
</ol>

<p>Outside of these 4 layers, I also have another category, <strong>Model repos</strong>, that are created by companies and researchers to share the code associated with their models. Examples of repos in this category are <code>CompVis/stable-diffusion</code>, <code>openai/whisper</code>, and <code>facebookresearch/llama</code>.</p>

<h3 id="ai_stack_over_time">AI stack over time</h3>
<p>I plotted the cumulative number of repos in each category month-over-month. There was an explosion of new toolings in 2023, after the introduction of Stable Diffusion and ChatGPT. The curve seems to flatten in September 2023 because of three potential reasons.</p>

<ol>
  <li>I only include repos with at least 500 stars in my analysis, and it takes time for repos to gather these many stars.</li>
  <li>Most low-hanging fruits have been picked. What is left takes more effort to build, hence fewer people can build them.</li>
  <li>People have realized that it’s hard to be competitive in the generative AI space, so the excitement has calmed down. Anecdotally, in early 2023, all AI conversations I had with companies centered around gen AI, but the recent conversations are more grounded. Several even brought up scikit-learn. I’d like to revisit this in a few months to verify if it’s true.</li>
</ol>

<center>
    <figure>
    <img alt="Generative AI Stack Over Time" src="https://huyenchip.com/assets/pics/ai-oss/2-ai-timeline.png">
    </figure>
</center>


<p>In 2023, the layers that saw the highest increases were the applications and application development layers. The infrastructure layer saw a little bit of growth, but it was far from the level of growth seen in other layers.</p>

<h4 id="applications">Applications</h4>

<p>Not surprisingly, the most popular types of applications are coding, bots (e.g. role-playing, WhatsApp bots, Slack bots), and information aggregation (e.g. “let’s connect this to our Slack and ask it to summarize the messages each day”).</p>

<center>
    <figure>
    <img alt="Breakdown of popular AI applications" src="https://huyenchip.com/assets/pics/ai-oss/3-ai-applications.png">
    </figure>
</center>


<h4 id="ai_engineering">AI engineering</h4>

<p>2023 was the year of AI engineering. Since many of them are similar, it’s hard to categorize the tools. I currently put them into the following categories: prompt engineering, AI interface, Agent, and AI engineering (AIE) framework.</p>

<p><strong>Prompt engineering</strong> goes way beyond fiddling with prompts to cover things like constrained sampling (structured outputs), long-term memory management, prompt testing &amp; evaluation, etc.</p>

<center>
    <figure>
    <img alt="A list of prompt engineering tools" src="https://huyenchip.com/assets/pics/ai-oss/4-prompt-engineering.png">
    </figure>
</center>


<p><strong>AI interface</strong> provides an interface for your end users to interact with your AI application. This is the category I’m the most excited about. Some of the interfaces that are gaining popularity are:</p>

<ul>
  <li>Web and desktop apps.</li>
  <li>Browser extensions that let users quickly query AI models while browsing.</li>
  <li>Bots via chat apps like Slack, Discord, WeChat, and WhatsApp.</li>
  <li>Plugins that let developers embed AI applications to applications like VSCode, Shopify, and Microsoft Offices. The plugin approach is common for AI applications that can use tools to complete complex tasks (agents).</li>
</ul>

<p><strong>AIE framework</strong> is a catch-all term for all platforms that help you develop AI applications. Many of them are built around RAG, but many also provide other toolings such as monitoring, evaluation, etc.</p>

<p><strong>Agent</strong> is a weird category, as many agent toolings are just sophisticated prompt engineering with potentially constrained generation (e.g. the model can only output the predetermined action) and plugin integration (e.g. to let the agent use tools).</p>

<center>
    <figure>
    <img alt="AI engineering stack over time" src="https://huyenchip.com/assets/pics/ai-oss/5-ai-engineering.png">
    </figure>
</center>


<h4 id="model_development">Model development</h4>

<p>Pre-ChatGPT, the AI stack was dominated by model development. Model development’s biggest growth in 2023 came from increasing interest in inference optimization, evaluation, and parameter-efficient finetuning (which is grouped under Modeling &amp; training).</p>

<p>Inference optimization has always been important, but the scale of foundation models today makes it crucial for latency and cost. The core approaches for optimization remain the same (quantization, low-ranked factorization, pruning, distillation), but many new techniques have been developed especially for the transformer architecture and the new generation of hardware. For example, in 2020, 16-bit quantization was considered state-of-the-art. Today, we’re seeing <a href="https://arxiv.org/abs/2212.09720">2-bit quantization</a> and <a href="https://arxiv.org/abs/2402.17764">even lower than 2-bit</a>.</p>

<p>Similarly, evaluation has always been essential, but with many people today treating models as blackboxes, evaluation has become even more so. There are many new evaluation benchmarks and evaluation methods, such as comparative evaluation (see <a href="https://huyenchip.com/2024/02/28/predictive-human-preference.html#correctness_of_chatbot_arena_ranking">Chatbot Arena</a>) and AI-as-a-judge.</p>

<center>
    <figure>
    <img alt="Model Development Stack Over Time" src="https://huyenchip.com/assets/pics/ai-oss/6-model-development.png">
    </figure>
</center>


<h4 id="infrastructure">Infrastructure</h4>

<p>Infrastructure is about managing data, compute, and toolings for serving, monitoring, and other platform work. Despite all the changes that generative AI brought, the open source AI infrastructure layer remained more or less the same. This could also be because infrastructure products are typically not open sourced.</p>

<p>The newest category in this layer is vector database with companies like Qdrant, Pinecone, and LanceDB. However, many argue this shouldn’t be a category at all. Vector search has been around for a long time. Instead of building new databases just for vector search, existing database companies like DataStax and Redis are bringing vector search into where the data already is.</p>

<h2 id="open_source_ai_developers">Open source AI developers</h2>

<p>Open source software, like many things, follows the long tail distribution. A handful of accounts control a large portion of the repos.</p>

<h3 id="the_rise_of_one_person_companies">One-person billion-dollar companies</h3>
<p>845 repos are hosted on 594 unique GitHub accounts. There are 20 accounts with at least 4 repos. These top 20 accounts host 195 of the repos, or 23% of all the repos on the list. These 195 repos have gained a total of 1,650,000 stars.</p>

<center>
    <figure>
    <img alt="Most active GitHub accounts" src="https://huyenchip.com/assets/pics/ai-oss/7-top-accounts.png">
    </figure>
</center>


<p>On Github, an account can be either an organization or an individual. 19/20 of the top accounts are organizations. Of those, 3 belong to Google: <code>google-research</code>, <code>google</code>, <code>tensorflow</code>.</p>

<p>The only individual account in these top 20 accounts is lucidrains. Among the top 20 accounts with the most number of stars (counting only gen AI repos), 4 are individual accounts:</p>

<ul>
  <li><a href="https://github.com/lucidrains">lucidrains</a> (Phil Wang): who can implement state-of-the-art models insanely fast.</li>
  <li><a href="https://github.com/ggerganov">ggerganov</a> (Georgi Gerganov): an optimization god who comes from a physics background.</li>
  <li><a href="https://github.com/lllyasviel">Illyasviel</a> (Lyumin Zhang): creator of Foocus and ControlNet who’s currently a Stanford PhD.</li>
  <li><a href="https://github.com/xtekky">xtekky</a>: a full-stack developer who created gpt4free.</li>
</ul>

<center>
    <figure>
    <img alt="Most active GitHub accounts" src="https://huyenchip.com/assets/pics/ai-oss/8-top-accounts-stars.png">
    </figure>
</center>


<p>Unsurprisingly, the lower we go in the stack, the harder it is for individuals to build. Software in the infrastructure layer is the least likely to be started and hosted by individual accounts, whereas more than half of the applications are hosted by individuals.</p>

<center>
    <figure>
    <img alt="Can you do this alone?" src="https://huyenchip.com/assets/pics/ai-oss/9-indie.png">
    </figure>
</center>


<p>Applications started by individuals, on average, have gained more stars than applications started by organizations. Several people have speculated that we’ll see many very valuable one-person companies (see <a href="https://fortune.com/2024/02/04/sam-altman-one-person-unicorn-silicon-valley-founder-myth/">Sam Altman’s interview</a> and <a href="https://www.reddit.com/r/ChatGPT/comments/1ajwj5z/one_person_billion_dollar_company/">Reddit discussion</a>). I think they might be right.</p>

<center>
    <figure>
    <img alt="Can you do this alone?" src="https://huyenchip.com/assets/pics/ai-oss/10-indie-stars.png">
    </figure>
</center>


<h3 id="1_million_commits">1 million commits</h3>

<p>Over 20,000 developers have contributed to these 845 repos. In total, they’ve made almost a million contributions!</p>

<p>Among them, the 50 most active developers have made over 100,000 commits, averaging over 2,000 commits each. See the full list of the top 50 most active open source developers <a href="https://huyenchip.com/llama-devs">here</a>.</p>

<center>
    <figure>
    <img alt="Most active open source developers" src="https://huyenchip.com/assets/pics/ai-oss/11-devs.png">
    </figure>
</center>


<h2 id="the_growing_china_open_source_ecosystem">The growing China's open source ecosystem</h2>

<p>It’s been known for a long time that China’s AI ecosystem has diverged from the US (I also mentioned that in a <a href="https://huyenchip.com/2020/12/27/real-time-machine-learning.html#mlops_china_vs_us">2020 blog post</a>). At that time, I was under the impression that GitHub wasn’t widely used in China, and my view back then was perhaps colored by China’s 2013 ban on GitHub.</p>

<p>However, this impression is no longer true. There are many, many popular AI repos on GitHub targeting Chinese audiences, such that their descriptions are written in Chinese. There are repos for models developed for Chinese or Chinese + English, such as <a href="https://github.com/QwenLM/Qwen">Qwen</a>, <a href="https://github.com/THUDM/ChatGLM3">ChatGLM3</a>, <a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca">Chinese-LLaMA</a>.</p>

<p>While in the US, many research labs have moved away from the RNN architecture for language models, the RNN-based model family <a href="https://github.com/BlinkDL/RWKV-LM">RWKV</a> is still popular.</p>

<p>There are also AI engineering tools providing ways to integrate AI models into products popular in China like WeChat, QQ, DingTalk, etc. Many popular prompt engineering tools also have mirrors in Chinese.</p>

<p>Among the top 20 accounts on GitHub, 6 originated in China:</p>

<ol>
  <li><a href="https://github.com/THUDM">THUDM</a>: Knowledge Engineering Group (KEG) &amp; Data Mining at Tsinghua University.</li>
  <li><a href="https://github.com/OpenGVLab">OpenGVLab</a>: General Vision team of Shanghai AI Laboratory</li>
  <li><a href="https://github.com/OpenBMB">OpenBMB</a>: Open Lab for Big Model Base, founded by ModelBest &amp; the NLP group at Tsinghua University.</li>
  <li><a href="https://github.com/InternLM">InternLM</a>: from Shanghai AI Laboratory.</li>
  <li><a href="https://github.com/open-mmlab">OpenMMLab</a>: from The Chinese University of Hong Kong.</li>
  <li><a href="https://github.com/QwenLM">QwenLM</a>: Alibaba’s AI lab, which publishes the Qwen model family.</li>
</ol>

<h2 id="live_fast_die_young">Live fast, die young</h2>

<p>One pattern that I saw last year is that many repos quickly gained a massive amount of eyeballs, then quickly died down. Some of my friends call this the “hype curve”. Out of these 845 repos with at least 500 GitHub stars, 158 repos (18.8%) haven’t gained any new stars in the last 24 hours, and 37 repos (4.5%) haven’t gained any new stars in the last week.</p>

<p>Here are examples of the growth trajectory of two of such repos compared to the growth curve of two more sustained software. Even though these two examples shown here are no longer used, I think they were valuable in showing the community what was possible, and it was cool that the authors were able to get things out so fast.</p>

<center>
    <figure>
    <img alt="Hype curve" src="https://huyenchip.com/assets/pics/ai-oss/12-hype-curve.png">
    </figure>
</center>


<h2 id="my_personal_favorite_ideas">My personal favorite ideas</h2>

<p>So many cool ideas are being developed by the community. Here are some of my favorites.</p>

<ul>
  <li>Batch inference optimization: <a href="https://github.com/FMInference/FlexGen">FlexGen</a>, <a href="https://github.com/ggerganov/llama.cpp/pull/1375">llama.cpp</a></li>
  <li>Faster decoder with techniques such as <a href="https://github.com/FasterDecoding/Medusa">Medusa</a>, <a href="https://github.com/hao-ai-lab/LookaheadDecoding">LookaheadDecoding</a></li>
  <li>Model merging: <a href="https://github.com/cg123/mergekit">mergekit</a></li>
  <li>Constrained sampling: <a href="https://github.com/outlines-dev">outlines</a>, <a href="https://github.com/guidance-ai/guidance">guidance</a>, <a href="https://github.com/sgl-project/sglang">SGLang</a></li>
  <li>Seemingly niche tools that solve one problem really well, such as <a href="https://github.com/arogozhnikov/einops">einops</a> and <a href="https://github.com/huggingface/safetensors">safetensors</a>.</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>Even though I included only 845 repos in my analysis, I went through several thousands of repos. I found this helpful for me to get a big-picture view of the seemingly overwhelming AI ecosystem. I hope the <a href="https://huyenchip.com/llama-police">list</a> is useful for you too. Please do let me know what repos I’m missing, and I’ll add them to the list!</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Buys DarwinAI Ahead of Major Generative AI Updates Coming in iOS 18 (112 pts)]]></title>
            <link>https://www.macrumors.com/2024/03/14/apple-acquires-darwinai/</link>
            <guid>39709835</guid>
            <pubDate>Thu, 14 Mar 2024 22:43:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2024/03/14/apple-acquires-darwinai/">https://www.macrumors.com/2024/03/14/apple-acquires-darwinai/</a>, See on <a href="https://news.ycombinator.com/item?id=39709835">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2024/03/14/apple-acquires-darwinai/"><p>Apple acquired Canada-based company DarwinAI earlier this year to build out its AI team, reports <em><a href="https://www.bloomberg.com/news/articles/2024-03-14/apple-aapl-buys-canadian-ai-startup-darwinai-as-part-of-race-to-add-features">Bloomberg</a></em>. DarwinAI created AI technology for inspecting components during the manufacturing process, and it also had a focus on making smaller and more efficient AI systems.</p>
<p><img src="https://images.macrumors.com/t/KnV8VAc6R5Dd_3U1Esmw774Zr2U=/400x0/article-new/2022/03/hey-siri-banner-apple.jpg?lossy" srcset="https://images.macrumors.com/t/KnV8VAc6R5Dd_3U1Esmw774Zr2U=/400x0/article-new/2022/03/hey-siri-banner-apple.jpg?lossy 400w,https://images.macrumors.com/t/j8j2WKQb8lEd30rj8l8Z1wR25JE=/800x0/article-new/2022/03/hey-siri-banner-apple.jpg?lossy 800w,https://images.macrumors.com/t/WnxAtULV_HFlWOr5EliFjsbm-OU=/1600x0/article-new/2022/03/hey-siri-banner-apple.jpg 1600w,https://images.macrumors.com/t/X4xoV100u9gMqO-t4CP7rdkoQvE=/2500x0/filters:no_upscale()/article-new/2022/03/hey-siri-banner-apple.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="hey siri banner apple" width="1200" height="630"><br>DarwinAI's website and social media accounts have been taken offline following Apple's purchase. Dozens of former DarwinAI companies have now joined Apple's artificial intelligence division. AI researcher Alexander Wong, who helped build DarwinAI, is now a director in Apple's AI group.</p>
<p>Apple confirmed the acquisition with the statement that it typically gives when questioned about purchases. "Apple buys smaller technology companies from time to time" but does not discuss its purpose or plans.</p>
<p>In an effort to catch up with Microsoft, Google, and others in the AI market, Apple is working hard to build artificial intelligence features for its next-generation <a href="https://www.macrumors.com/roundup/ios-18/">iOS 18</a> and macOS 15 operating systems.</p>
<p>If Apple wants to be able to rival Microsoft's Bing, OpenAI's ChatGPT, and other generative AI offerings, it will need to integrate generative AI into a range of products. Apple is testing large language models, and AI features are said to be coming to <a href="https://www.macrumors.com/guide/siri/">Siri</a>, Shortcuts, Messages, <a href="https://www.macrumors.com/guide/apple-music/">Apple Music</a>, and more.</p>
<p>Apple is aiming to have AI features run on-device for privacy reasons, and DarwinAI's efforts to make smaller AI systems could be of use to further that endeavor.</p>
<p>Apple CEO <a href="https://www.macrumors.com/guide/tim-cook/">Tim Cook</a> <a href="https://www.macrumors.com/2024/02/28/tim-cook-apple-generative-ai-break-new-ground/">has promised</a> that Apple will "break new ground" in generative AI in 2024. "We believe it will unlock transformative opportunities for our users," said Cook.</p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2024/03/11/iphone-16-pro-expected-later-this-year/">iPhone 16 Pro Expected Later This Year With These 10 New Features</a></h3><p>While the iPhone 16 Pro and iPhone 16 Pro Max are still around six months away from launching, there are already many rumors about the devices. Below, we have recapped new features and changes expected so far. These are some of the key changes rumored for the iPhone 16 Pro models as of March 2024:Larger displays: The iPhone 16 Pro and iPhone 16 Pro Max will be equipped with larger 6.3-inch...</p></div><div><h3><a href="https://www.macrumors.com/2024/03/12/iphone-se-4-expected-to-depreciate-heavily/">iPhone SE 4 Expected to Depreciate Heavily</a></h3><p>Resale value trends suggest the iPhone SE 4 may not hold its value as well as Apple's flagship models, according to SellCell. According to the report, Apple's iPhone SE models have historically depreciated much more rapidly than the company's more premium offerings. The third-generation iPhone SE, which launched in March 2022, experienced a significant drop in resale value, losing 42.6%...</p></div><div><h3><a href="https://www.macrumors.com/2024/03/11/2024-ipad-pro-key-rumors/">2024 iPad Pro: Key Rumors to Be Aware of Ahead of Announcement</a></h3><p>Apple's next-generation iPad Pro models are expected to be announced in a matter of weeks, so what can customers expect from the highly anticipated new machines? The 2022 iPad Pro was a minor update that added the M2 chip, Apple Pencil hover, and specification upgrades like Wi-Fi 6E and Bluetooth 5.3 connectivity. The iPad Pro as a whole has generally only seen relatively small updates in...</p></div><div><h3><a href="https://www.macrumors.com/2024/03/14/apple-wallet-app-ids-slow-adoption/">Apple Said iPhone Driver's Licenses Would Expand to These 8 U.S. States… Two Years Ago</a></h3><p>In just four U.S. states, residents can add their driver's license or ID to the Apple Wallet app on the iPhone and Apple Watch, providing a convenient and contactless way to display proof of identity or age at select airports, businesses, and venues. Adoption of the feature has been slow since Apple first announced it in September 2021, with IDs in the Wallet app only available in Arizona,...</p></div><div><h3><a href="https://www.macrumors.com/2024/03/11/apple-preparing-ios-17-4-1/">Apple Preparing iOS 17.4.1 Update for iPhone</a></h3><p>Apple appears to be internally testing iOS 17.4.1 for the iPhone, based on evidence of the software update in our website's logs this week. Our logs have revealed the existence of several iOS 17 versions before Apple released them, ranging from iOS 17.0.3 to iOS 17.3.1. iOS 17.4.1 should be a minor update that addresses software bugs and/or security vulnerabilities. It is unclear when...</p></div><div><h3><a href="https://www.macrumors.com/2024/03/12/apple-announces-app-downloads-from-websites/">Apple Announces Ability to Download iPhone Apps From Websites in EU</a></h3><p>Apple today announced three further changes for developers in the European Union, allowing them to distribute apps directly from webpages, choose how to design in-app promotions, and more. Apple last week enabled alternative app stores in the EU in iOS 17.4, allowing third-party app stores to offer a catalog of other developers' apps as well as the marketplace developer's own apps. As of...</p></div><div><h3><a href="https://www.macrumors.com/2024/03/14/apple-acquires-darwinai/">Apple Buys DarwinAI Ahead of Major Generative AI Updates Coming in iOS 18</a></h3><p>Thursday March 14, 2024 10:27 am PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>Apple acquired Canada-based company DarwinAI earlier this year to build out its AI team, reports Bloomberg. DarwinAI created AI technology for inspecting components during the manufacturing process, and it also had a focus on making smaller and more efficient AI systems. DarwinAI's website and social media accounts have been taken offline following Apple's purchase. Dozens of former DarwinAI ...</p></div><div><h3><a href="https://www.macrumors.com/2024/03/12/m3-macbook-air-vs-macbook-pro/">Video Comparison: M3 MacBook Air vs. M3 MacBook Pro</a></h3><p>Tuesday March 12, 2024 10:42 am PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>With the refresh of the MacBook Air models in March, Apple now has M3 versions of the 13-inch MacBook Air, 14-inch MacBook Pro, and 15-inch MacBook Air, all with the same chip inside. For those trying to decide between the MacBook Pro and the MacBook Air, we did a comparison video to highlight what you're getting with each machine. Subscribe to the MacRumors YouTube channel for more videos. ...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Genetically engineering koji mold to create a meat alternative (103 pts)]]></title>
            <link>https://newscenter.lbl.gov/2024/03/14/its-hearty-its-meaty-its-mold/</link>
            <guid>39709827</guid>
            <pubDate>Thu, 14 Mar 2024 22:43:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newscenter.lbl.gov/2024/03/14/its-hearty-its-meaty-its-mold/">https://newscenter.lbl.gov/2024/03/14/its-hearty-its-meaty-its-mold/</a>, See on <a href="https://news.ycombinator.com/item?id=39709827">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

      
<lbl-container wrapper-size="sm">
  <lbl-rich-text>
    <p>With animal-free dairy products and convincing vegetarian meat substitutes already on the market, it’s easy to see how biotechnology can change the food industry. Advances in genetic engineering are allowing us to harness microorganisms to produce cruelty-free products that are healthy for consumers and healthier for the environment.</p>
<p>One of the most promising sources of innovative foods is fungi – a diverse kingdom of organisms that naturally produce a huge range of tasty and nutritious proteins, fats, antioxidants, and flavor molecules. Chef-turned-bioengineer <a href="https://www.jbei.org/science-or-alchemy-transforming-the-future-of-food/" target="_blank" rel="noopener">Vayu Hill-Maini</a>, an affiliate in the Biosciences Area at Lawrence Berkeley National Laboratory (Berkeley Lab), is exploring the many possibilities for new tastes and textures that can be made from modifying the genes already present in fungi.</p>
<p>“I think it’s a fundamental aspect of synthetic biology that we’re benefiting from organisms that have evolved to be really good at certain things,” said Hill-Maini, who is a postdoctoral researcher at UC Berkeley in the lab of bioengineering expert <a href="https://keaslinglab.lbl.gov/jay-keasling/">Jay Keasling</a>. “What we’re trying to do is to look at what is the fungus making and try to kind of unlock and enhance it. And I think that’s an important angle that we don’t need to introduce genes from wildly different species. We’re investigating how we can stitch things together and unlock what’s already there.”</p>
<p>In their recent paper, <a href="https://www.nature.com/articles/s41467-024-46314-8" target="_blank" rel="noopener">published today in <em>Nature Communications</em></a>, Hill-Maini and colleagues at UC Berkeley, the Joint BioEnergy Institute, and the Novo Nordisk Foundation Center for Biosustainability studied a multicellular fungus called <em>Aspergillus oryzae</em>, also known as koji mold, that has been used in East Asia to ferment starches into sake, soy sauce, and miso for centuries. First, the team used CRISPR-Cas9 to develop a gene editing system that can make consistent and reproducible changes to the koji mold genome. Once they had established a toolkit of edits, they applied their system to make modifications that elevate the mold as a food source. First, Hill-Maini focused on boosting the mold’s production of heme – an iron-based molecule which is found in many lifeforms but is most abundant in animal tissue, giving meat its color and distinctive flavor. (A synthetically produced plant-derived heme is also what gives the Impossible Burger its meat-duping properties.) Next, the team punched up production of ergothioneine, an antioxidant only found in fungi that is associated with cardiovascular health benefits.</p>
<p>After these changes, the once-white fungi grew red. With minimal preparation – removing excess water and grinding – the harvested fungi could be shaped into a patty, then fried into a tempting-looking burger.</p>
<div id="attachment_54355"><p><a href="https://newscenter.lbl.gov/wp-content/uploads/2024/03/2024-02-02_featuredimage.jpg"><img decoding="async" aria-describedby="caption-attachment-54355" src="https://newscenter.lbl.gov/wp-content/uploads/2024/03/2024-02-02_featuredimage.jpg" alt="A small, rounded reddish mass that looks like a meat patty sitting in a frying pan beaded with oil" width="1347" height="1500" srcset="https://newscenter.lbl.gov/wp-content/uploads/2024/03/2024-02-02_featuredimage.jpg 1347w, https://newscenter.lbl.gov/wp-content/uploads/2024/03/2024-02-02_featuredimage-768x855.jpg 768w, https://newscenter.lbl.gov/wp-content/uploads/2024/03/2024-02-02_featuredimage-330x367.jpg 330w, https://newscenter.lbl.gov/wp-content/uploads/2024/03/2024-02-02_featuredimage-890x991.jpg 890w" sizes="(max-width: 1347px) 100vw, 1347px"></a></p><p id="caption-attachment-54355">The koji mold patty after frying. (Credit: Vayu Hill-Maini)</p></div>
<p>Hill-Maini’s next objective is to make the fungi even more appealing by tuning the genes that control the mold’s texture. “We think that there’s a lot of room to explore texture by varying the fiber-like morphology of the cells. So, we might be able to program the structure of the lot fibers to be longer which would give a more meat-like experience. And then we can think about boosting lipid composition for mouth feel and further nutrition,” said Hill-Maini, who was a Fellow of the Miller Institute for Basic Research in Science at UC Berkeley during the study. “I’m really excited about how can we further look at the fungus and, you know, tinker with its structure and metabolism for food.”</p>
<p>Though this work is just the beginning of the journey to tap into fungal genomes to create new foods, it showcases the huge potential of these organisms to serve as easy-to-grow protein sources that avoid the complex ingredients lists of current meat substitutes&nbsp; and the cost barriers and technical <a href="https://www.forbes.com/sites/chloesorvino/2023/06/27/everything-you-need-to-know-about-lab-grown-meat-now-that-its-here/?sh=7f49d298f8a5">difficulties hindering the launch of cultured meat</a>. Additionally, the team’s gene editing toolkit is a big leap forward for the field of synthetic biology as a whole. Currently, a great variety of biomanufactured goods are made by engineered bacteria and yeast, the single-celled cousins of mushrooms and mold. Yet despite humanity’s long history of domesticating fungi to eat directly or to make staples like miso, multicellular fungi have not yet been harnessed as engineered cellular factories to the same extent because their genomes are far more complex, and have adaptations that make gene editing a challenge. The CRISPR-Cas9 toolkit developed in this paper lays the foundation to easily edit koji mold and its many relatives.</p>
  </lbl-rich-text>
</lbl-container>







<lbl-container wrapper-size="sm">
  <lbl-rich-text>
    <p>“These organisms have been used for centuries to produce food, and they are incredibly efficient at converting carbon into a wide variety of complex molecules, including many that would be almost impossible to produce using a classic host like brewer’s yeast or <em>E. coli</em>,” said senior author <a href="https://keaslinglab.lbl.gov/jay-keasling/">Jay Keasling</a>, who is a senior scientist at Berkeley Lab and a professor at UC Berkeley. “By unlocking koji mold through the development of these tools, we are unlocking the potential of a huge new group of hosts that we can use to make foods, valuable chemicals, energy-dense biofuels, and medicines. It’s a thrilling new avenue for biomanufacturing.”</p>
<div id="attachment_54375"><p><a href="https://newscenter.lbl.gov/wp-content/uploads/2024/03/XBD-202403-036-013.jpg"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-54375" src="https://newscenter.lbl.gov/wp-content/uploads/2024/03/XBD-202403-036-013.jpg" alt="A young man in a long sleeve red top sitting outside on a picnic bench. He is wearing lab goggles and holding a fork. On the table in front of him is a plate containing a petri dish filled with fuzzy mold." width="360" height="539" srcset="https://newscenter.lbl.gov/wp-content/uploads/2024/03/XBD-202403-036-013.jpg 1002w, https://newscenter.lbl.gov/wp-content/uploads/2024/03/XBD-202403-036-013-768x1150.jpg 768w, https://newscenter.lbl.gov/wp-content/uploads/2024/03/XBD-202403-036-013-330x494.jpg 330w, https://newscenter.lbl.gov/wp-content/uploads/2024/03/XBD-202403-036-013-890x1332.jpg 890w" sizes="(max-width: 360px) 100vw, 360px"></a></p><p id="caption-attachment-54375">Vayu Hill-Maini is excited to see engineered fungi advance to new food products. Seen here on the JBEI balcony. (Credit: Marilyn Sargent/Berkeley Lab)</p></div>
<p>Given his culinary background, Hill-Maini is keen to ensure that the next generation of fungi-based products are not only palatable, but truly desirable to customers, including those with sophisticated tastes. In a separate study, he and Keasling collaborated with chefs at Alchemist, a two-Michelin-starred restaurant in Copenhagen, to play with the culinary potential of another multicellular fungus, <em>Neurospora intermedia. </em>This fungus is traditionally used in Indonesia to produce a staple food called oncom by fermenting the waste products left over from making other foods, such as tofu. Intrigued by its ability to convert leftovers into a protein-rich food, the scientists and chefs studied the fungus in the Alchemist test kitchen. They discovered <em>N. intermedia </em>produces and excretes many enzymes as it grows. When grown on starchy rice, the fungi produces an enzyme that liquifies the rice and makes it intensely sweet. “We developed a process with just three ingredients – rice, water, and fungus – to make a beautiful, striking orange-colored porridge,” said Hill-Maini. “That became a new dish on the tasting menu that utilizes fungal chemistry and color in a dessert. And I think that what it really shows is that there’s opportunity to bridge the laboratory and the kitchen.”</p>
<p>Hill-Maini’s work on the gene editing research described in this article is supported by the Miller Institute at UC Berkeley. Keasling’s lab is supported by the Novo Nordisk Foundation. Both received additional support from the Department of Energy (DOE) Office of Science. The Joint BioEnergy Institute is a DOE Bioenergy Research Center managed by Berkeley Lab.</p>
<p># # #</p>
<p>Lawrence Berkeley National Laboratory (Berkeley Lab) is committed to delivering solutions for humankind through research in clean energy, a healthy planet, and discovery science. Founded in 1931 on the belief that the biggest problems are best addressed by teams, Berkeley Lab and its scientists have been recognized with 16 Nobel Prizes. Researchers from around the world rely on the Lab’s world-class scientific facilities for their own pioneering research. Berkeley Lab is a multiprogram national laboratory managed by the University of California for the U.S. Department of Energy’s Office of Science.</p>
<p>DOE’s Office of Science is the single largest supporter of basic research in the physical sciences in the United States, and is working to address some of the most pressing challenges of our time. For more information, please visit&nbsp;<a href="http://energy.gov/science">energy.gov/science</a>.</p>
  </lbl-rich-text>
</lbl-container>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ban on same-sex marriage unconstitutional, Sapporo High Court rules (114 pts)]]></title>
            <link>https://www.japantimes.co.jp/news/2024/03/14/japan/crime-legal/same-sex-marriage-ruling/</link>
            <guid>39709657</guid>
            <pubDate>Thu, 14 Mar 2024 22:24:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.japantimes.co.jp/news/2024/03/14/japan/crime-legal/same-sex-marriage-ruling/">https://www.japantimes.co.jp/news/2024/03/14/japan/crime-legal/same-sex-marriage-ruling/</a>, See on <a href="https://news.ycombinator.com/item?id=39709657">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="jtarticle">
                                      <p>The Sapporo High Court on Thursday ruled that a ban on same-sex marriage is unconstitutional, with strong phrasing <s> </s>that is expected to pressure the government and lawmakers for action.</p><p>It is the first time a high court has handed down a ruling that said Japan's ban on same-sex marriage is unconstitutional.</p><p>The high court judgment followed a similar ruling at the Tokyo District Court earlier on Thursday, which said that the ban on same-sex marriage is in a “state of unconstitutionality” due to the lack of legal protections for same-sex couples. The Tokyo court, however, stopped short of issuing a stronger “unconstitutional” verdict.</p>
                    
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CEO of data privacy company Onerep.com founded dozens of people-search firms (531 pts)]]></title>
            <link>https://krebsonsecurity.com/2024/03/ceo-of-data-privacy-company-onerep-com-founded-dozens-of-people-search-firms/</link>
            <guid>39709089</guid>
            <pubDate>Thu, 14 Mar 2024 21:17:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://krebsonsecurity.com/2024/03/ceo-of-data-privacy-company-onerep-com-founded-dozens-of-people-search-firms/">https://krebsonsecurity.com/2024/03/ceo-of-data-privacy-company-onerep-com-founded-dozens-of-people-search-firms/</a>, See on <a href="https://news.ycombinator.com/item?id=39709089">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
												<p>The data privacy company <strong>Onerep.com</strong> bills itself as a Virginia-based service for helping people remove their personal information from almost 200 people-search websites. However, an investigation into the history of onerep.com finds this company is operating out of Belarus and Cyprus, and that its founder has launched dozens of people-search services over the years.</p>
<p>Onerep’s “Protect” service starts at $8.33 per month for individuals and $15/mo for families, and promises to remove your personal information from nearly 200 people-search sites. Onerep also markets its service to companies seeking to offer their employees the ability to have their data continuously removed from people-search sites.</p>
<div id="attachment_66754"><p><img aria-describedby="caption-attachment-66754" decoding="async" src="https://krebsonsecurity.com/wp-content/uploads/2024/03/onerep-permanente.png" alt="" width="250" height="436"></p><p id="caption-attachment-66754">A testimonial on onerep.com.</p></div>
<p>Customer case studies published on onerep.com state that it struck a deal to offer the service to employees of <strong>Permanente Medicine</strong>, which represents the doctors within the health insurance giant <strong>Kaiser Permanente</strong>. Onerep also says it has made inroads among police departments in the United States.</p>
<p>But a review of Onerep’s domain registration records and that of its founder reveal a different side to this company. Onerep.com says its founder and CEO is <strong>Dimitri Shelest</strong> from Minsk, Belarus, as does <a href="https://www.linkedin.com/in/dimitri-shelest-183626174/" target="_blank" rel="noopener">Shelest’s profile on LinkedIn</a>. Historic registration records indexed by <a href="https://www.domaintools.com/" target="_blank" rel="noopener">DomainTools.com</a> say Mr. Shelest was a registrant of onerep.com who used the email address <strong>dmitrcox2@gmail.com</strong>.</p>
<p>A search in the data breach tracking service <a href="https://constella.ai/" target="_blank" rel="noopener">Constella Intelligence</a> for the name Dimitri Shelest brings up the email address <strong>dimitri.shelest@onerep.com</strong>. Constella also finds that Dimitri Shelest from Belarus used the email address <strong>d.sh@nuwber.com</strong>, and the Belarus phone number <strong>+375-292-702786</strong>.</p>
<p>Nuwber.com is a people search service whose employees all appear to be from Belarus, and it is one of dozens of people-search companies that Onerep claims to target with its data-removal service. Onerep.com’s website disavows any relationship to Nuwber.com, stating quite clearly, “Please note that OneRep is not associated with Nuwber.com.”</p>
<p>However, there is an abundance of evidence suggesting Mr. Shelest is in fact the founder of Nuwber. Constella found that Minsk telephone number (375-292-702786) has been used multiple times in connection with the email address <strong>dmitrcox@gmail.com</strong>. Recall that Onerep.com’s domain registration records in 2018 list the email address dmitrcox2@gmail.com.</p>
<p>It appears Mr. Shelest sought to reinvent his online identity in 2015 by adding a “2” to his email address. A search on the Belarus phone number tied to Nuwber.com shows up in the domain records for <strong>askmachine.org</strong>, and DomainTools says this domain is tied to both dmitrcox@gmail.com and dmitrcox2@gmail.com.</p>
<div id="attachment_66783"><p><img aria-describedby="caption-attachment-66783" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/03/onerep-shellest.png" alt="" width="749" height="439" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/03/onerep-shellest.png 1275w, https://krebsonsecurity.com/wp-content/uploads/2024/03/onerep-shellest-768x450.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/03/onerep-shellest-782x458.png 782w" sizes="(max-width: 749px) 100vw, 749px"></p><p id="caption-attachment-66783">Onerep.com CEO and founder Dimitri Shelest, as pictured on the “about” page of onerep.com.</p></div>
<p>A search in DomainTools for the email address dmitrcox@gmail.com shows it is associated with the registration of at least 179 domain names, including dozens of mostly now-defunct people-search companies targeting citizens of Argentina, Brazil, Canada, Denmark, France, Germany, Hong Kong, Israel, Italy, Japan, Latvia and Mexico, among others.</p>
<p>Those include <a href="https://web.archive.org/web/20160324064030/https://nuwber.fr/" target="_blank" rel="noopener"><strong>nuwber.fr</strong></a>, a site registered in 2016 which was identical to <a href="https://web.archive.org/web/20160616212526/https://nuwber.com/" target="_blank" rel="noopener">the homepage of Nuwber.com at the time</a>. DomainTools shows the same email and Belarus phone number are in historic registration records for <a href="https://web.archive.org/web/20170709170147/https://nuwber.at/" target="_blank" rel="noopener">nuwber.at</a>, <a href="https://web.archive.org/web/20160804065746/https://nuwber.ch/" target="_blank" rel="noopener">nuwber.ch</a>, and <a href="https://web.archive.org/web/20160806212859/https://nuwber.dk/" target="_blank" rel="noopener">nuwber.dk</a> (all domains linked here are to their cached copies at archive.org, where available).</p>
<div id="attachment_66755"><p><a href="https://krebsonsecurity.com/wp-content/uploads/2024/03/nuwber-dot-com.png" target="_blank" rel="noopener"><img aria-describedby="caption-attachment-66755" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/03/nuwber-dot-com.png" alt="" width="749" height="477" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/03/nuwber-dot-com.png 1436w, https://krebsonsecurity.com/wp-content/uploads/2024/03/nuwber-dot-com-768x489.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/03/nuwber-dot-com-782x498.png 782w" sizes="(max-width: 749px) 100vw, 749px"></a></p><p id="caption-attachment-66755">Nuwber.com, circa 2015. Image: Archive.org.</p></div>
<p>A review of historic WHOIS records for onerep.com show it was registered for many years to a resident of Sioux Falls, SD for a completely unrelated site. But around Sept. 2015 the domain switched from the registrar GoDaddy.com to eNom, and the registration records were hidden behind privacy protection services. DomainTools indicates around this time onerep.com started using domain name servers from DNS provider constellix.com.&nbsp;Likewise, Nuwber.com first appeared in late 2015, was also registered through eNom, and also started using constellix.com for DNS at nearly the same time.</p>
<p>Listed <a href="https://www.linkedin.com/in/dzmitrybukuyazau/" target="_blank" rel="noopener">on LinkedIn</a> as a former product manager at OneRep.com between 2015 and 2018 is <strong>Dimitri Bukuyazau</strong>, who says their hometown is Warsaw, Poland. While this LinkedIn profile (linkedin.com/in/<strong>dzmitry</strong>bukuyazau) does not mention Nuwber, a search on this name in Google turns up <a href="https://web.archive.org/web/20200906155823/https://www.privacyduck.com/comparisons/privacyduck-vs-onerep-com-the-eastern-european-privacy-company/" target="_blank" rel="noopener">a 2017 blog post from privacyduck.com</a>, which laid out a number of reasons to support a conclusion that OneRep and Nuwber.com were the same company.</p>
<p>“Any people search profiles containing your Personally Identifiable Information that were on Nuwber.com were also mirrored identically on OneRep.com, down to the relatives’ names and address histories,” Privacyduck.com wrote. The post continued:</p>
<blockquote><p>“Both sites offered the same immediate opt-out process. Both sites had the same generic contact and support structure. They were – and remain – the same company (even PissedConsumer.com advocates this fact: https://nuwber.pissedconsumer.com/nuwber-and-onerep-20160707878520.html).”</p>
<p>“Things changed in early 2016 when OneRep.com began offering privacy removal services right alongside their own open displays of your personal information. At this point when you found yourself on Nuwber.com OR OneRep.com, you would be provided with the option of opting-out your data on their site for free – but also be highly encouraged to pay them to remove it from a slew of other sites (and part of that payment was removing you from their own site, Nuwber.com, as a benefit of their service).”</p></blockquote>
<p>Reached via LinkedIn, Mr. Bukuyazau declined to answer questions, such as whether he ever worked at Nuwber.com. However, Constella Intelligence finds two interesting email addresses for employees at nuwber.com: d.bu@nuwber.com, and d.bu+figure-eight.com@nuwber.com, which was registered under the name “<strong>Dzmitry</strong>.”</p>
<p>PrivacyDuck’s claims about how onerep.com appeared and behaved in the early days are not readily verifiable because the domain onerep.com has been completely excluded from the Wayback Machine at archive.org. The Wayback Machine will honor such requests if they come directly from the owner of the domain in question.</p>
<p>Still, Mr. Shelest’s name, phone number and email also appear in the domain registration records for a truly dizzying number of country-specific people-search services, including <strong>pplcrwlr.in</strong>, <a href="https://web.archive.org/web/20150205232653/http://pplcrwlr.fr/" target="_blank" rel="noopener">pplcrwlr.fr</a>, <a href="https://web.archive.org/web/20150206043953/http://pplcrwlr.dk/" target="_blank" rel="noopener">pplcrwlr.dk</a>, <a href="https://web.archive.org/web/20150206005352/http://pplcrwlr.jp/" target="_blank" rel="noopener">pplcrwlr.jp</a>, <strong>peeepl.br.com</strong>, <strong>peeepl.in</strong>, <a href="https://web.archive.org/web/20130307194029/http://peeepl.it/" target="_blank" rel="noopener">peeepl.it</a> and <a href="https://web.archive.org/web/20140320054902/http://peeepl.co.uk/" target="_blank" rel="noopener">peeepl.co.uk</a>.</p>
<p>The same details appear in the WHOIS registration records for the now-defunct people-search sites <a href="https://web.archive.org/web/20140208014115/http://waatpp.de/" target="_blank" rel="noopener">waatpp.de</a>, <a href="https://web.archive.org/web/20150205232901/http://waatp1.fr/" target="_blank" rel="noopener">waatp1.fr</a>, <a href="https://web.archive.org/web/20120112131753/http://azersab.com/" target="_blank" rel="noopener">azersab.com</a>, and <a href="https://web.archive.org/web/20120507115825/http://ahavoila.com/" target="_blank" rel="noopener">ahavoila.com</a>, a people-search service for French citizens.<span id="more-66752"></span></p>
<div id="attachment_66756"><p><a href="https://krebsonsecurity.com/wp-content/uploads/2024/03/waatp-de.png" target="_blank" rel="noopener"><img aria-describedby="caption-attachment-66756" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/03/waatp-de.png" alt="" width="750" height="345" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/03/waatp-de.png 1880w, https://krebsonsecurity.com/wp-content/uploads/2024/03/waatp-de-768x353.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/03/waatp-de-1536x707.png 1536w, https://krebsonsecurity.com/wp-content/uploads/2024/03/waatp-de-782x360.png 782w" sizes="(max-width: 750px) 100vw, 750px"></a></p><p id="caption-attachment-66756">The German people-search site waatp.de.</p></div>
<p>A search on the email address dmitrcox@gmail.com suggests Mr. Shelest was previously involved in rather aggressive email marketing campaigns. In 2010, an anonymous source <a href="https://krebsonsecurity.com/2010/09/spam-affialite-program-spamit-com-to-close/" target="_blank" rel="noopener">leaked to KrebsOnSecurity</a> the financial and organizational records of <a href="https://krebsonsecurity.com/tag/spamit/" target="_blank" rel="noopener"><strong>Spamit</strong></a>, which at the time was easily the largest Russian-language pharmacy spam affiliate program in the world.</p>
<p>Spamit paid spammers a hefty commission every time someone bought male enhancement drugs from any of their spam-advertised websites. Mr. Shelest’s email address stood out because immediately after the Spamit database was leaked, KrebsOnSecurity searched all of the Spamit affiliate email addresses to determine if any of them corresponded to social media accounts at <strong>Facebook.com</strong> (at the time, Facebook allowed users to search profiles by email address).</p>
<p>That mapping, which was done mainly by generous graduate students at my alma mater <strong>George Mason University</strong>, revealed that dmitrcox@gmail.com was used by a Spamit affiliate, albeit not a very profitable one. That same <a href="https://www.facebook.com/dmitry.shelest" target="_blank" rel="noopener">Facebook profile for Mr. Shelest</a> is still active, and it says he is married and living in Minsk (last update: 2021).</p>
<div id="attachment_66757"><p><a href="https://krebsonsecurity.com/wp-content/uploads/2024/03/peeepl.it_.png" target="_blank" rel="noopener"><img aria-describedby="caption-attachment-66757" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/03/peeepl.it_.png" alt="" width="750" height="345" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/03/peeepl.it_.png 1880w, https://krebsonsecurity.com/wp-content/uploads/2024/03/peeepl.it_-768x353.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/03/peeepl.it_-1536x707.png 1536w, https://krebsonsecurity.com/wp-content/uploads/2024/03/peeepl.it_-782x360.png 782w" sizes="(max-width: 750px) 100vw, 750px"></a></p><p id="caption-attachment-66757">The Italian people-search website peeepl.it.</p></div>
<p>Scrolling down Mr. Shelest’s Facebook page to posts made more than ten years ago show him liking the Facebook profile pages for a large number of other people-search sites, including <a href="https://web.archive.org/web/20120215164504/http://findita.com/" target="_blank" rel="noopener">findita.com</a>, <a href="https://web.archive.org/web/20120611000147/http://findmedo.com/" target="_blank" rel="noopener">findmedo.com</a>, <a href="https://web.archive.org/web/20120202191444/http://folkscan.com/" target="_blank" rel="noopener">folkscan.com</a>, <a href="https://web.archive.org/web/20120204021306/http://huntize.com/" target="_blank" rel="noopener">huntize.com</a>, <a href="https://web.archive.org/web/20120201003010/http://ifindy.com/" target="_blank" rel="noopener">ifindy.com</a>, <a href="https://web.archive.org/web/20130209191213/http://jupery.com/" target="_blank" rel="noopener">jupery.com</a>, <a href="https://web.archive.org/web/20120214040919/http://look2man.com/" target="_blank" rel="noopener">look2man.com</a>, <a href="https://web.archive.org/web/20130318231852/http://lookerun.com/" target="_blank" rel="noopener">lookerun.com</a>, <a href="https://web.archive.org/web/20120217181822/http://manyp.com/" target="_blank" rel="noopener">manyp.com</a>, peepull.com, <a href="https://web.archive.org/web/20130425195446/http://perserch.com/" target="_blank" rel="noopener">perserch.com</a>, <a href="https://web.archive.org/web/20140104023637/http://persuer.com/" target="_blank" rel="noopener">persuer.com</a>, <a href="https://web.archive.org/web/20121001072151/http://pervent.com/" target="_blank" rel="noopener">pervent.com</a>, <a href="https://web.archive.org/web/20130116123555/http://piplenter.com/" target="_blank" rel="noopener">piplenter.com</a>, <a href="https://web.archive.org/web/20120215174246/http://piplfind.com/" target="_blank" rel="noopener">piplfind.com</a>, <a href="https://web.archive.org/web/20130302034909/http://piplscan.com/" target="_blank" rel="noopener">piplscan.com</a>, <a href="https://web.archive.org/web/20110210162153/http://popopke.com/" target="_blank" rel="noopener">popopke.com</a>, <a href="https://web.archive.org/web/20120210030201/http://pplsorce.com/" target="_blank" rel="noopener">pplsorce.com</a>, <a href="https://web.archive.org/web/20130215180627/http://qimeo.com/" target="_blank" rel="noopener">qimeo.com</a>, <a href="https://web.archive.org/web/20120125085744/http://scoutu2.com/" target="_blank" rel="noopener">scoutu2.com</a>, <a href="https://web.archive.org/web/20120215092423/http://search64.com/" target="_blank" rel="noopener">search64.com</a>, <a href="https://web.archive.org/web/20120204035144/http://searchay.com/" target="_blank" rel="noopener">searchay.com</a>, <a href="https://web.archive.org/web/20120215082510/http://seekmi.com/" target="_blank" rel="noopener">seekmi.com</a>, <a href="https://web.archive.org/web/20120502175915/http://selfabc.com/" target="_blank" rel="noopener">selfabc.com</a>, <a href="https://web.archive.org/web/20120211043052/http://socsee.com/" target="_blank" rel="noopener">socsee.com</a>, <a href="https://web.archive.org/web/20130116150616/http://srching.com/" target="_blank" rel="noopener">srching.com</a>, <a href="https://web.archive.org/web/20120204100134/http://toolooks.com/" target="_blank" rel="noopener">toolooks.com</a>, <a href="https://web.archive.org/web/20120706142742/http://upearch.com/" target="_blank" rel="noopener">upearch.com</a>, <a href="https://web.archive.org/web/20120306122418/http://webmeek.com/" target="_blank" rel="noopener">webmeek.com</a>, and many country-code variations of <a href="https://web.archive.org/web/20131103141811/http://viadin.ca/" target="_blank" rel="noopener">viadin.ca</a> (e.g. <a href="https://web.archive.org/web/20130427065532/http://viadin.hk/" target="_blank" rel="noopener">viadin.hk</a>, <a href="https://web.archive.org/web/20130805064112/http://viadin.com/" target="_blank" rel="noopener">viadin.com</a> and <a href="https://web.archive.org/web/20130428035516/http://viadin.de/" target="_blank" rel="noopener">viadin.de</a>).</p>
<div id="attachment_66758"><p><a href="https://krebsonsecurity.com/wp-content/uploads/2024/03/popoke.png" target="_blank" rel="noopener"><img aria-describedby="caption-attachment-66758" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/03/popoke.png" alt="" width="750" height="345" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/03/popoke.png 1880w, https://krebsonsecurity.com/wp-content/uploads/2024/03/popoke-768x353.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/03/popoke-1536x707.png 1536w, https://krebsonsecurity.com/wp-content/uploads/2024/03/popoke-782x360.png 782w" sizes="(max-width: 750px) 100vw, 750px"></a></p><p id="caption-attachment-66758">The people-search website popopke.com.</p></div>
<p>Domaintools.com finds that all of the domains mentioned in the last paragraph were registered to the email address dmitrcox@gmail.com.</p>
<p>Mr. Shelest has not responded to multiple requests for comment. KrebsOnSecurity also sought comment from onerep.com, which likewise has not responded to inquiries about its founder’s many apparent conflicts of interest. In any event, these practices would seem to contradict the goal Onerep has stated on its site: “We believe that no one should compromise personal online security and get a profit from it.”</p>
<div id="attachment_66759"><p><a href="https://krebsonsecurity.com/wp-content/uploads/2024/03/findmedo.png" target="_blank" rel="noopener"><img aria-describedby="caption-attachment-66759" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/03/findmedo.png" alt="" width="748" height="319" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/03/findmedo.png 1880w, https://krebsonsecurity.com/wp-content/uploads/2024/03/findmedo-768x327.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/03/findmedo-1536x654.png 1536w, https://krebsonsecurity.com/wp-content/uploads/2024/03/findmedo-782x333.png 782w" sizes="(max-width: 748px) 100vw, 748px"></a></p><p id="caption-attachment-66759">The people-search website findmedo.com.</p></div>
<p><strong>Max Anderson</strong> is chief growth officer at <a href="https://www.360privacy.io/" rel="noopener" target="_blank">360 Privacy</a>, a legitimate privacy company that works to keep its clients’ data off of more than 400 data broker and people-search sites. Anderson said it is concerning to see a direct link between between a data removal service and data broker websites. </p>
<p>“I would consider it unethical to run a company that sells people’s information, and then charge those same people to have their information removed,” Anderson said. </p>
<p>Last week, KrebsOnSecurity published <a href="https://krebsonsecurity.com/2024/03/a-close-up-look-at-the-consumer-data-broker-radaris/" target="_blank" rel="noopener">an analysis of the people-search data broker giant <strong>Radaris</strong></a>, whose consumer profiles are deep enough to rival those of far more guarded data broker resources available to U.S. police departments and other law enforcement personnel.</p>
<p>That story revealed that the co-founders of Radaris are two native Russian brothers who operate multiple Russian-language dating services and affiliate programs. It also appears many of the Radaris founders’ businesses have ties to a California marketing firm that works with a Russian state-run media conglomerate currently sanctioned by the U.S. government.</p>
<p><img decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/03/search64.png" alt="" width="750" height="345" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/03/search64.png 1880w, https://krebsonsecurity.com/wp-content/uploads/2024/03/search64-768x353.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/03/search64-1536x707.png 1536w, https://krebsonsecurity.com/wp-content/uploads/2024/03/search64-782x360.png 782w" sizes="(max-width: 750px) 100vw, 750px"></p>
<p>KrebsOnSecurity will continue investigating the history of various consumer data brokers and people-search providers. If any readers have inside knowledge of this industry or key players within it, please consider reaching out to krebsonsecurity at gmail.com.</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FCC Officially Raises Minimum Broadband Metric from 25Mbps to 100Mbps (464 pts)]]></title>
            <link>https://www.pcmag.com/news/fcc-officially-raises-minimum-broadband-metric-from-25mbps-to-100mbps</link>
            <guid>39708957</guid>
            <pubDate>Thu, 14 Mar 2024 21:04:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pcmag.com/news/fcc-officially-raises-minimum-broadband-metric-from-25mbps-to-100mbps">https://www.pcmag.com/news/fcc-officially-raises-minimum-broadband-metric-from-25mbps-to-100mbps</a>, See on <a href="https://news.ycombinator.com/item?id=39708957">Hacker News</a></p>
Couldn't get https://www.pcmag.com/news/fcc-officially-raises-minimum-broadband-metric-from-25mbps-to-100mbps: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[More powerful Go execution traces (276 pts)]]></title>
            <link>https://go.dev/blog/execution-traces-2024</link>
            <guid>39708591</guid>
            <pubDate>Thu, 14 Mar 2024 20:30:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://go.dev/blog/execution-traces-2024">https://go.dev/blog/execution-traces-2024</a>, See on <a href="https://news.ycombinator.com/item?id=39708591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-slug="/blog/execution-traces-2024">
    
    <h2><a href="https://go.dev/blog/">The Go Blog</a></h2>
    

    
      
      
      
      <p>The <a href="https://go.dev/pkg/runtime/trace">runtime/trace</a> package contains a powerful tool for understanding and
troubleshooting Go programs.
The functionality within allows one to produce a trace of each goroutine’s execution over some
time period.
With the <a href="https://go.dev/pkg/cmd/trace"><code>go tool trace</code> command</a> (or the excellent open source
<a href="https://gotraceui.dev/" rel="noreferrer" target="_blank">gotraceui tool</a>), one may then visualize and explore the data within these
traces.</p>
<p>The magic of a trace is that it can easily reveal things about a program that are hard to see in
other ways.
For example, a concurrency bottleneck where lots of goroutines block on the same channel might be
quite difficult to see in a CPU profile, because there’s no execution to sample.
But in an execution trace, the <em>lack</em> of execution will show up with amazing clarity, and the stack
traces of blocked goroutines will quickly point at the culprit.</p>
<p><img src="https://go.dev/blog/execution-traces-2024/gotooltrace.png" alt="">
</p>
<p>Go developers are even able to instrument their own programs with <a href="https://go.dev/pkg/runtime/trace#Task">tasks</a>,
<a href="https://go.dev/pkg/runtime/trace#WithRegion">regions</a>, and <a href="https://go.dev/pkg/runtime/trace#Log">logs</a> that
they can use to correlate their higher-level concerns with lower-level execution details.</p>
<h2 id="issues">Issues</h2>
<p>Unfortunately, the wealth of information in execution traces can often be out of reach.
Four big issues with traces have historically gotten in the way.</p>
<ul>
<li>Traces had high overheads.</li>
<li>Traces didn’t scale well, and could become too big to analyze.</li>
<li>It was often unclear when to start tracing to capture a specific bad behavior.</li>
<li>Only the most adventurous gophers could programmatically analyze traces, given the lack of a
public package for parsing and interpreting execution traces.</li>
</ul>
<p>If you’ve used traces in the last few years, you’ve likely been frustrated by one or more of these
problems.
But we’re excited to share that over the last two Go releases we’ve made big progress in all four
of these areas.</p>
<h2 id="low-overhead-tracing">Low-overhead tracing</h2>
<p>Prior to Go 1.21, the run-time overhead of tracing was somewhere between 10–20% CPU for many
applications, which limits tracing to situational usage, rather than continuous usage like CPU
profiling.
It turned out that much of the cost of tracing came down to tracebacks.
Many events produced by the runtime have stack traces attached, which are invaluable to actually
identifying what goroutines where doing at key moments in their execution.</p>
<p>Thanks to work by Felix Geisendörfer and Nick Ripley on optimizing the efficiency of tracebacks,
the run-time CPU overhead of execution traces has been cut dramatically, down to 1–2% for many
applications.
You can read more about the work done here in <a href="https://blog.felixge.de/reducing-gos-execution-tracer-overhead-with-frame-pointer-unwinding/" rel="noreferrer" target="_blank">Felix’s great blog
post</a>
on the topic.</p>
<h2 id="scalable-traces">Scalable traces</h2>
<p>The trace format and its events were designed around relatively efficient emission, but required
tooling to parse and keep around the state of the entirety of a trace.
A few hundred MiB trace could require several GiB of RAM to analyze!</p>
<p>This issue is unfortunately fundamental to how traces are generated.
To keep run-time overheads low, all events are written to the equivalent of thread-local buffers.
But this means events appear out of their true order, and the burden is placed on the trace
tooling to figure out what really happened.</p>
<p>The key insight to making traces scale while keeping overheads low was to occasionally split the
trace being generated.
Each split point would behave a bit like simultaneously disabling and reenabling tracing in one
go.
All the trace data so far would represent a complete and self-contained trace, while the new trace
data would seamlessly pick up from where it left off.</p>
<p>As you might imagine, fixing this required <a href="https://go.dev/issue/60773">rethinking and rewriting a lot of the foundation of
the trace implementation</a> in the runtime.
We’re happy to say that the work landed in Go 1.22 and is now generally available.
<a href="https://go.dev/doc/go1.22#runtime/trace">A lot of nice improvements</a> came with the rewrite, including some
improvements to the <a href="https://go.dev/doc/go1.22#trace"><code>go tool trace</code> command</a> as well.
The gritty details are all in the <a href="https://github.com/golang/proposal/blob/master/design/60773-execution-tracer-overhaul.md" rel="noreferrer" target="_blank">design
document</a>,
if you’re curious.</p>
<p>(Note: <code>go tool trace</code> still loads the full trace into memory, but <a href="https://go.dev/issue/65315">removing this
limitation</a> for traces produced by Go 1.22+ programs is now feasible.)</p>
<h2 id="flight-recording">Flight recording</h2>
<p>Suppose you work on a web service and an RPC took a very long time.
You couldn’t start tracing at the point you knew the RPC was already taking a while, because the
root cause of the slow request already happened and wasn’t recorded.</p>
<p>There’s a technique that can help with this called flight recording, which you may already be
familiar with from other programming environments.
The insight with flight recording is to have tracing on continuously and always keep the most
recent trace data around, just in case.
Then, once something interesting happens, the program can just write out whatever it has!</p>
<p>Before traces could be split, this was pretty much a non-starter.
But because continuous tracing is now viable thanks to low overheads, and the fact that the runtime
can now split traces any time it needs, it turns out it was straightforward to implement flight
recording.</p>
<p>As a result, we’re happy to announce a flight recorder experiment, available in the
<a href="https://go.dev/pkg/golang.org/x/exp/trace#FlightRecorder">golang.org/x/exp/trace package</a>.</p>
<p>Please try it out!
Below is an example that sets up flight recording to capture a long HTTP request to get you started.</p>
<div>
<pre>    
    fr := trace.NewFlightRecorder()
    fr.Start()

    
    var once sync.Once
    http.HandleFunc("/my-endpoint", func(w http.ResponseWriter, r *http.Request) {
        start := time.Now()

        
        doWork(w, r)

        
        if time.Since(start) &gt; 300*time.Millisecond {
            
            once.Do(func() {
                
                var b bytes.Buffer
                _, err = fr.WriteTo(&amp;b)
                if err != nil {
                    log.Print(err)
                    return
                }
                
                if err := os.WriteFile("trace.out", b.Bytes(), 0o755); err != nil {
                    log.Print(err)
                    return
                }
            })
        }
    })
    log.Fatal(http.ListenAndServe(":8080", nil))
</pre>
</div>
<p>If you have any feedback, positive or negative, please share it to the <a href="https://go.dev/issue/63185">proposal
issue</a>!</p>
<h2 id="trace-reader-api">Trace reader API</h2>
<p>Along with the trace implementation rewrite came an effort to clean up the other trace internals,
like <code>go tool trace</code>.
This spawned an attempt to create a trace reader API that was good enough to share and that could
make traces more accessible.</p>
<p>Just like the flight recorder, we’re happy to announce that we also have an experimental trace reader
API that we’d like to share.
It’s available in the <a href="https://go.dev/pkg/golang.org/x/exp/trace#Reader">same package as the flight recorder,
golang.org/x/exp/trace</a>.</p>
<p>We think it’s good enough to start building things on top of, so please try it out!
Below is an example that measures the proportion of goroutine block events that blocked to wait on
the network.</p>
<div>
<pre>    
    r, err := trace.NewReader(os.Stdin)
    if err != nil {
        log.Fatal(err)
    }

    var blocked int
    var blockedOnNetwork int
    for {
        
        ev, err := r.ReadEvent()
        if err == io.EOF {
            break
        } else if err != nil {
            log.Fatal(err)
        }

        
        if ev.Kind() == trace.EventStateTransition {
            st := ev.StateTransition()
            if st.Resource.Kind == trace.ResourceGoroutine {
                id := st.Resource.Goroutine()
                from, to := st.GoroutineTransition()

                
                if from.Executing() &amp;&amp; to == trace.GoWaiting {
                    blocked++
                    if strings.Contains(st.Reason, "network") {
                        blockedOnNetwork++
                    }
                }
            }
        }
    }
    
    p := 100 * float64(blockedOnNetwork) / float64(blocked)
    fmt.Printf("%2.3f%% instances of goroutines blocking were to block on the network\n", p)
</pre>
</div>
<p>And just like the flight recorder, there’s a <a href="https://go.dev/issue/62627">proposal issue</a> that would
be a great place to leave feedback!</p>
<p>We’d like to quickly call out Dominik Honnef as someone who tried it out early, provided great
feedback, and has contributed support for older trace versions to the API.</p>
<h2 id="thank-you">Thank you!</h2>
<p>This work was completed, in no small part, thanks to the help of the those in the <a href="https://go.dev/issue/57175">diagnostics
working group</a>, started over a year ago as a collaboration between stakeholders from
across the Go community, and open to the public.</p>
<p>We’d like to take a moment to thank those community members who have attended the diagnostic
meetings regularly over the last year: Felix Geisendörfer, Nick Ripley, Rhys Hiltner, Dominik
Honnef, Bryan Boreham, thepudds.</p>
<p>The discussions, feedback, and work you all put in have been instrumental to getting us to where we
are today.
Thank you!</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pornhub Blocked in Texas (103 pts)]]></title>
            <link>https://variety.com/2024/digital/news/pornhub-texas-blocked-age-verification-law-1235942280/</link>
            <guid>39708347</guid>
            <pubDate>Thu, 14 Mar 2024 20:03:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://variety.com/2024/digital/news/pornhub-texas-blocked-age-verification-law-1235942280/">https://variety.com/2024/digital/news/pornhub-texas-blocked-age-verification-law-1235942280/</a>, See on <a href="https://news.ycombinator.com/item?id=39708347">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
	<a href="https://variety.com/t/pornhub/" id="auto-tag_pornhub" data-tag="pornhub">Pornhub</a> and other affiliated adult websites have blocked access to users in Texas, amid a legal battle with the Lone Star State’s attorney general over an age-verification law.</p>



<p>
	Last week, a federal appeals court <a href="https://www.ca5.uscourts.gov/opinions/pub/23/23-50627-CV0.pdf" target="_blank" rel="noreferrer noopener nofollow">upheld a Texas law</a> requiring pornography sites to institute age-verification measures to ensure only adults 18 and older are able to access them, while it also struck down a part of the law requiring porn sites to <a href="https://variety.com/2023/digital/news/pornhubs-texas-age-verification-law-violates-first-amendment-ruling-1235709902/">display “health warnings”</a> about the content. That came after a previous federal judge’s ruling that <a href="https://variety.com/2023/digital/news/pornhubs-texas-age-verification-law-violates-first-amendment-ruling-1235709902/">the Texas law violated the U.S. Constitution’s&nbsp;First Amendment</a>&nbsp;prohibition against free-speech restrictions. The Texas Attorney General’s Office immediately appealed that decision.<a rel="noreferrer noopener nofollow" href="https://www.kargo.com/privacy" target="_blank"></a>

	</p>




<p>
	A new message displayed Thursday to users with internet addresses in Texas on Pornhub (and other sites operated by parent company <a href="https://variety.com/t/aylo/" id="auto-tag_aylo" data-tag="aylo">Aylo</a>) explained that it was disabling access to comply with the law, as first <a href="https://www.houstonchronicle.com/politics/texas/article/pornhub-blocked-texas-age-verification-19021482.php" target="_blank" rel="noreferrer noopener nofollow">reported</a> by the Houston Chronicle.</p>



<p>
	“As you may know, your elected officials in Texas are requiring us to verify your age before allowing you access to our website. Not only does this impinge on the rights of adults to access protected speech, it fails strict scrutiny by employing the least effective and yet also most restrictive means of accomplishing Texas’s stated purpose of allegedly protecting minors,” the message reads in part.

</p>



<p>
	The Pornhub sites’ message continued, “Until the real solution is offered, we have made the difficult decision to completely disable access to our website in Texas. In doing so, we are complying with the law, as we always do, but hope that governments around the world will implement laws that actually protect the safety and security of users.”</p>



<p>
	According to the message, “While safety and compliance are at the forefront of our mission, providing identification every time you want to visit an adult platform is not an effective solution for protecting users online, and in fact, will put minors and your privacy at risk.”</p>



<p>
	Pornhub called the Texas age-verification “ineffective, haphazard and dangerous” and asserted that it will drive users “from those few websites which comply, to the thousands of websites, with far fewer safety measures in place, which do not comply. Very few sites are able to compare to the robust Trust and Safety measures we currently have in place. To protect minors and user privacy, any legislation must be enforced against all platforms offering adult content.”</p>



<p>
	The Texas law was signed by Republican Gov. Greg Abbott in June 2023. The legislation, Texas H.B. 11811, was scheduled to go into effect on Sept. 1, but it was on hold after the lawsuit filed by the Free Speech Coalition (a group that includes Pornhub’s parent company) resulted in a preliminary injunction staying its enforcement. The law applies to online publishers whose content is more than one-third “sexual material harmful to minors” and requires them to verify the age of all visitors using a government-issued ID or “public or private transactional data.”

	</p>




<p>
	Reached for comment, the company provided a statement from Alex Kekesi, VP of brand and community at Aylo, which said in part: “This is not the end. We are reviewing options and consulting with our legal team… We will continue to fight for our industry and the performers that legally earn a living, and we will continue to appeal through all available judicial recourse to recognize that this law is unconstitutional.”</p>



<p>
	Keksi also said in the statement that Aylo “has publicly supported age verification of users for years, but we believe that any law to this effect must ensure minors do not access content intended for adults and preserve user safety and privacy. We believe that the real solution for protecting minors and adults alike is to verify users’ ages at the point of access — the users’ devices — and to deny or permit access to age-restricted materials and websites based on that verification.”</p>



<p>
	Pornhub and the company’s network of other sites are also blocked or restricted in at least seven other U.S. states — Arkansas, Louisiana, Mississippi, Montana, North Carolina, Virginia and Utah — which have adopted similar laws.</p>



<p>
	<a href="https://variety.com/2023/digital/news/pornhub-parent-name-change-aylo-adult-entertainment-1235700312/">Aylo is owned by Canadian private-equity firm Ethical Capital Partners</a>, which acquired Pornhub’s predecessor company MindGeek for undisclosed financial terms last year. ECP has said it would focus on building the company’s “trust and safety” and to make it “the internet leader in fighting illegal online content.”</p>



	<h3>
		<a href="https://variety.com/vip/playstation-state-of-play-advantage-over-xbox-kojima-1235893959/"></a>	</h3>
















</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Mythical Non-Roboticist (151 pts)]]></title>
            <link>https://generalrobots.substack.com/p/the-mythical-non-roboticist</link>
            <guid>39707356</guid>
            <pubDate>Thu, 14 Mar 2024 18:29:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://generalrobots.substack.com/p/the-mythical-non-roboticist">https://generalrobots.substack.com/p/the-mythical-non-roboticist</a>, See on <a href="https://news.ycombinator.com/item?id=39707356">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>I worked on this idea for months before I decided it was a mistake. The second time I heard someone mention it, I thought, “That's strange, these two groups had the same idea. Maybe I should tell them it didn’t work for us.” The third and fourth time I rolled my eyes and ignored it. The fifth time I heard about a group struggling with this mistake I decided it was worth a blog post all on its own. I call this idea “The Mythical Non-Roboticist”.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facb94560-1afd-4ad2-aa06-aa4f7442662f_615x512.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facb94560-1afd-4ad2-aa06-aa4f7442662f_615x512.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facb94560-1afd-4ad2-aa06-aa4f7442662f_615x512.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facb94560-1afd-4ad2-aa06-aa4f7442662f_615x512.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facb94560-1afd-4ad2-aa06-aa4f7442662f_615x512.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facb94560-1afd-4ad2-aa06-aa4f7442662f_615x512.jpeg" width="391" height="325.51544715447153" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/acb94560-1afd-4ad2-aa06-aa4f7442662f_615x512.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:512,&quot;width&quot;:615,&quot;resizeWidth&quot;:391,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facb94560-1afd-4ad2-aa06-aa4f7442662f_615x512.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facb94560-1afd-4ad2-aa06-aa4f7442662f_615x512.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facb94560-1afd-4ad2-aa06-aa4f7442662f_615x512.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facb94560-1afd-4ad2-aa06-aa4f7442662f_615x512.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Unicorn, Non-Roboticist, Satyr</figcaption></figure></div><p><span>The idea goes something like this: Programming robots is hard. And there are some people with really arcane skills and PhDs who are really expensive and seem to be required for some reason. Wouldn’t it be nice if we could do robotics without them?</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-142615069" href="https://generalrobots.substack.com/p/the-mythical-non-roboticist#footnote-1-142615069" target="_self" rel="">1</a></span><span> What if everyone could do robotics? That would be great, right? We should make a software framework so that non-roboticists can program robots.</span></p><p><span>This idea is so close to a correct idea that it's hard to tell why it doesn’t work out. On the surface, it's not </span><em>wrong</em><span>: all else being equal, it would be good if programming robots was more accessible. The problem is that we don’t have a good recipe for making working robots. So we don’t know how to make that recipe easier to follow. In order to make things simple, people end up removing things that folks might need, because no one knows for sure what's absolutely required. It's like saying you want to invent an invisibility cloak and want to be able to make it from materials you can buy from Home Depot. Sure, that would be nice, but if you invented an invisibility cloak that required some mercury and neodymium to manufacture would you toss the recipe?</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3962fbc5-f4f5-4a8d-b201-f6ca334e5863_578x512.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3962fbc5-f4f5-4a8d-b201-f6ca334e5863_578x512.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3962fbc5-f4f5-4a8d-b201-f6ca334e5863_578x512.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3962fbc5-f4f5-4a8d-b201-f6ca334e5863_578x512.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3962fbc5-f4f5-4a8d-b201-f6ca334e5863_578x512.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3962fbc5-f4f5-4a8d-b201-f6ca334e5863_578x512.jpeg" width="302" height="267.51557093425606" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3962fbc5-f4f5-4a8d-b201-f6ca334e5863_578x512.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:512,&quot;width&quot;:578,&quot;resizeWidth&quot;:302,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3962fbc5-f4f5-4a8d-b201-f6ca334e5863_578x512.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3962fbc5-f4f5-4a8d-b201-f6ca334e5863_578x512.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3962fbc5-f4f5-4a8d-b201-f6ca334e5863_578x512.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3962fbc5-f4f5-4a8d-b201-f6ca334e5863_578x512.jpeg 1456w" sizes="100vw"></picture></div></a><figcaption>I actually only use free-trade, organic, non-GMO invisibility cloaks</figcaption></figure></div><p><span>In robotics, this mistake is based on a very true and very real observation: programming robots </span><em><strong>is</strong></em><span> super hard. Famously hard. It would be super-great if programming robots was easier. The issue is this: programming robots has two different kinds of hard parts.&nbsp;</span></p><p><span>The first kind of hard part is that robots deal with the real-world, imperfectly sensed and imperfectly actuated. Global mutable state is bad programming style because it's really hard to deal with, but to robot software the entire physical world is global mutable state, and you only get to unreliably observe it and hope your actions approximate what you wanted to achieve. Getting robotics to work at all is often at the very limit of what a person can reason about, and requires the flexibility to employ whatever heuristic might work for your special problem. This is the </span><strong>intrinsic</strong><span> complexity of the problem: robots live in complex worlds, and for every working solution there are millions of solutions that don’t work, and finding the right one is hard, and often very dependent on the task, robot, sensors and environment.</span></p><p><span>Folks look at that challenge, see that it is super hard, and decide that, sure, maybe some fancy roboticist could solve it in one particular scenario, but what about “normal” people? “We should make this possible for non-roboticists” they say. I call these users “Mythical Non-Roboticists” because once they are programming a robot, I feel they </span><em>become</em><span> roboticists. Isn’t anyone programming a robot for a purpose a roboticist? Stop gatekeeping, people.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F139ede5f-1150-4aeb-b6f6-54afc74cf172_882x512.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F139ede5f-1150-4aeb-b6f6-54afc74cf172_882x512.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F139ede5f-1150-4aeb-b6f6-54afc74cf172_882x512.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F139ede5f-1150-4aeb-b6f6-54afc74cf172_882x512.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F139ede5f-1150-4aeb-b6f6-54afc74cf172_882x512.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F139ede5f-1150-4aeb-b6f6-54afc74cf172_882x512.jpeg" width="526" height="305.3424036281179" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/139ede5f-1150-4aeb-b6f6-54afc74cf172_882x512.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:512,&quot;width&quot;:882,&quot;resizeWidth&quot;:526,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F139ede5f-1150-4aeb-b6f6-54afc74cf172_882x512.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F139ede5f-1150-4aeb-b6f6-54afc74cf172_882x512.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F139ede5f-1150-4aeb-b6f6-54afc74cf172_882x512.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F139ede5f-1150-4aeb-b6f6-54afc74cf172_882x512.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>“Why are those people programming robots?” “Not sure, but I’m positive they aren’t roboticists. I checked”</figcaption></figure></div><p>I call also them ‘Mythical’ because usually the “non-roboticist” implied is a vague, amorphous group. Don’t design for amorphous groups. If you can’t name three real people (that you have talked to), that your API is for, then you are designing for an amorphous group and only amorphous people will like your API.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa63f62f-1857-4729-a91a-edc603c28d70_512x711.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa63f62f-1857-4729-a91a-edc603c28d70_512x711.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa63f62f-1857-4729-a91a-edc603c28d70_512x711.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa63f62f-1857-4729-a91a-edc603c28d70_512x711.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa63f62f-1857-4729-a91a-edc603c28d70_512x711.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa63f62f-1857-4729-a91a-edc603c28d70_512x711.jpeg" width="302" height="419.37890625" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aa63f62f-1857-4729-a91a-edc603c28d70_512x711.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:711,&quot;width&quot;:512,&quot;resizeWidth&quot;:302,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa63f62f-1857-4729-a91a-edc603c28d70_512x711.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa63f62f-1857-4729-a91a-edc603c28d70_512x711.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa63f62f-1857-4729-a91a-edc603c28d70_512x711.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa63f62f-1857-4729-a91a-edc603c28d70_512x711.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>“ActivateBehaviorObjectFactory()? Oooh, I like it”</figcaption></figure></div><p>And with this hazy group of users in mind (and seeing how difficult everything is) folks think, “Surely we could make this easier for everyone else by papering over these things with simple APIs?”</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd341001f-2eda-421d-844e-3d4c199fcb8c_512x565.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd341001f-2eda-421d-844e-3d4c199fcb8c_512x565.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd341001f-2eda-421d-844e-3d4c199fcb8c_512x565.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd341001f-2eda-421d-844e-3d4c199fcb8c_512x565.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd341001f-2eda-421d-844e-3d4c199fcb8c_512x565.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd341001f-2eda-421d-844e-3d4c199fcb8c_512x565.png" width="304" height="335.46875" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d341001f-2eda-421d-844e-3d4c199fcb8c_512x565.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:565,&quot;width&quot;:512,&quot;resizeWidth&quot;:304,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd341001f-2eda-421d-844e-3d4c199fcb8c_512x565.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd341001f-2eda-421d-844e-3d4c199fcb8c_512x565.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd341001f-2eda-421d-844e-3d4c199fcb8c_512x565.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd341001f-2eda-421d-844e-3d4c199fcb8c_512x565.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>No. No you can’t. Stop it.&nbsp;</p><p><span>You can’t paper over intrinsic complexity with simple APIs because </span><strong>if your APIs are simple they can’t cover the complexity of the problem</strong><span>. You will inevitably end up with a beautiful looking API, with calls like “grasp_object” and “approach_person” which demo nicely in a hackathon kickoff but last about 15 minutes of someone actually trying to get some work done. It will turn out that, for their particular application, “grasp_object()” makes 3-4 wrong assumptions about “grasp” </span><em><strong>and</strong></em><span> “object” and doesn’t work for them at all.</span></p><p><span>This is made worse by the pervasive assumption that these people are less savvy (read: less intelligent) than the creators of this magical framework.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-142615069" href="https://generalrobots.substack.com/p/the-mythical-non-roboticist#footnote-2-142615069" target="_self" rel="">2</a></span><span> That feeling of superiority will cause the designers to cling desperately to their beautiful, simple ‘grasp_object()’s and resist adding the knobs and arguments needed to cover more use cases and allow the users to customize what they get.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F617a56d5-13c6-4818-ae70-de6fa5ecf0ab_512x517.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F617a56d5-13c6-4818-ae70-de6fa5ecf0ab_512x517.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F617a56d5-13c6-4818-ae70-de6fa5ecf0ab_512x517.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F617a56d5-13c6-4818-ae70-de6fa5ecf0ab_512x517.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F617a56d5-13c6-4818-ae70-de6fa5ecf0ab_512x517.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F617a56d5-13c6-4818-ae70-de6fa5ecf0ab_512x517.jpeg" width="512" height="517" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/617a56d5-13c6-4818-ae70-de6fa5ecf0ab_512x517.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:517,&quot;width&quot;:512,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F617a56d5-13c6-4818-ae70-de6fa5ecf0ab_512x517.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F617a56d5-13c6-4818-ae70-de6fa5ecf0ab_512x517.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F617a56d5-13c6-4818-ae70-de6fa5ecf0ab_512x517.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F617a56d5-13c6-4818-ae70-de6fa5ecf0ab_512x517.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>“Should let folks control how fast the robot moves?” “I don’t think so… Maybe we give them “slow” and “fast”. Anything else would be too confusing for a non-roboticist"</figcaption></figure></div><p>Ironically this foists a bunch of complexity on to the poor users of the API who have to come up with clever workarounds to get it to work at all.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c5c11a-7544-4a99-8abc-62eb3d2c9bbc_891x512.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c5c11a-7544-4a99-8abc-62eb3d2c9bbc_891x512.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c5c11a-7544-4a99-8abc-62eb3d2c9bbc_891x512.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c5c11a-7544-4a99-8abc-62eb3d2c9bbc_891x512.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c5c11a-7544-4a99-8abc-62eb3d2c9bbc_891x512.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c5c11a-7544-4a99-8abc-62eb3d2c9bbc_891x512.jpeg" width="891" height="512" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/38c5c11a-7544-4a99-8abc-62eb3d2c9bbc_891x512.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:512,&quot;width&quot;:891,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c5c11a-7544-4a99-8abc-62eb3d2c9bbc_891x512.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c5c11a-7544-4a99-8abc-62eb3d2c9bbc_891x512.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c5c11a-7544-4a99-8abc-62eb3d2c9bbc_891x512.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38c5c11a-7544-4a99-8abc-62eb3d2c9bbc_891x512.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>“So I got the arm to match the conveyor speed by monitoring the position and pre-empting the motion command, alternating ‘slow’ and ‘fast’ at 10Hz with a duty cycle depending on how we are tracking our target. The motion is pretty jerky but it works.”</figcaption></figure></div><p><span>The sad, salty, bitter icing on this cake-of-frustration is that, even if done really well, the goal of this kind of framework would be to expand the group of people who can do the work. And to achieve that, it would sacrifice some performance you can only get by super-specializing your solution to your problem. If we lived in a world where expert roboticists could program robots that worked really well, but there was so much demand for robots that there just wasn’t enough time for those folks to do all the programming, this would be a great solution.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-142615069" href="https://generalrobots.substack.com/p/the-mythical-non-roboticist#footnote-3-142615069" target="_self" rel="">3</a></span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8cd7c8-e3e4-4fc4-892e-6b13ae1e0f8c_512x910.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8cd7c8-e3e4-4fc4-892e-6b13ae1e0f8c_512x910.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8cd7c8-e3e4-4fc4-892e-6b13ae1e0f8c_512x910.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8cd7c8-e3e4-4fc4-892e-6b13ae1e0f8c_512x910.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8cd7c8-e3e4-4fc4-892e-6b13ae1e0f8c_512x910.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8cd7c8-e3e4-4fc4-892e-6b13ae1e0f8c_512x910.jpeg" width="252" height="447.890625" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ef8cd7c8-e3e4-4fc4-892e-6b13ae1e0f8c_512x910.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:910,&quot;width&quot;:512,&quot;resizeWidth&quot;:252,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8cd7c8-e3e4-4fc4-892e-6b13ae1e0f8c_512x910.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8cd7c8-e3e4-4fc4-892e-6b13ae1e0f8c_512x910.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8cd7c8-e3e4-4fc4-892e-6b13ae1e0f8c_512x910.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8cd7c8-e3e4-4fc4-892e-6b13ae1e0f8c_512x910.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>“Man, </span><em>another</em><span> robotics startup going public? I wish I knew some roboticists. I’m just an expert in fintech, LLMs and e-commerce.”</span></figcaption></figure></div><p><span>The obvious truth is that (outside of really constrained environments like manufacturing cells) even the very best collection of real bone-fide, card-carrying roboticists working at the best of their ability struggle to get close to a level of performance that makes the robots commercially viable, even with long timelines and mountains of funding.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-142615069" href="https://generalrobots.substack.com/p/the-mythical-non-roboticist#footnote-4-142615069" target="_self" rel="">4</a></span><span> We don’t have </span><strong>any</strong><span> headroom to sacrifice power and effectiveness for ease.</span></p><p><span>So should we give up making it easier? Is robotic development available only to a small group of elites with fancy PhDs?</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-142615069" href="https://generalrobots.substack.com/p/the-mythical-non-roboticist#footnote-5-142615069" target="_self" rel="">5</a></span><span> No to both! I have worked with tons of undergrad interns who have been completely able to do robotics.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-6-142615069" href="https://generalrobots.substack.com/p/the-mythical-non-roboticist#footnote-6-142615069" target="_self" rel="">6</a></span><span> I myself am mostly self-taught in robot programming.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-7-142615069" href="https://generalrobots.substack.com/p/the-mythical-non-roboticist#footnote-7-142615069" target="_self" rel="">7</a></span><span> While there is a lot of intrinsic complexity in making robots work, I don’t think there is any more than, say, video game development.&nbsp;</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0884f5ee-d157-4f7a-a513-3ba066e2818e_766x512.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0884f5ee-d157-4f7a-a513-3ba066e2818e_766x512.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0884f5ee-d157-4f7a-a513-3ba066e2818e_766x512.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0884f5ee-d157-4f7a-a513-3ba066e2818e_766x512.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0884f5ee-d157-4f7a-a513-3ba066e2818e_766x512.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0884f5ee-d157-4f7a-a513-3ba066e2818e_766x512.jpeg" width="466" height="311.47780678851177" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0884f5ee-d157-4f7a-a513-3ba066e2818e_766x512.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:512,&quot;width&quot;:766,&quot;resizeWidth&quot;:466,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0884f5ee-d157-4f7a-a513-3ba066e2818e_766x512.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0884f5ee-d157-4f7a-a513-3ba066e2818e_766x512.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0884f5ee-d157-4f7a-a513-3ba066e2818e_766x512.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0884f5ee-d157-4f7a-a513-3ba066e2818e_766x512.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>“Yeah, you’ll never understand the challenges of reasoning in 3D space, worrying about latency all the time, and always being compute constrained.”</figcaption></figure></div><p>In robotics, like in all things, experience helps, some things are teachable, and as you master many areas you can see things start to connect together. These skills are not magical or unique to robotics. We are not as special as we like to think we are.</p><p><span>But what about making programming robots easier? Remember way back at the beginning of the post when I said that there were two different kinds of hard parts? One is the intrinsic complexity of the problem, and that one will be hard no matter what.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-8-142615069" href="https://generalrobots.substack.com/p/the-mythical-non-roboticist#footnote-8-142615069" target="_self" rel="">8</a></span><span> But the second is the incidental complexity, or as I like to call it, the stupid bullshit complexity.</span></p><p><span>Robots are asynchronous, distributed, real-time systems with weird hardware. All of that will be hard to configure for stupid bullshit reasons. Those drivers need to work in the weird flavor of linux you want for hard real-time for your controls and getting that all set up will be hard for stupid bullshit reasons. You are abusing wifi so you can roam seamlessly without interruption but linux’s wifi will not want to do that. Your log files are huge and you have to upload them somewhere so they don’t fill up your robot. You’ll need to integrate with some cloud something or other and deal with its stupid bullshit.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-9-142615069" href="https://generalrobots.substack.com/p/the-mythical-non-roboticist#footnote-9-142615069" target="_self" rel="">9</a></span><span>&nbsp;</span></p><p><span>There is a ton of crap to deal with before you even get to complexity of dealing with 3D rotation, moving reference frames, time synchronization, messaging protocols. Those things have intrinsic complexity (you have to think about when something was observed and how to reason about it as other things have moved) and stupid bullshit complexity (There’s a weird bug because someone multiplied two transform matrices in the wrong order and now you’re getting an error message that deep in some protocol a quaternion is not normalized. WTF does that mean?)</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-10-142615069" href="https://generalrobots.substack.com/p/the-mythical-non-roboticist#footnote-10-142615069" target="_self" rel="">10</a></span></p><p><span>One of the biggest challenges of robot programming is wading through the sea of stupid bullshit you need to wrangle in order to </span><strong>start</strong><span> working on your interesting and challenging robotics problem.&nbsp;</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3ee3b80-7c34-4792-9491-77cdf7bbc11b_672x512.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3ee3b80-7c34-4792-9491-77cdf7bbc11b_672x512.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3ee3b80-7c34-4792-9491-77cdf7bbc11b_672x512.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3ee3b80-7c34-4792-9491-77cdf7bbc11b_672x512.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3ee3b80-7c34-4792-9491-77cdf7bbc11b_672x512.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3ee3b80-7c34-4792-9491-77cdf7bbc11b_672x512.jpeg" width="672" height="512" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a3ee3b80-7c34-4792-9491-77cdf7bbc11b_672x512.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:512,&quot;width&quot;:672,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3ee3b80-7c34-4792-9491-77cdf7bbc11b_672x512.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3ee3b80-7c34-4792-9491-77cdf7bbc11b_672x512.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3ee3b80-7c34-4792-9491-77cdf7bbc11b_672x512.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3ee3b80-7c34-4792-9491-77cdf7bbc11b_672x512.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>“I’m so excited. I’ve been working 12 hours everyday for 6 weeks, and I finally get to start doing robotics!”</figcaption></figure></div><p>So a simple heuristic to make good APIs is:</p><p>Design your APIs for someone as smart as you, but less tolerant of stupid bullshit.</p><p><span>That feels universal enough that I’m tempted to call it </span><strong>Holson’s Law of Tolerable API Design</strong><span>.&nbsp;</span></p><p>When you are using tools you’ve made, you know them well enough to know the rough edges and how to avoid them.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e97fe78-1f6a-4ecc-baa6-bb414829df41_512x646.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e97fe78-1f6a-4ecc-baa6-bb414829df41_512x646.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e97fe78-1f6a-4ecc-baa6-bb414829df41_512x646.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e97fe78-1f6a-4ecc-baa6-bb414829df41_512x646.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e97fe78-1f6a-4ecc-baa6-bb414829df41_512x646.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e97fe78-1f6a-4ecc-baa6-bb414829df41_512x646.jpeg" width="270" height="340.6640625" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1e97fe78-1f6a-4ecc-baa6-bb414829df41_512x646.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:646,&quot;width&quot;:512,&quot;resizeWidth&quot;:270,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e97fe78-1f6a-4ecc-baa6-bb414829df41_512x646.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e97fe78-1f6a-4ecc-baa6-bb414829df41_512x646.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e97fe78-1f6a-4ecc-baa6-bb414829df41_512x646.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e97fe78-1f6a-4ecc-baa6-bb414829df41_512x646.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>“Oh yeah, if you try to move the robot without calling enable() it segfaults. That's a safety feature… I guess? But also if you call it twice, that also segfaults. Just call it exactly once, ever.”</figcaption></figure></div><p><span>But rough edges are things that have to be held in a programmer’s memory while they are using your system. If you insist on making a robotics framework</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-11-142615069" href="https://generalrobots.substack.com/p/the-mythical-non-roboticist#footnote-11-142615069" target="_self" rel="">11</a></span><span>, you should strive to make it as powerful as you can with the least amount of stupid bullshit. Eradicate incidental complexity everywhere you can. You want to make APIs that have maximum flexibility but good defaults. I like python’s default-argument syntax for this because it means you can write APIs that can be used like:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f71fd47-87b0-4d5b-83e4-c0a7a1d0ed4a_1986x558.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f71fd47-87b0-4d5b-83e4-c0a7a1d0ed4a_1986x558.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f71fd47-87b0-4d5b-83e4-c0a7a1d0ed4a_1986x558.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f71fd47-87b0-4d5b-83e4-c0a7a1d0ed4a_1986x558.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f71fd47-87b0-4d5b-83e4-c0a7a1d0ed4a_1986x558.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f71fd47-87b0-4d5b-83e4-c0a7a1d0ed4a_1986x558.png" width="1456" height="409" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8f71fd47-87b0-4d5b-83e4-c0a7a1d0ed4a_1986x558.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:409,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:140758,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f71fd47-87b0-4d5b-83e4-c0a7a1d0ed4a_1986x558.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f71fd47-87b0-4d5b-83e4-c0a7a1d0ed4a_1986x558.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f71fd47-87b0-4d5b-83e4-c0a7a1d0ed4a_1986x558.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f71fd47-87b0-4d5b-83e4-c0a7a1d0ed4a_1986x558.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>It is possible to have easy things be simple </span><em>and</em><span> allow complex things. And please, please, please don’t make condescending APIs. Thanks!</span></p><p data-attrs="{&quot;url&quot;:&quot;https://generalrobots.substack.com/p/the-mythical-non-roboticist?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://generalrobots.substack.com/p/the-mythical-non-roboticist?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p>Thanks to Leila Takayama and Rodney Brooks for reading drafts of this.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Learning From DNA: a grand challenge in biology (108 pts)]]></title>
            <link>https://hazyresearch.stanford.edu/blog/2024-03-14-evo</link>
            <guid>39707017</guid>
            <pubDate>Thu, 14 Mar 2024 17:56:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hazyresearch.stanford.edu/blog/2024-03-14-evo">https://hazyresearch.stanford.edu/blog/2024-03-14-evo</a>, See on <a href="https://news.ycombinator.com/item?id=39707017">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="#team">Full Evo team</a></p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-03-14-evo/robot-dna.jpeg"></figure>
<blockquote>
<p>Evo is a long-context biological foundation model that generalizes across the fundamental modalities of biology: DNA, RNA, and proteins. It is capable of both prediction tasks and generative design, from molecular to whole genome scale (generating 650k+ tokens). Evo is trained at a single-nucleotide, byte-level resolution, on a large corpus of prokaryotic and phage genomic sequences.</p>
</blockquote>
<h2>Resources</h2>
<p><a href="https://www.biorxiv.org/content/10.1101/2024.02.27.582234v2">paper</a><br>
<a href="https://github.com/evo-design/evo">repo</a><br>
<a href="https://colab.research.google.com/github/evo-design/evo/blob/main/scripts/hello_evo.ipynb">colab</a><br>
<a href="https://api.together.xyz/playground/language/togethercomputer/evo-1-131k-base">together playground</a> (browser tool)<br>
<a href="https://evo.nitro.bio/">API wrapper</a> (by Nitro Bio)<br>
<a href="https://huggingface.co/togethercomputer/evo-1-131k-base">huggingface (ckpts)</a><br>
<a href="https://pypi.org/project/evo-model/0.1.0/">pip install</a></p>
<h2>What can we learn from DNA?</h2>
<p>We’ve long held the belief that biology is one of the primary domains that could truly benefit from long context models. Our first proof-of-concept, <a href="https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna">HyenaDNA</a>, showed it could process sequences up to 1M tokens at the byte-level for predictive tasks. We were thrilled to see such positive reception and interest from entities in and out of academia [<a href="https://www.forbes.com/sites/robtoews/2023/09/03/transformers-revolutionized-ai-what-will-replace-them/?sh=54465669c1fe">1</a>,<a href="https://time.com/6564430/ai-minister-uae/">2</a>,<a href="https://www.marktechpost.com/2023/07/04/stanford-researchers-introduce-hyenadna-a-long-range-genomic-foundation-model-with-context-lengths-of-up-to-1-million-tokens-at-single-nucleotide-resolution/">3</a>,<a href="https://www.cjco.com.au/article/news/ai-propels-genomics-forward-unveiling-hyenadnas-scalability-in-long-sequence-processing/">4</a>,<a href="https://cbirt.net/hyenadna-powers-long-range-genomic-sequence-modeling-at-single-nucleotide-resolution/">5</a>,<a href="https://www.jiqizhixin.com/articles/2023-7-7-28">6</a>]! As fun as this was, we knew we wanted to continue pushing capabilities of foundation models in biology.</p>
<p>So we set our sights on a much harder challenge: <em><strong>generative</strong></em> DNA design. The problem was, a good number of biologists asked, “what would you even do if you can design DNA?” We spent 5 months searching for a partner that shared the same vision, and that’s when we found the Arc Institute, where we joined forces to create a model called <strong>Evo</strong>. Our goal - to be able to generate an entire (prokaryotic) genome from scratch using AI.</p>
<p>Why? Understanding by creating was one motivation. But a real motivation that drove us here at Hazy Research, perhaps naively, was to create biomolecular machines using AI that can target cancer cells or any other disease we programmed it to. To create microbes for better biofuels, absorb carbon dioxide, or discover a new class of antibiotics. These were some of our dreams - and we didn’t succeed on any one of them.</p>
<p>And yet, we’ve never been <em>more</em> excited about the potential for generative DNA and biology.</p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-03-14-evo/ball-of-life-zoom-15.gif"><figcaption><p>Figure 1. Predicted protein structures from a single, Evo-generated sequence.</p></figcaption></figure>
<p>As we were training Evo, compiling the largest DNA pretraining dataset (we know of), it felt like we were observing a “GPT” moment in biology. A simple unsupervised task was getting competitive <strong>zero-shot</strong> performance by modeling across the <a href="https://en.wikipedia.org/wiki/Central_dogma_of_molecular_biology">central dogma</a> of biology, and generalizing across DNA, RNA and protein modalities.</p>
<p>One of the really exciting moments came when we were able to generate the world’s first AI designed CRISPR system (a complex of RNA and proteins), without requiring supervision. There were many other “firsts” that we observed throughout this project - but similar to natural language, there will be many more capabilities to emerge and yet to be discovered from the relatively untapped reservoir of biological data.</p>
<p>And so these are some of the reasons why we believe learning from DNA sequences is the next “grand challenge” of biology. If this blog doesn’t convince some folks in machine learning to work in biology, we will not have done our job!</p>
<p>It’s still very early days (think “blurry” GANs images), but the viability of scaling large biological foundation models is indeed promising. Let’s dive into the ML team "director's cut" on Evo.</p>
<h2>Contents</h2>
<p>Feel free to jump directly:</p>
<ul>
<li>Evo <a href="#highlights">highlights</a></li>
<li><a href="#language">challenges</a> in modeling DNA</li>
<li>DNA pretraining <a href="#scaling">scaling laws</a></li>
<li><a href="#mad">Mechanistic Architecture Design</a></li>
<li>CRISPR &amp; gene essentiality <a href="#results">results</a></li>
<li><a href="#future">future</a> directions</li>
</ul>
<h2>Evo highlights</h2>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-03-14-evo/arch.png"><figcaption><p>Figure 2. The Evo model architecture, based on StripedHyena.</p></figcaption></figure>
<p>With Hazy, Arc and Together AI, we trained Evo, a 7B parameter long context foundation model built to generate DNA sequences at single nucleotide resolution, and over 650k+ tokens long.</p>
<ul>
<li>
<p>Evo is trained on 2.7M prokaryotic and phage genomes (un-annotated), using next token prediction. We compiled a dataset called OpenGenome, which we’re releasing open source, containing 300B tokens.</p>
</li>
<li>
<p>Evo is based on the <a href="https://www.together.ai/blog/stripedhyena-7b">StripedHyena</a> architecture, an enhanced Hyena model that’s hybridized with rotary attention and trained in 2 stages using an efficient context extension, reaching a context of 131k tokens.</p>
</li>
<li>
<p>Beyond the technical innovations of the model (more in <a href="https://www.biorxiv.org/content/10.1101/2024.02.27.582234v2">paper</a>), the strength of the multimodal learning and generalization abilities of the model is what truly surprised us. Evo is able to learn across DNA, RNA and proteins, reaching competitive zero-shot performance on function prediction with state-of-the-art (SOTA) protein specific FMs like <a href="https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1.full.pdf">ESM2</a> and <a href="https://www.nature.com/articles/s41587-022-01618-2">ProGen</a>, notably, without explicitly being shown protein coding regions.</p>
</li>
<li>
<p>Evo understands at the whole genome level - using a gene essentiality test, Evo can predict which genes are essential to an organism’s survival based on small DNA mutations, also zero-shot (which currently is only possible in the wet lab).</p>
</li>
<li>
<p>Evo excels at generation, where we showcase the generation of molecules, to systems, and to whole genomes scale. One of the key breakthroughs we highlight, Evo can design novel CRISPR systems (including genes and RNA), an exciting frontier for creating new forms of genome editing tools.</p>
</li>
</ul>
<p>We describe the biological innovations more in our initial blog <a href="https://arcinstitute.org/news/blog/evo">release</a> and in greater detail in the <a href="https://www.biorxiv.org/content/10.1101/2024.02.27.582234v2">paper</a>, so we’ll focus on what makes modeling DNA challenging and an exciting frontier for ML.</p>
<h2>DNA as a language</h2>
<p>Here, we largely treat DNA as a language, and so why haven’t we been able to effectively learn from DNA before? Aren’t Transformers great at language? This is something that initially blocked us as well. As we started scaling *thousands* of DNA models over the past year, several key lessons have emerged (obvious to many computational biologists).</p>
<p><strong>DNA is not like natural language.</strong></p>
<p>DNA is more like several languages or modalities in one. Typically, it’s believed that these biological languages (DNA, RNA, proteins) are so complex that they each require specialized models to effectively model them. But genomes carry an entire set of DNA to make a complete organism. And so it begs the question, “is DNA all you need”?</p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-03-14-evo/dogma.png"><figcaption><p>Figure 3.  The fundamental “languages” of biology DNA, RNA, and proteins, united in the central dogma. (Photo credit: Khan Academy)</p></figcaption></figure>
<p>A key technical set of challenges in learning from DNA, among many, has been the long-range modeling (e.g. human genome is 3B nucleotides) as well as the resolution required to capture the effects of single mutations throughout evolution, which occur one nucleotide at a time. This can lead to varying “signal” strengths over the different modalities in DNA (discussed more in our first DNA <a href="https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna">blog</a>). Though very long context models are emerging [<a href="https://arxiv.org/abs/2403.05530">Gemini</a>, <a href="https://www.anthropic.com/news/claude-3-family">Claude</a>], no large-scale Transformer has been effectively used at single character / byte level. This turns out to be an “achilles heel” in Transformers for DNA language modeling.</p>
<h2>DNA scaling laws</h2>
<p>To compare across potential architectures, we conducted the first DNA (and byte-level tokenization!) scaling laws experiments via the <a href="https://arxiv.org/abs/2203.15556">Chinchilla</a> protocol. This involved training 300 models across prevailing language and DNA models: Transformer++, Mamba, Hyena, and StripedHyena, where we sweep across model size (6M to 1B) and dataset size, compute budgets and hyperparameters.</p>
<p>Scaling laws in natural language have helped propel LLM research forward by providing a guidebook on how to scale models and data with predictable performance. No such work has been done in DNA language modeling, and we hope to accelerate research in the field by releasing intermediate <a href="https://huggingface.co/togethercomputer/evo-1-131k-base/tree/main">checkpoints</a> during our training, hosted on HuggingFace. There’s a great deal of lessons learned that we hope to share with the community over time (perhaps a standalone blog?). There's so much we don't know about the best way to pretrain on DNA or with byte-level tokenization.</p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-03-14-evo/optimal2.png"><figcaption><p>Figure 5. Compute optimal scaling on DNA pretraining. Each point represents the lowest perplexity for a model after sweeping different compute allocations (model size and training tokens).</p></figcaption></figure>
<p>In DNA scaling, we found Transformer++ to yield significantly worse perplexity (a measure of next token prediction quality) at all compute budgets, a symptom of the inefficiency of the architecture at the byte resolution [<a href="https://arxiv.org/abs/2106.12672">Charformer</a>, <a href="https://arxiv.org/abs/2103.06874">CANINE</a>]. State-space and deep signal processing architectures are observed to improve on the scaling rate over Transformer++, with Hyena and StripedHyena resulting in the best scaling rate. This suggests that at scale, the prevailing architecture in language may not easily transfer to DNA.</p>
<h2>Mechanistic Architecture Design</h2>
<p>In upcoming work on how we designed <a href="https://www.together.ai/blog/stripedhyena-7b">StripedHyena</a> and other hybrid models, we dive deeper into understanding their advantages on DNA data by using a new framework we call Mechanistic Architecture Design (MAD). MAD builds on previous work in the lab [<a href="https://arxiv.org/abs/2212.14052">H3</a>, <a href="https://arxiv.org/abs/2302.10866">Hyena</a>, <a href="https://hazyresearch.stanford.edu/blog/2023-12-11-zoology1-analysis">MQAR</a>] in using targeted synthetic tasks that allowed us to test new architecture design choices. With MAD, we are able to show for the first time a connection between syntethic performance across various tasks and scaling laws.</p>
<p>These token manipulation tasks expand beyond previous recall-focused tasks, and include testing the ability to compress/aggregate multiple token representations (important in byte-level tokens) and to filter noise, where genomic sequences are known to contain a lot of “junk” DNA. Through synthetic tasks, we’re able to better understand where models do well or worse, and in turn, use this to drive the design of stronger models. Stay tuned.</p>
<h2>Zero-shot prediction and multimodal generation</h2>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-03-14-evo/cas9_gen3.gif"><figcaption><p>Figure 6. Generative design of CRISPR-Cas molecular complexes with Evo.</p></figcaption></figure>
<p>As a preview, we highlight one of the generative design capabilities of Evo. Today, generative models for biology usually focus on a single modality (at a time)—for example, only on proteins or on RNA. Instead, Evo can perform multimodal design to generate novel CRISPR systems, a task that requires creating large functional complexes of proteins and ncRNA (non-coding RNA), and is out of reach for existing generative models.</p>
<p>Typically, discovering new CRISPR systems requires searching through natural genomes for similar sequences that were literally taken from an organism. Promisingly, Evo enables a new approach to generating biological diversity by sampling sequences directly from a generative model, an exciting frontier for creating new forms of genome editing tools.</p>
<p>Strikingly, Evo can also understand biological function at the whole genome level. Using an in silico gene essentiality test on 56 whole-genomes (at 66 kbp), Evo can predict which genes are essential to an organism’s survival based on small DNA mutations. It can do so <em><strong>zero-shot</strong></em> and with <em><strong>no supervision</strong></em>. For comparison, a gene essentiality experiment in the laboratory could require 6 months to a year of experimental effort. In contrast, we replace this with a few forward passes through a neural network.</p>
<p>There are a range of capabilities (additional zero-shot predictions to generation of whole genome scale sequences) that we discuss in the <a href="https://arcinstitute.org/news/blog/evo">initial blog</a> and paper, and we encourage folks to check it out. But if that’s still too much time commitment for your comfort, we’re big fans of this <a href="https://x.com/JuliaBauman2/status/1765509688362492301?s=20">summary video</a> by Julia Bauman.</p>
<h2>Future directions</h2>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-03-14-evo/hyena_scientist.png"></figure>
<p>We’re not working in a vacuum, there’s been a tremendous amount of fundamental research in biology (including computational, synthetic, bioengineering). Our goal is to accelerate the research where we can with the tools we know best, and hopefully motivating a few ML researchers along the way.</p>
<p>In the spirit of foundation models in other domains, we believe that Evo and other models that emerge can serve as a foundation for building useful things on top.</p>
<p>The potential to draw from the language modeling community is promising, as we incorporate prompt engineering and alignment techniques to design sequences with higher controllability and quality. Multimodal learning, and injecting domain specific knowledge are all things we’re bull-ish on.</p>
<p>While data and models are certainly important, evaluations play an outsized importance in biology, and perhaps is the biggest barrier to entry for the ML community. How do you formulate a task, what pretraining and annotations are needed, and of course, what does it mean in terms of the mechanistic understanding in biology? These are just some of the many questions we continue to think about as we try to innovate, as well as respect the body of work before us.</p>
<p>Even more challenging, how do we learn from eukaryotes (humans, mammals), which have <em><strong>far</strong></em> more complex genomes than prokaryotes. Is simply scaling one solution?</p>
<p>We’re in the early innings of what we think is possible in designing DNA sequences. Dropping perplexity on the Pile is exciting, and can lead to better downstream benchmarks or chatbots. Better perplexity on DNA and other bio sequences can lead to creating new tools for countless scientific advances and the improvement of human health.</p>
<p>We believe there's a big appetite in the ML community to work on meaningful problems (in bio or other) with cutting edge technology. If you've made it this far in the blog, you likely have that same hunger. Go after that grand challenge in biology, chemistry, climate tech, or whatever domain excites you. Tinker, collborate, and <strong>build the thing</strong>.</p>
<h2>Correspondence</h2>
<p>Eric Nguyen, <a href="mailto:etnguyen@stanford.edu">etnguyen@stanford.edu</a><br>
<!-- -->Michael Poli, <a href="mailto:polimic03@gmail.com">polimic03@gmail.com</a></p>
<p>The full team also includes: Matt Durrant, Armin Thomas, Brian Kang, Jeremy Sullivan, Madelena Ng, Ashley Lewis, Aman Patel, Aarou Lou, Stefano Ermon, Stephen Baccus, Tina Hernandez-Boussard, Chris Ré, Patrick Hsu and Brian Hie.</p>
<h2>Acknowledgements</h2>
<p>We thank our labmates for their helpful feedback in writing this post: Jerry Liu, Michael Zhang, Jon Saad-Falcon, and Ben Viggiano.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Figma's databases team lived to tell the scale (525 pts)]]></title>
            <link>https://www.figma.com/blog/how-figmas-databases-team-lived-to-tell-the-scale/</link>
            <guid>39706968</guid>
            <pubDate>Thu, 14 Mar 2024 17:51:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.figma.com/blog/how-figmas-databases-team-lived-to-tell-the-scale/">https://www.figma.com/blog/how-figmas-databases-team-lived-to-tell-the-scale/</a>, See on <a href="https://news.ycombinator.com/item?id=39706968">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Our nine month journey to horizontally shard Figma’s Postgres stack, and the key to unlocking (nearly) infinite scalability.</p><div colorscheme="[object Object]"><div><p>Vertical partitioning was a relatively easy and very impactful scaling lever that bought us significant runway quickly. It was also a stepping stone on the path to horizontal sharding.</p></div><p>Figma’s database stack has grown almost 100x since 2020. This is a <em>good</em> problem to have because it means our business is expanding, but it also poses some tricky technical challenges. Over the past four years, we’ve made a significant effort to stay ahead of the curve and avoid potential growing pains. In 2020, we were running a single Postgres database hosted on AWS’s largest physical instance, and by the end of 2022, we had built out a distributed architecture with caching, read replicas, and a dozen vertically partitioned databases. We split groups of related tables—like “Figma files” or “Organizations”—into their own vertical partitions, which allowed us to make incremental scaling gains and maintain enough runway to stay ahead of&nbsp;our&nbsp;growth.</p><div><p><a href="https://www.figma.com/blog/how-figma-scaled-to-multiple-databases/"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAACa0lEQVQokUXS20/SARjG8a66VBTDY+aNlmbmKa2c81DZQUPbcKIMVEAEDwkrOzjRbJamE88BBvwU+HFQs0xXLpu15dZN6x+o+7rpb/gWWPPivf3sefc8R2LCTUhCLST5O8lcGeS8d4obPgc1vgUuio/JC/WRs2rgzKqOonA7F8IGCsJG0oJtSIMqJKFmYsPKgwspORIBj4la8j02GpZ26XTuc2dhk/7nDrSuEQq9PaQF1WQHFBT6b1EoKskT9WR6TWR4jSQGWqNQTKjpHxhqItVv5KYQZsz9nYBtj+2BUXaGDSxO6bjqMHNKuEepx8I5z22yl3vIWjFT4LFS5hkjb2WEVH8PkqCKSLgoeFw00iqusSp85Zt1ki/GNl6a+3hiHaTBNssV+xp19hDVDhclDjuV9hkM9jHMi9Mo5oPkL80gE3XRlFEwRdQjFxeZf+Fny9yJrUGOUm7isnaYyqdOFM4duoRPGIRdmu2bdD+bYM7ayv3eRmr0D8kdGkYmaA8TpgR01G1MMLhhxzLQTllxLgmyDJJKqikbGqF/7R2+rR/4N34y6v7I+KCF+ZZL1BedJiUrB5nqOvHOg3KiYHpYj37Xjm3/LYrpERLLCzkqkyItLeba+DBzHz6z/f4X669+MyrsYR0yM99Yhf5sJjk5J0nW1hLvUh2CiWENVZuP0GwLVCxNkmJRIlFXIuuVU+S4i2bTTc/6GzrE19QuL1Iz24fpgZwuQzkV3fWcmNAR71MT+//lSN3SoJrkQAfJPgMJgoY4t5K45WakoprkoJ400USaPzITHTJRQ4agJMulIF1QkSBqiI3s8W8pfwBARJixybsgNwAAAABJRU5ErkJggg==" alt="One large database silo connected to two smaller ones." data-lqip="true"><img data-loading="true" loading="lazy" alt="One large database silo connected to two smaller ones." src="https://cdn.sanity.io/images/599r6htc/localized/5c8ad1394a1cf21fc39adf7ca3d9f317c8b98b80-2120x1000.png?w=2120&amp;h=1000&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/5c8ad1394a1cf21fc39adf7ca3d9f317c8b98b80-2120x1000.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 1060w, https://cdn.sanity.io/images/599r6htc/localized/5c8ad1394a1cf21fc39adf7ca3d9f317c8b98b80-2120x1000.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 1590w, https://cdn.sanity.io/images/599r6htc/localized/5c8ad1394a1cf21fc39adf7ca3d9f317c8b98b80-2120x1000.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=2 2120w"></a></p></div><p>Despite our incremental scaling progress, we always knew that vertical partitioning could only get us so far. Our initial scaling efforts had focused on reducing Postgres CPU utilization. As our fleet grew larger and more heterogeneous, we started to monitor a range of bottlenecks. We used a combination of historical data and load-testing to quantify database scaling limits from CPU and IO to table size and rows written. Identifying these limits was crucial to predicting how much runway we had per shard. We could then prioritize scaling problems before they ballooned into major reliability risks.</p><p>The data revealed that some of our tables, containing several terabytes and billions of rows, were becoming too large for a single database. At this size, we began to see reliability impact during Postgres vacuums, which are essential background operations that keep Postgres from running out of transaction IDs and breaking down. Our highest write tables were growing so quickly that we would soon exceed the maximum IO operations per second (IOPS) supported by Amazon’s Relational Database Service (RDS). Vertical partitioning couldn’t save us here because the smallest unit of partitioning is a single table. To keep our databases from toppling, we needed a&nbsp;bigger&nbsp;lever.</p><h2 id="scaffolding-for-scale"><a href="#scaffolding-for-scale">Scaffolding for scale</a></h2><p>We outlined a number of goals and must-haves to tackle short-term challenges while setting us up for smooth long-term growth. We&nbsp;aimed&nbsp;to:</p><ul><li><strong>Minimize developer impact: </strong>We wanted to handle the majority of our complex relational data model supported by our application. Application developers could then focus on building exciting new features in Figma instead of refactoring large parts of our&nbsp;codebase.</li><li><strong>Scale out transparently:</strong> As we scale in the future, we don’t want to have to make additional changes at the application layer. This means that after any initial upfront work to make a table compatible, future scale-outs should be transparent to our product&nbsp;teams.</li><li><strong>Skip expensive backfills:</strong> We avoided solutions that involve backfilling large tables or every table at Figma. Given the size of our tables and Postgres throughput constraints, these backfills would have taken&nbsp;months.</li><li><strong>Make incremental progress: </strong>We identified approaches that could be rolled out incrementally as we de-risked major production changes. This reduced the risk of major outages and allowed the databases team to maintain Figma’s reliability throughout the&nbsp;migration.</li><li><strong>Avoid one-way migrations: </strong>We maintained the ability to roll back even after a physical sharding operation is completed. This reduced the risk of being stuck in a bad state when unknown <em>unknowns</em> occur.</li><li><strong>Maintain strong data consistency: </strong>We wanted to avoid complex solutions like double-writes that are challenging to implement without taking downtime or compromising on consistency. We also wanted a solution that would allow us to scale out with near-zero downtime.</li><li><strong>Play to our strengths: </strong>Since we were operating under tight deadline pressure, whenever possible, we favored approaches that could be rolled out incrementally on our fastest growing tables. We aimed to leverage existing expertise and&nbsp;technology.</li></ul><h2 id="exploring-our-options"><a href="#exploring-our-options">Exploring our options</a></h2><p>There are many popular open source and managed solutions for horizontally sharded databases that are compatible with Postgres or MySQL. During our evaluation, we explored CockroachDB, TiDB, Spanner, and Vitess. However, switching to any of these alternative databases would have required a complex data migration to ensure consistency and reliability across two different database stores. Additionally, over the past few years, we’ve developed a lot of expertise on how to reliably and performantly run RDS Postgres in-house. While migrating, we would have had to rebuild our domain expertise from scratch. Given our very aggressive growth rate, we had only months of runway remaining. De-risking an entirely new storage layer and completing an end-to-end-migration of our most business-critical use cases would have been extremely risky on the necessary timeline. We favored known low-risk solutions over potentially easier options with much higher uncertainty, where we had less control over the&nbsp;outcome.</p><p>NoSQL databases are another common scalable-by-default solution that companies adopt as they grow. However, we have a very complex relational data model built on top of our current Postgres architecture and NoSQL APIs don’t offer this kind of versatility. We wanted to keep our engineers focused on shipping great features and building new products instead of rewriting almost our entire backend application; NoSQL wasn’t a viable solution.</p><p>Given these tradeoffs, we began to explore building a horizontally sharded solution on top of our existing vertically partitioned RDS Postgres infrastructure. It didn’t make sense for our small team to re-implement a generic horizontally sharded relational database in-house; in doing so, we’d be competing with tools built by the likes of large open source communities or dedicated database vendors. However, because we were tailoring horizontal sharding to Figma’s specific architecture, we could get away with providing a much smaller feature set. For example, we chose not to support atomic cross-shard transactions because we could work around cross-shard transaction failures. We picked a colocation strategy that minimized the changes required at the application layer. This allowed us to support a subset of Postgres that was compatible with the majority of our product logic. We also were able to easily maintain backwards compatibility between sharded and unsharded postgres. If we ran into unknown <em>unknowns</em>, we could easily <a href="https://www.figma.com/blog/how-figma-scaled-to-multiple-databases/#the-logical-choice">roll back to unsharded</a> Postgres.</p><h2 id="the-path-to-horizontal-sharding"><a href="#the-path-to-horizontal-sharding">The path to horizontal sharding</a></h2><p>Even with these narrower requirements, we knew horizontal sharding would be our largest and most complex database project to date. Luckily, our incremental scaling approach over the past few years bought us the runway to make this investment. In late 2022, we set out to unlock nearly infinite database scalability, and horizontal sharding—the process of breaking up a single table or group of tables and splitting the data across multiple physical database instances—was the key. Once a table is horizontally sharded at the application layer, it can support any number of shards at the physical layer. We can always scale out further by simply running a physical shard split. These operations happen transparently in the background, with minimal downtime and no application level changes required. This capability would allow us to stay ahead of our remaining database scaling bottlenecks, removing one of the last major scaling challenges for Figma. If vertical partitioning let us accelerate to highway speeds, horizontal sharding could remove our speed limits and&nbsp;let&nbsp;us&nbsp;fly.</p><div><figure><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAACvElEQVQ4jVWTa1PTWhiF+dMgoqg90qYltml6SQ2IowIWAYvI8TKKOuN1BCo2TS/pLU3SNi3ieP7Bc2YnHOB8WB/2ntlrv2utd02ZvzTMkxC1SQhzcunuV4HaSQhxrloKre0bjJPT/CNN80eaxldmaO5FMHoqU8HjrkqjLNP+EqfzKU7ra4K6kcT0siHJOE/Fz2H4OcyaQruYYJBMMZZVJnKGoZLCKi1itNNMmX6exmcJZ2WOcXqGiTKDn7lCb2Mes3InIPrhZTjsp/nhqZhmBmv7Hp27z7GX9nGW32Ev79LY1TFaYsJhDut1nGHqNqfRGL9jEqexKJ6+QO27zLGX4chRA1JBbpoqrScaTvYxXq7EMF/C1dZp7uQuCJv7OZx7G4z0l/grb/CXXtBfe0j1QKXspDlyVQw/T+B3Q6G9dZuRcosTORJgqEawdqMYXSF5mKP+YZnO2j72WgWnWKO/Wqa9+YzKYY5DWwnkCkIRULWfof4xRq94He/+HO79Obqb89S+xal62f8Il+isvcVe/Ym9btB9cID1pETl4BLh+IzQVqm/j9J7dA1Xn8XVr9Jdv07ts3RB2NzP4i6vMyo8Z6TvMSqU6D9aofo9HUp21NA/sUp1hdZGFO/OAn4iih+P4qUWaO7EMDpnkq3XEqPULU4XIvyO/sVpLIKnRzDPQ0lTdlV+jrJhKJs6jraNV/gbT9vD1jZo7OTPQvHz1L/FsVev4RdmmWizDPVZOk9vYprJwLvjQTaY8niQCQm3lrALJVz9Fe7dl/QKWzSeaWeEQoaToV5NYh3JtA5kmmWZWlNBTB8s9kQLPBQIJBdjDGSJcWKRSWKRQVLCKknhYgujz+s3FshfVE/U7hLE2bRVGh9i9Is3GD6cD9DfvEldhOKKUERfA/z/cXh30elziM/cLKalBJYEaKfDmk40/gV+dzqM3x5jjwAAAABJRU5ErkJggg==" alt="A data silo on the left that breaks out into two other data silos on the right." width="804" height="536" data-lqip="true"><img data-loading="true" width="804" height="536" loading="lazy" alt="A data silo on the left that breaks out into two other data silos on the right." src="https://cdn.sanity.io/images/599r6htc/localized/5b69dc237c2ee309a970a0ab656d02e62f136c26-2160x1440.png?w=804&amp;h=536&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/5b69dc237c2ee309a970a0ab656d02e62f136c26-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 402w, https://cdn.sanity.io/images/599r6htc/localized/5b69dc237c2ee309a970a0ab656d02e62f136c26-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 603w, https://cdn.sanity.io/images/599r6htc/localized/5b69dc237c2ee309a970a0ab656d02e62f136c26-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format 804w, https://cdn.sanity.io/images/599r6htc/localized/5b69dc237c2ee309a970a0ab656d02e62f136c26-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 1206w, https://cdn.sanity.io/images/599r6htc/localized/5b69dc237c2ee309a970a0ab656d02e62f136c26-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1608w"></p></div><figcaption>Vertical partitioning</figcaption></figure><figure><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAACw0lEQVQ4jYWTa2/aZhiG+6fbKmmUlkAAG3yKgZK0lB3aqatWbUu3NSRBPXxIOBS/xgY3hNiAISVZVm3aH7gm280U7cs+XLL8Wr7f+34Ot4ZXD4kY/IfozL0o44gCo2/vEap3mCt3OK2tYh/l6AwKdF/msXM6dl5H/FLAXZS4Nfi9wuBTieFkC+/MwPMNBqGJuywj5lv0j2T8WpG5ViFUKvhVBdHM0+pLdF9J2KqG0FTEXhH3PBK8KDMUBcavUky/X2fywzofmxnEiUL7TMF6KzGtP2JR/olZ6UeCnceIhkzbkeg1Nunra9jGGnYzh/spEjwvMdrPcK7d5XP6NlfZ2wS1VUQnT8dXsY8lgrrMQjeYazr+joRo5mg5Mt1fs1hKGktNI/bzicPheYlxo8DS1PhD2eZKrTB5LCGOJXozHedE46SZJXiZihk1MthOgY5XpLMrYSkGlqpjRZEXXwRPmyaL+gsu629Y1hr4T3ewjmV6oR7HGAQGQ0+NGZwlZ/bUwGpksbX72MZ9xGHuhsN9hYtKhT/Np3w260yfKLFDKzRwAwOvLXFymGF0kME7yuOONYSv09vLIrQUtpFCHH6JHNVwvJfhUlnh7/Qqf22uMNu5h93K82Gi0e/KjL/JEBpZQj3LWS2N/T5HO6rhroxQDYRmJF2Ox2ZZxmvlCb5bY/FohfmTFUa7DxBuIenyO4mgViHces7UeI5frSL25XhsPvwWRU4h9BTi4NphNIehGdfHs4sMHYXBqY6zMOlNdcR7Cb+2xUz/iqn2NX7VjDvaciS6rzcR2jpCX09qGI3Nv1sSCV9WkmdEtCnLMq5dZPwizWx7g7C6wfjZBuIoEpRp/yzRK2j0FA3r9Y1NGcYk6xYTvV9fMjcZugqjthTj9Ys4EwMR6PQOJKztBPFGvuHw2tX/cZ0g+mdZxvFUnHfZhI9q/P0fzqkvRfmIEgIAAAAASUVORK5CYII=" alt="A data silo on the left that breaks out into two other data silos on the right, then four other silos." width="804" height="536" data-lqip="true"><img data-loading="true" width="804" height="536" loading="lazy" alt="A data silo on the left that breaks out into two other data silos on the right, then four other silos." src="https://cdn.sanity.io/images/599r6htc/localized/8903764c609f32287e38bd435f18b304d586efb0-2160x1440.png?w=804&amp;h=536&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/8903764c609f32287e38bd435f18b304d586efb0-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 402w, https://cdn.sanity.io/images/599r6htc/localized/8903764c609f32287e38bd435f18b304d586efb0-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 603w, https://cdn.sanity.io/images/599r6htc/localized/8903764c609f32287e38bd435f18b304d586efb0-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format 804w, https://cdn.sanity.io/images/599r6htc/localized/8903764c609f32287e38bd435f18b304d586efb0-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 1206w, https://cdn.sanity.io/images/599r6htc/localized/8903764c609f32287e38bd435f18b304d586efb0-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1608w"></p></div><figcaption>Horizontal sharding</figcaption></figure></div><p>Horizontal sharding was an order of magnitude more complex than our previous scaling efforts. When a table is split across multiple physical databases, we lose many of the reliability and consistency properties that we take for granted in ACID SQL databases. For&nbsp;example:</p><ul><li>Certain SQL queries become inefficient or impossible to&nbsp;support.</li><li>Application code must be updated to provide enough information to efficiently route queries to the correct shard(s) wherever possible.</li><li>Schema changes must be coordinated across all shards to ensure the databases stay in sync. Foreign keys and globally unique indexes can no longer be enforced by&nbsp;Postgres.</li><li>Transactions now span multiple shards, meaning Postgres can no longer be used to enforce transactionality. It is now possible that writes to some databases will succeed while others fail. Care must be taken to ensure product logic is resilient to these “partial commit failures” (imagine moving a team between two organizations, only to find half their data was&nbsp;missing!).</li></ul><p>We knew achieving full horizontal sharding would be a multi-year effort. We needed to de-risk the project as much as possible while delivering incremental value. Our first goal was to shard a relatively simple but very high traffic table in production as soon as possible. This would prove the viability of horizontal sharding while also extending our runway on our most loaded database. We could then continue building additional features as we worked to shard more complex groups of tables. Even the simplest possible feature set was still a significant undertaking. End to end, it took our team roughly nine months to shard our first&nbsp;table.</p><h2 id="our-unique-approach"><a href="#our-unique-approach">Our unique approach</a></h2><p>Our horizontal sharding work built on what many others do, but with some unusual design choices. Here are a few&nbsp;highlights:</p><ul><li><strong>Colos</strong>: We horizontally sharded groups of related tables into colocations (which we affectionately call “colos”), which shared the same sharding key and physical sharding layout. This provided a friendly abstraction for developers to interact with horizontally sharded&nbsp;tables.</li><li><strong>Logical sharding: </strong>We separated the concept of “logical sharding” at the application layer from “physical sharding” at the Postgres layer. We leveraged views to perform a safer and lower cost logical sharding rollout before we executed a riskier distributed physical failover.</li><li><strong>DBProxy query engine: </strong>We built a DBProxy service that intercepts SQL queries generated by our application layer, and dynamically routes queries to various Postgres databases. DBProxy includes a query engine capable of parsing and executing complex horizontally sharded queries. DBProxy also allowed us to implement features like dynamic load-shedding and request hedging.</li><li><strong>Shadow application readiness: </strong>We added a “shadow application readiness” framework capable of predicting how live production traffic would behave under different potential sharding keys. This gave product teams a clear picture of what application logic needed to be refactored or removed to prepare the application for horizontal sharding.</li><li><strong>Full logical replication</strong>: We avoided having to implement “filtered logical replication” (where only a subset of data is copied to each shard). Instead, we copied over the entire dataset and then only allowed reads/writes to the subset of data belonging to a&nbsp;given&nbsp;shard.</li></ul><h2 id="our-sharding-implementation"><a href="#our-sharding-implementation">Our sharding implementation</a></h2><p>One of the most important decisions in horizontal sharding is which shard key to use. Horizontal sharding adds many data model constraints that revolve around the shard key. For example, most queries need to include the shard key so that the request can be routed to the right shard. Certain database constraints, like foreign keys, only work when the foreign key is the sharding key. The shard key also needs to distribute data evenly across all shards to avoid hotspots that cause reliability issues or impact scalability.</p><div><p>Figma lives in the browser, and many users can collaborate in parallel on the same Figma file. This means that our product is powered by a fairly complex relational data model capturing file metadata, organization metadata, comments, file versions, and&nbsp;more.</p></div><p>We considered using the same sharding key for every table, but there was no single good candidate in our existing data model. To add a unified sharding key, we would have had to create a composite key, add the column to every table’s schema, run expensive backfills to populate it, and then substantially refactor our product logic. Instead, we tailored our approach to Figma’s unique data model and selected a handful of sharding keys like UserID, FileID, or OrgID. Almost every table at Figma could be sharded using one of&nbsp;these&nbsp;keys.</p><p>We introduced the concept of colos, which provide a friendly abstraction for product developers: Tables within a colo support cross-table joins and full transactions when restricted to a single sharding key. Most application code already interacted with the database this way, which minimized the work required by application developers to make a table ready for horizontal sharding.</p><div><figure><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB90lEQVQ4ja2TzU7bQBhF/dhVF5SQAIE4/olpHBKVJGpQAwGFpKq66YZVV/wYexxnBjui3cFDnGqmJaCIoi66uPLonM/XXnxjifsmTwmfnVf5a665jCUeQhKd++ZTHtkqf809hOguK75rkMxdRGyTRjbixibJHOLCJ1EeQtQNN07UDTNu5pjZVPPYNh26y0rON1HHJYp+maK/yaJfQQ1LiC8lZuMSt4PfXCcflA3TTg03/vAtin4FOSqTnG9hyeM17twNit09pHNAUQv5YVfIW2/Ig3fkdgNZ/4CqH1DYAXmwhmq9ZeHWyOsdpNPl1t5n4e8gR+tY6qhM4XuI+pBLf0rijFg4AUW4jgp2EM6AS/eMK29C6h4apsISymtz44258KfE7ikqaCGPK1jJic289ZHYn5jC2J8i3w9IOw4ibBL7p1y6E668KUnj1DDRcUj3Drn2Jlx4UyL9sXBAPLKxonGNebuD9Ppkbtc852GHqGcTd3yyxgEzp2tc1ugaFnVrZM02mdtjprnXY7bf5npcw4q+75J+3kEeVZGfqshhldlZlejbNvHXbbITzbeN02fNtNMzelZqd1Q1HbrLSn4GJLlHOneXEbeeWYFk4SPUM65cw7TTM8t3pGs6dNfTYq9E85fc3/hysfWWv3SlHm/Av/JHZ/7wf+YXqnU5r23j3J4AAAAASUVORK5CYII=" alt="Three data silos, each containing blocks for &quot;users table&quot; and &quot;user favorites table.&quot;" width="804" height="536" data-lqip="true"><img data-loading="true" width="804" height="536" loading="lazy" alt="Three data silos, each containing blocks for &quot;users table&quot; and &quot;user favorites table.&quot;" src="https://cdn.sanity.io/images/599r6htc/localized/8610966e627e683caaff88094d9dd8b75efa1a3b-2160x1440.png?w=804&amp;h=536&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/8610966e627e683caaff88094d9dd8b75efa1a3b-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 402w, https://cdn.sanity.io/images/599r6htc/localized/8610966e627e683caaff88094d9dd8b75efa1a3b-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 603w, https://cdn.sanity.io/images/599r6htc/localized/8610966e627e683caaff88094d9dd8b75efa1a3b-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format 804w, https://cdn.sanity.io/images/599r6htc/localized/8610966e627e683caaff88094d9dd8b75efa1a3b-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 1206w, https://cdn.sanity.io/images/599r6htc/localized/8610966e627e683caaff88094d9dd8b75efa1a3b-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1608w"></p></div><figcaption>Tables sharded by UserID and by FileID are each colocated together</figcaption></figure><figure><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAACLklEQVQ4ja2TS09TURSF75/TxKgTBzjSaGOF8FLsy1D6oIXbhvZKKX0/oEBbWrgtJhWupQKmCSaGxJEYMQiNxAn9EZ85B4LCRE0YrOx1vr3XHu2jfD5NcKHuH/5f1b38Vva7SaTOgaj7p+dM8uTF8FV+KdM948qHkwjG9xC1gwArX1X0gwAbh1O877zi3bHG629ByYWEF0z01g+n5OzKeUbsELuU6J6LgdYLTG8s9DbsmBoW+pojuNo27NtWnq5bMDfsmBs26QVzt+0MNgW3yYy5YeVZy8rsngulrzXMzdIA5vwknnyMvrzKjaUhbtWfcKfSz+N5L85clNHcDI/mvNyu9HO33kvPkgNLXsObizGcm6KnZGOwNYIy1LRgWnITyRSopdaIpRYwFbzc04d4WHYQzKapJGssJ1YJZFI8KDu4rz/HthCmkKpSS9RJJ4tYFoOMNO0oquFjupimmNapJ9cop3XChQz26jjeUpj57DIriRrVuM5cpoynGOJl1Ue0kKeaqqPHa5SSq0QXswQMH4q26Se4PIM6l2Qyn5B1ojiDWx/HUwmgFuL4szGpyfkYnoqKSx/HX4zIeX8uxkQujlqOoDV9KIufJgjseHC8dWIznDgMJ/4tF7O7HrS2m7HNMazGqJTwgomemLEbTslFJrDjRuxSPv6Mst3RMI5CbByFZN3qaOyeRGj/mKZ5HJZcSHjBRG/rSma7oyF2ycP+ck2Sh/3fX+0v+v31rkm/AF2cC8RZbsFmAAAAAElFTkSuQmCC" alt="Three data silos, each containing blocks for &quot;files table&quot; and &quot;file comments table.&quot;" width="804" height="536" data-lqip="true"><img data-loading="true" width="804" height="536" loading="lazy" alt="Three data silos, each containing blocks for &quot;files table&quot; and &quot;file comments table.&quot;" src="https://cdn.sanity.io/images/599r6htc/localized/53ad07418b852f047bf1f19719c63dbf2d211cc3-2160x1440.png?w=804&amp;h=536&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/53ad07418b852f047bf1f19719c63dbf2d211cc3-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 402w, https://cdn.sanity.io/images/599r6htc/localized/53ad07418b852f047bf1f19719c63dbf2d211cc3-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 603w, https://cdn.sanity.io/images/599r6htc/localized/53ad07418b852f047bf1f19719c63dbf2d211cc3-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format 804w, https://cdn.sanity.io/images/599r6htc/localized/53ad07418b852f047bf1f19719c63dbf2d211cc3-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 1206w, https://cdn.sanity.io/images/599r6htc/localized/53ad07418b852f047bf1f19719c63dbf2d211cc3-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1608w"></p></div></figure></div><p>Once we picked our sharding keys, we needed to ensure that there would be an even distribution of data across all backend databases. Unfortunately, many of the sharding keys that we had picked used auto-incrementing or Snowflake timestamp-prefixed IDs. This would have resulted in significant hotspots where a single shard contained the majority of our data. We explored migrating to more randomized IDs, but this required an expensive and time-consuming data migration. Instead, we decided to use the hash of the sharding key for routing. As long as we picked a sufficiently random hash function, we would ensure a uniform distribution of data. One downside of this is that range-scans on shard keys are less efficient, since sequential keys will be hashed to different database shards. However, this query pattern is not common in our codebase, so it was a trade-off we were willing to&nbsp;live&nbsp;with.</p><h3 id="the-logical-solution"><a href="#the-logical-solution">The “logical” solution</a></h3><p>To de-risk the horizontal sharding rollout, we wanted to isolate the process of preparing a table at the application layer from the physical process of running a shard split. To do this, we separated “logical sharding” from “physical sharding.” We could then decouple the two parts of our migration to implement and de-risk them independently. Logically sharding gave us confidence in our serving stack with a low-risk, percentage-based rollout. Rolling back logical sharding when we found bugs was a simple configuration change. Rolling back a physical shard operation is possible, but it requires more complex coordination to ensure data consistency.</p><p>Once a table is logically sharded, all reads and writes will act as if the table is already horizontally sharded. From a reliability, latency, and consistency perspective, we appear to be horizontally sharded, even though the data is still physically located on a single database host. When we are confident that logical sharding is working as expected, we then perform the physical sharding operation. This is the process of copying the data from a single database, sharding it across multiple backends, then re-routing read and write traffic through the new&nbsp;databases.</p><div><figure><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAACiUlEQVQ4jZWTTU8TURSG+3cIYWdiJMbfIAUJtCq00iIfBQora6gzDEPLtDOdGQq6Q01QpGpLwYBEiLjSjQjMFPoxLU3EBf6I15xTUsPGxMWbe3Pe5zzJ5GZc1sUSrJ9pWBUDtq3BPkzCPkg0zoIGq2rAOk83UjV4doWxNd5lx8USXMfnadhf4yhnonBeRFBZjqC8/AiV5xE4Lx+j9P4JrG9xDt1pRh0zyxHeoV1ykMt1dGaglBfwIzGOjBzAYtwPM+7HUsyP3GwQViqM0oaI4oaI41SYZ9QRQ2xGDuAwMc4OcrmOagbKWQH5xBC6kz1o1TvQot9Gm+6GT/FgVwuhmhXgZAV80kLwKb1ou2SI7U72IJ8YRiUrgFwsdHIittQR+JIeXE914prmRnuqC6OJu/iij6OaEzn7+hjP2i8ZYv1JD7bVUXY0hfWcCEcJY0MKYlHqhy714Znkw87MIGraFGrrYiPqFD7ODHJHDLGbUhCOMsmOpvAsJ6KgTGBldgCyfB+CfA9xuQ/vpABK2iRqOZFD97dSALFLhtiV2QGcKBPsaAorOQHr6jDupHrRuuBGi9mBtoVO+FQv9vSx5ifv6iH0q17uiGk13ehO9SKvjrCjKaRH2VKG8UDx4KbWiRuqG7e0LozNe7GvhvhBKJ/VEELzXu6IIXZA8fDu30epmyhtirC1MHbmHmI1FsCruQDWYkHsyUM4MadQ/CBy6L4rD+FNLMjM6lyAdwpamB3kch3/SsP+rqC8KcJZi8JZnYbzerpxZqIobkuwDhKc4vYMz64wa1HeJQe5XPzrkfTMRKFi4LRs4LSk46RsoOAYsOom95y6yTPqiCGWdmiXe/r17N9PQbH+kf9h/gBtoOkqRHektgAAAABJRU5ErkJggg==" alt="Two data silos with blocks for logical and physical shards." width="804" height="536" data-lqip="true"><img data-loading="true" width="804" height="536" loading="lazy" alt="Two data silos with blocks for logical and physical shards." src="https://cdn.sanity.io/images/599r6htc/localized/fc28f4926b31d5e5b9cc97a8fc622ac5788ca865-2160x1440.png?w=804&amp;h=536&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/fc28f4926b31d5e5b9cc97a8fc622ac5788ca865-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 402w, https://cdn.sanity.io/images/599r6htc/localized/fc28f4926b31d5e5b9cc97a8fc622ac5788ca865-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 603w, https://cdn.sanity.io/images/599r6htc/localized/fc28f4926b31d5e5b9cc97a8fc622ac5788ca865-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format 804w, https://cdn.sanity.io/images/599r6htc/localized/fc28f4926b31d5e5b9cc97a8fc622ac5788ca865-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 1206w, https://cdn.sanity.io/images/599r6htc/localized/fc28f4926b31d5e5b9cc97a8fc622ac5788ca865-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1608w"></p></div><figcaption>Four logical shards on two physical shards</figcaption></figure></div><h3 id="the-query-engine-that-could"><a href="#the-query-engine-that-could">The query engine that&nbsp;could</a></h3><p>To support horizontal sharding, we had to significantly re-architect our backend stack. Initially, our application services talked directly to our connection pooling layer, PGBouncer. However, horizontal sharding required much more sophisticated query parsing, planning, and execution. To support this, we built out a new golang service, DBProxy. DBProxy sits between the application layer and PGBouncer. It includes logic for load-shedding, improved observability, transaction support, database topology management, and a lightweight query&nbsp;engine.</p><div><figure><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAACJ0lEQVQ4jW2Sa1PaQBSG/fO1Eq3KLXibWhG1hVbH6bSOVju1dZRbjOzZBLAVUQRBfsTT2QQBLx/eycnmPc+eSya8fopx6QfztNGdRTwnR333hH9ZJ5CJPScbfjOewPtUEyHEHsAeTTa6m0L+rqJLOeT3Dup4GynkwrPuwPPwNDcEPoyBxmTO5G4BkQz6ZCeAiruF3C4Nk0feVBgboO7bgxdjMmAb6SWpdBNc3NiI8xH/YB9/7wCd/0ylsUTlPoHqJcNC+qM8oycV6l6KSjuJ04xRbkYpN2I43hJOaQ2nuIajlilfxcNv1zHcVgLphhW+bLmziK6nqZQ3Oc9ncOU9btPGvYsHiYFMfJvA9VdwixncwgbiraPvll8B3qzgF7ap7h3ifz/Ey28jzZVRW8bXt1GtBbzzLNX9H/jfjvD+7KIbq8ONj4DtRbSXQfI51FkW0ebmwQIeh9+3kc4CupZGilnUaRapbKBbYxUO196zqVzFcUozlM8sLmpR9H1Y3ePAg7hnoxpxnPIMpVOLi+o80gkX5JmlDIGdBFKwkM03SHoSObaQ2xg6+D1CYOC7T6KcafSnSXT6LepoCrmODsYyDmzF0YcRarF31Oei+F+nkYYxPgO24+hfEWr2DJezMfQXC3U59wqwnUBOLfz1KaofLPRPC7l5pULTSXEaf3OK6qqFHERQwcXPgWY2V/OokoUUIqj6XNDeC2AwwyjiWEg+gqrOhjMc+P4D9RhT0MygMvMAAAAASUVORK5CYII=" alt="The application layer flows to DB Proxy, PG Bouncer, and then to the database." width="804" height="536" data-lqip="true"><img data-loading="true" width="804" height="536" loading="lazy" alt="The application layer flows to DB Proxy, PG Bouncer, and then to the database." src="https://cdn.sanity.io/images/599r6htc/localized/2af44a9d935f95fa08d003b61041270b30d1bb07-2160x1440.png?w=804&amp;h=536&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/2af44a9d935f95fa08d003b61041270b30d1bb07-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 402w, https://cdn.sanity.io/images/599r6htc/localized/2af44a9d935f95fa08d003b61041270b30d1bb07-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 603w, https://cdn.sanity.io/images/599r6htc/localized/2af44a9d935f95fa08d003b61041270b30d1bb07-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format 804w, https://cdn.sanity.io/images/599r6htc/localized/2af44a9d935f95fa08d003b61041270b30d1bb07-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 1206w, https://cdn.sanity.io/images/599r6htc/localized/2af44a9d935f95fa08d003b61041270b30d1bb07-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1608w"></p></div><figcaption></figcaption></figure></div><p>The query engine is the heart of DBProxy. Its main components&nbsp;are:</p><ul><li>A query parser reads SQL sent by the application and transforms it into an Abstract Syntax Tree&nbsp;(AST).</li><li>A logical planner parses the AST and extracts the query type (insert, update, etc) and logical shard IDs from the&nbsp;query&nbsp;plan.</li><li>A physical planner maps the query from logical shard IDs to physical databases. It rewrites queries to execute on the appropriate physical&nbsp;shard.</li></ul><div><figure><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAeCAYAAAAsEj5rAAAACXBIWXMAABYlAAAWJQFJUiTwAAAEV0lEQVRIiaVW+1vaSBTln69aFV9AIqhrqVrr1m0Rl/oCVOQNSWbyAELCQ+Ql9X84+80EahC23e/bH+43yeTcM/eeuXMnHn0kYJ6Rvh+Fxjqylhc5y4uMuYq8vQb5aQvac2Cujz4S4FGHAQ4qtzdRam2g1HaMkSTIIq7L73AjL+BaeockXULG9KI4xpUfNyF1trg/6fkdwkkk6doqUpVlPFRW8FB17FZbQoIu8vFOX+Lf07XX7yzqnOVFvrHGibVnAR4WPun5+CpSZ/OnlVqbKDY2UGhsoNjYRKnpsraDYT5yl0XngzpwZPAYP0RMTB+xUYDaF6DUdkDlQ2jSZ6jSMTf2TMkBFDsIbShwrD5i4yuHxy2oMRKhM8LONvTCKdqXebSiJdS/ZWFH8niKyWjHM1CVI9BegBMazMe9KXMJH4Oops/xeEpR/UMCCWWh7xbQOtTRiRJopT9BuwGONd4SusN1E2rprzC/5GCE81A/ZKCFs6gdFFGPZEBLn6YIDXfKbzXkhF0RihKGdncGI/EdejzGzUjEoKW+QdH3oPadlPXfETIQ2376JEJt7YDYIRS0AAp6ANTegdoO8QX1Z+E/EP6Y7NpEkwDk7ibujEXcV5ag9LZe9eYBCE7a/0boCDyOciiAtLchV3ZQkIMoykHI1RBoR5wiNH6b8lhH2hWgKB9RiV/CvLhF9XsCWuIctLIPndfgWPPRLwjdO006AmjuBPbXAqxDBeaBhFokBZUcQRu6NRdnCad1cUUoHaB6dQPzPIVq9A7adQxE33edEvE19beEbzVhTrQZBDX2QWgYCv0AUtlzTtFPInHa152y+wNrGHTgB+37IXe2eFfJ1r28GbB51ghYab0lM+ZFyDpHobGGbN1pTaxdJcgCknQBGXOFN9ystYpia52T6/MinLywVQvNddwb75FhhLbTrafMZt2b9cNl3mT1518RDgPcIW2uQOn5eOqso7OFmDnPAShdHydljVV7DkwTuneYOfD7o77Kdaq8zJYFm1MHficDy8t93JUy3hSBA5kmLJVU5T1/Nl7cJTU+Zi8iSN/HZWH6qsPphT08lYEAbSBAefIhpS8jZSxDefLz0mHGSVnaY5zc8eFec+4f0vU782OcR7ZFEHUfVDkEkQ8gFcOQSmH+TulHKPUQvxLYyVGMPX4FTHDyBEcOoVR2QToBeMrlfVSurlE/y6AeScOKZmGf59COlWBfpKHmT6C02R0TgpGMwYpmONaOZtE4z6H1dxHtiwKMVBSSKcIj5Q5QP0uj8YnAOpLROCZofVbRPzXRiVDoDxEoTRGKsQvzMonWCeXY5jFF+0RF77SGQaQGMxFHubINT5nsQr+LonYdR+36hpt5E4eduEUteQUiHYI8ClCsILTMXzDjcf6d4dloxW9hJRPQcl8gWeyifxRArBBofW/GSH2H90QmutoTQBpBPq/OYHdBmkGO4XWojZyCnbHR7D/MXJzrX2e2H/5P+wdOtrQfyJNcZwAAAABJRU5ErkJggg==" alt="A query that flows to &quot;where&quot; and then leads to &quot;shard_key&quot; and &quot;other_col.&quot;" width="528" height="792" data-lqip="true"><img data-loading="true" width="528" height="792" loading="lazy" alt="A query that flows to &quot;where&quot; and then leads to &quot;shard_key&quot; and &quot;other_col.&quot;" src="https://cdn.sanity.io/images/599r6htc/localized/1547e54a020e816921f740b12b71f2b64f415652-1056x1584.png?w=528&amp;h=792&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/1547e54a020e816921f740b12b71f2b64f415652-1056x1584.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 264w, https://cdn.sanity.io/images/599r6htc/localized/1547e54a020e816921f740b12b71f2b64f415652-1056x1584.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 396w, https://cdn.sanity.io/images/599r6htc/localized/1547e54a020e816921f740b12b71f2b64f415652-1056x1584.png?w=528&amp;q=75&amp;fit=max&amp;auto=format 528w, https://cdn.sanity.io/images/599r6htc/localized/1547e54a020e816921f740b12b71f2b64f415652-1056x1584.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 792w, https://cdn.sanity.io/images/599r6htc/localized/1547e54a020e816921f740b12b71f2b64f415652-1056x1584.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1056w"></p></div><figcaption>Query parser</figcaption></figure><figure><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAeCAYAAAAsEj5rAAAACXBIWXMAABYlAAAWJQFJUiTwAAAFQklEQVRIiZ2W21MTZxjGuac31RGv+id0bO1FO+302kMHtNOpClXbau2U2k4dL+pYbbHioYKKQJIlEZBDAgioIAYQAwQSQkIOe8rmyFkion/Er7MbiFC5YHrxzJfN+77PPt/7Pe/u5mmvBdYQfaWvFqKvLKjLAmqmbhMIqC+zOdkaS65eR95mhMqSgKQ0EZ18SGyiF22ix1h1qIFOpFS9QfqmRticUHslENUJF6xo3l5me4Ik2gNEHRMk2v3MdYtMO70okgN1WSe0bJFw3kZ8rJ/p9giK1U+kzot6N0C6SWb6YQA10mG0ZOuEC1ZUz0MS3V40hw/N4SXW5iPePonWO4wstm5NYXSthxkBSb2HGuhCmegmPNpBZOw+qq8bOdiOlL6LurKFHkZXg/rJKRkL8pyVUKwOt/8O3lANYsqKvFhnqNvSKWu5LWQJRbURcayL4EAXoaddiL42pGkb0Zw6i5H7FuH6u0VX+ygvCEjeTpLdPpL2IHF7AO2xC0lpXmcZy1tK8zaSWXIHIy8KyJP3SfV4SXdOkbzvR3M+Q1abNngw+h9Sg3A91JdmxEwtwdlq/KLA1HgrU8MOAsN2/BONBGImwgvVSJla1BXzhlqDcP3+dbKpuSo8iUo88QrGtUrccgUjkRsGxpQKPLFszJuoJDh/J3fauR4aqlbMBvQEl3SVUfU6gdnbBOerCC68wZR+PV+Ff+YWI/I1RpRrhBerc/WGQmnRgpi0IcUamBKtuL01+IIWRK0BMV6PNCegLFuQlyyIqWyeGG3AGzDjnqghKNuQ4g2IaStyxkyeXqiO9JIYGCHRP0LcOUJyYJSZoXFSIy7kiB1x0Wx4MOrpIfV0lOTgKIn+LKafjjPjGiPqe0A4LZAnia3Ee92k7RHSDpHZdpm5DoWlrjgLvZIxHZE5ExGt3rjpbIfMjENirl1h4X6UTHeS5Z4kSZfLGII83bzq0BMSj8cMJPvGST3xMNs/SXrIjRxqQ9IVJm1o7j7STg+pPk82r8/DTL+P2UGfoT6c1BXOC4ixBmS1BUVtRVFbDPOKcjNhpZFQSiCyVEt43kRIsxGRmoyYrOj5LdmaaAtSsh75+aoP9aPXzapDemHiWaqS5sjfWIN/YQuVcTechf7bFiyjU77ChG6Z1RoDK5v4MPZaIJSpwTL1JwcfneLTtmN85jhqrFkc5fOO4/w48As98etvPRi0NR+uH5/AYjVXPL/zfksx75qL2G4qYrvlANvNRWwzFVFQd5B93SdoU8pRXpo3jF9u9Iw5XFUYydRSHy7jmLOU/Q9Osr/7BHs6v2Nv5/fse3CCwkc/cNZ1hifJG5sr1NaefytmxGUToaUaXOlKOpRyWqXLNAbLMLnPIXjO0xS5hF26zCPtGt7520Re1CK9NOX6l+uhTjY8c9NQdsd/wUCV/w+q/Beo8Jzj0sAZyofOcmvivPHfWqwmcJF2pZzJxTtZwterhLoynezAg5Psbi7mIx0tJcb6YeNhdtUf4oOGw+y+dyQbay5md1Mxn9i/oXTwV/pXt6+3zSDUpVcFLrDLXkK+pYh3dJgLya/aT/4/e8iv2EP+zb3k39xHfvUX2Zi5kG22LznYc4qHsWvGU3wDYU3wIh93fUtB6yF22g+xs/lrCqxfsqO6kB01hewwF1EgHKSg8St2th4y8t5zHOGws5Se2PWNhPKyic7oVX5znaF44GdKBk9Toq/OUkr6fnqDJ6WU9GfjxYOnOT54mnLvOYanb27csn4x9byaoXQlzsQNnMmtQe+de/aWscO3fKi/SzTjnfJ/sPahYOFfuepLpFHuVXQAAAAASUVORK5CYII=" alt="A query that flows to &quot;where&quot; that then leads to &quot;shard_key&quot; and &quot;other_col.&quot; Those then flow to &quot;logical select.&quot;" width="528" height="792" data-lqip="true"><img data-loading="true" width="528" height="792" loading="lazy" alt="A query that flows to &quot;where&quot; that then leads to &quot;shard_key&quot; and &quot;other_col.&quot; Those then flow to &quot;logical select.&quot;" src="https://cdn.sanity.io/images/599r6htc/localized/f01e5c7bb39a8b73ef5f44c4d944ebb4d1ab5872-1056x1584.png?w=528&amp;h=792&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/f01e5c7bb39a8b73ef5f44c4d944ebb4d1ab5872-1056x1584.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 264w, https://cdn.sanity.io/images/599r6htc/localized/f01e5c7bb39a8b73ef5f44c4d944ebb4d1ab5872-1056x1584.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 396w, https://cdn.sanity.io/images/599r6htc/localized/f01e5c7bb39a8b73ef5f44c4d944ebb4d1ab5872-1056x1584.png?w=528&amp;q=75&amp;fit=max&amp;auto=format 528w, https://cdn.sanity.io/images/599r6htc/localized/f01e5c7bb39a8b73ef5f44c4d944ebb4d1ab5872-1056x1584.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 792w, https://cdn.sanity.io/images/599r6htc/localized/f01e5c7bb39a8b73ef5f44c4d944ebb4d1ab5872-1056x1584.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1056w"></p></div><figcaption>Logical planner</figcaption></figure><figure><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAeCAYAAAAsEj5rAAAACXBIWXMAABYlAAAWJQFJUiTwAAAErklEQVRIiY2US1CbVRTHu0ZxxlEctS4qK1fOuGh14cZpbasjtaXgCLVjuxKppdhaq9WqA6W0Shte+b4EGpRHCZBUCBECLUJCAiTk+b1CE0hKEhIIYaYLF7r7O/cmgdQm4OKfc+459/5yz733O7ukBAtxnUHaCmsshKgCQlQJMaqkdkuKLcUUdK6YsZbYXckBCynBgI8xcHtvQ7BqIFqGIM4MQpoZoiJ+eixaBsFZ++Dxt0NYSwPZx4FiggEXYeCxqeHXzcCvdsCnnsdSvwP+PjvVUr+TarHPAa9uCh5XN/hVAmRyAzmbGovDs3QhgQQGCMRBxw81bgQ1bgT6nVgYNoJzZwGS2rOVnC6VSDD/TkV876yOWs62bcnM5sGSf+QiLBUfUcC9LMeMJMOsVwb3MkNjNL/CQFhlnrwU6pCSyU7jDPgVBYSwEkK4jcodZGHmZLDwjfA8VECIJON8WAk+ytI1T5ScBnJhFh5nN8RpHSSjHpJJD9Goh+f+ELiJIeqnY8L0INyeTvArbC6gHJ5AG8RJPZY0diyqHZuSuueo/Gp7Ur12+LVWCOZB8CElvVDpMSA9Azm4pXb4pyYQH1xEXBtEtN+HpS4OLoUZHuUMgj0CYgN+mlsdegCfZRRCqG17YMA0ib8MUfw9msD63SD8nW4KE1U2hHolPNJF8I9hA48My/BZDOCzAlNn6Akq4TXrERvjsD62gPCwGz7tLBYGLPBp5xAYnEd0hEdi/AEiY06Is3fBhxQUmPVSyAFz0q8QXf3wurSQnAMQM0TGkktDc4KrD54FFYRYrkuhQdIcyPtiM6T4z5ilEGrXsrxD8pP50r1EGyklsiid20jOlzJgGZ9eMiDEGTijLZgLNWJ2B1nDjXDFWnK1r+TAHm1Gt6cGP5ku4tupC7hszC6SqzNfglaso9CsQFLO9PItfDXxJV7/7WPsaT+OwtslKFSV4lVVSUqlKOwoxR5VCd688wmFWiNNdG1WoGn5FqrvncNrqo/wEnsUuxXHsFt5LGnTflsxXlYW442uMtRMX4Q1nANIrG2lCe2OH3HhfjUqx8/izHgVVaXhLCoNX+AMid2rQuV4Fb6ZPI9erhbOXCUTy8flmF9pxnRIRndrCskwFbwJg1QPg/c69U2pnDkkgyPaTC8yK1DMeAL8mhzcaiu4NTkc4SZMCfUwitfhXGmmMZIjcx674fUsOyTWsizDHa4WcvsVtNqvoNn6HX4xfo0G0yW02L6nMcbxAzRiHWyRps3NZC1ZiMuhla7h1Ggl3lKfxL7uE9jXVY69Ke3rLMfenhN4e+BTVE1UY8z/M91ZTiApo4urwf67p/Es+yHyG95Dfv0h5N84hPzrh5B/7SCevvU+CjqOo2SkArqFekjx7YBxOTq5GryjOYVnWouQV38QeTX7kXf1APLq3sVTtQeQ13AYz6mKUfzHZ9B5/wewh6vFYc1pvMgeRUFzEQpkHyRtyxG80FSE51uP4JWOEpTpKzC80w7JExjx3cDlyfM4oa9A2XBKxNdXoDwVOznyOa6aL2Ey0LD9GRLriLbgz0ADBefSqO8GjMGbcMdaU8CtBpPRvlItiLSx9LvaUakWltG+/gU6NlRL0/vzmgAAAABJRU5ErkJggg==" alt="A diagram showing the logical select plan that leads to logical single shard plan; then a physical select plan that leads to physical single shard query." width="528" height="792" data-lqip="true"><img data-loading="true" width="528" height="792" loading="lazy" alt="A diagram showing the logical select plan that leads to logical single shard plan; then a physical select plan that leads to physical single shard query." src="https://cdn.sanity.io/images/599r6htc/localized/451db126da5358fff366445be34f74bf5bdb6a2c-1056x1584.png?w=528&amp;h=792&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/451db126da5358fff366445be34f74bf5bdb6a2c-1056x1584.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 264w, https://cdn.sanity.io/images/599r6htc/localized/451db126da5358fff366445be34f74bf5bdb6a2c-1056x1584.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 396w, https://cdn.sanity.io/images/599r6htc/localized/451db126da5358fff366445be34f74bf5bdb6a2c-1056x1584.png?w=528&amp;q=75&amp;fit=max&amp;auto=format 528w, https://cdn.sanity.io/images/599r6htc/localized/451db126da5358fff366445be34f74bf5bdb6a2c-1056x1584.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 792w, https://cdn.sanity.io/images/599r6htc/localized/451db126da5358fff366445be34f74bf5bdb6a2c-1056x1584.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1056w"></p></div><figcaption>Physical planner</figcaption></figure></div><div><p>Think of <strong>“scatter-gather”</strong> like a database-wide game of hide-and-seek: You send out your query to every shard (scatter), then piece together answers from each (gather). Fun, but overdo it, and your speedy database starts feeling more like a snail, especially with complex queries.</p></div><p>Some queries are relatively easy to implement in a horizontally sharded world. For example, single-shard queries are filtered to a single shard key. Our query engine just needs to extract the shard key and route the query to the appropriate physical database. We can “push down” the complexity of the query execution into Postgres. However, if the query is missing a sharding key, our query engine has to perform a more complex <strong>“scatter-gather.”</strong> In this case, we need to fan out the query to all shards (the scatter phase) and then aggregate back results (the gather phase). In some cases, like complex aggregations, joins, and nested SQL, this scatter-gather can be very complex to implement<em>. </em>Additionally, having too many scatter-gathers would impact horizontal sharding scalability. Because the queries have to touch every single database, each scatter-gather contributes the same amount of load as it would if the database was&nbsp;unsharded.</p><p>If we supported full SQL compatibility, our DBProxy service would have begun to look a lot like the Postgres database query engine. We wanted to simplify our API to minimize DBProxy’s complexity, while also reducing the work required for our application developers, who would have to re-write any unsupported queries. To determine the right subset, we built out a “shadow planning” framework, which allowed users to define potential sharding schemes for their tables and then run shadow the logical planning phase on top of live production traffic. We logged the queries and associated query plans to a Snowflake database, where we could run offline analysis. From this data, we picked a query language that supported the most common 90% of queries, but avoided worst-case complexity in our query engine. For example, all range scan and point queries are allowed, but joins are only allowed when joining two tables in the same colo and the join is on the sharding&nbsp;key.</p><h3 id="a-view-of-the-future"><a href="#a-view-of-the-future">A view of the&nbsp;future</a></h3><p>We then needed to figure out how to encapsulate our logical shards. We explored partitioning the data using separate Postgres databases or Postgres schemas. Unfortunately, this would have required physical data changes when we logically sharded the application, which was just as complex as doing the physical shard&nbsp;split.</p><p>Instead, we chose to represent our shards with Postgres views. We could create multiple views per-table, each corresponding to the subset of data in a given shard. This would look like: <code>CREATE VIEW table_shard1 AS SELECT * FROM table WHERE hash(shard_key) &gt;= min_shard_range AND hash(shard_key) &lt; max_shard_range).</code> All reads and writes to the table would be sent through these&nbsp;views.</p><p>By creating sharded views on top of our existing unsharded physical databases, we could logically shard before we performed any risky physical reshard operations. Each view is accessed via its own sharded connection pooler service. The connection poolers still point to the unsharded physical instance, which gives the appearance of being sharded. We were able to de-risk the rollout of sharded reads and writes gradually via feature flags in the query engine and roll back at any time within seconds by just rerouting traffic back to the main table. By the time we ran our first reshard, we were confident in the safety of the sharded topology.</p><div><figure><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB2UlEQVQ4jY2T208TQRyF+RMJ0QdNKsRLKDwUNdo0rmzpdsvSgmJLWx8lJD4IMVEuLWlCGlMlERNotzu9YIDS1vsL+g98ZmaLVq2Fh5OZnN85XzKTmQHx7TGncr6m3fWX0sr7Q9LrzLs7oqOBf2Cy9CGJOExQrT+iWpmn4jxUknvpyZnMdHfEf4GfUjh7s9RXgxw+1The0mgvumouacqrrQZVRmZFP6A6SiuJnQvxbs5LQfewow9zEPQq7egjFCY9vJ0dpZSbwmktuJ1eQNEBitYCdkbnVWSE54EhXt67xJ7upaiPsaZdZiUwxJZ5heLGJKJ5TuD7TYNGbIK6cZvSzDQilqASS2BHLeqhWxzFfOxnDZV1Ot2+wKNNk5MHfr5bBu34E/bjz5Ta8UV+WAYnc36VOTewmg1Rjo6xPXWDQuQOtmXgWAZvInfZDl6nGB1FZIP9gU4XsJTRyZvDrPgHeRG4wK52laJ2jbXARZb9g+TCHnY37ve/Q+f02bSTlPMm5dRNbMtLdXqcpuWjZfmoWePKKyUnsPNhlRVnAj+ncOrz1F7PcJCL0MiYNNbDrjKm8uRMZmT2bKA8+pc0zsek+86OE4iGK7lXnpzJTL+f8vfg93/urV4dCfwJY2M3IXrZUd4AAAAASUVORK5CYII=" alt="An unsharded databases with views of four different shards." width="804" height="536" data-lqip="true"><img data-loading="true" width="804" height="536" loading="lazy" alt="An unsharded databases with views of four different shards." src="https://cdn.sanity.io/images/599r6htc/localized/654e4ee15b6ee16b9208ad020eb2ae654b34d0a8-2160x1440.png?w=804&amp;h=536&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/654e4ee15b6ee16b9208ad020eb2ae654b34d0a8-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 402w, https://cdn.sanity.io/images/599r6htc/localized/654e4ee15b6ee16b9208ad020eb2ae654b34d0a8-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 603w, https://cdn.sanity.io/images/599r6htc/localized/654e4ee15b6ee16b9208ad020eb2ae654b34d0a8-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format 804w, https://cdn.sanity.io/images/599r6htc/localized/654e4ee15b6ee16b9208ad020eb2ae654b34d0a8-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 1206w, https://cdn.sanity.io/images/599r6htc/localized/654e4ee15b6ee16b9208ad020eb2ae654b34d0a8-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1608w"></p></div><figcaption>By creating multiple views in an unsharded database, we can query the views as if the data was already physically sharded.</figcaption></figure></div><p>Of course, relying on views also introduced added risks. Views add a performance overhead and in some cases could fundamentally change how the Postgres query planner optimizes queries. To validate that approach, we collected a query corpus of sanitized production queries and ran load tests with and without views. We were able to confirm that views would only add a minimal performance overhead in most cases, and less than 10% in the worst cases. We also built out a shadow reads framework which could send all live read traffic through views, comparing the performance and correctness of view versus non-view queries. We were then able to confirm that views were a viable solution with minimal performance impact.</p><h3 id="tackling-our-topology"><a href="#tackling-our-topology">Tackling our topology</a></h3><p>To perform query routing, DBProxy has to understand the topology of our tables and physical databases. Because we had separated the concept of logical versus physical sharding, we needed a way to represent these abstractions within our topology. For example, we need to be able to map a table (users) to its shard key (user_id). Similarly, we needed to be able to map a logical shard ID (123) to the appropriate logical and physical databases. With vertical partitioning, we relied on a simple, hard-coded configuration file that mapped tables to their partition. However, as we moved towards horizontal sharding, we required something more sophisticated. Our topology would change dynamically during shard splits and DBProxy needed to quickly update its state to avoid routing requests to the wrong database. Because every change to topology is backwards compatible, these changes are never in the critical path for our site. We built out a database topology that encapsulated our complex horizontal sharding metadata and could deliver real-time updates in under&nbsp;a&nbsp;second.</p><div><figure><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAACzElEQVQ4jYWT22/TZhiH/T/2itCbMW2MIpCQuJq0TWuK1LV0FErTdEzTxhhS2EmcCjRpO5Ila9eSxKfEPcSJE9tx4qxx3KaFke5BMYUOuODikT7pkx69v1e/V1DcCG8yg3pI/523p/hrc4JUcZxUcYw/5TBr4lmy2UFyueNsZT/HfbyE8zhLQYohKNYsau0aau1blGoUuXwJufRVgFSZIGtMsqJ/zWplkjXjMk+0K6yvzmBkotQyUdz07+wtabSTRTTxF4SiEqOcfUh1bRE9fRPt7jmk2ADSrQHER5+wKo0EMsmZRm5EKJR+wM4k6S5usr9Q4mnCoJdo0UluoIm/IpSXF/D6nwkb/26a+jefURobYGt8AO36Sdb+DgdCuTEdrEEr3aCZzvI8vs1B3Kc37/FfvHMkrGVS7C1YHMQ7PL1v4MeSeDdu4/10G/NeDFGMBlFlZxqlGaGoX8daTrOzVGFvyXyNm85TlGMI1UyK7oJJb75Nd87Bv2fx7GGT5/MuzWQepfBjIOxHVpozKLVraOot9Ox9KrkHAeXsHJvSb8il746E/Qn/nf+HZ49aHMQ9eolt3JT4WtifsB9Zdq4iV8cpGmE2zGE2zTBadRjFGEW2rryKbELch8QRvXgbN/lKOIlUv4rSiCDXJ9Cs89TcD3E7J3D9E9itD9iyz6LYowjllQTbf2iHu6gdYrK7aGBnlpGK37NSOqpNzrhAyR6i5YXY2Qux0w3R9o9htT5Gtb5EKBRvsiXeoZx/8AZ6fo519WdEfTYQZjYuBgV/oofR66fY7rwU+t0Q3s4xnPZHFKwvEOR6FNmapV/wd7CjyPUIOXPqZamrlxHNUfT6GVreILv7x9ndH6TthzDdk6j2MMK7p/cWzbdwLqHZn1JpnKbuDeF4Q9TcU6zb51HqYwhqc4b38v/7bkwjWReRzAso1giqPYJsjiCZY8jOFC8A5ulhgMS77poAAAAASUVORK5CYII=" alt="A topology library that leads to a square (S3) and a cylinder (ETCD)." width="804" height="536" data-lqip="true"><img data-loading="true" width="804" height="536" loading="lazy" alt="A topology library that leads to a square (S3) and a cylinder (ETCD)." src="https://cdn.sanity.io/images/599r6htc/localized/8db2e5fe21409fc4c47fc77909a0f8f8f33383e6-2160x1440.png?w=804&amp;h=536&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/8db2e5fe21409fc4c47fc77909a0f8f8f33383e6-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 402w, https://cdn.sanity.io/images/599r6htc/localized/8db2e5fe21409fc4c47fc77909a0f8f8f33383e6-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 603w, https://cdn.sanity.io/images/599r6htc/localized/8db2e5fe21409fc4c47fc77909a0f8f8f33383e6-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format 804w, https://cdn.sanity.io/images/599r6htc/localized/8db2e5fe21409fc4c47fc77909a0f8f8f33383e6-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 1206w, https://cdn.sanity.io/images/599r6htc/localized/8db2e5fe21409fc4c47fc77909a0f8f8f33383e6-2160x1440.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1608w"></p></div><figcaption></figcaption></figure></div><p>Having a separate logical and physical topology allowed us to also simplify some of our database management. For example, in our non-production environments, we can keep the same logical topology as production, but serve the data from many fewer physical databases. This saves costs and reduces complexity without having too many changes across environments. The topology library also allowed us to enforce invariants across our topology (e.g. every shard ID should be mapped to exactly one physical database) that were critical to maintaining the correctness of our system as we built out horizontal sharding.</p><h3 id="the-physical-sharding-operation"><a href="#the-physical-sharding-operation">The physical sharding operation</a></h3><p>Once a table is ready for sharding, the last step is the physical failover from unsharded to sharded databases. We were able to reuse much of the same logic for horizontal sharding, but there were a few notable differences: Instead of moving data from 1 to 1 database, we were going from 1 to N. We needed to make the failover process resilient to new failure modes where the sharding operation could succeed on only a subset of our databases. Still, many of the riskiest components had already been de-risked during vertical partitioning. We were able to move much faster towards our first physical sharding operation than would have otherwise been&nbsp;possible.</p><h2 id="we-ve-come-a-long-way"><a href="#we-ve-come-a-long-way">We’ve come a&nbsp;long&nbsp;way</a></h2><p>When we started this journey, we knew that horizontal sharding would be a multi-year investment into Figma’s future scalability. We shipped our first horizontally sharded table in September 2023. We successfully failed over with only ten seconds of partial availability on database primaries and no availability impact on replicas. We saw no regressions in latency or availability after sharding. Since then we’ve been tackling relatively simple shards from our highest write rate databases. This year, we’ll shard increasingly complex databases, which have dozens of tables and thousands of code call-sites.</p><p>To remove our last scaling limits and truly take flight, we will need to horizontally shard every table at Figma. A fully horizontally sharded world will bring many other benefits: improved reliability, cost savings, and developer velocity. Along the way, we’ll need to solve all of these&nbsp;problems:</p><ul><li>Support for horizontally sharded schema&nbsp;updates</li><li>Globally unique ID generation for horizontally sharded primary&nbsp;keys</li><li>Atomic cross-shard transactions for business critical use-cases</li><li>Distributed globally unique indexes (currently unique indexes are only supported on indexes including the sharding&nbsp;key)</li><li>An ORM model that increases developer velocity and is seamlessly compatible with horizontal sharding</li><li>Fully automated reshard operations that can run shard splits with the click of&nbsp;a&nbsp;button</li></ul><p>Once we’ve bought ourselves sufficient runway, we will also reassess our original approach of in-house RDS horizontal sharding. We started this journey 18 months ago with extremely tight timeline pressure. NewSQL stores have continued to evolve and mature. We will finally have bandwidth to reevaluate the tradeoffs of continuing down our current path versus switching to an open source or managed solution.</p><p>We’ve made a lot of exciting progress on our horizontal sharding journey, but our challenges are just beginning<em>. </em>Stay tuned for more deep dives into different parts of our horizontal sharding stack. If you’re interested in working on projects like this, please reach out! We’re <a href="https://www.figma.com/careers/">hiring</a>.</p><svg xmlns="http://www.w3.org/2000/svg" width="93" height="13" fill="none"><circle cx="6.5" cy="6.5" r="6.5" fill="currentColor"></circle><path fill="currentColor" d="M39.834 0h13v13h-13zM86.5 0 93 13H80l6.5-13z"></path></svg><p>We couldn’t have shipped horizontal sharding without these current and former databases team members: Anna Saplitski, David Harju, Dinesh Garg, Dylan Visher, Erica Kong, Gordon Yoon, Gustavo Mezerhane, Isemi Ekundayo, Josh Bancroft, Junhson Jean-Baptiste, Kevin Lin, Langston Dziko, Maciej Szeszko, Mehant Baid, Ping-Min Lin, Rafael Chacon Vivas, Roman Hernandez, Tim Goh, Tim Liang, and&nbsp;Yiming&nbsp;Li.</p><p>We’d also like to thank all of our cross-functional partner teams, especially Amy Winkler, Braden Walker, Esther Wang, Kat Busch, Leslie Tu, Lin Xu, Michael Andrews, Raghav Anand, and Yichao&nbsp;Zhao.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A fast HNSW implementation in Rust (109 pts)]]></title>
            <link>https://github.com/swapneel/hnsw-rust</link>
            <guid>39706535</guid>
            <pubDate>Thu, 14 Mar 2024 17:14:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/swapneel/hnsw-rust">https://github.com/swapneel/hnsw-rust</a>, See on <a href="https://news.ycombinator.com/item?id=39706535">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">hnsw-rust - a fast HNSW implementation in Rust</h2><a id="user-content-hnsw-rust---a-fast-hnsw-implementation-in-rust" aria-label="Permalink: hnsw-rust - a fast HNSW implementation in Rust" href="#hnsw-rust---a-fast-hnsw-implementation-in-rust"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Technical Details</h3><a id="user-content-technical-details" aria-label="Permalink: Technical Details" href="#technical-details"></a></p>
<p dir="auto">hnsw-rust is a Rust implementation of The Hierarchical Navigable Small World (HNSW) algorithm. HNSW is a notable advancement in Approximate Nearest Neighbor (ANN) search in high-dimensional spaces, fundamentally altering our approach to these problems. The algorithm constructs a layered graph structure, where higher layers (less dense) are used for rapid global navigation, while lower layers (more dense) facilitate fine-grained local search. This structure mirrors the 'small world' phenomenon observed in social networks, where short path lengths exist between any two nodes (Watts and Strogatz, 1998).</p>
<p dir="auto">HNSW's search efficiency arises from its unique use of a greedy heuristic. It commences from a high layer and iteratively moves to the node closest to the target, transitioning down to denser layers until the nearest neighbors are refined. This method of layer traversal for nearest neighbor search finds its roots in earlier works like Kleinberg's small-world model, which also highlights efficient navigation in sparse, high-dimensional spaces (Kleinberg, 2000).</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/6643641/312931336-72a47b3d-7a7b-49b6-836c-70e2bc6efa98.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA0NzkxMDksIm5iZiI6MTcxMDQ3ODgwOSwicGF0aCI6Ii82NjQzNjQxLzMxMjkzMTMzNi03MmE0N2IzZC03YTdiLTQ5YjYtODM2Yy03MGUyYmM2ZWZhOTgucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDMxNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDAzMTVUMDUwMDA5WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MDhiODM5NjIwZjI1ZWYxMzcwZDZmMDA2MDcwZWYyNGQwMjMwZDlkNTYxNjZkMGQxYTA0YmUyZjQ3NDk1ZTQ5YyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.glnz42tJ1mxnaniRRkRc38K8Ce6dLjOprJFqSf4CN90"><img width="314" alt="image" src="https://private-user-images.githubusercontent.com/6643641/312931336-72a47b3d-7a7b-49b6-836c-70e2bc6efa98.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA0NzkxMDksIm5iZiI6MTcxMDQ3ODgwOSwicGF0aCI6Ii82NjQzNjQxLzMxMjkzMTMzNi03MmE0N2IzZC03YTdiLTQ5YjYtODM2Yy03MGUyYmM2ZWZhOTgucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDMxNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDAzMTVUMDUwMDA5WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MDhiODM5NjIwZjI1ZWYxMzcwZDZmMDA2MDcwZWYyNGQwMjMwZDlkNTYxNjZkMGQxYTA0YmUyZjQ3NDk1ZTQ5YyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.glnz42tJ1mxnaniRRkRc38K8Ce6dLjOprJFqSf4CN90"></a>
<p dir="auto">New nodes are inserted starting from the lowest layer, with their inclusion in each subsequent higher layer governed by a probabilistic threshold. This strategy is influenced by earlier research in dynamic random graphs and scale-free networks, which also deal with node connections based on probabilistic models (Barabási and Albert, 1999). The probability of ascending to higher layers decreases exponentially, a method validated by research emphasizing the balance between exploration and exploitation in search algorithms (Arya et al., 1998).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">References:</h3><a id="user-content-references" aria-label="Permalink: References:" href="#references"></a></p>
<p dir="auto">Malkov, Yu. A., &amp; Yashunin, D. A. (2016). Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs. arXiv preprint arXiv:1603.09320.
Watts, D. J., &amp; Strogatz, S. H. (1998). Collective dynamics of ‘small-world’ networks. Nature, 393(6684), 440-442.
Kleinberg, J. M. (2000). The small-world phenomenon: An algorithmic perspective. Proceedings of the 32nd annual ACM symposium on Theory of computing.
Barabási, A. L., &amp; Albert, R. (1999). Emergence of scaling in random networks. Science, 286(5439), 509-512.
Arya, S., Mount, D. M., Netanyahu, N. S., Silverman, R., &amp; Wu, A. Y. (1998). An optimal algorithm for approximate nearest neighbor searching fixed dimensions. Journal of the ACM (JACM), 45(6), 891-923.
Aumüller, M., Bernhardsson, E., &amp; Faithfull, A. (2017). ANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms. Information Systems, arXiv:1807.05614.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New Beeper Android app – Open beta test (120 pts)]]></title>
            <link>https://blog.beeper.com/p/new-beeper-android-app-open-beta</link>
            <guid>39706471</guid>
            <pubDate>Thu, 14 Mar 2024 17:08:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.beeper.com/p/new-beeper-android-app-open-beta">https://blog.beeper.com/p/new-beeper-android-app-open-beta</a>, See on <a href="https://news.ycombinator.com/item?id=39706471">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e0501b-201f-4f92-a6e0-5c8a23c1e2bb_2000x1125.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e0501b-201f-4f92-a6e0-5c8a23c1e2bb_2000x1125.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e0501b-201f-4f92-a6e0-5c8a23c1e2bb_2000x1125.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e0501b-201f-4f92-a6e0-5c8a23c1e2bb_2000x1125.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e0501b-201f-4f92-a6e0-5c8a23c1e2bb_2000x1125.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e0501b-201f-4f92-a6e0-5c8a23c1e2bb_2000x1125.png" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d5e0501b-201f-4f92-a6e0-5c8a23c1e2bb_2000x1125.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:311035,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e0501b-201f-4f92-a6e0-5c8a23c1e2bb_2000x1125.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e0501b-201f-4f92-a6e0-5c8a23c1e2bb_2000x1125.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e0501b-201f-4f92-a6e0-5c8a23c1e2bb_2000x1125.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5e0501b-201f-4f92-a6e0-5c8a23c1e2bb_2000x1125.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>It’s the day all you Beeper Android fans have all been waiting for…our new app is ready for you to try out!</p><p>Our team has been working hard on this stunner of an app. Forget everything you thought you knew about Beeper on Android. This app is a complete rewrite, built from scratch to be fast, clean and beautiful. We can’t wait to hear what you think.</p><p><span>If you have a Beeper account already, you can </span><a href="https://play.google.com/store/apps/details?id=com.beeper.android" rel="">download it</a><span> now. If you do not have an account, there is a waitlist (for just a bit longer!) or grab an </span><a href="https://help.beeper.com/quick-references/beeper-referrals-how-do-they-work" rel="">invite</a><span> from someone who is already on Beeper.</span></p><div><figure><a target="_blank" href="https://play.google.com/store/apps/details?id=com.beeper.android" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71fcb480-9325-4148-860b-e0a7ac7701a3_3840x2160.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71fcb480-9325-4148-860b-e0a7ac7701a3_3840x2160.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71fcb480-9325-4148-860b-e0a7ac7701a3_3840x2160.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71fcb480-9325-4148-860b-e0a7ac7701a3_3840x2160.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71fcb480-9325-4148-860b-e0a7ac7701a3_3840x2160.png" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/71fcb480-9325-4148-860b-e0a7ac7701a3_3840x2160.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:948902,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:&quot;https://play.google.com/store/apps/details?id=com.beeper.android&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71fcb480-9325-4148-860b-e0a7ac7701a3_3840x2160.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71fcb480-9325-4148-860b-e0a7ac7701a3_3840x2160.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71fcb480-9325-4148-860b-e0a7ac7701a3_3840x2160.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71fcb480-9325-4148-860b-e0a7ac7701a3_3840x2160.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><strong>FAST. Oh boy, is it fast.</strong></p><ul><li><p>Instant chat opens. Instant message sends.</p></li><li><p>All chats are cached locally on-device</p></li><li><p>Full message search</p></li></ul><p><strong>Beautiful new design</strong></p><ul><li><p>Minimal and Pro inbox options - less on-screen info vs more</p></li><li><p>New themes: OLED black and Material You</p></li><li><p>Tablet and foldable dual-pane view</p></li><li><p>Android OS chat bubbling</p></li><li><p>Home screen widget</p></li></ul><p><strong>And lots more…</strong></p><ul><li><p>Set up and modify your chat networks in the app, no desktop required</p></li><li><p>Experimental preview: on-device Signal bridge, now fully end-to-end encrypted</p></li></ul><p>Our first generation Android app was built as a fork of an open source Matrix client (Element Android). This allowed us to get our app up and running quickly, but proved difficult to optimize and improve. The design of our first app was clunky, and didn’t mesh perfectly with Android. We knew we had a lot to fix.</p><p>As all software developers know, there’s always tension between two development paths - rebuild in-place or a full rewrite. We attempted to rebuild as much as we could, but we realized&nbsp;over time that our architecture design goals were fundamentally different from other Matrix clients. While those clients are primarily meant to be Slack or IRC alternatives (designed for large unencrypted group channels), our priority is to build a great consumer chat app for encrypted DMs&nbsp;and smaller group chats.</p><p>Last year, we made the tough decision to begin a complete rewrite of our entire Android app.</p><p>On the interface design side, our design team created a unified design language and re-imagined every surface of the app. We took care to mesh Beeper into Android OS, taking cues from Material design and deeply integrating with native Android features like Chat Bubbles, Material You and dual-pane view for foldables.</p><p><span>The technical architecture has been re-engineered from the ground up for speed and performance. We redesigned how incoming chat messages flow from the internet, through the app, to be displayed on screen. All chats are now cached locally on your phone. We swapped the old Matrix Android library for </span><a href="https://github.com/mautrix/go" rel="">mautrix-go</a><span>, the same open-source Go library that powers all of our Matrix bridges. Similar to the technical architecture of </span><a href="https://blog.beeper.com/p/how-beeper-mini-works" rel="">iMessage in Beeper Mini</a><span>, our new Android app features an integrated on-device </span><a href="https://github.com/mautrix/signal" rel="">Signal bridge</a><span>. More details on-device bridges to come in the future.  The remainder of the Android app is written in Kotlin and Jetpack Compose.</span></p><p>Our goal is to make the app extraordinarily fast and snappy. We’re not done yet, but we’re getting darn close.</p><p>Keep in mind - this is a beta release! There are bugs - please help us improve the app by reporting any issues you spot (⚙️ Settings → Report a problem). We will do our best to fix them. Please send feature requests through the same button!</p><p><strong>Known issues</strong></p><ul><li><p>Initial sync may take up to 8 minutes (for large accounts).</p></li><li><p>Google Messages set up may take 2-3 tries to set up. Will be fixed soon</p></li><li><p>Tapping on message in search results does not open the chat</p></li><li><p>Scrolling up/down in the inbox or chat view may accidentally trigger a sideswipe action.</p></li><li><p>Interacting with full screen image viewer is a bit funky</p></li><li><p>Links in notifications are not tappable</p></li></ul><p><strong>Upcoming features and improvements:</strong></p><ul><li><p>Many UI tweaks and clean-ups</p></li><li><p>Deleting chats</p></li><li><p>On-device iMessage bridge (like Beeper Mini) is not enabled at this time. You may continue to use Beeper Mini alongside this app</p></li><li><p>WhatsApp and Google Messages on-device end-to-end encrypted bridges</p></li><li><p>Context menu for pinned chats</p></li><li><p>Tap other peoples name/avatar in group chats to send a DM</p></li><li><p>Network disconnection alerts</p></li><li><p>Android Auto</p></li><li><p>Gallery: sending and receiving</p></li><li><p>Archiving and marking as low priority</p></li><li><p>Viewing list of who has reacted to a message</p></li><li><p>Scheduled send</p></li><li><p>Mentions (mentioning folks in chats with @)</p></li><li><p>lots more!</p></li></ul><p><span>You need to have a Beeper account to use Beeper Android at this time. Get an </span><a href="https://help.beeper.com/quick-references/beeper-referrals-how-do-they-work" rel="">invite</a><span> or hang out on the waitlist for a bit longer!</span></p><p>We’re super proud of this app. It’s been a massive effort from the team. Many people have reached out to ask us how they can financially support Beeper. We’re excited (for our own sake) to say that we will be activating paid subscriptions in the very near future.</p><p>Eric + Beeper Team</p></div></article></div></div>]]></description>
        </item>
    </channel>
</rss>