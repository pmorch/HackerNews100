<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 13 Oct 2024 23:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[CRLF is obsolete and should be abolished (237 pts)]]></title>
            <link>https://fossil-scm.org/home/ext/crlf-harmful.md</link>
            <guid>41830717</guid>
            <pubDate>Sun, 13 Oct 2024 19:16:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fossil-scm.org/home/ext/crlf-harmful.md">https://fossil-scm.org/home/ext/crlf-harmful.md</a>, See on <a href="https://news.ycombinator.com/item?id=41830717">Hacker News</a></p>
Couldn't get https://fossil-scm.org/home/ext/crlf-harmful.md: Error: Parse Error: Invalid header value char]]></description>
        </item>
        <item>
            <title><![CDATA[Making the Tibetan language a first-class citizen in the digital world (122 pts)]]></title>
            <link>https://www.bdrc.io/blog/2024/10/10/tech-innovations-to-make-the-tibetan-language-a-first-class-citizen-in-the-digital-world/</link>
            <guid>41829926</guid>
            <pubDate>Sun, 13 Oct 2024 17:43:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bdrc.io/blog/2024/10/10/tech-innovations-to-make-the-tibetan-language-a-first-class-citizen-in-the-digital-world/">https://www.bdrc.io/blog/2024/10/10/tech-innovations-to-make-the-tibetan-language-a-first-class-citizen-in-the-digital-world/</a>, See on <a href="https://news.ycombinator.com/item?id=41829926">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
								<article id="post-11685">
			<div>
					<p>
						
                        						<h2 itemprop="name"><span itemprop="dateCreated">October 10, 2024</span></h2>
						
					</p>
				</div>
			<div>
        <div><p><img decoding="async" itemprop="image" src="https://www.bdrc.io/wp-content/uploads/2024/10/0001-scaled-1.jpg" alt=""></p><p><span></span><i><span>Om Mani Padme Hum mantra scroll printed by Schilling von Canstadt in 1835 in St Petersburg, for use in monasteries in Buryatia. </span></i></p></div><div>
			<p><span>An important part of BDRC's mission is technological innovation to make the Tibetan language a first-class citizen in the digital world. A major milestone in that enterprise was passed recently, as the open source software suite LibreOffice now supports an important feature of Tibetan: very long paragraphs.</span></p>
<p><span>Tibetans have invested tremendous energy and resources into writing, innovation, and technology since the introduction&nbsp; of Buddhism in the eighth century. Many of you will know the story of how the Tibetan script and classical language were invented for the purpose of translating Buddhist texts from Sanskrit (and Chinese) into a language Tibetans would understand. In the 14th Century Tibetans widely adopted the technology of woodblock printing as a means of mass producing the Tibetan translation of the scriptures and also the thousands of volumes of Buddhist texts written directly in Tibetan by Himalayan authors. Fast forwarding to more recent times, the creation of Tibetan computer fonts was a quantum leap in the evolution of Tibetan and its integration in the digital world. Of particular note was the inclusion of Tibetan in the Unicode Standard, the global standard for the codes that underlie computer fonts for all of the world's writing systems.&nbsp;</span></p>
<p><span>However, written Tibetan has important features that are still not supported by all tools or applications. One that may be surprising is very long paragraphs: in fact, the typographical notion of the paragraph does not really exist in a Tibetan text the way it does in European languages. As a result, Tibetan texts often need to be processed as a long stream of uninterrupted text with no forced line breaks, sometimes over hundreds or thousands of pages.&nbsp;</span></p>

		</div><div><div><p><img decoding="async" itemprop="image" src="https://www.bdrc.io/wp-content/uploads/2024/10/c5MCRRhVCKIYkaOtm9FfAA-copy.jpg" alt=""></p></div><div><p><img decoding="async" itemprop="image" src="https://www.bdrc.io/wp-content/uploads/2024/10/mantra-roll.webp" alt=""></p><p><span></span><i><span>Examples of long Tibetan scrolls of unbroken text that are placed inside prayer wheels.&nbsp;</span></i></p></div></div><div>	


	<div>
			<p><span>In general, word processing software programs were originally designed with English text in mind, meaning that they support relatively short paragraphs (possibly up to a few pages), with words separated by spaces that can have a flexible width. These assumptions don't work for written Tibetan, however, where paragraphs are virtually limitless and contain very few spaces. The result is that such a word processor performs very poorly, if it works at all, when opening a long Tibetan text, and it will therefore be impossible to use for any serious publication project.&nbsp;</span></p>
<p><span>LibreOffice is one of the only mature and stable open source word processors (i.e. inspired by MS Word) that is available for free on all possible platforms, including Linux. It is relevant to Tibetan Studies and the Tibetan community because it is free and open source, while popular commercial software programs such as Word or InDesign which can handle long Tibetan texts, are expensive and in many parts of Asia often used in pirated versions. So long as LibreOffice could not handle long paragraphs there was essentially no free tool to publish Tibetan.&nbsp;</span></p>
<p><span>In 2015, BDRC's CTO, Elie Roux, reported the issue to LibreOffice</span><span>. Unfortunately an intervention in the code of LibreOffice is a big project that would have required weeks of research and development, and therefore there was no evolution on that front. That is until a few weeks ago, when a developer named Jonathan Clark tackled the issue and fixed it! A long text like Longchenpa's Yishindzö (view on the BDRC archive at: <a href="http://purl.bdrc.io/resource/WA00KG02677">yid bzhin mdzod</a>), which consists of one "paragraph" of 153 letter size pages, now opens quickly and can be edited in LibreOffice.</span><span> The conversion of the RDF file of this particular text into a PDF used to stall after 45 minutes and it is now completed in 13 seconds! This is a remarkable improvement of several orders of magnitude, something quite rare in software optimization.</span></p>

		</div> 	

<div><p><img decoding="async" itemprop="image" src="https://www.bdrc.io/wp-content/uploads/2024/10/Untitled-design.png" alt=""></p>
<p><em><span>A long text like Longchenpa's Yishindzö, which consists of one "paragraph" of 153 letter size pages, now opens quickly and can be edited in LibreOffice. (View on the BDRC archive at: <a href="http://purl.bdrc.io/resource/WA00KG02677">yid bzhin mdzod</a>).</span></em></p>
</div></div><div>
			<p><span>BDRC's modest contribution to this breakthrough is just to have planted the seed many years ago, but we want to acknowledge and rejoice in Jonathan's good work, and the consequent strengthening of Tibetan in open source publication tools!</span></p>
<p><span>The support for very long paragraphs was integrated in <a href="https://www.libreoffice.org/download/download-libreoffice/">LibreOffice 24.8.2</a>, released on September 27 2024. We encourage you to try it out and send us comments on your favorite software to edit Tibetan, or the most important shortcomings you experience in this type of tool for Tibetan. Working together as a community, we can continue to innovate and keep Tibetan up to speed with the digital technology of other languages.&nbsp;</span></p>

		</div>
       </div>

			
	    	</article>													
 
 						                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Restic: Backups done right (132 pts)]]></title>
            <link>https://restic.net/</link>
            <guid>41829913</guid>
            <pubDate>Sun, 13 Oct 2024 17:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://restic.net/">https://restic.net/</a>, See on <a href="https://news.ycombinator.com/item?id=41829913">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <h2 id="introduction">Introduction</h2>
<p>Restic is a modern backup program that can back up your files:</p>
<ul>
<li>
<p>from <strong>Linux, BSD, Mac and Windows</strong></p>
</li>
<li>
<p>to <strong>many different storage types</strong>, including self-hosted and online services</p>
</li>
<li>
<p><strong>easily</strong>, being a single executable that you can run without a server or complex setup</p>
</li>
<li>
<p><strong>effectively</strong>, only transferring the parts that actually changed in the files you back up</p>
</li>
<li>
<p><strong>securely</strong>, by careful use of cryptography in every part of the process</p>
</li>
<li>
<p><strong>verifiably</strong>, enabling you to make sure that your files can be restored when needed</p>
</li>
<li>
<p><strong>freely</strong> - restic is entirely free to use and completely open source</p>
</li>
</ul>
<h2 id="quickstart">Quickstart</h2>
<p>A short recorded demo of restic:</p>

<p>To learn more about restic, checkout the user manual:</p>
<ul>
<li><a href="https://restic.readthedocs.io/en/stable/">Manual for restic (released version)</a></li>
<li><a href="https://restic.readthedocs.io/en/latest/">Manual for restic (latest development version)</a></li>
</ul>
<h2 id="installation">Installation</h2>
<p>To install, please follow the <a href="https://restic.readthedocs.io/en/stable/020_installation.html">Installation Instructions Page</a> in the manual or download the latest native binary on the <a href="https://github.com/restic/restic/releases/latest">GitHub Download Page</a>.</p>
<h2 id="compatibility">Compatibility</h2>
<p>Backward compatibility for backups is important so that our users are always able to restore saved data. Therefore restic follows <a href="http://semver.org/">Semantic Versioning</a> to clearly define which versions are compatible. The repository and data structures contained therein are considered the “Public API” in the sense of Semantic Versioning.</p>
<p>Once version 1.0.0 is released, we guarantee backward compatibility of all repositories within one major version; as long as we do not increment the major version, data can be read and restored. We strive to be fully backward compatible to all prior versions.</p>
<p>During initial development (versions prior to 1.0.0), maintainers and developers will do their utmost to keep backwards compatibility and stability, although there might be breaking changes without increasing the major version.</p>
<h2 id="contributing">Contributing</h2>
<p>Contributions are welcome! More information can be found in <a href="https://github.com/restic/restic/blob/master/CONTRIBUTING.md">the restic contribution guidelines</a>. A document describing the design of restic and the data structures stored on disc is contained in <a href="http://restic.readthedocs.io/en/latest/100_references.html#design">the design document</a>.</p>

<p>There are several ways to contact the restic project and its community:</p>
<ul>
<li>If you have <strong>usage or support questions</strong>, please post in <a href="https://forum.restic.net/">the <strong>forum</strong></a>.</li>
<li>If you discover a <strong>bug</strong> or have a <strong>feature</strong> suggestion, feel free to <a href="https://github.com/restic/restic/issues/new/choose">open a <strong>GitHub issue</strong></a>. Please make sure to fill out the issue template you are presented with when doing so.</li>
<li>If you would like to <strong>chat</strong> about restic with other users there is also the IRC channel <code>#restic</code> on <code>irc.libera.chat</code>, which you can <a href="https://web.libera.chat/gamja/#restic">access through your browser</a> if you don’t have an IRC client installed. Please note that support questions are preferably asked in the forum.</li>
<li>For <strong>other project related inquiries</strong> (<strong>not</strong> support requests), feel free to just write an e-mail to <code>alexander@bumpern.de</code> :)</li>
</ul>
<p><strong>Important</strong>: If you discover something that you believe to be a possible critical security problem, please do <strong>not</strong> open a GitHub issue but send an email directly to <code>alexander@bumpern.de</code>. If possible, please encrypt your email using PGP (<a href="https://restic.net/gpg-key-alex.asc">CF8F18F2844575973F79D4E191A6868BD3F7A907</a>).</p>
<h2 id="talks">Talks</h2>
<p>The following talks have been given about restic:</p>
<ul>
<li>2021-04-02: <a href="https://changelog.com/podcast/434">The Changelog: Restic has your backup (Podcast)</a></li>
<li>2016-01-31: Lightning Talk at the Go Devroom at FOSDEM 2016, Brussels, Belgium</li>
<li>2016-01-29: <a href="https://media.ccc.de/v/c4.openchaos.2016.01.restic">restic - Backups mal richtig</a>: Public lecture in German at <a href="https://koeln.ccc.de/">CCC Cologne e.V.</a> in Cologne, Germany</li>
<li>2015-08-23: <a href="https://media.ccc.de/browse/conferences/froscon/2015/froscon2015-1515-a_solution_to_the_backup_inconvenience.html">A Solution to the Backup Inconvenience</a>: Lecture at <a href="https://www.froscon.de/">FROSCON 2015</a> in Bonn, Germany</li>
<li>2015-02-01: <a href="https://www.youtube.com/watch?v=oM-MfeflUZ8&amp;t=11m40s">Lightning Talk at FOSDEM 2015</a>: A short introduction (with slightly outdated command line)</li>
<li>2015-01-27: <a href="https://video.fsmpi.rwth-aachen.de/cccac/4442">Talk about restic at CCC Aachen</a> (in German)</li>
</ul>
<h2 id="blog">Blog</h2>
<p>For more information regarding restic development, have a look at <a href="https://restic.net/blog">our blog</a>. The latest posts are:</p>
<ul>
<li>
<p>05 Sep 2024 » <a href="https://restic.net/blog/2024-09-05/restic-0.17.1-released/">Restic 0.17.1 Released</a></p>
</li>
<li>
<p>26 Jul 2024 » <a href="https://restic.net/blog/2024-07-26/restic-0.17.0-released/">Restic 0.17.0 Released</a></p>
</li>
<li>
<p>26 Jul 2024 » <a href="https://restic.net/blog/2024-07-26/rest-server-0.13.0-released/">REST-server 0.13.0 released</a></p>
</li>
</ul>
<h2 id="license">License</h2>
<p>Restic is licensed under “BSD 2-Clause License”. You can find the complete text in the file <code>LICENSE</code>.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ACF Plugin no longer available on WordPress.org (163 pts)]]></title>
            <link>https://www.advancedcustomfields.com/blog/acf-plugin-no-longer-available-on-wordpress-org/</link>
            <guid>41828958</guid>
            <pubDate>Sun, 13 Oct 2024 15:44:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.advancedcustomfields.com/blog/acf-plugin-no-longer-available-on-wordpress-org/">https://www.advancedcustomfields.com/blog/acf-plugin-no-longer-available-on-wordpress-org/</a>, See on <a href="https://news.ycombinator.com/item?id=41828958">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrap">

			<p>We were saddened and appalled by Matt Mullenweg’s actions this morning appropriating the Advanced Custom Fields plugin that our ACF team has been actively developing for the WordPress community since 2011.</p>

<p>Advanced Custom Fields is a sophisticated plugin with over 200,000 lines of code, which we continually develop, enhance, support and invest in to meet the needs of our users across WordPress.&nbsp; We’ve made 15+ releases over the past two years, since joining WP Engine, and added significant new functionality to the free plugin as well as continually improving performance and our security and testing practices to meet the ‘enterprise grade’ that our users deserve.</p>

<p>The change to our published distribution, and under our ‘slug’ which uniquely identifies the ACF plugin and code that our users trust in the WordPress.org plugin repository, is inconsistent with open source values and principles.&nbsp; The change made by Mullenweg is maliciously being used to update millions of existing installations of ACF with code that is unapproved and untrusted by the Advanced Custom Fields team.</p>

<p>We are directly able to protect WP Engine, Flywheel hosting and ACF PRO customers –&nbsp; you are not impacted and do not need to take any action. You will continue to get the latest innovations and updates from the experts in the ACF team. The ACF code on wordpress.org is no longer controlled by the ACF team.</p>

<blockquote data-width="500" data-dnt="true"><div lang="en" dir="ltr"><p>We have been made aware that the Advanced Custom Fields plugin on the WordPress directory has been taken over by WordPress dot org.</p><p>A plugin under active development has never been unilaterally and forcibly taken away from its creator without consent in the 21 year history of… <a href="https://t.co/eV0qakURLc">pic.twitter.com/eV0qakURLc</a></p></div>— Advanced Custom Fields (@wp_acf) <a href="https://twitter.com/wp_acf/status/1845169499064107049?ref_src=twsrc%5Etfw">October 12, 2024</a></blockquote>

<p>If you have a site managed elsewhere using the free version of ACF, in order to get genuine ACF updates you <a href="https://www.advancedcustomfields.com/blog/installing-and-upgrading-to-the-latest-version-of-acf/">must perform a one-time download</a> of the 6.3.8 version via advancedcustomfields.com to remain safe in the future. After this one-time download you will be able to update as usual via the WP Admin panel.</p>

<p>You can also follow the same process if your site has already been upgraded to the modified “Secure Custom Fields” plugin, to get back to a genuine version of ACF.</p>

<p>Mullenweg’s actions are extraordinarily concerning and pose the grave risk of upending and irreparably harming the entire WordPress ecosystem.&nbsp; His attempt to unilaterally take control of this open platform that we and so many other plugin developers and contributors have relied on, in the spirit of sharing plugins for all, provides further evidence of his serious abuse of trust, manifold conflicts of interest, and breach of the promises of openness and integrity in the community.</p>


			<h2>About the Author</h2>
<div>
	<p><img alt="" src="https://secure.gravatar.com/avatar/30ac8d2fc7fd828088a702342a3f3cea?s=96&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/30ac8d2fc7fd828088a702342a3f3cea?s=192&amp;d=mm&amp;r=g 2x" height="96" width="96" decoding="async">	</p>
	<p>Iain is the Product Manager for Advanced Custom Fields. He has a long history of building and growing WordPress plugins. Moonlights as a PhpStorm evangelist.</p>
</div>

			

							<p><i>For plugin support, please <a href="https://www.advancedcustomfields.com/contact/">contact our support team</a> directly, as comments aren't actively monitored.</i></p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ward Christensen (of BBS and XMODEM fame) has died (184 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Ward_Christensen</link>
            <guid>41828923</guid>
            <pubDate>Sun, 13 Oct 2024 15:39:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Ward_Christensen">https://en.wikipedia.org/wiki/Ward_Christensen</a>, See on <a href="https://news.ycombinator.com/item?id=41828923">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div lang="en" dir="ltr" id="mw-content-text">
<table><tbody><tr><th colspan="2"><p>Ward Christensen</p></th></tr><tr><td colspan="2"><span typeof="mw:File/Frameless"><a href="https://en.wikipedia.org/wiki/File:Ward_Christensen_and_the_First_BBS.jpg"><img alt="Ward Christensen and the First BBS" src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Ward_Christensen_and_the_First_BBS.jpg/220px-Ward_Christensen_and_the_First_BBS.jpg" decoding="async" width="220" height="105" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Ward_Christensen_and_the_First_BBS.jpg/330px-Ward_Christensen_and_the_First_BBS.jpg 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Ward_Christensen_and_the_First_BBS.jpg/440px-Ward_Christensen_and_the_First_BBS.jpg 2x" data-file-width="2048" data-file-height="975"></a></span><p>Ward Christensen and the First BBS</p></td></tr><tr><th scope="row">Born</th><td>October 23, 1945<br><div><p><a href="https://en.wikipedia.org/wiki/West_Bend,_Wisconsin" title="West Bend, Wisconsin">West Bend, Wisconsin</a>, United States</p></div></td></tr><tr><th scope="row">Died</th><td>October 11, 2024 (aged&nbsp;78)<br><div><p><a href="https://en.wikipedia.org/wiki/Rolling_Meadows,_Illinois" title="Rolling Meadows, Illinois">Rolling Meadows, Illinois</a>, United States</p></div></td></tr><tr><th scope="row">Known&nbsp;for</th><td>first <a href="https://en.wikipedia.org/wiki/Bulletin_board_system" title="Bulletin board system">bulletin board system</a> (BBS)<br><a href="https://en.wikipedia.org/wiki/XMODEM" title="XMODEM">XMODEM</a> Protocol</td></tr></tbody></table>
<p><b>Ward Christensen</b> (born 1945 in <a href="https://en.wikipedia.org/wiki/West_Bend,_Wisconsin" title="West Bend, Wisconsin">West Bend, Wisconsin</a>, United States) was the co-founder of the <a href="https://en.wikipedia.org/wiki/CBBS" title="CBBS">CBBS</a> bulletin board, the first <a href="https://en.wikipedia.org/wiki/Bulletin_board_system" title="Bulletin board system">bulletin board system</a> (BBS) ever brought online.<sup id="cite_ref-1"><a href="#cite_note-1"><span>[</span>1<span>]</span></a></sup> Christensen, along with partner <a href="https://en.wikipedia.org/wiki/Randy_Suess" title="Randy Suess">Randy Suess</a>,<sup id="cite_ref-2"><a href="#cite_note-2"><span>[</span>2<span>]</span></a></sup> members of the Chicago Area Computer Hobbyists' Exchange (CACHE), started development during a blizzard in <a href="https://en.wikipedia.org/wiki/Chicago" title="Chicago">Chicago</a>, <a href="https://en.wikipedia.org/wiki/Illinois" title="Illinois">Illinois</a>, and officially established CBBS four weeks later, on February 16, 1978. CACHE members frequently shared programs and had long been discussing some form of file transfer, and the two used the downtime during the blizzard to implement it.<sup id="cite_ref-3"><a href="#cite_note-3"><span>[</span>3<span>]</span></a></sup><sup id="cite_ref-4"><a href="#cite_note-4"><span>[</span>4<span>]</span></a></sup><sup id="cite_ref-5"><a href="#cite_note-5"><span>[</span>5<span>]</span></a></sup>
</p><p>Christensen was noted for building <a href="https://en.wikipedia.org/wiki/Software" title="Software">software</a> tools for his needs. He wrote a cassette-based <a href="https://en.wikipedia.org/wiki/Operating_system" title="Operating system">operating system</a> before <a href="https://en.wikipedia.org/wiki/Floppy_disk" title="Floppy disk">floppies</a> and <a href="https://en.wikipedia.org/wiki/Hard_disk_drive" title="Hard disk drive">hard disks</a> were common. When he lost track of the <a href="https://en.wikipedia.org/wiki/Source_code" title="Source code">source code</a> for some <a href="https://en.wikipedia.org/wiki/Computer_program" title="Computer program">programs</a>, he wrote ReSource, an iterative <a href="https://en.wikipedia.org/wiki/Disassembler" title="Disassembler">disassembler</a> for the <a href="https://en.wikipedia.org/wiki/Intel_8080" title="Intel 8080">Intel 8080</a>, to help him regenerate the <a href="https://en.wikipedia.org/wiki/Source_code" title="Source code">source code</a>. When he needed to send files to Randy Suess, he wrote <a href="https://en.wikipedia.org/wiki/XMODEM" title="XMODEM">XMODEM</a>.
</p><p><a href="https://en.wikipedia.org/wiki/Jerry_Pournelle" title="Jerry Pournelle">Jerry Pournelle</a> wrote in 1983 of a collection of <a href="https://en.wikipedia.org/wiki/CP/M" title="CP/M">CP/M</a> <a href="https://en.wikipedia.org/wiki/Public-domain_software" title="Public-domain software">public-domain software</a> that "probably 50 percent of the really good programs were written by Ward Christensen, a public benefactor."<sup id="cite_ref-pournelle198307_6-0"><a href="#cite_note-pournelle198307-6"><span>[</span>6<span>]</span></a></sup> Christensen received two 1992 <a href="https://en.wikipedia.org/wiki/Dvorak_Awards" title="Dvorak Awards">Dvorak Awards for Excellence in Telecommunications</a>, one with Randy Suess for developing the first BBS, and a lifetime achievement award "for outstanding contributions to PC telecommunications."<sup id="cite_ref-7"><a href="#cite_note-7"><span>[</span>7<span>]</span></a></sup> In 1993, he received the <a href="https://en.wikipedia.org/wiki/EFF_Pioneer_Award" title="EFF Pioneer Award">Pioneer Award</a> from the <a href="https://en.wikipedia.org/wiki/Electronic_Frontier_Foundation" title="Electronic Frontier Foundation">Electronic Frontier Foundation</a>.<sup id="cite_ref-8"><a href="#cite_note-8"><span>[</span>8<span>]</span></a></sup>
</p><p>Christensen worked at <a href="https://en.wikipedia.org/wiki/IBM" title="IBM">IBM</a> from 1968<sup id="cite_ref-9"><a href="#cite_note-9"><span>[</span>9<span>]</span></a></sup> until his retirement in 2012.  His last position with IBM was field technical sales specialist.
</p><p>In May 2005, Christensen and Suess were both featured in <i><a href="https://en.wikipedia.org/wiki/BBS:_The_Documentary" title="BBS: The Documentary">BBS: The Documentary</a></i>.<sup id="cite_ref-10"><a href="#cite_note-10"><span>[</span>10<span>]</span></a></sup>
</p>

<div><ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span><cite id="CITEREFZelchenko1998">Zelchenko, Peter (30 October 1998). <a rel="nofollow" href="https://www.chicagotribune.com/news/ct-xpm-1998-10-30-9901080059-story.html">"Jack Rickard, editor of Boardwatch magazine, saw it coming"</a>. <i><a href="https://en.wikipedia.org/wiki/Chicago_Tribune" title="Chicago Tribune">Chicago Tribune</a></i><span>. Retrieved <span>8 October</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Chicago+Tribune&amp;rft.atitle=Jack+Rickard%2C+editor+of+Boardwatch+magazine%2C+saw+it+coming&amp;rft.date=1998-10-30&amp;rft.aulast=Zelchenko&amp;rft.aufirst=Peter&amp;rft_id=https%3A%2F%2Fwww.chicagotribune.com%2Fnews%2Fct-xpm-1998-10-30-9901080059-story.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
<li id="cite_note-2"><span><b><a href="#cite_ref-2">^</a></b></span> <span><cite id="CITEREFMetz2019">Metz, Cade (2019-12-20). <a rel="nofollow" href="https://www.nytimes.com/2019/12/20/technology/randy-suess-dead.html">"Randy Suess, Computer Bulletin Board Inventor, Dies at 74"</a>. <i>The New York Times</i>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://search.worldcat.org/issn/0362-4331">0362-4331</a><span>. Retrieved <span>2021-10-03</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=Randy+Suess%2C+Computer+Bulletin+Board+Inventor%2C+Dies+at+74&amp;rft.date=2019-12-20&amp;rft.issn=0362-4331&amp;rft.aulast=Metz&amp;rft.aufirst=Cade&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2019%2F12%2F20%2Ftechnology%2Frandy-suess-dead.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
<li id="cite_note-3"><span><b><a href="#cite_ref-3">^</a></b></span> <span><cite id="CITEREFBarry">Barry, Rey. <a rel="nofollow" href="http://www.freewarehof.org/ward.html">"The Origin of Computer Bulletin Boards"</a>. Freeware Hall of Fame.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+Origin+of+Computer+Bulletin+Boards&amp;rft.pub=Freeware+Hall+of+Fame&amp;rft.aulast=Barry&amp;rft.aufirst=Rey&amp;rft_id=http%3A%2F%2Fwww.freewarehof.org%2Fward.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
<li id="cite_note-4"><span><b><a href="#cite_ref-4">^</a></b></span> <span><cite id="CITEREFGoodwins"><a href="https://en.wikipedia.org/wiki/Rupert_Goodwins" title="Rupert Goodwins">Goodwins, Rupert</a>. <a rel="nofollow" href="http://resources.zdnet.co.uk/articles/comment/0,1000002985,2130537,00.htm">"Online communities turn twenty-five"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Online+communities+turn+twenty-five&amp;rft.aulast=Goodwins&amp;rft.aufirst=Rupert&amp;rft_id=http%3A%2F%2Fresources.zdnet.co.uk%2Farticles%2Fcomment%2F0%2C1000002985%2C2130537%2C00.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20110607175741/http://www.smartcomputing.com/editorial/dictionary/detail.asp?guid=&amp;searchtype=&amp;DicID=19535&amp;RefType=Encyclopedia">"Ward Christensen"</a>. <i>Smart Computing Encyclopedia</i>. Archived from <a rel="nofollow" href="http://www.smartcomputing.com/editorial/dictionary/detail.asp?guid=&amp;searchtype=&amp;DicID=19535&amp;RefType=Encyclopedia">the original</a> on June 7, 2011.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Ward+Christensen&amp;rft.btitle=Smart+Computing+Encyclopedia&amp;rft_id=http%3A%2F%2Fwww.smartcomputing.com%2Feditorial%2Fdictionary%2Fdetail.asp%3Fguid%3D%26searchtype%3D%26DicID%3D19535%26RefType%3DEncyclopedia&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
<li id="cite_note-pournelle198307-6"><span><b><a href="#cite_ref-pournelle198307_6-0">^</a></b></span> <span><cite id="CITEREFPournelle1983">Pournelle, Jerry (July 1983). <a rel="nofollow" href="https://archive.org/stream/byte-magazine-1983-07-rescan/1983_07_BYTE_08-07_Videotex#page/n325/mode/2up">"Interstellar Drives, Osborne Accessories, DEDICATE/32, and Death Valley"</a>. <i>BYTE</i>. p.&nbsp;323<span>. Retrieved <span>28 August</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BYTE&amp;rft.atitle=Interstellar+Drives%2C+Osborne+Accessories%2C+DEDICATE%2F32%2C+and+Death+Valley&amp;rft.pages=323&amp;rft.date=1983-07&amp;rft.aulast=Pournelle&amp;rft.aufirst=Jerry&amp;rft_id=https%3A%2F%2Farchive.org%2Fstream%2Fbyte-magazine-1983-07-rescan%2F1983_07_BYTE_08-07_Videotex%23page%2Fn325%2Fmode%2F2up&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
<li id="cite_note-7"><span><b><a href="#cite_ref-7">^</a></b></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20160306152326/http://citivu.com/dvorak/index.html#1992">"Dvorak Awards for Excellence in Telecommunications"</a>. <i>citivu</i>. Archived from <a rel="nofollow" href="http://citivu.com/dvorak/index.html#1992">the original</a> on 2016-03-06.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=citivu&amp;rft.atitle=Dvorak+Awards+for+Excellence+in+Telecommunications&amp;rft_id=http%3A%2F%2Fcitivu.com%2Fdvorak%2Findex.html%231992&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
<li id="cite_note-8"><span><b><a href="#cite_ref-8">^</a></b></span> <span><cite><a rel="nofollow" href="https://w2.eff.org/Misc/EFF/Pioneer_Awards/2nd_pioneer_awards.announce">"Second Annual EFF Pioneer Awards"</a>. Electronic Frontier Foundation.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Second+Annual+EFF+Pioneer+Awards&amp;rft.pub=Electronic+Frontier+Foundation&amp;rft_id=http%3A%2F%2Fw2.eff.org%2FMisc%2FEFF%2FPioneer_Awards%2F2nd_pioneer_awards.announce&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
<li id="cite_note-9"><span><b><a href="#cite_ref-9">^</a></b></span> <span><a rel="nofollow" href="http://www.bbsdocumentary.com/software/AAA/AAA/CBBS/memories.txt"><i>re: R/1ST BBS QUESTIONS</i> (Msg 46394)</a> from Ward Christensen to Steve Culver, July 31, 1993.</span>
</li>
<li id="cite_note-10"><span><b><a href="#cite_ref-10">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.bbsdocumentary.com/">"BBS: TheDocumentary"</a>. <i>BBS: The Documentary</i><span>. Retrieved <span>15 September</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=BBS%3A+The+Documentary&amp;rft.atitle=BBS%3A+TheDocumentary&amp;rft_id=http%3A%2F%2Fwww.bbsdocumentary.com%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWard+Christensen"></span></span>
</li>
</ol></div>

<ul><li><a rel="nofollow" href="https://x.com/wardxmodem">Ward Christensen</a> on <a href="https://en.wikipedia.org/wiki/Twitter" title="Twitter">Twitter</a> <span typeof="mw:File/Frameless"><a href="https://www.wikidata.org/wiki/Q7968985#P2002" title="Edit this at Wikidata"><img alt="Edit this at Wikidata" src="https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png" decoding="async" width="10" height="10" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/15px-OOjs_UI_icon_edit-ltr-progressive.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png 2x" data-file-width="20" data-file-height="20"></a></span></li></ul>

<!-- 
NewPP limit report
Parsed by mw‐web.eqiad.main‐c595d44d9‐k22dq
Cached time: 20241012105402
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.459 seconds
Real time usage: 0.791 seconds
Preprocessor visited node count: 2284/1000000
Post‐expand include size: 28505/2097152 bytes
Template argument size: 1699/2097152 bytes
Highest expansion depth: 17/100
Expensive parser function count: 1/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 41202/5000000 bytes
Lua time usage: 0.303/10.000 seconds
Lua memory usage: 5908833/52428800 bytes
Number of Wikibase entities loaded: 1/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  745.019      1 -total
 41.60%  309.921      1 Template:Infobox_person
 26.83%  199.864      1 Template:Reflist
 20.60%  153.452      4 Template:Br_separated_entries
 20.29%  151.169      3 Template:Cite_news
 17.37%  129.374      1 Template:Birth_date
 15.41%  114.813      1 Template:BBS
 14.81%  110.314      1 Template:Navbox
  9.85%   73.366      1 Template:Short_description
  8.43%   62.831      1 Template:Wikidata_image
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:355056-0!canonical and timestamp 20241012105402 and revision id 1250720023. Rendering was triggered because: page-view
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Quiet Art of Attention (273 pts)]]></title>
            <link>https://billwear.github.io/art-of-attention.html</link>
            <guid>41828601</guid>
            <pubDate>Sun, 13 Oct 2024 15:01:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://billwear.github.io/art-of-attention.html">https://billwear.github.io/art-of-attention.html</a>, See on <a href="https://news.ycombinator.com/item?id=41828601">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p>There comes a moment in life, often in
      the quietest of hours, when one realizes
      that the world will continue on its
      wayward course, indifferent to our              desires or frustrations. And it is then,
      perhaps, that a subtle truth begins to
      emerge: the only thing we truly possess,
      the only thing we might, with enough
      care, exert some mastery over, is our
      mind. It is not a realization of
      resignation, but rather of liberation.
      For if the mind can be ordered, if it           can be made still in the midst of this
      restless life, then we have already
      discovered the key to a deeper kind of
      freedom.</p>

<p>But how does one begin? It is not with
      grand declarations or bold, sweeping
      changes. That would miss the point
      entirely. Rather, it is with a gentle
      attention to the present, a deliberate
      shift in the way we move through the
      world. We begin by paying attention to
      what our mind does—its wanderings, its
      anxieties, its compulsions. It is a
      garden untended, overgrown with concerns
      that may not even be our own. And the
      first step is simply to watch, to
      observe how the mind moves, without
      judgment, without rush.</p>

<p>In this quiet observation, we begin to
      see patterns. The mind leaps from one
      thing to another, rarely resting. It is
      caught in a web of habits, most of which
      we never consciously chose. But, once we
      notice this, a door opens. There is
      space, however small, between the
      thoughts. And in that space, if we are
      patient, we can decide how to respond
      rather than being dragged along by every
      impulse or fear. This is not about              control in the traditional sense, but
      about clarity. To act, not from reflex,
      but from intent.</p>

<p>It is a simple beginning, but one of
      great consequence. For when we reclaim
      our attention, even in this small way,
      we are no longer mere passengers on the
      journey. We become, in a sense, our own
      guides.</p>

<p>As we grow in this practice of
      attention, something else becomes clear:
      much of what occupies our thoughts is
      unnecessary. The mind is cluttered,
      filled with concerns that seem urgent
      but, on closer inspection, do little to
      serve our deeper well-being.
      Simplification is not just a matter of
      decluttering our physical
      surroundings—it is a way of thinking, of
      living. As we quiet the noise within, we
      see more clearly what truly matters. We
      focus, not on everything, but on the
      essentials. We pare down, not by force,
      but by choice.</p>

<p>This process of simplification is not an
      escape from complexity. It is, in fact,
      a way of engaging with it more
      meaningfully. There are things in life
      that are intricate, yes, but not
      everything needs our attention at once.
      What truly requires our effort can be
      approached in small steps, in manageable
      pieces. The mind works best when it is
      focused on one thing at a time, when it
      is allowed to give itself fully to the          task at hand. In this way, the most
      complex of undertakings becomes simple,
      not because it is easy, but because we
      have allowed it to unfold naturally, one
      step after the other.</p>

<p>It is tempting, in moments of ambition,
      to think that we must change everything
      all at once, that the path to mastery or
      peace requires a sudden, dramatic shift.
      But this is rarely the case. In truth,
      most lasting changes come from small,
      deliberate actions. It is in the
      repetition of these small actions, over
      time, that we build strength, that we
      build the habits of mind that lead to
      deeper clarity. Just as a mountain is
      climbed not in great leaps but in
      steady, measured steps, so too is the
      mind brought into alignment by daily,
      patient attention to the way we think.</p>

<p>But in this process, we must remember
      something important: life is not meant
      to be rushed through. It is not a race,
      nor is it a problem to be solved. It is
      an experience to be lived, and living
      well requires presence. To focus on one
      thing deeply, to give it your full
      attention, is to experience it fully.
      And when we do this, something
      remarkable happens. Time, which so often
      feels like it is slipping through our
      fingers, begins to slow. Moments become
      rich, textured. Even the simplest of
      tasks takes on a new significance when
      approached with care, with attention.</p>

<p>This is the quiet art of living well. It
      does not demand that we abandon the
      world, but that we engage with it more
      mindfully. It asks that we slow down,
      that we look more closely, that we
      listen more carefully. For in doing so,
      we discover that much of what we
      seek—clarity, peace, even strength—was
      always within reach. It was simply
      waiting for us to stop, to pay
      attention, and to begin again with
      intention.</p>

<p>The mind, like a garden, requires
      tending. It needs patience, a steady
      hand, and, above all, consistency. There
      will be days when it seems unruly, when
      old habits return, and when focus feels
      elusive. But these days, too, are part
      of the process. Each small effort, each
      moment of renewed attention, builds upon
      the last. Over time, these moments
      accumulate, and what was once difficult
      becomes second nature.</p>

<p>And so, the journey to mastery of the
      mind begins not with grand gestures but
      with the simplest of practices: the
      practice of paying attention. Attention
      to the present, attention to what truly
      matters, and attention to the quiet
      spaces in between. In this way, step by
      step, thought by thought, we move closer
      to that elusive state of clarity, of
      peace, and of freedom.</p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Starship Flight 5 Stream (1430 pts)]]></title>
            <link>https://twitter.com/SpaceX/status/1845152255944819015</link>
            <guid>41827362</guid>
            <pubDate>Sun, 13 Oct 2024 12:23:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/SpaceX/status/1845152255944819015">https://twitter.com/SpaceX/status/1845152255944819015</a>, See on <a href="https://news.ycombinator.com/item?id=41827362">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Large language models reduce public knowledge sharing on online Q&A platforms (310 pts)]]></title>
            <link>https://academic.oup.com/pnasnexus/article/3/9/pgae400/7754871</link>
            <guid>41827043</guid>
            <pubDate>Sun, 13 Oct 2024 11:26:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://academic.oup.com/pnasnexus/article/3/9/pgae400/7754871">https://academic.oup.com/pnasnexus/article/3/9/pgae400/7754871</a>, See on <a href="https://news.ycombinator.com/item?id=41827043">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widgetname="ArticleFulltext">





                    <h2 scrollto-destination="483096323" id="483096323">Abstract</h2>
<section><p>Large language models (LLMs) are a potential substitute for human-generated data and knowledge resources. This substitution, however, can present a significant problem for the training data needed to develop future models if it leads to a reduction of human-generated content. In this work, we document a reduction in activity on Stack Overflow coinciding with the release of ChatGPT, a popular LLM. To test whether this reduction in activity is specific to the introduction of this LLM, we use counterfactuals involving similar human-generated knowledge resources that should not be affected by the introduction of ChatGPT to such extent. Within 6 months of ChatGPT’s release, activity on Stack Overflow decreased by 25% relative to its Russian and Chinese counterparts, where access to ChatGPT is limited, and to similar forums for mathematics, where ChatGPT is less capable. We interpret this estimate as a lower bound of the true impact of ChatGPT on Stack Overflow. The decline is larger for posts related to the most widely used programming languages. We find no significant change in post quality, measured by peer feedback, and observe similar decreases in content creation by more and less experienced users alike. Thus, LLMs are not only displacing duplicate, low-quality, or beginner-level content. Our findings suggest that the rapid adoption of LLMs reduces the production of public data needed to train them, with significant consequences.</p></section>                    
<div id="pgae400-box1"><p>This study examines the impact of ChatGPT, a large language model, on online communities that contribute to public knowledge shared on the Internet. We found that ChatGPT has led to a 25% drop in activity on Stack Overflow, a key reference website where programmers share knowledge and solve problems. This substitution threatens the future of the open web, as interactions with AI models are not added to the shared pool of online knowledge. Moreover, this phenomenon could weaken the quality of training data for future models, as machine-generated content likely cannot fully replace human creativity and insight. This shift could have significant consequences for both the public Internet and the future of AI.</p></div>                    <h2 scrollto-destination="483096327" id="483096327" data-legacy-id="pgae400-s1">Introduction</h2>
<p>Over the last 30 years, humans have constructed a vast and open library of information on the web. Using powerful search engines, anyone with an internet connection can access valuable information from online knowledge repositories like Wikipedia, Stack Overflow, and Reddit. New content and discussions posted online are quickly integrated into this ever-growing ecosystem, becoming digital public goods used by people all around the world to learn new technologies and solve their problems (<span id="jumplink-pgae400-B1 pgae400-B2 pgae400-B3 pgae400-B4"></span>1–4).</p><p>These public goods are essential for training AI systems, in particular, large language models (LLMs) (<span id="jumplink-pgae400-B5"></span>5). For example, the LLM in ChatGPT (<span id="jumplink-pgae400-B6"></span>6) is trained to recognize patterns, facts, and information from vast repositories of online public text by predicting the next words in sequences. It answers users’ questions by generating responses that not only integrate and contextualize this information but also infer underlying meanings and connections. The remarkable effectiveness of ChatGPT is reflected in its quick adoption (<span id="jumplink-pgae400-B7"></span>7) and application across diverse fields, including auditing (<span id="jumplink-pgae400-B8"></span>8), astronomy (<span id="jumplink-pgae400-B9"></span>9), medicine (<span id="jumplink-pgae400-B10"></span>10), and chemistry (<span id="jumplink-pgae400-B11"></span>11). Randomized control trials show that using LLMs significantly boosts productivity and quality in computer programming, professional writing, customer support tasks, consulting, and writing job applications (<span id="jumplink-pgae400-B12 pgae400-B13 pgae400-B14 pgae400-B15 pgae400-B16"></span>12–16). Indeed, the widely reported successes of LLMs, like ChatGPT, suggest that we will observe a significant change in how people search for, create and share information online.</p><p>Ironically, if LLMs like ChatGPT, substitute for traditional methods of searching and interrogating the web, they could displace the very human behavior that generated their original training data. As people begin to use ChatGPT or similar LLMs instead of online knowledge repositories to find information, traffic and contributions to these repositories will likely decrease, diminishing the quantity and quality of these digital public goods. Previous work refers to this sort of displacement as the “paradox of re-use”: for example, the information on platforms like Wikipedia powers Google search (via information boxes and summaries) while reducing the need to visit Wikipedia (<span id="jumplink-pgae400-B17"></span>17, <span id="jumplink-pgae400-B18"></span>18). While such a shift could have significant social and economic implications, we have little evidence on whether people are indeed reducing their consumption and creation of valuable digital public goods as LLMs’ popularity grows.</p><p>The aim of this article is to evaluate the impact of LLMs on the generation of open data on popular question-and-answer (Q&amp;A) platforms. We focus on the effects of the most widely adopted LLM as of now—ChatGPT. Because ChatGPT performs relatively well on software programming tasks (<span id="jumplink-pgae400-B15"></span>15), we study Stack Overflow, the largest online Q&amp;A platform for software development and programming. Preliminary studies have shown that ChatGPT’s quality is competitive with answers from Stack Overflow in specific fields (<span id="jumplink-pgae400-B19"></span>19, <span id="jumplink-pgae400-B20"></span>20).</p><p>We present three results. First, we examine whether the release of ChatGPT has decreased the volume of posts, i.e. questions and answers, published on the platform. We estimate the causal effect of ChatGPT’s release on Stack Overflow activity using a difference-in-differences model. We compare the weekly posting activity on Stack Overflow against that of four comparable Q&amp;A platforms. These counterfactual platforms are less likely to be affected by ChatGPT either because their users experience difficulties with accessing ChatGPT or because ChatGPT performs poorly in questions discussed on those platforms.</p><p>We find that posting activity on Stack Overflow decreased by about <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mn xmlns="">25</mn><mi mathvariant="normal" xmlns="">%</mi></math></span> relative to the counterfactual platforms 6 months after the release of ChatGPT. We estimate the average effect across the 6 months to be <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mn xmlns="">15</mn><mi mathvariant="normal" xmlns="">%</mi></math>⁠</span>, reflecting a lagged kick-in and gradual adoption of ChatGPT. We interpret the <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mn xmlns="">25</mn><mi mathvariant="normal" xmlns="">%</mi></math></span> figure as a lower bound of the total impact of ChatGPT on Stack Overflow, as LLMs likely had some impact on even the counterfactual platforms. Additional evidence from the 2023 Stack Overflow Developer Survey supports the hypothesis that ChatGPT users are less likely to post on Stack Overflow and to visit the platform regularly.</p><p>Second, we investigate whether ChatGPT is simply displacing lower-quality posts on Stack Overflow. To do so, we use data on up- and downvotes, simple forms of social feedback provided by other users to rate posts. We observe little change in the votes posts received on Stack Overflow since the release of ChatGPT. In addition, we find significant declines in posting by users of all experience levels, from novice to expert. These results suggest that ChatGPT is displacing various Stack Overflow posts, including high-quality content.</p><p>Third, we study the heterogeneity of ChatGPT’s impact across different programming languages discussed on Stack Overflow. We test for these heterogeneities using an event study design. We observe that posting activity in some languages, like Python and Javascript, has decreased significantly more than the platform’s average. Using data on programming language popularity on GitHub, we find that the most widely used languages (and, hence, languages with richer data for training ChatGPT) tend to have larger relative declines in posting activity.</p><p>Our analysis points to several significant implications for the sustainability of the current AI ecosystem. The first is that the decreased production of open data will limit the training of future models (<span id="jumplink-pgae400-B21"></span>21). LLM-generated content itself is likely an ineffective substitute for training data generated by humans for the purpose of training new models (<span id="jumplink-pgae400-B22 pgae400-B23 pgae400-B24"></span>22–24). One analogy is that training an LLM on LLM-generated content is like making a photocopy of a photocopy, providing successively less satisfying results (<span id="jumplink-pgae400-B25"></span>25). While human feedback to LLMs may facilitate continued learning, data generated by interactions with privately owned LLMs belong to the owners of these LLMs.</p><p>This leads to the second issue: the initial advantage of the first mover, in this case OpenAI with its ChatGPT, compounds if the LLM effectively learns from interactions with users while crowding out the generation of new open data that competitors could use to improve their models. While it is well-known that increasing returns to users and data in the digital sector can lead to winner-take-all dynamics and technological lock-in (<span id="jumplink-pgae400-B26"></span>26, <span id="jumplink-pgae400-B27"></span>27), the transformation of the online commons into a private database presents a novel risk to consumer welfare. More broadly, a shift from open data to a more closed web will likely have significant second-order impacts on the ever-growing digital economy (<span id="jumplink-pgae400-B28"></span>28) and how we access, share, and evaluate information. These potential consequences have been overlooked in previous risk taxonomies of LLMs (<span id="jumplink-pgae400-B29"></span>29).</p><p>The rest of the article is organized as follows. We introduce our empirical set-up, including the data and models used in our analysis, in Data and methods section. Results section presents our results. In Discussion section, we discuss their implications. We argue that our findings of a significant decline in activity on Stack Overflow following the release of ChatGPT have important implications for the training of future models, competition in the AI sector, the provision of digital public goods, and how humans seek and share information.</p>                    <h2 scrollto-destination="483096339" id="483096339" data-legacy-id="pgae400-s2">Data and methods</h2>
                    <h3 scrollto-destination="483096340" id="483096340" data-legacy-id="pgae400-s2.1">Stack exchange and Segmentfault data</h3>
<p>To measure the effect ChatGPT can have on digital public goods, we compare the change in Stack Overflow’s activity with the activity on a set of similar platforms. These platforms are similar to Stack Overflow in that they are technical Q&amp;A platforms but are less prone to substitution by ChatGPT given their focus or target group. Specifically, we study the Stack Exchange platforms: Mathematics and Math Overflow and the Russian-language version of Stack Overflow. We also examine a Chinese-language Q&amp;A platform on computer programming called Segmentfault.</p><p>Mathematics and Math Overflow focus on university- and research-level mathematics questions, respectively. We consider these sites to be less susceptible to replacement by ChatGPT given that, during our study’s period of observation, the free-tier version of ChatGPT performed poorly (0–20th percentile) on advanced high-school mathematics exams (<span id="jumplink-pgae400-B6"></span>6) and was therefore unlikely to serve as a suitable alternative to these platforms.</p><p>The Russian Stack Overflow and the Chinese Segmentfault have similar scope as Stack Overflow, but target users located in Russia and China, respectively. We consider these platforms to be less affected by ChatGPT given that ChatGPT is officially unavailable in the Russian Federation, Belarus, Russian-occupied Ukrainian territory, and the People’s Republic of China. Although people in these places can and do access ChatGPT via VPNs, such barriers still represent a hurdle to widespread fast adoption (<span id="jumplink-pgae400-B30"></span>30).</p><p>We extract all posts (questions and answers) on Stack Overflow, Mathematics, Math Overflow, and Russian Stack Overflow from their launch to early June 2023 using <a href="https://archive.org/details/stackexchange" target="_blank">https://archive.org/details/stackexchange</a>. We scraped the data from Segmentfault directly. Our initial dataset comprises 58 million posts on Stack Overflow, over 900 thousand posts for the Russian-language version of Stack Overflow, 3.5 million posts on Mathematics Stack Exchange, 300 thousand posts for Math Overflow, and about 300 thousand for Segmentfault. We focus our analysis on data from January 2022 to the end of May 2023, noting that our findings are robust to alternative time windows.</p><p>For each post in the Stack Exchange sites, we additionally extract the post’s type (question or answer), the number of votes (up—positive feedback, or down—negative feedback) the post received, and the tags assigned to the post, where tags are predefined labels that summarize the content of the post, for instance, an associated programming language. In addition, we also extract the experience of the post’s author (i.e. number of previous posts). Using this information, we classify posts into those from “New”, “Inexperienced”, “Experienced”, and “Expert” users depending on whether the author had 0, 1–10, 11–100, or more than 100 posts, respectively at the time the post was published.<sup><span id="jumplink-FN1"></span>a</sup> For more details on the data from Q&amp;A platforms we used, we refer the reader to section.</p><p>Finally, we also investigated data from the 2023 Stack Overflow Developer Survey, conducted in mid-May 2023. It includes 89,184 responses from software developers living in 185 countries. We focus on user responses to the prompt “Which AI-powered tools did you use regularly over the past year?”, for which ChatGPT was an option to tick. We use this information to provide further suggestive evidence for the relationship between the adoption of ChatGPT and Stack Overflow activity at the individual programmer’s level while controlling for a rich set of characteristics, such as professional status, education, experience, and preferred programming language.</p>                    <h3 scrollto-destination="483096347" id="483096347" data-legacy-id="pgae400-s2.2">Models</h3>
                    <h4 scrollto-destination="483096348" id="483096348" data-legacy-id="pgae400-s2.2.1">Difference-in-differences</h4>
<p>We estimate the effect of ChatGPT for posting activity on Stack Overflow using a difference-in-differences method with four counterfactual platforms. We aggregate posting data at platform- and week-level and fit a regression model using ordinary least squares (OLS):<sup><span id="jumplink-FN2"></span>b</sup></p><div><div id="jumplink-M0001" content-id="M0001"><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow xmlns=""><mi mathvariant="normal">Log</mi></mrow><mo stretchy="false" xmlns="">(</mo><msub xmlns=""><mrow><mi mathvariant="normal">Posts</mi></mrow><mrow><mspace width=".1em"></mspace><mi>p</mi><mo>,</mo><mi>t</mi></mrow></msub><mo stretchy="false" xmlns="">)</mo><mo xmlns="">=</mo><msub xmlns=""><mi>α</mi><mrow><mspace width=".1em"></mspace><mi>p</mi></mrow></msub><mo xmlns="">+</mo><msub xmlns=""><mi>λ</mi><mi>t</mi></msub><mo xmlns="">+</mo><mi xmlns="">β</mi><mo xmlns="">×</mo><msub xmlns=""><mrow><mi mathvariant="normal">Treated</mi></mrow><mrow><mspace width=".1em"></mspace><mi>p</mi><mo>,</mo><mi>t</mi></mrow></msub><mo xmlns="">+</mo><msub xmlns=""><mi>ϵ</mi><mrow><mspace width=".1em"></mspace><mi>p</mi><mo>,</mo><mi>t</mi></mrow></msub><mo xmlns="">,</mo></math></p></div><p><span>(1)</span></p></div><p>where <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mrow><mi mathvariant="normal">Posts</mi></mrow><mrow><mspace width=".1em"></mspace><mi>p</mi><mo>,</mo><mi>t</mi></mrow></msub></math></span> is the number of posts on platform <em>p</em> in a week <em>t</em>, which we log-transform. <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mi>α</mi><mrow><mspace width=".1em"></mspace><mi>p</mi></mrow></msub></math></span> are platform fixed effects, <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mi>λ</mi><mi>t</mi></msub></math></span> are time (week) fixed effects, and <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mi>ϵ</mi><mrow><mspace width=".1em"></mspace><mi>p</mi><mo>,</mo><mi>t</mi></mrow></msub></math></span> is the error term.</p><p>The coefficient of interest is <em>β</em>, which captures the estimated effect of ChatGPT on posting activity on Stack Overflow relative to the less affected platforms: Treated equals one for weeks after the release of ChatGPT (starting with the week of 2022 November 27) when the platform <em>p</em> is Stack Overflow and zero otherwise. We report standard errors clustered at the monthly level to account for month-specific shocks common to all platforms. We note that <em>β</em> defines an estimate of the effect of ChatGPT on Stack Overflow relative to the counterfactuals averaged across the entire 6-month post-treatment period of our data. We focus our difference-in-differences estimations on the period between January 2022 and May 2023, covering 48 weeks before the release of ChatGPT and 25 weeks after it.<sup><span id="jumplink-FN3"></span>c</sup> However, to show that our results are not specific to the selected time window, we also repeat the estimations using a wider time period starting from January 2019.</p><p>The validity of the difference-in-differences approach relies on the assumption of parallel trends. While Fig. <span id="jumplink-pgae400-F1"></span>1b, illustrates that posting activity on Stack Overflow and the counterfactual platforms had developed in a similar way prior to the ChatGPT shock, we conduct several formal checks. First, we add platform-specific time trends that represent an interaction between a linear time trend and the average change in the number of posts on a platform between 2018 and pre-GPT. This allows us to check if the results are robust to the inclusion of differential time trends (<span id="jumplink-pgae400-B32"></span>32). Second, we estimate a generalized difference-in-differences model. Specifically, we employ a similar specification, but instead of <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">β</mi><mo xmlns="">×</mo><msub xmlns=""><mrow><mi mathvariant="normal">Treated</mi></mrow><mrow><mspace width=".1em"></mspace><mi>p</mi><mo>,</mo><mi>t</mi></mrow></msub></math>⁠</span>, we use <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><munder xmlns=""><mo>∑</mo><mi>t</mi></munder><msub xmlns=""><mi>β</mi><mi>t</mi></msub><mo xmlns="">×</mo><mi xmlns="">I</mi><mo stretchy="false" xmlns="">(</mo><mrow xmlns=""><mi mathvariant="normal">week</mi></mrow><mo xmlns="">=</mo><mi xmlns="">t</mi><mo stretchy="false" xmlns="">)</mo><mo xmlns="">×</mo><mi xmlns="">I</mi><mo stretchy="false" xmlns="">(</mo><mrow xmlns=""><mi mathvariant="normal">platform</mi><mspace width=".1em"></mspace></mrow><mo xmlns="">=</mo><mrow xmlns=""><mi mathvariant="normal">StackOverflow</mi></mrow><mo stretchy="false" xmlns="">)</mo></math>⁠</span>. We standardize the effects to 0 in the week before the public release of ChatGPT by dropping the indicator for that week from the regression. This model allows us to examine possible pretrends in our data. By estimating separate coefficients for the weeks <em>before</em> the release, we can check if posts on Stack Overflow had evolved similarly to the activity on counterfactual platforms prior to the release of ChatGPT. This specification also allows us to investigate the dynamics of the ChatGPT effect over time. Separate coefficients for 25 weeks <em>following</em> the release of ChatGPT show how the effects of ChatGPT realized over time as more users adopted the technology.</p>                    <div data-id="pgae400-f1" data-content-id="pgae400-f1" swap-content-for-modal="true"><p><img src="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/m_pgae400f1.jpeg?Expires=1731613731&amp;Signature=YFepQXrCPIxzAqXuYmMWOEBmLrKFL40uYS5Q--hUOQIkVcRDnZ7rd85dyg6PXbt7s7s0ep~2fKwV2NguWJyv5sx6zFx4ilIkwMbqbwlfot9l4FEBhkt8jPuv-L15I~t2tRfAGx~Dh4PbVSUp9psIByP~k25PuHW5k8xA8HSCRYmdXoUDsP5qwQ8PHGArHIf6nJG-ZWSuEAdP3T21YsfmyC1XswjYBuw1T1bOgYZiLw5Tyd4O0NTIIAUf8gU5R1X2Ak8lPFIGzhDqo5J5DwpX3JLx3eCaLG5bxOCQBVV~WtsLdhnFHx2H4bJ0GmXOlCYmcSG9igu22dBvXXOcxxqaSA__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="a) Time series of weekly posts to Stack Overflow since early 2016. The number of weekly posts decreases at a rate of about 7,000 posts each year from 2016 to 2022. In the 6 months after the release of ChatGPT, the weekly posting rate decreases by around 20,000 posts. b) Comparing posts to Stack Overflow, its Russian- and Chinese-language counterparts, and mathematics Q&amp;A platforms since early 2022. Post counts are standardized by the average and standard deviation of post counts within each platform prior to the release of ChatGPT. Posting activity on Stack Overflow falls significantly more relative to activity on other platforms." data-path-from-xml="pgae400f1.jpg"></p><div><p>Fig. 1.</p><p>a) Time series of weekly posts to Stack Overflow since early 2016. The number of weekly posts decreases at a rate of about 7,000 posts each year from 2016 to 2022. In the 6 months after the release of ChatGPT, the weekly posting rate decreases by around 20,000 posts. b) Comparing posts to Stack Overflow, its Russian- and Chinese-language counterparts, and mathematics Q&amp;A platforms since early 2022. Post counts are standardized by the average and standard deviation of post counts within each platform prior to the release of ChatGPT. Posting activity on Stack Overflow falls significantly more relative to activity on other platforms.</p></div></div><p>The advantage of the difference-in-differences method compared to a simple event study with Stack Overflow data only is that we estimate ChatGPT effects net of possible weekly shocks that are common across the technical Q&amp;A platforms. For the interpretation of the coefficient, we note that we estimate <em>relative</em> change in posting activity on Stack Overflow compared to activity on other platforms before vs. after the release of ChatGPT. To the extent that ChatGPT also affected activity on the counterfactual platforms, our estimates will be downward biased in the magnitude of the effect.</p><p>To investigate whether the decrease in posting was driven mainly by a decrease in the number of posts authored by new or inexperienced users, we run the same regression as in <span id="jumplink-M0001"></span><a href="#M0001">Eq. 1</a> separately for weekly posts made by users with different levels of prior experience. We assign each post the number of previous posts the user had made and differentiate between four groups of posts: posts by “new” users who have not posted before, posts by “inexperienced” users who posted between 1 and 10 times, posts by “experienced” users with between 11 and 100 prior posts, and posts by “expert” users who posted more than 100 times previously.</p>                    <h4 scrollto-destination="483096357" id="483096357" data-legacy-id="pgae400-s2.2.2">Event study</h4>
<p>When analyzing the effect of ChatGPT on activity across programming languages, we can no longer compare data from Stack Overflow with the counterfactual platforms. This is because the tags annotating posts are different between Stack Exchange platforms. Therefore, we study ChatGPT’s heterogeneous effects using an event-study specification. For each programming language <em>i</em> (identified by a tag), we model the standardized number of posts in a week <em>t</em> on Stack Overflow by fitting a simple linear time trend with seasonal effects:</p><div><div id="jumplink-M0002" content-id="M0002"><p><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mover><mrow><mi mathvariant="normal">Posts</mi></mrow><mo accent="false">¯</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo xmlns="">=</mo><msub xmlns=""><mi>β</mi><mn>0</mn></msub><mo xmlns="">+</mo><msub xmlns=""><mi>β</mi><mn>1</mn></msub><mi xmlns="">t</mi><mo xmlns="">+</mo><msub xmlns=""><mi>β</mi><mn>2</mn></msub><mrow xmlns=""><mi mathvariant="normal">ChatGPT</mi></mrow><mo xmlns="">+</mo><msub xmlns=""><mi>β</mi><mn>3</mn></msub><mo stretchy="false" xmlns="">(</mo><mi xmlns="">t</mi><mo xmlns="">×</mo><mrow xmlns=""><mi mathvariant="normal">ChatGPT</mi></mrow><mo stretchy="false" xmlns="">)</mo><mo xmlns="">+</mo><mi xmlns="">η</mi><mo xmlns="">+</mo><msub xmlns=""><mi>ϵ</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo xmlns="">,</mo></math></p></div><p><span>(2)</span></p></div><p>where <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mover xmlns=""><msub><mrow><mi mathvariant="normal">Posts</mi></mrow><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo accent="false">¯</mo></mover></math></span> stands for the standardized number of posts associated with a programming language <em>i</em> in a week <em>t</em>. We standardize the dependent variable in order to be better able to compare effects across programming languages with different numbers of posts.<sup><span id="jumplink-FN4"></span>d</sup><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mi>β</mi><mn>1</mn></msub><mo stretchy="false" xmlns="">(</mo><mi xmlns="">t</mi><mo stretchy="false" xmlns="">)</mo></math></span> captures the linear time trend and <em>η</em> are seasonal (month of year) fixed effects. ChatGPT equals one if the week <em>t</em> is after the release of ChatGPT and zero otherwise. Coefficient <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mi>β</mi><mn>2</mn></msub></math></span> captures the change in the intercept, while coefficient <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mi>β</mi><mn>3</mn></msub></math></span> reflects the change in the slope of the time trend following the release of ChatGPT. We report HAC standard errors.</p>                    <h4 scrollto-destination="483096361" id="483096361" data-legacy-id="pgae400-s2.2.3">Additional regression analysis with the Stack Overflow 2023 survey</h4>
<p>We run an additional model using Stack Overflow survey data to corroborate our findings. We compute the association between self-reported individual activity on Stack Overflow and the adoption of ChatGPT by estimating the following logistic regression:</p><div><div id="jumplink-M0003" content-id="M0003"><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="right left" rowspacing=".5em" columnspacing="thickmathspace" displaystyle="true" xmlns=""><mtr><mtd><mrow><mi mathvariant="normal">log</mi></mrow><mstyle scriptlevel="0"><mrow><mo maxsize="2.470em" minsize="2.470em">(</mo></mrow></mstyle><mrow><mfrac><msub><mrow><mi mathvariant="normal">Activity</mi></mrow><mrow><mi>d</mi><mo>,</mo><mi>i</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mrow><mi mathvariant="normal">Activity</mi></mrow><mrow><mi>d</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><mstyle scriptlevel="0"><mrow><mo maxsize="2.470em" minsize="2.470em">)</mo></mrow></mstyle></mtd><mtd><mo>=</mo><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><mo stretchy="false">(</mo><msub><mrow><mi mathvariant="normal">Use</mi><mspace width=".1em"></mspace><mi mathvariant="normal">ChatGPT</mi></mrow><mi>d</mi></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd></mtd><mtd><mspace width="1em"></mspace><mo>+</mo><msub><mi>β</mi><mn>2</mn></msub><mo stretchy="false">(</mo><msub><mi>X</mi><mi>d</mi></msub><mo stretchy="false">)</mo><mo>+</mo><msub><mrow><mi mathvariant="normal">Age</mi></mrow><mi>d</mi></msub><mo>+</mo><msub><mi>C</mi><mi>d</mi></msub><mo>+</mo><msub><mi>I</mi><mi>d</mi></msub><mo>+</mo><msub><mrow><mi mathvariant="normal">Lang</mi></mrow><mi>i</mi></msub><mo>+</mo><msub><mrow><mi mathvariant="normal">Type</mi></mrow><mi>d</mi></msub><mo>+</mo><msub><mi>ϵ</mi><mrow><mi>d</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>,</mo></mtd></mtr></mtable></math></p></div><p><span>(3)</span></p></div><p>where <em>d</em> stands for the developer and <em>i</em> denotes a programming language used by the developer. <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mrow><mi mathvariant="normal">Activity</mi></mrow><mrow><mi>d</mi><mo>,</mo><mi>i</mi></mrow></msub></math></span> corresponds to the probability of being a frequent Stack Overflow visitor/contributor. <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msub xmlns=""><mi>X</mi><mi>d</mi></msub></math></span> comprises a set of controls at the developer’s level: a dummy of whether the developer is a professional software engineer, education level, employment status, working mode (remote, hybrid, or in-person), and years of coding. We also add age (Age), country (<em>C</em>), industry (<em>I</em>), programming language (Lang), and developer type (Type) (e.g. researcher, front-end, back-end, full-stack, QA, etc.) fixed effects. Because most developers report using more than one programming language, we expand the dataset to the developer × language level. We apply weights (1/number of languages) to avoid double counts.<sup><span id="jumplink-FN5"></span>e</sup> We cluster standard errors at the programming language level to allow for common shocks. While the cross-sectional nature of the survey data does not allow us to interpret the results as causal, we try to reduce the endogeneity by controlling for a rich set of the above individual characteristics that are likely to influence both the adoption of ChatGPT and Stack Overflow contributions. In this way, we are comparing how the contributions to Stack Overflow vary between ChatGPT adopters and nonadopters, who are otherwise very similar to each other.</p>                    <h2 scrollto-destination="483096365" id="483096365" data-legacy-id="pgae400-s3">Results</h2>
                    <h3 scrollto-destination="483096366" id="483096366" data-legacy-id="pgae400-s3.1">Decrease in posting activity</h3>
<p>Figure <span id="jumplink-pgae400-F1"></span>1a shows the evolution of activity on Stack Overflow from January 2016 to June 2023. Up to 2022 there was a gradual decrease in activity from roughly 110,000 to 60,000 posts per week, that is roughly 7,000k posts less per week each year. However, after the release of ChatGPT (2022 November 30) posting activity decreased sharply, with the weekly average falling from around 60,000 posts to 40,000 within 6 months. Compared to the pre-ChatGPT trend, this decrease represents more than 5 years worth of deceleration in just half a year.</p><p>The decrease in activity on Stack Overflow is larger than for similar platforms for which we expect ChatGPT to be a less viable substitute. Figure <span id="jumplink-pgae400-F1"></span>1b shows the standardized posting activity on Stack Overflow, the Russian- and Chinese-language counterparts of Stack Overflow, and two mathematics Q&amp;A platforms. We standardize posting activity by the average and standard deviation of post counts within each platform prior to the release of ChatGPT.</p><p>Figure <span id="jumplink-pgae400-F1"></span>1b highlights that Stack Overflow activity deviates markedly from activity on the other platforms after the release of ChatGPT. The plot visualizes the standardized posting activity within each platform since early 2022. Smoothed weekly activity varies between plus and minus two standard deviations for all platforms for most of 2022. Events, such as the Chinese New Year and other holidays and the start of the Russian invasion of Ukraine, are visible. Following the release of ChatGPT, we observe a significant and persistent decline in activity on Stack Overflow.</p><p>Our difference-in-differences model reveals that Stack Overflow activity significantly declined after the release of ChatGPT, and that this effect became more pronounced over time. Table <span id="jumplink-pgae400-T1"></span>1 reports our estimates, the first column indicates that ChatGPT decreased posting activity on Stack Overflow by 15% (<span>⁠<span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mn xmlns="">1</mn><mo xmlns="">−</mo><msup xmlns=""><mi>e</mi><mrow><mo>−</mo><mn>0.163</mn></mrow></msup></math>⁠</span>). Note that this is a measure of the average effect across the 6 months of post-ChatGPT data we consider. If ChatGPT adoption is gradual, we expect that the effect observed at the end of the data will be larger than at the beginning.</p>                    <div content-id="pgae400-T1"><div id="pgae400-T1" data-id="pgae400-T1"><p><span id="label-16735">Table 1.</span></p><p>Results of a difference-in-differences model, estimating the change in activity observed weekly on stack overflow following the release of ChatGPT, relative to activity on four other platforms less likely to have been impacted.</p> </div><div><table role="table" aria-labelledby="
                        label-16735" aria-describedby="
                        caption-16735"><thead><tr><th></th><th>(1)</th><th>(2)</th><th>(3)</th></tr></thead><tbody><tr><td></td><td>Number of posts</td><td>Number of questions</td><td>Weekday posts</td></tr><tr><td>Variables</td><td></td><td></td><td></td></tr><tr><td>Stack Overflow × Post-GPT</td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.163</mn><mo>*</mo></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><mn xmlns="">0.105</mn><mo xmlns="">+</mo></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.151</mn><mo>*</mo></msup></math></span></td></tr><tr><td></td><td>(0.0584)</td><td>(0.0597)</td><td>(0.0613)</td></tr><tr><td>Observations</td><td>370</td><td>370</td><td>370</td></tr><tr><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span>-within</td><td>0.0458</td><td>0.0189</td><td>0.0294</td></tr></tbody></table></div><div><table><thead><tr><th></th><th>(1)</th><th>(2)</th><th>(3)</th></tr></thead><tbody><tr><td></td><td>Number of posts</td><td>Number of questions</td><td>Weekday posts</td></tr><tr><td>Variables</td><td></td><td></td><td></td></tr><tr><td>Stack Overflow × Post-GPT</td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.163</mn><mo>*</mo></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><mn xmlns="">0.105</mn><mo xmlns="">+</mo></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.151</mn><mo>*</mo></msup></math></span></td></tr><tr><td></td><td>(0.0584)</td><td>(0.0597)</td><td>(0.0613)</td></tr><tr><td>Observations</td><td>370</td><td>370</td><td>370</td></tr><tr><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span>-within</td><td>0.0458</td><td>0.0189</td><td>0.0294</td></tr></tbody></table></div><div><p><span><p>All regressions comprise platform fixed effects and week fixed effects. The standard error of the estimate clustered on month is reported in parentheses. <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span> (within) is derived after differencing out week and platform fixed effects. Significance codes: ***: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.001</mn></math>⁠</span>, <sup>**</sup>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.01</mn></math>⁠</span>, <sup>*</sup>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.05</mn></math>⁠</span>, <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">+</mo></math>⁠</span>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.1</mn></math>⁠</span>.</p></span></p></div></div><div><div id="pgae400-T1" data-id="pgae400-T1"><p><span id="label-16735">Table 1.</span></p><p>Results of a difference-in-differences model, estimating the change in activity observed weekly on stack overflow following the release of ChatGPT, relative to activity on four other platforms less likely to have been impacted.</p> </div><div><table role="table" aria-labelledby="
                        label-16735" aria-describedby="
                        caption-16735"><thead><tr><th></th><th>(1)</th><th>(2)</th><th>(3)</th></tr></thead><tbody><tr><td></td><td>Number of posts</td><td>Number of questions</td><td>Weekday posts</td></tr><tr><td>Variables</td><td></td><td></td><td></td></tr><tr><td>Stack Overflow × Post-GPT</td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.163</mn><mo>*</mo></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><mn xmlns="">0.105</mn><mo xmlns="">+</mo></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.151</mn><mo>*</mo></msup></math></span></td></tr><tr><td></td><td>(0.0584)</td><td>(0.0597)</td><td>(0.0613)</td></tr><tr><td>Observations</td><td>370</td><td>370</td><td>370</td></tr><tr><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span>-within</td><td>0.0458</td><td>0.0189</td><td>0.0294</td></tr></tbody></table></div><div><table><thead><tr><th></th><th>(1)</th><th>(2)</th><th>(3)</th></tr></thead><tbody><tr><td></td><td>Number of posts</td><td>Number of questions</td><td>Weekday posts</td></tr><tr><td>Variables</td><td></td><td></td><td></td></tr><tr><td>Stack Overflow × Post-GPT</td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.163</mn><mo>*</mo></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><mn xmlns="">0.105</mn><mo xmlns="">+</mo></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.151</mn><mo>*</mo></msup></math></span></td></tr><tr><td></td><td>(0.0584)</td><td>(0.0597)</td><td>(0.0613)</td></tr><tr><td>Observations</td><td>370</td><td>370</td><td>370</td></tr><tr><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span>-within</td><td>0.0458</td><td>0.0189</td><td>0.0294</td></tr></tbody></table></div><div><p><span><p>All regressions comprise platform fixed effects and week fixed effects. The standard error of the estimate clustered on month is reported in parentheses. <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span> (within) is derived after differencing out week and platform fixed effects. Significance codes: ***: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.001</mn></math>⁠</span>, <sup>**</sup>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.01</mn></math>⁠</span>, <sup>*</sup>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.05</mn></math>⁠</span>, <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">+</mo></math>⁠</span>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.1</mn></math>⁠</span>.</p></span></p></div></div><p>Indeed, our second specification observes exactly this trend. We visualize the weekly estimates of the relative change in the Stack Overflow activity in Fig. <span id="jumplink-pgae400-F2"></span>2. This figure shows the impact of ChatGPT is increasing over time and is greater in magnitude than the average post-ChatGPT effect estimated in Table <span id="jumplink-pgae400-T1"></span>1 by the end of our study period. By the end of April 2023, coinciding with a peak in traffic to ChatGPT, <sup><span id="jumplink-FN6"></span>f</sup> the estimated decrease in activity stabilizes at around 25%.</p>                    <div data-id="pgae400-f2" data-content-id="pgae400-f2" swap-content-for-modal="true"><p><img src="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/m_pgae400f2.jpeg?Expires=1731613731&amp;Signature=WXvvneTac~xvlDnzRAJ15zn9dlANlB7kPvy4bvTaS0HzOMfo94DYC3Mdk30ToYQJSr~MqFOO7qgOXxhENPecNKXFLUasXo3ZGOJxw02xXlClEPVm7pst-0FEXR8c9HYr~3p1CmvHSicYqHGdAjUlqlXMF2H7vafyxlF~DnVDWBirizRQrhSD7Iw4DqslNoEc5iDKJJoSubgf6EqJFhQ1mA01nMtODZkIRoE6D0m1ukGR-JTSxAfvT1sJyV3mSTpqQIVvF5DTM4S6EL22rFqd0HcL6RxYnvF3W5bJFzKcUp8E1ZUbLkOLEZBfeFEcbBxZAK7r6-5a6Z2kYmXEo-FyVg__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="Difference-in-differences analysis for posting activities. The dashed line marks the week of 2022 November 27—the release week of ChatGPT. Eight weeks after its introduction, we observe a steady decline in the activity of Stack Overflow. The plotted coefficients correspond to the interaction between a weekly dummy and posting on Stack Overflow. We normalize the effects to 0 in the week before the public release of ChatGPT by dropping the indicator for that week from the regression. The reported CIs are at 95%. The regression comprises platform fixed effects and week fixed effects." data-path-from-xml="pgae400f2.jpg"></p><div><p>Fig. 2.</p><p>Difference-in-differences analysis for posting activities. The dashed line marks the week of 2022 November 27—the release week of ChatGPT. Eight weeks after its introduction, we observe a steady decline in the activity of Stack Overflow. The plotted coefficients correspond to the interaction between a weekly dummy and posting on Stack Overflow. We normalize the effects to 0 in the week before the public release of ChatGPT by dropping the indicator for that week from the regression. The reported CIs are at 95%. The regression comprises platform fixed effects and week fixed effects.</p></div></div><p>We also tested for heterogeneity in subsets of the data, considering only questions (rather than counting both questions and answers) and posts on weekdays. In both subsets, our estimates did not deviate significantly from the main result: we estimate a 10% relative decrease in questions and 14% relative decrease in posts on weekdays (see the second and third column of Table <span id="jumplink-pgae400-T1"></span>1). Our results are robust to using alternative transformations of the outcome (Tables <span data-supplement-target="sup1"></span><span><a path-from-xml="sup1" href="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/pgae400_supplementary_data.pdf?Expires=1731613731&amp;Signature=yOqPdmGqAJzaymtZijxgcLBjJYhTgpFyyhshkYMXxLbEeXbNxnac4EOiGoHrB6fauLrC9wXzg67yvxqnLp-6qXrkZ6wInSXkDW-F1fbkbUCwySoIgqd4UBWW~CEsHAVnqxAYHDKaVbD0njho7DZ1C20k3OZsXsNjCDm3g8d0NeNJIK46-1fjAUsKYewq6IsuYLe2HSNu13qEzu~pRRVIMQ7jMK25x4paT57rGZMgkJLwF19qfFAVKsnfhsERlcmMxPQsANv09KeJpumtZS3BCwNzR1IVlzyVLvGnsZtO7rhy40ZGQKbxssxMuYj8b~mx5Pmm50pKjsjUZw18ZmlFSw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">S1 and S2</a></span>), adding platform-specific trends (Table <span data-supplement-target="sup1"></span><span><a path-from-xml="sup1" href="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/pgae400_supplementary_data.pdf?Expires=1731613731&amp;Signature=yOqPdmGqAJzaymtZijxgcLBjJYhTgpFyyhshkYMXxLbEeXbNxnac4EOiGoHrB6fauLrC9wXzg67yvxqnLp-6qXrkZ6wInSXkDW-F1fbkbUCwySoIgqd4UBWW~CEsHAVnqxAYHDKaVbD0njho7DZ1C20k3OZsXsNjCDm3g8d0NeNJIK46-1fjAUsKYewq6IsuYLe2HSNu13qEzu~pRRVIMQ7jMK25x4paT57rGZMgkJLwF19qfFAVKsnfhsERlcmMxPQsANv09KeJpumtZS3BCwNzR1IVlzyVLvGnsZtO7rhy40ZGQKbxssxMuYj8b~mx5Pmm50pKjsjUZw18ZmlFSw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">S3</a></span>), and extending the time window of the analysis (Table <span data-supplement-target="sup1"></span><span><a path-from-xml="sup1" href="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/pgae400_supplementary_data.pdf?Expires=1731613731&amp;Signature=yOqPdmGqAJzaymtZijxgcLBjJYhTgpFyyhshkYMXxLbEeXbNxnac4EOiGoHrB6fauLrC9wXzg67yvxqnLp-6qXrkZ6wInSXkDW-F1fbkbUCwySoIgqd4UBWW~CEsHAVnqxAYHDKaVbD0njho7DZ1C20k3OZsXsNjCDm3g8d0NeNJIK46-1fjAUsKYewq6IsuYLe2HSNu13qEzu~pRRVIMQ7jMK25x4paT57rGZMgkJLwF19qfFAVKsnfhsERlcmMxPQsANv09KeJpumtZS3BCwNzR1IVlzyVLvGnsZtO7rhy40ZGQKbxssxMuYj8b~mx5Pmm50pKjsjUZw18ZmlFSw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">S4</a></span>).</p><p>The decrease in Stack Overflow activity is consistent with the individual-level evidence from the 2023 Stack Overflow Developer Survey. In particular, we estimate the relationship between self-reported ChatGPT usage and Stack Overflow visit and contribution frequency using specification <span id="jumplink-M0003"></span><a href="#M0003">Eq. 3</a>. We consider several binary variables as the outcomes: <em>Contribute to Stack Overflow ever (weekly or more)</em> is equal to one if a developer has contributed to Stack Overflow at least once (weekly or more often) and zero otherwise, <em>Visit Stack Overflow daily</em> is equal to one if a developer reports visiting the platform once or more times per day. As participants were recruited through Stack Overflow and related platforms, they represent a selected group of engaged Stack Overflow users, and, therefore, their levels of activity are high: about 75% of all respondents have contributed to the platform at least once, and 42% visit it daily. We report the results in Table <span data-supplement-target="sup1"></span><span><a path-from-xml="sup1" href="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/pgae400_supplementary_data.pdf?Expires=1731613731&amp;Signature=yOqPdmGqAJzaymtZijxgcLBjJYhTgpFyyhshkYMXxLbEeXbNxnac4EOiGoHrB6fauLrC9wXzg67yvxqnLp-6qXrkZ6wInSXkDW-F1fbkbUCwySoIgqd4UBWW~CEsHAVnqxAYHDKaVbD0njho7DZ1C20k3OZsXsNjCDm3g8d0NeNJIK46-1fjAUsKYewq6IsuYLe2HSNu13qEzu~pRRVIMQ7jMK25x4paT57rGZMgkJLwF19qfFAVKsnfhsERlcmMxPQsANv09KeJpumtZS3BCwNzR1IVlzyVLvGnsZtO7rhy40ZGQKbxssxMuYj8b~mx5Pmm50pKjsjUZw18ZmlFSw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">S7</a></span>. The coefficients represent changes in log odds of the outcomes, and we also compute the average marginal effects of ChatGPT adoption on the likelihood of frequent contributions/visits.</p><p>We find that ChatGPT adopters are less likely to contribute to Stack Overflow and to visit the platform frequently compared to nonadopters of the same age, experience, education, employment status, working mode, industry, and programming language used. Moreover, even when we limit the sample to the most active respondents (i.e. those who have contributed at least once to Stack Overflow), we can still detect statistically significant differences in the probability of both contributing weekly and visiting daily between otherwise similar ChatGPT adopters and nonadopters. The magnitude of the effect is not very high. For instance, the average marginal effect of ChatGPT on the likelihood of contributing to Stack Overflow weekly or more is about 0.8 percentage points (or 2.7% lower probability). However, these results are likely to be downward biased because of the selection into survey participation: those who use Stack Overflow less frequently (including those who have reduced their activity because of ChatGPT) were less likely to respond.</p>                    <h4 scrollto-destination="483096377" id="483096377" data-legacy-id="pgae400-s3.1.1">Post and user heterogeneities</h4>
<p>A decrease in overall activity on Stack Overflow is not an issue if it is rather the less interesting questions that are outsourced to ChatGPT. We use a post’s score (i.e. difference between upvotes and downvotes) observed 5 weeks after its creation as a proxy of its value—good (bad) posts have a positive (negative) score, while neutral questions have a score of zero.</p><p>If ChatGPT is displacing bad questions, we would expect that after its release there would be a downward trend in the share of bad questions. However, as Fig. <span id="jumplink-pgae400-F3"></span>3a shows, while there was a slight uptick in the fraction of good questions, these were mostly replacing neutral questions and the trend of bad questions was flat. The short-lived increase in the fraction of good questions may be a result of ChatGPT inducing interest in novel topics, such as large language models, which usually results in good questions (see our Discussion section below on the increase in interest in CUDA). With respect to answers, there was no change in the trends of good, bad, and neutral answers. In general, there is a remarkable stability in the proportions post-ChatGPT. We confirm these results by estimating a difference-in-differences specification where the outcome is the number of up(down) votes that posts published in a given week receive over the first 5 weeks, normalized to the total number of posts from this week (Table <span data-supplement-target="sup1"></span><span><a path-from-xml="sup1" href="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/pgae400_supplementary_data.pdf?Expires=1731613731&amp;Signature=yOqPdmGqAJzaymtZijxgcLBjJYhTgpFyyhshkYMXxLbEeXbNxnac4EOiGoHrB6fauLrC9wXzg67yvxqnLp-6qXrkZ6wInSXkDW-F1fbkbUCwySoIgqd4UBWW~CEsHAVnqxAYHDKaVbD0njho7DZ1C20k3OZsXsNjCDm3g8d0NeNJIK46-1fjAUsKYewq6IsuYLe2HSNu13qEzu~pRRVIMQ7jMK25x4paT57rGZMgkJLwF19qfFAVKsnfhsERlcmMxPQsANv09KeJpumtZS3BCwNzR1IVlzyVLvGnsZtO7rhy40ZGQKbxssxMuYj8b~mx5Pmm50pKjsjUZw18ZmlFSw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">S5 and Fig. S1</a></span>). Unlike our previous results on posts, we do not detect any effect of ChatGPT.</p>                    <div data-id="pgae400-f3" data-content-id="pgae400-f3" swap-content-for-modal="true"><p><img src="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/m_pgae400f3.jpeg?Expires=1731613731&amp;Signature=Mad4b29cp5iv46g3rARmcVvhYjKqjNU4s4lEEhCn0l1nuykhHBkhVaricVh8UnErSYHkT4Efx9TuJiJkvtNeaJbpPBWqx8camMB~RQr-MUSzrOeXKZW8CQUXboBU8P7y~U-IQxac2XB60ft4tC27JYs8VWr7Wo2BpQ1nAQ5UQCaxDd9mlCpKOETV3xUkzCn5hyroDGJq-KJK3Xj4o5tTKy6GUdaJGqTdaktzTLssbmVZ7h18ctK4ht8zD0JflNIHpVSZOF1zsAaLAvnpUwBvJmhvamlPmXqUuEPCeQnVBZEpg1ovSwWLfxjpPjryH7MLcWVYCjE1WjKhr2XMu7VzKA__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="a) The weekly time series of the fraction of neutral, good, and bad questions and answers. Good (bad) post are those with a positive (negative) score after 5 weeks of its creation, while neutral questions have a zero score. The horizontal axis indicates the week of the post and the dashed line the week of the release of ChatGPT. We observe no major change in trends in bad posts since the release of ChatGPT. b) Weekly counts of questions and answers, respectively, by users, binned by experience level at the time of posting. We observe larger decreases in posting by users with previous experience post-ChatGPT." data-path-from-xml="pgae400f3.jpg"></p><div><p>Fig. 3.</p><p>a) The weekly time series of the fraction of neutral, good, and bad questions and answers. Good (bad) post are those with a positive (negative) score after 5 weeks of its creation, while neutral questions have a zero score. The horizontal axis indicates the week of the post and the dashed line the week of the release of ChatGPT. We observe no major change in trends in bad posts since the release of ChatGPT. b) Weekly counts of questions and answers, respectively, by users, binned by experience level at the time of posting. We observe larger decreases in posting by users with previous experience post-ChatGPT.</p></div></div><p>Votes do not capture all aspects of quality or more generally the ways in which ChatGPT may have influenced content on Stack Overflow. For example, users with different levels of experience contribute different kinds of content to the platform. New users tend to ask more basic questions, which ChatGPT may answer better. In contrast, experienced users may ask more sophisticated questions beyond the abilities of ChatGPT. A heterogeneous effect of ChatGPT on participation on Stack Overflow by users stratified by experience would have significant implications for content.</p><p>Table <span id="jumplink-pgae400-T2"></span>2 reports changes in activity estimated in a difference-in-differences specification, decomposed by prior user experience at the time of posting. Our estimates show that, while posts made by first-time users on Stack Overflow decreased only slightly relative to the control platforms, inexperienced, experienced, and expert users made significantly fewer posts on average after the release of ChatGPT.<sup><span id="jumplink-FN7"></span>g</sup> The point estimates (a reduction of about 21% relative to the counterfactual platforms) for both inexperienced and experienced users are almost identical, suggesting no significant difference in the decrease in activity. Table <span data-supplement-target="sup1"></span><span><a path-from-xml="sup1" href="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/pgae400_supplementary_data.pdf?Expires=1731613731&amp;Signature=yOqPdmGqAJzaymtZijxgcLBjJYhTgpFyyhshkYMXxLbEeXbNxnac4EOiGoHrB6fauLrC9wXzg67yvxqnLp-6qXrkZ6wInSXkDW-F1fbkbUCwySoIgqd4UBWW~CEsHAVnqxAYHDKaVbD0njho7DZ1C20k3OZsXsNjCDm3g8d0NeNJIK46-1fjAUsKYewq6IsuYLe2HSNu13qEzu~pRRVIMQ7jMK25x4paT57rGZMgkJLwF19qfFAVKsnfhsERlcmMxPQsANv09KeJpumtZS3BCwNzR1IVlzyVLvGnsZtO7rhy40ZGQKbxssxMuYj8b~mx5Pmm50pKjsjUZw18ZmlFSw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">S8</a></span> estimates separate effects of ChatGPT on questions and answers. Interestingly, while inexperienced users reduce the number of their questions and answers to a similar extent, the effects for experienced and expert users are more pronounced for posting answers. We could link the latter result to lower incentives to contribute to Stack Overflow: as fewer developers are using the platform, the visibility “premium” that could be earned by answering questions becomes lower.</p>                    <div content-id="pgae400-T2"><div id="pgae400-T2" data-id="pgae400-T2"><p><span id="label-19892">Table 2.</span></p><p>Results of a difference-in-differences model, estimating the change in activity observed weekly on stack overflow following the release of ChatGPT by user group, relative to activity on three other platforms (we exclude segment fault as we do not have access to user experience data) less likely to have been impacted.</p> </div><div><table role="table" aria-labelledby="
                        label-19892" aria-describedby="
                        caption-19892"><thead><tr><th></th><th>(1)</th><th>(2)</th><th>(3)</th><th>(4)</th></tr></thead><tbody><tr><td></td><td>Number of posts</td><td>Number of posts</td><td>Number of posts</td><td>Number of posts</td></tr><tr><td>VARIABLES</td><td>NewUser</td><td>InexperiencedUser</td><td>ExperiencedUser</td><td>ExpertUser</td></tr><tr><td>Stack Overflow × Post-GPT</td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.0833</mn><mo>*</mo></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.245</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.254</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.168</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td></tr><tr><td></td><td>(0.0376)</td><td>(0.0529)</td><td>(0.0424)</td><td>(0.0292)</td></tr><tr><td>Observations</td><td>296</td><td>296</td><td>296</td><td>296</td></tr><tr><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span>-within</td><td>0.0259</td><td>0.198</td><td>0.235</td><td>0.104</td></tr></tbody></table></div><div><table><thead><tr><th></th><th>(1)</th><th>(2)</th><th>(3)</th><th>(4)</th></tr></thead><tbody><tr><td></td><td>Number of posts</td><td>Number of posts</td><td>Number of posts</td><td>Number of posts</td></tr><tr><td>VARIABLES</td><td>NewUser</td><td>InexperiencedUser</td><td>ExperiencedUser</td><td>ExpertUser</td></tr><tr><td>Stack Overflow × Post-GPT</td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.0833</mn><mo>*</mo></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.245</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.254</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.168</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td></tr><tr><td></td><td>(0.0376)</td><td>(0.0529)</td><td>(0.0424)</td><td>(0.0292)</td></tr><tr><td>Observations</td><td>296</td><td>296</td><td>296</td><td>296</td></tr><tr><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span>-within</td><td>0.0259</td><td>0.198</td><td>0.235</td><td>0.104</td></tr></tbody></table></div><div><p><span><p>Posts by new users are posts by users with no previous posts at the time of posting. Inexperienced users have posted 1–10 times before, experienced users 11–100, and experts more than 100 times. All regressions comprise platform fixed effects and week fixed effects. The standard error of the estimate clustered on month is reported in parentheses. <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span> (within) is derived after differencing out week and platform fixed effects. Significance codes: ***: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.001</mn></math>⁠</span>, <sup>**</sup>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.01</mn></math>⁠</span>, <sup>*</sup>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.05</mn></math>⁠</span>, <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">+</mo></math>⁠</span>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.1</mn></math>⁠</span>.</p></span></p></div></div><div><div id="pgae400-T2" data-id="pgae400-T2"><p><span id="label-19892">Table 2.</span></p><p>Results of a difference-in-differences model, estimating the change in activity observed weekly on stack overflow following the release of ChatGPT by user group, relative to activity on three other platforms (we exclude segment fault as we do not have access to user experience data) less likely to have been impacted.</p> </div><div><table role="table" aria-labelledby="
                        label-19892" aria-describedby="
                        caption-19892"><thead><tr><th></th><th>(1)</th><th>(2)</th><th>(3)</th><th>(4)</th></tr></thead><tbody><tr><td></td><td>Number of posts</td><td>Number of posts</td><td>Number of posts</td><td>Number of posts</td></tr><tr><td>VARIABLES</td><td>NewUser</td><td>InexperiencedUser</td><td>ExperiencedUser</td><td>ExpertUser</td></tr><tr><td>Stack Overflow × Post-GPT</td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.0833</mn><mo>*</mo></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.245</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.254</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.168</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td></tr><tr><td></td><td>(0.0376)</td><td>(0.0529)</td><td>(0.0424)</td><td>(0.0292)</td></tr><tr><td>Observations</td><td>296</td><td>296</td><td>296</td><td>296</td></tr><tr><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span>-within</td><td>0.0259</td><td>0.198</td><td>0.235</td><td>0.104</td></tr></tbody></table></div><div><table><thead><tr><th></th><th>(1)</th><th>(2)</th><th>(3)</th><th>(4)</th></tr></thead><tbody><tr><td></td><td>Number of posts</td><td>Number of posts</td><td>Number of posts</td><td>Number of posts</td></tr><tr><td>VARIABLES</td><td>NewUser</td><td>InexperiencedUser</td><td>ExperiencedUser</td><td>ExpertUser</td></tr><tr><td>Stack Overflow × Post-GPT</td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.0833</mn><mo>*</mo></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.245</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.254</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">−</mo><msup xmlns=""><mn>0.168</mn><mrow><mo>*</mo><mo>*</mo><mo>*</mo></mrow></msup></math></span></td></tr><tr><td></td><td>(0.0376)</td><td>(0.0529)</td><td>(0.0424)</td><td>(0.0292)</td></tr><tr><td>Observations</td><td>296</td><td>296</td><td>296</td><td>296</td></tr><tr><td><span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span>-within</td><td>0.0259</td><td>0.198</td><td>0.235</td><td>0.104</td></tr></tbody></table></div><div><p><span><p>Posts by new users are posts by users with no previous posts at the time of posting. Inexperienced users have posted 1–10 times before, experienced users 11–100, and experts more than 100 times. All regressions comprise platform fixed effects and week fixed effects. The standard error of the estimate clustered on month is reported in parentheses. <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><msup xmlns=""><mi>R</mi><mn>2</mn></msup></math></span> (within) is derived after differencing out week and platform fixed effects. Significance codes: ***: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.001</mn></math>⁠</span>, <sup>**</sup>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.01</mn></math>⁠</span>, <sup>*</sup>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.05</mn></math>⁠</span>, <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mo xmlns="">+</mo></math>⁠</span>: <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.1</mn></math>⁠</span>.</p></span></p></div></div><p>Overall, our analysis shows little evidence that ChatGPT tends to replace low-quality posts and no evidence that it replaced posts by inexperienced users relative to experts and experienced users.</p>                    <h3 scrollto-destination="483096385" id="483096385" data-legacy-id="pgae400-s3.2">Heterogeneities across programming languages</h3>
<p>Next, we investigated differences in the impact of ChatGPT on posts about different programming languages, finding significant heterogeneities. In Facet A of Fig. <span id="jumplink-pgae400-F4"></span>4, we plot the estimated effects (slope changes in the linear time trend after the introduction of ChatGPT) for those 69 tags that we connected to a programming language on GitHub. We estimate a negative effect of ChatGPT for most tags, but the estimates range between a 0.25 standard deviation decrease in slope (i.e. change per week following the ChatGPT release) to a 0.03 standard deviation <em>increase</em>. We observe that some of the widely used languages like Python and Javascript are the most impacted by ChatGPT. Interestingly, the model estimates that posts about CUDA have increased (though not significantly) after ChatGPT was released. CUDA is an application programming interface created by Nvidia, a graphics card manufacturer, that facilitates the use of graphics cards for computational tasks, in particular for machine learning and AI. This exception again demonstrates the impact of ChatGPT on the world of computer programming: people are increasingly interested in software relating to AI.</p>                    <div data-id="pgae400-f4" data-content-id="pgae400-f4" swap-content-for-modal="true"><p><img src="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/m_pgae400f4.jpeg?Expires=1731613731&amp;Signature=1oZ7w0HBe0ElTKKgDektcwu2~M6scG70I0u3y5lGHZNEgTyPgguOADMKoj~oi5sSSzRF80~xZ8WSrxilZoiTo2XETnWq62GkAkzFY~QObzO3KGA7ETgpBIUeI3YpsPgacQFwWROPtPF92zDtQeAcGdiMz1OBVVvtlpL0qhL90eg49PHwhT6-f9ITgEnX0bZoNWOPznZ5VQHoqxbV~F1TP~pSQbD1GTZ-3LtUGX~ynAkqQHuqWKSTuJD7~u8mJ-4HUw6o-Jpkicmx1NVfPZ0~hj8R6DJiL2zIgXdMoR4A4W43VJl83BTbe8bmY8rQSWwxPCYiXPEpGJG55tiTHbd8qw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="a) The event study estimates of the effect of ChatGPT’s release on activity on a selection of tags on Stack Overflow. We report HAC-corrected 95% CIs. b) The relationship between estimated effects and salary data from the Stack Overflow Developer Survey. We find no significant relationship. c) The relationship between the number of GitHub repositories using a tag and the estimated effect of ChatGPT on that tag. In both b) and c), we plot a linear fit with bootstrapped 95% CIs. The dashed line in b) indicates that the correlation is not significant." data-path-from-xml="pgae400f4.jpg"></p><div><p>Fig. 4.</p><p>a) The event study estimates of the effect of ChatGPT’s release on activity on a selection of tags on Stack Overflow. We report HAC-corrected 95% CIs. b) The relationship between estimated effects and salary data from the Stack Overflow Developer Survey. We find no significant relationship. c) The relationship between the number of GitHub repositories using a tag and the estimated effect of ChatGPT on that tag. In both b) and c), we plot a linear fit with bootstrapped 95% CIs. The dashed line in b) indicates that the correlation is not significant.</p></div></div><p>Given that previous research suggests that high-wage jobs are more exposed to ChatGPT (<span id="jumplink-pgae400-B33"></span>33), we test whether the impact of ChatGPT is more predominant among better-paid languages. We source salary data from the 2022 Stack Overflow Developer Survey, focusing on US-based developers and calculating medians of reported salaries. In Fig. <span id="jumplink-pgae400-F4"></span>4b, we compare the estimated impact of ChatGPT on different languages against the salary data of developers using those languages. We observe no clear relationship between the estimated labor market value of a specific language and changes in posting behavior in that language post-ChatGPT.</p><p>To better understand the relationship between the size of the user base of a programming language and how it is impacted by ChatGPT, we compare our estimates with data from GitHub, the largest online platform for collaborative software development. Among other sources, ChatGPT was trained on data from GitHub. Because training data were collected up to September 2021, we use data on language use on GitHub up to June 2021. In Facet C of Fig. <span id="jumplink-pgae400-F4"></span>4, we visualize the relationship between the number of GitHub repositories (coding projects) in a specific language and the estimated impact of ChatGPT on that language. We observe that languages with more GitHub repositories tend to be more significantly impacted by the release of ChatGPT in terms of associated activity on Stack Overflow (Pearson’s <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mi xmlns="">ρ</mi><mo xmlns="">=</mo><mo xmlns="">−</mo><mn xmlns="">0.45</mn><mo xmlns="">,</mo><mi xmlns="">P</mi><mo xmlns="">&lt;</mo><mn xmlns="">0.001</mn></math>⁠</span>). This result is confirmed by estimating a difference-in-differences specification that compares the change in posting following the release of ChatGPT between more and less popular programming languages as measured by the number of GitHub commits attributed to a given language as of 2021 (Table <span data-supplement-target="sup1"></span><span><a path-from-xml="sup1" href="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/pgae400_supplementary_data.pdf?Expires=1731613731&amp;Signature=yOqPdmGqAJzaymtZijxgcLBjJYhTgpFyyhshkYMXxLbEeXbNxnac4EOiGoHrB6fauLrC9wXzg67yvxqnLp-6qXrkZ6wInSXkDW-F1fbkbUCwySoIgqd4UBWW~CEsHAVnqxAYHDKaVbD0njho7DZ1C20k3OZsXsNjCDm3g8d0NeNJIK46-1fjAUsKYewq6IsuYLe2HSNu13qEzu~pRRVIMQ7jMK25x4paT57rGZMgkJLwF19qfFAVKsnfhsERlcmMxPQsANv09KeJpumtZS3BCwNzR1IVlzyVLvGnsZtO7rhy40ZGQKbxssxMuYj8b~mx5Pmm50pKjsjUZw18ZmlFSw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">S6</a></span>).</p>                    <h3 scrollto-destination="483096390" id="483096390" data-legacy-id="pgae400-s3.3">Subsequent dynamics</h3>
<p>Using the Stack Exchange Data Explorer, we extended the timeseries of weekly posts to Stack Overflow to Spring 2024. We visualize this data in Fig. <span id="jumplink-pgae400-F5"></span>5. We observe a continued, if slower, decrease in weekly posting activity after the end of our statistical analyses. In raw terms, the number of weekly posts to Stack Overflow has fallen from 60,000 to 30,000 from May 2022 to May 2024, with much of that change happening in the 6 months following the release of ChatGPT. Again, this suggests that the 25% estimate of the effect of ChatGPT on Stack Overflow should be interpreted as a lower bound effect, which is likely still growing.</p>                    <div data-id="pgae400-f5" data-content-id="pgae400-f5" swap-content-for-modal="true"><p><img src="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/m_pgae400f5.jpeg?Expires=1731613731&amp;Signature=Gopr5dzWjofg9tEiwPD07o5ZfI29VIxgX1q-94F3bTpTluFRJUaoZKZwhW2vXSyx3o8ozAsf7PHzv9dQV1g9yRfY8uJPhJDjH-6~7pjaLD5JCmo60eoq8UckQo052jPDEmiZLNX0d7hpaRTe1YKj4KsFR-LWvkuTFcSYGYaUvTLJi2HcuGJ0JwR7T46OUjTDm4R2AXD0vy3hG~JhQopeaVuf6pIEfZNz2thCDEoAxntOKnJF4BsgL~PcSgYXqxJ3OXgud3zm0X3lIXm4PqXk2-Zr-RiVP9NBjyHQ1Xei2oqJmw4~aLDGIdrOOMnXwv-i29wh3zg4Harr~2WBPqLdRQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA" alt="An extended timeseries of the weekly posts to Stack Overflow. We highlight the release of ChatGPT and the conclusion of the data we use in the statistical analyses, respectively. After May 2023, the decline in posting activity continues, albeit at a slower rate." data-path-from-xml="pgae400f5.jpg"></p><div><p>Fig. 5.</p><p>An extended timeseries of the weekly posts to Stack Overflow. We highlight the release of ChatGPT and the conclusion of the data we use in the statistical analyses, respectively. After May 2023, the decline in posting activity continues, albeit at a slower rate.</p></div></div><p>An extension of the difference-in-differences analysis would not yield reliable estimates of the relative impact of LLMs of Stack Overflow for several reasons. First, the subsequent proliferation of ChatGPT or-better quality LLMs, including open source models and models available in Russia and China mean that the reference timeseries are no longer valid counterfactuals. Moreover, advances in LLM capabilities have significantly improved their performance in mathematical tasks. Thus, we do not extend our difference-in-differences analyses.</p>                    <h2 scrollto-destination="483096394" id="483096394" data-legacy-id="pgae400-s4">Discussion</h2>
<p>The rate at which people have adopted ChatGPT is one of the fastest in the history of technology (<span id="jumplink-pgae400-B7"></span>7). It is essential that we better understand what activities this new technology displaces and what second-order effects this substitution may have (<span id="jumplink-pgae400-B34"></span>34, <span id="jumplink-pgae400-B35"></span>35). This article shows that after the introduction of ChatGPT there was a sharp decrease in human content creation on Stack Overflow. We compare the decrease in activity on Stack Overflow with other Stack Exchange platforms where current LLMs are less likely to be used. Using a difference-in-differences model, we estimates a <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mn xmlns="">25</mn><mi mathvariant="normal" xmlns="">%</mi></math></span> decline to posts on Stack Overflow relative to the counterfactual platforms within 6 months of ChatGPT’s release. We interpret this as a lower bound as ChatGPT is likely to have had small but growing impact on the counterfactual platforms as well. The Stack Overflow Developer Survey confirms that people using ChatGPT were less likely to post questions or answers on Stack Overflow.</p><p>We observe no large change in social feedback on posts, measured using votes, nor in the experience composition of posting users following ChatGPT’s release. These results suggest that average post quality has not changed, nor has ChatGPT replaced only the new and inexperienced users. Posting activity related to more popular programming languages decreased more on average than that for more niche languages. Given that LLMs performance depends on the quantity of training data, this finding suggests that users are more likely to substitute Stack Overflow with ChatGPT with respect to languages LLMs are more knowledgeable about. Consequently, the widespread adoption of LLMs will likely decrease the provision of digital public goods including open data previously generated by interactions on the web.</p><p>Two of our results offer some limited reasons for optimism. While posting activity on Stack Overflow decreased among inexperienced, experienced, and expert users relative to the control platforms, content created by new users remained relatively stable. New users are known to be essential to the long-run health of online communities (<span id="jumplink-pgae400-B36"></span>36). However, this optimism should be nuanced given that, if new users start behaving as inexperienced users did, then new users will also be more to likely reduce their activity in Stack Overflow. The second is that the impact of ChatGPT was less on more niche languages used by fewer people, suggesting that online conversations around such languages and the valuable information they generate will continue.</p><p>Recent work by Burtch et al. (<span id="jumplink-pgae400-B37"></span>37) studying the evolution of activity on Stack Overflow and Reddit found similar results to ours. Using a synthetic control method to adjust for seasonality, the authors report a roughly 20% decrease in posting activity on Stack Overflow within 15 weeks of the release of ChatGPT, and find similar heterogeneities among programming languages. These findings complement ours, which are derived from a more conservative analysis using counterfactual platforms. One difference in our findings is that their method finds a sharp decrease in posts by new users, while we observe fewer posts by more experienced users on Stack Overflow compared to the counterfactual platforms. It would be valuable for future work to resolve this ambiguity given the importance of new users to platform health discussed above.</p><p>Our results and data have some shortcomings that point to other open questions about the use and impact of LLMs. First, while we can present strong evidence that ChatGPT decreased the posting activity in Stack Overflow, we can only partially assess quality of posting activity using data on upvotes and downvotes. Users may be posting more challenging questions, ones that LLMs cannot (yet) address, to Stack Overflow. Future work should examine whether continued activity on Stack Overflow is more complex or sophisticated on average than posts from prior to ChatGPT release. Similarly, ChatGPT may have reduced the volume of duplicate questions about simple topics, though this is unlikely to impact our main results as duplicates are estimated to account for only <span><span></span><math xmlns="http://www.w3.org/1998/Math/MathML"><mn xmlns="">3</mn><mi mathvariant="normal" xmlns="">%</mi></math></span> of posts (<span id="jumplink-pgae400-B38"></span>38), and we do not observe significant changes in voting outcomes.</p><p>A second limitation of our work is that we cannot observe the extent to which Russian- and Chinese-language users of the corresponding Q&amp;A platforms are actually hindered from accessing ChatGPT; indeed recent work has shown a spike in VPN and Tor activity following the blocking of ChatGPT in Italy (<span id="jumplink-pgae400-B30"></span>30). While our results are robust to excluding the Chinese and the Russian counterfactuals, given the potential economic importance of ChatGPT and similar LLMs, it is essential that we better understand how such bans and blocks impact the accessibility of these tools (<span id="jumplink-pgae400-B39"></span>39, <span id="jumplink-pgae400-B40"></span>40). Finally, we do not address the issue that ChatGPT may be used to generate Stack Overflow content. Stack Overflow policy effectively banned posts authored by ChatGPT within a week of its release. In any case, a significant amount of ChatGPT generated content on Stack Overflow would mean that our measures underestimate the magnitude of the ChatGPT effect.</p><p>Despite these shortcomings, our results have important implications for the future of digital public goods. Before the introduction of ChatGPT, more human-generated content was posted to Stack Overflow, forming a collective digital public good due to their nonrivalrous and nonexclusionary nature—anyone with internet access can view, absorb, and extend this information, without diminishing the value of the knowledge. Now, part of this information is rather fed into privately owned LLMs like ChatGPT. This represents a significant shift of knowledge from public to private domains.</p><p>This observed substitution effect also poses several issues for the future of AI. The first is that if language models crowd out open data creation, they will be limiting their own future training data and effectiveness. The second is that owners of the current leading models have exclusive access to user inputs and feedback, which, with a relatively smaller pool of open data, gives them a significant advantage against new competitors in training future models. Third, the decline of public resources on the web would reverse progress made by the web toward democratizing access to knowledge and information. Finally, the consolidation of humans searching for information around one or a few language models could narrow our explorations and focus our attention on mainstream topics. We briefly elaborate on these points, then conclude with a wider appeal for more research on the political economy of open data and AI, and how we can incentivize continued contributions to digital public goods.</p>                    <h4 scrollto-destination="483096404" id="483096404" data-legacy-id="pgae400-s4.1.1">Training future models</h4>
<p>Our findings suggest that the widespread adoption of ChatGPT may ironically make it difficult to train future models (<span id="jumplink-pgae400-B41"></span>41). Though researchers have already expressed concerns about running out of data for training AI models (<span id="jumplink-pgae400-B21"></span>21), our results show that the use of LLMs can slow down the creation of new (open) data. Given the growing evidence that data generated by LLMs are unlikely to effectively train new LLMs (<span id="jumplink-pgae400-B22"></span>22, <span id="jumplink-pgae400-B23"></span>23), modelers face the real problem of running out of useful data. While research on using synthetic data and mixed data to train LLMs is still ongoing, current results show that use of synthetic training data can degrade performance (<span id="jumplink-pgae400-B24"></span>24) and may even amplify biases in models (<span id="jumplink-pgae400-B42"></span>42). Human input and guidance can mitigate these issues to some extent, but in general it is still unclear if synthetic data can power continued advances in LLM capabilities.</p><p>If ChatGPT truly is a “blurry JPEG” of the web (<span id="jumplink-pgae400-B25"></span>25), then in the long run, it cannot effectively replace its most important input: data derived from human activity. Indeed, OpenAI’s recent strategic partnerships with Stack Overflow and Reddit demonstrate the value of this kind of data for the continued training of LLMs.<sup><span id="jumplink-FN8"></span>h</sup> The proliferation of LLMs has already impacted other forms of data creation: many Amazon Mechanical Turk workers now generate content (i.e. respond to surveys, evaluate texts) using ChatGPT (<span id="jumplink-pgae400-B43"></span>43). And though watermarks may help humans and models identify data creators (<span id="jumplink-pgae400-B44"></span>44), the general problem of determining whether, for example, a text is written by a human or LLM is difficult at scale (<span id="jumplink-pgae400-B45"></span>45).</p>                    <h3 scrollto-destination="483096407" id="483096407" data-legacy-id="pgae400-s4.2">Competition in the AI sector</h3>
<p>A firm’s early advantage in technological innovation often leads to significant market share via various mechanisms of path dependence (<span id="jumplink-pgae400-B46"></span>46). There are increasing returns to using ChatGPT as more people use it, as it can learn from user feedback (<span id="jumplink-pgae400-B26"></span>26). Our results indicate that ChatGPT is simultaneously decreasing the amount of open training data that competitors could use to build competing models while it captures user data for itself, which may lead to technological lock-in (<span id="jumplink-pgae400-B27"></span>27). Unlike synthetic data, data on user interactions with LLMs can be used to significantly improve and tune their performance (<span id="jumplink-pgae400-B47"></span>47). We suggest that besides increasing returns to scale from network effects, the transformation of public data commons into private databases presents another mechanism by which the tech sector can become even more concentrated.</p>                    <h3 scrollto-destination="483096409" id="483096409" data-legacy-id="pgae400-s4.3">Lost economic value</h3>
<p>Digital public goods generate value in many ways besides feeding LLMs and other algorithms. For instance, Wikipedia is an important source of information worldwide, but in developing countries, readers are more often motivated by intrinsic learning goals and tend to read articles in greater detail (<span id="jumplink-pgae400-B3"></span>3). Unequal access to AI may also compound inequalities in growth and innovation between countries (<span id="jumplink-pgae400-B40"></span>40).</p><p>Digital public goods also provide direct value to the many websites that extract data from open data to complement their core services with extra information (<span id="jumplink-pgae400-B4"></span>4). For instance, there is substantial interdependence between sites like Wikipedia, Reddit, and Stack Overflow and the search engines that use them to enrich responses to user queries via infoboxes (<span id="jumplink-pgae400-B17"></span>17, <span id="jumplink-pgae400-B48"></span>48), sometimes referred to as the “paradox of re-use” (<span id="jumplink-pgae400-B18"></span>18). In the case of search engines, putting links to knowledge sources within infoboxes has mitigated the issue to some degree (<span id="jumplink-pgae400-B49"></span>49), but LLMs like ChatGPT are substituting for search engines and are much less likely to link to sources. Their widespread adoption presents a significant threat to the overall sustainability of the web (<span id="jumplink-pgae400-B50"></span>50).</p><p>Creators of digital public goods may also lose out. Contributors to Stack Overflow or Open Source Software (OSS) often enjoy indirect benefits (<span id="jumplink-pgae400-B51"></span>51). For instance, while OSS itself provides significant value in the global economy (<span id="jumplink-pgae400-B52"></span>52), OSS contributions are valuable signals of a firm’s capabilities to investors (<span id="jumplink-pgae400-B53"></span>53). Individual contributions to Stack Overflow are used to signal ability on the labor market (<span id="jumplink-pgae400-B54"></span>54). Any general tendency of ChatGPT to crowd out contributions to digital public goods, may limit these valuable signals that reduce economic frictions. On the other hand, such signaling activity may serve as a powerful incentive to keep people contributing.</p>                    <h4 scrollto-destination="483096413" id="483096413" data-legacy-id="pgae400-s4.3.1">Narrowing of information seeking</h4>
<p>The substitution effect we report likely has important second-order effects on how people search for information and their exposure to new ideas. LLMs likely favor well-established perspectives and due to their efficiency decrease the need for users to forage for information. These features of LLMs may reinforce a trend observed earlier in the context of the web. Specifically, internet search engines are thought to have pushed science toward consensus and narrower topics by improving efficiency of information search and improving the visibility of mainstream information (<span id="jumplink-pgae400-B55"></span>55). LLMs may also disincentivize the use of new or niche tools because they most amplify our productivity with those tools for which it has much training data. For instance, ChatGPT may not be able to help users of a new programming language that is has not seen many examples of. Given that LLMs are poised to change how we do research (<span id="jumplink-pgae400-B56"></span>56), present a strong competitor to search engines (<span id="jumplink-pgae400-B57"></span>57), and will likely influence our news consumption (<span id="jumplink-pgae400-B58"></span>58), we need to understand what LLM efficiency implies for our contact with diverse sources of information and incentives to try new things.</p><p>More generally, models like ChatGPT are going to generate political and economic winners and losers like many previous breakthrough technologies. While early evidence shows that these models enhance productivity especially among new and inexperienced workers (<span id="jumplink-pgae400-B12"></span>12, <span id="jumplink-pgae400-B14"></span>14), there are other ways in which they may contribute to inequality between people and firms (<span id="jumplink-pgae400-B59"></span>59), for instance via potential negative side effects of automation (<span id="jumplink-pgae400-B33"></span>33, <span id="jumplink-pgae400-B60"></span>60). Our results suggest that the economics of data creation and ownership will become more salient: as data become more valuable, there will be growing interest in how creators of data can capture some of that value (<span id="jumplink-pgae400-B61"></span>61). These multifaceted aspects of the impact of LLMs suggest that the political economy of data and AI will be especially important in the next years (<span id="jumplink-pgae400-B58"></span>58, <span id="jumplink-pgae400-B62"></span>62, <span id="jumplink-pgae400-B63"></span>63).</p><p>In this context, our work highlights the specific issue that valuable digital public goods may be under-produced as a result of the proliferation of AI. A natural follow-up question is how we can incentivize the creation of such goods. While unemployment shocks are known to increase the provision of digital public goods (<span id="jumplink-pgae400-B64"></span>64), it would be an unsatisfying solution to suggest that people put out of work by automation will fill this gap. In the case of platforms like Stack Overflow, active users are often motivated by social feedback and gamification (<span id="jumplink-pgae400-B65"></span>65), but the continual onboarding of new users is what keeps these platforms relevant in the long run (<span id="jumplink-pgae400-B36"></span>36). For the sake of a sustainable open web and an AI ecosystem that draws on its data, we should think about how to keep people exchanging information and knowledge online.</p>                    <h2 scrollto-destination="483096417" id="483096417" data-legacy-id="pgae400-s5">Materials</h2>
                    <h4 scrollto-destination="483096419" id="483096419" data-legacy-id="pgae400-s5.1.1">Stack Exchange platform sites</h4>
<p>The raw dataset obtained from <a href="https://archive.org/details/stackexchange" target="_blank">https://archive.org/details/stackexchange</a> contains nearly all posting activity on the question and answer platforms hosted on the Stack Exchange network from its launch in 2008 to early June 2023. These include Stack Overflow, its Russian language version, and Math Overflow and Math Stack Exchange. Stack Overflow is the largest online Q&amp;A platform for topics relating to computer programming and software development. It provides a community-curated discussion of issues programmers face (<span id="jumplink-pgae400-B65"></span>65). Questions have multiple answers, and users debate the relative merits of solutions and alternatives in comments. A track record on Stack Overflow has value on the labor market as a signal of an individual’s skills (<span id="jumplink-pgae400-B54"></span>54).</p><p>The data contain over 58 million posts, including both questions and answers. Posts are linked to their posting users, from which we infer poster previous activity and can identify posts made by new users. Questions are annotated with tags indicating the topic of the post including programming languages used. Users can give posts upvotes or downvotes, providing posting users with social feedback and reputation points. The Russian language version of Stack Overflow (over 900 thousand posts) and the mathematics-oriented platforms Math Stack Exchange (over 3.5 million posts) and Math Overflow (over 300 thousand posts) have identically structured data dumps hosted in the same location.</p><p>Registered users can upvotes and downvote posts made on Stack Exchange platforms. These votes provide a valuable signal of the value of posts (<span id="jumplink-pgae400-B65"></span>65, <span id="jumplink-pgae400-B66"></span>66). They are the primary way users earn reputation points and status on Stack Exchange platforms. Votes also influence the ranking of posts in user feeds and search engine results, facilitating information filtering. Downvotes are used to moderate. The Stack Exchange data dump contains data on every vote cast, including the corresponding post, the date the vote was made, and whether it was an upvote or downvote.</p>                    <h4 scrollto-destination="483096423" id="483096423" data-legacy-id="pgae400-s5.1.2">Segmentfault</h4>
<p>Segmentfault is a Chinese language platform with a Q&amp;A platform for developers that has many similarities with the Stack Exchange sites. Users post questions on programming language topics and other users post answers. Questions are tagged by relevant languages and technologies, and there are similar gamification elements on the platform. We scraped data on all posts as of early June 2023, gathering over 300 thousand in total. We were careful to follow best practices when collecting this data, limiting strain on the host platform’s servers and retaining only anonymized data and metadata rather than content of posts (<span id="jumplink-pgae400-B67"></span>67).</p>                    <h4 scrollto-destination="483096425" id="483096425" data-legacy-id="pgae400-s5.1.3">Selection of tags</h4>
<p>Stack Overflow posts are annotated by tags which describe the concepts and technologies used in the post. For example, many tags indicate programming languages, web frameworks, database technologies, or programming concepts like functions or algorithms. Stack Overflow reconciles tags referring to the same things via a centralized synonym dictionary. We selected the 1,000 most used tags up to early June 2023 and focused on those 69 which could be directly linked to language statistics reported by GitHub, described next.</p>                    <h4 scrollto-destination="483096427" id="483096427" data-legacy-id="pgae400-s5.1.4">GitHub data on programming language use</h4>
<p>We use data from the June 2021 GHTorrent data dump (<span id="jumplink-pgae400-B68"></span>68) as a proxy measure for the amount of open data available for each programming language. The dataset reports which languages are used in each project or repository on GitHub. We simply count the number of repositories mentioning each language. We then link the languages with tags on Stack Overflow. As an alternative, we count the number of commits, elemental code contributions to repositories, to each repository, hence language. In the main article, we visualize the estimated effects of ChatGPT on specific tags that we can link to GitHub languages. We exclude some tags which refer to file formats or plain text, specifically: yaml, json, text, svg, markdown, and xml.</p>                    <h4 scrollto-destination="483096429" id="483096429" data-legacy-id="pgae400-s5.1.5">Stack Overflow Developer Survey</h4>
<p>The 2023 Stack Overflow Developer Survey was conducted from 2023 May 8 to May 19 and captured responses from 89,184 software developers across 185 countries. Respondents were recruited primarily through channels owned by Stack Overflow, therefore users that are highly engaged on Stack Overflow were more likely to notice the prompts to take the survey over the duration of the collection promotion.<sup><span id="jumplink-FN9"></span>i</sup> This survey includes self-disclosed information about respondents professional status, academic qualifications, employment type, remote work status, and years of coding experience. Moreover the survey asked participants ’Which AI powered tools did you use regularly over the past year’ and included ChatGPT as an option to tick.</p>                    <h2 scrollto-destination="483096431" id="483096431">Notes</h2>
<p><span><span><span rel="nofollow" data-fn-id="FN1">a</span></span><p>There is no standard classification of user experience based on the number of posts. We chose a log-binned classification since activity on Stack Overflow is heavy-tailed (<span id="jumplink-pgae400-B31"></span>31), and log base 10 is a commonly used base.</p></span></p><p><span><span><span rel="nofollow" data-fn-id="FN2">b</span></span><p>For robustness, we test an OLS specification with standardized outcomes and a specification with the raw count of posts that we fit using the Poisson pseudo-maximum likelihood method.</p></span></p><p><span><span><span rel="nofollow" data-fn-id="FN3">c</span></span><p>In this way, we do not include Covid-induced positive shock in 2020 and then the reversion to the trend in 2021.</p></span></p><p><span><span><span rel="nofollow" data-fn-id="FN4">d</span></span><p>We standardize the number of posts within each tag by subtracting the mean and dividing by the standard deviation. Both statistics are calculated using data up to the release of ChatGPT.</p></span></p><p><span><span><span rel="nofollow" data-fn-id="FN5">e</span></span><p>For example, if a developer reports using three languages, there will be three entries (one for each language) in our dataset for this developer, each with a weight of 1/3.</p></span></p><p><span><span><span rel="nofollow" data-fn-id="FN7">g</span></span><p>Prior to the release of ChatGPT, new users contributed 9.5 thousand posts per week; inexperienced users—almost 20 thousand; experienced users—about 17.5 thousand, and expert users—16 thousand.</p></span></p>                    <h2 scrollto-destination="483096441" id="483096441" data-legacy-id="ack1">Acknowledgments</h2>
<p>We thank Frank Neffke, Gergő Tóth, Christoffer Koch, Sándor Juhász, Martin Allen, Manran Zhu, Karl Wachs, László Czaller, Todd Davies, Thomas Fackler, César Hidalgo, Florian Englmaier, Helene Strandt, and James Evans for helpful comments and discussions.</p>                    <h2 scrollto-destination="483096443" id="483096443" data-legacy-id="pgae400-s6">Supplementary Material</h2>
<p><span data-supplement-target="sup1"></span><span><a path-from-xml="sup1" href="https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/pnasnexus/3/9/10.1093_pnasnexus_pgae400/1/pgae400_supplementary_data.pdf?Expires=1731613731&amp;Signature=yOqPdmGqAJzaymtZijxgcLBjJYhTgpFyyhshkYMXxLbEeXbNxnac4EOiGoHrB6fauLrC9wXzg67yvxqnLp-6qXrkZ6wInSXkDW-F1fbkbUCwySoIgqd4UBWW~CEsHAVnqxAYHDKaVbD0njho7DZ1C20k3OZsXsNjCDm3g8d0NeNJIK46-1fjAUsKYewq6IsuYLe2HSNu13qEzu~pRRVIMQ7jMK25x4paT57rGZMgkJLwF19qfFAVKsnfhsERlcmMxPQsANv09KeJpumtZS3BCwNzR1IVlzyVLvGnsZtO7rhy40ZGQKbxssxMuYj8b~mx5Pmm50pKjsjUZw18ZmlFSw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA">Supplementary material</a></span> is available at <em>PNAS Nexus</em> online.</p>                    <h2 scrollto-destination="483096445" id="483096445" data-legacy-id="pgae400-s7">Funding</h2>
<p>R.M.D.R.C. acknowledges funding from James S. McDonnell Foundation. J.W. acknowledges support from the Hungarian National Scientific Fund (OTKA FK 145960) and use of the HUN-REN Cloud (<span id="jumplink-pgae400-B69"></span>69) in the “Geographies of Creation, Learning and Use in Software” project. N.L. acknowledges support from CRC TRR 190 Rationality &amp; Competition.</p>                    <h2 scrollto-destination="483096447" id="483096447" data-legacy-id="pgae400-s8">Author Contributions</h2>
<p>R.M.D.R.C., N.L., and J.W. together conceived the idea, collected the data, created the models, analyzed the results, and wrote the manuscript.</p>                    <h2 scrollto-destination="483096449" id="483096449" data-legacy-id="pgae400-s9">Preprints</h2>
<p>A preprint of this article is published at <a href="https://arxiv.org/abs/2307.07367" target="_blank">https://arxiv.org/abs/2307.07367</a>.</p>                    <h2 scrollto-destination="483096451" id="483096451" data-legacy-id="pgae400-s10">Data Availability</h2>
<p>Data and code to reproduce our analyses are available on Zenodo: <a href="https://zenodo.org/records/12670482" target="_blank">https://zenodo.org/records/12670482</a>. The Stack Overflow data dump is available here: <a href="https://archive.org/details/stackexchange" target="_blank">https://archive.org/details/stackexchange</a>.</p>                    <h2 scrollto-destination="483096453" id="483096453" data-legacy-id="ref1">References</h2>
<div><div id="ref-auto-pgae400-B1" data-id="pgae400-B1" content-id="pgae400-B1" data-legacy-id="pgae400-B1"><p><span>1</span></p><div><p>Henzinger</p>  <p>M</p><p>, <span><p>Lawrence</p>  <p>S</p></span>. </p><p>2004</p><p>. </p><p>Extracting knowledge from the world wide web</p><p>. </p><p>Proc Natl Acad Sci U S A</p><p>. </p><p>101</p><p>:</p><p>5186</p><p>–</p><p>5191</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B2" data-id="pgae400-B2" content-id="pgae400-B2" data-legacy-id="pgae400-B2"><p><span>2</span></p><div><p>Hess</p>  <p>C</p><p>, <span><p>Ostrom</p>  <p>E</p></span>. </p><p>2003</p><p>. </p><p>Ideas, artifacts, and facilities: information as a common-pool resource</p><p>. </p><p>Law Contemp Probl</p><p>. </p><p>66</p><p>(</p><p>1/2</p><p>):</p><p>111</p><p>–</p><p>145</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B3" data-id="pgae400-B3" content-id="pgae400-B3" data-legacy-id="pgae400-B3"><p><span>3</span></p><div><p>Lemmerich</p>  <p>F</p><p>, <span><p>Sáez-Trumper</p>  <p>D</p></span>, <span><p>West</p>  <p>R</p></span>, <span><p>Zia</p>  <p>L</p></span>. </p><p>2019</p><p>. </p><p>Why the world reads Wikipedia: beyond English speakers. In: Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining; Melbourne. ACM. p. 618–626</p><p>.</p></div></div><div id="ref-auto-pgae400-B4" data-id="pgae400-B4" content-id="pgae400-B4" data-legacy-id="pgae400-B4"><p><span>4</span></p><div><p>Piccardi</p>  <p>T</p><p>, <span><p>Redi</p>  <p>M</p></span>, <span><p>Colavizza</p>  <p>G</p></span>, <span><p>West</p>  <p>R</p></span>. </p><p>2021</p><p>. </p><p>On the value of Wikipedia as a gateway to the web. In: Proceedings of the Web Conference 2021; IW3C2, Ljubljana. p. 249–260</p><p>.</p></div></div><div id="ref-auto-pgae400-B5" data-id="pgae400-B5" content-id="pgae400-B5" data-legacy-id="pgae400-B5"><p><span>5</span></p><div><p>Naveed</p>  <p>H</p><p>, <em>et al</em>. </p><p>2023</p><p>. A comprehensive overview of large language models, arXiv, arXiv:2307.06435, preprint: not peer reviewed. </p></div></div><div id="ref-auto-pgae400-B6" data-id="pgae400-B6" content-id="pgae400-B6" data-legacy-id="pgae400-B6"><p><span>6</span></p><div><p>OpenAI</p><p>. </p><p>2023</p><p>. </p><p>GPT-4 Technical Report</p><p>.</p></div></div><div id="ref-auto-pgae400-B7" data-id="pgae400-B7" content-id="pgae400-B7" data-legacy-id="pgae400-B7"><p><span>7</span></p><div><p>Teubner</p>  <p>T</p><p>, <span><p>Flath</p>  <p>CM</p></span>, <span><p>Weinhardt</p>  <p>C</p></span>, <span><p>van der Aalst</p>  <p>W</p></span>, <span><p>Hinz</p>  <p>O</p></span>. </p><p>2023</p><p>. </p><p>Welcome to the era of ChatGPT et al. the prospects of large language models</p><p>. </p><p>Bus Inf Syst Eng</p><p>. </p><p>65</p><p>(</p><p>2</p><p>):</p><p>95</p><p>–</p><p>101</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B8" data-id="pgae400-B8" content-id="pgae400-B8" data-legacy-id="pgae400-B8"><p><span>8</span></p><div><p>Gu</p>  <p>H</p><p>, <span><p>Schreyer</p>  <p>M</p></span>, <span><p>Moffitt</p>  <p>K</p></span>, <span><p>Vasarhelyi</p>  <p>MA</p></span>. </p><p>2023</p><p>. </p><p>Artificial intelligence co-piloted auditing. Available at SSRN 4444763</p><p>.</p></div></div><div id="ref-auto-pgae400-B9" data-id="pgae400-B9" content-id="pgae400-B9" data-legacy-id="pgae400-B9"><p><span>9</span></p><div><p>Smith</p>  <p>MJ</p><p>, <span><p>Geach</p>  <p>JE</p></span>. </p><p>2023</p><p>. </p><p>Astronomia ex machina: a history, primer and outlook on neural networks in astronomy</p><p>. </p><p>R Soc Open Sci</p><p>. </p><p>10</p><p>(</p><p>5</p><p>):</p><p>221454</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B10" data-id="pgae400-B10" content-id="pgae400-B10" data-legacy-id="pgae400-B10"><p><span>10</span></p><div><p>Kanjee</p>  <p>Z</p><p>, <span><p>Crowe</p>  <p>B</p></span>, <span><p>Rodman</p>  <p>A</p></span>. </p><p>2023</p><p>. </p><p>Accuracy of a generative artificial intelligence model in a complex diagnostic challenge</p><p>. </p><p>JAMA</p><p>. </p><p>330</p><p>(</p><p>1</p><p>):</p><p>78</p><p>–</p><p>80</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B11" data-id="pgae400-B11" content-id="pgae400-B11" data-legacy-id="pgae400-B11"><p><span>11</span></p><div><p>Guo</p>  <p>T</p><p>, <em>et al</em>. </p><p>2023</p><p>. What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks, arXiv, arXiv:2305.18365, preprint: not peer reviewed. </p></div></div><div id="ref-auto-pgae400-B12" data-id="pgae400-B12" content-id="pgae400-B12" data-legacy-id="pgae400-B12"><p><span>12</span></p><div><p>Brynjolfsson</p>  <p>E</p><p>, <span><p>Li</p>  <p>D</p></span>, <span><p>Raymond</p>  <p>LR</p></span>. </p><p>2023</p><p>. </p><p>Generative AI at work. Technical Report, National Bureau of Economic Research</p><p>.</p></div></div><div id="ref-auto-pgae400-B13" data-id="pgae400-B13" content-id="pgae400-B13" data-legacy-id="pgae400-B13"><p><span>13</span></p><div><p>Dell’Acqua</p>  <p>F</p><p>, <em>et al</em>. </p><p>2023</p><p>. </p><p>Navigating the jagged technological frontier: field experimental evidence of the effects of ai on knowledge worker productivity and quality. Harvard Business School Technology &amp; Operations Mgt. Unit Working Paper (24-013)</p><p>.</p></div></div><div id="ref-auto-pgae400-B14" data-id="pgae400-B14" content-id="pgae400-B14" data-legacy-id="pgae400-B14"><p><span>14</span></p><div><p>Noy</p>  <p>S</p><p>, <span><p>Zhang</p>  <p>W</p></span>. </p><p>2023</p><p>. </p><p>Experimental evidence on the productivity effects of generative artificial intelligence</p><p>. </p><p>Science</p><p>. </p><p>381</p><p>(</p><p>6654</p><p>):</p><p>187</p><p>–</p><p>192</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B15" data-id="pgae400-B15" content-id="pgae400-B15" data-legacy-id="pgae400-B15"><p><span>15</span></p><div><p>Peng</p>  <p>S</p><p>, <span><p>Kalliamvakou</p>  <p>E</p></span>, <span><p>Cihon</p>  <p>P</p></span>, <span><p>Demirer</p>  <p>M</p></span>. </p><p>2023</p><p>. The impact of AI on developer productivity: evidence from GitHub Copilot, arXiv, arXiv:2302.06590, preprint: not peer reviewed. </p></div></div><div id="ref-auto-pgae400-B16" data-id="pgae400-B16" content-id="pgae400-B16" data-legacy-id="pgae400-B16"><p><span>16</span></p><div><p>Wiles</p>  <p>E</p><p>, <span><p>Munyikwa</p>  <p>ZT</p></span>, <span><p>Horton</p>  <p>JJ</p></span>. </p><p>2023</p><p>. </p><p>Algorithmic writing assistance on Jobseekers’ resumes increases hires. Technical Report. National Bureau of Economic Research</p><p>.</p></div></div><div id="ref-auto-pgae400-B17" data-id="pgae400-B17" content-id="pgae400-B17" data-legacy-id="pgae400-B17"><p><span>17</span></p><div><p>McMahon</p>  <p>C</p><p>, <span><p>Johnson</p>  <p>I</p></span>, <span><p>Hecht</p>  <p>B</p></span>. </p><p>2017</p><p>. </p><p>The substantial interdependence of Wikipedia and Google: a case study on the relationship between peer production communities and information technologies. In: Proceedings of the International AAAI Conference on Web and Social Media; Montreal. AAAI. vol. 11, p. 142–151</p><p>.</p></div></div><div id="ref-auto-pgae400-B18" data-id="pgae400-B18" content-id="pgae400-B18" data-legacy-id="pgae400-B18"><p><span>18</span></p><div><p>Taraborelli</p>  <p>D</p><p>. </p><p>2015</p><p>. The sum of all human knowledge in the age of machines: a new research agenda for Wikimedia. In: ICWSM-15 Workshop on Wikipedia; Oxford. AAAI.</p></div></div><div id="ref-auto-pgae400-B19" data-id="pgae400-B19" content-id="pgae400-B19" data-legacy-id="pgae400-B19"><p><span>19</span></p><div><p>Delile</p>  <p>Z</p><p>, <em>et al</em>. </p><p>2023</p><p>. Evaluating privacy questions from stack overflow: can ChatGPT compete?, arXiv, arXiv:2306.11174, preprint: not peer reviewed. </p></div></div><div id="ref-auto-pgae400-B20" data-id="pgae400-B20" content-id="pgae400-B20" data-legacy-id="pgae400-B20"><p><span>20</span></p><div><p>Widjojo</p>  <p>P</p><p>, <span><p>Treude</p>  <p>C</p></span>. </p><p>2023</p><p>. Addressing compiler errors: stack overflow or large language models?, arXiv, arXiv:2307.10793, preprint: not peer reviewed. </p></div></div><div id="ref-auto-pgae400-B21" data-id="pgae400-B21" content-id="pgae400-B21" data-legacy-id="pgae400-B21"><p><span>21</span></p><div><p>Villalobos</p>  <p>P</p><p>, <em>et al</em>. </p><p>2022</p><p>. Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning, arXiv, arXiv:2211.04325, preprint: not peer reviewed. </p></div></div><div id="ref-auto-pgae400-B22" data-id="pgae400-B22" content-id="pgae400-B22" data-legacy-id="pgae400-B22"><p><span>22</span></p><div><p>Alemohammad</p>  <p>S</p><p>, <em>et al</em>. </p><p>2023</p><p>. </p><p>Self-consuming generative models go mad</p><p>, </p><p>arXiv, arXiv:2307.01850</p><p>, preprint.</p></div></div><div id="ref-auto-pgae400-B23" data-id="pgae400-B23" content-id="pgae400-B23" data-legacy-id="pgae400-B23"><p><span>23</span></p><div><p>Gudibande</p>  <p>A</p><p>, <em>et al</em>. </p><p>2023</p><p>. </p><p>The false promise of imitating proprietary LLMs, arXiv, arXiv:2305.15717, preprint: not peer reviewed</p><p>. </p></div></div><div id="ref-auto-pgae400-B24" data-id="pgae400-B24" content-id="pgae400-B24" data-legacy-id="pgae400-B24"><p><span>24</span></p><div><p>Shumailov</p>  <p>I</p><p>, <em>et al</em>. </p><p>2024</p><p>. </p><p>Ai models collapse when trained on recursively generated data</p><p>. </p><p>Nature</p><p>. </p><p>631</p><p>(</p><p>8022</p><p>):</p><p>755</p><p>–</p><p>759</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B25" data-id="pgae400-B25" content-id="pgae400-B25" data-legacy-id="pgae400-B25"><p><span>25</span></p><div><p>Chiang</p>  <p>T</p><p>. </p><p>2023</p><p>. </p><p>ChatGPT is a blurry JPEG of the web</p><p>. </p><p>The New Yorker</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B26" data-id="pgae400-B26" content-id="pgae400-B26" data-legacy-id="pgae400-B26"><p><span>26</span></p><div><p>Arthur</p>  <p>WB</p><p>. </p><p>1989</p><p>. </p><p>Competing technologies, increasing returns, and lock-in by historical events</p><p>. </p><p>Econ J</p><p>. </p><p>99</p><p>(</p><p>394</p><p>):</p><p>116</p><p>–</p><p>131</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B27" data-id="pgae400-B27" content-id="pgae400-B27" data-legacy-id="pgae400-B27"><p><span>27</span></p><div><p>David</p>  <p>PA</p><p>. </p><p>1985</p><p>. </p><p>Clio and the economics of QWERTY</p><p>. </p><p>Am Econ Rev</p><p>. </p><p>75</p><p>(</p><p>2</p><p>):</p><p>332</p><p>–</p><p>337</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B28" data-id="pgae400-B28" content-id="pgae400-B28" data-legacy-id="pgae400-B28"><p><span>28</span></p><div><p>Stojkoski</p>  <p>V</p><p>, <span><p>Koch</p>  <p>P</p></span>, <span><p>Coll</p>  <p>E</p></span>, <span><p>Hidalgo</p>  <p>CA</p></span>. </p><p>2024</p><p>. </p><p>Estimating digital product trade through corporate revenue data</p><p>. </p><p>Nat Commun</p><p>. </p><p>15</p><p>(</p><p>1</p><p>):</p><p>5262</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B29" data-id="pgae400-B29" content-id="pgae400-B29" data-legacy-id="pgae400-B29"><p><span>29</span></p><div><p>Weidinger</p>  <p>L</p><p>. </p><p>2022</p><p>. </p><p>Taxonomy of risks posed by language models. In: Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency; Seoul. ACM. p. 214–229</p><p>.</p></div></div><div id="ref-auto-pgae400-B30" data-id="pgae400-B30" content-id="pgae400-B30" data-legacy-id="pgae400-B30"><p><span>30</span></p><div><p>Kreitmeir</p>  <p>DH</p><p>, <span><p>Raschky</p>  <p>PA</p></span>. </p><p>2023</p><p>. </p><p>The Unintended Consequences of Censoring Digital Technology–Evidence from Italy’s ChatGPT Ban, arXiv, arXiv:2304.09339, preprint: not peer reviewed</p><p>. </p></div></div><div id="ref-auto-pgae400-B31" data-id="pgae400-B31" content-id="pgae400-B31" data-legacy-id="pgae400-B31"><p><span>31</span></p><div><p>Upadhyay</p>  <p>U</p><p>, <span><p>Valera</p>  <p>I</p></span>, <span><p>Gomez-Rodriguez</p>  <p>M</p></span>. </p><p>2017</p><p>. </p><p>Uncovering the dynamics of crowdlearning and the value of knowledge. In: Proceedings of the Tenth ACM International Conference on Web Search and Data Mining. p. 61–70; Cambridge (UK). ACM.</p></div></div><div id="ref-auto-pgae400-B32" data-id="pgae400-B32" content-id="pgae400-B32" data-legacy-id="pgae400-B32"><p><span>32</span></p><div><p>Bilinski</p>  <p>A</p><p>, <span><p>Hatfield</p>  <p>LA</p></span>. </p><p>2018</p><p>. </p><p>Nothing to see here? Non-inferiority approaches to parallel trends and other model assumptions, arXiv, arXiv:1805.03273, preprint: not peer reviewed</p><p>. </p></div></div><div id="ref-auto-pgae400-B33" data-id="pgae400-B33" content-id="pgae400-B33" data-legacy-id="pgae400-B33"><p><span>33</span></p><div><p>Eloundou</p>  <p>T</p><p>, <span><p>Manning</p>  <p>S</p></span>, <span><p>Mishkin</p>  <p>P</p></span>, <span><p>Rock</p>  <p>D</p></span>. </p><p>2024</p><p>. </p><p>GPTs are GPTs: labor market impact potential of LLMs</p><p>. </p><p>Science</p><p>. </p><p>384</p><p>(</p><p>6702</p><p>):</p><p>1306</p><p>–</p><p>1308</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B34" data-id="pgae400-B34" content-id="pgae400-B34" data-legacy-id="pgae400-B34"><p><span>34</span></p><div><p>Aghion</p>  <p>P</p><p>, <span><p>Howitt</p>  <p>P</p></span>. </p><p>1992</p><p>. </p><p>A model of growth through creative destruction</p><p>. </p><p>Econometrica</p><p>. </p><p>60</p><p>(</p><p>2</p><p>):</p><p>323</p><p>–</p><p>351</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B35" data-id="pgae400-B35" content-id="pgae400-B35" data-legacy-id="pgae400-B35"><p><span>35</span></p><div><p>Schumpeter</p>  <p>JA</p><p>. </p><p>1942</p><p>. </p><p>Capitalism, socialism, and democracy</p><p>. </p><p>New York</p><p>: </p><p>Routledge</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B36" data-id="pgae400-B36" content-id="pgae400-B36" data-legacy-id="pgae400-B36"><p><span>36</span></p><div><p>Danescu-Niculescu-Mizil</p>  <p>C</p><p>, <span><p>West</p>  <p>R</p></span>, <span><p>Jurafsky</p>  <p>D</p></span>, <span><p>Leskovec</p>  <p>J</p></span>, <span><p>Potts</p>  <p>C</p></span>. </p><p>2013</p><p>. </p><p>No country for old members: user lifecycle and linguistic change in online communities. In: Proceedings of the 22nd international conference on World Wide Web; Rio, ACM. p. 307–318</p><p>.</p></div></div><div id="ref-auto-pgae400-B37" data-id="pgae400-B37" content-id="pgae400-B37" data-legacy-id="pgae400-B37"><p><span>37</span></p><div><p>Burtch</p>  <p>G</p><p>, <span><p>Lee</p>  <p>D</p></span>, <span><p>Chen</p>  <p>Z</p></span>. </p><p>2024</p><p>. </p><p>The consequences of generative AI for online knowledge communities</p><p>. </p><p>Sci Rep</p><p>. </p><p>14</p><p>(</p><p>1</p><p>):</p><p>10413</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B38" data-id="pgae400-B38" content-id="pgae400-B38" data-legacy-id="pgae400-B38"><p><span>38</span></p><div><p>Correa</p>  <p>D</p><p>, <span><p>Sureka</p>  <p>A</p></span>. </p><p>2013</p><p>. </p><p>Fit or unfit: analysis and prediction of ’closed questions’ on Stack Overflow. In: Proceedings of the First ACM conference on Online Social Networks; Boston. ACM. p. 201–212</p><p>.</p></div></div><div id="ref-auto-pgae400-B39" data-id="pgae400-B39" content-id="pgae400-B39" data-legacy-id="pgae400-B39"><p><span>39</span></p><div><p>Bao</p>  <p>H</p><p>, <span><p>Sun</p>  <p>M</p></span>, <span><p>Teplitskiy</p>  <p>M</p></span>. </p><p>2024</p><p>. </p><p>Where there’s a will there’s a way: ChatGPT is used more for science in countries where it is prohibited</p><p>.</p></div></div><div id="ref-auto-pgae400-B40" data-id="pgae400-B40" content-id="pgae400-B40" data-legacy-id="pgae400-B40"><p><span>40</span></p><div><p>Gaessler</p>  <p>F</p><p>, <span><p>Piezunka</p>  <p>H</p></span>. </p><p>2023</p><p>. </p><p>Training with AI: evidence from chess computers</p><p>. </p><p>Strat Manag J</p><p>.</p><p>44</p><p>(</p><p>11</p><p>):</p><p>2724</p><p>–</p><p>2750</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B41" data-id="pgae400-B41" content-id="pgae400-B41" data-legacy-id="pgae400-B41"><p><span>41</span></p><div><p>Taleb</p>  <p>NN</p><p>. </p><p>2012</p><p>. </p><p>Antifragile: how to live in a world we don’t understand</p><p>. </p><p>vol. 3</p><p>. </p><p>London</p><p>: </p><p>Allen Lane</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B42" data-id="pgae400-B42" content-id="pgae400-B42" data-legacy-id="pgae400-B42"><p><span>42</span></p><div><p>Wyllie</p>  <p>S</p><p>, <span><p>Shumailov</p>  <p>I</p></span>, <span><p>Papernot</p>  <p>N</p></span>. </p><p>2024</p><p>. </p><p>Fairness feedback loops: training on synthetic data amplifies bias, arXiv, arXiv:2403.07857, preprint: not peer reviewed</p><p>. </p></div></div><div id="ref-auto-pgae400-B43" data-id="pgae400-B43" content-id="pgae400-B43" data-legacy-id="pgae400-B43"><p><span>43</span></p><div><p>Veselovsky</p>  <p>V</p><p>, <span><p>Ribeiro</p>  <p>MH</p></span>, <span><p>West</p>  <p>R</p></span>. </p><p>2023</p><p>. </p><p>Artificial artificial artificial intelligence: crowd workers widely use large language models for text production tasks, arXiv, arXiv:2306.07899, preprint: not peer reviewed</p><p>. </p></div></div><div id="ref-auto-pgae400-B44" data-id="pgae400-B44" content-id="pgae400-B44" data-legacy-id="pgae400-B44"><p><span>44</span></p><div><p>Tian-Zheng Wei</p>  <p>J</p><p>, <span><p>Wang</p>  <p>RY</p></span>, <span><p>Jia</p>  <p>R</p></span>. </p><p>2024</p><p>. </p><p>Proving membership in LLM pretraining data via data watermarks, arXiv, arXiv:2402.10892, preprint: not peer reviewed.</p> </div></div><div id="ref-auto-pgae400-B45" data-id="pgae400-B45" content-id="pgae400-B45" data-legacy-id="pgae400-B45"><p><span>45</span></p><div><p>Tang</p>  <p>R</p><p>, <span><p>Chuang</p>  <p>Y-N</p></span>, <span><p>Hu</p>  <p>X</p></span>. </p><p>2024</p><p>. </p><p>The science of detecting LLM-generated text</p><p>. </p><p>Commun ACM</p><p>. </p><p>67</p><p>(</p><p>4</p><p>):</p><p>50</p><p>–</p><p>59</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B46" data-id="pgae400-B46" content-id="pgae400-B46" data-legacy-id="pgae400-B46"><p><span>46</span></p><div><p>Page</p>  <p>SE</p><p>. </p><p>2006</p><p>. </p><p>Path dependence</p><p>. </p><p>Quart J Polit Sci</p><p>. </p><p>1</p><p>(</p><p>1</p><p>):</p><p>87</p><p>–</p><p>115</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B47" data-id="pgae400-B47" content-id="pgae400-B47" data-legacy-id="pgae400-B47"><p><span>47</span></p><div><p>Köpf</p>  <p>A</p><p>. </p><p>2024</p><p>. </p><p>Openassistant conversations-democratizing large language model alignment</p><p>. </p><p>Adv Neural Inf Process Syst</p><p>. </p><p>36</p><p>:</p><p>47669</p><p>–</p><p>47681</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B48" data-id="pgae400-B48" content-id="pgae400-B48" data-legacy-id="pgae400-B48"><p><span>48</span></p><div><p>Vincent</p>  <p>N</p><p>, <span><p>Johnson</p>  <p>I</p></span>, <span><p>Hecht</p>  <p>B</p></span>. </p><p>2018</p><p>. </p><p>Examining Wikipedia with a broader lens: quantifying the value of Wikipedia’s relationships with other large-scale online communities. In: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems; Quebec. ACM. p. 1–13</p><p>.</p></div></div><div id="ref-auto-pgae400-B49" data-id="pgae400-B49" content-id="pgae400-B49" data-legacy-id="pgae400-B49"><p><span>49</span></p><div><p>Vincent</p>  <p>N</p><p>, <span><p>Hecht</p>  <p>B</p></span>. </p><p>2021</p><p>. </p><p>A deeper investigation of the importance of Wikipedia links to search engine results</p><p>. </p><p>Proc ACM Hum-Comput Inter</p><p>. </p><p>5</p><p>(</p><p>CSCW1</p><p>):</p><p>1</p><p>–</p><p>15</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B50" data-id="pgae400-B50" content-id="pgae400-B50" data-legacy-id="pgae400-B50"><p><span>50</span></p><div><p>Vincent</p>  <p>N</p><p>. </p><p>2022</p><p>. </p><p>The paradox of reuse, language models edition. Data leverage</p><p>.</p></div></div><div id="ref-auto-pgae400-B51" data-id="pgae400-B51" content-id="pgae400-B51" data-legacy-id="pgae400-B51"><p><span>51</span></p><div><p>Lerner</p>  <p>J</p><p>, <span><p>Tirole</p>  <p>J</p></span>. </p><p>2002</p><p>. </p><p>Some simple economics of open source</p><p>. </p><p>J Ind Econ</p><p>. </p><p>50</p><p>(</p><p>2</p><p>):</p><p>197</p><p>–</p><p>234</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B52" data-id="pgae400-B52" content-id="pgae400-B52" data-legacy-id="pgae400-B52"><p><span>52</span></p><div><p>Greenstein</p>  <p>S</p><p>, <span><p>Nagle</p>  <p>F</p></span>. </p><p>2014</p><p>. </p><p>Digital dark matter and the economic contribution of Apache</p><p>. </p><p>Res Policy</p><p>. </p><p>43</p><p>(</p><p>4</p><p>):</p><p>623</p><p>–</p><p>631</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B53" data-id="pgae400-B53" content-id="pgae400-B53" data-legacy-id="pgae400-B53"><p><span>53</span></p><div><p>Conti</p>  <p>A</p><p>, <span><p>Peukert</p>  <p>C</p></span>, <span><p>Roche</p>  <p>MP</p></span>. </p><p>2021</p><p>. </p><p>Beefing IT up for your investor? Open sourcing and startup funding: evidence from GitHub. HBS Working Paper 22-001</p><p>.</p></div></div><div id="ref-auto-pgae400-B54" data-id="pgae400-B54" content-id="pgae400-B54" data-legacy-id="pgae400-B54"><p><span>54</span></p><div><p>Xu</p>  <p>L</p><p>, <span><p>Nian</p>  <p>T</p></span>, <span><p>Cabral</p>  <p>L</p></span>. </p><p>2020</p><p>. </p><p>What makes geeks tick? A study of stack overflow careers</p><p>. </p><p>Manage Sci</p><p>. </p><p>66</p><p>(</p><p>2</p><p>):</p><p>587</p><p>–</p><p>604</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B55" data-id="pgae400-B55" content-id="pgae400-B55" data-legacy-id="pgae400-B55"><p><span>55</span></p><div><p>Evans</p>  <p>JA</p><p>. </p><p>2008</p><p>. </p><p>Electronic publication and the narrowing of science and scholarship</p><p>. </p><p>Science</p><p>. </p><p>321</p><p>(</p><p>5887</p><p>):</p><p>395</p><p>–</p><p>399</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B56" data-id="pgae400-B56" content-id="pgae400-B56" data-legacy-id="pgae400-B56"><p><span>56</span></p><div><p>Grossmann</p>  <p>I</p><p>, <em>et al</em>. </p><p>2023</p><p>. </p><p>AI and the transformation of social science research</p><p>. </p><p>Science</p><p>. </p><p>380</p><p>(</p><p>6650</p><p>):</p><p>1108</p><p>–</p><p>1109</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B57" data-id="pgae400-B57" content-id="pgae400-B57" data-legacy-id="pgae400-B57"><p><span>57</span></p><div><p>Xu</p>  <p>R</p><p>, <span><p>Feng</p>  <p>Y</p></span>, <span><p>Chen</p>  <p>H</p></span>. </p><p>2023</p><p>. </p><p>ChatGPT vs. Google: a comparative study of search performance and user experience, arXiv, arXiv:2307.01135, preprint: not peer reviewed</p><p>. </p></div></div><div id="ref-auto-pgae400-B58" data-id="pgae400-B58" content-id="pgae400-B58" data-legacy-id="pgae400-B58"><p><span>58</span></p><div><p>Sandrini</p>  <p>L</p><p>, <span><p>Somogyi</p>  <p>R</p></span>. </p><p>2023</p><p>. </p><p>Generative ai and deceptive news consumption</p><p>. </p><p>Econ Lett</p><p>. </p><p>232</p><p>:</p><p>111317</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B59" data-id="pgae400-B59" content-id="pgae400-B59" data-legacy-id="pgae400-B59"><p><span>59</span></p><div><p>Rock</p>  <p>D</p><p>. </p><p>2019</p><p>. </p><p>Engineering value: the returns to technological talent and investments in artificial intelligence. Available at SSRN 3427412</p><p>.</p></div></div><div id="ref-auto-pgae400-B60" data-id="pgae400-B60" content-id="pgae400-B60" data-legacy-id="pgae400-B60"><p><span>60</span></p><div><p>Acemoglu</p>  <p>D</p><p>, <span><p>Restrepo</p>  <p>P</p></span>. </p><p>2019</p><p>. </p><p>Automation and new tasks: how technology displaces and reinstates labor</p><p>. </p><p>J Econ Perspect</p><p>. </p><p>33</p><p>(</p><p>2</p><p>):</p><p>3</p><p>–</p><p>30</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B61" data-id="pgae400-B61" content-id="pgae400-B61" data-legacy-id="pgae400-B61"><p><span>61</span></p><div><p>Li</p>  <p>H</p><p>, <span><p>Vincent</p>  <p>N</p></span>, <span><p>Chancellor</p>  <p>S</p></span>, <span><p>Hecht</p>  <p>B</p></span>. </p><p>2023</p><p>. </p><p>The dimensions of data labor: a road map for researchers, activists, and policymakers to empower data producers. In: Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency; Chicago. ACM. p. 1151–1161</p><p>.</p></div></div><div id="ref-auto-pgae400-B62" data-id="pgae400-B62" content-id="pgae400-B62" data-legacy-id="pgae400-B62"><p><span>62</span></p><div><p>Johnson</p>  <p>S</p><p>, <span><p>Acemoglu</p>  <p>D</p></span>. </p><p>2023</p><p>. </p><p>Power and progress: our thousand-year struggle over technology and prosperity</p><p>. </p><p>UK</p><p>: </p><p>Hachette</p><p>.</p><!--citationLinks: case 2--></div></div><div id="ref-auto-pgae400-B63" data-id="pgae400-B63" content-id="pgae400-B63" data-legacy-id="pgae400-B63"><p><span>63</span></p><div><p>Lehdonvirta</p>  <p>V</p><p>. </p><p>2022</p><p>. </p><p>Cloud empires: how digital platforms are overtaking the state and how we can regain control</p><p>. </p><p>Cambridge</p><p>: </p><p>MIT Press</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B64" data-id="pgae400-B64" content-id="pgae400-B64" data-legacy-id="pgae400-B64"><p><span>64</span></p><div><p>Kummer</p>  <p>M</p><p>, <span><p>Slivko</p>  <p>O</p></span>, <span><p>Zhang</p>  <p>X</p></span>. </p><p>2020</p><p>. </p><p>Unemployment and digital public goods contribution</p><p>. </p><p>Inform Syst Res</p><p>. </p><p>31</p><p>(</p><p>3</p><p>):</p><p>801</p><p>–</p><p>819</p><p>.</p><!--citationLinks: case 1--></div></div><div id="ref-auto-pgae400-B65" data-id="pgae400-B65" content-id="pgae400-B65" data-legacy-id="pgae400-B65"><p><span>65</span></p><div><p>Anderson</p>  <p>A</p><p>, <span><p>Huttenlocher</p>  <p>D</p></span>, <span><p>Kleinberg</p>  <p>J</p></span>, <span><p>Leskovec</p>  <p>J</p></span>. </p><p>2012</p><p>. </p><p>Discovering value from community activity on focused question answering sites: a case study of Stack Overflow. In: Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining; Beijing. ACM. p. 850–858</p><p>.</p></div></div><div id="ref-auto-pgae400-B66" data-id="pgae400-B66" content-id="pgae400-B66" data-legacy-id="pgae400-B66"><p><span>66</span></p><div><p>Mamykina</p>  <p>L</p><p>, <span><p>Manoim</p>  <p>B</p></span>, <span><p>Mittal</p>  <p>M</p></span>, <span><p>Hripcsak</p>  <p>G</p></span>, <span><p>Hartmann</p>  <p>B</p></span>. </p><p>2011</p><p>. </p><p>Design lessons from the fastest Q&amp;A site in the west. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems; Vancouver. ACM. p. 2857–2866</p><p>.</p></div></div><div id="ref-auto-pgae400-B67" data-id="pgae400-B67" content-id="pgae400-B67" data-legacy-id="pgae400-B67"><p><span>67</span></p><div><p>Franzke</p>  <p>AS</p><p>, <span><p>Bechmann</p>  <p>A</p></span>, <span><p>Zimmer</p>  <p>M</p></span>, <span><p>Ess</p>  <p>C</p></span>, </p><p>The Association of Internet Researchers</p><p>. </p><p>2020</p><p>. </p><p>Internet research: ethical guidelines 3.0. Technical Report, Association of Internet Researchers</p><p>.</p></div></div><div id="ref-auto-pgae400-B68" data-id="pgae400-B68" content-id="pgae400-B68" data-legacy-id="pgae400-B68"><p><span>68</span></p><div><p>Gousios</p>  <p>G</p><p>, <span><p>Spinellis</p>  <p>D</p></span>. </p><p>2012</p><p>. </p><p>GHTorrent: GitHub’s data from a firehose. In: 2012 9th IEEE Working Conference on Mining Software Repositories (MSR); Zurich. IEEE. p. 12–21</p><p>.</p></div></div><div id="ref-auto-pgae400-B69" data-id="pgae400-B69" content-id="pgae400-B69" data-legacy-id="pgae400-B69"><p><span>69</span></p><div><p>Héder</p>  <p>M</p><p>, <em>et al</em>. </p><p>2022</p><p>. </p><p>The past, present and future of the ELKH cloud</p><p>. </p><p>Inform Társadalom</p><p>. </p><p>22</p><p>(</p><p>2</p><p>):</p><p>128</p><p>.</p><!--citationLinks: case 1--></div></div></div>    <!-- /foreach in Model.Sections -->
    



        

        
                    <h2 id="authorNotesSectionTitle" scrollto-destination="authorNotesSectionTitle">Author notes</h2>
<p><span><p><strong>Competing Interest:</strong> The authors declare no competing interests.</p></span></p><p>© The Author(s) 2024. Published by Oxford University Press on behalf of National Academy of Sciences.</p><div><p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">https://creativecommons.org/licenses/by/4.0/</a>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</p></div><!-- /foreach -->

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Eating less can lead to a longer life: study in mice shows why (200 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-024-03277-6</link>
            <guid>41826449</guid>
            <pubDate>Sun, 13 Oct 2024 09:25:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-024-03277-6">https://www.nature.com/articles/d41586-024-03277-6</a>, See on <a href="https://news.ycombinator.com/item?id=41826449">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <header>
        <div>
            <ul data-test="article-identifier">
                <li data-test="article-category"><span>NEWS</span></li>
                <li><time datetime="2024-10-09">09 October 2024</time></li>
                
            </ul>

            

            <div>
                
                <p>
                    Weight loss and metabolic improvements do not explain the longevity benefits of severe dietary restrictions.
                </p>
            </div>
        </div>
        
            <div data-test="author-info">
    <ol>
        
            <li>
                
                    <span>Elie Dolgin</span>
                
                
                    <ol>
                        <li id="Aff0">
                            <p>Elie Dolgin is a science journalist in Somerville, Massachusetts.</p>
                        </li>
                    </ol>
                
                
                    
                
            </li>
        
    </ol>
</div>
        
    </header>
    
</div><div>
                    
                        <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-03277-6/d41586-024-03277-6_27701126.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-03277-6/d41586-024-03277-6_27701126.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Coloured scanning electron micrograph of fat cells shown in various shades of pink" loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-03277-6/d41586-024-03277-6_27701126.jpg">
  <figcaption>
   <p><span>Fat cells (artificially coloured). Restrictive diets cause fat loss and lengthen life, but the two effects are not necessarily linked.</span><span>Credit: Steve Gschmeissner/SPL</span></p>
  </figcaption>
 </picture>
</figure><p>Cutting calorie intake can lead to a leaner body — <a href="https://www.nature.com/articles/nature.2014.14963" data-track="click" data-label="https://www.nature.com/articles/nature.2014.14963" data-track-category="body text link">and a longer life</a>, an effect often chalked up to the weight loss and <a href="https://www.nature.com/articles/d41586-018-03431-x" data-track="click" data-label="https://www.nature.com/articles/d41586-018-03431-x" data-track-category="body text link">metabolic changes caused by consuming less food</a>. Now, one of the biggest studies<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup> of dietary restrictions ever conducted in laboratory animals challenges the conventional wisdom about how dietary restriction boosts longevity.</p><p>The study, involving nearly 1,000 mice fed low-calorie diets or subjected to regular bouts of <a href="https://www.nature.com/articles/d41586-024-02700-2" data-track="click" data-label="https://www.nature.com/articles/d41586-024-02700-2" data-track-category="body text link">fasting</a>, found that such regimens do indeed cause weight loss and related metabolic changes. But other factors — including <a href="https://www.nature.com/articles/d41586-024-00871-6" data-track="click" data-label="https://www.nature.com/articles/d41586-024-00871-6" data-track-category="body text link">immune health</a>, genetics and physiological indicators of resiliency — seem to better explain the link between cutting calories and increased lifespan.</p><p>“The metabolic changes are important,” says Gary Churchill, a mouse geneticist at the Jackson Laboratory in Bar Harbor, Maine, who co-led the study. “But they don’t lead to lifespan extension.”</p><p>To outside investigators, the results drive home the intricate and individualized nature of the body’s reaction to caloric restriction. “It’s revelatory about the complexity of this intervention,” says James Nelson, a biogerontologist at the University of Texas Health Science Center in San Antonio.</p><p>The study was published today in <i>Nature</i> by Churchill and his co-authors, including scientists at Calico Life Sciences in South San Francisco, California, the anti-ageing focused biotech company that funded the study.</p><h2>Counting calories</h2><p>Scientists have long known that caloric restriction, a regimen of long-term limits on food intake, lengthens lifespan in laboratory animals<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup>. Some studies<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup><sup>,</sup><sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup> have shown that intermittent fasting, which involves <a href="https://www.nature.com/articles/d41586-021-01578-8" data-track="click" data-label="https://www.nature.com/articles/d41586-021-01578-8" data-track-category="body text link">short bouts of food deprivation</a>, can also increase longevity.</p><p>To learn more about how such diets work, the researchers monitored the health and longevity of 960 mice, each a genetically distinct individual drawn from a diverse population that mirrors the genetic variability found in humans. Some mice were placed on calorie-limited diets, another group followed intermittent fasting regimens, and others were allowed to eat freely.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-024-03244-1" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-03277-6/d41586-024-03277-6_27700972.jpg"><p>Life expectancy rise in rich countries slows down: why discovery took 30 years to prove</p></a>
 </article><p>Cutting calories by 40% yielded the longest longevity bump, but intermittent fasting and less severe calorie restriction also increased average lifespan. The dieting mice also displayed favourable metabolic changes, such as reductions in body fat and blood sugar levels.</p><p>However, the effects of dietary restriction on metabolism and lifespan didn’t always change in lockstep. To the authors’ surprise, the mice that lost the most weight on a calorie-limited diet tended to die younger than did animals that lost relatively modest amounts.</p><p>This suggests that processes beyond simple metabolic regulation drive how the body responds to limited-calorie regimes. What mattered most for lengthening lifespan were traits related to immune health and red-blood-cell function. Also key was overall resilience, presumably encoded in the animals’ genes, to the stress of reduced food intake.</p><p>“The intervention is a stressor,” Churchill explains. The most-resilient animals lost the least weight, maintained immune function and lived longer.</p><h2>Leanness for longevity</h2><p>The findings could reshape how scientists think about studies of dietary restriction in humans. In one of the most comprehensive clinical trials of a low-calorie diet in healthy, non-obese individuals, researchers found<sup><a href="#ref-CR5" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">5</a></sup> that <a href="https://www.nature.com/articles/d41586-018-03431-x" data-track="click" data-label="https://www.nature.com/articles/d41586-018-03431-x" data-track-category="body text link">the intervention helped to dial down metabolic rates</a> — a short-term effect thought to signal longer-term benefits for lifespan.</p><p>But the mouse data from Churchill’s team suggest that metabolic measurements might reflect <a href="https://www.nature.com/articles/529154a" data-track="click" data-label="https://www.nature.com/articles/529154a" data-track-category="body text link">‘healthspan’ — the period of life spent free from chronic disease and disability</a> — but that other metrics are needed to say whether such ‘anti-ageing’ strategies can truly extend life.</p><p>Daniel Belsky, an epidemiologist who studies ageing at the Columbia University Mailman School of Public Health in New York City, cautions against over-extrapolating from mice to humans. But he also acknowledges that the study “adds to the growing understanding we have that healthspan and lifespan are not the same thing”.</p>
                    
                </div><div id="references" aria-labelledby="Bib1"><h2 id="Bib1">References</h2><div data-container-section="references" id="Bib1-content"><ol data-track-component="outbound reference" data-track-context="references section"><li data-counter="1."><p id="ref-CR1">Di Francesco, A. <i>et al.</i> <i>Nature</i> https://doi.org/10.1038/s41586-024-08026-3 (2024).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41586-024-08026-3" data-track-item_id="10.1038/s41586-024-08026-3" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41586-024-08026-3" aria-label="Article reference 1" data-doi="10.1038/s41586-024-08026-3">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Nature&amp;doi=10.1038%2Fs41586-024-08026-3&amp;publication_year=2024&amp;author=Di%20Francesco%2CA.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="2."><p id="ref-CR2">Fontana, L., Partridge, L. &amp; Longo, V. D. <i>Science</i> <b>328</b>, 321–326 (2010).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1126/science.1172539" data-track-item_id="10.1126/science.1172539" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.1172539" aria-label="Article reference 2" data-doi="10.1126/science.1172539">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20395504" aria-label="PubMed reference 2">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Science&amp;doi=10.1126%2Fscience.1172539&amp;volume=328&amp;pages=321-326&amp;publication_year=2010&amp;author=Fontana%2CL.&amp;author=Partridge%2CL.&amp;author=Longo%2CV.%20D.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="3."><p id="ref-CR3">Mitchell, S. <i>et al.</i> <i>Cell Metab.</i> <b>29</b>, 221–228.e3 (2019).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.cmet.2018.08.011" data-track-item_id="10.1016/j.cmet.2018.08.011" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cmet.2018.08.011" aria-label="Article reference 3" data-doi="10.1016/j.cmet.2018.08.011">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30197301" aria-label="PubMed reference 3">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Cell%20Metab.&amp;doi=10.1016%2Fj.cmet.2018.08.011&amp;volume=29&amp;pages=221-228.e3&amp;publication_year=2019&amp;author=Mitchell%2CS.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="4."><p id="ref-CR4">Duregon, E. <i>et al.</i> <i>Cell Metab.</i> <b>35</b>, 1179–1194.e5 (2023).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.cmet.2023.05.003" data-track-item_id="10.1016/j.cmet.2023.05.003" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cmet.2023.05.003" aria-label="Article reference 4" data-doi="10.1016/j.cmet.2023.05.003">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=37437544" aria-label="PubMed reference 4">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Cell%20Metab.&amp;doi=10.1016%2Fj.cmet.2023.05.003&amp;volume=35&amp;pages=1179-1194.e5&amp;publication_year=2023&amp;author=Duregon%2CE.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="5."><p id="ref-CR5">Ravussin, E. <i>et al.</i> <i>J. Gerentol. Ser. A</i> <b>70</b>, 1097–1104 (2015).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1093/gerona/glv057" data-track-item_id="10.1093/gerona/glv057" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1093%2Fgerona%2Fglv057" aria-label="Article reference 5" data-doi="10.1093/gerona/glv057">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=J.%20Gerentol.%20Ser.%20A&amp;doi=10.1093%2Fgerona%2Fglv057&amp;volume=70&amp;pages=1097-1104&amp;publication_year=2015&amp;author=Ravussin%2CE.">
                    Google Scholar</a>&nbsp;
                </p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/d41586-024-03277-6?format=refman&amp;flavour=references">Download references</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Playable Counter-Strike Diffusion World Model (trained on 2x4090, 5M frames) (393 pts)]]></title>
            <link>https://diamond-wm.github.io/</link>
            <guid>41826402</guid>
            <pubDate>Sun, 13 Oct 2024 09:18:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diamond-wm.github.io/">https://diamond-wm.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=41826402">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <h2>How does it work?</h2>
            <p>
              We train a diffusion model to predict the next frame of the game.
              The diffusion model takes into account the agent’s action and the previous frames
              to simulate the environment response.
            </p>
            <figure>
              <img src="https://diamond-wm.github.io/static/gifs/kungfu.gif" alt="The diffusion world model takes into account the agent's action and previous frames to generate the next frame.">
              <center><blockquote>The diffusion world model takes into account the agent's action and previous frames to generate the next frame.</blockquote></center>
            </figure>
          <p><br>
            The agent repeatedly provides new actions, and the diffusion model updates the game.
          </p>
          <p>
            The diffusion model acts as a world model in which the agent can learn to play.
          </p>
          <center>
            <img src="https://diamond-wm.github.io/static/gifs/imagination.gif" alt="Autoregressive generation with diffusion world model.">
            <figcaption><blockquote>Autoregressive generation enables the diffusion model to act as a world model in which the agent can learn to play.</blockquote></figcaption>
          </center>
          
            <p>
            To make the world model fast, we need to reduce the number of denoising steps.
            We found <a href="https://arxiv.org/abs/2006.11239">DDPM</a> (Ho et al. 2020) to become unstable with low numbers of denoising steps.
            In contrast, we found <a href="https://arxiv.org/abs/2206.00364">EDM</a> (Karras et al., 2022) to produce stable trajectories even for 1 denoising step.
          </p>
          <center>
            <img src="https://diamond-wm.github.io/static/gifs/ddpm.gif" alt="DDPM vs EDM based diffusion world models. The DDPM-based model becomes unstable for low numbers of denoising steps, while the EDM-based model remains stable.">
            <figcaption><blockquote>The DDPM-based model is unstable for low numbers of denoising steps due to accumulating autoregressive error, while the EDM-based model remains stable. Lower denoising steps enables a faster world model.</blockquote></figcaption>
          </center>
          
          <p>
            But in Boxing, 1-step denoising interpolates between possible outcomes and results in blurry predictions for the unpredictable black player.
          </p>
          <p>
            In contrast, using more denoising steps enables better selection of a particular mode, improving consistency over time.
          </p>
          <center>
            <img src="https://diamond-wm.github.io/static/gifs/boxing.gif" alt="Diffusion world model trajectories for the Atari game Boxing for varying numbers of denoising steps.">
            <figcaption><blockquote>Larger numbers of denoising steps n enable better mode selection for transitions with multiple modes. We therefore use n=3 for Diamond's diffusion world model.</blockquote></figcaption>
          </center>
          
          <p>
            Interestingly, the white player's movements are predicted correctly regardless of the number of denoising steps.
            This is because it is controlled by the policy, so its actions are given to the world model. This removes any ambiguity that can cause blurry predictions.
          </p>
          <p>
            We find that diffusion-based DIAMOND provides better modeling of important visual details than the discrete token-based <a href="https://arxiv.org/abs/2209.00588">IRIS</a>.
          </p>
          <center>
            <img src="https://diamond-wm.github.io/static/gifs/iris.gif" alt="Visualisation of IRIS and DIAMOND world's models on Asterix, Breakout and RoadRunner.">
            <figcaption><blockquote>DIAMOND's world model is able to better capture important visual details than the discrete token-based IRIS.</blockquote></figcaption>
          </center>
          <br>
          <div><p>
            Training an agent with reinforcement learning on this diffusion world model, DIAMOND achieves a mean human-normalized score of 1.46 on Atari 100k (46% better than human); a new best for agents trained in a world model on 100k frames.
            </p><p>
            
            Check out our <a href="https://arxiv.org/pdf/2405.12399">paper</a> for more details!
          </p></div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WordPress.org's latest move involves taking control of a WP Engine plugin (254 pts)]]></title>
            <link>https://www.theverge.com/2024/10/12/24268637/wordpress-org-matt-mullenweg-acf-fork-secure-custom-fields-wp-engine</link>
            <guid>41826082</guid>
            <pubDate>Sun, 13 Oct 2024 08:05:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/10/12/24268637/wordpress-org-matt-mullenweg-acf-fork-secure-custom-fields-wp-engine">https://www.theverge.com/2024/10/12/24268637/wordpress-org-matt-mullenweg-acf-fork-secure-custom-fields-wp-engine</a>, See on <a href="https://news.ycombinator.com/item?id=41826082">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>WordPress.org has taken over a popular WP Engine plugin in order “to remove commercial upsells and fix a security problem,” WordPress cofounder and Automattic CEO Matt Mullenweg <a href="https://wordpress.org/news/2024/10/secure-custom-fields/">announced today</a>. This “minimal” update, which he labels a fork of the Advanced Custom Fields (ACF) plugin, is <a href="https://wordpress.org/plugins/advanced-custom-fields/">now called</a> “Secure Custom Fields.”</p><p>It’s not clear what security problem Mullenweg is referring to in the post. He writes that he’s “invoking point 18 of the plugin directory guidelines,” <a href="https://github.com/wordpress/wporg-plugin-guidelines/blob/trunk/guideline-18.md">in which</a> the WordPress team reserves several rights, including removing a plugin, or changing it “without developer consent.” Mullenweg explains that the move has to do with WP Engine’s <a href="https://www.theverge.com/2024/10/3/24261016/wordpress-wp-engine-lawsuit-automattic-matt-mullenweg">recently-filed lawsuit</a> against him and Automattic.</p><div><blockquote><p>Similar situations have happened before, but not at this scale. This is a rare and unusual situation brought on by WP Engine’s legal attacks, we do not anticipate this happening for other plugins.</p></blockquote></div><p>WP Engine’s ACF team <a href="https://x.com/wp_acf/status/1845169499064107049">claimed on X</a> that WordPress has never “unilaterally and forcibly” taken a plugin “from its creator without consent.” It later <a href="https://x.com/wp_acf/status/1845190372764401908?s=46&amp;t=s7yjJ2YTk92nj3NQJLk0ww">wrote</a> that those who aren’t WP Engine, Flywheel, or ACF Pro customers will need to go to the ACF site and follow steps it <a href="https://www.advancedcustomfields.com/blog/installing-and-upgrading-to-the-latest-version-of-acf/#update-acf">published earlier</a> to “perform a 1-time download of the genuine 6.3.8 version” to keep getting updates.</p><p>As its name implies, the ACF plugin allows website creators to use custom fields when existing generic ones won’t do — something ACF’s <a href="https://www.advancedcustomfields.com/resources/getting-started-with-acf/">overview</a> of the plugin says is already a native, but “not very user friendly,” feature of WordPress. </p><p><em>The Verge</em> has reached out to Automattic, <a href="http://wordpress.org/">WordPress.org</a>, and WP Engine for comment.</p><p><em><strong>Update October 12th: </strong>Adjusted to add clarity about Mullenweg’s use of the “fork” label.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ACF Has Been Hijacked (223 pts)]]></title>
            <link>https://anderegg.ca/2024/10/13/acf-has-been-hijacked</link>
            <guid>41824852</guid>
            <pubDate>Sun, 13 Oct 2024 03:05:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anderegg.ca/2024/10/13/acf-has-been-hijacked">https://anderegg.ca/2024/10/13/acf-has-been-hijacked</a>, See on <a href="https://news.ycombinator.com/item?id=41824852">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
				<article>
					
					<h3>October 13, 2024</h3>
					<p>It’s super late at night on Thanksgiving weekend in Canada. I shouldn’t be thinking about weird internet drama, but here we are.</p>

<p><a href="https://anderegg.ca/2024/10/10/loyalty-test-checkbox">Since I last wrote</a> about the ongoing WordPress drama:</p>

<ul>
  <li>Matt Mullenweg promoted a “fork” of WordPress that wasn’t actually a fork.</li>
  <li>He then hijacked one of the most prominent plugins in the WordPress development world.</li>
</ul>

<p>The first point is pretty minor, but highlights the depths of strangeness at play. <a href="https://x.com/vinnysgreen">Vinny Green</a> created <a href="https://freewp.com/">a project called FreeWP</a> that… actually, I don’t actually know what it’s about. You need to read the site for yourself. It seems to be an announcement of an organization that includes a news site, a class action lawsuit, and <a href="https://freewp.com/faq/">some other things</a>. It’s not really clear to me, and it seems like it might be an elaborate troll.</p>

<p>This normally wouldn’t be news, <a href="https://wordpress.org/news/2024/10/spoon/">except Matt Mullenweg made it so</a>. It’s uncertain if Mullenweg even understood what Green was building, but Green was quick to point out that <a href="https://x.com/vinnysgreen/status/1844488053060141233">it wasn’t a fork of WordPress</a>. Mullenweg then amended his post to include <a href="https://aspirepress.org/about-us/">AspirePress</a>, and noted a spelling error. The whole thing seems strange, but I’m assuming that Mullenweg wrote the blog post to make fun of potential WordPress forks.</p>

<p>The bigger issue happened on Saturday when <a href="https://wordpress.org/news/2024/10/secure-custom-fields/">Automattic hijacked the Advanced Custom Fields (ACF) plugin</a>. As I’ve written in the past, <a href="https://anderegg.ca/2024/10/06/wordpress-vs-acf#:~:text=ACF%20is%20a%20WordPress%20plugin%20that%20is%20a%20requirement%20for%20many%20WordPress%20builds">ACF is a major plugin in the WordPress development world</a>, and a requirement for many custom websites. It’s also owned by WP Engine, the company Mullenweg is beefing with. In the previous link, I had guessed that Mullenweg intended to kick them out of the WordPress plugin directory. Turns out, they went one further.</p>

<p>In a post titled “<a href="https://wordpress.org/news/2024/10/secure-custom-fields/">Secure Custom Fields</a>”, and in the category “Security”, Mullenweg posted that he was invoking “<a href="https://github.com/wordpress/wporg-plugin-guidelines/blob/trunk/guideline-18.md">point 18</a>” of the plugin directory guidelines to hijack the ACF plugin. ACF is ostensibly offered under the GPL because it interfaces with WordPress. That said, it doesn’t have an explicit license listed on <a href="https://github.com/AdvancedCustomFields/acf">its GitHub page</a>. Still, it’s almost certainly legal and reasonable for Automattic to fork it and do whatever it wants with the source code.</p>

<p>But that’s not the point. ACF is something that WordPress users trust and expect to come from a specific source. That Automattic would unilaterally decide to hijack such a popular plugin is completely insane. I’m not sure how this differs from a supply-chain attack. As I’ve written, the reason for this is invented and <a href="https://anderegg.ca/2024/10/06/wordpress-vs-acf#:~:text=The%20issue%20here%20is%20that%20the%20ACF%20team%20has%20been%20blocked%20by%20Mullenweg%20from%20accessing%20WordPress.org%20and%20the%20infrastructure%20it%20provides.">brought on by Automattic’s blocking of WP Engine employees from WordPress.org</a>. Automattic just happened to find a mild vulnerability in ACF, and is now using <strong>the block Automattic imposed</strong> as a reason to take control of the plugin because the ACF team can’t update the plugin while the block is in place.</p>

<p>This is some Grade-A 100% bullshit.</p>

<p>The ACF site has been <a href="https://www.advancedcustomfields.com/">updated with a notice about the takeover</a>, but most users likely won’t see this. The team behind the WordPress plugin directory could now update ACF to make any changes they’d like. If they’re willing to do this, I wouldn’t trust any plugins hosted on WordPress.org.</p>

<p>I really don’t know what to say at this point. I assumed that ACF would be removed from the WordPress plugin directory, but I never would have guessed it would be hijacked. It seems like Mullenweg has lost the plot completely.</p>

<p>If you use WordPress for a living, I recommend strongly that you consider changing platforms.</p>

				</article>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FLUX is fast and it's open source (232 pts)]]></title>
            <link>https://replicate.com/blog/flux-is-fast-and-open-source</link>
            <guid>41824390</guid>
            <pubDate>Sun, 13 Oct 2024 01:28:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://replicate.com/blog/flux-is-fast-and-open-source">https://replicate.com/blog/flux-is-fast-and-open-source</a>, See on <a href="https://news.ycombinator.com/item?id=41824390">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    
    

<article>
  <hgroup>
    

    <p>
      Posted
      <time datetime="2024-10-10">
        October 10, 2024
      </time>
      by
      

      
      <a href="https://replicate.com/bfirsh">@bfirsh</a>

      
    </p>
  </hgroup>

  
<div>
  <p>FLUX is now much faster on Replicate, and we’ve made our optimizations open-source so you can see exactly how they work and build upon them.</p>
<p>Here are the end-to-end speeds:</p>
<ul>
<li><a href="https://replicate.com/black-forest-labs/flux-schnell">FLUX.1 [schnell]</a> at 512x512 and 4 steps: 0.29 seconds (P90: 0.49 seconds)</li>
<li><a href="https://replicate.com/black-forest-labs/flux-schnell">FLUX.1 [schnell]</a> at 1024x1024 and 4 steps: 0.72 seconds (P90: 0.95 seconds)</li>
<li><a href="https://replicate.com/black-forest-labs/flux-dev">FLUX.1 [dev]</a> at 1024x1024 and 28 steps: 3.03 seconds (P90: 3.90 seconds)</li>
</ul>
<p>This is from the west coast of the US using the Python client.</p>
<p>Here’s a demo of FLUX.1 [schnell]. (It’s live, just start typing!)</p>


<p>Here’s <a href="https://fast-flux-demo.replicate.workers.dev/">the full app, and source code</a>, if you’d like to check it out.</p>
<h2 id="how-did-we-do-it">How did we do it?</h2>
<p>Most of the models on Replicate are contributed by our community, but we maintain the FLUX models in collaboration with <a href="https://blackforestlabs.ai/">Black Forest Labs</a>.</p>
<p>We’ve done two main things to make FLUX faster:</p>
<ul>
<li>We optimized the model. We used Alex Redden’s <a href="https://github.com/aredden/flux-fp8-api">flux-fp8-api</a> as a starting point, then optimized it with <code>torch.compile</code> and used fast CuDNN attention kernels in the nightly Torch builds.</li>
<li>We added a new <a href="https://replicate.com/changelog/2024-10-09-synchronous-api">synchronous HTTP API</a> that makes all image models much faster on Replicate.</li>
</ul>
<p>The quantization in flux-fp8-api slightly changes the output of the model, but we have found it has little impact on the quality.</p>
<p><a href="https://flux-quality-comparison.vercel.app/">
<img src="https://d31rfu1d3w8e4q.cloudfront.net/static/blog/flux-is-fast/slow.webp" title="Guess which is which!">
</a>
<a href="https://flux-quality-comparison.vercel.app/">
<img src="https://d31rfu1d3w8e4q.cloudfront.net/static/blog/flux-is-fast/fast.webp" title="Guess which is which!">
</a>
</p>

<p>We’ve created a tool that compares the output of thousands of prompts on FLUX.1 [schnell] and FLUX.1 [dev]. We’re not cherry picking. <a href="https://flux-quality-comparison.vercel.app/">Take a look for yourself.</a></p>
<p>You can disable this by setting the <code>go_fast</code> input on the model to <code>false</code>.</p>
<p>We want to be open with you about how we’re optimizing the models. It’s notoriously hard to compare output between models and providers, and it’s often unclear whether providers are doing things that impact the quality of the model.</p>
<p>We’re just going to tell you how we did it and let you disable any optimizations. That means you’re not wondering whether the output you’re getting is the best quality it can be.</p>
<p>Most importantly, the code is open-source, so you can see exactly how it works: <a href="https://github.com/replicate/cog-flux">github.com/replicate/cog-flux</a></p>
<h2 id="open-source-should-be-fast-too">Open-source should be fast too</h2>
<p>Open-source models are often slow out of the box. Model providers then optimize these models to make them fast and release them behind proprietary APIs, without contributing the improvements back to the community.</p>
<p>We want to change that. We think open-source should be fast too.</p>
<p><a href="https://github.com/replicate/cog-flux">We’re open-sourcing all the improvements we make to FLUX.</a> We’re also collaborating with the <a href="https://github.com/ai-compiler-study">AI Compiler Study Group</a> and other AI researchers to make an open-source fast version of FLUX. </p>
<p>Making the FLUX optimizations open-source is not just the right thing to do, it also means all the experts in the world can collaborate together to make it the fastest. Pull requests welcome.</p>
<h2 id="its-going-to-get-faster">It’s going to get faster</h2>
<p>New techniques are coming out all the time to make models faster, and by collaborating with the community, you can be sure that they’re going to be on Replicate as fast as possible. Stay tuned.</p>
<h2 id="do-more-with-flux">Do more with FLUX</h2>
<p>You can do more than just run FLUX on Replicate. You can:</p>
<ul>
<li><a href="https://replicate.com/blog/fine-tune-flux">Fine-tune FLUX on your own data</a> (training and running trained models is going to much faster soon too!)</li>
<li><a href="https://github.com/replicate/cog-flux">Edit the code and deploy a custom version</a>, if you’re doing something advanced</li>
<li><a href="https://replicate.com/playground/">Try out the models and compare outputs on our new playground</a></li>
</ul>
<p><a href="https://x.com/replicate">Follow us on X</a> to keep up to speed.</p>
</div>


  
</article>



    
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Omni SenseVoice: High-Speed Speech Recognition with Words Timestamps (154 pts)]]></title>
            <link>https://github.com/lifeiteng/OmniSenseVoice</link>
            <guid>41824171</guid>
            <pubDate>Sun, 13 Oct 2024 00:48:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/lifeiteng/OmniSenseVoice">https://github.com/lifeiteng/OmniSenseVoice</a>, See on <a href="https://news.ycombinator.com/item?id=41824171">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Omni SenseVoice 🚀</h2><a id="user-content-omni-sensevoice-" aria-label="Permalink: Omni SenseVoice 🚀" href="#omni-sensevoice-"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The Ultimate Speech Recognition Solution</h2><a id="user-content-the-ultimate-speech-recognition-solution" aria-label="Permalink: The Ultimate Speech Recognition Solution" href="#the-ultimate-speech-recognition-solution"></a></p>
<p dir="auto">Built on <a href="https://github.com/FunAudioLLM/SenseVoice">SenseVoice</a>, Omni SenseVoice is optimized for lightning-fast inference and precise timestamps—giving you a smarter, faster way to handle audio transcription!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div data-snippet-clipboard-copy-content="omnisense transcribe [OPTIONS] AUDIO_PATH"><pre><code>omnisense transcribe [OPTIONS] AUDIO_PATH
</code></pre></div>
<p dir="auto">Key Options:</p>
<ul dir="auto">
<li><code>--language</code>: Automatically detect the language or specify (<code>auto, zh, en, yue, ja, ko</code>).</li>
<li><code>--textnorm</code>: Choose whether to apply inverse text normalization (<code>withitn for inverse normalized</code> or <code>woitn for raw</code>).</li>
<li><code>--device-id</code>: Run on a specific GPU (default: -1 for CPU).</li>
<li><code>--quantize</code>: Use a quantized model for faster processing.</li>
<li><code>--help</code>: Display detailed help information.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmark</h2><a id="user-content-benchmark" aria-label="Permalink: Benchmark" href="#benchmark"></a></p>
<p dir="auto"><code>omnisense benchmark -s -d --num-workers 2 --device-id 0 --batch-size 10 --textnorm woitn --language en benchmark/data/manifests/libritts/libritts_cuts_dev-clean.jsonl</code></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Optimize</th>
<th>GPU</th>
<th>WER ⬇️</th>
<th>RTF ⬇️</th>
<th>Speed Up 🔥</th>
</tr>
</thead>
<tbody>
<tr>
<td>baseline(onnx)</td>
<td>NVIDIA L4 GPU</td>
<td>4.47%</td>
<td>0.1200</td>
<td>1x</td>
</tr>
<tr>
<td>torch</td>
<td>NVIDIA L4 GPU</td>
<td>5.02%</td>
<td>0.0022</td>
<td>50x</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<ul dir="auto">
<li>With Omni SenseVoice, experience up to 50x faster processing without sacrificing accuracy.</li>
</ul>
<div data-snippet-clipboard-copy-content="# LibriTTS
DIR=benchmark/data
lhotse download libritts -p dev-clean benchmark/dataLibriTTS
lhotse prepare libritts -p dev-clean benchmark/data/LibriTTS/LibriTTS benchmark/data/manifests/libritts

lhotse cut simple --force-eager -r benchmark/data/manifests/libritts/libritts_recordings_dev-clean.jsonl.gz \
    -s benchmark/data/manifests/libritts/libritts_supervisions_dev-clean.jsonl.gz \
    benchmark/data/manifests/libritts/libritts_cuts_dev-clean.jsonl

omnisense benchmark -s -d --num-workers 2 --device-id 0 --batch-size 10 -
-textnorm woitn --language en benchmark/data/manifests/libritts/libritts_cuts_dev-clean.jsonl

omnisense benchmark -s --num-workers 4 --device-id 0 --batch-size 16 --textnorm woitn --language en benchmark/data/manifests/libritts/libritts_cuts_dev-clean.jsonl"><pre><code># LibriTTS
DIR=benchmark/data
lhotse download libritts -p dev-clean benchmark/dataLibriTTS
lhotse prepare libritts -p dev-clean benchmark/data/LibriTTS/LibriTTS benchmark/data/manifests/libritts

lhotse cut simple --force-eager -r benchmark/data/manifests/libritts/libritts_recordings_dev-clean.jsonl.gz \
    -s benchmark/data/manifests/libritts/libritts_supervisions_dev-clean.jsonl.gz \
    benchmark/data/manifests/libritts/libritts_cuts_dev-clean.jsonl

omnisense benchmark -s -d --num-workers 2 --device-id 0 --batch-size 10 -
-textnorm woitn --language en benchmark/data/manifests/libritts/libritts_cuts_dev-clean.jsonl

omnisense benchmark -s --num-workers 4 --device-id 0 --batch-size 16 --textnorm woitn --language en benchmark/data/manifests/libritts/libritts_cuts_dev-clean.jsonl
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing 🙌</h2><a id="user-content-contributing-" aria-label="Permalink: Contributing 🙌" href="#contributing-"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Step 1: Code Formatting</h4><a id="user-content-step-1-code-formatting" aria-label="Permalink: Step 1: Code Formatting" href="#step-1-code-formatting"></a></p>
<p dir="auto">Set up pre-commit hooks:</p>
<div data-snippet-clipboard-copy-content="pip install pre-commit==3.6.0
pre-commit install"><pre><code>pip install pre-commit==3.6.0
pre-commit install
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Step 2: Pull Request</h4><a id="user-content-step-2-pull-request" aria-label="Permalink: Step 2: Pull Request" href="#step-2-pull-request"></a></p>
<p dir="auto">Submit your awesome improvements through a PR. 😊</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The 1/8th Sleep (175 pts)]]></title>
            <link>https://near.blog/the-1-8th-sleep/</link>
            <guid>41824138</guid>
            <pubDate>Sun, 13 Oct 2024 00:43:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://near.blog/the-1-8th-sleep/">https://near.blog/the-1-8th-sleep/</a>, See on <a href="https://news.ycombinator.com/item?id=41824138">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>The eight sleep bed is a popular self-cooling bed. Sleeping on a colder surface not only feels great but often improves sleep as well. This post discusses a cheaper homemade option that I use instead.</p>


<div>
<figure><img decoding="async" src="https://near.blog/wp-content/uploads/2024/10/sleepy.png" alt="" width="394" height="457"><figcaption>A 12℉ difference between the bed surface and the surrounding air. Some ice was used in the cooler prior to the image as measuring surface temperature is challenging with the pictured thermometer.</figcaption></figure></div>


<h3>Why I didn’t Buy an Eight Sleep</h3>



<p>Although many of my friends are happy eight sleep customers, I was put off by the product due to the now-mandatory paired subscription ($200/yr with purchase, annually paired only, can cancel after), the excessive marketing featuring futuristic animations with terms like ‘intelligence’ and ‘autopilot’, constant paid referral links and name dropping of Huberman et al, and the high price points of $3,000 total and $366 for the addition of two pillows and a bed sheet.</p>



<p>I already have a mattress that I like, so I don’t feel the need to replace it. I also inherently dislike the thought of paying a subscription to sleep, and the proprietary nature of the data and cloud app certainly don’t help the case (although I found <a rel="noreferrer noopener" href="https://github.com/bobobo1618/ninesleep" target="_blank">one user who reversed it</a>).</p>







<h3>Popular Alternatives</h3>



<p>Here are a few categories of products in the space:</p>



<ul>
<li>Eight Sleep: $2400 to $7600 depending on options (full mattress)</li>
</ul>



<ul>
<li>Chilipad: $574 to $1300 depending on options (mattress topper only)</li>
</ul>



<ul>
<li>BedJet 3: $429 to $949 (mattress topper only, pretty loud)</li>
</ul>



<ul>
<li>Random Chinese Amazon items: $140-$300 (topper + evaporative cooler, generally not good products)</li>
</ul>



<ul>
<li>Cooling gel mattress pads: $90 (does not actually cool – just a high-ventilation material)</li>
</ul>







<h3>Current Solution</h3>



<p>Here is my current setup, <a rel="noreferrer noopener" href="https://x.com/xannysndtrannys/status/1830167334742606231" target="_blank">inspired fully by this tweet</a>:</p>



<ol>
<li>Low-quality cooling system with a topper: <a rel="noreferrer noopener" href="https://www.amazon.com/dp/B0BZZSQ7GR" target="_blank">Adamson B10 Gray Bed Cooling System</a> ($140)</li>



<li>Replace the above low-quality evaporative cooler with a <a rel="noreferrer noopener" href="https://www.amazon.com/dp/B08C2W5TYW" target="_blank">Poafamx Fish Tank Cooler</a> ($180)</li>



<li>A <a rel="noreferrer noopener" href="https://www.amazon.com/dp/B07RZKRM13" target="_blank">stronger pump</a> ($15) and <a rel="noreferrer noopener" href="https://www.amazon.com/dp/B0CLJNF8LV" target="_blank">small extension tubes</a> ($9)</li>



<li>A <a rel="noreferrer noopener" href="https://www.amazon.com/dp/B09HMZT1C5" target="_blank">small cooler</a> which allows me to get the temperature extra low before bed ($18) </li>



<li>A <a rel="noreferrer noopener" href="https://www.amazon.com/gp/product/B084JML1WN" target="_blank">Smart plug</a> in order to turn the system on and off from my phone ($20)</li>
</ol>



<p>The total came out to $382 (without counting <a rel="noreferrer noopener" href="https://near.blog/personal-finance-tips/" target="_blank">5% off via Amazon</a>) although you could reduce this by at least 15% by <a rel="noreferrer noopener" href="https://www.aliexpress.us/item/3256807457188700.html" target="_blank">purchasing</a> from <a rel="noreferrer noopener" href="https://www.aliexpress.us/item/3256802936516625.html" target="_blank">AliExpress</a> (or much more if you properly source from Alibaba). It’s also possible to save on step one by finding only the mattress topper, but I didn’t want to look through Alibaba for this. </p>



<p>This setup is 1/8th the cost of an eight sleep so I named it <strong>the 1/8th sleep</strong>.</p>







<h3>How It Works</h3>



<p>At first glance it may seem complicated to buy six items, but it’s actually a simple setup. The first item comes with a mattress topper which you can run water through (the low-quality cooling unit it came with was not used). It was placed under the first layer of sheets, making it difficult to notice and aesthetically nonmodifying.</p>



<p>Next you need a way to continuously pump water through the mattress topper, which the pump and cooler are used for (you could just have a pump but the cooler gives you a water reservoir which simplifies the process and allows for more control).</p>



<p>Lastly you need to cool the water, which the fish tank cooler is for (water flows into it and comes out cooler).</p>


<div>
<figure><img decoding="async" loading="lazy" src="https://near.blog/wp-content/uploads/2024/10/cooler.png" alt="" width="362" height="430"><figcaption>An image of the cooler used – I keep it set to as low as it will go which will usually be around 12℉ lower than the air temperature.</figcaption></figure></div>


<p>The extension tubes were purchased so that I could move the setup out of my bedroom entirely both to reduce noise (which was already low – the equivalent of a desktop PC fan) and to ensure any heat output went elsewhere. I placed the tubes near a baseboard and put them under the corner of a door, making it hard to notice I’ve modified my bed at all.</p>



<p>The smart plug allows me to perform a single tap on my phone to turn the system on or off.</p>



<p>As an added bonus you can put ice (or something even colder like dry ice) in the cooler if you’d like to sleep on an extra-cold surface.</p>



<p>The eight sleep bed comes with a sleep tracking app, although I have no reason to believe it is better than a whoop, oura, or apple watch. I use an apple watch to track my sleep which has the benefits of requiring no subscription and of allowing me to export and control my  data (<a rel="noreferrer noopener" href="https://claude.ai/" target="_blank">Claude</a> wrote me a full script to parse my apple health data with just minutes of work</p>







<h3>Tradeoffs Made</h3>



<p>All systems come with trade-offs. Here are some for the 1/8th sleep:</p>



<p>Pros:</p>



<ul>
<li>Significantly cheaper</li>



<li>Ability to excessively lower temperature via ice/dry ice</li>



<li>Ability to further modify the system, e.g. move the cooler out of the room, upgrade only the cooler, cover an arbitrary part of the bed</li>



<li>Can be paired with any mattress – users may keep their existing beds</li>



<li>You don’t have to pay a subscription to sleep</li>
</ul>



<p>Cons:</p>



<ul>
<li>Less aesthetic (This can be improved with a bit of effort – I may 3d-print an optimally-sized encasement)</li>



<li>No ability to heat the mattress</li>



<li>No fine-grained temperature controls via a phone app (I always set it as low as it will cool regardless)</li>



<li>No built-in sleep tracking (I use an Apple watch)</li>



<li>Easier to incorrectly set up: if you don’t tighten tube connectors you could cause a water leak. If you attempt a setup like this it must be thoroughly tested before applying it to your bed!</li>



<li>The mattress topper linked for this setup is for a twin bed, although you could purchase two or find a larger one</li>



<li>Eight sleep loses thousands of dollars in potential revenue (<em>contested</em> – many argue this is a pro)</li>
</ul>







<h3>Conclusion</h3>



<p>This setup is experimental and has only been used for one week. While I’m happy with the results thus far, I wouldn’t suggest it to anyone without an experimental/DIY mindset. This post was made not because the setup is optimal but simply because it seems better to post this than to post nothing at all. This post contains zero referral links.</p>



<p>Special thanks to everyone who responded to <a rel="noreferrer noopener" href="https://x.com/nearcyan/status/1830015788973215834" target="_blank">the initial tweet on the topic</a>, especially <a rel="noreferrer noopener" href="https://x.com/xannysndtrannys/status/1830167334742606231" target="_blank">this response</a> which inspired this setup.</p>



<p><a href="https://near.blog/" target="_blank" rel="noreferrer noopener">Back to my homepage</a></p>
			</div></div>]]></description>
        </item>
    </channel>
</rss>