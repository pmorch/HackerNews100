<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 11 Oct 2024 08:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Tesla Robotaxi (182 pts)]]></title>
            <link>https://www.tesla.com/we-robot</link>
            <guid>41805706</guid>
            <pubDate>Fri, 11 Oct 2024 03:24:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tesla.com/we-robot">https://www.tesla.com/we-robot</a>, See on <a href="https://news.ycombinator.com/item?id=41805706">Hacker News</a></p>
Couldn't get https://www.tesla.com/we-robot: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[$2 H100s: How the GPU Rental Bubble Burst (209 pts)]]></title>
            <link>https://www.latent.space/p/gpu-bubble</link>
            <guid>41805446</guid>
            <pubDate>Fri, 11 Oct 2024 02:19:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.latent.space/p/gpu-bubble">https://www.latent.space/p/gpu-bubble</a>, See on <a href="https://news.ycombinator.com/item?id=41805446">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><em><strong>Swyx’s note:</strong><span> we’re on a roll catching up with former guests! Apart from our recent guest spot on </span><a href="https://www.listennotes.com/podcasts/high-agency-the/why-your-ai-product-needs-ALy02ewNtDC/" rel="">Raza Habib’s chat with Hamel Husain</a><span> (see </span><a href="https://www.latent.space/p/humanloop" rel="">Raza’s first pod here</a><span>). </span></em></p><p><em><span>We’re delighted to welcome Eugene Cheah (see </span><a href="https://www.latent.space/p/rwkv" rel="">his first pod on RWKV last year</a><span>) as a rare guest </span><strong>writer </strong><span>for our newsletter</span><strong>.</strong><span> Eugene has now cofounded </span><a href="https://featherless.ai/" rel="">Featherless.AI</a><span>, an inference platform with the world’s largest collection of open source models (~2,000) instantly accessible via a single API for a </span><strong>flat rate</strong><span> ($10-$75+ a month).</span></em></p><p><em><span>Recently there has been a lot of excitement with NVIDIA’s new Blackwell series rolling out to OpenAI, with the company saying it is </span><a href="https://x.com/firstadopter/status/1844417947277852925" rel="">sold out for the next year</a><span> and Jensen noting that it could be the “</span><a href="https://x.com/The_AI_Investor/status/1844080690046058843" rel="">most successful product in the history of the industry</a><span>”. With cousin Lisa hot on his heels </span><a href="https://www.cnbc.com/2024/10/10/amd-launches-mi325x-ai-chip-to-rival-nvidias-blackwell-.html" rel="">announcing the MI3 25 X</a><span> and </span><a href="https://news.ycombinator.com/item?id=41702789" rel="">Cerebras filing for IPO</a><span>, it is time to dive deep on the GPU market again (see also </span><a href="https://www.latent.space/p/semianalysis" rel="">former guest</a><span> </span><a href="https://www.dwarkeshpatel.com/p/dylan-jon" rel="">Dylan Patel’s pod</a><span> for his trademark candid take on the industry of course): </span></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png" width="421" height="497.04655493482306" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1268,&quot;width&quot;:1074,&quot;resizeWidth&quot;:421,&quot;bytes&quot;:1493562,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><em><span>Do we yet have an answer to </span><a href="https://www.latent.space/p/mar-jun-2024" rel="">the $600bn question</a><span>? It is now consensus that the capex on foundation model training is the “</span><a href="https://x.com/GavinSBaker/status/1720819375517716610" rel="">fastest depreciating asset in history</a><span>”, but the jury on GPU infra spend is still out and </span><a href="https://www.latent.space/i/140396949/mixtral-sparks-a-gpuinference-race-to-the-bottom" rel="">the GPU Rich Wars are raging</a><span>.</span></em></p><p><em><span>What follows is Eugene’s take on GPU economics as he is now an inference provider, diving deep on the H100 market, as a possible read for what is to come for the Blackwell generation. Not financial advice! We also recommend </span><a href="https://blog.lepton.ai/the-missing-guide-to-the-h100-gpu-market-91ebfed34516" rel="">Yangqing Jia’s guide</a><span>.</span></em></p><p><em><strong>TLDR: Don’t buy H100s. The market has flipped from shortage ($8/hr) to oversupplied ($2/hr), because of reserved compute resales, open model finetuning, and decline in new foundation model co’s. Rent instead.</strong><p><span>(Unless you have some combination of discounted H100s, discounted electricity, or a Sovereign AI angle where the location of your GPU is critical to your customers, or you have billions and need a super large cluster for frontier model training)</span></p><p><span>For the general market, it makes little sense to be investing in new H100s today, when </span><strong>you can rent it at near cost, when you need it</strong><span>, with the current oversupply.</span></p></em></p><p><span>ChatGPT was launched in November 2022, built on the A100 series. The H100s arrived in March 2023. </span><strong>The pitch to investors and founders was simple: </strong><span>Compared to A100s, </span><strong>the new H100s were 3x more powerful, but only 2x the sticker price</strong><span>.</span></p><p>If you were faster to ramp up on H100s, you too, can build a bigger, better model, and maybe even leapfrog OpenAI to Artificial General Intelligence - If you have the capital to match their wallet! </p><p>With this desire, $10-100’s billions of dollars were invested into GPU-rich AI startups to build this next revolution. Which lead to ….</p><p><strong>The sudden surge in H100 demand</strong></p><p><span>Market prices shot through the roof, the original rental rates of H100 started at approximately </span><em><strong>$4.70 an hour</strong></em><span> but were going for </span><em><strong>over $8</strong></em><span>. For all the desperate founders rushing to train their models to convince their investors for their next $100 million round.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png" width="404" height="227.52747252747253" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:820,&quot;width&quot;:1456,&quot;resizeWidth&quot;:404,&quot;bytes&quot;:265694,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Nvidia, literally pitched to their investors &amp; datacenter customers, in their 2023 investor presentation - the “market opportunity” on renting H100s at $4/hr</figcaption></figure></div><p><span>For GPU farms, it felt like free money - if you can get these founders to rent your H100 SXMGPUs at $4.70 an hour or more, or even get them to pay it upfront, </span><strong>the payback period was &lt;1.5 years</strong><span>. From then on, it was free-flowing cash of over $100k per GPU, per year.</span></p><p>With no end to the GPU demand in sight, their investors agreed, with even larger investments…</p><p><span>Physical goods, unlike digital goods, suffer from lag time. Especially when there are </span><a href="https://www.ft.com/content/c7e9cfa9-3f68-47d3-92fc-7cf85bcb73b3" rel="">multiple shipment delays</a><span>.</span></p><p>For most of 2023, the H100 prices felt like they would forever be above $4.70 (unless you were willing to do a huge upfront downpayment)</p><p>At the start of 2024, the H100 prices reached approximately $2.85 across multiple providers.</p><p>As more providers come online, however… I started to get emails like this:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png" width="1456" height="825" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:825,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:413942,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>While I have not been successful with acquiring H100 nodes (8xH100) at $4/hour, I have confirmed multiple times, that you can do so at $8 - $16/hour</figcaption></figure></div><p>In Aug 2024, if you're willing to auction for a small slice of H100 time (days to weeks), you can start finding H100 GPUs for $1 to $2 an hour.</p><p><strong>We are looking at a &gt;= 40% price drop per year</strong><span>, especially for small clusters. NVIDIA’s marketing projection of $4 per GPU hour across 4 years, has evaporated away in under 1.5 years.</span></p><p><span>And that is horrifying because it means someone out there is potentially </span><a href="https://en.wikipedia.org/wiki/Bagholder" rel="">left holding the bag</a><span> - especially so if they just bought it as a new GPUs. So what is going on?</span></p><blockquote><p><em>This will be focusing on the economical cost, and the ROI on leasing, against various market rates. Not the opportunity cost, or buisness value.</em></p></blockquote><p>The average H100 SXM GPU in a data center costs $50k or more to set up, maintain, and operate (aka most of the CAPEX). Excluding electricity and cooling OPEX cost. More details on the calculation are provided later in this article.</p><p><span>But what does that mean for unit economics today, as an investment?</span><br><span>Especially if we assume a 5-year lifespan on the GPUs itself today.</span></p><p>Generally, there are two business models for leasing H100, which we would cover.</p><ul><li><p>Short on-demand leases (by the hour - by the week - or the month)</p></li><li><p>Longterm reservation (3-5 years)</p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png" width="1456" height="765" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:765,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:618581,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>In summary, for an on-demand workload</strong></p><ul><li><p><strong>&gt;$2.85</strong><span> : Beat stock market IRR</span></p></li><li><p><strong>&lt;$2.85</strong><span> : Loses to stock market IRR</span></p></li><li><p><strong>&lt;$1.65</strong><span> : Expect loss in investment</span></p></li></ul><p>For the above ROI and revenue forecast projection, we introduced “blended price”, where we assume a gradual drop to 50% in the rental price across 5 years.</p><p>This is arguably a conservative/optimistic estimate given the &gt;= 40% price drop per year we see now. But it’s a means of projecting an ROI while taking into account a certain % of price drop.</p><p>At $4.50/hour, even when blended, we get to see the original pitch for data center providers from NVIDIA, where they practically print money after 2 years. Giving an IRR (Internal rate of return) of 20+%.</p><p>However, at $2.85/hour, this is where it starts to be barely above 10% IRR.</p><p>Meaning, if you are buying a new H100 server today, and if the market price is less than $2.85/hour, you can barely beat the market, assuming 100% allocation (which is an unreasonable assumption). Anything, below that price, and you're better off with the stock market, instead of a H100 infrastructure company, as an investment.</p><p><strong>And if the price falls below $1.65/hour, you are doomed to make losses on the H100 over the 5 years, as an infra provider</strong><span>. Especially, if you just bought the nodes and cluster this year.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png" width="1456" height="760" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:760,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:642212,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Many infrastructure providers, especially the older ones - were not naive about this - Because they had been burnt firsthand by GPU massive rental price drops, after a major price pump, from the crypto days - they had seen this cycle before.</p><p><strong>So for this cycle, last year, they pushed heavily for a 3-5 year upfront commitment and/or payment at the $4+ price range. </strong><span>(typically with 50% to 100% upfront)</span><strong>. </strong><span>Today, they push the $2.85+ price range - locking in their profits.</span></p><p>This happened aggressively during the 2023 AI peak with various foundation model companies, especially in the image generation space, indirectly forced into high-priced 3-5 year contracts, just so to get to the front-of-the-line of a new cluster, and be first to make their target model, to help close the next round.</p><p>It may not be the most economical move, but it lets them move faster than the competition.</p><p>This, however, has led to some interesting market dynamics - if you are paying $3 or $4 per hour for your H100, for the next 3 years, locked into a contract.</p><p><span>When a model creator is done training a model, you have no more use for the cluster. </span><strong>What would they do? - they resell and start recouping some of the costs.</strong></p><p>From hardware to AI inference / finetune, it can be broadly viewed as the following</p><ul><li><p>Hardware vendors partnered with Nvidia (one-time purchase cost)</p></li><li><p>Datacenter Infrastructure providers &amp; partners (selling long-term reservations, on facility space and/or H100 nodes)</p></li><li><p><span>VC Funds, Large Companies, and Startups: that plann</span><em>ed</em><span> to build foundation models (or have already finished building their models)</span></p></li><li><p><strong>Resellers of capacity: Runpod, SFCompute, Together.ai, Vast.ai, GPUlist.ai</strong></p></li><li><p>Managed AI Inference / Finetune providers: who use a combination of the above</p></li></ul><p><span>While any layer down the stack may be vertically integrated (skipping the infra players for example), the key drivers here are the </span><strong>“Resellers of unused capacity” </strong><span>and the rise of “good enough” open weights models like </span><a href="https://www.latent.space/p/llama-3" rel="">Llama 3</a><span>, as they are all major influencing factors in the current H100 economical pressures.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png" width="1456" height="871" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:871,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:932212,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><em><strong><span>The rise of open weights models, on-par with closed-source models.</span><br><span>Is resulting in a fundamental shift in the market</span></strong></em></p><blockquote><p><em><strong>↑↑ Increased demand for AI inference &amp; fine-tuning</strong><p><span>Because many “open” models, lack proper “open source” licenses, but are being distributed freely, and used widely, even commercially. We will refer to them collectively as “open-weights” or “open” models instead here.</span></p></em></p></blockquote><p>In general, with multiple open-weights models of various sizes being built, so has the growth in demand for inference and fine-tuning them. This is largely driven by two major events</p><ul><li><p>The arrival of GPT4 class open models (eg. 405B LLaMA3, DeepSeek-v2)</p></li><li><p>The maturity and adoption of small (~8B) and medium (~70B) fine-tuned models</p></li></ul><p>Today, for the vast majority of use cases, enterprises may need, there are already off-the-shelf open-weights models. Which might be a small step behind proprietary models in certain benchmarks.</p><p>Provides an advantage with the following</p><ul><li><p><strong>Flexibility</strong><span>: Domain / Task specific finetunes</span></p></li><li><p><strong>Reliability</strong><span>: No more minor model updates, breaking use case (there is currently low community trust that model weights are not quietly changed without notification in public API endpoints, causing inexplicable regressions)</span></p></li><li><p><strong>Security &amp; Privacy</strong><span>: Assurance that their prompts and customer data are safe.</span></p></li></ul><p>All of this leads to the current continuous growth and adoption of open models, with the growth in demand for inference and finetunes.</p><p>But it does cause another problem…</p><blockquote><p><em><strong>↓↓ Shrinking foundation model creator market (Small &amp; Medium)</strong><p><span>We used “model creators” to collectively refer to organization that create models from scratch. For fine-tuners, we refer to them as “model finetuners”</span></p></em></p></blockquote><p>Many enterprises, and multiple small &amp; medium foundation model creator startups - especially those who raised on the pitch of “smaller, specialized domain-specific models”, are groups who had no long-term plans / goals for training large foundation models from scratch ( &gt;= 70B ).</p><p>For both groups, they both came to the realization that it is more economical and effective to fine-tune existing Open Weights models, instead of “training on their own”.</p><p><strong>This ended up creating a triple whammy in reducing the demand for H100s!</strong></p><ol><li><p><strong>Finetuning is significantly cheaper than training from scratch.</strong></p><ol><li><p>Because the demands for fine-tuning are significantly less in compute requirements (typically 4 nodes or less, usually a single node), compared to training from scratch (from 16 nodes, usually more, for 7B and up models).</p></li><li><p>This industry-wide switch essentially killed a large part of smaller cluster demands.</p></li></ol></li><li><p><strong>Scaling back on foundation model investment (at small/mid-tier)</strong></p><ol><li><p>In 2023, there was a huge wave of small and medium foundation models, within the text and image space.</p></li><li><p>Today, however, unless you are absolutely confident you can surpass llama3, or you are bringing something new to the table (eg. new architecture, 100x lower inference, 100+ languages, etc), there are ~no more foundation model cos being founded from scratch.</p></li><li><p>In general, the small &amp; medium, open models created by the bigger players (Facebook, etc), make it hard for smaller players to justify training foundation models - unless they have a strong differentiator to do so (tech or data) - or have plans to scale to larger models.</p></li><li><p>And this has been reflected lately with investors as well, as there has been a sharp decline in new foundation model creators’ funding. With the vast majority of smaller groups having switched over to finetuning. (this sentiment is combined with the recent less than desired exits for multiple companies).</p></li><li><p>Presently today, there is approximately worldwide by my estimate:</p><ul><li><p>&lt;20 Large model creator teams (aka 70B++, may create small models as well)</p></li><li><p>&lt;30 Small / Medium model creator teams (7B - 70B)</p></li></ul></li><li><p>Collectively there are less than &lt;50 teams worldwide who would be in the market for 16 nodes of H100s (or much more), at any point in time, to do foundation model training.</p></li><li><p>There are more than 50 clusters of H100 worldwide with more than 16 nodes.</p></li></ol></li><li><p><strong>Excess capacity from reserved nodes is coming online</strong></p><ol><li><p>For the cluster owners, especially the various foundation model startups and VCs, who made long reservations, in the initial “land grab” of the year 2023.</p></li><li><p><span>With the switch to finetuning, and the very long wait times of the H100’s</span><br><span>(it peaked at &gt;= 6 months), it is very well possible that many of these groups had already made the upfront payment before they made the change, essentially making their prepaid hardware “obsolete on arrival”.</span></p></li><li><p>Alternatively, those who had the hardware arrive on time, to train their first few models, had come to the same realization it would be better to fine-tune their next iteration of models. Instead of building on their own.</p></li><li><p><span>In both cases, they would have unused capacity, which comes online via </span><strong>“Compute Resellers”</strong><span> joining the market supply….</span></p></li></ol></li></ol><p>Another major factor, is how all the major Model Creators, such as Facebook, X.AI, and arguably OpenAI (if you count them as part of Microsoft) are moving away from an existing public provider, and building their own billion-dollar clusters, removing the demand that the existing clusters depend on.</p><p>The move is happening mostly for the following reasons:</p><ul><li><p>Existing ~1k node clusters (which costs &gt;$50M to build), is no longer big enough for them, to train bigger models</p></li><li><p>At a billion-dollar scale, it is better for accounting to purchase assets (of servers, land, etc), which has booked value (part of company valuation and assets), instead of pure expenses leasing.</p></li><li><p>If you do not have the people (they do), you could straight up buy small datacenters companies, who have the expertise to build this for you.</p></li></ul><p>With the demand gradually weaning away in stages. These clusters are coming online to the public cloud market instead.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png" width="1456" height="974" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:974,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1268863,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Vast.ai essentially does a free market system, where providers from all over the world, are forced to compete with each other</figcaption></figure></div><p>Recall all the H100 large shipment delays in 2023, or 6 months or more? They are coming online, now - along with the H200, B200, etc.</p><p>This is alongside, the various unused compute, coming online (from existing startups, enterprises or VCs as covered earlier).</p><p><span>The bulk of this is done via </span><strong>Compute Resellers</strong><span>, such as : together.ai, sfcompute, runpod, vast.ai, etc</span></p><p>In most cases, cluster owners have a small or medium cluster, (typically 8-64 nodes), that is underutilized. With the money already “spent” for the cluster.</p><p>With the primary goal is to recoup as much of the cost as possible, they rather undercut the market and guarantee an allocation, instead of competing with the main providers, and possibly have no allocation.</p><p>This is typically done either via a fixed rate, an auction system, or just a free market listing, etc. With the later 2 driving the market price downwards.</p><p>Another major factor, is once your outside of the training / fine-tune space. The inference space is filled with alternatives, especially if your running smaller models.</p><p>One do not need to pay for the premium invoked by H100’s Infiniband and/or nvidia.</p><p>H100 premium for training is priced into the hardware. For example nvidia themselves recommend the L40S, which is the more price competitive alternative for inference.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png" width="1456" height="439" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/df705375-9942-4ea5-86b9-b41d3661096b_1842x556.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:439,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:414838,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Which Is 1/3rd the performance, at 1/5th the price. But does not work well with multi-node training. Undercutting their very own H100 for this segment.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png" width="1456" height="461" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:461,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2982780,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Both AMD and Intel may be late into the game with their MX300, and Gaudi 3 respectively.</p><p>This has been tested and verified by us, having used these systems. They are generally:</p><ul><li><p>Cheaper than a H100 in purchase cost</p></li><li><p>Have more memory and compute than a H100, and outperforms on a single node.</p></li><li><p>Overall, they are great hardware!</p></li></ul><p>The catch? They have minor driver issues in training and are entirely unproven in large multi-node cluster training.</p><p>Which as we covered is largely irrelevant to the current landscape. To anyone but &lt;50 teams. The market for H100 has been moving towards inference and single or small cluster fine-tuning.</p><p>All of which these GPUs have been proven to work at. For the use cases, the vast majority of the market is asking for.</p><p>These 2 competitors are full drop-in replacements. With working off-the-shelf inference code (eg. VLLM) or finetuning code for most common model architectures (primarily LLaMA3, followed by others).</p><p>So, if you have compatibility sorted out. Its highly recommended to have a look.</p><p>With Ethereum moving towards proof of stake, ASIC dominating the bitcoin mining race, and the general crypto market condition.</p><p>GPU usage in mining for crypto has been a downward trend, and in several cases unprofitable. And has since been flooding the GPU public cloud market.</p><p>And while the vast majority of these GPUs are unusable for training, or even for inference, due to hardware constraints (low PCIe bandwidth, network, etc). The hardware has been flooding the market and has been repurposed for AI inference workloads.</p><p>In most cases if you are under &lt;10B, you can get decent performance with these GPUs, out of the box, for really low prices.</p><p>If you optimize it further (though various tricks), you can even get large 405B models to run on a small cluster of this hardware, cheaper then an H100 node (which is what is typically used)</p><p><em><span>H100 Prices are becoming commodity-prices cheap.</span><br><span>Or even being rented at a loss - if so, what now?</span></em></p><p>On a high level, it is expected that big clusters still get to charge a premium (&gt;=$2.90 / hour) because there is no other option. For those who truly need it.</p><p>We are starting to see this trend for example with Voltage Park:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png" width="1456" height="741" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:741,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:647709,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Where clusters with Infiniband are charged at a premium.</p><p>While the Ethernet-based instances, which are perfectly fine for inference are priced at a lower rate. Adjusting the prices for the respective use case/availability.</p><p>While there’s been a general decline in foundation model creator teams, it is hard to predict if there will be a resurgence, with the growth in open weights, and/or alternative architectures.</p><p>It is also, expected that in the future, we will see further segmentation by cluster sizes. Where a large 512-node cluster with Infiniband may be billed higher per GPU than a 16-node cluster.</p><p>There is a lot against you, if you price it below $2.25, depending on your OPEX, you risk potentially being unprofitable.</p><p>If you price it too high &gt;= $3, you might not be able to get sufficient buyers to fill capacity.</p><p>If you're late, you could not recoup the cost in the early $4/hour days.</p><p>Overall, these cluster investments will be rough for the key stakeholders and investors.</p><p>While I doubt it’s the case, if new clusters, make a large segment of the AI portfolio investments. We may see additional rippling effects in the funding ecosystem from burnt investors.</p><p>Instead of a negative outlook, a neutral outlook would be some of the unused compute foundation model creators, coming online, are already paid for.</p><p>The funding market has already priced in and paid for this cluster and its model training. And “extracted its value” which they used for their current and next funding round.</p><p><span>Most of these purchases were made before the popularity of </span><strong>Compute Resellers</strong><span>, the cost was already priced in.</span></p><p>If anything, the current revenue they get from their excess H100 compute, and the lowered prices we get, are beneficial to both parties</p><p>If so the negative market impact is minimal, while overall it’s a net positive win for the ecosystem.</p><p>Given that the open-weights model has entered the GPT-4 class arena. Falling H100 prices will be the multiplier unlock for open-weights AI adoption.</p><p>It will be more affordable, for hobbyists, AI developers, and engineers, to run, fine-tune, and tinker with these open models.</p><p><span>Especially if there is no major leap for GPT5++,</span><strong> </strong><span>because it will mean that the gap between open-weights and closed-source models will blur.</span></p><p>This is strongly needed, as the market is currently not sustainable. As there lacks the value capture on the application layer for paying users (which trickles down the platform, models, and infra layers)</p><p>In a way, if everyone is building shovels (including us), and applications with paying users are not being built (and collecting revenue and value).</p><p>But when AI inference and fine-tuning becomes cheaper than ever.</p><p>It can potentially kick off the AI application wave. If it has not already slowly started so.</p><p><em><strong>Spending on new H100’s hardware is likely a loss-maker</strong><p><span>Unless you have some combination of discounted H100s, discounted electricity, or a Sovereign AI angle where the location of your GPU is critical to your customers. Or you have billions and need a super large cluster.</span></p><p><span>If you're investing, consider investing elsewhere.</span><br><span>Or the stock market index itself for a better rate of returns. IMO</span></p></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png" width="1456" height="813" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:813,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:664297,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>At Featherless.AI - We currently host the world’s largest collection of OpenSource AI models, instantly accessible, serverlessly, with unlimited requests from $10 a month, at a fixed price.</p><p><span>We have indexed and made over 2,000 models ready for inference today. This is 10x the catalog of openrouter.ai, the largest model provider </span><em>aggregator</em><span>, </span><em>and </em><span>is the world’s largest collection of Open Weights models available serverlessly for instant inference. Without the need for any expensive dedicated GPUs</span></p><p>And our platform makes this possible, as it’s able to dynamically hot-swap between models in seconds.</p><p>It’s designed to be easy to use, with full OpenAI API compatibility, so you can just plug our platform in as a replacement to your existing AI API for your AI agents. Running in the background</p><p>And we do all of this; As we believe that AI should be easily accessible to everyone, regardless of language or social status.</p><p>On the technical side of things, related to this article.</p><p>It is a challenge having PetaBytes’s worth of AI models, and growing, running 24/7 - while being hardware profitable (we are), because we needed to optimize every layer of our platform, down to how we choose the GPU hardware.</p><p>In an industry, where the typical inference provider pitch is typically along the lines of winning with their, special data center advantages, and CUDA optimization that they perform on their own hardware. Hardware is CAPEX intensive. (Which is being pitched and funded even today)</p><p>We were saying the opposite, which defied most investors’ sensibilities - we were saying we would be avoiding buying new hardware like the plague.</p><p>We came to a realization, that most investors, their analysts, and founders failed to realize, thanks to the billions in hardware investments to date. GPUs are commodity hardware. Faster than all of us expected.</p><p>Few investors have even realized we have reached commodity-level prices at $2.85 in certain places, let alone loss-making prices of a dollar. Because most providers (ignoring certain exceptions), only show their full prices after quotation or after login.</p><p>And that was the trigger, which got me to write this article.</p><p>While we do optimize our inference CUDA and kernels as well. On the hardware side; We’ve bet on hardware commoditizing and have focussed instead on the orchestration layer above.</p><p>So for us, this is a mix of sources from, AWS spot (preferred), to various data center grade providers (eg. Tensordock, Runpod) with security and networking compliances that meet our standards.</p><p>Leveraging them with our own proprietary model hot swapping, which boots new models up in under a second. Keeping our fleet of GPUs right-sized to our workload, while using a custom version of our RWKV foundation model as a low-cost speculative decoder. All of which allows us to take full advantage of this market trend, and future GPU price drops, as newer (and older) GPUs come online to replace the H100s. And scale aggressively.</p><p><em>PS: If you are looking at building the world's largest inference platform, and are aligned with our goals - to make AI accessible to everyone, regardless of language or status. Reach out to us at: hello@featherless.ai</em></p><p><em><span>Head over to Eugene’s Blog </span></em></p><p><em><span> for </span><a href="https://substack.tech-talk-cto.com/p/d4ffab7a-3f0d-4e6e-ade0-e74409770196?postPreview=paid&amp;updated=2024-08-25T03%3A36%3A59.886Z&amp;audience=everyone&amp;free_preview=false&amp;freemail=true" rel="">more footnotes on xAI’s H100 cluster</a><span> we cut from this piece.</span></em><span> </span></p><p><strong>Additional Sources:</strong></p><ul><li><p><span>GPU data: </span><a href="https://www.techpowerup.com/gpu-specs/h100-sxm5-80-gb.c3900" rel="">Tech Power Up Database</a><span>. The A100 SXM had 624 bf16 TFlops, the H100 SXM was 1,979 bf16 TFlops</span></p></li><li><p><span>Microsoft &amp; AWS allocated over $40 billion in AI infra alone: </span><a href="https://www.wsj.com/tech/ai/big-tech-moves-more-ai-spending-abroad-088988de" rel="">Wall Street Journal</a></p></li><li><p><span>“600 Billion Dollars “ is about: </span><a href="https://www.sequoiacap.com/article/ais-600b-question/" rel="">Sequoia’s AI article</a></p></li><li><p><span>Nvidia investor slides for Oct 2014: </span><a href="https://s201.q4cdn.com/141608511/files/doc_presentations/2023/Oct/01/ndr_presentation_oct_2023_final.pdf" rel="">page 14 has the pitch for “data centers”</a></p></li><li><p><span>Semi Analysis: </span><a href="https://www.semianalysis.com/p/100000-h100-clusters-power-network" rel="">deepdive for H100 clusters, w/ 5 year lifespan approx for components</a></p></li><li><p><span>Spreadsheet for : </span><a href="https://docs.google.com/spreadsheets/d/1kZosZmvaecG6P4-yCPzMN7Ha3ubMcTmF9AeJNDKeo98/edit?usp=sharing" rel="">new H100 ROI (Aug 2024)</a></p></li><li><p><span>Spreadsheet for: </span><a href="https://docs.google.com/spreadsheets/d/1Ft3RbeZ-w43kYSiLfYc1vxO41mK5lmJpcPC9GOYHAWc/edit?usp=sharing" rel="">H100 Infiniband Cluster math (Aug 2024)</a></p></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WordPress Alternatives (129 pts)]]></title>
            <link>https://darn.es/wordpress-alternatives/</link>
            <guid>41805391</guid>
            <pubDate>Fri, 11 Oct 2024 02:03:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://darn.es/wordpress-alternatives/">https://darn.es/wordpress-alternatives/</a>, See on <a href="https://news.ycombinator.com/item?id=41805391">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      <heading-anchors>
        <div><p>📝</p><p>Editor note: Due to this article's unexpected attention, I've included a few more alternatives that people suggested. I've also added some contextual notes you should know before diving into these options.</p></div><p>Due to <em>gestures vaguely, </em>everything going on <a href="https://css-tricks.com/catching-up-on-the-wordpress-wp-engine-sitch/" rel="noreferrer">right now with WordPress</a>, I thought I'd put together a list of alternative CMSs that better fit the criteria someone might have for their website. The modern CMS landscape is super broad, with the very definition of "Content Management System" being stretched. Some see it as a full-package website platform, and some see it as just UI for their content stored elsewhere.</p><p>The criteria for this list are "Can it be downloaded, dropped onto a server, and you'll have a website?" This eliminates API and git-based CMSs, which I enjoy using; however, wiring a daisy chain of tools is just not viable for many.</p><figure><a href="https://ghost.org/"><div><p>Ghost: The best open source blog &amp; newsletter platform</p><p>Beautiful, modern publishing with email newsletters and paid subscriptions built-in. Used by Platformer, 404Media, Lever News, Tangle, The Browser, and thousands more.</p><p><img src="https://darn.es/img/Z24D4b6-favicon-1.ico" alt=""></p></div><p><img src="https://darn.es/img/Z1hjw9Q-ghost.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>People will already know I have a soft spot for Ghost. But what you might not know is what I'd recommend for hosting.</p><figure><a href="https://www.magicpages.co/"><div><p>Magic Pages</p><p>Get your Ghost CMS publication up and running in no time with Magic Pages’ Ghost CMS web hosting – starting at $4/month!</p><p><img src="https://darn.es/img/iczFH-favicon-196x196-1.jpg" alt=""><span>Magic Pages</span></p></div><p><img src="https://darn.es/img/Z1EYPIA-MagicPages.co-3.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Magic Pages is what I'm using for <a href="https://designsystems.wtf/" rel="noreferrer">Design Systems WTF</a>, and it's been great! The uptime is good, the price is very reasonable, and <a href="https://www.jannis.io/" rel="noreferrer">Jannis</a> provides a personal touch with support. In addition, this sidesteps Ghost's own hosting option (Ghost Pro), which I would be wary of due to <a href="https://x.com/amyhoy/status/1449482190224384000">past experiences with other customers</a>.</p><figure><a href="https://getkirby.com/"><div><p>Kirby is the CMS that adapts to you</p><p>Kirby is the content management system that adapts to any project. Made for developers, designers, creators and clients.</p><p><img src="https://darn.es/img/1CTUoB-favicon.1704303350.svg" alt=""><span>Kirby CMS</span></p></div><p><img src="https://darn.es/img/1RRwiG-opengraph.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>I have not used Kirby in client work, but I hear only good things. It's file-based, which seems super appealing to someone like myself who gets cold sweats when opening a database.</p><figure><a href="https://getindiekit.com/"><div><p>Indiekit</p><p>The little server that connects your website to the independent web.</p><p><img src="https://darn.es/img/1Py0ge-icon.svg" alt=""><span>Get Started</span></p></div><p><img src="https://darn.es/img/Z1QWsfN-opengraph-image.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Indiekit seems like an interesting option; it's also file-based but needs a database to manage existing content.</p><figure><a href="https://craftcms.com/"><div><p>Craft CMS</p><p>Craft is a flexible, user-friendly CMS for creating custom digital experiences on the web and beyond.</p><p><img src="https://darn.es/img/1o1WvD-apple-touch-icon.png" alt=""><span>Craft CMS</span></p></div><p><img src="https://darn.es/img/ZXh3Gs-social-craft-cms.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>It's a bit more of a commercial option with Craft CMS, but it does offer a free option for solo creators. Warning, though, as you'll need to spend time architecting your content structure by the looks of it.</p><figure><a href="https://www.classicpress.net/"><div><p>ClassicPress | Stable. Lightweight. Instantly Familiar.</p><p>ClassicPress is a community-led open source content management system. A fork of WordPress 4.9, it retains the WordPress classic editor as the default option.</p><p><img src="https://darn.es/img/1MLpdH-cropped-icon-gradient-500x500.png" alt=""><span>ClassicPress</span></p></div><p><img src="https://darn.es/img/1lFgeF-classicpress-cms-for-creators.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>ClassicPress appears to be a direct fork of WordPress but at version 6.2.3. It seems perfect for anyone looking for "the good old days." However, it uses the official WordPress plugin API, so it's not a 100% clean break if that's what you're going for.  Thanks to <a href="https://voxpelli.com/" rel="noreferrer">Pelle Wessman</a> for this suggestion.</p><figure><a href="https://statamic.com/"><div><p>Statamic is a powerful, highly scalable CMS built on Laravel.</p><p>The open source, flat-first, Laravel + Git powered CMS designed for building easy to manage websites.</p><p><img src="https://darn.es/img/Z2ddyQl-favicon-196x196.png" alt=""><span>Statamic</span></p></div><p><img src="https://darn.es/img/Z1MKo0C-card-2023.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>I've had several people suggest Statamic. It does look pretty good, plus they have a free solo plan (similar to Craft CMS). I think if the cofounder hadn't brazenly <a href="https://jaygeorge.co.uk/blog/intolerance">endorsed a horrendously damaging politician</a>, I'd have tried it.</p><p>I was going to suggest <a href="https://grabaperch.com/" rel="noreferrer">Perch</a> and <a href="http://buckets.io/" rel="noreferrer">Buckets</a> on this list, but public activity seems low for both. The Perch website even has SSL certificate issues, which isn't a good sign. Check them out if you're interested, but you have been warned.</p><h3 id="honourable-mention">Honourable mention</h3><figure><a href="http://anchorcms.com/"><div><p>Lifting Anchor</p><p>Help on how to use Anchor</p><p><img src="https://darn.es/img/p9SGH-1433533.png" alt=""><span>Anchor CMS</span></p></div><p><img src="https://darn.es/img/Z2kqVUC-screenshot.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Many years ago, I contributed to Anchor, a humble PHP-based CMS that grew a little community around itself. Sadly, the creator, Charlotte, passed away in 2020, and the remaining core team couldn't keep the project going while juggling other responsibilities. I think of it fondly and wish we could give it the time it deserves. </p><p>The theming and custom types aspects were wonderfully simple; heck, I even made a whole site dedicated to themes and sites built with it:</p><figure><a href="https://anchorthemes.com/"><div><p>Welcome - Anchor Themes</p><p>Themes and sites built for &lt;a href=“https://anchorcms.com”&gt;Anchor&lt;/a&gt;, obviously</p><p><img src="https://darn.es/img/xtffw-link-icon.svg" alt=""><span>Anchor Themes</span><span>David Darnes</span></p></div><p><img src="https://darn.es/img/Z1nRKSm-facebook.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>I'll try to keep this list up to date if I recall any others I've used in the past. Hopefully, you find this useful if you're seeking alternative CMSs.</p>
      </heading-anchors>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The FBI created a coin to investigate crypto pump-and-dump schemes (103 pts)]]></title>
            <link>https://www.theverge.com/2024/10/10/24267098/fbi-coin-crypto-token-nexgenai-sec-doj-fraud-investigation</link>
            <guid>41802823</guid>
            <pubDate>Thu, 10 Oct 2024 19:52:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/10/10/24267098/fbi-coin-crypto-token-nexgenai-sec-doj-fraud-investigation">https://www.theverge.com/2024/10/10/24267098/fbi-coin-crypto-token-nexgenai-sec-doj-fraud-investigation</a>, See on <a href="https://news.ycombinator.com/item?id=41802823">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The FBI created a cryptocurrency as part of an investigation into price manipulation in crypto markets, the <a href="https://www.justice.gov/usao-ma/pr/eighteen-individuals-and-entities-charged-international-operation-targeting-widespread">government revealed</a> on Wednesday. The FBI’s Ethereum-based token, NexFundAI, was created with the help of “cooperating witnesses.”</p><p>As a result of the investigation, the Securities and Exchange Commission <a href="https://www.sec.gov/newsroom/press-releases/2024-166">charged</a> three “market makers” and nine people for allegedly engaging in schemes to boost the prices of certain crypto assets. The Department of Justice charged 18 people and entities for “widespread fraud and manipulation” in crypto markets.</p><p>The defendants allegedly made false claims about their tokens and executed so-called “wash trades” to create the impression of an active trading market, prosecutors claim. The three market makers — ZMQuant, CLS Global, and MyTrade — allegedly wash traded or conspired to wash trade on behalf of NexFundAI, an Ethereum-based token they didn’t realize was created by the FBI.&nbsp;</p><p>“What the FBI uncovered in this case is essentially a new twist to old-school financial crime,” Jodi Cohen, the special agent in charge of the FBI’s Boston division, said in a statement. “What we uncovered has resulted in charges against the leadership of four cryptocurrency companies, and four crypto ‘market makers’ and their employees who are accused of spearheading a sophisticated trading scheme that allegedly bilked honest investors out of millions of dollars.”</p><p>Liu Zhou, a “market maker” working with MyTrade MM, allegedly told promoters of NexFundAI that MyTrade MM was better than its competitors because they “control the pump and dump” allowing them to “do inside trading easily.”</p><p>An FBI spokesperson <a href="https://www.coindesk.com/policy/2024/10/09/prosecutors-charge-two-crypto-market-makers-employees-with-market-manipulation-fraud/">told <em>CoinDesk</em></a> that there was limited trading activity on the coin but didn’t share additional information. On a Wednesday press call, Joshua Levy, the acting US attorney for the District of Massachusetts, said trading on the token was disabled, according to <em>CoinDesk</em>.</p><p>The DOJ has reportedly secured $25 million from “fraudulent proceeds” that will be returned to investors.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Copenhagen Book: general guideline on implementing auth in web applications (462 pts)]]></title>
            <link>https://thecopenhagenbook.com/</link>
            <guid>41801883</guid>
            <pubDate>Thu, 10 Oct 2024 18:37:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thecopenhagenbook.com/">https://thecopenhagenbook.com/</a>, See on <a href="https://news.ycombinator.com/item?id=41801883">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      
      <main>
<p>The Copenhagen Book provides a general guideline on implementing auth in web applications. It is free, open-source, and community-maintained. It may be opinionated or incomplete at times but we hope this fills a certain void in online resources. We recommend using this alongside the <a href="https://cheatsheetseries.owasp.org/index.html">OWASP Cheat Sheet Series</a>.</p>
<p>If you have any suggestions or concerns, consider opening a new issue.</p>
<p><em>Created by <a href="https://github.com/pilcrowOnPaper">Pilcrow</a></em></p>
</main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TypedDicts are better than you think (112 pts)]]></title>
            <link>https://blog.changs.co.uk/typeddicts-are-better-than-you-think.html</link>
            <guid>41801415</guid>
            <pubDate>Thu, 10 Oct 2024 17:56:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.changs.co.uk/typeddicts-are-better-than-you-think.html">https://blog.changs.co.uk/typeddicts-are-better-than-you-think.html</a>, See on <a href="https://news.ycombinator.com/item?id=41801415">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content">
    <article>
      <header>
        
      </header>

      <div>
<!-- /.post-info -->        <p><code>TypedDict</code> was introduced in <a href="https://peps.python.org/pep-0589/">PEP-589</a> which landed in Python 3.8.</p>
<p>The primary use case was to create type annotations for dictionaries. For example,</p>
<div><pre><span></span><code><span>class</span> <span>Movie</span><span>(</span><span>TypedDict</span><span>):</span>
    <span>title</span><span>:</span> <span>str</span>


<span>movie</span><span>:</span> <span>Movie</span> <span>=</span> <span>{</span><span>"title"</span><span>:</span> <span>"Avatar"</span><span>}</span>
</code></pre></div>

<p>I remember thinking at the time that this was pretty neat, but I tend to use <code>dataclass</code> or <code>pydantic</code> to represent 'record' type data. Instead I use dictionaries more as a collection, so the standard <code>dict[KT, VT]</code> annotation is enough.</p>
<h3>Non-totality</h3>
<p>I revisited typeddicts when I looked at implementing a HTTP patch endpoint.</p>
<p>Let's suppose I have a data structure represented by the following dataclass:</p>
<div><pre><span></span><code><span>@dataclass</span>
<span>class</span> <span>User</span><span>:</span>
    <span>id</span><span>:</span> <span>UUID</span>
    <span>name</span><span>:</span> <span>str</span>
    <span>subscription</span><span>:</span> <span>str</span> <span>|</span> <span>None</span> <span>=</span> <span>None</span>
</code></pre></div>

<p>Where <code>subscription = None</code> means no subscription.</p>
<p>Let's say we want to option to patch name, subscription. You might define the patch body using dataclass:</p>
<div><pre><span></span><code><span>@dataclass</span>
<span>class</span> <span>PatchUser</span><span>:</span>
    <span>name</span><span>:</span> <span>str</span> <span>|</span> <span>None</span> <span>=</span> <span>None</span>
    <span>subscription</span><span>:</span> <span>str</span> <span>|</span> <span>None</span> <span>=</span> <span>None</span>
</code></pre></div>

<p>Here we have a problem, for subscription does <code>None</code> mean don't change or remove subscription. </p>
<p>We can fix this a number of ways, for example, we can take the string <code>'none'</code> to mean no subscription instead, or make a new sentinel value called <code>NoChange</code> to indicate no changes.</p>
<p>These solutions all feel a little awkward, this is because dataclasses don't have a concept of a field being missing. But this is where dictionaries shine. Dictionaries are not general expected to have all the fields available. We get a <code>KeyError</code> if a field is missing and there are convenience methods such as <code>.get(key, [default])</code> to fetch a key that is not guaranteed to be present.</p>
<p>This makes <code>TypedDict</code> the ideal data structure in this scenario:</p>
<div><pre><span></span><code><span>class</span> <span>PatchUser</span><span>(</span><span>TypedDict</span><span>,</span> <span>total</span><span>=</span><span>False</span><span>):</span>
    <span>name</span><span>:</span> <span>str</span> <span>|</span> <span>None</span>
    <span>subscription</span><span>:</span> <span>str</span> <span>|</span> <span>None</span> <span>=</span> <span>None</span>
</code></pre></div>

<p>Since <code>total</code> is False here (by default it is set to True), <code>name</code> or <code>subscription</code> can be absent from the dictionary. Which represents the PATCH operation much better than a <code>dataclass</code> or Pydantic model.</p>
<p>Further additions in <a href="https://peps.python.org/pep-0655/">PEP-655</a> allows us to mark individual fields as <code>Required</code> or <code>NotRequired</code> which further increases its flexibility.</p>
<blockquote>
<p>If you're wondering about FastAPI support for TypedDict, <a href="https://docs.pydantic.dev/2.3/usage/types/dicts_mapping/#typeddict">Pydantic supports it out of the box</a>. So your TypedDict can be used in a FastAPI endpoint.</p>
</blockquote>
<h3>Using <code>TypedDict</code> as <code>**kwargs</code></h3>
<p><a href="https://peps.python.org/pep-0692/">PEP-692</a> introduced the ability to type variadic keyword arguments using <code>TypedDict</code>.</p>
<p>So the following two snippets are equivalent.
Without <code>TypedDict</code>:</p>
<div><pre><span></span><code><span>def</span> <span>my_function</span><span>(</span><span>*</span><span>,</span> <span>option1</span><span>:</span> <span>int</span><span>,</span> <span>option2</span><span>:</span> <span>str</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
    <span>...</span>
</code></pre></div>

<p>Using <code>TypedDict</code>:</p>
<div><pre><span></span><code><span>from</span> <span>typing</span> <span>import</span> <span>TypedDict</span><span>,</span> <span>Unpack</span>


<span>class</span> <span>Options</span><span>(</span><span>TypedDict</span><span>):</span>
    <span>option1</span><span>:</span> <span>int</span>
    <span>option2</span><span>:</span> <span>str</span>


<span>def</span> <span>my_function</span><span>(</span><span>**</span><span>options</span><span>:</span> <span>Unpack</span><span>[</span><span>Options</span><span>])</span> <span>-&gt;</span> <span>None</span><span>:</span>
    <span>...</span>
</code></pre></div>

<p>At a glance I can say that the TypedDict option is rather verbose. Though it does become more useful if Options were used in multiple function definitions.</p>
<div><pre><span></span><code><span>def</span> <span>my_function2</span><span>(</span><span>**</span><span>options</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
    <span>...</span>


<span>def</span> <span>my_function3</span><span>(</span><span>*</span><span>,</span> <span>other_option</span><span>:</span> <span>str</span><span>,</span> <span>**</span><span>options</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
    <span>...</span>
</code></pre></div>

<p>Where it truely shines is once again with non-totality.</p>
<p>Suppose we have the following scenario, where we want to create a custom version of pytest.fixture, but still pass through some arguments.</p>
<div><pre><span></span><code><span>def</span> <span>fixture</span><span>(</span><span>scope</span><span>:</span> <span>str</span> <span>=</span> <span>"module"</span><span>,</span> <span>autouse</span><span>:</span> <span>bool</span> <span>=</span> <span>False</span><span>):</span>
    <span>return</span> <span>pytest</span><span>.</span><span>fixture</span><span>(</span><span>scope</span><span>,</span> <span>autouse</span><span>)</span>
</code></pre></div>

<p>Here to get the typing right I not only have to find the type of each argument but also the default value. It would be better if we use <code>**kwargs</code> so we can just avoid passing the arguments through. And to keep type information we just need to use our trusty <code>TypedDict</code> once more:</p>
<div><pre><span></span><code><span>class</span> <span>FixtureOptions</span><span>(</span><span>TypedDict</span><span>,</span> <span>total</span><span>=</span><span>False</span><span>):</span>
    <span>scope</span><span>:</span> <span>str</span>
    <span>autouse</span><span>:</span> <span>bool</span>


<span>def</span> <span>fixture</span><span>(</span><span>**</span><span>options</span><span>:</span> <span>Unpack</span><span>[</span><span>FixtureOptions</span><span>]):</span>
    <span># Some custom implementations</span>
    <span>...</span>
    <span>return</span> <span>pytest</span><span>.</span><span>fixture</span><span>(</span><span>**</span><span>options</span><span>)</span>
</code></pre></div>

<p>Non-totallity means that we don't have to pass in scope and autouse. We can just have the default.</p>
<h4>Sentinels</h4>
<p>We can achieve similar behaviour with sentinels:</p>
<div><pre><span></span><code><span>UNSPECIFIED</span><span>:</span> <span>Any</span> <span>=</span> <span>object</span><span>()</span>  <span># Has to be Any type so it could be set as default for other types.</span>

<span>def</span> <span>my_func</span><span>(</span><span>option1</span><span>:</span> <span>bool</span> <span>=</span> <span>UNSPECIFIED</span><span>,</span> <span>...</span><span>)</span> <span>-&gt;</span> <span>...</span><span>:</span>
    <span>if</span> <span>option1</span> <span>is</span> <span>UNSPECIFIED</span><span>:</span>
        <span>...</span>
    <span>...</span>
</code></pre></div>

<p>Sentinels work well enough here, but we have to remember to handle them. Additionally type annotations for sentinels can be a bit awkward, here we made <code>UNSPECIFIED</code> an <code>Any</code> type, but it means that inside the function <code>option1</code> is only typed as <code>bool</code>. There are options to expose the sentinel type but they may add even more confusion.</p>
<h3>Using <code>TypedDict</code> to pass in dependencies</h3>
<p>We can do even more with <a href="https://peps.python.org/pep-0692/">PEP-692</a>! When I first learned about the PEP, I thought it was only about function signature. But reading through it more thoroughly, I discovered that another consequence of the PEP is that type checkers can now check for function invocation when using TypedDicts:</p>
<div><pre><span></span><code><span>def</span> <span>purge</span><span>(</span><span>queue</span><span>:</span> <span>str</span><span>,</span> <span>timeout</span><span>:</span> <span>float</span><span>)</span> <span>-&gt;</span> <span>...</span><span>:</span>
    <span>...</span>


<span>class</span> <span>Options</span><span>(</span><span>TypedDict</span><span>):</span>
    <span>queue</span><span>:</span> <span>str</span>
    <span>timeout</span><span>:</span> <span>float</span>


<span>class</span> <span>WrongOptions</span><span>(</span><span>TypedDict</span><span>):</span>
    <span>queue</span><span>:</span> <span>str</span>
    <span>timeout</span><span>:</span> <span>timedelta</span>


<span>options</span><span>:</span> <span>Options</span> <span>=</span> <span>...</span>
<span>purge</span><span>(</span><span>**</span><span>options</span><span>)</span>  <span># ✅</span>

<span>wrong_options</span><span>:</span> <span>WrongOptions</span> <span>=</span> <span>...</span>
<span>purge</span><span>(</span><span>**</span><span>wrong_options</span><span>)</span>  <span># ❌</span>
</code></pre></div>

<p>This feature is necessary in many situations such as cases where we pass through the kwargs. For example, in the <code>fixture</code> example, when we invoke <code>pytest.fixture(**options)</code> the type checker will perform proper type checking.</p>
<p>But we can use it in more creative ways.</p>
<h4>Dependency Injection</h4>
<p>Let's consider a situation where we have many resources that share some dependencies. </p>
<div><pre><span></span><code><span>class</span> <span>UserClient</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>db</span><span>:</span> <span>Engine</span><span>,</span> <span>user_service</span><span>:</span> <span>APIClient</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
        <span>...</span>


<span>class</span> <span>ProjectClient</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>db</span><span>:</span> <span>Engine</span><span>,</span> <span>user_service</span><span>:</span> <span>APIClient</span><span>,</span> <span>project_service</span><span>:</span> <span>APIClient</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
        <span>...</span>
</code></pre></div>

<p>We want a way to create all the dependencies in one place and pass in the dependencies.</p>
<p>Essentially we need something that is the union of all kwargs of the resources. That suddernly sounds a lot like a TypedDict:</p>
<div><pre><span></span><code><span>class</span> <span>Dependencies</span><span>(</span><span>TypedDict</span><span>):</span>
    <span>db</span><span>:</span> <span>Engine</span>
    <span>user_service</span><span>:</span> <span>APIClient</span>
    <span>project_service</span><span>:</span> <span>APIClient</span>


<span>def</span> <span>create_deps</span><span>(</span><span>...</span><span>)</span> <span>-&gt;</span> <span>Dependencies</span><span>:</span>
    <span>...</span>
</code></pre></div>

<p>Unfortunately this won't work since <code>UserClient</code> can't take <code>project_service</code> as a kwarg.</p>
<p>To fix this, we need to rewrite the resources such that we accept arbitrary arguments.</p>
<div><pre><span></span><code><span>class</span> <span>UserClient</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>...</span><span>,</span> <span>**</span><span>_</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
        <span>...</span>
<span>...</span>
</code></pre></div>

<p>And then we can do the injection like this:</p>
<div><pre><span></span><code><span>class</span> <span>ResourceWithMissing</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>other</span><span>:</span> <span>Any</span><span>,</span> <span>**</span><span>_</span><span>)</span> <span>-&gt;</span> <span>None</span><span>:</span>
        <span>...</span>


<span>def</span> <span>inject</span><span>(</span><span>deps</span><span>:</span> <span>Dependencies</span><span>):</span>
    <span>UserClient</span><span>(</span><span>**</span><span>deps</span><span>)</span>  <span># ✅</span>
    <span>ProjectClient</span><span>(</span><span>**</span><span>deps</span><span>)</span>  <span># ✅</span>
    <span>ResourceWithMissing</span><span>(</span><span>**</span><span>deps</span><span>)</span>  <span># ❌</span>
    <span>...</span>


<span>inject</span><span>(</span><span>create_deps</span><span>(</span><span>...</span><span>))</span>
</code></pre></div>

<p>With the solution complete, we can now rely on the type system to check the dependency injection to see if any arguments are incorrect or missing. </p>
<p>I will admit that changing resource signature with <code>**_</code> is not ideal, but this is a smaller change than most dependency injection frameworks. And we get static type checking which a lot of the frameworks won't support.</p>
<h3>Upcoming Features</h3>
<p><a href="https://peps.python.org/pep-0728/">PEP-728</a> will allow types of extra items to be defined, and a typed dict to be closed meaning no extra items can be defined.</p>
<p>This new change looks like it'll help us define record types more precisely.</p>
<p>I personally haven't thought of many other use cases for it, but as I've demonstrated above it's always worth reading through the PEP and experimenting with the new change. </p>
<p><a href="https://peps.python.org/pep-0705/">PEP-705</a> might already be out by the time you read this. This will allow for read only items to be specified.</p>
<p>This is primarily intended for situations where different typed dicts intuitively should be compatible but potential mutations (deletions) can create problems.</p>
      </div><!-- /.entry-content -->

    </article>
  </section><div id="contentinfo">
                        <address id="about">
                                Proudly powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a rel="nofollow" href="https://www.python.org/">Python</a>.
                        </address><!-- /#about -->

                        <p>The theme is by <a rel="nofollow" href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: HTML for People (373 pts)]]></title>
            <link>https://htmlforpeople.com</link>
            <guid>41801334</guid>
            <pubDate>Thu, 10 Oct 2024 17:47:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://htmlforpeople.com">https://htmlforpeople.com</a>, See on <a href="https://news.ycombinator.com/item?id=41801334">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">
      
  
<p>HTML isn’t only for people working in the tech field. It’s for anybody, the way documents are for anybody. HTML is just another type of document. A very special one—the one the web is built on.</p>
<p>I’m <a href="https://blakewatson.com/">Blake Watson</a>. I’ve been building websites since the early 2000s. Though I work professionally in the field, I feel strongly that <em>anyone</em> should be able to make a website with HTML if they want. This book will teach you how to do just that. It doesn’t require any previous experience making websites or coding. I will cover everything you need to know to get started in an approachable and friendly way.</p>
<p>Ready? Let’s do it!</p>


  <p>
    <a href="https://htmlforpeople.com/intro">Read the introduction</a>
    <a href="https://htmlforpeople.com/zero-to-internet-your-first-website">Start coding already!</a>
  </p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Recall is now an explorer.exe dependency (199 pts)]]></title>
            <link>https://github.com/ChrisTitusTech/winutil/issues/2697</link>
            <guid>41801331</guid>
            <pubDate>Thu, 10 Oct 2024 17:47:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ChrisTitusTech/winutil/issues/2697">https://github.com/ChrisTitusTech/winutil/issues/2697</a>, See on <a href="https://news.ycombinator.com/item?id=41801331">Hacker News</a></p>
<div id="readability-page-1" class="page"><div disabled="" sortable="">
          <p dir="auto">There are 3 settings you can use to practically disable copilot/recall completely. These will do the following things<br>
1: Remove the package so it doesn't even appear in search or app lists<br>
2: Remove the package so it doesn't activate if you hit the new copilot key on keyboards<br>
3: Remove the package so it doesn't even show up as an app that was installed in apps and features</p>
<p dir="auto">In group policy there are 2 settings</p>
<p dir="auto">"turn off windows copilot"<br>
(UserConfig\AdministrativeTemplates\WindowsComponents\Windows Copilot)</p>
<p dir="auto">"turn off saving snapshots to windows"<br>
(UserConfig\AdministrativeTemplates\WindowsComponents\Windows Ai)</p>
<p dir="auto">These 2 group policy options enter 2 registry keys located here</p>
<p dir="auto">HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows\WindowsCopilot<br>
HKEY_CURRENT_USER\Software\Policies\Microsoft\Windows\WindowsCopilot<br>
"TurnOffWindowsCopilot"<br>
Dword (1)</p>
<p dir="auto">HKEY_CURRENT_USER\Software\Policies\Microsoft\Windows\WindowsAI<br>
HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows\WindowsCopilot<br>
"DisableAIDataAnalysis"<br>
Dword (1)</p>
<p dir="auto">If you also use an app called "AppXPackagesManager"<br>
<a href="https://github.com/Savitarax/File-Resources">https://github.com/Savitarax/File-Resources</a><br>
(I am NOT the original creator of said app, I am merely providing the software to use)</p>
<p dir="auto">Copilot is an option that you can remove listed in user packages</p>
<p dir="auto">After doing these steps.<br>
I can't get copilot to activate in any way, nor can I find recall.</p>
<p dir="auto">Figured i'd list this to anyone that was looking for a pretty solid solution if they were looking for the template to do so.</p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Studios: Please Don't Spoil the Movie We Are Seated to See (216 pts)]]></title>
            <link>http://fxrant.blogspot.com/2024/06/studios-dont-spoil-movie-we-are-seated.html</link>
            <guid>41801300</guid>
            <pubDate>Thu, 10 Oct 2024 17:44:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://fxrant.blogspot.com/2024/06/studios-dont-spoil-movie-we-are-seated.html">http://fxrant.blogspot.com/2024/06/studios-dont-spoil-movie-we-are-seated.html</a>, See on <a href="https://news.ycombinator.com/item?id=41801300">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-8595952586617710821" itemprop="description articleBody">
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXlFhEI0V8S6K4imrSag-M5PNYBCJIV-ze_r-r6R06QUqdyY49YhxiO79peJKdwjV8mH06lM5R-JFgSU57mfctbt1eyM52NbW9L0F95gZqMaq8RIsQInxb7Jrkf8wi5V3Zw7TRod8OsP3I72jsP5ZgwGzaNAWfdjERvIgFLBtSSPvfvP9Aq5U/s720/alien-1979.jpg.webp" imageanchor="1"><img data-original-height="480" data-original-width="720" height="265" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXlFhEI0V8S6K4imrSag-M5PNYBCJIV-ze_r-r6R06QUqdyY49YhxiO79peJKdwjV8mH06lM5R-JFgSU57mfctbt1eyM52NbW9L0F95gZqMaq8RIsQInxb7Jrkf8wi5V3Zw7TRod8OsP3I72jsP5ZgwGzaNAWfdjERvIgFLBtSSPvfvP9Aq5U/w400-h265/alien-1979.jpg.webp" width="400"></a></p><p>I tweeted this incredibly non-controversial take and it got a huge reaction, so I thought I'd recycle the content for a blog post. Enjoy.</p><p>We took our kid to see "Alien" (1979) on the big screen during its one-week-only theatrical run. We told him there was a good chance of a pre-show featurette that would spoil the movie, so he needed to be ready to cover his eyes.</p><p>Well, that's exactly what happened.</p><p>My kid threw his hoodie over his eyes while a pre-show interview between Fede Alvarez and Ridley Scott appeared, featuring tons of behind-the-scenes photos of the alien, the chestburster scene, and discussion of the legacy of the classic film.</p><p>Why do this before the movie!??!</p><p>If even one person in that theater hadn't seen the film yet, it puts a huge damper on the surprise and delight that the movie would bring them, which is sad. We WANT to bring first-timers to theaters to see classic movies. Don't ruin it for them.</p><p><b>Play that shit AFTER the movie.</b></p><p>This has happened with several re-releases for me. Fathom did this to "Star Trek II: The Wrath of Khan" (pre-show highlighted a main character's death!) and "Close Encounters" (pre-show showed the effing aliens!). And now "Alien" (Disney/Fox).</p><p>The solution is simple: preserve the wonder for first-timers by putting these featurettes AFTER the movie. Tease it before the feature.</p><p>Anyway, <a href="https://www.fathomevents.com/events/close-encounters-of-the-third-kind-2024-re-release/">"Close Encounters of the Third Kind" is coming to theaters again this summer </a>(Fathom), so get ready to cover the eyes of first-timers before the show.</p><p><a href="https://x.com/tvaziri/status/1796955123337372032">Original tweet thread.</a></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Game Programming in Prolog (170 pts)]]></title>
            <link>https://thingspool.net/morsels/page-10.html</link>
            <guid>41800764</guid>
            <pubDate>Thu, 10 Oct 2024 16:50:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thingspool.net/morsels/page-10.html">https://thingspool.net/morsels/page-10.html</a>, See on <a href="https://news.ycombinator.com/item?id=41800764">Hacker News</a></p>
<div id="readability-page-1" class="page">

<a href="https://thingspool.net/morsels/list.html">Back to List</a>

<h3>Author: Youngjin Kang</h3>
<h3>Date: August 25, 2024</h3>

<hr>

<h3><b>Introduction</b></h3>
<p>As a fan of unconventional programming paradigms, I enjoy learning new programming languages which are drastically different from the typical object-oriented ones such as C#, Java, and the like. The most iconic of them are LISP (which is a powerful language for both functional programming as well as metalinguistic patterns in software development) and Prolog (which is one of the most popular languages in logic programming). Learning these languages is quite hard, compared to being acquainted with usual C-style imperative languages such as Ruby and Python, yet it has turned out to be one of the most effective ways of exercising one's brain.</p>
<p>By the time I started learning LISP via MIT's 1986 lecture series called "SICP (Structure and Interpretation of Computer Programs)" back in 2018, I was already quite familiar with some of its core concepts (such as lambda expressions, higher-order functions, etc) because they were already integrated as some of the main features of C#, which was the language I was using all the time as a Unity game developer. Also, my academic background in electrical engineering (signal processing in particular) helped me easily grasp the idea of "stream processing" which appeared in the latter half of the lecture series. Thus, learning LISP and its functional design patterns was not as difficult as I imagined it to be.</p>
<p>A major intellectual challenge, however, struck me when I began to study Prolog - the famous logic programming language which is notorious for its esoteric syntax. The grammar itself did not appear to be complicated at all; it was just as minimal as that of LISP. The way in which programming had to be done in Prolog, though, was stressful enough to fry the engine of my brain. The way it approached data structures (such as lists) and algorithms based upon mathematical relations was something so revolutionarily novel to me, that it seriously opened up a new horizon in my faculty of computational reasoning.</p>
<p>While Prolog's approach in software development was quite alien to me, I managed to notice a number of familiar associations between Prolog and many useful topics in engineering. I discovered, for example, that the so-called "relational databases" (e.g. MySQL) are named so not because they comprise data tables which are related to each other via references, but because each row of a data table can be considered an n-ary predicate (where 'n' is the number of columns in the table) in Prolog's syntax. Besides, I found out that the input/output behavior of each digital circuit component (e.g. logic gate) could be implemented as an n-ary relation (where 'n' is the total number of the input/output ports combined), implying that an "object", whether it be a piece of hardware or a piece of pure data in memory, may as well be defined as a relation in logic programming (just like an object may as well be defined as a function in functional programming). Furthermore, the declarative nature of Prolog strongly convinced me that it must be optimal for data-driven design.</p>
<p>These realizations soon led me to contemplate upon the notion that, maybe, logic programming has a great deal of potential in the design and implementation of highly complex systems, such as a video game's core gameplay mechanics. I began to ask myself, "Will it be possible to develop an entire game using the grammar of logic programming?"</p>
<p>Indeed, there are reasons why most game developers just stick to general-purpose programming languages (such as C#) for making games, aside from purely experimental purposes. Implementing an entire game based on Prolog, for instance, is perhaps too much of a challenge for those who are not hardcore mathematicians. Also, Prolog may not be the best language to use for parts of the project which are not necessarily made of a complex web of relations, such as simple I/O modules, graphics modules, audio modules, physics modules, and the like.</p>
<p>However, I believe that at least the core mechanics of a game can definitely be implemented using the language of Prolog, and that we will be able to solve a plethora of complex design problems by doing so. It is because a gameplay system which is structured in terms of a set of declarative statements will be far more robust, modular, and free of confusing edge cases (e.g. race conditions) than an imperative system.</p>
<p>For this alternative methodology to be successful, one must start by designing the system in terms of logical relations/predicates only, and nothing else (That is, no functions, no structs, no classes, no interfaces, no state variables, etc). This will allow us to construct a gameplay system which is purely driven by the soul of Prolog.</p>
<hr>

<h3><b>World and Actors</b></h3>
<p>The core idea in Prolog-based game programming is to utilize relations as the most primitive building blocks of the system, just like basic circuit components (e.g. resistors, transistors, capacitors, inductors, etc) are the most primitive building blocks of an electric circuit. It is sensible, therefore, to start this journey by considering the most rudimentary relations (e.g. unary and binary) first, and see if these elements can serve as the most essential nuts and bolts of the game.</p>
<img src="https://thingspool.net/morsels/e01.jpg" alt="Game Programming in Prolog - Part 1 (Figure 1)">
<p>Suppose that we are designing a game, and that the game consists of two major parts - world and actors (see the image above). The world is a scene in which everything is supposed to happen, and actors are objects which belong to the world. Examples of actors include "players", "enemies", "obstacles", "items", and pretty much any discrete entities which have their own names and attributes. Actors are able to interact with each other (as well as with themselves), from which various events occur. What we refer to as "gameplay" is a chain of such events.</p>
<p>We will begin formulating a gameplay system based off of this conceptual backbone. All you need to remember is that there is a world, and that the world contains a number of actors, each of which possesses its own state and behavior.</p>
<hr>

<h3><b>Tags</b></h3>
<p>First of all, let us identify each individual actor with a unique name. If there are two actors in the world, for instance, we will simply assume that the name "actor1" and "actor2" will be used to indicate the first and second actors, respectively.</p>
<img src="https://thingspool.net/morsels/e02.jpg" alt="Game Programming in Prolog - Part 1 (Figure 2)">
<p>The first piece of logic I will illustrate is the idea of tags. A tag is a keyword which, when attached to an actor, describes what the actor stands for. When an actor has the tag "bread" attached to it, for example, we should be able to tell that the actor is a piece of bread.</p>
<p>The Prolog code below assigns the tag "bread" to both actor1 and actor2, in the form of unary predicates (The tag "bread" itself is an unary relation, and "bread(actor1)" &amp; "bread(actor2)" are two separate instances of it). This implies that both actor1 and actor2 are pieces of bread.</p>
<div><pre><code>bread(actor1).
bread(actor2).</code></pre></div>
<img src="https://thingspool.net/morsels/e03.jpg" alt="Game Programming in Prolog - Part 1 (Figure 3)">
<p>An actor can have multiple tags as well. However, one may feel that it is a bit too tedious to manually assign a bunch of tags to each individual actor. For example, let us say that every piece of bread must also be labeled as flammable and decomposable. This means that, whenever an actor is associated with the tag "bread", we are obliged to always ensure that it is also associated with the tag "flammable" and "decomposable". Manually attaching these two additional tags to every "bread" actor is way too cumbersome and error-prone. Fortunately, the following pair of horn clauses neatly solve this problem. They enforce the following two rules:</p>
<p>(1) Whenever tag "bread" is assigned to actor X, tag "flammable" will automatically be assigned to actor X.<br>(2) Whenever tag "bread" is assigned to actor X, tag "decomposable" will automatically be assigned to actor X.</p>
<div><pre><code>flammable(X) :- bread(X).
decomposable(X) :- bread(X).</code></pre></div>
<img src="https://thingspool.net/morsels/e04.jpg" alt="Game Programming in Prolog - Part 1 (Figure 4)">
<p>These horn clauses, therefore, serve as part of the game's "config data" - a list of data entries in the game's technical design document (like the ones you would see on a spreadsheet) telling us the characteristics of each individual character type, skill type, mission type, and so forth. The tags called "flammable" and "decomposable" in our case, for instance, are characteristics which belong to the type-specifier called "bread", meaning that any actor which can be identified as "bread" is a composition of two properties called "flammable" and "decomposable".</p>
<p>A decent analogy can be found in Unity game engine, where we may create a prefab called "Bread" with two components in it - "Flammable" and "Decomposable". Or, in a general object-oriented programming environment, "Bread" may stand for the name of a class which implements two interfaces called "IFlammable" and "IDecomposable".</p>
<p>In a way, therefore, horn clauses in Prolog play the role of data type definitions.</p>
<img src="https://thingspool.net/morsels/e05.jpg" alt="Game Programming in Prolog - Part 1 (Figure 5)">
<p>Aside from these pre-configured tags (which all rely on the presence of the tag "bread"), one may as well attach a custom tag to an actor as needed. For example, imagine that a wizard happened to enchant actor2 (i.e. the second piece of bread). This means that, unlike actor1 which is an ordinary piece of bread, actor2 must be an "enchanted" piece of bread which is required to have the tag "enchanted" attached to it for the purpose of showing us that it has been enchanted. The code below ensures that this is the case.</p>

<img src="https://thingspool.net/morsels/e06.jpg" alt="Game Programming in Prolog - Part 1 (Figure 6)">
<p>The tags "flammable" and "decomposable" are characteristics of all pieces of bread, whereas the tag "enchanted" is a characteristic of only special pieces of bread which have been enchanted by a wizard.</p>
<hr>

<h3><b>Relationships</b></h3>
<p>So far, we have been using tags for specifying the characteristics of each individual actor. In a gameplay system, however, we also need to specify relationships between actors, such as ways in which they interact, etc.</p>
<p>In an ecosystem, predators chase preys and preys run from predators. In a dating simulator, a guy tries to flirt with girls and girls reject him. In a social simulator (such as The Sims), people are either friends or enemies of each other, or somewhere in between. In the game of chess, a bishop devours a rook diagonally and a rook devours a bishop orthogonally. These are all relationships out of which the game's dynamics emerge.</p>
<p>Defining actor-to-actor relationships in Prolog is pretty straightforward. Just like an unary predicate can be used to characterize a single actor, a binary predicate can be used to characterize a relationship between a pair of actors. And by means of a horn clause, such a relationship can be dynamically deduced from a set of requisite conditions.</p>
<p>The following code is an example of a relationship. Suppose that there is a third actor called "actor3", and that we have declared it as a human (by attaching the tag "human" to it). Since a human is able to eat a piece of bread, we can confidently assert that "X can eat Y if X is a human and Y is a piece of bread". Here, "X can eat Y" is a relationship which holds whenever X is associated with tag "human" and Y is associated with tag "bread".</p>
<div><pre><code>human(actor3).
canEat(X, Y) :- human(X), bread(Y).</code></pre></div>
<img src="https://thingspool.net/morsels/e07.jpg" alt="Game Programming in Prolog - Part 1 (Figure 7)">
<p>Here is another example. Since a piece of bread is decomposable (because anything which is identified as "bread" must also be identified as "decomposable"), we know that microbes such as fungi are capable of spoiling it. If there is an actor with the tag "fungus" attached to it, therefore, we will be able to tell that it must be able to spoil any other actor which is "decomposable". This is yet another case of a relationship between two types of actors; it is a relationship which says, "X can spoil Y if X is a fungus and Y is decomposable". The following code shows its definition.</p>
<div><pre><code>fungus(actor4).
canSpoil(X, Y) :- fungus(X), decomposable(Y).</code></pre></div>
<img src="https://thingspool.net/morsels/e08.jpg" alt="Game Programming in Prolog - Part 1 (Figure 8)">
<p>There is something still missing here, though. While I have demonstrated that it is possible to assign characteristics to individual actors as well as their mutual connections (i.e. relationships), I have not shown yet how to make these characteristics change over time. They all have been static so far, and the declarative nature of Prolog does not seem to offer an easy solution to make things dynamic.</p>
<p>If we want to create a game rather than a fixed landscape of how things are shaped permanently, we better let them move and interact as time goes by. In the next part of the series, I will explain how the game loop shall be conceptualized in Prolog.</p>
<p>(Will be continued in <a href="https://thingspool.net/morsels/page-11.html">Part 2</a>)</p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeskPad – A virtual monitor for screen sharing (795 pts)]]></title>
            <link>https://github.com/Stengo/DeskPad</link>
            <guid>41800602</guid>
            <pubDate>Thu, 10 Oct 2024 16:36:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Stengo/DeskPad">https://github.com/Stengo/DeskPad</a>, See on <a href="https://news.ycombinator.com/item?id=41800602">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h3 tabindex="-1" dir="auto">
  <a href="https://github.com/Stengo/DeskPad/blob/main/DeskPad/Assets.xcassets/AppIcon.appiconset/Icon-256.png">
  <img src="https://github.com/Stengo/DeskPad/raw/main/DeskPad/Assets.xcassets/AppIcon.appiconset/Icon-256.png?raw=true" alt="DeskPad Icon" width="128">
  </a>
</h3><a id="user-content-------" aria-label="Permalink: " href="#------"></a></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">DeskPad</h2><a id="user-content-deskpad" aria-label="Permalink: DeskPad" href="#deskpad"></a></p>
<p dir="auto">A virtual monitor for screen sharing</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">
  <a href="https://github.com/Stengo/DeskPad/blob/main/screenshot.jpg">
  <img src="https://github.com/Stengo/DeskPad/raw/main/screenshot.jpg?raw=true" alt="DeskPad Screenshot">
  </a>
</h3><a id="user-content--------1" aria-label="Permalink: " href="#-------1"></a></div>
<p dir="auto">Certain workflows require sharing the entire screen (usually due to switching through multiple applications), but if the presenter has a much larger display than the audience it can be hard to see what is happening.</p>
<p dir="auto">DeskPad creates a virtual display that is mirrored within its application window so that you can create a dedicated, easily shareable workspace.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">You can either download the <a href="https://github.com/Stengo/DeskPad/releases">latest release binary</a> or install via <a href="https://brew.sh/" rel="nofollow">Homebrew</a> by calling <code>brew install deskpad</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">DeskPad behaves like any other display. Launching the app is equivalent to plugging in a monitor, so macOS will take care of properly arranging your windows to their previous configuration.</p>
<p dir="auto">You can change the display resolution through the system preferences and the application window will adjust accordingly.</p>
<p dir="auto">Whenever you move your mouse cursor to the virtual display, DeskPad will highlight its title bar in blue and move the application window to the front to let you know where you are.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">
  <a href="https://github.com/Stengo/DeskPad/blob/main/demonstration.gif">
  <img src="https://github.com/Stengo/DeskPad/raw/main/demonstration.gif?raw=true" alt="DeskPad Demonstration" data-animated-image="">
  </a>
</h3><a id="user-content--------2" aria-label="Permalink: " href="#-------2"></a></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The science behind on-the-wrist blood pressure tracking (106 pts)]]></title>
            <link>https://www.empirical.health/blog/apple-watch-blood-pressure/</link>
            <guid>41799324</guid>
            <pubDate>Thu, 10 Oct 2024 14:40:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.empirical.health/blog/apple-watch-blood-pressure/">https://www.empirical.health/blog/apple-watch-blood-pressure/</a>, See on <a href="https://news.ycombinator.com/item?id=41799324">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>   <p>There are many <a href="https://www.bloomberg.com/news/articles/2023-11-01/apple-plans-hypertension-sleep-apnea-detection-for-next-watch">rumors</a> that an upcoming Apple Watch will measure blood pressure; similar features exist on Samsung watches internationally and are likely to come to the US once cleared by the FDA.</p>
<p>In 2018, I helped run one of the <a href="https://ojs.aaai.org/index.php/AAAI/article/view/11891">first studies</a> to show that health sensor data from wearables, when combined with a deep neural network, can pick up on signs of high blood pressure, sleep apnea, atrial fibrillation, and more. While the sensors have advanced in the last 7 years, the underlying science remains the same—and offers clues to the future.</p>
<p>In this post, I’ll try to explain the science behind blood pressure on the wrist (e.g., pulse wave velocity), past medical literature on using deep neural networks to glean signal from consumer wearables, likely limitations of wrist-based blood pressure, and how doctors and patients can incorporate it into medical practice.</p>
<h2 id="pulse-wave-analysis-for-blood-pressure">Pulse Wave Analysis for blood pressure</h2>
<p>When your heart beats, it sends a pressure wave—your pulse—throughout your body. As far back as antiquity, doctors could roughly sense blood pressure by pressing their finger against an artery. The first machine for measuring blood pressure, the sphygmograph, was invented in 1854. The modern modern blood pressure cuff, or sphygmomanometer, is its descendant.</p>
<p><img src="https://www.empirical.health/_astro/sphygmograph.CeSoVw_i_lHfGr.webp" alt="" width="1456" height="682" loading="lazy" decoding="async"></p>
<p>Sphygmomanometers require actively pressing the artery to measure pressure.</p>
<p>But what if you can’t apply force to the artery? On a watch, you only have an optical sensors—which are nearly touchless. But there are still clues that let you infer blood pressure from <em>speed</em> and <em>shape</em> of each pulse wave.</p>
<p><em>Pulse Wave Velocity (PWV)</em> is the speed at which the pulse propagates through the circulatory system. Similar to a string being pulled taught, when your blood pressure is higher, the wave travels faster.</p>
<p>If the Watch measures the precise times when (a) the heart contracts and (b) the pulse wave arrives at the wrist, then the difference tells you how your blood pressure is changing throughout the day. You can measure the exact time of the heart beat through an ECG (which Apple Watches, Samsungs, Fitbits, etc already have) or through the body’s mechanical response to the blood ejected from the heart during each heartbeat (this is called a ballistocardiogram).</p>
<p><img src="https://www.empirical.health/_astro/pulse_wave.C5LxX3ml_ZtGI4N.webp" alt="" width="850" height="510" loading="lazy" decoding="async"></p>
<p>Second, the shape of the wave offers indirect clues to blood pressure. The <em>pulse rise time</em> and the various wave amplitudes (pulse wave amplitude, pulse wave systolic peak, etc) correlate with blood pressure. All else being equal, a higher amplitude and a faster rise time correlate with more arterial stiffness and higher blood pressure.</p>
<h2 id="apple-watch-blood-pressure-would-need-to-be-calibrated-using-a-cuff">Apple Watch blood pressure would need to be calibrated using a cuff</h2>
<p>Over time, your blood pressure can drift. And every person’s parameters are a bit different. So wrist-based blood pressure needs to be calibrated initially by comparing them to a cuff, and then periodically re-calibrated (say, every 30 days).</p>
<h2 id="why-apple-watchs-future-blood-pressure-sensor-matters-for-health">Why Apple Watch’s future blood pressure sensor matters for health</h2>
<p>So why does this matter? Isn’t taking blood pressure part of the typical doctor visit?</p>
<h2 id="120-million-people-in-the-us-are-diagnosed-with-high-blood-pressure-but-only-23-have-it-under-control">120 million people in the US are diagnosed with high blood pressure, but only 23% have it under control.</h2>
<p>That means 77% of patients don’t have their blood pressure under control. These figures are from HHS, the American College of Cardiology, and the American Heart association. Blood pressure control often requires both stacking multiple medications and dietary changes that are challenging to achieve.</p>
<h2 id="high-blood-pressure-causes-heart-attacks">High blood pressure causes heart attacks</h2>
<p><img src="https://www.empirical.health/_astro/blood_pressure_heart_attacks.D8pRrgAw_ZiT2AK.webp" alt="" width="679" height="360" loading="lazy" decoding="async"></p>
<p>Alongside cholesterol or apoB, high blood pressure is one of main risk factors for a heart attack.</p>
<p>Every 20 mmHg increase in your systolic blood pressure, or 10 mmgHg increase in diastolic blood pressure, doubles your mortality.</p>
<p>To make it concrete — let’s say you’re a 40 year old man with good cholesterol numbers (160 total cholesterol, 60 HDL). If your blood pressure is 115/75 (good), your lifetime risk of a heart attack is only 5%. If your blood pressure is 150/110 (high), your lifetime risk is 10x higher — 50% (<a href="https://tools.acc.org/ldl/ascvd_risk_estimator/index.html#!/calulate/estimator/">source</a>).</p>
<h2 id="blood-pressure-is-sensitive-to-daily-counterintuitive-nutritional-choices">Blood pressure is sensitive to daily, counterintuitive nutritional choices</h2>
<p>People with high blood pressure are often recommended the DASH diet, where you try to eat <a href="https://www.heart.org/en/health-topics/high-blood-pressure/changes-you-can-make-to-manage-high-blood-pressure/how-potassium-can-help-control-high-blood-pressure">3,500 to 5,000 mg of potassium</a> per day and <a href="https://www.heart.org/en/health-topics/high-blood-pressure/changes-you-can-make-to-manage-high-blood-pressure/shaking-the-salt-habit-to-lower-high-blood-pressure">under 1,500 mg of sodium</a>. It’s surprisingly difficult to control your dietary potassium and sodium, however, since the set of foods are counterintuitive.</p>
<p>Hint: bananas don’t even make the top 10 of high-potassium foods. The set of high-potassium foods is very heterogenous—spinach, avocado, sweet potato, and white beans are all among the top 10. Sodium is even more insidious — much of our consumption comes from things like bread, which don’t necessarily taste salty.</p>
<p>Continuous glucose monitors have revolutionized treatment for those with diabetes. Could a continuous blood pressure monitor give similar, real-time feedback on dietary choices? With the benefit of AI, you can imagine that everybody would have a virtual nutritionalist.</p>
<h2 id="whats-next">What’s next?</h2>
<p>As of this writing (September 2024), blood pressure on the wrist is all just rumors. We’ll update this post as the situation changes.</p>
<p><img src="https://www.empirical.health/_astro/empirical_hypertension.Z_5qMrR7_Z2tHXLJ.webp" alt="" width="1920" height="1080" loading="lazy" decoding="async"></p>
<p>And if you own an external blood pressure cuff and want to manage your hypertension, try out
<a href="https://apps.apple.com/us/app/empirical-health-for-watch/id6449271489">Empirical Health on the App Store</a> today.</p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AAA Gaming on Asahi Linux (640 pts)]]></title>
            <link>https://rosenzweig.io/blog/aaa-gaming-on-m1.html</link>
            <guid>41799068</guid>
            <pubDate>Thu, 10 Oct 2024 14:16:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rosenzweig.io/blog/aaa-gaming-on-m1.html">https://rosenzweig.io/blog/aaa-gaming-on-m1.html</a>, See on <a href="https://news.ycombinator.com/item?id=41799068">Hacker News</a></p>
<div id="readability-page-1" class="page"> <header><p>10 Oct 2024</p></header><p>Gaming on Linux on M1 is here! We’re thrilled to release our Asahi game playing toolkit, which integrates our Vulkan 1.3 drivers with x86 emulation and Windows compatibility. Plus a bonus: conformant OpenCL 3.0.</p> <p>Asahi Linux now ships the only conformant <a href="https://www.khronos.org/conformance/adopters/conformant-products/opengl#submission_3470">OpenGL®</a>,<!--
[OpenGL® ES](https://www.khronos.org/conformance/adopters/conformant-products/opengles#submission_1045),--> <a href="https://www.khronos.org/conformance/adopters/conformant-products/opencl#submission_433">OpenCL™</a>, and <a href="https://www.khronos.org/conformance/adopters/conformant-products#submission_7910">Vulkan®</a> drivers for this hardware. As for gaming… while today’s release is an alpha, <a href="https://store.steampowered.com/app/870780/Control_Ultimate_Edition/"><strong>Control</strong></a> runs well!</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Control-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Control-small.avif" alt="Control"></a> </figure> <h2 id="installation">Installation</h2> <p>First, install <a href="https://asahilinux.org/fedora/">Fedora Asahi Remix</a>. Once installed, get the latest drivers with <code>dnf upgrade --refresh &amp;&amp; reboot</code>. Then just <code>dnf install steam</code> and play. While all M1/M2-series systems work, most games require 16GB of memory due to emulation overhead.</p> <h2 id="the-stack">The stack</h2> <p>Games are typically x86 Windows binaries rendering with DirectX, while our target is Arm Linux with Vulkan. We need to handle each difference:</p> <ul> <li><a href="https://fex-emu.com/">FEX</a> emulates x86 on Arm.</li> <li><a href="https://www.winehq.org/">Wine</a> translates Windows to Linux.</li> <li><a href="https://github.com/doitsujin/dxvk">DXVK</a> and <a href="https://github.com/HansKristian-Work/vkd3d-proton">vkd3d-proton</a> translate DirectX to Vulkan.</li> </ul> <p>There’s one curveball: page size. Operating systems allocate memory in fixed size “pages”. If an application expects smaller pages than the system uses, they will break due to insufficient alignment of allocations. That’s a problem: x86 expects 4K pages but Apple systems use 16K pages.</p> <p>While Linux can’t mix page sizes between processes, it <em>can</em> virtualize another Arm Linux kernel with a different page size. So we run games inside a tiny virtual machine using <a href="https://github.com/AsahiLinux/muvm">muvm</a>, passing through devices like the GPU and game controllers. The hardware is happy because the system is 16K, the game is happy because the virtual machine is 4K, and you’re happy because you can play <a href="https://store.steampowered.com/app/377160/Fallout_4/"><strong>Fallout 4</strong></a>.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Fallout4-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Fallout4-small.avif" alt="Fallout 4"></a> </figure> <h2 id="vulkan">Vulkan</h2> <p>The final piece is an adult-level Vulkan driver, since translating DirectX requires Vulkan 1.3 with many extensions. Back in April, I wrote <a href="https://rosenzweig.io/blog/vk13-on-the-m1-in-1-month.html">Honeykrisp</a>, the only Vulkan 1.3 driver for Apple hardware. I’ve since added DXVK support. Let’s look at some new features.</p> <h3 id="tessellation">Tessellation</h3> <p>Tessellation enables games like <a href="https://store.steampowered.com/app/292030/The_Witcher_3_Wild_Hunt/"><strong>The Witcher 3</strong></a> to generate geometry. The M1 has hardware tessellation, but it is too limited for DirectX, Vulkan, or OpenGL. We must instead tessellate with arcane compute shaders, as detailed in <a href="https://www.youtube.com/live/pDsksRBLXPk">today’s talk at XDC2024</a>.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Witcher3-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Witcher3-small.avif" alt="The Witcher 3"></a> </figure> <h3 id="geometry-shaders">Geometry shaders</h3> <p>Geometry shaders are an older, cruder method to generate geometry. Like tessellation, the M1 lacks geometry shader hardware so we emulate with compute. Is that fast? No, but geometry shaders are slow <a href="http://www.joshbarczak.com/blog/?p=667">even on desktop GPUs</a>. They don’t need to be fast – just fast enough for games like <a href="https://store.steampowered.com/app/1139900/Ghostrunner/"><strong>Ghostrunner</strong></a>.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Ghostrunner-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Ghostrunner-small.avif" alt="Ghostrunner"></a> </figure> <h3 id="enhanced-robustness">Enhanced robustness</h3> <p>“Robustness” permits an application’s shaders to access buffers out-of-bounds without crashing the hardware. In OpenGL and Vulkan, out-of-bounds loads may return arbitrary elements, and out-of-bounds stores may corrupt the buffer. Our OpenGL driver <a href="https://rosenzweig.io/blog/conformant-gl46-on-the-m1.html">exploits this definition</a> for efficient robustness on the M1.</p> <p>Some games require stronger guarantees. In DirectX, out-of-bounds loads return zero, and out-of-bounds stores are ignored. DXVK therefore requires <a href="https://docs.vulkan.org/guide/latest/robustness.html#_vk_ext_robustness2"><code>VK_EXT_robustness2</code></a>, a Vulkan extension strengthening robustness.</p> <p>Like before, we implement robustness with compare-and-select instructions. A naïve implementation would <em>compare</em> a loaded index with the buffer size and <em>select</em> a zero result if out-of-bounds. However, our GPU loads are vector while arithmetic is scalar. Even if we disabled page faults, we would need up to four compare-and-selects per load.</p> <div><pre><code><span><span>load</span> R, buffer, index * <span>16</span></span>
<span><span>ulesel</span> R[<span>0</span>], index, size, R[<span>0</span>], <span>0</span></span>
<span><span>ulesel</span> R[<span>1</span>], index, size, R[<span>1</span>], <span>0</span></span>
<span><span>ulesel</span> R[<span>2</span>], index, size, R[<span>2</span>], <span>0</span></span>
<span><span>ulesel</span> R[<span>3</span>], index, size, R[<span>3</span>], <span>0</span></span></code></pre></div> <p>There’s a trick: reserve <em>64 gigabytes</em> of zeroes using virtual memory voodoo. Since every 32-bit index multiplied by 16 fits in 64 gigabytes, any index into this region loads zeroes. For out-of-bounds loads, we simply replace the buffer address with the reserved address while preserving the index. Replacing a 64-bit address costs just two 32-bit compare-and-selects.</p> <div><pre><code><span><span>ulesel</span> buffer.lo, index, size, buffer.lo, RESERVED.lo</span>
<span><span>ulesel</span> buffer.hi, index, size, buffer.hi, RESERVED.hi</span>
<span><span>load</span> R, buffer, index * <span>16</span></span></code></pre></div> <p>Two instructions, not four.</p> <h2 id="next-steps">Next steps</h2> <p>Sparse texturing is next for Honeykrisp, which will unlock more DX12 games. The alpha already runs DX12 games that don’t require sparse, like <a href="https://store.steampowered.com/app/1091500/Cyberpunk_2077/"><strong>Cyberpunk 2077</strong></a>.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Cyberpunk2077-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Cyberpunk2077-small.avif" alt="Cyberpunk 2077"></a> </figure> <p>While many games are playable, newer AAA titles don’t hit 60fps <em>yet</em>. Correctness comes first. Performance improves next. Indie games like <a href="https://store.steampowered.com/app/367520/Hollow_Knight/"><strong>Hollow Knight</strong></a> do run full speed.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/HollowKnight-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/HollowKnight-small.avif" alt="Hollow Knight"></a> </figure> <p>Beyond gaming, we’re adding general purpose x86 emulation based on this stack. For more information, <a href="https://docs.fedoraproject.org/en-US/fedora-asahi-remix/x86-support/">see the FAQ</a>.</p> <p>Today’s alpha is a taste of what’s to come. Not the final form, but enough to enjoy <a href="https://store.steampowered.com/app/620/Portal_2/"><strong>Portal 2</strong></a> while we work towards “1.0”.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Portal2-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Portal2-small.avif" alt="Portal 2"></a> </figure> <h2 id="acknowledgements">Acknowledgements</h2> <p>This work has been years in the making with major contributions from…</p> <ul> <li><a href="https://rosenzweig.io/">Alyssa Rosenzweig</a></li> <li><a href="https://lina.yt/me">Asahi Lina</a></li> <li><a href="https://social.treehouse.systems/@chaos_princess">chaos_princess</a></li> <li><a href="https://github.com/davide125">Davide Cavalca</a></li> <li><a href="https://mastodon.social/@dougall">Dougall Johnson</a></li> <li><a href="https://ella.gay/">Ella Stanforth</a></li> <li><a href="https://www.gfxstrand.net/faith/welcome/">Faith Ekstrand</a></li> <li><a href="https://social.treehouse.systems/@janne">Janne Grunau</a></li> <li><a href="https://chaos.social/@karolherbst">Karol Herbst</a></li> <li><a href="https://social.treehouse.systems/@marcan">marcan</a></li> <li><a href="https://mary.zone/">Mary Guillemard</a></li> <li><a href="https://neal.gompa.dev/">Neal Gompa</a></li> <li><a href="https://sinrega.org/">Sergio López</a></li> <li><a href="https://github.com/TellowKrinkle">TellowKrinkle</a></li> <li><a href="https://github.com/teohhanhui">Teoh Han Hui</a></li> <li><a href="https://mastodon.gamedev.place/@robclark">Rob Clark</a></li> <li><a href="https://github.com/sonicadvance1">Ryan Houdek</a></li> </ul> <p>… Plus hundreds of developers whose work we build upon, spanning the Linux, Mesa, Wine, and FEX projects. Today’s release is thanks to the magic of open source.</p> <p>We hope you enjoy the magic.</p> <p>Happy gaming.</p> <p><a href="https://rosenzweig.io/">Back to home</a></p> </div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Tenno – Markdown and JavaScript = an Hybrid of Word and Excel (311 pts)]]></title>
            <link>https://tenno.app</link>
            <guid>41798477</guid>
            <pubDate>Thu, 10 Oct 2024 13:15:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tenno.app">https://tenno.app</a>, See on <a href="https://news.ycombinator.com/item?id=41798477">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Designing a Fast Concurrent Hash Table (121 pts)]]></title>
            <link>https://ibraheem.ca/posts/designing-papaya/</link>
            <guid>41798475</guid>
            <pubDate>Thu, 10 Oct 2024 13:15:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ibraheem.ca/posts/designing-papaya/">https://ibraheem.ca/posts/designing-papaya/</a>, See on <a href="https://news.ycombinator.com/item?id=41798475">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
    
    <p>I recently released <a href="https://github.com/ibraheemdev/papaya">papaya</a>, a fast and feature-complete concurrent hash table for Rust. In this post I want to dive into the design and research that went into creating it, as well as why you might consider using it over existing solutions. If you're looking for an overview of papaya, you might find it more useful to <a href="https://docs.rs/papaya">consult the documentation</a>.</p>
<h2 id="philosophy"><a href="#philosophy" aria-label="Anchor link for: philosophy">Philosophy</a></h2>
<p>Concurrent hash tables are a well explored topic, both in academic literature and open-source implementations. In some ways, they are the holy grail of concurrent data structures. On the other hand, a concurrent hash table is an inelegant blob of shared mutable data, often a marker of a poorly architectured program. Hash tables in general have many unfortunate properties, most of which are exacerbated in a concurrent context. However, despite their downsides, hash tables can be a necessary evil, especially for read-heavy use cases where alternative data structures are not competitive in terms of performance.</p>
<p>There are a few important properties that a concurrent hash table cares about:</p>
<ul>
<li>Read throughput/latency</li>
<li>Write throughput/latency</li>
<li>Memory usage</li>
</ul>
<p>Concurrent hash tables fall into a large spectrum depending on which of the above properties are prioritized. papaya cares a lot more about readers than writers. Reads should be extremely low latency and never blocked by a slow writer. However, it also cares a lot about predictable latency in general. While write throughput may not be exceptional, neither readers nor writers should ever suffer from latency spikes.</p>
<p>In general, use cases that care about write throughput are likely better served by alternative data structures such as <a href="https://arxiv.org/abs/1709.06056">hash tries</a>, which deserve more experimentation. papaya aims to instead serve read-heavy workloads.</p>
<p>Another huge consideration was to have an easy to use API. While papaya may use locks internally, with careful consideration, the API is lock-free and impossible to deadlock.</p>
<h2 id="basic-design"><a href="#basic-design" aria-label="Anchor link for: basic-design">Basic Design</a></h2>
<p>Consider a basic <code>RwLock&lt;HashMap&lt;K, V&gt;&gt;</code>. There are a few glaring issues with this, the primary one being that every write operation takes exclusive access of the hash map. Not only is this very expensive to synchronize, it also means that readers cannot proceed in the face of even a single writer.</p>
<p>However, even in a read-heavy or read-only workload, a <code>RwLock</code> is far from ideal. With a reader-writer lock, readers can execute in parallel. However, the lock is still a single-point of contention, even for readers. This means that every read operation will attempt to take exclusive access of the lock state to acquire the lock, resulting in an exorbitant amount of cache-coherency traffic and bringing scalability to a halt. This makes using reader-writer locks impractical for any scalable data-structure.</p>
<p>There is a small improvement we can make to greatly improve scalability: sharding. Instead of forcing every core to acquire the same lock, we can shard keys across multiple maps with a <code>Box&lt;[RwLock&lt;HashMap&lt;K, V&gt;&gt;]&gt;</code>, deciding which keys go into which map based on their hash. Now, with a sufficient number of shards, the contention is distributed across multiple locks. This is the strategy <a href="https://github.com/xacrimon/dashmap">dashmap</a> uses.</p>
<p>Sharding reduces contention, but it's far from ideal. Readers are still required to modify shared memory. That memory is less shared, but it's still shared, and writes to shared memory are expensive. Additionally, write operations still block readers, meaning even a small number of writers can greatly affect overall scalability. Locks in general pose a significant problem for read latency distributions in that a single slow writer can result in latency spikes for all readers.</p>
<p>So how do we do better? The simplest lock-free hash table looks something like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>HashMap&lt;K, V&gt; {
</span><span>    buckets: AtomicPtr&lt;[AtomicPtr&lt;Node&lt;K, V&gt;&gt;]&gt;
</span><span>}
</span><span>
</span><span>struct </span><span>Node&lt;K, V&gt; {
</span><span>    key: K,
</span><span>    value: V,
</span><span>    next: AtomicPtr&lt;Node&lt;K, V&gt;&gt;,
</span><span>}
</span></code></pre>
<p>There's a couple important layers here. The entire table is wrapped around an atomic pointer, allowing it to be atomically swapped out during a resize. Additionally, every key-value pair is behind an atomic pointer, with collisions forming a concurrent linked-list.</p>
<p>The use of atomic pointers is important. Most CPUs only support reading values up to 128-bits atomically, without tearing <sup><a href="#0">1</a></sup>. To support keys and values of arbitrary size, entries need to be allocated. This allows us to swap the <em>pointer</em> atomically.</p>
<p>Note that allocating every entry is a large fundamental design decision. It means sacrificing write throughput under heavy write load due to allocator pressure. However, this tradeoff is worth it, as it allows readers to access the table concurrently with writers. This is the design taken by C#'s <a href="https://learn.microsoft.com/en-us/dotnet/api/system.collections.concurrent.concurrentdictionary-2?view=net-8.0"><code>ConcurrentDictionary</code></a>. However, it introduces another crucial issue.</p>
<p>Now that every key-value pair is allocated, readers have to go through a pointer to access the key while iterating over the linked-list, implying a cache-miss. The cost of a cache-miss is even more severe in a concurrent setting as entries are being modified by writers, resulting in contention. We want to access as little shared memory as possible.</p>
<p>Cache locality is also the reason most modern hash tables opt for <em>open addressing</em> over closed chaining. With open addressing, each bucket contains a single key-value pair. Instead of using a linked-list to resolve collisions, writers <em>probe</em> subsequent buckets until an empty one is found. When readers encounter an empty bucket in the sequence, they can stop probing knowing the key is not in the map. This allows the entire table to be represent by a flat <code>[(K, V)]</code>, making access extremely cache-friendly.</p>
<p>At first glance, open addressing in a concurrent settings doesn't seem to provide much benefit, because entries are allocated anyways.</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>HashMap&lt;K, V&gt; {
</span><span>    buckets: AtomicPtr&lt;[AtomicPtr&lt;(K, V)&gt;]&gt;
</span><span>}
</span></code></pre>
<p>However, it opens the door for a crucial optimization. Along with the entries array, we can include a second array known as a metadata table. Each key-value pair has a corresponding byte of metadata containing a subset of its hash.</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>HashMap&lt;K, V&gt; {
</span><span>    table: AtomicPtr&lt;Table&lt;K, V&gt;&gt;
</span><span>}
</span><span>
</span><span>struct </span><span>Table&lt;K, V&gt; {
</span><span>    metadata: [AtomicU8],
</span><span>    entries: [AtomicPtr&lt;(K, V)&gt;],
</span><span>}
</span></code></pre>
<p>A metadata table allows reads to be extremely cache-efficient as they can probe the metadata instead of the entries. Note that because we only have 8 bits of metadata, there are still chances of false positives, but it's still a massive improvement.</p>
<p>Metadata tables are present in most modern hash tables, including <a href="https://abseil.io/about/design/swisstables">swiss tables</a>, the basis of <code>std::collections::HashMap</code>. They are even <em>more</em> crucial in a concurrent hash table as entries are allocated, making probing through entries directly impractical.</p>
<h2 id="probing-strategy"><a href="#probing-strategy" aria-label="Anchor link for: probing-strategy">Probing Strategy</a></h2>
<p>One of the biggest decisions to make with an open addressing table is the probing strategy. The probing strategy decides the order in which buckets are probed if the initial bucket is full. While there are many interesting strategies such as <a href="https://en.wikipedia.org/wiki/Cuckoo_hashing">cuckoo</a>, <a href="https://programming.guide/robin-hood-hashing.html">robin-hood</a>, or <a href="https://en.wikipedia.org/wiki/Hopscotch_hashing">hopscotch</a> hashing, these are expensive to implementing concurrently requiring extra synchronization, especially with a metadata table.</p>
<p>On the other hand, the existence of a metadata table means that probing becomes relatively cheap, and so a simpler probing strategy makes more sense. For example, <a href="https://github.com/rust-lang/hashbrown">hashbrown</a> uses a hybrid linear and quadratic probing strategy. Groups of 16 metadata entries are probed in parallel using a SIMD routine, while group-wise probing is quadratic. This allows for cache-efficient probing while avoiding <a href="https://en.wikipedia.org/wiki/Primary_clustering">primary clustering</a>, a common pitfall of linear probing.</p>
<p>Unfortunately, there is an issue with SIMD probing in a concurrent hash table — atomic loads must be aligned. This means we can't simply load the next 16 entries from the probing position, we have to load aligned groups. Unfortunately, it turns out that SIMD probing is not worth it when this alignment is required in my testing. In fact, swiss tables saw a 20% performance improvement when switching to unaligned reads due to increased entropy from the hash bits. For this reason, papaya sticks to a traditional quadratic probing strategy, as well as a power-of-two capacity for the typical fast modulo.</p>
<h2 id="load-factor"><a href="#load-factor" aria-label="Anchor link for: load-factor">Load Factor</a></h2>
<p>There is another important part of a hash table, its load factor. The load factor determines when the hash table is too full and should resize. Determining whether the load factor has been reached requires keep track of the number of entries in the hash table. However, maintaining a counter is very expensive in a concurrent setting as it forms another singular point of contention! While the counter is only accessed by writers, it still affects performance quite severely.</p>
<p>There are a couple of ways to work around this problem. The most obvious is to shard the length counter. While this reduces the contention when incrementing the counter, it makes accessing the total length even more expensive. papaya uses a sharded counter and exposes the length for convenience, but accessing all counter shards on every write is infeasible.</p>
<p>One solution is to rely instead of a probabilistic counter for resizing, similar to <a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a>. However, papaya takes a different approach, inspired by <a href="https://probablydance.com/2017/02/26/i-wrote-the-fastest-hashtable/">this article</a>. Instead of setting a load factor, the hash table sets a maximum <em>probe limit</em> based on the capacity of the table. Once the limit is reached, the table is resized. The probe limit is based on <code>log2</code> of the table's capacity, which tends to a ~80% load factor. I'd be interested in a formalization of probe limits and their relationship to load factor, but this number seems to work very consistently in practice, and avoids the need to synchronize a concurrent counter.</p>
<h2 id="deletion"><a href="#deletion" aria-label="Anchor link for: deletion">Deletion</a></h2>
<p>In open addressing, you can't simply unlink a value from the linked-list chain to delete it. Instead, you typically put down a marker value known as a <em>tombstone</em>. There are more complex deletion schemes such as <a href="https://codecapsule.com/2013/11/17/robin-hood-hashing-backward-shift-deletion/">backshift deletion</a>, but these are difficult to implement concurrently without introducing extra synchronization.</p>
<p>Tombstones are a bit unfortunate as they result in longer probe sequences. However, if an insert encounters a tombstone in its probe sequence, the entry can be reused for the new key. This somewhat mitigates the issue.</p>
<p>However, concurrent deletions pose a problem with a metadata table. Imagine the following sequence of events:</p>
<ul>
<li>Thread 1 inserts key <code>"a"</code></li>
<li>Thread 2 deletes key <code>"a"</code></li>
<li>Thread 2 inserts key <code>"b"</code> in the same slot</li>
<li>Thread 2 writes metadata <code>0b01</code></li>
<li>Thread 1 writes metadata <code>0b10</code> late</li>
</ul>
<p>Synchronizing the entry and its metadata are in separate locations, making them difficult to synchronize when slots are reused. One solution is to store a lock for each entry that is taken when storing an entry and its metadata. This ensures synchronization but is a significant slowdown for writers.</p>
<p>However, there is another option. Instead of using a lock, we can eliminate the problem entirely by not allowing entries to be reused after being deleted. This means that there is only one metadata value written to a given slot, so we don't have to worry about synchronization. This approach is taken by Cliff Click's <a href="https://www.youtube.com/watch?v=HJ-719EGIts">famous lock-free hash table</a>, although it uses it to synchronize keys and values instead of metadata. However, it is a pretty significant tradeoff, as it means workloads that insert and delete a lot of keys have to resize much more often to free up entries. We'll talk more about resizing later.</p>
<h2 id="memory-reclamation"><a href="#memory-reclamation" aria-label="Anchor link for: memory-reclamation">Memory Reclamation</a></h2>
<p>We've been overlooking a large problem up till now, memory reclamation. Concurrent deletion becomes a lot more difficult in a lock-free environment. In particular, there is no obvious way of telling when it is safe to free an object, as arbitrary readers may be concurrently accessing it.</p>
<p>The obvious solution to this problem is some form of reference counting. Unfortunately, reference counting is similar in cost to a reader-writer lock in that every access requires modifying shared memory. In particular, this is disastrous for synchronizing access to the table itself as it creates a single point of contention for all operations.</p>
<p>There are many algorithms to solve this problem. One popular scheme is <a href="https://www.cs.otago.ac.nz/cosc440/readings/hazard-pointers.pdf">hazard pointers</a>, which forces threads to announce access to a given object through a thread-local list of pointers. While this can be very memory efficient, it is also quite expensive for readers.</p>
<p>Another algorithm is <a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-579.pdf">epoch-based reclamation</a>. Instead of keeping track of individual objects, threads keep track of which <em>epoch</em> they are in, based on a global epoch counter that is incremented occasionally. Objects are retired in a given epoch, and once all threads have moved on from that epoch, they are safe to reclaim.</p>
<p>EBR is very lightweight. However, it is not as memory efficient as other algorithms as it tracks objects in <em>batches</em>. While this may be an acceptable tradeoff for the improved performance, EBR has a few other downsides.</p>
<p>The biggest downside with EBR and related schemes is that reclamation is not very predictable. A batch of objects can only be reclaimed once all threads have moved on from the epoch. This means that to reclaim a batch, you must check the status of all active threads, which is very expensive and requires accessing thread-local shared memory. This results in a tradeoff between reclamation balancing and performance depending on how often reclamation is attempted. For example, the <a href="https://github.com/crossbeam-rs/crossbeam/tree/master/crossbeam-epoch">crossbeam-epoch</a> crate checks for garbage every 128 operations. Importantly, the check must be performed by both readers and writers, causing reclamation to trigger unpredictably and leading to poor latency distributions.</p>
<p>Because papaya allocates every entry and does not reuse tombstones, memory efficiency is a very important factor. Unfortunately, existing memory reclamation algorithms were not up to par in my testing.</p>
<p>A few years ago, I stumbled across <a href="https://arxiv.org/pdf/1905.07903">hyaline</a>, an algorithm that solves a lot of these issues, which has since been implemented in the <a href="https://github.com/ibraheemdev/seize">seize</a> crate. In hyaline, the expensive cross-thread check is performed when a batch of objects is <em>retired</em>. The batch is propagated to all active threads just once. After this initial retirement phase, the batch is reclaimed using reference counting. This reclamation process is much more predictable, as threads can check for new garbage before every single operation without sacrificing performance. In practice, it tends to outperforms EBR due to the parallelism gains from workload balancing.</p>
<p>Hyaline also solves another problem with EBR, robustness. In EBR, a single slow thread can prevent the reclamation of all objects in a given epoch. Hyaline counteracts this by keeping track of the epoch an object is created in, filtering out slow threads when reclaiming new objects. These additional properties make hyaline a perfect fit for papaya.</p>
<h2 id="resizing"><a href="#resizing" aria-label="Anchor link for: resizing">Resizing</a></h2>
<p>Once a hash table gets too full it needs to resize, relocating all keys and values to a larger table. In a concurrent setting, this can be quite expensive. To reduce the cost of resizing, multiple threads can help out with the migration and copy entries in parallel.</p>
<p>There many tradeoffs to be made when implementing concurrent resizing. Ideally, readers should be unaffected by resizing. This would require all writers to complete the migration before making progress, allowing for a single source of truth for readers. However, resizing can be slow, introducing latency spikes for writers. For a large table, resizing can take hundreds of milliseconds or even <em>seconds</em> to complete. This is an unacceptable amount of latency for a large number of applications.</p>
<p>To avoid latency spikes, we can implement <em>incremental resizing</em>, where entries are incrementally copied over to the new table instead of blocking. This is an approach taken even by single-threaded hash tables, such as the <a href="https://crates.io/crates/griddle">griddle</a> crate.</p>
<p>Managing the state of two tables concurrently is tricky, but papaya implements a migration algorithm that allows concurrent updates to the old table and atomic copies to the new table. This does mean that during migration, many operations have to check both the new and old tables when searching for an entry. However, this is typically an acceptable tradeoff as resize operations are generally uncommon and slightly increased latency for a short period of time is better than extreme latency spikes.</p>
<p>Incremental resizing also counteracts the effect of permanent tombstones, as the cost of resizing is amortized. However, for flexibility, papaya supports both resizing modes as an option. When write throughput or read latency is the primary concern, blocking resizes can be used instead.</p>
<p>Note that resizing is the only case where papaya is not lock-free. Allocating the next table involves taking a lock to prevent excessive allocator pressure. Additionally, a write operation may block if its key is in the process of being copied to the new table. papaya uses a hybrid spinning strategy before falling back to blocking in this case. However, note that copying an entry does not involve allocating and is typically very fast. Blocking was an intentional design decision as true lock-free resizing is very expensive, but care was taken to mitigate any issues that might arise from blocking.</p>
<h2 id="additional-features"><a href="#additional-features" aria-label="Anchor link for: additional-features">Additional Features</a></h2>
<p>Along with all the performance characteristics mentioned above, papaya has some unique features. </p>
<p>Because papaya does not contain locks, performing complex operations is more challenging. Instead, papaya exposes a number of atomic operations. The most powerful of these is <a href="https://docs.rs/papaya/latest/papaya/struct.HashMap.html#method.compute"><code>HashMap::compute</code></a>, which allows updating an entry using a compare-and-swap (CAS) function:</p>
<pre data-lang="rust"><code data-lang="rust"><span>let</span><span> map </span><span>= </span><span>papaya::HashMap::new();
</span><span>
</span><span>let </span><span>compute </span><span>= </span><span>|</span><span>entry</span><span>| </span><span>match</span><span> entry {
</span><span>    </span><span>// Remove the value if it is even.
</span><span>    </span><span>Some</span><span>((_key, value)) </span><span>if</span><span> value </span><span>% </span><span>2 </span><span>== </span><span>0 </span><span>=&gt; </span><span>{
</span><span>        Operation::Remove
</span><span>    }
</span><span>
</span><span>    </span><span>// Increment the value if it is odd.
</span><span>    </span><span>Some</span><span>((_key, value)) </span><span>=&gt; </span><span>{
</span><span>        Operation::Insert(value </span><span>+ </span><span>1</span><span>)
</span><span>    }
</span><span>
</span><span>    </span><span>// Do nothing if the key does not exist
</span><span>    </span><span>None </span><span>=&gt; </span><span>Operation::Abort(()),
</span><span>};
</span><span>
</span><span>map.</span><span>pin</span><span>().</span><span>compute</span><span>(</span><span>'A'</span><span>, compute);
</span></code></pre>
<p>This allows performing complex operations despite the lack of locks.</p>
<p>Another unique feature of papaya is async support. One of the biggest downsides of <a href="https://github.com/xacrimon/dashmap">dashmap</a> is that it uses synchronous locks and so holding a reference to an item from a <code>Dashmap</code> will lead to a deadlock. Because papaya has a lock-free API, deadlocking is impossible. However, accessing the map still requires <a href="https://docs.rs/papaya/latest/papaya/#usage">acquiring a guard</a> for memory reclamation, i.e. the call to <code>pin</code> in the above example. This guard is <code>!Send</code> as it is tied to the current thread's memory reclamation state. However, papaya also exposes <a href="https://docs.rs/papaya/latest/papaya/struct.HashMap.html#method.pin_owned">owned guards</a>, which are <code>Send</code> and <code>Sync</code>, independent of any given thread. These are more expensive to create, but are allowed to be held across <code>.await</code> points when using a work-stealing scheduler:</p>
<pre data-lang="rust"><code data-lang="rust"><span>async </span><span>fn </span><span>run</span><span>(</span><span>map</span><span>: Arc&lt;HashMap&lt;</span><span>i32</span><span>, </span><span>String</span><span>&gt;&gt;) {
</span><span>    tokio::spawn(async </span><span>move </span><span>{
</span><span>        </span><span>let</span><span> map </span><span>=</span><span> map.</span><span>pin_owned</span><span>(); </span><span>// &lt;--
</span><span>        </span><span>for </span><span>(key, value) </span><span>in</span><span> map.</span><span>iter</span><span>() {
</span><span>            tokio::fs::write(</span><span>"db.txt"</span><span>, format!(</span><span>"</span><span>{key}</span><span>: </span><span>{value}\n</span><span>"</span><span>)).await;
</span><span>        }
</span><span>    });
</span><span>}
</span></code></pre>
<p>Async support is something I am very excited about and is not present in any existing concurrent hash tables that I am aware of.</p>
<h2 id="comparisons"><a href="#comparisons" aria-label="Anchor link for: comparisons">Comparisons</a></h2>
<p>There are a number of existing concurrent hash table crates. However, most of them lack in terms of read throughput and predictable latency compared to papaya. Additionally, async support is a difficult feature to find. However, there are cases where you might want to consider a different crate.</p>
<ul>
<li><a href="https://github.com/xacrimon/dashmap">dashmap</a> has a very simple design built on top of <a href="https://github.com/rust-lang/hashbrown">hashbrown</a>. It also closely mirrors the API of <code>std::collections::HashMap</code>. For write-heavy workloads, it may provide better performance. It is also lower overhead in terms of memory usage.</li>
<li><a href="https://github.com/wvwwvwwv/scalable-concurrent-containers">scc</a> is similar to dashmap but shards bucket locks even more aggressively. For write-heavy workloads it should probably be your first choice, although the code itself seemed quite complicated and difficult to audit.</li>
<li><a href="https://github.com/jonhoo/flurry">flurry</a> is a closed-addressing table with striped locks but lock-free reads. However, it suffers from performance and memory usage issues due to allocator pressure. papaya should outperform flurry in general for most workloads.</li>
<li><a href="https://github.com/jonhoo/evmap">evmap</a> is great for <em>extremely</em> read-heavy use cases. However, it is eventually consistent, and writes are relatively expensive. Scalability suffers under load even for 99% read-heavy workloads.</li>
<li><a href="https://github.com/robclu/leapfrog">leapfrog</a> provides excellent performance but is limited to 64-bit <code>Copy</code> values. This limitation is common in academic literature, and leapfrog falls back to spinlocks for arbitrary value types, which is unfortunate for a general purpose map.</li>
</ul>
<p>Consult the <a href="https://github.com/ibraheemdev/papaya/blob/master/BENCHMARKS.md">benchmarks</a> for more information, but as always, take them with a grain of salt. Always measure for your own workload.</p>
<hr>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[End-to-End Encrypted Cloud Storage in the Wild: A Broken Ecosystem (102 pts)]]></title>
            <link>https://brokencloudstorage.info/</link>
            <guid>41798359</guid>
            <pubDate>Thu, 10 Oct 2024 13:00:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brokencloudstorage.info/">https://brokencloudstorage.info/</a>, See on <a href="https://news.ycombinator.com/item?id=41798359">Hacker News</a></p>
<div id="readability-page-1" class="page"><div _ngcontent-ng-c1164979773=""><tab _ngcontent-ng-c3202822997="" heading="Sync" id="sync" role="tabpanel" aria-labelledby="sync-link"><sync-component _ngcontent-ng-c3202822997="" _nghost-ng-c3235941268="" ngh="0"><div _ngcontent-ng-c3235941268="" id="tld"><div _ngcontent-ng-c3235941268="" id="tld-description"><p _ngcontent-ng-c3235941268="">Sync is a Canadian cloud storage provider founded in 2011 with more than 2 million users worldwide, including high profile institutions such as the government of Canada, the university of Toronto, and the Canadian red cross. It claims to store more than 130 petabytes of data.</p><p _ngcontent-ng-c3235941268="">Our attacks allow a malicious server to break the confidentiality of uploaded files, as well as injecting files and tampering with their content.</p></div><div _ngcontent-ng-c3235941268="" id="container"><div _ngcontent-ng-c3235941268=""><h2 _ngcontent-ng-c3235941268="">Cryptographic Primitives</h2><ul _ngcontent-ng-c3235941268=""><li _ngcontent-ng-c3235941268="">Key Derivation <ul _ngcontent-ng-c3235941268=""><li _ngcontent-ng-c3235941268="">PBKDF2-SHA256 (random salt)</li></ul></li><li _ngcontent-ng-c3235941268="">Symmetric Encryption <ul _ngcontent-ng-c3235941268=""><li _ngcontent-ng-c3235941268="">AES-GCM</li></ul></li><li _ngcontent-ng-c3235941268="">Asymmetric Encryption <ul _ngcontent-ng-c3235941268=""><li _ngcontent-ng-c3235941268="">RSA-PKCS1v1.5</li></ul></li></ul></div><div _ngcontent-ng-c3235941268=""><h2 _ngcontent-ng-c3235941268="">Key Hierarchy</h2><p><img _ngcontent-ng-c3235941268="" src="https://brokencloudstorage.info/sync_key_hierarchy.png"></p></div><div _ngcontent-ng-c3235941268=""><h2 _ngcontent-ng-c3235941268="">Attacks</h2><table _ngcontent-ng-c3235941268=""><thead _ngcontent-ng-c3235941268=""></thead><tbody _ngcontent-ng-c3235941268=""><tr _ngcontent-ng-c3235941268=""><td _ngcontent-ng-c3235941268=""><i _ngcontent-ng-c3235941268=""></i><p _ngcontent-ng-c3235941268=""><span _ngcontent-ng-c3235941268="">Key Replacement</span></p></td><td _ngcontent-ng-c3235941268=""><i _ngcontent-ng-c3235941268=""></i><p _ngcontent-ng-c3235941268=""><span _ngcontent-ng-c3235941268="">Link Sharing Leakage</span></p></td></tr><tr _ngcontent-ng-c3235941268=""><td _ngcontent-ng-c3235941268=""><i _ngcontent-ng-c3235941268=""></i><p _ngcontent-ng-c3235941268=""><span _ngcontent-ng-c3235941268="">Tampering with Filenames</span></p></td><td _ngcontent-ng-c3235941268=""><i _ngcontent-ng-c3235941268=""></i><p _ngcontent-ng-c3235941268=""><span _ngcontent-ng-c3235941268="">Tampering with Metadata</span></p></td></tr><tr _ngcontent-ng-c3235941268=""><td _ngcontent-ng-c3235941268=""><i _ngcontent-ng-c3235941268=""></i><p _ngcontent-ng-c3235941268=""><span _ngcontent-ng-c3235941268="">Folder Injection</span></p></td><td _ngcontent-ng-c3235941268=""><i _ngcontent-ng-c3235941268=""></i><p _ngcontent-ng-c3235941268=""><span _ngcontent-ng-c3235941268="">File Injection</span></p></td></tr></tbody></table></div></div></div></sync-component></tab><tab _ngcontent-ng-c3202822997="" heading="pCloud" id="pcloud" role="tabpanel" aria-labelledby="pcloud-link"><pcloud-component _ngcontent-ng-c3202822997="" _nghost-ng-c1084061718="" ngh="0"><div _ngcontent-ng-c1084061718="" id="tld"><div _ngcontent-ng-c1084061718="" id="tld-description"><p _ngcontent-ng-c1084061718="">pCloud is a Swiss-based cloud storage provider founded in 2013 with more than 20 million users worldwide and claiming to be "Europe's most secure cloud storage"</p><p _ngcontent-ng-c1084061718="">Our attacks allow a malicious server to break the confidentiality of uploaded files, as well as injecting files and tampering with their content.</p></div><div _ngcontent-ng-c1084061718="" id="container"><div _ngcontent-ng-c1084061718=""><h2 _ngcontent-ng-c1084061718="">Cryptographic Primitives</h2><ul _ngcontent-ng-c1084061718=""><li _ngcontent-ng-c1084061718="">Key Derivation <ul _ngcontent-ng-c1084061718=""><li _ngcontent-ng-c1084061718="">PBKDF2-SHA512 (random salt)</li></ul></li><li _ngcontent-ng-c1084061718="">Symmetric Encryption <ul _ngcontent-ng-c1084061718=""><li _ngcontent-ng-c1084061718="">AES-CTR (variation)</li><li _ngcontent-ng-c1084061718="">Custom authenticated mode of operation</li></ul></li><li _ngcontent-ng-c1084061718="">Asymmetric Encryption <ul _ngcontent-ng-c1084061718=""><li _ngcontent-ng-c1084061718="">RSA-OAEP</li></ul></li></ul></div><div _ngcontent-ng-c1084061718=""><h2 _ngcontent-ng-c1084061718="">Key Hierarchy</h2><p><img _ngcontent-ng-c1084061718="" src="https://brokencloudstorage.info/pcloud_key_hierarchy.png"></p></div><div _ngcontent-ng-c1084061718=""><h2 _ngcontent-ng-c1084061718="">Attacks</h2><table _ngcontent-ng-c1084061718=""><thead _ngcontent-ng-c1084061718=""></thead><tbody _ngcontent-ng-c1084061718=""><tr _ngcontent-ng-c1084061718=""><td _ngcontent-ng-c1084061718=""><i _ngcontent-ng-c1084061718=""></i><p _ngcontent-ng-c1084061718=""><span _ngcontent-ng-c1084061718="">Key Replacement</span></p></td><td _ngcontent-ng-c1084061718=""><i _ngcontent-ng-c1084061718=""></i><p _ngcontent-ng-c1084061718=""><span _ngcontent-ng-c1084061718="">Key Overwriting</span></p></td></tr><tr _ngcontent-ng-c1084061718=""><td _ngcontent-ng-c1084061718=""><i _ngcontent-ng-c1084061718=""></i><p _ngcontent-ng-c1084061718=""><span _ngcontent-ng-c1084061718="">Unauthenticated Chunking</span></p></td><td _ngcontent-ng-c1084061718=""><i _ngcontent-ng-c1084061718=""></i><p _ngcontent-ng-c1084061718=""><span _ngcontent-ng-c1084061718="">Tampering with Files and Filedata</span></p></td></tr><tr _ngcontent-ng-c1084061718=""><td _ngcontent-ng-c1084061718=""><i _ngcontent-ng-c1084061718=""></i><p _ngcontent-ng-c1084061718=""><span _ngcontent-ng-c1084061718="">Tampering with Metadata</span></p></td><td _ngcontent-ng-c1084061718=""><i _ngcontent-ng-c1084061718=""></i><p _ngcontent-ng-c1084061718=""><span _ngcontent-ng-c1084061718="">Folder/File Injection</span></p></td></tr></tbody></table></div></div></div></pcloud-component></tab><tab _ngcontent-ng-c3202822997="" heading="Seafile" id="seafile" role="tabpanel" aria-labelledby="seafile-link"><seafile-component _ngcontent-ng-c3202822997="" _nghost-ng-c1696464851="" ngh="0"><div _ngcontent-ng-c1696464851="" id="tld"><div _ngcontent-ng-c1696464851="" id="tld-description"><p _ngcontent-ng-c1696464851="">Seafile is an open-source cloud storage provider founded in 2012 with more than one million users worldwide. In contrast to the other providers we analyze, Seafile also offers a self-hosting solution. Among the organizations using it are Kaspersky Lab, the Humboldt University in Berlin, the University of Strasbourg, and the University of Turku.</p><p _ngcontent-ng-c1696464851="">Our attacks allow a malicious server to speed-up bruteforcing of user passwords, as well as injecting files and tampering with their content.</p></div><div _ngcontent-ng-c1696464851="" id="container"><div _ngcontent-ng-c1696464851=""><h2 _ngcontent-ng-c1696464851="">Cryptographic Primitives</h2><ul _ngcontent-ng-c1696464851=""><li _ngcontent-ng-c1696464851="">Key Derivation <ul _ngcontent-ng-c1696464851=""><li _ngcontent-ng-c1696464851="">PBKDF2-SHA256</li><li _ngcontent-ng-c1696464851="">BytesToKey-SHA1</li></ul></li><li _ngcontent-ng-c1696464851="">Symmetric Encryption <ul _ngcontent-ng-c1696464851=""><li _ngcontent-ng-c1696464851="">AES-128-CBC</li><li _ngcontent-ng-c1696464851="">AES-256-CBC</li><li _ngcontent-ng-c1696464851="">AES-128-ECB</li></ul></li></ul></div><div _ngcontent-ng-c1696464851=""><h2 _ngcontent-ng-c1696464851="">Key Hierarchy</h2><p><img _ngcontent-ng-c1696464851="" src="https://brokencloudstorage.info/seafile_key_hierarchy.png"></p></div><div _ngcontent-ng-c1696464851=""><h2 _ngcontent-ng-c1696464851="">Attacks</h2><table _ngcontent-ng-c1696464851=""><thead _ngcontent-ng-c1696464851=""></thead><tbody _ngcontent-ng-c1696464851=""><tr _ngcontent-ng-c1696464851=""><td _ngcontent-ng-c1696464851=""><i _ngcontent-ng-c1696464851=""></i><p _ngcontent-ng-c1696464851=""><span _ngcontent-ng-c1696464851="">Protocol Downgrade</span></p></td><td _ngcontent-ng-c1696464851=""><i _ngcontent-ng-c1696464851=""></i><p _ngcontent-ng-c1696464851=""><span _ngcontent-ng-c1696464851="">Unauthenticated Encryption</span></p></td></tr><tr _ngcontent-ng-c1696464851=""><td _ngcontent-ng-c1696464851=""><i _ngcontent-ng-c1696464851=""></i><p _ngcontent-ng-c1696464851=""><span _ngcontent-ng-c1696464851="">Unauthenticated Chunking</span></p></td><td _ngcontent-ng-c1696464851=""><i _ngcontent-ng-c1696464851=""></i><p _ngcontent-ng-c1696464851=""><span _ngcontent-ng-c1696464851="">Tampering with Files</span></p></td></tr><tr _ngcontent-ng-c1696464851=""><td _ngcontent-ng-c1696464851=""><i _ngcontent-ng-c1696464851=""></i><p _ngcontent-ng-c1696464851=""><span _ngcontent-ng-c1696464851="">Tampering with Metadata</span></p></td><td _ngcontent-ng-c1696464851=""><i _ngcontent-ng-c1696464851=""></i><p _ngcontent-ng-c1696464851=""><span _ngcontent-ng-c1696464851="">Folder/File Injection</span></p></td></tr></tbody></table></div></div></div></seafile-component></tab><tab _ngcontent-ng-c3202822997="" heading="Icedrive" id="icedrive" role="tabpanel" aria-labelledby="icedrive-link"><icedrive-component _ngcontent-ng-c3202822997="" _nghost-ng-c3334744677="" ngh="0"><div _ngcontent-ng-c3334744677="" id="tld"><div _ngcontent-ng-c3334744677="" id="tld-description"><p _ngcontent-ng-c3334744677="">Icedrive is a UK-based cloud storage provider founded in 2019 with an estimated 150k users worldwide.</p><p _ngcontent-ng-c3334744677="">Our attacks allow a malicious server to break the integrity of uploaded files, as well as injecting files and tampering with their content.</p></div><div _ngcontent-ng-c3334744677="" id="container"><div _ngcontent-ng-c3334744677=""><h2 _ngcontent-ng-c3334744677="">Cryptographic Primitives</h2><ul _ngcontent-ng-c3334744677=""><li _ngcontent-ng-c3334744677="">Key Derivation <ul _ngcontent-ng-c3334744677=""><li _ngcontent-ng-c3334744677="">PBKDF2-SHA256</li></ul></li><li _ngcontent-ng-c3334744677="">Symmetric Encryption <ul _ngcontent-ng-c3334744677=""><li _ngcontent-ng-c3334744677="">TwoFish-CBC</li><li _ngcontent-ng-c3334744677="">TwoFish (in custom mode)</li></ul></li></ul></div><div _ngcontent-ng-c3334744677=""><h2 _ngcontent-ng-c3334744677="">Key Hierarchy</h2><p><img _ngcontent-ng-c3334744677="" src="https://brokencloudstorage.info/icedrive_key_hierarchy.png"></p></div><div _ngcontent-ng-c3334744677=""><h2 _ngcontent-ng-c3334744677="">Attacks</h2><table _ngcontent-ng-c3334744677=""><thead _ngcontent-ng-c3334744677=""></thead><tbody _ngcontent-ng-c3334744677=""><tr _ngcontent-ng-c3334744677=""><td _ngcontent-ng-c3334744677=""><i _ngcontent-ng-c3334744677=""></i><p _ngcontent-ng-c3334744677=""><span _ngcontent-ng-c3334744677="">Unauthenticated Encryption</span></p></td><td _ngcontent-ng-c3334744677=""><i _ngcontent-ng-c3334744677=""></i><p _ngcontent-ng-c3334744677=""><span _ngcontent-ng-c3334744677="">Unauthenticated Chunking</span></p></td></tr><tr _ngcontent-ng-c3334744677=""><td _ngcontent-ng-c3334744677=""><i _ngcontent-ng-c3334744677=""></i><p _ngcontent-ng-c3334744677=""><span _ngcontent-ng-c3334744677="">Tampering with File Locations and Names</span></p></td><td _ngcontent-ng-c3334744677=""><i _ngcontent-ng-c3334744677=""></i><p _ngcontent-ng-c3334744677=""><span _ngcontent-ng-c3334744677="">Tampering with Metadata</span></p></td></tr><tr _ngcontent-ng-c3334744677=""><td _ngcontent-ng-c3334744677=""><i _ngcontent-ng-c3334744677=""></i><p _ngcontent-ng-c3334744677=""><span _ngcontent-ng-c3334744677="">File Injection</span></p></td></tr></tbody></table></div></div></div></icedrive-component></tab><tab _ngcontent-ng-c3202822997="" heading="Tresorit" id="tresorit" role="tabpanel" aria-labelledby="tresorit-link"><tresorit-component _ngcontent-ng-c3202822997="" _nghost-ng-c3070533193="" ngh="0"><div _ngcontent-ng-c3070533193="" id="tld"><div _ngcontent-ng-c3070533193="" id="tld-description"><p _ngcontent-ng-c3070533193="">Tresorit is a Swiss-based cloud storage provider founded in 2011 with an estimated 10 million users worldwide.</p><p _ngcontent-ng-c3070533193="">Our attacks allow a malicious server to present non-authentic keys when sharing files and to tamper with some metadata in the storage.</p></div><div _ngcontent-ng-c3070533193="" id="container"><div _ngcontent-ng-c3070533193=""><h2 _ngcontent-ng-c3070533193="">Cryptographic Primitives</h2><ul _ngcontent-ng-c3070533193=""><li _ngcontent-ng-c3070533193="">Key Derivation <ul _ngcontent-ng-c3070533193=""><li _ngcontent-ng-c3070533193="">scrypt</li><li _ngcontent-ng-c3070533193="">PBKDF2</li></ul></li><li _ngcontent-ng-c3070533193="">Symmetric Encryption <ul _ngcontent-ng-c3070533193=""><li _ngcontent-ng-c3070533193="">AES-GCM (key material)</li><li _ngcontent-ng-c3070533193="">AES-CFB (data)</li></ul></li><li _ngcontent-ng-c3070533193="">Asymmetric Encryption <ul _ngcontent-ng-c3070533193=""><li _ngcontent-ng-c3070533193="">RSA-OAEP</li></ul></li></ul></div><div _ngcontent-ng-c3070533193=""><h2 _ngcontent-ng-c3070533193="">Key Hierarchy</h2><p><img _ngcontent-ng-c3070533193="" src="https://brokencloudstorage.info/tresorit_key_hierarchy.png"></p></div><div _ngcontent-ng-c3070533193=""><h2 _ngcontent-ng-c3070533193="">Attacks</h2><table _ngcontent-ng-c3070533193=""><thead _ngcontent-ng-c3070533193=""></thead><tbody _ngcontent-ng-c3070533193=""><tr _ngcontent-ng-c3070533193=""><td _ngcontent-ng-c3070533193=""><i _ngcontent-ng-c3070533193=""></i><p _ngcontent-ng-c3070533193=""><span _ngcontent-ng-c3070533193="">Key Replacement</span></p></td><td _ngcontent-ng-c3070533193=""><i _ngcontent-ng-c3070533193=""></i><p _ngcontent-ng-c3070533193=""><span _ngcontent-ng-c3070533193="">Tampering with Metadata</span></p></td></tr></tbody></table></div></div></div></tresorit-component></tab></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Owe your banker £1k you are at his mercy; owe him £1m the position is reversed (2019) (145 pts)]]></title>
            <link>https://quoteinvestigator.com/2019/04/23/bank/</link>
            <guid>41798027</guid>
            <pubDate>Thu, 10 Oct 2024 12:08:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quoteinvestigator.com/2019/04/23/bank/">https://quoteinvestigator.com/2019/04/23/bank/</a>, See on <a href="https://news.ycombinator.com/item?id=41798027">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-135737">
		
	
	<div>
		<p><strong>John Maynard Keynes? Paul Bareau? John Paul Getty? Anonymous?</strong></p>
<p><img fetchpriority="high" decoding="async" src="https://77c054.p3cdn1.secureserver.net/wp-content/uploads/2019/04/money08.jpg" alt="" width="540" height="221" srcset="https://77c054.p3cdn1.secureserver.net/wp-content/uploads/2019/04/money08.jpg 540w, https://77c054.p3cdn1.secureserver.net/wp-content/uploads/2019/04/money08-300x123.jpg 300w" sizes="(max-width: 540px) 100vw, 540px"><strong>Dear Quote Investigator:</strong> The relationship between bankers and borrowers is symbiotic and occasionally counter-intuitive. Here is a pertinent adage:</p>
<blockquote><p>If you owe the bank $100, that’s your problem; if you owe the bank $100 million, that’s the bank’s problem.</p></blockquote>
<p>The prominent economist John Maynard Keynes apparently made a similar remark using pounds sterling instead of dollars. Would you please explore this topic?</p>
<p><strong>Quote Investigator:</strong> The earliest strong match known to <strong>QI</strong> occurred in a memo that Keynes circulated to the British War Cabinet in 1945; however, the attribution was anonymous. Emphasis added to excerpts by <strong>QI</strong>:<span><a role="button" tabindex="0" href="#f+135737+1+1"><sup id="footnote_plugin_tooltip_135737_1_1">[1]</sup></a><span><span id="r+135737+1+1"></span></span><span id="footnote_plugin_tooltip_text_135737_1_1">1979, The Collected Writings of John Maynard Keynes: Volume 24: Activities 1944-1946: The Transition to Peace, Edited by Donald Moggridge, Section: Overseas Financial Policy in Stage III (Revised&nbsp;… <a href="#f+135737+1+1">Continue reading</a></span></span></p>
<blockquote><p>On such conditions, by cunning and kindness, we have persuaded the outside world to lend us upwards of the prodigious total of £3,000 million. The very size of these sterling debts is itself a protection. The old saying holds. <strong>Owe your banker £1,000 and you are at his mercy; owe him £1 million and the position is reversed.</strong></p></blockquote>
<p>Below are additional selected citations in chronological order.</p>
<p><span id="more-135737"></span>In 1942 H. L. Mencken included a thematically related adage in his remarkable compendium “A New Dictionary of Quotations on Historical Principles from Ancient and Modern Sources”<span><a role="button" tabindex="0" href="#f+135737+1+2"><sup id="footnote_plugin_tooltip_135737_1_2">[2]</sup></a><span><span id="r+135737+1+2"></span></span><span id="footnote_plugin_tooltip_text_135737_1_2">1942, A New Dictionary of Quotations on Historical Principles from Ancient and Modern Sources, Selected and Edited by H. L. Mencken (Henry Louis Mencken), Topic: Bank, Quote Page 82, Alfred A. Knopf.&nbsp;… <a href="#f+135737+1+2">Continue reading</a></span></span></p>
<blockquote><p><strong>If you owe a bank enough money you own it.</strong><br>
Author unidentified</p></blockquote>
<p>In 1943 “Esar’s Comic Dictionary” by Evan Esar included the same adage listed in Mencken’s work.<span><a role="button" tabindex="0" href="#f+135737+1+3"><sup id="footnote_plugin_tooltip_135737_1_3">[3]</sup></a><span><span id="r+135737+1+3"></span></span><span id="footnote_plugin_tooltip_text_135737_1_3"> 1943, Esar’s Comic Dictionary by Evan Esar, Entry: bank, Quote Page 21, Harvest House, New York. (Verified on paper) </span></span></p>
<p>In 1945 Keynes circulated a memo containing the expression under analysis as mentioned previously. The memo appeared in volume 24 of “The Collected Writings of John Maynard Keynes” which was published in 1979.</p>
<p>In February 1947 “Time” magazine credited Keynes with a slightly different phrasing of the quip:<span><a role="button" tabindex="0" href="#f+135737+1+4"><sup id="footnote_plugin_tooltip_135737_1_4">[4]</sup></a><span><span id="r+135737+1+4"></span></span><span id="footnote_plugin_tooltip_text_135737_1_4"> 1947 February 17, Time, Foreign News: Whose Mercy?, Time Inc, New York. (Verified via online archive at time.com on April 21, 2018)</span></span></p>
<blockquote><p>Before his death, Lord Keynes had spoken his mind about those sterling debts: <strong>“If you owe your bank manager a thousand pounds, you are at his mercy. If you owe him a million pounds, he is at your mercy.”</strong></p></blockquote>
<p>Four days later the saying was printed in the “San Francisco Chronicle” of California with an ascription to Keynes:<span><a role="button" tabindex="0" href="#f+135737+1+5"><sup id="footnote_plugin_tooltip_135737_1_5">[5]</sup></a><span><span id="r+135737+1+5"></span></span><span id="footnote_plugin_tooltip_text_135737_1_5"> 1947 February 21, San Francisco Chronicle, Editorial: Indian Problem Headed For a Showdown, Quote Page 12, Column 1, San Francisco, California. (GenealogyBank)</span></span></p>
<blockquote><p>Britain owes India £1,250,000,000, a sum which must be paid in British products if at all.</p>
<p>The late Lord Keynes said in this connection: <strong>“If you owe your bank manager a thousand pounds, you are at his mercy. If you owe him a million pounds, he is at yours.”</strong></p></blockquote>
<p>In April 1947 the periodical “Nation’s Business” published a version using dollars instead of pounds sterling:<span><a role="button" tabindex="0" href="#f+135737+1+6"><sup id="footnote_plugin_tooltip_135737_1_6">[6]</sup></a><span><span id="r+135737+1+6"></span></span><span id="footnote_plugin_tooltip_text_135737_1_6">1947 April, Nation’s Business, Volume 35, Number 4, On the Lighter Side of the Capital, Journalistic thermostats, Quote Page 94, Column 2, Chamber of Commerce of the U.S., Washington D.C.&nbsp;… <a href="#f+135737+1+6">Continue reading</a></span></span></p>
<blockquote><p>Some one revived a conclusion reached by one of the early Americans:<br>
<strong>“If you owe a banker $100 he’s got you. If you owe him $100,000 you’ve got him.”</strong></p></blockquote>
<p>In 1951 economist Paul Bareau employed the saying while delivering a speech at the Insurance Institute of London:<span><a role="button" tabindex="0" href="#f+135737+1+7"><sup id="footnote_plugin_tooltip_135737_1_7">[7]</sup></a><span><span id="r+135737+1+7"></span></span><span id="footnote_plugin_tooltip_text_135737_1_7">1953, Journal of the Insurance Institute of London, Volume 40, Session 1951-1952, The Place of Sterling in the Modern World by Paul Bareau (Economist, Asst. City Editor, News Chronicle), Description:&nbsp;… <a href="#f+135737+1+7">Continue reading</a></span></span></p>
<blockquote><p>To-day it is the customer, the debtor, who has the upper hand, although that in itself need not be a reason for immediate weakness. I always remember Keynes’s saying: <strong>“When you owe your bank manager £1,000 you are at his mercy. But if you owe him £1,000,000 he is at your mercy.”</strong></p></blockquote>
<p>In 1959 a newspaper in Salt Lake City, Utah published a variant involving the bank’s board of directors:<span><a role="button" tabindex="0" href="#f+135737+1+8"><sup id="footnote_plugin_tooltip_135737_1_8">[8]</sup></a><span><span id="r+135737+1+8"></span></span><span id="footnote_plugin_tooltip_text_135737_1_8"> 1959 August 14, The Salt Lake Tribune, Dan Valentine’s Nothing Serious, Quote Page B1, Column 2, Salt Lake City, Utah. (Newspapers_com) </span></span></p>
<blockquote><p>SAM, THE SAD CYNIC, SAYS:<br>
<strong>Owe a bank $1,000 an it’ll foreclose, owe it a million and it’ll elect you to the board of directors!</strong></p></blockquote>
<p>In 1963 the “Edwardsville Intelligencer” of Illinois printed another instance that did not mention precise amounts of currency:<span><a role="button" tabindex="0" href="#f+135737+1+9"><sup id="footnote_plugin_tooltip_135737_1_9">[9]</sup></a><span><span id="r+135737+1+9"></span></span><span id="footnote_plugin_tooltip_text_135737_1_9"> 1963 December 17, Edwardsville Intelligencer, Credits for the U.S.S.R., Quote Page 4, Column 2, Edwardsville, Illinois. (Newspapers_com) </span></span></p>
<blockquote><p>Keynes once said that <strong>if you owe your bank manager a modest debt you are in his power; if you owe a huge debt, he is in yours.</strong></p></blockquote>
<p>In 1975 a newspaper in Binghamton, New York printed another variant reminiscent of the saying in Mencken’s compendium:<span><a role="button" tabindex="0" href="#f+135737+1+10"><sup id="footnote_plugin_tooltip_135737_1_10">[10]</sup></a><span><span id="r+135737+1+10"></span></span><span id="footnote_plugin_tooltip_text_135737_1_10"> 1975 June 18, The Evening Press, N.Y. City Rescue Has Price by George F. Will, Quote Page 6A, Column 3, Binghamton, New York. (Newspapers_com) </span></span></p>
<blockquote><p>Beame was operating on this principle: <strong>If you owe the bank $100, the bank owns you; if you owe $1 million, you own the bank.</strong></p></blockquote>
<p>In 2000 the fanciful tabloid “Weekly World News” assigned the saying to business titan John Paul Getty:<span><a role="button" tabindex="0" href="#f+135737+1+11"><sup id="footnote_plugin_tooltip_135737_1_11">[11]</sup></a><span><span id="r+135737+1+11"></span></span><span id="footnote_plugin_tooltip_text_135737_1_11"> 2000 May 23, Weekly World News, Just For Laughs, Quote Page 5, Column 4, Weekly World News Inc., Lantana, Florida. (Google Books Full View) <a href="https://books.google.com/books?id=MPADAAAAMBAJ&amp;q=%22owe+the%22#v=snippet&amp;">link</a> </span></span></p>
<blockquote><p><strong>If you owe the bank $100, that’s your problem. If you owe the bank $100 million, that’s the bank’s problem.</strong> — John Paul Getty</p></blockquote>
<p>In conclusion, John Maynard Keynes helped to popularize this expression when he included it in a 1945 memo, but he disclaimed credit by calling it an “old saying”. The originator remains anonymous.</p>
<p>Image Notes: Illustration of money flying through the air from PublicDomainPictures at Pixabay. Image has been cropped and resized.</p>
<p>(Great thanks to George Mannes and Adam Rose whose inquiries led QI to formulate this question and perform this exploration. Both of them noted that the expression had been attributed to J. Paul Getty and John Maynard Keynes. Thanks to the volunteer editors of Wikiquote who presented the important 1945 citation in “The Collected Writings of John Maynard Keynes”. Thanks to Barry Popik for his <a href="https://www.barrypopik.com/index.php/new_york_city/entry/if_you_owe_a_bank_thousands_you_have_a_problem_if_you_owe_a_bank_millions_t">helpful research</a>. Many thanks to Rand Hartsell of the University of Illinois, Urbana-Champaign for precisely locating and verifying the citation in the “Journal of the Insurance Institute of London”.)</p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

				
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why we're helping more wikis move away from Fandom (920 pts)]]></title>
            <link>https://weirdgloop.org/blog/why-were-helping-more-wikis-move-away-from-fandom</link>
            <guid>41797719</guid>
            <pubDate>Thu, 10 Oct 2024 11:21:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://weirdgloop.org/blog/why-were-helping-more-wikis-move-away-from-fandom">https://weirdgloop.org/blog/why-were-helping-more-wikis-move-away-from-fandom</a>, See on <a href="https://news.ycombinator.com/item?id=41797719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Hi! You may have seen that Weird Gloop is now hosting the <a href="https://wiki.leagueoflegends.com/" target="_blank" rel="noopener noreferrer">official League of Legends Wiki</a>. We’ve spent the last couple months working with the Riot folks and the League wiki editors to move it off of Fandom, and turn it into something the players will (hopefully!) really dig. I also love that it got started because one of the Riot guys plays a ton of Old School RuneScape and thinks <a href="https://oldschool.runescape.wiki/" target="_blank" rel="noopener noreferrer">our wiki</a> is awesome. How cool is that??</p>
<p>I want this to kick off a new era where communities and developers take control from Fandom, and make some really great wikis. We’ve already been doing a bit of this, starting when we helped the <a href="https://minecraft.wiki/" target="_blank" rel="noopener noreferrer">Minecraft Wiki</a> leave Fandom, but I think it’s time for me (and the rest of our group) to be more explicit about what we want to do.
</p>
<p>So if you’re any of these things:</p>
<ul>
<li>A frustrated wiki editor trying to figure out your options</li>
<li>A community manager trying to get internal support for an official wiki</li>
<li>Someone contemplating making a new wiki</li>
</ul>
<p><strong>I will give you (free, very specific) advice on how to get your wiki off Fandom, and make a kickass wiki somewhere else</strong>. We might even be able to host you ourselves.</p>
<p>If you think this sounds cool, <a href="https://weirdgloop.org/contact">come talk to me</a>.</p>
<ul id="markdown-toc">
<li><a href="#why-do-we-actually-care" id="markdown-toc-why-do-we-actually-care">Why do we actually care?</a></li>
<li><a href="#why-ditching-fandom-is-cool-and-based" id="markdown-toc-why-ditching-fandom-is-cool-and-based">Why ditching Fandom is cool and based</a></li>
<li><a href="#what-im-offering" id="markdown-toc-what-im-offering">What I’m offering</a></li>
<li>
<a href="#how-to-not-turn-into-fandom-20-with-these-2-simple-tricks" id="markdown-toc-how-to-not-turn-into-fandom-20-with-these-2-simple-tricks">How to not turn into Fandom 2.0 (with these 2 simple tricks)</a> <ul>
<li><a href="#point-1---wiki-communities-need-to-be-able-to-freely-leave-their-host" id="markdown-toc-point-1---wiki-communities-need-to-be-able-to-freely-leave-their-host">Point 1 - wiki communities need to be able to freely leave their host</a></li>
<li><a href="#point-2---global-branding-is-extremely-negative-value-for-wiki-farms" id="markdown-toc-point-2---global-branding-is-extremely-negative-value-for-wiki-farms">Point 2 - global branding is extremely negative value for wiki farms</a></li>
</ul>
</li>
</ul>
<h2 id="why-do-we-actually-care">Why do we actually care?</h2>
<p><a href="https://archive.ph/kwt1b" target="_blank" rel="noopener noreferrer">This post</a> (and many others) have done a much better job than I could, explaining from a reader’s perspective why Fandom is bad place to host a wiki, but I thought it might be useful to give my take on it as a long-time wiki editor.</p>
<p>I love wikis. I think it’s unbelievably cool that this completely insane idea (“what if we just had a website that anyone can edit?”) doesn’t descend into anarchy, and instead self-organizes into a fun, project-oriented community. I think that despite its flaws, Wikipedia is the single coolest thing the internet has ever done. And wikis on niche topics feel like some of the last remnants of a friendlier, more collaborative, early 2000s web. I loved contributing to wikis, building something with other people, and feeling a sense of ownership (and pride that so many people were using stuff I made).</p>
<p>Which is why it’s so concerning that Fandom has taken this wonderful concept and turned it into one of the most dreadful parts of the internet. Being deeply involved with the RuneScape Wiki on Fandom had a huge psychological cost – what wonderful thing did they add today that made our wiki harder to use? Scammy green link ads? Comically bad videos on the top of our most popular pages? Garbage AI-generated Q&amp;A? Ads that take up literally 100% of the content window?</p>
<p>I (and so many others) had spent countless nights trying to make the best possible resource for RuneScape, and it was brutal to realize that it didn’t matter how hard we worked or creative we were – our wiki was never gonna be that great, because Fandom was in charge. That sense of ownership and pride…slowly turned into feeling like my passion was being exploited by a company that didn’t want the same things I did.</p>
<p>We weren’t the only ones feeling this way, of course – some wiki communities got fed up and moved somewhere independent. But here’s the key thing you need to understand: even when a wiki community unanimously wants to leave, Fandom keeps their copy of the wiki up, even though it no longer has a community. Google remembers years of people searching, linking, and visiting the Fandom wiki URLs, and continues to rank the increasingly stale Fandom results first. Since roughly 85% of a wiki’s traffic comes from Google, it’s nearly impossible for the new wiki to win without fixing this ranking disparity. It’s an extremely draining thing to do – nobody likes to spend their waking hours competing against the thing they helped lovingly build.</p>
<p>Historically, independent wikis have had an extremely hard time winning this battle. Most of the traffic stayed on the Fandom wiki, and the independent wikis often fizzled out. This had a chilling effect on the remaining communities, and emboldened Fandom to further prioritize revenue extraction.</p>
<p>That’s the key takeaway: <strong>if leaving Fandom was easy, they wouldn’t be able to enshittify as much as they have</strong>.</p>
<p>But don’t lose hope! Google has gotten much friendlier to independent wikis over the last decade. With a large, sustained effort, we were able to recover 95% of RuneScape Wiki traffic within the first year.</p>
<h2 id="why-ditching-fandom-is-cool-and-based">Why ditching Fandom is cool and based</h2>
<p>The main advantage of leaving Fandom is likely clear to anyone who’s ever visited one of their wikis without an ad blocker. But there’s more than that! When you have a site that people are happy to go to (instead of something they’re forced to grimace and use), you get all these wonderful secondary effects that are worth mentioning.</p>
<p>For starters: on average, moving away from Fandom doubles the number of people editing. I’ve seen the pattern across every wiki we’ve ever moved off Fandom, but here’s a pretty striking graph from OSRS Wiki:</p>
<p><img src="https://weirdgloop.org/images/posts/osrs-edit-count.png" alt="Graph of OSRS editors by editcount per month" width="650"></p>
<p>It’s incredibly consistent: way more people show up and want to help, when they feel like they’re contributing to something that isn’t taking advantage of them.</p>
<p>It’s not a coincidence that OSRS Wiki got really good once we left Fandom in 2018. Once we had way more people wanting to contribute (and the only objective was “make the best possible wiki for the game”) the wiki magically got way better! Crazy!</p>
<p>Departing from Fandom has also opened the door for a number of custom technical projects that otherwise would have been downright impossible to implement on the old wiki. <a href="https://oldschool.runescape.wiki/w/RuneScape:Lookup" target="_blank" rel="noopener noreferrer">In-game item lookup</a>, <a href="https://oldschool.runescape.wiki/w/RuneScape:WikiSync" target="_blank" rel="noopener noreferrer">WikiSync</a> and <a href="https://prices.runescape.wiki/osrs/" target="_blank" rel="noopener noreferrer">real-time prices</a> are core parts of our offering now, with hundreds of thousands of users. They’re all made possible by the new flexibility we gained when we took control of the hosting.</p>
<h2 id="what-im-offering">What I’m offering</h2>
<p>I think a lot of people would love to get their wiki off Fandom, but it’s extremely not obvious what that even involves, so it’s hard to formulate a plan. <strong>I will help you figure out a viable, detailed strategy for you to get your wiki off Fandom, and bring the traffic along</strong>.</p>
<p>In the next couple weeks, we’ll be posting some general advice on this blog that goes through the main steps and pitfalls involved with leaving Fandom. Most of it should be broadly applicable, but the real power comes from looking at the specifics of your topic (how big is it? does it change frequently? is it a game? are you the rights-holder?) and tailoring the plan to fit.</p>
<p>As far as where you host it…there’s plenty of decent options. Wiki hosting is not nearly as hard as Fandom makes it out to be – for example, if you’re the Path of Exile devs and you already host a bunch of PHP web stuff, then hosting the wiki yourself is objectively a really good option.</p>
<p>Sometimes Weird Gloop will be the good option for your situation, and being totally honest, sometimes it won’t be. And that’s okay! I want to help communities get away from Fandom, regardless of who’s running the servers.</p>
<p>I will say, I don’t think we would ever do a “self-service” thing where you could just sign up and immediately make a wiki. We want to do projects where we get to know the community, and closely support every wiki we host.</p>
<h2 id="how-to-not-turn-into-fandom-20-with-these-2-simple-tricks">How to not turn into Fandom 2.0 (with these 2 simple tricks)</h2>
<p>As we’ve started hosting more wikis besides RuneScape, some people have asked a pretty reasonable question: what’s stopping us from eventually getting enshittified, just like Fandom (or the other wiki farms that eventually sold to Fandom)?</p>
<p>From my perspective, there are two key choices that Fandom made that have had major negative consequences for communities. And we’re just going to do the exact opposite on both points.</p>
<h3 id="point-1---wiki-communities-need-to-be-able-to-freely-leave-their-host">Point 1 - wiki communities need to be able to freely leave their host</h3>
<p>You can probably tell that I think wiki editors (as opposed to hosts) are the ones who create the vast majority of the value on a wiki. So the premise is simple:</p>
<p><strong>If a wiki community is unhappy, and they have a better option somewhere else, they should be able to leave and take their stuff with them</strong>. We won’t prop up the old wiki, Weekend-at-Bernie’s style, abusing the dominant Google position that the wiki editors built up while they were on our platform.</p>
<p>In my opinion, <em>this is really the only rule that matters</em>. If you have the ability to leave (and take your revenue-driving wiki with you) when things go to shit, then your host has an extremely strong incentive to not let things completely go to shit.</p>
<p>There’s a long history of wiki farms vaguely handwaving that they’d agree to something like this, and then backtracking later. So why believe us?</p>
<p>It helps that Weird Gloop literally only exists because we were on the losing end of this sort of situation with Fandom back in 2018, and that we have no outside investors or debt (the company’s owned by wiki nerds)…but I don’t think that’s convincing enough on its own. So we’ve been voluntarily entering into agreements with the wikis we host (<a href="https://meta.minecraft.wiki/w/Memorandum_of_Understanding_with_Weird_Gloop" target="_blank" rel="noopener noreferrer">here’s an example</a>) where we set very clear obligations for what happens if the wiki community wants to go somewhere else (hint: it’s all about the domain). If we ever start going down the same path as Fandom, everyone can just leave! I would love to see other wiki platforms start to do this, because I think it’s the only way you really solve the problem.</p>
<h3 id="point-2---global-branding-is-extremely-negative-value-for-wiki-farms">Point 2 - global branding is extremely negative value for wiki farms</h3>
<p>If you go to any page on a Fandom wiki, even if you’ve got an ad blocker…you’ll be greeted by an absurd amount of Fandom-related branding: a gaudy sidebar that links to Fan Central (whatever that is), a bunch of other links to wikis that aren’t relevant to you, buttons to follow Fandom on Instagram, TikTok, to take “Fan Quizzes”. The brand strategy seems like it was cooked up by a bunch of market researchers who think that people are fans of…media properties in general? It’s super cringey and totally irrelevant to the people who are on Fandom wiki to, say, look up the stats of a new pickaxe they got.</p>
<p>It’s easy to laugh about how bad the branding and identity is, but there’s a bigger issue: the fact that it’s so overwhelmingly branded as “Fandom” (as opposed to, say, the Warframe Wiki) makes it way harder for each of the individual wikis to develop an public identity, because anything they do will be subordinate to the (very loud) global brand. These individual wikis are the only popular thing that Fandom has ever operated, <strong>and the focus on global branding makes each individual wiki worse</strong>.</p>
<p>Our position: the actual wikis should be front and center, because it’s way more important for the wiki itself to have a great reputation, rather than sucking all the oxygen out to make sure people know who owns the platform. We have extremely minimal branding (<a href="https://minecraft.wiki/" target="_blank" rel="noopener noreferrer">can you even find it?</a>), and I can’t imagine ever trying to put wikis on subdomains of weirdgloop.org (or anywhere else) unless there were no decent domain options. We don’t actually get anything out of everyday readers knowing who we are.</p>
<hr>
<p>That’s all I’ve got right now. If you liked this and want to talk to me about wiki things, please <a href="https://weirdgloop.org/contact">come say hi</a> – it doesn’t matter if you have a big wiki or a small wiki (or no wiki at all!) – I really just love talking to people about this stuff.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Let's talk about Animation Quality (215 pts)]]></title>
            <link>https://theorangeduck.com/page/animation-quality</link>
            <guid>41797462</guid>
            <pubDate>Thu, 10 Oct 2024 10:38:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theorangeduck.com/page/animation-quality">https://theorangeduck.com/page/animation-quality</a>, See on <a href="https://news.ycombinator.com/item?id=41797462">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        <hr>


<h3>02/10/2024</h3>

<p>Attending both CVPR and SIGGRAPH this year it was interesting to observe and discuss the difference between how vision and graphics researchers approach the field of animation. There was a sense from some researchers that the reviews from the graphics communities like SIGGRAPH could be too tough, and rejected papers based on petty irrelevant issues like shadow quality. While on the other side there were feelings that the vision community are willfully ignoring problems in their results while marching forward to develop new models. And I think both of these feelings really come to the forefront on the issue of animation quality, so I thought it might be a good opportunity to examine that subject a bit more.</p>

<hr>

<h2>High Quality Animation Data</h2>

<p>I think one of the best ways to start looking at animation quality is to look at some of the different sources of animation data in the community and compare them. For example, here is a video of some data from the <a href="https://github.com/simonalexanderson/MotoricaDanceDataset" target="blank">Motorica Dance Dataset</a> which is one of the highest quality publicly available datasets of motion capture in the graphics community:</p>



<p>Here is another animation, this time with the skeleton visualized.</p>



<p>This is the kind of data that is often typical in the AAA game and VFX industry. This data is sampled at 120 Hz, with finger and toe motions, captured with an optical system which can triangulate marker positions with sub-mm accuracy and solve onto a skeleton with realistic proportions and carefully positioned joints that accurately fit the actor. It's easy to see all of the detail and nuance that exists in data like this. The foot contacts are perfectly clean, there is no perceptual noise or foot sliding, and when skinned by a talented artist onto a well made character you get something that looks very natural and realistic with all sorts of subtle details.</p>

<p>When I look at this kind of animation data I see it as a target. Whatever animation systems we build should aim to generate animation of this quality. But I also see it as a kind of ceiling - because when we are evaluating our methodologies, the maximum quality of the data is a bit like the sensitivity of our measuring device - once the differences become indistinguishable we can't evaluate them reliably anymore.</p>

<p>So let's start by trying to work out some basic facts about this kind of data. What sort of precision is required when capturing, storing, or generating it?</p>

<hr>

<h2>Temporal Resolution</h2>

<p>I'll start with the temporal dimension... and I'll keep my analysis stupidly simple here... because if you talk to someone who actually knows about signal processing, they will tell you that almost everything we do when it comes to handling the temporal aspect of animation data is already wrong - from up-sampling to down-sampling and interpolation.</p>

<p>(For an accessible intro for graphics people as to what exactly is wrong I highly recommend <a href="https://bartwronski.com/2020/04/14/bilinear-texture-filtering-artifacts-alternatives-and-frequency-domain-analysis/" target="blank">Bart Wronski's blog</a> or the classic <a href="http://alvyray.com/Memos/CG/Microsoft/6_pixel.pdf">A Pixel Is Not A Little Square</a>. I'm also committing a bit of a sin here by drawing lines connecting the data-samples in my graphs... but all of that is a subject for another day.)</p>

<p>Anyway, if we take this motion from the dataset:</p>



<p>And plot a one second window of the shoulder rotation over time...</p>

<p>
<img src="https://theorangeduck.com/media/uploads/MotionQuality/rotation.png" alt="shoulder rotation">
</p>

<p>...we can see that even at 60 Hz the signal is not entirely smooth. Once we are down to 15 Hz we have a signal that is just obviously far too coarse to capture the nuances of the motion.</p> 

<p>If we plot the velocity of these signals (computed by finite difference) you can see we can get differences of over 90 degrees per second in the velocity we compute between the 60Hz signal and the 15Hz one.</p>

<p>
<img src="https://theorangeduck.com/media/uploads/MotionQuality/velocity.png" alt="shoulder rotation">
</p>

<p>Also take a second to note how quickly the angular velocities can change in this kind of data. Here we go from a velocity of around +150 degrees per second in one direction to -120 degrees per second in the opposite direction, within the space of 0.2 seconds. In fact, I would argue that even at 120 Hz, when we have these large peaks, the velocities are not really smooth at this sampling frequency:</p>

<p>
<img src="https://theorangeduck.com/media/uploads/MotionQuality/velocity120.png" alt="shoulder rotation">
</p>

<p>The differences in acceleration are therefore obviously even worse, and completely degraded for the 15 Hz signal:</p>

<p>
<img src="https://theorangeduck.com/media/uploads/MotionQuality/acceleration.png" alt="shoulder rotation">
</p>

<p>Here is an overlay of a clip of animation sampled at 60 Hz, and the same clip downsampled to 15 Hz (by taking every 4th frame) and then linearly upsampled again.</p>



<p>At normal playback speed, most of the time the visual difference is fairly minimal. However if we slow the playback down we can see issues: the temporal down-sampling has introduced errors of at least several centimeters in some places.</p>

<p>The fact is this: the reason we often deal with animation data sampled at 60 Hz is not because no higher frequency information exists and this is a sufficient frequency to record losslessly all the information in the data, but because the rate monitors refresh at is 60 Hz. As I described in my article on <a href="https://theorangeduck.com/page/cubic-interpolation-quaternions">cubic interpolation of quaternions</a> - as soon as we start to sample the data at other rates, unless we are careful, sampling frequency artefacts can re-appear very quickly.</p>

<p>And I think we can conclude that although we can sometimes get away with displaying animation at lower sampling frequencies than 60 Hz without much perceptual difference, if we really want to hit the motion quality shown above, and also in terms of storage and processing, 60 Hz is already kind of the minimum we should be aiming for if we want to preserve the temporal information in the data - and that if you are planning on doing any kind of real signal processing on your data (such as frequency transformations, low-pass filters, or even extraction of velocities, accelerations, or torques) you should try to aim for higher resolution if you can.</p>

<p>If you want a ballpark figure, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0021929022000902">this paper</a> provides a more detailed analysis and suggests that to preserve 90% accuracy for the acceleration information at least 180 Hz is required when it comes to human motion.</p>

<hr>

<h2>Numerical Precision</h2>

<p>So that is temporal resolution. What about in terms of the numerical precision of each frame?</p>

<p>Here is a quick experiment that shows how important this is too. This is what our animation data looks like if I round all the quaternion floating point values to three decimal places and then re-normalize before performing the forward kinematics step:</p>



<p>As you can see, with three decimal places of accuracy, the quantization error is pretty obvious (take a look at the head) and we see the character jumping discretely from pose to pose. So numerically we also need a pretty high amount of precision in our floating point values to represent each pose.</p>

<p>Here is another similar experiment. In this case I masked out the bottom 16 bits of the floating point mantissa.</p>



<p>The error is again pretty obvious. So to get perceptually accurate motion we probably need something like 10 of the 23 bits of the floating point mantissa to be correct.</p>

<p>Obviously a huge part of this is the error propagation that we get down the joint chain... but either way, it means if we have any kind of numerical system that processes, generates, or in any way touches the local joint rotations in animation data we should make sure it is numerically accurate to as many decimal places as possible. For example, 16-bit floats are probably a no-go for animation generation if we want any chance of achieving the quality of animation shown at the beginning of this article - they are only accurate to at most five decimal places in the zero to one range (they have a 10-bit mantissa) which leaves extremely little room for error.</p>

<p>Take a moment to consider how different this is to the image-generation domain where images are usually stored with just 8-bits of precision per channel. Not only does this mean they require much less precision as 8-bits means pixel channels can only take one of 256 values, but since humans are so much less sensitive to pixel color differences we don't even really need this - noise, quantization error, and other artefacts are often not even noticeable to the human eye anyway. As a ballpack figure I would say animation generation needs to be at least one hundred times more numerically accurate than image generation if we are dealing with local joint rotations.</p>

<p>We can do a simple experiment to show this. Here I add Gaussian noise to the quaternions of the local joint rotations with a standard deviation of 0.001 and then re-normalize. As you can see the motion it totally broken:</p>



<p>On the other hand, starting with this image...</p>

<p>
    <img src="https://theorangeduck.com/media/uploads/MotionQuality/tomatoes_original.png" alt="tomatoes original">
</p>

<p>Here is what I get if I add noise with a standard deviation of 0.01. The different is barely noticeable...</p>

<p>
    <img src="https://theorangeduck.com/media/uploads/MotionQuality/tomatoes_noisy.png" alt="tomatoes noisy">
</p>

<p>So to summarize: if we want to produce animation with the quality shown at the beginning of this article, 60 Hz is really the minimum frequency we should be dealing with when it comes to the temporal resolution, and we need numerical accuracy of at least five or six decimal places on our local joint rotations. Anything less than that and we can already expect to be producing visible errors.</p>

<hr>

<h2>Data Issues</h2>

<p>Now that we have an idea of what high quality motion capture data looks like and what the constraints are, let's take a look at some of the other data sets in the community.</p>

<p>First a brief disclaimer: my goal is not to be critical of these datasets. They have been incredible workhorses for the vision and graphics communities and I really appreciate all the work that has gone into them from so many people. I've used them extensively in my own papers! My goal here is simply to talk about some of the issues you might face when trying to use them as a researcher.</p>

<p>For example, let's look at some data from the <a href="https://moyo.is.tue.mpg.de/index.html">MOYO</a> yoga part of the <a href="https://amass.is.tue.mpg.de/">AMASS</a> dataset. Here I have extracted the skeleton via the regression method. I'm going to put it on a skinned character too, but first let's look at just the skeletal data itself:</p>

<p>We'll start with the obvious problems and the first one is right there - the rest pose:</p>



<p>I'm unconvinced that the actor had their knees bent outward like that when they were doing a T-pose. That means we already have some kind of constant systematic error in our data when it comes to joint rotations and positioning. Not only is that going to make it difficult to skin and retarget this data but having such a large systematic error right from the start doesn't give us much confidence regarding what else might have gone wrong with the capture and processing.</p>

<p>The next most obvious issue is that the character's feet penetrate the floor. This fix is more simple - we just need to vertically translated the character in all the data. No big deal - but it does mean we can't just naively throw this data into a network along with a bunch of other data without manual correction and cleaning.</p>

<p>Third is that the data is missing hand, finger, and toe movements - although that is pretty common for most publicly available datasets.</p>

<p>If we browse a few of the clips in this dataset we can see there are many places where we have glitches and jitters:</p>



<p>All of these sections should be trimmed from the database before training. These kind of glitches and outliers can be really damaging to training of a neural network and often causing exploding gradients and NaNs to appear in losses. Techniques like gradient clipping often hide/handle these kinds of data issues but even with that, there can be big negative affects on things such as normalization.</p>

<p>Lots of the clips also have more subtle issues like this kind of low level noise that is very hard to spot (take a look at the spine about 5 seconds in):</p>



<p>We can remove this noise by filtering the data, but as discussed earlier, at a sampling rate of 60 Hz we cannot perfectly capture the fastest component of human motion. This means that there is important motion information encoded in the higher frequencies of our signal. Applying any low-pass filter for denoising would also inevitably erase parts of the actual signal.</p>

<p>
<img src="https://theorangeduck.com/media/uploads/MotionQuality/rotation_noise.png" alt="rotation noise">
</p>

<p>For example, if I apply this same Gaussian filter to the Motorica data we can see it can end up modifying the data by several centimeters in places and removes some subtle details. Here look at how the hand motion gets smoothed out:</p>



<p>I've done my best to retarget a medium-sized AMASS skeleton to the UE5 Manny so we can see some of the issues on the same skinned character. This is obviously not a perfect comparison, but nonetheless on a skinned character a whole new domain of artefacts become visible. Obviously we have missing finger and toe motion, but we also can see some joints don't twist properly, sometimes the spine pops in a weird way, we get odd shoulder motion, or the feet slide a lot.</p>



<p>Large parts of the <a href="http://mocap.cs.cmu.edu/">CMU motion capture database</a> (which AMASS contains) and many of the other large databases we use in graphics contain similar issues. Here are a few random clips from the CMU portion of AMASS where we can see all sorts of artefacts. Again, this is not an exactly fair comparison since I struggled to build a retargeting setup which worked well given the variety of rest poses in the data - and of course I have cherry picked some bad examples - but it still gives you an idea of the data.</p>



<p>As I mentioned before - I really appreciate the work that has gone into these kinds of datasets which have been powering the academic community for a long time and led to a huge amount of progress. But these datasets also come with some downsides:</p>

<p>First, data of this quality is simply too far off being practically usable in the video games or VFX industry. This immediately makes it very hard to evaluate fairly the practicality of the proposed method since we simply don't know how it might behave when trained on higher quality data.</p>

<p>Second, when examining the animation generated by a system trained on this kind of data it becomes impossible to distinguish between animation artifacts caused by the method itself, and those which are present in the training data and the system is just faithfully reproducing.</p>

<p>Third, when systems trained on this kind of data report error metrics such as joint position improvements of a few centimeters or small improvements to the FID score, given we know the ground truth data has obvious systematic errors on the order of tens of centimeters as well as other glitches and noise, it brings into question if these kind of metric differences are even meaningful any longer.</p>

<hr>

<h2>Mixed Data</h2>

<p>An argument I've heard often is this: Does it really matter if the quality is variable when there is so much <em>more</em> data in databases like AMASS than there are in other datasets? Can't I train a model on a large dataset of low quality motion and then fine-tune it on the high quality data? The mantra we hear a lot in deep learning for this kind of training regime is "Noise cancels out, signal adds up" - which is absolutely true.</p>

<p>I think this is a very interesting argument, and it's an approach which has worked fantastically in other domains (Speech Synthesis being one example). There are a lot of very smart people in the field betting on this kind of idea and I will be interested to see what kind of results it produces.</p>

<p>While I still don't really know if this will work, my gut feels skeptical. In my experience most of the time when you combine two animation datasets, you end up with a multi-modal dataset made up of two clusters that don't necessarily interact properly with each other.</p>

<p>In fact, we can somewhat see this already, because AMASS is a collection of many different datasets and capture sessions. If we look at the rest pose from part of the CMU portion of the AMASS dataset:</p>



<p>And then look at the rest pose from the Yoga portion:</p>



<p>We can see already just by eyeballing that depending on which session the data comes from the whole rest pose is completely different. If we train a model on this data it is going to need to learn how to switch between different styles of rest pose depending on what kind of motion we are asking it to produce as output. These datasets might be on the same skeleton structure but that does not mean they are "combined".</p>

<p>To me, the idea that taking a system trained on low quality dancing data (for example), training it on high quality locomotion data, and expecting it to produce high quality dancing data (and not low quality dancing data) seems like wishful thinking. To put it bluntly, the common mantra which much more reflects my experience with deep learning is this one: "garbage in, garbage out".</p>

<p>As mentioned at the beginning of the article, I like to think of the training data as your ceiling. It represents the best possible quality you can expect to produce from your model, which in most cases you will only be able to approach exponentially. That is why targeting the highest quality source data and preserving that quality through processing steps is so important.</p>

<p>In fact, I often find that as soon as you put animation data into a neural network in any form you can expect some loss in quality. We can see this by training a very simple auto-encoder on the high quality data from the Motorica dataset we showed before:</p>

<p>(In red is the ground truth and in green is the reproduction.)</p>



<p>Even after some careful training which accounts for errors propagating down the joint chain, we still get some error in the autoencoder's reconstructions on the training set which manifest in small amounts of foot sliding and other posing errors.</p>

<p>In this case I used 8 layers of 512 neurons for both the encoder and decoder and a 256-dimensional latent space. Perhaps my network was just not large enough? More layers generally equals better right? Well in this case the opposite is true. In fact, we <em>can</em> get almost perfect reproductions if we just use 2 layers instead of 8:</p>



<p>So not only can quality degrade as soon as we enter any kind of neural network, but it seems that the more layers that data passes through the worse things are.</p>

<p>All of these sources of error accumulate and propagate and obfuscate our ability to evaluate methods and architectural differences objectively and fairly. Methods that produce foot sliding on top of data (or other methods) that already have foot sliding, <em>hide</em> the foot sliding they introduce, and make the sources of error infinitely more difficult to decouple and attribute to specific methodological decisions.</p>

<p>Also, the models we train end up wasting their capacity learning to emulate the errors, noise, multiple-modes, artefacts, and whatever other undesirable things might be in the data. For example, here is the same 2 layer auto-encoder trained on of our AMASS yoga data:</p>



<p>Now all of the care we took to retain precision is working against us because we're attempting to reproduce the errors and noise. If we're not careful you can see how easy it would be to come to the wrong conclusion about model architectures. Even with this basic example we're in a situation where with good data quality we might conclude "more layers = worse", while with poor data quality "more layers = better" (as, for example, it might remove the noise more).</p>

<p>It might seem like I am nit-picking on tiny issues, and that errors of a couple of centimeters here are there are really just not a big deal compared to the new opportunities that Machine Learning opens up... but a lesson I think we can learn from the rendering community is that hacks, approximations, and other sources of errors might not seem like a big deal - until suddenly they are - because at some point you end up with a final result that looks like a hot mess - and you have no idea what to do - because you have hundreds of different sources of errors, none of which you really understand or can control - compounding together in a deeply complex way. That is the situation the rendering folks have been untying over the last 15 years.</p>

<hr>

<h2>Before Deep Learning</h2>

<p>When talking about animation quality it's a good exercise to go back and look at SIGGRAPH papers from 10 years ago before deep learning started to take off. Here is a selection of my favorites:</p>

<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/G_bLwfzYqF4?si=8ispuSq0TSr-m_M6&amp;start=111" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p>

<p><a href="https://grail.cs.washington.edu/projects/motion-fields/">Motion Fields for Interactive Character Animation (2010)</a></p>

<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/rd03xOukcsw?si=8ZhzfydqFcZzULs5&amp;start=161" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p>

<p><a href="https://www.cs.ubc.ca/~van/papers/2012-TOG-TerrainRunner/index.html">Terrain Runner: Control, Parameterization, Composition, and Planning for Highly Dynamic Motions (2013)</a></p>

<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/hr3pdDl5IAg?si=gYFULf92ZS1fLoHE&amp;start=126" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p>

<p><a href="https://graphics.stanford.edu/projects/ccclde/">Continuous Character Control with Low-Dimensional Embeddings (2012)</a></p>



<p><a href="https://people.computing.clemson.edu/~sjoerg/mocap.html">Data-driven Finger Motion Synthesis for Gesturing Characters (2012)</a></p>

<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/6rRpKF0pTwI?si=cIn0gjwsLBOViL1e&amp;start=72" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p>

<p><a href="https://web.archive.org/web/20161021195120id_/http://humanmotion.ict.ac.cn:80/papers/2015P1_StyleTransfer/Realtime%20Style%20Transfer%20for%20Unlabeled%20Heterogeneous%20Human%20Motion.pdf">Realtime Style Transfer for Unlabeled Heterogeneous Human Motion (2015)</a></p>

<p>Although all these methods have issues (and there were lots of papers from this time which did not look nearly as good), in many ways, in terms of pure animation quality what they produced is better than some of the results from deep learning methods we see now. In the very least, the animation they produce is almost always clean and artefact free: it does not have noise, or jumps and jitters, and in general it does not even contain much foot sliding.</p>

<p>If our newer methods cannot attain this quality bar then the elephant in the room is that we may not actually be making progress in the field - we might just be trading off one thing for another.</p>

<blockquote><strong>So let's talk more about animation quality - let's work out how to tackle it and not try push it under the rug because it is an inconvenient topic when it comes to publishing animation papers that use deep learning.</strong></blockquote>

<hr>

<h2>Improving Animation Quality</h2>

<p>So what can we do to try and improve animation quality across the board?</p>

<p>I think those of us in the industry have some responsibility here. Almost all games and VFX companies don't share their data with the academic world (with the exception of Ubisoft who have released some very high quality animation datasets), so how can we penalize academics for not reaching a bar they don't even know exists? We need to make more of an effort to share what we can, not just our data, but also our processes and considerations when we capture it.</p>

<p>And because I believe in putting your money where your mouth is I've done a few things in preparation for this blog post that I hope will help:</p>

<!--

<p>1) I've skinned a simple character (to the best of my limited abilities) to the skeleton used in the <a href="https://github.com/ubisoft/ubisoft-laforge-ZeroEGGS">ZeroEGGS</a> dataset to provide a clean and clear way to evaluate overall motion quality including fingers, toes, hand and feet contacts.</p>

<p style='text-align:center;'>
 <video autoplay loop muted width="640" height="360">
  <source src="/media/uploads/MotionQuality/geno_wireframe.m4v" type="video/mp4">
</video>
</p>

<p>2) I've spent some time re-solving in MotionBuilder the <a href="https://github.com/ubisoft/ubisoft-laforge-animation-dataset/tree/master/lafan1">LaFAN dataset</a> from the original marker data and preparing a new version which is better quality and closer to what we used internally: it has toe and finger motion (at least one marker's worth of finger motion), is more carefully retargeted to this new skeleton, and is solved at 60fps instead of 30fps. Given I'm not a MotionBuilder expert, and the limited finger markers in the original data, the result is still far from perfect - but I hope it can provide a reasonable baseline for researchers given the size and diversity of the dataset. You can find it <a href="#">here</a>.</p>

<p style='text-align:center;'>
 <video autoplay loop muted width="640" height="360">
  <source src="/media/uploads/MotionQuality/geno_lafan.m4v" type="video/mp4">
</video>
</p>

<p>3) Epic Games have agreed to release the animation data from their recent <a href="https://dev.epicgames.com/documentation/en-us/unreal-engine/game-animation-sample-project-in-unreal-engine">Gameplay Animation Sample</a> under a non-commercial research license. This is a production quality locomotion dataset of X hours of motion that was lovingly prepared and cleaned, and was used in Fortnite's Motion Matching release. Even if it only contains locomotion, it's still probably the highest quality large animation dataset I have seen that is publicly available for research. So if you have a method which can produce animation data of this quality, I don't think anyone can argue with you that it is not production quality. This dataset I have also retargeted to the same skeleton so that it can be combined easily with data from the LaFAN re-solve. You can find it <a href="#">here</a>.</p>

<p style='text-align:center;'>
 <video autoplay loop muted width="640" height="360">
  <source src="/media/uploads/MotionQuality/geno_gas.m4v" type="video/mp4">
</video>
</p>

<p>4) I've retargeted the Motorica Dance Dataset to this skeleton in MotionBuilder so that it can be combined with the previous two datasets. You can find it <a href="#">here</a>.</p>

<p style='text-align:center;'>
 <video autoplay loop muted width="640" height="360">
  <source src="/media/uploads/MotionQuality/geno_motorica.m4v" type="video/mp4">
</video>
</p>

<p>5) I've also done a basic adjustment of the ZeroEGGS dataset so that it looks correct on this new simple character. You can find it <a href="#">here</a>.</p>

<p style='text-align:center;'>
 <video autoplay loop muted width="640" height="360">
  <source src="/media/uploads/MotionQuality/geno_zeroeggs.m4v" type="video/mp4">
</video>
</p>

<p>That gives us at least four large datasets all on a common character skeleton with fingers, toes, and a skinned mesh, covering Locomotion, Jumping, Climbing, Vaulting, Dance, Speech, Get-ups, Falls, and everything else in LaFAN. I hope that can be considered a pretty good starting point for anyone who is interested in improving the motion quality in their animation research.</p>

-->

<p>1) I've skinned a simple character (to the best of my limited abilities) to the skeleton used in Ubisoft's <a href="https://github.com/ubisoft/ubisoft-laforge-ZeroEGGS">ZeroEGGS</a> dataset to provide a clean and clear way to evaluate overall motion quality including fingers, toes, hand and feet contacts.</p>



<p>2) I've spent some time re-solving in MotionBuilder Ubisoft's <a href="https://github.com/ubisoft/ubisoft-laforge-animation-dataset/tree/master/lafan1">LaFAN dataset</a> from the original marker data and preparing a new version which is better quality and closer to what we used internally: it has toe and finger motion (at least one marker's worth of finger motion), is more carefully retargeted to this new skeleton, and is solved at 60fps instead of 30fps. Given I'm not a MotionBuilder expert, and the limited finger markers in the original data, the result is still far from perfect - but I hope it can provide a reasonable baseline for researchers given the size and diversity of the dataset. You can find it <a href="https://github.com/orangeduck/lafan1-resolved">here</a>.</p>



<p>3) I've retargeted the Motorica Dance Dataset to this skeleton in MotionBuilder so that it can be combined with the previous two datasets. You can find it <a href="https://github.com/orangeduck/motorica-retarget">here</a>.</p>



<p>4) I've also done a basic adjustment of the ZeroEGGS dataset so that it looks correct on this new simple character. You can find it <a href="https://github.com/orangeduck/zeroeggs-retarget">here</a>.</p>



<p>That gives us at least three large datasets all on a common character skeleton with fingers, toes, and a skinned mesh, covering Dance, Speech, and everything in LaFAN. I hope that can be considered a pretty good starting point for anyone who is interested in improving the motion quality in their animation research.</p>

<p>Finally, if anyone else releases simliar quality datasets I promise I will also make a good attempt to retarget it to the same setup.</p>

<p>With that said, I think there are things to be done in the academic world too. We need to strike a balance between rewarding risk-taking, innovation, and unique ideas - but we also need to make sure we don't lower the bar to the point where our results become irrelevant to the industry or we lose the ability to evaluate what works and what doesn't. Most importantly: let us not avoid the discussion of animation quality just because it may be inconvenient for publishing.</p>

<blockquote><em>Friendly Reminder: All of these datasets are NOT licensed for commercial use. They are for research use only. The quickest way to ruin any good-will from the industry in terms of releasing data is to use this data (or models trained on this data) in a commercial product without acquiring a commercial license from the original owners of the datasets.</em></blockquote>

<hr>

<h2>Conclusion</h2>

<p>I'd like to end on something more philosophical.</p>

<p>There has been a huge amount of change in the tech world over the last 10 years and almost all companies (including the one I work for) are shifting their focus away from being content creators, to being storefront owners and algorithmic content curators. The new business model is this: instead of employing the best artists and creators you can find to make content, you instead pay your users to create the content (be that apps, games, videos, recipes, whatever), and that content is then evaluated and filtered algorithmically for other users - not based on some subjective evaluation of quality - but based on how much it engagement it generates.</p>

<p>The automated, engagement based nature of this process has created a race to the bottom. YouTube users are creating hundreds of <a href="https://en.wikipedia.org/wiki/Elsagate">brain-hacking Spiderman-Elsa videos</a> to farm views from undeveloped minds. Game developers are <a href="https://www.youtube.com/watch?v=E8Lhqri8tZk">auto-generating thousands of gambling games</a> for the Apple app store. Everything is being flooded with low-quality, engagement-hacking crap. The people who are actually trying to build quality content are being forced to sink or swim - optimize for engagement or else be forgotten.</p>

<p>There are many people involved in deep learning who are trying very hard to sell you the idea that in this new world of big-data, recommendation algorithms, large language models, and deep learning, quantity is the only thing that matters: more data, more GPUs, more papers, more students, more layers, more neurons, more users, more hours of engagement. Take a second to consider why it might be in the interests of the people shouting this the loudest, to convince everyone it is really true.</p>

<p>But ask almost anyone what they actually want in this world and they will say the opposite. They want less content, of higher quality.</p>

<p>And if that is what all of us want, then that is what we need to try and do: focus on producing quality content ourselves, and reward others when they do so too.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nixiesearch: Running Lucene over S3, and why we're building a new search engine (112 pts)]]></title>
            <link>https://nixiesearch.substack.com/p/nixiesearch-running-lucene-over-s3</link>
            <guid>41797041</guid>
            <pubDate>Thu, 10 Oct 2024 09:11:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nixiesearch.substack.com/p/nixiesearch-running-lucene-over-s3">https://nixiesearch.substack.com/p/nixiesearch-running-lucene-over-s3</a>, See on <a href="https://news.ycombinator.com/item?id=41797041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><blockquote><p><a href="https://haystackconf.com/eu2024/" rel="nofollow ugc noopener">This is a long-read version of a Haystack EU24 conference talk by Roman Grebennikov. Check out the slides or watch the video [TODO] if you’re a Gen Z.</a></p></blockquote><p>If you’re running a large search engine deployment, you already have a personal list of things that can go wrong on a daily basis.&nbsp;</p><p>In this post we are going to:</p><ul><li><p>Complain about existing search engines ops complexity. I’m looking at you, Elasticsearch and SOLR.</p></li><li><p>Hypothesize about running Lucene-powered search in a stateless mode over S3 block store. Why do you even need a stateless search over a block store?</p></li><li><p>Introduce Nixiesearch, a new stateless search engine, and how we struggled to make it work nicely with S3. And how it ended with RAG, ML inference and hybrid search.</p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png" width="793" height="752" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:752,&quot;width&quot;:793,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>A prize wheel search engineers spin every day. Image by author.</figcaption></figure></div><p>Unlike regular back-end applications, search engines nowadays are considered special and require additional “like a database” handling. The prize wheel above summarises author’s personal incident experience with Elasticsearch, OpenSearch and SOLR — but other modern vector search engines such as Weaviate and Qdrant are not immune.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png" width="1456" height="820" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:820,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Curse of stateful, blessing of stateless apps. Image by author.</figcaption></figure></div><p><span>Each of your search nodes is stateful and contains a tiny part of a distributed shared mutable index. And if you’ve ever read at least one book on systems design and engineering (such as the </span><a href="https://dataintensive.net/" rel="nofollow ugc noopener">“Designing Data Intensive Applications” by M. Kleppmann</a><span>), you’ll perfectly know that things aren’t going to be smooth sailing when there’s a “</span><strong>distributed shared mutable</strong><span>” in the name:</span></p><ul><li><p><strong>Each node depends on an attached storage</strong><span> — which makes cloud deployments much more fragile with EBS disks, PersistentVolume and PersistentVolumeClaims over DaemonSets.</span></p></li><li><p><span>To perform </span><strong>auto-scaling you need to redistribute state across nodes</strong><span>. Since scaling is usually triggered when the cluster is already under load, and the redistribution process also adds additional load — we usually just over-provision instead.</span></p></li></ul><p>Worse still, this internal state also spawns outside of the search engine itself, as the entire indexing pipeline is also tied to it.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png" width="1456" height="717" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d3480bd1-799e-4912-b409-32f3874d9443_1600x788.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:717,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Indexing pipeline as a Rube Goldberg Machine. Image by author.</figcaption></figure></div><p>The idea of stateless search engines is not new, as they already exist in the wild:</p><ul><li><p><a href="https://github.com/Yelp/nrtsearch" rel="nofollow ugc noopener">Yelp NRTSearch</a><span>, built on top of </span><a href="https://blog.mikemccandless.com/2017/09/lucenes-near-real-time-segment-index.html" rel="nofollow ugc noopener">Lucene near-real-time segment replication</a><span>. Works over gRPC and supports only lexical search.</span></p></li><li><p><a href="https://opensearch.org/" rel="nofollow ugc noopener">OpenSearch</a><span> when configured for </span><a href="https://opensearch.org/docs/latest/tuning-your-cluster/availability-and-recovery/segment-replication/index/" rel="nofollow ugc noopener">segment replication</a><span> over </span><a href="https://opensearch.org/docs/latest/tuning-your-cluster/availability-and-recovery/remote-store/index/" rel="nofollow ugc noopener">remote-backed storage</a><span>. With this mode of operation, index is stored externally, but the cluster state is not.</span></p></li><li><p><a href="https://quickwit.io/" rel="nofollow ugc noopener">Quickwit</a><span> for log search over S3 storage. A great alternative to the ELK stack, but not for consumer-facing search.</span></p></li><li><p><a href="https://turbopuffer.com/" rel="nofollow ugc noopener">Turbopuffer</a><span> doing HNSW-like vector search over S3-based block storage. SaaS only, but written in Rust.</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png" width="956" height="358" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dfad2822-1e58-4240-b71b-a04b606cb439_956x358.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:358,&quot;width&quot;:956,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Choosing between easy operations and costs. Image by author.</figcaption></figure></div><p>The main point of these search engines is to decouple computation (e.g. the actual search process) from the storage. There are also examples in the industry of large companies migrating to stateless search:</p><ul><li><p><a href="https://www.uber.com/en-NL/blog/lucene-version-upgrade/" rel="nofollow ugc noopener">Lucene: Uber’s Search Platform Version Upgrade.</a></p></li><li><p><a href="https://careers.doordash.com/blog/introducing-doordashs-in-house-search-engine/" rel="nofollow ugc noopener">Introducing DoorDash’s in-house search engine</a><span>.</span></p></li><li><p><span>Amazon: </span><a href="https://www.youtube.com/watch?v=EkkzSLstSAE" rel="nofollow ugc noopener">E-Commerce search at scale on Apache Lucene.</a></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif" width="1456" height="511" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:511,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>A DoorDash search engine indexing flow. Image taken from </span><a href="https://careers.doordash.com/blog/introducing-doordashs-in-house-search-engine/" rel="nofollow ugc noopener">[1]</a><span>.</span></figcaption></figure></div><p>With all these great in-house search engines being not available for general audience, what should we do?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png" width="1235" height="1235" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1235,&quot;width&quot;:1235,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Writing a search engine from scratch in 2024 might be considered risky. Image by author.</figcaption></figure></div><p><a href="https://github.com/nixiesearch/nixiesearch" rel="nofollow ugc noopener">Nixiesearch</a><span> is a Lucene-based search stateless search engine, inspired by the Uber, Doordash and Amazon designs from the articles above:</span></p><ul><li><p>Started as a proof-of-concept that Lucene could be run over the S3-compatible block storage.</p></li><li><p>Went further with many modern features implemented on top: hybrid search, RAG and ML inference.</p></li></ul><p><span>There have been a number of experiments back in the days to add S3 support to Lucene, starting from 15 years ago by</span><a href="https://www.elastic.co/de/blog/author/shay-banon" rel="nofollow ugc noopener"> Shay Banon</a><span> (a creator of Elasticsearch, and now a CEO of Elastic):</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png" width="1220" height="742" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:742,&quot;width&quot;:1220,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Lucene over AWS S3 in 2007. Screenshot taken from kimchy.github.com.</figcaption></figure></div><p>Lucene is has an IO abstraction called Directory — a high-level API to support almost any data access method:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png" width="795" height="319" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:319,&quot;width&quot;:795,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:55974,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Simplified Lucene Directory interface.</figcaption></figure></div><p><span>There are many Directory implementations available ranging from practical ones like </span><a href="https://lucene.apache.org/core/9_12_0/core/org/apache/lucene/store/MMapDirectory.html" rel="nofollow ugc noopener">MMapDirectory</a><span> and </span><a href="https://lucene.apache.org/core/9_12_0/core/org/apache/lucene/store/ByteBuffersDirectory.html" rel="nofollow ugc noopener">ByteBuffersDirectory</a><span>, to the much more experimental and obscure </span><a href="https://github.com/unkascrack/lucene-jdbcdirectory" rel="nofollow ugc noopener">JDBCDirectory</a><span>. And the Directory API maps quite nicely to the S3 API operations, so it’s no surprise that there’s already one brave soul who’s made the </span><a href="https://github.com/albogdano/lucene-s3directory" rel="nofollow ugc noopener">S3Directory</a><span>:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png" width="1242" height="805" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:805,&quot;width&quot;:1242,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Experimental S3 Directory. A screenshot from </span><a href="https://github.com/albogdano/lucene-s3directory" rel="nofollow ugc noopener">[2]</a><span>.</span></figcaption></figure></div><p>Lucene itself is a synchronous library built on the main assumption that your index data is stored locally, but this is no longer the case with S3Directory.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png" width="1456" height="723" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:723,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>S3 access latency is almost 100x bigger. Image by author.</figcaption></figure></div><p><span>The S3Directory itself is implemented by naively mapping Directory calls directly to the S3 API without any intermediate caching, and its </span><a href="https://github.com/albogdano/lucene-s3directory?tab=readme-ov-file#performance" rel="nofollow ugc noopener">README</a><span> mentions that performance is not great:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png" width="866" height="271" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:271,&quot;width&quot;:866,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Integration test time for S3Directory. A screenshot from </span><a href="https://github.com/albogdano/lucene-s3directory" rel="nofollow ugc noopener">[2]</a><span>.</span></figcaption></figure></div><p>With 100x higher data access latency, it’s not a surprise to see 100x slower end-to-end times.&nbsp;</p><p>An obvious improvement to this approach is to introduce the read-through caching support — that’s what we did in the first version of Nixiesearch.</p><p>The main assumption behind the caching idea is that the Lucene index may have hot spots, and there is no reason to make an S3 API call for index blocks we have already read in the past:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png" width="1456" height="656" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/af33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:656,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Cached S3 Directory. Image by author.</figcaption></figure></div><p><span>With this approach, the S3Directory is implemented just as an ephemeral caching layer on top of the regular MMapDirectory, with the main benefit of making the whole </span><strong>search application stateless</strong><span>! But what’s the latency cost?</span></p><p>How slow is S3 in practice?</p><p>Today, S3 has become a protocol, and not just an AWS service, but being implemented by GCP, Minio and many otherproviders. The AWS S3 reference implementation has two API flavors:</p><ul><li><p>Traditional S3: geographically redundant, with higher latency.</p></li><li><p><a href="https://aws.amazon.com/s3/storage-classes/express-one-zone/" rel="nofollow ugc noopener">S3 Express</a><span>: limited to a single AZ, but with much lower latency.</span></p></li></ul><p><span>We implemented a small </span><a href="https://github.com/nixiesearch/s3bench/" rel="nofollow ugc noopener">nixiesearch/s3bench</a><span> tool to measure a first byte latency for </span><a href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html" rel="nofollow ugc noopener">S3 GetObject API</a><span> request across different endpoints:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png" width="1456" height="466" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:466,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>S3 vs S3 express first byte latency. Image by author.</figcaption></figure></div><p>The main observation we made is that the first byte latency of S3 Express is actually only 5ms instead of 20ms for a traditional S3. And this latency is independent of block size.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png" width="692" height="427" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:427,&quot;width&quot;:692,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Last byte latency for different block sizes. Image by author.</figcaption></figure></div><p><span>For the last byte latency both curves for S3 and S3Express are almost parallel, so we can assume that the only difference between these two implementations is in much lower constant value of initial request latency, due </span><a href="https://quickwit.io/blog/s3-express-speculations" rel="nofollow ugc noopener">to S3Express using pre-signed access tokens</a><span> — so there is no need to perform a full IAM role evaluation on each request.</span></p><p>Synthetic micro-benchmark latencies are nice, but what are the real-life numbers we get when searching through a read-through block cache for S3Directory? Let’s do an experiment:</p><ul><li><p><span>Take a well-known dataset like </span><a href="https://huggingface.co/datasets/BeIR/msmarco" rel="nofollow ugc noopener">MSMARCO</a><span>, with sample sizes of 10k, 100k and 1M documents.</span></p></li><li><p><span>Implement a traditional </span><a href="https://lucene.apache.org/core/9_2_0/core/org/apache/lucene/search/KnnVectorQuery.html" rel="nofollow ugc noopener">HNSW “vector search”</a><span> index on top of S3-backed Lucene. We will use a </span><a href="https://huggingface.co/intfloat/e5-base-v2" rel="nofollow ugc noopener">Microsoft e5-base-v2</a><span> embedding model with 768 dimensions, and a single-segment index.</span></p></li><li><p><span>We will use the default Lucene HNSW index settings of </span><strong>M=16 (so the number of neighbors for each node in the graph — this number is going to hit us hard later)</strong><span> and efConstruction=100.</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png" width="532" height="494" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:494,&quot;width&quot;:532,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>First cold request latency for 3 index sizes over S3-backed Lucene. Image by author.</figcaption></figure></div><p>There are a lot of numbers in the table above, but here are two most surprising facts:</p><ul><li><p><span>For a small 10k docs dataset, the first request </span><strong>fetched almost 30% of the entire index</strong><span> while traversing the HNSW graph.</span></p></li><li><p><span>For a larger 1M docs index, we performed over 300 read operations to S3, resulting </span><strong>in a whopping 3 seconds of latency</strong><span>.</span></p></li></ul><p>What is going on here, why do we need so many reads to perform a simple nearest neighbors search?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png" width="1456" height="989" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:989,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>A simplified HNSW graph traversal. Blue = neighbors, Red = jumps. Image by author.</figcaption></figure></div><p><span>Because Lucene uses an </span><a href="https://en.wikipedia.org/wiki/Hierarchical_navigable_small_world" rel="nofollow ugc noopener">HNSW algorightm</a><span> to perform a nearest neighbors search, it needs to traverse a large multi-layer graph:</span></p><ul><li><p><span>Each layer of the HNSW graph contains </span><strong>links to neighbor nodes</strong><span>. Top layers have only long links, and bottom layers have only short links.</span></p></li><li><p><span>At each step of the graph traversal, we </span><strong>pick a node and evaluate a distance between a query and a neighbor </strong><span>— so we need to load this neighbor embedding from the storage.</span></p></li><li><p><span>Each such evaluation is effectively a </span><strong>random cold read from S3</strong><span>, adding yet another +10ms to the request processing latency.</span></p></li></ul><p>You may be wondering, “OK, but this is a first cold request, will it get better over time?”&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png" width="1456" height="466" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:466,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>e2e latency for request #1-#32, and index read rate. Image by author.</figcaption></figure></div><p>Unfortunately we found out that the read distribution over the HNSW index still stays quite random, and the initial hypothesis that there are index hotspots is not true:</p><ul><li><p><span>e2e latency goes down from 3 sec to </span><strong>1 sec on request #32</strong><span>. Things are getting better, but 1 second search latency is still not acceptable for a consumer-facing search.</span></p></li><li><p>On request #32 we are already reading 30% of the entire index.</p></li></ul><p>So the idea of doing HNSW search over S3-backed block storage is nice in theory, but not very practical in reality, isn’t it?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png" width="500" height="348" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:348,&quot;width&quot;:500,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>A meme from 2005. Based on a drawing by </span><a href="https://www.facebook.com/strangedrawings" rel="nofollow ugc noopener">Borya_Spec</a><span>.</span></figcaption></figure></div><p><span>But it’s not all doom and gloom: the upcoming release of Lucene 10 includes the ongoing initiative </span><a href="https://github.com/apache/lucene/issues/13179" rel="nofollow ugc noopener">“Improve Lucene’s I/O Concurrency”</a><span>:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png" width="1133" height="827" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:827,&quot;width&quot;:1133,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Lucene-13179 issue. A screenshot from </span><a href="https://github.com/apache/lucene" rel="nofollow ugc noopener">[3].</a></figcaption></figure></div><p>To summarize the idea behind this enhancement:</p><ul><li><p><span>Lucene is not concurrent inside: e.g. full of regular for-loops over data structures. Such loops are CPU efficient, but </span><strong>not always IO </strong><span>efficient</span><strong> because they’re sequential</strong><span>.</span></p></li><li><p><span>Let’s introduce an </span><strong>IndexInput.prefetch</strong><span> method: it hints to the underlying Directory implementation which parts of the data we are going to read really soon, so it can start prefetching in the background.</span></p></li></ul><p>This will dramatically improve the situation for the HNSW search over S3Directory:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png" width="1456" height="1110" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1110,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Parallel S3 requests instead of sequential. Image by author.</figcaption></figure></div><p><span>This makes all neighbor lookups parallel instead of being sequential! Since Lucene uses M=16 as the default value for its HNSW implementation, then we need only </span><strong>N=num_layers</strong><span> hops to S3 (in practice in the range of 3–5) instead of </span><strong>N*M=num_layers*num_neighbors</strong><span>!</span></p><p>Since S3 has not-so-nice latency but almost unlimited concurrency, this can improve the situation a lot:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png" width="724" height="450" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:450,&quot;width&quot;:724,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>S3 concurrent throughput for M2/M5/C5 EC2 instances, based on data from</span><a href="https://github.com/dvassallo/s3-benchmark" rel="nofollow ugc noopener"> [4].</a></figcaption></figure></div><p>So in practice you can reach a 1.1 GB/s S3-EC2 throughput even on low-cost EC2 instances with just 16 concurrent requests, maxing out the default 10 Gbps network adapter.</p><p>For Nixiesearch the Lucene 10 approach looks promising and we plan to re-evaluate the numbers when the LUCENE-13179 will get implemented. But as for now, Nixiesearch is using a more traditional segment-based replication approach:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png" width="1365" height="573" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:573,&quot;width&quot;:1365,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Stateless index. Image by author.</figcaption></figure></div><p>Segment-based replication is not a new thing and has been available in NRTSearch and OpenSearch for years. But the search nodes are still not stateless: there is index schema and cluster metadata that you can access and modify.</p><p>If you come from regular back-end app development, a regular deployment flow looks like this:</p><ol><li><p>Commit to Git, make a PR.</p></li><li><p>PR gets reviewed, and later the CI/CD system takes care of rolling/blue-green deployment.</p></li></ol><p>In contrast to this approach, to change an index configuration, you need to:</p><ol><li><p>Send an HTTP POST request to a prod cluster. Pray.&nbsp;</p></li><li><p><span>Earth shakes, lights go on and off. You get a </span><strong>“hey wtf” </strong><span>Slack message from the CEO.</span></p></li></ol><p>Can the index management be more like we do in a regular backend development?</p><p><span>Nixiesearch has </span><strong>no API to change runtime configuration, nor API to create and modify indexes</strong><span>. Everything is defined in a static config file that the server loads on startup:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png" width="795" height="464" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:464,&quot;width&quot;:795,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:34848,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>A simple Nixiesearch index definition.</figcaption></figure></div><p>In this snippet we:</p><ul><li><p><span>define a “</span><strong>helloworld</strong><span>” index with two “</span><strong>title</strong><span>” and “</span><strong>price</strong><span>” fields</span></p></li><li><p><span>A </span><strong>title</strong><span> field has a text type, intended for semantic search, and uses the “</span><strong>text</strong><span>” model to perform the embedding process.</span></p></li><li><p><span>A </span><strong>price</strong><span> field is numeric and can be filtered, sorted and faceted over.</span></p></li><li><p><span>We also define an </span><a href="https://huggingface.co/intfloat/e5-small-v2" rel="nofollow ugc noopener">e5-small-v2</a><span> ONNX model for ML inference.</span></p></li></ul><p>And after that we can send regular search queries:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png" width="795" height="429" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:429,&quot;width&quot;:795,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:20377,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>But how can we change index settings if it’s immutable?</p><p><span>Things are changed in Nixiesearch the same way as in traditional cloud deployment strategies by doing </span><strong>a rolling (or a blue-green) deployment</strong><span>. You never change the setup, you create a new one aside and gradually switch traffic when the new deployment reports an OK healthcheck.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png" width="1456" height="1026" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1026,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Blue-green deployment with Nixiesearch. Image by author.</figcaption></figure></div><p><span>Since search engine deployment depends not only on configuration, but also on an externally stored index in S3, Nixiesearch also validates if the </span><strong>configuration change is backward compatible</strong><span> and can use the same index.</span></p><ul><li><p>If the change is backward compatible (e.g. change in caching settings), then the service reports OK to the readiness probe and starts accepting traffic.</p></li><li><p><span>If the change is </span><strong>incompatible and requires reindexing</strong><span>, then the readiness probe is not going to be OK until you reindex.</span></p></li></ul><p>But since the entire index is just one directory on S3, reindexing is not as complicated as with traditional search engines.</p><p>Most of traditional search engines like Elastic/OpenSearch/Solr use push-based approach to indexing:</p><ul><li><p>Submit a batch of document updates to the HTTP REST API of the prod cluster,</p></li><li><p>The cluster starts crunching the batch and updates the distributed index.</p></li></ul><p><span>While this is a traditional approach, it still has a number of major drawbacks: you </span><strong>need to control the back-pressure</strong><span> (so the cluster won’t be overwhelmed with indexing) and </span><strong>the same nodes are used for searching and indexing</strong><span> (so your search latency will be affected).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png" width="794" height="575" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:575,&quot;width&quot;:794,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Do we really need to do the push based indexing in 2024? Image by author, based on </span><a href="https://de.wikipedia.org/wiki/Black_Panther_%28Film%29" rel="nofollow ugc noopener">Black Panther movie</a><span>.</span></figcaption></figure></div><p>But as the index is not anymore tied to the cluster, the whole reindexing process can happen offline:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif" width="1456" height="899" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:899,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Offline reindexing in Nixiesearch. Image by author.</figcaption></figure></div><p><span>With offline indexing, prior to starting the search cluster, you run a separate batch task to rebuild the index and </span><strong>publish it to S3 into a separate location</strong><span>. And then perform a rolling deployment as usual.</span></p><p>But does the design decision to have stateless searchers have any drawbacks?</p><p>Stateless searcher architecture also assumes that searchers don’t talk to each other, as otherwise they’re no longer stateless. And sharding is an example of a feature that cannot be easily implemented on top of a stateless architecture.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png" width="1321" height="1259" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1259,&quot;width&quot;:1321,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>No-sharding vs with-sharding architecture. Image by author.</figcaption></figure></div><p>This does not mean that there will be no sharding support in the future. We just accept the fact that we still do not yet know the best way to implement it without breaking the promise of remaining stateless.</p><p>From another practical perspective, in a consumer-facing search (e.g. e-commerce and enterprise doc search) you rarely have datasets so large that you cannot do without sharding:</p><ul><li><p><strong>A 1M docs MSMARCO index we used for testing is just 3GB on disk</strong><span>. With </span><strong>int8</strong><span> quantization it’ll be about 800MB.</span></p></li><li><p>Indexes of 1B/1T docs are usually seen in log/APM/trace search tasks, and Nixiesearch will definitely not going be the best solution for this.</p></li></ul><p>On the way to better ops, Nixiesearch tries to have as few dependencies as possible. The same goes for internal ML embedding and LLM models. Besides simpler deployment, this has also the following advantages:</p><ul><li><p><strong>Latency</strong><span>: a </span><a href="https://medium.com/nixiesearch/how-to-compute-llm-embeddings-3x-faster-with-model-quantization-25523d9b4ce5" rel="nofollow ugc noopener">CPU ONNX inference of the e5-base-v2 model is about 5 ms</a><span>. This is much faster than sending a network request to an external SaaS embedding API.</span></p></li><li><p><strong>Privacy</strong><span>: your private data (i.e. documents and queries) does not leave your secure perimeter.</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png" width="640" height="627" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:627,&quot;width&quot;:640,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>When your AI is just your prompt. Image by author.</figcaption></figure></div><p>As of v0.3.0, Nixiesearch supports the following model families:</p><ul><li><p><span>For a vector search, </span><strong>any ONNX-compatible sentence-transformers model should work</strong><span>. The model translation can be done either with </span><a href="https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model" rel="nofollow ugc noopener">Huggingface optimum-cli</a><span>, or with an in-house </span><a href="https://github.com/nixiesearch/onnx-convert" rel="nofollow ugc noopener">onnx-convert tool</a><span> (which can also perform model quantization).</span></p></li><li><p><span>For RAG,</span><strong><span> any GGUF model that works with </span><a href="https://github.com/ggerganov/llama.cpp" rel="nofollow ugc noopener">llama-cpp</a></strong><span> should be compatible.</span></p></li><li><p><span>Of course, you can optionally use SaaS-based embedding and completion models from </span><a href="https://cohere.com/" rel="nofollow ugc noopener">Cohere</a><span>, </span><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings" rel="nofollow ugc noopener">Google</a><span>, </span><a href="https://www.mixedbread.ai/" rel="nofollow ugc noopener">Mixedbread</a><span> and </span><a href="https://openai.com/" rel="nofollow ugc noopener">OpenAI</a><span>.</span></p></li></ul><p>Furthermore, both indexing and serving services can be run on the GPU:&nbsp;</p><pre><code>docker run --gpus=all nixiesearch/nixiesearch:0.3.3-amd64-gpu index </code></pre><p>With ONNX model support for both CPU and GPU inference, an obvious next step would be to implement reranking support:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png" width="1456" height="631" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:631,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Modern search pipeline, as seen by author. Image by author.</figcaption></figure></div><p>Since reranking is part of the common retrieval pipeline, we can then:</p><ul><li><p><span>Retrieve the top-N (N=100) documents with hybrid search by combining lexical and semantic retrieval with </span><a href="https://www.nixiesearch.ai/features/search/#hybrid-search-with-reciprocal-rank-fusion" rel="nofollow ugc noopener">RRF</a><span> — Reciprocal Rank Fusion.</span></p></li><li><p><span>Rerank all found documents using a more advanced Cross-Encoder model, such as </span><a href="https://huggingface.co/BAAI/bge-reranker-v2-m3/tree/main" rel="nofollow ugc noopener">BAAI/bge-reranker-v2-m3</a><span>.</span></p></li><li><p>Summarize the top-M (M=10) most relevant documents wusing the RAG approach.</p></li></ul><p>Everything is done with local inference, no data left the searcher node.</p><p>A common pain point of existing search engines is the need for a complicated ML-driven indexing pipeline to compute custom embeddings, do text processing and so on. Can we automate most of the typical ones?</p><p>We see such a pipeline becoming less of a separate application, but more of a feature of the engine itself:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png" width="795" height="294" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a8123bdb-0533-4d68-825b-37382356c3b0_795x294.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:294,&quot;width&quot;:795,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:18623,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The plan is to introduce a </span><strong>multi-modal search with CLIP-like models</strong><span> — but with the embedding and search process handled completely transparently by the engine itself.</span></p><p><span>We also plan to introduce an </span><strong>LLM-driven DSL for text transformation</strong><span>: so you can handle summarization and classification tasks (e.g. “does this doc belong to category A?”).</span></p><p>Yes, such an approach will require GPU for indexing, but since indexing can be a one-time task, it does not require a GPU powered node running for 24x365. And low-end GPU nodes are not that expensive these days: EC2 g4.large with Nvidia T40 GPU is ~$300/month.</p><p>Nixiesearch is an actively developed project with many rough edges. Some features may be missing. Docs are imperfect — but they do exist!&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png" width="741" height="334" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:334,&quot;width&quot;:741,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>And there will definitely be breaking changes in both configuration and index format. But with every Github issue you submit, it’s going to be more and more stable.</p><p><span>For the current </span><a href="https://github.com/nixiesearch/nixiesearch/releases/tag/0.3.3" rel="nofollow ugc noopener">v0.3.3</a><span>, Nixiesearch can already do quite a lot of things:</span></p><ul><li><p>Segment based replication over the S3-compatible block storage.</p></li><li><p>Filters, facets, sorting and autocomplete.</p></li><li><p>Semantic and hybrid retrieval.</p></li><li><p>RAG and local ML inference.</p></li></ul><p>If you want to know more, here are the links:</p><ul><li><p><strong>Github</strong><span>: </span><a href="https://github.com/nixiesearch/nixiesearch" rel="nofollow ugc noopener">nixiesearch/nixiesearch</a><span> (only 78 stars so far at the moment of publication)</span></p></li><li><p><strong>Docs microsite</strong><span>: </span><a href="https://nixiesearch.ai/" rel="nofollow ugc noopener">nixiesearch.ai</a></p></li><li><p><strong>Slack</strong><span>: </span><a href="https://nixiesearch.ai/slack" rel="nofollow ugc noopener">nixiesearch.ai/slack</a></p></li></ul></div></article></div></div>]]></description>
        </item>
    </channel>
</rss>