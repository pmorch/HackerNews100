<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 20 Mar 2024 16:00:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[8 Google Employees Invented Modern AI. Here's the Inside Story (148 pts)]]></title>
            <link>https://www.wired.com/story/eight-google-employees-invented-modern-ai-transformers-paper/</link>
            <guid>39766170</guid>
            <pubDate>Wed, 20 Mar 2024 13:29:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/eight-google-employees-invented-modern-ai-transformers-paper/">https://www.wired.com/story/eight-google-employees-invented-modern-ai-transformers-paper/</a>, See on <a href="https://news.ycombinator.com/item?id=39766170">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>Eight names are</span> listed as authors on “Attention Is All You Need,” a scientific paper written in the spring of 2017. They were all <a href="https://www.wired.com/story/google-prepares-for-a-future-where-search-isnt-king/">Google</a> researchers, though by then one had left the company. When the most tenured contributor, Noam Shazeer, saw an early draft, he was surprised that his name appeared first, suggesting his contribution was paramount. “I wasn’t thinking about it,” he says.</p><div data-testid="GenericCallout"><figure><p><span><strong>/ NAME: NOAM SHAZEER / OCCUPATION: COFOUNDER AND CEO OF CHARACTER AI</strong></span></p></figure></div><p>It’s always a delicate balancing act to figure out how to list names—who gets the coveted lead position, who’s shunted to the rear. Especially in a case like this one, where each participant left a distinct mark in a true group effort. As the researchers hurried to finish their paper, they ultimately decided to “sabotage” the convention of ranking contributors. They added an asterisk to each name and a footnote: “Equal contributor,” it read. “Listing order is random.” The writers sent the paper off to a prestigious artificial intelligence conference just before the deadline—and kicked off a revolution.</p><p>Approaching its seventh anniversary, <a data-offer-url="https://arxiv.org/abs/1706.03762" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://arxiv.org/abs/1706.03762&quot;}" href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank">the “Attention” paper</a> has attained legendary status. The authors started with a thriving and improving technology—a variety of AI called neural networks—and made it into something else: a digital system so powerful that its output can feel like the product of <a href="https://www.wired.com/story/plaintext-groq-mindblowing-chatbot-answers-instantly/#intcid=recommendations_wired-bottom-recirc-v4_efa0ebfa-8b30-44aa-826f-37cbd7ba731a_similar2-3">an alien intelligence</a>. Called transformers, this architecture is <a href="https://www.wired.com/story/artificial-intelligence-neural-networks/">the not-so-secret sauce</a> behind all those <a href="https://www.wired.com/story/fast-forward-forget-chatbots-ai-agents-are-the-future/#intcid=recommendations_wired-bottom-recirc-v4_efa0ebfa-8b30-44aa-826f-37cbd7ba731a_similar2-3">mind-blowing AI products</a>, including <a href="https://www.wired.com/tag/chatgpt/">ChatGPT</a> and graphic generators such as Dall-E and Midjourney. Shazeer now jokes that if he knew how famous the paper would become, he “might have worried more about the author order.” All eight of the signers are now microcelebrities. “I have people asking me for selfies—because I’m on a paper!” says Llion Jones, who is (randomly, of course) name number five.</p><div data-testid="GenericCallout"><figure><p><span><strong>/ NAME: LLION JONES / OCCUPATION: COFOUNDER OF SAKANA AI</strong></span></p></figure></div><p>“Without transformers I don’t think we’d be here now,” says <a href="https://www.wired.com/story/geoffrey-hinton-ai-chatgpt-dangers/">Geoffrey Hinton</a>, who is not one of the authors but is perhaps the world’s most <a href="https://www.wired.com/story/ai-pioneer-explains-evolution-neural-networks/">prominent AI scientist</a>. He’s referring to the ground-shifting times we live in, <a href="https://www.wired.com/story/what-openai-really-wants/">as OpenAI and other companies build systems</a> that rival and in some cases surpass human output.</p><p>All eight authors have since left Google. Like millions of others, they are now working in some way with systems powered by what they created in 2017. I talked to the Transformer Eight to piece together the anatomy of a breakthrough, a gathering of human minds to create a machine that might well save the last word for itself.</p><div data-testid="GenericCallout"><figure><p><span><strong>/ NAME: JAKOB USZKOREIT / OCCUPATION: COFOUNDER AND CEO OF INCEPTIVE</strong></span></p></figure></div><p><span>The story of</span> transformers begins with the fourth of the eight names: Jakob Uszkoreit.</p><p>Uszkoreit is the son of Hans Uszkoreit, a well-known computational linguist. As a high school student in the late 1960s, Hans was imprisoned for 15 months in his native East Germany for protesting the Soviet invasion of Czechoslovakia. After his release, he escaped to West Germany and studied computers and linguistics in Berlin. He made his way to the US and was working in an artificial intelligence lab at SRI, a research institute in Menlo Park, California, when Jakob was born. The family eventually returned to Germany, where Jakob went to university. He didn’t intend to focus on language, but as he was embarking on graduate studies, he took an internship at Google in its Mountain View office, where he landed in the company’s translation group. He was in the family business. He abandoned his PhD plans and, in 2012, decided to join a team at Google that was working on a system that could respond to users’ questions on the search page itself without diverting them to other websites. Apple had just announced Siri, a virtual assistant that promised to deliver one-shot answers in casual conversation, and the Google brass smelled a huge competitive threat: Siri could eat up their search traffic. They started paying a lot more attention to Uszkoreit’s new group.</p><p>“It was a false panic,” Uszkoreit says. Siri never really threatened Google. But he welcomed the chance to dive into systems where computers could engage in a kind of dialog with us. At the time, recurrent neural networks—once an academic backwater—had suddenly started outperforming other methods of AI engineering. The networks consist of many layers, and information is passed and repassed through those layers to identify the best responses. Neural nets were racking up huge wins in fields such as image recognition, and an AI renaissance was suddenly underway. Google was frantically <a href="https://www.wired.com/2016/06/how-google-is-remaking-itself-as-a-machine-learning-first-company/">rearranging its workforce</a> to adopt the techniques. The company wanted systems that could churn out humanlike responses—to auto-complete sentences in emails or create relatively simple customer service chatbots.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>But the field was running into limitations. Recurrent neural networks struggled to parse longer chunks of text. Take a passage like <em>Joe is a baseball player, and after a good breakfast he went to the park and got two hits.</em> To make sense of “two hits,” a language model has to remember the part about baseball. In human terms, it has to be paying attention. The accepted fix was something called “long short-term memory” (LSTM), an innovation that allowed language models to process bigger and more complex sequences of text. But the computer still handled those sequences strictly sequentially—word by tedious word—and missed out on context clues that might appear later in a passage. “The methods we were applying were basically Band-Aids,” Uszkoreit says. “We could not get the right stuff to really work at scale.”</p><p>Around 2014, he began to concoct a different approach that he referred to as self-attention. This kind of network can translate a word by referencing <em>any</em> other part of a passage. Those other parts can clarify a word’s intent and help the system produce a good translation. “It actually considers everything and gives you an efficient way of looking at many inputs at the same time and then taking something out in a pretty selective way,” he says. Though AI scientists are careful not to confuse the metaphor of neural networks with the way the biological brain actually works, Uszkoreit does seem to believe that self-attention is somewhat similar to the way humans process language.</p><p>Uszkoreit thought a self-attention model could potentially be faster and more effective than recurrent neural nets. The way it handles information was also perfectly suited to the powerful parallel processing chips that were being produced en masse to support the machine learning boom. Instead of using a linear approach (look at every word in sequence), it takes a more parallel one (look at a bunch of them together). If done properly, Uszkoreit suspected, you could use self-attention <em>exclusively</em> to get better results.</p><p>Not everyone thought this idea was going to rock the world, including Uszkoreit’s father, who had scooped up two Google Faculty research awards while his son was working for the company. “People raised their eyebrows, because it dumped out all the existing neural architectures,” Jakob Uszkoreit says. Say goodbye to recurrent neural nets? Heresy! “From dinner-table conversations I had with my dad, we weren’t necessarily seeing eye to eye.”</p><p>Uszkoreit persuaded a few colleagues to conduct experiments on self-attention. Their work showed promise, and in 2016 they published a paper about it. Uszkoreit wanted to push their research further—the team’s experiments used only tiny bits of text—but none of his collaborators were interested. Instead, like gamblers who leave the casino with modest winnings, they went off to apply the lessons they had learned. “The thing <em>worked</em>,” he says. “The folks on that paper got excited about reaping the rewards and deploying it in a variety of different places at Google, including search and, eventually, ads. It was an amazing success in many ways, but I didn’t want to leave it there.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Uszkoreit felt that self-attention could take on much bigger tasks. <em>There’s another way to do this</em>, he’d argue to anyone who would listen, and some who wouldn’t, outlining his vision on whiteboards in Building 1945, named after its address on Charleston Road on the northern edge of the Google campus.</p><div data-testid="GenericCallout"><figure><p><span><strong>/ NAME: ILLIA POLOSUKHIN / OCCUPATION: COFOUNDER OF NEAR</strong></span></p></figure></div><p>One day in 2016, Uszkoreit was having lunch in a Google café with a scientist named Illia Polosukhin. Born in Ukraine, Polosukhin had been at Google for nearly three years. He was assigned to the team providing answers to direct questions posed in the search field. It wasn’t going all that well. “To answer something on Google.com, you need something that’s very cheap and high-performing,” Polosukhin says. “Because you have milliseconds” to respond. When Polosukhin aired his complaints, Uszkoreit had no problem coming up with a remedy. “He suggested, why not use self-attention?” says Polosukhin.</p><p>Polosukhin sometimes collaborated with a colleague named Ashish Vaswani. Born in India and raised mostly in the Middle East, he had gone to the University of Southern California to earn his doctorate in the school’s elite machine translation group. Afterward, he moved to Mountain View to join Google—specifically a newish organization called <a href="https://www.wired.com/2013/05/neuro-artificial-intelligence/">Google Brain</a>. He describes Brain as “a radical group” that believed “neural networks were going to advance human understanding.” But he was still looking for a big project to work on. His team worked in Building 1965 next door to Polosukhin’s language team in 1945, and he heard about the self-attention idea. Could that be the project? He agreed to work on it.</p><div data-testid="GenericCallout"><figure><p><span><strong>/ NAME: ASHISH VASWANI / OCCUPATION: COFOUNDER AND CEO OF ESSENTIAL AI</strong></span></p></figure></div><p>Together, the three researchers drew up a design document called “Transformers: Iterative Self-Attention and Processing for Various Tasks.” They picked the name “transformers” from “day zero,” Uszkoreit says. The idea was that this mechanism would <em>transform</em> the information it took in, allowing the system to extract as much understanding as a human might—or at least give the illusion of that. Plus Uszkoreit had fond childhood memories of playing with the Hasbro action figures. “I had two little Transformer toys as a very young kid,” he says. The document ended with a cartoony image of six Transformers in mountainous terrain, zapping lasers at one another.</p><p>There was also some swagger in the sentence that began the paper: “We are awesome.”</p><p>In early 2017, Polosukhin left Google to start his own company. By then new collaborators were coming onboard. An Indian engineer named Niki Parmar had been working for an American software company in India when she moved to the US. She earned a master’s degree from USC in 2015 and was recruited by all the Big Tech companies. She chose Google. When she started, she joined up with Uszkoreit and worked on model variants to improve Google search.</p><div data-testid="GenericCallout"><figure><p><span><strong>/ NAME: NIKI PARMAR / OCCUPATION: COFOUNDER OF ESSENTIAL AI</strong></span></p></figure></div><p>Another new member was Llion Jones. Born and raised in Wales, he loved computers “because it was not normal.” At the University of Birmingham he took an AI course and got curious about neural networks, which were presented as a historical curiosity. He got his master’s in July 2009 and, unable to find a job during the recession, lived on the dole for months. He found a job at a local company and then applied to Google as a “hail Mary.” He got the gig and eventually landed in Google Research, where his manager was Polosukhin. One day, Jones heard about the concept of self-attention from a fellow worker named Mat Kelcey, and he later joined up with Team Transformers. (Later, Jones ran into Kelcey and briefed him on the transformer project. Kelcey wasn’t buying it. “I told him, ‘I’m not sure that’s going to work,’ which is basically the biggest incorrect prediction of my life,” Kelcey says now.)</p><p>The transformer work drew in other Google Brain researchers who were also trying to improve <a href="https://www.wired.com/story/how-chatgpt-works-large-language-model/">large language models</a>. This third wave included Łukasz Kaiser, a Polish-born theoretical computer scientist, and his intern, Aidan Gomez. Gomez had grown up in a small farming village in Ontario, Canada, where his family would tap maple trees every spring for syrup. As a junior at the University of Toronto, he “fell in love” with AI and joined the machine learning group—Geoffrey Hinton’s lab. He began contacting people at Google who had written interesting papers, with ideas for extending their work. Kaiser took the bait and invited him to intern. It wasn’t until months later that Gomez learned those internships were meant for doctoral students, not undergrads like him.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Kaiser and Gomez quickly understood that self-attention looked like a promising, and more radical, solution to the problem they were addressing. “We had a deliberate conversation about whether we wanted to merge the two projects,” says Gomez. The answer was yes.</p><p>The transformer crew set about building a self-attention model to translate text from one language to another. They measured its performance using a benchmark called BLEU, which compares a machine’s output to the work of a human translator. From the start, their new model did well. “We had gone from no proof of concept to having something that was at least on par with the best alternative approaches to LSTMs by that time,” Uszkoreit says. But compared to long short-term memory, “it wasn’t better.”</p><p>They had reached a plateau—until one day in 2017, when Noam Shazeer heard about their project, by accident. Shazeer was a veteran Googler—he’d joined the company in 2000—and an in-house legend, starting with his work on the company’s early ad system. Shazeer had been working on deep learning for five years and recently had become interested in large language models. But these models were nowhere close to producing the fluid conversations that he believed were possible.</p><p>As Shazeer recalls it, he was walking down a corridor in Building 1965 and passing Kaiser’s workspace. He found himself listening to a spirited conversation. “I remember Ashish was talking about the idea of using self-attention, and Niki was very excited about it. I’m like, wow, that sounds like a great idea. This looks like a fun, smart group of people doing something promising.” Shazeer found the existing recurrent neural networks “irritating” and thought: “Let’s go replace them!”</p><p>Shazeer’s joining the group was critical. “These theoretical or intuitive mechanisms, like self-attention, always require very careful implementation, often by a small number of experienced ‘magicians,’ to even show any signs of life,” says Uszkoreit. Shazeer began to work his sorcery right away. He decided to write his own version of the transformer team’s code. “I took the basic idea and made the thing up myself,” he says. Occasionally he asked Kaiser questions, but mostly, he says, he “just acted on it for a while and came back and said, ‘Look, it works.’” Using what team members would later describe with words like “magic” and “alchemy” and “bells and whistles,” he had taken the system to a new level.</p><p>“That kicked off a sprint,” says Gomez. They were motivated, and they also wanted to hit an upcoming deadline—May 19, the filing date for papers to be presented at the biggest AI event of the year, the Neural Information Processing Systems conference in December. As what passes for winter in Silicon Valley shifted to spring, the pace of the experiments picked up. They tested two models of transformers: one that was produced with 12 hours of training and a more powerful version called Big that was trained over three and a half days. They set them to work on English-to-German translation.</p><p>The basic model outperformed all competitors—and Big earned a BLEU score that decisively shattered previous records while also being more computationally efficient. “We had done it in less time than anyone out there,” Parmar says. “And that was only the beginning, because the number kept improving.” When Uszkoreit heard this, he broke out an old bottle of champagne he had lying around in his mountain expedition truck.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>The last two weeks before the deadline were frantic. Though officially some of the team still had desks in Building 1945, they mostly worked in 1965 because it had a better espresso machine in the micro-kitchen. “People weren’t sleeping,” says Gomez, who, as the intern, lived in a constant debugging frenzy and also produced the visualizations and diagrams for the paper. It’s common in such projects to do ablations—taking things out to see whether what remains is enough to get the job done.</p><p>“There was every possible combination of tricks and modules—which one helps, which doesn’t help. Let’s rip it out. Let’s replace it with this,” Gomez says. “Why is the model behaving in this counterintuitive way? Oh, it’s because we didn’t remember to do the masking properly. Does it work yet? OK, move on to the next. All of these components of what we now call the transformer were the output of this extremely high-paced, iterative trial and error.” The ablations, aided by Shazeer’s implementations, produced “something minimalistic,” Jones says. “Noam is a wizard.”</p><p>Vaswani recalls crashing on an office couch one night while the team was writing the paper. As he stared at the curtains that separated the couch from the rest of the room, he was struck by the pattern on the fabric, which looked to him like synapses and neurons. Gomez was there, and Vaswani told him that what they were working on would transcend machine translation. “Ultimately, like with the human brain, you need to unite all these modalities—speech, audio, vision—under a single architecture,” he says. “I had a strong hunch we were onto something more general.”</p><p>In the higher echelons of Google, however, the work was seen as just another interesting AI project. I asked several of the transformers folks whether their bosses ever summoned them for updates on the project. Not so much. But “we understood that this was potentially quite a big deal,” says Uszkoreit. “And it caused us to actually obsess over one of the sentences in the paper toward the end, where we comment on future work.”</p><p>That sentence anticipated what might come next—the application of transformer models to basically all forms of human expression. “We are excited about the future of attention-based models,” they wrote. “We plan to extend the transformer to problems involving input and output modalities other than text” and to investigate “images, audio and video.”</p><p>A couple of nights before the deadline, Uszkoreit realized they needed a title. Jones noted that the team had landed on a radical rejection of the accepted best practices, most notably LSTMs, for one technique: attention. The Beatles, Jones recalled, had named a song “All You Need Is Love.” Why not call the paper “Attention Is All You Need”?</p><p><em>The Beatles?</em></p><p>“I’m British,” says Jones. “It literally took five seconds of thought. I didn’t think they would use it.”</p><p>They continued collecting results from their experiments right up until the deadline. “The English-French numbers came, like, five minutes before we submitted the paper,” says Parmar. “I was sitting in the micro-kitchen in 1965, getting that last number in.” With barely two minutes to spare, they sent off the paper.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Google, as almost all tech companies do, quickly filed provisional patents on the work. The reason was not to block others from using the ideas but to build up its patent portfolio for defensive purposes. (The company has a philosophy of “if technology advances, Google will reap the benefits.”)</p><p>When the transformer crew heard back from the conference peer reviewers, the response was a mix. “One was positive, one was extremely positive, and one was, ‘This is OK,’” says Parmar. The paper was accepted for one of the evening poster sessions.</p><p>By December, the paper was generating a buzz. Their four-hour session on December 6 was jammed with scientists wanting to know more. The authors talked until they were hoarse. By 10:30 pm, when the session closed, there was still a crowd. “Security had to tell us to leave,” says Uszkoreit. Perhaps the most satisfying moment for him was when computer scientist Sepp Hochreiter came up and praised the work—quite a compliment, considering that Hochreiter was the coinventor of long short-term memory, which transformers had just booted as the go-to hammer in the AI toolkit.</p><p><span>Transformers did not</span> instantly take over the world, or even Google. Kaiser recalls that around the time of the paper’s publication, Shazeer proposed to Google executives that the company abandon the entire search index and train a huge network with transformers—basically to transform how Google organizes information. At that point, even Kaiser considered the idea ridiculous. Now the conventional wisdom is that it’s <a href="https://www.wired.com/story/google-io-just-added-generative-ai-to-search/">a matter of time</a>.</p><p>A startup called OpenAI was <a href="https://www.wired.com/story/what-openai-really-wants/">much faster to pounce</a>. Soon after the paper was published, OpenAI’s chief researcher, Ilya Sutskever—who had known the transformer team during his time at Google—suggested that one of its scientists, Alex Radford, work on the idea. The results were the first GPT products. As OpenAI CEO Sam Altman told me last year, “When the transformer paper came out, I don’t think anyone at Google realized what it meant.”</p><p>The picture internally is more complicated. “It was pretty evident to us that transformers could do really magical things,” says Uszkoreit. “Now, you may ask the question, why wasn’t there <a href="https://www.wired.com/story/google-rebrands-ai-chatbot-gemini/">ChatGPT by Google</a> back in 2018? Realistically, we could have had GPT-3 or even 3.5 probably in 2019, maybe 2020. The big question isn’t, did they see it? The question is, why didn’t we do anything with the fact that we had seen it? The answer is tricky.”</p><div data-testid="GenericCallout"><figure><p><span><strong>/ NAME: AIDAN GOMEZ / OCCUPATION: COFOUNDER AND CEO OF COHERE</strong></span></p></figure></div><p>Many tech critics point to Google’s transition from an innovation-centered playground to a bottom-line-focused bureaucracy. As Gomez <a href="https://www.ft.com/content/37bb01af-ee46-4483-982f-ef3921436a50">told</a> the <em>Financial Times</em>, “They weren’t modernizing. They weren’t adopting this tech.” But that would have taken a lot of daring for a giant company whose technology led the industry and reaped huge profits for decades. Google did begin to integrate transformers into products in 2018, starting with its translation tool. Also that year, it introduced a new transformer-based language model called BERT, which it started to apply to search the year after.</p><p>But these under-the-hood changes seem timid compared to OpenAI’s quantum leap and Microsoft’s <a href="https://www.wired.com/story/microsofts-satya-nadella-is-betting-everything-on-ai/">bold integration</a> of transformer-based systems into its product line. When I asked CEO Sundar Pichai last year why his company wasn’t first to launch a large language model like ChatGPT, he argued that in this case Google found it advantageous to let others lead. “It’s not fully clear to me that it might have worked out as well. The fact is, we can do more after people had seen how it works,” he said.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>There <em>is</em> the undeniable truth that all eight authors of the paper have left Google. Polosukhin’s company, Near, built a blockchain whose tokens have a market capitalization around $4 billion. Parmar and Vaswani paired up as business partners in 2021 to start Adept (estimated valuation of $1 billion) and are now on their <em>second</em> company, called Essential AI ($8 million in funding). Llion Jones’ Tokyo-based Sakana AI is valued at $200 million. Shazeer, who left in October 2021, cofounded Character AI (estimated valuation of $5 billion). Aidan Gomez, the intern in the group, cofounded Cohere in Toronto in 2019 (estimated valuation of $2.2 billion). Jakob Uszkoreit’s biotech company, Inceptive, is valued at $300 million. All those companies (except Near) are based on transformer technology.</p><div data-testid="GenericCallout"><figure><p><span><strong>/ NAME: LUKASZ KAISER / OCCUPATION: RESEARCHER AT OPENAI</strong></span></p></figure></div><p>Kaiser is the only one who hasn’t founded a company. He joined OpenAI and is one of the inventors of a new technology called <a href="https://www.wired.com/story/fast-forward-clues-hint-openai-shadowy-q-project/">Q*</a>, which Altman said last year will “push the veil of ignorance back and the frontier of discovery forward.” (When I attempted to quiz Kaiser on this in our interview, the OpenAI PR person almost leaped across the table to silence him.)</p><p>Does Google miss these escapees? Of course, in addition to others who have migrated from the company to new AI startups. (Pichai reminded me, when I asked him about the transformer departures, that industry darling OpenAI also has seen defections: “The AI area is very, very dynamic,” he said.) But Google can boast that it created an environment that supported the pursuit of unconventional ideas. “In a lot of ways Google has been way ahead—they invested in the right minds and created the environment where we could explore and push the envelope,” Parmar says. “It’s not crazy that it took time to adopt it. Google had so much more at stake.”</p><p>Without that environment: no transformer. Not only were the authors all Google employees, they also worked out of the same offices. Hallway encounters and overheard lunch conversations led to big moments. The group is also culturally diverse. Six of the eight authors were born outside the United States; the other two are children of two green-card-carrying Germans who were temporarily in California and a first-generation American whose family had fled persecution, respectively.</p><p>Uszkoreit, speaking from his office in Berlin, says that innovation is all about the right conditions. “It’s getting people who are super excited about something who are at the right point in their life,” he says. “If you have that and have fun while you do it, and you’re working on the right problems—and you’re lucky—the magic happens.”</p><p>Something magical also happened between Uszkoreit and his famous father. After all those dinner table debates, Hans Uszkoreit, his son reports, has now cofounded a company that is building large language models. Using transformers, of course.</p><hr><p><em>Let us know what you think about this article. Submit a letter to the editor at</em> <em><a href="mailto:mail@wired.com">mail@wired.com</a>.</em></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First beta of Nintendo Switch emulator Suyu goes live (114 pts)]]></title>
            <link>https://overkill.wtf/nintendo-switch-emulator-suyu-beta/</link>
            <guid>39766138</guid>
            <pubDate>Wed, 20 Mar 2024 13:26:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://overkill.wtf/nintendo-switch-emulator-suyu-beta/">https://overkill.wtf/nintendo-switch-emulator-suyu-beta/</a>, See on <a href="https://news.ycombinator.com/item?id=39766138">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<div>
<p>
Suyu is a new Switch emulator compatible with the Steam Deck that launches in public beta today.
</p>
<p>One of the first Nintendo Switch emulators to rise <a href="https://overkill.wtf/switch-emulator-yuzu/">from Yuzu's ashes</a> is here: Suyu, based on Yuzu's source code, launches in a first public beta today. Their changelog looks as follows:</p><blockquote>- Full rebrand<br>- ICNS Icon generation<br>- Error handling<br>- Qlaunch initial integration(buggy/requires further testing; requires V17.0.0 firmware or newer)<br>- Gitlab ci for automated builds<br>- Require all keys to be user provided, along with firmware<br>- Improved Addons Manager<br>- Various crash fixes<br>- Initial work for MacOS support<br>- Fix for video playback AMD devices<br>- Enabled more features on AMD proprietary drivers<br>- Multiplayer API re-implemented<br>- Removed all telemetry<br>- New UI options/improvements<br>- QOL changes</blockquote><p>Unlike Yuzu, the Suyu team does things differently in regards to Nintendo Switch emulation than their predecessors. <a href="https://overkill.wtf/how-to-setup-yuzu-for-steam-deck/">Whereas Yuzu only required prod.keys</a>, Suyu will also require you to dump your title.keys and firmware from a hacked Switch to run games. The devs also clarified that they are doing this for nonprofit (there's no Patreon) and do not condone piracy.</p><p>The developer team is releasing <a href="https://gitlab.com/suyu-emu/suyu/-/releases/v0.0.2-master?ref=overkill.wtf">binaries for Linux, Windows, Android and even an experimental build for macOS</a> (Yuzu was unavailable for Apple's platform). I tried the build on Steam Deck (it works) and will work on a guide.</p>
</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Paris preserves its mixed society by pouring billions into public housing (134 pts)]]></title>
            <link>https://www.nytimes.com/2024/03/17/realestate/paris-france-housing-costs.html</link>
            <guid>39765692</guid>
            <pubDate>Wed, 20 Mar 2024 12:48:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/03/17/realestate/paris-france-housing-costs.html">https://www.nytimes.com/2024/03/17/realestate/paris-france-housing-costs.html</a>, See on <a href="https://news.ycombinator.com/item?id=39765692">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/03/17/realestate/paris-france-housing-costs.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[People hate the idea of car-free cities until they live in one (145 pts)]]></title>
            <link>https://www.wired.com/story/car-free-cities-opposition/</link>
            <guid>39765281</guid>
            <pubDate>Wed, 20 Mar 2024 12:00:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/car-free-cities-opposition/">https://www.wired.com/story/car-free-cities-opposition/</a>, See on <a href="https://news.ycombinator.com/item?id=39765281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>London had a</span> problem. In 2016, more than 2 million of the city’s residents—roughly a quarter of its population—lived in areas with illegal levels of air pollution; areas that also contained nearly 500 of the city’s schools. That same air pollution was prematurely killing as many as <a data-offer-url="https://www.gov.uk/government/publications/air-pollution-applying-all-our-health/air-pollution-applying-all-our-health#:~:text=The%20annual%20mortality%20of%20human,and%2036,000%20deaths%20every%20year." data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.gov.uk/government/publications/air-pollution-applying-all-our-health/air-pollution-applying-all-our-health#:~:text=The%20annual%20mortality%20of%20human,and%2036,000%20deaths%20every%20year.&quot;}" href="https://www.gov.uk/government/publications/air-pollution-applying-all-our-health/air-pollution-applying-all-our-health#:~:text=The%20annual%20mortality%20of%20human,and%2036,000%20deaths%20every%20year." rel="noopener" target="_blank">36,000 people a year</a>. Much of it was coming from transport: <a data-offer-url="https://www.centreforlondon.org/blog/reimagining-london-transport/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.centreforlondon.org/blog/reimagining-london-transport/&quot;}" href="https://www.centreforlondon.org/blog/reimagining-london-transport/" rel="noopener" target="_blank">a quarter</a> of the city’s carbon emissions were from moving people and goods, with three-quarters of that emitted by road traffic.</p><p>But in the years since, carbon emissions have fallen. There’s also been a <a href="https://www.theguardian.com/environment/2020/oct/03/dramatic-plunge-in-london-air-pollution-since-2016-report-finds">94 percent reduction</a> in the number of people living in areas with illegal levels of nitrogen dioxide, a pollutant that causes lung damage. The reason? London has spent years and millions of pounds reducing the number of motorists in the city.</p><p>It’s far from alone. From Oslo to Hamburg and Ljubljana to Helsinki, cities across Europe have started working to reduce their road traffic in an effort to curb air pollution and climate change.</p><p>But while it’s certainly having an impact (Ljubljana, one of the earliest places to transition away from cars, has seen sizable reductions in carbon emissions and <a data-offer-url="https://www.ljubljana.si/en/news/air-quality-in-ljubljana/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.ljubljana.si/en/news/air-quality-in-ljubljana/&quot;}" href="https://www.ljubljana.si/en/news/air-quality-in-ljubljana/" rel="noopener" target="_blank">air pollution</a>), going car-free is a lot harder than it seems. Not only has it led to politicians and urban planners facing death threats and being doxxed, it has forced them to rethink the entire basis of city life.</p><p>London’s car-reduction policies come in a variety of forms. There are charges for dirtier vehicles and for driving into the city center. Road layouts in residential areas have been redesigned, with one-way systems and bollards, barriers, and planters used to reduce through-traffic (creating what are known as “low-traffic neighborhoods”—or LTNs). And schemes to get more people cycling and using public transport have been introduced. The city has avoided the kind of outright car bans seen elsewhere in Europe, such as in Copenhagen, but nevertheless things have changed.</p><p>“The level of traffic reduction is transformative, and it’s throughout the whole day,” says Claire Holland, leader of the council in Lambeth, a borough in south London. Lambeth now sees 25,000 fewer daily car journeys than before its LTN scheme was put in place in 2020, even after adjusting for the impact of the pandemic. Meanwhile, there was a 40 percent increase in cycling and similar rises in walking and scooting over that same period.</p><p>What seems to work best is a carrot-and-stick approach—creating positive reasons to take a bus or to cycle rather than just making driving harder. “In crowded urban areas, you can’t just make buses better if those buses are still always stuck in car traffic,” says Rachel Aldred, professor of transport at the University of Westminster and director of its Active Travel Academy. “The academic evidence suggests that a mixture of positive and negative characteristics is more effective than either on their own.”</p><p>For countries looking to cut emissions, cars are an obvious target. They make up a big proportion of a country’s carbon footprint, accounting for <a href="https://www.europarl.europa.eu/news/en/headlines/society/20190313STO31218/co2-emissions-from-cars-facts-and-figures-infographics">one-fifth of all emissions</a> across the European Union. Of course, urban driving doesn’t make up the majority of a country’s car use, but the kind of short journeys taken when driving in the city are some of the most obviously wasteful, making cities an ideal place to start if you’re looking to get people out from behind the wheel. That, and the fact that many city residents are already car-less (just 40 percent of people in Lambeth own cars, for example) and that cities tend to have better public transport alternatives than elsewhere.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Plus, traffic-reduction programmes also have impacts beyond reducing air pollution and carbon emissions. In cities like Oslo and Helsinki, thanks to car-reduction policies, entire years have passed without a single road traffic death. It’s even been suggested that needing less parking could free up space to help ease the chronic housing shortage felt in so many cities.</p><p>But as effective as policies to end or reduce urban car use have been, they’ve almost universally faced huge opposition. When Oslo proposed in 2017 that its city center should be car-free, the backlash saw the idea branded as a “Berlin Wall against motorists.” The plan ended up being downgraded into a <a data-offer-url="http://www.aftenposten.no/osloby/Vil-ikke-lenger-tvinge-bilene-ut-av-sentrum-607083b.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;http://www.aftenposten.no/osloby/Vil-ikke-lenger-tvinge-bilene-ut-av-sentrum-607083b.html&quot;}" href="http://www.aftenposten.no/osloby/Vil-ikke-lenger-tvinge-bilene-ut-av-sentrum-607083b.html" rel="noopener" target="_blank">less ambitious scheme</a> consisting of smaller changes, like removing car parking and building cycle lanes to try to lower the number of vehicles.</p><p>In London, the introduction of LTNs has also <a data-offer-url="https://www.newstatesman.com/world/uk/2020/11/low-traffic-neighbourhoods-LTNs-London-car-street-cycling-walking-culture-war-pollution-gentrification" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newstatesman.com/world/uk/2020/11/low-traffic-neighbourhoods-LTNs-London-car-street-cycling-walking-culture-war-pollution-gentrification&quot;}" href="https://www.newstatesman.com/world/uk/2020/11/low-traffic-neighbourhoods-LTNs-London-car-street-cycling-walking-culture-war-pollution-gentrification" rel="noopener" target="_blank">led to a massive backlash</a>. In the east London borough of Hackney, one councilor and his family were sent death threats due to their support for the programme. Bollards were regularly graffitied, while pro-LTN activists were accused of “social cleansing.” It was suggested that low-traffic areas would drive up house prices and leave the only affordable accommodation on unprotected roads. “It became very intimidating,” says Holland. “I had my address tweeted out twice, with sort of veiled threats from people who didn’t even live in the borough saying that we knew they knew where I lived.”</p><p>Part of that response is a testament to how much our cities, and by extension, our lives are designed around cars. In the US, between <a href="https://www.vox.com/a/new-economy-future/cars-cities-technologies">50 and 60 percent</a> of the downtowns of many cities are dedicated to parking alone. While in the UK that figure tends to be smaller, designing streets to be accessible to a never-ending stream of traffic has been the central concern of most urban planning since the Second World War. It’s what led to the huge sprawl of identikit suburban housing on the outskirts of cities like London, each sporting its own driveway and ample road access.</p><p>“If you propose this idea to the average American, the response is: if you take my car away from me, I will die,” says J. H. Crawford, the author of the book <a data-offer-url="https://www.carfree.com/book/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.carfree.com/book/&quot;}" href="https://www.carfree.com/book/" rel="noopener" target="_blank"><em>Carfree Cities</em></a> and a leading figure in the movement to end urban car use. “If you do that overnight, without making any other provisions, that’s actually approximately correct.” Having the right alternatives to cars is therefore vital to reducing city traffic.</p><p>And any attempts to reduce urban car use tend to do better when designed from the bottom up. Barcelona’s <a href="https://www.youtube.com/watch?v=ZORzsubQA_M&amp;ab_channel=Vox">“superblocks” programme</a>, which takes sets of nine blocks within its grid system and limits cars to the roads around the outside of the set (as well as reducing speed limits and removing on-street parking) was shaped by <a data-offer-url="https://ajuntament.barcelona.cat/ecologiaurbana/en/bodies-involved/citizen-participation/superblocks" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://ajuntament.barcelona.cat/ecologiaurbana/en/bodies-involved/citizen-participation/superblocks&quot;}" href="https://ajuntament.barcelona.cat/ecologiaurbana/en/bodies-involved/citizen-participation/superblocks" rel="noopener" target="_blank">having resident input on every stage of the process</a>, from design to implementation. Early indicators suggest the policy has been <a data-offer-url="https://www.barcelona.cat/infobarcelona/en/tema/urban-planning-and-infrastructures/superblocks-are-having-positive-effects-on-health-and-well-being_1097301.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.barcelona.cat/infobarcelona/en/tema/urban-planning-and-infrastructures/superblocks-are-having-positive-effects-on-health-and-well-being_1097301.html&quot;}" href="https://www.barcelona.cat/infobarcelona/en/tema/urban-planning-and-infrastructures/superblocks-are-having-positive-effects-on-health-and-well-being_1097301.html" rel="noopener" target="_blank">wildly popular with residents</a>, has seen nitrogen dioxide air pollution fall by 25 percent in some areas, and <a href="https://www.sciencedirect.com/science/article/pii/S0160412019315223#:~:text=The%20Superblocks%20were%20estimated%20to,CI%3A%200.6%E2%80%932.8">will prevent</a> an estimated 667 premature deaths each year, saving an estimated 1.7 billion euros.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>When it comes to design, there’s also the question of access. Whether it’s emergency services needing to get in or small businesses awaiting deliveries, there’s an important amount of “last mile” traffic—transport that gets people or things to the actual end point of their journey—that is vital to sustaining an urban area. If you want to reduce traffic, you have to work around that and think of alternative solutions—such as allowing emergency vehicles access to pedestrianized areas, or even using automatic number plate recognition to exempt emergency vehicles from the camera checks that are used to police through-traffic in LTNs (which is what Lambeth is doing, Holland says).</p><p>But even then, it’s often just hard to convince people an entirely different city layout is possible. Getting people to accept that how they live alongside cars can be changed—say, with an LTN—takes time. But government surveys of the UK’s recently implemented LTNs have indicated that support from residents for such schemes increases over time. “If you start seeing more and more of those kinds of things, things become thinkable,” explains Aldred. If you start unpicking the idea that car use can’t be changed, “it starts to become possible to do more and more things without cars for people.”</p><p>The other issue is that, to put it simply, cars are never just cars. They’re interwoven into our culture and consumption as symbols of affluence, independence, and success, and the aspiration to achieve those things in future. “A man who, beyond the age of 26, finds himself on a bus can count himself a failure,” the British prime minister Margaret Thatcher <a href="https://www.economist.com/britain/2006/09/28/the-wheels-on-the-bus">reportedly once said</a>. “That’s how we got in this mess in the first place, though,” says Crawford. “Everybody saw that the rich people were driving cars, and they wanted to too.”</p><p>That divide goes some way to explaining why the opposition to car-reduction schemes is often so extreme and can devolve into a “culture war”—which is what Holland has found in her experience with LTNs. But that struggle also outlines an important fact about car-free urban areas—that once cities make the decision to reduce or remove cars, they rarely go back. No one I spoke to for this piece could name a recent sizable pedestrianization or traffic-reduction scheme that had been reversed once it had been given time to have an effect.</p><p>Many of the cities that pioneered reducing car use—like <a href="https://www.bbc.com/future/article/20140318-five-car-free-city-experiments">Copenhagen in the 1970s</a>—are rated today as some of the best places to live in the world. Even with London’s experimental and often unpopular LTN scheme, 100 of the 130 low-traffic areas created have been kept in place, Aldred says.</p><p>“Generally speaking, if a sensible program is adopted to really reduce or eliminate car usage in a central urban area, it seems to stick,” says Crawford. “If you go back a year or two later, people will just say: well, this is the best thing we ever did.”</p><hr><p>Reaching net zero emissions by 2050 will require innovative solutions at a global scale. In this series, in partnership with the Rolex Perpetual Planet initiative, WIRED highlights individuals and communities working to solve some of our most pressing environmental challenges. It’s produced in partnership with Rolex but all content is editorially independent. <a href="https://www.wired.co.uk/topic/rolex-planet-pioneers">Find out more</a>.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Michel Talagrand Wins Abel Prize for Work Wrangling Randomness (126 pts)]]></title>
            <link>https://www.quantamagazine.org/michel-talagrand-wins-abel-prize-for-work-wrangling-randomness-20240320/</link>
            <guid>39764954</guid>
            <pubDate>Wed, 20 Mar 2024 11:12:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/michel-talagrand-wins-abel-prize-for-work-wrangling-randomness-20240320/">https://www.quantamagazine.org/michel-talagrand-wins-abel-prize-for-work-wrangling-randomness-20240320/</a>, See on <a href="https://news.ycombinator.com/item?id=39764954">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Random processes take place all around us. It rains one day but not the next; stocks and bonds gain and lose value; traffic jams coalesce and disappear. Because they’re governed by numerous factors that interact with one another in complicated ways, it’s impossible to predict the exact behavior of such systems. Instead, we think about them in terms of probabilities, characterizing outcomes as likely or rare.</p>
<p>Today, the French probability theorist <a href="https://michel.talagrand.net/">Michel Talagrand</a> was awarded the Abel Prize, one of the highest honors in mathematics, for developing a deep and sophisticated understanding of such processes. The prize, presented by the king of Norway, is modeled on the Nobel and comes with 7.5 million Norwegian kroner (about $700,000). When he was told he had won, “my mind went blank,” Talagrand said. “The type of mathematics I do was not fashionable at all when I started. It was considered inferior mathematics. The fact that I was given this award is absolute proof this is not the case.”</p>
<p>Other mathematicians agree. Talagrand’s work “changed the way I view the world,” said <a href="https://web.math.princeton.edu/~naor/">Assaf Naor</a> of Princeton University. Today, added <a href="https://www.ntnu.edu/employees/helge.holden">Helge Holden</a>, the chair of the Abel prize committee, “it is becoming very popular to describe and model real-world events by random processes. Talagrand’s toolbox comes up immediately.”</p>
<p>Talagrand views his own life as a chain of unlikely events. He barely passed grade school in Lyon: Though he was interested in science, he didn’t like to study. When he was 5 years old, he lost sight in his right eye after his retina detached; at age 15, he suffered three retinal detachments in his other eye, forcing him to spend a month in the hospital, eyes bandaged, fearing he’d go blind. His father, a mathematics professor, visited him every day, keeping his mind busy by teaching him math. “This is how I learned the power of abstraction,” Talagrand <a href="https://michel.talagrand.net/longbio.pdf">wrote in 2019</a> after winning the Shaw Prize, another major math award that comes with a $1.2 million bounty. (Talagrand is using some of this money, along with his Abel winnings, to found a prize of his own, “recognizing the achievements of young researchers in the areas to which I have devoted my life.”)</p>

<p>He missed half a year of school while he recovered, but he was inspired to start focusing on his studies. He excelled in mathematics, and after graduating from college in 1974, he was hired by the French National Center for Scientific Research, Europe’s largest research institute, where he worked until his retirement in 2017. During that time, he obtained his doctorate; fell in love with his future wife, a statistician, at first sight (he proposed to her three days after meeting her); and gradually developed an interest in probability, publishing hundreds of papers on the topic.</p>
<p>That wasn’t preordained. Talagrand began his career studying high-dimensional geometric spaces. “For 10 years, I had not discovered what I was good at,” he said. But he does not regret this detour. It eventually led him to probability theory, where “I had this other viewpoint … that gave me a way to look at things differently,” he said. It enabled him to examine random processes through the lens of high-dimensional geometry.</p>
<p>“He brings in his geometric intuition to solve purely probabilistic questions,” Naor said.</p>
<p>A random process is a collection of events whose outcomes vary according to chance in a way that can be modeled — like a sequence of coin flips, or the trajectories of atoms in a gas, or daily rainfall totals. Mathematicians want to understand the relationship between individual outcomes and aggregate behavior. How many times do you have to flip a coin to figure out whether it’s fair? Will a river overflow its banks?</p>
<p>Talagrand focused on processes whose outcomes are distributed according to a bell-shaped curve called a Gaussian. Such distributions are common in nature and have a number of desirable mathematical properties. He wanted to know what can be said with certainty about extreme outcomes in these situations. So he proved a set of inequalities that put tight upper and lower bounds on possible outcomes. “To obtain a good inequality is a piece of art,” Holden said. That art is useful: Talagrand’s methods can give an optimal estimate of, say, the highest level a river might rise to in the next 10 years, or the magnitude of the strongest potential earthquake.</p>
<p>When we’re dealing with complex, high-dimensional data, finding such maximum values can be tough.</p>
<p>Say you want to assess the risk of a river flooding — which will depend on factors like rainfall, wind and temperature. You can model the river’s height as a random process. Talagrand spent 15 years developing a technique called generic chaining that allowed him to create a high-dimensional geometric space related to such a random process. His method “gives you a way to read the maximum from the geometry,” Naor said.</p>
<p>The technique is very general and therefore widely applicable. Say you want to analyze a massive, high-dimensional data set that depends on thousands of parameters. To draw a meaningful conclusion, you want to preserve the data set’s most important features while characterizing it in terms of just a few parameters. (For example, this is one way to analyze and compare the complicated structures of different proteins.) Many state-of-the-art methods achieve this simplification by applying a random operation that maps the high-dimensional data to a lower-dimensional space. Mathematicians can use Talagrand’s generic chaining method to determine the maximal amount of error that this process introduces — allowing them to determine the chances that some important feature isn’t preserved in the simplified data set.</p>
<p>Talagrand’s work wasn’t just limited to analyzing the best and worst possible outcomes of a random process. He also studied what happens in the average case.</p>
<p>In many processes, random individual events can, in aggregate, lead to highly deterministic outcomes. If measurements are independent, then the totals become very predictable, even if each individual event is impossible to predict. For instance, flip a fair coin. You can’t say anything in advance about what will happen. Flip it 10 times, and you’ll get four, five or six heads — close to the expected value of five heads — about 66% of the time. But flip the coin 1,000 times, and you’ll get between 450 and 550 heads 99.7% of the time, a result that’s even more concentrated around the expected value of 500. “It is exceptionally sharp around the mean,” Holden said.</p>
<p>“Even though something has so much randomness, the randomness cancels itself out,” Naor said. “What initially seemed like a horrible mess is actually organized.”</p>
<p>This phenomenon, known as concentration of measure, occurs in much more complicated random processes, too. Talagrand came up with a collection of inequalities that make it possible to quantify that concentration, and proved that it arises in many different contexts. His techniques marked a departure from previous work in the area. Proving the first such inequality, he wrote in his 2019 essay, was “a magical experience.” He was “in a state of constant elation.”</p>
<p>He’s particularly proud of one of his subsequent concentration inequalities. “It’s not easy to get a result which tries to think about the universe and that at the same time has a one-page proof that’s easy to explain,” he said. (He recalls with delight that he once used a cab service whose owner recognized his name, having learned the inequality during a probability class in business school. “That was extraordinary,” he said.)</p>
<p>Like his generic chaining method, Talagrand’s concentration inequalities appear all over mathematics. “It’s amazing how far it goes,” Naor said. “Talagrand inequalities are the screws that hold things together.”</p>
<p>Consider an optimization problem where you have to sort items of different sizes into bins — a model of resource allocation. When you have a lot of items, it’s very difficult to figure out the smallest number of bins you’ll need. But Talagrand’s inequalities can tell you how many bins you’re likely to need if the items’ sizes are random.</p>
<p>Similar methods have been used to prove concentration phenomena in combinatorics, physics, computer science, statistics and other settings.</p>
<p>More recently, Talagrand applied his understanding of random processes to prove an important conjecture about spin glasses, disordered magnetic materials created by random, often conflicting interactions. Talagrand was frustrated that, though spin glasses are mathematically well defined, physicists understood them better than mathematicians. “It was a thorn in our foot,” he said. He proved a result — about the so-called free energy of spin glasses — that provided a foundation for a more mathematical theory.</p>
<p>Throughout his career, Talagrand’s research has been marked by “this ability to just step back and find the general principles that are reusable everywhere,” Naor said. “He revisits and revisits, and thinks about something from all kinds of perspectives. And eventually he puts out an insight that becomes a workhorse, that everyone is using.”</p>

<p>“I like to understand simple things very well, because my brain is very slow,” Talagrand said. “So I think about them for a very, very long time.” He’s driven, he said, by the desire to “understand something deeply, in a pure way, which makes the theory much easier. Then the next generation can start from there and make progress on their own terms.”</p>
<p>Over the past decade, he has achieved this by writing textbooks — not just about random processes and spin glasses, but also about an area he doesn’t work in at all, quantum field theory. He had wanted to learn about it, but realized that all the textbooks he could find were written by and for physicists, not mathematicians. So he wrote one himself. “After you can no longer invent things, you can explain them,” he said.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: macOS Reminder Sync for Obsidian Tasks (118 pts)]]></title>
            <link>https://turquoisehexagon.co.uk/remindersync/</link>
            <guid>39764919</guid>
            <pubDate>Wed, 20 Mar 2024 11:07:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://turquoisehexagon.co.uk/remindersync/">https://turquoisehexagon.co.uk/remindersync/</a>, See on <a href="https://news.ycombinator.com/item?id=39764919">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><img src="https://serverlez-static-prod.s3.amazonaws.com/images/512-mac.png" alt="App Logo">
        </p>
        <div>
            
            <p>Sync your Obsidian Tasks to MacOS Reminders.app!
            </p>
            <p><a href="https://apps.apple.com/us/app/reminder-sync-for-obsidian/id6478323460?mt=12&amp;itsct=apps_box_badge&amp;itscg=30200">
                <img src="https://tools.applemediaservices.com/api/badges/download-on-the-mac-app-store/black/en-us?size=250x83&amp;releaseDate=1710892800" alt="Download on the Mac App Store">
            </a>
        </p></div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why does an extraneous build step make my Zig app 10x faster? (141 pts)]]></title>
            <link>https://mtlynch.io/zig-extraneous-build/</link>
            <guid>39764287</guid>
            <pubDate>Wed, 20 Mar 2024 09:18:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mtlynch.io/zig-extraneous-build/">https://mtlynch.io/zig-extraneous-build/</a>, See on <a href="https://news.ycombinator.com/item?id=39764287">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>For the past few months, I’ve been curious about two technologies: the Zig programming language and Ethereum cryptocurrency. To learn more about both, I’ve been using Zig to write a bytecode interpreter for the Ethereum Virtual Machine.</p><p>Zig is a great language for performance optimization, as it gives you fine-grained control over memory and control flow. To motivate myself, I’ve been benchmarking my Ethereum implementation against the official Go implementation.</p><figure><figcaption><p>At the beginning of this process, my hobby Ethereum Zig implementation underperformed the official Go implementation by about 40%.</p></figcaption></figure><p>Recently, I made what I thought was a simple refactoring to my benchmarking script, but my app’s performance tanked. I identified the relevant change as the difference between these two commands:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ <span>echo</span> <span>'60016000526001601ff3'</span> | xxd -r -p | zig build run -Doptimize=ReleaseFast
</span></span><span><span>execution time:  58.808µs
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="bash"><span><span>$ <span>echo</span> <span>'60016000526001601ff3'</span> | xxd -r -p | ./zig-out/bin/eth-zvm
</span></span><span><span>execution time:  438.059µs
</span></span></code></pre></div><p><code>zig build run</code> is just a shortcut command for compiling a binary and executing it. It should be equivalent to the following two commands:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>zig build
</span></span><span><span>./zig-out/bin/eth-zvm
</span></span></code></pre></div><p>How could an additional build step cause my program to run almost 10x <em>faster</em>?</p><h2 id="creating-a-minimal-reproduction-of-the-phenomenon">Creating a minimal reproduction of the phenomenon<a href="#creating-a-minimal-reproduction-of-the-phenomenon" arialabel="Anchor"> 🔗︎</a></h2><p>To debug the performance mystery, I tried simplifying my app until it was no longer a bytecode interpreter and was just a program that counted the number of bytes it read from stdin:</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>// src/main.zig
</span></span></span><span><span><span></span><span>
</span></span></span><span><span><span></span><span>const</span><span> </span>std<span> </span>=<span> </span><span>@import</span>(<span>"std"</span>);<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>pub</span><span> </span><span>fn</span><span> </span>countBytes(reader:<span> </span>anytype)<span> </span>!<span>u32</span><span> </span>{<span>
</span></span></span><span><span><span>    </span><span>var</span><span> </span>count:<span> </span><span>u32</span><span> </span>=<span> </span><span>0</span>;<span>
</span></span></span><span><span><span>    </span><span>while</span><span> </span>(<span>true</span>)<span> </span>{<span>
</span></span></span><span><span><span>        </span>_<span> </span>=<span> </span>reader.readByte()<span> </span><span>catch</span><span> </span>|err|<span> </span><span>switch</span><span> </span>(err)<span> </span>{<span>
</span></span></span><span><span><span>            </span><span>error</span>.EndOfStream<span> </span>=&gt;<span> </span>{<span>
</span></span></span><span><span><span>                </span><span>return</span><span> </span>count;<span>
</span></span></span><span><span><span>            </span>},<span>
</span></span></span><span><span><span>            </span><span>else</span><span> </span>=&gt;<span> </span>{<span>
</span></span></span><span><span><span>                </span><span>return</span><span> </span>err;<span>
</span></span></span><span><span><span>            </span>},<span>
</span></span></span><span><span><span>        </span>};<span>
</span></span></span><span><span><span>        </span>count<span> </span>+=<span> </span><span>1</span>;<span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>pub</span><span> </span><span>fn</span><span> </span>main()<span> </span>!<span>void</span><span> </span>{<span>
</span></span></span><span><span><span>    </span><span>var</span><span> </span>reader<span> </span>=<span> </span>std.io.getStdIn().reader();<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>var</span><span> </span>timer<span> </span>=<span> </span><span>try</span><span> </span>std.time.Timer.start();<span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span>start<span> </span>=<span> </span>timer.lap();<span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span>count<span> </span>=<span> </span><span>try</span><span> </span>countBytes(&amp;reader);<span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span>end<span> </span>=<span> </span>timer.read();<span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span>elapsed_micros<span> </span>=<span> </span><span>@as</span>(<span>f64</span>,<span> </span><span>@floatFromInt</span>(end<span> </span>-<span> </span>start))<span> </span>/<span> </span>std.time.ns_per_us;<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>const</span><span> </span>output<span> </span>=<span> </span>std.io.getStdOut().writer();<span>
</span></span></span><span><span><span>    </span><span>try</span><span> </span>output.print(<span>"bytes:           {}</span><span>\n</span><span>"</span>,<span> </span>.{count});<span>
</span></span></span><span><span><span>    </span><span>try</span><span> </span>output.print(<span>"execution time:  {d:.3}µs</span><span>\n</span><span>"</span>,<span> </span>.{elapsed_micros});<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>With the simplified app, I could still see the performance difference. When I ran the byte counter with <code>zig build run</code>, it ran in 13 microseconds:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ <span>echo</span> <span>'00010203040506070809'</span> | xxd -r -p | zig build run -Doptimize=ReleaseFast
</span></span><span><span>bytes:           <span>10</span>
</span></span><span><span>execution time:  13.549µs
</span></span></code></pre></div><p>When I ran the compiled binary directly, it took 12x as long to run, completing in 162 microseconds:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ <span>echo</span> <span>'00010203040506070809'</span> | xxd -r -p | ./zig-out/bin/count-bytes
</span></span><span><span>bytes:           <span>10</span>
</span></span><span><span>execution time:  162.195µs
</span></span></code></pre></div><p>My test consisted of three commands in a bash pipeline:</p><ol><li><code>echo</code> prints a sequence of ten hex-encoded bytes (<code>0x00</code>, <code>0x01</code>, …).</li><li><code>xxd</code> converts <code>echo</code>’s hex-encoded bytes to binary-encoded bytes.</li><li><code>zig build run</code> compiles and executes my byte counter program, counting the number of binary-encoded bytes that <code>xxd</code> emitted.</li></ol><p>The only difference between <code>zig build run</code> and <code>./zig-out/bin/count-bytes</code> was that the second command runs the already-compiled app, whereas the first one recompiles the app.</p><p>Again, I was dumbfounded.</p><p>How could does an extra compilation step make the program <em>faster</em>? Does a Zig app somehow run quicker when it’s fresh out of the oven?</p><p>At this point, I was stumped. I had read my source code over and over, and I couldn’t understand how compiling and running an application could be faster than running the already-compiled binary.</p><p>Zig is still a new language, so there had to be something about Zig I’d misunderstood. Surely, if experienced Zig programmers looked at my code, they’d spot my error instantly.</p><p>I <a href="https://ziggit.dev/t/zig-build-run-is-10x-faster-than-compiled-binary/3446?u=mtlynch">posted my question on Ziggit</a>, a discussion forum for Zig. The first few responses said I had a problem with “input buffering” but they didn’t have concrete suggestions to fix it or investigate further.</p><p>Andrew Kelly, Zig’s founder and lead developer made <a href="https://ziggit.dev/t/zig-build-run-is-10x-faster-than-compiled-binary/3446/8?u=mtlynch">a surprise appearance in the thread</a>. He couldn’t explain the phenomenon I was seeing, but he pointed out that I was making a different performance mistake:</p><p><a href="https://mtlynch.io/zig-extraneous-build/akelly-post.png"><img sizes="(min-width: 768px) 812px, 98vw" srcset="https://mtlynch.io/zig-extraneous-build/akelly-post_huf06dd251d6a987194ccda146e520380d_22912_300x0_resize_lanczos_3.png 300w,
https://mtlynch.io/zig-extraneous-build/akelly-post_huf06dd251d6a987194ccda146e520380d_22912_600x0_resize_lanczos_3.png 600w,
https://mtlynch.io/zig-extraneous-build/akelly-post_huf06dd251d6a987194ccda146e520380d_22912_800x0_resize_lanczos_3.png 800w,
https://mtlynch.io/zig-extraneous-build/akelly-post.png 812w" src="https://mtlynch.io/zig-extraneous-build/akelly-post.png" alt="Looks like you’re doing 1 syscall per byte read? That’s going to perform extremely poorly. My guess is that the extra steps of using the build system incidentally introduced some buffering. Not sure why though. The build system is making the child process inherit the file descriptors directly." loading="lazy"></a></p><p>Finally, my friend <a href="https://www.agwa.name/">Andrew Ayer</a> saw my post about this on Mastodon and <a href="https://m.mtlynch.io/@agwa@agwa.name/112039058255070708">solved the mystery</a>:</p><p><a href="https://mtlynch.io/zig-extraneous-build/agwa-masto.png"><img sizes="(min-width: 768px) 580px, 98vw" srcset="https://mtlynch.io/zig-extraneous-build/agwa-masto_hu3331ab99a5fa1e60d0e3808234226068_45232_300x0_resize_lanczos_3.png 300w,
https://mtlynch.io/zig-extraneous-build/agwa-masto.png 580w" src="https://mtlynch.io/zig-extraneous-build/agwa-masto.png" alt="Do you still see the 10x disparity with significantly larger inputs (i.e. > 1MB)? Do you still the disparity if you redirect stdin from a file instead of a pipe? My guess is that when you execute the program directly, xxd and count-bytes start at the same time, so the pipe buffer is empty when count-bytes first tries to read from stdin, requiring it to wait until xxd fills it. But when you use zig build run, xxd gets a head start while the program is compiling, so by the time count-bytes reads from stdin, the pipe buffer has been filled." loading="lazy"></a></p><p>Andrew Ayer got it exactly right, and I’ll break it down below.</p><p>Sidenote: Andrew Ayer also had the key insight that <a href="https://mtlynch.io/notes/picoshare-perf/#ram-bloat-is-fine-but-crashes-are-not">solved my last performance mystery</a>.</p><h2 id="my-mental-model-of-bash-pipelines-is-wrong">My mental model of bash pipelines is wrong<a href="#my-mental-model-of-bash-pipelines-is-wrong" arialabel="Anchor"> 🔗︎</a></h2><p>I had never thought too carefully about bash pipelines, but Andrew’s comment made me realize my mental model was wrong.</p><p>Imagine a simple bash pipeline like the following:</p><p>My mental model was that <code>jobA</code> would start and run to completion and then <code>jobB</code> would start with <code>jobA</code>’s output as its input.</p><figure><a href="https://mtlynch.io/zig-extraneous-build/jobs-serial.webp"><img sizes="(min-width: 768px) 572px, 98vw" srcset="https://mtlynch.io/zig-extraneous-build/jobs-serial_hu88d6b1301a67af8b3bbc95e6127380c6_1026_300x0_resize_q90_h2_lanczos_2.webp 300w,
https://mtlynch.io/zig-extraneous-build/jobs-serial.webp 570w" src="https://mtlynch.io/zig-extraneous-build/jobs-serial.webp" alt="Gantt chart of jobB starting after jobA finishes" loading="lazy"></a><figcaption><p>My incorrect mental model of how jobs in a bash pipeline work</p></figcaption></figure><p>It turns out that all commands in a bash pipeline start at the same time.</p><figure><a href="https://mtlynch.io/zig-extraneous-build/jobs-parallel.webp"><img sizes="(min-width: 768px) 572px, 98vw" srcset="https://mtlynch.io/zig-extraneous-build/jobs-parallel_hu94f5e650d86741a6659cd402fe2ceb69_3624_300x0_resize_q90_h2_lanczos_2.webp 300w,
https://mtlynch.io/zig-extraneous-build/jobs-parallel.webp 570w" src="https://mtlynch.io/zig-extraneous-build/jobs-parallel.webp" alt="Gantt chart of jobA and jobB starting simultaneously, but jobB is longer because it has to wait for jobA's results" loading="lazy"></a><figcaption><p>The actual way that jobs in a bash pipeline work</p></figcaption></figure><p>To demonstrate parallel execution in a bash pipeline, I wrote a proof of concept with two simple bash scripts.</p><p><code>jobA</code> starts, sleeps for three seconds, prints to stdout, sleeps for two more seconds, then exits:</p><div><div><pre tabindex="0"><code data-lang="bash"><span><span><span>#!/usr/bin/env bash
</span></span></span><span><span><span></span>
</span></span><span><span><span>function</span> print_status() {
</span></span><span><span>    <span>local</span> <span>message</span>=<span>"</span><span>$1</span><span>"</span>
</span></span><span><span>    <span>local</span> <span>timestamp</span>=<span>$(</span>date +<span>"%T.%3N"</span><span>)</span>
</span></span><span><span>    <span>echo</span> <span>"</span><span>$timestamp</span><span> </span><span>$message</span><span>"</span> &gt;&amp;<span>2</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span>print_status <span>'jobA is starting'</span>
</span></span><span><span>
</span></span><span><span>sleep <span>3</span>
</span></span><span><span>
</span></span><span><span><span>echo</span> <span>'result of jobA is...'</span>
</span></span><span><span>
</span></span><span><span>sleep <span>2</span>
</span></span><span><span>
</span></span><span><span><span>echo</span> <span>'42'</span>
</span></span><span><span>
</span></span><span><span>print_status <span>'jobA is terminating'</span>
</span></span></code></pre></div><p><a href="https://mtlynch.io/zig-extraneous-build/jobA" download="">download jobA</a></p></div><p><code>jobB</code> starts, waits for input on stdin, then prints everything it can read from stdin until stdin closes:</p><div><div><pre tabindex="0"><code data-lang="bash"><span><span><span>#!/usr/bin/env bash
</span></span></span><span><span><span></span>
</span></span><span><span><span>function</span> print_status() {
</span></span><span><span>    <span>local</span> <span>message</span>=<span>"</span><span>$1</span><span>"</span>
</span></span><span><span>    <span>local</span> <span>timestamp</span>=<span>$(</span>date +<span>"%T.%3N"</span><span>)</span>
</span></span><span><span>    <span>echo</span> <span>"</span><span>$timestamp</span><span> </span><span>$message</span><span>"</span> &gt;&amp;<span>2</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span>print_status <span>'jobB is starting'</span>
</span></span><span><span>
</span></span><span><span>print_status <span>'jobB is waiting on input'</span>
</span></span><span><span><span>while</span> <span>read</span> line; <span>do</span>
</span></span><span><span>  print_status <span>"jobB read '</span><span>${</span><span>line</span><span>}</span><span>' from input"</span>
</span></span><span><span><span>done</span> &lt; /dev/stdin
</span></span><span><span>print_status <span>'jobB is done reading input'</span>
</span></span><span><span>
</span></span><span><span>print_status <span>'jobB is terminating'</span>
</span></span></code></pre></div><p><a href="https://mtlynch.io/zig-extraneous-build/jobB" download="">download jobB</a></p></div><p>If I run <code>jobA</code> and <code>jobB</code> in a bash pipeline, exactly 5.009 seconds elapse between the <code>jobB is starting</code> and <code>jobB is terminating</code> messages:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ ./jobA | ./jobB
</span></span><span><span>09:11:53.326 jobA is starting
</span></span><span><span>09:11:53.326 jobB is starting
</span></span><span><span>09:11:53.328 jobB is waiting on input
</span></span><span><span>09:11:56.330 jobB <span>read</span> <span>'result of jobA is...'</span> from input
</span></span><span><span>09:11:58.331 jobA is terminating
</span></span><span><span>09:11:58.331 jobB <span>read</span> <span>'42'</span> from input
</span></span><span><span>09:11:58.333 jobB is <span>done</span> reading input
</span></span><span><span>09:11:58.335 jobB is terminating
</span></span></code></pre></div><p>If I adjust the execution so that <code>jobA</code> and <code>jobB</code> run in sequence instead of a pipeline, only 0.008 seconds elapse between <code>jobB</code>’s <code>starting</code> and <code>terminating</code> messages:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ ./jobA &gt; /tmp/output &amp;&amp; ./jobB &lt; /tmp/output
</span></span><span><span>16:52:10.406 jobA is starting
</span></span><span><span>16:52:15.410 jobA is terminating
</span></span><span><span>16:52:15.415 jobB is starting
</span></span><span><span>16:52:15.417 jobB is waiting on input
</span></span><span><span>16:52:15.418 jobB <span>read</span> <span>'result of jobA is...'</span> from input
</span></span><span><span>16:52:15.420 jobB <span>read</span> <span>'42'</span> from input
</span></span><span><span>16:52:15.421 jobB is <span>done</span> reading input
</span></span><span><span>16:52:15.423 jobB is terminating
</span></span></code></pre></div><h2 id="revisiting-my-byte-counter">Revisiting my byte counter<a href="#revisiting-my-byte-counter" arialabel="Anchor"> 🔗︎</a></h2><p>Once I understood that all commands in a bash pipeline run in parallel, the behavior I was seeing in my byte counter made sense:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ <span>echo</span> <span>'00010203040506070809'</span> | xxd -r -p | zig build run -Doptimize=ReleaseFast
</span></span><span><span>bytes:           <span>10</span>
</span></span><span><span>execution time:  13.549µs
</span></span><span><span>
</span></span><span><span>$ <span>echo</span> <span>'00010203040506070809'</span> | xxd -r -p | ./zig-out/bin/count-bytes
</span></span><span><span>bytes:           <span>10</span>
</span></span><span><span>execution time:  162.195µs
</span></span></code></pre></div><p>It looks like the time to run the <code>echo '00010203040506070809' | xxd -r -p</code> part of the pipeline takes about 150 microseconds. The <code>zig build run</code> step must take at least 150 microseconds.</p><p>By the time the <code>count-bytes</code> application actually begins in the <code>zig build</code> version, it doesn’t have to wait for the previous jobs to complete. The input is already waiting on stdin.</p><figure><a href="https://mtlynch.io/zig-extraneous-build/count-bytes-zig-run.webp"><img sizes="(min-width: 768px) 572px, 98vw" srcset="https://mtlynch.io/zig-extraneous-build/count-bytes-zig-run_hu6a9d20b88fe604bdafc0746c9fdac319_4000_300x0_resize_q90_h2_lanczos_2.webp 300w,
https://mtlynch.io/zig-extraneous-build/count-bytes-zig-run.webp 570w" src="https://mtlynch.io/zig-extraneous-build/count-bytes-zig-run.webp" alt="Gantt chart where echo, xxd, and zig build run start at the same time, but the execute phase of zig build run starts after echo and xxd are complete" loading="lazy"></a><figcaption><p>With <code>zig build run</code>, there’s a delay before my application executes, so previous jobs in the pipeline have already completed by the time <code>count-bytes</code> starts.</p></figcaption></figure><p>When I skip the <code>zig build</code> step and run the compiled binary directly, <code>count-bytes</code> starts immediately and the timer begins. The problem is that <code>count-bytes</code> has to sit around waiting ~150 microseconds for the <code>echo</code> and <code>xxd</code> commands to deliver input to stdin.</p><figure><a href="https://mtlynch.io/zig-extraneous-build/count-bytes-compiled.webp"><img sizes="(min-width: 768px) 572px, 98vw" srcset="https://mtlynch.io/zig-extraneous-build/count-bytes-compiled_hu57b45cd7d5e7b5c80beccb83ecea8677_5502_300x0_resize_q90_h2_lanczos_2.webp 300w,
https://mtlynch.io/zig-extraneous-build/count-bytes-compiled.webp 570w" src="https://mtlynch.io/zig-extraneous-build/count-bytes-compiled.webp" alt="Gantt chart where echo, xxd, and count-bytes all start at the same time, but count-bytes can't begin processing input until 150 microseconds after starting, as it's waiting on results from xxd" loading="lazy"></a><figcaption><p>When I run <code>count-bytes</code> directly, it has to wait around for ~150 microseconds until <code>echo</code> and <code>xxd</code> feed input to stdin.</p></figcaption></figure><h2 id="fixing-my-benchmark">Fixing my benchmark<a href="#fixing-my-benchmark" arialabel="Anchor"> 🔗︎</a></h2><p>Fixing my benchmark was <a href="https://github.com/mtlynch/eth-zvm/pull/27">simple</a>. Instead of running my application as part of a bash pipeline, I split the preparation stage and the execution stage into separate commands:</p><div><pre tabindex="0"><code data-lang="bash"><span><span><span># Convert the hex-encoded input to binary encoding.</span>
</span></span><span><span>$ <span>INPUT_FILE_BINARY</span>=<span>"</span><span>$(</span>mktemp<span>)</span><span>"</span>
</span></span><span><span>$ <span>echo</span> <span>'60016000526001601ff3'</span> | xxd -r -p &gt; <span>"</span><span>${</span><span>INPUT_FILE_BINARY</span><span>}</span><span>"</span>
</span></span><span><span>
</span></span><span><span><span># Read the binary-encoded input into the virtual machine.</span>
</span></span><span><span>$ ./zig-out/bin/eth-zvm &lt; <span>"</span><span>${</span><span>INPUT_FILE_BINARY</span><span>}</span><span>"</span>
</span></span><span><span>execution time:  67.378µs
</span></span></code></pre></div><p>My benchmark dropped from the 438 microseconds I was seeing before down to just 67 microseconds.</p><figure><figcaption><p>Difference in measured performance of my Zig app after I fixed my benchmarking script</p></figcaption></figure><h2 id="applying-andrew-kellys-performance-fix">Applying Andrew Kelly’s performance fix<a href="#applying-andrew-kellys-performance-fix" arialabel="Anchor"> 🔗︎</a></h2><p>Recall that Andrew Kelly <a href="https://ziggit.dev/t/zig-build-run-is-10x-faster-than-compiled-binary/3446/8?u=mtlynch">pointed out</a> that I was doing one syscall for every byte I read.</p><div><pre tabindex="0"><code data-lang="zig"><span><span><span>var</span><span> </span>reader<span> </span>=<span> </span>std.io.getStdIn().reader();<span>
</span></span></span><span><span><span></span>...<span>
</span></span></span><span><span><span></span><span>while</span><span> </span>(<span>true</span>)<span> </span>{<span>
</span></span></span><span><span><span>      </span>_<span> </span>=<span> </span>reader.readByte()<span> </span>{<span> </span><span>// Slow! One syscall per byte
</span></span></span><span><span><span></span><span>          </span>...<span>
</span></span></span><span><span><span>      </span>};<span>
</span></span></span><span><span><span>      </span>...<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span></code></pre></div><p>So, every time my application called <code>readByte</code> in the loop, it had to halt execution, request an input read from the OS and then resume when the OS delivered the single byte.</p><p>The fix <a href="https://github.com/mtlynch/eth-zvm/pull/26">was simple</a>. I had to use a buffered reader. Instead of reading a single byte at a time from the OS, I’d use Zig’s built-in <code>std.io.bufferedReader</code>, which causes my application to read large chunks of data from the OS. That way, I only have to make a fraction of the syscalls.</p><p>Here’s the entire change:</p><div><pre tabindex="0"><code data-lang="diff"><span><span><span>diff --git a/src/main.zig b/src/main.zig
</span></span></span><span><span><span>index d6e50b2..a46f8fa 100644
</span></span></span><span><span><span></span><span>--- a/src/main.zig
</span></span></span><span><span><span></span><span>+++ b/src/main.zig
</span></span></span><span><span><span></span><span>@@ -7,7 +7,9 @@ pub fn main() !void {
</span></span></span><span><span><span></span>     const allocator = gpa.allocator();
</span></span><span><span>     defer _ = gpa.deinit();
</span></span><span><span>
</span></span><span><span><span>-    var reader = std.io.getStdIn().reader();
</span></span></span><span><span><span></span><span>+    const in = std.io.getStdIn();
</span></span></span><span><span><span>+    var buf = std.io.bufferedReader(in.reader());
</span></span></span><span><span><span>+    var reader = buf.reader();
</span></span></span><span><span><span></span>
</span></span><span><span>     var evm = vm.VM{};
</span></span><span><span>     evm.init(allocator);
</span></span></code></pre></div><p>I re-ran my example, and it sped up performance by another 11 microseconds, a modest 16% speedup.</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ zig build -Doptimize=ReleaseFast &amp;&amp; ./zig-out/bin/eth-zvm &lt; <span>"</span><span>${</span><span>INPUT_FILE_BINARY</span><span>}</span><span>"</span>
</span></span><span><span>execution time:  56.602µs
</span></span></code></pre></div><figure><figcaption><p>Buffering input reads increased performance by another 16%.</p></figcaption></figure><h2 id="benchmarking-a-larger-input">Benchmarking a larger input<a href="#benchmarking-a-larger-input" arialabel="Anchor"> 🔗︎</a></h2><p>My Ethereum interpreter currently only supports a small subset of Ethereum’s opcodes. The most complex computation my interpreter can do at this point is add numbers together.</p><p>For example, here’s an Ethereum application that counts to three by pushing <code>1</code> to the stack three times and then adding the values together:</p><div><pre tabindex="0"><code data-lang="text"><span><span>PUSH1 1    # Stack now contains [1]
</span></span><span><span>PUSH1 1    # Stack now contains [1, 1]
</span></span><span><span>PUSH1 1    # Stack now contains [1, 1, 1]
</span></span><span><span>ADD        # Stack now contains [2, 1]
</span></span><span><span>ADD        # Stack now contains [3]
</span></span></code></pre></div><p>The largest application I tested in my benchmarks was Ethereum bytecode that counted to 1,000 by adding <code>1</code> values together.</p><p>After Andrew Kelly’s tip helped me <a href="https://github.com/mtlynch/eth-zvm/pull/26">reduce syscalls</a>, my “count to 1,000” application’s runtime dropped from 2,024 microseconds to just 58 microseconds, a 35x speedup. I was now beating the official Ethereum implementation by almost a factor of two.</p><figure><figcaption><p>Buffering my input reads allowed my Zig implementation to run about 2x faster than the official Ethereum implementation on the largest Ethereum application in my test set.</p></figcaption></figure><h2 id="cheating-my-way-to-maximum-performance">Cheating my way to maximum performance<a href="#cheating-my-way-to-maximum-performance" arialabel="Anchor"> 🔗︎</a></h2><p>I was excited to see my Zig implementation finally outperforming the official Go version, but I wanted to see just how much I could leverage Zig to improve performance.</p><p>One common bottleneck in software is memory allocation. The program has to stop and wait for the OS to allocate RAM, which may involve shuffling around data to find enough contiguous space.</p><p>Zig has a memory allocator called the fixed buffer allocator. Instead of the memory allocator requesting memory from the OS, you provide the allocator a fixed buffer of bytes, and it uses only those bytes to allocate memory.</p><p>I can cheat my benchmarks by compiling a version of my Ethereum interpreter that’s limited to 2 KB of memory allocated from the stack:</p><div><pre tabindex="0"><code data-lang="diff"><span><span><span>diff --git a/src/main.zig b/src/main.zig
</span></span></span><span><span><span>index a46f8fa..9e462fe 100644
</span></span></span><span><span><span></span><span>--- a/src/main.zig
</span></span></span><span><span><span></span><span>+++ b/src/main.zig
</span></span></span><span><span><span></span><span>@@ -3,9 +3,9 @@ const stack = @import("stack.zig");
</span></span></span><span><span><span></span> const vm = @import("vm.zig");
</span></span><span><span>
</span></span><span><span> pub fn main() !void {
</span></span><span><span><span>-    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
</span></span></span><span><span><span>-    const allocator = gpa.allocator();
</span></span></span><span><span><span>-    defer _ = gpa.deinit();
</span></span></span><span><span><span></span><span>+    var buffer: [2000]u8 = undefined;
</span></span></span><span><span><span>+    var fba = std.heap.FixedBufferAllocator.init(&amp;buffer);
</span></span></span><span><span><span>+    const allocator = fba.allocator();
</span></span></span><span><span><span></span>
</span></span><span><span>     const in = std.io.getStdIn();
</span></span><span><span>     var buf = std.io.bufferedReader(in.reader());
</span></span></code></pre></div><p>I call this a “cheat” as I’m optimizing for my specific benchmarks. There are certainly valid Ethereum programs that require more than 2 KB of memory, but I’m just curious how fast I can go with this optimization.</p><p>Let’s see what performance looks like if I know my max memory requirement at compile time:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ ./zig-out/bin/eth-zvm &lt; <span>"</span><span>${</span><span>COUNT_TO_1000_INPUT_BYTECODE_FILE</span><span>}</span><span>"</span>
</span></span><span><span>execution time:  34.4578µs
</span></span></code></pre></div><p>Cool! With a fixed memory buffer, my Ethereum implementation runs my “count to 1,000” bytecode in 34 microseconds, nearly 3x faster than the official Go implementation.</p><figure><figcaption><p>If I know the maximum memory requirements of my Ethereum interpreter at compile time, I can outperform the official implementation by 3x.</p></figcaption></figure><h2 id="conclusion">Conclusion<a href="#conclusion" arialabel="Anchor"> 🔗︎</a></h2><p>My takeaway from this experience is to benchmark performance early and often.</p><p>By adding a benchmarking script to my continuous integration and archiving the results, it was easy for me to identify when my measurements changed. Had I relegated benchmarking to a manual, periodic task, it would have been difficult for me to identify exactly what caused the difference in my measurements.</p><p>This experience also underscores the importance of understanding your metrics. Before hitting this bug, I hadn’t considered that my benchmark included the time waiting for other processes to fill stdin.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Regex character "$" doesn't mean "end-of-string" (296 pts)]]></title>
            <link>https://sethmlarson.dev/regex-$-matches-end-of-string-or-newline</link>
            <guid>39763750</guid>
            <pubDate>Wed, 20 Mar 2024 07:50:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sethmlarson.dev/regex-$-matches-end-of-string-or-newline">https://sethmlarson.dev/regex-$-matches-end-of-string-or-newline</a>, See on <a href="https://news.ycombinator.com/item?id=39763750">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>This article is about a bit of surprising behavior I recently discovered
using Python's regex module (<code>re</code>) while <a href="https://github.com/python/release-tools/pull/92#discussion_r1484470272">developing SBOM tooling for CPython</a>.</p>

<p>Folks who've worked with regular expressions before might know about <code>^</code> meaning "start-of-string"
and correspondingly see <code>$</code> as "end-of-string". So the pattern <code>cat$</code> would match the string <code>"lolcat"</code> but not <code>"internet cat video"</code>.</p>

<p>The behavior of <code>^</code> made me think that <code>$</code> was similar, but they aren't always symmetrical
and the behavior is <em>platform-dependent</em>. Specifically for Python with multiline mode <em>disabled</em>
the <code>$</code> character can match either the end of a string <em>or a trailing newline before the end of a string</em>.</p>

<p>So if you're trying to match a string without a newline at the end, <strong>you can't only use <code>$</code> in Python!</strong>
My expectation was having multiline mode disabled wouldn't have had this newline-matching behavior, but that isn't the case.</p>

<p>Next logical question is how does one match the end of a string without a newline in Python?</p>

<p>After doing more research on <a href="https://docs.python.org/3/library/re.html#regular-expression-syntax">Python</a>
and <a href="https://www.regular-expressions.info/anchors.html">other regular expression syntaxes</a>
I also found <code>\z</code> and <code>\Z</code> as candidates for "end-of-string" characters.</p>

<p>Multi-line mode is enabled with <a href="https://docs.python.org/3/library/re.html#re.MULTILINE"><code>re.MULTILINE</code></a> in Python, the docs have the following to say:</p>

<blockquote>
  <p>When <code>re.MULTILINE</code> is specified the pattern character '$' matches at the end of the string and at the end of each
  line (immediately preceding each newline). By default, '$' only matches at the end of the string and immediately before the newline (if any) at the end of the string.</p>
</blockquote>

<p>Let's see how these features work together across multiple platforms:</p>

<table>
<thead>
<tr>
  <th>Pattern matches <code>"cat\n"</code>?</th>
  <th><code>"cat$"</code> multiline</th>
  <th><code>"cat$"</code> no multiline</th>
  <th><code>"cat\z"</code></th>
  <th><code>"cat\Z"</code></th>
</tr>
</thead>
<tbody>
<tr>
  <td>PHP</td>
  <td>✅</td>
  <td>✅</td>
  <td>❌</td>
  <td>✅</td>
</tr>
<tr>
  <td>ECMAScript</td>
  <td>✅</td>
  <td>❌</td>
  <td>⚠️</td>
  <td>⚠️</td>
</tr>
<tr>
  <td>Python</td>
  <td>✅</td>
  <td>✅</td>
  <td>⚠️</td>
  <td>❌</td>
</tr>
<tr>
  <td>Golang</td>
  <td>✅</td>
  <td>❌</td>
  <td>❌</td>
  <td>⚠️</td>
</tr>
<tr>
  <td>Java 8</td>
  <td>✅</td>
  <td>✅</td>
  <td>❌</td>
  <td>✅</td>
</tr>
<tr>
  <td>.NET 7.0</td>
  <td>✅</td>
  <td>✅</td>
  <td>❌</td>
  <td>✅</td>
</tr>
<tr>
  <td>Rust</td>
  <td>✅</td>
  <td>❌</td>
  <td>❌</td>
  <td>⚠️</td>
</tr>
</tbody>
</table>

<ul>
<li>✅: Pattern matches the string <code>"cat\n"</code></li>
<li>❌: Pattern does not match the string <code>"cat\n"</code></li>
<li>⚠️: Pattern is invalid or character not supported.</li>
</ul>

<p>Summarizing the above table, if matching a trailing newline is acceptable then <code>$</code> with multiline mode works consistently across all platforms,
but if we wanted to <em>not match</em> a trailing newline then things get more complicated.</p>

<p>To not match a trailing newline, use <code>\z</code> on all platforms except Python and
ECMAScript where you'll need to use <code>\Z</code> or <code>$</code> without multiline mode respectively.
Hope you learned something about regular expressions today!</p>

<p>Note: The table of data was gathered from <a href="https://regex101.com/">regex101.com</a>, I didn't test using the actual runtimes.</p>

<blockquote>
    <p>
        <strong>Thanks for reading!</strong> ♡ Did you find this article helpful and want more content like it?
        <nobr>Get notified of new posts</nobr> by subscribing to the <a href="https://sethmlarson.dev/feed">RSS feed</a> or the <a href="https://buttondown.email/sethmlarson">email newsletter</a>.
    </p>
    
</blockquote>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EFF: Tell Congress: We Can't Afford More Bad Patents (128 pts)]]></title>
            <link>https://act.eff.org/action/tell-congress-we-can-t-afford-more-bad-patents</link>
            <guid>39763104</guid>
            <pubDate>Wed, 20 Mar 2024 05:28:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://act.eff.org/action/tell-congress-we-can-t-afford-more-bad-patents">https://act.eff.org/action/tell-congress-we-can-t-afford-more-bad-patents</a>, See on <a href="https://news.ycombinator.com/item?id=39763104">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <h5>Creativity &amp; Innovation</h5>
        
        <p><img src="https://s3-us-west-1.amazonaws.com/actioncenter/action_pages/featured_images/000/000/554/original/patent-troll-warning.png?1710277642" alt="Patent troll warning"></p><p>Congress is pushing two bills that would bring back some of the worst patents and empower patent trolls.</p>

<p>The Patent Eligibility Restoration Act (PERA), S. 2140, would throw out crucial rules that ban patents on many abstract ideas. Courts will be ordered to approve patents on things like ordering food on a mobile phone or doing basic financial functions online. If PERA Passes, the floodgates will open for these vague software patents that will be used to sue small companies and individuals. This bill even allows for a type of patent on human genes that the Supreme Court rightly disallowed in 2013.</p>

<p>A second bill, the PREVAIL Act, S. 2220, would sharply limit the public’s right to challenge patents that never should have been granted in the first place.</p>

        


          <p>
          <label for="learn-more">Learn More</label></p><div id="description">
                <p>Patent trolls—companies that have no product or service of their own, but simply make patent infringement demands on others—are a big problem. They’ve cost our economy billions of dollars. For a small company, a patent troll demand letter can be ruinous.</p>

<p>We took a big step towards fighting off patent trolls in 2014, when a landmark Supreme Court ruling, the Alice Corp. v. CLS Bank case, established that you can’t get a patent by adding “on a computer” to an abstract idea. In 2012, Congress also expanded the ways that a patent can be challenged at the patent office.</p>

<p>These two bills, PERA and PREVAIL, would roll back both of those critical protections against patent trolls. We know that the bill sponsors, Sens. Thom Tillis (R-NC) and Chris Coons (D-DE) are pushing hard for these bills to move forward. We need your help to tell Congress that it’s the wrong move.</p>

              </div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Improved My Rust Compile Times by 75% (137 pts)]]></title>
            <link>https://benw.is/posts/how-i-improved-my-rust-compile-times-by-seventy-five-percent</link>
            <guid>39762807</guid>
            <pubDate>Wed, 20 Mar 2024 04:14:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://benw.is/posts/how-i-improved-my-rust-compile-times-by-seventy-five-percent">https://benw.is/posts/how-i-improved-my-rust-compile-times-by-seventy-five-percent</a>, See on <a href="https://news.ycombinator.com/item?id=39762807">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><img src="https://benwis.imgix.net/girl_waiting_code_compile.png?auto=format&amp;auto=compress" alt="A woman with a pixie haircut looking at a computer in an office environment, waiting for her code to compile."><span>A woman with a pixie haircut looking at a computer in an office environment, waiting for her code to compile.</span><div><blockquote>
<p>If you're looking for a talented Rust developer, or a good senior software engineer, please reach out. I'm looking for a full time role doing something neat, and am available for contract gigs. Contact info is in the footer. Thank you!</p>
</blockquote>
<blockquote>
<p>There's now a Part 2, where I cover a couple more options. Check it out <a href="https://benw.is/posts/how-i-improved-my-rust-compile-times-part2">here</a></p>
</blockquote>
<p>One of Rust's often mentioned pain points is slow compile times. In order to have nice things like the borrow checker, safety, and zero cost abstractions, we pay in time spent compiling. Web developers, especially those that come from one of the Javascript frameworks, really, really like fast reload times. I was fairly active in the Remix discord early on, and the number of people who asked about HMR(Hot Module Reloading) <a href="https://github.com/remix-run/remix/discussions/2384">was insane</a>. Hot Module Reloading is when the web framework will replaces javascript, html, and css modules in an existing page as they are changed without losing page state. The alternative, Live Page Reloading, will trigger a page reload when a change is detected. For Remix, the time difference between the two is signifigantly less than it would be in Rust, where we'd have to recompile.</p>
<p>You can imagine, as a Leptos team member, we hear a lot of the same discussion. Leptos, as a frontend Rust web framework, sits at the intersection of these two worlds We see it on the <a href="https://discord.gg/x8NhWWYTV2">Leptos Discord</a> all the time.</p>
<blockquote>
<p>"Now, if only rebuild was faster… 🫣"</p>
</blockquote>
<blockquote>
<p>"Anyone have some tips to improve compile times? Leptos is amazing but having to wait 5-10 seconds to render an extra div is very annoying"</p>
</blockquote>
<p>So Leptos basically has two approaches to solve this, improve Rust's compile time itself, and/or come up with a way to output changes to parts of the page without needing to recompile first. In this piece, we'll explore both.</p>

                        <h2>
                            <a id="improving-rust-compile-times" href="#improving-rust-compile-times">
                                Improving Rust Compile Times
                            </a>
                        </h2>
                        
<p>In order to test different changes, we need to establish a baseline. In default Rust, on my machine, how long will it take to compile my web app?</p>

                        <h3>
                            <a id="the-test-machines" href="#the-test-machines">
                                The Test Machines
                            </a>
                        </h3>
                        
<p>I have a quite beefy test machine on which I usually compile. On my machine I'll be compiling a simpler project, and my friend Alex kindly agreed to run test his startup Houski's <a href="https://www.houski.ca/">site</a>,</p>

                        <h3>
                            <a id="my-machine" href="#my-machine">
                                My Machine
                            </a>
                        </h3>
                        
<ul>
<li>AMD 5950x processor, </li>
<li>72GB RAM</li>
<li>SATA SSD system drive. </li>
<li>7200RPM spinning disk storage drives</li>
<li>NVME drives</li>
<li>NixOS linux distro</li>
<li>Rust 1.75 nightly</li>
</ul>

                        <h3>
                            <a id="houski-s-machine" href="#houski-s-machine">
                                Houski's Machine
                            </a>
                        </h3>
                        
<ul>
<li>AMD 7900x processor</li>
<li>128GB RAM</li>
<li>NVME drive</li>
<li>Ubuntu linux distro</li>
<li>Rust 1.75 nightly</li>
</ul>

                        <h3>
                            <a id="leptos-build-process" href="#leptos-build-process">
                                Leptos Build Process
                            </a>
                        </h3>
                        
<p>A Leptos web app typicallys undergoes a two stage build process. </p>
<ol>
<li>Build Webassembly frontend module using cargo. Optimize and package it with warm-bindgen.</li>
<li>Build the server binary with cargo.</li>
</ol>
<p>I chose to benchmark the whole process for most of these demos, but since it's essentially two cargo steps, any relative changes would affect any cargo command.</p>
<blockquote>
<p>Note: Discord User @PaulH(and a couple others) let me know about this <a href="https://github.com/leptos-rs/cargo-leptos/pull/203">bug</a> that prevents the server build from using the dependencies built for the previous compilation of the server/webassembly frontend build due to changes in the RUSTFLAGS. You can fix this by having cargo-leptos &gt; 0.2.1 and adding this to you Cargo.toml under your Leptos settings block. I did not test with this enabled.</p>
<div><p>TOML markup</p><pre data-lang="toml"><i>[</i><i>package</i><i>.</i><i>metadata</i><i>.</i><i>leptos</i><i>]</i>
<i>separate-front-target-dir</i> <i>=</i> <i>true</i>
</pre></div></blockquote>

                        <h2>
                            <a id="optimizations" href="#optimizations">
                                Optimizations
                            </a>
                        </h2>
                        
                        <h3>
                            <a id="optimization-level" href="#optimization-level">
                                Optimization Level
                            </a>
                        </h3>
                        
<p>This advice comes from Bevy, which recommends setting the opt-level for development higher in order to possibly reduce dev compile times and improve performance. By default, the Rust compiler sets an opt-level of 0 for development builds. We're going to give it an opt-level of 1 for our code, and an opt-level of 3 for all the dependencies of our code.</p>
<p>This does come with the drawback of much less useful error messages if they come from our dependencies. So you might have to adjust the levels if you run into tricky bugs.</p>
<p>Since optimization takes additional time, I expect the clean compile times to increase, but we may see the increased optimizations make the code easier to compile in incremental builds. We also had some positive anecdotes on our Discord.</p>
<blockquote>
<p>"fwiw @gbj @Alex Wilkinson i dropped my compile time from between 5-30 minutes to 6 seconds by putting this in my workspace Cargo.toml</p>
</blockquote>
<p>We can enable adding these blocks to our <code>Cargo.toml</code> file. Nice and easy.</p>
<div><p>TOML markup</p><pre data-lang="toml"><i>[</i><i>profile</i><i>.</i><i>dev</i><i>]</i>
<i>opt-level</i> <i>=</i> 1
<i>[</i><i>profile</i><i>.</i><i>dev</i><i>.</i><i>package</i><i>.</i><i>"*"</i><i>]</i>
<i>opt-level</i> <i>=</i> 3
</pre></div>
                        <h3>
                            <a id="mold" href="#mold">
                                Mold
                            </a>
                        </h3>
                        
<p>For those unfamiliar with how what goes on inside the Rust compiler, it typically follows a few basic steps. It reads in source code, which is converted to a variety of types of IR(Intermediate Representation), and optimizations are performed during that conversion. Then that IR is fed to a code generator, provided by LLVM, which converts the IR into object files, and then the linker links together those object files and other system libs into one executable binary. Way more details about how that works can be found <a href="https://rustc-dev-guide.rust-lang.org/overview.html">here</a>. Fascinating reading, but a bit too deep for our discussion here.</p>
<p><a href="https://github.com/rui314/mold">Mold</a> is a new linker developed by Rui Ueyama, with the goal of increasing linker performance by parallelizing the load as much as possible, and benchmarks have shown it to be signifigantly faster than Rust's default linker.</p>
<p>For Linux and Mac, the default linker is ld, run by cc. Windows is a different story, using Microsoft's MVC link.exe. If you're running Linux, you can use mold directly. If you're on Mac, a paid version of mold called Sold is available for Mac. If Mold generates benefits for you, I encourage you to buy Sold(very affordable) or sponsor Rui on his Github Sponsors <a href="https://github.com/sponsors/rui314">page</a>. Windows users, unforunately, are not supported at this time. Support for Windows in Sold is in development.</p>
<p>A fairly signifigant amount of time is spent linking your Rust binary, especially during incremental builds, so this could provide some real benefits.</p>
<p>On Linux, it's actually really easy to use, just <a href="https://github.com/rui314/mold">install Mold</a> and then prepend you cargo commands with <code>mold -run</code>. For example, <code>mold -run cargo build</code>. It can also be enabled in <code>.cargo/config.toml</code>, like this:</p>
<div><p>TOML markup</p><pre data-lang="toml"><i>[</i><i>target</i><i>.</i><i>x86_64-unknown-linux-gnu</i><i>]</i>
<i>linker</i> <i>=</i> <i>"clang"</i>
<i>rustflags</i> <i>=</i> <i>[</i><i>"-C"</i><i>,</i> <i>"link-arg=-fuse-ld=/path/to/mold"</i><i>]</i>
</pre></div>
<p>where <code>/path/to/mold</code> is an absolute path to the mold executable. This is also how you enable Sold, just replace the mold path with the sold path and the target to the one for your Mac.</p>

                        <h3>
                            <a id="cranelift" href="#cranelift">
                                Cranelift
                            </a>
                        </h3>
                        
<p>In the previous optimization, we replaced the linker the Rust compiler uses. Let's try replacing the code generator, Cranelift is an alternative code generator, used instead of LLVM in the build step. While it's not good at doing as many optimizations as LLVM, it is good at spitting out code fast. It was recently integrated as an option for code generation in Rust 1.73 nightly for x86_64 linux targets. Other platforms will need to setup cranelift seperately, see their <a href="https://github.com/rust-lang/rustc_codegen_cranelift">README</a>.</p>
<div><p>bash</p><pre data-lang="bash">rustup update nightly #install nightly if you haven't already
rustup component add rustc-codegen-cranelift-preview --toolchain nightly
</pre></div>
<p>To use it with Cargo, it can be enabled by enabling the unstable <code>codegen-backend</code> feature, and then setting the <code>codegen-backend= "cranelift"</code> value for a profile. That can be done in <code>.cargo/config.toml</code> like so:</p>
<div><p>TOML markup</p><pre data-lang="toml"><i>[</i><i>unstable</i><i>]</i>
<i>codegen-backend</i> <i>=</i> <i>true</i>
<i>[</i><i>profile</i><i>.</i><i>server-dev</i><i>]</i>
<i>codegen-backend</i> <i>=</i> <i>"cranelift"</i>
</pre></div>
<p>If you don't need to worry about multi-target builds, you can also enable it below with a target. This does conflict with the desire to not use cranelift during production builds, so I wouldn't recommend it. Rust doesn't support per target profile builds, so creating your own profile is a lot more flexible.</p>
<div><p>TOML markup</p><pre data-lang="toml"><i>[</i><i>target</i><i>.</i><i>x86_64-unknown-linux-gnu</i><i>]</i>
<i>rustflags</i> <i>=</i> <i>[</i><i>"-Zcodegen-backend=cranelift"</i><i>]</i>
</pre></div>
<p>Keep in mind that Cranelift is still in development, and may have some issues with missing intrinsics. So your crate may or may not work in it natively. If you do find missing intrinsics, I encourage you to create an issue in their repo <a href="https://github.com/rust-lang/rustc_codegen_cranelift">here</a>, there may be a workaround available.</p>

                        <h3>
                            <a id="methodology" href="#methodology">
                                Methodology
                            </a>
                        </h3>
                        
<p>I'm primarily interested in optimizing build times for Rust projects in development. This means I'm not concerned about filesize, optimizations, or run speed, as long as they are not signifigantly affected. We're interested in the time it takes to build our Rust Leptos apps, both from a clean state, and with incremental compilation.</p>
<p>To do that we're going to use <a href="https://github.com/sharkdp/hyperfine">hyperfine</a> a command line benchmarking tool, to run 'cargo leptos build', which as described earlier does Leptos's two step compilation. For clean builds, we will run <code>cargo clean</code> before each build, in order to delete any files cached by Rust. Incremental builds are a bit harder.</p>
<p>For incremental builds, we'll simulate a developer modifying a single HTML tag in a Leptos component. To do that, we'll run our handy unix friend <code>sed</code> to insert the current date and time into an HTML tag. For this purpose, I've chosen the <code>&lt;dfn&gt;</code> tag, because I've never seen it used, and thus I probably don't have to worry about multiple replacements tags. After some finagling, I ended up with this sed command:</p>
<div><p>bash</p><pre data-lang="bash">sed -i -e "s|&lt;dfn&gt;[^&lt;]*&lt;/dfn&gt;|&lt;dfn&gt;$(date +%m%s)&lt;/dfn&gt;|g" src/routes/index.rs
</pre></div>
<p>Each configuration is tested six times, with two warmup runs to fill any caches and reduce variability in the output. Without the warmup runs variance was a bit wider, but the results were still consistent. Six runs is probably overkill for this test, but what can I say, I got CPU time for days, or more accurately nights.</p>
<p>We'll measure the time it takes to complete six runs, with two warmup runs to setup filesystem caches, and hopefully provide consistent results.</p>
<p>I will be compiling my <a href="https://benwis/">site</a>, and Alex will be compiling <a href="https://houski.ca/">Houski's site</a>. Houski's is a lot more complicated than this blog, with heavy usage of Polars, Serialization/Deserialization with Serde, and plenty more routes. Neither of us has tried to reduce these times much or done any special kinds of optimization.</p>

                        <h2>
                            <a id="it-s-benchmarking-time" href="#it-s-benchmarking-time">
                                It's Benchmarking Time
                            </a>
                        </h2>
                        
                        <h3>
                            <a id="baseline" href="#baseline">
                                Baseline
                            </a>
                        </h3>
                        
<p>For both of us, these will be the times to beat.</p>
<table><thead><tr><th>Clean Compilation</th><th></th><th></th></tr></thead><tbody>
<tr><td>Site</td><td>Mean(s)</td><td>Standard Deviation(s)</td></tr>
<tr><td>benw.is 7200RPM</td><td>88.387</td><td>1.817</td></tr>
<tr><td>benw.is NVME</td><td>80.057</td><td>0.268</td></tr>
<tr><td>houski.ca</td><td>124.993</td><td>0.483</td></tr>
</tbody></table>
<table><thead><tr><th>Incremental Compilation</th><th></th><th></th></tr></thead><tbody>
<tr><td>Site</td><td>Mean(s)</td><td>Standard Deviation(s)</td></tr>
<tr><td>benw.is 7200RPM</td><td>20.461</td><td>0.801</td></tr>
<tr><td>benw.is NVME</td><td>19.097</td><td>0.078</td></tr>
<tr><td>houski.ca</td><td>40.818</td><td>0.252</td></tr>
</tbody></table>
<p>Not a bad baseline. Interesting to me was the variation in the 7200RPM build time for both clean and incremental builds, which might suggest that the drive was struggling to provide the data at the same rate consistently, or perhaps something else was contending with the benchmark for IO access.</p>
<p>Twenty or forty seconds is a pretty substantial time to rebuild the site in the dev profile, hopefully we can improve that.</p>

                        <h3>
                            <a id="mold-enabled" href="#mold-enabled">
                                Mold Enabled
                            </a>
                        </h3>
                        
<p>Let's enable the Mold linker as described earlier, and see how that changes things!</p>
<table><thead><tr><th>Clean Compilation</th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>Site</td><td>Mean(s)</td><td>Standard Deviation(s)</td><td>Delta from Baseline(s)</td><td>% Delta from Baseline</td></tr>
<tr><td>benw.is 7200RPM</td><td>74.35</td><td>0.96</td><td>14.037</td><td>15.88129476</td></tr>
<tr><td>benw.is NVME</td><td>67.206</td><td>0.386</td><td>12.851</td><td>16.05231273</td></tr>
<tr><td>houski.ca</td><td>102.892</td><td>0.416</td><td>22.101</td><td>17.68179018</td></tr>
</tbody></table>
<table><thead><tr><th>Incremental Compilation</th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>Site</td><td>Mean(s)</td><td>Standard Deviation(s)</td><td>Delta from Baseline(s)</td><td>% Delta from Baseline</td></tr>
<tr><td>benw.is 7200RPM</td><td>5.842</td><td>0.268</td><td>14.619</td><td>71.44812082</td></tr>
<tr><td>benw.is NVME</td><td>5.615</td><td>0.051</td><td>13.482</td><td>70.59747604</td></tr>
<tr><td>houski.ca</td><td>19.6</td><td>0.067</td><td>21.218</td><td>51.98196874</td></tr>
</tbody></table>
<p>Woah! That's quite a bit faster. For my site, clean compilation time decreased by 12.04s(13.6%) on spinning disk and  12.80s(16.0%) for NVME. Houski dropped 22.10s(17.7s). Nice boost there.</p>
<p>Incremental compilation times changed signifigantly as well. My site on spinning disk dropped 14.62s(71.4%) and 13.5s(70.6%). That's a radical reduction, which makes some sense. Incremental compilation spends a majority of it's time in the linking step, so any upgrades there will disproportionally affect it. </p>
<p>As far as Leptos users go, mold only supports compiling for x84_64_linux, so the webassembly compile time remained unchanged. Most of these optimizations only affect the server build, but I will be careful to note which ones don't.</p>
<p>With few downsides, and such a huge upside, Mold/Sold seems like a no-brainer, especially if you are doing a lot of incremental builds!</p>

                        <h3>
                            <a id="optimization-levels" href="#optimization-levels">
                                Optimization Levels
                            </a>
                        </h3>
                        
<p>What will adding more optimization do to compile times?</p>
<table><thead><tr><th>Clean Compilation</th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>Site</td><td>Mean(s)</td><td>Standard Deviation(s)</td><td>Delta from Baseline(s)</td><td>% Delta from Baseline</td></tr>
<tr><td>benw.is 7200RPM</td><td>174.399</td><td>0.488</td><td>-86.012</td><td>-97.31295326</td></tr>
<tr><td>benw.is NVME</td><td>166.847</td><td>0.41</td><td>-86.79</td><td>-108.4102577</td></tr>
<tr><td>houski.ca</td><td>288.562</td><td>0.674</td><td>-163.569</td><td>-130.8625283</td></tr>
</tbody></table>
<table><thead><tr><th>Incremental Compilation</th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>Site</td><td>Mean(s)</td><td>Standard Deviation(s)</td><td>Delta from Baseline(s)</td><td>% Delta from Baseline</td></tr>
<tr><td>benw.is 7200RPM</td><td>16.201</td><td>1.303</td><td>4.26</td><td>20.82009677</td></tr>
<tr><td>benw.is NVME</td><td>14.334</td><td>0.147</td><td>4.763</td><td>24.94109022</td></tr>
<tr><td>houski.ca</td><td>32.489</td><td>0.348</td><td>8.329</td><td>20.40521339</td></tr>
</tbody></table>
<p>I did expect a clean compile to be slower, but this is quite a bit slower. Compared to baseline, it takes twice as long for clean builds. Incremental builds do net a twenty percent improvement, which is nice to see. Contained within the listed build time is a doubling of both the webassembly and server builds, which makes sense but should be noted. For me, I don't think the benefits of setting this are worth the delays in clean compile times.</p>

                        <h3>
                            <a id="cranelift" href="#cranelift">
                                Cranelift
                            </a>
                        </h3>
                        <table><thead><tr><th>Clean Compilation</th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>Site</td><td>Mean(s)</td><td>Standard Deviation(s)</td><td>Delta from Baseline(s)</td><td>% Delta from Baseline</td></tr>
<tr><td>benw.is 7200RPM</td><td>67.989</td><td>0.763</td><td>20.398</td><td>23.07805447</td></tr>
<tr><td>benw.is NVME</td><td>63.645</td><td>0.247</td><td>16.412</td><td>20.50039347</td></tr>
<tr><td>houski.ca</td><td></td><td></td><td></td><td></td></tr>
</tbody></table>
<table><thead><tr><th>Incremental Compilation</th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>Site</td><td>Mean(s)</td><td>Standard Deviation(s)</td><td>Delta from Baseline(s)</td><td>% Delta from Baseline</td></tr>
<tr><td>benw.is 7200RPM</td><td>9.201</td><td>0.092</td><td>11.26</td><td>55.03152339</td></tr>
<tr><td>benw.is NVME</td><td>9.345</td><td>0.212</td><td>9.752</td><td>51.0656124</td></tr>
<tr><td>houski.ca</td><td></td><td></td><td></td><td></td></tr>
</tbody></table>
<p>Cranelift, I had high hopes for you, and you delivered! It improved both clean and incremental compilations compared to the baseline. You may notice that the houski app lines here are blank, which are due to a failure to compile due to missing llvm intrinsics. </p>
<p>After my testing completed, they merged this <a href="https://benw.is/posts/%5B#1417%5D(https://github.com/rust-lang/rustc_codegen_cranelift/pull/1417)">PR</a> which added the missing <code>llvm.x86.avx.ldu.dq.256</code> instrinsic, and @bjorn3 mentioned that I can avoid the missing aes intrinsics with a rust flag <code>RUSTFLAGS="--cfg aes_force_soft"</code>. Either that, or this <a href="https://github.com/rust-lang/rustc_codegen_cranelift/pull/1425">PR</a> gets merged into rustup nightly. I hope to update this section soon. </p>
<p>If you are missing intrinsics, you can hook up a debugger to see what is causing the trap, or you can use <code>cargo vendor</code> and do a search for the the intrinsic name or part of the intrinsic name. I'd be sure to mention anything you're missing in the <a href="https://github.com/rust-lang/rustc_codegen_cranelift/issues/171#issuecomment-1803136179">rustc cranelift repo</a> and occasionally check in to see if there's a fix coming. @bjorn3 is on a roll there.</p>

                        <h3>
                            <a id="o3-mold" href="#o3-mold">
                                O3 + Mold
                            </a>
                        </h3>
                        <table><thead><tr><th>Clean Compilation</th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>Site</td><td>Mean(s)</td><td>Standard Deviation(s)</td><td>Delta from Baseline(s)</td><td>% Delta from Baseline</td></tr>
<tr><td>benw.is 7200RPM</td><td>170.241</td><td>1.016</td><td>-81.854</td><td>-92.60864154</td></tr>
<tr><td>benw.is NVME</td><td>168.047</td><td>1.018</td><td>-87.99</td><td>-109.9091897</td></tr>
<tr><td>houski.ca</td><td>271.949</td><td>0.735</td><td>-146.956</td><td>-117.571384</td></tr>
</tbody></table>
<table><thead><tr><th>Incremental Compilation</th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>Site</td><td>Mean(s)</td><td>Standard Deviation(s)</td><td>Delta from Baseline(s)</td><td>% Delta from Baseline</td></tr>
<tr><td>benw.is 7200RPM</td><td>5.789</td><td>0.051</td><td>14.672</td><td>71.70715019</td></tr>
<tr><td>benw.is NVME</td><td>5.727</td><td>0.041</td><td>13.37</td><td>70.01099649</td></tr>
<tr><td>houski.ca</td><td>18.15</td><td>0.202</td><td>22.668</td><td>55.53432309</td></tr>
</tbody></table>
<p>Our first combination of features. Unforunately, this seems to have the slow clean compile times of O3, without decreasing compile times more than mold alone.</p>

                        <h3>
                            <a id="o3-mold-cranelift" href="#o3-mold-cranelift">
                                O3 + Mold + Cranelift
                            </a>
                        </h3>
                        <table><thead><tr><th>Clean Compilation</th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>Site</td><td>Mean(s)</td><td>Standard Deviation(s)</td><td>Delta from Baseline(s)</td><td>% Delta from Baseline</td></tr>
<tr><td>benw.is 7200RPM</td><td>107.401</td><td>1.05</td><td>-19.014</td><td>-21.51221333</td></tr>
<tr><td>benw.is NVME</td><td>103.544</td><td>0.88</td><td>-23.487</td><td>-29.33784678</td></tr>
<tr><td>houski.ca</td><td></td><td></td><td></td><td></td></tr>
</tbody></table>
<table><thead><tr><th>Incremental Compilation</th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>Site</td><td>Mean(s)</td><td>Standard Deviation(s)</td><td>Delta from Baseline(s)</td><td>% Delta from Baseline</td></tr>
<tr><td>benw.is 7200RPM</td><td>4.992</td><td>0.322</td><td>15.469</td><td>75.60236548</td></tr>
<tr><td>benw.is NVME</td><td>4.767</td><td>0.073</td><td>14.33</td><td>75.03796408</td></tr>
<tr><td>houski.ca</td><td></td><td></td><td></td><td></td></tr>
</tbody></table>
<p>Let's enable all the things! The incremental compiles are faster than any of the previously tested options, but the clean compiles are still suffering a penalty from O3. Cranelift seems to reduce the penalty from O3, but time will tell whether it is better than Mold + Cranelift alone. I suspect this is because Cranelift optimizes less than llvm, which is in line with their documentation.</p>

                        <h3>
                            <a id="cranelift-o3" href="#cranelift-o3">
                                Cranelift + O3
                            </a>
                        </h3>
                        <table><thead><tr><th>Clean Compilation</th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>Site</td><td>Mean(s)</td><td>Standard Deviation(s)</td><td>Delta from Baseline(s)</td><td>% Delta from Baseline</td></tr>
<tr><td>benw.is 7200RPM</td><td>110.251</td><td>0.687</td><td>-21.864</td><td>-24.737</td></tr>
<tr><td>benw.is NVME</td><td>106.732</td><td>0.526</td><td>-26.675</td><td>-33.323</td></tr>
<tr><td>houski.ca</td><td></td><td></td><td></td><td></td></tr>
</tbody></table>
<table><thead><tr><th>Incremental Compilation</th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>Site</td><td>Mean(s)</td><td>Standard Deviation(s)</td><td>Delta from Baseline(s)</td><td>% Delta from Baseline</td></tr>
<tr><td>benw.is 7200RPM</td><td>9.096</td><td>0.351</td><td>11.365</td><td>55.54469479</td></tr>
<tr><td>benw.is NVME</td><td>8.73</td><td>0.076</td><td>10.367</td><td>54.28601351</td></tr>
<tr><td>houski.ca</td><td></td><td></td><td></td><td></td></tr>
</tbody></table>
<p>Adding further support to the Cranelift optimization hypothesis, we see similiar performance in the clean compilation, but slower performance in incremental without Mold enabled. It is interesting that this ran faster than the previous test with Mold enabled, although I am not sure the difference is signifigant. Perhaps Mold and Cranelift or Mold and O3 have some negative interaction I don't recognize.</p>

                        <h3>
                            <a id="cranelift-mold" href="#cranelift-mold">
                                Cranelift + Mold
                            </a>
                        </h3>
                        <table><thead><tr><th>Clean Compilation</th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>Site</td><td>Mean(s)</td><td>Standard Deviation(s)</td><td>Delta from Baseline(s)</td><td>% Delta from Baseline</td></tr>
<tr><td>benw.is 7200RPM</td><td>65.343</td><td>1.243</td><td>23.044</td><td>26.07170738</td></tr>
<tr><td>benw.is NVME</td><td>59.777</td><td>0.191</td><td>20.28</td><td>25.33195098</td></tr>
<tr><td>houski.ca</td><td></td><td></td><td></td><td></td></tr>
</tbody></table>
<table><thead><tr><th>Incremental Compilation</th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>Site</td><td>Mean(s)</td><td>Standard Deviation(s)</td><td>Delta from Baseline(s)</td><td>% Delta from Baseline</td></tr>
<tr><td>benw.is 7200RPM</td><td>4.831</td><td>0.251</td><td>15.63</td><td>76.38922829</td></tr>
<tr><td>benw.is NVME</td><td>4.654</td><td>0.067</td><td>14.443</td><td>75.62968005</td></tr>
<tr><td>houski.ca</td><td></td><td></td><td></td><td></td></tr>
</tbody></table>
<p>Here's the big money, Cranelift and Mold together. This offered the fastest clean compilation times, with a roughly 25% reduction, and incremental, sitting pretty at around 75% reduction. Interestingly enough, there seemed to be little difference between the NVME and the 7200RPM drives here, suggesting that these two might be better at using disk I/O than some of the other configurations. </p>

                        <h2>
                            <a id="hot-reloading" href="#hot-reloading">
                                Hot Reloading
                            </a>
                        </h2>
                        
<p>For Rust web frameworks, we can try to simulate the performance of HMR, but we won't be able to substitute JS modules. Our attempt to do this is in cargo-leptos, and can be used with the <code>--hot-reload</code> flag. It will attempt to send html and css patches to the browser over a web socket connection when a change inside a <code>view</code> macro is detected that does not involve a change to a  rust code block, and compile in the background. For me it works fairly well.</p>
<p>I didn't try to instrument and test it, as hot reload times are nearly instant. Any changes to the rust code will require a incremental recompilation though, which is why it's so useful to improve that time. JS frameworks will probably have an edge there for the foreseeable future. </p>

                        <h2>
                            <a id="conclusion" href="#conclusion">
                                Conclusion
                            </a>
                        </h2>
                        
<p>Enabling Mold and Cranelift netted me a 75% incremental and 25% clean compilation time reduction, which is quite signifigant. Using Cranelift requires nightly, or setting it up outside rustup, which some projects might find undesirable. Since Mold requires Linux, and Sold requires Mac(and paying), for me this further increases the argument that Rust web development should be done on some for of Linux or Mac, and not on Windows. WSL2 may help improve things, but may <a href="https://medium.com/for-linux-users/wsl-2-why-you-should-use-real-linux-instead-4ee14364c18">be slower</a>. Since I have a Mac laptop and a Linux workstation, I will be buying Sold for my Mac and using Cranelift for my projects that can. </p>
<p>If you feel like running some of these tests on your hardware or with your project, I have automated the process somewhat and put that repo on Github <a href="https://github.com/benwis/customs">here</a>.</p>
<p>Let me know what your experience was, and whether this helped you with your project by reaching out on mastodon at <code>@benwis@hachyderm.io</code>.</p>
</div></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Long Covid brain fog may be due to damaged blood vessels in the brain (200 pts)]]></title>
            <link>https://www.sciencenews.org/article/long-covid-brain-fog-blood-brain-barrier-damage</link>
            <guid>39762776</guid>
            <pubDate>Wed, 20 Mar 2024 04:06:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencenews.org/article/long-covid-brain-fog-blood-brain-barrier-damage">https://www.sciencenews.org/article/long-covid-brain-fog-blood-brain-barrier-damage</a>, See on <a href="https://news.ycombinator.com/item?id=39762776">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		

<article id="post-3137509">
	<header>
	<div>
			<h2>Long COVID brain fog may be due to damaged blood vessels in the brain</h2>

							<p>
					The result suggests there is a biological basis for this symptom				</p>
					</div>

		<figure>
		<p><img width="1030" height="580" src="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?fit=1030%2C580&amp;ssl=1" alt="A foggy image of a woman with long, dark hair swept to the left." decoding="async" srcset="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?w=1440&amp;ssl=1 1440w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?resize=680%2C383&amp;ssl=1 680w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?resize=800%2C450&amp;ssl=1 800w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?resize=330%2C186&amp;ssl=1 330w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?resize=1030%2C580&amp;ssl=1 1030w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?resize=1380%2C776&amp;ssl=1 1380w" sizes="(max-width: 1030px) 100vw, 1030px" data-attachment-id="3137511" data-permalink="https://www.sciencenews.org/030624_mr_covid-brainfog_feat" data-orig-file="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?fit=1440%2C810&amp;ssl=1" data-orig-size="1440,810" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="030624_mr_covid-brainfog_feat" data-image-description="" data-image-caption="<div>
<p><span class=&quot;None&quot;>Brain fog is a debilitating symptom commonly reported by people with long COVID. Now, scientists have linked the symptom to leaky boundaries in the brain. </span></p>
</div>
" data-medium-file="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?fit=680%2C383&amp;ssl=1" data-large-file="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?fit=800%2C450&amp;ssl=1">		</p>

					<figcaption>
									<span>
						
<p><span>Brain fog is a debilitating symptom commonly reported by people with long COVID. Now, scientists have linked the symptom to leaky boundaries in the brain. </span></p>

					</span>
				
									<span>
						<span>baona/iStock/Getty Images Plus</span>
					</span>
							</figcaption>
			</figure>
	</header>

	

		
		
	<div data-component="video-embed">
				




<p>Leakiness in the brain could explain the memory and concentration problems linked to long COVID.&nbsp;</p>



<p>In patients with brain fog,&nbsp;<a href="https://www.nature.com/articles/s41593-024-01576-9" target="_blank" rel="noopener">MRI scans revealed signs of damaged blood vessels in their brains</a>, researchers reported February 22 in&nbsp;<em>Nature Neuroscience</em>. In these people, dye injected into the bloodstream leaked into their brains and pooled in regions that play roles in language, memory, mood and vision.&nbsp;</p>



<p>It’s the first time anyone’s shown that long COVID patients can have leaky blood brain barriers, says study coauthor Matthew Campbell, a geneticist at Trinity College Dublin in Ireland. That barrier, tightly knit cells lining blood vessels, typically keeps&nbsp;riffraff&nbsp;out of the brain, like bouncers guarding a nightclub.&nbsp;</p>





<p>If the barrier breaks down, bloodborne viruses, cells and other interlopers can sneak into the brain’s tissues and wreak havoc, says&nbsp;Avindra Nath,&nbsp;a&nbsp;neurologist&nbsp;at the&nbsp;National Institutes of Health in Bethesda, Md. It’s too early to say definitively whether that’s happening in people with long COVID, but the new study provides evidence that “brain fog has a biological basis,” says Nath, who wasn’t involved with the work. That alone is important for&nbsp;patients, he says, because their symptoms may be otherwise discounted by physicians.&nbsp;</p>



<p>For some people, brain fog can feel like a slowdown in thinking or difficulty recalling short-term memories, Campbell says. For example,&nbsp;“patients&nbsp;will go for a drive, and forget where they’re driving to.” That might sound trivial, he says, but it actually pushes people into panic mode.&nbsp;</p>



<p>Campbell’s team studies repetitive head trauma. They knew that traumatic brain injuries can disrupt the blood brain barrier — and that people with these injuries sometimes report having brain fog. That mental muddling reminded the team of what people with long COVID can experience. Maybe the blood brain barrier disruption seen in some concussion patients applies to long COVID brain fog, too, the researchers surmised.</p>



<p>Evidence for SARS-CoV-2’s damaging effects on the brain has been mounting for years. Studies in cells and animals suggest the virus can crumble components of the blood brain barrier. And autopsies of people who have died from COVID-19 reveal barrier breakdowns, Nath and others have shown.</p>


<div>
<figure><img decoding="async" width="680" height="398" src="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_inline.jpg?resize=680%2C398&amp;ssl=1" alt="Side-by-side black and white brain scans. The scan on the left contains a few colored speckles. The scan on the right contains many more colored speckles." srcset="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_inline.jpg?w=680&amp;ssl=1 680w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_inline.jpg?resize=654%2C383&amp;ssl=1 654w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_inline.jpg?resize=318%2C186&amp;ssl=1 318w" sizes="(max-width: 680px) 100vw, 680px" data-recalc-dims="1"><figcaption><span><p><span>In long COVID patients with brain fog (brain scan at right), dye injected into the bloodstream tends to leak into the brain (see colored speckles) more so than in people without brain fog (left).</span></p></span><span><p><span><span lang="DE">C. Greene <i>et al.</i>/<i>Nature Neuroscience</i> 2024</span></span></p></span></figcaption></figure></div>


<p>But until now, no one knew if this kind of damage persisted long after the initial infection subsided. The team scanned the brains of 32 people, 10 of whom had recovered from COVID-19, and 22 with long COVID. Of those with long COVID,&nbsp;&nbsp;half reported having brain fog.&nbsp;</p>



<p>An injected dye lit up all the participants’&nbsp;brains during MRI brain scans. In people recovered from COVID, the dye had trouble crossing the blood brain barrier. Likewise, in long COVID patients without brain fog, the dye mostly stayed put, confined within blood vessels. But in eight of 11 participants with brain fog, the dye tended to escape from blood vessels and enter brain tissue.&nbsp;</p>



<p>“It was just so clear,” Campbell says. He remembers one of the first people scanned, someone with severe brain fog. Their temporal lobes, brain regions that sit behind the eyes, were&nbsp;“just flooded with this dye,” he says. The researchers’&nbsp;work suggests that&nbsp;“brain fog wasn’t just a figment of [patients’] imagination,” Campbell says.&nbsp;“It was a very, very real thing that they were reporting.”</p>



<p>The new findings offer an opportunity to think about potential therapies, Nath says. Perhaps researchers can find a way to slow down the blood brain barrier’s breakdown — or reverse it.</p>



			</div>
</article><!-- #post-## -->


<section>
	<h3>
		More Stories from Science News on <a href="https://www.sciencenews.org/topic/health-medicine">Health &amp; Medicine</a>
	</h3>
	

</section>
<div>
		<h3>From the Nature Index</h3>
		<p><span>Paid Content</span>
	</p></div>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Study Puts Fermented Foods, Not Fire, as Pivotal Moment in Human Brain Growth (251 pts)]]></title>
            <link>https://plantbasednews.org/news/science/fermented-foods-human-brain-growth/</link>
            <guid>39762588</guid>
            <pubDate>Wed, 20 Mar 2024 03:20:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://plantbasednews.org/news/science/fermented-foods-human-brain-growth/">https://plantbasednews.org/news/science/fermented-foods-human-brain-growth/</a>, See on <a href="https://news.ycombinator.com/item?id=39762588">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="316538">
<div>


<p>Fermented foods may have helped evolution of large brains in humans, according to a recent <a href="https://www.nature.com/articles/s42003-023-05517-3" target="_blank" rel="noreferrer noopener">study</a>.</p>
<p>The human brain began increasing in size around 2.5 million years ago. But scientists have been unsure of what mechanism drove that change. Fire and the invention of cooking has often been thought to have been the key, by enabling our ancestors to get enough nourishment to spur our evolution.</p>
<p>But the new study notes that the archaeological evidence shows that human brain expansion predated fire use by up a million years.</p>
<p>As brains need a lot of calories to keep functioning, the researchers believe another dietary change helped to kickstart the growth of early humans’ brains. They posit that fermented foods, as a dietary option accessible to our ancestors, were responsible.</p>
<h2 id="h-external-fermentation-hypothesis">External Fermentation Hypothesis</h2>
<div>
<figure><img decoding="async" width="1200" height="767" src="https://plantbasednews.org/app/uploads/2024/03/plant-based-news-fermented-foods-brain-growth-1200x767.jpeg" alt="Jars of various fermented vegetables" srcset="https://plantbasednews.org/app/uploads/2024/03/plant-based-news-fermented-foods-brain-growth-1200x767.jpeg 1200w, https://plantbasednews.org/app/uploads/2024/03/plant-based-news-fermented-foods-brain-growth-600x384.jpeg 600w, https://plantbasednews.org/app/uploads/2024/03/plant-based-news-fermented-foods-brain-growth-768x491.jpeg 768w, https://plantbasednews.org/app/uploads/2024/03/plant-based-news-fermented-foods-brain-growth-1536x982.jpeg 1536w, https://plantbasednews.org/app/uploads/2024/03/plant-based-news-fermented-foods-brain-growth-2048x1310.jpeg 2048w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption><span>Adobe Stock</span> Fermented foods are popular in the modern world, and they may have aided brain growth of our ancestors</figcaption></figure></div>
<p>The researchers propose the External Fermentation Hypothesis to explain what helped our brains grow. Food ferments inside our guts, but the researchers believe that the food must have been fermented before being eaten.&nbsp;</p>
<p>According to the study, fermentation makes it easier for humans to absorb macronutrients and micronutrients. It also makes carbohydrates and proteins more digestible.</p>
<p>Backing up this hypothesis is the fact that humans have relatively smaller large intestines than other primates. This indicates that our ancestors were eating food that was already partly broken down by fermentation.&nbsp;</p>
<p>“Reduced gut sizes could only evolve if our ancestors were able to exploit a more nutrient-dense and easily digestible food source,” explain the researchers in the study. As a result, less energy would have been needed to support digestion, freeing it up for use by the brain instead.</p>
<h2>A happy accident</h2>
<p>Our ancestors probably didn’t choose fermented foods for their brain health, but fermented foods by accident. The study suggests that our early ancestors may stored food in common locations, intermittently eating some and adding more. Using the same storage spots could have helped a stable microbial ecosystem to develop that would aid fermentation.</p>
<p>“This was not necessarily an intentional endeavor,” Erin Hecht, co-author on the study and Assistant Professor of Human Evolutionary Biology at Harvard University, <a href="https://news.harvard.edu/gazette/story/2024/02/did-fermented-foods-fuel-brain-growth/" target="_blank" rel="noreferrer noopener">told <em>The Harvard Gazette</em></a>. “It may have been an accidental side effect of caching food. And maybe, over time, traditions or superstitions could have led to practices that promoted fermentation or made fermentation more stable or more reliable.”</p>
<h2>Supporting mental health</h2>
<p>Katherine Bryant, lead author and researcher at Aix-Marseille University, suggests that the External Fermentation Hypothesis could have implications for research into modern diets.</p>
<p>“This hypothesis also gives us as scientists even more reasons to explore the role of fermented foods on human health and the maintenance of a healthy gut microbiome,” she told<em> The Harvard Gazette</em>. “There have been a number of studies in recent years linking gut microbiome to not only physical but mental health.”</p>
<p>Indeed, fermented foods such as kimchi and tempeh are becoming increasingly popular for their benefits to gut health. Gut health expert Professor Tim Spector <a href="https://plantbasednews.org/lifestyle/health-and-fitness/benefits-of-fermented-foods/">recommends</a> eating a small amount of fermented foods every day. This encourages diversity in the gut microbiome.</p>
<h6>More like this:</h6>
<ul>
<li><a href="https://plantbasednews.org/lifestyle/food/sprouting-healthy-food/">What Is Sprouting? How To Grow Healthy Food ‘For Pennies’</a></li>
<li><a href="https://plantbasednews.org/lifestyle/health-and-fitness/sleep-apnoea-plant-based/">Healthy Plant-Based Diets Cut Sleep Apnoea Risk, Study Finds</a></li>
<li><a href="https://plantbasednews.org/lifestyle/health-and-fitness/protein-rich-vegetables/">8 Protein-Rich Vegetables To Add To Your Meals</a></li>
</ul>
</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Let's create a Tree-sitter grammar (172 pts)]]></title>
            <link>https://www.jonashietala.se/blog/2024/03/19/lets_create_a_tree-sitter_grammar/</link>
            <guid>39762495</guid>
            <pubDate>Wed, 20 Mar 2024 02:55:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jonashietala.se/blog/2024/03/19/lets_create_a_tree-sitter_grammar/">https://www.jonashietala.se/blog/2024/03/19/lets_create_a_tree-sitter_grammar/</a>, See on <a href="https://news.ycombinator.com/item?id=39762495">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      

      
<article>
  <header>
    
    
    


  </header>

   <p>One of my favorite features in Neovim is the Tree-sitter integration.
It allows for fast syntax highlighting that works well even in an error state (often the case when you’re editing code), and it has additional semantics (you can differentiate between function parameters and local variables).</p>
<p>With <a href="https://github.com/nvim-treesitter/nvim-treesitter-textobjects">nvim-treesitter-textobjects</a> you can also jump between nodes (such as <code>]c</code> to jump to next class) or target deletion (<code>cif</code> to delete the function body and enter insert mode).
An amazing feature as it works across languages, no matter how they look like, as long as they have a Tree-sitter grammar (most do).</p>
<p>But, you might wonder what does a Tree-sitter grammar look like,
and how do you create one for a language?</p>
<p>I started thinking about this and before I knew it I was <a href="https://github.com/treeman/tree-sitter-djot">trying to make my own parser</a> for <a href="https://djot.net/">Djot</a> (a markup language similar to Markdown).
There are some good tutorials on how to get started, but not on some more advanced things.</p>
<p>I spent quite some time trying to figure it out—while refreshing my 10 year old knowledge of grammars—and decided to document some of my findings.</p>
<p>The post spiraled out of control a little, and it will go through:</p>
<ol>
<li>
How to use an external scanner
</li>
<li>
Using Tree-sitter’s built-in conflicts resolution
</li>
<li>
Syntax highlighting with language injection
</li>
<li>
Use the grammar from Neovim for syntax highlighting and textobjects
</li>
<li>
Embed the grammar into this Blog for syntax highlighting
</li>
</ol>
<p>The source code for the complete grammar we’ll develop <a href="https://github.com/treeman/tree-sitter-sdjot">can be found on GitHub</a>.</p>
<section id="Our-subset">
<h2><a href="#Our-subset">Our subset</a></h2>
<p>For the purpose of this blog post, we’ll implement a small subset of <a href="https://djot.net/">Djot</a>, where we’ll support:</p>
<ul>
<li>
Paragraphs
</li>
<li>
Divs
</li>
<li>
Code blocks
</li>
<li>
Emphasis
</li>
</ul>
<p>This will allow us to parse markup like this:</p>
<div><pre><code><span>This is a</span>
<span>multiline <span><span>_</span>paragraph<span>_</span></span></span>
<span></span>
<span></span><span>:::</span>
<span>This is a paragraph inside a div</span>
<span></span><span>:::</span>

<span><span>```</span><span>gleam</span></span>
<span><span>let</span> <span>x</span> <span>=</span> <span>2</span>;</span>
<span><span>```</span></span>
</code></pre></div>
<p>(Yes, the <code>sdjot</code> highlight uses our grammar.)</p>
<p>At first blush, this seems like it’s too simple to require anything more complex than some simple grammar rules, but later on we’ll see that even these simple rules requires a bit more effort to fully support.</p>
</section>
<section id="Simple-beginnings">
<h2><a href="#Simple-beginnings">Simple beginnings</a></h2>
<p>The point of this post isn’t to go through how the Tree-sitter grammar description in <code>grammar.js</code> works.
The <a href="https://tree-sitter.github.io/tree-sitter/creating-parsers">Tree-sitter docs</a> goes through how to get started.
I named the project <code>sdjot</code> and this is the <code>grammar.js</code> we’ll start with:</p>
<div><pre><code><span>module<span>.</span><span>exports</span> <span>=</span> grammar<span><span><span><span>(</span></span></span></span><span><span><span><span>{</span>
  name<span>:</span> <span><span><span>"</span>sdjot<span>"</span></span></span><span>,</span>

        <span><span>extras</span></span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>_<span>)</span></span></span> =<span>&gt;</span> <span><span>[</span><span><span><span>"</span><span>\r</span><span>"</span></span></span><span>]</span></span><span>,</span>

  rules<span>:</span> <span><span>{</span>
    <span><span>document</span></span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> repeat<span><span><span><span>(</span></span></span></span><span><span>$<span>.</span><span>_block</span><span>)</span></span></span><span>,</span>

        <span><span>_block</span></span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> choice<span><span><span><span>(</span></span></span></span><span><span>$<span>.</span><span>div</span><span>,</span> $<span>.</span><span>code_block</span><span>,</span> $<span>.</span><span>paragraph</span><span>,</span> <span><span><span>"</span><span>\n</span><span>"</span></span></span><span>)</span></span></span><span>,</span>

        <span><span>div</span></span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span>
      prec<span>.</span><span><span>left</span></span><span><span><span><span>(</span></span></span></span><span><span>seq<span><span><span><span>(</span></span></span></span><span><span>$<span>.</span><span>div_marker</span><span>,</span> <span><span><span>"</span><span>\n</span><span>"</span></span></span><span>,</span> repeat<span><span><span><span>(</span></span></span></span><span><span>$<span>.</span><span>_block</span><span>)</span></span></span><span>,</span> $<span>.</span><span>div_marker</span><span>,</span> <span><span><span>"</span><span>\n</span><span>"</span></span></span><span>)</span></span></span><span>)</span></span></span><span>,</span>
    <span><span>div_marker</span></span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>_<span>)</span></span></span> =<span>&gt;</span> <span><span><span>"</span>:::<span>"</span></span></span><span>,</span>

        <span><span>code_block</span></span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span>
      seq<span><span><span><span>(</span></span></span></span><span><span>
        $<span>.</span><span>code_block_marker</span><span>,</span>
        optional<span><span><span><span>(</span></span></span></span><span><span>$<span>.</span><span>language</span><span>)</span></span></span><span>,</span>
        <span><span><span>"</span><span>\n</span><span>"</span></span></span><span>,</span>
        optional<span><span><span><span>(</span></span></span></span><span><span>$<span>.</span><span>code</span><span>)</span></span></span><span>,</span>
        $<span>.</span><span>code_block_marker</span><span>,</span>
      <span>)</span></span></span><span>,</span>
    <span><span>code_block_marker</span></span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>_<span>)</span></span></span> =<span>&gt;</span> <span><span><span>"</span>```<span>"</span></span></span><span>,</span>
    <span><span>code</span></span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>_<span>)</span></span></span> =<span>&gt;</span> repeat1<span><span><span><span>(</span></span></span></span><span><span>seq<span><span><span><span>(</span></span></span></span><span><span><span><span><span>/</span><span><span>[</span><span>^</span><span>\n</span><span>]</span></span><span>*</span><span>/</span></span></span><span></span><span>,</span> <span><span><span>"</span><span>\n</span><span>"</span></span></span><span>)</span></span></span><span>)</span></span></span><span>,</span>
    <span><span>language</span></span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>_<span>)</span></span></span> =<span>&gt;</span> <span><span><span>/</span><span><span>[</span><span>^</span><span>\s</span><span>]</span></span><span>+</span><span>/</span></span></span><span></span><span>,</span>

            <span><span>paragraph</span></span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> seq<span><span><span><span>(</span></span></span></span><span><span>repeat<span><span>1</span></span><span><span><span><span>(</span></span></span></span><span><span>seq<span><span><span><span>(</span></span></span></span><span><span>$<span>.</span><span>_inline</span><span>,</span> <span><span><span>"</span><span>\n</span><span>"</span></span></span><span>)</span></span></span><span>)</span></span></span><span>,</span> <span><span><span>"</span><span>\n</span><span>"</span></span></span><span>)</span></span></span><span>,</span>

            <span><span>_inline</span></span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> repeat1<span><span><span><span>(</span></span></span></span><span><span>choice<span><span><span><span>(</span></span></span></span><span><span>$<span>.</span><span>emphasis</span><span>,</span> $<span>.</span><span>_text</span><span>)</span></span></span><span>)</span></span></span><span>,</span>
    <span><span>emphasis</span></span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> prec<span>.</span><span><span>left</span></span><span><span><span><span>(</span></span></span></span><span><span>seq<span><span><span><span>(</span></span></span></span><span><span><span><span><span>"</span>_<span>"</span></span></span><span>,</span> $<span>.</span><span>_inline</span><span>,</span> <span><span><span>"</span>_<span>"</span></span></span><span>)</span></span></span><span>)</span></span></span><span>,</span>
    <span><span>_text</span></span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>_<span>)</span></span></span> =<span>&gt;</span> <span><span><span>/</span><span><span>[</span><span>^</span><span>\n</span><span>]</span></span><span>/</span></span></span><span></span><span>,</span>
  <span>}</span></span><span>,</span>
<span>}</span></span><span>)</span></span></span><span>;</span>
</span></code></pre></div>
<p>It recognizes paragraphs with text and emphasis, and it identifies divs and code blocks.</p>
<p>We can create an <code>example-file</code> with these contents:</p>
<div><pre><code><span>:::</span>
<span>A paragraph <span><span>_</span>with emphasis<span>_</span></span> inside a div</span>
<span></span>
<span></span><span>:::</span>
</code></pre></div>
<p>And parse it with the <code>tree-sitter</code> cli:</p>
<div><pre><code><span>$ </span><span>tree-sitter</span> parse example-file
(document [0, 0] - [5, 0]
  (div [0, 0] - [4, 0]
    (div_marker [0, 0] - [0, 3])
    (paragraph [1, 0] - [3, 0]
      (emphasis [1, 12] - [1, 27]))
    (div_marker [3, 0] - [3, 3])))
</code></pre></div>
<p>Et voilà!</p>
<section id="Missing-features">
<h3><a href="#Missing-features">Missing features</a></h3>
<p>But I told you it wasn’t supposed to be this easy, and there are features missing from our parser.
Most notably:</p>
<ol type="A">
<li>
There can be an arbitrary number of <code>:</code>, allowing divs to be nested.
</li>
<li>
Closing a div should close other open blocks (divs and paragraphs in our case).
</li>
</ol>
<p>In essence, we need to be able to parse this:</p>
<div><pre><code><span>:::</span>
<span>Top-level div</span>
<span></span>
<span></span><span>::::</span>
<span>A paragraph inside a second div,</span>
<span>both closed when the top-level div is closedj</span>
<span></span><span>:::</span>
</code></pre></div>
<p>This is… Complicated.</p>
<p>Sure, we can work around the varying levels of <code>:</code> with something hacky like enumerating the number of colons, using something like this:</p>
<div><pre><code><span><span>div</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> choice<span><span><span><span>(</span></span></span></span><span><span>$<span>.</span><span>_div3</span><span>,</span> $<span>.</span><span>_div4</span><span>,</span> $<span>.</span><span>_div5</span><span>,</span> $<span>.</span><span>_div6</span><span>,</span> $<span>.</span><span>_div7</span><span>,</span> $<span>.</span><span>_div8</span><span>)</span></span></span><span>,</span>
<span>_div3</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> seq<span><span><span><span>(</span></span></span></span><span><span><span><span><span>/</span>:<span>{3}</span><span>/</span></span></span><span></span><span>,</span> $<span>.</span><span>_inside_div</span><span>,</span> <span><span><span>/</span>:<span>{3}</span><span>/</span></span></span><span></span><span>,</span> <span><span><span>"</span><span>\n</span><span>"</span></span></span><span>)</span></span></span><span>,</span>
<span>_div4</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> seq<span><span><span><span>(</span></span></span></span><span><span><span><span><span>/</span>:<span>{4}</span><span>/</span></span></span><span></span><span>,</span> $<span>.</span><span>_inside_div</span><span>,</span> <span><span><span>/</span>:<span>{4}</span><span>/</span></span></span><span></span><span>,</span> <span><span><span>"</span><span>\n</span><span>"</span></span></span><span>)</span></span></span><span>,</span>
<span>_div5</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> seq<span><span><span><span>(</span></span></span></span><span><span><span><span><span>/</span>:<span>{5}</span><span>/</span></span></span><span></span><span>,</span> $<span>.</span><span>_inside_div</span><span>,</span> <span><span><span>/</span>:<span>{5}</span><span>/</span></span></span><span></span><span>,</span> <span><span><span>"</span><span>\n</span><span>"</span></span></span><span>)</span></span></span><span>,</span>
<span>_div6</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> seq<span><span><span><span>(</span></span></span></span><span><span><span><span><span>/</span>:<span>{6}</span><span>/</span></span></span><span></span><span>,</span> $<span>.</span><span>_inside_div</span><span>,</span> <span><span><span>/</span>:<span>{6}</span><span>/</span></span></span><span></span><span>,</span> <span><span><span>"</span><span>\n</span><span>"</span></span></span><span>)</span></span></span><span>,</span>
<span>_div7</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> seq<span><span><span><span>(</span></span></span></span><span><span><span><span><span>/</span>:<span>{7}</span><span>/</span></span></span><span></span><span>,</span> $<span>.</span><span>_inside_div</span><span>,</span> <span><span><span>/</span>:<span>{7}</span><span>/</span></span></span><span></span><span>,</span> <span><span><span>"</span><span>\n</span><span>"</span></span></span><span>)</span></span></span><span>,</span>
<span>_div8</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> seq<span><span><span><span>(</span></span></span></span><span><span><span><span><span>/</span>:<span>{8}</span><span>/</span></span></span><span></span><span>,</span> $<span>.</span><span>_inside_div</span><span>,</span> <span><span><span>/</span>:<span>{8}</span><span>/</span></span></span><span></span><span>,</span> <span><span><span>"</span><span>\n</span><span>"</span></span></span><span>)</span></span></span><span>,</span>
<span>_inside_div</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> prec<span>.</span><span><span>left</span></span><span><span><span><span>(</span></span></span></span><span><span><span><span><span>"</span><span>\n</span><span>"</span></span></span><span>,</span> repeat<span><span><span><span>(</span></span></span></span><span><span>$<span>.</span><span>_block</span><span>)</span></span></span><span>)</span></span></span><span>,</span>
</span></code></pre></div>
<p>But it’s not <em>neat</em>, and automatically closing contained blocks is much harder (to my brain it seems impossible, but I’m no expert).</p>
<p>With an external scanner we can do this (and more).</p>
</section>
</section>
<section id="External-scanner">
<h2><a href="#External-scanner">External scanner</a></h2>
<p>A Tree-sitter parser is actually a C program.
The grammar we’ve seen has been described in JavaScript, but it’s only used as a description to generate the parser in C.
If you’re a masochist, you can take a look at it in <code>src/parser.c</code> after running <code>tree-sitter generate</code>.</p>
<p>An external scanner is just some custom C code that’s inserted into the parser, and it allows us to override the parser precedence, keep track of a context state, or whatever else we might need or want to do.</p>
<p>To get started the <a href="https://tree-sitter.github.io/tree-sitter/creating-parsers#external-scanners">official docs</a> was pretty good.
Basically you need to:</p>
<ol>
<li>
Create a <code>src/scanner.c</code> and include it in <code>binding.gyp</code> <code>bindings/rust/build.rs</code>.
</li>
<li>
Setup <code>externals</code> tokens in <code>grammar.js</code> and a matching C enum in <code>scanner.c</code>.
</li>
<li>
Define and implement five C functions.
</li>
</ol>
<p>Let’s take a look.</p>
<section id="Div-markers-closes-open-paragraphs">
<h3><a href="#Div-markers-closes-open-paragraphs">Div markers closes open paragraphs</a></h3>
<p>Let’s start by closing a paragraph early when a <code>:::</code> is encountered.
This is simpler because we can solve this without storing any state.</p>
<p>When parsing <code>$.paragraph</code> we’ll give the parser a choice between ending the paragraph on a newline or on our new <code>$._close_paragraph</code> token:</p>
<div><pre><code><span><span>paragraph</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span>
  seq<span><span><span><span>(</span></span></span></span><span><span>repeat<span><span>1</span></span><span><span><span><span>(</span></span></span></span><span><span>seq<span><span><span><span>(</span></span></span></span><span><span>$<span>.</span><span>_inline</span><span>,</span> <span><span><span>"</span><span>\n</span><span>"</span></span></span><span>)</span></span></span><span>)</span></span></span><span>,</span> choice<span><span><span><span>(</span></span></span></span><span><span><span><span><span>"</span><span>\n</span><span>"</span></span></span><span>,</span> $<span>.</span><span>_close_paragraph</span><span>)</span></span></span><span>)</span></span></span><span>,</span>
</span></code></pre></div>
<p><code>$._close_paragraph</code> is handled by the external scanner, which is specified using the <code>externals</code> field:</p>
<div><pre><code><span><span>externals</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> <span><span>[</span>$<span>.</span><span>_close_paragraph</span><span>]</span></span><span>,</span>
</span></code></pre></div>
<p>Now let’s turn our attention to <code>src/scanner.c</code>.
The tokens in <code>externals</code> gets assigned an incremented number, starting from 0…
Just like an enum in C!</p>
<div><pre><code><span><span><span>//</span> We only have a single element right now, but keep in mind that the order
</span><span><span>//</span> must match the `externals` array in `grammar.js`.
</span><span>typedef</span> <span>enum</span> <span><span>{</span> CLOSE_PARAGRAPH <span>}</span></span> <span>TokenType</span><span>;</span>
</span></code></pre></div>
<p>The five functions we need to implement are these:</p>
<div><pre><code><span><span><span>//</span> You should replace `sdjot` with whatever project name you chose.
</span><span>bool</span> <span><span>tree_sitter_sdjot_external_scanner_scan</span></span><span><span><span>(</span></span></span><span><span><span>void</span> <span>*</span><span>payload</span><span>,</span> TSLexer <span>*</span><span>lexer</span><span>,</span>
                                             <span>const</span> <span>bool</span> <span>*</span><span>valid_symbols</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  <span> All the scanning goes here.
</span>  <span>return</span> <span>false</span><span>;</span>
</span></span><span><span><span>}</span></span></span>

<span><span>//</span> If we need to allocate/deallocate state, we do it in these functions.
</span><span>void</span> <span>*</span><span><span>tree_sitter_sdjot_external_scanner_create</span></span><span><span><span>(</span></span></span><span><span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span> <span>return</span> <span>NULL</span><span>;</span> </span></span><span><span><span>}</span></span></span>
<span>void</span> <span><span>tree_sitter_sdjot_external_scanner_destroy</span></span><span><span><span>(</span></span></span><span><span><span>void</span> <span>*</span><span>payload</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span></span><span><span><span>}</span></span></span>

<span><span>//</span> If we have state, we should load and save it in these functions.
</span><span>unsigned</span> <span><span>tree_sitter_sdjot_external_scanner_serialize</span></span><span><span><span>(</span></span></span><span><span><span>void</span> <span>*</span><span>payload</span><span>,</span>
                                                      <span>char</span> <span>*</span><span>buffer</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  <span>return</span> <span>0</span><span>;</span>
</span></span><span><span><span>}</span></span></span>
<span>void</span> <span><span>tree_sitter_sdjot_external_scanner_deserialize</span></span><span><span><span>(</span></span></span><span><span><span>void</span> <span>*</span><span>payload</span><span>,</span> <span>char</span> <span>*</span><span>buffer</span><span>,</span>
                                                    <span>unsigned</span> <span>length</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span></span><span><span><span>}</span></span></span>
</span></code></pre></div>
<p>Because we won’t use any state, we’ll only have to update the <code>scan</code> function.</p>
<p>What you’re supposed to do is check <code>valid_symbols</code> for the tokens we can return at any point in time, and return <code>true</code> if any was found:</p>
<div><pre><code><span><span>bool</span> <span><span>tree_sitter_sdjot_external_scanner_scan</span></span><span><span><span>(</span></span></span><span><span><span>void</span> <span>*</span><span>payload</span><span>,</span> TSLexer <span>*</span><span>lexer</span><span>,</span>
                                             <span>const</span> <span>bool</span> <span>*</span><span>valid_symbols</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  <span>if</span> <span><span>(</span>valid_symbols<span><span>[</span>CLOSE_PARAGRAPH<span>]</span></span> <span>&amp;&amp;</span> <span><span>parse_close_paragraph</span><span><span>(</span></span></span><span><span>lexer</span></span><span><span><span>)</span></span></span><span>)</span></span> <span><span>{</span>
    <span>return</span> <span>true</span><span>;</span>
  <span>}</span></span>
  <span>return</span> <span>false</span><span>;</span>
</span></span><span><span><span>}</span></span></span>
</span></code></pre></div>
<p>To decide if we’re going to close the paragraph early, we’ll look ahead for any <code>:::</code>, and if so we’ll close it without consuming any characters.
This might not be the most efficient solution because we’ll have to parse the <code>:::</code> again, but it gets the job done.</p>
<p>The matched token should be stored in 
<code><span>lexer<span>-&gt;</span>result_symbol</span></code>:</p>
<div><pre><code><span><span>static</span> <span>bool</span> <span><span>parse_close_paragraph</span></span><span><span><span>(</span></span></span><span><span>TSLexer <span>*</span><span>lexer</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  <span> Mark the end before advancing so that the CLOSE_PARAGRAPH token doesn't
</span>  <span> consume any characters.
</span>  lexer<span>-&gt;</span><span><span>mark_end</span><span><span>(</span></span></span><span><span>lexer</span></span><span><span><span>)</span></span></span><span>;</span>

  <span>uint8_t</span> colons <span>=</span> <span><span>consume_chars</span><span><span>(</span></span></span><span><span>lexer<span>,</span> <span><span>'</span>:<span>'</span></span></span></span><span><span><span>)</span></span></span><span>;</span>
  <span>if</span> <span><span>(</span>colons <span>&gt;=</span> <span>3</span><span>)</span></span> <span><span>{</span>
    lexer<span>-&gt;</span>result_symbol <span>=</span> CLOSE_PARAGRAPH<span>;</span>
    <span>return</span> <span>true</span><span>;</span>
  <span>}</span></span> <span>else</span> <span><span>{</span>
    <span>return</span> <span>false</span><span>;</span>
  <span>}</span></span>
</span></span><span><span><span>}</span></span></span>
</span></code></pre></div>
<p>Note that the resulting token will mark any symbol we advance over as owned by that token.
So <code>:::</code> would be marked as <code>_close_paragraph</code> (which will be ignored by the output since it begins with an underscore), instead of <code>div_marker</code>.
To prevent this, we turn <code>_close_paragraph</code> into a zero-width token by marking the end before advancing the lexer.</p>
<p>How do we advance the lexer?
We call 
<code><span>lexer<span>-&gt;</span>advance</span></code>:</p>
<div><pre><code><span><span>static</span> <span>uint8_t</span> <span><span>consume_chars</span></span><span><span><span>(</span></span></span><span><span>TSLexer <span>*</span><span>lexer</span><span>,</span> <span>char</span> <span>c</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  <span>uint8_t</span> count <span>=</span> <span>0</span><span>;</span>
  <span>while</span> <span><span>(</span>lexer<span>-&gt;</span>lookahead <span>==</span> c<span>)</span></span> <span><span>{</span>
    lexer<span>-&gt;</span><span><span>advance</span><span><span>(</span></span></span><span><span>lexer<span>,</span> <span>false</span></span></span><span><span><span>)</span></span></span><span>;</span>
    <span>+</span><span>+</span>count<span>;</span>
  <span>}</span></span>
  <span>return</span> count<span>;</span>
</span></span><span><span><span>}</span></span></span>
</span></code></pre></div>
<p>This is almost all we can do with the lexer.
We only process one character at a time, cannot look behind, and our only tool to look ahead is to <code>mark_end</code> at the correct place.
(We can also query the current column position.)</p>
<p>With this we have a working external scanner and div tags now close paragraphs:</p>
<div><pre><code><span>:::</span>
<span>A paragraph inside a div</span>
<span></span><span>:::</span>
</code></pre></div>
<div><pre><code><span>$ </span><span>tree-sitter</span> parse example-file
(document [0, 0] - [4, 0]
  (div [0, 0] - [3, 0]
    (div_marker [0, 0] - [0, 3])
    (paragraph [1, 0] - [2, 0])
    (div_marker [2, 0] - [2, 3])))
</code></pre></div>
</section>
<section id="Nested-blocks">
<h3><a href="#Nested-blocks">Nested blocks</a></h3>
<p>To automatically close other open blocks we need to add some context to our parser, which means we’ll need state management.</p>
<p>The small subset we’re implementing is only concerned with closing divs—because it would be a terribly long post otherwise—but I’ll try to implement this in a general manner, to be more indicative of a real-world parser.</p>
<p>Our strategy is this:</p>
<ol>
<li>
<p>A div can have a varying number of <code>:</code> that must match.</p>
<p>Therefore we’ll parse colons in an external scanner and store it on a stack.</p>
</li>
<li>
<p>When we find a div marker, we’ll need to decide if it should start a new div, or close an existing one.</p>
<p>We’ll look at the stack of open blocks and see if we find a match.</p>
</li>
<li>
<p>If we have need to close a nested div, that is if we want to close a div further down the stack, we need to close the nested div(s) first.</p>
<p>Thus we’ll introduce a <code>block_close</code> marker that ends a div, and leave the ending div marker as optional.</p>
</li>
</ol>
<p>First we’ll ask the grammar to let the external scanner manage the begin and end tokens.
We’ll use a <code>_block_close</code> marker to end the div, and leave the end marker optional.
(You could probably use a <code>choice()</code> between the two, but this made more sense to me when I was implementing it.)</p>
<div><pre><code><span><span>div</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span>
  prec<span>.</span><span><span>left</span></span><span><span><span><span>(</span></span></span></span><span><span>
    seq<span><span><span><span>(</span></span></span></span><span><span>
                  alias<span><span><span><span>(</span></span></span></span><span><span>$<span>.</span><span>_div_marker_begin</span><span>,</span> $<span>.</span><span>div_marker</span><span>)</span></span></span><span>,</span>
      <span><span><span>"</span><span>\n</span><span>"</span></span></span><span>,</span>
      repeat<span><span><span><span>(</span></span></span></span><span><span>$<span>.</span><span>_block</span><span>)</span></span></span><span>,</span>
      $<span>.</span><span>_block_close</span><span>,</span>
      optional<span><span><span><span>(</span></span></span></span><span><span>alias<span><span><span><span>(</span></span></span></span><span><span>$<span>.</span><span>_div_marker_end</span><span>,</span> $<span>.</span><span>div_marker</span><span>)</span></span></span><span>)</span></span></span>
    <span>)</span></span></span>
  <span>)</span></span></span><span>,</span>

<span>externals</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> <span><span>[</span>
  $<span>.</span><span>_close_paragraph</span><span>,</span>
  $<span>.</span><span>_block_close</span><span>,</span>
  $<span>.</span><span>_div_marker_begin</span><span>,</span>
  $<span>.</span><span>_div_marker_end</span><span>,</span>

  <span><span>//</span> This is used in the scanner internally,
</span>  <span><span>//</span> but shouldn't be used by the grammar.
</span>  $<span>.</span><span>_ignored</span><span>,</span>
<span>]</span></span><span>,</span>
</span></code></pre></div>
<p>And remember to update the list of external tokens in the scanner (order matters):</p>
<div><pre><code><span><span>typedef</span> <span>enum</span> <span><span>{</span>
  CLOSE_PARAGRAPH<span>,</span>
  BLOCK_CLOSE<span>,</span>
  DIV_MARKER_BEGIN<span>,</span>
  DIV_MARKER_END<span>,</span>
<span>  IGNORED
</span><span>}</span></span> <span>TokenType</span><span>;</span>
</span></code></pre></div>
<p>Then to our stack of blocks.</p>
<p>I used a <code>Block</code> type to keep track of the type and number of colons:</p>
<div><pre><code><span><span><span>//</span> In a real implementation we'll have more block types.
</span><span>typedef</span> <span>enum</span> <span><span>{</span> DIV <span>}</span></span> <span>BlockType</span><span>;</span>

<span>typedef</span> <span>struct</span> <span><span>{</span>
  BlockType type<span>;</span>
  <span>uint8_t</span> level<span>;</span>
<span>}</span></span> <span>Block</span><span>;</span>
</span></code></pre></div>
<p>I know that <code>level</code> isn’t the best name, but I couldn’t find a very good general name for the number of colons, indentation level, etc.
With sum types you could model it in a clearer way, like this:</p>
<div><pre><code><span><span><span>enum</span> <span>Block</span> <span><span>{</span>
    Div <span><span>{</span> colons<span>:</span> <span>u32</span> </span><span><span>}</span></span><span>,</span>
    Footnote <span><span>{</span> indent<span>:</span> <span>u32</span> </span><span><span>}</span></span><span>,</span>
    <span> etc
</span></span><span><span>}</span></span></span>
</span></code></pre></div>
<blockquote>
<p>I will, in fact, claim that the difference between a bad programmer and a good one
is whether he considers his code or his data structures more important.
Bad programmers worry about the code.
Good programmers worry about data structures and their relationships.
</p>

</blockquote>
<p>But I digress, I’ll go with <code>level</code> like a bad programmer.</p>
<p>Another joy of programming C is that you’ll get to re-implement standard data structures such as a growable stack.
It’s not truly difficult, but it’s annoying and bug-prone.</p>
<p>Luckily, during the time I’m writing this blog post, <a href="https://github.com/tree-sitter/tree-sitter/releases/tag/v0.22.1">tree-sitter 0.22.1</a> was released with an array implementation.
So now I don’t have to show you my shoddy stack implementation, and we can use their array for our stack instead.</p>
<p>We’ll shove our <code>Array</code> of <code>Block*</code> into a <code>Scanner</code> struct, because we’ll need to track more data later:</p>
<div><pre><code><span><span><span>#include</span> <span><span>"</span>tree_sitter/array.h<span>"</span></span>
</span>
<span>typedef</span> <span>struct</span> <span><span>{</span>
  <span><span>Array</span><span><span>(</span></span></span><span><span>Block <span>*</span></span></span><span><span><span>)</span></span></span> <span>*</span> open_blocks<span>;</span>
<span>}</span></span> <span>Scanner</span><span>;</span>
</span></code></pre></div>
<p>When you manage state in tree-sitter, you need to do some data management in the <code>tree_sitter_</code> functions we defined earlier.</p>
<p>Allocations are managed in the <code>_create</code> and <code>_destroy</code> functions.
Also new for 0.22.1 is the recommendation to use <code>ts_</code> functions for allocations, to allow consumers to override the default allocator:</p>
<div><pre><code><span><span><span>#include</span> <span><span>"</span>tree_sitter/alloc.h<span>"</span></span>
</span>
<span>void</span> <span>*</span><span><span>tree_sitter_sdjot_external_scanner_create</span></span><span><span><span>(</span></span></span><span><span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  Scanner <span>*</span>s <span>=</span> <span><span>(</span>Scanner <span>*</span><span>)</span></span><span><span>ts_malloc</span><span><span>(</span></span></span><span><span><span>sizeof</span><span><span>(</span></span><span>Scanner</span><span><span>)</span></span></span></span><span><span><span>)</span></span></span><span>;</span>

  <span> This is how you create an empty array
</span>  s<span>-&gt;</span>open_blocks <span>=</span> <span><span>ts_malloc</span><span><span>(</span></span></span><span><span><span>sizeof</span><span><span>(</span></span><span><span><span>Array</span><span><span>(</span></span></span><span><span>Block <span>*</span></span></span><span><span><span>)</span></span></span></span><span><span>)</span></span></span></span><span><span><span>)</span></span></span><span>;</span>
  <span><span>array_init</span><span><span>(</span></span></span><span><span>s<span>-&gt;</span>open_blocks</span></span><span><span><span>)</span></span></span><span>;</span>

  <span>return</span> s<span>;</span>
</span></span><span><span><span>}</span></span></span>

<span>void</span> <span><span>tree_sitter_sdjot_external_scanner_destroy</span></span><span><span><span>(</span></span></span><span><span><span>void</span> <span>*</span><span>payload</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  Scanner <span>*</span>s <span>=</span> <span><span>(</span>Scanner <span>*</span><span>)</span></span>payload<span>;</span>

  <span> I haven't shown the allocation of the blocks yet,
</span>  <span> but keep in mind that `array_delete` does not deallocate any memory
</span>  <span> you store in the array itself.
</span>  <span>for</span> <span><span>(</span><span>size_t</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> s<span>-&gt;</span>open_blocks<span>-&gt;</span>size<span>;</span> <span>+</span><span>+</span>i<span>)</span></span> <span><span>{</span>
        <span><span>ts_free</span><span><span>(</span></span></span><span><span><span><span>array_get</span><span><span>(</span></span></span><span><span>s<span>-&gt;</span>open_blocks<span>,</span> i</span></span><span><span><span>)</span></span></span></span></span><span><span><span>)</span></span></span><span>;</span>
  <span>}</span></span>

  <span> The array is a growable one, `array_delete` ensures that the
</span>  <span> memory is deleted.
</span>  <span><span>array_delete</span><span><span>(</span></span></span><span><span>s<span>-&gt;</span>open_blocks</span></span><span><span><span>)</span></span></span><span>;</span>

  <span><span>ts_free</span><span><span>(</span></span></span><span><span>s</span></span><span><span><span>)</span></span></span><span>;</span>
</span></span><span><span><span>}</span></span></span>
</span></code></pre></div>
<p>I allocate the blocks in a <code>push_block</code> helper:</p>
<div><pre><code><span><span>static</span> <span>void</span> <span><span>push_block</span></span><span><span><span>(</span></span></span><span><span>Scanner <span>*</span><span>s</span><span>,</span> BlockType <span>type</span><span>,</span> <span>uint8_t</span> <span>level</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  Block <span>*</span>b <span>=</span> <span><span>ts_malloc</span><span><span>(</span></span></span><span><span><span>sizeof</span><span><span>(</span></span><span>Block</span><span><span>)</span></span></span></span><span><span><span>)</span></span></span><span>;</span>
  b<span>-&gt;</span>type <span>=</span> type<span>;</span>
  b<span>-&gt;</span>level <span>=</span> level<span>;</span>

  <span> Grows the stack automatically.
</span>  <span><span>array_push</span><span><span>(</span></span></span><span><span>s<span>-&gt;</span>open_blocks<span>,</span> b</span></span><span><span><span>)</span></span></span><span>;</span>
</span></span><span><span><span>}</span></span></span>
</span></code></pre></div>
<p>You also need to define the serialize functions.
These store and retrieve the managed state, to allow tree-sitter to backtrack.</p>
<div><pre><code><span><span>unsigned</span> <span><span>tree_sitter_sdjot_external_scanner_serialize</span></span><span><span><span>(</span></span></span><span><span><span>void</span> <span>*</span><span>payload</span><span>,</span>
                                                      <span>char</span> <span>*</span><span>buffer</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  Scanner <span>*</span>s <span>=</span> <span><span>(</span>Scanner <span>*</span><span>)</span></span>payload<span>;</span>
  <span>unsigned</span> size <span>=</span> <span>0</span><span>;</span>
  <span>for</span> <span><span>(</span><span>size_t</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> s<span>-&gt;</span>open_blocks<span>-&gt;</span>size<span>;</span> <span>+</span><span>+</span>i<span>)</span></span> <span><span>{</span>
    Block <span>*</span>b <span>=</span> <span>*</span><span><span>array_get</span><span><span>(</span></span></span><span><span>s<span>-&gt;</span>open_blocks<span>,</span> i</span></span><span><span><span>)</span></span></span><span>;</span>
    buffer<span><span>[</span>size<span>+</span><span>+</span><span>]</span></span> <span>=</span> <span><span>(</span><span>char</span><span>)</span></span>b<span>-&gt;</span>type<span>;</span>
    buffer<span><span>[</span>size<span>+</span><span>+</span><span>]</span></span> <span>=</span> <span><span>(</span><span>char</span><span>)</span></span>b<span>-&gt;</span>level<span>;</span>
  <span>}</span></span>
  <span>return</span> size<span>;</span>
</span></span><span><span><span>}</span></span></span>

<span>void</span> <span><span>tree_sitter_sdjot_external_scanner_deserialize</span></span><span><span><span>(</span></span></span><span><span><span>void</span> <span>*</span><span>payload</span><span>,</span> <span>char</span> <span>*</span><span>buffer</span><span>,</span>
                                                    <span>unsigned</span> <span>length</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  Scanner <span>*</span>s <span>=</span> <span><span>(</span>Scanner <span>*</span><span>)</span></span>payload<span>;</span>
  <span><span>array_init</span><span><span>(</span></span></span><span><span>s<span>-&gt;</span>open_blocks</span></span><span><span><span>)</span></span></span><span>;</span>
  <span>size_t</span> size <span>=</span> <span>0</span><span>;</span>
  <span>while</span> <span><span>(</span>size <span>&lt;</span> length<span>)</span></span> <span><span>{</span>
    BlockType type <span>=</span> <span><span>(</span>BlockType<span>)</span></span>buffer<span><span>[</span>size<span>+</span><span>+</span><span>]</span></span><span>;</span>
    <span>uint8_t</span> level <span>=</span> <span><span>(</span><span>uint8_t</span><span>)</span></span>buffer<span><span>[</span>size<span>+</span><span>+</span><span>]</span></span><span>;</span>
    <span><span>push_block</span><span><span>(</span></span></span><span><span>s<span>,</span> type<span>,</span> level</span></span><span><span><span>)</span></span></span><span>;</span>
  <span>}</span></span>
</span></span><span><span><span>}</span></span></span>
</span></code></pre></div>
<p>And that’s the (initial) state management taken care of!</p>

</section>
<section id="Div-markers">
<h3><a href="#Div-markers">Div markers</a></h3>
<p>Of course, we haven’t used our state yet.
Let’s change that.</p>
<p>First, let’s add the <code>parse_div</code> entry point to our scan function:</p>
<div><pre><code><span><span>bool</span> <span><span>tree_sitter_sdjot_external_scanner_scan</span></span><span><span><span>(</span></span></span><span><span><span>void</span> <span>*</span><span>payload</span><span>,</span> TSLexer <span>*</span><span>lexer</span><span>,</span>
                                             <span>const</span> <span>bool</span> <span>*</span><span>valid_symbols</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  Scanner <span>*</span>s <span>=</span> <span><span>(</span>Scanner <span>*</span><span>)</span></span>payload<span>;</span>

  <span> Paragraph needs to be closed before we try to close divs.
</span>  <span>if</span> <span><span>(</span>valid_symbols<span><span>[</span>CLOSE_PARAGRAPH<span>]</span></span> <span>&amp;&amp;</span> <span><span>parse_close_paragraph</span><span><span>(</span></span></span><span><span>lexer</span></span><span><span><span>)</span></span></span><span>)</span></span> <span><span>{</span>
    <span>return</span> <span>true</span><span>;</span>
  <span>}</span></span>

  <span> Check `valid_symbols` inside `parse_div` because of multiple valid symbols.
</span>  <span>if</span> <span><span>(</span><span><span>parse_div</span><span><span>(</span></span></span><span><span>s<span>,</span> lexer<span>,</span> valid_symbols</span></span><span><span><span>)</span></span></span><span>)</span></span> <span><span>{</span>
    <span>return</span> <span>true</span><span>;</span>
  <span>}</span></span>

  <span>return</span> <span>false</span><span>;</span>
</span></span><span><span><span>}</span></span></span>
</span></code></pre></div>
<p>Because advancing the lexer is primitive, and we cannot “go back a char”, it’s important to only advance it if we really need to.
Therefore we always need to check <code>valid_symbols</code> before we continue:</p>
<div><pre><code><span><span>static</span> <span>bool</span> <span><span>parse_div</span></span><span><span><span>(</span></span></span><span><span>Scanner <span>*</span><span>s</span><span>,</span> TSLexer <span>*</span><span>lexer</span><span>,</span> <span>const</span> <span>bool</span> <span>*</span><span>valid_symbols</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  <span>if</span> <span><span>(</span><span>!</span>valid_symbols<span><span>[</span>DIV_MARKER_BEGIN<span>]</span></span> <span>&amp;&amp;</span> <span>!</span>valid_symbols<span><span>[</span>DIV_MARKER_END<span>]</span></span><span>)</span></span> <span><span>{</span>
    <span>return</span> <span>false</span><span>;</span>
  <span>}</span></span>

  <span> ...
</span></span></span><span><span><span>}</span></span></span>
</span></code></pre></div>
<p>Next we’ll need to consume all colons we’re at, and only continue if we see at least three:</p>
<div><pre><code><span><span>static</span> <span>uint8_t</span> <span><span>consume_chars</span></span><span><span><span>(</span></span></span><span><span>TSLexer <span>*</span><span>lexer</span><span>,</span> <span>char</span> <span>c</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  <span>uint8_t</span> count <span>=</span> <span>0</span><span>;</span>
  <span>while</span> <span><span>(</span>lexer<span>-&gt;</span>lookahead <span>==</span> c<span>)</span></span> <span><span>{</span>
    lexer<span>-&gt;</span><span><span>advance</span><span><span>(</span></span></span><span><span>lexer<span>,</span> <span>false</span></span></span><span><span><span>)</span></span></span><span>;</span>
    <span>+</span><span>+</span>count<span>;</span>
  <span>}</span></span>
  <span>return</span> count<span>;</span>
</span></span><span><span><span>}</span></span></span>

<span>static</span> <span>bool</span> <span><span>parse_div</span></span><span><span><span>(</span></span></span><span><span>Scanner <span>*</span><span>s</span><span>,</span> TSLexer <span>*</span><span>lexer</span><span>,</span> <span>const</span> <span>bool</span> <span>*</span><span>valid_symbols</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  <span> ...
</span>
  <span>uint8_t</span> colons <span>=</span> <span><span>consume_chars</span><span><span>(</span></span></span><span><span>lexer<span>,</span> <span><span>'</span>:<span>'</span></span></span></span><span><span><span>)</span></span></span><span>;</span>
  <span>if</span> <span><span>(</span>colons <span>&lt;</span> <span>3</span><span>)</span></span> <span><span>{</span>
    <span>return</span> <span>false</span><span>;</span>
  <span>}</span></span>

  <span> ...
</span></span></span><span><span><span>}</span></span></span>
</span></code></pre></div>
<p>Opening a new div is simple; we push the block and register the number of colons:</p>
<div><pre><code><span><span><span>push_block</span><span><span>(</span></span></span><span><span>s<span>,</span> DIV<span>,</span> colons</span><span><span>)</span></span></span><span>;</span>
lexer<span>-&gt;</span>result_symbol <span>=</span> DIV_MARKER_BEGIN<span>;</span>
<span>return</span> <span>true</span><span>;</span>
</span></code></pre></div>
<p>But to the decide if we should open or close a div, we need a way to search through the stack.
This function does that, while also returning how many blocks deep into the stack we found the div (which we’ll use shortly):</p>
<div><pre><code><span><span><span>//</span> How many blocks from the top of the stack can we find a matching block?
</span><span><span>//</span> If it's directly on the top, returns 1.
</span><span><span>//</span> If it cannot be found, returns 0.
</span><span>static</span> <span>size_t</span> <span><span>number_of_blocks_from_top</span></span><span><span><span>(</span></span></span><span><span>Scanner <span>*</span><span>s</span><span>,</span> BlockType <span>type</span><span>,</span>
                                        <span>uint8_t</span> <span>level</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  <span>for</span> <span><span>(</span><span>int</span> i <span>=</span> s<span>-&gt;</span>open_blocks<span>-&gt;</span>size <span>-</span> <span>1</span><span>;</span> i <span>&gt;=</span> <span>0</span><span>;</span> <span>-</span><span>-</span>i<span>)</span></span> <span><span>{</span>
    Block <span>*</span>b <span>=</span> <span>*</span><span><span>array_get</span><span><span>(</span></span></span><span><span>s<span>-&gt;</span>open_blocks<span>,</span> i</span></span><span><span><span>)</span></span></span><span>;</span>
    <span>if</span> <span><span>(</span>b<span>-&gt;</span>type <span>==</span> type <span>&amp;&amp;</span> b<span>-&gt;</span>level <span>==</span> level<span>)</span></span> <span><span>{</span>
      <span>return</span> s<span>-&gt;</span>open_blocks<span>-&gt;</span>size <span>-</span> i<span>;</span>
    <span>}</span></span>
  <span>}</span></span>
  <span>return</span> <span>0</span><span>;</span>
</span></span><span><span><span>}</span></span></span>

<span>static</span> <span>bool</span> <span><span>parse_div</span></span><span><span><span>(</span></span></span><span><span>Scanner <span>*</span><span>s</span><span>,</span> TSLexer <span>*</span><span>lexer</span><span>,</span> <span>const</span> <span>bool</span> <span>*</span><span>valid_symbols</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  <span> ...
</span>
  <span>size_t</span> from_top <span>=</span> <span><span>number_of_blocks_from_top</span><span><span>(</span></span></span><span><span>s<span>,</span> DIV<span>,</span> colons</span></span><span><span><span>)</span></span></span><span>;</span>

  <span> We could check if either DIV_MARKER_BEGIN or DIV_MARKER_END are valid here,
</span>  <span> but as the grammar is set up they're both always valid at the same time.
</span>  <span>if</span> <span><span>(</span>from_top <span>&gt;</span> <span>0</span><span>)</span></span> <span><span>{</span>
      <span>}</span></span> <span>else</span> <span><span>{</span>
        lexer<span>-&gt;</span><span><span>mark_end</span><span><span>(</span></span></span><span><span>lexer</span></span><span><span><span>)</span></span></span><span>;</span>
    <span><span>push_block</span><span><span>(</span></span></span><span><span>s<span>,</span> DIV<span>,</span> colons</span></span><span><span><span>)</span></span></span><span>;</span>
    lexer<span>-&gt;</span>result_symbol <span>=</span> DIV_MARKER_BEGIN<span>;</span>
    <span>return</span> <span>true</span><span>;</span>
  <span>}</span></span>
</span></span><span><span><span>}</span></span></span>
</span></code></pre></div>
<p>But we have a problem: when we want to close the div, we want to be able to output multiple tokens.</p>
<p>For example, with this type of input:</p>
<div><pre><code><span>:::</span>
<span>:::::</span>
<span>:::::::</span>
<span>text</span>
<span></span><span>:::</span>
</code></pre></div>
<p>We’ll have a stack of 3 divs when we see the closing <code>:::</code> marker:</p>
<div><pre><code>7 (top)
5
3 (the one we want to close)
</code></pre></div>
<p>In the code above, <code>from_top</code> will be <code>3</code> and we need to output 4 tokens: 3 <code>BLOCK_CLOSE</code> (one for each div) and 1 <code>DIV_MARKER_END</code> (for the last <code>:::</code>).
But the scanner can only output a single token at a time.</p>
<p>The way I solved this is by introducing more state to the Scanner.
Specifically, I introduced a <code>blocks_to_close</code> variable that we’ll use to output <code>BLOCK_CLOSE</code>, and some variables to output (and consume) the <code>DIV_MARKER_END</code>.</p>
<div><pre><code><span><span>typedef</span> <span>struct</span> <span><span>{</span>
  <span><span>Array</span><span><span>(</span></span></span><span><span>Block <span>*</span></span></span><span><span><span>)</span></span></span> <span>*</span> open_blocks<span>;</span>

  <span><span>//</span> How many BLOCK_CLOSE we should output right now?
</span>  <span>uint8_t</span> blocks_to_close<span>;</span>

  <span><span>//</span> Delayed output of a token.
</span>  TokenType delayed_token<span>;</span>
  <span><span>//</span> Allows us to consume the width of a delayed token.
</span>  <span>uint8_t</span> delayed_token_width<span>;</span>
<span>}</span></span> <span>Scanner</span><span>;</span>
</span></code></pre></div>
<p>We need to remember to update the create and serialize functions too.</p>
<p>Serialize:</p>
<div><pre><code><span>buffer<span><span>[</span>size<span>+</span><span>+</span><span>]</span></span> <span>=</span> <span><span>(</span><span>char</span><span>)</span></span>s<span>-&gt;</span>blocks_to_close<span>;</span>
buffer<span><span>[</span>size<span>+</span><span>+</span><span>]</span></span> <span>=</span> <span><span>(</span><span>char</span><span>)</span></span>s<span>-&gt;</span>delayed_token<span>;</span>
buffer<span><span>[</span>size<span>+</span><span>+</span><span>]</span></span> <span>=</span> <span><span>(</span><span>char</span><span>)</span></span>s<span>-&gt;</span>delayed_token_width<span>;</span>
</span></code></pre></div>
<p>Deserialize:</p>
<div><pre><code><span>s<span>-&gt;</span>blocks_to_close <span>=</span> <span><span>(</span><span>uint8_t</span><span>)</span></span>buffer<span><span>[</span>size<span>+</span><span>+</span><span>]</span></span><span>;</span>
s<span>-&gt;</span>delayed_token <span>=</span> <span><span>(</span>TokenType<span>)</span></span>buffer<span><span>[</span>size<span>+</span><span>+</span><span>]</span></span><span>;</span>
s<span>-&gt;</span>delayed_token_width <span>=</span> <span><span>(</span><span>uint8_t</span><span>)</span></span>buffer<span><span>[</span>size<span>+</span><span>+</span><span>]</span></span><span>;</span>
</span></code></pre></div>
<p>We’ll use <code>IGNORED</code> as the unused token, so we’ll need to reset it when we create the scanner:</p>
<div><pre><code><span>s<span>-&gt;</span>blocks_to_close <span>=</span> <span>0</span><span>;</span>
s<span>-&gt;</span>delayed_token <span>=</span> IGNORED<span>;</span>
</span></code></pre></div>
<p>Now when we scan we should first check <code>blocks_to_close</code> and then <code>delayed_token</code>, before we scan other things:</p>
<div><pre><code><span><span>bool</span> <span><span>tree_sitter_sdjot_external_scanner_scan</span></span><span><span><span>(</span></span></span><span><span><span>void</span> <span>*</span><span>payload</span><span>,</span> TSLexer <span>*</span><span>lexer</span><span>,</span>
                                             <span>const</span> <span>bool</span> <span>*</span><span>valid_symbols</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  Scanner <span>*</span>s <span>=</span> <span><span>(</span>Scanner <span>*</span><span>)</span></span>payload<span>;</span>

  <span>if</span> <span><span>(</span>valid_symbols<span><span>[</span>BLOCK_CLOSE<span>]</span></span> <span>&amp;&amp;</span> <span><span>handle_blocks_to_close</span><span><span>(</span></span></span><span><span>s<span>,</span> lexer</span></span><span><span><span>)</span></span></span><span>)</span></span> <span><span>{</span>
    <span>return</span> <span>true</span><span>;</span>
  <span>}</span></span>

  <span>if</span> <span><span>(</span><span><span>output_delayed_token</span><span><span>(</span></span></span><span><span>s<span>,</span> lexer<span>,</span> valid_symbols</span></span><span><span><span>)</span></span></span><span>)</span></span> <span><span>{</span>
    <span>return</span> <span>true</span><span>;</span>
  <span>}</span></span>

  <span> Scan the other stuff
</span></span></span><span><span><span>}</span></span></span>
</span></code></pre></div>
<p>When we see <code>blocks_to_close &gt; 0</code>, we should output a <code>BLOCK_CLOSE</code> and remove the top block (with some sanity checks for good measure):</p>
<div><pre><code><span><span>static</span> <span>void</span> <span><span>remove_block</span></span><span><span><span>(</span></span></span><span><span>Scanner <span>*</span><span>s</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  <span>if</span> <span><span>(</span>s<span>-&gt;</span>open_blocks<span>-&gt;</span>size <span>&gt;</span> <span>0</span><span>)</span></span> <span><span>{</span>
    <span><span>ts_free</span><span><span>(</span></span></span><span><span><span><span>array_pop</span><span><span>(</span></span></span><span><span>s<span>-&gt;</span>open_blocks</span></span><span><span><span>)</span></span></span></span></span><span><span><span>)</span></span></span><span>;</span>
    <span>if</span> <span><span>(</span>s<span>-&gt;</span>blocks_to_close <span>&gt;</span> <span>0</span><span>)</span></span> <span><span>{</span>
      <span>-</span><span>-</span>s<span>-&gt;</span>blocks_to_close<span>;</span>
    <span>}</span></span>
  <span>}</span></span>
</span></span><span><span><span>}</span></span></span>

<span>static</span> <span>bool</span> <span><span>handle_blocks_to_close</span></span><span><span><span>(</span></span></span><span><span>Scanner <span>*</span><span>s</span><span>,</span> TSLexer <span>*</span><span>lexer</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  <span>if</span> <span><span>(</span>s<span>-&gt;</span>open_blocks<span>-&gt;</span>size <span>==</span> <span>0</span><span>)</span></span> <span><span>{</span>
    <span>return</span> <span>false</span><span>;</span>
  <span>}</span></span>

  <span> If we reach eof with open blocks, we should close them all.
</span>  <span>if</span> <span><span>(</span>lexer<span>-&gt;</span><span><span>eof</span><span><span>(</span></span></span><span><span>lexer</span></span><span><span><span>)</span></span></span> <span>||</span> s<span>-&gt;</span>blocks_to_close <span>&gt;</span> <span>0</span><span>)</span></span> <span><span>{</span>
    lexer<span>-&gt;</span>result_symbol <span>=</span> BLOCK_CLOSE<span>;</span>
    <span><span>remove_block</span><span><span>(</span></span></span><span><span>s</span></span><span><span><span>)</span></span></span><span>;</span>
    <span>return</span> <span>true</span><span>;</span>
  <span>}</span></span>
  <span>return</span> <span>false</span><span>;</span>
</span></span><span><span><span>}</span></span></span>
</span></code></pre></div>
<p>With this we can output multiple <code>BLOCK_CLOSE</code>, and now to handle delayed tokens:</p>
<div><pre><code><span><span>static</span> <span>bool</span> <span><span>output_delayed_token</span></span><span><span><span>(</span></span></span><span><span>Scanner <span>*</span><span>s</span><span>,</span> TSLexer <span>*</span><span>lexer</span><span>,</span>
                          <span>const</span> <span>bool</span> <span>*</span><span>valid_symbols</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  <span>if</span> <span><span>(</span>s<span>-&gt;</span>delayed_token <span>==</span> IGNORED <span>||</span> <span>!</span>valid_symbols<span><span>[</span>s<span>-&gt;</span>delayed_token<span>]</span></span><span>)</span></span> <span><span>{</span>
    <span>return</span> <span>false</span><span>;</span>
  <span>}</span></span>

  lexer<span>-&gt;</span>result_symbol <span>=</span> s<span>-&gt;</span>delayed_token<span>;</span>
  s<span>-&gt;</span>delayed_token <span>=</span> IGNORED<span>;</span>
  <span> With `delayed_token_width` we can consume the ending `:::`, for example.
</span>  <span>while</span> <span><span>(</span>s<span>-&gt;</span>delayed_token_width<span>-</span><span>-</span><span>)</span></span> <span><span>{</span>
    lexer<span>-&gt;</span><span><span>advance</span><span><span>(</span></span></span><span><span>lexer<span>,</span> <span>false</span></span></span><span><span><span>)</span></span></span><span>;</span>
  <span>}</span></span>
  lexer<span>-&gt;</span><span><span>mark_end</span><span><span>(</span></span></span><span><span>lexer</span></span><span><span><span>)</span></span></span><span>;</span>
  <span>return</span> <span>true</span><span>;</span>
</span></span><span><span><span>}</span></span></span>
</span></code></pre></div>
<p>Another way to design this is to have a stack of delayed tokens and then just pop that.
It’s certainly more powerful, I just happened to choose this way when I was playing around with it because it’s more explicit and it felt a little easier to follow what was happening.</p>
<p>Either way, we can now implement the div end handling. In <code>parse_div</code>:</p>
<div><pre><code><span><span>size_t</span> from_top <span>=</span> <span><span>number_of_blocks_from_top</span><span><span>(</span></span></span><span><span>s<span>,</span> DIV<span>,</span> colons</span></span><span><span><span>)</span></span></span><span>;</span>

<span>if</span> <span><span>(</span>from_top <span>&gt;</span> <span>0</span><span>)</span></span> <span><span>{</span>
  <span><span>//</span> Found a div we should close.
</span>  <span><span>close_blocks_with_final_token</span><span><span>(</span></span></span><span><span>s<span>,</span> lexer<span>,</span> from_top<span>,</span> DIV_MARKER_END<span>,</span> colons</span></span><span><span><span>)</span></span></span><span>;</span>
  <span>return</span> <span>true</span><span>;</span>
<span>}</span></span> <span>else</span> <span><span>{</span>
  lexer<span>-&gt;</span><span><span>mark_end</span><span><span>(</span></span></span><span><span>lexer</span></span><span><span><span>)</span></span></span><span>;</span>
  <span><span>push_block</span><span><span>(</span></span></span><span><span>s<span>,</span> DIV<span>,</span> colons</span></span><span><span><span>)</span></span></span><span>;</span>
  lexer<span>-&gt;</span>result_symbol <span>=</span> DIV_MARKER_BEGIN<span>;</span>
  <span>return</span> <span>true</span><span>;</span>
<span>}</span></span>
</span></code></pre></div>
<p><code>close_blocks_with_final_token</code> is a general helper that sets up the number of blocks to close and the final token:</p>
<div><pre><code><span><span>static</span> <span>void</span> <span><span>close_blocks_with_final_token</span></span><span><span><span>(</span></span></span><span><span>Scanner <span>*</span><span>s</span><span>,</span> TSLexer <span>*</span><span>lexer</span><span>,</span>
                                          <span>size_t</span> <span>count</span><span>,</span> TokenType <span>final</span><span>,</span>
                                          <span>uint8_t</span> <span>final_token_width</span><span>)</span></span></span><span> </span><span><span><span>{</span></span></span><span><span>
  <span><span>remove_block</span><span><span>(</span></span></span><span><span>s</span></span><span><span><span>)</span></span></span><span>;</span>
  s<span>-&gt;</span>blocks_to_close <span>=</span> s<span>-&gt;</span>blocks_to_close <span>+</span> count <span>-</span> <span>1</span><span>;</span>
  lexer<span>-&gt;</span>result_symbol <span>=</span> BLOCK_CLOSE<span>;</span>
  s<span>-&gt;</span>delayed_token <span>=</span> final<span>;</span>
  s<span>-&gt;</span>delayed_token_width <span>=</span> final_token_width<span>;</span>
</span></span><span><span><span>}</span></span></span>
</span></code></pre></div>
<p>Now we can finally try to close divs:</p>
<div><pre><code><span>:::::</span>
<span>:::</span>
<span>:::::::</span>
<span>Divception</span>
<span></span><span>:::</span>
</code></pre></div>
<div><pre><code><span>$ </span><span>tree-sitter</span> parse example-file
(document [0, 0] - [6, 0]
  (div [0, 0] - [6, 0]
    (div_marker [0, 0] - [0, 5])
    (div [1, 0] - [4, 3]
      (div_marker [1, 0] - [1, 3])
      (div [2, 0] - [4, 0]
        (div_marker [2, 0] - [2, 7])
        (paragraph [3, 0] - [4, 0]))
      (div_marker [4, 0] - [4, 3]))))
</code></pre></div>
<p>We can see that it parses without error, the last marker closes the <em>second</em> div correctly, and the last marker captures the final <code>:::</code>.</p>
<p>While I’m jumping to a working implementation directly in this post, when I first did this that was of course not the case.
I found the <code>-d</code> argument useful to see what characters are consumed and what token is output in each step.</p>
<p>Here’s a part of the output (when scanning the final <code>:::</code>), with some comments to point out some interesting things:</p>
<div><pre><code><span>$ </span><span>tree-sitter</span> parse example-file -d
...
process version:0, version_count:1, state:34, row:4, col:0
lex_external state:4, row:4, column:0
  consume character:':'                         // Scan `:::`
  consume character:':'
  consume character:':'
lexed_lookahead sym:_close_paragraph, size:0    // Output _close_paragraph
reduce sym:paragraph_repeat1, child_count:2
shift state:17
process version:0, version_count:1, state:17, row:4, col:0
lex_external state:3, row:4, column:0           // Still on first `:`
  consume character:':'                         // Scan `:::` again
  consume character:':'
  consume character:':'
lexed_lookahead sym:_block_close, size:0        // Close div with _block_close
reduce sym:paragraph, child_count:2
shift state:12
process version:0, version_count:1, state:12, row:4, col:0
lex_external state:5, row:4, column:0           // Still on first `:`
lexed_lookahead sym:_block_close, size:0        // Close second div with _block_close
reduce sym:div, child_count:4
shift state:12
process version:0, version_count:1, state:12, row:4, col:0
lex_external state:5, row:4, column:0           // Still on first `:`
  consume character:':'                         // Consume `:::`
  consume character:':'
  consume character:':'
lexed_lookahead sym:div_marker, size:3          // div_marker is size 3, marks `:::`
shift state:23
</code></pre></div>
<p>While the output seems confusing, when you know what to look for it’s very useful.
I’ve found that a deliberate process, where I look at a single character at a time, helps me get through the problems I’ve encountered so far.</p>
</section>
</section>
<section id="Handling-conflicts">
<h2><a href="#Handling-conflicts">Handling conflicts</a></h2>
<p>Our grammar works pretty well, but there are issues you might want to fix.
One issue, that took much longer to figure out than I care to admit, is adding a fallback to text when a markup rule doesn’t match.</p>
<p>A simple example for our grammar is a single underscore in a paragraph:</p>

<p>I’d assume this would produce a paragraph with text, but instead we get an error:</p>
<div><pre><code><span>$ </span><span>tree-sitter</span> parse example-file
(document [0, 0] - [2, 0]
  (ERROR [0, 0] - [0, 3]))
</code></pre></div>
<p>This is weird, because one of the main selling points of Tree-sitter is the GLR algorithm, which should explore the different interpretations to find something that succeeds.
But for some reason, it doesn’t trigger for us.</p>
<p>Let’s take a look.
These are the relevant lines from the grammar:</p>
<div><pre><code><span><span>_inline</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> repeat<span><span>1</span></span><span><span><span><span>(</span></span></span></span><span><span>choice<span><span><span><span>(</span></span></span></span><span><span>$<span>.</span><span>emphasis</span><span>,</span> $<span>.</span><span>_text</span><span>)</span></span></span><span>)</span></span></span><span>,</span>
<span>emphasis</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> prec<span>.</span><span><span>left</span></span><span><span><span><span>(</span></span></span></span><span><span>seq<span><span><span><span>(</span></span></span></span><span><span><span><span><span>"</span>_<span>"</span></span></span><span>,</span> $<span>.</span><span>_inline</span><span>,</span> <span><span><span>"</span>_<span>"</span></span></span><span>)</span></span></span><span>)</span></span></span><span>,</span>
<span>_text</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>_<span>)</span></span></span> =<span>&gt;</span> <span><span><span>/</span><span><span>[</span><span>^</span><span>\n</span><span>]</span></span><span>/</span></span></span><span></span><span>,</span>
</span></code></pre></div>
<p>When we try to match a <code>_</code> then the grammar can match either <code>emphasis</code> or <code>_text</code> because <code>_</code> matches both 
<code><span><span><span><span>"</span>_<span>"</span></span></span></span></code> and 
<code><span><span><span><span>/</span><span><span>[</span><span>^</span><span>\n</span><span>]</span></span><span>/</span></span></span><span></span></span></code>.
The issue seems to be that Tree-sitter doesn’t recognize this as a conflict.</p>
<p>If we instead add a fallback with a <code>_</code> string then Tree-sitter will treat it as a conflict:</p>
<div><pre><code><span><span>_inline</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> repeat<span><span>1</span></span><span><span><span><span>(</span></span></span></span><span><span>choice<span><span><span><span>(</span></span></span></span><span><span>$<span>.</span><span>emphasis</span><span>,</span> $<span>.</span><span>_text</span><span>,</span> $<span>.</span><span>_fallback</span><span>)</span></span></span><span>)</span></span></span><span>,</span>
<span>emphasis</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> prec<span>.</span><span><span>left</span></span><span><span><span><span>(</span></span></span></span><span><span>seq<span><span><span><span>(</span></span></span></span><span><span><span><span><span>"</span>_<span>"</span></span></span><span>,</span> $<span>.</span><span>_inline</span><span>,</span> <span><span><span>"</span>_<span>"</span></span></span><span>)</span></span></span><span>)</span></span></span><span>,</span>
<span><span>//</span> prec.dynamic() is used during conflict resolution to choose which
</span><span><span>//</span> branch to choose if multiple succeed.
</span><span>_fallback</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>_<span>)</span></span></span> =<span>&gt;</span> prec<span>.</span><span><span>dynamic</span></span><span><span><span><span>(</span></span></span></span><span><span><span>-</span><span><span>100</span></span><span>,</span> <span><span><span>"</span>_<span>"</span></span></span><span>)</span></span></span><span>,</span>
<span>_text</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>_<span>)</span></span></span> =<span>&gt;</span> <span><span><span>/</span><span><span>[</span><span>^</span><span>\n</span><span>]</span></span><span>/</span></span></span><span></span><span>,</span>
</span></code></pre></div>
<p>And when we call <code>tree-sitter generate</code> we’re made aware of the conflict:</p>
<div><pre><code><span>$ </span><span>tree-sitter</span> generate
Unresolved conflict for symbol sequence:

  '_'  •  '_'  …

Possible interpretations:

  1:  (_fallback  '_')  •  '_'  …
  2:  (emphasis  '_'  •  _inline  '_')  (precedence: 0, associativity: Left)

Possible resolutions:

  1:  Specify a higher precedence in `emphasis` than in the other rules.
  2:  Specify a higher precedence in `_fallback` than in the other rules.
  3:  Specify a left or right associativity in `_fallback`
  4:  Add a conflict for these rules: `emphasis`, `_fallback`
</code></pre></div>
<p>What we want to do is mark them as a conflict that’s supposed to exist in the grammar using the <code>conflicts</code> field:</p>
<div><pre><code><span><span>conflicts</span><span>:</span> <span><span><span><span>(</span></span></span></span><span><span>$<span>)</span></span></span> =<span>&gt;</span> <span><span>[</span><span><span>[</span>$<span>.</span><span>emphasis</span><span>,</span> $<span>.</span><span>_fallback</span><span>]</span></span><span>]</span></span><span>,</span>
</span></code></pre></div>
<p>And now we can parse paragraphs containing only a single <code>_</code> without errors.</p>
<p>So it seems like Tree-Sitter doesn’t recognize a conflict between a string and a regex.
Another gotcha is that it doesn’t seem like you can trigger the GLR algorithm with a token returned by an external scanner, because the external scanner overrules Tree-sitter’s lexing behavior.</p>
</section>
<section id="Some-tests">
<h2><a href="#Some-tests">Some tests</a></h2>
<p>Using <code>tree-sitter parse example-file</code> (with or without the <code>-d</code> or <code>-D</code> flags, try them if you haven’t) is fine for experimental tests, but we really should add the different test cases as proper unit tests.
Tree-sitter has a built-in test harness for this purpose.</p>
<p>Let’s add the very first test case to <code>test/corpus/syntax.txt</code>:</p>
<div><pre><code>===============================================================================
Parsing goal
===============================================================================
This is a
multiline _paragraph_

:::
This is a paragraph inside a div
:::

```gleam
let x = 2;
```

-------------------------------------------------------------------------------

(document
  (paragraph (emphasis))
  (div
    (div_marker)
    (paragraph)
    (div_marker))
  (code_block
    (code_block_marker)
    (language)
    (code)
    (code_block_marker)))
</code></pre></div>
<p>And run it:</p>
<div><pre><code>$ tree-sitter test
  syntax:
    ✓ Parsing goal
</code></pre></div>
<p>Yay!</p>
<p>We should add (a lot) more tests here, but I won’t bother writing them out in this already too long blog post.</p>
</section>
<section id="Using-tree-sitter-for-something-useful">
<h2><a href="#Using-tree-sitter-for-something-useful">Using tree-sitter for something useful</a></h2>
<p>I like a theoretical excursion as much as the next nerd, but I started looking at Tree-sitter because I wanted to <em>do</em> something with the Grammar, not just play around with it all day.
Let’s end the post by seeing some things we can use it for.</p>
<section id="Syntax-highlighting">
<h3><a href="#Syntax-highlighting">Syntax highlighting</a></h3>
<p>Syntax highlighting is made using queries from the <code>highlights.scm</code> file.
It’s common to have it placed in the <code>src</code> directory in the same repository as the grammar, but it’s not required.</p>
<p>Here’s an example <code>src/highlights.scm</code> file that highlights the different elements of our markup:</p>
<div><pre><code><span><span><span>(</span>div_marker<span>)</span></span> @punctuation.delimiter
<span><span>(</span>code_block_marker<span>)</span></span> @punctuation.delimiter

<span><span>(</span>emphasis <span><span>"</span>_<span>"</span></span> @punctuation.delimiter<span>)</span></span> @markup.italic
<span><span>(</span>language<span>)</span></span> @tag.attribute

<span><span>(</span>code_block<span>)</span></span> @markup.raw
<span><span>(</span>paragraph<span>)</span></span> @markup
</span></code></pre></div>
<p>What colors to choose is a bit arbitrary, these works well enough I suppose.</p>
<p>See the <a href="https://tree-sitter.github.io/tree-sitter/syntax-highlighting">documentation</a> for more details on how the queries and highlighting works.</p>
</section>
<section id="Language-injection">
<h3><a href="#Language-injection">Language injection</a></h3>
<p>One big question I had when starting writing my grammar was how to mix multiple parsers in the same document, to for example highlight code blocks using the specified language:</p>

<p>Turns out, this is quite straightforward.</p>
<p>With the initial grammar, the code block parses into:</p>
<div><pre><code>(code_block
  (code_block_marker)
  (language)
  (code)
  (code_block_marker)))
</code></pre></div>
<p>Which we’ll use in <code>src/injections.scm</code> to specify that we want to parse <code>(code)</code> using the grammar specified in <code>(language)</code>:</p>
<div><pre><code><span><span><span>(</span>code_block
  <span><span>(</span>language<span>)</span></span> @injection.language
  <span><span>(</span>code<span>)</span></span> @injection.content<span>)</span></span>
</span></code></pre></div>
<p>When we’ll embed the grammar into a program with highlighting support, it will delegate the text inside the code block to the injected language.</p>
</section>
<section id="Using-our-grammar-with-Neovim">
<h3><a href="#Using-our-grammar-with-Neovim">Using our grammar with Neovim</a></h3>
<figure><img alt="" src="https://www.jonashietala.se/images/sdjot_neovim.png">
</figure>
<p>I typically install Tree-sitter grammars in Neovim using <code>:TSInstall</code> provided by <a href="https://github.com/nvim-treesitter/nvim-treesitter">nvim-treesitter</a>.
But you can <a href="https://github.com/nvim-treesitter/nvim-treesitter#adding-parsers">install local Tree-sitter grammars</a> as well:</p>
<div><pre><code><span><span>local</span> parser_config <span>=</span> <span>require</span>(<span><span>"</span>nvim-treesitter.parsers<span>"</span></span>).get_parser_configs()
parser_config.sdjot <span>=</span> {
    install_info <span>=</span> {
        <span><span>--</span> Change this url to your grammar
</span>        url <span>=</span> <span><span>"</span>~/code/tree-sitter-sdjot<span>"</span></span>,
        <span><span>--</span> If you use an external scanner it needs to be included here
</span>        files <span>=</span> { <span><span>"</span>src/parser.c<span>"</span></span>, <span><span>"</span>src/scanner.c<span>"</span></span> },
        generate_reqires_npm <span>=</span> <span>false</span>,
        requires_generate_from_grammar <span>=</span> <span>false</span>,
    },
    <span><span>--</span> The filetype you want it registered as
</span>    filetype <span>=</span> <span><span>"</span>sdjot<span>"</span></span>,
}
</span></code></pre></div>
<p>Just make sure you have a <code>"tree-sitter"</code> section in the grammar’s <code>package.json</code>:</p>
<div><pre><code><span><span><span>"</span>tree-sitter<span>"</span></span>: <span><span>[</span>
  <span><span>{</span>
    <span><span><span>"</span>scope<span>"</span></span></span><span><span>:</span> <span><span>"</span>source.sdjot<span>"</span></span><span>,</span></span>
    <span><span><span>"</span>file-types<span>"</span></span></span><span><span>:</span> <span><span>[</span>
      <span><span>"</span>sdj<span>"</span></span>
    <span>]</span></span><span>,</span></span>
    <span><span><span>"</span>injection-regex<span>"</span></span></span><span><span>:</span> <span><span>"</span>sdjot<span>"</span></span><span>,</span></span>
    <span><span><span>"</span>highlights<span>"</span></span></span><span><span>:</span> <span><span>[</span>
      <span><span>"</span>queries/highlights.scm<span>"</span></span>
    <span>]</span></span>
  </span><span>}</span></span>
<span>]</span></span>,
</span></code></pre></div>
<p>With this you can do <code>:TSInstall sjdot</code> and <code>:TSUpdate sdjot</code> when you make changes.</p>
<p><code>:TSInstall</code> doesn’t install queries automatically though.
What I did was symlink the queries directory into Neovims config directory:</p>
<div><pre><code><span>ln</span> -s ~/code/tree-sitter-sdjot/queries ~/.config/nvim/queries/sdjot
</code></pre></div>
<p><code>:TSPlaygroundToggle</code> is very useful for debugging the grammar, and <code>:Inspect</code> shows you the highlight groups under your cursor.
It might be good to check out <code>:help treesitter-highlight-groups</code> if you want to play with your theme, as the theme needs to support the highlight groups we use for coloring to appear.</p>
<p>You also need to have the Tree-sitter grammar for the injected language installed, if you want to highlight the contents of code blocks.</p>
</section>
<section id="Jumping-and-selecting-with-textobjects">
<h3><a href="#Jumping-and-selecting-with-textobjects">Jumping and selecting with textobjects</a></h3>
<p>I mentioned <a href="https://github.com/nvim-treesitter/nvim-treesitter-textobjects">nvim-treesitter-textobjects</a> as a good example of why Tree-sitter is about more than syntax highlighting.</p>
<p>To make use of our grammar we can add some capture groups to <code>src/textobjects.scm</code>.
For example we can register our code blocks as “functions”:</p>
<div><pre><code><span><span><span>(</span>code_block <span><span>(</span>code<span>)</span></span> @function.inner<span>)</span></span> @function.outer
</span></code></pre></div>
<p>The objects are arbitrary, but <code>@function</code> is one of the standard objects so I guess it might make sense.</p>
<p>With the symlink ready, you need to register keymaps with <a href="https://github.com/nvim-treesitter/nvim-treesitter-textobjects">nvim-treesitter-textobjects</a> and you’re good to go.
I have it setup so I can jump between <code>@function.outer</code> with <code>[f</code> and <code>]f</code>, and selections with <code>af</code> and <code>if</code>.</p>
<p>This means that with the above textobject definition I can for example jump to the next code block with <code>]f</code> and then remove all the code inside with <code>cif</code> to end up in insert mode, ready to replace it with some new code.</p>
<p>Although this example is a bit arbitrary, this general functionality is <strong><em>extremely</em></strong> useful for programming languages.
For a markup language like <a href="https://djot.net/">Djot</a>, jumping between headings might be a more relevant use-case.</p>
</section>
<section id="Embedding-the-grammar-with-Rust">
<h3><a href="#Embedding-the-grammar-with-Rust">Embedding the grammar with Rust</a></h3>
<p>One of the selling points of Tree-sitter is that you should be able to embed it in any application.
Such as this blog!</p>
<p>I’ve been wanting to add Tree-sitter powered highlighting to my blog for a while, and now I have an excuse to do just that.</p>
<p>This blog is a static site generator written in Rust, and <a href="https://docs.rs/tree-sitter-highlight/latest/tree_sitter_highlight/">tree-sitter-highlight</a> looks like a suitable library to try.
Let’s add it to our <code>Cargo.toml</code>:</p>
<div><pre><code><span><span>[</span><span><span>dependencies</span></span><span>]</span>
<span><span>tree-sitter-highlight</span></span> <span>=</span> <span><span>"</span>^0.20.0<span>"</span></span>
<span><span>tree-sitter-sdjot</span></span> <span>=</span> <span>{</span> <span><span>git</span></span> <span>=</span> <span><span>"</span>https://github.com/treeman/tree-sitter-sdjot.git<span>"</span></span> <span>}</span>
</span></code></pre></div>
<p>I use a slightly older version because some other grammars I want depend on the older version, and it’s a big pain but they all need to use a matching version.
Bleh.</p>
<p><a href="https://docs.rs/tree-sitter-highlight/latest/tree_sitter_highlight/">According to the docs</a> we first need to setup a <code>HighlightConfiguration</code>.
I used <code>lazy_static!</code> to create a global map with configurations to be used by any parallel rendering on the blog.
It’s not the most beautiful code I’ve written, but it gets the job done:</p>
<div><pre><code><span><span><span>//</span> All highlights needs to be listed explicitly.
</span><span>static</span> <span>HIGHLIGHT_NAMES</span><span>:</span> <span>&amp;</span><span><span>[</span><span>&amp;</span><span>str</span><span>]</span></span> <span>=</span> <span>&amp;</span><span><span>[</span>
    <span><span>//</span> I have +100 entries here, this is for sdjot.
</span>   <span><span>"</span>markup<span>"</span></span><span>,</span>
   <span><span>"</span>markup.italic<span>"</span></span><span>,</span>
   <span><span>"</span>markup.raw<span>"</span></span><span>,</span>
   <span><span>"</span>punctuation.delimiter<span>"</span></span><span>,</span>
   <span><span>"</span>tag.attribute<span>"</span></span><span>,</span>
<span>]</span></span><span>;</span>

<span>lazy_static!</span> <span><span>{</span>
    <span>static</span> <span>ref</span> <span>CONFIGS</span><span>:</span> <span>HashMap<span>&lt;</span>String, HighlightConfiguration<span>&gt;</span></span> <span>=</span> <span>init_configurations</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
</span><span><span>}</span></span>

<span><span><span>fn</span> </span><span>init_configurations</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>HashMap<span>&lt;</span>String, HighlightConfiguration<span>&gt;</span></span></span> </span><span><span><span>{</span>
    <span><span>[</span>
                <span><span>(</span>
            <span><span>"</span>sdjot<span>"</span></span><span>,</span>
            <span>HighlightConfiguration<span>::</span></span>new<span><span>(</span>
                <span>tree_sitter_sdjot<span>::</span></span>language<span><span>(</span></span><span><span>)</span></span><span>,</span>
                <span>tree_sitter_sdjot<span>::</span></span><span>HIGHLIGHTS_QUERY</span><span>,</span>
                <span>tree_sitter_sdjot<span>::</span></span><span>INJECTIONS_QUERY</span><span>,</span>
                <span><span>"</span><span>"</span></span><span>,</span>
            </span><span><span>)</span></span>
            .<span>unwrap</span><span><span>(</span></span><span><span>)</span></span><span>,</span>
        </span><span><span>)</span></span><span>,</span>
    <span>]</span></span>
    .<span>into_iter</span><span><span>(</span></span><span><span>)</span></span>
    .<span>map</span><span><span>(</span><span><span><span>|</span></span></span><span><span><span><span>(</span></span><span><span>name</span><span>,</span> <span>mut</span> <span>config</span></span><span><span>)</span></span><span>|</span></span> </span><span><span><span>{</span>
        config.<span>configure</span><span><span>(</span><span>&amp;</span><span>HIGHLIGHT_NAMES</span></span><span><span>)</span></span><span>;</span>
        <span><span>(</span>name.<span>to_string</span><span><span>(</span></span><span><span>)</span></span><span>,</span> config</span><span><span>)</span></span>
    </span><span><span>}</span></span></span></span><span><span>)</span></span>
    .<span>collect</span><span><span>(</span></span><span><span>)</span></span>
</span><span><span>}</span></span></span>
</span></code></pre></div>
<p>Notice how all highlight names we’re interested in have to be explicitly specified.
This is a big pain, especially if you’re going to include many larger grammars.</p>
<p>The names can be filtered for with <a href="https://github.com/BurntSushi/ripgrep">ripgrep</a> with something like this:</p>
<div><pre><code><span>rg</span> <span>"@[<span>\w</span>.]+"</span> -INo --trim highlights.scm <span>|</span> <span>sort</span> <span>|</span> <span>uniq</span>
</code></pre></div>
<p>I already have syntax highlighting via <a href="https://github.com/trishume/syntect">syntect</a>, so I wrap the highlighters in their own types:</p>
<div><pre><code><span><span><span>enum</span> <span>HighlighterType</span>&lt;'a&gt; <span><span>{</span>
    Syntect<span><span>(</span><span>SyntectHighlighter<span>&lt;</span><span>'a</span><span>&gt;</span></span></span><span><span>)</span></span><span>,</span>
    Treesitter<span><span>(</span><span>TreesitterHighlighter<span>&lt;</span><span>'a</span><span>&gt;</span></span></span><span><span>)</span></span><span>,</span>
</span><span><span>}</span></span></span>

<span><span>pub</span> <span>struct</span> </span><span><span><span>TreesitterHighlighter</span><span><span>&lt;</span><span>'a</span><span>&gt;</span></span></span></span><span> </span><span><span><span>{</span>
    <span>config</span><span>:</span> <span>&amp;</span><span>'a</span> HighlightConfiguration,
</span><span><span>}</span></span></span>

<span><span>impl</span></span><span><span><span>&lt;</span><span>'a</span><span>&gt;</span></span></span><span> <span>TreesitterHighlighter</span><span><span>&lt;</span><span>'a</span><span>&gt;</span></span> </span><span><span><span>{</span>
    <span><span><span>pub</span> <span>fn</span> </span><span>find</span></span><span><span><span>(</span><span>lang_id</span><span>:</span> <span>&amp;</span><span>str</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Option<span>&lt;</span><span>Self</span><span>&gt;</span></span></span> </span><span><span><span>{</span>
        <span>CONFIGS</span>.<span>get</span><span><span>(</span>lang_id</span><span><span>)</span></span>.<span>map</span><span><span>(</span><span><span><span>|</span></span></span><span><span><span>config</span><span>|</span></span> </span><span><span>Self</span> <span><span>{</span> config </span><span><span>}</span></span></span></span><span><span>)</span></span>
    </span><span><span>}</span></span></span>
</span><span><span>}</span></span></span>
</span></code></pre></div>
<p>The interesting part is of course the <code>highlight</code> method, that takes a string of code and applies syntax highlighting on it:</p>
<div><pre><code><span><span><span>impl</span></span><span><span><span>&lt;</span><span>'a</span><span>&gt;</span></span></span><span> <span>TreesitterHighlighter</span><span><span>&lt;</span><span>'a</span><span>&gt;</span></span> </span><span><span><span>{</span>
    <span><span><span>pub</span> <span>fn</span> </span><span>highlight</span></span><span><span><span>(</span><span>&amp;</span><span>self</span>, <span>code</span><span>:</span> <span>&amp;</span><span>str</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>Result<span>&lt;</span>String<span>&gt;</span></span></span> </span><span><span><span>{</span>
        <span>let</span> <span>mut</span> highlighter <span>=</span> <span>Highlighter<span>::</span></span>new<span><span>(</span></span><span><span>)</span></span><span>;</span>

        <span>let</span> highlights <span>=</span> highlighter.<span>highlight</span><span><span>(</span><span>self</span>.config<span>,</span> code.<span>as_bytes</span><span><span>(</span></span><span><span>)</span></span><span>,</span> <span>None</span><span>,</span> <span><span><span>|</span></span></span><span><span><span>lang</span><span>|</span></span> </span><span><span><span>{</span>
                                    <span>let</span> res <span>=</span> <span>CONFIGS</span>.<span>get</span><span><span>(</span>lang</span><span><span>)</span></span><span>;</span>
            <span>if</span> <span>!</span>res.<span>is_some</span><span><span>(</span></span><span><span>)</span></span> <span><span>{</span>
                <span>warn!</span><span><span>(</span><span><span>"</span>Couldn't find treesitter grammar for `{lang}` to inject<span>"</span></span></span><span><span>)</span></span><span>;</span>
            </span><span><span>}</span></span>
            res
        </span><span><span>}</span></span></span></span><span><span>)</span></span><span>?</span><span>;</span>

        <span>let</span> <span>mut</span> renderer <span>=</span> <span>HtmlRenderer<span>::</span></span>new<span><span>(</span></span><span><span>)</span></span><span>;</span>
        renderer.<span>render</span><span><span>(</span>highlights<span>,</span> code.<span>as_bytes</span><span><span>(</span></span><span><span>)</span></span><span>,</span> <span>&amp;</span><span>|</span>attr<span>|</span> <span><span>{</span>
                    </span><span><span>}</span></span></span><span><span>)</span></span><span>?</span><span>;</span>
        <span>let</span> res <span>=</span> renderer.<span>lines</span><span><span>(</span></span><span><span>)</span></span>.<span>join</span><span><span>(</span><span><span>"</span><span>"</span></span></span><span><span>)</span></span><span>;</span>
        <span>Ok</span><span><span>(</span>res</span><span><span>)</span></span>
    </span><span><span>}</span></span></span>
</span><span><span>}</span></span></span>
</span></code></pre></div>
<p>I want to point out the API in <code>HtmlRenderer</code>, where we stumble upon a very annoying problem:
what should we return from the callback, and how should we do that?</p>
<p>What the callback does is inject the return value into the <code>span</code> element, like this:</p>
<div><pre><code><span><span><span>&lt;</span><span>span</span> <span>CALLBACK_RESULT</span> <span>&gt;</span></span>highlight<span><span>&lt;/</span><span>span</span><span>&gt;</span></span>
</span></code></pre></div>
<p>So we’d like to return something like 
<code><span><span><span>"</span>class=<span>\"</span>markup italic<span>\"</span><span>"</span></span></span></code>, using <code>attr</code> which is only a <code>usize</code> into <code>HIGHLIGHT_NAMES</code>:</p>
<div><pre><code><span>renderer.<span>render</span><span><span>(</span>highlights<span>,</span> code.<span>as_bytes</span><span><span>(</span></span><span><span>)</span></span><span>,</span> <span>&amp;</span><span>|</span>attr<span>|</span> <span><span>{</span>
    <span>format!</span><span><span>(</span></span><span><span><span>r</span><span>#</span>"class="<span>{}</span>"<span>"#</span></span></span><span><span>,</span> <span>HIGHLIGHT_NAMES</span><span><span>[</span>attr.<span>0</span><span>]</span></span>.<span>replace</span><span><span>(</span><span><span>"</span>.<span>"</span></span><span>,</span> <span><span>"</span> <span>"</span></span></span><span><span>)</span></span><span>)</span></span>.<span>as_bytes</span><span><span>(</span></span><span><span>)</span></span>
</span><span><span>}</span></span></span><span><span>)</span></span><span>?</span><span>;</span>
</span></code></pre></div>
<p>Because we return a slice of bytes into a string that’s created inside the callback, of course the Rust compiler will be mad at us:</p>
<div><pre><code>error[E0515]: cannot return value referencing temporary value
  --&gt; src/markup/syntax_highlight/treesitter_highlighter.rs:33:13
   |
33 |             format!(r#"class="{}""#, HIGHLIGHT_NAMES[attr.0].replace(".", " ")).as_bytes()
   |             -------------------------------------------------------------------^^^^^^^^^^^
   |             |
   |             returns a value referencing data owned by the current function
   |             temporary value created here
</code></pre></div>
<p>How to dynamically create a string from inside the callback… That outlives the callback itself?</p>
<p>Not easily I tell you.</p>
<p>It would be so much easier if the callback would return a <code>Cow&lt;str&gt;</code> or something.
I wonder how the designers of the API expects it to be used?
This surely isn’t a very unique requirement, to wrap the attribute in a <code>class</code>?</p>
<p>Oh well.
One way to solve this is to store the generated strings in a container that outlives the callback, and reference that (yeah it’s a <code>Fn</code> callback, but there are hacky ways around that).
Or you could, you know, write your own <code>HtmlRenderer</code>.</p>
<p>Or you could pre-generate the classes and reference them:</p>
<div><pre><code><span><span>lazy_static!</span> <span><span>{</span>
    <span>static</span> <span>ref</span> <span>CLASSES</span><span>:</span> <span>Vec<span>&lt;</span>String<span>&gt;</span></span> <span>=</span> <span>HIGHLIGHT_NAMES</span>
        .<span>iter</span><span><span>(</span></span><span><span>)</span></span>
        .<span>map</span><span><span>(</span><span><span><span>|</span></span></span><span><span><span>name</span><span>|</span></span> </span><span><span>format!</span><span><span>(</span></span><span><span><span>r</span><span>#</span>"class="<span>{}</span>"<span>"#</span></span></span><span><span>,</span> name.<span>replace</span><span><span>(</span><span><span>"</span>.<span>"</span></span><span>,</span> <span><span>"</span> <span>"</span></span></span><span><span>)</span></span><span>)</span></span></span></span><span><span>)</span></span>
        .<span>collect</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
</span><span><span>}</span></span>
</span></code></pre></div>
<div><pre><code><span>renderer.<span>render</span><span><span>(</span>highlights<span>,</span> code.<span>as_bytes</span><span><span>(</span></span><span><span>)</span></span><span>,</span> <span>&amp;</span><span>|</span>attr<span>|</span> <span><span>{</span>
    <span>CLASSES</span><span><span>[</span>attr.<span>0</span><span>]</span></span>.<span>as_bytes</span><span><span>(</span></span><span><span>)</span></span>
</span><span><span>}</span></span></span><span><span>)</span></span><span>?</span><span>;</span>
</span></code></pre></div>
<p>This should be the fastest option and is the one I currently use…
But speed isn’t a bottleneck here and I’d rather just return a <code>String</code> with <code>format!</code> and be done with it.</p>
<hr>
<p>With this I’ve integrated Tree-sitter based syntax highlighting into my blog!</p>
<div><pre><code><span><span>```</span></span>
<span>With great powers comes great responsibility</span>
<span><span>```</span></span>
</code></pre></div>
<p>I could start moving the various languages over from Syntect to Tree-sitter…
But I won’t.</p>
<p>There are some issues:</p>
<ol type="A">
<li>
<p>You need a compatible version of <code>tree-sitter</code> for all grammars.</p>
<p>The more grammars you add the more painful the upgrade path becomes.</p>
</li>
<li>
<p>Syntect gives better highlighting for some languages (like Rust and C).</p>
<p>Neovim has their own highlighter implementation and has made tweaks to certain grammars and gets much nicer highlighting than I got out of the box.</p>
<p>Integrating that code into my site generator is probably possible, but not a rabbit hole I want to jump into right now.</p>
</li>
<li>
<p>The highlighter library feels a bit immature.</p>
<p>A newer library broke the highlight groups I got from some grammars and I don’t see any support for how to add a language specific class to <code>span</code> for injected languages.</p>
</li>
</ol>
<p>Because of these issues I’ll evaluate what highlighter to use on a case-by-case basis, with Syntect as the default choice.</p>
</section>
</section>
<section id="Within-edge-cases-lies-complexity">
<h2><a href="#Within-edge-cases-lies-complexity">Within edge-cases lies complexity</a></h2>
<p>If you’ve read the post to the end, congratulations.
You made it!</p>
<p>I don’t claim to be an expert at grammars or Tree-sitter, and I’m sure there are plenty of things that can be improved with the way the grammar is made.
But I hope it can be helpful as a starting point if you’re curious on how to write a Tree-sitter grammar of your own.</p>
<p>See the <a href="https://github.com/treeman/tree-sitter-djot">tree-sitter-djot repo</a> for how I developed the grammar further to support the full <a href="https://htmlpreview.github.io/?https://github.com/jgm/djot/blob/master/doc/syntax.html">Djot specification</a> (but remember, I’m not an expert).</p>
<p>Just one word of advice before you go.
Writing a grammar for simple rules is pretty easy, but in the real world things can get messy quickly.
This is especially true if you need to juggle multiple conflicting rules in the external scanner—keeping a sane structure is challenging.</p>
<p>(Even in our simple grammar there are bugs, but I don’t care to fix them.)</p>
<blockquote>
<p>The night is dark and full of terrors
</p>

</blockquote>
</section>
 

  
</article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Soul: A SQLite REST and Realtime Server (129 pts)]]></title>
            <link>https://thevahidal.github.io/soul/</link>
            <guid>39762315</guid>
            <pubDate>Wed, 20 Mar 2024 02:14:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thevahidal.github.io/soul/">https://thevahidal.github.io/soul/</a>, See on <a href="https://news.ycombinator.com/item?id=39762315">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      
      

      <p>
    <img src="https://thevahidal.github.io/soul/docs/logo.png" height="150px">
    </p><p>
        A SQLite REST and Realtime server
    </p>


<p><a href="https://justforfunnoreally.dev/"><img src="https://img.shields.io/badge/justforfunnoreally-dev-9ff" alt="justforfunnoreally.dev badge"></a>
<a href="#contributors"><img src="https://img.shields.io/github/all-contributors/thevahidal/soul?color=ee8449&amp;style=flat-square" alt="All Contributors"></a>
<a href="https://trackgit.com/"><img src="https://us-central1-trackgit-analytics.cloudfunctions.net/token/ping/la8rmyedi6oogy87pxla" alt="trackgit"></a></p>

<h2 id="installation">Installation</h2>

<p>Install Soul CLI with npm</p>



<h2 id="usage">Usage</h2>

<p>Soul is command line tool, after installing it,
Run <code>soul -d sqlite.db -p 8000</code> and it’ll start a REST API on <a href="http://localhost:8000/">http://localhost:8000</a> and a Websocket server on <a href="ws://localhost:8000/">ws://localhost:8000</a>.</p>

<div><pre><code>Usage: soul <span>[</span>options]


Options:
      <span>--version</span>             Show version number                        <span>[</span>boolean]
  <span>-d</span>, <span>--database</span>            SQLite database file or :memory: <span>[</span>string] <span>[</span>required]
  <span>-p</span>, <span>--port</span>                Port to listen on                           <span>[</span>number]
  <span>-r</span>, <span>--rate-limit-enabled</span>  Enable rate limiting                       <span>[</span>boolean]
  <span>-c</span>, <span>--cors</span>                CORS whitelist origins                <span>[</span>string]
  <span>-S</span>, <span>--studio</span>              Start Soul Studio <span>in </span>parallel              <span>[</span>boolean]
      <span>--help</span>                Show <span>help</span>                                  <span>[</span>boolean]

</code></pre></div>

<p>Then to test Soul is working run the following command</p>

<div><pre><code>curl http://localhost:8000/api/tables
</code></pre></div>

<p>It should return a list of the tables inside <code>sqlite.db</code> database.</p>

<h2 id="documentation">Documentation</h2>

<p>API documentation is available while the project is running at <a href="http://localhost:8000/api/docs">http://localhost:8000/api/docs</a></p>

<p>There’s also a list of all endpoints examples at <a href="https://thevahidal.github.io/soul/docs/api-examples.html">docs/api-examples.md</a></p>

<p>For websocket examples, check <a href="https://thevahidal.github.io/soul/docs/ws-examples.html">docs/ws-examples.md</a></p>

<h2 id="extending-soul">Extending Soul</h2>

<p>Soul is able to be extended (e.g. Adding custom APIs) via extensions, you can find a list of extensions at <a href="https://thevahidal.github.io/soul/docs/extensions-examples.html">docs/extensions-examples.md</a></p>

<h2 id="soul-mates">Soul-mates</h2>

<p>A collection of projects that revolve around the Soul ecosystem.</p>

<ul>
  <li>
    <p><a href="https://github.com/thevahidal/soul-studio">Soul Studio</a> provides a GUI to work with your database.</p>

    <p>Right now Soul Studio is in early stages of development and not useful to work with.</p>

    <p>
      <img src="https://thevahidal.github.io/soul/docs/soul-studio.png">
  </p>
  </li>
  <li>
    <p><a href="https://github.com/DeepBlueCLtd/RCO-Soul">RCO-Soul</a> The purpose of this project is to demonstrate how to run a React admin client using Soul as a REST API service.</p>
  </li>
</ul>

<h2 id="development">Development</h2>

<div><pre><code>git clone https://github.com/thevahidal/soul <span># Clone project</span>

<span>cp</span> .env.sample .env <span># Duplicate sample environment variables</span>
nano .env <span># Update the environment variables</span>

npm <span>install</span> <span># Install dependencies</span>
npm run dev <span># Start the dev server</span>
</code></pre></div>



<p><a href="https://bit.ly/soul-discord">Join</a> the discussion in our Discord server and help making Soul together.</p>

<h2 id="license">License</h2>

<p><a href="https://choosealicense.com/licenses/mit/">MIT</a></p>

<h2 id="contributing">Contributing</h2>

<p>Contributions are always welcome!</p>

<p>See <code>CONTRIBUTING.md</code> for ways to get started and please adhere to <code>CODE OF CONDUCT</code>.</p>

<h2 id="contributors-">Contributors ✨</h2>

<p>Thanks goes to these wonderful people (<a href="https://allcontributors.org/docs/en/emoji-key">emoji key</a>):</p>

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tbody>
    <tr>
      <td><a href="http://linktr.ee/thevahidal"><img src="https://avatars.githubusercontent.com/u/20302825?v=4?s=100" width="100px;" alt="Vahid Al"><br><sub><b>Vahid Al</b></sub></a><br><a href="https://github.com/thevahidal/soul/commits?author=thevahidal" title="Code">💻</a> <a href="https://github.com/thevahidal/soul/pulls?q=is%3Apr+reviewed-by%3Athevahidal" title="Reviewed Pull Requests">👀</a></td>
      <td><a href="https://github.com/AbegaM"><img src="https://avatars.githubusercontent.com/u/70259638?v=4?s=100" width="100px;" alt="Abenezer Melkamu"><br><sub><b>Abenezer Melkamu</b></sub></a><br><a href="https://github.com/thevahidal/soul/commits?author=AbegaM" title="Code">💻</a></td>
      <td><a href="https://github.com/IanMayo"><img src="https://avatars.githubusercontent.com/u/1108513?v=4?s=100" width="100px;" alt="Ian Mayo"><br><sub><b>Ian Mayo</b></sub></a><br><a href="https://github.com/thevahidal/soul/commits?author=IanMayo" title="Code">💻</a> <a href="https://github.com/thevahidal/soul/pulls?q=is%3Apr+reviewed-by%3AIanMayo" title="Reviewed Pull Requests">👀</a></td>
      <td><a href="https://godot.id/"><img src="https://avatars.githubusercontent.com/u/40712686?v=4?s=100" width="100px;" alt="Hanz"><br><sub><b>Hanz</b></sub></a><br><a href="https://github.com/thevahidal/soul/commits?author=HanzCEO" title="Code">💻</a></td>
      <td><a href="https://github.com/KoenDG"><img src="https://avatars.githubusercontent.com/u/1440619?v=4?s=100" width="100px;" alt="Koen De Groote"><br><sub><b>Koen De Groote</b></sub></a><br><a href="https://github.com/thevahidal/soul/commits?author=KoenDG" title="Code">💻</a></td>
      <td><a href="https://github.com/TahaKhanAbdalli"><img src="https://avatars.githubusercontent.com/u/50602678?v=4?s=100" width="100px;" alt="Muhammad Taha Khan"><br><sub><b>Muhammad Taha Khan</b></sub></a><br><a href="https://github.com/thevahidal/soul/commits?author=TahaKhanAbdalli" title="Code">💻</a></td>
    </tr>
  </tbody>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- ALL-CONTRIBUTORS-LIST:END -->

<p>This project follows the <a href="https://github.com/all-contributors/all-contributors">all-contributors</a> specification.</p>


      
      
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HIV in cell culture can be completely eliminated using CRISPR-Cas gene editing [pdf] (279 pts)]]></title>
            <link>https://www.escmid.org/fileadmin/src/media/PDFs/2News_Discussions/Press_activities/2024/HIVCRISPRV4_1_.pdf</link>
            <guid>39761283</guid>
            <pubDate>Tue, 19 Mar 2024 23:12:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.escmid.org/fileadmin/src/media/PDFs/2News_Discussions/Press_activities/2024/HIVCRISPRV4_1_.pdf">https://www.escmid.org/fileadmin/src/media/PDFs/2News_Discussions/Press_activities/2024/HIVCRISPRV4_1_.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=39761283">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[It's official: Europe turns to the Falcon 9 to launch its navigation satellites (143 pts)]]></title>
            <link>https://arstechnica.com/space/2024/03/its-official-europe-turns-to-the-falcon-9-to-launch-its-navigation-satellites/</link>
            <guid>39761179</guid>
            <pubDate>Tue, 19 Mar 2024 22:56:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/space/2024/03/its-official-europe-turns-to-the-falcon-9-to-launch-its-navigation-satellites/">https://arstechnica.com/space/2024/03/its-official-europe-turns-to-the-falcon-9-to-launch-its-navigation-satellites/</a>, See on <a href="https://news.ycombinator.com/item?id=39761179">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      And yet it moves    —
</h4>
            
            <h2 itemprop="description">The European Union agreed to pay a 30 percent premium for Falcon 9 launches.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2020/02/getty-spacex-starlink-falcon-9-800x499.jpg" alt="A SpaceX Falcon 9 rocket launching into the sky.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2020/02/getty-spacex-starlink-falcon-9.jpg" data-height="1310" data-width="2100">Enlarge</a> <span>/</span> A SpaceX Falcon 9 rocket carrying 60 Starlink satellites launches from Cape Canaveral Air Force Station on January 29, 2020. </p></figcaption>  </figure>

  




<!-- cache hit 1:single/related:66bcf73f5ea75b9b0d95f4523be283ce --><!-- empty -->
<p>The European Union has reached an agreement with the United States that will allow for the launch of four Galileo navigation satellites on SpaceX's Falcon 9 rocket.</p>
<p>According to Politico, the security agreement permits staff working for the EU and European Space Agency to have access to the launch pad at all times and, should there be a mishap with the mission, the first opportunity to retrieve debris.</p>
<p>With the agreement, final preparations can begin for two launches of two satellites each, on the Falcon 9 rocket from Florida. These Galileo missions will occur later this year. The satellites, which each weigh about 700 kg, will be launched into an orbit about 22,000 km above the planet.</p>
<p>The heightened security measures are due to the proprietary technology incorporated into the satellites, which cost hundreds of millions of euros to build; they perform a similar function to US-manufactured Global Positioning System satellites. The Florida launches will be the first time Galileo satellites, which are used for civilian and military purposes, have been exported outside of European territory.</p>
<p>Due to the extra overhead related to the national security mission, the European Union agreed to pay 180 million euros for the two launches, or about $196 million. This represents about a 30 percent premium over the standard launch price of $67 million for a Falcon 9 launch.</p>                                            
                                                        
<h2>A launcher crisis</h2>
<p>Somewhat to the ESA's embarrassment, the continent has had to purchase several launches from its direct competitor in launch, SpaceX, during the last two years. In 2023, Europe launched its Euclid space telescope on a Falcon 9 rocket, and later this year, an ESA Earth observation satellite and an ESA asteroid probe will launch on Falcon 9 missions.</p>
<p>The reasons are twofold. First, the ESA broke off work with the Russian space corporation Roscosmos after the Russian invasion of Ukraine. After this conflict began, Europe stopped flying its missions on the medium-lift Soyuz rocket. A modified version of this booster had been launching from Europe's spaceport in French Guiana.</p>
<p>The other reason is due to ongoing delays with the development of the Ariane 6 rocket. This booster was originally due to make its debut four years ago, but the new rocket has undergone several development and technical delays. Europe's launcher crisis became acute last year when the continent retired its long-flying Ariane 5 rocket, leaving it without a ready replacement.</p>
<p>However, this lack of access to space should come to an end soon. The ESA has shipped stages of the first flight hardware for the Ariane 6 rocket to its French Guiana spaceport. While the ESA has not set a specific launch date, it is working toward a window that extends from June 15 through July 31.</p>
<p>Assuming this test flight of the Ariane 6 goes well, the vehicle has a lengthy manifest of missions, including future Galileo satellites, other European spacecraft, as well as Project Kuiper satellites for its primary commercial customer, Amazon.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Users ditch Glassdoor, stunned by site adding real names without consent (116 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2024/03/glassdoor-adding-users-real-names-job-info-to-profiles-without-consent/</link>
            <guid>39761176</guid>
            <pubDate>Tue, 19 Mar 2024 22:56:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2024/03/glassdoor-adding-users-real-names-job-info-to-profiles-without-consent/">https://arstechnica.com/tech-policy/2024/03/glassdoor-adding-users-real-names-job-info-to-profiles-without-consent/</a>, See on <a href="https://news.ycombinator.com/item?id=39761176">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/03/GettyImages-1287581237-800x533.jpg" alt="Users ditch Glassdoor, stunned by site adding real names without consent">
      <figcaption></figcaption>  </figure>

  




<!-- cache hit 1:single/related:ad8534f5c396b5dddfb9f9d6cb0cf58c --><!-- empty -->
<p>Glassdoor, where employees go to leave anonymous reviews of employers, has recently begun adding real names to user profiles without users' consent, a Glassdoor user named Monica was shocked to discover last week.</p>
<p>"Time to delete your Glassdoor account and data," Monica, a Midwest-based software professional, warned other Glassdoor users in a blog. (Ars will only refer to Monica by her first name so that she can speak freely about her experience using Glassdoor to review employers.)</p>
<p>Monica joined Glassdoor about 10 years ago, she said, leaving a few reviews for her employers, taking advantage of other employees' reviews when considering new opportunities, and hoping to help others survey their job options. This month, though, she abruptly deleted her account after she contacted Glassdoor support to request help removing information from her account. She never expected that instead of removing information, Glassdoor's support team would take the real name that she provided in her support email and add it to her Glassdoor profile—despite Monica repeatedly and explicitly not consenting to Glassdoor storing her real name.</p>
<p>Although it's common for many online users to link services at sign-up to Facebook or Gmail accounts to verify identity and streamline logins, for years, Glassdoor has notably allowed users to sign up for its service anonymously. But in 2021, Glassdoor acquired Fishbowl, a professional networking app that integrated with Glassdoor last July. This acquisition meant that every Glassdoor user was automatically signed up for a Fishbowl account. And because Fishbowl requires users to verify their identities, Glassdoor's terms of service changed to require all users to be verified.</p>                                            
                                                        
<p>While users can remain anonymous, this change raises some potential concerns about data privacy and anonymity, Aaron Mackey, a lawyer for the Electronic Frontier Foundation (EFF), told Ars.</p>
<p>The EFF regularly defends Glassdoor users from being unmasked by retaliating employers. Particularly for employees who fear retaliation for reviews, Mackey said that Glassdoor users could historically choose never to share their real names, and the company now storing names for all users makes it much more likely that users could be linked to their reviews should Glassdoor's data ever be subpoenaed or leaked. That's what had Monica so concerned, too.</p>
<p>"Glassdoor now requires your real name and will add it to older accounts without your consent if they learn it, and your only option is to delete your account," Monica's blog warned. "They do not care that this puts people at risk with their employers. They do not care that this seems to run counter to their own data-privacy policies."</p>
<p>Monica soon discovered that deleting her Glassdoor account would not prevent them from storing her name, instead only deactivating her account. She decided to go through with a data erasure request, which Glassdoor estimated could take up to 30 days. In the meantime, her name remained on her profile, where it wasn't publicly available to employers but it could be used to link her to job reviews if Glassdoor introduced a bug in an update or data was ever breached, she feared.</p>
<p>"Since we require all users to have their names on their profiles, we will need to update your profile to reflect this," one Glassdoor employee wrote while reassuring her that "your anonymity will still be protected."</p>
<p>"No one has the ability to see your user profile and the contents within it, meaning no one, including your employer, will be able to see your details," Glassdoor's employee wrote.</p>
<p>"I do not consent," Monica responded. "I would delete my account before allowing that."</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Mechanics of Proof (118 pts)]]></title>
            <link>https://hrmacbeth.github.io/math2001/index.html</link>
            <guid>39760379</guid>
            <pubDate>Tue, 19 Mar 2024 21:23:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hrmacbeth.github.io/math2001/index.html">https://hrmacbeth.github.io/math2001/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=39760379">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="the-mechanics-of-proof" itemprop="articleBody" role="main" itemscope="itemscope" itemtype="http://schema.org/Article">

<p>This is a book dealing with how to write careful, rigorous mathematical proofs.
The book is paired with code in the computer formalization language
<a href="https://leanprover.github.io/about/">Lean</a>. Head over to the associated GitHub repository,
<a href="https://github.com/hrmacbeth/math2001">https://github.com/hrmacbeth/math2001</a>, to download this code to your own computer or to open it in
the cloud on Gitpod.</p>
<p>This book is aimed at the early university level and has been written for the course
Math 2001, at Fordham University.  Please reach out to the author,
<a href="https://faculty.fordham.edu/hmacbeth1/">Heather Macbeth</a>, with comments and corrections.</p>
<div>
<ul>
<li><a href="https://hrmacbeth.github.io/math2001/00_Introduction.html">Preface</a><ul>
<li><a href="https://hrmacbeth.github.io/math2001/00_Introduction.html#about-this-book">About this book</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/00_Introduction.html#why-lean">Why Lean?</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/00_Introduction.html#contents-and-prerequisites">Contents and prerequisites</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/00_Introduction.html#note-for-instructors">Note for instructors</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/00_Introduction.html#acknowledgements">Acknowledgements</a></li>
</ul>
</li>
</ul>
</div>
<div>
<ul>
<li><a href="https://hrmacbeth.github.io/math2001/01_Proofs_by_Calculation.html">1. Proofs by calculation</a><ul>
<li><a href="https://hrmacbeth.github.io/math2001/01_Proofs_by_Calculation.html#proving-equalities">1.1. Proving equalities</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/01_Proofs_by_Calculation.html#proving-equalities-in-lean">1.2. Proving equalities in Lean</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/01_Proofs_by_Calculation.html#tips-and-tricks">1.3. Tips and tricks</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/01_Proofs_by_Calculation.html#proving-inequalities">1.4. Proving inequalities</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/01_Proofs_by_Calculation.html#a-shortcut">1.5. A shortcut</a></li>
</ul>
</li>
<li><a href="https://hrmacbeth.github.io/math2001/02_Proofs_with_Structure.html">2. Proofs with structure</a><ul>
<li><a href="https://hrmacbeth.github.io/math2001/02_Proofs_with_Structure.html#intermediate-steps">2.1. Intermediate steps</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/02_Proofs_with_Structure.html#invoking-lemmas">2.2. Invoking lemmas</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/02_Proofs_with_Structure.html#or-and-proof-by-cases">2.3. “Or” and proof by cases</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/02_Proofs_with_Structure.html#and">2.4. “And”</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/02_Proofs_with_Structure.html#existence-proofs">2.5. Existence proofs</a></li>
</ul>
</li>
<li><a href="https://hrmacbeth.github.io/math2001/03_Parity_and_Divisibility.html">3. Parity and divisibility</a><ul>
<li><a href="https://hrmacbeth.github.io/math2001/03_Parity_and_Divisibility.html#definitions-parity">3.1. Definitions; parity</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/03_Parity_and_Divisibility.html#divisibility">3.2. Divisibility</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/03_Parity_and_Divisibility.html#modular-arithmetic-theory">3.3. Modular arithmetic: theory</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/03_Parity_and_Divisibility.html#modular-arithmetic-calculations">3.4. Modular arithmetic: calculations</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/03_Parity_and_Divisibility.html#bezout-s-identity">3.5. Bézout’s identity</a></li>
</ul>
</li>
<li><a href="https://hrmacbeth.github.io/math2001/04_Proofs_with_Structure_II.html">4. Proofs with structure, II</a><ul>
<li><a href="https://hrmacbeth.github.io/math2001/04_Proofs_with_Structure_II.html#for-all-and-implication">4.1. “For all” and implication</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/04_Proofs_with_Structure_II.html#if-and-only-if">4.2. “If and only if”</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/04_Proofs_with_Structure_II.html#there-exists-a-unique">4.3. “There exists a unique”</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/04_Proofs_with_Structure_II.html#contradictory-hypotheses">4.4. Contradictory hypotheses</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/04_Proofs_with_Structure_II.html#proof-by-contradiction">4.5. Proof by contradiction</a></li>
</ul>
</li>
<li><a href="https://hrmacbeth.github.io/math2001/05_Logic.html">5. Logic</a><ul>
<li><a href="https://hrmacbeth.github.io/math2001/05_Logic.html#logical-equivalence">5.1. Logical equivalence</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/05_Logic.html#the-law-of-the-excluded-middle">5.2. The law of the excluded middle</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/05_Logic.html#normal-form-for-negations">5.3. Normal form for negations</a></li>
</ul>
</li>
<li><a href="https://hrmacbeth.github.io/math2001/06_Induction.html">6. Induction</a><ul>
<li><a href="https://hrmacbeth.github.io/math2001/06_Induction.html#introduction">6.1. Introduction</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/06_Induction.html#recurrence-relations">6.2. Recurrence relations</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/06_Induction.html#two-step-induction">6.3. Two-step induction</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/06_Induction.html#strong-induction">6.4. Strong induction</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/06_Induction.html#pascal-s-triangle">6.5. Pascal’s triangle</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/06_Induction.html#the-division-algorithm">6.6. The Division Algorithm</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/06_Induction.html#the-euclidean-algorithm">6.7. The Euclidean algorithm</a></li>
</ul>
</li>
<li><a href="https://hrmacbeth.github.io/math2001/07_Number_Theory.html">7. Number theory</a><ul>
<li><a href="https://hrmacbeth.github.io/math2001/07_Number_Theory.html#infinitely-many-primes">7.1. Infinitely many primes</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/07_Number_Theory.html#gauss-and-euclid-s-lemmas">7.2. Gauss’ and Euclid’s lemmas</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/07_Number_Theory.html#the-square-root-of-two">7.3. The square root of two</a></li>
</ul>
</li>
<li><a href="https://hrmacbeth.github.io/math2001/08_Functions.html">8. Functions</a><ul>
<li><a href="https://hrmacbeth.github.io/math2001/08_Functions.html#injectivity-and-surjectivity">8.1. Injectivity and surjectivity</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/08_Functions.html#bijectivity">8.2. Bijectivity</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/08_Functions.html#composition-of-functions">8.3. Composition of functions</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/08_Functions.html#product-types">8.4. Product types</a></li>
</ul>
</li>
<li><a href="https://hrmacbeth.github.io/math2001/09_Sets.html">9. Sets</a><ul>
<li><a href="https://hrmacbeth.github.io/math2001/09_Sets.html#introduction">9.1. Introduction</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/09_Sets.html#set-operations">9.2. Set operations</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/09_Sets.html#the-type-of-sets">9.3. The type of sets</a></li>
</ul>
</li>
<li><a href="https://hrmacbeth.github.io/math2001/10_Relations.html">10. Relations</a><ul>
<li><a href="https://hrmacbeth.github.io/math2001/10_Relations.html#reflexive-symmetric-antisymmetric-transitive">10.1. Reflexive, symmetric, antisymmetric, transitive</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/10_Relations.html#equivalence-relations">10.2. Equivalence relations</a></li>
</ul>
</li>
</ul>
</div>
<p><strong>Appendices</strong></p>
<div>
<ul>
<li><a href="https://hrmacbeth.github.io/math2001/Index_of_Tactics.html">Index of Lean tactics</a></li>
<li><a href="https://hrmacbeth.github.io/math2001/Mainstream_Lean.html">Transitioning to mainstream Lean</a></li>
</ul>
</div>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Root cause of Alzheimer's may be fat buildup in brain cells, research suggests (302 pts)]]></title>
            <link>https://medicalxpress.com/news/2024-03-root-alzheimer-fat-buildup-brain.html</link>
            <guid>39760333</guid>
            <pubDate>Tue, 19 Mar 2024 21:19:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medicalxpress.com/news/2024-03-root-alzheimer-fat-buildup-brain.html">https://medicalxpress.com/news/2024-03-root-alzheimer-fat-buildup-brain.html</a>, See on <a href="https://news.ycombinator.com/item?id=39760333">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/root-cause-of-alzheime.jpg" data-src="https://scx2.b-cdn.net/gfx/news/2024/root-cause-of-alzheime.jpg" data-sub-html="Representative immunofluorescence images of human frontal cortex adjacent to the tissue used in snRNA-seq experiments stained for microglia marker IBA1 (green), ACSL1 (red) and DAPI (blue) in an aged-matched healthy control subject (left), an AD-APOE3/3 subject (middle) and an AD-APOE4/4 subject. Credit: <i>Nature</i> (2024). DOI: 10.1038/s41586-024-07185-7">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/root-cause-of-alzheime.jpg" alt="Root cause of Alzheimer's may be fat buildup in brain cells" title="Representative immunofluorescence images of human frontal cortex adjacent to the tissue used in snRNA-seq experiments stained for microglia marker IBA1 (green), ACSL1 (red) and DAPI (blue) in an aged-matched healthy control subject (left), an AD-APOE3/3 subject (middle) and an AD-APOE4/4 subject. Credit: Nature (2024). DOI: 10.1038/s41586-024-07185-7" width="800" height="462">
             <figcaption>
                Representative immunofluorescence images of human frontal cortex adjacent to the tissue used in snRNA-seq experiments stained for microglia marker IBA1 (green), ACSL1 (red) and DAPI (blue) in an aged-matched healthy control subject (left), an AD-APOE3/3 subject (middle) and an AD-APOE4/4 subject. Credit: <i>Nature</i> (2024). DOI: 10.1038/s41586-024-07185-7
            </figcaption>        </figure>
    </div><p>A team of neurologists, stem cell specialists and molecular biologists affiliated with several institutions in the U.S. and led by a group at Stanford University School of Medicine has found evidence that the root cause of Alzheimer's disease may be fat buildup in brain cells. The <a href="https://www.nature.com/articles/s41586-024-07185-7" target="_blank">study</a> is published in the journal <i>Nature</i>.</p>


                                        
                                                                                        
                                                                                    <p>Prior research has suggested that Alzheimer's disease is caused by a buildup of beta-amyloid in plaques that grow between <a href="https://medicalxpress.com/tags/nerve+cells/" rel="tag">nerve cells</a>. Other work has also implicated a protein called tau, which can build up in <a href="https://medicalxpress.com/tags/brain+cells/" rel="tag">brain cells</a>. Thus, most work involved in developing ways to prevent, slow or stop the disease is based on reducing or eliminating such buildups. But as the researchers with this new effort have found, there may be something else at the root of the development of the disease.</p>
<p>Back when Alzheimer's disease was first identified by Alois Alzheimer, he noted that in addition to the plaques and tau buildup, there was also a buildup of fat droplets in brain cells. Since that time, little effort has been made to determine whether they might be the cause of the disease.</p>
<p>The research team therefore focused on the function of the APOE gene—prior research has shown that it encodes for a protein involved in transporting fat droplets into nerve cells. Prior research has also shown that there are four APOE variants, numbered 1 through 4, and that one of them, APOE4, carries the most fat into brain cells, while APOE2 brings the least.</p>
<p>The team wondered if the APOE variants carried different risks for developing Alzheimer's disease. To find out, they conducted a few experiments.</p>
<p>In the first experiment, the researchers used single cell RNA sequencing to identify the proteins inside of a test nerve cell. They applied what they found to <a href="https://medicalxpress.com/tags/tissue+samples/" rel="tag">tissue samples</a> collected from people who died of Alzheimer's disease who had dual copies of APOE4 or APOE3.</p>
<p>They found that the brains of people with the APOE4 gene had more <a href="https://medicalxpress.com/tags/immune+cells/" rel="tag">immune cells</a> with a type of enzyme that boosted movement of fat <a href="https://medicalxpress.com/tags/droplets/" rel="tag">droplets</a> into brain cells. In another experiment, they found that applying amyloid to brain cells of people with the APOE4 or APOE3 variants made the cells accumulate more fat.</p>
<p>According to the researchers, the results indicate that buildup of amyloid in the brain triggers the push of fat into brain cells, leading to Alzheimer's disease.</p>

                                                                                
                                        											<div>
												                                                    <p><strong>More information:</strong>
                                                    Michael S. Haney et al, APOE4/4 is linked to damaging lipid droplets in Alzheimer's disease microglia, <i>Nature</i> (2024). <a data-doi="1" href="https://dx.doi.org/10.1038/s41586-024-07185-7" target="_blank">DOI: 10.1038/s41586-024-07185-7</a>
																								
																								</p>
																							</div>
                                        											
										                                                                                    <p>
                                                © 2024 Science X Network
                                            </p>
                                                                                
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                 Root cause of Alzheimer's may be fat buildup in brain cells, research suggests (2024, March 19)
                                                 retrieved 19 March 2024
                                                 from https://medicalxpress.com/news/2024-03-root-alzheimer-fat-buildup-brain.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New algorithm unlocks high-resolution insights for computer vision (158 pts)]]></title>
            <link>https://news.mit.edu/2024/featup-algorithm-unlocks-high-resolution-insights-computer-vision-0318</link>
            <guid>39759906</guid>
            <pubDate>Tue, 19 Mar 2024 20:28:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.mit.edu/2024/featup-algorithm-unlocks-high-resolution-insights-computer-vision-0318">https://news.mit.edu/2024/featup-algorithm-unlocks-high-resolution-insights-computer-vision-0318</a>, See on <a href="https://news.ycombinator.com/item?id=39759906">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          

            <p>Imagine yourself glancing at a busy street for a few moments, then trying to sketch the scene you saw from memory. Most people could draw the rough positions of the major objects like cars, people, and crosswalks, but almost no one can draw every detail with pixel-perfect accuracy. The same is true for most modern computer vision algorithms: They are fantastic at capturing high-level details of a scene, but they lose fine-grained details as they process information.</p>

<p>Now, MIT researchers have created a system called “<a href="https://mhamilton.net/featup.html" target="_blank">FeatUp</a>” that lets algorithms capture all of the high- and low-level details of a scene at the same time — almost like Lasik eye surgery for computer vision.</p>

<p>When computers learn to “see” from looking at images and videos, they build up “ideas” of what's in a scene through something called “features.” To create these features, deep networks and visual foundation models break down images into a grid of tiny squares and process these squares as a group to determine what's going on in a photo. Each tiny square is usually made up of anywhere from 16 to 32 pixels, so the resolution of these algorithms is dramatically smaller than the images they work with. In trying to summarize and understand photos, algorithms lose a ton of pixel clarity.&nbsp;</p>

<p>The FeatUp algorithm can stop this loss of information and boost the resolution of any deep network without compromising on speed or quality. This allows researchers to quickly and easily improve the resolution of any new or existing algorithm. For example, imagine trying to interpret the predictions of a lung cancer detection algorithm with the goal of localizing the tumor. Applying FeatUp before interpreting the algorithm using a method like class activation maps (CAM) can yield a dramatically more detailed (16-32x) view of where the tumor might be located according to the model.</p>        

      </div><div>
          

            <p>FeatUp not only helps practitioners understand their models, but also can improve a panoply of different tasks like object detection, semantic segmentation (assigning labels to pixels in an image with object labels), and depth estimation. It achieves this by providing more accurate, high-resolution features, which are crucial for building vision applications ranging from autonomous driving to medical imaging.</p>

<p>“The essence of all computer vision lies in these deep, intelligent features that emerge from the depths of deep learning architectures. The big challenge of modern algorithms is that they reduce large images to&nbsp; very small grids of 'smart' features, gaining intelligent insights but losing the finer details,” says Mark Hamilton, an MIT PhD student in electrical engineering and computer science, MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) affiliate, and a co-lead author on a <a href="https://marhamilresearch4.blob.core.windows.net/feature-upsampling-public/FeatUp_ICLR_2024.pdf">paper</a> about the project. “FeatUp helps enable the best of both worlds: highly intelligent representations with the original image’s resolution. These high-resolution features significantly boost performance across a spectrum of computer vision tasks, from enhancing object detection and improving depth prediction to providing a deeper understanding of your network's decision-making process through high-resolution analysis.”&nbsp;</p>

<p><strong>Resolution renaissance&nbsp;</strong></p>

<p>As these large AI models become more and more prevalent, there’s an increasing need to explain what they’re doing, what they’re looking at, and what they’re thinking.&nbsp;</p>

<p>But how exactly can FeatUp discover these fine-grained details? Curiously, the secret lies in wiggling and jiggling images.&nbsp;</p>

<p>In particular, FeatUp applies minor adjustments (like moving the image a few pixels to the left or right) and watches how an algorithm responds to these slight movements of the image. This results in hundreds of deep-feature maps that are all slightly different, which can be combined into a single crisp, high-resolution, set of deep features. “We imagine that some high-resolution features exist, and that when we wiggle them and blur them, they will match all of the original, lower-resolution features from the wiggled images. Our goal is to learn how to refine the low-resolution features into high-resolution features using this 'game' that lets us know how well we are doing,” says Hamilton. This methodology is analogous to how algorithms can create a 3D model from multiple 2D images by ensuring that the predicted 3D object matches all of the 2D photos used to create it. In FeatUp’s case, they predict a high-resolution feature map that’s consistent with all of the low-resolution feature maps formed by jittering the original image.</p>

<p>The team notes that standard tools available in PyTorch were insufficient for their needs, and introduced a new type of deep network layer in their quest for a speedy and efficient solution. Their custom layer, a special joint bilateral upsampling operation, was over 100 times more efficient than a naive implementation in PyTorch. The team also showed this new layer could improve a wide variety of different algorithms including semantic segmentation and depth prediction. This layer improved the network’s ability to process and understand high-resolution details, giving any algorithm that used it a substantial performance boost.&nbsp;</p>

<p>“Another application is something called small object retrieval, where our algorithm allows for precise localization of objects. For example, even in cluttered road scenes algorithms enriched with FeatUp can see tiny objects like traffic cones, reflectors, lights, and potholes where their low-resolution cousins fail. This demonstrates its capability to enhance coarse features into finely detailed signals,” says Stephanie Fu ’22, MNG ’23, a PhD student at the University of California at Berkeley and another co-lead author on the new FeatUp paper. “This is especially critical for time-sensitive tasks, like pinpointing a traffic sign on a cluttered expressway in a driverless car. This can not only improve the accuracy of such tasks by turning broad guesses into exact localizations, but might also make these systems more reliable, interpretable, and trustworthy.”</p>

<p><strong>What next?</strong></p>

<p>Regarding future aspirations, the team emphasizes FeatUp’s potential widespread adoption within the research community and beyond, akin to data augmentation practices. “The goal is to make this method a fundamental tool in deep learning, enriching models to perceive the world in greater detail without the computational inefficiency of traditional high-resolution processing,” says Fu.</p>

<p>“FeatUp represents a wonderful advance towards making visual representations really useful, by producing them at full image resolutions,” says Cornell University computer science professor Noah Snavely, who was not involved in the research. “Learned visual representations have become really good in the last few years, but they are almost always produced at very low resolution — you might put in a nice full-resolution photo, and get back a tiny, postage stamp-sized grid of features. That’s a problem if you want to use those features in applications that produce full-resolution outputs. FeatUp solves this problem in a creative way by combining classic ideas in super-resolution with modern learning approaches, leading to beautiful, high-resolution feature maps.”</p>

<div><p>“We hope this simple idea can have broad application. It provides high-resolution versions of image analytics that we’d thought before could only be low-resolution,” says senior author William T. Freeman, an MIT professor of electrical engineering and computer science professor and CSAIL member.</p><p>

Lead authors Fu and Hamilton are accompanied by MIT PhD students Laura Brandt SM ’21 and Axel Feldmann SM ’21, as well as Zhoutong Zhang SM ’21, PhD ’22, all current or former affiliates of MIT CSAIL. Their research is supported, in part, by a National Science Foundation Graduate Research Fellowship<strong>,</strong> by the National Science Foundation and Office of the Director of National Intelligence, by the U.S. Air Force Research Laboratory, and by the U.S. Air Force Artificial Intelligence Accelerator. The group will present their work in May at the International Conference on Learning Representations.</p></div>        

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lego price per part over the years (336 pts)]]></title>
            <link>https://brickinsights.com/statistics/ppp</link>
            <guid>39759693</guid>
            <pubDate>Tue, 19 Mar 2024 20:01:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brickinsights.com/statistics/ppp">https://brickinsights.com/statistics/ppp</a>, See on <a href="https://news.ycombinator.com/item?id=39759693">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <h2>Can we really compare Price Per Part (PPP) between sets?</h2>
      <p>A couple of weeks ago I received this question by <a href="https://rebrickable.com/users/jaredhinton/profile/" target="_blank" rel="noopener">Jared Hinton</a>:</p>
      <p><em>"Just a quick question on the PPP data. Most years have a average PPP of 20c+ per part. This sounds very high, especially when most sets come way under that (which are then marked as good value). Do Duplo sets get included in the years average PPP figure? Because they tend to be alot higher than LEGO sets. This is the only thing I could think of why the year average is almost double what I'd expect it to be."</em></p>
      <p>This is a very good observation, and something I hadn't really thought about. I built the PPP comparison when Brick Insights first launched in 2018, almost as an afterthought. I wanted to see if a set was worth buying from a MOCing point of view, and threw it in to help guide me. It was definitely time to take another look at those calculations.</p>
      <p>All numbers in this article are adjusted according to inflation to make them comparable over the years. This isn't something we did before this investigation, so thank you for that too, Jared! <a href="https://brickinsights.com/faq">Check the FAQ</a> to learn the details about this. All categories referenced in this article are collected from Brickset, where we get most of our set data. That's also in the FAQ.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Retina – eBPF distributed networking observability tool for Kubernetes (152 pts)]]></title>
            <link>https://github.com/microsoft/retina</link>
            <guid>39759627</guid>
            <pubDate>Tue, 19 Mar 2024 19:54:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/retina">https://github.com/microsoft/retina</a>, See on <a href="https://news.ycombinator.com/item?id=39759627">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Retina</h2><a id="user-content-retina" aria-label="Permalink: Retina" href="#retina"></a></p>
<p dir="auto"><a href="https://goreportcard.com/report/github.com/microsoft/retina" rel="nofollow"><img src="https://camo.githubusercontent.com/3fec060b6ca3b8336abfe76f0d699a59a942324d5c0e8fbe791b3c710303d053/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f6d6963726f736f66742f726574696e61" alt="goreport" data-canonical-src="https://goreportcard.com/badge/github.com/microsoft/retina"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/abfcc83487967c2e2ee70aa26df8d74ea9b70998fd383b491ae2f8dadf9e8019/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f6d6963726f736f66742f726574696e612e737667"><img src="https://camo.githubusercontent.com/abfcc83487967c2e2ee70aa26df8d74ea9b70998fd383b491ae2f8dadf9e8019/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f6d6963726f736f66742f726574696e612e737667" alt="GitHub release" data-canonical-src="https://img.shields.io/github/v/release/microsoft/retina.svg"></a> <a href="https://godoc.org/github.com/microsoft/retina" rel="nofollow"><img src="https://camo.githubusercontent.com/a7a2adf339bfb1b7442f42660598da0a83ba3b450d3fa5e28cb4d236ee170bb4/68747470733a2f2f676f646f632e6f72672f6769746875622e636f6d2f6d6963726f736f66742f726574696e613f7374617475732e737667" alt="retina-publish" data-canonical-src="https://godoc.org/github.com/microsoft/retina?status.svg"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8461bcd16ae5629435cd3547ca2d5e47206d6a739fdba55f08ea007f2703fe61/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75653f6c696e6b3d68747470732533412532462532466769746875622e636f6d2532466d6963726f736f6674253246726574696e61253246626c6f622532466d61696e2532464c4943454e5345"><img src="https://camo.githubusercontent.com/8461bcd16ae5629435cd3547ca2d5e47206d6a739fdba55f08ea007f2703fe61/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75653f6c696e6b3d68747470732533412532462532466769746875622e636f6d2532466d6963726f736f6674253246726574696e61253246626c6f622532466d61696e2532464c4943454e5345" alt="license" data-canonical-src="https://img.shields.io/badge/license-MIT-blue?link=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fretina%2Fblob%2Fmain%2FLICENSE"></a></p>
<p dir="auto"><a href="https://github.com/microsoft/retina/actions/workflows/test.yaml?query=branch%3Amain"><img src="https://github.com/microsoft/retina/actions/workflows/test.yaml/badge.svg?branch=main" alt="retina-test"></a> <a href="https://retina.sh/" rel="nofollow"><img src="https://github.com/microsoft/retina/actions/workflows/docs.yaml/badge.svg?branch=main" alt="retinash"></a> <a href="https://github.com/microsoft/retina/actions/workflows/images.yaml?query=branch%3Amain"><img src="https://github.com/microsoft/retina/actions/workflows/images.yaml/badge.svg?branch=main" alt="retina-publish"></a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/retina/actions/workflows/codeql.yaml/badge.svg?branch=main"><img src="https://github.com/microsoft/retina/actions/workflows/codeql.yaml/badge.svg?branch=main" alt="retina-codeql-img"></a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/retina/actions/workflows/golangci-lint.yaml/badge.svg?branch=main"><img src="https://github.com/microsoft/retina/actions/workflows/golangci-lint.yaml/badge.svg?branch=main" alt="retina-golangci-lint-img"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">Retina is a cloud-agnostic, open-source <strong>Kubernetes network observability platform</strong> that provides a <strong>centralized hub for monitoring application health, network health, and security</strong>. It provides actionable insights to cluster network administrators, cluster security administrators, and DevOps engineers navigating DevOps, SecOps, and compliance use cases.</p>
<p dir="auto">Retina <strong>collects customizable telemetry</strong>, which can be exported to <strong>multiple storage options</strong> (such as Prometheus, Azure Monitor, and other vendors) and <strong>visualized in a variety of ways</strong> (like Grafana, Azure Log Analytics, and other vendors).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong><a href="https://ebpf.io/what-is-ebpf#what-is-ebpf" rel="nofollow">eBPF</a>-based</strong> Network Observability platform for Kubernetes workloads.</li>
<li><strong>On-Demand</strong> and <strong>Configurable</strong>.</li>
<li>Actionable, industry-standard <strong>Prometheus metrics</strong>.</li>
<li>Streamlined <strong>Packet Captures</strong> for deep dives.</li>
<li><strong>Cloud-agnostic</strong>, supporting multiple OS (like Linux, Windows, Azure Linux).</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why Retina?</h2><a id="user-content-why-retina" aria-label="Permalink: Why Retina?" href="#why-retina"></a></p>
<p dir="auto">Retina lets you <strong>investigate network issues on-demand</strong> and <strong>continuously monitor your clusters</strong>. For scenarios where Retina shines, see the intro docs <a href="https://retina.sh/docs/intro" rel="nofollow">here</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">See <a href="http://retina.sh/" rel="nofollow">retina.sh</a> for documentation and examples.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Capabilities</h2><a id="user-content-capabilities" aria-label="Permalink: Capabilities" href="#capabilities"></a></p>
<p dir="auto">Retina has two major features:</p>
<ul dir="auto">
<li><a href="https://retina.sh/docs/metrics/modes" rel="nofollow">Metrics</a></li>
<li><a href="https://retina.sh/docs/captures" rel="nofollow">Captures</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Metrics Quick Install Guide</h3><a id="user-content-metrics-quick-install-guide" aria-label="Permalink: Metrics Quick Install Guide" href="#metrics-quick-install-guide"></a></p>
<p dir="auto">Prerequisites: Go, Helm</p>
<ol dir="auto">
<li>
<p dir="auto">Clone the repo, then install Retina on your Kubernetes cluster</p>

</li>
<li>
<p dir="auto">Follow steps in <a href="https://retina.sh/docs/installation/prometheus-unmanaged" rel="nofollow">Using Prometheus and Grafana</a> to set up metrics collection and visualization.</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Captures Quick Start Guide</h3><a id="user-content-captures-quick-start-guide" aria-label="Permalink: Captures Quick Start Guide" href="#captures-quick-start-guide"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Captures via CLI</h4><a id="user-content-captures-via-cli" aria-label="Permalink: Captures via CLI" href="#captures-via-cli"></a></p>
<p dir="auto">Currently, Retina CLI only supports Linux.</p>
<ul dir="auto">
<li>
<p dir="auto">Option 1: Download from Release</p>
<p dir="auto">Download <code>kubectl-retina</code> from the latest <a href="https://github.com/microsoft/retina/releases">Retina release</a>.
Feel free to move the binary to <code>/usr/local/bin/</code>, or add it to your <code>PATH</code> otherwise.</p>
</li>
<li>
<p dir="auto">Option 2: Build from source</p>
<p dir="auto">Requirements:</p>
<ul dir="auto">
<li>go 1.21 or newer</li>
<li>GNU make</li>
</ul>
<p dir="auto">Clone the Retina repo and execute:</p>
<div dir="auto" data-snippet-clipboard-copy-content="make install-kubectl-retina"><pre>make install-kubectl-retina</pre></div>
</li>
</ul>
<p dir="auto">Execute Retina:</p>
<div dir="auto" data-snippet-clipboard-copy-content="kubectl-retina capture create --help"><pre>kubectl-retina capture create --help</pre></div>
<p dir="auto">For further CLI documentation, see <a href="https://github.com/microsoft/retina/blob/captures/cli.md">Capture with Retina CLI</a>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Captures via CRD</h4><a id="user-content-captures-via-crd" aria-label="Permalink: Captures via CRD" href="#captures-via-crd"></a></p>
<p dir="auto">Prerequisites: Go, Helm</p>
<ol dir="auto">
<li>
<p dir="auto">Clone the repo, then install Retina with Capture operator support on your Kubernetes cluster</p>
<div dir="auto" data-snippet-clipboard-copy-content="make helm-install-with-operator"><pre>make helm-install-with-operator</pre></div>
</li>
<li>
<p dir="auto">Follow steps in <a href="https://retina.sh/docs/captures/#option-2-capture-crd-custom-resource-definition" rel="nofollow">Capture CRD</a> for documentation of the CRD and examples for setting up Captures.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <a href="https://cla.opensource.microsoft.com/" rel="nofollow">https://cla.opensource.microsoft.com</a>.</p>
<p dir="auto">When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.</p>
<p dir="auto">This project has adopted the <a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow">Microsoft Open Source Code of Conduct</a>.
For more information see the <a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow">Code of Conduct FAQ</a> or
contact <a href="mailto:opencode@microsoft.com">opencode@microsoft.com</a> with any additional questions or comments.</p>
<p dir="auto"><a href="https://retina.sh/docs/contributing" rel="nofollow">Read more about how to begin contributing here.</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Office Hours and Community Meetings</h3><a id="user-content-office-hours-and-community-meetings" aria-label="Permalink: Office Hours and Community Meetings" href="#office-hours-and-community-meetings"></a></p>
<p dir="auto">We host a periodic open community meeting. <a href="https://retina.sh/docs/contributing/#office-hours-and-community-meetings" rel="nofollow">Find the details here.</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Trademarks</h2><a id="user-content-trademarks" aria-label="Permalink: Trademarks" href="#trademarks"></a></p>
<p dir="auto">This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow <a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general" rel="nofollow">Microsoft's Trademark &amp; Brand Guidelines</a>.
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">See the <a href="https://github.com/microsoft/retina/blob/main/LICENSE">LICENSE</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Code of Conduct</h2><a id="user-content-code-of-conduct" aria-label="Permalink: Code of Conduct" href="#code-of-conduct"></a></p>
<p dir="auto">This project has adopted the <a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow">Microsoft Open Source Code of Conduct</a>. For more information see the <a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow">Code of Conduct FAQ</a> or contact <a href="mailto:opencode@microsoft.com">opencode@microsoft.com</a> with any additional questions or comments.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contact</h2><a id="user-content-contact" aria-label="Permalink: Contact" href="#contact"></a></p>
<p dir="auto">For bugs or feature requests, open an <a href="https://github.com/microsoft/retina/issues">issue</a>.<br>
For security or vulnerability concerns, see <a href="https://github.com/microsoft/retina/blob/main/SECURITY.md">SECURITY.md</a>.<br>
For other communication, contact the maintainers at <a href="mailto:retina@microsoft.com">retina@microsoft.com</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: jnv: interactive JSON filter using jq (300 pts)]]></title>
            <link>https://github.com/ynqa/jnv</link>
            <guid>39759325</guid>
            <pubDate>Tue, 19 Mar 2024 19:23:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ynqa/jnv">https://github.com/ynqa/jnv</a>, See on <a href="https://news.ycombinator.com/item?id=39759325">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">jnv</h2><a id="user-content-jnv" aria-label="Permalink: jnv" href="#jnv"></a></p>
<p dir="auto"><em>jnv</em> is designed for navigating JSON,
offering an interactive JSON viewer and <code>jq</code> filter editor.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/6745370/313697735-1d1495e8-5755-487f-bbf3-03e1d4edab08.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA4OTY3MDQsIm5iZiI6MTcxMDg5NjQwNCwicGF0aCI6Ii82NzQ1MzcwLzMxMzY5NzczNS0xZDE0OTVlOC01NzU1LTQ4N2YtYmJmMy0wM2UxZDRlZGFiMDguZ2lmP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDMyMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDAzMjBUMDEwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTZiMzgxYWE5MGMyMzcwYmM0Y2M4ZWIzZjgxNzFjMDQwYWIzMWU1YzhkZGE3ODliOTE0ZjUxODhhNjJjYTUyOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.wj1eUtrbL2DLjx7_gtVeMrEINa6vQcrOQtNKHb9MVho"><img src="https://private-user-images.githubusercontent.com/6745370/313697735-1d1495e8-5755-487f-bbf3-03e1d4edab08.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA4OTY3MDQsIm5iZiI6MTcxMDg5NjQwNCwicGF0aCI6Ii82NzQ1MzcwLzMxMzY5NzczNS0xZDE0OTVlOC01NzU1LTQ4N2YtYmJmMy0wM2UxZDRlZGFiMDguZ2lmP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDMyMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDAzMjBUMDEwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTZiMzgxYWE5MGMyMzcwYmM0Y2M4ZWIzZjgxNzFjMDQwYWIzMWU1YzhkZGE3ODliOTE0ZjUxODhhNjJjYTUyOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.wj1eUtrbL2DLjx7_gtVeMrEINa6vQcrOQtNKHb9MVho" alt="demo" data-animated-image=""></a></p>
<p dir="auto">Inspired by <a href="https://github.com/simeji/jid">jid</a>
and <a href="https://github.com/fiatjaf/jiq">jiq</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Interactive JSON viewer and <code>jq</code> filter editor
<ul dir="auto">
<li>Syntax highlighting for JSON</li>
</ul>
</li>
<li>Accept JSON from stdin, file, URL</li>
<li>Auto-completion for the filter
<ul dir="auto">
<li>Only supports:
<ul dir="auto">
<li><a href="https://jqlang.github.io/jq/manual/#identity" rel="nofollow">Identity</a></li>
<li><a href="https://jqlang.github.io/jq/manual/#object-identifier-index" rel="nofollow">Object Identifier-Index</a></li>
<li><a href="https://jqlang.github.io/jq/manual/#array-index" rel="nofollow">Array Index</a></li>
</ul>
</li>
</ul>
</li>
<li>Hint message to evaluate the filter</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Homebrew</h3><a id="user-content-homebrew" aria-label="Permalink: Homebrew" href="#homebrew"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install ynqa/tap/jnv"><pre>brew install ynqa/tap/jnv</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Cargo</h3><a id="user-content-cargo" aria-label="Permalink: Cargo" href="#cargo"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Requirements</h4><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li><a href="https://www.gnu.org/software/automake/" rel="nofollow">automake</a></li>
</ul>

<div dir="auto"><p dir="auto">Note</p><p dir="auto"><em>jnv</em> does not require users to install <code>jq</code> on their system,
because it utilizes <a href="https://github.com/ynqa/j9">j9</a> Rust bindings.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>

<p dir="auto">Or</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Keymap</h2><a id="user-content-keymap" aria-label="Permalink: Keymap" href="#keymap"></a></p>
<table>
<thead>
<tr>
<th>Key</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td><kbd>Ctrl + C</kbd></td>
<td>Exit <code>jnv</code></td>
</tr>
<tr>
<td><kbd>Tab</kbd></td>
<td>jq filter auto-completion</td>
</tr>
<tr>
<td><kbd>←</kbd></td>
<td>Move the cursor one character to the left</td>
</tr>
<tr>
<td><kbd>→</kbd></td>
<td>Move the cursor one character to the right</td>
</tr>
<tr>
<td><kbd>Ctrl + A</kbd></td>
<td>Move the cursor to the start of the filter</td>
</tr>
<tr>
<td><kbd>Ctrl + E</kbd></td>
<td>Move the cursor to the end of the filter</td>
</tr>
<tr>
<td><kbd>Backspace</kbd></td>
<td>Delete a character of filter at the cursor position</td>
</tr>
<tr>
<td><kbd>Ctrl + U</kbd></td>
<td>Delete all characters of filter</td>
</tr>
<tr>
<td><kbd>↑</kbd>, <kbd>Ctrl + K</kbd></td>
<td>Move the cursor one entry up in JSON viewer</td>
</tr>
<tr>
<td><kbd>↓</kbd>, <kbd>Ctrl + J</kbd></td>
<td>Move the cursor one entry down in JSON viewer</td>
</tr>
<tr>
<td><kbd>Ctrl + H</kbd></td>
<td>Move to the last entry in JSON viewer</td>
</tr>
<tr>
<td><kbd>Ctrl + L</kbd></td>
<td>Move to the first entry in JSON viewer</td>
</tr>
<tr>
<td><kbd>Enter</kbd></td>
<td>Toggle expand/collapse in JSON viewer</td>
</tr>
<tr>
<td><kbd>Ctrl + P</kbd></td>
<td>Expand all folds in JSON viewer</td>
</tr>
<tr>
<td><kbd>Ctrl + N</kbd></td>
<td>Collapse all folds in JSON viewer</td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="JSON navigator and interactive filter leveraging jq

Usage: jnv [OPTIONS] [INPUT]

Examples:
- Read from a file:
        jnv data.json

- Read from standard input:
        cat data.json | jnv

Arguments:
  [INPUT]
          Optional path to a JSON file. If not provided or if &quot;-&quot; is specified, reads from standard input

Options:
  -e, --edit-mode <EDIT_MODE>
                  Specifies the edit mode for the interface.
                  Acceptable values are &quot;insert&quot; or &quot;overwrite&quot;.
                  - &quot;insert&quot; inserts a new input at the cursor's position.
                  - &quot;overwrite&quot; mode replaces existing characters with new input at the cursor's position.
          [default: insert]

  -i, --indent <INDENT>
                  Affect the formatting of the displayed JSON,
                  making it more readable by adjusting the indentation level.
          [default: 2]

  -n, --no-hint
                  When this option is enabled, it prevents the display of
                  hints that typically guide or offer suggestions to the user.

  -d, --expand-depth <EXPAND_DEPTH>
                  Specifies the initial depth to which JSON nodes are expanded in the visualization.
                  Note: Increasing this depth can significantly slow down the display for large datasets.
          [default: 3]

  -l, --suggestion-list-length <SUGGESTION_LIST_LENGTH>
                  Controls the number of suggestions displayed in the list,
                  aiding users in making selections more efficiently.
          [default: 3]

  -h, --help
          Print help (see a summary with '-h')

  -V, --version
          Print version"><pre>JSON navigator and interactive filter leveraging jq

Usage: jnv [OPTIONS] [INPUT]

Examples:
- Read from a file:
        jnv data.json

- Read from standard input:
        cat data.json <span>|</span> jnv

Arguments:
  [INPUT]
          Optional path to a JSON file. If not provided or <span>if</span> <span><span>"</span>-<span>"</span></span> is specified, reads from standard input

Options:
  -e, --edit-mode <span>&lt;</span>EDIT_MODE<span>&gt;</span>
                  Specifies the edit mode <span>for</span> the interface.
                  Acceptable values are <span><span>"</span>insert<span>"</span></span> or <span><span>"</span>overwrite<span>"</span></span>.
                  - <span><span>"</span>insert<span>"</span></span> inserts a new input at the cursor<span><span>'</span>s position.</span>
<span>                  - "overwrite" mode replaces existing characters with new input at the cursor<span>'</span></span>s position.
          [default: insert]

  -i, --indent <span>&lt;</span>INDENT<span>&gt;</span>
                  Affect the formatting of the displayed JSON,
                  making it more readable by adjusting the indentation level.
          [default: 2]

  -n, --no-hint
                  When this option is enabled, it prevents the display of
                  hints that typically guide or offer suggestions to the user.

  -d, --expand-depth <span>&lt;</span>EXPAND_DEPTH<span>&gt;</span>
                  Specifies the initial depth to which JSON nodes are expanded <span>in</span> the visualization.
                  Note: Increasing this depth can significantly slow down the display <span>for</span> large datasets.
          [default: 3]

  -l, --suggestion-list-length <span>&lt;</span>SUGGESTION_LIST_LENGTH<span>&gt;</span>
                  Controls the number of suggestions displayed <span>in</span> the list,
                  aiding users <span>in</span> making selections more efficiently.
          [default: 3]

  -h, --help
          Print <span>help</span> (see a summary with <span><span>'</span>-h<span>'</span></span>)

  -V, --version
          Print version</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intel 8080 emulator. 19th IOCCC. Best of Show (229 pts)]]></title>
            <link>https://nanochess.org/emulator.html</link>
            <guid>39758667</guid>
            <pubDate>Tue, 19 Mar 2024 18:15:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nanochess.org/emulator.html">https://nanochess.org/emulator.html</a>, See on <a href="https://news.ycombinator.com/item?id=39758667">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h2>Intel 8080 emulator. 19th IOCCC. Best of Show.</h2>
<div><p>Sections:
</p><ul>
<li><a href="#sour">Source code</a></li>
<li><a href="#howc">How to compile it</a></li>
<li><a href="#litt">A little course on CP/M</a></li>
<li><a href="#what">What is an 8080?</a></li>
<li><a href="#port">Porting it</a></li>
<li><a href="#howi">How it works</a></li>
<li><a href="#othe">Other notes</a></li>
<li><a href="#usef">Useful links</a></li>
</ul>
</div>
<p>
After winning the IOCCC for the <a href="https://nanochess.org/chess1.html">first time</a>, I had
the idea of writing an emulator of the 8080 processor in 2000 characters of C,
after patterning experimentally the more than 200 instructions and doing
measures of byte count, I realized that it was possible and I made it.
Then I added CP/M support as a extra feature. I was completely astonished when
I won Best of Show of 19th IOCCC :).
</p>
<div><p>
This program was one of three winning entries I sent for the 19th edition, the
other two were a <a href="https://nanochess.org/chess2.html">graphical chess program</a> and a
<a href="https://nanochess.org/knight.html">knight's tour solver</a>.
</p></div>
<h2>Source code</h2>
<p>This program emulates a complete Intel® 8080
processor, along with a teletype and a disk
controller, just like at the start of the personal
computers revolution (circa 1975).</p>
<p>Here is the source code, written in C language:</p>
<pre><small>                               #include &lt;stdio.h&gt;
           #define n(o,p,e)=y=(z=a(e)%16 p x%16 p o,a(e)p x p o),h(
                                #define s 6[o]
             #define p z=l[d(9)]|l[d(9)+1]&lt;&lt;8,1&lt;(9[o]+=2)||++8[o]
                                #define Q a(7)
           #define w 254&gt;(9[o]-=2)||--8[o],l[d(9)]=z,l[1+d(9)]=z&gt;&gt;8
                               #define O )):((
                  #define b (y&amp;1?~s:s)&gt;&gt;"\6\0\2\7"[y/2]&amp;1?0:(
                               #define S )?(z-=
                    #define a(f)*((7&amp;f)-6?&amp;o[f&amp;7]:&amp;l[d(5)])
                               #define C S 5 S 3
                       #define D(E)x/8!=16+E&amp;198+E*8!=x?
                             #define B(C)fclose((C))
                       #define q (c+=2,0[c-2]|1[c-2]&lt;&lt;8)
                          #define m x=64&amp;x?*c++:a(x),
                         #define A(F)=fopen((F),"rb+")
                    unsigned char o[10],l[78114],*c=l,*k=l
                          #define d(e)o[e]+256*o[e-1]
#define h(l)s=l&gt;&gt;8&amp;1|128&amp;y|!(y&amp;255)*64|16&amp;z|2,y^=y&gt;&gt;4,y^=y&lt;&lt;2,y^=~y&gt;&gt;1,s|=y&amp;4
+64506; FILE *u, *v, *e, *V; int x,y,z,Z; main(r,U)char**U;{

     { { { } } }       { { { } } }       { { { } } }       { { { } } }
    { { {   } } }     { { {   } } }     { { {   } } }     { { {   } } }
   { { {     } } }   { { {     } } }   { { {     } } }   { { {     } } }
   { { {     } } }   { { {     } } }   { { {     } } }   { { {     } } }
   { { {     } } }   { { {     } } }   { { {     } } }   { { {     } } }
    { { {   } } }    { { {     } } }    { { {   } } }    { { {     } } }
      { { ; } }      { { {     } } }      { { ; } }      { { {     } } }
    { { {   } } }    { { {     } } }    { { {   } } }    { { {     } } }
   { { {     } } }   { { {     } } }   { { {     } } }   { { {     } } }
   { { {     } } }   { { {     } } }   { { {     } } }   { { {     } } }
   { { {     } } }   { { {     } } }   { { {     } } }   { { {     } } }
    { { {   } } }     { { {   } } }     { { {   } } }     { { {   } } }
     { { { } } }       { { { } } }       { { { } } }       { { { } } }

                                   for(v A((u A((e A((r-2?0:(V A(1[U])),"C")
),system("stty raw -echo min 0"),fread(l,78114,1,e),B(e),"B")),"A")); 118-(x
=*c++); (y=x/8%8,z=(x&amp;199)-4 S 1 S 1 S 186 S 2 S 2 S 3 S 0,r=(y&gt;5)*2+y,z=(x&amp;
207)-1 S 2 S 6 S 2 S 182 S 4)?D(0)D(1)D(2)D(3)D(4)D(5)D(6)D(7)(z=x-2 C C C C
C C C C+129 S 6 S 4 S 6 S 8 S 8 S 6 S 2 S 2 S 12)?x/64-1?((0 O a(y)=a(x) O 9
[o]=a(5),8[o]=a(4) O 237==*c++?((int (*)())(2-*c++?fwrite:fread))(l+*k+1[k]*
256,128,1,(fseek(e=5[k]-1?u:v,((3[k]|4[k]&lt;&lt;8)&lt;&lt;7|2[k])&lt;&lt;7,Q=0),e)):0 O y=a(5
),z=a(4),a(5)=a(3),a(4)=a(2),a(3)=y,a(2)=z O c=l+d(5) O y=l[x=d(9)],z=l[++x]
,x[l]=a(4),l[--x]=a(5),a(5)=y,a(4)=z O 2-*c?Z||read(0,&amp;Z,1),1&amp;*c++?Q=Z,Z=0:(
Q=!!Z):(c++,Q=r=V?fgetc(V):-1,s=s&amp;~1|r&lt;0) O++c,write(1,&amp;7[o],1) O z=c+2-l,w,
c=l+q O p,c=l+z O c=l+q O s^=1 O Q=q[l] O s|=1 O q[l]=Q O Q=~Q O a(5)=l[x=q]
,a(4)=l[++x] O s|=s&amp;16|9&lt;Q%16?Q+=6,16:0,z=s|=1&amp;s|Q&gt;159?Q+=96,1:0,y=Q,h(s&lt;&lt;8)
O l[x=q]=a(5),l[++x]=a(4) O x=Q%2,Q=Q/2+s%2*128,s=s&amp;~1|x O Q=l[d(3)]O x=Q  /
128,Q=Q*2+s%2,s=s&amp;~1|x O l[d(3)]=Q O s=s&amp;~1|1&amp;Q,Q=Q/2|Q&lt;&lt;7 O Q=l[d(1)]O s=~1
&amp;s|Q&gt;&gt;7,Q=Q*2|Q&gt;&gt;7 O l[d(1)]=Q O m y n(0,-,7)y) O m z=0,y=Q|=x,h(y) O m z=0,
y=Q^=x,h(y) O m z=Q*2|2*x,y=Q&amp;=x,h(y) O m Q n(s%2,-,7)y) O m Q n(0,-,7)y)  O
m Q n(s%2,+,7)y) O m Q n(0,+,7)y) O z=r-8?d(r+1):s|Q&lt;&lt;8,w O p,r-8?o[r+1]=z,r
[o]=z&gt;&gt;8:(s=~40&amp;z|2,Q=z&gt;&gt;8) O r[o]--||--o[r-1]O a(5)=z=a(5)+r[o],a(4)=z=a(4)
+o[r-1]+z/256,s=~1&amp;s|z&gt;&gt;8 O ++o[r+1]||r[o]++O o[r+1]=*c++,r[o]=*c++O z=c-l,w
,c=y*8+l O x=q,b z=c-l,w,c=l+x) O x=q,b c=l+x) O b p,c=l+z) O a(y)=*c++O r=y
,x=0,a(r)n(1,-,y)s&lt;&lt;8) O r=y,x=0,a(r)n(1,+,y)s&lt;&lt;8))));
system("stty cooked echo"); B((B((V?B(V):0,u)),v)); }</small></pre>
<h2>How to compile it</h2>
<p>First, download the source code from <a href="https://nanochess.org/toledo2.c">here</a>.
It requires an *NIX compatible system (find porting notes below).</p>
<p>To compile use:</p>
<pre><small>    cc toledo2.c -o toledo2</small></pre>
<div><p>The emulator needs an initial memory image to do something
usable, so it will need two files
(</p><tt><a href="https://nanochess.org/c_basic.bin">c_basic.bin</a></tt><p> and
</p><tt><a href="https://nanochess.org/c_bios.bin">c_bios.bin</a></tt><p>).
Rename </p><tt>c_basic.bin</tt><p> to C and run
the emulator, and et voila! you have the public
domain Palo Alto Tiny BASIC (by Li-Chen Wang),
published on the very first volume of the now extinct Dr. Dobb's
Journal magazine.</p></div>
<p>Type using uppercase letters, here are three example
programs, press Enter after each line:</p>
<pre><small>   10 PRINT "Hello, world!"
   LIST
   RUN

   10 FOR A=1 TO 10       10 INPUT A
   20 PRINT A             20 INPUT B
   30 NEXT A              30 PRINT A+B
   LIST                   LIST
   RUN                    RUN</small></pre>
<p>Press Ctrl+Z to quit, by the way, the segmentation
fault is normal at this point.</p>
<p>All good programmers started learning BASIC, now,
what about a CP/M emulator?</p>
<p>Download the following file (not included because
of possible copyright and blah, blah):</p>
<pre><small>  <a href="http://www.retroarchive.org/cpm/os/KAYPROII.ZIP">http://www.retroarchive.org/cpm/os/KAYPROII.ZIP</a></small></pre>
<p>Extract CPM64.COM from the SOURCE directory, and
copy it to files named A and B (these will be the
disk drives). Now rename the provided c_bios.bin to C
and run the emulator.</p>
<p>Now you have a running CP/M operating system!, with two
files on A: drive, HALT.COM to stop the emulator
(so it closes drives) and IMPORT.COM to introduce
new files.</p>
<p>To get a complete CP/M system, you will need the
following files from the KAYPROII.ZIP, SOURCE
directory:</p>
<pre><small>  ASM.COM  DDT.COM   DUMP.COM   ED.COM   LOAD.COM
  PIP.COM  STAT.COM  SUBMIT.COM XSUB.COM</small></pre>
<p>To import them, you must run the emulator with an
argument, by example:</p>
<pre><small>  prog DDT.COM</small></pre>
<p>When the A&gt; prompt appears, do:</p>
<pre><small>  IMPORT DDT.COM</small></pre>
<p>When it ends, do HALT, so the file is saved, and
you can start the same process with another file.</p>
<p>The WS33-KPR directory of the KAYPROII.ZIP file also
contains a Wordstar version that works with this emulator. There is also
an interesting game, the classical Adventure by Crowther and Woods in the
ADVENTUR directory, at that time was amazing that a microcomputer could
contain such big game.</p>
<p>At this time I have tested successfully the
following software from
<a href="http://www.retroarchive.org/">www.retroarchive.org</a>:</p>
<pre>Languages
<small>  <a href="http://www.retroarchive.org/cpm/lang/c80v30.zip">http://www.retroarchive.org/cpm/lang/c80v30.zip</a>
  <a href="http://www.retroarchive.org/cpm/lang/Mbasic.com">http://www.retroarchive.org/cpm/lang/Mbasic.com</a>
</small>
Spreadsheet
<small>  <a href="http://www.retroarchive.org/cpm/business/MULTPLAN.ZIP">http://www.retroarchive.org/cpm/business/MULTPLAN.ZIP</a>
</small>
Games
<small>  <a href="http://www.retroarchive.org/cpm/games/zork123_80.zip">http://www.retroarchive.org/cpm/games/zork123_80.zip</a>
</small>
Utilities
<small>  <a href="http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/ARC-LBR/UNARC16.ARK">http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/ARC-LBR/UNARC16.ARK</a>
  <a href="http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/ARC-LBR/UNARC16.MSG">http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/ARC-LBR/UNARC16.MSG</a>
  <a href="http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/ARC-LBR/DELBR12.ARK">http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/ARC-LBR/DELBR12.ARK</a>
  <a href="http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/SQUSQ/USQ-20.COM">http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/SQUSQ/USQ-20.COM</a>
  <a href="http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/SQUSQ/UNCR8080.LBR">http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/SQUSQ/UNCR8080.LBR</a>
  <a href="http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/DIRUTL/XDIR3-12.LBR">http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/DIRUTL/XDIR3-12.LBR</a>
  <a href="http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/DIRUTL/CU.LBR">http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/DIRUTL/CU.LBR</a>
  <a href="http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/FILCPY/SWEEP40.LBR">http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/FILCPY/SWEEP40.LBR</a></small></pre>
<p>Some programs require installation to configure
the terminal, locate ANSI or VT-100.</p>
<h2>A little course on CP/M</h2>
<p>The CP/M user's manuals are available on
<a href="http://www.retroarchive.org/">www.retroarchive.org</a>. But
if you remember how to use MS-DOS then you can use CP/M very easily. Here is a
little reference of the command line:</p>
<pre><b>Internal commands:</b>
A:              Change current drive to A
B:              Change current drive to B
DIR             List files in drive
DIR *.TXT       List all files with TXT extension
TYPE FILE.TXT   Shows content of FILE.TXT
ERA FILE.TXT    Erases file FILE.TXT
USER 1          Change to user 1 (0-15 available)
                It is something as subdirectories.
                So you can separate your files.

<b>External commands:</b>
STAT            Show used/free space on drive
STAT *.*        Show file sizes.
DDT PROG.COM    Debug PROG.COM.
                To quit use Ctrl+C

                Dump address 0100 (hex):
                  D0100</pre>
<p><img src="https://nanochess.org/img/captura1.gif" alt="Emulator running Wordstar under CP/M"><br>Emulator running Wordstar
under CP/M.</p>
<h2>What is an 8080?</h2>
<p>It is simply the little brother of the Zilog Z80,
it has no extended registers (AF', BC', DE', HL',
IX or IY), no relative jumps, and every instruction
beginning with CB, DD, ED or FD doesn't exist.</p>
<p>The flags are only S (Sign, bit 7), Z (Zero, bit 6),
P (Parity, bit 2) and C (Carry, bit 0).</p>
<p>The 8080 processor was created by Federico Faggin and
Masatoshi Shima in 1974, afterwards both would design the Zilog Z80 in 1976,
these two processors were pretty important and influential for the rise of
microcomputers.</p>
<h2>Porting it</h2>
<div><p>It is easy if your platform has </p><tt>getch</tt><p>/</p><tt>kbhit</tt><p>
and ANSI terminal</p></div>
<pre><small>    read    --&gt;  Z=kbhit()?getch():0
    write   --&gt;  putchar(7[o])
    system  --&gt;  nothing</small></pre>
<p>Also add the following to trap Ctrl-C:</p>
<pre><small>    #include &lt;signal.h&gt;
    signal(SIGINT, SIG_IGN);</small></pre>
<div><p>On PC/DOS you will need to add </p><tt>ANSI.SYS</tt><p> to
</p><tt>CONFIG.SYS</tt></div>
<div><p>In *NIX the </p><tt>min 0</tt><p> for </p><tt>stty</tt><p> is required,
circa 2001 it was not required.</p></div>
<h2>How it works (SPOILER)</h2>
<p>The l array contains the 64K memory, it is
initialized with a boot image loaded from the 'C'
file, the program counter is the c pointer, and
registers are on o[]. The main loops reads every
op-code and separates it in one of three common
forms, a lot of trinary operators selects the
instruction.</p>
<p>Execution starts at 0000, you can write your own boot
or monitor program, or even your own operating system playing with the
'C' file.</p>
<pre><small>   o[0] = B register   o[1] = C register
   o[2] = D register   o[3] = E register
   o[4] = H register   o[5] = L register
   o[6] = Flags        o[7] = A or accumulator</small></pre>
<p>The following instructions do peripheral operation:</p>
<pre><small>   76           Quits emulator
   DB 00        Reads key pressed status
   DB 01        Reads key
   DB 02        Reads byte from file (Carry=EOF)
   D3 xx        Writes byte from acc. to console
   ED ED 02     Reads sector (128 bytes)
   ED ED 03     Writes sector (128 bytes)

   Memory addresses:

   FBFA = Low source/target address
   FBFB - High source/target address
   FBFC - Sector (0-127)
   FBFD - Cylinder (low byte)
   FBFE - Cylinder (high byte)
   FBFF - Drive (1/2)</small></pre>
<p>The BIOS is tailor made for this emulator and
helps to simplify it. Though the size of each virtual hard disk drive can
reach 1 GB, CP/M only handles up to 8 MB.</p>
<h2>Other notes:</h2>
<ul>
<li>The 8080 runs at your computer speed divided
between a number that I have not calculated.
</li><li>This obfuscated processor was created using
obfuscated code produced by an obfuscated mind,
no brains were harmed during its production,
except those that tried to read the code.
</li><li>The original version of this code was eaten
by my C++ dog.
</li><li>I intended to make it simple to understand,
it uses only four C keywords.
</li><li>Also I discovered that braces are very useful
for commenting.
</li><li>Why to bother with prototypes?, every good C
programmer can develop its C programs using
only one function.
</li></ul>
<h2>And now it is 2024</h2>
<p>This emulator was developed eighteen years ago when the computers had 32-bit processors and it used a hole in the C language syntax where you could pass a pointer on an integer. In fact, this is the IOCCC objective: make C compilers to do things these shouldn't be supposed to do.</p>
<p>However, the C compilers for 64-bit processors don't allow it anymore as pointers are 64-bit and the int types are 32-bit, so compilers stop with an error (in especial in macOS because clang).</p>
<p>The source code in this webpage is already updated to use FILE * instead of int, and this way we brought CP/M to year 2024, hehe.</p>
<p>If you are curious about it, the previous version of the emulator can be downloaded <a href="https://nanochess.org/toledo2_orig.c">here</a> or in the IOCCC website. In 2020, Mark Abene sent me a workaround for x86-64 computers:</p>
<pre><small>    gcc -static toledo2.c -o toledo2</small></pre>
<p>But this doesn't work in macOS.</p>
<h2>Useful links:</h2>
<ul>
<li><a href="http://www.nicholson.com/rhn/basic/">Palo Alto Tiny BASIC source
code</a>, this is a modified version of the original.</li>
<li><a href="http://www.retroarchive.org/cpm/archive/unofficial/source.html">CP/M
source code</a>, the foundation of an operating system: monotask,
no memory manager, only console and file services provided.</li>
<li><a href="http://stardot.org.uk/forums/viewtopic.php?f=3&amp;t=9821&amp;p=116643">Someone received an 8080 computer as a gift</a>,
dumped the ROM, and someone
else found a way to test them with my 8080 emulator. :)</li>
</ul>
<p>Last modified: Feb/06/2024</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What does Alan Kay think about LLMs? (177 pts)]]></title>
            <link>https://www.quora.com/What-does-Alan-Kay-think-about-programming-and-teaching-programming-with-copilots-and-LLMs-of-today</link>
            <guid>39758391</guid>
            <pubDate>Tue, 19 Mar 2024 17:48:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quora.com/What-does-Alan-Kay-think-about-programming-and-teaching-programming-with-copilots-and-LLMs-of-today">https://www.quora.com/What-does-Alan-Kay-think-about-programming-and-teaching-programming-with-copilots-and-LLMs-of-today</a>, See on <a href="https://news.ycombinator.com/item?id=39758391">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Natural language instructions induce generalization in networks of neurons (172 pts)]]></title>
            <link>https://www.nature.com/articles/s41593-024-01607-5</link>
            <guid>39757665</guid>
            <pubDate>Tue, 19 Mar 2024 16:47:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/s41593-024-01607-5">https://www.nature.com/articles/s41593-024-01607-5</a>, See on <a href="https://news.ycombinator.com/item?id=39757665">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <div id="Sec1-section" data-title="Main"><h2 id="Sec1">Main</h2><div id="Sec1-content"><p>In a laboratory setting, animals require numerous trials in order to acquire a new behavioral task. This is in part because the only means of communication with nonlinguistic animals is simple positive and negative reinforcement signals. By contrast, it is common to give written or verbal instructions to humans, which allows them to perform new tasks relatively quickly. Further, once humans have learned a task, they can typically describe the solution with natural language. The dual ability to use an instruction to perform a novel task and, conversely, produce a linguistic description of the demands of a task once it has been learned are two unique cornerstones of human communication. Yet, the computational principles that underlie these abilities remain poorly understood.</p><p>One influential systems-level explanation posits that flexible interregional connectivity in the prefrontal cortex allows for the reuse of practiced sensorimotor representations in novel settings<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Cole, M. W. et al. Multi-task connectivity reveals flexible hubs for adaptive task control. Nature Neurosci. 16, 1348–1355 (2013)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR1" id="ref-link-section-d167182994e339">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Miller, E. K. &amp; Cohen, J. D. An integrative theory of prefrontal cortex function. Annu. Rev. Neurosci. 24, 167–202 (2001)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR2" id="ref-link-section-d167182994e342">2</a></sup>. More recently, multiple studies have observed that when subjects are required to flexibly recruit different stimulus-response patterns, neural representations are organized according to the abstract structure of the task set<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Bernardi, S. et al. The geometry of abstraction in the hippocampus and prefrontal cortex. Cell 183, 954–967 (2020)." href="#ref-CR3" id="ref-link-section-d167182994e346">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Minxha, J., Adolphs, R., Fusi, S., Mamelak, A. N. &amp; Rutishauser, U. Flexible recruitment of memory-based choice representations by the human medial frontal cortex. Science 368, eaba3313 (2020)." href="#ref-CR4" id="ref-link-section-d167182994e346_1">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Takuya, I. et al. Compositional generalization through abstract representations in human and artificial neural networks. In Proc. 36th Conference on Neural Information Processing Systems (eds Koyejo, S. et al.) 32225–32239 (Curran Associates, Inc., 2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR5" id="ref-link-section-d167182994e349">5</a></sup>. Lastly, recent modeling work has shown that a multitasking recurrent neural network (RNN) will share dynamical motifs across tasks with similar demands<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Driscoll, L., Shenoy, K. &amp; Sussillo, D. Flexible multitask computation in recurrent networks utilizes shared dynamical motifs. Preprint at bioRxiv 
                https://doi.org/10.1101/2022.08.15.503870
                
               (2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR6" id="ref-link-section-d167182994e353">6</a></sup>. This work forms a strong basis for explanations of flexible cognition in humans but leaves open the question of how linguistic information can reconfigure a sensorimotor network so that it performs a novel task well on the first attempt. Overall, it remains unclear what representational structure we should expect from brain areas that are responsible for integrating linguistic information in order to reorganize sensorimotor mappings on the fly.</p><p>These questions become all the more pressing given that recent advances in machine learning have led to artificial systems that exhibit human-like language skills<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Brown, Tom, et al. Language models are few-shot learners. In Proc. 34th International Conference on Neural Information Processing Systems 1877–1901 (Curran Associates Inc., 2020)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR7" id="ref-link-section-d167182994e360">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Ramesh, A. et al. Zero-shot text-to-image generation. In Proc. 38th International Conference on Machine Learning (eds Marina, M. &amp; Tong, Z.) 8821–8831 (PMLR, 2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR8" id="ref-link-section-d167182994e363">8</a></sup>. Recent works have matched neural data recorded during passive listening and reading tasks to activations in autoregressive language models (that is, GPT<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Radford, A. et al. Language models are unsupervised multitask learners. OpenAI 1, 9 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR9" id="ref-link-section-d167182994e367">9</a></sup>), arguing that there is a fundamentally predictive component to language comprehension<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Schrimpf, M. et al. The neural architecture of language: integrative modeling converges on predictive processing. Proc. Natl Acad. Sci. USA 
                https://doi.org/10.1073/pnas.2105646118
                
               (2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR10" id="ref-link-section-d167182994e371">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Goldstein, A. et al. Shared computational principles for language processing in humans and deep language models. Nature Neurosci. 25, 369–380 (2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR11" id="ref-link-section-d167182994e374">11</a></sup>. Additionally, some high-profile machine learning models do show the ability to use natural language as a prompt to perform a linguistic task or render an image, but the outputs of these models are difficult to interpret in terms of a sensorimotor mapping that we might expect to occur in a biological system<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Chowdhery, A. et al. Palm: scaling language modeling with pathways. J. Mach. Learn. Res. 24, 11324–11436 (2023)." href="#ref-CR12" id="ref-link-section-d167182994e378">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Thoppilan, R. et al. Lamda: language models for dialog applications. Preprint at 
                https://arxiv.org/abs/2201.08239
                
               (2022)." href="#ref-CR13" id="ref-link-section-d167182994e378_1">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Rombach, R. et al. High-resolution image synthesis with latent diffusion models. In Proc. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 10674–10685 (IEEE, 2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR14" id="ref-link-section-d167182994e381">14</a></sup>. Alternatively, recent work on multimodal interactive agents may be more interpretable in terms of the actions they take, but utilize a perceptual hierarchy that fuses vision and language at early stages of processing, making them difficult to map onto functionally and anatomically distinct language and vision areas in human brains<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Zitkovich, B. et al. Rt-2: vision-language-action models transfer web knowledge to robotic control. In Proc. 7th Conference on Robot Learning (eds Tan, J. et al.) 2165-2183 (PMLR, 2023)." href="#ref-CR15" id="ref-link-section-d167182994e385">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Abramson, J. et al. Imitating interactive intelligence. Preprint at 
                https://arxiv.org/abs/2012.05672
                
               (2021)." href="#ref-CR16" id="ref-link-section-d167182994e385_1">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="DeepMind Interactive Agents Team. Creating multimodal interactive agents with imitation and self-supervised learning. Preprint at 
                https://arxiv.org/abs/2112.03763
                
               (2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR17" id="ref-link-section-d167182994e388">17</a></sup>.</p><p>We, therefore, seek to leverage the power of language models in a way that results in testable neural predictions detailing how the human brain processes natural language in order to generalize across sensorimotor tasks.</p><p>To that end, we train an RNN (sensorimotor-RNN) model on a set of simple psychophysical tasks where models process instructions for each task using a pretrained language model. We find that embedding instructions with models tuned to sentence-level semantics allow sensorimotor-RNNs to perform a novel task at 83% correct, on average. Generalization in our models is supported by a representational geometry that captures task subcomponents and is shared between instruction embeddings and sensorimotor activity, thereby allowing a composition of practice skills in a novel setting. We also find that individual neurons modulate their tuning based on the semantics of instructions. We demonstrate how a network trained to interpret linguistic instructions can invert this understanding and produce a linguistic description of a previously unseen task based on the information in motor feedback signals. We end by discussing how these results can guide research on the neural basis of language-based generalization in the human brain.</p></div></div><div id="Sec2-section" data-title="Results"><h2 id="Sec2">Results</h2><div id="Sec2-content"><h3 id="Sec3">Instructed models and task set</h3><p>We train sensorimotor-RNNs on a set of 50 interrelated psychophysical tasks that require various cognitive capacities that are well studied in the literature<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e411">18</a></sup>. Two example tasks are presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig1">1a,b</a> as they might appear in a laboratory setting. For all tasks, models receive a sensory input and task-identifying information and must output motor response activity (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig1">1c</a>). Input stimuli are encoded by two one-dimensional maps of neurons, each representing a different input modality, with periodic Gaussian tuning curves to angles (over (0, 2<i>π</i>)). Output responses are encoded in the same way. Inputs also include a single fixation unit. After the input fixation is off, the model can respond to the input stimuli. Our 50 tasks are roughly divided into 5 groups, ‘Go’, ‘Decision-making’, ‘Comparison’, ‘Duration’ And ‘Matching’, where within-group tasks share similar sensory input structures but may require divergent responses. For instance, in the decision-making (DM) task, the network must respond in the direction of the stimulus with the highest contrast, whereas in the anti-decision-making (AntiDM) task, the network responds to the stimulus with the weakest contrast (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig1">1a</a>). Thus, networks must properly infer the task demands for a given trial from task-identifying information in order to perform all tasks simultaneously (see Methods for task details; see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">13</a> for example trials of all tasks).</p><div data-test="figure" data-container-section="figure" id="figure-1" data-title="Tasks and models."><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Fig. 1: Tasks and models.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="684"></picture></a></div><p><b>a</b>,<b>b</b>, Illustrations of example trials as they might appear in a laboratory setting. The trial is instructed, then stimuli are presented with different angles and strengths of contrast. The agent must then respond with the proper angle during the response period. <b>a</b>, An example AntiDM trial where the agent must respond to the angle presented with the least intensity. <b>b</b>, An example COMP1 trial where the agent must respond to the first angle if it is presented with higher intensity than the second angle otherwise repress response. <b>c</b>, Diagram of model inputs and outputs. Sensory inputs (fixation unit, modality 1, modality 2) are shown in red and model outputs (fixation output, motor output) are shown in green. Models also receive a rule vector (blue) or the embedding that results from passing task instructions through a pretrained language model (gray). A list of models tested is provided in the inset.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>In our models, task-identifying input is either nonlinguistic or linguistic. We use two nonlinguistic control models. First, in SIMPLENET, the identity of a task is represented by one of 50 orthogonal rule vectors. Second, STRUCTURENET uses a set of 10 orthogonal structure vectors, each representing a dimension of the task set (that is, respond weakest versus strongest direction), and tasks are encoded using combinations of these vectors (see Supplementary Notes <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">3</a> for the full set of structure combinations). As a result, STRUCTURENET fully captures all the relevant relationships among tasks, whereas SIMPLENET encodes none of this structure.</p><p>Instructed models use a pretrained transformer architecture<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Vaswani, A. et al. Attention is all you need. In Proc. 31st International Conference on Neural Information Processing Systems 6000–6010 (Curran Associates Inc., 2017)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR19" id="ref-link-section-d167182994e474">19</a></sup> to embed natural language instructions for the tasks at hand. For each task, there is a corresponding set of 20 unique instructions (15 training, 5 validation; see Supplementary Notes <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">2</a> for the full instruction set). We test various types of language models that share the same basic architecture but differ in their size and also their pretraining objective. We tested two autoregressive models, a standard and a large version of GPT2, which we call GPT and GPT (XL), respectively. Previous work has demonstrated that GPT activations can account for various neural signatures of reading and listening<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Goldstein, A. et al. Shared computational principles for language processing in humans and deep language models. Nature Neurosci. 25, 369–380 (2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR11" id="ref-link-section-d167182994e481">11</a></sup>. BERT is trained to identify masked words within a piece of text<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Devlin, J., Chang, M., Lee, K. &amp; Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. Preprint at 
                http://arxiv.org/abs/1810.04805
                
               (2018)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR20" id="ref-link-section-d167182994e485">20</a></sup>, but it also uses an unsupervised sentence-level objective, in which the network is given two sentences and must determine whether they follow each other in the original text. SBERT is trained like BERT but receives additional tuning on the Stanford Natural Language Inference task, a hand-labeled dataset detailing the logical relationship between two candidate sentences (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a>)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Reimers, N. &amp; Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. Preprint at 
                https://arxiv.org/abs/1908.10084
                
               (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR21" id="ref-link-section-d167182994e493">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Bowman, S. R., Angeli, G., Potts, C. &amp; Manning, C. D. A large annotated corpus for learning natural language inference. Preprint at 
                http://arxiv.org/abs/1508.05326
                
               (2015)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR22" id="ref-link-section-d167182994e496">22</a></sup>. Lastly, we use the language embedder from CLIP, a multimodal model that learns a joint embedding space of images and text captions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Radford, A. et al. &quot;Learning transferable visual models from natural language supervision. In Proc. 38th International Conference on Machine Learning (eds Marina, M. &amp; Tong, Z.) 8748–8763 (PMLR, 2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR23" id="ref-link-section-d167182994e500">23</a></sup>. We call a sensorimotor-RNN using a given language model LANGUAGEMODELNET and append a letter indicating its size. The various sizes of models are given in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig1">1c</a>. For each language model, we apply a pooling method to the last hidden state of the transformer and pass this fixed-length representation through a set of linear weights that are trained during task learning. This results in a 64-dimensional instruction embedding across all models (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a>). Language model weights are frozen unless otherwise specified. Finally, as a control, we also test a bag-of-words (BoW) embedding scheme that only uses word count statistics to embed each instruction.</p><p>First, we verify our models can perform all tasks simultaneously. For instructed models to perform well, they must infer the common semantic content between 15 distinct instruction formulations for each task. We find that all our instructed models can learn all tasks simultaneously except for GPTNET, where performance asymptotes are below the 95% threshold for some tasks. Hence, we relax the performance threshold to 85% for models that use GPT (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">1</a>; see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a> for training details). We additionally tested all architectures on validation instructions (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">2</a>). SBERTNET (L) and SBERTNET are our best-performing models, achieving an average performance of 97% and 94%, respectively, on validation instructions, demonstrating that these networks infer the proper semantic content even for entirely novel instructions.</p><h3 id="Sec4">Generalization to novel tasks</h3><p>We next examined the extent to which different language models aided generalization to novel tasks. We trained individual networks on 45 tasks and then tested performance when exposed to the five held-out tasks. We use unequal-variance <i>t</i>-tests to make comparisons among the performance of different models. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig2">2</a> shows results with <i>P</i> values for the most relevant comparisons (a full matrix of comparisons across all models can be found in Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">3</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">4</a>)</p><div data-test="figure" data-container-section="figure" id="figure-2" data-title="Model performance on novel tasks."><figure><figcaption><b id="Fig2" data-test="figure-caption-text">Fig. 2: Model performance on novel tasks.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="520"></picture></a></div><p><b>a</b>, Learning curves for the first 100 exposures to held-out tasks averaged over all tasks. Data are presented as the mean ± s.d. across different <i>n</i> = 5 random initializations of sensorimotor-RNN weights. For all subplots, asterisks indicate significant differences among performance according to a two-sided unequal-variance <i>t</i>-test. Most relevant comparisons are presented in plots (for all subplots, not significant (NS), <i>P</i> &gt; 0.05, *<i>P</i> &lt; 0.05, **<i>P</i> &lt; 0.01, ***<i>P</i> &lt; 0.001; STRUCTURENET versus SBERTNET (L): <i>t</i> = 3.761, <i>P</i> = 1.89 × 10<sup>−4</sup>; SBERTNET (L) versus SBERTNET: <i>t</i> = 2.19, <i>P</i> = 0.029; SBERTNET versus CLIPNET: <i>t</i> = 6.22, <i>P</i> = 1.02 × 10<sup>−9</sup>; CLIPNET versus BERTNET: <i>t</i> = 1.037, <i>P</i> = 0.300; BERTNET versus GPTNET (XL): <i>t</i> = −1.122, <i>P</i> = 0.262; GPTNET (XL) versus GPTNET: <i>t</i> = 6.22, <i>P</i> = 1.04 × 10<sup>−9</sup>; GPTNET versus BOWNET: <i>t</i> = −3.346, <i>P</i> = 8.85 × 10<sup>−</sup><sup>4</sup>; BOWNET versus SIMPLENET: <i>t</i> = 10.25, <i>P</i> = 2.091 × 10<sup>−22</sup>). A full table of pairwise comparisons can be found in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">3</a>. <b>b</b>, Distribution of generalization performance (that is, first exposure to novel task) across models. <b>c</b>–<b>f</b>, Performance across different test conditions for <i>n</i> = 5 different random initialization of sensorimotor-RNN weights where each point indicates average performance across tasks for a given initialization. <b>c</b>, Generalization performance for tasks where instructions are swapped at test time (STRUCTURENET versus SBERTNET (L): <i>t</i> = −0.15, <i>P</i> = 0.875; SBERTNET (L) versus SBERTNET: <i>t</i> = −2.102, <i>P</i> = 0.036; SBERTNET versus CLIPNET: <i>t</i> = −0.162, <i>P</i> = 0.871; CLIPNET versus BERTNET: <i>t</i> = 0.315, <i>P</i> = 0.752; BERTNET versus GPTNET (XL): <i>t</i> = 0.781, <i>P</i> = 0.435; GPTNET (XL) versus GPTNET: <i>t</i> = 1.071, <i>P</i> = 0.285; GPTNET versus BOWNET: <i>t</i> = −2.702, <i>P</i> = 0.007; BOWNET versus SIMPLENET: <i>t</i> = −3.471, <i>P</i> = 5.633<sup>−4</sup>). A full table of pairwise comparisons can be found in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">4</a>. <b>d</b>, Generalization performance for models where tasks from the same family are held out during training (STRUCTURENET versus SBERTNET (L): <i>t</i> = 0.629, <i>P</i> = 0.530; SBERTNET (L) versus SBERTNET: <i>t</i> = −0.668, <i>P</i> = 0.504; SBERTNET versus CLIPNET: <i>t</i> = 8.043, <i>P</i> = 7.757 × 10<sup>−15</sup>; CLIPNET versus BERTNET: <i>t</i> = −0.306, <i>P</i> = 0.759; BERTNET versus GPTNET (XL): <i>t</i> = 0.163, <i>P</i> = 0.869; GPTNET (XL) versus GPTNET: <i>t</i> = 1.534, <i>P</i> = 0.126; GPTNET versus BOWNET: <i>t</i> = −6.418, <i>P</i> = 3.26 × 10<sup>−10</sup>; BOWNET versus SIMPLENET: <i>t</i> = 14.23, <i>P</i> = 8.561<sup>−39</sup>). A full table of pairwise comparisons can be found in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">4</a>. <b>e</b>, Generalization performance for models where the last layers of language models are allowed to fine-tune to the loss from sensorimotor tasks (STRUCTURENET versus SBERTNET (L): <i>t</i> = 1.203, <i>P</i> = 0.229; SBERTNET (L) versus SBERTNET: <i>t</i> = 2.399, <i>P</i> = 0.016; SBERTNET versus CLIPNET: <i>t</i> = 5.186, <i>P</i> = 3.251 × 10<sup>−7</sup>; CLIPNET versus BERTNET: <i>t</i> = −3.002, <i>P</i> = 0.002; BERTNET versus GPTNET (XL): <i>t</i> = 0.522, <i>P</i> = 0.601; GPTNET (XL) versus GPTNET: <i>t</i> = 2.631, <i>P</i> = 0.009; GPTNET versus BOWNET: <i>t</i> = 4.440, <i>P</i> = 1.134 × 10<sup>−5</sup>; BOWNET versus SIMPLENET: <i>t</i> = 10.255, <i>P</i> = 2.091 × 10<sup>−22</sup>). A full table of pairwise comparisons can be found in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">4</a>. <b>f</b>, Average difference in performance between tasks that use standard imperative instructions and those that use instructions with conditional clauses and require a simple deductive reasoning component. Colored asterisks at the bottom of the plot show <i>P</i> values for a two-sided, unequal-variance <i>t</i>-test between the null distribution constructed using random splits of the task set (transparent points represent mean differences for random splits; STRUCTURENET: <i>t</i> = −36.46, <i>P</i> = 4.34 × 10<sup>−23</sup>; SBERTNET (L): <i>t</i> = −16.38, <i>P</i> = 3.02 × 10<sup>−5</sup>; SBERTNET: <i>t</i> = −15.35, <i>P</i> = 3.920 × 10<sup>−5</sup>; CLIPNET: <i>t</i> = −44.68, <i>P</i> = 5.32 × 10<sup>−</sup><sup>13</sup>; BERTNET: <i>t</i> = −25.51, <i>P</i> = 3.14 × 10<sup>−8</sup>; GPTNET (XL): <i>t</i> = −16.99, <i>P</i> = 3.61 × 10<sup>−6</sup>; GPTNET: <i>t</i> = −9.150, <i>P</i> = 0.0002; BOWNET: <i>t</i> = −70.99, <i>P</i> = 4.566 × 10<sup>−35</sup>; SIMPLENET: <i>t</i> = 19.60, <i>P</i> = 5.82 × 10<sup>−6</sup>), and asterisks at the top of plot indicate <i>P</i>-value results from a <i>t</i>-test comparing differences with STRUCTURENET and our other instructed models (versus SBERTNET (L): <i>t</i> = 3.702, <i>P</i> = 0.0168; versus SBERTNET: <i>t</i> = 6.592, <i>P</i> = 0.002; versus CLIPNET: <i>t</i> = 30.35, <i>P</i> = 2.367 × 10<sup>−7</sup>; versus BERTNET: <i>t</i> = 7.234, <i>P</i> = 0.0007; versus GPTNET (XL): <i>t</i> = 5.282, <i>P</i> = 0.004; versus GPTNET: <i>t</i> = −1.745, <i>P</i> = 0.149; versus BOWNET: <i>t</i> = 75.04, <i>P</i> = 9.96 × 10<sup>−11</sup>; versus SIMPLENET: <i>t</i> = −30.95, <i>P</i> = 2.86 × 10<sup>−</sup><sup>6</sup>; see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a> and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">6</a>. for full comparisons).</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>Our uninstructed control model SIMPLENET performs at 39%, on average, on the first presentation of a novel task (zero-shot generalization). This serves as a baseline for generalization. Note that despite the orthogonality of task rules provided to SIMPLENET, exposure to the task set allows models to learn patterns that are common to all tasks (for example, always repress response during fixation). Therefore, 39% is not chance-level performance per se, but rather performance achieved by a network trained and tested on a task set with some common requirements for responding. GPTNET, exhibits a zero-shot generalization of 57%. This is a significant improvement over SIMPLENET (<i>t</i> = 8.32, <i>P</i> = 8.24 × 10<sup>−16</sup>). Strikingly, increasing the size of GPT by an order of magnitude to the 1.5 billion parameters used by GPT (XL) only resulted in modest gains over BOWNET (64%), with GPTNET (XL) achieving 68% on held-out tasks (<i>t</i> = 2.04, <i>P</i> = 0.047). By contrast, CLIPNET (S), which uses 4% of the number of parameters utilized by GPTNET (XL), is nonetheless able to achieve the same performance (68% correct, <i>t</i> = 0.146, <i>P</i> = 0.88). Likewise, BERTNET achieves a generalization performance that lags only 2% behind GPTNETXL in the mean (<i>t</i> = −1.122, <i>P</i> = 0.262). By contrast, models with knowledge of sentence-level semantics show marked improvements in generalization, with SBERTNET performing an unseen task at 79% correct on average. Finally, our best-performing model, SBERTNET (L), can execute a never-before-seen task with a performance of 83% correct, on average, lagging just a few percentage points behind STRUCTURENET (88% correct), which receives the structure of the task set hand-coded in its rule vectors.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig2">2b</a> shows a histogram of the number of tasks for which each model achieves a given level of performance. Again, SBERTNET (L) manages to perform over 20 tasks set nearly perfectly in the zero-shot setting (for individual task performance for all models across tasks, see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">3</a>).</p><p>To validate that our best-performing models leveraged the semantics of instructions, we presented the sensory input for one held-out task while providing the linguistic instructions for a different held-out task. Models that truly rely on linguistic information should be most penalized by this manipulation and, as predicted, we saw the largest decrease in performance for our best models (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig2">2c</a>).</p><p>We also tested a more stringent hold-out procedure where we purposefully chose 4–6 tasks from the same family of tasks to hold out during training (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig2">2d</a>). Overall, performance decreased in this more difficult setting, although our best-performing models still showed strong generalization, with SBERTNET (L) and SBERTNET achieving 71% and 72% correct on novel tasks, respectively, which was not significantly different from STRUCTURENET at 72% (<i>t</i> = 0.629, <i>P</i> = 0.529; <i>t</i> = 0.064, <i>P</i> = 0.948; for SBERTNET (L) and SBERTNET, respectively).</p><p>In addition, we tested models in a setting where we allow the weights of language models to tune according to the loss experienced during sensorimotor training (see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a> for tuning details). This manipulation improved the generalization performance across all models, and for our best-performing model, SBERTNET (L), we see that generalization is as strong as for STRUCTURENET (86%, <i>t</i> = 1.204, <i>P</i> = 0.229).</p><p>Following ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e1086">18</a></sup>, we tested models in a setting where task-type information for a given task was represented as a composition of information for related tasks in the training set (that is, AntiDMMod1 = (rule(AntiDMMod2) − rule(DMMod2)) + rule(DMMod1)). In this setting, we did find that the performance of SIMPLENET improved (60% correct). However, when we combined embedded instructions according to the same compositional rules, our linguistic models dramatically outperformed SIMPLENET. This suggests that training in the context of language more readily allows a simple compositional scheme to successfully configure task responses (see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">5</a> for full results and compositional encodings).</p><p>Finally, we tested a version of each model where outputs of language models are passed through a set of nonlinear layers, as opposed to the linear mapping used in the preceding results. We found that this manipulation reduced performance, suggesting that this added power leads to overfitting on training tasks, and that a simpler linear mapping is better suited to generalization (see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a> for details and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">4</a> for full results).</p><p>The discrepancy in performance between our instructed models suggests that in order to represent linguistic information such that it can successfully configure sensorimotor networks, it is not sufficient to simply use any very powerful language processing system. Rather, model success can be delineated by the extent to which they are exposed to sentence-level semantics during pretraining. Our best-performing models SBERTNET (L) and SBERTNET are explicitly trained to produce good sentence embeddings, whereas our worst-performing model, GPTNET, is only tuned to the statistics of upcoming words. Both CLIPNET (S) and BERTNET are exposed to some form of sentence-level knowledge. CLIPNET (S) is interested in sentence-level representations, but trains these representations using the statistics of corresponding vision representations. BERTNET performs a two-way classification of whether or not input sentences are adjacent in the training corpus. That the 1.5 billion parameters of GPTNET (XL) doesn’t markedly improve performance relative to these comparatively small models speaks to the fact that model size isn’t the determining factor. Lastly, although BoW removes key elements of linguistic meaning (that is, syntax), the simple use of word occurrences encodes information primarily about the similarities and differences between the sentences. For instance, simply representing the inclusion or exclusion of the words ‘stronger’ or ‘weaker’ is highly informative about the meaning of the instruction.</p><p>We also investigated which features of language make it difficult for our models to generalize. Thirty of our tasks require processing instructions with a conditional clause structure (for example, COMP1) as opposed to a simple imperative (for example, AntiDM). Tasks that are instructed using conditional clauses also require a simple form of deductive reasoning (if <i>p</i> then <i>q</i> else <i>s</i>). Neuroimaging literature exploring the relationship between such deductive processes and language areas has reached differing conclusions, with some early studies showing that deduction recruits regions that are thought to support syntactic computations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Goel, V., Gold, B., Kapur, S. &amp; Houle, S. Neuroanatomical correlates of human reasoning. J. Cogn. Neurosci. 10, 293–302 (1998)." href="#ref-CR24" id="ref-link-section-d167182994e1117">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Goel, V., Buchel, C., Frith, C. &amp; Dolan, R. J. Dissociation of mechanisms underlying syllogistic reasoning. Neuroimage 12, 504–514 (2000)." href="#ref-CR25" id="ref-link-section-d167182994e1117_1">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Reverberi, C. et al. Neural basis of generation of conclusions in elementary deduction. Neuroimage 38, 752–762 (2007)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR26" id="ref-link-section-d167182994e1120">26</a></sup> and follow-up studies claiming that deduction can be reliably dissociated from language areas<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Noveck, I. A., Goel, V. &amp; Smith, K. W. The neural basis of conditional reasoning with arbitrary content. Cortex 40, 613–622 (2004)." href="#ref-CR27" id="ref-link-section-d167182994e1124">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Monti, M. M., Osherson, D. N., Martinez, M. J. &amp; Parsons, L. M. Functional neuroanatomy of deductive inference: a language-independent distributed network. Neuroimage 37, 1005–1016 (2007)." href="#ref-CR28" id="ref-link-section-d167182994e1124_1">28</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Monti, M. M., Parsons, L. M. &amp; Osherson, D. N. The boundaries of language and thought in deductive inference. Proc. Natl Acad. Sci. USA 106, 12554–12559 (2009)." href="#ref-CR29" id="ref-link-section-d167182994e1124_2">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Coetzee, J. P. &amp; Monti, M. M. At the core of reasoning: dissociating deductive and non-deductive load. Hum. Brain Mapp. 39, 1850–1861 (2018)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR30" id="ref-link-section-d167182994e1127">30</a></sup>. One theory for this variation in results is that baseline tasks used to isolate deductive reasoning in earlier studies used linguistic stimuli that required only superficial processing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Monti, M. M. &amp; Osherson, D. N. Logic, language and the brain. Brain Res. 1428, 33–42 (2012)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR31" id="ref-link-section-d167182994e1132">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Prado, J. The relationship between deductive reasoning and the syntax of language in broca’s area: a review of the neuroimaging literature. L’année Psychol. 118, 289–315 (2018)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR32" id="ref-link-section-d167182994e1135">32</a></sup>.</p><p>To explore this issue, we calculated the average difference in performance between tasks with and without conditional clauses/deductive reasoning requirements (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig2">2f</a>). All our models performed worse on these tasks relative to a set of random shuffles. However, we also saw an additional effect between STRUCTURENET and our instructed models, which performed worse than STRUCTURENET by a statistically significant margin (see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">6</a> for full comparisons). This is a crucial comparison because STRUCTURENET performs deductive tasks without relying on language. Hence, the decrease in performance between STRUCTURENET and instructed models is in part due to the difficulty inherent in parsing syntactically more complicated language. The implication is that we may see engagement of linguistic areas in deductive reasoning tasks, but this may simply be due to the increased syntactic demands of corresponding instructions (rather than processes that recruit linguistic areas to explicitly aid in the deduction). This result largely agrees with two reviews of the deductive reasoning literature, which concluded that the effects in language areas seen in early studies were likely due to the syntactic complexity of test stimuli<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Monti, M. M. &amp; Osherson, D. N. Logic, language and the brain. Brain Res. 1428, 33–42 (2012)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR31" id="ref-link-section-d167182994e1149">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Prado, J. The relationship between deductive reasoning and the syntax of language in broca’s area: a review of the neuroimaging literature. L’année Psychol. 118, 289–315 (2018)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR32" id="ref-link-section-d167182994e1152">32</a></sup>.</p><h3 id="Sec5">Shared structure in language and sensorimotor networks</h3><p>We then turned to an investigation of the representational scheme that supports generalization. First, we note that like in other multitasking models, units in our sensorimotor-RNNs exhibited functional clustering, where similar subsets of neurons show high variance across similar sets of tasks (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">7</a>). Moreover, we found that models can learn unseen tasks by only training sensorimotor-RNN input weights and keeping the recurrent dynamics constant (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">8</a>). Past work has shown that these properties are characteristic of networks that can reuse the same set of underlying neural resources across different settings<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Driscoll, L., Shenoy, K. &amp; Sussillo, D. Flexible multitask computation in recurrent networks utilizes shared dynamical motifs. Preprint at bioRxiv 
                https://doi.org/10.1101/2022.08.15.503870
                
               (2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR6" id="ref-link-section-d167182994e1170">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e1173">18</a></sup>. We then examined the geometry that exists between the neural representations of related tasks. We plotted the first three principal components (PCs) of sensorimotor-RNN hidden activity at stimulus onset in SIMPLENET, GPTNETXL, SBERTNET (L) and STRUCTURENET performing modality-specific DM and AntiDM tasks. Here, models receive input for a decision-making task in both modalities but must only attend to the stimuli in the modality relevant for the current task. Importantly, AntiDMMod1 is held out of training in the following examples. In addition, we plotted the PCs of either the rule vectors or the instruction embeddings in each task (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig3">3</a>).</p><div data-test="figure" data-container-section="figure" id="figure-3" data-title="Structured representations in instructed models."><figure><figcaption><b id="Fig3" data-test="figure-caption-text">Fig. 3: Structured representations in instructed models.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="602"></picture></a></div><p><b>a</b>–<b>d</b>, The first three PCs of sensorimotor hidden activity and task-info representations for models trained with AntiDMMod1 held out. Solid arrows represent an abstract ‘Pro’ versus ‘Anti’ axis, and dashed arrows represent an abstract ‘Mod1’ versus ‘Mod2’ axis. <b>a</b>, STRUCTURENET. <b>b</b>, SBERTNET (L). <b>c</b>, GPTNET (XL). <b>d</b>, SIMPLENET. <b>e</b>, Correlation between held-out task CCGP and zero-shot performance (Pearson’s <i>r</i> = 0.606, <i>P</i> = 1.57 × 10<sup>−46</sup>). <b>f</b>, CCGP scores for held-out tasks for each layer in the model hierarchy. Significance scores indicate <i>P-</i>value results from pairwise two-sided unequal-variance <i>t</i>-tests performed among model distributions of CCGP scores on held-out tasks for sensorimotor-RNN (NS <i>P</i> &gt; 0.05, *<i>P</i> &lt; 0.05, **<i>P</i> &lt; 0.01, ***<i>P</i> &lt; 0.001; STRUCTURENET versus SBERTNET (L): <i>t</i> = 13.67, <i>P</i> = 2.44 × 10<sup>−36</sup>; SBERTNET (L) versus SBERTNET: <i>t</i> = 5.061, <i>P</i> = 5.84 × 10<sup>−7</sup>; SBERTNET versus CLIPNET: <i>t</i> = 2.809, <i>P</i> = 0.005; CLIPNET versus BERTNET: <i>t</i> = 0.278, <i>P</i> = 0.780; BERTNET versus GPTNET (XL): <i>t</i> = 2.505, <i>P</i> = 0.012; GPTNET (XL) versus GPTNET: <i>t</i> = 3.180, <i>P</i> = 0.001; GPTNET versus BOWNET: <i>t</i> = −4.176, <i>P</i> = 3.50 × 10<sup>−5</sup>; BOWNET versus SIMPLENET: <i>t</i> = 23.0.8, <i>P</i> = 1.10<sup>−80</sup>; see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">9</a> for full comparisons as well as <i>t</i>-test results for embedding layer CCGP scores).</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>For STRUCTURENET, hidden activity is factorized along task-relevant axes, namely a consistent ‘Pro’ versus ‘Anti’ direction in activity space (solid arrows), and a ‘Mod1’ versus ‘Mod2’ direction (dashed arrows). Importantly, this structure is maintained even for AntiDMMod1, which has been held out of training, allowing STRUCTURENET to achieve a performance of 92% correct on this unseen task. This factorization is also reflected in the PCs of rule embeddings. Strikingly, SBERTNET (L) also organizes its representations in a way that captures the essential compositional nature of the task set using only the structure that it has inferred from the semantics of instructions. This is the case for language embeddings, which maintain abstract axes across AntiDMMod1 instructions (again, held out of training). As a result, SBERTNET (L) is able to use these relevant axes for AntiDMMod1 sensorimotor-RNN representations, leading to a generalization performance of 82%. By contrast, GPTNET (XL) fails to properly infer a distinct ‘Pro’ versus ‘Anti’ axes in either sensorimotor-RNN representations or language embeddings leading to a zero-shot performance of 6% on AntiDMMod1 (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig3">3b</a>). Finally, we find that the orthogonal rule vectors used by simpleNet preclude any structure between practiced and held-out tasks, resulting in a performance of 22%.</p><p>To more precisely quantify this structure, we measure the cross-conditional generalization performance (CCGP) of these representations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Bernardi, S. et al. The geometry of abstraction in the hippocampus and prefrontal cortex. Cell 183, 954–967 (2020)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR3" id="ref-link-section-d167182994e1326">3</a></sup>. CCGP measures the ability of a linear decoder trained to differentiate one set of conditions (that is, DMMod2 and AntiDMMod2) to generalize to an analogous set of test conditions (that is, DMMod1 and AntiDMMod1). Intuitively, this captures the extent to which models have learned to place sensorimotor activity along abstract task axes (that is, the ‘Anti’ dimension). Notably, high CCGP scores and related measures have been observed in experiments that required human participants to flexibly switch between different interrelated tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Minxha, J., Adolphs, R., Fusi, S., Mamelak, A. N. &amp; Rutishauser, U. Flexible recruitment of memory-based choice representations by the human medial frontal cortex. Science 368, eaba3313 (2020)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR4" id="ref-link-section-d167182994e1330">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Ito, T., Yang, G. R., Laurent, P., Schultz, D. H. &amp; Cole, M. W. Constructing neural network models from brain data reveals representational transformations linked to adaptive behavior. Nat. Commun. 13, 673 (2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR33" id="ref-link-section-d167182994e1333">33</a></sup>.</p><p>We measured CCGP scores among representations in sensorimotor-RNNs for tasks that have been held out of training (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a>) and found a strong correlation between CCGP scores and zero-shot performance (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig3">3e</a>). Additionally, we find that swapping task instructions for held-out tasks dramatically reduces CCGP scores for all our instructed models, indicating that the semantic of instructions is crucial for maintaining structured representations (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">9</a>).</p><p>We then looked at how structure emerges in the language processing hierarchy. CCGP decoding scores for different layers in our model are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig3">3f</a>. For each instructed model, scores for 12 transformer layers (or the last 12 layers for SBERTNET (L) and GPTNET (XL)), the 64-dimensional embedding layer and the Sensorimotor-RNN task representations are plotted. We also plotted CCGP scores for the rule embeddings used in our nonlinguistic models. Among models, there was a notable discrepancy in how abstract structure emerges. Autoregressive models (GPTNETXL, GPTNET), BERTNET and CLIPNET (S), showed a low CCGP throughout language model layers followed by a jump in the embedding layer. This is because weights feeding into the embedding layer are tuned during sensorimotor training. The implication of this spike is that most of the useful representational processing in these models actually does not occur in the pretrained language model per se, but rather in the linear readout, which is exposed to task structure via training. By contrast, our best-performing models SBERTNET and SBERTNET (L) use language representations where high CCGP scores emerge gradually in the intermediate layers of their respective language models. Because semantic representations already have such a structure, most of the compositional inference involved in generalization can occur in the comparatively powerful language processing hierarchy. As a result, representations are already well organized in the last layer of language models, and a linear readout in the embedding layer is sufficient for the sensorimotor-RNN to correctly infer the geometry of the task set and generalize well.</p><p>This analysis strongly suggests that models exhibiting generalization do so by leveraging structured semantic representations to properly relate practiced and novel tasks in sensorimotor space, thereby allowing a composition of practiced behaviors in an unseen setting.</p><h3 id="Sec6">Semantic modulation of single-unit tuning properties</h3><p>Next, we examined tuning profiles of individual units in our sensorimotor-RNNs. We found that individual neurons are tuned to a variety of task-relevant variables. Critically, however, we find neurons where this tuning varies predictably within a task group and is modulated by the semantic content of instructions in a way that reflects task demands.</p><p>For instance, in the ‘Go’ family of tasks, unit 42 shows direction selectivity that shifts by <i>π</i> between ‘Pro’ and ‘Anti’ tasks, reflecting the relationship of task demands in each context (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig4">4a</a>). This flip in selectivity is observed even for the AntiGo task, which was held out during training.</p><div data-test="figure" data-container-section="figure" id="figure-4" data-title="Semantic modulation of single-unit tuning properties."><figure><figcaption><b id="Fig4" data-test="figure-caption-text">Fig. 4: Semantic modulation of single-unit tuning properties.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig4_HTML.png?as=webp"><img aria-describedby="Fig4" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="564"></picture></a></div><p><b>a</b>, Tuning curves for a SBERTNET (L) sensorimotor-RNN unit that modulates tuning according to task demands in the ‘Go’ family. <b>b</b>, Tuning curves, for a SBERTNET (L) sensorimotor-RNN unit in the ‘matching’ family of tasks plotted in terms of difference in angle between two stimuli. <b>c</b>, Full activity traces for modality-specific ‘DM’ and ‘AntiDM’ tasks for different levels of relative stimulus strength. <b>d</b>, Full activity traces for tasks in the ‘comparison’ family of tasks for different levels of relative stimulus strength.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>For the ‘Matching’ family of tasks, unit 14 modulates activity between ‘match’ (DMS, DMC) and ‘non-match’ (DNMS, DNMC) conditions. In ‘non-match’ trials, the activity of this unit increases as the distance between the two stimuli increases. By contrast, for ‘matching’ tasks, this neuron is most active when the relative distance between the two stimuli is small. Hence, in both cases this neuron modulates its activity to represent when the model should respond, changing selectivity to reflect opposing task demands between ‘match’ and ‘non-match’ trials. This is true even for DMS, which has been held out of training.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig4">4c</a> shows traces of unit 3 activity in modality-specific versions of DM and AntiDM tasks (AntiDMMod1 is held out of training) for different levels of contrast (contrast = <i>s</i><i>t</i><i>r</i><sub>stim1</sub> − <i>s</i><i>t</i><i>r</i><sub>stim2</sub>). In all tasks, we observed ramping activity where the rate of ramping is relative to the strength of contrast. This motif of activity has been reported in previous studies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Shadlen, M. N. &amp; Newsome, W. T. Neural basis of a perceptual decision in the parietal cortex (area lip) of the rhesus monkey. J. Neurophysiol. 86, 1916–1936 (2001)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR34" id="ref-link-section-d167182994e1433">34</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Huk, A. C. &amp; Shadlen, M. N. Neural activity in macaque parietal cortex reflects temporal integration of visual motion signals during perceptual decision making. J. Neurosci. 25, 10420–10436 (2005)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR35" id="ref-link-section-d167182994e1436">35</a></sup>. However, in our models, we observe that an evidence-accumulating neuron can swap the sign of its integration in response to a change in linguistic instructions, which allows models to meet opposing demands of ‘Pro’ and ‘Anti’ versions of the task, even for previously unseen tasks.</p><p>Interestingly, we also found that unsuccessful models failed to properly modulate tuning preferences. For example, with GPTNET (XL), which failed to factorize along a ‘Pro’ versus ‘Anti’ axis (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig3">3b</a>) and had poor generalization on AntiDMMod1, we also find neurons that failed to swap their sign of integration in the held-out setting (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">10</a>).</p><p>Finally, we see a similar pattern in the time course of activity for trials in the ‘Comparison’ family of tasks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig4">4d</a>). In the COMP1 task, the network must respond in the direction of the first stimulus if it has higher intensity than the second stimulus, and must not respond otherwise. In COMP2, it must only respond to the second stimulus if the second stimulus is higher intensity. For ‘Anti’ versions, the demands of stimulus ordering are the same except the model has to choose the stimuli with the weakest contrast. Even with this added complexity, we found individual neurons that modulate their tuning with respect to task demands, even for held-out tasks (in this case COMP2). For example, unit 82 is active when the network should repress response. For ‘COMP1’, this unit is highly active with negative contrast (that is, <i>s</i><i>t</i><i>r</i><sub>stim2</sub> &gt; <i>s</i><i>t</i><i>r</i><sub>stim1</sub>), but flips this sensitivity in COMP2 and is highly active with positive contrast (that is, <i>s</i><i>t</i><i>r</i><sub>stim1</sub> &gt; <i>s</i><i>t</i><i>r</i><sub>stim2</sub>). Importantly, this relation is reversed when the goal is to select the weakest stimuli. Hence, despite these subtle syntactic differences in instruction sets, the language embedding can reverse the tuning of this unit in a task-appropriate manner.</p><h3 id="Sec7">Linguistic communication between networks</h3><p>We now seek to model the complementary human ability to describe a particular sensorimotor skill with words once it has been acquired. To do this, we inverted the language-to-sensorimotor mapping our models learn during training so that they can provide a linguistic description of a task based only on the state of sensorimotor units. First, we constructed an output channel (production-RNN; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5a–c</a>), which is trained to map sensorimotor-RNN states to input instructions. We then present the network with a series of example trials while withholding instructions for a specific task. During this phase all model weights are frozen, and models receive motor feedback in order to update the embedding layer activity in order to reduce the error of the output (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5b</a>). Once the activity in the embedding layer drives sensorimotor units to achieve a performance criterion, we used the production-RNN to decode a linguistic description of the current task. Finally, to evaluate the quality of these instructions, we input them into a partner model and measure performance across tasks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5c</a>). All instructing and partner models used in this section are instances of SBERTNET (L) (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a>).</p><div data-test="figure" data-container-section="figure" id="figure-5" data-title="Communication between networks."><figure><figcaption><b id="Fig5" data-test="figure-caption-text">Fig. 5: Communication between networks.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig5_HTML.png?as=webp"><img aria-describedby="Fig5" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="735"></picture></a></div><p><b>a</b>, Illustration of self-supervised training procedure for the language production network (blue). The red dashed line indicates gradient flow. <b>b</b>, Illustration of motor feedback used to drive task performance in the absence of linguistic instructions. <b>c</b>, Illustration of the partner model evaluation procedure used to evaluate the quality of instructions generated from the instructing model. <b>d</b>, Three example instructions produced from sensorimotor activity evoked by embeddings inferred in <b>b</b> for an AntiDMMod1 task. <b>e</b>, Confusion matrix of instructions produced again using the method described in <b>b</b>. <i>y</i> axis indicates input–output task used to infer an embedding, and <i>x</i> axis indicates whether the instruction produced from the resulting sensorimotor activity was included in one of the instruction sets used during self-supervised training or else was a ‘novel’ formulation. <b>f</b>, Performance of partner models in different training regimes given produced instructions or direct input of embedding vectors. Each point represents the average performance of a partner model across tasks using instructions from decoders train with different random initializations. Dots indicate the partner model was trained on all tasks, whereas diamonds indicate performance on held-out tasks. Axes indicate the training regime of the instructing model. Full statistical comparisons of performance can be found in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">12</a>.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>Some example decoded instructions for the AntiDMMod1 task (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5d</a>; see Supplementary Notes <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">4</a> for all decoded instructions). To visualize decoded instructions across the task set, we plotted a confusion matrix where both sensorimotor-RNN and production-RNN are trained on all tasks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5e</a>). Note that many decoded instructions were entirely ‘novel’, that is, they were not included in the training set for the production-RNN (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a>). Novel instructions made up 53% of decoded instructions across all tasks.</p><p>To test the quality of these novel instructions, we evaluated a partner model’s performance on instructions generated by the first network (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5c</a>; results are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5f</a>). When the partner model is trained on all tasks, performance on all decoded instructions was 93% on average across tasks. Communicating instructions to partner models with tasks held out of training also resulted in good performance (78%). Importantly, performance was maintained even for ‘novel’ instructions, where average performance was 88% for partner models trained on all tasks and 75% for partner models with hold-out tasks. Given that the instructing and partner models share the same architecture, one might expect that it is more efficient to forgo the language component of communication and simply copy the embedding inferred by one model into the input of the partner model. This resulted in only 31% correct performance on average and 28% performance when testing partner models on held-out tasks. Although both instructing and partner networks share the same architecture and the same competencies, they nonetheless have different synaptic weights. Hence, using a neural representation tuned for the set of weights within the one agent won’t necessarily produce good performance in the other.</p><p>We also tested an instructing model using a sensorimotor-RNN with tasks held out of training. We emphasize here that during training the production-RNN attempts to decode from sensorimotor hidden states induced by instructions for tasks the network has never experienced before (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5a</a>), whereas during test time, instructions are produced from sensorimotor states that emerge entirely as a result of minimizing a motor error (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5b,c</a>). We nonetheless find that, in this setting, a partner model trained on all tasks performs at 82% correct, while partner models with tasks held out of training perform at 73%. Here, 77% of produced instructions are novel, so we see a very small decrease of 1% when we test the same partner models only on novel instructions. Like above, context representations induce a relatively low performance of 30% and 37% correct for partners trained on all tasks and with tasks held out, respectively.</p><p>Lastly, we tested our most extreme setting where tasks have been held out for both sensorimotor-RNNs and production-RNNs (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5f</a>). We find that produced instructions induce a performance of 71% and 63% for partner models trained on all tasks and with tasks held out, respectively. Although this is a decrease in performance from our previous set-ups, the fact that models can produce sensible instructions at all in this double held-out setting is striking. The fact that the system succeeds to any extent speaks to strong inductive biases introduced by training in the context of rich, compositionally structured semantic representations.</p></div></div><div id="Sec8-section" data-title="Discussion"><h2 id="Sec8">Discussion</h2><div id="Sec8-content"><p>In this study, we use the latest advances in natural language processing to build tractable models of the ability to interpret instructions to guide actions in novel settings and the ability to produce a description of a task once it has been learned. RNNs can learn to perform a set of psychophysical tasks simultaneously using a pretrained language transformer to embed a natural language instruction for the current task. Our best-performing models can leverage these embeddings to perform a brand-new model with an average performance of 83% correct. Instructed models that generalize performance do so by leveraging the shared compositional structure of instruction embeddings and task representations, such that an inference about the relations between practiced and novel instructions leads to a good inference about what sensorimotor transformation is required for the unseen task. Finally, we show a network can invert this information and provide a linguistic description for a task based only on the sensorimotor contingency it observes.</p><p>Our models make several predictions for what neural representations to expect in brain areas that integrate linguistic information in order to exert control over sensorimotor areas. Firstly, the CCGP analysis of our model hierarchy suggests that when humans must generalize across (or switch between) a set of related tasks based on instructions, the neural geometry observed among sensorimotor mappings should also be present in semantic representations of instructions. This prediction is well grounded in the existing experimental literature where multiple studies have observed the type of abstract structure we find in our sensorimotor-RNNs also exists in sensorimotor areas of biological brains<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Bernardi, S. et al. The geometry of abstraction in the hippocampus and prefrontal cortex. Cell 183, 954–967 (2020)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR3" id="ref-link-section-d167182994e1615">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Panichello, M. F. &amp; Buschman, T. J. Shared mechanisms underlie the control of working memory and attention. Nature 592, 601–605 (2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR36" id="ref-link-section-d167182994e1618">36</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Nieh, E. H. et al. Geometry of abstract learned knowledge in the hippocampus. Nature 595, 80–84 (2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR37" id="ref-link-section-d167182994e1621">37</a></sup>. Our models theorize that the emergence of an equivalent task-related structure in language areas is essential to instructed action in humans. One intriguing candidate for an area that may support such representations is the language selective subregion of the left inferior frontal gyrus. This area is sensitive to both lexico-semantic and syntactic aspects of sentence comprehension, is implicated in tasks that require semantic control and lies anatomically adjacent to another functional subregion of the left inferior frontal gyrus, which is implicated in flexible cognition<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Fedorenko, E. &amp; Blank, I. A. Broca’s area is not a natural kind. Trends Cogn. Sci. 24, 270–284 (2020)." href="#ref-CR38" id="ref-link-section-d167182994e1625">38</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Fedorenko, E., Duncan, J. &amp; Kanwisher, N. Language-selective and domain-general regions lie side by side within broca’s area. Curr. Biol. 22, 2059–2062 (2012)." href="#ref-CR39" id="ref-link-section-d167182994e1625_1">39</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gao, Z. et al. Distinct and common neural coding of semantic and non-semantic control demands. NeuroImage 236, 118230 (2021)." href="#ref-CR40" id="ref-link-section-d167182994e1625_2">40</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Duncan, J. The multiple-demand (MD) system of the primate brain: mental programs for intelligent behaviour. Trends Cogn. Sci. 14, 172–179 (2010)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR41" id="ref-link-section-d167182994e1628">41</a></sup>. We also predict that individual units involved in implementing sensorimotor mappings should modulate their tuning properties on a trial-by-trial basis according to the semantics of the input instructions, and that failure to modulate tuning in the expected way should lead to poor generalization. This prediction may be especially useful to interpret multiunit recordings in humans. Finally, given that grounding linguistic knowledge in the sensorimotor demands of the task set improved performance across models (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig2">2e</a>), we predict that during learning the highest level of the language processing hierarchy should likewise be shaped by the embodied processes that accompany linguistic inputs, for example, motor planning or affordance evaluation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Buccino, G., Colagé, I., Gobbi, N. &amp; Bonaccorso, G. Grounding meaning in experience: a broad perspective on embodied language. Neurosci. Biobehav. Rev. 69, 69–78 (2016)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR42" id="ref-link-section-d167182994e1635">42</a></sup>.</p><p>One notable negative result of our study is the relatively poor generalization performance of GPTNET (XL), which used at least an order of magnitude more parameters than other models. This is particularly striking given that activity in these models is predictive of many behavioral and neural signatures of human language processing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Schrimpf, M. et al. The neural architecture of language: integrative modeling converges on predictive processing. Proc. Natl Acad. Sci. USA 
                https://doi.org/10.1073/pnas.2105646118
                
               (2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR10" id="ref-link-section-d167182994e1642">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Goldstein, A. et al. Shared computational principles for language processing in humans and deep language models. Nature Neurosci. 25, 369–380 (2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR11" id="ref-link-section-d167182994e1645">11</a></sup>. Given this, future imaging studies may be guided by the representations in both autoregressive models and our best-performing models to delineate a full gradient of brain areas involved in each stage of instruction following, from low-level next-word prediction to higher-level structured-sentence representations to the sensorimotor control that language informs.</p><p>Our models may guide future work comparing compositional representations in nonlinguistic subjects like nonhuman primates. Comparison of task switching (without linguistic instructions) between humans and nonhuman primates indicates that both use abstract rule representations, although humans can make switches much more rapidly<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Mansouri, F. A., Freedman, D. J. &amp; Buckley, M. J. Emergence of abstract rules in the primate brain. Nat. Rev. Neurosci. 21, 595–610 (2020)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR43" id="ref-link-section-d167182994e1652">43</a></sup>. One intriguing parallel in our analyses is the use of compositional rules vectors (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">5</a>). Even in the case of nonlinguistic SIMPLENET, using these vectors boosted generalization. Importantly, however, this compositionality is much stronger for our best-performing instructed models. This suggests that language endows agents with a more flexible organization of task subcomponents, which can be recombined in a broader variety of contexts.</p><p>Our results also highlight the advantages of linguistic communication. Networks can compress the information they have gained through experience of motor feedback and transfer that knowledge to a partner network via natural language. Although rudimentary in our example, the ability to endogenously produce a description of how to accomplish a task after a period of practice is a hallmark human language skill. The failure to transfer performance by sharing latent representations demonstrates that to communicate information in a group of independent networks of neurons, it needs to pass through a representational medium that is equally interpretable by all members of the group. In humans and for our best-performing instructed models, this medium is language.</p><p>A series of works in reinforcement learning has investigated using language and language-like schemes to aid agent performance. Agents receive language information through step-by-step descriptions of action sequences<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Oh, J. Singh, S., Lee, H. &amp; Kohli, P. Zero-shot task generalization with multi-task deep reinforcement learning. In Proc. 34th International Conference on Machine Learning 2661–2670 (JMLR.org, 2017)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR44" id="ref-link-section-d167182994e1666">44</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Chaplot, D. S., Mysore Sathyendra, K., Pasumarthi, R. K., Rajagopal, D., &amp; Salakhutdinov, R. Gated-attention architectures for task-oriented language grounding. In Proc. 32nd AAAI Conference on Artificial Intelligence Vol. 32 (AAAI Press, 2018)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR45" id="ref-link-section-d167182994e1669">45</a></sup>, or by learning policies conditioned on a language goal<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Sharma, P., Torralba, A. &amp; Andreas, J. Skill induction and planning with latent language. Preprint at 
                https://arxiv.org/abs/2110.01517
                
              (2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR46" id="ref-link-section-d167182994e1673">46</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Jiang, Y., Gu, S., Murphy, K. &amp; Finn, C. Language as an abstraction for hierarchical deep reinforcement learning. In Proc. 33rd International Conference on Neural Information Processing Systems 9419–943132 (Curran Associates Inc., 2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR47" id="ref-link-section-d167182994e1676">47</a></sup>. These studies often deviate from natural language and receive linguistic inputs that are parsed or simply refer directly to environmental objects. Some larger versions of the pretrained language models we use to embed instructions also display instructions following behavior, that is, GPT-3 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Brown, Tom, et al. Language models are few-shot learners. In Proc. 34th International Conference on Neural Information Processing Systems 1877–1901 (Curran Associates Inc., 2020)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR7" id="ref-link-section-d167182994e1680">7</a></sup>), PALM<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Chowdhery, A. et al. Palm: scaling language modeling with pathways. J. Mach. Learn. Res. 24, 11324–11436 (2023)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR12" id="ref-link-section-d167182994e1684">12</a></sup>, LaMDA<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Thoppilan, R. et al. Lamda: language models for dialog applications. Preprint at 
                https://arxiv.org/abs/2201.08239
                
               (2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR13" id="ref-link-section-d167182994e1688">13</a></sup> and InstructGPT<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Ouyang, L. et al. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 27730–27744 (Curran Associates, Inc., 2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR48" id="ref-link-section-d167182994e1693">48</a></sup> in the modality of language and DALL-E<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Ramesh, A. et al. Zero-shot text-to-image generation. In Proc. 38th International Conference on Machine Learning (eds Marina, M. &amp; Tong, Z.) 8821–8831 (PMLR, 2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR8" id="ref-link-section-d167182994e1697">8</a></sup> and Stable Diffusion<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Rombach, R. et al. High-resolution image synthesis with latent diffusion models. In Proc. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 10674–10685 (IEEE, 2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR14" id="ref-link-section-d167182994e1701">14</a></sup> in a language to image modality. The semantic and syntactic understanding displayed in these models is impressive. However, the outputs of these models are difficult to interpret in terms of guiding the dynamics of a downstream action plan. Finally, recent work has sought to engineer instruction following agents that can function in complex or even real-world environments<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Abramson, J. et al. Imitating interactive intelligence. Preprint at 
                https://arxiv.org/abs/2012.05672
                
               (2021)." href="#ref-CR16" id="ref-link-section-d167182994e1705">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="DeepMind Interactive Agents Team. Creating multimodal interactive agents with imitation and self-supervised learning. Preprint at 
                https://arxiv.org/abs/2112.03763
                
               (2022)." href="#ref-CR17" id="ref-link-section-d167182994e1705_1">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e1708">18</a></sup>. While these models exhibit impressive behavioral repertoires, they rely on perceptual systems that fuse linguistic and visual information making them difficult to compare to language representations in human brains, which emerge from a set of areas specialized for processing language. In all, none of these models offer a testable representational account of how language might be used to induce generalization over sensorimotor mappings in the brain.</p><p>Our models by contrast make tractable predictions for what population and single-unit neural representations are required to support compositional generalization and can guide future experimental work examining the interplay of linguistic and sensorimotor skills in humans. By developing interpretable models that can both understand instructions as guiding a particular sensorimotor response, and communicate the results of sensorimotor learning as an intelligible linguistic instruction, we have begun to explain the power of language in encoding and transferring knowledge in networks of neurons.</p></div></div><div id="Sec9-section" data-title="Methods"><h2 id="Sec9">Methods</h2><div id="Sec9-content"><h3 id="Sec10">Model architecture</h3><h4 id="Sec11">Sensorimotor-RNN</h4><p>The base model architecture and task structure used in this paper follows<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e1731">18</a></sup>. All networks of sensorimotor units denoted sensorimotor-RNN are gated recurrent units (GRU)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Chung, J., Gulcehre, C., Cho, K. &amp; Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. Preprint at 
                https://arxiv.org/abs/1412.3555
                
               (2014)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR49" id="ref-link-section-d167182994e1735">49</a></sup> using rectified linear unit (ReLU) nonlinearities with 256 hidden units each. Inputs to the networks consist of (1) sensory inputs, <i>X</i><sub><i>t</i></sub> and (2) task-identifying information, <i>I</i><sub><i>t</i></sub>. We initialize hidden activity in the GRU as <span>\({h}^{0}\in {{\mathbb{R}}}^{256}\)</span> with values set to 0.1. All networks of sensorimotor units use the same hidden state initialization, so we omit <i>h</i><sup>0</sup> in network equations. At each time step, a readout layer Linear<sub>out</sub> decodes motor activity, <span>\(\hat{{y}_{t}}\)</span>, from the activity of recurrent hidden units, <i>h</i><sub><i>t</i></sub>, according to:</p><div id="Equa"><p><span>$$\begin{array}{ll}{h}_{t}={{{\rm{SensorimotorRNN}}}}\Big({X}_{t},{I}_{t};{h}_{t-1}\Big)\qquad\qquad{h}_{t}\in {{\mathbb{R}}}^{256}\\ {\hat{y}}_{t}=\sigma \Big({{{{\rm{Linear}}}}}_{{{{\rm{out}}}}}({h}_{t})\Big)\qquad\qquad\qquad\qquad\qquad\quad{\hat{y}}_{t}\in {{\mathbb{R}}}^{33}\end{array}$$</span></p></div><p>where <i>σ</i> denotes the sigmoid function. Sensory inputs <i>X</i><sub><i>t</i></sub> are made up of three channels, two sensory modalities <span>\({x}_{{{\mathrm{mod}}}\,1,t}\)</span> and <span>\({x}_{{{\mathrm{mod}}}\,2,t}\)</span>, and a fixation channel <i>x</i><sub>fix,<i>t</i></sub>. Both <span>\({x}_{{{\mathrm{mod}}}\,1,t},{x}_{{{\mathrm{mod}}}\,2,t}\in {{\mathbb{R}}}^{32}\)</span> and stimuli in these modalities are represented as hills of activity with peaks determined by units’ preferred directions around a one-dimensional circular variable. For an input at direction <i>θ</i>, the activity of a given input unit <i>u</i><sub><i>i</i></sub> with preferred direction <i>θ</i><sub><i>i</i></sub> is</p><div id="Equb"><p><span>$${u}_{i}=str \times 0.8\exp \left[-0.5 \times {\left(\frac{8| \theta -{\theta }_{i}| }{\pi }\right)}^{2}\right]$$</span></p></div><p>where <i>s</i><i>t</i><i>r</i> is the coefficient describing stimulus strength. The fixation channel <span>\({x}_{{{{\rm{fix}}}},t}\in {{\mathbb{R}}}^{1}\)</span> is a single unit simulating a fixation cue for the network. In all, sensory input <span>\({X}_{t}=({x}_{mod1,t},{x}_{mod2,t},{x}_{fix,t})\in {{\mathbb{R}}}^{65}\)</span>. Motor output, <span>\({\hat{{y}}_{t}}\)</span> consists of both a 32-dimensional ring representing directional responses to the input stimulus as well as a single unit representing model fixation, so that <span>\({\hat{{y}}_{t}}\in {{\mathbb{R}}}^{33}\)</span>.</p><p>For all models, task-identifying information <span>\({I}_{t}\in {{\mathbb{R}}}^{64}\)</span>. Task-identifying information is presented throughout the duration of a trial and remains constant such that <span>\({I}_{t}={I}_{t{\prime} }\forall t,t{\prime}\)</span>. For all models, task-identifying info <i>I</i><sub><i>t</i></sub> and sensory input <i>X</i><sub><i>t</i></sub> are concatenated as inputs to the sensorimotor-RNN.</p><h4 id="Sec12">Nonlinguistic models</h4><p>For SIMPLENET, we generate a set of 64-dimensional orthogonal task rules by constructing an orthogonal matrix using the Python package scipy.stats.ortho_group, and assign rows of this matrix to each task type. For STRUCTURENET, we generate a set of ten orthogonal, 64-dimensional vectors in the same manner, and each of these represents a dimension of the task set (that is, respond weakest versus strongest direction, respond in the same versus opposite direction, pay attention only to stimuli in the first modality, and so on). Rule vectors for tasks are then simple combinations of each of these ten basis vectors. For a full description of structure rule vectors, see Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">3</a>.</p><p>We also test SIMPLENETPLUS and STRUCTURENETPLUS, which use an additional hidden layer with 128 units and ReLU nonlinearities to process orthogonal tasks rules <i>I</i><sub><i>t</i></sub> into a vector <span>\(\bar{{I}_{t}}\)</span> which is used by sensorimotor-RNN as task-identifying information.</p><div id="Equc"><p><span>$$\begin{array}{ll}{\bar{{I}_{t}}}^{{\prime} }=\rm{ReLU}({{{{\rm{Linear}}}}}_{{{{\rm{RuleEmb}}}}1}({I}_{t}))&amp;{\bar{{I}_{t}}}^{{\prime} }\in {{\mathbb{R}}}^{128}\\ {\bar{{I}_{t}}}^{{\prime} }=\rm{ReLU}({{{{\rm{Linear}}}}}_{{{{\rm{RuleEmb}}}}2}({I}_{t}^{{\prime} }))&amp;{\bar{{I}_{t}}}^{{\prime} }\in {{\mathbb{R}}}^{128}\\ \bar{{I}_{t}}=\rm{ReLU}({{{{\rm{Linear}}}}}_{{{{\rm{RuleEmb}}}}3}({\bar{{I}_{t}}}^{{\prime} }))&amp;\bar{{I}_{t}}\in {{\mathbb{R}}}^{64}\end{array}$$</span></p></div><p>Full results for these models are included in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">4</a>.</p><h4 id="Sec13">Pretrained transformers</h4><p>The main language models we test use pretrained transformer architectures to produce <i>I</i>. Importantly, transformers differ in the type of pretraining objective used to tune the model parameters. GPT is trained to predict the next word given a context of words<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Radford, A. et al. Language models are unsupervised multitask learners. OpenAI 1, 9 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR9" id="ref-link-section-d167182994e3362">9</a></sup>. GPT (XL) follows the same objective but trains for longer on a larger dataset<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Radford, A. et al. Better language models and their implications. 
                https://openai.com/blog/better-language-models/
                
               (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR50" id="ref-link-section-d167182994e3366">50</a></sup>. Both models are fully autoregressive. BERT, by contrast, takes bidirectional language inputs and is tasked with predicting masked words that appear in the middle of input phrases. Additionally, BERT is trained on a simple sentence prediction task where the model must determine if input sentence 1 is followed by input sentence 2 in the training corpus. Extending this principle, SBERT is explicitly trained to produce fixed-length embeddings of whole sentences<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Reimers, N. &amp; Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. Preprint at 
                https://arxiv.org/abs/1908.10084
                
               (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR21" id="ref-link-section-d167182994e3370">21</a></sup>. It takes pretrained BERT networks and uses them in a siamese architecture<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Bromley, J. et al. Signature verification using a ‘siamese’ time delay neural network. Int. J. Pattern Recognit. Artif. Intell. 7, 669–688 (1993)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR51" id="ref-link-section-d167182994e3374">51</a></sup>, which allows the weights of the model to be tuned in a supervised fashion according to the Stanford Natural Language Inference dataset<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Bowman, S. R., Angeli, G., Potts, C. &amp; Manning, C. D. A large annotated corpus for learning natural language inference. Preprint at 
                http://arxiv.org/abs/1508.05326
                
               (2015)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR22" id="ref-link-section-d167182994e3379">22</a></sup>. Natural language inference is a three-way categorization task where the network must infer the logical relationship between sentences: whether a premise sentence implies, contradicts or is unrelated to a hypothesis sentence. Finally, CLIP is trained to jointly embed images and language<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Radford, A. et al. &quot;Learning transferable visual models from natural language supervision. In Proc. 38th International Conference on Machine Learning (eds Marina, M. &amp; Tong, Z.) 8748–8763 (PMLR, 2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR23" id="ref-link-section-d167182994e3383">23</a></sup>. It uses data from captioned images and is asked to properly categorize which text and images pairs match or are mismatched in the dataset via a contrastive loss.</p><p>Importantly, the natural output of a transformer is a matrix of size <span>\({\dim }_{{{{\rm{trans}}}}.}\times {{{\mathcal{T}}}}\)</span>, the inherent dimensionality of the transformer by the length of the input sequence. To create an embedding space for sentences it is standard practice to apply a pooling method to the transformer output, which produces a fixed-length representation for each instruction.</p><p>For GPT, GPT (XL), BERT and SBERT, we use an average pooling method. Suppose we have an input instruction <span>\({w}_{1}\ldots {w}_{{{{\mathcal{T}}}}}\)</span>. Following standard practice with pretrained language models, the input to our transformers is tokenized with special ‘cls’ and ‘eos’ tokens at the beginning and end of the input sequence. We then compute <i>I</i> as follows:</p><div id="Equd"><p><span>$$\begin{array}{ll}{h}^{\rm{tran.}}={{{\rm{transformer}}}}\Big({{\mbox{[cls]}}}\,,{w}_{1}\ldots {w}_{{{{\mathcal{T}}}}},\,{{\mbox{[eos]}}}\Big),\qquad{h}^{\rm{tran.}}\in {{\mathbb{R}}}^{{\dim }_{{{{\rm{trans}}}}.}\times {{{\mathcal{T}}}}+2}\\ {h}^{I}={{{\rm{mean}}}}({h}^{\rm{tran.}}),\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad{h}^{I}\in {{\mathbb{R}}}^{{\dim }_{{{{{\rm{trans}}}}}.}}\\ I={{{{\rm{Linear}}}}}_{{{{\rm{embed}}}}}({h}^{I})\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad I\in {{\mathbb{R}}}^{64}\end{array}$$</span></p></div><p>We chose this average pooling method primarily because a previous study<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Reimers, N. &amp; Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. Preprint at 
                https://arxiv.org/abs/1908.10084
                
               (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR21" id="ref-link-section-d167182994e3845">21</a></sup> found that this resulted in the highest-performing SBERT embeddings. Another alternative would be to simply use the final hidden representation of the ‘cls’ token as a summary of the information in the entire sequence (given that BERT architectures are bidirectional, this token will have access to the whole sequence).</p><div id="Eque"><p><span>$$\begin{array}{ll}{h}^{\rm{tran.}}={{{\rm{transformer}}}}\Big(\,{{\mbox{[cls]}}}\,,{w}_{1}\ldots {w}_{{{{\mathcal{T}}}}},\,{{\mbox{[eos]}}}\,\Big),\qquad{h}^{\rm{tran.}}\in {{\mathbb{R}}}^{{\dim }_{{{{{\rm{trans}}}}}.}\times {{\,{\mathcal{T}}}}+2}\\ {h}^{I}=({h}_{{{{\rm{cls}}}}}^{\rm{tran.}})\qquad\qquad\qquad\qquad\qquad\qquad\quad\qquad\quad\;\;{h}^{I}\in {{\mathbb{R}}}^{{\dim }_{{{{{\rm{trans}}}}}.}}\end{array}$$</span></p></div><p>Where <span>\({h}_{{{{\rm{cls}}}}}^{\rm{tran.}}\)</span> denote the last hidden representation for the ‘cls’ token. Ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Reimers, N. &amp; Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. Preprint at 
                https://arxiv.org/abs/1908.10084
                
               (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR21" id="ref-link-section-d167182994e4176">21</a></sup> found this pooling method performed worse than average pooling, so we don’t include these alternatives in our results. For GPT and GPT (XL), we also tested a pooling method where the fixed-length representation for a sequence was taken from the transformer output of the ‘eos’ token. In this case:</p><div id="Equf"><p><span>$$\begin{array}{ll}{h}^{\rm{tran.}}={{{\rm{transformer}}}}\Big(\,{{\mbox{[cls]}}}\,,{w}_{1}\ldots {w}_{{{{\mathcal{T}}}}},\,{{\mbox{[eos]}}}\,\Big),\qquad{h}^{\rm{tran.}}\in {{\mathbb{R}}}^{{\dim }_{{{{\rm{trans}}}}.}\times {{\;{\mathcal{T}}}}+2}\\ {h}^{I}=({h}_{{{{\rm{eos}}}}}^{\rm{tran.}}),\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad\quad{h}^{I}\in {{\mathbb{R}}}^{{\dim }_{{{{\rm{trans}}}}.}}\\ I={{{{\rm{Linear}}}}}_{{{{\rm{embed}}}}}({h}^{I}),\qquad\qquad\qquad\qquad\qquad\qquad\quad I\in {{\mathbb{R}}}^{64}\end{array}$$</span></p></div><p>We found that GPT failed to achieve even a relaxed performance criterion of 85% across tasks using this pooling method, and GPT (XL) performed worse than with average pooling, so we omitted these models from the main results (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">11</a>). For CLIP models we use the same pooling method as in the original multiModal training procedure, which takes the outputs of the [cls] token as described above.</p><p>For all the above models, we also tested a version where the information from the pretrained transformers is passed through a multilayer perceptron with a single hidden layer of 256 hidden units and ReLU nonlinearities. We found that this manipulation reduced performance across all models, verifying that a simple linear embedding is beneficial to generalization performance.</p><p>For GPT, BERT and SBERT, <span>\({\dim }_{{{{\rm{trans}}}}.}=768\)</span> and each model uses a total of ~100 million parameters; for SBERT (L) <span>\({\dim }_{{{{\rm{trans}}}}.}=1,024\)</span> and the model uses ~300 million parameters; GPT (XL) <span>\({\dim }_{{{{\rm{trans}}}}.}=1,600\)</span> and the model uses ~1.5 billion parameters; for CLIP, <span>\({\dim }_{{{{\rm{trans}}}}.}=512\)</span> and the model uses ~60 million parameters. Full PyTorch implementations, including all pretrained weights and model hyperparameters, can be accessed at the Huggingface library (<a href="https://huggingface.co/docs/transformers/">https://huggingface.co/docs/transformers/</a>)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Wolf, T. et al. Transformers: state-of-the-art natural language processing. In Proc. 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (eds Liu, Q. &amp; Schlangen, D.) 38–45 (Association for Computational Linguistics, 2020)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR52" id="ref-link-section-d167182994e4731">52</a></sup>.</p><h4 id="Sec14">BoW model</h4><p>For our BoW model, instructions are represented as a vector of binary activations the size of the instruction vocabulary, where each unit indicates the inclusion or exclusion of the associated word in the current instruction. For our instruction set, <span>∣</span>vocab<span>∣</span> = 181. This vector is then projected through a linear layer into 64-dimensional space.</p><div id="Equg"><p><span>$$\begin{array}{ll}{h}_{i}^{{{{\rm{BoW}}}}}=\left\{\begin{array}{ll}1\quad\,{{\mbox{if}}}\,\,{w}_{i}\in ({w}_{1}\ldots {w}_{{{{\mathcal{T}}}}})\\ 0\quad\,{{\mbox{otherwise}}}\,\end{array}\right.\qquad\qquad{h}^{{{{\rm{BoW}}}}}\in {{\mathbb{R}}}^{| \rm{vocab}| }\\ I={{{{\rm{Linear}}}}}_{{{{\rm{embed}}}}}({h}^{{{{\rm{BoW}}}}}),\qquad\qquad\qquad\qquad\qquad\quad I\in {{\mathbb{R}}}^{64}\end{array}$$</span></p></div><h4 id="Sec15">Blank slate language models</h4><p>Given that tuning the last layers of language models resulted in improved performance (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig2">2e</a>), we tested two additional models to determine if training a blank slate language model trained exclusively on the loss from sensorimotor tasks would improve performance. These models consist of passing BoW representations through a multilayer perceptron and passing pretrained BERT word embeddings through one layer of a randomly initialized BERT encoder. Both models performed poorly compared to pretrained models (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">4.5</a>), confirming that language pretraining is essential to generalization.</p><h3 id="Sec16">Tasks sets</h3><p>Tasks were divided into five interrelated subgroups: ‘go’, ‘decision-making’, ‘matching’, and ‘comparison’ and ‘duration’. Depending on the task, multiple stimuli may appear during the stimulus epoch. Also, depending on the task, models may be required to respond in a particular direction or repress response altogether. Unless otherwise specified, zero-mean Gaussian noise is added independently at each time step and to each input unit and the variance of this noise is drawn randomly from <span>\({\mathbb{U}}[0.1,0.15]\)</span>. The timing of stimuli differs among the tasks type. However, for all tasks, trials can be divided into preparatory, stimulus and response epochs. The stimulus epoch can be subdivided into three parts—stim1, delay and stim23—although these distinct parts aren’t used by all tasks. A trial lasts for a total of <i>T</i> = 150 time steps. Let <i>d</i><i>u</i><i>r</i><sub>epoch</sub> denote the duration in simulated time steps of a given epoch. Then</p><div id="Equh"><p><span>$$\begin{array}{rcl}&amp;&amp;du{r}_{{{{\rm{response}}}}} \sim\Big\{i| 20 &lt; i\le 25;i\in {\mathbb{N}}\Big\}\\ &amp;&amp;du{r}_{{{{\rm{stim}}}}1},du{r}_{{{{\rm{stim}}}}2} \sim\Big\{i| 37 &lt; i\le 50;i\in {\mathbb{N}}\Big\}\\ &amp;&amp;du{r}_{{{{\rm{delay}}}}} \sim\Big\{i| 15 &lt; i\le 25;i\in {\mathbb{N}}\Big\}\\ &amp;&amp;du{r}_{{{{\rm{prep}}}}.}=150-\Big(du{r}_{{{{\rm{response}}}}}+du{r}_{{{{\rm{stim}}}}1}+du{r}_{{{{\rm{stim}}}}2}+du{r}_{{{{\rm{delay}}}}}\Big)\end{array}$$</span></p></div><p>For tasks that don’t utilize a delay structure, stim1, stim2 and delay epochs are grouped together in a single stimulus epoch where <span>\(du{r}_{{{{\rm{stimulus}}}}}=du{r}_{{{{\rm{stim}}}}1}+du{r}_{{{{\rm{stim}}}}2}+du{r}_{{{{\rm{delay}}}}}\)</span>. Unless otherwise specified, a fixation cue with a constant strength <i>s</i><i>t</i><i>r</i><sub>fix</sub> = 1 is activated throughout the preparatory and stimulus epochs. For example trials of each task, see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">13</a>.</p><h4 id="Sec17">‘Go’ tasks</h4><p>The ‘Go’ family of tasks includes ‘Go’, ‘RTGo’, ‘AntiGo’, ‘AntiRTGo’ and modality-specific versions of each task denoted with either ‘Mod1’ and ‘Mod2’. In both the ‘Go’ and ‘AntiGo’ tasks, a single stimulus is presented at the beginning of the stimulus epoch. The direction of the presented stimulus is generated by drawing from a uniform distribution between 0 and 2<i>π</i>, that is, <span>\({\theta }_{{{{\rm{stim}}}}} \sim {\mathbb{U}}[0,2\pi ]\)</span>. The stimulus will appear in either modality 1 or modality 2 with equal probability. The strength of the stimulus is given by <span>\(st{r}_{{{{\rm{stim}}}}} \sim {\mathbb{U}}[1.0,1.2]\)</span>. In the ‘Go’ task, the target response is in the same direction as the presented stimulus, that is, <span>\({\theta }_{{{{\rm{stim}}}}}={\theta }_{{{{\rm{target}}}}}\)</span>, while in the ‘AntiGo’ task the direction of the response should be in the opposite of the stimulus direction, <span>\({\theta }_{{{{\rm{stim}}}}}+\pi ={\theta }_{{{{\rm{target}}}}}\)</span>. For modality-specific versions of each task, a stimulus direction is drawn in each modality <span>\({\theta }_{{{{\rm{stim}}}},{{{\rm{mod}}}}1} \sim {\mathbb{U}}[0,2\pi ]\)</span> and <span>\({\theta }_{{{{\rm{stim}}}},{{{\rm{mod}}}}2} \sim {\mathbb{U}}[0,2\pi ]\)</span> and for modality-specific Go-type tasks</p><div id="Equi"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}},{{{\rm{mod}}}}1} &amp;{{\mbox{if}}}\,\,\,{{\mbox{Mod1 task}}}\\ {\theta }_{{{{\rm{stim}}}},{{{\rm{mod}}}}2} &amp;{{\mbox{if}}}\,\,\,{{\mbox{Mod2 task}}}\end{array}\right.$$</span></p></div><p>while for modality-specific AntiGo-type tasks</p><div id="Equj"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}},{{{\rm{mod}}}}1}+\pi &amp;{{\mbox{if}}}\,\,\,{{\mbox{Mod1 task}}}\,\\ {\theta }_{{{{\rm{stim}}}},{{{\rm{mod}}}}2}+\pi &amp;{{\mbox{if}}}\,\,\,{{\mbox{Mod2 task}}}\end{array}\right.$$</span></p></div><p>For ‘RT’ versions of the ‘Go’ tasks, stimuli are only presented during the response epoch and the fixation cue is never extinguished. Thus, the presence of the stimulus itself serves as the response cue and the model must respond as quickly as possible. Otherwise, stimuli persist through the duration of the stimulus epoch.</p><h4 id="Sec18">‘Decision-making’ tasks</h4><p>The ‘decision-making’ family of tasks includes ‘DM’ (decision-making), ‘AntiDM’, ‘MultiDM’ (multisensory decision-making), ‘AntiMultiDM,’ modality-specific versions of each of these tasks and, finally, confidence-based versions of ‘DM’ and ‘AntiDM.’ For all tasks in this group, two stimuli are presented simultaneously and persist throughout the duration of the stimulus epoch. They are drawn according to <span>\({\theta }_{{{{\rm{stim}}}}1} \sim {\mathbb{U}}[0,2\pi ]\)</span> and <span>\({\theta }_{{{{\rm{stim}}}}2} \sim {\mathbb{U}}\)</span><span>\([({\theta }_{{{{\rm{stim}}}}1}-0.2\pi ,{\theta }_{{{{\rm{stim}}}}1}-0.6\pi )\cup ({\theta }_{{{{\rm{stim}}}}1}+0.2\pi ,{\theta }_{{{{\rm{stim}}}}1}+0.6\pi )]\)</span>. A base strength applied to both stimuli is drawn such that <span>\(st{r}_{\rm{base}} \sim {\mathbb{U}}[1.0,1.2]\)</span>. A contrast is drawn from a discrete distribution such that <i>c</i> ~ {−0.175, −0.15, −0.1, 0.1, 0.15, 0.175} so the stimulus strength associated with each direction in a trial are given by <span>\(st{r}_{{{{\rm{stim}}}}1}=st{r}_{\rm{base}}+c\)</span> and <span>\(st{r}_{{{{\rm{stim}}}}2}=\)</span> <span>\({str}_{\rm{base}}-c\)</span>.</p><p>For the ‘DM’ task,</p><div id="Equk"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1}\quad &amp;\,{{\mbox{if}}}\,\,st{r}_{{{{\rm{stim}}}}1} &gt; st{r}_{{{{\rm{stim}}}}2}\\ {\theta }_{{{{\rm{stim}}}}2}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>and for the the ‘AntiDM’ task,</p><div id="Equl"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1}\quad &amp;\,{{\mbox{if}}}\,\,st{r}_{{{{\rm{stim}}}}1} &lt; st{r}_{{{{\rm{stim}}}}2}\\ {\theta }_{{{{\rm{stim}}}}2}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>For these versions of the tasks, the stimuli are presented in either modality 1 or modality 2 with equal probability. For the multisensory versions of each task, stimuli directions are drawn in the same manner and presented across both modalities so that <span>\({\theta }_{{{{\rm{stim}}}}1,{{{\rm{mod}}}}1}={\theta }_{{{{\rm{stim}}}}1,{{{\rm{mod}}}}2}\)</span> and <span>\({\theta }_{{{{\rm{stim}}}}2,{{{\rm{mod}}}}1}={\theta }_{{{{\rm{stim}}}}2,{{{\rm{mod}}}}2}\)</span>. Base strengths are drawn independently for each modality. Contrasts for both modalities are drawn from a discrete distribution such that <span>\({c}_{{{\mathrm{mod}}}\,1},{c}_{{{\mathrm{mod}}}\,2} \sim \left\{0.2,0.175,\right.\)</span><span>\(\left.0.15,0.125,-0.125,-0.15,-0.175,-0.2\right\}\)</span>. If both <span>\(| {c}_{{{\mathrm{mod}}}\,1}| -| {c}_{{{\mathrm{mod}}}\,2}| =0\)</span> then contrasts are redrawn to avoid zero-contrast trials during training. If both <span>\({c}_{{{\mathrm{mod}}}\,1}\)</span> and <span>\({c}_{{{\mathrm{mod}}}\,2}\)</span> have the same sign, then contrasts are redrawn to ensure that the trial requires integrating over both modalities as opposed to simply performing a ‘DM’ task in a single modality. Criteria for target responses are measured as the strength of a given direction summed over both modalities. So, for ‘MultiDM’</p><div id="Equm"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1,{{\mathrm{mod}}}\,1}\quad &amp;\,{{\mbox{if}}}\,\,st{r}_{{{{\rm{stim}}}}1,{{{\rm{mod}}}}1}+st{r}_{{{{\rm{stim}}}}1,{{{\rm{mod}}}}2} &gt; st{r}_{{{{\rm{stim}}}}2,{{{\rm{mod}}}}1}\\&amp;+st{r}_{{{{\rm{stim}}}}2,{{{\rm{mod}}}}2}\\ {\theta }_{{{{\rm{stim}}}}2,{{{\rm{mod}}}}1}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>and for ‘AntiMultiDM’</p><div id="Equn"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1,{{\mathrm{mod}}}\,1}\quad &amp;\,{{\mbox{if}}}\,\,st{r}_{{{{\rm{stim}}}}1,{{{\rm{mod}}}}1}+st{r}_{{{{\rm{stim}}}}1,{{{\rm{mod}}}}2} &lt; st{r}_{{{{\rm{stim}}}}2,{{{\rm{mod}}}}1}\\&amp;+st{r}_{{{{\rm{stim}}}}2,{{{\rm{mod}}}}2}\\ {\theta }_{{{{\rm{stim}}}}2,{{{\rm{mod}}}}1}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>Stimuli for modality-specific versions of each task are generated in the same way as multisensory versions of the task. Criteria for target response are the same as standard versions of ‘DM’ and ‘AntiDM’ tasks applied only to stimuli in the relevant modality.</p><p>In confidence-based decision-making tasks (‘ConDM’ and ‘ConAntiDM’), the stimuli directions are drawn in the same way as above. Stimuli are shown in either modality 1 or modality 2 with equal probability. In each trial, <i>s</i><i>t</i><i>r</i><sub>base</sub> = 1. The contrast and noise for each trial is based on the thresholded performance of a SIMPLENET model trained on all tasks except ‘ConDM’ and ‘ConAntiDM’. Once this model has been trained, we establish a threshold across levels of noise and contrasts for which the model can perform a ‘DM’ or an ‘AntiDM’ task at 95% correct. We then draw contrasts and noises for trials from above and below this threshold with equal probability during training. In trials where the noise and contrast levels fell below the 95% correct threshold, the model must repress response, and otherwise perform the decision-making task (either ‘DM’ or ‘AntiDM’).</p><h4 id="Sec19">‘Comparison’ tasks</h4><p>Our comparison task group includes ‘COMP1’, ‘COMP2’, ‘MultiCOMP1’, ‘MultiCOMP2’, ‘Anti’ versions of each of these tasks, as well as modality-specific versions of ‘COMP1’ and ‘COMP2’ tasks. This group of tasks is designed to extend the basic decision-making framework into a setting with more complex control demands. These tasks utilize the delay structure in the stimulus epoch so that stim1 appears only during the stim1 epoch, followed by a delay, and finally stim2. This provides a temporal ordering on the stimuli. In ‘COMP1’, the model must respond to the first stimulus only if it has greater strength than the second and otherwise repress a response that is</p><div id="Equo"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1}\quad &amp;\,{{\mbox{if}}}\,\,st{r}_{{{{\rm{stim}}}}1} &gt; st{r}_{{{{\rm{stim}}}}2}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>Likewise, in ‘COMP2’, the model must respond to the second direction if it presented with greater strength than the first otherwise repress response that is</p><div id="Equp"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}2}\quad &amp;\,{{\mbox{if}}}\,\,st{r}_{{{{\rm{stim}}}}2} &gt; {{{{\rm{str}}}}}_{{{{\rm{stim}}}}1}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>In ‘Anti’ versions of the task the ordering criteria is the same except for stimuli with least strength, that is, for ‘AntiCOMP1’</p><div id="Equq"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1}\quad &amp;\,{{\mbox{if}}}\,\,{{{{\rm{str}}}}}_{{{{\rm{stim}}}}1} &lt; {{{{\rm{str}}}}}_{{{{\rm{stim}}}}2}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>and for ‘AntiCOMP2’</p><div id="Equr"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}2}\quad &amp;\,{{\mbox{if}}}\,\,{{{{\rm{str}}}}}_{{{{\rm{stim}}}}2} &lt; {{{{\rm{str}}}}}_{{{{\rm{stim}}}}1}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>In multisensory settings, the criteria for target direction are analogous to the multisensory decision-making tasks where strength is integrated across modalities. Likewise, for modality-specific versions, the criteria are only applied to stimuli in the relevant modality. Stimuli directions and strength for each of these tasks are drawn from the same distributions as the analogous task in the ‘decision-making’ family. However, during training, we make sure to balance trials where responses are required and trials where models must repress response.</p><h4 id="Sec20">‘Duration’ tasks</h4><p>The ‘duration’ family of tasks includes ‘Dur1’, ‘Dur2’, ‘MultiDur1’, ‘MultiDur2’, ‘Anti’ versions of each of these tasks and modality-specific versions of ‘Dur1’ and ‘Dur2’ tasks. These tasks require models to perform a time estimation task with the added demand or stimuli ordering determining relevance for response. Like in ‘comparison’ tasks, stim1 is presented followed by a delay and then stim2. For ‘Dur1’ trials</p><div id="Equs"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1}\quad &amp;\,{{\mbox{if}}}\,\,du{r}_{{{{\rm{stim}}}}1} &gt; du{r}_{{{{\rm{stim}}}}2}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>Likewise, for ‘Dur2’</p><div id="Equt"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}2}\quad &amp;\,{{\mbox{if}}}\,\,du{r}_{{{{\rm{stim}}}}2} &gt; du{r}_{{{{\rm{stim}}}}1}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>In ‘Anti’ versions of these tasks, the correct response is in the direction of the stimulus with the shortest duration given the ordering criteria is met. Hence, for ‘AntiDur1’</p><div id="Equu"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1}\quad &amp;\,{{\mbox{if}}}\,\,du{r}_{{{{\rm{stim}}}}1} &lt; du{r}_{{{{\rm{stim}}}}2}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>and for ‘AntiDur2’</p><div id="Equv"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}2}\quad &amp;\,{{\mbox{if}}}\,\,du{r}_{{{{\rm{stim}}}}2} &lt; du{r}_{{{{\rm{stim}}}}1}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>Across these tasks directions are drawn according to <span>\({\theta }_{{{{\rm{stim}}}}1} \sim {\mathbb{U}}[0,2\pi ]\)</span> and <span>\({\theta }_{{{{\rm{stim}}}}2} \sim {\mathbb{U}}[({\theta }_{{{{\rm{stim}}}}1}-0.2\pi ,{\theta }_{{{{\rm{stim}}}}1}-0.6\pi )\cup ({\theta }_{{{{\rm{stim}}}}1}+0.2\pi ,{\theta }_{{{{\rm{stim}}}}1}+0.6\pi )]\)</span>. Stimulus strengths are drawn according to <span>\(st{r}_{{{{\rm{stim}}}}1},st{r}_{{{{\rm{stim}}}}2} \sim {\mathbb{U}}[0.8,1.2]\)</span>. To set the duration of each stimulus, we first draw <span>\(du{r}_{{{{\rm{long}}}}} \sim\)</span> <span>\(\{i| 35 &lt; i\le 50,i\in {\mathbb{N}}\}\)</span> and <span>\(du{r}_{{{{\rm{short}}}}} \sim \{i| 25 &lt; i\le (du{r}_{{{{\rm{long}}}}}-8),i\in {\mathbb{N}}\}\)</span>. During training, we determine which trials for a given task should and should not require a response in order to evenly balance repress and respond trials. We then assign <i>d</i><i>u</i><i>r</i><sub>long</sub> and <i>d</i><i>u</i><i>r</i><sub>short</sub> to either stim1 or stim2 so that the trial requires the appropriate response given the particular task type.</p><p>Again, criteria for correct response in the multisensory and modality-specific versions of each tasks follow analogous tasks in the ‘decision-making’ and ‘comparison’ groups where multisensory versions of the task require integrating total duration over each modality, and modality-specific tasks require only considering durations in the given task modality. For multisensory tasks, we draw duration value <span>\(du{r}_{{{{\rm{long}}}}} \sim \{i| 75 &lt; i\le 100,i\in {\mathbb{N}}\}\)</span> and then split this value <i>d</i><i>u</i><i>r</i><sub>long0</sub> = <i>d</i><i>u</i><i>r</i><sub>long </sub>× 0.55 and <i>d</i><i>u</i><i>r</i><sub>long1</sub> = <i>d</i><i>u</i><i>r</i><sub>long </sub>× 0.45. We also draw a value <i>d</i><i>u</i><i>r</i><sub>short</sub> = <i>d</i><i>u</i><i>r</i><sub>long</sub> − Δ<i>d</i><i>u</i><i>r</i> where <span>\(\Delta dur \sim \{i| 15 &lt; i\le 25,i\in {\mathbb{N}}\}\)</span>. This value is then subdivided further into <i>d</i><i>u</i><i>r</i><sub>short0</sub> = <i>d</i><i>u</i><i>r</i><sub>long1</sub> + Δ<i>d</i><i>u</i><i>r</i><sub>short</sub> where <span>\(\Delta du{r}_{{{{\rm{short}}}}} \sim\)</span> <span>\(\{i| 19 &lt; i\le 15,i\in {\mathbb{N}}\}\)</span> and <i>d</i><i>u</i><i>r</i><sub>short1</sub> = <i>d</i><i>u</i><i>r</i><sub>Short</sub> − <i>d</i><i>u</i><i>r</i><sub>short0</sub>. Short and long durations can then be allocated to the ordered stimuli according to task type. Drawing durations in this manner ensures that, like in ‘decision-making’ and ‘comparison’ groups, correct answers truly require models to integrate durations over both modalities, rather than simply performing the task in a given modality to achieve correct responses.</p><h4 id="Sec21">‘Matching’ tasks</h4><p>The ‘matching’ family of tasks consists of ‘DMS’ (delay match to stimulus), ‘DNMS’ (delay non-match to stimulus), ‘DMC’ (delay match to category) and ‘DMNC’ (delay non-match to category) tasks. For all tasks, stim1 is presented at the beginning of the stimulus epoch, followed by a delay, and the presentation of stim2. The stimulus strength is drawn according to <span>\(st{r}_{{{{\rm{stim}}}}1},st{r}_{{{{\rm{stim}}}}2} \sim {\mathbb{U}}[0.8,1.2]\)</span>. The input modality for any given trial is chosen at random with equal probability. In both ‘DMS’ and ‘DNMS’ tasks, trials are constructed as ‘matching stim’ trials or ‘mismatching stim’ trials with equal probability. In ‘matching stim’ trials <span>\({\theta }_{{{{\rm{stim}}}}1} \sim {\mathbb{U}}[0,2\pi ]\)</span> and <span>\({\theta }_{{{{\rm{stim}}}}2}={\theta }_{{{{\rm{stim}}}}1}\)</span>. In ‘mismatch stim’ trials, <span>\({\theta }_{{{{\rm{stim}}}}1} \sim {\mathbb{U}}[0,2\pi ]\)</span> and</p><div id="Equw"><p><span>$${\theta }_{{{{\rm{stim}}}}2} \sim {\mathbb{U}}[({\theta }_{{{{\rm{stim}}}}1}-0.2\pi ,{\theta }_{{{{\rm{stim}}}}1}-0.6\pi )\cup ({\theta }_{{{{\rm{stim}}}}1}+0.2\pi ,{\theta }_{{{{\rm{stim}}}}1}+0.6\pi )].$$</span></p></div><p>For ‘DMS’, models must respond in the displayed direction if the stimuli match, otherwise repress response,</p><div id="Equx"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1}\quad &amp;\,{{\mbox{if}}}\,\,{\theta }_{{{{\rm{stim}}}}1}={\theta }_{{{{\rm{stim}}}}2}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>and for ‘DNMS’, models must respond to the second direction if both directions are mismatched,</p><div id="Equy"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}2}\quad &amp;\,{{\mbox{if}}}\,\,{\theta }_{{{{\rm{stim}}}}1}\ne {\theta }_{{{{\rm{stim}}}}2}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>‘DMC’ and ‘DNMC’ tasks are organized in a similar manner. The stimulus input space is divided evenly into two categories such that cat1 = {<i>θ</i>: 0 &lt; <i>θ</i>≤<i>π</i>} and cat2 = {<i>θ</i>: <i>π</i> &lt; <i>θ</i>≤2<i>π</i>}. For ‘DMC’ and ‘DNMC’ tasks, trials are constructed as ‘matching cat.’ trials or ‘mismatching cat.’ trials with equal probability. In ‘matching cat.’ trials <span>\({\theta }_{{{{\rm{stim}}}}1} \sim {\mathbb{U}}[0,2\pi ]\)</span> and <span>\({\theta }_{{{{\rm{stim}}}}2} \sim {\mathbb{U}}({{{\mbox{cat}}}}_{{{{\rm{stim}}}}1})\)</span>, where <span>\({\mathbb{U}}({{{\mbox{cat}}}}_{{{{\rm{stim}}}}1})\)</span> is a uniform draw from the category of stim1. In ‘mismatch stim’ trials, <span>\({\theta }_{{{{\rm{stim}}}}1} \sim {\mathbb{U}}[0,2\pi ]\)</span> and <span>\({\theta }_{{{{\rm{stim}}}}2} \sim {\mathbb{U}}(-{{{\mbox{cat}}}}_{{{{\rm{stim}}}}1})\)</span> where <span>\(-{{{\mbox{cat}}}}_{{{{\rm{stim}}}}1}\)</span> is the opposite category as stim1. For ‘DMC’, the model must respond in the first direction if both stimuli are presented in the same category otherwise repress response,</p><div id="Equz"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1}\quad &amp;\,{{\mbox{if}}}\,\,{{{\mbox{cat}}}}_{{{{\rm{stim}}}}1}={{{\mbox{cat}}}}_{{{{\rm{stim}}}}2}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>and for ‘DNMC’, the model should respond to the second direction if both stimuli are presented in opposite categories otherwise repress response,</p><div id="Equaa"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}2}\quad &amp;\,{{\mbox{if}}}\,\,{{{\mbox{cat}}}}_{{{{\rm{stim}}}}1}\ne {{{\mbox{cat}}}}_{{{{\rm{stim}}}}2}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><h3 id="Sec22">Target output and correct criteria</h3><p>The target output <span>\(y\in {{\mathbb{R}}}^{33\times T}\)</span> for a trial entails maintaining fixation in <i>y</i><sub>1</sub> = <i>y</i><sub>fix</sub> during the stimulus epoch, and then either responding in the correct direction or repressing activity in the remaining target response units <i>y</i><sub>2…33</sub> in the response epoch. Since the model should maintain fixation until response, target for fixation is set at <i>y</i><sub>fix</sub> = 0.85 during preparatory and stimulus epochs and <i>y</i><sub>fix</sub> = 0.05 in the response epoch. When a response is not required, as in the preparatory and stimulus epochs and with repressed activity in the response epoch, unit <i>i</i> takes on a target activity of <i>y</i><sub><i>i</i></sub> = 0.05. Alternatively, when there is a target direction for response,</p><div id="Equab"><p><span>$${y}_{i}=0.8\exp \left[-0.5 \times {\left(\frac{8| {\theta }_{{{{\rm{target}}}}}-{\theta }_{i}| }{\pi }\right)}^{2}\right]+0.05$$</span></p></div><p>where <i>θ</i><sub><i>i</i></sub> is the preferred direction for unit <i>i</i>. Like in sensory stimuli, preferred directions for target units are evenly spaced values from [0, 2<i>π</i>] allocated to the 32 response units.</p><p>For a model response to count as correct, it must maintain fixation, that is, <span>\({\hat{y}}_{{{{\rm{fix}}}}} &gt; 0.5\)</span> during preparatory and stimulus epochs. When no response is required <span>\({\hat{y}}_{i} &lt; 0.15\)</span>. When a response is required, response activity is decoded using a population vector method and <span>\({\theta }_{{{{\rm{resp}}}}.}\in ({\theta }_{{{{\rm{target}}}}}-\frac{\pi }{10},{\theta }_{{{{\rm{target}}}}}+\frac{\pi }{10})\)</span>. If the model fails to meet any of these criteria, the trial response is incorrect.</p><h3 id="Sec23">Model training</h3><p>Again following ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e11643">18</a></sup>, model parameters are updated in a supervised fashion according to a masked mean squared error loss (mMSE) computed between the model motor response, <span>\({\hat{y}}_{1\ldots T}=\hat{y}\)</span>, and the target, <i>y</i><sub>1…<i>T</i></sub> = <i>y</i>, for each trial.</p><div id="Equac"><p><span>$$L={{{\rm{mMSE}}}}(\,y,\hat{y})={\rm{mask}} \times {\Big\langle {\left({\,y}_{t}-{{\hat{y}_{t}}}\right)}^{2}\Big\rangle }_{t}$$</span></p></div><p>Here, the multiplication sign denotes element-wise multiplication. Masks weigh the importance of different trial epochs. During preparatory and stimulus epochs, mask weights are set to 1; during the first five time steps of the response epoch, the mask value is set to 0; and during the remainder of the response epoch, the mask weight is set to 5. The mask value for the fixation is twice that of other values at all time steps.</p><p>For all models, we update Θ = {sensorimotor-RNN, Linear<sub>out</sub>} during training on our task set. For instructed models, we additionally update Linear<sub>embed</sub> in the process of normal training. We train models using standard PyTorch machinery and an Adam optimizer. An epoch consists of 2,400 mini-batches, with each mini-batch consisting of 64 trials. For all models, we use the same initial learning rate as in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e11872">18</a></sup>, <i>l</i><i>r</i> = 0.001. We found that in the later phases of training, model performance oscillated based on which latest task presented during training, so we decayed the learning rate for each epoch by a factor of <i>γ</i> = 0.95, which allowed performance to converge smoothly. Following ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e11885">18</a></sup>, models train until they reach a threshold performance of 95% across all tasks (and train for a minimum of 35 epochs). We found that training for GPTNET tended to asymptote below performance threshold for multisensory versions of comparison tasks. This held true over a variety of training hyperparameters and learning rate scheduler regimes. Hence, we relax the performance threshold of GPTNET to 85%. For each model type, we train five models that start from five different random initializations. Where applicable, results are averaged over these initializations.</p><h4 id="Sec24">Language model fine-tuning</h4><p>When fine-tuning models, we allow the gradient from the motor loss experienced during sensorimotor training to fine-tune the weights in the final layers of the transformer language models. During normal training, we checkpoint a copy of our instructed models after training for 30 epochs. We then add the last three transformer layers to the set of trainable parameters, and reset the learning rates to <i>l</i><i>r</i> = 1 × 10<sup>−</sup><sup>4</sup> for Θ = {sensorimotor-RNN, Linear<sub>out</sub>} and <i>l</i><i>r</i><sup>lang</sup> = 3 × 10<sup>−4</sup> for Θ<sup>lang</sup> = {Linear<sub>embed</sub>, transformer<sub>−3,−2,−1</sub>} where transformer<sub>−3,−2,−1</sub> denotes the parameters of the last three layers of the relevant transformer architecture. We used these reduced learning rates to avoid completely erasing preexisting linguistic knowledge. Similarly for RNN parameters, we found the above learning rate avoided catastrophic forgetting of sensorimotor knowledge while also allowing the RNN to adapt to updated language embeddings across all models. Autoregressive models were much more sensitive to this procedure, often collapsing at the beginning of fine-tuning. Hence, for GPTNETXL and GPTNET, we used <i>l</i><i>r</i><sup>lang</sup> = 5 × 10<sup>−5</sup>, which resulted in robust learning. Models train until they reach a threshold performance of 95% across training tasks or 85% correct for GPTNET.</p><h3 id="Sec25">Hold-out testing</h3><p>During hold-out testing, we present models with 100 batches of one of the tasks that had been held out of training. For the instructed model, the only weights allowed to update during this phase are Θ = {sensorimotor-RNN, Linear<sub>out</sub>, Linear<sub>embed</sub>}. All weights of SIMPLENET and STRUCTURENET are trainable in this context. In this hold-out setting, we found that in more difficult tasks for some of our more poorly performing models, the standard hyperparameters we used during training resulted in unstable learning curves for novel tasks. To stabilize performance and thereby create fair comparisons across models, we used an increased batch size of 256. We then began with the standard learning rate of 0.001 and decreased this by increments of 0.0005 until all models showed robust learning curves. This resulted in a learning rate of 8 × 10<sup>−4</sup>. All additional results shown in the <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec36">Supplementary Information</a> section 4 follow this procedure.</p><h3 id="Sec26">CCGP calculation</h3><p>To calculate CCGP, we trained a linear decoder on a pair of tasks and then tested that decoder on alternative pairs of tasks that have an analogous relationship. We grouped tasks into eight dichotomies: ‘Go’ versus ‘Anti’, ‘Standard’ versus ‘RT’, ‘Weakest’ versus ‘Strongest’, ‘Longest’ versus ‘Shortest’, ‘First Stim.’ versus ‘Second Stim’, ‘Stim Match’ versus ‘Category Match’, ‘Matching’ versus ‘Non-Matching’ and ‘Mod1’ versus ‘Mod2’. As an example, the ‘Go’ versus ‘Anti’ dichotomy includes (‘Go’, ‘AntiGo’), (‘GoMod1’, ‘AntiGoMod1’), (‘GoMod2’, ‘AntiGoMod2’), (‘RTGo’, ‘AntiRTGo’), (‘RTGoMod1’, ‘AntiRTGoMod1’) and (‘RTGoMod2’, ‘AntiRTGoMod2’) task pairs. For ‘RNN’ task representations, we extracted activity at the time of stimulus onset for 250 example trials. For language representations, we input the instruction sets for relevant tasks to our language model and directly analyze activity in the ‘embedding’ layer or take the sequence-averaged activity in each transformer layer. For nonlinguistic models, we simply analyze the space of rule vectors. Train and test conditions for decoders were determined by dichotomies identified across the task set (Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">1</a>). To train and test decoders, we used sklearn.svm.LinearSVC Python package. The CCGP score for a given task is the average decoding score achieved across all dichotomies where the task in question was part of either the train set or the test set. For model scores reported in the main text, we only calculate CCGP scores for models where the task in question has been held out of training. In Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">9</a>, we report scores on tasks where models have been trained on all tasks, and for models where instructions have been switched for the hold-out task.</p><p>For Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig3">3e</a>, we calculated Pearson’s <i>r</i> correlation coefficient between performance on held-out tasks and CCGP scores per task, as well as a <i>P</i>-value testing against the null hypothesis that these metrics are uncorrelated and normally distributed (using the scipy.stats.pearsonr function). Full statistical tests for CCGP scores of both RNN and embedding layers from Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig3">3f</a> can be found in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">9</a>. Note that transformer language models use the same set of pretrained weights among random initialization of Sensorimotor-RNNs, thus for language model layers, the Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig3">3f</a> plots show the absolute scores of those language models.</p><h3 id="Sec27">Conditional clause/deduction task analysis</h3><p>We first split our task set into two groups (listed below): tasks that included conditional clauses and simple deductive reasoning components (30 tasks) and those where instructions include simple imperatives (20 tasks). We computed the difference in performance across the mean of generalization performance for each group across random initialization for each model (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig2">2f</a>). We compared these differences to a null distribution constructed by performing a set of 50 random shuffles of the task set into groups of 30 and 20 tasks and computing differences in the same way, again using two-sided unequal-variance <i>t</i>-tests. Because STRUCUTRENET is a nonlinguistic model, we then compared performance of STRUCUTRENET to our instructed models to disassociate the effects of performing tasks with a deductive reasoning component versus processing instructions with more complicated conditional clause structure. Results of all statistical tests are reported in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">6</a>).</p><p>Simple imperative tasks include: ‘Go’, ‘AntiGo’, ‘RTGo’, ‘AntiRTGo’, ‘GoMod1’, ‘GoMod2’, ‘AntiGoMod1’, ‘AntiGoMod2’, ‘RTGoMod1’, ‘AntiRTGoMod2’, ‘RTGoMod2’, ‘AntiRTGoMod2’, ‘DM’, ‘AntiDM’, ‘MultiDM’, ‘AntiMultiDM’, ‘DMMod1’, ‘DMMod2’, ‘AntiDMMod1’ and ‘AntiDMMod2’.</p><p>Conditional clause/deduction tasks include: ‘ConDM’, ‘ConAntiDM’, ‘Dur1’, ‘Dur2’, ‘MultiDur1’, ‘MultiDur2’, ‘AntiDur1’, ‘AntiDur2’, ‘AntiMultiDur1’, ‘AntiMultiDur2’, ‘Dur1Mod1’, ‘Dur1Mod2’, ‘Dur2Mod1’, ‘Dur2Mod2’, ‘COMP1’, ‘COMP2’, ‘MultiCOMP1’, ‘MultiCOMP2’, ‘AntiCOMP1’, ‘AntiCOMP2’, ‘AntiMultiCOMP1’, ‘AntiMultiCOMP2’, ‘COMP1Mod1’, ‘COMP1Mod2’, ‘COMP2Mod1’, ‘COMP2Mod2’, ‘DMS’, ‘DNMS’, ‘DMC’ and ‘DMNC’.</p><h3 id="Sec28">Language production training</h3><h4 id="Sec29">Self-supervised language production network training</h4><p>Our language production framework is inspired by classic sequence-to-sequence modeling using RNNs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Sutskever, I., Vinyals, O. &amp; Le., Q. V. Sequence to sequence learning with neural networks. In Proc. 27th International Conference on Neural Information Processing Systems 3104–3112 (MIT Press, 2014)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR53" id="ref-link-section-d167182994e12022">53</a></sup>. Our Production-RNN is a GRU with 256 hidden units using ReLU nonlinearities. At each step in the sequence, a set of decoder weights, Linear<sub>words</sub>, attempts to decode the next token, <i>w</i><sub><i>τ</i>+1</sub>, from the hidden state of the recurrent units. The hidden state of the Production-RNN is initialized by concatenating the time average and maximum sensorimotor activity of a SBERTNET (L) and passing that through weights Linear<sub>sm</sub>. The linguistic instruction used to drive the initializing sensorimotor activity is in turn used as the target set of tokens for the Production-RNN outputs. The first input to the Production-RNN is always a special start-of-sentence token, and the decoder runs until an end-of-sentence token is decoded or until input reaches a length of 30 tokens. Suppose <span>\({w}_{1,k}\ldots {w}_{{{{\mathcal{T}}}},k}\in {\rm{Instruc{t}}}_{k}^{i}\)</span> is the sequence of tokens in instruction <i>k</i> where <i>k</i> is in the instruction set for task <i>i</i> and <i>X</i><sup><i>i</i></sup> is sensory input for a trial of task <i>i</i>. For brevity, we denote the process by which language models embed instructions as Embed() (see ‘Pretrained transformers’). The decoded token at the <i>τ</i><sup>th</sup> position, <span>\({\hat{w}}_{\tau ,k}\)</span>, is then given by</p><div id="Equad"><p><span>$$\begin{array}{ll}{h}_{T}^{sm}={{{\rm{SensorimotorRNN}}}}\left({X}^{i},Embed\left({w}_{1,k}\ldots {w}_{{{{\mathcal{T}}}},k}\right)\right)\quad\quad{h}_{T}^{sm}\in {{\mathbb{R}}}^{T\times 256}\\ sm\_out=\left.\right({{{{\rm{mean}}}}}_{T}\left({h}_{T}^{sm}\right),\mathop{\max }\limits_{T}\left({h}_{T}^{sm}\right)\quad\quad\quad\quad\quad\quad\quad\quad\quad\;\;{sm}\_{out}\in {{\mathbb{R}}}^{512}\\ \overline{{h}_{0}^{{{{\rm{decoder}}}}}}={{{\rm{relu}}}}\left({{{{\rm{Linear}}}}}_{{{{\rm{sm}}}}}(sm\_out)\right)\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\;\overline{{h}_{0}^{{{{\rm{decoder}}}}}}\in {{\mathbb{R}}}^{256}\\ {h}_{0}^{{{{\rm{decoder}}}}}={{{\rm{Dropout}}}}\left(\overline{{h}_{0}^{{{{\rm{decoder}}}}}}\right)\;\;\;\quad\quad\quad\quad\quad\quad\quad\quad\quad\qquad\quad{h}_{0}^{{{{\rm{decoder}}}}}\in {{\mathbb{R}}}^{256}\\ {h}_{\tau }^{{{{\rm{decoder}}}}}={{{\rm{ProductionRNN}}}}\left({\hat{w}}_{1,k}\ldots {\hat{w}}_{\tau -1,k};{h}_{0}^{{{{\rm{decoder}}}}}\right),\quad\quad\quad{h}_{\tau }^{{{{\rm{decoder}}}}}\in {{\mathbb{R}}}^{256}\\ {p}_{{\hat{w}}_{\tau ,k}}={{{\rm{softmax}}}}\left({{{{\rm{Linear}}}}}_{{{{\rm{words}}}}}\left({h}_{\tau ,k}^{{{{\rm{decoder}}}}}\right)\right)\quad\quad\quad\quad\quad\quad\quad\quad\quad{p}_{{\hat{w}}_{\tau ,k}}\in {{\mathbb{R}}}^{| vocab| },\\ {\hat{w}}_{\tau ,k}={{{\rm{argmax}}}}\left({p}_{{\hat{w}}_{\tau ,k}}\right)\end{array}$$</span></p></div><p>The model parameters Θ<sup>production</sup> = {Linear<sub>sm</sub>, Linear<sub>words</sub>, Production-RNN} are trained using cross-entropy loss between the <span>\({p}_{{\hat{w}}_{\tau ,i}}\)</span> and the instruction token <i>w</i><sub><i>τ</i>,<i>k</i></sub> provided to the sensorimotor-RNN as input. We train for 80 epochs of 2,400 batches with 64 trials per batch and with task type randomly interleaved. We found that using an initial learning rate of 0.001 sometimes caused models to diverge in early phases of training, so we opted for a learning rate of 1× 10<sup>−4</sup>, which led to stable early training. To alleviate similar oscillation problems detected in sensorimotor training, we also decayed the learning rate by <i>γ</i> = 0.99 per epoch. Additionally, the use of a dropout layer with a dropout rate of 0.05 improved performance. We also used a teacher forcing curriculum, where for some ratio of training batches, we input the ground truth instruction token <i>w</i><sub><i>τ</i>,<i>k</i></sub> at each time step instead of the models decoded word <span>\({\hat{w}}_{\tau ,k}\)</span>. At each epoch, <span>\({\rm{teacher}}\,{{\mbox{\_}}}{\rm{forcing}}{{\mbox{\_}}}\)</span> <span>\({\rm{ratio}}=0.5 \times \frac{80-{{{\rm{epoch}}}}}{80}\)</span>.</p><h4 id="Sec30">Obtaining embedding layer activity using motor feedback</h4><p>For a task, <i>i</i>, we seek to optimize a set of embedding activity vectors <span>\({E}^{i}\in {{\mathbb{R}}}^{64}\)</span> such that when they are input as task-identifying information, the model will perform the task in question. Crucially, we freeze all model weights Θ = {sensorimotor-RNN, Linear<sub>out</sub>, Linear<sub>embedding</sub>} and only update <i>E</i><sup><i>i</i></sup> according to the standard supervised loss on the motor output. For notional clarity, GRU dependence on the previous hidden state <i>h</i><sub><i>t</i>−1</sub> has been made implicit in the following equations.</p><div id="Equae"><p><span>$$\begin{array}{rcl}{\hat{y}}{\,}^{i}&amp;=&amp;\sigma \Big({{{{\rm{Linear}}}}}_{{{{\rm{out}}}}}\left({{{\rm{SensorimotorRNN}}}}({X}^{\,i},{E}^{i})\right)\Big)\\ L&amp;=&amp;{\rm{mMSE}}(\;y,\hat{y})\end{array}$$</span></p></div><p>We optimized a set of 25 embedding vectors for each task, again using an Adam optimizer. Here the optimization space has many suboptimal local minimums corresponding to embeddings for related tasks. Hence, we used a high initial learning rate of <i>l</i><i>r</i> = 0.05, which we decayed by <i>γ</i> = 0.8 for each epoch. This resulted in more robust learning than lower learning rates. An epoch lasts for 800 batches with a batch length of 64, and we train for a minimum of 1 epoch or until we reach a threshold performance of 90% or 85% on ‘DMC’ and ‘DNMC’ tasks.</p><h4 id="Sec31">Producing task instructions</h4><p>To produce task instructions, we simply use the set <i>E</i><sup><i>i</i></sup> as task-identifying information in the input of the sensorimotor-RNN and use the Production-RNN to output instructions based on the sensorimotor activity driven by <i>E</i><sup><i>i</i></sup>. For each task, we use the set of embedding vectors to produce 50 instructions per task. We repeat this process for each of the 5 initializations of sensorimotor-RNN, resulting in 5 distinct language production networks, and 5 distinct sets of learned embedding vectors. Reported results for each task are averaged over these 5 networks. For the confusion matrix (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5d</a>), we report the average percentage that decoded instructions are in the training instruction set for a given task or a novel instruction. Partner model performance (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5e</a>) for each network initialization is computed by testing each of the 4 possible partner networks and averaging over these results.</p><h3 id="Sec32">Sample sizes/randomization</h3><p>No statistical methods were used to predetermine sample sizes but following ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e13740">18</a></sup> we used five different random weight initializations per language model tested. Randomization of weights was carried out automatically in Python and PyTorch software packages. Given this automated randomization of weights, we did not use any blinding procedures in our study. No data were excluded from analyses.</p><h3 id="Sec33">Software</h3><p>All simulation and data analysis was performed in Python 3.7.11. PyTorch 1.10 was used to implement and train models (this includes Adam optimizer implementation). Transformers 4.16.2 was used to implement language models and all pretrained weights for language models were taken from the Huggingface repository (<a href="https://huggingface.co/docs/transformers/">https://huggingface.co/docs/transformers/</a>). We also used scikit-learn 0.24.1 and scipy 1.7.3 to perform analyses.</p><h3 id="Sec34">Reporting summary</h3><p>Further information on research design is available in the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM2">Nature Portfolio Reporting Summary</a> linked to this article.</p></div></div>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The New Inflection (185 pts)]]></title>
            <link>https://inflection.ai/the-new-inflection</link>
            <guid>39757368</guid>
            <pubDate>Tue, 19 Mar 2024 16:23:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://inflection.ai/the-new-inflection">https://inflection.ai/the-new-inflection</a>, See on <a href="https://news.ycombinator.com/item?id=39757368">Hacker News</a></p>
<div id="readability-page-1" class="page"><div justify="center" id=""><p>From day one at Inflection, we’ve been driven by a simple mission: to create a personal intelligence for everyone. To do this, we trained one of the <a href="https://inflection.ai/inflection-2-5" target="_blank" rel="noopener noreferrer">best LLMs in the world</a>, and created our first personal AI, Pi, which couples extraordinary EQ with industry leading IQ, and is now used by millions of people a week.</p>
<p>As an AI studio we have <a href="https://inflection.ai/company" target="_blank" rel="noopener noreferrer">long planned</a> to make our technology available to developers and enterprises. And over the last year, we’ve heard countless times that people haven’t been able to replicate the unique conversational style of Pi with publicly available models, and would love to get access to our model and fine tuning infrastructure. There is a huge opportunity for Inflection here.</p>
<p>Our plan going forward is to lean into our AI studio business, where custom generative AI models are crafted, tested and fine tuned for commercial customers. Our success at training, tailoring and improving the performance of large AI models makes us uniquely well placed to be the AI platform for businesses around the world.</p>
<p>As part of this, we’re thrilled to announce that we will now host Inflection-2.5 on Microsoft Azure helping us get it into the hands of creators everywhere. We’ll also be ensuring it comes to other cloud hosting platforms in the near future. The API itself isn’t available today, but will be up and running very soon. To sign up for early access and help us test it, please register your interest <a href="https://docs.google.com/forms/d/e/1FAIpQLScM9Iz1KzaRlfgDrYrldoPDnXbhO5LW3-hqmQCd56YpheEN7g/viewform" target="_blank" rel="noopener noreferrer">here</a>. Between API access and select high-level partnerships with great customers, our AIs can now spread to even larger new user bases while helping put cutting-edge AI capabilities in the hands of thousands of developers.</p>
<p>This renewed emphasis on our API also comes with some important changes in the company. Today we are also announcing that two of our three co-founders, Mustafa and Karén, will be leaving Inflection to start Microsoft AI, a new division at Microsoft that will bring together their consumer AI efforts, as well as Copilot, Bing and Edge. We’re grateful for all their amazing work in getting Inflection to this stage, and wish them luck for this new chapter.</p>
<p>We are delighted to welcome a new CEO, <a href="https://www.linkedin.com/in/seanwhite/" target="_blank" rel="noopener noreferrer">Sean White</a>, who has decades of experience working at the cutting edge of technology, research and business. He is a visionary leader poised to take Inflection into this new era. Our third co-founder, Reid Hoffman will continue on our board and remains excited to take these next steps in building personal intelligence for everyone.</p>
<p>We are hugely proud of what we’ve achieved with Pi. There will be no immediate changes to the service and we’re committed to ensuring that users get ongoing access to great AI experiences in the future. Our privacy and data policies to ensure users are protected remain in place and unchanged. As ever, no data will be shared with any third parties without users' explicit consent.</p></div></div>]]></description>
        </item>
    </channel>
</rss>