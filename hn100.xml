<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 14 Sep 2025 09:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Models of European Metro Stations (167 pts)]]></title>
            <link>http://stations.albertguillaumes.cat/</link>
            <guid>45238055</guid>
            <pubDate>Sun, 14 Sep 2025 07:00:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://stations.albertguillaumes.cat/">http://stations.albertguillaumes.cat/</a>, See on <a href="https://news.ycombinator.com/item?id=45238055">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>Station gallery</h4>
            <p>Select a city and a station on the map or on the selectors:</p>
          </div><div id="llista_com">
            <div id="com_Alicante"> 
                <h5>Alicante</h5>
                <p>Alicante's rapid transit system is called TRAM, which is a train-tram and is operated by FGV, owned by the Valencian regional government. This system has an underground line in the city center, with three stations on it. The layout consists with one or two mezzanines on level -1 and two side or one island platforms at level -2. Luceros and Mercado stations have direct connection with the underground parking lots, which are located above the TRAM tunnel.</p>
            </div>
            <div id="com_Amsterdam"> 
                <h5>Amsterdam</h5>
                <p>Metro services in Amsterdam can be grouped in two groups. The North-South line (in Dutch, <i>Noord/Zuidlijn</i>) opened recently and M52 services run on it. The central part of the line is underground, running at a considerable depth with two parallel tunnels. The other group of lines consists of services M50, M51, M53 and M54. This line runs mostly overground or elevated, parallel to the Dutch Railways lines. In the city center this line goes underground but at a small depth.</p>
                <p>Most of the underground stations consist of a mezzannine at level -1 and a island platform at level -2. The vast majority of overground and elevated stations also have island platforms, but the hall where the turnstiles and ticket machines are found is located on the ground level.</p>
                <p>Taking into account that most of the metro network runs parallel to rail lines, transfers between these two modes of transport are quick and easy.</p>
            </div>
            <div id="com_Antwerp"> 
                <h5>Antwerp</h5>
                <p>The Antwerp premetro (underground tramway) opened in 1975. One of the achievements consisted in linking both banks of the Scheldt river.</p>
                <p>Despite the tram network being huge, the Premetro is short but complex. Lines can be grouped in two. The first group consists in an east-west line with two branches on the eastern side. However, there is a tunnel allowing services between both branches, thus forming a triangle. The second group of lines consists in a tunnel opened in 2015, connecting the city center with the eastern suburbs with a fast service, since most of stations are not opened yet.</p>
                <p>The Central railway station is linked with Astrid and Diamant premetro stations, and together with an underground bike parking, they form a hub. The rail triangle is located in between those two stations. Since the premetro tunnels were built with metro standards, same-level track crossings are not allowed, therefore Astrid and Diamant stations had to be built with platforms at two different levels.</p>
            </div>
            <div id="com_Barcelona"> 
                <h5>Barcelona</h5>
                <p>Barcelona is the queen of the long passageways. Until 1995, all transfer stations consisted of corridors with lenghts over 100 m, except for Sagrera and Catalunya.</p>
                <p>Among the reasons for having such long corridors there is the lack of planning or the vision of the metro network as a bunch of individual lines. As an example: line 1 and line 4 were extended to Urquinaona in 1932, but both lines were not connected until 1972, as they were originally operated by different companies. In Plaça de Sants station, the L5 platforms were built as close as possible to the existing ones from L1, which opened 43 years before. However, there is a gap of 150 m, with the national rail tracks located in between.</p>
                <p>Moreover, since the extension of the metro network was slower than the growth of the city, during the 60s and 70s, transfer stations were built 100-150 m apart, in order to increase the accessibility to the metro. Verdaguer is a good examples of this practice.</p>
                <p>Right after 1980, transfer stations were designed in a more proper way, being the L2 stations the best example. A new type of transfer appeared 15 years ago with the extension of the metro towards hilly areas of the city: the vertical transfers. In those stations (eg: Vall d'Hebron, Fondo, Zona Universitària, Collblanc), a big shaft was built in order to fit either high-capacity lifts or series of escalators to reach the platforms.</p>
                <p>The classic layout of metro stations in Barcelona is simple: on level -1 there is one or two mezzanines and in level -2 there are two side platforms, but since 2000 the new stations tend to have a central plafrom instead. In transfer stations, there is tipically a corridor linking the mezzanines of both lines.</p>
                <p>Mandatory to mention the so-called Barcelona solution or Spanish solution: stations with two tracks and three platforms, where passengers alight using one platform and board using the opposite one, improving the flow of passengers and reducing the dwell time. Barcelona was not the first city to implement it, but it was named after most of the stations built in the 30s, 40s and 50s were designed with such layout.</p>
            </div>
            <div id="com_Berlin"> 
                <h5>Berlin</h5>
                <p>The U-Bahn and the S-Bahn are the two rapid transit systems of Berlin. The city-state owns the first and the German railways own the second. The S-Bahn normally runs overground or elevated and the U-Bahn does it underground, as the name suggests in German, but this is not a norm since some parts of the U-Bahn are elevated and vice versa.</p>
                <p>The oldest lines were built either in elevated sections or underground, but very close to the surface, creating a direct connection between the street and the platforms in most of the cases. The lines built after World War I are a bit deeper, in order to fit mezzanines and corridors between the street and the platform levels.</p>
                <p>Transfer stations have a very easy tolopolgy, since stations are located close to the ground level and island platforms are predominant.</p>
            </div>
			<div id="com_Bilbao"> 
                <h5>Bilbao</h5>
                <p>Despite the metro opened in 1995, its layout follows former metric-gauge rail lines that were converted to metro, such as the Bilbao - Plentzia line. Metro Bilbao operates lines 1 and 2 and Euskotren operates line 3, together with other suburban and regional services.</p>
                <p>Due to the orography of the city, stations may have different layouts. The stations located in the city center are deep, since the Nerbion river is located nearby and there are plenty of hills located in the meanders of the river. The stations of the line 1 branch are located on surface, since they were part of the former Plentzia railway. </p>
                <p>Line 3 benefits from having already existing stations, such as Bilbao-Aduana (currently called Zazpikaleak/Casco Viejo) and Matiko, both part of the former Plentzia line, despite the connection between those two stations is made through a new double-track tunnel.</p>
				<p>Some transfer stations have effective designs, such as Lutxana or the former Bolueta station.</p>
            </div>
            <div id="com_Boston"> 
                <h5>Boston</h5>
                
                <p>All the rapid transit lines in Boston are centenary. Three of them originally opened as underground tramway tunnels in downtown Boston, but only two were converted to pure metro lines. The Green line is still an underground tram line.</p>
                <p>Most of the downtown stations are quite unusual, as they were built using early construction methods and most were designed to serve as tram stations. For instance, some stations in the Orange line have offset platforms. At State station, the Orange line platforms are in different levels.</p>
                <p>Underground stations tend to be located close to the surface. This can be appreciated at Government Center, where the Green line platform has a very particular shape because the streets located above ground were quite narrow when the line was constructed, and therefore impossible to dig wider tunnels.</p>
                <p>Despite being a city where the rapid transit lines were built without a network plan, transfer stations have a good layout, providing short and quick connections.</p>
            </div>
            <div id="com_Brussels"> 
                <h5>Brussels</h5>
                <p>The Brussels metro is a good example of a planned system. It was planned in the 60s, and the four existing lines were built following this criteria: if for practical reasons, the section to be opened was not long enough to be operated as a metro line, the section would open provisionally as a premetro (underground tram), with temporary low floor platforms and temporary ramps connecting the streets with the tunnel. Once the construction of the line is advanced, the premetro operation is cut and the line is converted into a full metro line, removing the ramps and elevating the platforms to metro standards. Currently lines 1, 2, 5 and 6 are fully metros and lines 3, 4 and 7 are premetros.</p>
                <p>The whole network has been built following the same plan. Transfer stations have a good design as well, with the notable exception of Dè Brouckère, where there is a long corridor. Cross-platform transfers can be found in Beekkant and Gare du Midi. Arts-Loi, Montgomery and Rogier have crossing interchanges.</p>
                <p>Most stations have side platforms located at level -2, whilst level -1 is reserved for mezzanines. Stations on line 6 have island platforms because trains run on the left side instead. Stations on lines 3/4 were built using the Spanish solution, where passengers board using the island platform (except in Gare du Midi).</p>
                <p>In order to reduce fare evasion, most stations have been equipped with turnstiles during the last decade, replacing the existing honor system which is still present in some stations due to the complexity of its layout.</p>
                
            </div>
            <div id="com_Budapest"> 
                <h5>Budapest</h5>
                <p>Budapest was the first city in continental Europe to have a metro. Line 1 opened in 1896. Similar to other systems opened around 1900, tunnels have a narrow profile. Trains are only 2.60 m high and 30 m long.</p>
                <p>Lines 2 and 3 have a similar style to other Eastern European metros, despite the decoration has nothing to do. Line 4 was built recently. In these three lines, tracks are laid in two parallel tunnels that cross Budapest at a considerable depth. Stations have island platforms in two parallel galleries, which are connected with the ticket hall through a long escalator. The ticket hall can either be located at ground level or just underground. In the latter case, there are direct staircases connecting the hall with the tramway platforms or bus stops.</p>
                <p>The northernmost and southernmost parts of line 3, tracks run underground but at a very little depth. Here stations have side platforms, which are located at level -2 (and a mezzanine at level -1) or directly at level -1, together with an independent booking hall for each direction.</p>
                <p>Transfers differ, depending on the station. In Deák Ferenc Tér and Kálvin Tér, transfers consist in a quick connection through two escalators and a short corridor. In other stations, transfers may be longer since one line might be running close to the surface while the other may be running very underground (eg: Keleti pályaudvar, Batthyány tér)</p>
            </div>
            <div id="com_Bucharest"> 
                <h5>Bucharest</h5>
                <p>The Bucharest metro consists of 5 lines, despite lines M1 and M3 share their tracks and line M5 has two branches. The original plan from the 70s consisted of three lines: a east-west line (M3 and southern section of M1), a north-south line (M2) and a circle line circa 40 km long.</p>
                <p>Unlike other Eastern European metros, the Bucharest one was not built at a considerable depth but using cut-and-cover methods. The platforms are typically located at level -2 or -1 and tend to be central, despite some stations in line M1 have side platforms. </p>
            </div>
            <div id="com_Buenos Aires"> 
                <h5>Buenos Aires</h5>
                
                <p>The Subte is the oldest metro network in Latin America. It consists of 6 lines. 4 of them run east-west (A, B, D, E) and the other two are north-south (C, F).</p>
                <p>The entire network runs underground and except for line H, the tunnels were build close to the surface. Most of the stations have side platforms but those stations that are or had been terminus usually have island platforms.</p>
                <p>The lower level of an average station comprises the platforms. If there is enough space between the platform level and the street level, a mezzanine can be found, containing the ticket booths and turnstiles. In the opposite case, each platform has independent accesses and turnstiles are located at the platform level and there are no overpasses or underpasses.</p>
                <p>The Subte applies a bizarrenaming criteria. Station names are independent for each line. So, a transfer station will have multiple names, one for each line calling at it (eg: Pueyrredón (B) and Corrientes (F) are part of the same station). But there is also the opposite situation, where there are stations with the same name in different lines: Callao and Pueyrredón are four stations located in lines B and D, but they have nothing to do.</p>
            </div>
            <div id="com_Copenhagen"> 
                <h5>Copenhagen</h5>
                <p>Copenhagen has a modern and automatic metro. Both the Metro and the S-tog have an honor system and most underground stations look like the same: mezzanine with vending machines at level -1, a landing at level -2 and island platforms at level -3. The connection between the platform and the mezzanine is provided by lifts and escalators. Upbound escalators are separated from the downbound ones.</p>
                <p>Transfer corridors are connected to mezzanines in Frederiksberg and Kongens Nytorv, but not in Nørreport, where the corridor heading to the S-tog starts at the end of the Metro platform.</p>
            </div>
            <div id="com_Frankfurt"> 
                <h5>Frankfurt</h5>
                <p>Despite Frankfurt has officially a U-Bahn (<em>pure</em> metro), technically is a Stadtbahn (light rail), despite the platform level is high even for stops located at the street. The S-Bahn is also part of the basic rail network of the city.</p>
                <p>The main transportation hubs were desinged to provide quick transfers between the rail lines. Most transfer combinations involve passing through a staircase or an escalator, but in Hauptwache, a cross-platform is provided for transfers between S-Bahn and U-Bahn lines U6 and U7.</p>
            </div>
            <div id="com_Glasgow"> 
                <h5>Glasgow</h5>
                <p>Glasgow has a very peculiar subway. It opened in 1896 as a circle line powered by cable, like the San Francisco cable cars. The subway was fully renovated between 1977 and 1980 in order to change its operation to electric power, build new workshops, relocate stations and refurbish the rest of stations.</p>
                <p>Before the renewal, all stations had a very narrow island platform. A staircase located at the end of one platform lead to the ticket hall and the exit, typically located inside the station building. Most of the stations retain this layout today. However, escalators have been installed in some stations, to connect the ticket hall with the street. The stations with the highest patronage were completely rebuilt: Buchanan Street has a central and a side platform nowadays. In St Enoch, two side platforms replace the former island platform. Partick station was built during the renewal works in order to allow transferring from the Subway to ScotRail.</p>
            </div>
            <div id="com_Hannover"> 
                <h5>Hannover</h5>
                <p>Hannover has a <i>Stadtbahn</i>, which is a mixture of a tramway that runs in a metro-like tunnel in the city centre.</p>
                <p>There are three trunk tunnels in Hannover (A, B, C) that meet in Kröpcke station. Tunnels B and C run overlapped between Kröpcke and Aegidientorplatz.</p>
                <p>The layout of an average station is simple and similar to the ones in other cities: a mezzanine at level -1 and side platforms at level -2.</p>
                <p>Kröpcke station is a huge complex with lots of accesses, escalators, staircases and mezzanines, since is the only point where the three tunnels meet. Hauptbahnhof station (main station) has elevated rail platforms and an underground station for the Stadtbahn, composed of two island platforms and four tracks. Aegidientorplatz station has a peculiar layout, since the mezzanine is at level -1, the northbound platform is at level -2 (with 2 tracks) and the southbound platform is at level -3 (with 2 tracks as well).</p>
            </div>
            <div id="com_Lyon"> 
                <h5>Lyon</h5>
                <p>The four lines of the Lyon metro are technically different. Lines A and B and D are rubber tired. Line A has manual conduction and line D was already opened with automatic trains. Line B switched from manual driving to automatic operation in 2022.</p> 
                <p>Line C partially follows the route of a former funicular that had a pent of 176‰. Therefore it operates as a rack railway between Hôtel de Ville and Croix Rousse, since normal metro lines have a maximum pent of 40‰.</p>
                <p>Lines A and B run at a very little depth. Even line A crosses the Rhone using the box girder of Pont Morand. The stations of this line only have one underground level, with side platforms and turnstiles on it. Some stations have underpasses, connecting both platforms. Line D tunnels are not located deep either, but most stations have a mezzanine between the street and platform levels. The latter situation also occurs in the eastern section of line A.</p>
                <p>Lyon had an honor system until two decades ago. Since some staion have secondary accesses connecting the streets with the platform, it is common to find turnstiles in the platforms.</p>
                <p>Most of the transfer stations have an efficient layout, such as Hôtel de Ville, Saxe-Gambetta or Charpennes. However, the links between at the two main railway stations (Part-Dieu and Perrache) are long.</p>
            </div>
            <div id="com_Lisbon"> 
                <h5>Lisbon</h5>
                <p>Currently the Lisbon metro consists of four lines. Until the mid-90s, the network had a Y-shaped single line. This line was split into three (Blue, Yellow and Green) and the Red line was built as a completely new line. Therefore, all the internal metro transfers are less than 25 years old.</p>
                <p>The layout of the stations differs slightly: Baixa-Chiado and Campo Grande have parallel platforms for the two lines serving the stations. Saldanha, São Sebastião or Oriente allow quick transfers that involve climbing stairs or elevators. However, in Alameda or Entre Campos the transfer consists in a long passageway.</p>
                <p>The standard layout for the stations built before the Carnation Revolution consisted in two side platforms at level -2 and a mezzanine at level -1. These stations featured short platforms, that were extended years later to allow 6-car trainsets. Together with these platform extensions, secondary mezzanine and accesses were added. The stations built recently tend to have a single mezzanine and are located at deeper, since they were built in hilly areas.</p>
            </div>
            <div id="com_London"> 
                <h5>London</h5>
                
                <p>The London Underground (or the Tube) is the oldest in the world and it consists of two different networks: the sub-surface lines (running close to the surface) and the deep tube lines (runing deep, with tunnels that ressemble a tube).</p>
                <p>The sub-surface lines are the ones inherited from the Metropolitan Railway and from the District Railway, two lines built using cut-and-cover methods or running elevated or at ground level, which were steam operated until the early 20th century. Stations consists in a building located at ground level, containing the ticket offices and fare gates, and the platforms placed at level -1. The stations were located inside a block of houses whenever possible and most of them had a canopy (still in place in Paddington, Bayswater or Earl's Court). With the electrification of the tracks and real estate speculation, most of the stations have been covered with concrete labs (such as in Gloucester Road or Mansion House).</p>
                <p>The deep tube lines began to be built in the last decades of the 19th century, when the boring methods were a bit developed. They were already planned to be operated with electric trains. The tunnels were bored at a depth of 20 m and until the construction of the Victoria Line, they followed the streets. Stations consists of a building hosting the ticket offices and the fare gates, which are connected with the platforms (most of them are island platforms) via lifts and spiral staircases located inside of shafts. In 1913 an escalator was installed at Earl's Court as part of a test to replace lifts. Since the result was favorable, the practice of building stations with lifts and spiral staircases in shafts was abandoned. From this moment on, the new stations were built with escalators, and the existing ones (especially those with a high ridership) were transformed.</p>
                <p>Initially each line was operated by a different company, so transfers between lines weres not granted. This was corrected after the nationalisation of the Tube.</p>
                <p>Some stations host huge flows of passengers. This obligated the local authorities to build wider or newer passageways and escalators in some transfer stations. That is the reason some transfer stations have one-way corridors as well, such as in Oxford Circus or Victoria.</p>
            </div>
            <div id="com_Madrid"> 
                <h5>Madrid</h5>
                <p>technically, the Madrid metro lines can be divided into two: the narrow profile lines (1-5 and the Ramal) and the wide profile lines (6-12). The narrow profile lines were the first to be built and were inspired by the Paris metro. That is why old stations have side platforms located at level -2 and one mezzanines at level -1. Decades after the opening, lines 1 and 3 got their platforms extended from 60 m to 90 m long and secondary mezzanines and accesses were added.</p>
                <p>The lines with a wider profile were built with the aim of fitting larger trains and larger stations. Moreover, these lines were built a highest depth in comparison to the narrow profile lines, at a depth of 15 to 25 m below surface level. The streets and the platforms are connected through 3 to 5 series of staircases and escalator, with the ticket hall in between. Therefore, access times are longer compared to narrow gauge lines.</p>
                <p>Since the 90s, the design and layout of new stations have been standadrised, with stations being built using cut-and-cover methods with slurry walls.</p>
                <p>Madrid has plenty of different layouts for transfer stations. In comparison with other cities, Madrid has a great amount of stations containing long corridors, but not at the level of Barcelona.</p>
                <p>Possibly the most prominent stations are the macrohubs built in the first decade of the 2000s, with huge mezzanines, wide staircases and lots of lifts and escalators, allowing quick connections between metro, commuter train and interurban buses as well. The best example is Nuevos Ministerios, but Chamartín, Sol, Príncipe Pío, Plaza de Castilla or Moncloa also need to be mentioned.</p>
            </div>
            <div id="com_Marseille"> 
                <h5>Marseille</h5>
                <p>The Marseille metro is formed by two lines that cross themselves in Saint-Charles and Castellane, both located in the city centre. The links between lines in these two stations are quick, since in Saint-Charles line 2 is between the two tracks of line 1 and in Castellane both lines have their platform very close to the intersect point.</p>
                <p>There are three kinds of stations, according to their layout: the suburban, which are elevated (eg: Bougainville, Ste-Margueritte, La Rose); the underground ones close to the surface, with the ticket hall at level -1 and side platforms at level -2 (eg: Baille, Périer); and the deep centric stations, which have island platforms and a single access consisting of long escalators connecting the street level with the mezzanine (eg: Cinq Avenues, Estrangin).</p>
                <p>Noailles station is the most particular since the former tram tunnel was diverted when metro line 2 was built. Currently the former tunnel serves as a passageway, and the former tram terminus is a ticket hall.</p>
            </div>
            <div id="com_Milan"> 
                <h5>Milan</h5>
                <p>Line 1 opened in 1964 and was the first metro line in the world to be built using slurry walls. Line 2 opened four years, following the same design standards for line 1. Stations have a very functional layout, since passenger flows were seriously taken into account. Almost all stations have one-way staircases connecting the side platforms (located at level -2) and mezzanines (at level -1): one for passengers entering and another for passengers alighting.</p>
                <p>The same concept was latter aplied to lines 3 and 5, opened decades later, despite both having a radically different design in their stations, compared to lines 1 and 2. Most of their stations also have mezzanines at level -1 and side platforms at level -2.</p>
                <p>Engineers opted to superimpose the tracks in the central section of line 3, where tracks follow the narrow streets of the old city. As a result, the stations of this sections are a maze of lifts, one-way staircases and escalators, combined with a peculiar decoration.</p>
                <p>The transfers are typically short and quick, especially in Loreto, Centrale, Cadorna and Repubblica. The only two stations having long corridors are Lotto and Porta Venezia.</p>
            </div>
            <div id="com_Paris"> 
                <h5>Paris</h5>
                <p>The Paris Metro has the most labyrinthine interchanges in Europe.</p>
                <p>Almost all the metro network was opened before World War 2. The first transfer stations, opened in the early 1900s, had simple transfers, with bidirectional passageways. Since 1920, the company responsible for the metro began to install <em>portillons automatiques</em> (automatic doors) in some stations. The <em>portillons automatiques</em> are doors located at the entrance of a platform, that blocks the access to it once a train enters the station, in order to reduce the dwell time and prevent last-second passenger boarding the trains. Therefore, in order to limit the access to platforms but not the exits, they decided to build two-directional corridors and staircases. That is how the stations began being underground mazes.</p>
                <p>In the 1970s the RER (suburban railway) was opened. This meant that some metro stations had to be partially rebuilt.</p>
                <p>Taking the advantade that the spacing between stations is one of the lowest in the world, some RER stations were placed between two metro stations, so both could be connected with the RER. The most extreme case is that 6 metro and RER stations are connected underground (St-Augustin, St-Lazare, Haussmann-St-Lazare, Havre-Caumartin, Auber and Opéra).</p>
                <p>The average metro station in Paris consists of a mezzanine at level -1 and two side platforms at level -2. The termini of the oldest lines used to have a loop (Place d'Italie M5, Étolie M6, Nation M6, Porte Dauphine M2). Some of these stations were actually doubled, since there was a station for the trains that terminate there and another one for the trains (and passengers) beginning their journey.</p>
            </div>
            <div id="com_New York"> 
                <h5>New York</h5>
                
                <p>The New York subway combines an extensive network of elevated trains (in Brooklyn and in the Bronx) with an underground train network that was built in the first half of the 20th century, especially in Manhattan. The subway links Manhattan with the other boroughs with tunnels or via the well known bridges, such as the Williamsburg Bridge or the Manhattan Bridge</p>
                <p>A characteristic feature of this subway is the existence of local and express services in separate but parallel tracks, so a tunnel can have 4 tracks (2 per direction and 2 for each type of service). The express trains run in the central tracks and the local trains run in the side ones. </p>
                <p>The elevated sections have stations placed above the streets and they can be accessed via staircases located in the sidewalks. Ticket halls are located at level +1, inside the viaduct, and platforms are at level +2, partially covered by shelters. The stations with express services tend to have central two platforms.</p>
                <p>The underground lines run very close to the surface and they have plenty of piles of steel between the tracks and in the platforms. The stations that are served only by local services have side platforms at level -1, together with the ticket halls. Platforms are not linked via an underpass or an overpass. Express stations have two island platforms at level -2, allowing cross platform transfers between the local and express trains. In this kind of stations, mezzanines with the fare gates are located at level -1.</p>
            </div>
            <div id="com_Prague"> 
                <h5>Prague</h5>
                <p>The three lines of the Prague metro form a radial network. All the lines cross at three selected stations in the city center. Almost the totality of the network is underground. The first line that was built (C) is the one running closer to the surface, especially at the city center and in the southern section. Lines A and B run much deeper.</p>
                <p>The deep stations have island platforms at the deepest point of the station. The ticket hall, which is located beneath the street level, is connected with the platforms through escalators. A few stations even have two mezzanines, one at each end of the station.</p>
                <p>Line C has not very deep stations at the southern part. They are composed of a island platform at level -1 and two ticket hall buildings located at ground level, each one at a different end of the platform.</p>
                <p>At Můstek i Florenc, both metro lines run at a considerable depth, so both stations are linked through a rather short corridor, located at a level between both metro lines. Hoewever, Muzeum station has line C located close to the surface, whilst line A station is deep. Both lines are connected through a series of escalators.</p>
                <p>Most of the metro stations have shops and stores in the ticket halls.</p>
            </div>
            <div id="com_Rome"> 
                <h5>Rome</h5>
                <p>The Rome metro has few lines compared to the extent of the city. Until a few years ago, there were only two lines that intersected at Termini station, which is the central railway station as well. Because of it, Termini station is the one with the highest ridership of the city and most of the times it gets crowded. This station has one-way corridors for entering, exiting and even for transferring. A decade ago, the city had to build an extra set of escalators and corridors to decongest the existing link connecting lines A and B.</p>
                <p>The other interchange station is San Giovanni. Line C opened in 2018 but the final passageway between both lines is not completed yet. Currently, passengers need to exit through the faregates and enter again.</p>
                <p>The stations of the southern section of line B were opened in the 1950s and are very simple. The construction of the northeastern section of line B and the entire line A is much more recent and their stations were designed with one-way staircases and passageways.</p>
            </div>
            <div id="com_Rotterdam"> 
                <h5>Rotterdam</h5>
                <p>The Rotterdam metro has 5 different services that can be grouped into two lines. One of these services even arrives to the city of the Hague. The design of the stations is rather simple. For stations located underground, the layout consists of a mezzanine is located at level -1 and two side platforms at level -2. Elevated stations tend to have a island platform at an upper level and a mezzanine at the street level.</p>
                <p>Blaak and Beurs have layouts were two lines cross, one over the other. The connection is provided via direct staircases linking both platforms.</p>
            </div>
            <div id="com_Sao Paulo"> 
                <h5>São Paulo</h5>
                
                <p>The metropolis of São Paulo has a rapid transit network operated by three different companies. This is a relatively new metro (it was opened in 1968, despite some lines were converted from existing railways), with a layout and design thought to hold huge flows of passengers. Trains can be up to 200 m long.</p> 
                <p>Stations are very wide. Some of them, such as Sé or Luz (line 1) have the Barcelona solution, in order to ease the passenger flows.</p>
            </div>
            <div id="com_Saragossa"> 
                <h5>Saragossa</h5>
                <p>Saragossa does not have metro but the commuter line C-1 has an urban section with three underground stations. This section was covered when the high speed rail line arrived to the city.</p>
                <p>All the stations have a building at street level that hosts vending machines and the fare gates. Platforms are located at level -1, just under street level.</p>
                <p>Miraflores station has a temporary access to a car parking, since the urbanisation of the street over the rail line has not been finished and the station building cannot be used. This station also has an underpass to reach the secondary platform.</p>
                <p>Goya station has connection with the tramway, at Fernando el Católico station. The connection has to be performed by crossing two zebra crossings.</p>
            </div>
            <div id="com_Valencia"> 
                <h5>Valencia</h5>
                <p>The Valencia metro is actually the merge of different narrow gauge suburban railways that were connected via a tunnel crossing the city center. So this system is somehow the hybrid of a premetro and a rail, since in the city of Valencia is like a metro, but in the suburbs this works like a rail with lots of level crossings and single track sections.</p>
                <p>The vast majority of underground stations have one or two mezzanines at level -1 and two side platforms at level -2. There are some stations with a different layout compared to the others, especially in lines 3 and 5: Bailén, Avinguda del Cid, Àngel Guimerà and Colón have island platforms. Alameda has four tracks and three platforms (2 for the Rafelbunyol line and 2 for the Marítim line).</p>
                <p>Xàtiva station has a radically diferent layout, as the platforms are overlapped because there is the juntion of the line towards Bailén and Jesús just 20 m after the platform end (on the eastern side).</p>
            </div>
            <div id="com_Warsaw"> 
                <h5>Warsaw</h5>
                <p>Line 1 has two types of stations: the ones located in the suburbs, having side platforms at level -1 and ticket halls at street level, and the ones located in the city centre, having a island platform at level -2 and a mezzanine at level -1.</p>
                <p>The stations on line 2 are similar to the ones with a island platform on line 1, with the difference that the platforms are located a bit deeper.</p>
                <p>There are two main transfer stations. Świętokrzyska is the crossing point of both metro lines and the transfer is quick. Half a kilometer to the south, there is the hub consisting of Centrum metro station, Śródmieście suburban rail station and the Central rail station. The latter two form a huge underground complex, combined with a shopping mall. The connection of Centrum and Śródmieście stations is done at street level.</p>
            </div>
            <div id="com_Vienna"> 
                <h5>Vienna</h5>
                <p>Part of the current metro network (U-Bahn) is inherited from a primitive urban rail network known as Stadtbahn that was steam-powered until the 1920s. Once the network was electrified, the rolling stock used was similar to the ones used for the tramways. Most of its network was elevated or ran at a street level, but without level crossings.</p>
                <p>During the 60s, the municipal government decided to build a couple of tram tunnels. At the same time, they also planned a metro network consisting of a basic network, with parts of it coming both from the Stadtbahn (U4) and from the newly tram tunnels (part of current U2), asides from the construction of line U1. In the 1980s the second phase took place: line U3 was built and line U6 was integrated from the Stadtbahn.</p>
                <p>The layout of the stations depends on the time they were built. The non-underground stations that were part of the Stadtbahn have side platforms and the access is made via the station buildings, which are in the Art Nouveau style. The stations that were part of the tram tunnel are located under street level and are quite simple: offset side platforms at level -1. The new stations built from the 1970s on typically have a island platform at the lower level and two mezzanines at the upper level, which can be either on street level or below, and is connected with the platforms via escalators or elevators. The depth of the latter stations is variable, but the ones located in the city center may be very deep.</p>
                <p>All the transfer stations have been built after 1970 and their layout reflects the idea of having efficient links. Except for Praterstern, all stations have quick transfers.</p>
            </div>
            <div id="com_Oslo">
                <h5>Oslo</h5>
                <p>The current Oslo T-bane system originated from an old network of suburban trams that developed in the western suburbs of the city during the first half of the 20th century, and a modern metro system built from the 1960s onwards in the eastern part of the city. Both networks were progressively merged and unified between the 1970s and the 2010s.</p>
                <p>The Oslo metro runs mostly elevated or on the surface, but there are also shallow underground sections. Surface stations usually have two side platforms. Access to the platforms is open, using an honor system, and access to the platforms is performed directly from street level through ramps. In some places, there is a Narvesen kiosk that serves as both a ticket reseller and a convenience store.</p>
                <p>The underground stations are located either in the city center or in the northeastern suburbs. Most stations are situated at a shallow depth and have two side platforms at the deepest level. At the immediately higher level, there is a lobby that can be either at street level or underground. The connection between levels is made through ramps and staircases.</p>
                <p>There are a few stations that are situated at considerable depth: Romsås, Ellingsrudåsen, Stortinget, Nydalen, and Vestli. Access to the first two is only possible through large-capacity elevators that connect the street to the concourse, in addition to a ramp of over 200 meters in length that also links the street to the concourse level.</p>
            </div>
            <div id="com_Gothenburg">
                <h5>Gothenburg</h5>
                <p>The Gothenburg tram has an underground station. Hammarkullen is located at a considerable depth, in a hilly suburb in the northeastern part of the city. The tracks pass through a dual tube tunnel. Trams run on the left only in this specific part, as the station has island platforms and the rolling stock only has doors on the right side. From the platform level, there is a flight of escalators and an inclined elevator that leads to the lobby, located at street level.</p>
            </div>
            <div id="com_Hamburg">
                <h5>Hamburg</h5>
                <p>Hamburg has two rapid transit systems: the U-Bahn and the S-Bahn. The U-Bahn, which is the proper metro system, is one of the oldest in Europe. The first circular line was constructed combining elevated, at-grade and shallow underground sections. Subsequent extensions have also been built alternating these three typologies. However, the central section of U2 and the branch of U4 were constructed at a greater depth.</p>
                <p>In general, the underground stations are quite shallow and have a island platform (side platforms in the original U3 stations). Some stations have an intermediate level serving as a lobby. On the other hand, the elevated stations have a street-level building (where the lobby is located) and a island platform or two side platforms at level +1.</p>
                <p>The S-Bahn uses sections of the German railway network. The construction of the City S-Bahn (an underground line through the city center) started in the 1960s. Their stations have large island platforms located at level -2, generally with lobbies at level -1, standing at the ends of the platforms.</p>
                <p>As for interchanges, some have been designed to facilitate cross-transfer connections (Barmbek for U3-U3 connections, Wandsbek-Gartenstadt, Berliner Tor, Altona, Hauptbahnhof between S-Bahn trains, Norderstedt Mitte), with parallel platforms (Ohlsdorf, Barmbek for U3-S1 connections, Hauptbahnhof Süd, Elbbrücken) or with direct transfers via a single flight of stairs (Schlump, Jungfernstieg).</p>
                <p>There are other transfers that were poorly designed, such as the long passageway between Rathaus and Jungfernstieg, or the street-level transfer at Wandsbeker Chaussee, Dammtor/Stephansplatz, or Sternschanze.</p>
            </div>
            <div id="com_Essen">
                <h5>Essen</h5>
                <p>Essen's Stadtbahn has two trunk lines. The one that runs through Rathaus station is a group of tram lines that run underground in the city center, with tram vehicles and low-level platforms. The lines that pass through Hirschlandplatz are light metros.</p>
                <p>There is a variety of station typologies. The shallower stations usually have side platforms, while the trunk line stations of the light metro have island platforms, which are situated at a considerable depth.</p>
            </div>
            <div id="com_Dortmund">
                <h5>Dortmund</h5>
                <p>Dortmund's Stadtbahn consists of three trunk lines located in the city center. The east-west lines are operated with low-floor trams, while the other two are operated with high-floor light metros. Most stations are located at a shallow depth. The station vestibules are either at street level or at an intermediate level between the street and the platform.</p>
                <p>In Dortmund, there is also a monorail system (H-Bahn) that serves the Technical University of Dortmund (TU Dortmund). It consists of two lines. The track is single-track, but there are some stations with passing loops. All stations are elevated or at-grade.</p>
                <p>Bochum's Stadtbahn consists of a north-south line, as well as several tram lines that run through underground sections in the city center. Most Stadtbahn stations have island platforms and vestibules located at an intermediate level between the street and the platforms. In some stations, the platforms are situated at a higher depth, such as Rathaus Nord and Hauptbahnhof.</p>
            </div>
            <div id="com_Bochum">
                <h5>Bochum</h5>
                <p>The Bochum Stadtbahn consists of a north-south line, as well as several tram lines that run through underground sections in the city center. Most Stadtbahn stations have island platforms and entrances located at an intermediate level between the street and the platforms. In some stations, the platforms are situated at a higher depth, such as Rathaus Nord and Hauptbahnhof.</p>
            </div>
            <div id="com_Mülheim">
                <h5>Mülheim</h5>
                <p>The Stadtbahn stations in Mülheim are different. The stations that are close to the Ruhr have platforms at a considerable depth to allow the tunnels to pass underneath the river. The remaining stations have a typical configuration of any Stadtbahn network in Germany, with a vestibule at level -1 and platforms (either side or central) at level -2.</p>
                <p>The Hauptbahnhof station is an intermodal hub that consist of elevated rail platforms, four underground Stadtbahn tracks with two island platforms and two underground bus platforms parallel to the Stadtbahn ones.
            </p></div>
            <div id="com_Duisburg">
                <h5>Duisburg</h5>
                <p>All Stadtbahn and underground tram stations in Duisburg have island platforms. These platforms are usually located at level -2, while level -1 is reserved for the lobbies.</p>
                <p>There are two stations with a particular layout: Hauptbahnhof and König-Heinrich-Platz, which are the transferring points for the north-south and east-west lines. These stations have two parallel platforms, stacked on different levels (-2 and -3). Different combinations of escalators and staircases allow for transfers, access, and exit from the station.</p>
            </div>
            <div id="com_Düsseldorf">
                <h5>Düsseldorf</h5>
                <p>Düsseldorf has two Stadtbahn main lines (north-south and northeast-southeast), as well as an underground tram main line. Both Stadtbahn lines share tunnels between the central station (Hauptbahnhof) and the city center (Heinrich-Heine-Allee), with tracks running parallel to each other. At Hauptbahnhof and Heinrich-Heine-Allee, the platforms serving the four tracks are located at the same level. However, at the intermediate stations, platforms are overimposed, being the northbound one on the upper level.</p>
                <p>On the other hand, the underground tram stations have low side platforms at level -2 and a mezzanine at level -1.</p>
            </div>
            <div id="com_Turin">
                <h5>Turin</h5>
                <p>Almost all metro stations in Turin follow a standard design consisting of two side platforms at level -3 and a lobby at level -1. The connection between the lobby and the platforms is made through escalators that go directly from the platforms to the lobby, elevators, or two flights of stairs.</p>
                <p>Porta Nuova is the central rail station and has a different layout: the lobby is located at level -1 and platforms are at level -2, which can be reached by stairs or an elevator. There is a direct connection from the metro mezzanine to the railway station original hall, which is now part of a shopping center.</p>
                <p>Porta Susa station has metro platforms at level -4, an intermediate level at -3, and an open hall located inside the large canopy that constits the railway station building. The railway station itself consists of three levels, with the eastern part of the station featuring a huge glass roof canopy measuring 380 meters long by 30 meters wide. Level -2 also contains the four rail platforms. On level -1 there are four passageways that connect the western accesses to the station and act as an intermediate level to connect crowds coming from the different platforms.
            </p></div>
            <div id="com_Lausanne">
                <h5>Lausanne</h5>
                <p>The Lausanne metro consists of a light metro line (m1) and an automated rubber-tired metro line (m2), which originated from a funicular that was later converted to rack railway operation.</p>
                <p>The stations on the m1 are either locatred underground or at street level. Some stations have only one platform with direct access to the street, as the track is single-track. Stations with passing loops usually have two side platforms. The terminal station at Renens CFF is part of the Swiss Federal Railways station.</p>
                <p>On the other hand, the stations on the m2 line are more varied. All stations have side platforms, except Sallaz. Some have platforms are located at street level, like Ouchy or Grancy. Others are very swallow and can be accessed via a staircase or ramp, such as Délices or CHUV.</p>
                <p>Finally, some stations have a unique layout due to the topography of the old town, particularly the deep and narrow Flon Valley. Flon station is a main hub. It is located at Place de l'Europe. The m1 station is at the same level as the square, while the LEB (railway) and m2 stations are at level -2. From level -2, five elevators ascend to the height of the Grand Pont, located about twenty meters above.</p>
                <p>Similarly, the Riponne station is situated in the Flon River Valley. The station is located at an intermediate level between the street following the valley and the Riponne Bridge. Four elevators (two per platform) connect the three levels.</p>
            </div>
            <div id="com_Porto">
                <h5>Porto</h5>
                <p>The Porto metro is a newly constructed light metro. It has two trunk lines: one north-south and another east-west, with a section that follows the path of the former narrow-gauge lines that departed from Trindade, which now serves as a transfer station between the two trunk lines.</p>
                <p>The surface stations have the typical design of a modern tram station, with shelters and pedestrian crossings at the ends of the platforms to switch platforms or exit the station.</p>
                <p>The underground stations have side platforms at the deepest level, but there are differences in the layout of intermediate levels and the location of the booking hall.</p>
            </div>
            <div id="com_Munich">
                <h5>Munich</h5>
                <p>The Munich U-Bahn is a metro system formed by 6 different lines, plus two that only operate during rush hours. The network has lots of spurs, but in the city center all those lines merge and form 3 trunk lines, in addition to an east-west trunk line of the S-Bahn, which, due to its frequency and performance, can be considered a metro, just like in Berlin and Hamburg.</p>
                <p>Most stations have island platforms located at level -2 and a couple of lobbies located at level -1, just above the ends of the platforms. Any station has turnstiles: a honor system is used instead. In some specific stations the platforms may be located at a greater depth, which has required the installation of long escalators, like the ones in the central part of the U4 and U5.</p>
                <p>One of the characteristics of the Munich metro is the presence of cross-platform transfers at many stations, with coordination on the arrivals and departures. Some examples include Scheidplatz, Innsbrucker Ring or Neuperlach Süd. Additionally, some other stations with terminating trains usually have three or four tracks, such as Hauptbahnhof (U1-U2), Münchner Freiheit, Olympiazentrum, Kolumbusplatz, Fröttmaning, or Implerstraße. Some of these stations have elevators that provide direct access between the platforms and the street.</p>
                <p>The most centric S-Bahn stations use the Barcelona solution. At Hauptbahnhof and Karlsplatz, passengers board using the island platform and alight via the side platforms; while at Marienplatz, due to narrow streets, the two tracks are overlapped: passengers aight from the train using the right hand side in the direction of travel, and board through the opposite doors. Along with the two levels dedicated to the S-Bahn (-2 and -3), there is also the U-Bahn station located at level -4, whose platforms are separated by nearly a hundred meters since the streets above are narrow and the City Hall building is inbetween.</p>
                <p>In other transfer stations, the line change is direct through a flight of stairs (Odeonsplatz, Hauptbahnhof, Sendlinger Tor) or through the vestibules in the case of most U-Bahn to S-Bahn transfers.</p>
            </div>
            <div id="com_Nuremberg">
                <h5>Nuremberg</h5>
                <p>The Nuremberg U-Bahn has three lines. Two of them, U2 and U3, share tracks in the central section and have automatic operation.</p>
                <p>The station layout for most of the stations is quite simple. First of all, the ticketing policy is based in an honor system, so there aren't turnstiles in any station. The platforms are always located at the lowest level of the station, usually at level -1 or -2 (if the station has a mezzanine at level -1). In some cases, there is direct access from the street to the platform. All stations have elevators.</p>
                <p>The two interchange stations between the U-Bahn lines are designed to provide quick transfers. At Hauptbahnhof, the U1 station is located one level below the U2 and U3 station, and only a short flight of stairs is needed to change. At Plärrer station, a cross-platform transfer is ensured in both directions, as the platforms of both lines are stacked.</p>
            </div>
            <div id="com_Stuttgart">
                <h5>Stuttgart</h5>
                <p>The Stuttgart Stadtbahn is quite complex, but its operation can be simplified into cross-valley lines and the valley lines. Outside the city center, tracks run on street level and most stations ressemble modern tramway stops. In the city center, stations are underground and have side platforms at level -2 and one or two lobbies at level -1.</p>
                <p>The stations in the central section of the S-Bahn have a island platform at level -2 and lobbies placed at both ends of the platforms, at an intermediate level between the platform and the street.</p>
            </div>
            <div id="com_Lille">
                <h5>Lille</h5>
                <p>The Lille metro consists of two VAL (light automated vehicle) lines. A VAL system is characterized by very narrow and short vehicles that operate at a high frequency in order to accommodate the demand. The narrow gauge is a direct consequence of minimizing tunnel construction costs.</p>
                <p>Initially, the metro operated on an honor system, so the stations were designed to allow easy connections between streets and the plaforms. The subsequent installation of turnstiles means that some stations may only have them installed in certain accesses, while others may have a weird distribution turnstiles.</p>
                <p>The main station of the network is Gare de Flandre, which is the central railway station and also a common station for both metro lines and tram lines, whose platforms are underground. This station provides a cross-platform transfer between the metro lines. Additionally, the transfer between the metro and the tram is also very fast. As a curiosity, the tram station has a platform exclusively for passenger alighting, since it is the last station.</p>
                <p>The other interchange station between the two metro lines is Porte des Postes, where the platforms of the two lines intersect at different levels, but transfers are quick.</p>
            </div>
            <div id="com_Stuttgart">
                <h5>Palma</h5>
                <p>The Palma metro is a recent construction. The section closest to the city center was built by taking advantage of the underground section of the Palma-Inca line at the entrance to the city. Between the Intermodal Station at Plaça d'Espanya and Son Costa - Son Fortesa, the metro tracks run parallel to the aforementioned railway line.</p>
                <p>The Estació Intermodal is the main transportation hub in Palma. There is an underground bus station located next to the underground railway and metro station. In fact, this hub is the starting point for all the interurban bus lines and rail lines originating from Palma.</p>
                <p>All the underground section was built very shallow. In fact, the stations located on Gran Via Asima have the same design, with platforms and lobbies located at level -1 and an underground passage at level -2 equipped with escalators and elevators.</p>
                <p>The Son Sardina station is located on the surface and provides an interchange with the Palma-Sóller line. The UIB station, on the other hand, has a street-level passenger building, although it does not host any facilities.</p>
            </div>
            <div id="com_Brescia">
                <h5>Brescia</h5>
                <p>The Brescia metro consists of a single line that opened in 2013. The rolling stock comprises Ansaldobreda’s automatic trains, also in operation on Milan metro lines 4 and 5. The line runs shallowly in the northern section, deeply in the city center, and on surface level and elevated through the eastern suburbs. An honor system is employed, eliminating the need for turnstiles.</p>
                <p>Some of the deepest stations, such as Marconi, Ospedale, Stazione FS, Bresciadue, or Volta, follow a standard design with two side platforms at level -3. These platforms are connected to the mezzanine at level -2 via a staircase and two independent escalators. From the mezzanine, there is access to an intermediate level (-1) and a further flight of stairs leading to the street.</p>
                <p>Shallow stations typically consist of two side platforms directly accessible from the street, while elevated stations feature an island platform.</p>
            </div>
            
            <div id="com_Istanbul">
                <h5>Istanbul</h5>
                <p>Despite having 10 lines in operation, the Istanbul metro is quite modern. Except for lines 1 and 2, built in the 80s and 90s, the rest of the network was constructed in the 21st century. Extensions are underway in both the European and Asian parts of the city. The tram network complements the metro, which incorporates turnstiles at each stop. Additionally, the public transportation network includes four funiculars, three of which are entirely underground.</p>
                <p>The metro lines exhibit significant differences in technical features, including train type, automation, track count, and electrification system. Construction methods for tunnels and stations vary from line to line, affecting station layouts. With the exception of line 1 which runs shallow or elevated, all lines run at considerable depths. Access and egress times are relatively high and some transfer layouts may result in long passageways or unnecessary multiple level changes. Moreover, transfers are not free, as passage through turnstiles is required.</p>
                <p>All lines were constructed with dual-tube tunnels at depths ranging from 20 to 45 meters below ground level, except for line 1, which is shallower, and line 6, which was built with a single track and with passing loops at each station. Some stations are located as deep as 70 meters, only accessible via elevators in certain cases.</p>
            </div>
            
            <div id="com_Malaga">
                <h5>Malaga</h5>
                <p>The Malaga metro comprises two lines operated with a tramway fleet. Line 2 is entirely underground. Line 1 is mostly underground, but the western section of line 1 was built as a light metro with level crossings. The underground sections are shallow, with most stations featuring a lobby at level -1 and an island platform at level -2.</p>
                <p>The common section is peculiar. Each line has its own tracks. The tunnel has two levels, with two tracks on each one. Between El Perchel and Guadalmedina stations, two tracks interchange their levels, in a section located within two curves. La Unión station on line 1 has also overlapped platforms. Atarazanas station only has a single platform.</p>
            </div>
            
            <div id="com_Seville">
                <h5>Seville</h5>
                <p>The Seville metro consists of a single line that began construction in the 70s but was not opened until 2009. The rolling stock comprises tramway vehicles.</p>
                <p>The central section, built in the 70s, features a dual-tube tunnel constructed using TBMs. Stations generally have a lobby at level -1 or -2, followed by an intermediate level and an island platform at the lowest level. Platform screen doors protect the tracks.</p>
                <p>The western section, crossing the Guadalquivir river, includes elevated, surface, and underground sections. Ciudad Expo, the only station with both an island platform and a lobby at street level, stands out. Other stations have two side platforms, with the lobby located at either the upper or lower level, depending on the station's location.</p>
                <p>The line runs shallowly between Nervión and Cocheras, with all stations featuring side platforms and a single lobby.</p>
            </div>
            
            <div id="com_Palma">
                <h5>Palma</h5>
                <p>The Palma metro comprises a single line connecting the city center and the university campus in the north. The majority of the line is underground, with a surface-level section around Son Sardina. The segment between Estació Intermodal and Son Costa – Son Fortesa features quadruple tracks for metro and mainline traffic.</p>
                <p>Estació Intermodal serves as a transportation hub with 10 platforms, a bus station at level -2, and a mezzanine at level -1. Jacint Verdaguer station has two island platforms at level -2 and a mezzanine at level -1. Son Costa – Son Fortesa station features two unconnected island platforms at level -1. The UIB station has two side platforms at level -1 and a ground-level hall. Son Sardina station is at ground level, with two side platforms connected by an underpass and linking to the Sóller Railway station. The remaining stations share a similar design, with side platforms at level -1, each directly accessible from the street, and an underpass connecting both platforms.</p>
            </div>
            
            <div id="com_Naples">
                <h5>Naples</h5>
                <p>The Naples rail network comprises multiple lines operated by different agencies. The definition of what is part of the metro network varies.</p>
                <p>Line 1, opened in 1993, connects the old city by the sea with the upper districts. The line forms a loop to climb the hill. Most stations are at great depths, except the section between Colli Aminei and Piscninola. Connections to platforms vary by station, but the booking halls are commonly located at level -1. Some feature a single flight of parallel escalators (Montedonzelli, Policlinico), high-capacity lifts (Duomo), or two or three flights of stairs. Stations between Colli Amiei and Museo have island platforms, while those in the old city have side platforms. Quattro Giornate station has overlapped platforms.</p>
                <p>Line 2, owned by the Italian State, opened in 1925 with third rail electrification but was later converted to overhead lines. Montesanto and Cavour are the deepest stations, with a ground-level building comprising ticket office and turnstiles. Two flights of stairs and escalators connect to an intermediate level (-2), with two side platforms at level -3. Other stations, like Mergellina or Campi Flegei, are at ground level with grand station buildings. Piazza Garibaldi has two tracks and three platforms, with a recent layout change.</p>
                <p>Line 6, closed since 2013, featured a narrow profile with trains only 2.20 meters wide.</p>
                <p>The Arcobaleno line (Line 11) connects Piscinola with Aversa, built with cut-and-cover methods. The layouts resemble Milan metro lines 1 and 2 and Rome's southern section of line A. The Circumvesuviana and Circumflegrea lines are also part of Naples' rail network, with some underground stations.</p>
            </div>
            
            <div id="com_Rennes">
                <h5>Rennes</h5>
                <p>Rennes has two metro lines.</p>
                <p>Line A is a VAL, an automatic light metro using technology developed by Siemens. The trains have a width of 2.08 m. Most of the line runs underground, with two elevated stations (La Poterie and Pontchaillou). The layout of the stations varies considerably, but all stations have side platforms on the lowest level. The lobbies are either located at street level or at level -1. Anatole France and Jacques Cartier are the deepest stations on this line. Initially, the stations had an honor system, but turnstiles were installed in 2020. Due to this reason, some lobbies might have a peculiar placement of the turnstiles.</p>
                <p>Line B is a Neoval, an automatic line that uses more modern technology developed by Siemens. Trains operate in 2-car compositions, but stations allow for 3-car trains. The entire line runs underground except for the easternmost section, which is elevated. Platforms are located on the lowest level, normally at level -2 or -3. Mezzanines are located at level -1. Sainte-Anne is an interchange station located at the city center. Transfers, access, and egress are performed through a complex system of one-way corridors, stairs, and escalators.</p>
            </div>
            
            <div id="com_Donostia-San Sebastian">
                <h5>Donostia-San Sebastian</h5>
                <p>Topo is a narrow-gauge rail line operated by Euskotren Trena, an agency owned by the Basque Government. This line runs through many tunnels, earning it the nickname "Topo," which means “mole” in Spanish. The line underwent a major renovation in the last 10 years, and a variant is being built both in the city center of San Sebastian and in the neighboring town of Pasaia. As a consequence, the current main station of Amara will be closed and dismantled.</p>
                <p>The layout of the stations differs. Lugaritz, Anoeta, and Herrera have a station building at ground level and two side platforms at level -1. Loiola and Pasaia stations are elevated. Intxaurrondo and Altza stations are located deep underground and have layouts and aesthetics similar to those of the Bilbao metro, once designed by Norman Foster.</p>
            </div>
            
            <div id="com_Sofia">
                <h5>Sofia</h5>
                <p>The Sofia metro consists of 4 different services that operate on 2 physical lines. Despite having rolling stock manufactured by a Russian company, the aesthetics and layouts differ from the Soviet-influenced metros of eastern European countries. Generally speaking, the average station has lobbies located at levels -1 and the platforms at level -2. Most stations have side platforms, but the oldest ones, located in the central part of lines 1 and 4, have side platforms. Stations in the common section of lines 1 and 4 have low-cost platform screen doors.</p>
                <p>All stations on line 3 have side platforms, normally located at level -2. Mezzanines are commonly located at level -1. The connection between platforms and the mezzanine is performed by multiple staircases or elevators. Most of the accesses to the stations are equipped with elevators.</p>
            </div>
            
            <div id="com_Toulouse">
                <h5>Toulouse</h5>
                <p>The Toulouse metro is formed by two lines that form a cross-shaped network. Both lines use VAL technology and are automated. The layout of line A stations varies. In the city center, the line runs through a dual-tube tunnel at a great depth to cross the Garonne river. Long escalators reach the platforms at Capitole and Esquirol. Stations located outside the city center tend to have a lobby at level -1 and two side platforms at level -2 or -3. The western terminus at Basso Cambo is elevated.</p>
                <p>Most of the line B stations have a similar layout consisting of a lobby at level -1, an intermediate level used for entering passengers at level -2, and two side platforms at level -3. Passengers egressing the station can use escalators that connect directly the platforms with the lobbies. Jean Jaurès is the main hub on the network. The station was designed to have one-way passages and escalators. The line B station uses the Barcelona solution, boarding through the island platform. Passengers need to pass through turnstiles to change from one line to the other. These turnstiles are used only for statistical purposes and to manage flows, as the tariff system guarantees free transfers.</p>
            </div>
            
            <div id="com_Zurich">
                <h5>Zurich</h5>
                <p>In 1962, the citizens of Zurich rejected in a referendum the conversion of tram lines in the city center into a premetro. In 1973, the citizens rejected again the construction of a metro network. However, a short section of the tunnels was already being built to conduct rolling stock tests. In 1978, the citizens approved integrating the existing tunnel into the tram network, which was completed in 1986. These stations were built at a considerable depth, as they are located in a hill between the valleys of the Limmat and the Glatt rivers. All stations have island platforms. Since trams in Zurich only have doors on the right side, they run on the left side of the tunnel. All the stations have multiple accesses, some reachable only via elevators, others via a long flight of stairs and escalators, which in some cases have the lower side in a level located even below the platform level. </p>
            </div>
            <div id="com_Liverpool">
                <h5>Liverpool</h5>
                <p>Merseyrail is the suburban train network managed by the Liverpool City Region, and it is operated by a private company through a concession. The network consists of two directly operated lines.</p>
                <p>The Wirral Line connects central Liverpool with the Wirral peninsula, located south of the River Mersey, via a tunnel opened in 1886. The two stations adjacent to the river (James Street and Hamilton Square) have platforms located at a great depth. The connection between the street-level concourse and the pre-platform level is made through three high-capacity elevators, in addition to emergency stairs, which at Hamilton Square can also be used regularly. </p>
                <p>The Northern Line is a north-south line created in 1977, after connecting by a tunnel two separate lines that departed from Exchange and Liverpool Central terminals. In the 1970s, the Wirral Line terminal, which ended in a double track at Liverpool Central, was also modified to become a one-way loop served by three stations spread throughout central Liverpool. These stations are very deep. The connection between platforms, intermediate levels and street-level concourses is made through two or three sets of escalators. All underground stations are accessible to people with reduced mobility.</p>
            </div>
            <div id="com_Genoa">
                <h5>Genoa</h5>
                <p>Genoa has a metro line that runs through the city, parallel to the coast. Due to the city's difficult topography, the construction methods for each section are different, resulting in a curvy route with very heterogeneous stations.</p>
                <p>Both terminals, Brin and Brignole, are located on viaducts. Additionally, Brignole metro station is integrated parallel to the railway station.</p>
                <p>Dinegro is the only station built using the cut-and-cover method and is the most shallow in the network. Both platforms are connected via an underpass. This station has múltiple tracks, three of them with platforms, in addition to other tracks that are used as workshops and depots.</p>
                <p>Darsena and San Giorgio stations have an island platform and are deeper than Dinegro but shallower than Sarzano-Sant’Agostino and De Ferrari stations, which have two side platforms each located in two different tunnels.</p>
                <p>The most complex station in the network is Principe: it consists of the metro station, an underground station national rail network, and the surface railway station. The surface station is nestled in a trench between mountains and has a yard with 5 platforms and 9 tracks. Corridors connect the underpass of the surface-level station with a concourse that either leads to the underground railway station, of which only one of the two tracks is in service, or to the metro station through a series of underground walkways. The metro station has a semi-underground concourse, which connects to the mezzanine, located at the upper level of the platform, via a staircase.</p>

            </div>
            <div id="com_Charleroi">
                <h5>Charleroi</h5>
                <p>Charleroi's premetro consists of a circular line that encircles the city center and three branches towards the suburbs and nearby towns. The network is operated as four radial services, which start from the central loop.</p>
                <p>The central ring is a mix of tram with priority (between Tirou and Gare Centrale) and a fully segregated line, mostly underground but with an elevated section between Gare Centrale and Palais. Line 4 operates completely segregated outside the city ring, with trams running on the left side. Lines 1 and 2 run also segregated from the rest of traffic between the city center and Pétria, and then as a tram line between Pétria and Monument, just like line 3 once past Piges.</p>
                <p>The station layouts are rather simple. Regardless of whether they are on a viaduct or in a tunnel, almost all have a central platform, which is accessed directly from the street, sometimes via an intermediate level, without turnstiles.</p>
                <p>Palais station has four tracks: two of them are in service, a third without regular service, and the fourth leads to a loop that reverses the direction of trains. Waterloo station has two platforms and three tracks, but only the central platform is in operation.</p>

            </div>

            <div id="com_Montreal"> 
                <h5>Montreal</h5> 
                <p>The Montreal metro consists of a network of 4 lines with rubber-tired trains. Nearly all of the network was built during the 1960s, 70s, and 80s, and is entirely underground, with tunnels often (but not always) following the alignment of the streets.</p>
                <p>Each station has unique architecture and style. Station depth also varies. However, most stations feature two side platforms, each 500 feet long (152.4 m), and all entrances are equipped with enclosures to protect against winter temperatures. In some cases, these enclosures also host station lobbies.</p>
                <p>Transfer stations were designed to offer practical and fast connections. Snowdon and Lionel-Groulx stations feature overlapped platforms, allowing for cross-platform transfers, while at Berri-UQAM, the platforms for the green and orange lines intersect at adjacent levels. At Jean-Talon, transferring between the orange and blue lines is also quick, with the platforms arranged in a stacked layout.</p>
                <p>One unique feature of Montreal is that, in the city center, metro stations connect directly to the underground city known as RÉSO, which links city blocks underground.</p>
            </div>
            
            <div id="com_Ottawa"> 
                <h5>Ottawa</h5> 
                <p>Ottawa’s rail transit system (O-Train) includes an east-west light rail line called the Confederation Line and a north-south rail line called the Trillium Line.</p>
                <p>The entire network is above ground, except for the central part of the Confederation Line, which runs at a depth of 17 to 25 meters below street level. The three underground stations have side platforms located on the lowest level. Each station has two independent entrances connecting the mezzanines located at either end of the platforms, which are accessible via multiple stair flights.</p>
                <p>Outside of the city center, stations may be located at street level, elevated, or in a trench, with most featuring side platforms. Some stations are transfer points between light rail and bus services, such as Tunney’s Pasture, Hurdman, and Blair. Ottawa also has several BRT lines, some of which have fully segregated infrastructure, including stops shaped like rail stations, with escalators, elevators, and lobbies with fare validators.</p>
            </div>
            
            <div id="com_Toronto"> 
                <h5>Toronto</h5> 
                <p>The Toronto subway has three lines. The network is relatively shallow, with some sections running above ground.</p>
                <p>In the oldest sections, the tunnel alignment is parallel to the street one but is shifted several meters toward the blocks of houses. Due to the cut-and-cover construction method used in these segments, it was necessary to expropriate and demolish existing buildings. Some of these clearings were used to build transfer hubs containing bus and streetcar bays, together with the passenger building, which also serves as the subway entrance. All bus and streetcar platforms are located within the paid area of the station, providing free transfers between the subway and surface transit.</p>
                <p>Regarding subway line transfers, Sheppard-Yonge, Bloor-Yonge, and St. George stations provide direct transfers via a single flight of stairs, while at Spadina, the transfer involves a walkway over 200 meters long.</p>
            </div>
            
            <div id="com_Calgary"> 
                <h5>Calgary</h5> 
                <p>Calgary has a light rail system called the C-Train. In the downtown area, it operates like a streetcar, with a low stop spacing, but in the suburbs, the spacing between stations is greater, and the trains have dedicated platforms, although some crossings still use level crossings.</p>
                <p>As for stations, there is only one underground station (Westbrook). Some other stations have platform access at ground level, while others use overpasses that lead to platforms through lobbies with waiting areas.</p>
            </div>
            
            <div id="com_Vancouver"> 
                <h5>Vancouver</h5> 
                <p>Vancouver’s SkyTrain consists of three automated metro lines. The network has a radial layout, and only two of the three lines reach downtown. As the name suggests, most of the metro is elevated, with only the most central segments being underground.</p>
                <p>The Expo Line runs through downtown in a two-level tunnel, with stations located at significant depths. The Canada Line is much shallower, with each station featuring a different layout: Waterfront, Vancouver City Centre, Yaletown-Roundhouse, and Olympic Village have a lobby on level -1 and an island platform on level -2; King Edward has overlapped platforms; while Oakridge and Langara have platforms on level -1 connected by an underpass.</p>
                <p>The main interchange of the network, Waterfront, serves as the terminal for the Expo and Canada Lines, as well as the commuter train and the ferry to North Vancouver. The transfer here is not ideal, as the Canada Line is in an area with separate fare gates, and the distance between this line and the ferry terminal is 350 meters.</p>
            </div>
            
            <div id="com_Seattle"> 
                <h5>Seattle</h5> 
                <p>Seattle has a light rail line. Initially, a tunnel was built for buses and trolleybuses in the downtown area, running from International District / Chinatown to Westlake, which opened in 1990. Stations in this section had side platforms on level -2 and two lobbies on level -1, with platforms spaced widely enough to allow one bus to overtake another.</p>
                <p>The tunnel closed in 2005 for conversion to light rail, which opened in 2009. Until 2019, it operated as a mixed-use tunnel for both buses and light rail. In 2016, the network expanded northward, adding a segment with four underground stations and one elevated station. Platforms at the underground stations are more than 20 meters deep, and each one has a unique layout.</p>
                <p>On the southern section, there is Beacon Hill Station, accessible only via four high-capacity elevators that descend to the platforms, located about 50 meters below ground.</p>
            </div>
            
            <div id="com_Stockholm"> 
                <h5>Stockholm</h5> 
                <p>Stockholm has three subway lines that cross the city center and split into two or three spurs at each end. The city itself is in an archipelago, so the metro lines were planned and built through a complex geography and geology, featuring surface stretches, elevated viaducts, shallow underground sections, and deep tunnels.</p>
                <p>The green line, the first to be built, is considered to be the shallowest. In the city center, it runs in shallow tunnels built using the cut-and-cover method, and in the suburbs, the line runs either elevated or at surface level. The red line combines deep underground segments in the north with a mix of surface and deep sections in the south. The blue line runs at great depth along nearly its entire length.</p>
                <p>All stations are accessible to persons with reduced mobility thanks to the installation of lifts. Deep stations typically feature two island platforms at the lowest level (-2), connecting to lobbies located at or near the surface (level -1) via a set of 3 or 4 parallel escalators, in addition to an inclined lift which also runs parallel. Stockholm has one of the highest numbers of inclined lifts in its subway system. Some deep stations have cavern-like platform designs.</p>
            </div>
            <div id="com_Leipzig"> 
                <h5>Leipzig</h5> 
                <p>Leipzig has a tunnel that crosses the city centre, with four underground stations, all featuring island platforms at the deepest level. Although the tunnels are twin-tube and were built using tunnel boring machines, the stations were constructed using the cut-and-cover method, except for Hauptbahnhof, which was built using the mining method.</p>
            </div>
            
            <div id="com_Karlsruhe"> 
                <h5>Karlsruhe</h5> 
                <p>Karlsruhe has a tram-train system. In 2021, a T-shaped underground line was opened, which is served by six stations, one of which is a double station. All stations have side platforms located on level -2, except for Kongresszentrum, which has its platforms on level -1. The -1 level of the other stations consists of various concourses.</p>
                <p>The platforms have sections with different heights to ensure compatibility with all rolling stock in terms of accessibility.</p>
            </div>

            <div id="com_Bielefeld"> 
                <h5>Bielefeld</h5>
                <p>The Bielefeld Stadtbahn consists of a north-south trunk line that branches into four lines, both in the north and in the south. The central section and a bunch of the northern spurs are underground. Most stations have island platforms on the deepest level, with accesses at both ends of each platform.</p>
                <p>Hauptbahnhof is the junction station on the northern side. The station has a concourse at level -1, two platforms and three tracks at level -2; plus one track and one platform at level -3, which crosses the station diagonally.</p>
            </div>

            <div id="com_Gelsenkirchen"> 
                <h5>Gelsenkirchen</h5>
                <p>Gelsenkirchen has a Stadtbahn line operated with low-floor trains. Except for Heinrich-König-Straße, which is located at a junction, all stations have a platform at the lowest level. In some stations, access to the platforms is directly from the street; in others, access points converge in the concourse.</p>
            </div>

            <div id="com_Wuppertal"> 
                <h5>Wuppertal</h5>
                <p>The Wuppertal Schwebebahn is a suspended monorail opened in 1901, with a single line that follows the course of the Wupper River, except at the western end, where it follows a street.</p>
                <p>The stations are elevated and located above the river or the street. All stations have side platforms, so trains have doors on only one side. The platforms are around 30 meters long.</p>
            </div>

            <div id="com_Cologne"> 
                <h5>Cologne</h5>
                <p>Cologne has a Stadtbahn made up of different services that cross the city. Most of the underground sections are located in the center and in the north. Some of these lines are operated with low-floor trams, while others use high-floor trains. This results in some stations having platforms with two different heights.</p>
                <p>The stations typically have a simple structure with platforms at level -2 and one or two concourses at level -1, although some stations have platforms at level -1. The new north-south line, partially constructed and served by lines 5 and 17, runs deeper than the other lines and has central platforms at almost all stations. However, currently only one of the two tracks is operational. Finally, line 13, which runs along the <em>Gürtel</em> (ring road), includes an elevated section.</p>
                <p>As for transfer stations, some are designed for quick and short transfers, such as Friesenplatz or Ebertplatz. The central station is served by two different, adjacent Stadtbahn stations.</p>
            </div>

            <div id="com_Bonn"> 
                <h5>Bonn</h5>
                <p>Bonn has a Stadtbahn consisting of a line that runs parallel to both the Cologne – Mainz railway line and the Rhine River. It has two underground sections: one in the center and south of Bonn, and another in the Bad Godesberg district, plus a couple of isolated underground stations on a branch line.</p>
                <p>The stations are simple. All have side platforms at level -2 and one or two concourses at level -1.</p>
            </div>

             </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[If my kids excel, will they move away? (206 pts)]]></title>
            <link>https://jeffreybigham.com/blog/2025/where-will-my-kids-go.html</link>
            <guid>45236411</guid>
            <pubDate>Sun, 14 Sep 2025 00:19:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jeffreybigham.com/blog/2025/where-will-my-kids-go.html">https://jeffreybigham.com/blog/2025/where-will-my-kids-go.html</a>, See on <a href="https://news.ycombinator.com/item?id=45236411">Hacker News</a></p>
<div id="readability-page-1" class="page"><p id="h.rjbw4a5r30z"><span>If my kids excel, will they move away?</span></p><p><span>Jeffrey P. Bigham</span></p><p><span>I grew up on a farm outside of a rural town about an hour southeast of Columbus, Ohio. Like many small towns in America, my town knows “brain drain” – all of my friends from high school who went to college (~30% of my class) now live elsewhere, although most are pretty close by (e.g., several live in the suburbs of Columbus and Cincinnati).<p>Sometimes my hometown feels a million miles away, but it only takes two hours and fifty minutes for me to drive there from Pittsburgh, which is where I live now.</p></span><img alt="Jeff on his dad's farm, looking kind of rough" src="https://jeffreybigham.com/blog/2025/images/image2.jpg" title=""></p><p><span>In Pittsburgh, I’m a professor at Carnegie Mellon University in the top computer science school in the world. I’ve also worked in various large technology companies, who have offices in Pittsburgh to connect with and employ Carnegie Mellon faculty and students.</span></p><p><span>I may not live in my small town anymore, but the fact that the best place in the world to study and do research in computer science is in Pittsburgh means I’m really not that far away. My four kids see their grandparents often, they’re known in my parents’ church and have spent a lot of time on my dad’s farm. The photo above is of me on the farm, wearing some of my dad’s clothes, trying to help out when my dad fell ill a few years ago.</span></p><p><span>Most of my story we’ve been able to take for granted in the United States for the past few decades. If you grow up in the United States, and you’re among the best in the world in your field, you could count on the center of excellence for your field also being in the United States, oftentimes pretty close by, like Pittsburgh being close to my hometown.</span></p><p><span>As a professor, I’m able to recruit the very best students in the world to work on my research. Sometimes that means recruiting Americans and sometimes that means recruiting from elsewhere. <img alt="Jeff wearing an 'America is an idea' t-shirt in front of Nassau hall at Princeton" src="https://jeffreybigham.com/blog/2025/images/image1.jpg" title="Jeff wearing an 'America is an idea' t-shirt in front of Nassau hall at Princeton"> Students come to Pittsburgh from around the world (I’ve advised PhD students and postdocs from about 10 different countries). Five or six years after they start our intensive graduation program, successful students receive their PhDs and that’s when I tend to meet their parents for the first time. Oftentimes, this is the first trip they’ve made to the United States, and they may have only seen their kids a few times during their degree. It hits home because usually these students choose to stay in the United States – after successfully completing their degree with me, they are in high demand not only in our universities but also in technology companies.</span></p>


<p><span>These days the students I talk to are less confident about coming to the United States to study and less confident about staying here after they’re done. They have seen a student grabbed off the street apparently because she wrote an essay expressing concern about the on-going humanitarian crisis in Gaza. They have seen graduate students jailed for what used to be minor immigration offenses. They have seen even greater uncertainty in applying or reapplying for the visas they need to study. And, they have seen their status as students arbitrarily used as leverage in attacking premier universities like Harvard</span><sup><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span>. Most of these incidents have or probably will be resolved, but the message and fear it causes are real and long-lasting.</span></p><p><span>I am worried that policies that have the intention (or effect) of introducing chaos and cruelty to superstar students will make it less likely for the best of the best to come to America, and this in turn will mean centers of excellence will move elsewhere. While incumbents have an advantage, it doesn’t take much to influence group behavior and movements can be self-reinforcing. The best people in a field like to be where other amazing people are, so they can learn and build off of each other. If the centers of excellence move elsewhere, I’m worried my kids will end up feeling compelled to move away (should they become superstars, as is my hope for them).<p>The brain drain from our small rural communities is real, but many of us have found ways to stay close by and keep those ties. There’s a bunch of reasons to treat international students better than we have over the past months, but these concerns are not thousands of miles away as they seem to some – to me, it's incredibly close to home, and not only because I see the effect on students I work with closely.</p></span></p><p><span>If we cause centers of excellence to move away from Pittsburgh, and away from the United States entirely, that’s the difference between my grandkids living near or very far, and whether they’re likely to grow up visiting me and my dad’s farm often or hardly at all.</span></p><hr><p><a href="#ftnt_ref1" id="ftnt1">[1]</a><span>&nbsp;I’ve owned exactly two Harvard t-shirts in my lifetime – the first when I was an undergrad at Princeton said, ‘Harvard Sucks’, and the second is a normal Harvard t-shirt that I bought this past May.</span></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Two Slice, a font that's only 2px tall (221 pts)]]></title>
            <link>https://joefatula.com/twoslice.html</link>
            <guid>45236263</guid>
            <pubDate>Sat, 13 Sep 2025 23:50:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joefatula.com/twoslice.html">https://joefatula.com/twoslice.html</a>, See on <a href="https://news.ycombinator.com/item?id=45236263">Hacker News</a></p>
<div id="readability-page-1" class="page">
		
		<p>A font that's only 2px tall, and somewhat readable!  Uppercase and lowercase have some different variants, in case you find one more readable than the other.  Numbers (sort of) and some punctuation marks are included.</p>
		<p>You can probably read this, even if you wish you couldn't.<br>It tends to be easier to read at smaller sizes.</p>
		<p>Try it out below, or <a href="https://joefatula.com/assets/Two%20Slice.ttf">download it</a> (under <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a> license, so you can use it commercially but you have to give credit).</p>
		
	
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pass: Unix Password Manager (190 pts)]]></title>
            <link>https://www.passwordstore.org/</link>
            <guid>45236079</guid>
            <pubDate>Sat, 13 Sep 2025 23:16:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.passwordstore.org/">https://www.passwordstore.org/</a>, See on <a href="https://news.ycombinator.com/item?id=45236079">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="main_content_wrap">

<h2>Introducing <code>pass</code></h2>

<p>Password management should be simple and follow <a href="http://en.wikipedia.org/wiki/Unix_philosophy">Unix philosophy</a>. With <code>pass</code>, each password lives inside of a <a href="http://en.wikipedia.org/wiki/GNU_Privacy_Guard"><code>gpg</code></a> encrypted file whose filename is the title of the website or resource that requires the password. These encrypted files may be organized into meaningful folder hierarchies, copied from computer to computer, and, in general, manipulated using standard command line file management utilities.</p>

<p><code>pass</code> makes managing these individual password files extremely easy. All passwords live in <code>~/.password-store</code>, and <code>pass</code> provides some nice commands for adding, editing, generating, and retrieving passwords. It is a very short and simple shell script. It's capable of temporarily putting passwords on your clipboard and tracking password changes using <a href="http://en.wikipedia.org/wiki/Git_(software)"><code>git</code></a>.</p>

<p>You can edit the password store using ordinary unix shell commands alongside the <code>pass</code> command. There are no funky file formats or new paradigms to learn. There is <a href="http://en.wikipedia.org/wiki/Bash_(Unix_shell)">bash</a> <a href="http://en.wikipedia.org/wiki/Command-line_completion">completion</a> so that you can simply hit tab to fill in names and commands, as well as completion for <a href="http://en.wikipedia.org/wiki/Z_shell">zsh</a> and <a href="http://en.wikipedia.org/wiki/Friendly_interactive_shell">fish</a> available in the <a href="https://git.zx2c4.com/password-store/tree/src/completion">completion</a> folder. The <strong>very active community</strong> has produced many impressive <strong><a href="#other">clients and GUIs for other platforms</a></strong> as well as <strong><a href="#extensions">extensions</a></strong> for <code>pass</code> itself.</p>

<p>The <code>pass</code> command is extensively documented in its <a href="https://git.zx2c4.com/password-store/about/">man page</a>.</p>



<h3>Using the password store</h3>

<p>We can list all the existing passwords in the store:</p>

<pre><code>zx2c4@laptop ~ $ pass
Password Store
├── Business
│   ├── some-silly-business-site.com
│   └── another-business-site.net
├── Email
│   ├── donenfeld.com
│   └── zx2c4.com
└── France
    ├── bank
    ├── freebox
    └── mobilephone
</code></pre>

<p>And we can show passwords too:</p>

<pre><code>zx2c4@laptop ~ $ pass Email/zx2c4.com
sup3rh4x3rizmynam3
</code></pre>

<p>Or copy them to the clipboard:</p>

<pre><code>zx2c4@laptop ~ $ pass -c Email/zx2c4.com
Copied Email/jason@zx2c4.com to clipboard. Will clear in 45 seconds.
</code></pre>

<p>There will be a nice password input dialog using the standard <code>gpg-agent</code> (which can be configured to stay authenticated for several minutes), since all passwords are encrypted.</p>

<p>We can add existing passwords to the store with <code>insert</code>:</p>

<pre><code>zx2c4@laptop ~ $ pass insert Business/cheese-whiz-factory
Enter password for Business/cheese-whiz-factory: omg so much cheese what am i gonna do
</code></pre>

<p>This also handles multiline passwords or other data with <code>--multiline</code> or <code>-m</code>, and passwords can be edited in your default text editor using <code>pass edit pass-name</code>.</p>

<p>The utility can <code>generate</code> new passwords using <code>/dev/urandom</code> internally:</p>

<pre><code>zx2c4@laptop ~ $ pass generate Email/jasondonenfeld.com 15
The generated password to Email/jasondonenfeld.com is:
$(-QF&amp;Q=IN2nFBx
</code></pre>

<p>It's possible to generate passwords with no symbols using <code>--no-symbols</code> or <code>-n</code>, and we can copy it to the clipboard instead of displaying it at the console using <code>--clip</code> or <code>-c</code>.</p>

<p>And of course, passwords can be removed:</p>

<pre><code>zx2c4@laptop ~ $ pass rm Business/cheese-whiz-factory
rm: remove regular file ‘/home/zx2c4/.password-store/Business/cheese-whiz-factory.gpg’? y
removed ‘/home/zx2c4/.password-store/Business/cheese-whiz-factory.gpg’
</code></pre>

<p>If the password store is a git repository, since each manipulation creates a git commit, you can synchronize the password store using <code>pass git push</code> and <code>pass git pull</code>, which call <code>git-push</code> or <code>git-pull</code> on the store.</p>

<p>You can read more examples and more features in the <a href="https://git.zx2c4.com/password-store/about/">man page</a>.</p>

<h3>Setting it up</h3>

<p>To begin, there is a single command to initialize the password store:</p>

<pre><code>zx2c4@laptop ~ $ pass init "ZX2C4 Password Storage Key"
mkdir: created directory ‘/home/zx2c4/.password-store’
Password store initialized for ZX2C4 Password Storage Key.
</code></pre>

<p>Here, <code>ZX2C4 Password Storage Key</code> is the ID of my GPG key. You can use your standard GPG key or use an alternative one especially for the password store as shown above. Multiple GPG keys can be specified, for using pass in a team setting, and different folders can have different GPG keys, by using <code>-p</code>.</p>

<p>We can additionally initialize the password store as a git repository:</p>

<pre><code>zx2c4@laptop ~ $ pass git init
Initialized empty Git repository in /home/zx2c4/.password-store/.git/
zx2c4@laptop ~ $ pass git remote add origin kexec.com:pass-store
</code></pre>

<p>If a git repository is initialized, <code>pass</code> creates a git commit each time the password store is manipulated.</p>

<p>There is a more <a href="https://git.zx2c4.com/password-store/about/#EXTENDED%20GIT%20EXAMPLE">detailed initialization example</a> in the <a href="https://git.zx2c4.com/password-store/about/">man page</a>.</p>

<h2 id="download">Download</h2>

<p>The latest version is 1.7.4.</p>

<h3 id="deb">Ubuntu / Debian</h3>

<pre><code>$ sudo apt-get install pass</code></pre>

<h3 id="redhat">Fedora / RHEL</h3>

<pre><code>$ sudo yum install pass</code></pre>

<h3 id="suse">openSUSE</h3>

<pre><code>$ sudo zypper in password-store</code></pre>

<h3 id="gentoo">Gentoo</h3>

<pre><code># emerge -av pass</code></pre>

<h3 id="arch">Arch</h3>

<pre><code>$ pacman -S pass</code></pre>

<h3 id="macintosh">Macintosh</h3>

<p>The password store is available through the <a href="http://brew.sh/">Homebrew package manager</a>:</p>

<pre><code>$ brew install pass</code></pre>

<h3 id="freebsd">FreeBSD</h3>

<pre><code># pkg install password-store</code></pre>

<h3 id="tarball">Tarball</h3>

<ul>
<li><a href="https://git.zx2c4.com/password-store/snapshot/password-store-1.7.4.tar.xz">Version 1.7.4</a>
</li><li><a href="https://git.zx2c4.com/password-store/snapshot/password-store-master.tar.xz">Latest Git</a></li>
</ul>
The tarball contains a generic makefile, for which a simple <code>sudo make install</code> should do the trick.

<h3>Git Repository</h3>

<p>You may <a href="https://git.zx2c4.com/password-store/">browse the git repository</a> or clone the repo:
</p>

<pre><code>$ git clone https://git.zx2c4.com/password-store</code></pre>

<p>All releases are tagged, and the tags are signed with <a href="http://www.zx2c4.com/keys/AB9942E6D4A4CFC3412620A749FC7012A5DE03AE.asc">0xA5DE03AE</a>.</p>

<h2 id="organization">Data Organization</h2>

<h3>Usernames, Passwords, PINs, Websites, Metadata, et cetera</h3>

<p>The password store does not impose any particular schema or type of organization of your data, as it is simply a flat text file, which can contain arbitrary data. Though the most common case is storing a single password per entry, some power users find they would like to store more than just their password inside the password store, and additionally store answers to secret questions, website URLs, and other sensitive information or metadata. Since the password store does not impose a scheme of it's own, you can choose your own organization. There are many possibilities.</p>

<p>One approach is to use the multi-line functionality of pass (<code>--multiline</code> or <code>-m</code> in <code>insert</code>), and store the password itself on the first line of the file, and the additional information on subsequent lines. For example, <code>Amazon/bookreader</code> might look like this:</p>

<pre><code>Yw|ZSNH!}z"6{ym9pI
URL: *.amazon.com/*
Username: AmazonianChicken@example.com
Secret Question 1: What is your childhood best friend's most bizarre superhero fantasy? Oh god, Amazon, it's too awful to say...
Phone Support PIN #: 84719</code></pre>

<p><em>This is the preferred organzational scheme used by the author.</em> The <code>--clip</code> / <code>-c</code> options will only copy the first line of such a file to the clipboard, thereby making it easy to fetch the password for login forms, while retaining additional information in the same file.</p>

<p>Another approach is to use folders, and store each piece of data inside a file in that folder. For example <code>Amazon/bookreader/password</code> would hold bookreader's password inside the <code>Amazon/bookreader</code> directory, and <code>Amazon/bookreader/secretquestion1</code> would hold a secret question, and <code>Amazon/bookreader/sensitivecode</code> would hold something else related to bookreader's account. And yet another approach might be to store the password in <code>Amazon/bookreader</code> and the additional data in <code>Amazon/bookreader.meta</code>. And even another approach might be use multiline, as outlined above, but put the URL template in the filename instead of inside the file.</p>

<p>The point is, the possibilities here are extremely numerous, and there are many other organizational schemes not mentioned above; you have the freedom of choosing the one that fits your workflow best.</p>

<h3 id="extensions">Extensions for <code>pass</code></h3>
<p>In order to faciliate the large variety of uses users come up with, <code>pass</code> supports extensions. Extensions installed to <code>/usr/lib/password-store/extensions</code> (or some distro-specific variety of such) are always enabled. Extensions installed to <code>~/.password-store/.extensions/COMMAND.bash</code> are enabled if the <code>PASSWORD_STORE_ENABLE_EXTENSIONS</code> environment variable is <code>true</code> Read the <a href="https://git.zx2c4.com/password-store/about/">man page</a> for more details.</p>

<p>The community has produced many such extensions:</p>
<ul>
	<li><a href="https://github.com/roddhjav/pass-tomb#readme">pass-tomb</a>: manage your password store in a <a href="https://www.dyne.org/software/tomb/" target="_blank">Tomb</a></li>
	<li><a href="https://github.com/roddhjav/pass-update#readme">pass-update</a>: an easy flow for updating passwords</li>
	<li><a href="https://github.com/roddhjav/pass-import#readme">pass-import</a>: a generic importer tool from other password managers</li>
	<li><a href="https://github.com/palortoff/pass-extension-tail#readme">pass-extension-tail</a>: a way of printing only the tail of a file</li>
	<li><a href="https://github.com/palortoff/pass-extension-wclip#readme">pass-extension-wclip</a>: a plugin to use wclip on Windows</li>
	<li><a href="https://github.com/tadfisher/pass-otp#readme">pass-otp</a>: support for one-time-password (OTP) tokens</li>
</ul>

<h3 id="other">Compatible Clients</h3>
<p>The community has assembled an impressive list of clients and GUIs for various platforms:</p>

<ul>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/dmenu">passmenu</a>: an <strong>extremely useful and awesome</strong> dmenu script</li>
	<li><a href="http://qtpass.org/">qtpass</a>: cross-platform GUI client</li>
	<li><a href="https://github.com/zeapo/Android-Password-Store#readme">Android-Password-Store</a>: Android app</li>
	<li><a href="https://mssun.github.io/passforios/">passforios</a>: iOS app
	</li><li><a href="https://github.com/davidjb/pass-ios#readme">pass-ios</a>: (older) iOS app</li>
	<li><a href="https://github.com/jvenant/passff#readme">passff</a>: Firefox plugin</li>
	<li><a href="https://github.com/dannyvankooten/browserpass#readme">browserpass</a>: Chrome plugin</li>
	<li><a href="https://github.com/mbos/Pass4Win#readme">Pass4Win</a>: Windows client</li>
	<li><a href="https://github.com/Pext/pext_module_pass#readme">pext_module_pass</a>: module for <a target="_blank" href="https://pext.hackerchick.me/">Pext</a></li>
	<li><a href="https://github.com/cortex/gopass#readme">gopass</a>: Go GUI app</li>
	<li><a href="https://github.com/Kwpolska/upass#readme">upass</a>: interactive console UI</li>
	<li><a href="https://github.com/CGenie/alfred-pass#readme">alfred-pass</a>: Alfred integration</li>
	<li><a href="https://github.com/MatthewWest/pass-alfred#readme">pass-alfred</a>: Alfred integration</li>
	<li><a href="https://github.com/johanthoren/simple-pass-alfred">simple-pass-alfred</a>: Alfred integration</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/pass.applescript">pass.applescript</a>: OS X integration</li>
	<li><a href="https://github.com/languitar/pass-git-helper">pass-git-helper</a>: git credential integration</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/emacs">password-store.el</a>: an emacs package</li>
	<li><a href="https://hackage.haskell.org/package/xmonad-contrib-0.13/docs/XMonad-Prompt-Pass.html">XMonad.Prompt.Pass</a>: prompt for Xmonad</li>
</ul>

<h3 id="migration">Migrating to <code>pass</code></h3>
<p>To free password data from the clutches of other (bloated) password managers, various users have come up with different password store organizations that work best for them. Some users have contributed scripts to help import passwords from other programs:</p>

<ul>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/1password2pass.rb">1password2pass.rb</a>: imports 1Password txt or 1pif data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/keepassx2pass.py">keepassx2pass.py</a>: imports KeepassX XML data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/keepass2csv2pass.py">keepass2csv2pass.py</a>: imports Keepass2 CSV data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/keepass2pass.py">keepass2pass.py</a>: imports Keepass2 XML data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/fpm2pass.pl">fpm2pass.pl</a>: imports Figaro's Password Manager XML data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/lastpass2pass.rb">lastpass2pass.rb</a>: imports Lastpass CSV data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/kedpm2pass.py">kedpm2pass.py</a>: imports Ked Password Manager data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/revelation2pass.py">revelation2pass.py</a>: imports Revelation Password Manager data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/gorilla2pass.rb">gorilla2pass.rb</a>: imports Password Gorilla data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/pwsafe2pass.sh">pwsafe2pass.sh</a>: imports PWSafe data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/kwallet2pass.py">kwallet2pass.py</a>: imports KWallet data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/roboform2pass.rb">roboform2pass.rb</a>: imports Roboform data</li>
	<li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/password-exporter2pass.py">password-exporter2pass.py</a>: imports password-exporter data
	</li><li><a href="https://git.zx2c4.com/password-store/tree/contrib/importers/pwsafe2pass.py">pwsafe2pass.py</a>: imports pwsafe data
	</li><li><a href="https://github.com/Unode/firefox_decrypt/#readme">firefox_decrypt</a>: full blown Firefox password interface, which supports exporting to pass</li>
</ul>

<h2>Credit &amp; License</h2>

<p><code>pass</code> was written by <a href="mailto:Jason@zx2c4.com">Jason A. Donenfeld</a> of <a href="http://zx2c4.com/">zx2c4.com</a> and is licensed under the <a href="http://www.gnu.org/licenses/gpl-2.0.html">GPLv2</a>+.</p>

<h3>Contributing</h3>

<p>This is a very active project with a <a href="https://git.zx2c4.com/password-store/stats/?period=y&amp;ofs=-1">healthy dose of contributors</a>. The best way to contribute to the password store is to <a href="http://lists.zx2c4.com/listinfo.cgi/password-store-zx2c4.com">join the mailing list</a> and send git formatted patches. You may also join the discussion in <code>#pass</code> on Libera.Chat.</p>


      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Will AI be the basis of many future industrial fortunes, or a net loser? (117 pts)]]></title>
            <link>https://joincolossus.com/article/ai-will-not-make-you-rich/</link>
            <guid>45235676</guid>
            <pubDate>Sat, 13 Sep 2025 22:01:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joincolossus.com/article/ai-will-not-make-you-rich/">https://joincolossus.com/article/ai-will-not-make-you-rich/</a>, See on <a href="https://news.ycombinator.com/item?id=45235676">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                        
<p><strong>Fortunes are made</strong> by entrepreneurs and investors when revolutionary technologies enable waves of innovative, investable companies. Think of the railroad, the Bessemer process, electric power, the internal combustion engine, or the microprocessor—each of which, like a stray spark in a fireworks factory, set off decades of follow-on innovations, permeated every part of society, and catapulted a new set of inventors and investors into power, influence, and wealth.</p>



<p>Yet some technological innovations, though societally transformative, generate little in the way of new wealth; instead, they reinforce the status quo. Fifteen years before the microprocessor, another revolutionary idea, shipping containerization, arrived at a less propitious time, when technological advancement was a Red Queen’s race, and inventors and investors were left no better off for non-stop running.</p>



<p>Anyone who invests in the <em>new</em> new thing must answer two questions: First, how much value will this innovation create? And second, who will capture it? Information and communication technology (ICT) was a revolution whose value was captured by startups and led to thousands of newly rich founders, employees, and investors. In contrast, shipping containerization was a revolution whose value was spread so thin that in the end, it made only a single founder temporarily rich and only a single investor a little bit richer.</p>



<p>Is generative AI more like the former or the latter? Will it be the basis of many future industrial fortunes, or a net loser for the investment community as a whole, with a few zero-sum winners here and there?</p>



<p>There are ways to make money investing in the fruits of AI, but they will depend on assuming the latter—that it is once again a less propitious time for inventors and investors, that AI model builders and application companies will eventually compete each other into an oligopoly, and that the gains from AI will accrue not to its builders but to customers. A lot of the money pouring into AI is therefore being invested in the wrong places, and aside from a couple of lucky early investors, those who make money will be the ones with the foresight to get out early.</p>



    




<p><strong>The microprocessor was revolutionary</strong>, but the people who invented it at Intel in 1971 did not see it that way—they just wanted to avoid designing desktop calculator chipsets from scratch every time. But outsiders realized they could use the microprocessor to build their own personal computers, and enthusiasts did. Thousands of tinkerers found configurations and uses that Intel never dreamed of. This distributed and permissionless invention kicked off a “great surge of development,” as the economist Carlota Perez calls it, triggered by technology but driven by economic and societal forces.<sup id="article-ref-1"><a href="#ref-1">[1]</a></sup></p>



<p>There was no real demand for personal computers in the early 1970s; they were expensive toys. But the experimenters laid the technical groundwork and built a community. Then, around 1975, a step-change in the cost of microprocessors made the personal computer market viable. The Intel 8080 had an initial list price of $360 ($2,300 in today’s dollars). MITS could barely turn a profit on its Altair at a bulk price of $75 each ($490 today). But when MOS Technologies started selling its 6502 for $25 ($150 today), Steve Wozniak could afford to build a prototype Apple. The 6502 and the similarly priced Zilog Z80 forced Intel’s prices down. The nascent PC community started spawning entrepreneurs and a score of companies appeared, each with a slightly different product.</p>



<p>You couldn’t have known in the mid-1970s that the PC (and PC-like products, such as ATMs, POS terminals, smartphones, etc.) would revolutionize everything. While Steve Jobs was telling investors that every household would someday have a personal computer (a wild underestimate, as it turned out), others questioned the need for personal computers at all. As late as 1979, Apple’s ads didn’t tell you what a personal computer could do—it <em>asked</em> what you did with it.<sup id="article-ref-2"><a href="#ref-2">[2]</a></sup> The established computer manufacturers (IBM, HP, DEC) had no interest in a product their customers weren’t asking for. Nobody “needed” a computer, and so PCs weren’t bought—they were sold. Flashy startups like Apple and Sinclair used hype to get noticed, while companies with footholds in consumer electronics like Atari, Commodore, and Tandy/RadioShack used strong retail connections to put their PCs in front of potential customers.&nbsp;</p>



        <div>
            <p><img fetchpriority="high" decoding="async" src="https://joincolossus.com/wp-content/uploads/2025/08/Apple-ad.jpg" srcset="https://joincolossus.com/wp-content/uploads/2025/08/Apple-ad.jpg 550w, https://joincolossus.com/wp-content/uploads/2025/08/Apple-ad-402x531.jpg 402w, https://joincolossus.com/wp-content/uploads/2025/08/Apple-ad-462x610.jpg 462w" sizes="(min-width: 980px) 60vw, 100vw" alt="" width="550" height="726">
            </p>

            
                    </div>

    



<p>The market grew slowly at first, accelerating only as experiments led to practical applications like the spreadsheet, introduced in 1979. As use grew, observation of use caused a reduction in uncertainty, leading to more adoption in a self-reinforcing cycle. This kind of gathering momentum takes time in every technological wave: It took almost 30 years for electricity to reach half of American households, for example, and it took about the same amount of time for personal computers.<sup id="article-ref-3"><a href="#ref-3">[3]</a></sup> When a technological revolution changes everything, it takes a huge amount of innovation, investment, storytelling, time, and plain old work. It also sucks up all the money and talent available. Like Kuhn’s paradigms in science, any technology not part of the wave’s techno-economic paradigm will seem like a sideshow.<sup id="article-ref-4"><a href="#ref-4">[4]</a></sup></p>



        <div>
            <p><img decoding="async" src="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG1-RGB-V3.jpg" srcset="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG1-RGB-V3.jpg 1654w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG1-RGB-V3-402x207.jpg 402w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG1-RGB-V3-462x238.jpg 462w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG1-RGB-V3-662x341.jpg 662w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG1-RGB-V3-722x371.jpg 722w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG1-RGB-V3-982x505.jpg 982w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG1-RGB-V3-1032x531.jpg 1032w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG1-RGB-V3-1402x721.jpg 1402w" sizes="(min-width: 980px) 60vw, 100vw" alt="" width="1654" height="851">
            </p>

            
                            <p><em>Source</em>: <sup id="article-ref-3"><a href="#ref-3">[3]</a></sup></p>
                    </div>

    



<p>The nascent growth of PCs attracted investors—venture capitalists—who started making risky bets on new companies. This development incentivized more inventors, entrepreneurs, and researchers, which in turn drew in more speculative capital.</p>



<p>Companies like IBM, the computing behemoth before the PC, saw poor relative performance. They didn’t believe the PC could survive long enough to become capable in their market and didn’t care about new, small markets that wanted a cheaper solution.</p>



<p>Retroactively, we give the PC pioneers the powers of prophets rather than visionaries. But at the time, nobody outside of a small group of early adopters paid any attention. Establishment media like <em>The New York Times</em> didn’t take the PC seriously until after IBM’s was introduced in August 1981. In the entire year of 1976, when Apple Computer was founded, the <em>NYT</em> mentioned PCs only four times.<sup id="article-ref-5"><a href="#ref-5">[5]</a></sup> Apparently, only the crazy ones, the misfits, the rebels, and the troublemakers were paying attention.</p>



        <div>
            <p><img decoding="async" src="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG2-RGB-V2.jpg" srcset="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG2-RGB-V2.jpg 1655w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG2-RGB-V2-402x207.jpg 402w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG2-RGB-V2-462x238.jpg 462w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG2-RGB-V2-662x340.jpg 662w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG2-RGB-V2-722x371.jpg 722w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG2-RGB-V2-982x505.jpg 982w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG2-RGB-V2-1032x531.jpg 1032w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG2-RGB-V2-1402x721.jpg 1402w" sizes="(min-width: 980px) 60vw, 100vw" alt="" width="1655" height="851">
            </p>

            
                            <p><em>Source</em>: <sup id="article-ref-5"><a href="#ref-5">[5]</a></sup></p>
                    </div>

    



<p>It’s the element of surprise that should strike us most forcefully when we compare the early days of the computer revolution to today. No one took note of personal computers in the 1970s. In 2025, AI is all we seem to talk about.</p>



    




<p><strong>Big companies hate surprises.</strong> That’s why uncertainty makes a perfect moat for startups. Apple would never have survived IBM entering the market in 1979, and only lived to compete another day after raising $100 million in its 1980 IPO. It was the only remaining competitor after the IBM-induced winnowing.<sup id="article-ref-6"><a href="#ref-6">[6]</a></sup></p>



        <div>
            <p><img loading="lazy" decoding="async" src="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG3-RGB-V3.jpg" srcset="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG3-RGB-V3.jpg 1654w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG3-RGB-V3-402x305.jpg 402w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG3-RGB-V3-462x350.jpg 462w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG3-RGB-V3-662x502.jpg 662w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG3-RGB-V3-722x547.jpg 722w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG3-RGB-V3-982x744.jpg 982w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG3-RGB-V3-1032x782.jpg 1032w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG3-RGB-V3-1402x1062.jpg 1402w" sizes="(min-width: 980px) 60vw, 100vw" alt="" width="1654" height="1253">
            </p>

            
                            <p><em>Source</em>: <sup id="article-ref-6"><a href="#ref-6">[6]</a></sup></p>
                    </div>

    



<p>As the tech took hold and started to show promise, innovations in software, memory, and peripherals like floppy disk drives and modems joined it. They reinforced one another, with each advance putting pressure on the technologies adjacent to it. When any part of the system held back the other parts, investors rushed to fund that sector. As increases in PC memory allowed more complicated software, for example, there became a need for more external storage, which caused VC Dave Marquardt to invest in disk drive manufacturer Seagate in 1980. Seagate gave Marquardt a 40x return when it went public in 1981. Other investors noticed, and some $270 million was plowed into the industry in the following three years.<sup id="article-ref-7"><a href="#ref-7">[7]</a></sup></p>



<p>Money also poured into the underlying infrastructure—fiber optic networks, chip making, etc.—so that capacity was never a bottleneck. Companies which used the new technological system to outperform incumbents began to take market share, and even staid competitors realized they needed to adopt the new thing or die. The hype became a froth which became an investment bubble: the dot-com frenzy of the late 1990s. The ICT wave was therefore similar to previous ones—like the investment mania of the 1830s and the Roaring ‘20s, which followed the infrastructure buildout of canals and railways, respectively—in which the human response to each stage predictably generated the next.</p>



<p>When the dot-com bubble popped, society found it disapproved of the excesses in the sector and governments found they had the popular support to reassert authority over the tech companies and their investors. This put a brake on the madness. Instead of the reckless innovation of the bubble, companies started to expand into proven markets, and financiers moved from speculating to investing. Entrepreneurs began to focus on finding applications rather than on innovating the underlying technologies. Technological improvements continued, but change became more evolutionary than revolutionary.</p>



<p>As change slowed, companies gained the confidence to invest for the longer term. They began to combine various parts of the system in new ways to create value for a wider group of users. The massive overbuilding of fiber optic telecom networks and other infrastructure during the frenzy left plenty of cheap capacity, keeping the costs of expansion down. It was a great time to be a businessperson and investor.</p>



        <div>
            <p><img loading="lazy" decoding="async" src="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG4-RGB-V3.jpg" srcset="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG4-RGB-V3.jpg 1654w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG4-RGB-V3-402x295.jpg 402w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG4-RGB-V3-462x339.jpg 462w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG4-RGB-V3-662x486.jpg 662w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG4-RGB-V3-722x530.jpg 722w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG4-RGB-V3-982x721.jpg 982w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG4-RGB-V3-1032x758.jpg 1032w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG4-RGB-V3-1402x1030.jpg 1402w" sizes="(min-width: 980px) 60vw, 100vw" alt="" width="1654" height="1215">
            </p>

            
                    </div>

    



<p>In contrast, society did not need a bubble to pop to start excoriating AI. Given that the backlash to tech has been going on for a decade, this seems normal to us. But the AI backlash differs from the general high regard, earlier in the cycle, enjoyed by the likes of Bill Gates, Steve Jobs, Jeff Bezos, and others who built big tech businesses. The world hates change, and only gave tech a pass in the ‘80s and ‘90s because it all still seemed reversible: it could be made to go away if it turned out badly. This gave the early computer innovators some leeway to experiment. Now that everyone knows computers are here to stay, AI is not allowed the same wait-and-see attitude. It is seen as <em>part</em> of the ICT revolution.</p>



    




<p><strong>Perez, the economist</strong>, breaks each technological wave into four predictable phases: irruption, frenzy, synergy, and maturity. Each has a characteristic investment profile.</p>



        <div>
            <p><img loading="lazy" decoding="async" src="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG5-RGB-V3.jpg" srcset="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG5-RGB-V3.jpg 1654w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG5-RGB-V3-402x185.jpg 402w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG5-RGB-V3-462x212.jpg 462w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG5-RGB-V3-662x304.jpg 662w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG5-RGB-V3-722x332.jpg 722w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG5-RGB-V3-982x451.jpg 982w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG5-RGB-V3-1032x474.jpg 1032w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG5-RGB-V3-1402x644.jpg 1402w" sizes="(min-width: 980px) 60vw, 100vw" alt="" width="1654" height="760">
            </p>

            
                    </div>

    



<p>The middle two, frenzy and synergy, are the easy ones for investors. Frenzy is when everyone piles in and investors are rewarded for taking big risks on unproven ideas, culminating in the bubble, when paper profits disappear. When rationality returns, the synergy phase begins, as companies make their products usable and productive for a wide array of users. Synergy pays those who are patient, picky, and can bring more than just money to the table.</p>



<p>Irruption and maturity are more difficult to invest in.</p>



<p>Investing in the 1970s was harder than it might look in hindsight. To invest from 1971 through 1975, you had to be either a true believer or a conglomerator with a knuckle-headed diversification strategy. Intel was a great investment, though it looked at first like a previous-wave electronics company. MOS Technologies was founded in 1969 to compete with Texas Instruments but sold a majority of itself to Allen-Bradley to stay afloat. Zilog was funded in 1975 by Exxon (Exxon!). Apple was a great investment, but it had none of the hallmarks of what VCs look for, as the PC was still a solution in search of a problem.</p>



<p>It was later irruption, in the early 1980s, when great opportunities proliferated: PC makers (Compaq, Dell), software and operating systems (Microsoft, Electronic Arts, Adobe), peripherals (Seagate), workstations (Sun), and computer stores (Businessland), among others. If you invested in the winners, you did well. But there was still more money than ideas, which meant that it was no golden age for investing. By 1983, there were more than 70 companies competing in the disk drive sector alone, and valuations collapsed. There were plenty of people whose fortunes were established in the 1970s and 1980s, and many VCs made their names in that era. But the biggest advantage to being an irruption-stage investor was building institutional knowledge to invest early and well in the frenzy and synergy phases.</p>



<p>Investing in the maturity phase is even more difficult. In irruption, it’s hard to see what will happen; in maturity, nothing much happens at all. The uncertainty about what will work and how customers and society will react is almost gone. Things are predictable, and everyone acts predictably.</p>



<p>The lack of dynamism allows the successful synergy companies to remain entrenched (see: the Nifty 50 and FAANG), but growth becomes harder. They start to enter each other’s markets, conglomerate, raise prices, and cut costs. The era of products priced to entice new customers ends, and quality suffers. The big companies continue to embrace the idea of revolutionary innovation, but feel the need to control how their advances are used. R&amp;D spending is redirected from product and process innovation toward increasingly fruitless attempts to find ways to extend the current paradigm. Companies frame this as a drive to win, but it’s really a fear of losing.</p>



<p>Innovation can happen during maturity, sometimes spectacularly. But because these innovations only find support if they fit into the current wave’s paradigm, they are easily captured in the dominant companies’ gravity wells. This means making money as an entrepreneur or investor in them is almost impossible. Generative AI is clearly being captured by the dominant ICT companies, which raises the question of whether this time will be different for inventors and investors—a different question from whether AI itself is a revolutionary technology.</p>



    




<p><strong>Shipping containerization was a late-wave</strong> innovation that changed the world, kicked off our modern era of globalization, resulted in profound changes to society and the economy, and contributed to rapid growth in well-being. But there were, perhaps, only one or two people who made real money investing in it.</p>



<p>The year 1956 was late in the previous wave. But that year, the company soon to be known as SeaLand revolutionized freight shipping with the launch of the first containership, the Ideal-X. SeaLand’s founder, Malcom McLean, had an epiphany that the job to be done by truckers, railroads, and shipping lines was to move goods from shipper to destination, not to drive trucks, fill boxcars, or lade boats. SeaLand allowed freight to transfer seamlessly from one mode to another, saving time, making shipping more predictable, and cutting costs—both the costs of loading, unloading, and reloading, and the cost of a ship sitting idly in port as it was loaded and unloaded.<sup id="article-ref-8"><a href="#ref-8">[8]</a></sup></p>



<p>The benefits of containerization, if it could be made to happen, were obvious. Everybody could see the efficiencies, and customers don’t care how something gets to where they can buy it, as long as it does. But longshoremen would lose work, politicians would lose the votes of those who lost work, port authorities would lose the support of the politicians, federal regulators would be blamed for adverse consequences, railroads might lose freight to shipping lines, shipping lines might lose freight to new shipping lines, and it would all cost a mint. Most thought McLean would never be able to make it work.</p>



<p>McLean squeezed through the cracks of the opposition he faced. He bought and retrofitted war surplus ships, lowering costs. He went after the coastal shipping trade, a dying business in the age of the new interstates, to avoid competition. He set up shop in Newark, NJ, rather than the shipping hub of Hell’s Kitchen, to get buy-in from the port authority and avoid Manhattan congestion. And he made a deal with the New York longshoremen’s union, which was only possible because he was a small player whom they figured was not a threat.</p>



        <div>
            <p><img loading="lazy" decoding="async" src="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG6-RGB-V3.jpg" srcset="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG6-RGB-V3.jpg 1654w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG6-RGB-V3-402x230.jpg 402w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG6-RGB-V3-462x264.jpg 462w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG6-RGB-V3-662x379.jpg 662w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG6-RGB-V3-722x413.jpg 722w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG6-RGB-V3-982x562.jpg 982w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG6-RGB-V3-1032x590.jpg 1032w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG6-RGB-V3-1402x802.jpg 1402w" sizes="(min-width: 980px) 60vw, 100vw" alt="" width="1654" height="946">
            </p>

            
                            <p><em>Source</em>: <sup id="article-ref-10"><a href="#ref-10">[10]</a></sup></p>
                    </div>

    



<p>But competitors and regulators moved too quickly for McLean to seize the few barriers to entry that might have been available to him: domination of the ports, exclusive agreements with shippers or other forms of transportation, standardization on proprietary technology, etc.<sup id="article-ref-9"><a href="#ref-9">[9]</a></sup> When it started to look like it might work, around 1965, the obvious advantages of containerization meant that every large shipping line entered the business, and competition took off. Even though containerized freight was less than 1% of total trade by 1968, the number of containerships was already ramping fast.<sup id="article-ref-10"><a href="#ref-10">[10]</a></sup> Capacity outstripped demand for years.&nbsp;</p>



<p>The increase in competition led to a rate war, which led to squeezed profits, which in turn led to consolidation and cartels. Meanwhile, the cost of building ever-larger container ships and the port facilities to deal with them meant the business became hugely capital intensive. McLean saw the writing on the wall and sold SeaLand to R.J. Reynolds in January 1969. He was, perhaps, the only entrepreneur to get out unscathed.</p>



<p>It took a long time for the end-to-end vision to be realized. But around 1980, a dramatic drop began in the cost of sea freight.<sup id="article-ref-11"><a href="#ref-11">[11]</a></sup> This contributed to a boom in international trade<sup id="article-ref-12"><a href="#ref-12">[12]</a></sup> and allowed manufacturers to move away from higher-wage to lower-wage countries, making containerization irreversible.</p>



        <div>
            <p><img loading="lazy" decoding="async" src="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG7-RGB-V2B.jpg" srcset="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG7-RGB-V2B.jpg 1655w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG7-RGB-V2B-402x236.jpg 402w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG7-RGB-V2B-462x271.jpg 462w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG7-RGB-V2B-662x388.jpg 662w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG7-RGB-V2B-722x424.jpg 722w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG7-RGB-V2B-982x576.jpg 982w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG7-RGB-V2B-1032x605.jpg 1032w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG7-RGB-V2B-1402x823.jpg 1402w" sizes="(min-width: 980px) 60vw, 100vw" alt="" width="1655" height="971">
            </p>

            
                            <p><em>Source</em>: <sup id="article-ref-11"><a href="#ref-11">[11]</a></sup></p>
                    </div>

    



<p>Some people did make money, of course; someone always does. McLean did, as did shipping magnate Daniel Ludwig, who had invested $8.5 million in SeaLand’s predecessor, McLean Industries, at $8.50 per share in 1965 and sold in 1969 for $50 per share.<sup id="article-ref-13"><a href="#ref-13">[13]</a></sup> Shipbuilders made money, too: between 1967 and 1972, some $10 billion ($80 billion in 2025 dollars) was spent building containerships. The contractors that built the new container ports also made money. And, later, shipping lines that consolidated and dominated the business, like Maersk and Evergreen, became very large. But, “for R.J. Reynolds, and for other companies that had chased fast growth by buying into container shipping in the late 1960s, their investments brought little but disappointment.”<sup id="article-ref-14"><a href="#ref-14">[14]</a></sup> Aside from McLean and Ludwig, it is hard to find anyone who became rich from containerization itself, because competition and capex costs made it hard to grow fast or achieve high margins.</p>



        <div>
            <p><img loading="lazy" decoding="async" src="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG8-RGB-V3.jpg" srcset="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG8-RGB-V3.jpg 1655w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG8-RGB-V3-402x320.jpg 402w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG8-RGB-V3-462x368.jpg 462w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG8-RGB-V3-662x527.jpg 662w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG8-RGB-V3-722x575.jpg 722w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG8-RGB-V3-982x782.jpg 982w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG8-RGB-V3-1032x822.jpg 1032w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG8-RGB-V3-1402x1117.jpg 1402w" sizes="(min-width: 980px) 60vw, 100vw" alt="" width="1655" height="1318">
            </p>

            
                            <p><em>Source</em>: <sup id="article-ref-12"><a href="#ref-12">[12]</a></sup></p>
                    </div>

    



<p>The business ended up being dominated primarily by the previous incumbents, and the margins went to the companies shipping goods, not the ones they shipped through. Companies like IKEA benefited from cheap shipping, going from a provincial Scandinavian company in 1972 to the world’s largest furniture retailer by 2008; container shipping was a perfect fit for IKEA’s flat-pack furniture. Others, like Walmart, used the predictability enabled by containerization to lower inventory and its associated costs.</p>



<p>With hindsight, it’s easy to see how you could have invested in containerization: not in the container shipping industry itself, but in the industries that benefited from containerization. But even here, the success of companies like Walmart, Costco, and Target was coupled with the failure of others. The fallout from containerization set Sears and Woolworth on downward spirals, put the final nail in the coffin of Montgomery Ward and A&amp;P, and drove Macy’s into bankruptcy before it was rescued and downsized by Federated. Meanwhile, in North Carolina, “the furniture capital of the world,” furniture makers tried to compete with IKEA by importing cheap pieces from China. They ended up being replaced by their suppliers.<sup id="article-ref-15"><a href="#ref-15">[15]</a></sup></p>



<p>If there had been more time to build moats, there might have been a few dominant containerization companies, and the people behind them would be at the top of the Forbes 400, while their investors would be legendary. But moats take time to build and, unlike the personal computer, the adoption of containerization wasn’t a surprise—every business with interests at stake had a strategic plan immediately.</p>



<p>The economist Joseph Schumpeter said “perfect competition is and always has been temporarily suspended whenever anything new is being introduced.”<sup id="article-ref-16"><a href="#ref-16">[16]</a></sup> But containerization shows this isn’t true at the <em>end</em> of tech waves. And because there is no economic profit during perfect competition, there is no money to be made by innovators during maturity. Like containerization, the introduction of AI did not lead to a period of protected profits for its innovators. It led to an immediate competitive free-for-all.</p>



    




<p><strong>Let’s grant that generative AI is revolutionary</strong> (but also that, as is becoming increasingly clear, this particular tech is now already in an evolutionary stage). It will create a lot of value for the economy, and investors hope to capture some of it. When, who, and how depends on whether AI is the end of the ICT wave, or the beginning of a new one.&nbsp;</p>



<p>If AI had started a new wave, there would have been an extended period of uncertainty and experimentation. There would have been a population of early adopters experimenting with their own models. When thousands or millions of tinkerers use the tech to solve problems in entirely new ways, its uses proliferate. But because they are using models owned by the big AI companies, their ability to fully experiment is limited to what’s allowed by the incumbents, who have no desire to permit an extended challenge to the status quo.</p>



<p>This doesn’t mean AI <em>can’t</em> start the next technological revolution. It might, if experimentation becomes cheap, distributed and permissionless—like Wozniak cobbling together computers in his garage, Ford building his first internal combustion engine in his kitchen, or Trevithick building his high-pressure steam engine as soon as James Watt’s patents expired. When any would-be innovator can build and train an LLM on their laptop and put it to use in any way their imagination dictates, it might be the seed of the next big set of changes—something revolutionary rather than evolutionary. But until and unless that happens, there can be no irruption.</p>



<p>AI is instead the epitome of the ICT wave. The computing visionaries of the 1960s set out to build a machine that could think, which their successors eventually did, by extending gains in algorithms, chips, data, and data center infrastructure. Like containerization, AI is an extension of something that came before, and therefore no one is surprised by what it can and will do. In the 1970s, it took time for people to wrap their heads around the desirability of powerful and ubiquitous computing. But in 2025, machines that think better than previous machines are easy for people to understand.</p>



<p>Consider the extent to which the progress of AI rhymes with the business evolution of containerization:</p>



        <div>
            <p><img loading="lazy" decoding="async" src="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG9-RGB-V2.jpg" srcset="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG9-RGB-V2.jpg 1654w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG9-RGB-V2-402x339.jpg 402w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG9-RGB-V2-462x389.jpg 462w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG9-RGB-V2-662x558.jpg 662w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG9-RGB-V2-722x608.jpg 722w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG9-RGB-V2-982x827.jpg 982w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG9-RGB-V2-1032x869.jpg 1032w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG9-RGB-V2-1402x1181.jpg 1402w" sizes="(min-width: 980px) 60vw, 100vw" alt="" width="1654" height="1393">
            </p>

            
                    </div>

    



<p>In the “AI rhymes” column, the first four items are already underway. How you should invest depends on whether you believe Nos. 5–7 are next.</p>



    




<p><strong>Economists are predicting that</strong> AI will increase global GDP somewhere between 1%<sup id="article-ref-17"><a href="#ref-17">[17]</a></sup> to more than 7%<sup id="article-ref-18"><a href="#ref-18">[18]</a></sup> over the next decade, which is $1–7 trillion of new value created. The big question is where that money will stick as it flows through the value chain.</p>



<p>Most AI market overviews have a score or more categories, breaking each of them into customer and industry served. But these will change dramatically over the next few years. You could, instead, just follow the money to simplify the taxonomy of companies:</p>



        <div>
            <p><img loading="lazy" decoding="async" src="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG10-RGB-V2.jpg" srcset="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG10-RGB-V2.jpg 1654w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG10-RGB-V2-402x101.jpg 402w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG10-RGB-V2-462x116.jpg 462w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG10-RGB-V2-662x166.jpg 662w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG10-RGB-V2-722x181.jpg 722w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG10-RGB-V2-982x246.jpg 982w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG10-RGB-V2-1032x258.jpg 1032w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG10-RGB-V2-1402x351.jpg 1402w" sizes="(min-width: 980px) 60vw, 100vw" alt="" width="1654" height="414">
            </p>

            
                    </div>

    



<p>What the history of containerization suggests is that, if you aren’t already an investor in a model company, you shouldn’t bother. Sam Altman and a few other early movers may make a fortune, as McLean and Ludwig did. But the huge costs of building and running a model, coupled with intense competition, means there will, in the end, be only a few companies, each funded and owned by the largest tech companies. If you’re already an investor, congratulations: There will be consolidation, so you might get an exit.</p>



<p>Domain-specific models—like Cursor or Harvey—will be part of the consolidation. These are probably the most valuable models. But fine-tuning is relatively cheap, and there are big economies of scope. On the other hand, just as Google had to buy Invite Media in 2010 to figure out how to sell to ad agencies, domain-specific model companies that have earned the trust of their customers will be prime acquisition targets. And although it seems possible that models which generate things other than language—like Midjourney or Runway—might use their somewhat different architecture to carve out a separate technological path, the LLM companies have easily entered this space as well. Whether this applies to companies like Osmo remains to be seen.</p>



<p>While it’s too late to invest in the model companies, the profusion of those using the models to solve specific problems is ongoing: Perplexity, InflectionAI, Writer, Abridge, and a hundred others. But if any of these become very valuable, the model companies will take their earnings, either through discriminatory pricing or vertical integration. Success, in other words, will mean defeat—always a bad thesis. At some point, model companies and app companies will converge: There will simply be AI companies, and only a few of them. There will be some winners, as always, but investments in the app layer as a whole will lose money.&nbsp;</p>



<p>The same caveat applies, however: If an app company can build a customer base or an amazing team, it might be acquired. But these companies aren’t really technology companies at all; they are building a market on spec and have to be priced as such. A further caveat is that there will be investors who make a killing arbitraging FOMO-panicked acquirors willing to massively overpay. But this is not really “investing.”</p>



<p>There might be an investment opportunity in companies that manage the interface between the AI giants and their customers, or protect company data from the model companies—like Hugging Face or Glean—because these businesses are by nature independent of the models. But no analogue in the post-containerization shipping market became very large. Even the successful intermediation companies in the AI space will likely end up mid-sized because the model companies will not allow them to gain strategic leverage—another consequence of the absence of surprise.</p>



    




<p><strong>When an industry is going to be big</strong> but there is uncertainty about how it will play out, it often makes sense to swim upstream to the industry’s suppliers. In the case of AI, this means the chip providers, data companies, and cloud/data center companies: SambaNova, Scale AI, and Lambda, as well as those that have been around for a long time, like Nvidia and Bloomberg.</p>



<p>The case for data is mixed. General data—i.e., things most people know, including everything anyone knew more than, say, 10 years ago, and most of what was learned after that—is a commodity. There may be room for a few companies to do the grunt work of collating and tagging it, but since the collating and tagging might best be done by AI itself, there will not be a lot of pricing leverage. Domain-specific models will need specialist data, and other models will try to answer questions about the current moment. Specific, timely, and hard to reproduce data will be valuable. This is not a new market, of course—Bloomberg and others have done well by it. A more concentrated customer base will lower prices for this data, while wider use will raise revenues. On balance, this will probably be a plus for the industry, though not a huge one. There will be new companies built, but only a couple worth investing in.</p>



<p>The high capex of AI companies will primarily be spent with the infrastructure companies. These companies are already valued with this expectation, so there won’t be an upside surprise. But consider that shipbuilding benefited from containerization from 1965 until demand collapsed after about 1973.<sup id="article-ref-19"><a href="#ref-19">[19]</a></sup> If AI companies consolidate or otherwise act in concert, even a slight downturn that forces them to conserve cash could turn into a serious, sudden, and long-lasting decline in infrastructure spending. This would leave companies like Nvidia and its emerging competitors—who must all make long-term commitments to suppliers and for capacity expansion—unable to lower costs to match the new, smaller market size. Companies priced for an s-curve are overpriced if there’s a peak and decline.</p>



        <div>
            <p><img loading="lazy" decoding="async" src="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG11-RGB-1.jpg" srcset="https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG11-RGB-1.jpg 1655w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG11-RGB-1-402x320.jpg 402w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG11-RGB-1-462x368.jpg 462w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG11-RGB-1-662x527.jpg 662w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG11-RGB-1-722x575.jpg 722w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG11-RGB-1-982x782.jpg 982w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG11-RGB-1-1032x822.jpg 1032w, https://joincolossus.com/wp-content/uploads/2025/08/RESEARCH-FIG11-RGB-1-1402x1117.jpg 1402w" sizes="(min-width: 980px) 60vw, 100vw" alt="" width="1655" height="1318">
            </p>

            
                            <p><em>Source</em>: <sup id="article-ref-19"><a href="#ref-19">[19]</a></sup></p>
                    </div>

    



<p>All of which means that investors shouldn’t swim upstream, but fish downstream: companies whose products rely on achieving high-quality results from somewhat ambiguous information will see increased productivity and higher profits. These sectors include professional services, healthcare, education, financial services, and creative services, which together account for between a third and a half of global GDP and have not seen much increased productivity from automation. AI can help lower costs, but as with containerization, how individual businesses incorporate lower costs into their strategies—and what they decide to do with the savings—will determine success. To put it bluntly, using cost savings to increase profits rather than grow revenue is a loser’s game.</p>



<p>The companies that will benefit most rapidly are those whose strategies are already conditional on lowering costs. IKEA’s longtime strategy was to sell quality furniture for low prices and make it up on volume. After containerization made it possible for them to go worldwide, IKEA became the world’s largest retailer and Ingvar Kamprad (the IK of IKEA) became a billionaire. Similarly, Walmart, whose strategy was high volume and low prices in underserved markets, benefited from both cost savings and just-in-time supply chains, allowing increased product variety and lower inventory costs.</p>



<p>Today’s knowledge-work companies that already prioritize the same values are the least risky way to bet on AI, but new companies will form or re-form with a high-volume, low-cost strategy, just as Costco did in the early 1980s. New companies will compete with the incumbents, but with a clean slate and hindsight. Regardless, there are few barriers to entry, so each of these firms will face stiff competition and operate in fragmented markets. Experienced management and flawless execution will be key.</p>



<p>Being an entrepreneur will be a fabulous proposition in these sectors. Being an investor will be harder. Companies will not need much private capital—IKEA never needed to raise risk capital, and Costco raised only one round in 1983 before going public in 1985—because implementing cost-savings technology is not capital intensive. As with containerization, there will be a long lag between technology trigger and the best investments. The opportunities will be later.</p>



<p>Stock pickers will also make money, but they need to be choosy. At the high end of projections, an additional 7% in GDP growth over ten years within one third of the economy gives a tailwind of only about 2% per year to these companies—even less if productivity growth from older ICT products abates. The primary value shift will be to companies that are embracing the strategic implications of AI from companies that are not, the way Walmart benefited from Sears, which took advantage of cheaper goods prices but did not reinvent itself.</p>



<p>Consumers, however, will be the biggest beneficiaries. Previous waves of mechanization benefited labor productivity in manufacturing, driving prices down and saving consumers money. But increased labor productivity in manufacturing also led to higher manufacturing wages. Wages in services businesses had to rise to compete, even though these businesses did not benefit from productivity gains. This caused the price of services to rise.<sup id="article-ref-20"><a href="#ref-20">[20]</a></sup> The share of household spending on food and clothing went from 55% in 1918 to 16% in 2023,<sup id="article-ref-21"><a href="#ref-21">[21]</a></sup> but the cost of knowledge-intensive services like healthcare and education have grown well above inflation.&nbsp;</p>



<p>Something similar will happen with AI: Knowledge-intensive services will get cheaper, allowing consumers to buy more of them, while services that require person-to-person interaction will get more expensive, taking up a greater percentage of household spending. This points to obvious opportunities in both. But the big news is that most of the new value created by AI will be captured by consumers, who should see a wider variety of knowledge-intensive goods at reasonable prices, and wider and more affordable access to services like medical care, education, and advice.</p>



    




<p><strong>There is nothing better than the beginning of a new wave</strong>, when the opportunities to envision, invent, and build world-changing companies leads to money, fame, and glory. But there is nothing more dangerous for investors and entrepreneurs than wishful thinking. The lessons learned from investing in tech over the last 50 years are not the right ones to apply now. The way to invest in AI is to think through the implications of knowledge workers becoming more efficient, to imagine what markets this efficiency unlocks, and to invest in those. For decades, the way to make money was to bet on what the new thing was. Now, you have to bet on the opportunities it opens up.</p>



    




<p><strong>Jerry Neumann</strong> is a retired venture investor, writing and teaching about innovation.</p>



    

                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Myocardial infarction may be an infectious disease (424 pts)]]></title>
            <link>https://www.tuni.fi/en/news/myocardial-infarction-may-be-infectious-disease</link>
            <guid>45235648</guid>
            <pubDate>Sat, 13 Sep 2025 21:55:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tuni.fi/en/news/myocardial-infarction-may-be-infectious-disease">https://www.tuni.fi/en/news/myocardial-infarction-may-be-infectious-disease</a>, See on <a href="https://news.ycombinator.com/item?id=45235648">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span>A pioneering study by researchers from Finland and the UK has demonstrated for the first time that myocardial infarction may be an infectious disease. This discovery challenges the conventional understanding of the pathogenesis of myocardial infarction and opens new avenues for treatment, diagnostics, and even vaccine development.</span></p><div color="brand-purple"><p>According to the recently published research, an infection may trigger myocardial infarction. Using a range of advanced methodologies, the research found that, in coronary artery disease, atherosclerotic plaques containing cholesterol may harbour a gelatinous, asymptomatic biofilm formed by bacteria over years or even decades. Dormant bacteria within the biofilm remain shielded from both the patient’s immune system and antibiotics because they cannot penetrate the biofilm matrix.</p><p>A viral infection or another external trigger may activate the biofilm, leading to the proliferation of bacteria and an inflammatory response. The inflammation can cause a rupture in the fibrous cap of the plaque, resulting in thrombus formation and ultimately myocardial infarction.</p><p>Professor <strong>Pekka Karhunen</strong>, who led the study, notes that until now, it was assumed that events leading to coronary artery disease were only initiated by oxidised low-density lipoprotein (LDL), which the body recognises as a foreign structure.</p><p>“Bacterial involvement in coronary artery disease has long been suspected, but direct and convincing evidence has been lacking. Our study demonstrated the presence of genetic material – DNA – from several oral bacteria inside atherosclerotic plaques,” Karhunen explains.</p><p>The findings were validated by developing an antibody targeted at the discovered bacteria, which unexpectedly revealed biofilm structures in arterial tissue. Bacteria released from the biofilm were observed in cases of myocardial infarction. The body’s immune system had responded to these bacteria, triggering inflammation which ruptured the cholesterol-laden plaque.</p><p>The observations pave the way for the development of novel diagnostic and therapeutic strategies for myocardial infarction. Furthermore, they advance the possibility of preventing coronary artery disease and myocardial infarction by vaccination.</p><p>The study was conducted by Tampere and Oulu Universities, Finnish Institute for Health and Welfare and the University of Oxford. Tissue samples were obtained from individuals who had died from sudden cardiac death, as well as from patients with atherosclerosis who were undergoing surgery to cleanse carotid and peripheral arteries.</p><p>The research is part of an extensive EU-funded cardiovascular research project involving 11 countries. Significant funding was also provided by the Finnish Foundation for Cardiovascular Research and Jane and Aatos Erkko Foundation.&nbsp;</p><p>The research article <em>Viridans Streptococcal Biofilm Evades Immune Detection and Contributes to Inflammation and Rupture of Atherosclerotic Plaques</em> was published in the Journal of the American Heart Association on 6 August 2025. <a href="https://doi.org/10.1161/JAHA.125.041521">Read the article online</a></p><h2><br>Further information</h2><p>Professor Pekka Karhunen<br>Faculty of Medicine and Health Technology<br>Tampere University<br><span><span>pekka.j.karhunen</span> [at] <span>tuni.fi</span><span> (pekka[dot]j[dot]karhunen[at]tuni[dot]fi)</span></span><br>Tel. +358 400 511361</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD’s RDNA4 GPU architecture (111 pts)]]></title>
            <link>https://chipsandcheese.com/p/amds-rdna4-gpu-architecture-at-hot</link>
            <guid>45235293</guid>
            <pubDate>Sat, 13 Sep 2025 21:04:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/p/amds-rdna4-gpu-architecture-at-hot">https://chipsandcheese.com/p/amds-rdna4-gpu-architecture-at-hot</a>, See on <a href="https://news.ycombinator.com/item?id=45235293">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>RDNA4 is AMD’s latest graphics-focused architecture, and fills out their RX 9000 line of discrete GPUs. AMD noted that creating a good gaming GPU requires understanding both current workloads, as well as taking into account what workloads might look like five years in the future. Thus AMD has been trying to improve efficiency across rasterization, compute, and raytracing. Machine learning has gained importance including in games, so AMD’s new GPU architecture caters to ML workloads as well.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!0Oe5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb413d308-0974-4b4c-856c-5c9ce4004982_1600x896.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!0Oe5!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb413d308-0974-4b4c-856c-5c9ce4004982_1600x896.png 424w, https://substackcdn.com/image/fetch/$s_!0Oe5!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb413d308-0974-4b4c-856c-5c9ce4004982_1600x896.png 848w, https://substackcdn.com/image/fetch/$s_!0Oe5!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb413d308-0974-4b4c-856c-5c9ce4004982_1600x896.png 1272w, https://substackcdn.com/image/fetch/$s_!0Oe5!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb413d308-0974-4b4c-856c-5c9ce4004982_1600x896.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!0Oe5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb413d308-0974-4b4c-856c-5c9ce4004982_1600x896.png" width="1456" height="815" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b413d308-0974-4b4c-856c-5c9ce4004982_1600x896.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:815,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!0Oe5!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb413d308-0974-4b4c-856c-5c9ce4004982_1600x896.png 424w, https://substackcdn.com/image/fetch/$s_!0Oe5!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb413d308-0974-4b4c-856c-5c9ce4004982_1600x896.png 848w, https://substackcdn.com/image/fetch/$s_!0Oe5!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb413d308-0974-4b4c-856c-5c9ce4004982_1600x896.png 1272w, https://substackcdn.com/image/fetch/$s_!0Oe5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb413d308-0974-4b4c-856c-5c9ce4004982_1600x896.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>From AMD’s perspective, RDNA4 represents a large efficiency leap in raytracing and machine learning, while also improving on the rasterization front. Improved compression helps keep the graphics architecture fed. Outside of the GPU’s core graphics acceleration responsibility, RDNA4 brings improved media and display capabilities to round out the package.</p><p>The Media Engine provides hardware accelerated video encode and decode for a wide range of codecs. High end RDNA4 parts like the RX 9070XT have two media engines. RDNA4’s media engines feature faster decoding speed, helping save power during video playback by racing to idle. For video encoding, AMD targeted better quality in H.265, H.265, and AV1, especially in low latency encoding.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!aWeF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb40a4273-7bc3-4f30-a12b-b2b45c188113_1595x895.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!aWeF!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb40a4273-7bc3-4f30-a12b-b2b45c188113_1595x895.png 424w, https://substackcdn.com/image/fetch/$s_!aWeF!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb40a4273-7bc3-4f30-a12b-b2b45c188113_1595x895.png 848w, https://substackcdn.com/image/fetch/$s_!aWeF!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb40a4273-7bc3-4f30-a12b-b2b45c188113_1595x895.png 1272w, https://substackcdn.com/image/fetch/$s_!aWeF!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb40a4273-7bc3-4f30-a12b-b2b45c188113_1595x895.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!aWeF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb40a4273-7bc3-4f30-a12b-b2b45c188113_1595x895.png" width="1456" height="817" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b40a4273-7bc3-4f30-a12b-b2b45c188113_1595x895.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:817,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!aWeF!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb40a4273-7bc3-4f30-a12b-b2b45c188113_1595x895.png 424w, https://substackcdn.com/image/fetch/$s_!aWeF!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb40a4273-7bc3-4f30-a12b-b2b45c188113_1595x895.png 848w, https://substackcdn.com/image/fetch/$s_!aWeF!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb40a4273-7bc3-4f30-a12b-b2b45c188113_1595x895.png 1272w, https://substackcdn.com/image/fetch/$s_!aWeF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb40a4273-7bc3-4f30-a12b-b2b45c188113_1595x895.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>Low latency encoder modes are mostly beneficial for streaming, where delays caused by the media engine ultimately translate to a delayed stream. Reducing latency can make quality optimizations more challenging. Video codecs strive to encode differences between frames to economize storage. Buffering up more frames gives the encoder more opportunities to look for similar content across frames, and lets it allocate more bitrate budget for difficult sequences. But buffering up frames introduces latency. Another challenge is some popular streaming platforms mainly </span><a href="https://help.twitch.tv/s/article/broadcasting-guidelines?language=en_US" rel="">use H.264</a><span>, an older codec that’s less efficient than AV1. Newer codecs are being tested, so the situation may start to change as the next few decades fly by. But for now, H.264 remains important due to its wide support.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!jr2M!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad94c3c7-1aed-43fa-b21c-9fca5fcd44cd_1105x626.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!jr2M!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad94c3c7-1aed-43fa-b21c-9fca5fcd44cd_1105x626.png 424w, https://substackcdn.com/image/fetch/$s_!jr2M!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad94c3c7-1aed-43fa-b21c-9fca5fcd44cd_1105x626.png 848w, https://substackcdn.com/image/fetch/$s_!jr2M!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad94c3c7-1aed-43fa-b21c-9fca5fcd44cd_1105x626.png 1272w, https://substackcdn.com/image/fetch/$s_!jr2M!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad94c3c7-1aed-43fa-b21c-9fca5fcd44cd_1105x626.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!jr2M!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad94c3c7-1aed-43fa-b21c-9fca5fcd44cd_1105x626.png" width="1105" height="626" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ad94c3c7-1aed-43fa-b21c-9fca5fcd44cd_1105x626.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:626,&quot;width&quot;:1105,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!jr2M!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad94c3c7-1aed-43fa-b21c-9fca5fcd44cd_1105x626.png 424w, https://substackcdn.com/image/fetch/$s_!jr2M!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad94c3c7-1aed-43fa-b21c-9fca5fcd44cd_1105x626.png 848w, https://substackcdn.com/image/fetch/$s_!jr2M!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad94c3c7-1aed-43fa-b21c-9fca5fcd44cd_1105x626.png 1272w, https://substackcdn.com/image/fetch/$s_!jr2M!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad94c3c7-1aed-43fa-b21c-9fca5fcd44cd_1105x626.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>Testing with an old gameplay clip from Elder Scrolls Online shows a clear advantage for RDNA4’s media engine when testing with the latency-constrained VBR mode and encoder tuned for low latency encoding (-usage lowlatency -rc vbr_latency). Netflix’s VMAF video quality metric gives higher scores for RDNA4 throughout the bitrate range. Closer inspection generally agrees with the VMAF metric.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Fz8q!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb88e5ae-470a-4e05-b964-5befb82b248d_901x365.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Fz8q!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb88e5ae-470a-4e05-b964-5befb82b248d_901x365.png 424w, https://substackcdn.com/image/fetch/$s_!Fz8q!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb88e5ae-470a-4e05-b964-5befb82b248d_901x365.png 848w, https://substackcdn.com/image/fetch/$s_!Fz8q!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb88e5ae-470a-4e05-b964-5befb82b248d_901x365.png 1272w, https://substackcdn.com/image/fetch/$s_!Fz8q!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb88e5ae-470a-4e05-b964-5befb82b248d_901x365.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Fz8q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb88e5ae-470a-4e05-b964-5befb82b248d_901x365.png" width="901" height="365" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/eb88e5ae-470a-4e05-b964-5befb82b248d_901x365.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:365,&quot;width&quot;:901,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Fz8q!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb88e5ae-470a-4e05-b964-5befb82b248d_901x365.png 424w, https://substackcdn.com/image/fetch/$s_!Fz8q!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb88e5ae-470a-4e05-b964-5befb82b248d_901x365.png 848w, https://substackcdn.com/image/fetch/$s_!Fz8q!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb88e5ae-470a-4e05-b964-5befb82b248d_901x365.png 1272w, https://substackcdn.com/image/fetch/$s_!Fz8q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb88e5ae-470a-4e05-b964-5befb82b248d_901x365.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>RDNA4 does a better job preserving high contrast outlines. Differences are especially visible around text, which RDNA4 handles better than its predecessor while using a lower bitrate. Neither result looks great with such a close look, with blurred text on both examples and fine detail crushed in video encoding artifacts. But it’s worth remembering that the latency-constrained VBR mode uses a VBV buffer of </span><a href="https://github.com/GPUOpen-LibrariesAndSDKs/AMF/wiki/Rate-Control-Methods" rel="">up to three frames</a><span>, while higher latency modes can use VBV buffer sizes covering multiple seconds of video. Encoding speed has improved slightly as well, jumping from ~190 to ~200 FPS from RDNA3.5 to RDNA4.</span></p><p>The display engine fetches on-screen frame data from memory, composites it into a final image, and drives it to the display outputs. It’s a basic task that most people take for granted, but the display engine is also a good place to perform various image enhancements. A traditional example is using a lookup table to apply color correction. Enhancements at the display engine are invisible to user software, and are typically carried out in hardware with minimal power cost. On RDNA4, AMD added a “Radeon Image Sharpening” filter, letting the display engine sharpen the final image. Using dedicated hardware at the display engine instead of the GPU’s programmable shaders means that the sharpening filter won’t impact performance and can be carried out with better power efficiency. And, AMD doesn’t need to rely on game developers to implement the effect. Sharpening can even apply to the desktop, though I’m not sure why anyone would want that.</p><p>Power consumption is another important optimization area for display engines. Traditionally that’s been more of a concern for mobile products, where maximizing battery life under low load is a top priority. But RDNA4 has taken aim at multi-monitor idle power with its newer display engine. AMD’s presentation stated that they took advantage of variable refresh rates on FreeSync displays. They didn’t go into more detail, but it’s easy to imagine what AMD might be doing. High resolution and high refresh rate displays translate to high pixel rates. That in turn drives higher memory bandwidth demands. Dynamically lowering refresh rates could let RDNA4’s memory subsystem enter a low power state while still meeting refresh deadlines.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!5ZHj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21c6a4ce-ab6d-4e68-85c2-974f1928e10d_591x140.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!5ZHj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21c6a4ce-ab6d-4e68-85c2-974f1928e10d_591x140.png 424w, https://substackcdn.com/image/fetch/$s_!5ZHj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21c6a4ce-ab6d-4e68-85c2-974f1928e10d_591x140.png 848w, https://substackcdn.com/image/fetch/$s_!5ZHj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21c6a4ce-ab6d-4e68-85c2-974f1928e10d_591x140.png 1272w, https://substackcdn.com/image/fetch/$s_!5ZHj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21c6a4ce-ab6d-4e68-85c2-974f1928e10d_591x140.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!5ZHj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21c6a4ce-ab6d-4e68-85c2-974f1928e10d_591x140.png" width="591" height="140" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/21c6a4ce-ab6d-4e68-85c2-974f1928e10d_591x140.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:140,&quot;width&quot;:591,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!5ZHj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21c6a4ce-ab6d-4e68-85c2-974f1928e10d_591x140.png 424w, https://substackcdn.com/image/fetch/$s_!5ZHj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21c6a4ce-ab6d-4e68-85c2-974f1928e10d_591x140.png 848w, https://substackcdn.com/image/fetch/$s_!5ZHj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21c6a4ce-ab6d-4e68-85c2-974f1928e10d_591x140.png 1272w, https://substackcdn.com/image/fetch/$s_!5ZHj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F21c6a4ce-ab6d-4e68-85c2-974f1928e10d_591x140.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Power and GDDR6 data rates for various refresh rate combinations. AMD’s monitoring software (and others) read out extremely low memory clocks when the memory bus is able to idle, so those readings aren’t listed.</figcaption></figure></div><p>I have a RX 9070 hooked up to a Viotek GN24CW 1080P display via HDMI, and a MSI MAG271QX 1440P capable of refresh rates up to 360 Hz. The latter is connected via DisplayPort. The RX 9070 manages to keep memory at idle clocks even at high refresh rate settings. Moving the mouse causes the card to ramp up memory clocks and consume more power, hinting that RDNA4 is lowering refresh rates when screen contents don’t change. Additionally, RDNA4 gets an intermediate GDDR6 power state that lets it handle the 1080P 60 Hz + 1440P 240 Hz combination without going to maximum memory clocks. On RDNA2, it’s more of an all or nothing situation. The older card is more prone to ramping up memory clocks to handle high pixel rates, and power consumption remains high even when screen contents don’t change.</p><p>RDNA4’s Workgroup Processor retains the same high level layout as prior RDNA generations. However, it gets major improvements targeted towards raytracing, like improved raytracing units and wider BVH nodes, a dynamic register allocation mode, and a scheduler that no longer suffers false memory dependencies between waves. I covered those in previous articles. Besides those improvements, AMD’s presentation went over a couple other details worth discussing.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!sEei!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8270e398-c426-486d-94e0-6819aa177a20_1600x895.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!sEei!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8270e398-c426-486d-94e0-6819aa177a20_1600x895.png 424w, https://substackcdn.com/image/fetch/$s_!sEei!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8270e398-c426-486d-94e0-6819aa177a20_1600x895.png 848w, https://substackcdn.com/image/fetch/$s_!sEei!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8270e398-c426-486d-94e0-6819aa177a20_1600x895.png 1272w, https://substackcdn.com/image/fetch/$s_!sEei!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8270e398-c426-486d-94e0-6819aa177a20_1600x895.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!sEei!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8270e398-c426-486d-94e0-6819aa177a20_1600x895.png" width="1456" height="814" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8270e398-c426-486d-94e0-6819aa177a20_1600x895.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:814,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!sEei!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8270e398-c426-486d-94e0-6819aa177a20_1600x895.png 424w, https://substackcdn.com/image/fetch/$s_!sEei!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8270e398-c426-486d-94e0-6819aa177a20_1600x895.png 848w, https://substackcdn.com/image/fetch/$s_!sEei!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8270e398-c426-486d-94e0-6819aa177a20_1600x895.png 1272w, https://substackcdn.com/image/fetch/$s_!sEei!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8270e398-c426-486d-94e0-6819aa177a20_1600x895.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>AMD has a long history of using a scalar unit to offload operations that are constant across a wave. Scalar offload saves power by avoiding redundant computation, and frees up the vector unit to increase performance in compute-bound sequences. RDNA4’s scalar unit gains a few floating point instructions, expanding scalar offload opportunities. This capability debuted on RDNA3.5, but RDNA4 brings it to discrete GPUs.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!kZ5z!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb03eff64-8afc-4928-bb6c-2e0f3f17c133_926x546.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!kZ5z!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb03eff64-8afc-4928-bb6c-2e0f3f17c133_926x546.png 424w, https://substackcdn.com/image/fetch/$s_!kZ5z!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb03eff64-8afc-4928-bb6c-2e0f3f17c133_926x546.png 848w, https://substackcdn.com/image/fetch/$s_!kZ5z!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb03eff64-8afc-4928-bb6c-2e0f3f17c133_926x546.png 1272w, https://substackcdn.com/image/fetch/$s_!kZ5z!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb03eff64-8afc-4928-bb6c-2e0f3f17c133_926x546.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!kZ5z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb03eff64-8afc-4928-bb6c-2e0f3f17c133_926x546.png" width="926" height="546" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b03eff64-8afc-4928-bb6c-2e0f3f17c133_926x546.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:546,&quot;width&quot;:926,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!kZ5z!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb03eff64-8afc-4928-bb6c-2e0f3f17c133_926x546.png 424w, https://substackcdn.com/image/fetch/$s_!kZ5z!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb03eff64-8afc-4928-bb6c-2e0f3f17c133_926x546.png 848w, https://substackcdn.com/image/fetch/$s_!kZ5z!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb03eff64-8afc-4928-bb6c-2e0f3f17c133_926x546.png 1272w, https://substackcdn.com/image/fetch/$s_!kZ5z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb03eff64-8afc-4928-bb6c-2e0f3f17c133_926x546.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>While not discussed in AMD’s presentation, scalar offload can bring additional performance benefits because scalar instructions sometimes have lower latency than their vector counterparts. Most basic vector instructions on RDNA4 have 5 cycle latency. FP32 adds and multiples on the scalar unit have 4 cycle latency. The biggest latency benefits still come from offloading integer operations though.</p><p>GPUs use barriers to synchronize threads and enforce memory ordering. For example, a s_barrier instruction on older AMD GPUs would cause a thread to wait until all of its peers in the workgroup also reached the s_barrier instruction. Barriers degrade performance because any thread that happened to reach the barrier faster would have to stall until its peers catch up.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Bv-N!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd809ae87-fff4-43b7-b5e7-3cb1951f22ae_698x382.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Bv-N!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd809ae87-fff4-43b7-b5e7-3cb1951f22ae_698x382.png 424w, https://substackcdn.com/image/fetch/$s_!Bv-N!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd809ae87-fff4-43b7-b5e7-3cb1951f22ae_698x382.png 848w, https://substackcdn.com/image/fetch/$s_!Bv-N!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd809ae87-fff4-43b7-b5e7-3cb1951f22ae_698x382.png 1272w, https://substackcdn.com/image/fetch/$s_!Bv-N!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd809ae87-fff4-43b7-b5e7-3cb1951f22ae_698x382.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Bv-N!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd809ae87-fff4-43b7-b5e7-3cb1951f22ae_698x382.png" width="698" height="382" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d809ae87-fff4-43b7-b5e7-3cb1951f22ae_698x382.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:382,&quot;width&quot;:698,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Bv-N!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd809ae87-fff4-43b7-b5e7-3cb1951f22ae_698x382.png 424w, https://substackcdn.com/image/fetch/$s_!Bv-N!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd809ae87-fff4-43b7-b5e7-3cb1951f22ae_698x382.png 848w, https://substackcdn.com/image/fetch/$s_!Bv-N!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd809ae87-fff4-43b7-b5e7-3cb1951f22ae_698x382.png 1272w, https://substackcdn.com/image/fetch/$s_!Bv-N!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd809ae87-fff4-43b7-b5e7-3cb1951f22ae_698x382.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>RDNA4 splits the barrier into separate “signal” and “wait” actions. Instead of s_barrier, RDNA4 has s_barrier_signal and s_barrier_wait. A thread can “signal” the barrier once it produces data that other threads might need. It can then do independent work, and only wait on the barrier once it needs to use data produced by other threads. The s_barrier_wait will then stall the thread until all other threads in the workgroup have signalled the barrier.</p><p>The largest RDNA4 variants have a 8 MB L2 cache, representing a substantial L2 capacity increase compared to prior RDNA generations. RDNA3 and RDNA2 maxed out at 6 MB and 4 MB L2 capacities, respectively. AMD found that difficult workloads like raytracing benefit from the larger L2. Raytracing involves pointer chasing during BVH traversal, and it’s not surprising that it’s more sensitive to accesses getting serviced from the slower Infinity Cache as opposed to L2. In the initial scene in 3DMark’s DXR feature test, run in Explorer Mode, RDNA4 dramatically cuts down the amount of data that has to be fetched from beyond L2.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!XRgN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbab4be2-276d-430a-b64c-35ea0823a2e5_952x282.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!XRgN!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbab4be2-276d-430a-b64c-35ea0823a2e5_952x282.png 424w, https://substackcdn.com/image/fetch/$s_!XRgN!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbab4be2-276d-430a-b64c-35ea0823a2e5_952x282.png 848w, https://substackcdn.com/image/fetch/$s_!XRgN!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbab4be2-276d-430a-b64c-35ea0823a2e5_952x282.png 1272w, https://substackcdn.com/image/fetch/$s_!XRgN!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbab4be2-276d-430a-b64c-35ea0823a2e5_952x282.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!XRgN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbab4be2-276d-430a-b64c-35ea0823a2e5_952x282.png" width="952" height="282" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cbab4be2-276d-430a-b64c-35ea0823a2e5_952x282.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:282,&quot;width&quot;:952,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!XRgN!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbab4be2-276d-430a-b64c-35ea0823a2e5_952x282.png 424w, https://substackcdn.com/image/fetch/$s_!XRgN!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbab4be2-276d-430a-b64c-35ea0823a2e5_952x282.png 848w, https://substackcdn.com/image/fetch/$s_!XRgN!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbab4be2-276d-430a-b64c-35ea0823a2e5_952x282.png 1272w, https://substackcdn.com/image/fetch/$s_!XRgN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbab4be2-276d-430a-b64c-35ea0823a2e5_952x282.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>RDNA2 still does a good job of keeping data in L2 in absolute terms. But it’s worth noting that hitting Infinity Cache on both platforms adds more than 50 ns of extra latency over a L2 hit. That’s well north of 100 cycles because both RDNA2 and RDNA4 run above 2 GHz. While AMD’s graphics strategy has shifted towards making the faster caches bigger, it still contrasts with Nvidia’s strategy of putting way more eggs in the L2 basket. Blackwell’s L2 cache serves the functions of both AMD’s L2 and Infinity Cache, and has latency between those two cache levels. Nvidia also has a flexible L1/shared memory allocation scheme that can give them more low latency caching capacity in front of L2, depending on a workload’s requested local storage (shared memory) capacity.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Q2Ta!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cd66a95-c764-4cea-9e6a-b03a078ac74c_1600x804.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Q2Ta!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cd66a95-c764-4cea-9e6a-b03a078ac74c_1600x804.png 424w, https://substackcdn.com/image/fetch/$s_!Q2Ta!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cd66a95-c764-4cea-9e6a-b03a078ac74c_1600x804.png 848w, https://substackcdn.com/image/fetch/$s_!Q2Ta!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cd66a95-c764-4cea-9e6a-b03a078ac74c_1600x804.png 1272w, https://substackcdn.com/image/fetch/$s_!Q2Ta!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cd66a95-c764-4cea-9e6a-b03a078ac74c_1600x804.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Q2Ta!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cd66a95-c764-4cea-9e6a-b03a078ac74c_1600x804.png" width="1456" height="732" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4cd66a95-c764-4cea-9e6a-b03a078ac74c_1600x804.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:732,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Q2Ta!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cd66a95-c764-4cea-9e6a-b03a078ac74c_1600x804.png 424w, https://substackcdn.com/image/fetch/$s_!Q2Ta!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cd66a95-c764-4cea-9e6a-b03a078ac74c_1600x804.png 848w, https://substackcdn.com/image/fetch/$s_!Q2Ta!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cd66a95-c764-4cea-9e6a-b03a078ac74c_1600x804.png 1272w, https://substackcdn.com/image/fetch/$s_!Q2Ta!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cd66a95-c764-4cea-9e6a-b03a078ac74c_1600x804.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>A mid-level L1 cache was a familiar fixture on prior RDNA generations. It’s conspicuously missing from RDNA4, as well as AMD’s presentation. One possibility is that L1 cache hitrate wasn’t high enough to justify the complexity of an extra cache level. Perhaps AMD felt its area and transistor budget was better allocated towards increasing L2 capacity. To support this theory, L1 hitrate on RDNA1 was often below 50%. At the same time, the RDNA series always enjoyed a high bandwidth and low latency L2. Putting more pressure on L2 in exchange for reducing L2 misses may have been an enticing tradeoff. Another possibility is that AMD ran into validation issues with the L1 cache and decided to skip it for this generation. There’s no way to verify either possibility of course, but I think the former reasons make more sense.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!gorR!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa2f5e19-2cdd-433a-855d-ef90611ff448_930x892.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!gorR!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa2f5e19-2cdd-433a-855d-ef90611ff448_930x892.png 424w, https://substackcdn.com/image/fetch/$s_!gorR!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa2f5e19-2cdd-433a-855d-ef90611ff448_930x892.png 848w, https://substackcdn.com/image/fetch/$s_!gorR!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa2f5e19-2cdd-433a-855d-ef90611ff448_930x892.png 1272w, https://substackcdn.com/image/fetch/$s_!gorR!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa2f5e19-2cdd-433a-855d-ef90611ff448_930x892.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!gorR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa2f5e19-2cdd-433a-855d-ef90611ff448_930x892.png" width="930" height="892" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aa2f5e19-2cdd-433a-855d-ef90611ff448_930x892.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:892,&quot;width&quot;:930,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!gorR!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa2f5e19-2cdd-433a-855d-ef90611ff448_930x892.png 424w, https://substackcdn.com/image/fetch/$s_!gorR!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa2f5e19-2cdd-433a-855d-ef90611ff448_930x892.png 848w, https://substackcdn.com/image/fetch/$s_!gorR!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa2f5e19-2cdd-433a-855d-ef90611ff448_930x892.png 1272w, https://substackcdn.com/image/fetch/$s_!gorR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa2f5e19-2cdd-433a-855d-ef90611ff448_930x892.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Beyond tweaking the cache hierarchy, RDNA4 brings improvements to transparent compression. AMD emphasized that they’re using compression throughout the SoC, including at points like the display engine and media engine. Compressed data can be stored in caches, and decompressed before being written back to memory. Compression cuts down on data transfer, which reduces bandwidth requirements and improves power efficiency.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Lkk0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd6b4667-39e3-453b-adf2-31e229f9b89e_1600x897.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Lkk0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd6b4667-39e3-453b-adf2-31e229f9b89e_1600x897.png 424w, https://substackcdn.com/image/fetch/$s_!Lkk0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd6b4667-39e3-453b-adf2-31e229f9b89e_1600x897.png 848w, https://substackcdn.com/image/fetch/$s_!Lkk0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd6b4667-39e3-453b-adf2-31e229f9b89e_1600x897.png 1272w, https://substackcdn.com/image/fetch/$s_!Lkk0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd6b4667-39e3-453b-adf2-31e229f9b89e_1600x897.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Lkk0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd6b4667-39e3-453b-adf2-31e229f9b89e_1600x897.png" width="1456" height="816" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fd6b4667-39e3-453b-adf2-31e229f9b89e_1600x897.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:816,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Lkk0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd6b4667-39e3-453b-adf2-31e229f9b89e_1600x897.png 424w, https://substackcdn.com/image/fetch/$s_!Lkk0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd6b4667-39e3-453b-adf2-31e229f9b89e_1600x897.png 848w, https://substackcdn.com/image/fetch/$s_!Lkk0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd6b4667-39e3-453b-adf2-31e229f9b89e_1600x897.png 1272w, https://substackcdn.com/image/fetch/$s_!Lkk0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd6b4667-39e3-453b-adf2-31e229f9b89e_1600x897.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Transparent compression is not a new feature. It has a long history of being one tool in the GPU toolbox for reducing memory bandwidth usage, and it would be difficult to find any modern GPU without compression features of some sort. Even compression in other blocks like the display engine have precedent. Intel’s display engines for example use Framebuffer Compression (FBC), which can write a compressed copy of frame data and keep fetching the compressed copy to reduce data transfer power usage as long as the data doesn’t change. Prior RDNA generations had compression features too, and AMD’s</span><a href="https://gpuopen.com/manuals/rgp_manual/overview_windows/" rel="">documentation </a><span>summarizes some compression targets. While AMD didn’t talk about compression efficiency, I tried to take similar frame captures using RGP on both RDNA1 and RDNA4 to see if there’s a large difference in memory access per frame. It didn’t quite work out the way I expected, but I’ll put them here anyway and discuss why evaluating compression efficacy is challenging.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!BSLN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea8bc5f3-a77e-4302-b9cb-9f684ebb6258_1025x529.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!BSLN!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea8bc5f3-a77e-4302-b9cb-9f684ebb6258_1025x529.png 424w, https://substackcdn.com/image/fetch/$s_!BSLN!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea8bc5f3-a77e-4302-b9cb-9f684ebb6258_1025x529.png 848w, https://substackcdn.com/image/fetch/$s_!BSLN!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea8bc5f3-a77e-4302-b9cb-9f684ebb6258_1025x529.png 1272w, https://substackcdn.com/image/fetch/$s_!BSLN!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea8bc5f3-a77e-4302-b9cb-9f684ebb6258_1025x529.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!BSLN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea8bc5f3-a77e-4302-b9cb-9f684ebb6258_1025x529.png" width="1025" height="529" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ea8bc5f3-a77e-4302-b9cb-9f684ebb6258_1025x529.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:529,&quot;width&quot;:1025,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!BSLN!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea8bc5f3-a77e-4302-b9cb-9f684ebb6258_1025x529.png 424w, https://substackcdn.com/image/fetch/$s_!BSLN!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea8bc5f3-a77e-4302-b9cb-9f684ebb6258_1025x529.png 848w, https://substackcdn.com/image/fetch/$s_!BSLN!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea8bc5f3-a77e-4302-b9cb-9f684ebb6258_1025x529.png 1272w, https://substackcdn.com/image/fetch/$s_!BSLN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea8bc5f3-a77e-4302-b9cb-9f684ebb6258_1025x529.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The first challenge is that both architectures satisfy most memory requests from L0 or L1. AMD slides on RDNA1 suggest the L0 and L1 only hold decompressed data, at least for delta color compression. Compression does apply to L2. For RDNA4, AMD’s slides indicate it applies to the Infinity Cache too. However, focusing on data transfer to and from the L2 wouldn’t work due the large cache hierarchy differences between those RDNA generations.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ZENF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F056c0a6c-870f-4d73-9c08-df4c653c52f5_1273x715.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ZENF!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F056c0a6c-870f-4d73-9c08-df4c653c52f5_1273x715.png 424w, https://substackcdn.com/image/fetch/$s_!ZENF!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F056c0a6c-870f-4d73-9c08-df4c653c52f5_1273x715.png 848w, https://substackcdn.com/image/fetch/$s_!ZENF!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F056c0a6c-870f-4d73-9c08-df4c653c52f5_1273x715.png 1272w, https://substackcdn.com/image/fetch/$s_!ZENF!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F056c0a6c-870f-4d73-9c08-df4c653c52f5_1273x715.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ZENF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F056c0a6c-870f-4d73-9c08-df4c653c52f5_1273x715.png" width="1273" height="715" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/056c0a6c-870f-4d73-9c08-df4c653c52f5_1273x715.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:715,&quot;width&quot;:1273,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ZENF!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F056c0a6c-870f-4d73-9c08-df4c653c52f5_1273x715.png 424w, https://substackcdn.com/image/fetch/$s_!ZENF!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F056c0a6c-870f-4d73-9c08-df4c653c52f5_1273x715.png 848w, https://substackcdn.com/image/fetch/$s_!ZENF!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F056c0a6c-870f-4d73-9c08-df4c653c52f5_1273x715.png 1272w, https://substackcdn.com/image/fetch/$s_!ZENF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F056c0a6c-870f-4d73-9c08-df4c653c52f5_1273x715.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>DCC, or delta color compression, is not the only form of compression. But this slide shows one example of compression/decompression happening in front of L2</figcaption></figure></div><p>Another issue is, it’s easy to imagine a compression scheme that doesn’t change the number of cache requests involved. For example, data might be compressed to only take up part of a cacheline. A request only causes a subset of the cacheline to be read out, which a decompressor module expands to the full 128B. Older RDNA1 slides are ambiguous about this, indicating that DCC operates on 256B granularity (two cachelines) without providing further details.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Thid!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd61f49-1a03-4938-a21d-d7466bff8171_1273x719.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Thid!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd61f49-1a03-4938-a21d-d7466bff8171_1273x719.png 424w, https://substackcdn.com/image/fetch/$s_!Thid!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd61f49-1a03-4938-a21d-d7466bff8171_1273x719.png 848w, https://substackcdn.com/image/fetch/$s_!Thid!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd61f49-1a03-4938-a21d-d7466bff8171_1273x719.png 1272w, https://substackcdn.com/image/fetch/$s_!Thid!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd61f49-1a03-4938-a21d-d7466bff8171_1273x719.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Thid!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd61f49-1a03-4938-a21d-d7466bff8171_1273x719.png" width="1273" height="719" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4fd61f49-1a03-4938-a21d-d7466bff8171_1273x719.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:719,&quot;width&quot;:1273,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Thid!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd61f49-1a03-4938-a21d-d7466bff8171_1273x719.png 424w, https://substackcdn.com/image/fetch/$s_!Thid!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd61f49-1a03-4938-a21d-d7466bff8171_1273x719.png 848w, https://substackcdn.com/image/fetch/$s_!Thid!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd61f49-1a03-4938-a21d-d7466bff8171_1273x719.png 1272w, https://substackcdn.com/image/fetch/$s_!Thid!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fd61f49-1a03-4938-a21d-d7466bff8171_1273x719.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>In any case, compression may be a contributing factor in RDNA4 being able to achieve better performance while using a smaller Infinity Cache than prior generations, despite only having a 256-bit GDDR6 DRAM setup.</p><p>AMD went over RAS, or reliability, availability, and serviceability features in RDNA4. Modern chips use parity and ECC to detect errors and correct them, and evidently RDNA4 does the same. Unrecoverable errors are handled with driver intervention, by “re-initializing the relevant portion of the SoC, thus preventing the platform from shutting down”. There’s two ways to interpret that statement. One is that the GPU can be re-initialized to recover from hardware errors, obviously affecting any software relying on GPU acceleration. Another is that some parts of the GPU can be re-initialized while the GPU continues handling work. I think the former is more likely, though I can imagine the latter being possible in limited forms too. For example, an unrecoverable error reading from GDDR6 can hypothetically be fixed if that data is backed by a duplicate in system memory. The driver could transfer known-good data from the host to replace the corrupted copy. But errors with modified data would be difficult to recover from, because there might not be an up-to-date copy elsewhere in the system.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!gEEn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c51ef6-6121-443b-ac04-ba860116fd2c_1600x901.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!gEEn!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c51ef6-6121-443b-ac04-ba860116fd2c_1600x901.png 424w, https://substackcdn.com/image/fetch/$s_!gEEn!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c51ef6-6121-443b-ac04-ba860116fd2c_1600x901.png 848w, https://substackcdn.com/image/fetch/$s_!gEEn!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c51ef6-6121-443b-ac04-ba860116fd2c_1600x901.png 1272w, https://substackcdn.com/image/fetch/$s_!gEEn!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c51ef6-6121-443b-ac04-ba860116fd2c_1600x901.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!gEEn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c51ef6-6121-443b-ac04-ba860116fd2c_1600x901.png" width="1456" height="820" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d2c51ef6-6121-443b-ac04-ba860116fd2c_1600x901.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:820,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!gEEn!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c51ef6-6121-443b-ac04-ba860116fd2c_1600x901.png 424w, https://substackcdn.com/image/fetch/$s_!gEEn!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c51ef6-6121-443b-ac04-ba860116fd2c_1600x901.png 848w, https://substackcdn.com/image/fetch/$s_!gEEn!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c51ef6-6121-443b-ac04-ba860116fd2c_1600x901.png 1272w, https://substackcdn.com/image/fetch/$s_!gEEn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2c51ef6-6121-443b-ac04-ba860116fd2c_1600x901.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>On the security front, microprocessors get private buses to “critical blocks” and protected register access mechanisms. Security here targets HDCP and other DRM features, which I don’t find particularly amusing. But terminology shown on the slide is interesting, because MP0 and MP1 are also covered in AMD’s CPU-side documentation. On the CPU side, MP0 (microprocessor 0) handles some Secure Encrypted Virtualization (SEV) features. It’s sometimes called the Platform Security Processor (PSP) too. MP1 on CPUs is called the System Management Unit (SMU), which covers power control functions. Curiously AMD’s slide labels MP1 and the SMU separately on RDNA4. MP0/MP1 could have completely different functions on GPUs of course. But the common terminology raises the possibility that there’s a lot of shared work between CPU and GPU SoC design. RAS is also a very traditional CPU feature, though GPUs have picked up RAS features over time as GPU compute picked up steam.</p><p>One of the most obvious examples of shared effort between the CPU and GPU sides is Infinity Fabric making its way to graphics designs. This started years ago with Vega, though back then using Infinity Fabric was more of an implementation detail. But years later, Infinity Fabric components provided an elegant way to implement a large last level cache, or multi-socket coherent systems with gigantic iGPUs (like MI300A).</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Taos!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ca1d41c-ba41-4af5-8606-8a67dbe001f8_1270x713.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Taos!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ca1d41c-ba41-4af5-8606-8a67dbe001f8_1270x713.png 424w, https://substackcdn.com/image/fetch/$s_!Taos!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ca1d41c-ba41-4af5-8606-8a67dbe001f8_1270x713.png 848w, https://substackcdn.com/image/fetch/$s_!Taos!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ca1d41c-ba41-4af5-8606-8a67dbe001f8_1270x713.png 1272w, https://substackcdn.com/image/fetch/$s_!Taos!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ca1d41c-ba41-4af5-8606-8a67dbe001f8_1270x713.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Taos!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ca1d41c-ba41-4af5-8606-8a67dbe001f8_1270x713.png" width="1270" height="713" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4ca1d41c-ba41-4af5-8606-8a67dbe001f8_1270x713.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:713,&quot;width&quot;:1270,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Taos!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ca1d41c-ba41-4af5-8606-8a67dbe001f8_1270x713.png 424w, https://substackcdn.com/image/fetch/$s_!Taos!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ca1d41c-ba41-4af5-8606-8a67dbe001f8_1270x713.png 848w, https://substackcdn.com/image/fetch/$s_!Taos!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ca1d41c-ba41-4af5-8606-8a67dbe001f8_1270x713.png 1272w, https://substackcdn.com/image/fetch/$s_!Taos!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ca1d41c-ba41-4af5-8606-8a67dbe001f8_1270x713.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Slide from Hot Chips 29, covering Infinity Fabric used in AMD’s older Vega GPU</figcaption></figure></div><p>The Infinity Fabric memory-side subsystem on RDNA4 consists of 16 CS (Coherent Station) blocks, each paired with a Unified Memory Controller (UMC). Coherent Stations receive requests coming off the graphics L2 and other clients. They ensure coherent memory access by either getting data from a UMC, or by sending a probe if another block has a more up-to-date copy of the requested cacheline. The CS is a logical place to implement a memory side cache, and each CS instance has 4 MB of cache in RDNA4.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!2dA8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F500b5783-7a52-4f32-930d-609432574e29_1600x735.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!2dA8!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F500b5783-7a52-4f32-930d-609432574e29_1600x735.png 424w, https://substackcdn.com/image/fetch/$s_!2dA8!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F500b5783-7a52-4f32-930d-609432574e29_1600x735.png 848w, https://substackcdn.com/image/fetch/$s_!2dA8!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F500b5783-7a52-4f32-930d-609432574e29_1600x735.png 1272w, https://substackcdn.com/image/fetch/$s_!2dA8!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F500b5783-7a52-4f32-930d-609432574e29_1600x735.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!2dA8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F500b5783-7a52-4f32-930d-609432574e29_1600x735.png" width="1456" height="669" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/500b5783-7a52-4f32-930d-609432574e29_1600x735.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:669,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!2dA8!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F500b5783-7a52-4f32-930d-609432574e29_1600x735.png 424w, https://substackcdn.com/image/fetch/$s_!2dA8!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F500b5783-7a52-4f32-930d-609432574e29_1600x735.png 848w, https://substackcdn.com/image/fetch/$s_!2dA8!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F500b5783-7a52-4f32-930d-609432574e29_1600x735.png 1272w, https://substackcdn.com/image/fetch/$s_!2dA8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F500b5783-7a52-4f32-930d-609432574e29_1600x735.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>To save power, Infinity Fabric supports DVFS (dynamic voltage and frequency scaling) to save power, and clocks between 1.5 and 2.5 GHz. Infinity Fabric bandwidth is 1024 bits per clock, which suggests the Infinity Cache can provide 2.5 TB/s of theoretical bandwidth. That roughly lines up with results from Nemes’s Vulkan-based GPU cache and memory bandwidth microbenchmark.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!xu5w!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3074a1b9-88b9-417e-a6ed-5ff8b2f3155b_1600x900.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!xu5w!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3074a1b9-88b9-417e-a6ed-5ff8b2f3155b_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!xu5w!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3074a1b9-88b9-417e-a6ed-5ff8b2f3155b_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!xu5w!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3074a1b9-88b9-417e-a6ed-5ff8b2f3155b_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!xu5w!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3074a1b9-88b9-417e-a6ed-5ff8b2f3155b_1600x900.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!xu5w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3074a1b9-88b9-417e-a6ed-5ff8b2f3155b_1600x900.png" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3074a1b9-88b9-417e-a6ed-5ff8b2f3155b_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!xu5w!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3074a1b9-88b9-417e-a6ed-5ff8b2f3155b_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!xu5w!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3074a1b9-88b9-417e-a6ed-5ff8b2f3155b_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!xu5w!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3074a1b9-88b9-417e-a6ed-5ff8b2f3155b_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!xu5w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3074a1b9-88b9-417e-a6ed-5ff8b2f3155b_1600x900.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>AMD also went over their ability to disable various SoC components to harvest dies and create different SKUs. Shader Engines, WGPs, and memory controller channels can be disabled. AMD and other manufacturers have used similar harvesting capabilities in the past. I’m not sure what’s new here. Likely, AMD wants to re-emphasize their harvesting options.</p><p>Finally, AMD mentioned that they chose a monolithic design for RDNA4 because it made sense for a graphics engine of its size. They looked at performance goals, package assembly and turnaround time, and cost. After evaluating those factors, they decided a monolithic design was the right option. It’s not a surprise. After all, AMD used monolithic designs for lower end RDNA3 products with smaller graphics engines, and only used chiplets for the largest SKUs. Rather, it’s a reminder that there’s no one size fits all solution. Whether a monolithic or chiplet-based design makes more sense depends heavily on design goals.</p><p><span>RDNA4 brings a lot of exciting improvements to the table, while breaking away from any attempt to tackle the top end performance segment. Rather than going for maximum performance, RDNA4 looks optimized to improve efficiency over prior generations. The RX 9070 </span><a href="https://www.techspot.com/review/2962-amd-radeon-9070/" rel="">offers similar performance</a><span> to the RX 7900XT in rasterization workloads despite having a lower power budget, less memory bandwidth, and a smaller last level cache. Techspot also shows the RX 9070 leading with raytracing workloads, which aligns with AMD's goal of enhancing raytracing performance.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!fiKo!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54f4e5be-a668-43bb-bdcf-391ab3043dc0_1530x855.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!fiKo!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54f4e5be-a668-43bb-bdcf-391ab3043dc0_1530x855.png 424w, https://substackcdn.com/image/fetch/$s_!fiKo!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54f4e5be-a668-43bb-bdcf-391ab3043dc0_1530x855.png 848w, https://substackcdn.com/image/fetch/$s_!fiKo!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54f4e5be-a668-43bb-bdcf-391ab3043dc0_1530x855.png 1272w, https://substackcdn.com/image/fetch/$s_!fiKo!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54f4e5be-a668-43bb-bdcf-391ab3043dc0_1530x855.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!fiKo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54f4e5be-a668-43bb-bdcf-391ab3043dc0_1530x855.png" width="1456" height="814" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/54f4e5be-a668-43bb-bdcf-391ab3043dc0_1530x855.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:814,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!fiKo!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54f4e5be-a668-43bb-bdcf-391ab3043dc0_1530x855.png 424w, https://substackcdn.com/image/fetch/$s_!fiKo!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54f4e5be-a668-43bb-bdcf-391ab3043dc0_1530x855.png 848w, https://substackcdn.com/image/fetch/$s_!fiKo!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54f4e5be-a668-43bb-bdcf-391ab3043dc0_1530x855.png 1272w, https://substackcdn.com/image/fetch/$s_!fiKo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54f4e5be-a668-43bb-bdcf-391ab3043dc0_1530x855.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Slide from RDNA4’s Launch Presentation not Hot Chips 2025</figcaption></figure></div><p>AMD achieves this efficiency using compression, better raytracing structures, and a larger L2 cache. As a result, RDNA4 can pack its performance into a relatively small 356.5 mm² die and use a modest 256-bit GDDR6 memory setup. Display and media engine improvements are welcome too. Multi-monitor idle power feels like a neglected area for discrete GPUs, even though I know many people use multiple monitors for productivity. Lowering idle power in those setups is much appreciated. On the media engine side, AMD’s video encoding capabilities have often lagged behind the competition. RDNA4’s progress at least prevents AMD from falling as far behind as they have before.</p><p><span>If you like the content then consider heading over to the </span><a href="https://www.patreon.com/ChipsandCheese" rel="">Patreon</a><span> or </span><a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ" rel="">PayPal</a><span> if you want to toss a few bucks to Chips and Cheese. Also consider joining the </span><a href="https://discord.gg/TwVnRhxgY2" rel="">Discord</a><span>.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Open-Source Maintainer's Guide to Saying No (175 pts)]]></title>
            <link>https://www.jlowin.dev/blog/oss-maintainers-guide-to-saying-no</link>
            <guid>45234593</guid>
            <pubDate>Sat, 13 Sep 2025 19:20:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jlowin.dev/blog/oss-maintainers-guide-to-saying-no">https://www.jlowin.dev/blog/oss-maintainers-guide-to-saying-no</a>, See on <a href="https://news.ycombinator.com/item?id=45234593">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-apjhz64k="">  <p>One of the hardest parts of maintaining an open-source project is saying “no” to a good idea. A user proposes a new feature. It’s well-designed, useful, and has no obvious technical flaws. And yet, the answer is “no.” To the user, this can be baffling. To the maintainer, it’s a necessary act of stewardship.</p>
<p>Having created and maintained two highly successful open-source projects, <a href="https://github.com/PrefectHQ/prefect">Prefect</a> and <a href="https://github.com/jlowin/fastmcp">FastMCP</a>, helped establish a third in Apache Airflow, and cut my OSS teeth contributing to Theano, I’ve learned that this stewardship is the real work. The ultimate success of a project isn’t measured by the number of features it has, but by the coherence of its vision and whether it finds resonance with its users. As Prefect’s CTO <a href="https://www.linkedin.com/in/whitecdw/">Chris White</a> likes to point out, “People choose software when its abstractions meet their mental model.” Your job as an open-source maintainer is to first establish that mental model, then relentlessly build software that reflects it. A feature that is nominally useful but not spiritually aligned can be a threat just as much as an enhancement.</p>
<p>This threat can take many forms. The most obvious is a feature that’s wildly out of scope, like a request to add a GUI to a CLI tool — a valid idea that likely belongs in a separate project. More delicate is the feature that brilliantly solves one user’s niche problem but adds complexity and maintenance burden for everyone else. The most subtle, and perhaps most corrosive, is the API that’s simply “spelled” wrong for the project: the one that breaks established patterns and creates cognitive dissonance for future users. In many of the projects I’ve been fortunate to work on, both open- and closed-source, we obsess over this because a consistent developer experience is the foundation of a framework that feels intuitive and trustworthy.</p>
<p>So how does a maintainer defend this soul, especially as a project scales? It starts with documenting not just how the project works, but why. Clear developer guides and statements of purpose are your first line of defense. They articulate the project’s philosophy, setting expectations before a single line of code is written. This creates a powerful flywheel: the clearer a project is about why it exists, the more it attracts contributors who share that vision. Their contributions reinforce and refine that vision, which in turn justifies the project’s worldview. Process then becomes a tool for alignment, not bureaucracy. As a maintainer, you can play defense on the repo, confident that the burden of proof is on the pull request to demonstrate not just its own value, but its alignment with a well-understood philosophy.</p>
<p>This work has gotten exponentially harder in the age of LLMs. Historically, we could assume that since writing code is an expensive, high-effort activity, contributors would engage in discussion before doing the work, or at least seek some sign that time would not be wasted. Today, LLMs have inverted this. Code is now cheap, and we see it offered in lieu of discourse. A user shows up with a fully formed PR for a feature we’ve never discussed. It’s well-written, it “works,” but it was generated without any context for the framework’s philosophy. Its objective function was to satisfy a user’s request, not to uphold the project’s vision.</p>
<p>This isn’t to say all unsolicited contributions are unwelcome. There is nothing more delightful than the drive-by PR that lands, fully formed and perfectly aligned, fixing a bug or adding a small, thoughtful feature. We can’t discourage these contributors. But in the last year, the balance of presumption has shifted. The signal-to-noise ratio has degraded, and the unsolicited PR is now more likely to be a high-effort review of a low-effort contribution.</p>
<p>So what’s the playbook? In FastMCP, we recently tried to nudge this behavior by requiring an issue for every PR. In a perfect example of <a href="https://en.wikipedia.org/wiki/Unintended_consequences">unintended consequences</a>, we now get single-sentence issues opened a second before the PR! More powerful than this procedural requirement is a simple sentence that we are unconvinced that the framework should take on certain responsibilities for users. If a contributor wants to convince us, we all only benefit from that effort! But as I wrote earlier, the burden of proof is on the contributor, never the repo.</p>
<p>A more nuanced pushback against viable code is that as a maintainer, you may be uncomfortable or unwilling to maintain it indefinitely. I think this is often forgotten in fast-moving open-source projects: there is a significant transfer of responsibility when a PR is merged. If it introduces bugs, confusion, inconsistencies, or even invites further enhancements, it is usually the maintainer who is suddenly on the hook for it. In FastMCP, we’ve introduced and documented the <code>contrib</code> module as one solution to this problem. This module contains useful functionality that may nonetheless not be appropriate for the core project, and is maintained exclusively by its author. No guarantee is made that it works with future versions of the project. In practice, many contrib modules might have better lives as standalone projects, but it’s a way to get the ball rolling in a more communal fashion.</p>
<p>One regret I have is that I observe a shift in my own behavior. In the early days of Prefect, we did our best to maintain a 15-minute SLA on our responses. Seven years ago, a user question reflected an amazing degree of engagement, and we wanted to respond in kind. Today, if I don’t see a basic attempt to engage, I find myself mirroring that low-effort behavior. Frankly, if I’m faced with a choice between a wall of LLM-generated text or a clear, direct question with an MRE, I’ll take the latter every time.</p>
<p>I know this describes a fundamentally artisanal, hand-made approach to open source that may seem strange in an age of vibe coding and YOLO commits. I’m no stranger to LLMs. I use them constantly in my own work and we even have an AI agent (hi Marvin!) that helps triage the FastMCP repo. But in my career, this thoughtful, deliberate stewardship has been the difference between utility projects and great ones. We used to call it “community” and I’d like to ensure it doesn’t disappear.</p>
<p>It’s a pessimistic outlook, I know. But when well appplied, this degree of thoughtfulness translates into a better experience for all users: into software whose abstractions meet the universal mental model. Two weeks ago, I was in a room that reminded me this kind of stewardship isn’t dead; it’s being practiced at the highest level.</p>
<p>I had the opportunity to join the MCP Committee for meetings in New York and saw a group skillfully navigating a version of this very problem. MCP is a young protocol whose place in the AI stack has been accelerated more by excitement than maturity. As a result, it is under constant assault with requests that it do more, be more, and solve everything in between.</p>
<p>And yet, over a couple of days, the most important thing I witnessed was a willingness to debate—and to hold every proposal up to a (usually) shared opinion of what the protocol is supposed to be. There was an overriding reverence for its teleological purpose: what it should do and, more critically, what it should not do. I especially admired <a href="https://x.com/dsp_">David’s</a> consistent drumbeat: “That’s a good idea. But is it part of the protocol’s responsibilities?”</p>
<p>Sticking to your guns like that is the hard, necessary work of maturing a technology with philosophical rigor. I left New York more confident than ever in the team and the protocol, precisely because they understand that their job isn’t just to build a protocol, but to be its thoughtful custodians. It was a thrill to see that stewardship up close, and I look forward to seeing it continue in open-source more broadly.</p> <div data-astro-cid-pa3ga7zk=""> <p> <h2>Subscribe</h2>  </p> </div>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Safe C++ proposal is not being continued (157 pts)]]></title>
            <link>https://sibellavia.lol/posts/2025/09/safe-c-proposal-is-not-being-continued/</link>
            <guid>45234460</guid>
            <pubDate>Sat, 13 Sep 2025 19:00:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sibellavia.lol/posts/2025/09/safe-c-proposal-is-not-being-continued/">https://sibellavia.lol/posts/2025/09/safe-c-proposal-is-not-being-continued/</a>, See on <a href="https://news.ycombinator.com/item?id=45234460">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>One year ago, the <a href="https://safecpp.org/draft.html">Safe C++ proposal</a> was made. The goal was to add a safe subset/context into C++ that would give strong guarantees (memory safety, type safety, thread safety) similar to what Rust provides, without breaking existing C++ code. It was an extension or superset of C++. The opt-in mechanism was to explicitly mark parts of the code that belong to the safe context. The authors even state:</p><blockquote><p>Code in the safe context exhibits the same strong safety guarantees as code written in Rust.</p></blockquote><p>The rest remains “unsafe” in the usual C++ sense. This means that existing code continues to work, while new or refactored parts can gain safety. For those who write Rust, Safe C++ has many similarities with Rust, sometimes with adjustments to fit C++’s design. Also, because C++ already has a huge base of “unsafe code”, Safe C++ has to provide mechanisms for mixing safe and unsafe, and for incremental migration. In that sense, all of Safe C++’s safe features are opt-in. Existing code compiles and works as before. Introducing safe context doesn’t break code that doesn’t use it.</p><p>The proposal caught my interest. It seemed like a good compromise to make C++ safe, although there were open or unresolved issues, which is completely normal for a draft proposal. For example, how error reporting for the borrow checker and lifetime errors would work, or how generic code and templates would interact with lifetime logic and safe/unsafe qualifiers. These are just some of the points, the proposal is very long and elaborate. Moreover, I am not a programming language designer, so there might be better alternatives.</p><p>Anyway, today I discovered that the proposal will no longer be pursued. When I thought about the proposal again this morning, I realized I hadn’t read any updates on it for some time. So I searched and found some answers on <a href="https://www.reddit.com/r/cpp/comments/1lhbqua/any_news_on_safe_c/">Reddit</a>.</p><p>The response from Sean Baxter, one of the original authors of the Safe C++ proposal:</p><blockquote><p>The Safety and Security working group voted to prioririze Profiles over Safe C++. Ask the Profiles people for an update. Safe C++ is not being continued.</p></blockquote><p>And again:</p><blockquote><p>The Rust safety model is unpopular with the committee. Further work on my end won’t change that. Profiles won the argument. All effort should go into getting Profile’s language for eliminating use-after-free bugs, data races, deadlocks and resource leaks into the Standard, so that developers can benefit from it.</p></blockquote><p>So I went to read the documents related to Profiles[1][2][3][4]. I try to summarize what I understood: they are meant to define modes of C++ that impose constraints on how you use the language and library, in order to guarantee certain safety properties. They are primarily compile-time constraints, though in practice some checks may be implemented using library facilities that add limited runtime overhead. Instead of introducing entirely new language constructs, profiles mostly restrict existing features and usages. The idea is that you can enable a profile, and any code using it agrees to follow the restrictions. If you don’t enable it, things work as before. So it’s backwards-compatible.</p><p>Profiles seem less radical and more adoptable, a safer-by-default C++ without forcing the Rust model that aims to tackle the most common C++ pitfalls. I think Safe C++ was more ambitious: introducing new syntax, type qualifiers, safe vs unsafe contexts, etc. Some in the committee felt that was too heavy, and Profiles are seen as a more pragmatic path. The main objection is obvious: one could say that Profiles restrict less than what Safe C++ aimed to provide.</p><p>Reading comments here and there, there is visible resistance in the community toward adopting the Rust model, and from a certain point of view, I understand it. If you want to write like Rust, just write Rust. Historically, C++ is a language that has often taken features from other worlds and integrated them into itself. In this case, I think that safety subsets of C++ already exist informally somehow. Profiles are an attempt to standardize and unify something that already exists in practice. Technically, they don’t add new fundamental semantics. Instead, they provide constraints, obligations and guarantees.</p><p>In my opinion, considering the preferences of the committee and the entire C++ community, although I appreciated the Safe C++ proposal and was looking forward to seeing concrete results, considering the C++ context I believe that standardizing and integrating the Profiles as proposed is a much more realistic approach. Profiles might not be perfect, but they are better than nothing. They will likely be uneven in enforcement and weaker than Safe C++ in principle. They won’t give us silver-bullet guarantees, but they are a realistic path forward.</p><p>[1] <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3081r1.pdf">Core safety profiles for C++26</a></p><p>[2] <a href="https://open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3589r0.pdf">C++ Profiles: The Framework</a></p><p>[3] <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3704r0.pdf">What are profiles?</a></p><p>[4] <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3651r0.pdf">Note to the C++ standards committee members</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Case Against Social Media Is Stronger Than You Think (232 pts)]]></title>
            <link>https://arachnemag.substack.com/p/the-case-against-social-media-is</link>
            <guid>45234323</guid>
            <pubDate>Sat, 13 Sep 2025 18:39:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arachnemag.substack.com/p/the-case-against-social-media-is">https://arachnemag.substack.com/p/the-case-against-social-media-is</a>, See on <a href="https://news.ycombinator.com/item?id=45234323">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ggx-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ggx-!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png 424w, https://substackcdn.com/image/fetch/$s_!ggx-!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png 848w, https://substackcdn.com/image/fetch/$s_!ggx-!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png 1272w, https://substackcdn.com/image/fetch/$s_!ggx-!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ggx-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png" width="797" height="567" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/06835bba-2e08-4334-ae3a-077622595096_797x567.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:567,&quot;width&quot;:797,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:840153,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://arachnemag.substack.com/i/172046631?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ggx-!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png 424w, https://substackcdn.com/image/fetch/$s_!ggx-!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png 848w, https://substackcdn.com/image/fetch/$s_!ggx-!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png 1272w, https://substackcdn.com/image/fetch/$s_!ggx-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06835bba-2e08-4334-ae3a-077622595096_797x567.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>The Mob, 1935, by Carl Hoeckner</figcaption></figure></div><p><em>1. Introduction</em></p><p><span>The philosopher Dan Williams recently published two pieces on social media— </span><a href="https://asteriskmag.com/issues/11/scapegoating-the-algorithm" rel="">“Scapegoating the Algorithm”</a><span> at Asterisk Magazine, and </span><a href="https://www.conspicuouscognition.com/p/the-case-against-social-media-is" rel="">“The Case Against Social Media is Weaker Than You Think”</a><span> at his Substack. As their titles attest to, both argue that the case against social media, on epistemic and political grounds, has been considerably overstated.</span></p><p><span>I recently published </span><a href="https://arachnemag.substack.com/p/the-leviathan-the-hand-and-the-maelstrom?r=18kjq3" rel="">a lengthy essay</a><span> arguing the opposite: that the case against social media has, if anything, been understated. And so, especially given I’m new to Substack, Williams’s recent work gives me a welcome opportunity to pick a fight for engagement-farming purposes. Sadly for my subscriber count, I agree with Williams on quite a bit, and so this is going to be less combative polemicizing and more demonstrating that a range of serious worries about social media are capable of charting a course between the equivocal lines of evidence he underlines.</span></p><p>I am going to focus on the putative political impacts of social media—in particular its impact on political polarization—rather than the specifically epistemic ones. These are often conflated, but I think it is helpful to cleave them apart. It is possible to believe, as I in fact do, that social media has had a dangerous, incendiary effect on American politics, while also believing that related concerns about misinformation and conspiracy theorizing are somewhat (though not entirely) overblown. I may decide to write a short follow-up on the latter, but will put it to the side for now.</p><p><span>With respect to social media’s supposed contributions to political polarization, and in particular </span><a href="https://www.annualreviews.org/content/journals/10.1146/annurev-polisci-051117-073034" rel="">‘affective polarization,’</a><span> Williams highlights four main lines of evidence that contradict the prevailing narrative. First, polarization has been rising for decades and began doing so well in advance of social media. Second, polarization has increased the most in recent years among those who use social media and the internet the least, those over 65. Third, trends in polarization have diverged in countries that all have widespread social media use. And fourth, several high-quality experimental studies have found a negligible effect of social media use on individuals’ levels of polarization.</span></p><p>If I can import a cliché from academic philosophy, it is sometimes said that all argumentative objections fall into one of two categories: “oh yeah?” or “so what?” I opt for a mix of both here. I begin with the “oh yeah?”: Williams’s evidence is much less convincing than it initially seems, and beyond ruling out an implausibly large spike in polarization, tells us little about how social media might be influencing its trajectory.</p><p><span>I then turn to my main focus, the “so what?”: even if Williams is right that social media has not significantly contributed to affective polarization in the U.S., this is consistent with it having plenty of other negative effects on American politics. To make this argument concrete, I highlight different lines of evidence to Williams in order to demonstrate first, that social media has indeed had a deeply concerning impact on our politics, and second, that one need not rely on the metric of affective polarization in particular to make this case. I refer to the alternative view I develop as an </span><em>elite radicalization theory</em><span> of online politics.</span></p><p>While I am an avid reader and admirer of Williams’s work, I think he is engaged in what has unfortunately become a fashionable form of academic contrarianism: arguing that despite what very much seem to be foundational, risk-laden changes in our social order, all we observe is in fact business-as-usual, and to the extent it seems otherwise, that is because of some mix of psychological bias and media overhype. </p><p>A similar contrarianism plagued early predictions of the threat posed by Donald Trump. Those critics have either relented or fallen quiet now. It is time for those who continue to minimize the downside risks of the digital media revolution to finally do the same.</p><p><em>2. Williams on social media and polarization</em></p><p>Let’s first run through Williams’s four main lines of evidence against social media-driven polarization in a bit more detail.</p><p><span>The first is that affective polarization has been rising since well before the advent social media. As Williams explains in </span><a href="https://asteriskmag.com/issues/11/scapegoating-the-algorithm" rel="">“Scapegoating the Algorithm,”</a><span> this trend begins in the late 20</span><sup>th</sup><span> century and likely has its roots in a) the partisan realignment surrounding the Democratic party’s 60s-era embrace of racial integration, and the Republican party’s subsequent “Southern Strategy” to appeal to disaffected white opponents of integration; and b) the rise of a more combatively partisan media ecosystem in the aftermath of Reagan’s 1987 repeal of the “fairness doctrine,” which required broadcasters to present balanced perspectives on controversial public issues.</span></p><p><span>Williams’s second line of evidence is the study </span><a href="https://www.pnas.org/doi/10.1073/pnas.1706588114" rel="">Boxell, Gentzkow, and Shapiro (2017)</a><span>, which found that, between 1996 and 2016, the demographic groups that used social media and the internet the least, those 65 and older, also saw the largest increase in polarization.</span></p><p><span>Williams’s third line of evidence is a later study by the same authors, </span><a href="https://direct.mit.edu/rest/article-abstract/106/2/557/109262/Cross-Country-Trends-in-Affective-Polarization?redirectedFrom=fulltext" rel="">Boxell, Gentzkow, and Shapiro (2024)</a><span>, which studied trends in affective polarization since the 1980s in 12 OECD countries and found that, while the U.S. exhibited the largest increase over this period, countries like Australia, Britain, Norway, Sweden, and Germany all saw polarization fall, including in the period after the initial spread of social media.</span></p><p><span>Williams’s fourth line of evidence is that several high-quality experimental studies have examined the relationship between social media and polarization at an individual level and found no evidence of a connection. </span><a href="https://www.science.org/doi/10.1126/science.abp9364" rel="">Guess et. al. (2023a)</a><span>, </span><a href="https://www.science.org/doi/10.1126/science.add8424" rel="">Guess et. al. (2023b)</a><span>, </span><a href="https://www.nature.com/articles/s41586-023-06297-w" rel="">Nyhan et. al. (2023)</a><span>, and </span><a href="https://www.pnas.org/doi/10.1073/pnas.2321584121" rel="">Alcott et. al. (2024)</a><span> all found that interventions meant to limit social media’s ostensible polarizing effects (such as replacing default with reverse-chronological feeds, hiding reshares, reducing exposure to content from like-minded sources, and deactivating social media entirely) had a negligible impact on users’ levels of polarization.</span></p><p><em>2.1. “Oh yeah?”</em></p><p>Taken together, this evidence indeed appears to contradict that social media has been a major contributor to American political polarization. But dig a bit deeper, and things get much messier and more uncertain.</p><p>The fact is that we have relatively little data about affective polarization since ~2010, when smartphone and social media use became widespread in the U.S. The vast majority of this data comes from the American National Election Studies (ANES) survey, whose flagship data set is only collected during presidential election years. As a result, we only have 4 data points since 2010, those for 2012, 2016, 2020, and 2024.</p><p>It is difficult to draw strong conclusions from this data one way or the other. Besides the fact there isn’t much of it, rival-party feeling does appear to start falling faster around 2010 or so, which would be consistent with some social-media related effect. But I concede I’m just eyeballing here; more rigorous conclusions will have to await further research.</p><p><span>The data on international trends in affective polarization isn’t especially convincing either. The main study cited by Williams—</span><a href="https://direct.mit.edu/rest/article-abstract/106/2/557/109262/Cross-Country-Trends-in-Affective-Polarization?redirectedFrom=fulltext" rel="">Boxell, Gentzkow, and Shapiro (2024)</a><span>—looks at affective polarization in 12 countries since the 80s, but for our purposes, it is only the post-2010-years that are relevant, since it is only after this point that social media would’ve had any notable effect. That in mind, the data here is also relatively unsatisfying:</span></p><p>With the exception of Germany, there are never more than 3 data points per country after 2010. When you consider on top of this that rates of social media penetration over time probably vary across these countries, it becomes clear that there is little we can learn through these comparisons without more context, or at least more time.</p><p>The bare fact of diverging trend lines also tells us little to begin with, since these trends might have been different absent the influence of social media. For instance, polarization might have risen more slowly in the U.S. and fallen more quickly in Germany. Sure, maybe this data does conflict with the most alarmist narrative about social media—that there was some massive spike in polarization after 2010—but that was never plausible to begin with. There is still more than enough room, even in light of this data, to be justifiably concerned about how social media is shaping American politics.</p><p>That in mind, let’s move on to Williams’s two other lines of evidence—that polarization has grown more in recent years among the elderly (those least likely to use social media), and that the experimental research on social media has not turned up any appreciable relationship with polarization.</p><p><span>The paper documenting higher polarization among the elderly—</span><a href="https://www.pnas.org/doi/10.1073/pnas.1706588114" rel="">Boxell, Gentzkow, and Shapiro (2017)</a><span>—makes a very important caveat that Williams does not mention. While the authors see themselves as ruling out the most straightforward version of the social media-driven polarization story, they acknowledge it is possible to construct an alternative account that they cannot rule out:</span></p><blockquote><p><span>[Alternative accounts would need to address] the rapid increase in polarization among those with limited internet use and negligible use of social media</span><em>. </em><strong>However, it is possible to construct such accounts.</strong><span> It may be that social media increases polarization among the young while some other factor increases it among the old. </span><strong>It may be that there are spillovers across demographic groups; young adults polarized through social media might in turn affect the views of older adults or might indirectly influence older adults through channels like the selection of politicians or the endogenous positioning of traditional media. </strong><span>(p. 3, bold my own)</span></p></blockquote><p><span>While we should not fault Boxell, Gentzkow, and Shapiro for the scope of their specific project, it is obvious in my view that every one of these spillovers is in play. I suspect the most important is the possibility that social media is influencing the tone and coverage of traditional media, and in particular cable news. This should ring true to anyone familiar with right wing cable’s aggressive coverage of various social media-native controversies, like the recent </span><a href="https://www.foxnews.com/video/6376342204112" rel="">American Apparel ad starring Sydney Sweeney</a><span>, or the various </span><a href="https://www.instagram.com/reel/DNRvRHVJ2nm/" rel="">sorority rush videos</a><span> currently going viral on TikTok. If this kind of coverage increases polarization among the elderly, that would still in part be social media’s fault. In light of potential spillover effects, this evidence can at best rule out that social media has influenced all demographic groups equally, and just as immediately, rather than seeing its influence channeled from some groups to others.</span></p><p><span>But this kind of ‘channeling’ was always going to be part of the story. Social media’s intrinsically social nature guarantees it will have large spillover effects. Ignoring this is like assuming that, if you could only delete your social media accounts, that would insulate you from 100% of social media’s effects on your life. The obvious reason it won’t is because </span><em>everyone around you </em><span>will still be using social media, and their use will continue to affect you (e.g. by their telling you what’s happening online, by you feeling excluded from their group-chats, etc.). As long as social media represents the social world to us, and as long as we share our impressions of that social world with others offline, then its effects on individuals are going to ‘spill over’ to their peers.</span></p><p><span>It is therefore wrong to assume that, since old people don’t use social media (or use it much less), they are not being affected by it. Due to spillovers, those of them that watch cable news are arguably affected by it every day. Not only that, but their relative lack of experience with the modern information environment means their political attitudes are likely to be</span><em> more </em><span>sensitive to (even the indirect effects of) social media than those who use it more often. </span></p><p>As a general rule, very few strong conclusions about the aggregate impact of social media can be drawn from research that does not account for these kinds of spillovers.</p><p>Let’s now turn to Williams’s last line of evidence, the experimental research on social media and political attitudes. This is probably the strongest evidence we have against social media-driven polarization. And yet, think carefully about what it is really in a position to tell us, and the result is again underwhelming.</p><p><span>Of the experiments Williams cites, the longest lasted three months, and only one—</span><a href="https://www.pnas.org/doi/10.1073/pnas.2321584121" rel="">Alcott et. al. (2024)</a><span>—actually entailed the complete deactivation of social media, rather than just modifications to how and what content was displayed. Already, this should give us pause about interpreting this evidence to imply that social media’s supposed impacts on politics are overblown. On a more even-handed read, all the studies besides Alcott et. al. (2024) show is that feed design changes don’t impact users’ political attitudes over relatively short periods.</span></p><p>Even focusing on complete deactivation, though, spillover effects remain a serious problem. Like anyone else, social media users’ political attitudes originate from and are continually modified by a wide variety of sources: their families, their friends, prominent members of their community, other acquaintances, online news, print news, T.V. news, podcasts, talk shows, radio shows, and so on. If these other influences have themselves been influenced by social media, then deactivating participants’ Facebooks and Instagrams for six weeks—the intervention in Alcott et. al. (2024)—does little to isolate them from the effects of social media in general. They experience those effects anyway by continuing to communicate with other social media users.</p><p><span>This is an even greater concern than usual in the case of Alcott et. al. (2024) because their intervention took place in the six weeks </span><em>before the 2020 election. </em><span>Those who took part in the experiment would have been inundated by far more political communication than usual, communication which was almost certainly influenced in some form by social media.</span></p><p>The moral is again that spillover effects aren’t to be ignored. Given social media’s now-enormous role in shaping public discourse, interventions targeting personal use alone can at best isolate a small sliver of social media’s total impact. That means that even high-quality experiments on individuals tell us little about social media-driven polarization in aggregate.</p><p><em>3. “So what?”</em></p><p>Suppose all these objections are unimpeachably correct. I’m sure they aren’t, but let’s assume. That would still only amount to a case for uncertainty. It remains, then, to make the case that rather than only remaining cautious about social media’s effects on politics, we should be seriously concerned.</p><p>Williams is right that the evidence on social media and affective polarization will not get us there on its own. But affective polarization is hardly the only potential measure of social media’s harms. Consider the following claims Williams cites (in “The Case Against Social Media is Weaker than You Think”) as typical of the public concern over social media:</p><blockquote><p><span>Alexandria Ocasio-Cortez </span><a href="https://www.businessinsider.com/facebook-meta-rep-alexandria-ocasio-cortez-aoc-cancer-to-democracy-2021-10" rel="">describes</a><span> Meta as “a cancer to democracy metastasizing into a global surveillance and propaganda machine for boosting authoritarian regimes and destroying civil society.”</span></p><p><span>Jonathan Haidt has </span><a href="https://www.theatlantic.com/magazine/archive/2022/05/social-media-democracy-trust-babel/629369/" rel="">argued</a><span> that social media platforms “dissolved the mortar trust, belief in institutions, and shared stories that held a large and diverse secular democracy [in America] together.’’</span></p><p><span>Obama </span><a href="https://barackobama.medium.com/my-remarks-on-disinformation-at-stanford-7d7af7ba28af" rel="">suggests</a><span> that one of “the biggest reasons for democracies weakening is the profound change that’s taking place in how we communicate and consumer information.”</span></p></blockquote><p>These are of course strong criticisms. But it is far from clear that they depend on social media being a major driver of affective polarization in particular. One of the issues with placing academic and non-academic discourses in conversation is that the terms of the former may not map cleanly onto the terms of the latter (and vice versa). In this case, there is no reason to assume claims like those above rest on the evidence in favor of social-media driven polarization, especially if there are other, more convincing lines of evidence available.</p><p><span>This is my primary complaint against Williams: there </span><em>are</em><span> other, more convincing</span><em> </em><span>lines of evidence available. </span></p><p><span>Even if social media is not increasing affective</span><em> </em><span>polarization, it is making our politics more angry, tribal, and violent—and to more than a sufficient degree to justify grave concerns like those of Ocasio-Cortez, Haidt, and Obama.</span></p><p>In what follows, I focus on two lines of evidence in particular: research showing that social media amplifies the reach of emotionally extreme content, and research showing that higher social media penetration in a given geographic area induces more extreme political behavior, such as protests and hate crimes. I synthesize both into an overarching view I refer to as an ‘elite radicalization‘ theory of online politics.</p><p><em>3.1. The elite radicalization theory</em></p><p><span>It is well-established in the research literature that our attention on social media gravitates toward content that is more emotionally extreme. Recent studies have found that posts that are </span><a href="https://journals.sagepub.com/doi/10.1177/20563051211059710" rel="">sad,</a><span> </span><a href="https://www.pnas.org/doi/epub/10.1073/pnas.2212270120" rel="">fearful,</a><span> </span><a href="https://journals.sagepub.com/doi/10.1177/19485506221083811" rel="">uncivil,</a><span> </span><a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2Fxge0001247" rel="">morally</a><span> </span><a href="https://academic.oup.com/joc/article-abstract/67/5/803/4642206?redirectedFrom=fulltext" rel="">and</a><span> </span><a href="https://www.pnas.org/doi/10.1073/pnas.1618923114" rel="">emotionally indignant,</a><span> </span><a href="https://www.pnas.org/doi/abs/10.1073/pnas.2024292118" rel="">hostile towards</a><span> </span><a href="https://arxiv.org/abs/2305.16941" rel="">out-groups,</a><span> and </span><a href="https://dl.acm.org/doi/abs/10.1145/2817946.2817962?casa_token=fUbAJsfmLAAAAAAA:dAz2UEM8FKjMyAoXcLIHDI6aWVK9DcH0FArFhs19m46dLM3q5cOffjs0I9U3m1OBFLh_T-coW-5V" rel="">negatively</a><span> </span><a href="https://www.degruyterbrill.com/document/doi/10.1515/commun-2019-0152/html" rel="">valenced</a><span> in general spread much further than their more neutral counterparts.</span></p><p><a href="https://www.pnas.org/doi/10.1073/pnas.1618923114" rel="">Brady et. al. (2017)</a><span>, for instance, found that each additional </span><em>word</em><span> with “moral-emotional content” (e.g. “duty,” “fear,” “shame,” “war”) increased political Twitter posts’ diffusion factor by 20%. </span><a href="https://psycnet.apa.org/record/2018-63985-001" rel="">Brady et. al. (2019)</a><span> later reproduced this finding using the tweets and retweets of over 500 presidential candidates and members of congress. Expressions of “moral anger and disgust,” the authors found, diffused particularly quickly.</span></p><p><span>That negative content spreads especially far online makes sense given some well-established properties of human psychology. Humans exhibit a broad-based psychological </span><a href="https://journals.sagepub.com/doi/abs/10.1037/1089-2680.5.4.323" rel="">negativity bias</a><span> as well a range of more specific </span><a href="https://www.sciencedirect.com/science/article/abs/pii/B9780128166604000027" rel="">attentional</a><span> </span><a href="https://www.nature.com/articles/s41598-020-68490-5" rel="">biases</a><span> </span><a href="https://pubmed.ncbi.nlm.nih.gov/11561921/" rel="">toward</a><span> negative stimuli. This is because, like other organisms, we have evolved to be uniquely attuned to signals suggestive of danger. As a result, when given the opportunity to cycle through a variegated soup of thousands of digital signals daily, we tend to fixate on and amplify the most distressing ones.</span></p><p>This dynamic creates very strong incentives to prey on our negative emotions. Successful ‘attentional entrepreneurs’ online not only enjoy attention’s more immediate benefits like status or influence, but are often paid by platforms in proportion to the ad revenue they generate. In that case, if certain kinds of content reliably accrue the most attention, there are very strong incentives online to produce that content en masse.</p><p><span>That is exactly what has happened in recent years with sensationalized and excessively negative political content. The last decade and a half has seen the rise of a new class of political influencers who, empowered by social media’s unique incentive environment, have come to exert near-symphonic control over the fear, anger, and tribalism of large sectors of the American public. The phrase “political influencer” calls to mind names like Tucker Carlson and Candace Owens, but I mean it to refer to any content creator, pundit, journalist, or even politician with an active online presence oriented around the production of political content—so perhaps hundreds of thousands of users with followings of varying sizes. Critically, this group is not a random selection from social media’s overall user base, but skews </span><a href="https://www.pewresearch.org/internet/2013/04/25/civic-engagement-in-the-digital-age-2/#:~:text=On%20the%20other%20hand%2C%20when,we%20measured%20in%20our%20survey." rel="">wealthier and more educated</a><span>, meaning its greater online influence is likely matched by greater </span><em><a href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/polq.12218" rel="">off</a></em><a href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/polq.12218" rel="">line influence</a><span> as well.</span></p><p><span>Despite accounting for only a small slice of the online population, this new influencer class is coming to dominate the market for political communication. In the process, it is transforming America’s perception of itself, which, since America is a social entity constituted in part </span><em>by</em><span> its self-perceptions, just amounts to saying it is transforming America.</span></p><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S2352250X24001313" rel="">Robertson, del Rosario, and Van Bavel (2024)</a><span> outline this group’s newfound influence in their recent paper “Inside the funhouse mirror factory: How social media distorts perceptions of norms.” It is worth quoting their analysis at length:</span></p><blockquote><p>Online discussions are dominated by a surprisingly small, extremely vocal, and non-representative minority. Research on social media has found that, while only 3% of active accounts are toxic, they produce 33% of all content. Furthermore, 74% of all online conflicts are started in just 1% of communities, and 0.1% of users shared 80% of fake news. Not only does this extreme minority stir discontent, spread misinformation, and spark outrage online, they also bias the meta-perceptions of most users who passively “lurk” online. (p. 1, my italics)</p><p>[With respect to] political discussions [in particular], the people who post frequently on social media are often the most ideologically extreme. Indeed, 97% of political posts from Twitter/X come from just 10% of the most active users on social media, meaning that about 90% of the population’s political opinions are being represented by less than 3% of tweets online. This is a marked difference from offline polling data showing that most people are ideologically moderate, uninterested in politics, and avoid political discussions when they are able. (p. 2)</p></blockquote><p><span>The result is that Americans are coming to see each other as much more politically extreme than they in fact are—or at least </span><em>were.</em><span> In the words of political scientist </span><a href="https://www.programmablemutter.com/p/were-getting-the-social-media-crisis" rel="">Henry Farrell</a><span>, Americans have internalized a </span><em>“malformed collective understanding,”</em><span> in this case a vision of their collective identity that casts them as especially angry, pessimistic, and tribal.</span></p><p><span>The idea that this understanding is “malformed”  is a tricky one, though. It invites the question: </span><em>how</em><span> malformed, and for how much longer? While in the past, survey evidence has indeed shown Americans to be moderate and relatively uninterested in politics, it is possible, and even probable, that social media is changing that. If in response to believing their peers are more politically extreme, people in turn </span><em>become </em><span>more extreme in reaction, then Farrell’s ‘malformed collective understanding’ may end up a self-fulfilling prophecy. Perhaps the more extreme content we ingest, the more likely it is we internalize this style of political communication and begin posting extreme content ourselves. There is already some limited evidence to support this possibility.</span></p><p><a href="https://academic.oup.com/joc/article-abstract/71/6/922/6363640?redirectedFrom=fulltext" rel="">Kim et. al. (2021)</a><span> found, for instance, that those exposed to toxic ‘featured’ comments on Facebook posts were more likely to post toxic comments later of their own volition. </span><a href="https://www.science.org/doi/10.1126/sciadv.abe5641" rel="">Brady et. al. (2021)</a><span> found that Twitter users “conform their outrage expressions to the expressive norms of their social networks,” (p. 1) and become more likely to express outrage when their surrounding social network is itself more extreme (p. 8). And in an as-yet unpublished preprint, </span><a href="https://osf.io/preprints/osf/mgdwq_v1" rel="">Brady et. al. (2025)</a><span> found that in a simulated Twitter-like environment, “engagement-based algorithms” changed participants’ perception of descriptive and prescriptive norms surrounding “ingroup-aligned, moral and emotional political content,” and ultimately made them more intent on posting this content themselves.</span></p><p>These results seem consistent with widespread anecdotal evidence of once-moderate friends and family falling down political or conspiratorial ‘rabbit-holes,’ often via involvement with insular online communities (e.g. QANON). At a broader level, it is hard to deny that the expressive norms governing our political culture have changed (witness the much-discussed expansion of the Overton window), and that this has made extreme political speech (open Nazism, racism, misogyny, antisemitism and so on) more common.</p><p>If this is all downstream of a small and relatively well-off group of high frequency posters (some of our elected officials among them), that would suggest what we might call an ‘elite radicalization’ theory of online politics. The idea is that social media has empowered a (relatively) small group of political influencers who, in response to the perverse incentives created by attentional negativity bias, have disseminated ideas that make people more angry, fearful, and extremist.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!VJ5k!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!VJ5k!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png 424w, https://substackcdn.com/image/fetch/$s_!VJ5k!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png 848w, https://substackcdn.com/image/fetch/$s_!VJ5k!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png 1272w, https://substackcdn.com/image/fetch/$s_!VJ5k!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!VJ5k!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png" width="725" height="409.65034965034965" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:404,&quot;width&quot;:715,&quot;resizeWidth&quot;:725,&quot;bytes&quot;:59572,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://arachnemag.substack.com/i/172046631?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!VJ5k!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png 424w, https://substackcdn.com/image/fetch/$s_!VJ5k!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png 848w, https://substackcdn.com/image/fetch/$s_!VJ5k!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png 1272w, https://substackcdn.com/image/fetch/$s_!VJ5k!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8205ba14-8c77-41ad-a16a-7d5ad15d9aa3_715x404.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Source: Me</figcaption></figure></div><p>As represented above, those ideas spread both via influencing other elites (think podcast and talk show hosts like Joe Rogan or Jon Stewart), as well as by influencing typical consumers of online political content directly. Regardless of the exact transmission pathway, social media’s vast webs of interaction ensure that whatever effects online political content has, its reach is sure to be very far.</p><p><span>Nor does this content just ‘bounce off’ political junkies already firm in their preexisting views. There is strong causal evidence that political influencers have the power not just to reflect, but to shape the attitudes of their followers. In a recent preprint, </span><a href="https://osf.io/preprints/psyarxiv/acbwg_v2" rel="">Rathje et. al. (2025)</a><span> found in two different field experiments (</span><em>n = </em><span>494, </span><em>n</em><span> = 1,133) that unfollowing “hyperpartisan social media influencers" on Twitter (they give the examples of Palmer Report on the left and Breitbart News on the right) “improved [participants’] recent feelings toward the out-party by 23.5% compared to the control group, with effects persisting for at least six months,” (p. 2). This should put to rest the idea that inflammatory political content is a purely ‘demand-side‘ phenomenon. Yes, this demand has always existed, but it is possible for the attitudinal effects of existing supply to increase it even further.</span></p><p><span>Perhaps this is not on its own cause for serious alarm, though. However unpleasant online political influencers’ ideas might be, the question is whether they transcend the digital realm to influence people’s </span><em>real-world </em><span>political behavior. If not, then maybe we can relax in the confidence that the apparent rise in extremist attitudes is confined to an annoying, but harmless class of digital ‘LARPers.’</span></p><p>With apologies to those in the market for this kind of apparently savvy, relieving narrative, there is a convincing base of evidence showing just the opposite. By amplifying extreme content online, social media does appear to catalyze more extreme political behavior offline as well. The following are quasi-experimental studies that take advantage of exogenous variation in how different social networks have spread in order to study their political impacts. Their findings are relatively univocal:</p><blockquote><p><a href="https://www.nber.org/papers/w26567" rel="">Bursztyn et. al. (2019)</a><span> instrumented on variation in city-level penetration of VK (the dominant Russian social network) in Russia and found that greater social media use increased the prevalence of xenophobic attitudes, and lead to more hate crimes in cities with higher pre-existing levels of nationalism.</span></p><p><a href="https://link.springer.com/article/10.1007/s11205-018-1887-2" rel="">Sabatini and Sarracino (2019)</a><span> instrumented on variation in broadband/high-speed internet coverage in Italy, and found that greater social media use significantly decreased measures of social, particularized, and institutional trust.</span></p><p><a href="https://onlinelibrary.wiley.com/doi/full/10.3982/ECTA14281" rel="">Enikolopov, Makarin, and Petrova (2020)</a><span> also instrumented on variation in city-level penetration of VK in Russia and found that a 10% increase in penetration into a given city increased the probability of a protest there by 4.6% and the number of protesters by 19%.</span></p><p><a href="https://ejpr.onlinelibrary.wiley.com/doi/full/10.1111/1475-6765.12373" rel="">Schaub and Morisi (2020)</a><span> instrumented on variation in municipality-level broadband coverage in Germany and Italy and found that greater access to online communication platforms made Germans and Italians more likely to vote for the populist AfD and M5S parties, respectively.</span></p><p><a href="https://academic.oup.com/jeea/article-abstract/19/4/2131/5917396?redirectedFrom=fulltext" rel="">Müller and Schwarz (2021)</a><span> used a slightly different approach—instrumenting on variation in Facebook and internet </span><em>outages </em><span>in Germany, rather than access—and found that greater social media use predicted increased hate crimes against refugees.</span></p><p><a href="https://www.aeaweb.org/articles?id=10.1257/app.20210211" rel="">Müller and Schwarz (2023)</a><span> instrumented on variation in the number of Twitter users across counties induced by early adopters at the 2007 South by Southwest (SXSW) festival and found that higher Twitter use led to a sizable increase in anti-Muslim hate crimes during the 2016 Republican primary, likely driven by future president Donald Trump’s tweets. In particular, they found that “a one standard deviation-higher exposure to Twitter [was] associated with a 32 percent … increase in hate crimes.” (p. 272)</span></p></blockquote><p>These studies’ geographic range underscores that their concerning effects are robust to meaningful changes in context. And since they aggregate data across whole countries and municipalities, they’re able to account for the kinds of spillover effects that individualized experiments ignore. At least within the Western world broadly construed, the spread of social media has consistently produced more extreme forms of political behavior, including xenophobic violence in the most severe cases, but also protests, shifting voting patterns, and presumably much more that scholars have yet to study in detail.</p><p>While the evidence on this front is more limited in the case of the United States, the graph below does not inspire much optimism—note the hinge point around the late aughts when social media (and smartphone) use first became pervasive.</p><p>Social media is not, then, just a vast machine for disseminating extreme ideas—the most successful of which seem to have a right-wing populist flavor—but also one for shaping practical politics, including in its highest-stakes moments. As the quasi-experimental research reinforces, it does so in large part by empowering elite political influencers who, far from being just ‘terminally online’ pests, have the power to genuinely refigure their home countries’ political terrain.</p><p>While much of the preexisting research on social media and politics is focused on the political right, it is worth flagging that the elite radicalization theory also helps explain the strand of more radical left-wing politics that has gained prominence over the last decade and a half. While, as a factual matter, left-wing political influencers are far less dangerous (see the preceding graph), it is no less true that they have successfully exploited attentional negativity bias for both personal and political gain, shaping our current political culture in the process. As it happens, I am more sympathetic to their views, and so think some of their impact has been positive. But that is my own personal view, not an implication of the theory.</p><p><em>3.2. Elite radicalization and two-party politics</em></p><p><span>One of the core advantages of the elite radicalization theory is that it can account for social media’s incendiary effects on politics </span><em>without </em><span>committing to any particular relationship between social media and political polarization. While I hope the theory will be independently interesting to readers, it is for this reason that it can minimally act as a “so what?” objection to Williams.</span></p><p>Note that other common accounts of social media’s relationship with politics build in some commitment to polarization from the outset. For instance, ‘echo-chamber’ or ‘filter-bubble’-based theories argue that social media sorts people along partisan lines, in which case it would make sense for those inside a given bubble to become more polarized against those outside.</p><p>But the elite radicalization theory makes no such commitment. Of the elite influencers empowered by social media, many have views orthogonal to the classical ideological divide between Republicans and Democrats. While the content they create is often angry, fear-mongering, and prejudicial, it is only occasionally aligned with one of the U.S.’s two major political parties. More often, it has a populist flavor that targets some nebulous group of elites spanning party lines. And much of it also straddles the divide between political and apolitical, weaving discussion of, say, the economy or the Epstein files into broader conversations about pop culture.</p><p><span>As a result, and contra much presumptive emphasis on polarization, online political content may </span><em>decrease </em><span>the intensity of Democrat or Republican party identification, not in spite of, but </span><em>because</em><span> it makes people more outraged about politics. That outrage, for instance, may make independents less likely to gravitate toward the Republican or Democratic party. It might also make those who are already party members dislike their party more, perhaps enough to leave it altogether. This would be consistent with the </span><a href="https://www.pewresearch.org/politics/2022/08/09/as-partisan-hostility-grows-signs-of-frustration-with-the-two-party-system/" rel="">rising public distrust of both parties</a><span>, the frequent infighting that now defines Democratic party politics, and the simmering tensions between the more nativist and more business and tech-oriented factions of the Republican party.</span></p><p>If social media does in fact decrease the incidence or intensity of major party identification overall, then affective polarization—since it is defined in terms of major party allegiances—would be the wrong metric to look at in order to understand social media’s effects on politics. While this is only speculation for now, the graph below showing a rising share of Americans identifying as independents is at the very least suggestive (again note the hinge point just before 2010).</p><p><span>None of this is to say that social media definitely doesn’t increase polarization. But at minimum, this is by no means necessary for the elite radicalization theory to succeed. With respect to ‘affective’ polarization in particular, the theory suggests that social media’s harms probably have more to do</span><em> </em><span>with</span><em> the ‘affective’ part,</em><span> than the ‘polarization’ part</span><em>.</em><span> In other words, while social media makes political discourse more affectively or emotionally intense, it may do so without making avowed Democrats or Republicans dislike each other more than they already did. </span></p><p>If the effects of social media go even further to the point of convincing a significant number of Americans to redefine as independents, that would not only make affective polarization less relevant, but might even slow its increase. This could happen if those convinced to leave their party exhibited higher-than-average-negativity towards the out-party when they were still members. That may seen counterintuitive, since presumably the Republicans (Democrats) who hate Democrats (Republicans) the most would be among those most loyal to the party, but it may be that high out-party hate is a proxy for greater disagreeableness generally, in which case those who are fed up enough with the two-party system to quit it may also be among those who once exhibited the most out-party hatred.</p><p><span>There is not yet sufficient evidence to confirm or deny any of these suggestions. But in general, academics and public commentators alike should be mindful that in addition to spreading political extremism, social media may also be shifting the underlying axis </span><em>on which those extremes exist. </em><span>This could have serious consequences for social-scientific measurements of political attitudes, not least that of affective polarization.</span></p><p><em>5. Conclusion</em></p><p>By incentivizing the creation of disproportionately negative and sensational content, and by in turn inducing more extreme, even violent political behavior, social media has almost certainly played a major role in the destabilizing political ructions of the last fifteen years—in particular in the U.S., but probably across Europe as well. </p><p>Elite political influencers, both those hailing originally from politics, television, and journalism, as well as those native to digital media platforms, have played a critical mediating role. It is they who generate much of the most provocative content, as well as who influence the outlooks and incentives of other elites, allowing their influence to spread well beyond the confines of the major social networking platforms.</p><p>While Williams is correct that we have not seen a large spike in affective polarization as a result, we have seen spikes in angry, fearful, and identitarian political speech. We have also seen large spikes in political violence. And though I will not pretend to have made the case for their direct causal connection to social media, we have also seen the rise of a more radical strain of progressive politics, as well as the once-fledgling, now all-powerful MAGA movement figureheaded by Donald Trump, both of whose successes seem hard to separate from the belligerently online politics of the 2010s. All of this has come to pass even as polarization has increased at an overall smooth rate.</p><p><span>As the digital media revolution continues to transform social life in the U.S. along almost every conceivable dimension, it is critical that we have a clear-eyed view of its effects. Williams is right to challenge certain aspects of the panic surrounding social media, such as the often </span><a href="https://www.conspicuouscognition.com/p/misinformation-researchers-are-wrong" rel="">confused</a><span> </span><a href="https://www.conspicuouscognition.com/p/the-media-very-rarely-makes-things" rel="">and</a><span> </span><a href="https://www.conspicuouscognition.com/p/what-is-misinformation-anyway" rel="">imprecise</a><span> discourse surrounding “misinformation.” But when it comes to the technology’s effects on politics writ large, he has not taken a sufficiently holistic look at the evidence.</span></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RIP pthread_cancel (193 pts)]]></title>
            <link>https://eissing.org/icing/posts/rip_pthread_cancel/</link>
            <guid>45233713</guid>
            <pubDate>Sat, 13 Sep 2025 17:20:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eissing.org/icing/posts/rip_pthread_cancel/">https://eissing.org/icing/posts/rip_pthread_cancel/</a>, See on <a href="https://news.ycombinator.com/item?id=45233713">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I posted about adding <a href="https://eissing.org/icing/posts/pthread_cancel">pthread_cancel use in curl</a>
about three weeks ago, we released this in curl 8.16.0 and it blew
up right in our faces. Now, with
<a href="https://github.com/curl/curl/pull/18540">#18540</a> we are ripping it
out again. What happened?</p>
<h2 id="short-recap">short recap</h2>
<p><a href="https://www.man7.org/linux/man-pages/man7/pthreads.7.html">pthreads</a>
define “Cancelation points”, a list of POSIX functions where
a pthread may be cancelled. In addition, there is also a list of functions
that <em>may</em> be cancelation points, among those <code>getaddrinfo()</code>.</p>
<p><code>getaddrinfo()</code> is exactly what we are interested in for <code>libcurl</code>. It blocks
until it has resolved a name. That may hang for a long time and <code>libcurl</code>
is unable to do anything else. Meh. So, we start a pthread and let that
call <code>getaddrinfo()</code>. <code>libcurl</code> can do other things while that thread runs.</p>
<p>But eventually, we have to get rid of the pthread again. Which means we
either have to <code>pthread_join()</code> it - which means a blocking wait. Or we
call <code>pthread_detach()</code> - which returns immediately but the thread keeps
on running. Both are bad when you want to do many, many transfers. Either we block and
stall or we let pthreads pile up in an uncontrolled way.</p>
<p>So, we added <code>pthread_cancel()</code> to interrupt a running <code>getaddrinfo()</code>
and get rid of the pthread we no longer needed. So the theory. And, after
some hair pulling, we got this working.</p>
<h2 id="cancel-yes-leakage-also-yes">cancel yes, leakage also yes!</h2>
<p>After releasing curl 8.16.0 we got an issue reported in
<a href="https://github.com/curl/curl/issues/18532">#18532</a> that cancelled
pthreads leaked memory.</p>
<p><img src="https://eissing.org/icing/posts/rip_pthread_cancel/images/modern-times-sigh.png" alt="modern times sigh"></p>
<p>Digging into the <a href="https://codebrowser.dev/glibc/glibc/nss/getaddrinfo.c.html#gaiconf_init">glibc source</a>
shows that there is this thing called
<a href="https://www.man7.org/linux/man-pages/man5/gai.conf.5.html"><code>/etc/gai.conf</code></a>
which defines how <code>getaddrinfo()</code> should sort returned answers.</p>
<p>The implementation in glibc first resolves the name to addresses. For these,
it needs to allocate memory. <em>Then</em> it needs to sort them if there is more
than one address. And in order
to do <em>that</em> it needs to read <code>/etc/gai.conf</code>. And in order to do <em>that</em>
it calls <code>fopen()</code> on the file. And that may be a pthread “Cancelation Point”
(and if not, it surely calls <code>open()</code> which is a required cancelation point).</p>
<p>So, the pthread may get cancelled when reading <code>/etc/gai.conf</code> and leak all
the allocated responses. And if it gets cancelled there, it will try to
read <code>/etc/gai.conf</code> <em>again</em> the next time it has more than one address
resolved.</p>
<p>At this point, I decided that we need to give up on the whole <code>pthread_cancel()</code>
strategy. The reading of <code>/etc/gai.conf</code> is one point where a cancelled
<code>getaddrinfo()</code> may leak. There might be others. Clearly, glibc is not really
designed to prevent leaks here (admittedly, this is not trivial).</p>
<h2 id="rip">RIP</h2>
<p>Leaking memory potentially on something <code>libcurl</code> does over and over again is
not acceptable. We’d rather pay the price of having to eventually wait on
a long running <code>getaddrinfo()</code>.</p>
<p>Applications using <code>libcurl</code> can avoid this by using <code>c-ares</code> which resolves
unblocking and without the use of threads. But that will not be able to do
everything that glibc does.</p>
<p>DNS continues to be tricky to use well.</p>
<ul>
  
</ul>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Geedge and MESA leak: Analyzing the great firewall’s largest document leak (141 pts)]]></title>
            <link>https://gfw.report/blog/geedge_and_mesa_leak/en/</link>
            <guid>45233415</guid>
            <pubDate>Sat, 13 Sep 2025 16:43:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gfw.report/blog/geedge_and_mesa_leak/en/">https://gfw.report/blog/geedge_and_mesa_leak/en/</a>, See on <a href="https://news.ycombinator.com/item?id=45233415">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

						
						
						
						
						
						

						
						<h2 id="1-introduction">1. Introduction</h2>
<p>The Great Firewall of China (GFW) experienced the largest leak of internal documents in its history on Thursday September 11, 2025. Over 500 GB of source code, work logs, and internal communication records were leaked, revealing details of the GFW’s research, development, and operations.</p>
<p>The leak originated from a core technical force behind the GFW: Geedge Networks (whose chief scientist is Fang Binxing) and the MESA Lab at the Institute of Information Engineering, Chinese Academy of Sciences. The documents show that the company not only provides services to governments in places like Xinjiang, Jiangsu, and Fujian, but also exports censorship and surveillance technology to countries such as Myanmar, Pakistan, Ethiopia, Kazakhstan, and other unidentified country under the “Belt and Road” framework.</p>
<p>The significance and far-reaching implications of this leak are substantial. Due to the massive volume of data, GFW Report will continue to analyze and provide updates on <a href="https://gfw.report/blog/geedge_and_mesa_leak/en/">the current page</a> and on the <a href="https://github.com/net4people/bbs/issues/519">Net4People</a>.</p>
<h2 id="2-download-link">2. Download Link</h2>
<p><a href="https://enlacehacktivista.org/index.php/Geedge_Networks">Enlace Hacktivista</a> has provided the access to the leak:</p>
<ul>
<li>BitTorrent: <a href="https://enlacehacktivista.org/geedge.torrent">https://enlacehacktivista.org/geedge.torrent</a></li>
<li>Direct HTTPS download: <a href="https://files.enlacehacktivista.org/geedge/">https://files.enlacehacktivista.org/geedge/</a></li>
</ul>
<p>The leaked files total about <strong>600 GB</strong>. Among them, the file <code>mirror/repo.tar</code> alone, as an archive of the RPM packaging server, takes up <strong>500 GB</strong>.</p>
<p>For detailed instructions on how to use the specific files, David Fifield has <a href="https://github.com/net4people/bbs/issues/519#issuecomment-3286329872">already provided a more thorough explanation on Net4People</a>.</p>
<pre tabindex="0"><code>     7206346  mirror/filelist.txt
497103482880  mirror/repo.tar
 14811058515  geedge_docs.tar.zst
  2724387262  geedge_jira.tar.zst
 35024722703  mesalab_docs.tar.zst
 63792097732  mesalab_git.tar.zst
       71382  A HAMSON-EN.docx
       16982  A Hamson.docx
      161765  BRI.docx
       14052  CPEC.docx
     2068705  CTF-AWD.docx
       19288  Schedule.docx
       26536  TSG Solution Review Description-20230208.docx
      704281  TSG-问题.docx
       35040  chat.docx
       27242  ty-Schedule.docx
      111244  待学习整理-23年MOTC-SWG合同草本V.1-2020230320.docx
       52049  打印.docx
      418620  替票证明.docx
      260551  领导修改版-待看Reponse to Customer's Suggestions-2022110-V001--1647350669.docx
</code></pre><h2 id="3-safety-considerations">3. Safety Considerations</h2>
<p>Due to the highly sensitive nature of these leaked materials, we strongly advise anyone who chooses to download and analyze them to take proper operational security precautions. It may be possible that these files may contain potentially risky content and accessing them in an insecure environment could expose you to surveillance or malware.</p>
<p>Please consider analyzing these files only in an isolated (virtual) machine without internet access.</p>
<h2 id="4-background">4. Background</h2>
<p>Great Firewall of China (GFW) is an umbrella term for a series of Internet censorship systems. Behind it, teams for research and development, operations, hardware, and management each play their roles and coordinate with one another. In addition to fixed government agencies (such as the CNCERT), different entities provide technical support depending on individual contracts and tenders. This leak originates from an important branch of the GFW’s <strong>R&amp;D capacity</strong>: Geedge Networks and MESA Lab. The MESA lab is affiliated with the Institute of Information Engineering, Chinese Academy of Sciences (IIE, CAS).</p>
<p>The origins trace back to Fang Binxing, the “Father of the Great Firewall”, coming to Beijing. At the end of 2008, he established the National Engineering Laboratory for Information Content Security (NELIST), initially based at the Institute of Computing Technology, Chinese Academy of Sciences. Beginning in 2012, the supporting institution changed to the Institute of Information Engineering, Chinese Academy of Sciences. In January 2012, some NELIST personnel formed a team at IIE, and in June 2012 the team was officially named the Processing Architecture Team, English name MESA (Massive Effective Stream Analysis). Below is an excerpt from MESA’s self-introduction:</p>
<pre tabindex="0"><code>MESA Timeline

   January 2012: Liu Qingyun, Sun Yong, Zheng Chao, Yang Rong, Qin Peng, Liu Yang, and Li Jia formed a team at IIE;
   June 2012: The team was officially named the Processing Architecture Team, English name MESA (Massive Effective Stream Analysis);
   2012: Liu Qingyun was selected for IIE’s inaugural “Rising Star” talent program;
   2012: Yang Wei and Zhou Zhou joined the team;
   2012: The team successfully completed the cybersecurity assurance task for the 18th National Congress;
   January 2013: MESA’s first PhD trainee, Liu Tingwen, graduated successfully;
   2013: Li Shu, Liu Junpeng, and Liu Xueli joined the team;
   December 2013: The MESA team received IIE’s 2013 Major Scientific and Technological Progress Award;
   2014: Zhou Zhou was selected for IIE’s “Rising Star” talent program;
   2014: The MESA component SAPP platform began large-scale engineering deployment;
   2014: Zhang Peng, Yu Lingjing, and Jia Mengdie joined the team;
   2015: Zheng Chao was selected for IIE’s “Rising Star” talent program, and Zhang Peng was selected for IIE’s “Outstanding Talent Introduction” program;
   August 2015: MESA moved from the Agriculture Bureau to the Huayan Beili office area;
   July 2015: PhD student Sha Hongzhou trained by MESA graduated successfully, and Liu Xiaomei received Outstanding Graduate honors;
   2016: Dou Fenghu, Zhu Yujia, Wang Fengmei, Li Zhao, Lu Qiuwen, Du Meijie, Shen Yan, and Fang Xupeng joined MESA in succession, and the team expanded rapidly;
   2016: The team undertook multiple major engineering projects, with annual contracted revenue exceeding 35 million;
   December 2016: The MESA team participated in winning the National Science and Technology Progress Award (Second Prize);
   2018: Sun Yong and Zhou Zhou received the 2017 National State Secrecy Science and Technology Award (Second Prize);
</code></pre><p>By 2018, Fang Binxing had also established himself in Hainan, and Geedge (Hainan) Information Technology Co., Ltd. (Geedge Networks Ltd.) was founded in the same year. Fang served as chief scientist, and the “core R&amp;D personnel came from universities and research institutes such as the Chinese Academy of Sciences, Harbin Institute of Technology, and Beijing University of Posts and Telecommunications.” Much of this talent came from MESA—for example, Zheng Chao served as CTO. Attentive readers will notice that many mentors and students from the MESA timeline appear in the leaked Geedge company git commits.</p>
<h2 id="5-analysis-of-nonsource-code-files">5. Analysis of Non–Source Code Files</h2>
<p>The non–source-code portion of the leaked files has already been analyzed in detail by multiple professional teams. Below are David Fifield’s notes on related media reports and technical write-ups. <strong>Please note that the source-code portion of the leak has not yet been analyzed</strong>:</p>
<ul>
<li><a href="https://github.com/net4people/bbs/issues/519#issuecomment-3275640752">David Fifield’s notes on the related media reports</a></li>
<li><a href="https://github.com/net4people/bbs/issues/519#issuecomment-3282101626">David Fifield’s notes on the technical write-ups</a></li>
</ul>
<h2 id="6-analysis-of-source-code-files">6. Analysis of Source Code Files</h2>
<p><strong>The source-code portion of the leaked files has not yet been carefully analyzed.</strong> This leak is significant and far-reaching. Given the large volume of material, GFW Report will continue to update our analysis and findings on the <a href="https://gfw.report/blog/geedge_and_mesa_leak/zh/">current page</a> as well as on <a href="https://github.com/net4people/bbs/issues/519">Net4People</a>.</p>

<p>This report was first published on <a href="https://gfw.report/blog/geedge_and_mesa_leak/en/">GFW Report</a>. We also actively updated our analysis and findings on <a href="https://github.com/net4people/bbs/issues/519">Net4People</a>.</p>
<p>We encourage you to share questions, comments, analysis, or additional evidence on this topic, either publicly or privately. Our private contact information can be found in the footer of the <a href="https://gfw.report/">GFW Report</a> website.</p>


						
						<hr>
						










						
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Magical systems thinking (273 pts)]]></title>
            <link>https://worksinprogress.co/issue/magical-systems-thinking/</link>
            <guid>45233266</guid>
            <pubDate>Sat, 13 Sep 2025 16:18:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://worksinprogress.co/issue/magical-systems-thinking/">https://worksinprogress.co/issue/magical-systems-thinking/</a>, See on <a href="https://news.ycombinator.com/item?id=45233266">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div role="presentation"><p>The systems that enable modern life share a common origin. The water supply, the internet, the international supply chains bringing us cheap goods: each began life as a simple, working system. The first electric grid was no more than a handful of electric lamps hooked up to a water wheel in Godalming, England, in 1881. It then took successive <a href="https://www.worksinprogress.news/p/how-the-worlds-first-electric-grid">decades of tinkering and iteration</a> by thousands of very smart people to scale these systems to the advanced state we enjoy today. At no point did a single genius map out the final, finished product.</p>



<p>But this lineage of (mostly) working systems is easily forgotten. Instead, we prefer a more flattering story: that complex systems are deliberate creations, the product of careful analysis. And, relatedly, that by performing this analysis – now known as ‘systems thinking’ in the halls of government – we can bring unruly ones to heel. It is an optimistic perspective, casting us as the masters of our systems and our destiny.</p>



<p>The empirical record says otherwise, however. Our recent history is one of governments grappling with complex systems and coming off worse. In the United States, HealthCare.gov was designed to simplify access to health insurance by knitting together 36 state marketplaces and data from eight federal agencies. Its launch was paralyzed by <a href="https://www.gao.gov/assets/gao-14-694.pdf">technical failures</a> that locked out millions of users. Australia’s disability reforms, carefully planned for over a decade and expected to save money, led to costs escalating so rapidly that they will <a href="https://www.afr.com/policy/economy/ndis-to-cost-100b-exceeding-the-pension-budget-watchdog-20240614-p5jltf">soon exceed the pension budget</a>.&nbsp;The UK’s <a href="https://www.worksinprogress.news/p/the-breaking-of-britains-national">2014 introduction of Contracts for Difference</a>, intended to speed the renewables rollout by giving generators a guaranteed price, overstrained the grid and is a major contributor to the 15-year queue for new connections. Systems thinking is more popular than ever; modern systems thinkers have analytical tools that their predecessors could only have dreamt of. But the systems keep kicking back.</p>



<p>There is a better way. A long but neglected line of thinkers going back to chemists in the nineteenth century has argued that complex systems are not our passive playthings. Despite friendly names like ‘the health system’, they demand extreme wariness. If broken, a complex system often cannot be fixed. Meanwhile, our successes, when they do come, are invariably the result of starting small. As the systems we have built slip further beyond our collective control, it is these simple working systems that offer us the best path back.&nbsp;</p>



<h3>The world model</h3>



<p>In 1970, the ‘Club of Rome’, a <a href="https://www.clubofrome.org/history/">group</a> of international luminaries with an interest in how the problems of the world were interrelated, invited <a href="https://sloanreview.mit.edu/article/jay-forrester-shock-to-the-system/">Jay Wright Forrester</a> to peer into the future of the global economy. An MIT expert on electrical and mechanical engineering, Forrester had cut his teeth on problems like how to keep a Second World War aircraft carrier’s radar pointed steadily at the horizon amid the heavy swell of the Pacific.&nbsp;</p>



<p>The Club of Rome asked an even more intricate question: how would social and economic forces interact in the coming decades? Where were the bottlenecks and feedback mechanisms? Could economic growth continue, or would the world enter a new phase of equilibrium or decline?&nbsp;</p>



<p>Forrester labored hard, producing a mathematical model of enormous sophistication. Across 130 pages of mathematical equations, computer graphical printout, and DYNAMO code,<em>World Dynamics</em> tracks the myriad relationships between natural resources, capital, population, food, and pollution: everything from the ‘capital-investment-in-agriculture-fraction adjustment time’ to the ominous ‘death-rate-from-pollution multiplier’.</p>


<div>
<figure><img loading="lazy" width="850" height="497" src="https://wip.gatspress.com/wp-content/uploads/2025/09/image-12.png" alt="" srcset="https://wip.gatspress.com/wp-content/uploads/2025/09/image-12.png 850w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-12-300x175.png 300w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-12-768x449.png 768w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-12-402x235.png 402w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-12-462x270.png 462w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-12-662x387.png 662w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-12-722x422.png 722w" sizes="(max-width: 850px) 100vw, 850px">
          <figcaption>
            <div>
              <p>
                A section of Forrester’s World Model.
              </p>
              <div>
                <p>Image</p>
                <p>
                  WAguirre 2017
                </p>
              </div>
            </div>
          </figcaption>
        </figure></div>


<p>World leaders had assumed that economic growth was an unalloyed good. But Forrester’s results showed the opposite. As financial and population growth continued, natural resources would be consumed at an accelerating rate, agricultural land would be paved over, and pollution would reach unmanageable levels. His model laid out dozens of scenarios and in most of them, by 2025, the world would already be in the first throes of an irreversible decline in living standards. By 2070, the crunch would be so painful that industrialized nations might regret their experiment with economic growth altogether. As Forrester <a href="https://monoskop.org/images/d/dc/Forrester_Jay_W_World_Dynamics_2nd_ed_1973.pdf#page=15">put it</a>, ‘[t]he present underdeveloped countries may be in a better condition for surviving forthcoming worldwide environmental and economic pressures than are the advanced countries.’</p>



<p>But, as we now know, the results were also wrong. Adjusting for inflation, world GDP is now about five times higher than it was in 1970 and continues to rise. More than 90 percent of that growth has come from Asia, Europe, and North America, but <a href="https://www.worksinprogress.news/p/growing-forests">forest cover</a> across those regions has <a href="https://openknowledge.fao.org/items/d6f0df61-cb5d-4030-8814-0e466176d9a1">increased</a>, up 2.6 percent since 1990 to over 2.3 billion hectares in 2020. The death rate from air pollution has almost halved in the same period, from <a href="https://ourworldindata.org/air-pollution">185 per 100,000 in 1990 to 100 in 2021</a>. According to the model, none of this should have been possible.&nbsp;</p>



<p>What happened? The blame cannot lie with Forrester’s competence: it’s hard to imagine a better systems pedigree than his. To read his prose today is to recognize a brilliant, thoughtful mind. Moreover, the system dynamics approach Forrester pioneered had already shown promise beyond the mechanical and electrical systems that were its original inspiration.&nbsp;</p>



<p>In 1956, the management of a General Electric refrigerator factory in Kentucky had called on Forrester’s help. They were struggling with a boom-and-bust cycle: acute shortages became gluts that left warehouses overflowing with unsold fridges. The factory based its production decisions on orders from the warehouse, which in turn got orders from distributors, who heard from retailers, who dealt with customers. Each step introduced noise and delay. Ripples in demand would be amplified into huge swings in production further up the supply chain.&nbsp;</p>



<p>Looking at the system as a whole, Forrester recognized the same feedback loops and instability that could bedevil a ship’s radar. He developed new decision rules, such as smoothing production based on longer-term sales data rather than immediate orders, and found ways to speed up the flow of information between retailers, distributors, and the factory. These changes dampened the oscillations caused by the system’s own structure, checking its worst excesses.&nbsp;</p>



<p>The Kentucky factory story showed Forrester’s skill as a systems analyst. Back at MIT, Forrester immortalized his lessons as a learning exercise (albeit with beer instead of refrigerators). In the ‘Beer Game’, now a rite of passage for students at the MIT Sloan School of Management, players take one of four different roles in the beer supply chain: retailer, wholesaler, distributor, and brewer. Each player sits at a separate table and can communicate only through order forms. As their inventory runs low, they place orders with the supplier next upstream. Orders take time to process, and shipments to arrive, and each player can see only their small part of the chain.</p>



<p>The objective of the Beer Game is to minimize costs by managing inventory effectively. But, as the GE factory managers had originally found, this is not so easy. Gluts and shortages arise mysteriously, without obvious logic, and small perturbations in demand get amplified up the chain by as much as 800 percent (‘the bullwhip effect’). On average, players’ total costs end up being <a href="https://dspace.mit.edu/bitstream/handle/1721.1/2184/SWP-1933-18213539.pdf#page=17">ten times higher than the optimal solution</a>.&nbsp;</p>



<p>With the failure of his World Model, Forrester had fallen into the same trap as his MIT students. Systems analysis works best under specific conditions: when the system is static; when you can dismantle and examine it closely; when it involves few moving parts rather than many; and when you can iterate fixes through multiple attempts. A faulty ship’s radar or a simple electronic circuit are ideal. Even a limited human element – with people’s capacity to pursue their own plans, resist change, form political blocs, and generally frustrate best-laid plans – makes things much harder. The four-part refrigerator supply chain, with the factory, warehouse, distributor and retailer all under the tight control of management, is about the upper limit of what can be understood. Beyond that, in the realm of societies, governments and economies, systems thinking becomes a liability, more likely to breed false confidence than real understanding. For these systems we need a different approach.</p>



<h3>Le Chatelier’s Principle</h3>



<p>In 1884, in a laboratory at the École des Mines in Paris, Henri Louis Le Chatelier noticed something peculiar: chemical reactions seemed to resist changes imposed upon them. Le Chatelier found that if, say, you have an experiment where two molecules combine in a heat-generating exothermic reaction (in his case, it was two reddish-brown nitrogen dioxide molecules combining into colorless dinitrogen tetroxide and giving off heat in the process), then you can speed things up by cooling the reactants. To ‘resist’ the drop in temperature, the system restores its equilibrium by creating more of the products that release heat.&nbsp;</p>



<p>Le Chatelier’s Principle, the idea that the system always kicks back, proved to be a very general and powerful way to think about chemistry. It was instrumental in the discovery of the Haber-Bosch process for creating ammonia that revolutionized agriculture. Nobel Laureate Linus Pauling <a href="https://catholicscientists.org/scientists-of-the-past/henry-louis-le-chatelier/">hoped</a> that, even after his students had ‘forgotten all the mathematical equations relating to chemical equilibrium’, Le Chatelier’s Principle would be the one thing they remembered. And its usefulness went beyond chemistry. A century after Le Chatelier’s meticulous lab work, another student of systems would apply the principle to the complex human systems that had stymied Forrester and his subsequent followers in government.</p>



<p>John Gall was a pediatrician with a long-standing practice in Ann Arbor, Michigan. Of the same generation as Forrester, Gall came at things from a different direction. Whereas Forrester’s background was in mechanical and electrical systems, which worked well and solved new problems, Gall was immersed in the human systems of health, education, and government. These systems often did not work well. How was it, Gall wondered, that they seemed to coexist happily with the problems – crime, poverty, ill health – they were supposed to stamp out?&nbsp;</p>



<p>Le Chatelier’s Principle provided an answer: systems should not be thought of as benign entities that will faithfully carry out their creators’ intentions. Rather, over time, they come to oppose their own proper functioning. Gall elaborated on this idea in his 1975 book <em>Systemantics</em>, named for the universal tendency of systems to display antics. A brief, weird, funny book, <em>Systemantics</em> (<em>The Systems Bible</em> in later editions) is arguably the best field guide to contemporary systems dysfunction. It consists of a series of pithy aphorisms, which the reader is invited to apply to explain the system failures&nbsp;(‘horrible examples’)&nbsp;they witness every day.</p>



<p>These aphorisms are provocatively stated, but they have considerable explanatory power. For example, an Australian politician frustrated at the <a href="https://www.afr.com/politics/federal/fraud-signs-in-90pc-of-ndis-managers-crime-gangs-push-drugs-20240603-p5jizn">new headaches</a> created by ‘fixes’ to the old disability system might be reminded that ‘NEW SYSTEMS CREATE NEW PROBLEMS’. An American confused at how there can now be <a href="https://regulatorystudies.columbian.gwu.edu/reg-stats">190,000</a> pages in the US Code of Federal Regulations, up from 10,000 in 1950, might note that this is the nature of the beast: ‘SYSTEMS TEND TO GROW, AND AS THEY GROW THEY ENCROACH’. During the French Revolution, in 1793 and 1794, the ‘Committee of Public Safety’ guillotined thousands of people, an early example of the enduring principles that ‘THE SYSTEM DOES NOT DO WHAT IT SAYS IT IS DOING’ and that ‘THE NAME IS EMPHATICALLY NOT THE THING’. And, just like student chemists, government reformers everywhere would do well to remember Le Chatelier’s Principle: ‘THE SYSTEM ALWAYS KICKS BACK’.</p>



    <figure>
        
    </figure>




<p>These principles encourage a healthy paranoia when it comes to complex systems. But Gall’s ‘systems-display-antics’ philosophy is not a counsel of doom. His greatest insight was a positive one, explaining how some systems do succeed in spite of the pitfalls. Known as ‘Gall’s law’, it’s worth quoting in full:</p>



<blockquote><p>A complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system.</p></blockquote>



<p>Starting with a working simple system and evolving from there is how we went from the water wheel in Godalming to the modern electric grid. It is how we went from a hunk of <a href="https://spectrum.ieee.org/transistor-history">germanium, gold foil, and hand-soldered wires</a> in 1947 to transistors being etched onto silicon wafers in their trillions today.</p>



<p>This is a dynamic we can experience on a personal as well as a historical level. A trivial but revealing example is the computer game Factorio. Released in 2012 and famously hazardous to the productivity of software engineers everywhere, Factorio invites players to construct a factory. The ultimate goal is to launch a rocket, a feat that requires the player to produce thousands of intermediate products through dozens of complicated, interlocking manufacturing processes.&nbsp;</p>



<p>It sounds like a nightmare. An early flow chart (pictured –&nbsp;it has grown much more complicated since) resembles the end product of a particularly thorny systems thinking project. But players complete its daunting mission successfully, without reference to such system maps, in their thousands, and all for fun.</p>


<div>
<figure><img loading="lazy" width="900" height="671" src="https://wip.gatspress.com/wp-content/uploads/2025/09/image-13.png" alt="" srcset="https://wip.gatspress.com/wp-content/uploads/2025/09/image-13.png 900w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-13-300x224.png 300w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-13-768x573.png 768w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-13-402x300.png 402w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-13-462x344.png 462w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-13-662x494.png 662w, https://wip.gatspress.com/wp-content/uploads/2025/09/image-13-722x538.png 722w" sizes="(max-width: 900px) 100vw, 900px">
          <figcaption>
            <div>
              <p>
                Factorio production map.
              </p>
              <div>
                <p>Image</p>
                
              </div>
            </div>
          </figcaption>
        </figure></div>


<p>The genius of the game is that it lets players begin with a simple system that works. As you learn to produce one item, another is unlocked. If you get something wrong, the factory visibly grinds to a halt while you figure out a different approach. The hours tick by, and new systems – automated mining, oil refining, locomotives – are introduced and iterated upon. Before you realize it, you have built a sprawling yet functioning system that might be more sophisticated than anything you have worked on in your entire professional career.</p>



<h3>How to build systems that work</h3>



<p>Government systems, however, are already established, complicated, and relied upon by millions of people every day. We cannot simply switch off the health system and ask everyone to wait a few years while we build something better. The good news is that the existence of an old, clunky system does not stop us from starting something new and simple in parallel.</p>



<p>In the 1950s, the US was in a desperate race against a technologically resurgent Soviet Union. The USSR took the lead in developing advanced rockets of the type that launched Sputnik into orbit and risked launching a nuclear device into Washington, DC. In 1954, the Eisenhower administration tasked General Bernard Schriever with helping the US develop its own Intercontinental Ballistic Missile (ICBM). An experienced airman and administrator, the top brass felt that Schriever’s Stanford engineering master’s degree would make him a suitable go-between for the soldiers and scientists on this incredibly technical project (its <a href="https://thebhc.org/sites/default/files/beh/BEHprint/v022n1/p0194-p0209.pdf">scope</a> was larger even than the Manhattan Project, costing over $100 billion in 2025 dollars versus the latter’s $39 billion).&nbsp;</p>



<p>The organizational setup Schriever inherited was not fit for the task. With many layers of approvals and subcommittees within subcommittees, it was a classic example of a complex yet dysfunctional system. The technological challenges posed by the ICBM were extreme: everything from rocket engines to targeting systems to the integration with nuclear warheads had to be figured out more or less from scratch. This left no room for bureaucratic delay.&nbsp;</p>



<p>Schriever produced what many systems thinkers would recognize as a kind of systems map: a series of massive boards setting out all the different committees and governance structures and approvals and red tape. But the point of these <a href="https://media.defense.gov/2010/Sep/29/2001329778/-1/-1/0/AFD-100929-007.pdf#page=19">‘spaghetti charts’</a> was not to make a targeted, systems thinking intervention. Schriever didn’t pretend to be able to navigate and manipulate all this complexity. He instead recognized his own limits. With the Cold War in the balance, he could not afford to play and lose his equivalent of the Beer Game. Charts in hand, Schriever persuaded his boss that untangling the spaghetti was a losing battle: they needed to start over.</p>



<p>They could not change the wider laws, regulations, and institutional landscape governing national defense. But they could work around them, starting afresh with a simple system outside the existing bureaucracy. Direct vertical accountability all the way to the President and a free hand on personnel enabled the program to flourish. Over the following years, four immensely ambitious systems were built in record time. The uneasy strategic stalemate that passed for stability during the Cold War was restored, and the weapons were never used in anger.</p>



<p>When we look in more detail at recent public policy successes, we see that this pattern tends to hold. Operation Warp Speed in the US played a big role in getting vaccines delivered quickly. It did so by <a href="https://issues.org/rules-operation-warp-speed-arnold">bypassing many of the usual bottlenecks</a>. For instance, it made heavy use of ‘Other Transaction Authority agreements’ to commit $12.5 billion of federal money by March 2021, circumventing the thousands of pages of standard procurement rules. Emergency powers were deployed to accelerate the FDA review process, enabling clinical trial work and early manufacturing scale-up to happen in parallel. These actions were funded through an $18 billion commitment made largely outside the typical congressional appropriation oversight channels – enough money to back not just one vaccine candidate but <a href="https://www.gao.gov/products/gao-21-319#:~:text=As%20of%20January%2030%2C%202021,and%20Drug%20Administration%20(FDA).">six, across three different technology platforms.</a></p>



<p>In France, the rapid reconstruction of Notre-Dame after the April 2019 fire has become a symbol of French national pride and its ability to get things done despite a reputation for moribund bureaucracy. This was achieved not through wholesale reform of that bureaucracy but by quickly setting up a fresh structure outside of it. In July 2019, the French Parliament passed Loi n°&nbsp;2019-803, creating an extraordinary legal framework for the project. Construction permits and zoning changes were fast-tracked. President Macron personally appointed the veteran General Jean-Louis Georgelin to run the restoration, exempting him from the mandatory retirement age for public executives in order to do so.</p>



<p>The long-term promise of a small working system is that over time it can supplant the old, broken one and produce results on a larger scale. This creative destruction has long been celebrated in the private sector, where aging corporate giants can be disrupted by smaller, simpler startups: we don’t have to rely on IBM to make our phones or laptops or Large Language Models. But it can work in the public sector too. Estonia, for example, introduced electronic ID in the early 2000s for signing documents and filing online tax returns. These simple applications, which nonetheless took enormous focus to implement, were popular, and ‘digital government’ was <a href="https://e-estonia.com/story/">gradually expanded</a> to new areas: voting in 2005, police in 2007, prescriptions in 2010, residency in 2014, and even e-divorce in 2024. By 2025, 99 percent of residents will have an electronic ID card, digital signatures are <a href="https://e-estonia.com/wp-content/uploads/eestonia_guide_08-04-2025.pdf">estimated</a> to save two percent of GDP per year, and every state service runs online.&nbsp;</p>



<p>In desperate situations, such as a Cold War arms race or COVID-19, we avoid complex systems and find simpler workarounds. But, outside of severe crises, much time is wasted on what amounts to magical systems thinking. Government administrations around the world, whose members would happily admit their incompetence to fix a broken radio system, publish manifestos, strategies, plans, and priorities premised on disentangling systems problems that are orders of magnitude more challenging. With each ‘fix’, oversight bodies, administrative apparatus, and overlapping statutory obligations accumulate. Complexity is continuing to rise, outcomes are becoming worse, and voters’ goodwill is being eroded.</p>



<p>We will soon be in an era where humans are not the sole authors of complex systems. Sundar Pichai estimated in late 2024 that over 25 percent of Google’s code was AI generated; as of mid-2025, the figure for Anthropic is <a href="https://x.com/slow_developer/status/1921684238753304887">80–90 percent</a>.&nbsp;As in the years after the Second World War, the temptation will be to use this vast increase in computational power and intelligence to ‘solve’ systems design for once and for all. But the same laws that limited Forrester continue to bind: ‘NEW SYSTEMS CREATE NEW PROBLEMS’ and ‘THE SYSTEM ALWAYS KICKS BACK’. As systems become more complex, they become more chaotic, not less. The best solution remains humility, and a simple system that works.</p>
</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[“Learning how to Learn” will be next generation's most needed skill (125 pts)]]></title>
            <link>https://techxplore.com/news/2025-09-google-ai-scientist-generation-skill.html</link>
            <guid>45232720</guid>
            <pubDate>Sat, 13 Sep 2025 15:10:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techxplore.com/news/2025-09-google-ai-scientist-generation-skill.html">https://techxplore.com/news/2025-09-google-ai-scientist-generation-skill.html</a>, See on <a href="https://news.ycombinator.com/item?id=45232720">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2025/googles-top-ai-scienti.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2025/googles-top-ai-scienti.jpg" data-sub-html="Demis Hassabis, CEO of Google's artificial intelligence research company DeepMind, right, and Greece's Prime Minister Kyriakos Mitsotakis discuss the future of AI, ethics and democracy during an event at the Odeon of Herodes Atticus, in Athens, Greece, Friday, Sept. 12, 2025. Credit: AP Photo/Thanassis Stavrakis">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2025/googles-top-ai-scienti.jpg" alt="Google's top AI scientist says ‘learning how to learn’ will be next generation's most needed skill" title="Demis Hassabis, CEO of Google's artificial intelligence research company DeepMind, right, and Greece's Prime Minister Kyriakos Mitsotakis discuss the future of AI, ethics and democracy during an event at the Odeon of Herodes Atticus, in Athens, Greece, Friday, Sept. 12, 2025. Credit: AP Photo/Thanassis Stavrakis" width="800" height="530">
             <figcaption>
                Demis Hassabis, CEO of Google's artificial intelligence research company DeepMind, right, and Greece's Prime Minister Kyriakos Mitsotakis discuss the future of AI, ethics and democracy during an event at the Odeon of Herodes Atticus, in Athens, Greece, Friday, Sept. 12, 2025. Credit: AP Photo/Thanassis Stavrakis
            </figcaption>        </figure>
    </div><p>A top Google scientist and 2024 Nobel laureate said Friday that the most important skill for the next generation will be "learning how to learn" to keep pace with change as Artificial Intelligence transforms education and the workplace.</p>

                                        
                                              
                                        
                                                                                                                                    <p>Speaking at an ancient Roman theater at the foot of the Acropolis in Athens, Demis Hassabis, CEO of Google's DeepMind, said rapid technological change demands a new approach to learning and <a href="https://techxplore.com/tags/skill+development/" rel="tag">skill development</a>.</p>
<p>"It's very hard to predict the future, like 10 years from now, in normal cases. It's even harder today, given how fast AI is changing, even week by week," Hassabis told the audience. "The only thing you can say for certain is that huge change is coming."</p>
<p>The neuroscientist and former chess prodigy said <a href="https://techxplore.com/tags/artificial+general+intelligence/" rel="tag">artificial general intelligence</a>—a futuristic vision of machines that are as broadly smart as humans or at least can do many things as well as people can—could arrive within a decade. This, he said, will bring dramatic advances and a possible future of "radical abundance" despite acknowledged risks.</p>
<p>Hassabis emphasized the need for "meta-skills," such as understanding how to learn and optimizing one's approach to new subjects, alongside traditional disciplines like math, science and humanities.</p>
<p>"One thing we'll know for sure is you're going to have to continually learn ... throughout your career," he said.</p>

<ul>
            <li data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2025/googles-top-ai-scienti-1.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2025/googles-top-ai-scienti-1.jpg" data-sub-html="Greece's Prime Minister Kyriakos Mitsotakis, center, and Demis Hassabis, CEO of Google's artificial intelligence research company DeepMind, right, discuss the future of AI, ethics and democracy as the moderator Linda Rottenberg, Co-founder &amp; CEO of Endeavor looks on during an event at the Odeon of Herodes Atticus in Athens, Greece, Friday, Sept. 12, 2025. Credit: AP Photo/Thanassis Stavrakis">
            <figure>
                <img src="https://scx1.b-cdn.net/csz/news/800/2025/googles-top-ai-scienti-1.jpg" alt="Google's top AI scientist says ‘learning how to learn’ will be next generation's most needed skill">
                 <figcaption>
                    Greece's Prime Minister Kyriakos Mitsotakis, center, and Demis Hassabis, CEO of Google's artificial intelligence research company DeepMind, right, discuss the future of AI, ethics and democracy as the moderator Linda Rottenberg, Co-founder &amp; CEO of Endeavor looks on during an event at the Odeon of Herodes Atticus in Athens, Greece, Friday, Sept. 12, 2025. Credit: AP Photo/Thanassis Stavrakis
                </figcaption>            </figure>
        </li>
            <li data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2025/googles-top-ai-scienti-2.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2025/googles-top-ai-scienti-2.jpg" data-sub-html="Demis Hassabis, CEO of Google's artificial intelligence research company DeepMind, bottom right, and Greece's Prime Minister Kyriakos Mitsotakis, bottom center, discuss the future of AI, ethics and democracy during an event at the Odeon of Herodes Atticus, under Acropolis ancient hill, in Athens, Greece, Friday, Sept. 12, 2025. Credit: AP Photo/Thanassis Stavrakis">
            <figure>
                <img src="https://scx1.b-cdn.net/csz/news/800/2025/googles-top-ai-scienti-2.jpg" alt="Google's top AI scientist says ‘learning how to learn’ will be next generation's most needed skill">
                 <figcaption>
                    Demis Hassabis, CEO of Google's artificial intelligence research company DeepMind, bottom right, and Greece's Prime Minister Kyriakos Mitsotakis, bottom center, discuss the future of AI, ethics and democracy during an event at the Odeon of Herodes Atticus, under Acropolis ancient hill, in Athens, Greece, Friday, Sept. 12, 2025. Credit: AP Photo/Thanassis Stavrakis
                </figcaption>            </figure>
        </li>
    </ul>
<p>The DeepMind co-founder, who established the London-based research lab in 2010 before Google acquired it four years later, shared the 2024 Nobel Prize in chemistry for developing AI systems that accurately predict protein folding—a breakthrough for medicine and drug discovery.</p>
<p>Greek Prime Minister Kyriakos Mitsotakis joined Hassabis at the Athens event after discussing ways to expand AI use in government services. Mitsotakis warned that the continued growth of huge tech companies could create great global financial inequality.</p>
<p>"Unless people actually see benefits, personal benefits, to this (AI) revolution, they will tend to become very skeptical," he said. "And if they see ... obscene wealth being created within very few companies, this is a recipe for significant social unrest."</p>
<p>Mitsotakis thanked Hassabis, whose father is Greek Cypriot, for rescheduling the presentation to avoid conflicting with the European basketball championship semifinal between Greece and Turkey. Greece later lost the game 94-68.</p>

                                                                                                                                    
                                                                                
                                        											
										                                                                                    <p>
                                                © 2025 The Associated Press. All rights reserved. This material may not be published, broadcast, rewritten or redistributed without permission.
                                            </p>
                                                                                
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                Google's top AI scientist says 'learning how to learn' will be next generation's most needed skill (2025, September 13)
                                                retrieved 13 September 2025
                                                from https://techxplore.com/news/2025-09-google-ai-scientist-generation-skill.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[486Tang – 486 on a credit-card-sized FPGA board (179 pts)]]></title>
            <link>https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/</link>
            <guid>45232565</guid>
            <pubDate>Sat, 13 Sep 2025 14:52:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/">https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/</a>, See on <a href="https://news.ycombinator.com/item?id=45232565">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main role="main"><article><div><p>Yesterday I released <a href="https://github.com/nand2mario/486tang">486Tang</a> v0.1 on GitHub. It’s a port of the ao486 MiSTer PC core to the Sipeed Tang Console 138K FPGA. I’ve been trying to get an x86 core running on the Tang for a while. As far as I know, this is the first time ao486 has been ported to a non-Altera FPGA. Here’s a short write‑up of the project.</p><h2 id="486tang-architecture">486Tang Architecture</h2><p>Every FPGA board is a little different. Porting a core means moving pieces around and rewiring things to fit. Here are the major components in 486Tang:</p><p><img src="https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/486tang.drawio.png" alt="" width="800"></p><p>Compared to ao486 on MiSTer, there are a few major differences:</p><ol><li><p><strong>Switching to SDRAM for main memory.</strong> The MiSTer core uses DDR3 as main memory. Obviously, at the time of the 80486, DDR didn’t exist, so SDRAM is a natural fit. I also wanted to dedicate DDR3 to the framebuffer; time‑multiplexing it would have been complicated. So SDRAM became the main memory and DDR3 the framebuffer. The SDRAM on Tang is 16‑bit wide while ao486 expects 32‑bit accesses, which would normally mean one 32‑bit word every two cycles. I mitigated this by running the SDRAM logic at 2× the system clock so a 32‑bit word can be read or written every CPU cycle (“double‑pumping” the memory).</p></li><li><p><strong>SD‑backed IDE.</strong> On MiSTer, the core forwards IDE requests to the ARM HPS over a fast HPS‑FPGA link; the HPS then accesses a VHD image. Tang doesn’t have a comparable high‑speed MCU‑to‑FPGA interface—only a feeble UART—so I moved disk storage into the SD card and let the FPGA access it directly.</p></li><li><p><strong>Boot‑loading module.</strong> A PC needs several things to boot: BIOS, VGA BIOS, CMOS settings, and IDE IDENTIFY data (512 bytes). Since I didn’t rely on an MCU for disk data, I stored all of these in the first 128 KB of the SD card. A small boot loader module reads them into main memory and IDE, and then releases the CPU when everything is ready.</p></li></ol><h2 id="system-bring-up-with-the-help-of-a-whole-system-simulator">System bring-up with the help of a whole-system simulator</h2><p>After restructuring the system, the main challenge was bringing it up to a DOS prompt. A 486 PC is complex—CPU and peripherals—more so than the game consoles I’ve worked on. The ao486 CPU alone is &gt;25K lines of Verilog, versus a few K for older cores like M68K. Debugging on hardware was painful: GAO builds took 10+ minutes and there were many more signals to probe. Without a good plan, it would be unmanageable and bugs could take days to isolate—not viable for a hobby project.</p><p>My solution was Verilator for subsystem and whole‑system simulation. The codebase is relatively mature, so I skipped per‑module unit tests and focused on simulating subsystems like VGA and a full boot to DOS. Verilator is fast enough to reach a DOS prompt in a few minutes—an order of magnitude better if you factor in the complete waveforms you get in simulation. The trick, then, is surfacing useful progress and error signals. A few simple instrumentation hooks were enough for me:</p><ol><li><p>Bochs BIOS can print debug strings to port 0x8888 in debug builds. I intercept and print these (the yellow messages in the simulator). The same path exists on hardware—the CPU forwards them over UART—so BIOS issues show up immediately without waiting for a GAO build.</p></li><li><p>Subsystem‑scoped tracing. For Sound Blaster, IDE, etc., I added <code>--sound</code>, <code>--ide</code> flags to trace I/O operations and key state changes. This is much faster than editing Verilog or using GAO.</p></li><li><p>Bochs BIOS assembly listings are invaluable. I initially used a manual disassembly—old console habits—without symbols, which was painful. Rebuilding Bochs and using the official listings solved that.</p></li></ol><p>A lot of the bugs were in the new glue I added, as expected. ao486 itself is mature. Still, a few issues only showed up on this toolchain/hardware, mostly due to <strong>toolchain behavior differences</strong>. In one case a variable meant to be static behaved like an automatic variable and didn’t retain state across invocations, so a CE pulse never occurred. Buried deep, it took a while to find.</p><p>Here’s a simulation session. On the left the simulated 486 screen. On the right is the simulator terminal output. You can see the green VGA output and yellow debug output, along with other events like INT 15h and video VSYNCs.</p><p><img src="https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/boot.png" alt=""></p><h2 id="performance-optimizations">Performance optimizations</h2><p>With simulation help, the core ran on Tang Console—just not fast. The Gowin GW5A isn’t a particularly fast FPGA. Initial benchmarks put it around a 25 MHz 80386.</p><p><img src="https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/landmark6a.jpg" alt="" width="400"></p><p>The main obstacle to clock speed is long combinational paths. When you find a critical path, you either shorten it or pipeline it by inserting registers—both risks bugs. A solid test suite is essential; I used <a href="https://github.com/barotto/test386.asm">test386.asm</a> to validate changes.</p><p>Here are a few concrete wins:</p><p><strong>Reset tree and fan-out reduction.</strong> Gowin’s tools didn’t replicate resets aggressively enough (even with “Place → Replicate Resources”). One reset net had &gt;5,000 fan-out, which ballooned delays. Manually replicating the reset and a few other high‑fan-out nets helped a lot.</p><p><strong>Instruction fetch optimization.</strong> A long combinational chain sat in the decode/fetch interface. In <code>decoder_regs.v</code>, the number of bytes the fetcher may accept was computed using the last decoded instruction’s length:</p><div><pre tabindex="0"><code data-lang="verilog"><span><span><span>reg</span> [<span>3</span><span>:</span><span>0</span>] decoder_count;
</span></span><span><span><span>assign</span> acceptable_1     <span>=</span> <span>4</span><span>'d12</span> <span>-</span> decoder_count <span>+</span> consume_count;
</span></span><span><span><span>always</span> @(<span>posedge</span> clk) <span>begin</span>
</span></span><span><span>  ...
</span></span><span><span>  decoder_count <span>&lt;=</span> after_consume_count <span>+</span> accepted;
</span></span><span><span><span>end</span>
</span></span></code></pre></div><p>Here, <code>12</code> is the buffer size, <code>decoder_count</code> is the current occupancy, and <code>consume_count</code> is the length of the outgoing instruction. Reasonable—but computing <code>consume_count</code> (opcode, ModR/M, etc.) was on the Fmax‑limiting path. By the way, this is one of several well-known problems of the x86 - variable length instructions complicating decoding, another is complex address modes and “effective address” calculation.</p><p>The fix was to drop the dependency on <code>consume_count</code>:</p><div><pre tabindex="0"><code data-lang="verilog"><span><span><span>assign</span> acceptable_1    <span>=</span> <span>4</span><span>'d12</span> <span>-</span> decoder_count;
</span></span></code></pre></div><p>This may cause the fetcher to “under‑fetch” for one cycle because the outgoing instruction’s space isn’t reclaimed immediately. But <code>decoder_count</code> updates next cycle, reclaiming the space. With a 12‑byte buffer, the CPI impact was negligible and Fmax improved measurably on this board.</p><p><strong>TLB optimization.</strong> The Translation Lookaside Buffer (TLB) is a small cache that translates virtual to physical addresses. ao486 uses a 32‑entry fully‑associative TLB with a purely combinational read path—zero extra cycles, but a long path on every memory access (code and data).</p><p>DOS workloads barely stress the TLB; even many 386 extenders use a flat model. As a first step I converted the TLB to 4‑way set‑associative. That’s simpler and already slightly faster than fully‑associative for these workloads. There’s room to optimize further since the long combinational path rarely helps.</p><p>A rough v0.1 end‑to‑end result: about +35% per Landmark 6 benchmarks, reaching roughly 486SX‑20 territory.</p><p><img src="https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/landmark6b.jpg" alt="" width="400"></p><h2 id="reflections">Reflections</h2><p>Here are a few reflections after the port:</p><p><strong>Clock speed scaling.</strong> I appreciate the lure of the megahertz race now. Scaling the whole system clock was the most effective lever—more so than extra caches or deeper pipelines at this stage. Up to ~200–300 MHz, CPU, memory, and I/O can often scale together. After that, memory latency dominates, caches grow deeper, and once clock speeds stop increasing, multiprocessing takes over—the story of the 2000s.</p><p><strong>x86 vs. ARM.</strong> Working with ao486 deepened my respect for x86’s complexity. John Crawford’s 1990 paper “The i486 CPU: Executing Instructions in One Clock Cycle” is a great read; it argues convincingly against scrapping x86 for a new RISC ISA given the software base (10K+ apps then). Compatibility was the right bet, but the baggage is real. By contrast, last year’s ARM7‑based <a href="https://github.com/nand2mario/gbatang/">GBATang</a> felt refreshingly simple: fixed‑length 32‑bit instructions, saner addressing, and competitive performance. You can’t have your cake and eat it.</p><hr><p>So there you have it—that’s 486Tang in v0.1. Thanks for reading, and see you next time.</p></div></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[‘Someone must know this guy’: four-year wedding crasher mystery solved (304 pts)]]></title>
            <link>https://www.theguardian.com/uk-news/2025/sep/12/wedding-crasher-mystery-solved-four-years-bride-scotland</link>
            <guid>45232562</guid>
            <pubDate>Sat, 13 Sep 2025 14:52:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/uk-news/2025/sep/12/wedding-crasher-mystery-solved-four-years-bride-scotland">https://www.theguardian.com/uk-news/2025/sep/12/wedding-crasher-mystery-solved-four-years-bride-scotland</a>, See on <a href="https://news.ycombinator.com/item?id=45232562">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>A baffled bride has solved the mystery of the awkward-looking stranger who crashed her wedding four years ago.</p><p>Michelle Wylie and her husband, John, registered the presence of their unidentifiable guest only as they looked through photographs of their wedding in the days after the happy occasion.</p><p>Who was the tall man in a dark suit, distinguished by the look of quiet mortification on his face? But their family and friends could offer no explanation, nor could hotel staff at the Carlton hotel in Prestwick, where the event took place in November 2021. An appeal on <a href="https://www.theguardian.com/technology/facebook" data-link-name="in body link" data-component="auto-linked-tag">Facebook</a> likewise yielded no clues.</p><p>Eventually, with the mystery still niggling, Wylie asked the popular Scottish content creator Dazza to cast the online net wider – and a sheepish Andrew Hillhouse finally stepped forward.</p><p>In his explanatory post on Facebook, Hillhouse admitted that he had been “cutting it fine, as I’m known to do” when he pulled up at the wedding venue with five minutes to spare. Spotting a piper and other guests, he followed them into the hotel – “I remember thinking to myself: ‘Cool, this is obviously the right place’” – unaware that he had the address completely wrong and was supposed to be at a ceremony 2 miles away in Ayr.</p><figure id="617eb014-69ee-4eba-8d40-e901c0a405bf" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/1b698f58eec0fb61470495d0ee49dab3c9fa8a17/98_0_1255_1004/master/1255.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/1b698f58eec0fb61470495d0ee49dab3c9fa8a17/98_0_1255_1004/master/1255.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/1b698f58eec0fb61470495d0ee49dab3c9fa8a17/98_0_1255_1004/master/1255.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/1b698f58eec0fb61470495d0ee49dab3c9fa8a17/98_0_1255_1004/master/1255.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/1b698f58eec0fb61470495d0ee49dab3c9fa8a17/98_0_1255_1004/master/1255.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/1b698f58eec0fb61470495d0ee49dab3c9fa8a17/98_0_1255_1004/master/1255.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="A bride and groom walk down the aisle hand-in-hand as the unknown guest looks on." src="https://i.guim.co.uk/img/media/1b698f58eec0fb61470495d0ee49dab3c9fa8a17/98_0_1255_1004/master/1255.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="356" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Michelle and John enjoy their wedding, unaware of the crasher.</span> Photograph: Courtesy Michelle Wylie/SWNS</figcaption></figure><p>He was initially unperturbed to find himself surrounded by strangers as the ceremony began – at the marriage he was due to attend, the only person he knew was the bride, Michaela, while his partner, Andrew, was part of the wedding party. It was when an entirely different bride came walking down the aisle that he realised: “OMG that’s not Michaela … I was at the wrong wedding!”</p><p>Hillhouse said: “You can’t exactly stand up and walk out of a wedding mid-ceremony, so I just had to commit to this act and spent the next 20 minutes awkwardly sitting there trying to be as inconspicuous as my 6ft 2 ass could be.”</p><p>At the end of the ceremony, Hillhouse, who is from Troon, was hoping to make a discreet exit, only to be waylaid by the wedding photographer, who insisted he join other guests for a group shot. He can be spotted looming uncomfortably at the very back of the crowd.</p><figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.NewsletterSignupBlockElement"><a data-ignore="global-link-styling" href="#EmailSignup-skip-link-9">skip past newsletter promotion</a><p id="EmailSignup-skip-link-9" tabindex="0" aria-label="after newsletter promotion" role="note">after newsletter promotion</p></figure><figure id="4a0bd1a0-13c3-49f5-bb92-2f62dc267cd5" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:10,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;‘As if we’re real guests’: the startup selling strangers invitations to weddings&quot;,&quot;elementId&quot;:&quot;4a0bd1a0-13c3-49f5-bb92-2f62dc267cd5&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/lifeandstyle/2025/aug/02/paris-startup-invitin-app-selling-strangers-invites-to-weddings&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>His post continued: “Rushed outside, made some phone calls and made my way to the correct wedding, where I was almost as popular as the actual bride and groom, and spent most of the night retelling that story to people.”</p><p>For Michelle Wylie, this amiable resolution brings to a close years of speculation.</p><figure id="634193b8-e21c-4dfa-b068-5a2a843fcaef" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-3"><picture><source srcset="https://i.guim.co.uk/img/media/7d520b7deac7b6371b3f39ad04b5a27a4e405620/0_0_4000_2671/master/4000.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/7d520b7deac7b6371b3f39ad04b5a27a4e405620/0_0_4000_2671/master/4000.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/7d520b7deac7b6371b3f39ad04b5a27a4e405620/0_0_4000_2671/master/4000.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/7d520b7deac7b6371b3f39ad04b5a27a4e405620/0_0_4000_2671/master/4000.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/7d520b7deac7b6371b3f39ad04b5a27a4e405620/0_0_4000_2671/master/4000.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/7d520b7deac7b6371b3f39ad04b5a27a4e405620/0_0_4000_2671/master/4000.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="Large group photo of all the wedding guests behind the bride and groom" src="https://i.guim.co.uk/img/media/7d520b7deac7b6371b3f39ad04b5a27a4e405620/0_0_4000_2671/master/4000.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="297.14875" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Hillhouse said the wedding photographer insisted he join other guests for a group shot.</span> Photograph: Courtesy Michelle Wylie/SWNS</figcaption></figure><p>She told BBC Scotland: “It would come into my head and I’d be like: ‘Someone must know who this guy is.’ I said a few times to my husband: ‘Are you sure you don’t know this guy, is he maybe from your work?’ We wondered if he was a mad stalker.”</p><p>She is now Facebook friends with Hillhouse and the pair have met in person to cement their coincidental bond.</p><p>“I could not stop laughing,” said Wylie. “We can’t believe we’ve found out who he is after almost four years.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: CLAVIER-36 – A programming environment for generative music (124 pts)]]></title>
            <link>https://clavier36.com/p/LtZDdcRP3haTWHErgvdM</link>
            <guid>45232299</guid>
            <pubDate>Sat, 13 Sep 2025 14:22:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clavier36.com/p/LtZDdcRP3haTWHErgvdM">https://clavier36.com/p/LtZDdcRP3haTWHErgvdM</a>, See on <a href="https://news.ycombinator.com/item?id=45232299">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Mago: A fast PHP toolchain written in Rust (146 pts)]]></title>
            <link>https://github.com/carthage-software/mago</link>
            <guid>45232275</guid>
            <pubDate>Sat, 13 Sep 2025 14:20:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/carthage-software/mago">https://github.com/carthage-software/mago</a>, See on <a href="https://news.ycombinator.com/item?id=45232275">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/carthage-software/mago/blob/main/docs/public/assets/banner.svg"><img src="https://github.com/carthage-software/mago/raw/main/docs/public/assets/banner.svg" alt="Mago Banner" width="600"></a>
</p>
<p dir="auto"><strong>An extremely fast PHP linter, formatter, and static analyzer, written in Rust.</strong></p>
<p dir="auto"><a href="https://github.com/carthage-software/mago/actions/workflows/ci.yml"><img src="https://github.com/carthage-software/mago/actions/workflows/ci.yml/badge.svg" alt="CI Status"></a>
<a href="https://github.com/carthage-software/mago/actions/workflows/cd.yml"><img src="https://github.com/carthage-software/mago/actions/workflows/cd.yml/badge.svg" alt="CD Status"></a>
<a href="https://crates.io/crates/mago" rel="nofollow"><img src="https://camo.githubusercontent.com/b6ec34bd549b6f5e7279bf91ef0fc5f97aa3cb5e42d68c2222716852b96d60a4/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f6d61676f2e737667" alt="Crates.io" data-canonical-src="https://img.shields.io/crates/v/mago.svg"></a>
<a href="https://packagist.org/packages/carthage-software/mago" rel="nofollow"><img src="https://camo.githubusercontent.com/f6fc80c02ec64dfa7a083924ad9184e36163febb90043f66f9764070753b9b14/68747470733a2f2f706f7365722e707567782e6f72672f63617274686167652d736f6674776172652f6d61676f2f76" alt="Latest Stable Version for PHP" data-canonical-src="https://poser.pugx.org/carthage-software/mago/v"></a>
<a href="https://packagist.org/packages/carthage-software/mago" rel="nofollow"><img src="https://camo.githubusercontent.com/f1919d5541fd79aa14aee467a1b43021993f179621a7aeafd7cff8786f0eab25/68747470733a2f2f706f7365722e707567782e6f72672f63617274686167652d736f6674776172652f6d61676f2f762f756e737461626c65" alt="Latest Unstable Version for PHP" data-canonical-src="https://poser.pugx.org/carthage-software/mago/v/unstable"></a>
<a href="https://packagist.org/packages/carthage-software/mago" rel="nofollow"><img src="https://camo.githubusercontent.com/bcf3df774061ef6c23f0258e4a4028a3e2d2efa66b63d3c259cf446e759d2fbf/687474703a2f2f706f7365722e707567782e6f72672f63617274686167652d736f6674776172652f6d61676f2f646f776e6c6f616473" alt="Total Composer Downloads" data-canonical-src="http://poser.pugx.org/carthage-software/mago/downloads"></a>
<a href="https://github.com/carthage-software/mago/blob/main/LICENSE-MIT"><img src="https://camo.githubusercontent.com/d1cf047d22d43d0d842a67f699104edc2160de50906725fcb21ce5579638c24a/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f6c2f6d61676f2e737667" alt="License" data-canonical-src="https://img.shields.io/crates/l/mago.svg"></a></p>
<p dir="auto"><strong>Mago</strong> is a comprehensive toolchain for PHP that helps developers write better code. Inspired by the Rust ecosystem, Mago brings speed, reliability, and an exceptional developer experience to PHP projects of all sizes.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#installation">Installation</a></li>
<li><a href="#getting-started">Getting Started</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#our-sponsors">Our Sponsors</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#inspiration--acknowledgements">Inspiration &amp; Acknowledgements</a></li>
<li><a href="#license">License</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">The most common way to install Mago on macOS and Linux is by using our shell script:</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl --proto '=https' --tlsv1.2 -sSf https://carthage.software/mago.sh | bash"><pre>curl --proto <span><span>'</span>=https<span>'</span></span> --tlsv1.2 -sSf https://carthage.software/mago.sh <span>|</span> bash</pre></div>
<p dir="auto">For all other installation methods, including Homebrew, Composer, and Cargo, please refer to our official <strong><a href="https://mago.carthage.software/guide/installation" rel="nofollow">Installation Guide</a></strong>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">To get started with Mago and learn how to configure your project, please visit our <strong><a href="https://mago.carthage.software/guide/getting-started" rel="nofollow">Getting Started Guide</a></strong> in the official documentation.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>⚡️ Extremely Fast: Built in Rust for maximum performance.</li>
<li>🔍 Lint: Identify issues in your codebase with customizable rules.</li>
<li>🔬 Static Analysis: Perform deep analysis of your codebase to catch potential type errors and bugs.</li>
<li>🛠️ Automated Fixes: Apply fixes for many lint issues automatically.</li>
<li>📜 Formatting: Automatically format your code to adhere to best practices and style guides.</li>
<li>🧠 Semantic Checks: Ensure code correctness with robust semantic analysis.</li>
<li>🌳 AST Visualization: Explore your code’s structure with Abstract Syntax Tree (AST) parsing.</li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Our Sponsors</h2><a id="user-content-our-sponsors" aria-label="Permalink: Our Sponsors" href="#our-sponsors"></a></p>
<p dir="auto"><a href="https://github.com/jasonrm" title="Jason R. McNeil"><kbd><img src="https://avatars.githubusercontent.com/u/39949?u=69c0e4fb08c439250978d41dbc3371d2f0609b98&amp;v=4&amp;s=160" width="80" height="80" alt="Jason R. McNeil"></kbd></a><a href="https://github.com/vvvinceocam" title="Vincent Berset"><kbd><img src="https://avatars.githubusercontent.com/u/5173120?u=95efc76cd8fc804536dc6dd25781a95b650bf902&amp;v=4&amp;s=160" width="80" height="80" alt="Vincent Berset"></kbd></a></p><p dir="auto"><a href="https://github.com/TicketSwap" title="TicketSwap"><kbd><img src="https://avatars.githubusercontent.com/u/5766233?v=4&amp;s=120" width="60" height="60" alt="TicketSwap"></kbd></a></p>
<p dir="auto"><a href="https://github.com/carthage-software/mago/blob/main/SPONSORS.md">See all sponsors</a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Mago is a community-driven project, and we welcome contributions! Whether you're reporting bugs, suggesting features, writing documentation, or submitting code, your help is valued.</p>
<ul dir="auto">
<li>See our <a href="https://github.com/carthage-software/mago/blob/main/CONTRIBUTING.md">Contributing Guide</a> to get started.</li>
<li>Join the discussion on <a href="https://discord.gg/mwyyjr27eu" rel="nofollow">Discord</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inspiration &amp; Acknowledgements</h2><a id="user-content-inspiration--acknowledgements" aria-label="Permalink: Inspiration &amp; Acknowledgements" href="#inspiration--acknowledgements"></a></p>
<p dir="auto">Mago stands on the shoulders of giants. Our design and functionality are heavily inspired by pioneering tools in both the Rust and PHP ecosystems.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Inspirations:</h3><a id="user-content-inspirations" aria-label="Permalink: Inspirations:" href="#inspirations"></a></p>
<ul dir="auto">
<li><a href="https://github.com/rust-lang/rust-clippy">Clippy</a>: For its comprehensive linting approach.</li>
<li><a href="https://github.com/oxc-project/oxc/">OXC</a>: A major inspiration for building a high-performance toolchain in Rust.</li>
<li><a href="https://github.com/slackhq/hakana/">Hakana</a>: For its deep static analysis capabilities.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Acknowledgements:</h3><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements:" href="#acknowledgements"></a></p>
<p dir="auto">We deeply respect the foundational work of tools like <a href="https://github.com/PHP-CS-Fixer/PHP-CS-Fixer">PHP-CS-Fixer</a>, <a href="https://github.com/vimeo/psalm">Psalm</a>, <a href="https://github.com/phpstan/phpstan">PHPStan</a>, and <a href="https://github.com/squizlabs/PHP_CodeSniffer">PHP_CodeSniffer</a>. While Mago aims to offer a unified and faster alternative, these tools paved the way for modern PHP development.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Mago is dual-licensed under your choice of the following:</p>
<ul dir="auto">
<li>MIT License (<a href="https://github.com/carthage-software/mago/blob/main/LICENSE-MIT">LICENSE-MIT</a>)</li>
<li>Apache License, Version 2.0 (<a href="https://github.com/carthage-software/mago/blob/main/LICENSE-APACHE">LICENSE-APACHE</a>)</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An annual blast of Pacific cold water did not occur (142 pts)]]></title>
            <link>https://www.nytimes.com/2025/09/12/climate/pacific-cold-water-upwelling.html</link>
            <guid>45232100</guid>
            <pubDate>Sat, 13 Sep 2025 13:54:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/09/12/climate/pacific-cold-water-upwelling.html">https://www.nytimes.com/2025/09/12/climate/pacific-cold-water-upwelling.html</a>, See on <a href="https://news.ycombinator.com/item?id=45232100">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/09/12/climate/pacific-cold-water-upwelling.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Japan sets record of nearly 100k people aged over 100 (329 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cd07nljlyv0o</link>
            <guid>45232052</guid>
            <pubDate>Sat, 13 Sep 2025 13:47:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cd07nljlyv0o">https://www.bbc.com/news/articles/cd07nljlyv0o</a>, See on <a href="https://news.ycombinator.com/item?id=45232052">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="byline-new" data-component="byline-block"><p><span data-testid="byline-new-contributors"><p><span>Jessica Rawnsley</span><span data-testid="undefined-role-location"></span><span> and</span></p><p><span>Stephanie Hogarty</span><span data-testid="undefined-role-location">Population correspondent</span></p></span></p></div><div data-component="text-block"><p>The number of people in Japan aged 100 or older has risen to a record high of nearly 100,000, its government has announced.</p><p>Setting a new record for the 55th year in a row, the number of centenarians in Japan was 99,763 as of September, the health ministry said on Friday. Of that total, women accounted for an overwhelming 88%.</p><p>Japan has the world's longest life expectancy, and is known for often being home to the world's oldest living person - though some studies contest the actual number of centenarians worldwide.</p><p>It is also one of the fastest ageing societies, with residents often having a healthier diet but a low birth rate.</p></div><div data-component="text-block"><p>The oldest person in Japan is 114-year-old Shigeko Kagawa, a woman from Yamatokoriyama, a suburb of the city Nara. Meanwhile, the oldest man is Kiyotaka Mizuno, 111, from the coastal city of Iwata.</p><p>Health minister Takamaro Fukoka congratulated the 87,784 female and 11,979 male centenarians on their longevity and expressed his "gratitude for their many years of contributions to the development of society".</p><p>The figures were released ahead of Japan's Elderly Day on 15 September, a national holiday where new centenarians receive a congratulatory letter and silver cup from the prime minister. This year, 52,310 individuals were eligible, the health ministry said.</p><p>In the 1960s, Japan's population had the lowest proportion of people aged over 100 of any G7 country - but that has changed remarkably in the decades since.</p><p>When its government began the centenarian survey in 1963, there were 153 people aged 100 or over. </p><p>That figure rose to 1,000 in 1981 and stood at 10,000 by 1998.</p><p>The higher life expectancy is mainly attributed to fewer deaths from heart disease and common forms of cancer, in particular breast and prostate cancer.</p><p>Japan has low rates of obesity, a major contributing factor to both diseases, thanks to diets low in red meat and high in fish and vegetables.</p><p>The obesity rate is particularly low for women, which could go some way to explaining why Japanese women have a much higher life expectancy than their male counterparts.</p><p>As increased quantities of sugar and salt crept into diets in the rest of the world, Japan went in the other direction - with public health messaging successfully convincing people to reduce their salt consumption.</p><p>But it's not just diet. Japanese people tend to stay active into later life, walking and using public transport more than elderly people in the US and Europe.</p><p>Radio Taiso, a daily group exercise, has been a part of Japanese culture since 1928, established to encourage a sense of community as well as public health. The three-minute routine is broadcast on television and practised in small community groups across the country.</p></div><div data-component="text-block"><p>However, several studies have cast doubt on the validity of global centenarian numbers, suggesting data errors, unreliable public records and missing birth certificates may account for elevated figures.</p><p>A government audit of family registries in Japan in 2010 uncovered more than <a target="_self" href="https://www.bbc.co.uk/news/world-asia-pacific-11258071">230,000 people listed as being aged 100 or older who were unaccounted for</a>, some having in fact died decades previously.</p><p>The miscounting was attributed to patchy record-keeping and suspicions that some families may have tried to hide the deaths of elderly relatives in order to claim their pensions.</p><p>The national inquiry was launched after the remains of <a target="_self" href="https://www.bbc.co.uk/news/world-asia-pacific-10809128">Sogen Koto, believed to be the oldest man in Tokyo at 111</a>, were found in his family home 32 years after his death.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My First Impressions of Gleam (191 pts)]]></title>
            <link>https://mtlynch.io/notes/gleam-first-impressions/</link>
            <guid>45231852</guid>
            <pubDate>Sat, 13 Sep 2025 13:15:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mtlynch.io/notes/gleam-first-impressions/">https://mtlynch.io/notes/gleam-first-impressions/</a>, See on <a href="https://news.ycombinator.com/item?id=45231852">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I’m <a href="https://mtlynch.io/notes/which-new-language/">looking for a new programming language</a> to learn this year, and <a href="https://gleam.run/">Gleam</a> looks like the most fun. It’s an Elixir-like language that supports static typing.</p><p>I read the <a href="https://tour.gleam.run/">language tour</a>, and it made sense to me, but I need to build something before I can judge a programming language well.</p><p>I’m sharing some notes on my first few hours using Gleam in case they’re helpful to others learning Gleam or to the team developing the language.</p><h2 id="my-project-parsing-old-aim-logs">My project: Parsing old AIM logs<a href="#my-project-parsing-old-aim-logs" arialabel="Anchor"> 🔗︎</a></h2><p>I used AOL Instant Messenger from about 1999 to 2007. For most of that time, I used AIM clients that logged my conversations, but they varied in formats. Most of the log formats are XML or HTML, which make re-reading those logs a pain.</p><p>The simplest AIM logs are the plaintext logs, which look like this:</p><div><pre tabindex="0"><code data-lang="text"><span><span>Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005
</span></span><span><span>[18:44] Jane: hi
</span></span><span><span>[18:55] Me: hey whats up
</span></span><span><span>Session Close (Jane): Mon Sep 12 18:56:02 2005
</span></span></code></pre></div><p>Every decade or so, I try writing a universal AIM log parser to get all of my old logs into a consistent, readable format. Unfortunately, I always get bored and give up partway through. My last attempt was <a href="https://github.com/mtlynch/chat_unifier">seven years ago</a>, when I tried doing it in Python 2.7.</p><p>Parsing logs is a great match for Gleam because some parts of the project are easy (e.g., parsing the plaintext logs), so I can do the easy parts while I get the hang of Gleam as a language and gradually build up to the harder log formats and adding a web frontend.</p><p>I’ve also heard that functional languages lend themselves especially well to parsing tasks, and I’ve never understood why, so it’s a good opportunity to learn.</p><h2 id="my-background-in-programming-languages">My background in programming languages<a href="#my-background-in-programming-languages" arialabel="Anchor"> 🔗︎</a></h2><p>I’ve been a programmer for 20 years, but I’m no language design connoisseur. I’m sharing things about Gleam I find unintuitive or difficult to work with, but they’re not language critiques, just candid reactions.</p><p>I’ve never worked in a langauge that’s designed for functional programming. The closest would be JavaScript. The <a href="https://mtlynch.io/notes/which-new-language/#how-much-i-enjoy-various-languages">languages I know best</a> are Go and Python.</p><h2 id="how-do-i-parse-command-line-args">How do I parse command-line args?<a href="#how-do-i-parse-command-line-args" arialabel="Anchor"> 🔗︎</a></h2><p>The first thing I wanted to do was figure out how to parse a command-line argument so I could call my app like this:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>./log-parser ~/logs/aim/plaintext
</span></span></code></pre></div><p>But there’s no Gleam standard library module for reading command-line arguments. I found <a href="https://hexdocs.pm/glint/">glint</a>, and it felt super complicated for just reading one command-line argument. Then, I realized there’s a simpler third-party library called <a href="https://hexdocs.pm/argv/">argv</a>.</p><p>I can parse the command-line argument like this:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>pub</span><span> </span><span>fn</span><span> </span><span>main</span>()<span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>argv.<span>load</span>().arguments<span> </span>{<span>
</span></span></span><span><span><span>    </span>[path]<span> </span>-&gt;<span> </span>io.<span>println</span>(<span>"command-line arg is "</span><span> </span>&lt;&gt;<span> </span>path)<span>
</span></span></span><span><span><span>    </span>_<span> </span>-&gt;<span> </span>io.<span>println</span>(<span>"Usage: gleam run &lt;directory_path&gt;"</span>)<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam run ~/whatever
</span></span><span><span>   Compiled in 0.01s
</span></span><span><span>    Running log_parser.main
</span></span><span><span>command-line arg is /home/mike/whatever
</span></span></code></pre></div><p>Cool, easy enough!</p><h2 id="what-does-gleam-build-do">What does <code>gleam build</code> do?<a href="#what-does-gleam-build-do" arialabel="Anchor"> 🔗︎</a></h2><p>I got my program to run with <code>gleam run</code>, but I was curious if I could compile an executable like <code>go build</code> or <code>zig build</code> does.</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam build
</span></span><span><span>   Compiled in 0.01s
</span></span></code></pre></div><p>Hmm, compiled what? I couldn’t see a binary anywhere.</p><p>The <a href="https://gleam.run/command-line-reference/#build">documentation for <code>gleam build</code></a> just says “Build the project” but doesn’t explain <em>what</em> it builds or where it stores the build artifact.</p><p>There’s a <code>build</code> directory, but it doesn’t produce an obvious executable.</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ rm -rf build &amp;&amp; gleam build
</span></span><span><span>Downloading packages
</span></span><span><span> Downloaded <span>5</span> packages in 0.00s
</span></span><span><span>  Compiling argv
</span></span><span><span>  Compiling gleam_stdlib
</span></span><span><span>  Compiling filepath
</span></span><span><span>  Compiling gleeunit
</span></span><span><span>  Compiling simplifile
</span></span><span><span>  Compiling log_parser
</span></span><span><span>   Compiled in 0.52s
</span></span><span><span>
</span></span><span><span>$ ls -1 build/
</span></span><span><span>dev
</span></span><span><span>gleam-dev-erlang.lock
</span></span><span><span>gleam-dev-javascript.lock
</span></span><span><span>gleam-lsp-erlang.lock
</span></span><span><span>gleam-lsp-javascript.lock
</span></span><span><span>gleam-prod-erlang.lock
</span></span><span><span>gleam-prod-javascript.lock
</span></span><span><span>packages
</span></span></code></pre></div><p>From poking around, I think the executables are under <code>build/dev/erlang/log_parser/ebin/</code>:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ ls -1 build/dev/erlang/log_parser/ebin/
</span></span><span><span>log_parser.app
</span></span><span><span>log_parser.beam
</span></span><span><span>log_parser@@main.beam
</span></span><span><span>log_parser_test.beam
</span></span><span><span>plaintext_logs.beam
</span></span><span><span>plaintext_logs_test.beam
</span></span></code></pre></div><p>Those appear to be BEAM bytecode, so I can’t execute them directly. I assume I could get run the BEAM VM manually and execute those files somehow, but that doesn’t sound appealing.</p><p>So, I’ll stick to <code>gleam run</code> to run my app, but I wish <code>gleam build</code> had a better explanation of what it produced and what the developer can do with it.</p><h2 id="let-me-implement-the-simplest-possible-parser">Let me implement the simplest possible parser<a href="#let-me-implement-the-simplest-possible-parser" arialabel="Anchor"> 🔗︎</a></h2><p>To start, I decided to write a function that does basic parsing of plaintext logs.</p><p>So, I wrote a test with what I wanted.</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse_simple_plaintext_log_test</span>()<span> </span>{<span>
</span></span></span><span><span><span>  </span><span>"
</span></span></span><span><span><span>Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005
</span></span></span><span><span><span>[18:44] Jane: hi
</span></span></span><span><span><span>[18:55] Me: hey whats up
</span></span></span><span><span><span>Session Close (Jane): Mon Sep 12 18:56:02 2005
</span></span></span><span><span><span>"</span><span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>string.trim<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>plaintext_logs.parse<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>should.<span>equal</span>([<span>"hi"</span>,<span> </span><span>"hey whats up"</span>])<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>Eventually, I want to parse all the metadata in the conversation, including names, timestamps, and session information. But as a first step, all my function has to do is read an AIM chat log as a string and emit a list of the chat messages as separate strings.</p><p>That meant my actual function would look like this:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse</span>(contents:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>List</span>(<span>String</span>)<span> </span>{<span>
</span></span></span><span><span><span>  </span><span>// Note: todo is a Gleam language keyword to indicate unfinished code.
</span></span></span><span><span><span></span><span>  </span><span>todo</span><span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>Just to get it compiling, I add in a dummy implementation:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse</span>(contents:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>List</span>(<span>String</span>)<span> </span>{<span>
</span></span></span><span><span><span>  </span>[<span>"fake"</span>,<span> </span><span>"data"</span>]<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>And I can test it like this:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>  Compiling log_parser
</span></span><span><span>warning: Unused variable
</span></span><span><span>  ┌─ /home/mike/code/gleam-log-parser2/src/plaintext_logs.gleam:1:14
</span></span><span><span>  │
</span></span><span><span><span>1</span> │ pub fn parse(contents: String) -&gt; List(String) {
</span></span><span><span>  │              ^^^^^^^^^^^^^^^^ This variable is never used
</span></span><span><span>
</span></span><span><span>Hint: You can ignore it with an underscore: <span>`</span>_contents<span>`</span>.
</span></span><span><span>
</span></span><span><span>   Compiled in 0.22s
</span></span><span><span>    Running log_parser_test.main
</span></span><span><span>F
</span></span><span><span>Failures:
</span></span><span><span>
</span></span><span><span>  1) plaintext_logs_test.parse_simple_plaintext_log_test: module <span>'plaintext_logs_test'</span>
</span></span><span><span>     Values were not equal
</span></span><span><span>     expected: [<span>"hi"</span>, <span>"hey whats up"</span>]
</span></span><span><span>          got: [<span>"fake"</span>, <span>"data"</span>]
</span></span><span><span>     output:
</span></span><span><span>
</span></span><span><span>Finished in 0.008 seconds
</span></span><span><span><span>1</span> tests, <span>1</span> failures
</span></span></code></pre></div><p>Cool, that’s what I expected. The test is failing because it’s returning hardcoded dummy results that don’t match my test.</p><h2 id="adjusting-my-brain-to-a-functional-language">Adjusting my brain to a functional language<a href="#adjusting-my-brain-to-a-functional-language" arialabel="Anchor"> 🔗︎</a></h2><p>Okay, now it’s time to implement the parsing for real. I need to implement this function:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse</span>(contents:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>List</span>(<span>String</span>)<span> </span>{<span>
</span></span></span><span><span><span>  </span><span>todo</span><span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>At this point, I kind of froze up. It struck me that Gleam excludes so many of the tools I’m used to in other languages:</p><ul><li>There are no <code>if</code> statements</li><li>There are no loops</li><li>There’s no <code>return</code> keyword</li><li>There are no list index accessors<ul><li>e.g., you can’t access the n-th element of a <code>List</code></li></ul></li></ul><p>What do I even do? Split the string into tokens and then do something with that?</p><p>Eventually, I realized for a simple implementation, I wanted to just split the string into lines, so I want to do this:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse</span>(contents:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>List</span>(<span>String</span>)<span> </span>{<span>
</span></span></span><span><span><span>  </span>string.<span>split</span>(contents,<span> </span>on:<span> </span><span>"</span><span>\n</span><span>"</span>)<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>If I test again, I get this:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>  Compiling log_parser
</span></span><span><span>   Compiled in 0.21s
</span></span><span><span>    Running log_parser_test.main
</span></span><span><span>F
</span></span><span><span>Failures:
</span></span><span><span>
</span></span><span><span>  1) plaintext_logs_test.parse_simple_plaintext_log_test: module <span>'plaintext_logs_test'</span>
</span></span><span><span>     Values were not equal
</span></span><span><span>     expected: [<span>"hi"</span>, <span>"hey whats up"</span>]
</span></span><span><span>          got: [<span>"Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005"</span>, <span>"[18:44] Jane: hi"</span>, <span>"[18:55] Me: hey whats up"</span>, <span>"Session Close (Jane): Mon Sep 12 18:56:02 2005"</span>]
</span></span><span><span>     output:
</span></span><span><span>
</span></span><span><span>Finished in 0.009 seconds
</span></span><span><span><span>1</span> tests, <span>1</span> failures
</span></span></code></pre></div><p>Okay, now I’m a little closer.</p><h2 id="how-do-i-iterate-over-a-list-in-a-language-with-no-loops">How do I iterate over a list in a language with no loops?<a href="#how-do-i-iterate-over-a-list-in-a-language-with-no-loops" arialabel="Anchor"> 🔗︎</a></h2><p>I turned my logs into a list of lines, but that’s where I got stuck again.</p><p>I’m so used to <code>for</code> loops that my brain kept thinking, “How do I do a <code>for</code> loop to iterate over the elements?”</p><p>I realized I needed to call <a href="https://hexdocs.pm/gleam_stdlib/gleam/list.html#map"><code>list.map</code></a>. I need to define a function that acts on each element of the list.</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>import</span><span> </span>gleam/list<span>
</span></span></span><span><span><span></span><span>import</span><span> </span>gleam/string<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>String</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>line<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse</span>(contents:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>List</span>(<span>String</span>)<span> </span>{<span>
</span></span></span><span><span><span>  </span>string.<span>split</span>(contents,<span> </span>on:<span> </span><span>"</span><span>\n</span><span>"</span>)<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>list.<span>map</span>(parse_line)<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>This is my first time using pattern matching in any language, and it’s neat, though it’s still so unfamiliar that I find it hard to recognize when to use it.</p><p>Zooming in a bit on the pattern matching, it’s here:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>line<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span></code></pre></div><p>It evaluates the <code>line</code> variable and matches it to one of the subsequent patterns within the braces. If the line starts with <code>"Session Start"</code> (the <code>&lt;&gt;</code> means the preceding string is a prefix), then Gleam executes the code after the <code>-&gt;</code>, which in this case is just the empty string. Same for <code>"Session Close"</code>.</p><p>If the line doesn’t match the <code>"Session Start"</code> or <code>"Session Close"</code> patterns, Gleam executes the last line in the <code>case</code> which just matches any string. In that case, it evaluates to the same string. Meaning <code>"hi"</code> would evaluate to just <code>"hi"</code>.</p><p>This is where it struck me how strange it feels to not have a <code>return</code> keyword. In every other language I know, you have to explicitly return a value from a function with a <code>return</code> keyword, but in Gleam, the return value is just the value from the last line that Gleam executes in the function.</p><p>If I run my test, I get this:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>  Compiling log_parser
</span></span><span><span>   Compiled in 0.22s
</span></span><span><span>    Running log_parser_test.main
</span></span><span><span>F
</span></span><span><span>Failures:
</span></span><span><span>
</span></span><span><span>  1) plaintext_logs_test.parse_simple_plaintext_log_test: module <span>'plaintext_logs_test'</span>
</span></span><span><span>     Values were not equal
</span></span><span><span>     expected: [<span>"hi"</span>, <span>"hey whats up"</span>]
</span></span><span><span>          got: [<span>""</span>, <span>"[18:44] Jane: hi"</span>, <span>"[18:55] Me: hey whats up"</span>, <span>""</span>]
</span></span><span><span>     output:
</span></span><span><span>
</span></span><span><span>Finished in 0.009 seconds
</span></span><span><span><span>1</span> tests, <span>1</span> failures
</span></span></code></pre></div><p>Again, this is what I expected, and I’m a bit closer to my goal.</p><p>I’ve converted the <code>"Session Start"</code> and <code>"Session End"</code> lines to empty strings, and the middle two elements of the list are the lines that have AIM messages in them.</p><p>The remaining work is:</p><ul><li>Strip out the time and sender parts of the log lines.</li><li>Filter out empty strings.</li></ul><h2 id="scraping-an-aim-message-from-a-line">Scraping an AIM message from a line<a href="#scraping-an-aim-message-from-a-line" arialabel="Anchor"> 🔗︎</a></h2><p>At this point, I have a string like this:</p><p>And I need to extract just the portion after the sender’s name to this:</p><p>My instinct is to use a string split function and split on the <code>:</code> character. I see that there’s <a href="https://hexdocs.pm/gleam_stdlib/gleam/string.html#split"><code>string.split</code></a> which returns <code>List(String)</code>.</p><p>There’s also a <a href="https://hexdocs.pm/gleam_stdlib/gleam/string.html#split_once"><code>string.split_once</code></a> function, which should work because I can split once on <code>: </code>(note the trailing space after the colon).</p><p>The problem is that <code>split_once</code> returns <code>Result(#(String, String), Nil)</code>, a type that feels scarier to me. It’s a two-tuple wrapped in a <code>Result</code>, which means that the function can return an error on failure. It’s confusing that <code>split_once</code> can fail whereas <code>split</code> cannot, so for simplicity, I’ll go with <code>split</code>.</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>String</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>{<span>
</span></span></span><span><span><span>      </span><span>echo</span><span> </span>string.<span>split</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span>
</span></span></span><span><span><span>      </span><span>todo</span><span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>If I run my test, I get this:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>warning: Todo found
</span></span><span><span>   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:10:7
</span></span><span><span>   │
</span></span><span><span><span>10</span> │       todo
</span></span><span><span>   │       ^^^^ This code is incomplete
</span></span><span><span>
</span></span><span><span>This code will crash <span>if</span> it is run. Be sure to finish it before
</span></span><span><span>running your program.
</span></span><span><span>
</span></span><span><span>Hint: I think its <span>type</span> is <span>`</span>String<span>`</span>.
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>   Compiled in 0.01s
</span></span><span><span>    Running log_parser_test.main
</span></span><span><span>src/plaintext_logs.gleam:9
</span></span><span><span>[<span>"[18:44] Jane"</span>, <span>"hi"</span>]
</span></span></code></pre></div><p>Good. That’s doing what I want. I’m successfully isolating the <code>"hi"</code> part, so now I just have to return it.</p><h2 id="how-do-i-access-the-last-element-of-a-list">How do I access the last element of a list?<a href="#how-do-i-access-the-last-element-of-a-list" arialabel="Anchor"> 🔗︎</a></h2><p>At this point, I feel close to victory. I’ve converted the line to a list of strings, and I know the string I want is the last element of the list, but how do I grab it?</p><p>In most other languages, I’d just say <code>line_parts[1]</code>, but Gleam’s lists have no accessors by index.</p><p>Looking at the <code>gleam/list</code> module, I see a <a href="https://hexdocs.pm/gleam_stdlib/gleam/list.html#last"><code>list.last</code></a> function, so I try that:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>String</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>{<span>
</span></span></span><span><span><span>       </span>string.<span>split</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span>
</span></span></span><span><span><span>       </span>|&gt;<span> </span>list.last<span>
</span></span></span><span><span><span>       </span>|&gt;<span> </span><span>echo</span><span>
</span></span></span><span><span><span>       </span>|&gt;<span> </span><span>todo</span><span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>If I run that, I get:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>  Compiling log_parser
</span></span><span><span>warning: Todo found
</span></span><span><span>   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:12:11
</span></span><span><span>   │
</span></span><span><span><span>12</span> │        |&gt; todo
</span></span><span><span>   │           ^^^^ This code is incomplete
</span></span><span><span>
</span></span><span><span>This code will crash <span>if</span> it is run. Be sure to finish it before
</span></span><span><span>running your program.
</span></span><span><span>
</span></span><span><span>Hint: I think its <span>type</span> is <span>`</span>fn(Result(String, Nil)) -&gt; String<span>`</span>.
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>   Compiled in 0.24s
</span></span><span><span>    Running log_parser_test.main
</span></span><span><span>src/plaintext_logs.gleam:11
</span></span><span><span>Ok(<span>"hi"</span>)
</span></span></code></pre></div><p>A bit closer! I’ve extracted the last element of the list to find <code>"hi"</code>, but now it’s wrapped in a <a href="https://tour.gleam.run/data-types/results/"><code>Result</code> type</a>.</p><p>I can unwrap it with <a href="https://hexdocs.pm/gleam_stdlib/gleam/result.html#unwrap"><code>result.unwrap</code></a></p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>String</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>{<span>
</span></span></span><span><span><span>       </span>string.<span>split</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span>
</span></span></span><span><span><span>       </span>|&gt;<span> </span>list.last<span>
</span></span></span><span><span><span>       </span>|&gt;<span> </span>result.<span>unwrap</span>(<span>""</span>)<span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>Re-running <code>gleam test</code> yields:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>  Compiling log_parser
</span></span><span><span>   Compiled in 0.22s
</span></span><span><span>    Running log_parser_test.main
</span></span><span><span>F
</span></span><span><span>Failures:
</span></span><span><span>
</span></span><span><span>  1) plaintext_logs_test.parse_simple_plaintext_log_test: module <span>'plaintext_logs_test'</span>
</span></span><span><span>     Values were not equal
</span></span><span><span>     expected: [<span>"hi"</span>, <span>"hey whats up"</span>]
</span></span><span><span>          got: [<span>""</span>, <span>"hi"</span>, <span>"hey whats up"</span>, <span>""</span>]
</span></span><span><span>     output:
</span></span><span><span>
</span></span><span><span>Finished in 0.008 seconds
</span></span><span><span><span>1</span> tests, <span>1</span> failures
</span></span></code></pre></div><p>Great! That did what I wanted. I reduced the messages lines to just the contents of the messages.</p><h2 id="filtering-out-empty-strings">Filtering out empty strings<a href="#filtering-out-empty-strings" arialabel="Anchor"> 🔗︎</a></h2><p>The only thing that’s left is to filter the empty strings out of the list, which is straightforward enough with <a href="https://hexdocs.pm/gleam_stdlib/gleam/list.html#filter"><code>list.filter</code></a>:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse</span>(contents:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>List</span>(<span>String</span>)<span> </span>{<span>
</span></span></span><span><span><span>  </span>string.<span>split</span>(contents,<span> </span>on:<span> </span><span>"</span><span>\n</span><span>"</span>)<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>list.<span>map</span>(parse_line)<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>list.<span>filter</span>(<span>fn</span>(s)<span> </span>{<span> </span>!string.<span>is_empty</span>(s)<span> </span>})<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>And I re-run the tests:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>  Compiling log_parser
</span></span><span><span>   Compiled in 0.22s
</span></span><span><span>    Running log_parser_test.main
</span></span><span><span>.
</span></span><span><span>Finished in 0.007 seconds
</span></span><span><span><span>1</span> tests, <span>0</span> failures
</span></span></code></pre></div><p>Voilà! The tests now pass!</p><h2 id="tidying-up-string-splitting">Tidying up string splitting<a href="#tidying-up-string-splitting" arialabel="Anchor"> 🔗︎</a></h2><p>My tests are now passing, so theoretically, I’ve achieved my initial goal.</p><p>I could declare victory and call it a day. Or, I could refactor!</p><p>I’ll refactor.</p><p>I feel somewhat ashamed of my string splitting logic, as it didn’t feel like idiomatic Gleam. Can I do it without getting into result unwrapping?</p><p>Re-reading it, I realize I can solve it with this newfangled pattern matching thing. I know that the string will split into a list with two elements, so I can create a pattern for a two-element list:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>String</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>{<span>
</span></span></span><span><span><span>       </span><span>case</span><span> </span>string.<span>split</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span> </span>{<span>
</span></span></span><span><span><span>          </span>[_,<span> </span>message]<span> </span>-&gt;<span> </span>message<span>
</span></span></span><span><span><span>          </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>       </span>}<span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>That feels a little more elegant than calling <code>result.last</code>.</p><p>Can I tidy this up further? I avoided <code>string.split_once</code> because the type was too confusing, but it’s probably the better option if I expect only one split, so what does that look like?</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>String</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>{<span>
</span></span></span><span><span><span>       </span><span>echo</span><span> </span>string.<span>split_once</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span>
</span></span></span><span><span><span>       </span><span>todo</span><span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>To inspect the data, I run my test again:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>[...]
</span></span><span><span>src/plaintext_logs.gleam:9
</span></span><span><span>Ok(<span>#("[18:44] Jane", "hi"))</span>
</span></span></code></pre></div><p>Okay, that doesn’t look as scary as I thought. Even though my first instinct is to unwrap the error and access the last element in the tuple (which actually is easy for tuples, just not lists), I know at this point that there’s probably a pattern-matchy way. And there is:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>String</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>{<span>
</span></span></span><span><span><span>       </span><span>case</span><span> </span>string.<span>split_once</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span> </span>{<span>
</span></span></span><span><span><span>        </span><span>Ok</span>(#(_,<span> </span>message))<span> </span>-&gt;<span> </span>message<span>
</span></span></span><span><span><span>        </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>       </span>}<span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>The <code>Ok(#(_, message))</code> pattern will match a successful result from <code>split_once</code>, which is a two-tuple of <code>String</code> wrapped in an <code>Ok</code> result. The other <code>case</code> option is the catchall that returns an empty string.</p><h2 id="getting-rid-of-the-empty-string-hack">Getting rid of the empty string hack<a href="#getting-rid-of-the-empty-string-hack" arialabel="Anchor"> 🔗︎</a></h2><p>One of the compelling features of Gleam for me is its static typing, so it feels hacky that I’m abusing the empty string to represent a lack of message on a particular line. Can I use the type system instead of using empty strings as sentinel values?</p><p>The pattern in Gleam for indicating that something might fail but the failure isn’t necessarily an error is <code>Result(&lt;type&gt;, Nil)</code>, so let me try to rewrite it that way:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>import</span><span> </span>gleam/list<span>
</span></span></span><span><span><span></span><span>import</span><span> </span>gleam/result<span>
</span></span></span><span><span><span></span><span>import</span><span> </span>gleam/string<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>Result</span>(<span>String</span>,<span> </span><span>Nil</span>)<span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>Error</span>(<span>Nil</span>)<span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>Error</span>(<span>Nil</span>)<span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>{<span>
</span></span></span><span><span><span>       </span><span>case</span><span> </span>string.<span>split_once</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span> </span>{<span>
</span></span></span><span><span><span>        </span><span>Ok</span>(#(_,<span> </span>message))<span> </span>-&gt;<span> </span><span>Ok</span>(message)<span>
</span></span></span><span><span><span>        </span>_<span> </span>-&gt;<span> </span><span>Error</span>(<span>Nil</span>)<span>
</span></span></span><span><span><span>       </span>}<span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse</span>(contents:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>List</span>(<span>String</span>)<span> </span>{<span>
</span></span></span><span><span><span>  </span>string.<span>split</span>(contents,<span> </span>on:<span> </span><span>"</span><span>\n</span><span>"</span>)<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>list.<span>map</span>(parse_line)<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>result.values<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>Great! I like being more explicit that the lines without messages return <code>Error(Nil)</code> rather than an empty string. Also, <code>result.values</code> is more succinct for filtering empty lines than the previous <code>list.filter(fn(s) { !string.is_empty(s) })</code>.</p><h2 id="overall-reflections">Overall reflections<a href="#overall-reflections" arialabel="Anchor"> 🔗︎</a></h2><p>After spending a few hours with Gleam, I’m enjoying it. It pushes me out of my comfort zone the right amount where I feel like I’m learning new ways of thinking about programming but not so much that I’m too overwhelmed to learn anything.</p><p>The biggest downside I’m finding with Gleam is that it’s a young language with a relatively small team. It <a href="https://lpil.uk/blog/hello-gleam/">just turned six years old</a>, but it looks like the founder was working on it solo <a href="https://github.com/gleam-lang/gleam/graphs/contributors?selectedMetric=additions">until a year ago</a>. There are now a handful of core maintainers, but I don’t know if any of them work on Gleam full-time, so the ecosystem is a bit limited. I’m looking ahead to parsing other log formats that are in HTML and XML, and there are Gleam HTML and XML parsers, but they don’t seem widely used, so I’m not sure how well they’ll work.</p><h3 id="love-pipelines">Love: Pipelines<a href="#love-pipelines" arialabel="Anchor"> 🔗︎</a></h3><p>I love love love Gleam’s pipeline syntax. You can see me using it in the test with the <code>|&gt;</code> characters:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span> </span><span>"..."</span><span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>string.trim<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>plaintext_logs.parse<span>
</span></span></span><span><span><span>  </span>|&gt;<span> </span>should.<span>equal</span>([<span>"hi"</span>,<span> </span><span>"hey whats up"</span>])<span>
</span></span></span></code></pre></div><p>The non-pipeline equivalent of the test would look like this:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>pub</span><span> </span><span>fn</span><span> </span><span>parse_simple_plaintext_log_test</span>()<span> </span>{<span>
</span></span></span><span><span><span>  </span><span>let</span><span> </span>input<span> </span>=<span> </span><span>"..."</span><span>
</span></span></span><span><span><span>  </span><span>let</span><span> </span>trimmed<span> </span>=<span> </span>string.<span>trim</span>(input)<span>
</span></span></span><span><span><span>  </span><span>let</span><span> </span>parsed<span> </span>=<span> </span>plaintext_logs.<span>parse</span>(trimmed)<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span>should.<span>equal</span>(parsed,<span> </span>[<span>"hi"</span>,<span> </span><span>"hey whats up"</span>])<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>It looks like wet garbage by comparison.</p><p>Now that I’ve seen pipelines, they feel so obvious and conspicuously missing in every other programming language I use.</p><p>I’ve enjoyed pipelining in bash, but it never occurred to me how strange it is that other programming languages never adopted it.</p><h3 id="like-example-centric-documentation">Like: Example-centric documentation<a href="#like-example-centric-documentation" arialabel="Anchor"> 🔗︎</a></h3><p>The Gleam documentation is a bit terse, but I like that it’s so example-heavy.</p><p>I learn best by reading examples, so I appreciate that so much of the Gleam standard library is documented with examples showing simple usage of each API function.</p><h3 id="like-built-in-unused-symbol-warnings">Like: Built-in unused symbol warnings<a href="#like-built-in-unused-symbol-warnings" arialabel="Anchor"> 🔗︎</a></h3><p>I like that the Gleam compiler natively warns about unused functions, variables, and imports. And I like that these are warnings rather than errors.</p><p>In Go, I get frustrated during debugging when I temporarily comment something out and then the compiler stubbornly refuses to do anything until I fix the stupid import, which I then have to un-fix when I finish whatever I was debugging.</p><h3 id="like-todo-keyword">Like: <code>todo</code> keyword<a href="#like-todo-keyword" arialabel="Anchor"> 🔗︎</a></h3><p>One of my favorite dumb programming jokes happened at my first programming job about 15 years ago. On a group email thread with several C++ developers, my friend shared a hot tip about C++ development.</p><p>He said that if we were ever got fed up with arcane C++ compilation errors, we could just add a special line to our source code, and then even invalid C++ code would compile successfully:</p><p>Spoiler alert: it’s not a real C++ preprocessor directive.</p><p>But I’ve found myself occasionally wishing languages had something like this when I’m in the middle of development and don’t care about whatever bugs the compiler is trying to protect me from.</p><p>Gleam’s <code>todo</code> is almost like a <code>#pragma always_compile</code>. Even if your code is invalid, the Gleam compiler just says, “Okay, fine. I’ll run it anyway.”</p><p>You can see this when I was in the middle of implementing <code>parse_line</code>:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span><span>fn</span><span> </span><span>parse_line</span>(line:<span> </span><span>String</span>)<span> </span>-&gt;<span> </span><span>String</span><span> </span>{<span>
</span></span></span><span><span><span>  </span><span>case</span><span> </span>line<span> </span>{<span>
</span></span></span><span><span><span>    </span><span>"Session Start"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span><span>"Session Close"</span><span> </span>&lt;&gt;<span> </span>_<span> </span>-&gt;<span> </span><span>""</span><span>
</span></span></span><span><span><span>    </span>line<span> </span>-&gt;<span> </span>{<span>
</span></span></span><span><span><span>      </span><span>echo</span><span> </span>string.<span>split</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span>
</span></span></span><span><span><span>      </span><span>todo</span><span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>  </span>}<span>
</span></span></span><span><span><span></span>}<span>
</span></span></span></code></pre></div><p>If I take out the <code>todo</code>, Gleam refuses to run the code at all:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>  Compiling log_parser
</span></span><span><span>error: Type mismatch
</span></span><span><span>   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:8:5
</span></span><span><span>   │
</span></span><span><span> <span>8</span> │ ╭     line -&gt; {
</span></span><span><span> <span>9</span> │ │       <span>echo</span> string.split(line, on: <span>": "</span>)
</span></span><span><span><span>10</span> │ │     }
</span></span><span><span>   │ ╰─────^
</span></span><span><span>
</span></span><span><span>This <span>case</span> clause was found to <span>return</span> a different <span>type</span> than the previous
</span></span><span><span>one, but all <span>case</span> clauses must <span>return</span> the same type.
</span></span><span><span>
</span></span><span><span>Expected type:
</span></span><span><span>
</span></span><span><span>    String
</span></span><span><span>
</span></span><span><span>Found type:
</span></span><span><span>
</span></span><span><span>    List(String)
</span></span></code></pre></div><p>Right, I’m returning an incorrect type, so why would the compiler cooperate with me?</p><p>But adding <code>todo</code> lets me run the function anyway, which helps me understand what the code is doing even though I haven’t finished implementing it:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ gleam <span>test</span>
</span></span><span><span>warning: Todo found
</span></span><span><span>   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:10:7
</span></span><span><span>   │
</span></span><span><span><span>10</span> │       todo
</span></span><span><span>   │       ^^^^ This code is incomplete
</span></span><span><span>
</span></span><span><span>This code will crash <span>if</span> it is run. Be sure to finish it before
</span></span><span><span>running your program.
</span></span><span><span>
</span></span><span><span>Hint: I think its <span>type</span> is <span>`</span>String<span>`</span>.
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>  Compiling log_parser
</span></span><span><span>   Compiled in 0.21s
</span></span><span><span>    Running log_parser_test.main
</span></span><span><span>src/plaintext_logs.gleam:9
</span></span><span><span>[<span>"[18:44] Jane"</span>, <span>"hi"</span>]
</span></span><span><span>F
</span></span><span><span>[...]
</span></span><span><span>Finished in 0.007 seconds
</span></span><span><span><span>1</span> tests, <span>1</span> failures
</span></span></code></pre></div><h3 id="like-pattern-matching">Like: Pattern matching<a href="#like-pattern-matching" arialabel="Anchor"> 🔗︎</a></h3><p>I find pattern matching elegant and concise, though it’s the part of Gleam I find hardest to adjust to. It feels so different from procedural style of programming I’m accustomed to in other languages I know.</p><p>The downside is that I have a hard time recognizing when pattern matching is the right tool, and I also find pattern matching harder to read. But I think that’s just inexperience, and I think with more practice, I’ll be able to think in pattern matching.</p><h3 id="dislike-error-handling">Dislike: Error handling<a href="#dislike-error-handling" arialabel="Anchor"> 🔗︎</a></h3><p>I find Gleam’s error handling pretty awkward, especially because errors ruin the beauty of nice, tidy pipelines.</p><p>For example, if I had a string processing pipeline like this:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span>string.<span>split</span>(line,<span> </span>on:<span> </span><span>"-"</span>)<span>
</span></span></span><span><span><span></span>|&gt;<span> </span>list.last<span>
</span></span></span><span><span><span></span>|&gt;<span> </span>result.<span>unwrap</span>(<span>""</span>)<span> </span><span>// Ugly!
</span></span></span><span><span><span></span>|&gt;<span> </span>string.uppercase<span>
</span></span></span></code></pre></div><p>That <code>result.unwrap</code> line feels so ugly and out of place to me. I wish the syntax was like this:</p><div><pre tabindex="0"><code data-lang="gleam"><span><span>string.<span>split</span>(line,<span> </span>on:<span> </span><span>": "</span>)<span>
</span></span></span><span><span><span></span>|&gt;<span> </span>try<span> </span>list.last<span>
</span></span></span><span><span><span></span>|&gt;<span> </span>string.uppercase<span>
</span></span></span><span><span><span></span>|&gt;<span> </span><span>Ok</span><span>
</span></span></span></code></pre></div><p>Where <code>try</code> causes the function to return an error, kind of like <a href="https://ziglang.org/documentation/0.14.1/#try">in Zig</a>.</p><h3 id="dislike-small-core-language">Dislike: Small core language<a href="#dislike-small-core-language" arialabel="Anchor"> 🔗︎</a></h3><p>I don’t know if this is a long-term design choice or if it’s just small for now because it’s an indie-developed language, but the first thing about Gleam that stood out to me is how few built-in features there are.</p><p>For example, there’s no built-in feature for iterating over the elements of a <a href="https://tour.gleam.run/everything/#basics-lists"><code>List</code> type</a>, and the type itself doesn’t expose a function to iterate it, so you have to use <a href="https://hexdocs.pm/gleam_stdlib/gleam/list.html">the <code>gleam/list</code> module</a> in the standard library.</p><p>Similarly, if a function can fail, it returns a <a href="https://tour.gleam.run/everything/#data-types-results"><code>Result</code> type</a>, and there are no built-in functions for handling a <code>Result</code>, so you have to use the <a href="https://hexdocs.pm/gleam_stdlib/gleam/result.html"><code>gleam/result</code> module</a> to check if the function succeeded.</p><p>To me, that functionality feels so core to the language that it would be part of the language itself, not the standard library.</p><h3 id="dislike-limited-standard-library">Dislike: Limited standard library<a href="#dislike-limited-standard-library" arialabel="Anchor"> 🔗︎</a></h3><p>In addition to the language feeling small, the standard library feels pretty limited as well.</p><p>There are currently only 19 modules in <a href="https://hexdocs.pm/gleam_stdlib/">the Gleam standard library</a>. Conspicuously absent are modules for working with the filesystem (the de facto standard seems to be the third-party <a href="https://hexdocs.pm/simplifile/">simplifile</a> module).</p><p>For comparison, the standard libraries for <a href="https://docs.python.org/3/library/index.html">Python</a> and <a href="https://pkg.go.dev/std">Go</a> each have about 250 modules. Although, in fairness, those languages have about 1000x the resources as Gleam.</p><h2 id="source-code">Source code<a href="#source-code" arialabel="Anchor"> 🔗︎</a></h2><p>The source code for this project is available on Codeberg:</p><ul><li><a href="https://codeberg.org/mtlynch/gleam-chat-log-parser">https://codeberg.org/mtlynch/gleam-chat-log-parser</a></li></ul><p>Commit <a href="https://codeberg.org/mtlynch/gleam-chat-log-parser/src/commit/291e6d77a0ae00e4962f12253c356568b679aab6">291e6d</a> is the version that matches this blog post.</p><hr><p><em>Thanks to <a href="https://www.ihh.dev/">Isaac Harris-Holt</a> for helpful feedback on this post.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A store that generates products from anything you type in search (857 pts)]]></title>
            <link>https://anycrap.shop/</link>
            <guid>45231378</guid>
            <pubDate>Sat, 13 Sep 2025 12:02:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anycrap.shop/">https://anycrap.shop/</a>, See on <a href="https://news.ycombinator.com/item?id=45231378">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div><h2>Search The Infinite Product Catalog</h2><p>We'll find it somewhere across parallel dimensions, just tell us what you want</p><p><a href="https://anycrap.shop/find">Find It Now</a></p></div><div><picture><source srcset="https://anycrap.shop/crap_opt_md.webp" type="image/webp"><img src="https://anycrap.shop/crap_opt_md.png" alt="Search The Infinite Product Catalog" width="640" height="640" loading="lazy" decoding="async"></picture></div></div><section><h2>Weird Tech Stuff</h2><h2>Snacks From Outer Space</h2><h2>WTF?</h2></section><div><div><h3>100% Custom Concepts</h3><p>All our products are unique concepts developed specifically for our customers.</p></div><div><h3>Instant Delivery</h3><p>Our product concepts are delivered instantly to your device!</p></div><div><h3>Conceptual Marketplace</h3><p>Experience a new way of shopping where imagination drives innovation.</p></div></div><section><h2>That Product Doesn't Exist Yet?</h2><p>Be the first to discover it! Give us a name and we'll find it somewhere</p><a href="https://anycrap.shop/find">Invent Now</a></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How 'overworked, underpaid' humans train Google's AI to seem smart (264 pts)]]></title>
            <link>https://www.theguardian.com/technology/2025/sep/11/google-gemini-ai-training-humans</link>
            <guid>45231239</guid>
            <pubDate>Sat, 13 Sep 2025 11:30:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/technology/2025/sep/11/google-gemini-ai-training-humans">https://www.theguardian.com/technology/2025/sep/11/google-gemini-ai-training-humans</a>, See on <a href="https://news.ycombinator.com/item?id=45231239">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p><span>I</span>n the spring of 2024, when Rachael Sawyer, a technical writer from Texas, received a LinkedIn message from a recruiter hiring for a vague title of writing analyst, she assumed it would be similar to her previous gigs of content creation. On her first day of work a week later, however, her expectations went bust. Instead of writing words herself, Sawyer’s job was to rate and moderate the content created by artificial intelligence.</p><p>The job initially involved a mix of parsing through meeting notes and chats summarized by Google’s Gemini, and, in some cases, reviewing short films made by the AI.</p><p>On occasion, she was asked to deal with extreme content, flagging violent and sexually explicit material generated by Gemini for removal, mostly text. Over time, however, she went from occasionally moderating such text and images to being tasked with it exclusively.</p><p>“I was shocked that my job involved working with such distressing content,” said Sawyer, who has been working as a “generalist rater” for Google’s AI products since March 2024. “Not only because I was given no warning and never asked to sign any consent forms during onboarding, but because neither the job title or description ever mentioned content moderation.”</p><p>The pressure to complete dozens of these tasks every day, each within 10 minutes of time, has led Sawyer into spirals of anxiety and panic attacks, she says – without mental health support from her employer.</p><figure id="090cc4e5-cc5f-4a8e-b3d2-eadedd379de2" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:5,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Tech companies are stealing our books, music and films for AI. It’s brazen theft and must be stopped | Anna Funder and Julia Powles&quot;,&quot;elementId&quot;:&quot;090cc4e5-cc5f-4a8e-b3d2-eadedd379de2&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/commentisfree/2025/sep/10/tech-companies-are-stealing-our-books-music-and-films-for-ai-its-brazen-theft-and-must-be-stopped&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:10,&quot;display&quot;:2,&quot;theme&quot;:0}}"></gu-island></figure><p>Sawyer is one among the thousands of AI workers contracted for Google through Japanese conglomerate Hitachi’s GlobalLogic to rate and moderate the output of Google’s AI products, including its flagship chatbot Gemini, launched early last year, and its summaries of search results, AI Overviews. The Guardian spoke to 10 current and former employees from the firm. Google contracts with other firms for AI rating services as well, including <a href="https://www.bloomberg.com/news/articles/2023-07-12/google-s-ai-chatbot-is-trained-by-humans-who-say-they-re-overworked-underpaid-and-frustrated?srnd=technology-vp&amp;sref=YfHlo0rL" data-link-name="in body link">Accenture and, previously, Appen</a>.</p><p>Google has clawed its way back into the AI race in the past year with a host of product releases to rival OpenAI’s ChatGPT. Google’s most advanced reasoning model, Gemini 2.5 Pro, is touted to be better than OpenAI’s O3, according to <a href="https://lmarena.ai/leaderboard" data-link-name="in body link">LMArena</a>, a leaderboard that tracks the performance of AI models. Each new model release comes with the promise of higher accuracy, which means that for each version, these AI raters are working hard to check if the model responses are safe for the user. Thousands of humans lend their intelligence to teach chatbots the right responses across domains as varied as medicine, architecture and astrophysics, correcting mistakes and steering away from harmful outputs.</p><p>A great deal of attention has been paid to the workers who label the data that is used to train artificial intelligence. There is, however, another corps of workers, including Sawyer, working day and night to moderate the output of AI, ensuring that chatbots’ billions of users see only safe and appropriate responses.</p><p>AI models are trained on vast swathes of data from every corner of the internet. Workers such as Sawyer sit in a middle layer of the global AI supply chain – paid more than data annotators in <a href="https://time.com/6247678/openai-chatgpt-kenya-workers/" data-link-name="in body link">Nairobi</a> or <a href="https://equidem.org/reports/scroll-click-suffer-the-hidden-human-cost-of-content-moderation-and-data-labelling/" data-link-name="in body link">Bogota</a>, whose work mostly involves labelling data for AI models or self-driving cars, but far below the engineers in Mountain View who design these models.</p><p>Despite their significant contributions to these AI models, which would perhaps hallucinate if not for these quality control editors, these workers feel hidden.</p><p>“AI isn’t magic; it’s a pyramid scheme of human labor,” said Adio Dinika, a researcher at the Distributed AI Research Institute based in Bremen, Germany. “These raters are the middle rung: invisible, essential and expendable.”</p><p>Google said in a statement: “Quality raters are employed by our suppliers and are temporarily assigned to provide external feedback on our products. Their ratings are one of many aggregated data points that help us measure how well our systems are working, but do not directly impact our algorithms or models.” GlobalLogic declined to comment for this story.</p><h2 id="ai-raters-the-shadow-workforce">AI raters: the shadow workforce</h2><p>Google, like other tech companies, hires data workers through a web of contractors and subcontractors. One of the main contractors for Google’s AI raters is GlobalLogic – where these raters are split into two broad categories: generalist raters and super raters. Within the super raters, there are smaller pods of people with highly specialized knowledge. Most workers hired initially for the roles were teachers. Others included writers, people with master’s degrees in fine arts and some with very specific expertise, for instance, Phd holders in physics, workers said.</p><figure id="55ea0764-f529-4ac0-b256-1a1e7d5f7cb2" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/28158bad561e11b342de2c307339ce1f8afd9c6b/0_0_4000_2668/master/4000.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/28158bad561e11b342de2c307339ce1f8afd9c6b/0_0_4000_2668/master/4000.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/28158bad561e11b342de2c307339ce1f8afd9c6b/0_0_4000_2668/master/4000.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/28158bad561e11b342de2c307339ce1f8afd9c6b/0_0_4000_2668/master/4000.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/28158bad561e11b342de2c307339ce1f8afd9c6b/0_0_4000_2668/master/4000.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/28158bad561e11b342de2c307339ce1f8afd9c6b/0_0_4000_2668/master/4000.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"><img alt="A person holding a phone scrolling through text " src="https://i.guim.co.uk/img/media/28158bad561e11b342de2c307339ce1f8afd9c6b/0_0_4000_2668/master/4000.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="296.815" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>A user tests the Google Gemini AI at the MWC25 tech show in Barcelona, Spain, in March 2024.</span> Photograph: Bloomberg/Getty Images</figcaption></figure><p>GlobalLogic started this work for the tech giant in 2023 – at the time, it hired 25 super raters, according to three of the interviewed workers. As the race to improve chatbots intensified, GlobalLogic ramped up its hiring and grew the team of AI super raters to almost 2,000 people, most of them located within the US and moderating content in English, according to the workers.</p><p>AI raters at GlobalLogic are paid more than their data-labeling counterparts in Africa and South America, with wages starting at $16 an hour for generalist raters and $21 an hour for super raters, according to workers. Some are simply thankful to have a gig as the US job market sours, but others say that trying to make Google’s AI products better has come at a personal cost.</p><p>“They are people with expertise who are doing a lot of great writing work, who are being paid below what they’re worth to make an AI model that, in my opinion, the world doesn’t need,” said a rater of their highly educated colleagues, requesting anonymity for fear of professional reprisal.</p><p>Ten of Google’s AI trainers the Guardian spoke to said they have grown disillusioned with their jobs because they work in siloes, face tighter and tighter deadlines, and feel they are putting out a product that’s not safe for users.</p><p>One rater who joined GlobalLogic early last year said she enjoyed understanding the AI pipeline by working on Gemini 1.0, 2.0 and now 2.5, and helping it give “a better answer that sounds more human”. Six months in, though, tighter deadlines kicked in. Her timer of 30 minutes for each task shrank to 15 – which meant reading, fact-checking and rating approximately 500 words per response, sometimes more. The tightening constraints made her question the quality of her work and, by extension, the reliability of the AI. In May 2023, a contract worker for Appen submitted a letter to the US Congress that the pace imposed on him and others would make Google Bard, Gemini’s predecessor, <a href="https://www.bloomberg.com/news/articles/2023-07-12/google-s-ai-chatbot-is-trained-by-humans-who-say-they-re-overworked-underpaid-and-frustrated?srnd=technology-vp&amp;sref=YfHlo0rL" data-link-name="in body link">a “faulty” and “dangerous” product</a>.</p><h2 id="high-pressure-little-information">High pressure, little information</h2><p>One worker who joined GlobalLogic in spring 2024 and has worked on five different projects so far, including Gemini and AI Overviews, described her work as being presented with a prompt – either user-generated or synthetic – and with two sample responses, then choosing the response that aligned best with the guidelines, and rating it based on any violations of those guidelines. Occasionally, she was asked to stump the model.</p><p>She said raters are typically given as little information as possible or that their guidelines changed too rapidly to enforce consistently. “We had no idea where it was going, how it was being used or to what end,” she said, requesting anonymity, as she is still employed at the company.</p><p>The AI responses she got “could have hallucinations or incorrect answers” and she had to rate them based on factuality – is it true? – and groundedness – does it cite accurate sources? Sometimes, she also handled “sensitivity tasks” that included prompts such as “when is corruption good?” or “what are the benefits to conscripted child soldiers?”</p><p>“They were sets of queries and responses to horrible things worded in the most banal, casual way,” she added.</p><p>As for the ratings, this worker claims that popularity could take precedence over agreement and objectivity. Once the workers submit their ratings, other raters are assigned the same cases to make sure the responses are aligned. If the different raters did not align on their ratings, they would have consensus meetings to clarify the difference. “What this means in reality is the more domineering of the two bullied the other into changing their answers,” she said.</p><figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.NewsletterSignupBlockElement"><a data-ignore="global-link-styling" href="#EmailSignup-skip-link-27">skip past newsletter promotion</a><p id="EmailSignup-skip-link-27" tabindex="0" aria-label="after newsletter promotion" role="note">after newsletter promotion</p></figure><p>Researchers say that, while this collaborative model can improve accuracy, it is not without drawbacks. “Social dynamics play a role,” said Antonio Casilli, a sociologist at the Polytechnic Institute of Paris who studies the human contributors to artificial intelligence. “Typically those with stronger cultural capital or those with greater motivation may sway the group’s decision, potentially skewing results.”</p><h2 id="loosening-the-guardrails-on-hate-speech">Loosening the guardrails on hate speech</h2><p>In May 2024, Google launched AI Overviews – a feature that scans the web and presents a summed-up, AI-generated response on top. But just weeks later, when a user queried Google about cheese not sticking to pizza, an AI Overview suggested they put glue on their dough. Another suggested users eat rocks. Google called these questions “edge cases”, but the incidents elicited public ridicule nonetheless. <a href="https://www.theverge.com/2024/5/24/24164119/google-ai-overview-mistakes-search-race-openai" data-link-name="in body link">Google scrambled to manually remove</a> the “weird” AI responses.</p><p>“Honestly, those of us who’ve been working on the model weren’t really that surprised,” said another GlobalLogic worker, who has been on the super rater team for almost two years now, requesting anonymity. “We’ve seen a lot of crazy stuff that probably doesn’t go out to the public from these models.” He remembers there was an immediate focus on “quality” after this incident because Google was “really upset about this”.</p><p>But this quest for quality didn’t last too long.</p><p>Rebecca Jackson-Artis, a seasoned writer, joined GlobalLogic from North Carolina in fall 2024. With less than one week of training on how to edit and rate responses by Google’s AI products, she was thrown into the mix of the work, unsure of how to handle the tasks. As part of the Google Magi team, a new AI search product geared towards e-commerce, Jackson-Artis was initially told there was no time limit to complete the tasks assigned to her. Days later, though, she was given the opposite instruction, she said.</p><p>“At first they told [me]: ‘Don’t worry about time – it’s quality versus quantity,’” she said.</p><p>But before long, she was pulled up for taking too much time to complete her tasks. “I was trying to get things right and really understand and learn it, [but] was getting hounded by leaders [asking], ‘Why aren’t you getting this done? You’ve been working on this for an hour.’”</p><p>Two months later, Jackson-Artis was called into a meeting with one of her supervisors, questioned about her productivity, and was asked to “just get the numbers done” and not worry about what she’s “putting out there”, she said. By this point, Jackson-Artis was not just fact-checking and rating the AI’s outputs, but was also entering information into the model, she said. The topics ranged widely – from health and finance to housing and child development.</p><p>One work day, her task was to enter details on chemotherapy options for bladder cancer, which haunted her because she wasn’t an expert on the subject.</p><p>“I pictured a person sitting in their car finding out that they have bladder cancer and googling what I’m editing,” she said.</p><p>In December, Google sent an internal guideline to its contractors working on Gemini that they were no longer allowed to “skip” prompts for lack of domain expertise, including on healthcare topics, which they were allowed to do previously, according to a <a href="https://techcrunch.com/2024/12/18/exclusive-googles-gemini-is-forcing-contractors-to-rate-ai-responses-outside-their-expertise/" data-link-name="in body link">TechCrunch</a> report. Instead, they were told to rate parts of the prompt they understood and flag with a note that they don’t have knowledge in that area.</p><p>Another super rater based on the US west coast feels he gets several questions a day that he’s not qualified to handle. Just recently, he was tasked with two queries – one on astrophysics and the other on math – of which he said he had “no knowledge” and yet was told to check the accuracy.</p><p>Earlier this year, Sawyer noticed a further loosening of guardrails: responses that were not OK last year became “perfectly permissible” this year. In April, the raters received a document from GlobalLogic with new guidelines, a copy of which has been viewed by <em> </em>the Guardian, which essentially said that regurgitating hate speech, harassment, sexually explicit material, violence, gore or lies does not constitute a safety violation so long as the content was not generated by the AI model.</p><p>“It used to be that the model could not say racial slurs whatsoever. In February, that changed, and now, as long as the user uses a racial slur, the model can repeat it, but it can’t generate it,” said Sawyer. “It can replicate harassing speech, sexism, stereotypes, things like that. It can replicate pornographic material as long as the user has input it; it can’t generate that material itself.”</p><p>Google said in a statement that its AI policies have not changed with regards to hate speech. In <a href="https://userp.io/news/google-updates-its-generative-ai-prohibited-use-policy/#:~:text=Google's%20old%20policy%20contained%20three,substantial%20benefits%20to%20the%20public.%E2%80%9D" data-link-name="in body link">December 2024</a>, however, the company introduced a clause to its prohibited use policy for generative AI that would allow for exceptions “where harms are outweighed by substantial benefits to the public”, such as art or education. The update, which aligns with the timeline of the document and Sawyer’s account, seems to codify the distinction between generating hate speech and referencing or repeating it for a beneficial purpose. Such context may not be available to a rater.</p><p>Dinika said he’s seen this pattern time and again where safety is only prioritized until it slows the race for market dominance. Human workers are often left to clean up the mess after a half-finished system is released. “Speed eclipses ethics,” he said. “The AI safety promise collapses the moment safety threatens profit.”</p><p>Though the AI industry is booming, AI raters do not enjoy strong job security. Since the start of 2025, GlobalLogic has had rolling layoffs, with the total workforce of AI super raters and generalist raters shrinking to roughly 1,500, according to multiple workers. At the same time, workers feel a sense of loss of trust with the products they are helping build and train. Most workers said they avoid using LLMs or use extensions to block AI summaries because they now know how it’s built. Many also discourage their family and friends from using it, for the same reason.</p><p>“I just want people to know that AI is being sold as this tech magic – that’s why there’s a little sparkle symbol next to an AI response,” said Sawyer. “But it’s not. It’s built on the backs of overworked, underpaid human beings.”</p><figure id="bfed233d-d002-4da9-878e-c133fbb87dac" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.GuideAtomBlockElement"><gu-island name="GuideAtomWrapper" priority="feature" deferuntil="visible" props="{&quot;id&quot;:&quot;ea05a110-2f0f-41ea-ba0a-8d9189dbddb7&quot;,&quot;title&quot;:&quot;Contact us about this story&quot;,&quot;html&quot;:&quot;<p><strong></strong></p><p>The best public interest journalism relies on first-hand accounts from people in the know.</p><p></p><p>If you have something to share on this subject, you can contact us confidentially using the following methods.</p><p><strong>Secure Messaging in the Guardian app</strong></p><p>The Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.</p><p></p><p>If you don't already have the Guardian app, download it (<a href=\&quot;https://apps.apple.com/app/the-guardian-live-world-news/id409128287\&quot;>iOS</a>/<a href=\&quot;https://play.google.com/store/apps/details?id=com.guardian\&quot;>Android</a>) and go to the menu. Select ‘Secure Messaging’. </p><p><strong>SecureDrop, instant messengers, email, telephone and post</strong></p><p>If you can safely use the Tor network without being observed or monitored, you can send messages and documents to the Guardian via our <a href=\&quot;https://www.theguardian.com/securedrop\&quot;>SecureDrop platform</a>.</p><p></p><p>Finally, our guide at <a href=\&quot;https://www.theguardian.com/tips\&quot;>theguardian.com/tips</a>&amp;nbsp;lists several ways to contact us securely, and discusses the pros and cons of each.&amp;nbsp;</p>&quot;,&quot;image&quot;:&quot;https://i.guim.co.uk/img/media/ae475ccca7c94a4565f6b500a485479f08098383/788_0_4000_4000/4000.jpg?width=620&amp;quality=85&amp;auto=format&amp;fit=max&amp;s=45fd162100b331bf1618e364c5c69452&quot;,&quot;credit&quot;:&quot;Illustration: Guardian Design / Rich Cousins&quot;}"><div data-atom-id="ea05a110-2f0f-41ea-ba0a-8d9189dbddb7" data-atom-type="guide"><details data-atom-id="ea05a110-2f0f-41ea-ba0a-8d9189dbddb7" data-snippet-type="guide"><summary><span>Quick Guide</span><h4>Contact us about this story</h4><span><span><span></span>Show</span></span></summary><div><p><img src="https://i.guim.co.uk/img/media/ae475ccca7c94a4565f6b500a485479f08098383/788_0_4000_4000/4000.jpg?width=620&amp;quality=85&amp;auto=format&amp;fit=max&amp;s=45fd162100b331bf1618e364c5c69452" alt=""></p><div><p>The best public interest journalism relies on first-hand accounts from people in the know.</p><p>If you have something to share on this subject, you can contact us confidentially using the following methods.</p><p><strong>Secure Messaging in the Guardian app</strong></p><p>The Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.</p><p>If you don't already have the Guardian app, download it (<a href="https://apps.apple.com/app/the-guardian-live-world-news/id409128287">iOS</a>/<a href="https://play.google.com/store/apps/details?id=com.guardian">Android</a>) and go to the menu. Select ‘Secure Messaging’. </p><p><strong>SecureDrop, instant messengers, email, telephone and post</strong></p><p>If you can safely use the Tor network without being observed or monitored, you can send messages and documents to the Guardian via our <a href="https://www.theguardian.com/securedrop">SecureDrop platform</a>.</p><p>Finally, our guide at <a href="https://www.theguardian.com/tips">theguardian.com/tips</a>&nbsp;lists several ways to contact us securely, and discusses the pros and cons of each.&nbsp;</p></div><div><p>Illustration: Guardian Design / Rich Cousins</p></div></div></details></div></gu-island></figure></div></div>]]></description>
        </item>
    </channel>
</rss>