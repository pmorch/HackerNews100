<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 09 Aug 2025 12:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[What the windsurf sale means for the AI coding ecosystem (132 pts)]]></title>
            <link>https://ethanding.substack.com/p/windsurf-gets-margin-called</link>
            <guid>44843801</guid>
            <pubDate>Sat, 09 Aug 2025 03:24:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ethanding.substack.com/p/windsurf-gets-margin-called">https://ethanding.substack.com/p/windsurf-gets-margin-called</a>, See on <a href="https://news.ycombinator.com/item?id=44843801">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!VsZo!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!VsZo!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png 424w, https://substackcdn.com/image/fetch/$s_!VsZo!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png 848w, https://substackcdn.com/image/fetch/$s_!VsZo!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png 1272w, https://substackcdn.com/image/fetch/$s_!VsZo!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!VsZo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png" width="750" height="500" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:500,&quot;width&quot;:750,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:563685,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://ethanding.substack.com/i/168562735?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!VsZo!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png 424w, https://substackcdn.com/image/fetch/$s_!VsZo!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png 848w, https://substackcdn.com/image/fetch/$s_!VsZo!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png 1272w, https://substackcdn.com/image/fetch/$s_!VsZo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>imagine you relaunch your company twice, and manage to become one of the fastest-growing saas companies in history — literally record-breaking. go from zero to $82m arr in eight months. get enterprise customers like nvidia and palantir. go on every single vc podcast to tell people about it.</p><p>then give it all away for almost free. in 72 hours. over a weekend.</p><p>that's exactly what the windsurf founders did last week. everyone's so busy talking about the talent or the equity of the team that was left behind, they’re not asking “why did you sell one of the fastest growing products in history for nothing?”</p><p><span>after the acquisition, windsurf was left with about $100M on the balance sheet — cognition </span><a href="https://x.com/deedydas/status/1945684159411912742" rel="">acquired the company for $250M</a><span> — this means the enterprise value of the company [ignoring the cash] was $150M…</span></p><p>you have a business that went from 0 - $82M ARR in 8 months that no one wanted to touch, and the best offer was for a less than 2x multiple?</p><p>what the fuck was so broken that the founders would rather leave it behind for almost nothing, and</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!bSh0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!bSh0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png 424w, https://substackcdn.com/image/fetch/$s_!bSh0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png 848w, https://substackcdn.com/image/fetch/$s_!bSh0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png 1272w, https://substackcdn.com/image/fetch/$s_!bSh0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!bSh0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png" width="636" height="938" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/be930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:938,&quot;width&quot;:636,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:193754,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://ethanding.substack.com/i/168562735?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!bSh0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png 424w, https://substackcdn.com/image/fetch/$s_!bSh0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png 848w, https://substackcdn.com/image/fetch/$s_!bSh0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png 1272w, https://substackcdn.com/image/fetch/$s_!bSh0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>let me walk you through a totally normal corporate divorce since [checks notes] last week:</p><p><strong>thursday, july 11</strong><span>: openai's $3b acquisition of windsurf falls apart. after months of negotiations, they walk away.</span></p><p><strong>also thursday, july 11</strong><span>: google announces they're paying $2.4b to hire windsurf's ceo and 41 researchers for deepmind. not to acquire windsurf. just the humans. the same day openai walks. what a coincidence!</span></p><p><strong>friday afternoon</strong><span>: windsurf's founders—or what's left of them—make their first call to cognition. not "let's explore options." not "let's take our time." just: "you want a company?"</span></p><p><strong>monday morning</strong><span>: deal signed. cognition gets the entire business—$82m arr, 200+ employees, all the ip—for basically the loose change in reed hastings' couch.</span></p><p>so let's recap: google swoops in the exact moment openai bails, cherrypicks the leadership team for $2.4b, and leaves behind a headless $82m arr company like a half-eaten sandwich.</p><p>at $57 million per engineer, that's either the greatest talent arbitrage in history or the most expensive acquihire cope of all time.</p><p>the punchline? google's deal explicitly excluded the actual business. they looked at $82m of arr and said "nah, we're good."</p><p>when buyers are paying billions for your employees but won't touch your revenue with a ten-foot pole, you're not running a business—you're running an extremely expensive temp agency.</p><p>windsurf's ceo once let slip that their $10/month plan had "not much margin." this is like saying the titanic had "not much buoyancy."</p><p>the whisper network on twitter has been screaming about this for months:</p><ul><li><p><strong><a href="https://x.com/jsnnsa/status/1941306461402829189" rel="">@jsnnsa claims -300% negative margins</a></strong><span>: "revenue? sure but their revenue is what will kill them. you pay them 1$ they pay ant 3$"</span></p></li><li><p><strong><a href="https://x.com/_opencv_/status/1944908287516008707" rel="">@</a></strong><em><strong><a href="https://x.com/_opencv_/status/1944908287516008707" rel="">opencv</a></strong></em><strong><a href="https://x.com/_opencv_/status/1944908287516008707" rel=""> goes even harder with -500% margins</a></strong><span>: "Cursor is dead in 12 months. It's worse than anyone knows. Money is being incinerated"</span></p></li><li><p><strong><a href="https://x.com/tenobrus/status/1904422446389706905" rel="">@tenobrus spells it out</a></strong><span>: "both cursor and windsurf are absolutely bleeding VC money on every call btw"</span></p></li></ul><p><span>when </span><a href="https://x.com/andersonbcdefg/status/1904427149463105620" rel="">@andersonbcdefg</a><span> points out that claude code charges $5/day ($150/month) as a "good proxy for how much those 'should' cost," you start to see the problem. cursor charges $20/month. windsurf charges $15/month. they're selling dollar bills for quarters.</span></p><p>based on the forensics:</p><ul><li><p>cursor: 50 employees, $42m/month revenue, but $53m/month implied burn</p></li><li><p>revenue per user: $20/month</p></li><li><p>api costs for power users: $80-200/month</p></li><li><p>margin on heavy users: -300% to -500%</p></li><li><p>every new customer makes the problem worse</p></li></ul><p>here's why this is a death spiral:</p><ul><li><p>you can't raise prices (claude code is your competition)</p></li><li><p>you can't cut costs (api bills are fixed)</p></li><li><p>you can't stop growing (that's your only story)</p></li><li><p>you can't pivot (you've raised too much)</p></li><li><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Au7c!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Au7c!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png 424w, https://substackcdn.com/image/fetch/$s_!Au7c!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png 848w, https://substackcdn.com/image/fetch/$s_!Au7c!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png 1272w, https://substackcdn.com/image/fetch/$s_!Au7c!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Au7c!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png" width="500" height="750" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:750,&quot;width&quot;:500,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:465826,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://ethanding.substack.com/i/168562735?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Au7c!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png 424w, https://substackcdn.com/image/fetch/$s_!Au7c!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png 848w, https://substackcdn.com/image/fetch/$s_!Au7c!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png 1272w, https://substackcdn.com/image/fetch/$s_!Au7c!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div></li></ul><p>at that point - you need to either find another way to bring down the costs or… look for an exit?</p><p>i noted something in my last piece:</p><blockquote><p><span>it's a three-variable equation: </span><strong>burn rate vs. tech timeline vs. brand toxicity</strong><span>. spend too fast, you die before the tech arrives. get too toxic, you're radioactive when it matters. but nail the balance? you're the default choice when the future arrives.</span></p><div data-component-name="DigestPostEmbed"><a href="https://ethanding.substack.com/p/levered-beta-is-all-you-need" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!IS5S!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9aec1fc-6628-4b82-90d5-a85dc469724d_790x316.png"><img src="https://substackcdn.com/image/fetch/$s_!IS5S!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9aec1fc-6628-4b82-90d5-a85dc469724d_790x316.png" sizes="100vw" alt="levered beta is all you need " width="140" height="140"></picture></div></a></div></blockquote><p>windsurf and cursor were playing the same game — levering up huge by using VC money to subsidize anthropic’s models (rumor says Cursor is 20% of Anthropic’s growth). their tech timeline wasn’t frontier performance, but their own ability to develop cost efficient models before they run out of money, or lose all their customers to claude code</p><ol><li><p>use vc money to subsidize api costs</p></li><li><p>collect massive training data from developers</p></li><li><p>train your own models before money runs out</p></li><li><p>flip from -500% margins to positive overnight</p></li></ol><p><span>that's why cursor tried to poach anthropic researchers (</span><a href="https://x.com/ArfurRock/status/1945212904610922813" rel="">and the researchers staying at anthropic might be a sign they can’t do it</a><span>). that's why windsurf rushed out SWE-1. they weren't building coding tools—they were building datasets with a fancy ui.</span></p><p><span>as </span><a href="https://x.com/jsnnsa/status/1941306461402829189" rel="">@jsnnsa noted</a><span>: "if their plan is to compete with anthropic then they'll be bankrupt by next summer. they have no research talent, little capital, and a money-hungry dumpster fire burning a hole in their pocket"</span></p><p>cursor might do totally fine despite losing the anthropic researchers. they've got thrive's infinite money printer, they raised early and have the lead, maybe they clutch out a kimi-type model. my prediction is that anthropic acquires them — they might already be in talks, but either way with the surge in popularity of claude code, cursor might be feeling like bear sterns in summer of 2007</p><p>windsurf was behind and out of time, bear sterns in 2008. they needed either:</p><ul><li><p>api costs to drop 10x (not happening)</p></li><li><p>their models to beat anthropic (delusional)</p></li><li><p>someone else to hold the bag (ding ding ding)</p></li></ul><p>when you're burning millions per month and your supplier launches a competing product at 1/7th your price point, you don't have a margin problem—you have a margin call.</p><p><span>windsurf's fire sale proves what we've been avoiding: the coding part has no value capture — and claude code is coming. they're racing against irrelevance. </span><strong>and they're not alone</strong></p><p>bolt and replit aren't bleeding cash like cursor/windsurf—they're not subsidizing every api call. but they're sitting on a different time bomb:</p><ul><li><p>mediocre margins at best</p></li><li><p>users build an app then aren’t reliably going to do more stuff</p></li><li><p>claude code is coming for their lunch</p></li><li><p>their entire value prop evaporates the second anthropic decides to add a "deploy" button</p></li></ul><p>but here's the twist: at least we're not low-code tools, bc since i last wrote, the low-code apocalypse arrived:</p><ul><li><p><strong>figma</strong><span> launched text-to-ui</span></p></li><li><p><strong>retool</strong><span> released ai app builder</span></p></li><li><p><strong>airtable</strong><span> dropped their own "describe what you want" feature</span></p></li><li><p><strong>notion</strong><span> is pushing ai-powered app creation</span></p></li></ul><p>low-code ui builders are out. whatever the fuck this text-to-app paradigm is, it's in. squarespace and webflow announcements incoming in 3... 2... 1...</p><p>but these are if the coding layer is worthless, what isn't?</p><ul><li><p><strong>hosting infrastructure</strong><span>: netlify doesn't write code, but bolt drives massive traffic to it. when everyone's building apps with ai, someone still needs to host them. boring, profitable, sustainable. replit rolls this in house, notion has a light version of this.</span></p></li><li><p><strong>databases</strong><span>: supabase is laughing. every ai-generated app needs data storage. they don't care if you wrote the code or claude did. this is why turning their tables into databases, retool released hosted databases, and airtable is entering the space at all</span></p></li><li><p><strong>workflow automaton:</strong><span> running workloads on cron jobs with ec2 instances is very much still valuable — you see retool, zapier, and airtable also enter from that perspective</span></p></li><li><p><strong>seo optimization</strong><span>: webflow still owns this. lovable and bolt roadmaps are desperately trying to add seo features because that's where actual value lives, and notion has made some forrays</span></p></li><li><p><strong>basically all the services that low code platforms had to bulid out in the past</strong><span>, like auth / security / rbac / latency</span></p></li></ul><p>the infrastructure that manages code is still worth something — and at this point i’d say more of replit’s value is in being able to serve the code you make in it the way netlify does, than in bolt — bolt is more of a distribution engine for netlify where replit is bundling the whole thing together. if bolt can't figure out some defensible angle (maybe they carve out seo as a niche?), i might be writing them to zero.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!lu7T!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!lu7T!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png 424w, https://substackcdn.com/image/fetch/$s_!lu7T!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png 848w, https://substackcdn.com/image/fetch/$s_!lu7T!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png 1272w, https://substackcdn.com/image/fetch/$s_!lu7T!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!lu7T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png" width="419" height="405.8495670995671" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:895,&quot;width&quot;:924,&quot;resizeWidth&quot;:419,&quot;bytes&quot;:240683,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://ethanding.substack.com/i/168562735?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!lu7T!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png 424w, https://substackcdn.com/image/fetch/$s_!lu7T!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png 848w, https://substackcdn.com/image/fetch/$s_!lu7T!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png 1272w, https://substackcdn.com/image/fetch/$s_!lu7T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>let's be clear: $2.4 billion isn't a fire sale. everyone kinda made the money they wanted. just not for the reason VC investments typically make money</p><p>google didn't buy a company. they bought 42 researchers who'd spent $200+ million of vc money learning how to train coding models. at $57 million per head, that's the most expensive education program in history.</p><p>here's what actually happened:</p><ul><li><p>vcs pump $200m into windsurf</p></li><li><p>windsurf uses that money to buy gpus and training data</p></li><li><p>42 engineers learn incredibly valuable skills</p></li><li><p>google pays $2.4b for those skills</p></li><li><p>the actual business (82m arr) gets sold for pocket change</p></li></ul><p>the company—its revenue, customers, product—was worth maybe 1.8x revenue. that's a rounding error on a series a. but the expertise? that's worth billions.</p><p>this is the insane state of ai talent wars. windsurf accidentally discovered the ultimate arbitrage: raise vc money to train yourself on expensive gpus, then sell your newly-acquired expertise to the highest bidder. it’d be like if uber and lyft both collapsed, but waymo was willing to acquihire their teams for $20B, just for the expertise they developed on rolling out ridesharing operations</p><p>imagine pitching this to sequoia: "give us hundreds of millions, we'll use it to teach ourselves such valuable skills that google will acquihire us for $50m each." they'd throw you out. but that's exactly what happened.</p><p>it's a great outcome for the founders, and investors, employees. genuinely, congrats to them. but treating this like a successful business exit is missing the point entirely.</p><p>windsurf wasn't a company. it was an accidentally subsidized training program that discovered the most valuable output wasn't code — it was coders who knew how to build coding models.</p><p>the vcs funded a $200m scholarship program and google paid the tuition refund.</p><p>but here's the thing: this escape hatch isn't available to everyone. windsurf got bailed out because they accidentally became an ai research lab, and cursor is probably going to be fine as well, for the same reason.</p><p>the windsurf "acquisition" set a precedent, but it's a dangerous one. everyone's looking at the $2.4b number and missing the real story: a business doing $82m arr couldn't find a buyer at any price.</p><p>that's not a success story. that's a warning.</p><p>the margin calls are coming. not everyone gets to answer them by selling their homework.</p><p>thanks for Nikunj Kothari and Benn Stancil for reviewing</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dial-up Internet to be discontinued (162 pts)]]></title>
            <link>https://help.aol.com/articles/dial-up-internet-to-be-discontinued</link>
            <guid>44843369</guid>
            <pubDate>Sat, 09 Aug 2025 01:37:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://help.aol.com/articles/dial-up-internet-to-be-discontinued">https://help.aol.com/articles/dial-up-internet-to-be-discontinued</a>, See on <a href="https://news.ycombinator.com/item?id=44843369">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article"><p>AOL routinely evaluates its products and services and has decided to discontinue Dial-up Internet. This service will no longer be available in AOL plans. As a result, on September 30, 2025 this service and the associated software, the AOL Dialer software and AOL Shield browser, which are optimized for older operating systems and dial-up internet connections, will be discontinued.</p>
<p>This change will not affect any other benefits in your AOL plan, which you can access any time on your <a href="https://mybenefits.aol.com/?ncid=mbr_advadolnk00000009" target="_new" title="Go to you Benefits page.">AOL plan dashboard</a>. To manage or cancel your account, <a href="https://myaccount.aol.com/" target="_new" title="Go to your account settings page.">visit MyAccount</a>.</p>
<p>For more information or if you have questions about your account, call:</p>
<ul><li><strong>U.S.</strong> - 1-888-265-5555</li><li><strong>Canada</strong> - 1-888-265-4357</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Our European search index goes live (103 pts)]]></title>
            <link>https://blog.ecosia.org/launching-our-european-search-index/</link>
            <guid>44841741</guid>
            <pubDate>Fri, 08 Aug 2025 21:12:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.ecosia.org/launching-our-european-search-index/">https://blog.ecosia.org/launching-our-european-search-index/</a>, See on <a href="https://news.ycombinator.com/item?id=44841741">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>We’ve started delivering search results from our new European-based search index to Ecosia users! This will help us build the kind of ethical and fair internet we believe in.&nbsp;</p><p>Last year <a href="https://blog.ecosia.org/eusp/"><u>we launched European Search Perspective (EUSP)</u></a>, a joint venture with Qwant. The launch marked a big step forward in our journey towards tech independence and digital sovereignty for Europe.</p><p>Now, we’ve taken the next step: our users in France are receiving a portion of their search results directly from EUSP’s own index. We’re aiming to serve 50% of French search queries by the end of the year, and will soon start rolling out to other countries.</p><h3 id="what-does-having-an-independent-search-index-mean"><strong>What does having an independent search index mean?&nbsp;</strong></h3><p>A search index is the database of information that search engines pull their results from. Until now, only a handful of companies have built their own – which means most smaller search engines have to depend on them to provide search results.</p><p>That’s why EUSP has developed <a href="https://staan.ai/"><u>Staan</u></a> (Search Trusted API Access Network), a search index aimed to support a sovereign, privacy-first search infrastructure for Europe. It’s built for alternative search engines and AI companies that need fast, reliable access to the latest web data, while safeguarding user privacy and data security.</p><p>By using Staan as one of our sources, we can start to build greater digital independence and transparency.</p><h3 id="why-is-this-independence-important"><strong>Why is this independence important?&nbsp;</strong></h3><p>Having our own search infrastructure is a critical step towards plurality; a healthy and diverse search market reflecting multiple perspectives, and building Europe’s own digital tools.&nbsp;</p><p>Much of Europe’s search, cloud, and AI layers are built on American Big Tech stacks, which puts whole sectors at the mercy of political or commercial agendas. To put it bluntly: if Big Tech decided to pull the plug, Europe would be in trouble.</p><p>Creating a fully independent search index means we have more control. We can better serve our users, develop ethical AI, and double down on our mission to build tech that benefits people and the planet.</p><h3 id="an-open-foundation-for-competition-privacy-and-innovation"><strong>An open foundation for competition, privacy and innovation</strong></h3><p>Unlike Ecosia’s steward-owned model, EUSP is structured to allow outside investment –&nbsp; enabling long-term scaling of its infrastructure. EUSP’s search index is also available to other tech companies, offering a foundation for competition, data privacy, and innovation in areas like generative AI.&nbsp;</p><h3 id="what-does-all-this-mean-for-you"><strong>What does all this mean for you?</strong>&nbsp;</h3><p>At first, you probably won’t notice much change when using Ecosia – this update is behind the scenes. But when we look at the bigger picture, it’s a meaningful step that helps you keep growing your climate impact.&nbsp;</p><p>How? Because it strengthens Europe’s long-term competitiveness, democratic control, and stability. If we have more autonomy over our own tools, we can focus on shaping the greener, fairer tech future we want, and continue expanding our mission to tackle the climate crisis, together.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: How can ChatGPT serve 700M users when I can't run one GPT-4 locally? (452 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44840728</link>
            <guid>44840728</guid>
            <pubDate>Fri, 08 Aug 2025 19:27:28 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44840728">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="bigbox"><td><table><tbody><tr id="44840728"><td><span></span></td><td><center><a id="up_44840728" href="https://news.ycombinator.com/vote?id=44840728&amp;how=up&amp;goto=item%3Fid%3D44840728"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=44840728">Ask HN: How can ChatGPT serve 700M users when I can't run one GPT-4 locally?</a></span></td></tr><tr><td colspan="2"></td><td><span><span id="score_44840728">116 points</span> by <a href="https://news.ycombinator.com/user?id=superasn">superasn</a> <span title="2025-08-08T19:27:28 1754681248"><a href="https://news.ycombinator.com/item?id=44840728">2 hours ago</a></span> <span id="unv_44840728"></span> | <a href="https://news.ycombinator.com/hide?id=44840728&amp;goto=item%3Fid%3D44840728">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20How%20can%20ChatGPT%20serve%20700M%20users%20when%20I%20can%27t%20run%20one%20GPT-4%20locally%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=44840728&amp;auth=df93e80f283c5ff48819997a35cd155bd4cf7c12">favorite</a> | <a href="https://news.ycombinator.com/item?id=44840728">62&nbsp;comments</a></span></td></tr><tr><td colspan="2"></td><td><div><p>Sam said yesterday that chatgpt handles ~700M weekly users. Meanwhile, I can't even run a single GPT-4-class model locally without insane VRAM or painfully slow speeds.</p><p>Sure, they have huge GPU clusters, but there must be more going on - model optimizations, sharding, custom hardware, clever load balancing, etc.</p><p>What engineering tricks make this possible at such massive scale while keeping latency low?</p><p>Curious to hear insights from people who've built large-scale ML systems.</p></div></td></tr><tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr></tbody></table><br>
</td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Build durable workflows with Postgres (138 pts)]]></title>
            <link>https://www.dbos.dev/blog/why-postgres-durable-execution</link>
            <guid>44840693</guid>
            <pubDate>Fri, 08 Aug 2025 19:24:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dbos.dev/blog/why-postgres-durable-execution">https://www.dbos.dev/blog/why-postgres-durable-execution</a>, See on <a href="https://news.ycombinator.com/item?id=44840693">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>When we started building a durable workflows library, the most critical architectural decision we faced was what data store to use for workflow metadata. The core durable workflow operations are simple–checkpointing workflow state and recovering an interrupted workflow from its latest checkpoint. Almost any data store can handle these operations, but choosing the right one is critical to ensure workflows are scalable and performant.</p><p>In this blog post, we’ll dive deep into why we chose to build on Postgres. While there are good nontechnical reasons for the decision (Postgres is popular and open-source with a vibrant community and over 40 years of battle-testing), we’ll focus on the technical reasons–key Postgres features that make it easier to develop a robust and performant workflows library. In particular, we’ll look at:</p><ol role="list"><li>How Postgres concurrency control (particularly its support for locking clauses) enable scalable distributed queues.</li><li>How the relational data model (plus careful use of secondary indexes) enables performant observability tooling over workflow metadata.</li><li>How Postgres transactions enable exactly-once execution guarantees for steps performing database operations.</li></ol><h2>Building Scalable Queues</h2><p>It’s often useful to enqueue durable workflows for later execution. However, using a database table as a queue is tricky because of the risk of contention. To see why that’s a problem, let’s look at how database-backed queues work.&nbsp;</p><p>In a database-backed workflow queue, clients enqueue workflows by adding them to a queues table, and workers dequeue and process the oldest enqueued workflows (assuming a FIFO queue). Naively, each worker runs a query like this to find the N oldest enqueued workflows, then dequeues them:</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894e8dd31d1c21c43fb9ad1_AD_4nXfg9qKIxkxY5Zpo2Pw14wum0vuFrq6lzvMCf5NrauJaXHJa5EeaL-sfyD7tYE3OcV8oASA4utitnDGTaNVcTGegQswZQZpOLHhMnsg5D6cTq9kgIDSKsMYsbyd9z2qN19KHpbIr-Q.png" loading="lazy" alt="SQL query to retrieve tasks from durable queues"></p></figure><p>The problem is that if you have many workers concurrently pulling new tasks from the queue, they all try to dequeue the same workflows. However, each workflow can only be dequeued by a single worker, so most workers will fail to find new work and have to try again. If there are enough workers, this contention creates a bottleneck in the system, limiting how rapidly tasks can be dequeued.</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894f405ac7da4fd1c11f336_Storing-workflow-state-in-Postgres.png" loading="lazy" alt=""></p></figure><p>Fortunately, Postgres provides a solution: locking clauses. Here's an example of a query using them:</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894e8dd8c41c852c7e0f674_AD_4nXf0B0MCzqgtonGjb9BPutnuYHkD9zI0tmlhrzb2zRj9PowCICOPJI5E84haepmjmUVWaANmwaoSWCXfaa9Pb_RVe-kQ-95z2yqs5Z9Qyz3gpqG06_kqTzxl5V5ykg3X1xuFgyQVtQ.png" loading="lazy" alt="Postgres SELECT FOR UPDATE SKIP LOCKED query example"></p></figure><p>Selecting rows in this way does two things. First, it locks the rows so that other workers cannot also select them. Second, it skips rows that are already locked, selecting not the N oldest enqueued workflows, but the N oldest enqueued workflows <strong>that are not already locked by another worker</strong>. That way, many workers can concurrently pull new workflows <strong>without contention</strong>. One worker selects the oldest N workflows and locks them, the second worker selects the next oldest N workflows and locks those, and so on.</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894f3e5aa45514af18ed1f7_Postgres-durable-execution-database.png" loading="lazy" alt=""></p></figure><p>Thus, by greatly reducing contention, Postgres enables a durable workflow system to process tens of thousands of workflows per second across thousands of workers.</p><h2>Making Workflows Observable</h2><p>One benefit of durable workflows is that they provide built-in observability. If every step in every workflow is checkpointed to a durable store, you can scan those checkpoints to monitor workflows in real time and to visualize workflow execution. For example, a workflow dashboard can show you every workflow that ran in the last hour, or every workflow that errored in the past month:</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894e8dd8c41c852c7e0f67d_AD_4nXcEfN3ZxPtX1f1DoZrSXUR5yy7Q-Hjh_37L1oS8JNtIiNX6Syk2V4YPXZ-Nvero9EigvvqOmuUfqPkCM5YM1caCJifOHGjP6InqJkYu8pWbQ5dF9uS8ynZznzlvJhFwoXcN0g0JOQ.png" loading="lazy" alt="Observing durable workflow history"></p></figure><p>To implement observability, you need to be able to run performant queries over workflow metadata. Postgres excels for this because virtually any workflow observability query can be easily expressed in SQL. For example, here’s a query to find all workflows that errored in the last month.</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894e8dd8c41c852c7e0f671_AD_4nXfFo1KTBYl86FzIfe6IIkKUDFm9Wxex-Z4hl1bD1z2kQHGKzNQfzwnpPYFmZ6rKL0suFvafIb0fW-UadctI9yQryoJcHUEpjcCX4azVuQxV0mS3mTHKHWw_5tNmQ9XirIjx5py5ag.png" loading="lazy" alt="Example SQL query to retrieve durable workflow status"></p></figure><p>These queries might seem obvious, but it's impossible to overstate how powerful this is. It's only possible because Postgres's relational model allows you to express complex filtering and analytical operations declaratively in SQL, leveraging decades of query optimization research. Many popular systems with simpler data models, such as key-value stores, have no such support.</p><p>Postgres also provides the tools to make these observability queries performant at scale (&gt;10M workflows): secondary indexes. Secondary indexes allow Postgres to rapidly find all workflows that have a particular attribute or range of attributes. They’re expensive to construct and maintain, so they have to be used carefully: you can’t index every workflow attribute without adding prohibitive overhead.</p><p>To strike a balance between query performance and runtime overhead, we added secondary indexes to a small number of fields that are the most selective in frequently run queries. Because most workflow observability queries are time-based (typically a dashboard displaying all workflows in a time range), the most important index is on the created_at column. We additionally added indexes to two other attributes that are frequently searched without specifying a time range: executor_id (users often want to find all workflows that ran on a given server) and status (users frequently want to find all workflows that ever errored).</p><p>‍</p><h2>Implementing Exactly-Once Semantics</h2><p>Typically, durable workflows guarantee that steps execute <strong>at least once</strong>. The way it works is that after each step completes, its outcome is checkpointed in a data store:</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894e8dd8c41c852c7e0f664_AD_4nXfKuFmryR-dSWb6SVWe0gUvn9dR-htEefBDMaN12rcS-Or0Xf15vJvXSopkDn9eRMfY6W9qzNesrHuRuY-9JAXqZN8rqPLIxhpd1IniuTT32eTxo6OEgdMPvIKTqG7jD1YJTEkEDg.jpeg" loading="lazy" alt="Durable workflow checkpointing diagram"></p></figure><p>If the program fails, upon restart the workflow looks up its latest checkpoint and resumes from its last completed step. This means that if a workflow fails while executing a step, the step may execute twice–once before the failure, then again after the workflow recovers from the failure. Because steps may be executed multiple times, they should be idempotent or otherwise resilient to re-execution.</p><p>By building durable workflows on Postgres, we can do better and guarantee that steps execute <strong>exactly once</strong>–if those steps perform database operations. To do this, we leverage Postgres transactions. The trick is to execute the entire step in a single database transaction, and to “piggyback” the step checkpoint as part of the transaction. That way, if the workflow fails while the step is executing, the entire transaction is rolled back and nothing happens. But if the workflow fails after the transaction commits, the step checkpoint is already written so the step is not re-executed.</p><p>For example, let’s say a step inserts a record into a database table</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894e8dd8c41c852c7e0f66a_AD_4nXfYy-7kyUOTTm5MKv3i0RmtsPJDd4uBn3t3rybq91mErk3aJ7R31gFu3EuOx-Zidu0SV8UJcEV7cQH0C-ohlCYd5F9GofbMjgh7xz22_nK2dlkkc4Fjv8DOw6-vzdfBn2qYWiUegg.png" loading="lazy" alt=""></p></figure><p>This step isn’t idempotent, so executing it twice would be bad–the order would be inserted into the table twice. However, because the step consists solely of a database operation, we can perform the step and checkpoint its outcome in the same transaction, like this:</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894e8dd8c41c852c7e0f6bf_AD_4nXdc2Acac_XJH2Mmoxu1umX0YJ1ibdOm1lP4HeQzGswhiRkMBiIz0LyZDFqsMEyTtjTxKcxtih3RLac43cW6INe82s0kPSwx6FpxkKvHMdw_PmB4jixFOziDtOsiP6XQ2HxolF-a6w.png" loading="lazy" alt="example of an idempotent step in a durable workflow"></p></figure><p>Thus, the step either fully completes or commits (including its checkpoint) or fails and completely rolls back–the step is guaranteed to execute <strong>exactly once</strong>.</p><h2>Learn More</h2><p>If you like hacking with Postgres, we’d love to hear from you. At DBOS, our goal is to make durable workflows as lightweight and easy to work with as possible. Check it out:</p><ul role="list"><li>Quickstart:<a href="https://docs.dbos.dev/quickstart"> https://docs.dbos.dev/quickstart</a>&nbsp;</li><li>GitHub:<a href="https://github.com/dbos-inc"> https://github.com/dbos-inc</a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Efrit: A native elisp coding agent running in Emacs (131 pts)]]></title>
            <link>https://github.com/steveyegge/efrit</link>
            <guid>44840654</guid>
            <pubDate>Fri, 08 Aug 2025 19:20:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/steveyegge/efrit">https://github.com/steveyegge/efrit</a>, See on <a href="https://news.ycombinator.com/item?id=44840654">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Efrit - AI-Powered Emacs Coding Assistant</h2><a id="user-content-efrit---ai-powered-emacs-coding-assistant" aria-label="Permalink: Efrit - AI-Powered Emacs Coding Assistant" href="#efrit---ai-powered-emacs-coding-assistant"></a></p>
<p dir="auto"><em>A sophisticated AI coding agent that leverages Emacs' native programmability through direct Elisp evaluation.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">Efrit is a conversational AI assistant that integrates seamlessly with Emacs, providing multiple interfaces for different types of tasks:</p>
<ul dir="auto">
<li><strong>efrit-chat</strong> - Multi-turn conversational interface for complex discussions and code development</li>
<li><strong>efrit-do</strong> - Natural language command execution for quick tasks</li>
<li><strong>efrit</strong> - Command interface for structured interactions</li>
<li><strong>efrit-agent-run</strong> - Advanced agent loop for multi-step automation</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key Features</h2><a id="user-content-key-features" aria-label="Permalink: Key Features" href="#key-features"></a></p>
<ul dir="auto">
<li><strong>Direct Elisp Evaluation</strong>: Leverages Emacs' native programmability without complex abstractions</li>
<li><strong>Multi-turn Conversations</strong>: Maintains context across multiple exchanges with configurable turn limits</li>
<li><strong>Tool Integration</strong>: Can execute Emacs functions, manipulate buffers, and interact with the environment</li>
<li><strong>Safety-First Design</strong>: Confirmation systems and comprehensive error handling</li>
<li><strong>Dark Theme Friendly</strong>: Adaptive colors that work with any Emacs theme</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites</h3><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li><strong>Emacs</strong>: Version 28.1 or later</li>
<li><strong>Anthropic API Key</strong>: Get yours from <a href="https://console.anthropic.com/" rel="nofollow">Anthropic Console</a></li>
<li><strong>Internet Connection</strong>: Required for Claude API access</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quick Installation</h3><a id="user-content-quick-installation" aria-label="Permalink: Quick Installation" href="#quick-installation"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Clone the repository</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/steveyegge/efrit.git
cd efrit"><pre>git clone https://github.com/steveyegge/efrit.git
<span>cd</span> efrit</pre></div>
</li>
<li>
<p dir="auto"><strong>Add to your Emacs configuration</strong> (<code>~/.emacs.d/init.el</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="(add-to-list 'load-path &quot;/path/to/efrit&quot;)
(require 'efrit)"><pre>(<span>add-to-list</span> <span>'load-path</span> <span><span>"</span>/path/to/efrit<span>"</span></span>)
(<span>require</span> <span>'efrit</span>)</pre></div>
</li>
<li>
<p dir="auto"><strong>Configure your API key</strong> in <code>~/.authinfo</code>:</p>
<div data-snippet-clipboard-copy-content="machine api.anthropic.com login personal password YOUR_API_KEY_HERE"><pre><code>machine api.anthropic.com login personal password YOUR_API_KEY_HERE
</code></pre></div>
</li>
<li>
<p dir="auto"><strong>Restart Emacs</strong> and test with <code>M-x efrit-chat</code></p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Alternative Installation Methods</h3><a id="user-content-alternative-installation-methods" aria-label="Permalink: Alternative Installation Methods" href="#alternative-installation-methods"></a></p>
<p dir="auto"><strong>Emergency Emacs Setup</strong> (for quick testing):</p>
<div dir="auto" data-snippet-clipboard-copy-content="emacs -q -l /path/to/efrit/efrit.el"><pre>emacs -q -l /path/to/efrit/efrit.el</pre></div>
<p dir="auto"><strong>Using straight.el</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="(straight-use-package
 '(efrit :type git :host github :repo &quot;steveyegge/efrit&quot;))
(require 'efrit)"><pre>(<span>straight-use-package</span>
 '(efrit <span>:type</span> git <span>:host</span> github <span>:repo</span> <span><span>"</span>steveyegge/efrit<span>"</span></span>))
(<span>require</span> <span>'efrit</span>)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Available Commands</h3><a id="user-content-available-commands" aria-label="Permalink: Available Commands" href="#available-commands"></a></p>
<ul dir="auto">
<li><strong><code>M-x efrit-chat</code></strong> - Multi-turn conversational interface</li>
<li><strong><code>M-x efrit-do</code></strong> - Natural language command execution</li>
<li><strong><code>M-x efrit</code></strong> - Command interface</li>
<li><strong><code>M-x efrit-agent-run</code></strong> - Advanced agent loop</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Key Bindings</h3><a id="user-content-key-bindings" aria-label="Permalink: Key Bindings" href="#key-bindings"></a></p>
<ul dir="auto">
<li><code>C-c C-e e</code> - efrit-chat</li>
<li><code>C-c C-e d</code> - efrit-do</li>
<li><code>C-c C-e c</code> - efrit command interface</li>
<li><code>C-c C-e a</code> - efrit-agent-run</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Visual Demonstrations</h4><a id="user-content-visual-demonstrations" aria-label="Permalink: Visual Demonstrations" href="#visual-demonstrations"></a></p>
<p dir="auto"><strong>Multi-Buffer Creation with efrit-do</strong></p>
<p dir="auto">Starting with a simple request:</p>
<div data-snippet-clipboard-copy-content="M-x efrit-do
> write an ode in one buffer, and a sonnet in another, both about Vim"><pre><code>M-x efrit-do
&gt; write an ode in one buffer, and a sonnet in another, both about Vim
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/steveyegge/efrit/blob/main/ode-sonnet-vim.jpg"><img src="https://github.com/steveyegge/efrit/raw/main/ode-sonnet-vim.jpg" alt="Initial ode and sonnet about Vim"></a></p>
<p dir="auto"><strong>Conversational Context - Making Modifications</strong></p>
<p dir="auto">efrit-do maintains context, so you can refine previous work:</p>
<div data-snippet-clipboard-copy-content="M-x efrit-do  
> Can you make them more snarky?"><pre><code>M-x efrit-do  
&gt; Can you make them more snarky?
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/steveyegge/efrit/blob/main/ode-sonnet-snarky.jpg"><img src="https://github.com/steveyegge/efrit/raw/main/ode-sonnet-snarky.jpg" alt="Snarky versions of the ode and sonnet"></a></p>
<p dir="auto">This demonstrates efrit-do's key feature: <strong>conversational continuity</strong>. It remembers what it just created and can modify, improve, or completely rewrite previous work based on your feedback.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">More Usage Examples</h4><a id="user-content-more-usage-examples" aria-label="Permalink: More Usage Examples" href="#more-usage-examples"></a></p>
<p dir="auto"><strong>Conversational Development</strong>:</p>
<div data-snippet-clipboard-copy-content="M-x efrit-chat
> Can you help me write a function to count lines in the current buffer?
> Now modify it to exclude empty lines and comments"><pre><code>M-x efrit-chat
&gt; Can you help me write a function to count lines in the current buffer?
&gt; Now modify it to exclude empty lines and comments
</code></pre></div>
<p dir="auto"><strong>Quick Commands</strong>:</p>
<div data-snippet-clipboard-copy-content="M-x efrit-do
> open the scratch buffer and insert &quot;hello world&quot;
> find all TODO comments in the current project"><pre><code>M-x efrit-do
&gt; open the scratch buffer and insert "hello world"
&gt; find all TODO comments in the current project
</code></pre></div>
<p dir="auto"><strong>Multi-step Tasks</strong>:</p>
<div data-snippet-clipboard-copy-content="M-x efrit-chat
> Create a haiku in one buffer and a limerick in another buffer"><pre><code>M-x efrit-chat
&gt; Create a haiku in one buffer and a limerick in another buffer
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Configuration</h3><a id="user-content-basic-configuration" aria-label="Permalink: Basic Configuration" href="#basic-configuration"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Model and token settings
(setq efrit-model &quot;claude-3-5-sonnet-20241022&quot;)
(setq efrit-max-tokens 8192)

;; Multi-turn conversation settings
(setq efrit-multi-turn-enabled t)
(setq efrit-multi-turn-simple-max-turns 3)

;; efrit-do buffer behavior
(setq efrit-do-show-errors-only t)  ; Only show buffer on errors

;; Debug settings (optional)
(setq efrit-debug-enabled nil)"><pre><span><span>;</span>; Model and token settings</span>
(<span>setq</span> efrit-model <span><span>"</span>claude-3-5-sonnet-20241022<span>"</span></span>)
(<span>setq</span> efrit-max-tokens <span>8192</span>)

<span><span>;</span>; Multi-turn conversation settings</span>
(<span>setq</span> efrit-multi-turn-enabled <span>t</span>)
(<span>setq</span> efrit-multi-turn-simple-max-turns <span>3</span>)

<span><span>;</span>; efrit-do buffer behavior</span>
(<span>setq</span> efrit-do-show-errors-only <span>t</span>)  <span><span>;</span> Only show buffer on errors</span>

<span><span>;</span>; Debug settings (optional)</span>
(<span>setq</span> efrit-debug-enabled <span>nil</span>)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Advanced Configuration</h3><a id="user-content-advanced-configuration" aria-label="Permalink: Advanced Configuration" href="#advanced-configuration"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Timeouts and API settings
(setq efrit-multi-turn-timeout 300)
(setq efrit-api-timeout 30)

;; Custom key bindings
(global-set-key (kbd &quot;C-c a&quot;) 'efrit-chat)
(global-set-key (kbd &quot;C-c d&quot;) 'efrit-do)"><pre><span><span>;</span>; Timeouts and API settings</span>
(<span>setq</span> efrit-multi-turn-timeout <span>300</span>)
(<span>setq</span> efrit-api-timeout <span>30</span>)

<span><span>;</span>; Custom key bindings</span>
(<span>global-set-key</span> (<span>kbd</span> <span><span>"</span>C-c a<span>"</span></span>) <span>'efrit-chat</span>)
(<span>global-set-key</span> (<span>kbd</span> <span><span>"</span>C-c d<span>"</span></span>) <span>'efrit-do</span>)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Core Philosophy</h3><a id="user-content-core-philosophy" aria-label="Permalink: Core Philosophy" href="#core-philosophy"></a></p>
<p dir="auto">Efrit follows the principle of <strong>Elisp-centricity</strong>: rather than building complex tool abstractions, it provides the AI with direct access to Emacs' powerful Elisp evaluation capabilities. This approach offers unlimited flexibility while staying within Emacs' natural paradigm.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Core Components</h3><a id="user-content-core-components" aria-label="Permalink: Core Components" href="#core-components"></a></p>
<ul dir="auto">
<li><strong>efrit.el</strong> - Main entry point and package coordination</li>
<li><strong>efrit-chat.el</strong> - Multi-turn conversational interface with Claude API</li>
<li><strong>efrit-do.el</strong> - Natural language command interface</li>
<li><strong>efrit-multi-turn.el</strong> - Multi-turn conversation state management</li>
<li><strong>efrit-tools.el</strong> - Core functionality engine with Elisp evaluation</li>
<li><strong>efrit-debug.el</strong> - Optional debugging and logging system</li>
<li><strong>efrit-agent.el</strong> - Advanced agent loop for complex automation</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Common Issues</h3><a id="user-content-common-issues" aria-label="Permalink: Common Issues" href="#common-issues"></a></p>
<p dir="auto"><strong>"Cannot open load file: efrit"</strong></p>
<ul dir="auto">
<li>Verify the path in your <code>load-path</code> is correct</li>
<li>Ensure <code>efrit.el</code> exists in that directory</li>
</ul>
<p dir="auto"><strong>"API key not found"</strong></p>
<ul dir="auto">
<li>Check <code>~/.authinfo</code> file exists and has correct format</li>
<li>Test with: <code>M-x auth-source-search RET machine api.anthropic.com RET</code></li>
</ul>
<p dir="auto"><strong>Connection timeout</strong></p>
<ul dir="auto">
<li>Check internet connection and API key validity</li>
<li>Try increasing <code>efrit-api-timeout</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Debug Mode</h3><a id="user-content-debug-mode" aria-label="Permalink: Debug Mode" href="#debug-mode"></a></p>
<p dir="auto">Enable debug logging for troubleshooting:</p>
<div dir="auto" data-snippet-clipboard-copy-content="(setq efrit-debug-enabled t)"><pre>(<span>setq</span> efrit-debug-enabled <span>t</span>)</pre></div>
<p dir="auto">Then check: <code>M-x efrit-debug-show</code></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building and Testing</h3><a id="user-content-building-and-testing" aria-label="Permalink: Building and Testing" href="#building-and-testing"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build
make compile

# Run tests
make test

# Install system-wide
make install"><pre><span><span>#</span> Build</span>
make compile

<span><span>#</span> Run tests</span>
make <span>test</span>

<span><span>#</span> Install system-wide</span>
make install</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contributing</h3><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">See <a href="https://github.com/steveyegge/efrit/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> for detailed guidelines on:</p>
<ul dir="auto">
<li>Development setup</li>
<li>Code standards and conventions</li>
<li>Testing procedures</li>
<li>Submitting changes</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Version History</h2><a id="user-content-version-history" aria-label="Permalink: Version History" href="#version-history"></a></p>
<p dir="auto"><strong>v0.2.0</strong> (2025-01-07) - Major Stability Release</p>
<ul dir="auto">
<li>✅ Fixed API integration issues and HTTP 400 errors</li>
<li>✅ Enhanced token limits (1024 → 8192 tokens)</li>
<li>✅ Improved message ordering and dark theme compatibility</li>
<li>✅ Added multi-turn conversation system with configurable limits</li>
<li>✅ Consolidated documentation and cleaned up codebase</li>
<li>✅ Production-ready with comprehensive error handling</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Licensed under the Apache License, Version 2.0. See <a href="https://github.com/steveyegge/efrit/blob/main/LICENSE">LICENSE</a> for details.</p>
<hr>
<p dir="auto"><em>Efrit: Where AI meets the power of Emacs.</em></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jim Lovell, Apollo 13 commander, has died (531 pts)]]></title>
            <link>https://www.nasa.gov/news-release/acting-nasa-administrator-reflects-on-legacy-of-astronaut-jim-lovell/</link>
            <guid>44840582</guid>
            <pubDate>Fri, 08 Aug 2025 19:12:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nasa.gov/news-release/acting-nasa-administrator-reflects-on-legacy-of-astronaut-jim-lovell/">https://www.nasa.gov/news-release/acting-nasa-administrator-reflects-on-legacy-of-astronaut-jim-lovell/</a>, See on <a href="https://news.ycombinator.com/item?id=44840582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>The following is a statement from acting NASA Administrator Sean Duffy on the passing of famed Apollo astronaut Jim Lovell. He passed away Aug. 7, in Lake Forest, Illinois. He was 97 years old.</p>
<p>“NASA sends its condolences to the family of Capt. Jim Lovell, whose life and work inspired millions of people across the decades. Jim’s character and steadfast courage helped our nation reach the Moon and turned a potential tragedy into a success from which we learned an enormous amount. We mourn his passing even as we celebrate his achievements.</p>
<p>“From a pair of pioneering Gemini missions to the successes of Apollo, Jim helped our nation forge a historic path in space that carries us forward to upcoming Artemis missions to the Moon and beyond.</p>
<p>“As the Command Module Pilot for Apollo 8, Jim and his crewmates became the first to lift off on a Saturn V rocket and orbit the Moon, proving that the lunar landing was within our reach. As commander of the Apollo 13 mission, his calm strength under pressure helped return the crew safely to Earth and demonstrated the quick thinking and innovation that informed future NASA missions.</p>
<p>“Known for his wit, this unforgettable astronaut was nicknamed Smilin’ Jim by his fellow astronauts because he was quick with a grin when he had a particularly funny comeback.</p>
<p>“Jim also served our country in the military, and the Navy has lost a proud academy graduate and test pilot. Jim Lovell embodied the bold resolve and optimism of both past and future explorers, and we will remember him always.”</p>
<p>For more information about Lovell’s NASA career, and his agency biography, visit:</p>
<p><a href="https://www.nasa.gov/former-astronaut-james-a-lovell"><strong>https://www.nasa.gov/former-astronaut-james-a-lovell</strong></a></p>
<p>-end-</p>
<p>Grace Bartlinski / Cheryl Warner<br>Headquarters, Washington<br>202-358-1600<br><a href="mailto:grace.bartlinksi@nasa.gov">grace.bartlinksi@nasa.gov</a> / <a href="mailto:cheryl.m.warner@nasa.gov">cheryl.m.warner@nasa.gov</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[M5 MacBook Pro No Longer Coming in 2025 (105 pts)]]></title>
            <link>https://www.macrumors.com/2025/07/10/no-m5-macbook-pro-2025/</link>
            <guid>44840281</guid>
            <pubDate>Fri, 08 Aug 2025 18:43:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2025/07/10/no-m5-macbook-pro-2025/">https://www.macrumors.com/2025/07/10/no-m5-macbook-pro-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=44840281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2025/07/10/no-m5-macbook-pro-2025/"><p>Apple does not plan to refresh any Macs with updated M5 chips in 2025, according to <em><a href="https://www.bloomberg.com/news/articles/2025-07-10/apple-plans-new-macbook-pro-iphone-17e-and-ipads-by-early-2026">Bloomberg</a></em>'s <a href="https://www.macrumors.com/guide/mark-gurman/">Mark Gurman</a>. Updated <a href="https://www.macrumors.com/roundup/macbook-air/">MacBook Air</a> and <a href="https://www.macrumors.com/roundup/macbook-pro/">MacBook Pro</a> models are now planned for the first half of 2026.</p>
<p><img src="https://images.macrumors.com/t/vy3QHefJFR7xsmS0U55hnHufPQ4=/400x0/article-new/2024/08/macbook-pro-blue-green.jpg?lossy" srcset="https://images.macrumors.com/t/vy3QHefJFR7xsmS0U55hnHufPQ4=/400x0/article-new/2024/08/macbook-pro-blue-green.jpg?lossy 400w,https://images.macrumors.com/t/PxiXq-dLu0UCf97Sh_kQvsYMPnQ=/800x0/article-new/2024/08/macbook-pro-blue-green.jpg?lossy 800w,https://images.macrumors.com/t/NYXXBItdSxPZv0difK-ijSNGi58=/1600x0/article-new/2024/08/macbook-pro-blue-green.jpg 1600w,https://images.macrumors.com/t/Q5Z7-K6FP-0AquOer4ECYfk0XV0=/2500x0/filters:no_upscale()/article-new/2024/08/macbook-pro-blue-green.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="macbook pro blue green" width="2500" height="1406"><br>Gurman previously said that Apple would debut the M5 ‌MacBook Pro‌ models in late 2025, but his newest report suggests that Apple is "considering" pushing them back to 2026. Apple is now said to be "internally targeting" a launch early next year.</p>
<p>The current M4, M4 Pro, and M4 Max ‌MacBook Pro‌ models were announced in October 2024 and released in November 2024, so pushing the M5 models back to 2026 would see Apple skipping a yearly refresh. It is typical for new Macs to come out in October or November after the September <a href="https://www.macrumors.com/guide/iphone/">iPhone</a> event.</p>
<p>Gurman does not give a reason why Apple is potentially "delaying" the launch of the M5 ‌MacBook Pro‌ models, but he says the timing is fluid, so there may still be a chance that we get the new Macs before the end of the year.</p>
<p>The M5 ‌MacBook Pro‌ models will have few changes beyond the M5 chip update, because Apple is planning for bigger changes in for the M6 ‌MacBook Pro‌. The next ‌MacBook Pro‌ models are expected to transition to OLED displays and new case designs. Rumors have suggested the OLED ‌MacBook Pro‌ would come in 2026, but if Apple is planning to launch the M5 ‌MacBook Pro‌ models in 2026, that might mean the OLED model will be pushed to 2027. Alternatively, Apple could debut the M5 ‌MacBook Pro‌ in early 2026 and the OLED version in late 2026, but that would be unusual.</p>
<p>Apple is also planning to release M5 ‌MacBook Air‌ models in 2026, which will replace the current M4 models. Other rumors suggest Apple is working on a MacBook that has an A18 Pro chip in it for 2026, but Gurman didn't mention it. </p>
<p>The M5 ‌MacBook Pro‌ and ‌MacBook Air‌ models could be accompanied by a new display that Apple has in the works. Apple is developing an external monitor that is expected to be a follow up to the 2022 Studio Display. It is expected to launch in early 2026.</p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2025/08/05/iphone-17-pro-launching-next-month/">iPhone 17 Pro Launching Next Month With These 12 New Features</a></h3><p>The calendar has turned to August, and that means the iPhone 17 series is just one month away. Apple has yet to officially announce an event, but it has been rumored that the devices will be announced on Tuesday, September 9.
Subscribe to the MacRumors YouTube channel for more videos. 
Below is the August 2025 edition of our iPhone 17 Pro rumor recap, for an up-to-date overview of what to...</p></div><div><h3><a href="https://www.macrumors.com/2025/08/05/airpods-pro-3-weeks-away-what-we-know/">AirPods Pro 3 Could Be Just Weeks Away – Here's What We Know</a></h3><p>Tuesday August 5, 2025 2:03 am PDT by <a href="https://www.macrumors.com/author/tim-hardwick/" rel="author">Tim Hardwick</a></p><p>Despite being over two years old, Apple's AirPods&nbsp;Pro&nbsp;2 still dominate the premium wireless‑earbud space, thanks to a potent mix of top‑tier audio, class‑leading noise cancellation, and Apple's habit of delivering major new features through software updates. Rumors suggest AirPods Pro 3 could arrive as soon as September 2025 alongside the iPhone 17 lineup, giving prospective AirPods...</p></div><div><h3><a href="https://www.macrumors.com/2025/08/05/apple-watch-ultra-3-display-size/">iOS 26 Beta Reveals Apple Watch Ultra 3 Display Size</a></h3><p>Tuesday August 5, 2025 11:21 am PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>The latest iOS 26 beta includes imagery that confirms Apple's work on a new version of the Apple Watch Ultra, which is set to come out this fall. MacRumors contributor Aaron Perris found an Apple Watch image with a resolution that does not correspond to any current Apple Watch models. 
The image suggests that the upcoming Apple Watch Ultra 3 could have a slightly larger display size, with a...</p></div><div><h3><a href="https://www.macrumors.com/2025/08/05/new-apple-tv-still-launching-this-year/">New Apple TV Still Launching This Year</a></h3><p>Apple is still on track to release a new Apple TV model later this year, according to a reliable source speaking to MacRumors.
According to a source familiar with the company's plans, Apple is highly likely to replace the current Apple TV 4K with a new model later this year. The current model will be discontinued. 
Today's Apple TV 4K came out in 2022, featuring the A15 Bionic chip,...</p></div><div><h3><a href="https://www.macrumors.com/2025/08/01/ios-26-adaptive-power-mode/">iOS 26's New Battery Life Mode is Limited to These iPhone Models</a></h3><p>iOS 26 introduces an Adaptive Power Mode on the iPhone, alongside the existing Low Power Mode.
Apple says that Adaptive Power Mode can make "small performance adjustments" when necessary to extend an iPhone's battery life, including slightly lowering the display brightness or allowing some activities to "take a little longer."
The full description of Adaptive Power Mode, from the iOS 26...</p></div><div><h3><a href="https://www.macrumors.com/2025/08/05/ios-26-beta-5-features/">Everything New in iOS 26 Beta 5</a></h3><p>Tuesday August 5, 2025 2:20 pm PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>Apple seeded the fifth developer beta of iOS 26 today, and while the number of significant changes has dropped, there are quite a few smaller tweaks. Apple is continuing to refine button placement, animations, and design in preparation for launching iOS 26 in September.
Camera
Apple added a toggle in the Camera app to allow users to toggle on Classic Mode, a setting that reverses the scroll ...</p></div><div><h3><a href="https://www.macrumors.com/2025/08/05/magsafe-charger-firmware-update-2a168/">Apple Releases Updated MagSafe Charger Firmware</a></h3><p>Tuesday August 5, 2025 12:20 pm PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>Apple today released a firmware update for the 25W MagSafe Charger that is compatible with the iPhone 12 and later and the latest AirPods. The new firmware is version 2A168, up from the 2A146 firmware that came out last year. In the Settings app, the new firmware is version 148, up from 136.
Apple introduced the 2024 MagSafe charger alongside the iPhone 16 models back in September, and it is ...</p></div><div><h3><a href="https://www.macrumors.com/2025/08/06/airpods-charging-ios-26/">Apple Upgrades AirPods Charging in iOS 26</a></h3><p>Wednesday August 6, 2025 1:10 pm PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>In the fifth beta of iOS 26, Apple appears to have subtly upgraded AirPods charging. Code in iOS 26 says that the AirPods Charging case "now more clearly indicates charging status," and that the AirPods will remind you when it's time to charge.
A screenshot shared on social media shows an AirPods splash screen with the same wording and an image of what the light on the AirPods charging case...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I want everything local – Building my offline AI workspace (834 pts)]]></title>
            <link>https://instavm.io/blog/building-my-offline-ai-workspace</link>
            <guid>44840013</guid>
            <pubDate>Fri, 08 Aug 2025 18:19:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://instavm.io/blog/building-my-offline-ai-workspace">https://instavm.io/blog/building-my-offline-ai-workspace</a>, See on <a href="https://news.ycombinator.com/item?id=44840013">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><blockquote>
<p>I want everything local — no cloud, no remote code execution.</p>
</blockquote>
<p>That’s what a friend said. That one-line requirement, albeit simple, would need multiple things to work in tandem to make it happen.</p>
<p>What does a mainstream LLM (Large Language Model) chat app like ChatGPT or Claude provide at a high level?</p>
<ul>
<li>Ability to use chat with a cloud hosted LLM,</li>
<li>Ability to run code generated by them mostly on their cloud infra, sometimes locally via shell,</li>
<li>Ability to access the internet for new content or services.</li>
</ul>
<p>With so many LLMs being open source / open weights, shouldn't it be possible to do all that locally? But just local LLM is not enough, we need a truely isolated environment to run code as well.</p>
<p>So, LLM for chat, Docker to containerize code execution, and finally a browser access of some sort for content.</p>
<hr>
<h2>🧠 The Idea</h2>
<p>We wanted a system where:</p>
<ul>
<li>LLMs run completely <strong>locally</strong></li>
<li><strong>Code executes inside a lightweight VM</strong>, not on the host machine</li>
<li>Bonus: <strong>headless browser</strong> for automation and internet access</li>
</ul>
<p><img src="https://instavm.io/blog-images/Pasted%20image%2020250730003357.png"></p>
<p>The idea was to perform tasks which require privacy to be executed completely locally, starting from planning via LLM to code execution inside a container. For instance, if you wanted to edit your photos or videos, how could you do it without giving your data to OpenAI/Google/Anthropic? Though they take security seriously (more than many), it's just a matter of one slip leading to your private data being compromised, a case in point being the early days of ChatGPT when user chats were <a href="https://www.bloomberg.com/news/articles/2023-03-21/openai-shut-down-chatgpt-to-fix-bug-exposing-user-chat-titles">accessible</a> from another's account!</p>
<hr>
<h2>The Stack We Used</h2>
<ul>
<li><strong>LLMs</strong>: <a href="https://ollama.com/">Ollama</a> for local models (also private models for now)</li>
<li><strong>Frontend UI</strong>: <a href="https://github.com/assistant-ai/assistant-ui"><code>assistant-ui</code></a></li>
<li><strong>Sandboxed VM Runtime</strong>: <a href="https://github.com/apple/container"><code>container</code></a> by Apple</li>
<li><strong>Orchestration</strong>: <a href="https://github.com/instavm/coderunner"><code>coderunner</code></a></li>
<li><strong>Browser Automation</strong>: <a href="https://playwright.dev/">Playwright</a></li>
</ul>
<blockquote>
<p>💡 We ran this entirely on Apple Silicon, using <code>container</code> for isolation.</p>
</blockquote>
<h3>🛠️  Our Attempt at a Mac App</h3>
<p>We started with zealous ambition: make it feel native. We tried using <code>a0.dev</code>, hoping it could help generate a Mac app. But it appears to be meant more for iOS app development — and getting it to work for MacOS was painful, to say the least.</p>
<p>Even with help from the "world's best" LLMs, things didn't go quite as smoothly as we had expected. They hallucinated steps, missed platform-specific quirks, and often left us worse off.</p>
<p>Then we tried wrapping a <code>NextJS</code> app inside Electron. It took us longer than we'd like to admit. As of this writing, it looks like there's just no (clean) way to do it.</p>
<p>So, we gave up on the Mac app. The local web version of <code>assistant-ui</code> was good enough — simple, configurable, and didn't fight back.</p>
<p><img src="https://instavm.io/blog-images/Pasted%20image%2020250729142902.png"></p>
<h3>Assistant UI</h3>
<p>We thought <code>Assistant-UI</code> provided multiple LLM support out-of-the-box, as their landing page shows a drop-down of models. But, no.
So, we had to look for examples on how to go about it, and <code>ai-sdk</code> appeared to be the popular choice.
Finally we had a dropdown for model selection. We decided not to restrict the set to just local models, as smaller local models are not quite there just yet. Users can get familiar with the tool and its capabilities, and later as small local models become better, they can just switch to being completely local.
<img src="https://instavm.io/blog-images/Pasted%20image%2020250729225206.png"></p>
<h3>Tool-calling</h3>
<p>Our use-case also required us to have models that support tool-calling. While some models do, Ollama has not implemented the tool support for them. For instance:</p>
<pre><code>responseBody: '{"error":"registry.ollama.ai/library/deepseek-r1:8b does not support tools"}',
</code></pre>
<p>And to add to the confusion, Ollama has decided to put this model under tool calling category on their site. Understandably, with the fast-moving AI landscape, it can be difficult for community driven projects to keep up.</p>
<p>At the moment, essential information like whether a model has tool-support or not, pricing per token, for various models are so fickle. A model's official page mentions tool-support but then tools like Ollama take a while to implement them. Anyway, we shouldn't complain - it's open source, we could've contributed.</p>
<h3>Containerized execution</h3>
<p>After the UI was MVP-level sorted, we moved on to the isolated VM part. Recently Apple released a tool called 'Container'. Yes, that's right. So, we checked it out and it seemed better than Docker as it provided one isolated VM per container - a perfect fit for running AI generated code.
So, we deployed a Jupyter server in the VM, exposed it as MCP (Model Context Protocol) tool, and made it available at <code>http://coderunner.local:8222/mcp</code>.</p>
<p>The advantage of MCPing vs a exposing an API is that existing tools that work with MCPs can use this right away. For instance, Claude Desktop and Gemini CLI can start executing AI-generated code with a simple config.</p>
<pre><code>"mcpServers": {
    "coderunner": {
      "httpUrl": "http://coderunner.local:8222/mcp"
    }
}
</code></pre>
<p>As you can see below, Claude figured out it should use the tool <code>execute_python_code</code> exposed from our isolated VM via the MCP endpoint.
<img src="https://instavm.io/blog-images/Pasted%20image%2020250730135012.png">
Aside - if you want to just use the <code>coderunner</code> bit as an MCP to execute code with your existing tools, the code for <code>coderunner</code> is <a href="https://github.com/instavm/coderunner">public</a>.</p>
<blockquote>
<h4>Apple container</h4>
<p>A tangent - if you're planning to work with Apple <code>container</code> and building VM images using it, have an abundance of patience. The build keeps failing with <code>Trap</code> error or just hangs without any output. To continue, you should <code>pkill</code> all container processes and restart the <code>container</code> tool. Then remove the <code>buildkit</code> image so that the next <code>build</code> process fetches a fresh one.
And repeat the three steps till it successfully works; this can take hours. We are excited to see Apple container mature as it moves beyond its early stages.</p>
</blockquote>
<p>Back to our app, we tested the <code>UI + LLMs + CodeRunner</code> on a task to <code>edit a video</code> and it worked!</p>
<p><img src="https://instavm.io/blog-images/WhatsApp%20Image%202025-07-11%20at%2016.50.13.jpeg">
		<em>I asked it to address me as Lord Voldemort as a sanity check for system instructions</em></p>
<p>After the coderunner was verified to be working, we decided to add the support of a headless browser. The main reason was to allow the app to look for new/updated tools/information online, for example, browsing github to find installation instruction for some tool it doesn't yet know about. Another reason was laying the foundation for <code>research</code>.
We chose Playwright for the task. We deployed it in the same container and exposed it as an MCP tool. Here is one task we asked it to do -</p>
<p><img src="https://instavm.io/blog-images/WhatsApp%20Image%202025-07-25%20at%2014.10.38.jpeg"></p>
<p>With this our basic set up was ready: <strong>Local LLM + Sandboxed arbitrary code execution + Headless browser</strong> for latest information.</p>
<hr>
<h2>What It Can Do (Examples)</h2>
<ol>
<li><strong>Do research on a topic</strong></li>
<li><strong>Generate and render charts</strong> from CSV using plain English</li>
<li><strong>Edit videos</strong> (via <code>ffmpeg</code>) — e.g., “cut between 0:10 and 1:00”</li>
<li><strong>Edit images</strong> — resize, crop, convert formats</li>
<li><strong>Install tools from GitHub</strong> in a containerized space</li>
<li><strong>Use a headless browser</strong> to fetch pages and summarize content etc.</li>
</ol>
<hr>
<h2>Volumes and Isolation</h2>
<p>We mapped a volume from
<code>~/.coderunner/assets</code> (host)
to
<code>/app/uploads</code> (container)</p>
<p>So files edited/generated stay in a safe shared space, <strong>but code never touches the host system</strong>.</p>
<hr>
<h2>Limitations &amp; Next Steps</h2>
<ul>
<li>Currently <strong>only works on Apple Silicon</strong> (macOS 26 is optional)</li>
<li>Needs better UI for managing tools and output streaming</li>
<li>Headless browser is classified as bot by various sites</li>
</ul>
<hr>
<h2>Final Thoughts</h2>
<p>This is more than a just an experiment. It's a philosophy shift <strong>bringing compute and agency back to your machine</strong>. No cloud dependency. No privacy tradeoffs. While the best models will probably be always with the giants, we hope that we will still have local tools which can get our day-to-day work done with the privacy we deserve.</p>
<blockquote>
<p>We didn't just imagine it. We built it. And now, you can use it too.</p>
<p>Check out <code>coderunner-ui</code> on <a href="https://github.com/instavm/coderunner-ui">Github</a> to get started, and let us know what you think. We welcome feedback, issues and contributions.</p>
</blockquote>
<hr>
<h2>🔗 Resources</h2>
<ul>
<li><a href="https://github.com/assistant-ai/assistant-ui">assistant-ui</a></li>
<li><a href="https://github.com/instavm/coderunner">instavm/coderunner</a></li>
<li><a href="https://github.com/apple/container">Apple/container</a></li>
<li><a href="https://github.com/instavm/coderunner-ui">instavm/coderunner-ui</a></li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The surprise deprecation of GPT-4o for ChatGPT consumers (377 pts)]]></title>
            <link>https://simonwillison.net/2025/Aug/8/surprise-deprecation-of-gpt-4o/</link>
            <guid>44839842</guid>
            <pubDate>Fri, 08 Aug 2025 18:04:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/Aug/8/surprise-deprecation-of-gpt-4o/">https://simonwillison.net/2025/Aug/8/surprise-deprecation-of-gpt-4o/</a>, See on <a href="https://news.ycombinator.com/item?id=44839842">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-permalink-context="/2025/Aug/8/surprise-deprecation-of-gpt-4o/">

<p>8th August 2025</p>



<p>I’ve been dipping into the <a href="https://reddit.com/r/chatgpt">r/ChatGPT</a> subreddit recently to see how people are reacting to <a href="https://simonwillison.net/2025/Aug/7/gpt-5/">the GPT-5 launch</a>, and so far the vibes there are not good. <a href="https://www.reddit.com/r/ChatGPT/comments/1mkae1l/gpt5_ama_with_openais_sam_altman_and_some_of_the/">This AMA thread</a> with the OpenAI team is a great illustration of the single biggest complaint: a lot of people are <em>very</em> unhappy to lose access to the much older GPT-4o, previously ChatGPT’s default model for most users.</p>
<p>A big surprise for me yesterday was that OpenAI simultaneously retired access to their older models as they rolled out GPT-5, at least in their consumer apps. Here’s a snippet from <a href="https://help.openai.com/en/articles/6825453-chatgpt-release-notes">their August 7th 2025 release notes</a>:</p>
<blockquote>
<p>When GPT-5 launches, several older models will be retired, including GPT-4o, GPT-4.1, GPT-4.5, GPT-4.1-mini, o4-mini, o4-mini-high, o3, o3-pro.</p>
<p>If you open a conversation that used one of these models, ChatGPT will automatically switch it to the closest GPT-5 equivalent. Chats with 4o, 4.1, 4.5, 4.1-mini, o4-mini, or o4-mini-high will open in GPT-5, chats with o3 will open in GPT-5-Thinking, and chats with o3-Pro will open in GPT-5-Pro (available only on Pro and Team).</p>
</blockquote>
<p>There’s no deprecation period at all: when your consumer ChatGPT account gets GPT-5, those older models cease to be available.</p>

<p id="sama"><strong>Update 12pm Pacific Time</strong>: Sam Altman on Reddit <a href="https://www.reddit.com/r/ChatGPT/comments/1mkae1l/comment/n7nelhh/">six minutes ago</a>:</p>
<blockquote>
<p>ok, we hear you all on 4o; thanks for the time to give us the feedback (and the passion!). we are going to bring it back for plus users, and will watch usage to determine how long to support it.</p>
</blockquote>
<p>See also <a href="https://x.com/sama/status/1953893841381273969">Sam’s tweet</a> about updates to the GPT-5 rollout.</p>

<p>Rest of my original post continues below:</p>

<hr>

<p>(This only affects ChatGPT consumers—the API still provides the old models, their <a href="https://platform.openai.com/docs/deprecations">deprecation policies are published here</a>.)</p>
<p>One of the expressed goals for GPT-5 was to escape the terrible UX of the model picker. Asking users to pick between GPT-4o and o3 and o4-mini was a notoriously bad UX, and resulted in many users sticking with that default 4o model—now a year old—and hence not being exposed to the advances in model capabilities over the last twelve months.</p>
<p>GPT-5’s solution is to automatically pick the underlying model based on the prompt. On paper this sounds great—users don’t have to think about models any more, and should get upgraded to the best available model depending on the complexity of their question.</p>
<p>I’m already getting the sense that this is <strong>not</strong> a welcome approach for power users. It makes responses much less predictable as the model selection can have a dramatic impact on what comes back.</p>
<p>Paid tier users can select “GPT-5 Thinking” directly. Ethan Mollick is <a href="https://www.oneusefulthing.org/p/gpt-5-it-just-does-stuff">already recommending deliberately selecting the Thinking mode</a> if you have the ability to do so, or trying prompt additions like “think harder” to increase the chance of being routed to it.</p>
<p>But back to GPT-4o. Why do many people on Reddit care so much about losing access to that crusty old model? I think <a href="https://www.reddit.com/r/ChatGPT/comments/1mkae1l/comment/n7js2sf/">this comment</a> captures something important here:</p>
<blockquote>
<p>I know GPT-5 is designed to be stronger for complex reasoning, coding, and professional tasks, but <strong>not all of us need a pro coding model</strong>. Some of us rely on 4o for creative collaboration, emotional nuance, roleplay, and other long-form, high-context interactions. Those areas feel different enough in GPT-5 that it impacts my ability to work and create the way I’m used to.</p>
</blockquote>
<p>What a fascinating insight into the wildly different styles of LLM-usage that exist in the world today! With <a href="https://simonwillison.net/2025/Aug/4/nick-turley/">700M weekly active users</a> the variety of usage styles out there is incomprehensibly large.</p>
<p>Personally I mainly use ChatGPT for research, coding assistance, drawing pelicans and foolish experiments. <em>Emotional nuance</em> is not a characteristic I would know how to test!</p>
<p>Professor Casey Fiesler <a href="https://www.tiktok.com/@professorcasey/video/7536223372485709086">on TikTok</a> highlighted OpenAI’s post from last week <a href="https://openai.com/index/how-we%27re-optimizing-chatgpt/">What we’re optimizing ChatGPT for</a>, which includes the following:</p>
<blockquote>
<p>ChatGPT is trained to respond with grounded honesty. There have been instances where our 4o model fell short in recognizing signs of delusion or emotional dependency. […]</p>
<p>When you ask something like “Should I break up with my boyfriend?” ChatGPT shouldn’t give you an answer. It should help you think it through—asking questions, weighing pros and cons. New behavior for high-stakes personal decisions is rolling out soon.</p>
</blockquote>
<p>Casey points out that this is an ethically complicated issue. On the one hand ChatGPT should be much more careful about how it responds to these kinds of questions. But if you’re already leaning on the model for life advice like this, having that capability taken away from you without warning could represent a sudden and unpleasant loss!</p>
<p>It’s too early to tell how this will shake out. Maybe OpenAI will extend a deprecation period for GPT-4o in their consumer apps?</p>
<p><em><strong>Update</strong>: That’s exactly what they’ve done, see <a href="https://simonwillison.net/2025/Aug/8/surprise-deprecation-of-gpt-4o/#sama">update above</a>.</em></p>
<p>GPT-4o remains available via the API, and there are no announced plans to deprecate it there. It’s possible we may see a small but determined rush of ChatGPT users to alternative third party chat platforms that use that API under the hood.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A message from Intel CEO Lip-Bu Tan to all company employees (155 pts)]]></title>
            <link>https://newsroom.intel.com/corporate/my-commitment-to-you-and-our-company</link>
            <guid>44839705</guid>
            <pubDate>Fri, 08 Aug 2025 17:48:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsroom.intel.com/corporate/my-commitment-to-you-and-our-company">https://newsroom.intel.com/corporate/my-commitment-to-you-and-our-company</a>, See on <a href="https://news.ycombinator.com/item?id=44839705">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
    <main id="main" tabindex="-1">
        
<article id="post-7242" aria-label="My commitment to you and our company">
	<div>

			

			                <div data-url="">

                        <!-- <button id="imageToggle" aria-pressed="false">
                            <span id="hideImage">Hide</span><span id="showImage" class="hidden">Show</span> Image
                        </button> -->

                        

                        
                            <p><img width="1200" height="675" src="https://newsroom.intel.com/wp-content/uploads/2025/03/intel-bio-headshot-lip-bu-tan-1200x675.jpg" alt="A smiling man with glasses and short hair wearing a dark suit and white shirt, set against a gray background with pixelated squares." decoding="async" fetchpriority="high" srcset="https://newsroom.intel.com/wp-content/uploads/2025/03/intel-bio-headshot-lip-bu-tan-1200x675.jpg 1200w, https://newsroom.intel.com/wp-content/uploads/2025/03/intel-bio-headshot-lip-bu-tan-300x169.jpg 300w, https://newsroom.intel.com/wp-content/uploads/2025/03/intel-bio-headshot-lip-bu-tan-1024x576.jpg 1024w, https://newsroom.intel.com/wp-content/uploads/2025/03/intel-bio-headshot-lip-bu-tan-768x432.jpg 768w, https://newsroom.intel.com/wp-content/uploads/2025/03/intel-bio-headshot-lip-bu-tan-1536x864.jpg 1536w, https://newsroom.intel.com/wp-content/uploads/2025/03/intel-bio-headshot-lip-bu-tan-1400x788.jpg 1400w, https://newsroom.intel.com/wp-content/uploads/2025/03/intel-bio-headshot-lip-bu-tan.jpg 1920w" sizes="(max-width: 750px) 100vw, 750px"></p><p>Lip-Bu Tan is chief executive officer of Intel Corporation and serves on the company’s board of directors. He was appointed to his position in March 2025.</p>
                            
                                            </div>
				
			
			<p>


				

									<h2>A message from Intel CEO Lip-Bu Tan to all company employees.</h2>
				
			</p>

			

			<div nonce="sI2vUjwNRX/OMdRrCafv5Q==">

					
						

					<!--?xml encoding="utf-8" ?--><p><em>The following note from Lip-Bu Tan was sent to all Intel Corporation employees on August 7, 2025:</em></p><p><span data-contrast="auto">Dear Team,</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p><p><span data-contrast="auto">I know there has been a lot in the news today, and I want to take a moment to address it directly with you. </span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p><p>Let me start by saying this: The United States has been my home for more than 40 years. I love this country and am profoundly grateful for the opportunities it has given me. I also love this company. Leading Intel at this critical moment is not just a job – it’s a privilege. This industry has given me so much, our company has played such a pivotal role, and it's the honor of my career to work with you all to restore Intel's strength and create the innovations of the future. Intel's success is essential to U.S. technology and manufacturing leadership, national security, and economic strength. This is what fuels our business around the world. It’s what motivated me to join this team, and it’s what drives me every day to advance the important work we’re doing together to build a stronger future.</p><p><span data-contrast="auto">There has been a lot of misinformation circulating about my past roles at Walden International and Cadence Design Systems. I want to be absolutely clear: Over 40+ years in the industry, I’ve built relationships around the world and across our diverse ecosystem – and I have always operated within the highest legal and ethical standards. My reputation has been built on trust – on doing what I say I’ll do, and doing it the right way. This is the same way I am leading Intel.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p><p><span data-contrast="auto">We are engaging with the Administration to address the matters that have been raised and ensure they have the facts. I fully share the President’s commitment to advancing U.S. national and economic security, I appreciate his leadership to advance these priorities, and I’m proud to lead a company that is so central to these goals.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p><p><span data-contrast="auto">The Board is fully supportive of the work we are doing to transform our company, innovate for our customers, and execute with discipline – and we are making progress. It’s especially exciting to see us ramping toward high-volume manufacturing using the most advanced semiconductor process technology in the country later this year. It will be a major milestone that’s a testament to your work and the important role Intel plays in the U.S. technology ecosystem.&nbsp;</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p><p><span data-contrast="auto">Looking ahead, our mission is clear, and our opportunity is enormous. I’m proud to be on this journey with you.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p><p><span data-contrast="auto">Thank you for everything you’re doing to strengthen our company for the future.&nbsp;</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p><p><span data-contrast="auto">Lip-Bu</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
				</div><!-- .entry-content-wrapper -->

		</div><!-- .post-content-->

</article><!-- #post-## -->
    </main><!-- #main -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Someone keeps stealing, flying, fixing and returning this man's 1958 Cessna (107 pts)]]></title>
            <link>https://www.latimes.com/california/story/2025-08-08/mystery-plane-thief</link>
            <guid>44839681</guid>
            <pubDate>Fri, 08 Aug 2025 17:45:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.latimes.com/california/story/2025-08-08/mystery-plane-thief">https://www.latimes.com/california/story/2025-08-08/mystery-plane-thief</a>, See on <a href="https://news.ycombinator.com/item?id=44839681">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-element="story-body" data-subscriber-content=""> <p>While Jason Hong was celebrating his 75th birthday, he suddenly found himself thinking about his 1958 Cessna Skyhawk, a white and red single-engine beauty with colorful stripes that he calls his “old treasure.”</p><p>He doesn’t fly it much anymore, but given the occasion he resolved to visit his plane as soon as he could to “say hi,” like a lifelong friend you see around holidays and special occasions.</p><p>Hong headed to Corona Municipal Airport after church on July 27, but when he got there, the plane was not where he’d left it. Hong was dumbfounded.</p><p>“I got confused,” he said. “I thought, did I park it somewhere else, did the airport manager move it? But I looked all over.”</p><p>It was gone. </p><p>Hong was so shocked, he initially didn’t know who to reach out to about a missing, stolen plane. He wondered, did someone fly it out of the airport unnoticed? How long had it been missing? </p><p>The questions piled up. But the mystery only deepened.</p><p>As Hong would come to find out, the colorful aircraft had been flown across Southern California by an unknown pilot, unnoticed, in a series of joyrides — or joy flights — at least twice before and then simply returned to the airport. Both Hong and police were left scratching their heads.</p><p>The first time he discovered it missing, Hong reported it to Corona police, unsure that he’d ever see the plane he’s owned for nearly 30 years again. After all, he thought, who steals an entire plane?</p><p>Then on the morning of July 29, he got a call from La Verne Police, telling him his plane was found in Brackett Field Airport. </p><p>“There’s my airplane, sitting there in the airport,” Hong said, finding cigarette butts and garbage strewn about in the cockpit.</p><p>He barely took time to process what happened when, frustrated, he decided to pull out the battery from the plane, close it up, and go home. The plane wouldn’t start without the battery, he figured, and he could come back the next weekend when he had time to clean and inspect it. </p><p>Except that, when he returned that Sunday, Aug. 3, the plane had vanished again.</p><p>Hong reported the plane missing again with La Verne Police, and wondered what was going on. It wasn’t long before he got another call. This time, El Monte Police told him his plane was <a href="https://www.instagram.com/p/DM8EwnNRROR/?hl=en" target="_blank">sitting at San Gabriel Valley Airport</a>. </p><p>When Hong got there to inspect his plane, his confusion only grew. </p><p>“I found it with a battery,” he said. </p><p>It hasn’t been just Hong who has found himself befuddled by his disappearing and reappearing plane. </p><p>“This plane just keeps disappearing out of the blue,” said Sgt. Robert Montanez of the Corona Police Department. “It’s just weird.”</p><p>Montanez said when Hong reported his plane missing the first time, he’d last seen the aircraft in May in the small Corona airport.</p><p>For police, a case of an entire plane being stolen was so rare, that officers used the same form used for stolen cars, to take Hong’s report. </p><p>Officers are also aware that the plane has been taken multiple times, and returned, making the incidents more perplexing. But Montanez said there’s no immediate indication as to who the culprit is. </p><p>“There’s no camera video, there’s no real leads as to who stole the plane,” Montanez said.</p><p>After finding his plane a second time, Hong said he’s tried to put details of the thefts together, but the more he learns the more he grows confused about the circumstances. </p><p>Hong looked up his plane on Flight Aware, a site that tracks flights and aircraft, and found that on his 75th birthday, someone took off with his plane from La Verne airport at 9:54 p.m., for a 51-minute flight that at one point neared Palm Springs. </p><p>A few hours later, on July 26, the colorfully striped plane was in the air once again, this time for a brief 22-minute flight from Riverside County toward La Verne that started at about 1:30 a.m.</p><p>It was the next day that Hong would discover it missing.</p><p>At first, Hong said, he thought it might have been a random incident, but the details of the repeating incidents didn’t make sense to him, he said. </p><p>The multiple flights indicate that, whoever has taken his plane has had some sort of flight training, since they’ve been able to land the plane on multiple occasions. </p><p>“Landing is not easy, so they’re trained,” he said.</p><p>Hong said he’s also found a headset in the plane, as well as a new battery to replace the one he removed, meaning this mysterious pilot had spent hundreds of dollars on equipment to get his plane back in the air.</p><p>The replacement of the battery, Hong said, also suggests its someone familiar with not just flying, but the mechanics of the plane as they seemed to have the tools and know-how about the type of battery needed, and how to install it.</p><p>Having his airplane stolen has been frustrating, Hong said. But learning that the suspect has also been spending money and equipment to use — and return — the plane has just been confusing. </p><p>“Someone breaks into your house, they’re looking for jewelry or cash right?” he said, trying to reason with the circumstances. “But in this case, what’s the purpose? It’s like someone breaks my window, and then they put a new one up.”</p><p>The fact that someone has been traveling in it to different airports also puzzles him.</p><p>The 75-year-old Yorba Linda resident said he’s spoken to regular pilots and employees at the San Gabriel Valley Airport in El Monte, who said that they saw the plane flying in and out of the airport multiple times in July. </p><p>“On and off, they flew in and out, in and out, almost an entire month without knowing,” he said. “This is really a rare situation.”</p><p>One regular at the airport, Hong said, told him he saw a woman, about 5 feet, 3 inches tall, and in her 40s or 50s, flying and sitting in the plane on multiple occasions. The man told Hong he had a conversation with her at one point, and distinctly remembered her because she was often seen sitting in the cockpit during the day, making people at the airport wonder why she wouldn’t just relax in the air-conditioned airport lounge. </p><p>“Very strange,” Hong said. </p><p>For now, Hong has chained his plane in San Gabriel Valley Airport and said he’s uncomfortable flying it until he can thoroughly inspect it. </p><p>Other than that, he’s not sure what to do to keep his plane grounded, or to find out who has been secretly flying it out. </p><p>“I have no idea what to do,” he said. “It’s the strangest thing.”</p><div data-list-id="00000192-be42-da32-a3db-ff76fc3b0000" data-module-id="00000192-be42-da32-a3db-ff76fc3b0000" data-excluded-ids="00000196-fea4-da0a-ab96-fef4849a0000" data-click="enhancement" data-align-center="">  <p data-element="element-header" data-click="liZZListTitleCTA">  <h3 data-element="element-header-title" data-counter="3">More to Read </h3>  </p>      </div> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tor: How a military project became a lifeline for privacy (338 pts)]]></title>
            <link>https://thereader.mitpress.mit.edu/the-secret-history-of-tor-how-a-military-project-became-a-lifeline-for-privacy/</link>
            <guid>44838378</guid>
            <pubDate>Fri, 08 Aug 2025 15:45:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thereader.mitpress.mit.edu/the-secret-history-of-tor-how-a-military-project-became-a-lifeline-for-privacy/">https://thereader.mitpress.mit.edu/the-secret-history-of-tor-how-a-military-project-became-a-lifeline-for-privacy/</a>, See on <a href="https://news.ycombinator.com/item?id=44838378">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>A story of secrecy, resistance, and the fight for digital freedom.</p><figure><img width="700" height="420" src="https://thereader.mitpress.mit.edu/wp-content/uploads/2025/07/Tor-lead-700x420.jpg" alt="" decoding="async" fetchpriority="high"><figcaption>Photo credit: <a href="https://unsplash.com/photos/a-computer-with-a-white-screen-sitting-on-a-table-AP7tG4LTeXA">Alan W, via Unsplash</a></figcaption></figure><p>I’m sitting in a cold, scuffed, and dirty plastic chair on a crowded train, watching freezing fog stream past the window — one of the many unpleasant but strangely enjoyable everyday experiences of life in the United Kingdom. Despite the train carriage hailing from the mid-1980s, there is something resembling Wi-Fi service, and so I connect, hoping to sneak in a few hours of PhD research. I load up a website — or so I think — but instead reach a block page courtesy of the train’s Wi-Fi provider.</p><p>Sighing, I load up the Tor Browser and type in the address. The website loads instantly.</p><p>Tor is mostly known as the <em>Dark Web </em>or <em>Dark Net</em>, seen as an online Wild West where crime runs rampant. Yet it’s partly <a href="https://blog.torproject.org/transparency-openness-and-our-2021-and-2022-financials/" target="_blank" rel="nofollow">funded by the U.S. government</a>, and the BBC and Facebook both have Tor-only versions to allow users in authoritarian countries to reach them.</p><p>At its simplest, Tor is a distributed digital infrastructure that makes you anonymous online. It is a network of servers spread around the world, accessed using a browser called the Tor Browser, which you can download for free from the Tor Project website. When you use the Tor Browser, your signals are encrypted and bounced around the world before they reach the service you’re trying to access. This makes it difficult for governments to trace your activity or block access, as the network just routes you through a country where that access isn’t restricted.</p><h3><strong>The dark net rises</strong></h3><p>Today, privacy technologies like Tor underpin our digital society. From VPNs and encrypted messengers like WhatsApp to the basic security features in our digital systems, they’re essential tools for defending against cybercrime.</p><p>But, because you can’t protect yourself from digital crime without also protecting yourself from mass surveillance by the state, these technologies are the site of constant battles between security and law enforcement interests. The UK in particular is <a href="https://www.theguardian.com/technology/2025/jul/24/what-are-the-new-uk-online-safety-rules-and-how-will-they-be-enforced" target="_blank" rel="nofollow">currently convulsed over attempts to use law and technology to fight online harm</a>.</p><p>Recent debates focus on harms like the algorithmic spread of radicalizing and hateful content, issues often treated as if they emerge magically from technology itself, rather than from social policy, corporate greed, or an increasingly radicalized social elite.</p><figure><blockquote><p>Cypherpunks warned that the internet could quickly turn from a utopian dream into an authoritarian nightmare.</p></blockquote></figure><p>We’re in an undoubtedly odd situation. Governments are increasingly clamping down on the internet, yet the technologies to circumvent these blocks are readily available (and, in the case of Tor, completely free) and often funded, developed, used, and promoted by the same governments. By going back 30 years to the founding of the World Wide Web and the development of the technologies that would become Tor, we can get some surprising insights into why.</p><h3><strong>Cryptowars</strong></h3><p>Besieged as they are by the ongoing Oasis revival and the continuing dominance of Carhartt in the fashion market, readers in the UK will need little introduction to the cultural landscape of the 1990s. But in addition to the baggy jeans and Britpop, the early days of the commercial internet were also a time of immense possibility and conflict, when many aspects of the technologies and design of our digital societies were being fought over.</p><p>Some of the most important of these battles were the so-called Crypto Wars. A group of radical hackers and computer scientists known as the Cypherpunks spent the 1980s and early ‘90s adapting military-grade encryption for public use. They warned that the internet could quickly turn from a utopian dream into an authoritarian nightmare. As <em>technolibertarians</em>, they believed that encryption was vital to realizing the potential of the internet, that it would permanently break up the power of the big media corporations, banks, and governments and give it to private individuals.</p><p>Law enforcement, on the other hand, was increasingly furious at the spread of mass communications platforms slipping beyond their control. But the spirit of the age was against them. Many of the internet’s core architects saw encryption as essential to keeping large and complex systems running without interference. Global businesses, too, generally favored privacy. Operating in a global market, they wanted to protect their competitive edge in an emerging digital economy.</p><p>Perhaps most importantly, at the heart of the U.S. government were an ascendant set of ideas that saw the internet as the ultimate neoliberal project: a borderless marketplace where free-flowing information would lead to optimal prices, ideas, and solutions. Full of messianic cultural confidence following the fall of the Soviet Union, they believed that if information were allowed to flow, the values of American capitalism would triumph on their own merits.</p><p>It was in this chaotic, high-stakes environment, full of strange alliances and clashing visions,  that the technologies behind the Dark Web were born.</p><h3><strong>Spies, submarines, and secrets</strong></h3><p>Tor’s story began in an office of the U.S. Naval Research Laboratory (NRL). Down the hall, satellites and radar dishes hung suspended in enormous voids, giant black pyramids bristled from the walls of vast anechoic testing chambers, and robot arms flexed in dark flooded pools, being poked and prodded by scuba divers armed with sensors. But the foundations of Tor were laid in a much more prosaic setting — a shared computer lab.</p><p>Three military researchers — David Goldschlag, Mike Reed, and Paul Syverson — had been discussing a foundational aspect of the internet infrastructure. The rise of the new, commercial internet presented challenges for military users, as these global systems were vital for communications but difficult to secure.</p><p>Encryption technologies were already available to protect the <em>content</em> of messages. But above the content, the network itself had a range of security issues. The internet’s traffic routing systems and protocols were reliant on addressing metadata, equivalent to the <em>to </em>and <em>from </em>addresses on an envelope. Much as the address of the recipient is crucial for the delivery of a piece of mail, so are these metadata fundamental to the internet’s design and necessarily visible to the infrastructure providers who run its networks.</p><p>The routing design of the internet worked well for the U.S. government’s domestic interests, as it allowed the state to establish itself at key control points and surveil user traffic. However, the spread of the internet around the world had given other governments this power over their own domestic communication networks. This means that intelligence and military personnel abroad who wanted to make contact with their handlers in the United States or communicate with their base of operations were vulnerable.</p><p>Whenever the Navy utilized cryptosystems and communication networks that linked up to the internet, substantial amounts of valuable additional information were exposed to the people who ran the infrastructure. For example, if a CIA spy was in a foreign nation and sent a message over the internet back to the CIA’s home servers, ISPs in that foreign nation could observe where the message was sent and infer the spy’s affiliation.</p><p>This was a clear question for military research: how to keep internet traffic between the U.S. and other nations secret, not only in content (which you could protect with encryption), but also in origin and destination. And the three NRL researchers sought to solve it.</p><h3><strong>Onions</strong></h3><p>The researchers wanted to find a way to do the seemingly impossible — to give the military the benefits of a global, high-speed communications network without exposing them to the vulnerabilities of the metadata that the network relied on to operate.</p><p>Enter Onion routing. Onion routing has undergone many changes and refinements over the years, but the basic principle has remained the same: The routing information used to navigate the internet is first hidden under three layers of encryption, like a Russian doll. It is from these layers that onion routing gets its name. This “onion” of routing information is then sent into a network of onion routers: servers, or relays, located around the world that bounce the traffic around and between themselves. Each of these relays decrypts a layer of encryption to reveal the address of the next server in the network, until the final server reveals the destination of the traffic and makes a connection to the target web service. None of them can see both the origin and the destination of the traffic.</p><figure><blockquote><p>The routing information used to navigate the internet is first hidden under three layers of encryption, like a Russian doll.</p></blockquote></figure><p>This technical design has immediate social consequences, which were apparent to the NRL designers from the early stages. First, the infrastructure could not be run by the U.S. Navy, for if this were the case, then only people who trusted the U.S. Navy would use it. In an onion routing design, anonymity is produced by the size of the crowd — the more people using the system, the more privacy it provides.</p><p>There are other implications, as well. For a CIA agent to use Tor without suspicion in non-U.S. nations, for example, there would need to be plenty of citizens in these nations using Tor for everyday internet browsing. Similarly, if the only users in a particular country are whistleblowers, civil rights activists and protesters, the government may well simply arrest anyone connecting to your anonymity network. As a result, an onion routing system had to be open to as wide a range of users and maintainers as possible, so that the mere fact that someone was using the system wouldn’t reveal anything about their identity or their affiliations.</p><p>This philosophy of a system open to the general public, in which small numbers of high-risk users could hide in cover traffic from more everyday users, underpins what became the onion routing paradigm, the predecessor to Tor.</p><h3><strong>Cypherpunk hackers and the U.S. military</strong></h3><p>Anonymity loves company — so Tor needed to be sold to the general public. That necessity led to an unlikely alliance between cypherpunks and the U.S. Navy.</p><p>The NRL researchers behind Onion routing knew it wouldn’t work unless everyday people used it, so they reached out to the cypherpunks and invited them into conversations about design and strategy to reach the masses.</p><p>The NRL researchers met several members of the cypherpunk community in person at the Information Hiding Workshop in Oakland in 1997, where they discussed the possibility of collaboration. There, over vegetarian lasagna, salad, and (what else?) roasted onions, they discussed the technical possibilities and paradigms that might underpin a mass-use anonymity system. As they did, they also talked through broader values and motivations that might unite their strange, hybrid community.</p><p>Observing these two worlds — the military academics and the cypherpunks — interacting, through sharing test results, theoretical discussions, phone calls, emails, and eating the occasional roasted onion, we see the beginnings of a distinctive idea of what privacy means. Somewhere between the cypherpunk’s everyday, radical, decentralized vision of privacy and the high-security traffic protection desired by the military, a shared idea was forming. This saw privacy as being strongly shaped by the clusters of power and control built into digital infrastructure.</p><p>This understanding of privacy as a <em>structure</em> would unite an odd coalition around Tor over the next three decades: activists, journalists, drug buyers, hackers, and the military itself.</p><h3><strong>Scrambling for safety</strong></h3><p>This strange story of a group of libertarian hackers teaming up with the U.S. military amid the aftershocks of the Cold War presents a more nuanced picture of privacy than the familiar lone-user-versus-state narrative. It shows different groups coming together to change how — through laws, technologies, practices, and cultural values — we police the boundaries between different material systems of power. Understood in this way, we can see privacy as setting out where the domain of the community, of the family, of the state, of a corporation, of an institution or an individual begins and ends.</p><p>Take the UK’s Online Safety Act. It’s justified by policymakers as a tool to protect women and children from harm, with the Technology Minister going as far as to say that opposing the Act <a href="https://www.politico.eu/article/nigel-farage-on-the-side-predator-jimmy-savile-says-uk-minister-peter-kyle-online-safety-act/" target="_blank" rel="nofollow">puts you “on the side of predators”</a> and child abusers. Law enforcement often argues that privacy technologies undermine their ability to prevent and investigate crime, particularly crime against women and children. This frames the issue as a trade-off between individual rights and collective safety. But many feminists would argue exactly the opposite: that the police have long painted women and children as uniquely weak and vulnerable in order to cement their own claim to power.</p><figure><blockquote><p>Undermining the very tools that give communities security is a poor strategy for keeping them safe.</p></blockquote></figure><p>In fact, breaking encryption in practice intensifies surveillance of women and children, undermining their rights to self-determination and autonomy, under the justification of protecting them. Yet it’s often powerful men in their families, communities, or institutions that women need to protect themselves from. For many, the prospect of police being able to track their intimate lives, or their attempts to access reproductive healthcare, is extremely threatening.</p><p>The state’s claim to protect the vulnerable often masks efforts to exert control. In fact, robust, well-funded, value-driven and democratically accountable content moderation — by well-paid workers with good conditions — is a far better solution than magical tech fixes to social problems (which also do little to tackle real social issues of misogyny, racism, and violence) or surveillance tools.</p><p>Indeed, undermining the very tools that give communities security is a poor strategy for keeping them safe. As more of our online lives are funneled into the centralized AI infrastructures — controlled by a small and increasingly radicalized tech elite — tools like Tor are becoming ever more important. Beyond offering privacy and protection from cybercrime in an increasingly insecure global landscape, they point to a more optimistic future for the internet, one in which we rebuild trust in our social institutions to address harm, rather than surrender that role to unaccountable technologies of control.</p><hr><p><strong><em>Ben Collier</em></strong><em> is Senior Lecturer in Digital Methods in the Department of Science, Technology, and Innovation Studies at the School of Social and Political Science, University of Edinburgh. He is the author of “<a href="https://mitpress.mit.edu/9780262548182/tor/" target="_blank">Tor: From the Dark Web to the Future of Privacy</a>.” An open access edition of the book is freely available for download <a href="https://direct.mit.edu/books/oa-monograph/5761/TorFrom-the-Dark-Web-to-the-Future-of-Privacy" target="_blank">here</a>.</em></p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-5 vs. Sonnet: Complex Agentic Coding (163 pts)]]></title>
            <link>https://elite-ai-assisted-coding.dev/p/copilot-agentic-coding-gpt-5-vs-claude-4-sonnet</link>
            <guid>44838303</guid>
            <pubDate>Fri, 08 Aug 2025 15:38:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://elite-ai-assisted-coding.dev/p/copilot-agentic-coding-gpt-5-vs-claude-4-sonnet">https://elite-ai-assisted-coding.dev/p/copilot-agentic-coding-gpt-5-vs-claude-4-sonnet</a>, See on <a href="https://news.ycombinator.com/item?id=44838303">Hacker News</a></p>
Couldn't get https://elite-ai-assisted-coding.dev/p/copilot-agentic-coding-gpt-5-vs-claude-4-sonnet: Error: getaddrinfo ENOTFOUND elite-ai-assisted-coding.dev]]></description>
        </item>
        <item>
            <title><![CDATA[AI must RTFM: Why tech writers are becoming context curators (143 pts)]]></title>
            <link>https://passo.uno/from-tech-writers-to-ai-context-curators/</link>
            <guid>44837875</guid>
            <pubDate>Fri, 08 Aug 2025 15:04:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://passo.uno/from-tech-writers-to-ai-context-curators/">https://passo.uno/from-tech-writers-to-ai-context-curators/</a>, See on <a href="https://news.ycombinator.com/item?id=44837875">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header>
	
	<nav>
		
		<a href="https://passo.uno/about">About</a>
		
		<a href="https://passo.uno/posts">Posts</a>
		
		<a href="https://bsky.app/profile/theletterf.bsky.social">Bluesky</a>
		
		<a href="https://hachyderm.io/@remoquete">Mastodon</a>
		
		<a href="https://www.linkedin.com/in/fabrizioferri/">LinkedIn</a>
		
		<a href="https://passo.uno/posts/index.xml">RSS</a>
		
		
	</nav>
</header>

<main>
	<article>
		
		

		

		<section>
			<p>I’ve been noticing a trend among developers that use AI: they are increasingly writing and structuring docs in context folders so that the AI powered tools they use can build solutions autonomously and with greater accuracy. They now strive to understand information architecture, semantic tagging, docs markup. All of a sudden they’ve discovered docs, so they write more than they code. Because AI must RTFM now.</p>
<p>It’s docs-driven development. It’s also technical writing. We should welcome our colleagues into the fold of technical communication and seriously start thinking about becoming context writers and maintainers. In a way, we’ve always been that, building the skills and techniques that allow owners of brains – either organic or simulated – to find their way in complex systems and accomplish tasks.</p>
<h2 id="all-ai-requires-to-do-the-right-thing-is-great-context-and-a-gentle-nudge">All AI requires to do the right thing is great context and a gentle nudge</h2>
<p>Picture large language models (LLMs) as elaborate machines that take inputs (in most cases it’s just text), turn it into discrete pieces (tokens) and process it through an incredibly convoluted conveyor belt. At the other end of the machine is the output, usually in the form of helpful commentary, feedback, and commands issued to various system tools. The <a href="https://en.wikipedia.org/wiki/Chinese_room">Chinese room</a>, finally incarnated.</p>
<p>The quality of the output is a function of the quality of the input. We enter requests and get back what an average of all human thoughts could have led us to, eventually. There’s nothing magic to it: if you write clear, accurate, and well structured requests (prompts), chances are that the LLM will respond in kind. It’s hard work, but it pays off, because development time is substantially reduced.</p>
<p>That’s why, with every release of a new LLM, coders pay close attention to the size of the <a href="https://www.ibm.com/think/topics/context-window">context window</a>, that is, the amount of information one can feed to an LLM. A context window of one million tokens means you can easily feed the entire <em>The Lord of the Rings</em> trilogy to, say, Gemini, and start asking questions about it. And you would still have room for <em>The Hobbit and</em> <em>The</em> <em>Silmarillion</em>.</p>
<h2 id="what-is-a-context-curator-and-why-we-need-that-role">What is a Context curator and why we need that role</h2>
<p>Engineers are finding out that writing, that long shunned soft skill, is now key to their efforts. In <a href="https://www.anthropic.com/engineering/claude-code-best-practices">Claude Code: Best Practices for Agentic Coding</a>, one of the key steps is creating a CLAUDE.md file that contains instructions and guidelines on how to develop the project, like which commands to run. But that’s only the beginning. Folks now <a href="https://www.reddit.com/r/ClaudeAI/comments/1lxylfs/claude_code_docs_as_context/">suggest</a> maintaining elaborate context folders.</p>
<p><strong>A context curator, in this sense, is a technical writer who is able to orchestrate and execute a content strategy around both human and AI needs, or even focused on AI alone.</strong> Context is so much better than content (a much abused word that means little) because it’s tied to <em>meaning</em>. Context is situational, relevant, necessarily limited. AI needs context to shape its thoughts.</p>
<p>In <a href="https://passo.uno/build-tech-writing-tools-llms/">Own the prompt</a>, where I described how I built <a href="https://github.com/theletterf/aikidocs">a tool</a> to write docs using context from the terminal or the browser, I argued that technical writers should lead the way and own AI-powered docs processes, including the curation of context. In my <a href="https://passo.uno/tech-writing-predictions-2025/">predictions for this year</a> (and probably the next, too), the importance of context was already present as the rise of docs-as-data.</p>
<blockquote>
<p><em>Picture a developer inserting a docs cartridge into his AI powered code editor: the presentation layer is going to be largely irrelevant, the docs powering the answers of locally executed LLMs to aid developers in their coding quests. In a multi-channel content strategy, LLM-tailored output is going to be an additional, incredibly relevant channel.</em></p>
</blockquote>
<h2 id="writing-is-designing-and-co-developing-again">Writing is designing and co-developing (again)</h2>
<p>Four years ago, <a href="https://passo.uno/posts/how-to-assist-api-design-as-a-technical-writer/">I argued</a> that technical writers can play a key role in API design and development, because words are everywhere and we writers are uniquely well positioned to select the right words. Back then, OpenAPI was the device that allowed the magic of crafting design using just words. Today, the spell extends to all kinds of software development. <a href="https://passo.uno/thinking-better-through-ai/">We can conjure software ourselves</a> now.</p>
<p>At this point, the most bleeding edge tech writing shops are serving <a href="https://llmstxt.org/">llms.txt</a> files and <a href="https://github.com/romansky/dom-to-semantic-markdown">LLM-optimized Markdown</a>. We’d take this a step further and prepackage context in a way that LLMs can easily consume. A standard for this still doesn’t exist, but I can see some flavor of DITA or another markup making a comeback, together with UIs that let users download which docs, for which version, etc.</p>
<p><img src="https://passo.uno/uploads/repomix.png" alt="Repomix"></p>
<blockquote>
<p><a href="https://repomix.com/">Repomix</a> allows anybody to select and package code and docs for LLMs</p>
</blockquote>
<p>The endgame is to be able to make content accessible to LLMs and humans alike, to let them extract knowledge tailored to their needs. Tech writers become context writers when they put on the art gallery curator hat, eager to show visitors the way and help them understand what they’re seeing. It’s yet another hat, but that’s both the curse and the blessing of our craft: like <a href="https://en.wikipedia.org/wiki/Bard_(Dungeons_%26_Dragons)">bards in DnD</a>, we’re the jacks of all trades that save the day (and the campaign).<br></p>
		</section>

		
		</article>
</main>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI is impressive because we've failed at personal computing (199 pts)]]></title>
            <link>https://rakhim.exotext.com/ai-is-impressive-because-we-ve-failed-at-semantic-web-and-personal-computing</link>
            <guid>44837783</guid>
            <pubDate>Fri, 08 Aug 2025 14:57:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rakhim.exotext.com/ai-is-impressive-because-we-ve-failed-at-semantic-web-and-personal-computing">https://rakhim.exotext.com/ai-is-impressive-because-we-ve-failed-at-semantic-web-and-personal-computing</a>, See on <a href="https://news.ycombinator.com/item?id=44837783">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>Unless someone wrote an article about that exact thing, a plain full-text search engine cannot answer a question like this:</p>
<blockquote>
<p>What animal is featured on a flag of a country where the first small British colony was established in the same year that Sweden's King Gustav IV Adolf declared war on France?</p>
</blockquote>
<p>But ChatGPT got the correct answer in a few seconds.</p>
<p><img src="https://img.exotext.com/1/_1IITv5hhOjWL-WdlODDM.png" alt="">
<em>Flag of Dominica features the Sisserou parrot, which is only found in Dominica. Great Britain established a small colony on the island in 1805.</em></p>
<p>Google's AI widget failed miserably, by the way.</p>
<p><img src="https://img.exotext.com/1/S4vSDU4NTMtPN3BuxyNbJ.png" alt=""></p>
<p>One of the best applications of modern LLM-based AI is surfacing answers from the chaos of the internet. Its success can be partly attributed to our failure to build systems that organize information well in the first place.</p>
<p>This product pattern is not new. Take Google Drive: a glorified file system in the cloud with folders and files, but it offers a worse experience than almost any desktop file management application of the last 30 years. Organizing your stuff there is hard and tedious. So Google took a shortcut: full-text search. Just dump everything in, and type to find it later. </p>
<p>The pattern of giving up on structure and relying on search has quietly become the dominant paradigm. "Search" here is a wide term: it can mean classic text-matching across indexed data, or complex multi-dimensional token matching across unwieldy models and weights. Why create a well-organized e-commerce site, just add a search bar and oversaturate each item's page with keywords. Why write high-quality user documentation, just add a support chat bot.</p>
<p>Remember Semantic Web? The web was supposed to evolve into semantically structured, linked, machine-readable data that would enable amazing opportunities. That never happened. Not only data remains unstructured and lacking metadata, even the representation of the unstructured data became difficult for machines to read due to the switch from plain, somewhat-structured HTML to JS-driven dynamic pile of <code>div</code>s.</p>
<p>We also never achieved truly personal computing. Computers could've been personal knowledge bases, with structured semantic connections akin to HyperCard, that take advantage of the semantic web and open standards. </p>
<p>My point is that if all knowledge were stored in a structured way with rich semantic linking, then very primitive natural language processing algorithms could parse question like the example at the beginning of the article, and could find the answer using orders of magnitude fewer computational resources. And most importantly: the knowledge and the connections would remain accessible and comprehensible, not hidden within impenetrable AI models.</p>
<p>AI is not a triumph of elegant design, but a brute-force workaround. LLMs like ChatGPT can infer structure from chaos. They scan the unstructured web and build ephemeral semantic maps across everything. It's not knowledge in the classic sense.. or perhaps it is exactly what knowledge is?</p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google's Genie is more impressive than GPT5 (199 pts)]]></title>
            <link>https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant</link>
            <guid>44837646</guid>
            <pubDate>Fri, 08 Aug 2025 14:46:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant">https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant</a>, See on <a href="https://news.ycombinator.com/item?id=44837646">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>The goal of AGI is to make programs that can do lots of things. Unfortunately, it's not all that easy to program “do lots of things” into a computer. Like, if you're writing python, there's no `import everything` library – you have to somehow teach your program a bunch of different tasks and skills. Naively, you could spend a lot of time hand coding every possible scenario that may occur – some gigantic switch statement that has a unique handler for every possible input. But this is obviously going to take too long and is extremely inefficient and really only theoretically possible.</p><p><span>So the watchwords of AGI are compression and generalization. You want to make a program that is pretty small in terms of compute and memory and so on, but has a lot of abilities that allow it to cover a very large ‘action space.’</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-170425838" href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant#footnote-1-170425838" target="_self" rel="">1</a></span></p><p><span>One way to teach your program how to generalize across things is by using deep learning. At a high level, you can show a deep neural network terabytes of data, and it will learn how to represent that data in a compressed form. The big large language models take in ~all of the text ever written, and are maybe a few tens-of-gigabytes in size,</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-170425838" href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant#footnote-2-170425838" target="_self" rel="">2</a></span><span> and yet seem to be able to replicate much of the training data. Perhaps the most surprising revelation of the last few years is that in addition to getting really good at spitting out realistic looking text, these LLMs also picked up more generic skills. The most evocative example of this for me was realizing that the original GPT-3 models – the ones that preceded ChatGPT, that had no ‘post training’ or ‘instruction tuning’ – could play a decent game of chess, even though the model surely didn't understand chess, and probably didn't really understand 2D grids. And since that moment back in 2020, it has become extremely obvious that these things can do quite a fair bit beyond just mimicking text.</span></p><p>A lot of AI research these days is basically exclusively about how to make large language models better. Naturally, some people focus on “large” – if you make the model bigger, it can get better! But some people also focus on “language” – LLMs are only compressing text, but what if it compressed more kinds of data? A model that could represent text and images is probably better than one that can only represent text. And a model that could represent text and images and video is probably better than one that can only represent text and images.</p><p><span>If you assume that model representation capacity is directly tied to usefulness, you'll eventually reach a conclusion that looks something like this: “a model that can accurately represent the entire world is going to be pretty damn useful.” Imagine asking a model a question like “what's the weather in Tibet” and instead of doing something </span><em>lame</em><span> like check weather.com, it does something </span><em>awesome</em><span> like stimulate Tibet exactly so that it can tell you the weather based on the simulation. And giving a robot the ability to represent the world may allow it to do things like plan complex movements, navigate environments, and otherwise interact with real world environments. After all, this is approximately how humans work. In order to pick up my mug of not-coffee, I have to have an internal representation of my hand, the table, the mug, where my arm is going to go, how my hand is going to grip, what gravity is, what object-corporeality is, etc. etc. More mundanely, world models will probably allow people to make, like, better, more realistic AI generated Tiktoks that don't turn into spaghetti after a few minutes (and I'm sure nothing bad will come of that).</span></p><p><span>These “World Models” are considered a pretty long shot frontier in the AI world. Hopefully for obvious reasons — simulating an entire world for extended periods of time with any kind of accuracy is really hard! You need mountains and mountains of data, most of it video. And as a result, there aren't a lot of people who are really even trying in this space.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-170425838" href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant#footnote-3-170425838" target="_self" rel="">3</a></span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-170425838" href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant#footnote-4-170425838" target="_self" rel="">4</a></span><span> But you know who has a mountain of video data?</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!3_o3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!3_o3!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 424w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 848w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 1272w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!3_o3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png" width="1456" height="971" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!3_o3!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 424w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 848w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 1272w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>This is, possibly, the worst photo of these guys that I have ever seen.</figcaption></figure></div><p><span>About 3 days ago, Google announced </span><a href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/" rel="">Genie 3</a><span>. Genie stands for Generative Interactive Environments. The best way to understand Genie is by analogy. GPT and Gemini let you create text descriptions of a time and place. And Veo and Sora let you turn text descriptions into video. Genie lets you take a text description into a </span><em>video game</em><span>, a space that you can, at least primitively, </span><em>interact with</em><span>.</span></p><div id="youtube2-PDKhUknuQDg" data-attrs="{&quot;videoId&quot;:&quot;PDKhUknuQDg&quot;,&quot;startTime&quot;:&quot;53&quot;,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/PDKhUknuQDg?start=53&amp;rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p>It is kind of incredible? The reason Genie gets the headliner title over GPT-5 (below) is that Genie is really, truly something different.</p><p><span>Now, you can only really interact with a world Genie creates for a few minutes. But that is a massive step up from where we were previously. And it points to the future. Coherence over long context windows used to be a very difficult problem for language models too – if you've been reading my </span><a href="https://theahura.substack.com/p/ilyas-30-papers-to-carmack-table" rel="">ml paper review series</a><span> you'll know that a solid part of the last 10 years of AI research has been motivated in part by this very problem. Last I checked we did a pretty good job with it; similar progress is possible for world models. More importantly, Genie 3 is now at a point where you can start using it for a wide range of other tasks, including training other models. You don't need to drive millions of miles in a Waymo if you can artificially create long-tail distribution events and train on those!</span></p><p><span>For folks who are interested in the more technical aspects of how this thing works, you're a bit out of luck. Publishing at Google is a bit weird these days. Any research papers that get written up first go into an internal pool. If any of the Gemini product teams want to productionize research out of that pool, the paper doesn't get published. As a result, I suspect we won't see papers for Genie 3 (or even its predecessor, Genie 2) any time soon. Here's the </span><a href="https://arxiv.org/abs/2402.15391" rel="">Genie 1 paper</a><span> though. I'll try and review it soon.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!i96C!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!i96C!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 424w, https://substackcdn.com/image/fetch/$s_!i96C!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 848w, https://substackcdn.com/image/fetch/$s_!i96C!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 1272w, https://substackcdn.com/image/fetch/$s_!i96C!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!i96C!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png" width="1080" height="1031" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1031,&quot;width&quot;:1080,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;r/OpenAI - Sam Altman twitter hype is out of control again. we are not gonna deploy AGI next month, nor have we built it. we have some very cool stuff for you but pls chill and cut your expectations 100x! 11:32 AM · Jan 20, 2025 · 26.9K Views •&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="r/OpenAI - Sam Altman twitter hype is out of control again. we are not gonna deploy AGI next month, nor have we built it. we have some very cool stuff for you but pls chill and cut your expectations 100x! 11:32 AM · Jan 20, 2025 · 26.9K Views •" title="r/OpenAI - Sam Altman twitter hype is out of control again. we are not gonna deploy AGI next month, nor have we built it. we have some very cool stuff for you but pls chill and cut your expectations 100x! 11:32 AM · Jan 20, 2025 · 26.9K Views •" srcset="https://substackcdn.com/image/fetch/$s_!i96C!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 424w, https://substackcdn.com/image/fetch/$s_!i96C!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 848w, https://substackcdn.com/image/fetch/$s_!i96C!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 1272w, https://substackcdn.com/image/fetch/$s_!i96C!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!MCMu!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!MCMu!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 424w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 848w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 1272w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!MCMu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png" width="600" height="302" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:302,&quot;width&quot;:600,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!MCMu!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 424w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 848w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 1272w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!AdQj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!AdQj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 424w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 848w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 1272w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!AdQj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png" width="720" height="342" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:342,&quot;width&quot;:720,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;This tweet by Sam Altman could be very important. Is he talking about  OpenAI? : r/singularity&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="This tweet by Sam Altman could be very important. Is he talking about  OpenAI? : r/singularity" title="This tweet by Sam Altman could be very important. Is he talking about  OpenAI? : r/singularity" srcset="https://substackcdn.com/image/fetch/$s_!AdQj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 424w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 848w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 1272w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>This you?</figcaption></figure></div><p>The big story about GPT-5 is about what it isn't.</p><p>It isn't a world-changing super-intelligent insane-step-up on the intelligence ladder. It isn't God. It isn't close to God.</p><p>Now, if you've been reading my blog for any length of time, you'll know that I didn't really ever suspect OpenAI would be the one to stumble upon God in the machine, even though that is in some sense their explicit purpose. I tend to think Google is going to do it, mostly by accident, and will probably also end up sitting on the research for too long until OpenAI-2-electric-boogaloo comes around and tries to eat their lunch, again.</p><p>But still. There was so much hype around GPT-5, and now all that hype has deflated.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!1RO6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!1RO6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 424w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 848w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 1272w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!1RO6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png" width="961" height="542" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:542,&quot;width&quot;:961,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!1RO6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 424w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 848w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 1272w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>The betting markets were not impressed by GPT-5. I am reading this graph as "there is a high expectation that Google will announce Gemini-3 in August", and not as "Gemini 2.5 is better than GPT-5". EDIT: this may be incorrect — the polymarket is using the style-removed benchmark </span><a href="https://lmarena.ai/leaderboard/text/overall-no-style-control" rel="">here</a><span>, where Gemini 2.5 still ranks higher than GPT-5.</span></figcaption></figure></div><p><span>Starting about a year ago, people began to complain that AI had hit a wall because GPT-5 was not yet released. Some folks (cough Gary cough) were even starting to make claims like "GPT-4 is the best AI we're ever going to get". At the time, </span><a href="https://theahura.substack.com/p/tech-things-gpt-pro-and-the-state" rel="">I pushed back</a><span>, blaming our short attention spans and need for immediate gratification:</span></p><blockquote><p>But is AI stagnating?</p><p>There is a strict sense in which consumer AI may not feel like it's growing at the same rate as it did from 2020 to 2023. That period was a particularly magic time where we had a surplus of chips that we had to catch up to. Like a gas expanding to fill a volume, our chip utilization has caught up, so releases may not be at such a rapid clip.</p><p>Some of the problem here is that consumers are just getting impatient. The first version of GPT3 was published in May, 2020. GPT4 was launched in March, 2023. That’s 34 months. It’s only been ~20 months since GPT4 was released, there’s a bit more time to go before OpenAI starts ‘falling behind schedule’. We haven’t had the capability to even create large enough GPU clusters until recently. And it is also plausible that the release of stronger LLMs tracks more to self driving cars than to iPhones. The hypecycle for self driving cars was at its peak around 2014-2015. Even though the technology wasn’t quite consumer ready by then, the estimated ‘release date’ was still within only a few short years. In 2024 there are readily available self driving cars in several cities. From a research perspective, the folks saying that self driving cars would be ready within a few years of 2014 were more right than those saying it would never be ready at all.</p><p>As for the people who are arguing that AI is obviously dead and the whole field was doomed to failure because it's "just statistics" or "just linear algebra", idk, this feels a lot like shifting goal posts. Standard LLMs are exposed to way less data than the average human baby, the fact that they can do anything at all is a miracle, the fact that they can regularly pass competence tests like the SAT or the Bar should be endlessly awe inspiring. For some reason people keep wondering when we'll have AGI, even though it's literally here and accessible through a web browser. In any case, the cope isn't going to stop the AI from taking everyone's jobs (mine included).</p></blockquote><p>I stand by basically all of what I said. But I also have to eat some of the intent behind my words here. With GPT-5, I was clearly expecting something closer to the step function increase in functionality that we saw between GPT-3 and 4. Unfortunately, we really did hit a serious industry-wide asymptote in our ability to get more out of next-token-prediction. In retrospect, I think GPT-5 was always going to be disappointing. I'm sympathetic to the OpenAI team here, people were expecting literal miracles. But also, Sam definitely played a role in building up hype — and, as a result, increased the mountain OpenAI would have to eventually summit.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!nK2Q!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!nK2Q!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 424w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 848w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 1272w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!nK2Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png" width="745" height="565" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b447decd-22cb-46e6-839b-7e716719f063_745x565.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:565,&quot;width&quot;:745,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:271822,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://theahura.substack.com/i/170425838?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!nK2Q!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 424w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 848w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 1272w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Sam tweeted this the day before the GPT-5 announcement, come on!</figcaption></figure></div><p><span>So what is GPT-5? It's basically GPT-4, but better. It's still early, but it seems to be </span><em>significantly</em><span> more consistent, which is no small feat. OpenAI already has most of the consumer brand recognition; there are many people for whom "LLM" and "AI" are synonymous with "ChatGPT". But I suspect that those of us who swap models frequently will begin to use OpenAI as a daily driver again.</span></p><p><span>This shouldn't be understated. GPT has not been a part of my daily life at all since approximately January of this year, when I fully switched to Claude. And when I switched over to using Gemini for code and Claude for everything else, I took the extra step of uninstalling the ChatGPT app from my phone. More generally, I think there's been a bit of a 'vibe shift' in the Bay and among AI researchers and practitioners. People are starting to realize the sheer weight of Google's TPU farms, while OpenAI talent is getting siphoned off by </span><em>liquid </em><span>billion-dollar offers on one side (e.g. Meta) and </span><em>even more </em><span>ideological startups on the other (e.g. Safe Superintelligence, whatever Mira Murati is up to). Friends who are way more plugged in than I am</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-170425838" href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant#footnote-5-170425838" target="_self" rel="">5</a></span><span> describe an anti-OpenAI "coalition" forming, with many of the folks who had been burned by Sam's aggressive commercialization lining up to give the company a black eye. If you were more social-graph-minded, you may read a lot into Alexandr Wang — Sam Altman's ex-roommate and close confidant — leaving </span><a href="http://scale.ai/" rel="">Scale.AI</a><span> for Meta.</span></p><p><span>In this context, being the best in class is really important. Important people are losing faith, and those important people talk to other important people who have money. OpenAI needs to justify their extremely high valuation and their capex burn. If I’m right </span><a href="https://theahura.substack.com/p/tech-things-gpt-pro-and-the-state" rel="">that LLMs are a winner-take-all game</a><span>, OpenAI has to position itself as the winner.</span></p><p>They may have a tough time doing so though if they can't get their graphs straight. I mean what the hell are these?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Mx83!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Mx83!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 424w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 848w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 1272w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Mx83!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png" width="744" height="824" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:824,&quot;width&quot;:744,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Mx83!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 424w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 848w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 1272w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Ok, ok, that's great and all. But </span><em>what is GPT-5? How does it work?</em><span> Well, OpenAI isn't exactly going to release a public research paper about their latest and greatest. But we can go off the model card.</span></p><blockquote><p>GPT-5 is a unified system with a smart and fast model that answers most questions, a deeper reasoning model for harder problems, and a real-time router that quickly decides which model to use based on conversation type, complexity, tool needs, and explicit intent…In the near future, we plan to integrate these capabilities into a single model.</p></blockquote><p><span>In other words, GPT-5 is a bunch of smaller models in a trenchcoat. I've long believed that many of the consumer-facing web chat interfaces were powered by many models instead of one really big model. It is simply more cost effective. I've written in the past about </span><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" rel="">'the bitter lesson'</a><span>, which can roughly be summarized as "scaling compute and data will lead to more long term progress than hand crafted heuristics and rules". But a natural corollary to the bitter lesson is that, </span><em>for a fixed budget</em><span>, human crafted systems are often more efficient. So here. Unfortunately, there just isn't that much additional information for how it works beyond that.</span></p><p>Even though Anthropic already had their big Claude 4 release a few months ago, they didn't want to feel left out, so they released Claude Opus 4.1. Rather appropriately titled, it really is just a slightly better version of Opus 4. I'll take it.</p><p>Claude hasn't really been at the top of any of the leaderboards for a while. And yet I and many very technical and AI-savvy people continue to use it. This is…somewhat odd? Why do I purposely use a worse model?</p><p><span>I think the short answer is that it's not worse. Teaching to the test is as much a problem in AI as it is in education. I mean this is standard </span><a href="https://en.wikipedia.org/wiki/Goodhart%27s_law" rel="">Goodhart's law</a><span> stuff — the tests are meant to be proxies for competence, not targets. Even though Claude doesn't top leaderboards, it feels better to use. And any other gaps in model quality are simply papered over by Anthropic's focus on the user experience. As a developer, Claude is just way better. The artifact system is great, and the claude code CLI is a seamless experience.</span></p><p>A friend of mine described Anthropic as the Apple to OpenAI's Microsoft. And, like, yea, I see it. I guess Google is still just Google in this metaphor, idk.</p><p>I feel like every other month I hear about some random famous tech person raising a bajillion dollars to start an AI company. John Carmack raised $20M in 2022. Ilya raised $1b in 2024, and then another $2b a few months ago. Mira Murati raised $2b in July.</p><p>Ilya's company is valued at $32b. As far as I can tell, the only thing it has produced is the 370 words on its website. That is $86,486,486.49 per word. And 148 of those words are about Daniel Gross stepping away from the company. At least Ilya has a website, Carmack has literally disappeared.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!8fQm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8fQm!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 424w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 848w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 1272w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!8fQm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png" width="649" height="748" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:748,&quot;width&quot;:649,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8fQm!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 424w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 848w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 1272w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>apparently he's been hitting the gym</figcaption></figure></div><p>What is going on! Where are all of these people? What happened to the billions of dollars? Do they realize how many taco bell burritos you can buy with a billion dollars? Where are these people??? If any of you know what is going on at any of these places, please tell me.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Astronomy Photographer of the Year 2025 shortlist (224 pts)]]></title>
            <link>https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/2025-shortlist</link>
            <guid>44837434</guid>
            <pubDate>Fri, 08 Aug 2025 14:29:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/2025-shortlist">https://www.rmg.co.uk/whats-on/astronomy-photographer-year/galleries/2025-shortlist</a>, See on <a href="https://news.ycombinator.com/item?id=44837434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-off-canvas-main-canvas="">
    
  

  

  




  

  

  

  <main role="main">
    
    <div id="block-rmg-theme-content">
  
    
      <article data-content-type="topic">

  
    

  
  <div>
              <div>
        
                
            <p>The shortlist for the ZWO <a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year" data-gtm-name="CTA" data-gtm-detail="formatted content">Astronomy Photographer of the Year</a> 2025 competition has been unveiled.</p><p>From a blood moon hanging over Shanghai to a family portrait of the Solar System and a close-up of a comet's streaming tails, distant astronomical wonders are photographed in magnificent detail for all to admire.</p><p>Now in its 17th year, in 2025 the competition received a record number of entries, with just over <span>5,880 photographs submitted from 68 different countries.</span></p><p><span>See a small selection of shortlisted images below, and stay tuned to discover this year's full shortlist, winners and runners-up at a </span><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/2025-awards-ceremony-live" data-gtm-name="CTA" data-gtm-detail="formatted content"><span>special online awards ceremony on 11 September</span></a><span>.</span></p>
      
      </div>
              <div>
        <h2>
            Keep up to date with the competition
      </h2>        
            <p>Sign up to our space newsletter for exclusive astronomy news,&nbsp;guides and events, and be among the first to see this year's Astronomy Photographer of the Year winners</p>
      
      </div>
              <div>
              
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Blood%20Moon%20Rising%20Behind%20the%20City%20Skyscrapers%20%C2%A9%20Tianyao%20Yang.jpg.webp?itok=HqPGwVH0" width="900" height="1200" alt="Huge blood red moon rising at night behind Shanghai skyscrapers" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Tianyao Yang
      </span>
                  </p></div>
      <div>
                  <h3>
            Blood Moon Rising Behind the City Skyscrapers by Tianyao Yang
      </h3>
                <div>
          
            <p><em><span>Jiading District, Shanghai, China</span></em></p><p><span>This photograph captures a red Full Moon rising beside Shanghai’s tallest skyscrapers in Lujiazui. Taken from a distance of 26.5 km (16.5 miles) from the skyscrapers in a single exposure, this image’s alignment took five years of planning. The Full Moon appears perfectly positioned next to the illuminated skyline, creating a striking contrast.&nbsp;</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/A-270201-2025-5_The_Arctic_Flower.jpg.webp?itok=bEz--Lma" width="802" height="1200" alt="Photo showing icy snowy mountain landscape with vast sky above filled with green and purple aurorae, at the top is a large firework-shape aurora in green and purple" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Vincent Beudez
      </span>
                  </p></div>
      <div>
                  <h3>
            The Arctic Flower by Vincent Beudez
      </h3>
                <div>
          
            <p><em><span>Sjursnes, Tromsø, Norway</span></em></p><p>In April, there is no ‘true’ night in northern Norway. This is why the Northern Lights look much more blue than usual. Vincent Beudez captured the visually pleasing aurora shape above the Norwegian background.</p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Gateway%20to%20the%20Galaxy%20%C2%A9%20Yujie%20Zhang.jpg.webp?itok=CgE4EpnW" width="1200" height="971" alt="Vivid Milky Way core vertical in sky over sculpture of black stones with water in foreground" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Yujie Zhang
      </span>
                  </p></div>
      <div>
                  <h3>
            Gateway to the Galaxy by Yujie Zhang
      </h3>
                <div>
          
            <p><em><span>Songyang County, China</span></em></p><p><span>Under the night sky, several black geometric buildings appear to stand on the water’s surface, resembling gateways to the galaxy. The bright Milky Way stretches across the sky behind them, with stars twinkling. The reflections of the buildings shimmer in the water, blending reality and illusion, as if opening a passage to the mysteries of the Universe, inspiring endless reverie and a longing to explore the vast starry sky.&nbsp;</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/OS-255963-2025-1_500%2C000_km_Solar_Prominence_Eruption_Captured_in_Full.jpg.webp?itok=TBjaG3-w" width="1088" height="1200" alt="Photo of the Sun in bright oranges and yellows with a solar prominence coming out of the bottom of it" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © PengFei Chou
      </span>
                  </p></div>
      <div>
                  <h3>
            500,000-km Solar Prominence Eruption by PengFei Chou
      </h3>
                <div>
          
            <p><em><span>Eastern New District, Xinxing County, Guangdong province</span></em></p><p><span>On 7 November 2024, the Sun experienced a massive solar prominence eruption, with a length exceeding 500,000 km (311,000 miles). The eruption lasted approximately one hour from its initial outburst to its conclusion. The eruption phase of the prominence is composed of more than 20 stacked data sets highlighting the entire process of this spectacular event.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-07/G-372747-2025-7_8_Panels_Mosaic_of_M31%2C_resolved_stars%2C_nebula_and_central_bulge..jpg.webp?itok=gs5Hx8es" width="1200" height="836" alt="Photo of Andromeda Galaxy up close in vivid reds and purples" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Chuhong Yu, Jingyao Hong, Xi Zhu, Yaguang Wan
      </span>
                  </p></div>
      <div>
                  <h3>
            Eight-Panel Mosaic of M31: Stars, Nebulae and Central Bulge by Chuhong Yu, Jingyao Hong, Xi Zhu, Yaguang Wan
      </h3>
                <div>
          
            <p><em><span>Daocheng County, Garzê Tibetan Autonomous Prefecture, Sichuan, China</span></em></p><p><span>This image shows countless resolved stars, emission nebula and a mysterious central bulge. The photo is incredibly detailed, the mist surrounding the galaxy is actually tens of thousands of yellowish tiny stars.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Into%20the%20Past%20%C2%A9%20Jim%20Hildreth.jpg.webp?itok=xri1UE1g" width="1200" height="600" alt="Desolate, dry and cracked Utah landscape, below a starry Milky Way arching in the sky in purples, blues and oranges" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Jim Hildreth 
      </span>
                  </p></div>
      <div>
                  <h3>
            Into the Past by Jim Hildreth
      </h3>
                <div>
          
            <p><em>Moonscape Overlook, Wayne County, Utah, USA</em></p><p><span>This impressive panorama is a view from the Utah desert. 23,000 pixels wide, the photograph shows the desolate, character rich landscape, below a starry Milky Way.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Moonrise%20Perfection%20Over%20the%20Dolomites%20%C2%A9%20Fabian%20Dalpiaz.jpg.webp?itok=nAIpIMYp" width="1200" height="800" alt="Photo showing top of mountain range in the Dolomites in Italy, with the Moon fitting neatly in a groove in the mountain" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Fabian Dalpiaz
      </span>
                  </p></div>
      <div>
                  <h3>
            Moonrise Perfection Over the Dolomites by Fabian Dalpiaz
      </h3>
                <div>
          
            <p><em><span>Santuario di Pietralba, Deutschnofen, South Tyrol, Italy</span></em></p><p><span>The </span><a href="https://www.rmg.co.uk/stories/space-astronomy/full-moon-calendar-2025" data-entity-type="node" data-entity-uuid="5b407ae4-0c46-4f0d-bb4f-8d563d647d2a" data-entity-substitution="canonical" title="Full Moon calendar 2025" data-gtm-name="CTA" data-gtm-detail="formatted content"><span>full Moon</span></a><span> rises above the rugged peaks of the Dolomites. With no clouds in sight and in flawless conditions, the golden light of sunset bathes the mountains, creating harmony between Earth and sky.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/S-128936-2025-3_Dragon_Tree_Trails.jpg.webp?itok=y5mnDM6c" width="1200" height="960" alt="Photo of lone tree in the centre of a flat landscape with distant hill, in the sky are multicoloured star trails forming a perfect circle around the tree" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Benjamin Barakat
      </span>
                  </p></div>
      <div>
                  <h3>
            Dragon Tree Trails by Benjamin Barakat
      </h3>
                <div>
          
            <p><em><span>Firmihin Forest, Hidaybu District, Yemen</span></em></p><p><span>A solitary dragon tree stands tall in the heart of Socotra’s Dragon Blood Tree forest – an otherworldly landscape unlike anywhere else on Earth. The final image is composed of 300 individual exposures.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Total%20Solar%20Eclipse%20%C2%A9%20Louis%20Egan.jpg.webp?itok=6TyBWkYi" width="1200" height="235" alt="Photo showing progression of Moon moving in front of the Sun incrementally, with a total eclipse in the middle" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Louis Egan
      </span>
                  </p></div>
      <div>
                  <h3>
            Total Solar Eclipse by Louis Egan
      </h3>
                <div>
          
            <p><em><span>Shortlisted in ZWO Young Astronomy Photographer of the Year. Coaticook, Quebec, Canada</span></em></p><p>This 22-megapixel panorama shows the different stages of the full <a href="https://www.rmg.co.uk/stories/space-astronomy/solar-eclipses-explained" data-entity-type="node" data-entity-uuid="7a208546-504d-4e07-9e2f-8c9af23e8058" data-entity-substitution="canonical" title="Solar eclipses explained" data-gtm-name="CTA" data-gtm-detail="formatted content">solar eclipse</a>, with a high dynamic range (HDR) image of totality in the middle. This reveals both the bright corona and finer details otherwise lost in standard exposures. The final image uses approximately 200 images with varying exposure times to create a HDR totality, before combining everything together.&nbsp;</p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Comet%20Over%20Waikiki%20%C2%A9%20Ran%20Shen.jpg.webp?itok=1t1VTiy2" width="800" height="1200" alt="Photo showing lit up city landscape with a large white comet in the sky" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Ran Shen
      </span>
                  </p></div>
      <div>
                  <h3>
            Comet Over Waikiki by Ran Shen
      </h3>
                <div>
          
            <p><em>Honolulu, Hawaii, USA</em></p><p><span>Taken on the evening of 12 October 2024 at Pu'u O Kaimukī Park, Ran Shen joined many residents and astrophotographers in Honolulu, Hawaii, to witness the passage of Comet C/2023 A3 (Tsuchinshan-ATLAS), one of the most anticipated astronomical events of the year.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-07/G-68776-2025-6_Fireworks.jpg.webp?itok=mPO0tkBa" width="1200" height="719" alt="Photo of M33, the Triangulum Galaxy, resembling fireworks with bright spots of white, purple and reds against black background" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Bence Tóth, Péter Feltóti, Bertalan Kecskés
      </span>
                  </p></div>
      <div>
                  <h3>
            Fireworks by Bence Tóth, Péter Feltóti and Bertalan Kecskés
      </h3>
                <div>
          
            <p><em><span>Sződliget, Pest and Törökkoppány, Somogy, Hungary</span></em></p><p><span>The image shows M33, the Triangulum Galaxy, from a new perspective. Due to tidal interaction with M31, there is very prominent star-forming activity in M33, which results in a spectacular structure of emission nebulae. During processing, a separate SHO picture was created with a strong SII/H-alpha presence, the glowing red structures in the picture, and blended with a high-resolution LRGB processing of the continuum data, representing the ‘background’ light.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Solar%20System%20Portrait%20%C2%A9%20Sophie%20Paulin.jpg.webp?itok=NDLLDyLk" width="1200" height="803" alt="Long landscape image showing all planets in the Solar System" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Sophie Paulin
      </span>
                  </p></div>
      <div>
                  <h3>
            Solar System Portrait by Sophie Paulin
      </h3>
                <div>
          
            <p><em><span>Bobingen, Bavaria, Germany</span></em></p><p><span>This image presents all the planets of our Solar System, excluding Earth, showcasing their unique characteristics. Mercury, the closest to the Sun, is a barren, cratered world, while Venus is shrouded in thick clouds. Mars, the Red Planet, has vast deserts and the largest volcano in the Solar System. The gas giants, Jupiter and Saturn, dominate with their immense size and swirling storms, while Saturn’s rings make it especially striking. Uranus and Neptune, the ice giants, are rich in methane, giving them their blue hue.&nbsp;</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/A-206876-2025-1_Auroroa%20Over%20Mono%20Lake%3B%20a%20Rare%20Dance%20of%20Light.jpg.webp?itok=k7yLkKhh" width="1200" height="800" alt="Photo showing landscape, on the bottom half is a lake with rock protrusions and in the top half is the sky with bright green and purple aurorae, which is reflected in the lake" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Daniel Zafra
      </span>
                  </p></div>
      <div>
                  <h3>
            Aurora Over Mono Lake: A Rare Dance of Light by Daniel Zafra
      </h3>
                <div>
          
            <p><em><span>Mono Lake, Mono County, USA</span></em></p><p><span>This photograph captures the rare occurrence of </span><a href="https://www.rmg.co.uk/stories/space-astronomy/what-causes-northern-lights-aurora-borealis-explained" data-entity-type="node" data-entity-uuid="eef7b0b8-8e1b-4b1e-b3fc-633b7b405305" data-entity-substitution="canonical" title="What causes the Northern Lights? Aurora borealis explained" data-gtm-name="CTA" data-gtm-detail="formatted content"><span>Northern Lights</span></a><span> in California. Vibrant ribbons of magenta and green light up the sky, reflecting in the still waters among the rock formations.&nbsp;&nbsp;&nbsp;&nbsp;</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/NGC%206164%20and%20NGC%206165%20The%20Dragon%27s%20Egg%20%C2%A9%20Charles%20Pevsner.jpg.webp?itok=IyURF4ch" width="1200" height="800" alt="Bright star in the centre of a small egg-shaped pink nebula surrounded by protrusions of purple nebulae clouds, against a deep purple sky smattered with stars" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Charles Pevsner 
      </span>
                  </p></div>
      <div>
                  <h3>
            NGC 6164 and NGC 6165: The Dragon's Egg by Charles Pevsner
      </h3>
                <div>
          
            <p><em><span>Deep Sky Chile Observatory, Camino del Observatorio, Río Hurtado, Chile</span></em></p><p><span>At the centre of this image is the bright star HD148937, part of a luminous triple-star system at the centre of the Dragon’s Egg Nebula (NGC 6164 and 6165) that lights up the nebula structure. Charles Pevsner was originally attracted to this target because of the striking symmetry of the magenta lobes of the Dragon’s Egg, but his favourite element ended up being the wispy outer shell.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/OM-423073-2025-5_Moonrise_on_Villebois-Lavalette.jpg.webp?itok=wwODG6F8" width="1200" height="668" alt="Landscape photo of French town called Villebois-Lavalette with a large orange moon in the sky with the top half visible" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Flavien Beauvais&nbsp;
      </span>
                  </p></div>
      <div>
                  <h3>
            Moonrise Over Villebois-Lavalette by Flavien Beauvais
      </h3>
                <div>
          
            <p><em><span>La Font Aride, Saint-Amant-de-Montmoreau, France</span></em></p><p><span>This unique photograph was taken 6.4 km (4 miles) from the château of Villebois-Lavalette, just north of Bordeaux. The distortions are related to the distance between the imaged Moon and the foreground but also with respect to the atmospheric disturbance, hence the curves on the surface of the Moon.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Close-up%20of%20a%20Comet%20%C2%A9%20Gerald%20Rhemann%20and%20Michael%20J%C3%A4ger.jpg.webp?itok=Hbk_-H0L" width="807" height="1200" alt="Photo showing bright vivid comet streaming in white, with another tail in bright blue" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Gerald Rhemann and Michael Jäger
      </span>
                  </p></div>
      <div>
                  <h3>
            Close-up of a Comet by Gerald Rhemann and Michael Jäger
      </h3>
                <div>
          
            <p><em><span>Tivoli Astrofarm, Windhoek Rural, Namibia</span></em></p><p><span>The photographers travelled to Namibia to view Comet C/2023 A3 (Tsuchinshan-ATLAS) in the southern hemisphere. Due to the angle of the observation, the dust and ion tails seem to have overlapped, but the impact of solar winds on the day caused noticeable kinks in the ion tail.&nbsp;</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/S-245523-2025-1_Cave_of_Stars.jpg.webp?itok=bLdwTF7z" width="960" height="1200" alt="Photo taken from inside a cave looking out on a seascape with the Milky Way diagonal in the sky above" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Yoshiki Abe
      </span>
                  </p></div>
      <div>
                  <h3>
            Cave of Stars by Yoshiki Abe
      </h3>
                <div>
          
            <p><em><span>Nagato, Yamaguchi, Japan</span></em></p><p><span>Realising that it was possible to photograph the Milky Way from this remote cave, Yoshiki Abe waited for the perfect conditions to take the image. This is a composite photograph. Both parts were taken on the same night and at the same location, but the foreground was shot during the blue hour then the tripod was shifted to capture the Milky Way.&nbsp;</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/OM-328097-2025-4_Lunar_Occultation_of_Saturn.jpg.webp?itok=TsF7EaBX" width="1200" height="1112" alt="Photo of the Moon which fades into darkness at the top, with a dotted line to the left which disappears behind the left limb of the moon momentarily" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Chayaphon Phanitloet
      </span>
                  </p></div>
      <div>
                  <h3>
            Lunar Occultation of Saturn by Chayaphon Phanitloet
      </h3>
                <div>
          
            <p><em><span>Bua Yai, Bua Yai District, Nakhon Ratchasima, Thailand</span></em></p><p><span>This is a composite image that brings images of both the Moon and Saturn together to show the lunar occultation of Saturn. A lunar occultation of Saturn occurs when the Moon passes in front of Saturn, temporarily blocking its light from Earth. This event is brief and can be observed as the Moon obscures the planet.</span><em><span>&nbsp;</span></em></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-07/Progression%20of%20Baily%27s%20Beads%20%C2%A9%20Damien%20Cannane.jpg.webp?itok=nnmzEN-Z" width="1200" height="400" alt="Long photo showing progression of solar eclipse, with baily's beads appearing" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Damien Cannane
      </span>
                  </p></div>
      <div>
                  <h3>
            Progression of Baily's Beads by Damien Cannane
      </h3>
                <div>
          
            <p><em><span>Dexter, Missouri, USA</span></em></p><p><span>Baily’s Beads are bright spots around the Moon during a solar eclipse that are caused by sunlight passing through lunar valleys. This composite shows the progression, from left to right, from the first ‘diamond ring’ – a moment when one last bright point of sunlight shines beside the faint corona, resembling a diamond on a ring – fading through Baily's Beads into totality and beyond until a 'diamond ring' occurs again as the Sun starts to reappear.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Electric%20Threads%20of%20the%20Lightning%20Spaghetti%20Nebula%20%C2%A9%20Shaoyu%20Zhang.jpg.webp?itok=rwE5ERqC" width="1200" height="800" alt="Vivid photo of a nebula in reds, purples and blues, resembling a large bubble" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            © Shaoyu Zhang
      </span>
                  </p></div>
      <div>
                  <h3>
            Electric Threads of the Lightning Spaghetti Nebula by Shaoyu Zhang
      </h3>
                <div>
          
            <p><em><span>Deep Sky Chile Observatory, Río Hurtado, Chile and Xiangcheng, Garzê Tibetan Autonomous Prefecture, Sichuan, China</span></em></p><p><span>This full-spectrum image of the Spaghetti Nebula unveils the faint and elusive nature of this supernova remnant (SNR), hidden behind a vast cloud of dust that obstructs its emission light. To enhance its visual appeal, Shaoyu Zhang dedicated considerable time to capturing OIII data, intensifying the blue and green hues, while allowing SII and H-alpha to support high dynamic range stretching for added depth.&nbsp;</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
            <div>
          <div>
            
            <article>
  
      
  <div>
    <p>Image</p>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/large/public/2025-06/Neon%20Sun%20%C2%A9%20Peter%20Ward.jpg.webp?itok=gcrlOnCM" width="1200" height="1200" alt="Square image showing black background on which is a ring in purples and greens" loading="lazy">


</p>
          </div>

  </article>

      
          </div>
                      <p><span>
            Data from NASA, processed by Peter Ward
      </span>
                  </p></div>
      <div>
                  <h3>
            Neon Sun by Peter Ward
      </h3>
                <div>
          
            <p><em>Shortlisted in the </em><a href="https://www.rmg.co.uk/whats-on/astronomy-photographer-year/competition/annie-maunder-open-category" data-entity-type="node" data-entity-uuid="910d9a7d-642b-4398-9d7f-1e0d94d625ae" data-entity-substitution="canonical" title="The Annie Maunder Open Category" data-gtm-name="CTA" data-gtm-detail="formatted content"><em>Annie Maunder Open Category</em></a><em>. Original data from NASA SDO 171, 193, 304 nanometre from 1 June 2024&nbsp;</em></p><p><span>The data from NASA’s Solar Dynamics Observer (SDO) probe was used here to show the Sun’s inner corona in a way that hints at a process similar to that which energises colourful neon lights on Earth. Images taken by the SDO in the ultraviolet spectrum were remapped to a more vibrant palette, with the same coronal data turned ‘inside out’ to surround the Sun, creating the illusion of it being enclosed in a neon tube. The data was then polar inversed to mirror the inner coronal image and colour saturation was increased.</span></p>
      
                      
                  </div>
      </div>
    </div>
              <div>
                  <h2>
            Learn more about Astronomy Photographer of the Year
      </h2>
                        <div>
                                                <div>
                  <p><img src="https://www.rmg.co.uk/sites/default/files/styles/slider_image/public/2024-06/OS-2913-85%20Gigantic%20Solar%20Prominence%20in%20Motion%20from%2029th%20January%202023.jpg.webp?itok=GFbL8bGu" alt="" loading="lazy">
                                                          </p>
                  <div>
                      
                                            <p>
                        See the remarkable shortlisted and winning images from 2024's competition for free at the National Maritime Museum, open until 11 August.
                      </p>
                    </div>
                                    
                                  </div>
                                                            <div>
                  <p><img src="https://www.rmg.co.uk/sites/default/files/styles/slider_image/public/2024-08/Arctic%20Dragon%20%C2%A9%20Carina%20Letelier%20Baeza%20-%20Astronomy%20Photographer%20of%20the%20Year%202024%20Aurorae.jpg.webp?itok=sZF5QeGz" alt="" loading="lazy">
                                                          </p>
                  <div>
                      
                                            <p>
                        Explore the winning and shortlisted images from previous years of Astronomy Photographer of the Year
                      </p>
                    </div>
                                    
                                  </div>
                                                            <div>
                  <div>
                     <p><img src="https://cdn.shopify.com/s/files/1/2459/8861/files/Collection-14-cover_bf105e6d-909b-4cb4-9d71-8ad23e7a1bad.jpg?v=1751968077" alt="" loading="lazy"></p><p>Shop</p>                                                                                </div>
                  <div>
                      
                                              <p>
                                                                                    £30.00
                                                                              </p>
                                            <p>
                        Note: This title is currently available to pre-order. Pre-orders will be dispatched on 12 September, ahead of the general publication release date of 25 September 2025...
                      </p>
                    </div>
                                    
                                  </div>
                              </div>
      </div>
              <div>
      <div>
      <h2>
            Read more
      </h2>
        <p>
            Explore the universe with Royal Observatory Greenwich astronomers and curators.
      </p>
  </div>
      <div>
                  
                  
                  
                  <div>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/slider_image/public/2024-11/vlcsnap-2024-11-21-14h07m48s961.jpg.webp?itok=jB6fJmIn" alt="" loading="lazy">
              </p>
              <div>
                <h3>
                  <a href="https://www.rmg.co.uk/stories/space-astronomy/finding-community-through-astrophotography-astronomy-photographer-year">Finding community through astrophotography</a>
                </h3>
                <p>
                  Sophie Paulin and Tom Williams struck up a friendship in an online astrophotography forum. Discover how they combined their expertise to win a prize in Astronomy Photographer of the Year
                </p>
              </div>
            </div>
                  
                  
                  <div>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/slider_image/public/migrations/PS-43711-19_The%20Dreamlike%20Sky%20Above%20the%20Sea%20of%20Clouds%20%C2%A9%20Likai%20Lin.jpg.webp?itok=jUNzE3Xt" alt="" loading="lazy">
              </p>
              <div>
                <h3>
                  <a href="https://www.rmg.co.uk/stories/space-astronomy/astronomy-naked-eye">Astronomy with the naked eye</a>
                </h3>
                <p>
                  Learn what you could see in the night sky with no equipment from the Royal Observatory Greenwich; from galaxies and meteor showers to comets, star clusters, cloud formations and more
                </p>
              </div>
            </div>
                  
                  
              </div>
      

    </div>
              <div>
      <div>
                      <p>
      <h2>
            Sponsors and supporters
      </h2>
    </p>
                    </div>
      <div>
                              <div>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/logo/public/2025-06/zwo%20x%20seestar%20-%20vertical_0.png.webp?itok=cOOIHojk" width="169" height="96" alt="Logo that says ZWO x Seestar" loading="lazy">



      
</p>
            </div>
                      <div>
              <p><img src="https://www.rmg.co.uk/sites/default/files/styles/logo/public/2022-06/BBC%20Sky%20At%20Night%20logo.png.webp?itok=BFOpS0pz" width="193" height="96" alt="BBC Sky at Night logo in black" loading="lazy">



      
</p>
            </div>
                        </div>
    </div>
              <div>
        
                
            <p>Header image:<em> NGC 6164 and NGC 6165: The Dragon's Egg</em> © Charles Pevsner</p>
      
      </div>
          </div>

</article>


  </div>  </main>

  
    

  

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting good results from Claude code (379 pts)]]></title>
            <link>https://www.dzombak.com/blog/2025/08/getting-good-results-from-claude-code/</link>
            <guid>44836879</guid>
            <pubDate>Fri, 08 Aug 2025 13:45:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dzombak.com/blog/2025/08/getting-good-results-from-claude-code/">https://www.dzombak.com/blog/2025/08/getting-good-results-from-claude-code/</a>, See on <a href="https://news.ycombinator.com/item?id=44836879">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="gh-main">
    <article>

        <header>
            <span>
                Posted <time datetime="2025-08-08">08 Aug 2025</time>
                    in
                    <a href="https://www.dzombak.com/blog/tag/ai/">AI</a>
            </span>

            


            
        </header>

        <div>




            <p>I've been experimenting with LLM programming agents over the past few months. Claude Code has become my favorite.</p><p>It is not without issues, but it's allowed me to write ~12 programs/projects in relatively little time, and I feel I would not have been able to do all this in the same amount of time without it. Most of them, I wouldn't even have bothered to write without Claude Code, simply because they'd take too much of my time. (A list is included at the end of this post.)</p><p>I'm still far from a Claude Code expert, and I have a backlog of blog posts and documentation to review that might be useful. But — and this is critical — you don't have to read everything that's out there to start seeing results. You don't even need to read <em>this</em> post; just type some prompts in and see what comes out.</p><p>That said, because I just wrote this up for a job application, <strong>here's how I'm getting good results from Claude Code</strong>. I've embedded links to some examples where appropriate.</p><ul><li>A key is writing a clear spec ahead of time, which provides context to the agent as it works in the codebase.</li><li>Having a document for the agent that outlines the project’s structure and how to run e.g. builds and linters is helpful.</li><li>Asking the agent to perform a code review on its own work is surprisingly fruitful.</li><li>Finally, I have a personal “global” agent guide describing best practices for agents to follow, specifying things like problem-solving approach, use of TDD, etc. <em>(This file is listed near the end of this post.)</em></li></ul><p>Then there's the question of <strong>validating LLM-written code.</strong></p><p>AI-generated code <em>is</em> often incorrect or inefficient.</p><p>It’s important for me to call out that <strong>I believe I’m ultimately responsible for the code that goes into a PR with my name on it, regardless of how it was produced</strong>.</p><p>Therefore, especially in any professional context, I manually review all AI-written code and test cases. I’ll add test cases for anything I think is missing or needs improvement, either manually or by asking the LLM to write those cases (which I then review).</p><p>At the end of the day, manual review is necessary to verify that behavior is implemented correctly and tested properly.</p><h2 id="personal-global-agent-guide">Personal "global" agent guide</h2><p>This lives at <code>~/.claude/CLAUDE.md</code>:</p><pre><code># Development Guidelines

## Philosophy

### Core Beliefs

- **Incremental progress over big bangs** - Small changes that compile and pass tests
- **Learning from existing code** - Study and plan before implementing
- **Pragmatic over dogmatic** - Adapt to project reality
- **Clear intent over clever code** - Be boring and obvious

### Simplicity Means

- Single responsibility per function/class
- Avoid premature abstractions
- No clever tricks - choose the boring solution
- If you need to explain it, it's too complex

## Process

### 1. Planning &amp; Staging

Break complex work into 3-5 stages. Document in `IMPLEMENTATION_PLAN.md`:

```markdown
## Stage N: [Name]
**Goal**: [Specific deliverable]
**Success Criteria**: [Testable outcomes]
**Tests**: [Specific test cases]
**Status**: [Not Started|In Progress|Complete]
```
- Update status as you progress
- Remove file when all stages are done

### 2. Implementation Flow

1. **Understand** - Study existing patterns in codebase
2. **Test** - Write test first (red)
3. **Implement** - Minimal code to pass (green)
4. **Refactor** - Clean up with tests passing
5. **Commit** - With clear message linking to plan

### 3. When Stuck (After 3 Attempts)

**CRITICAL**: Maximum 3 attempts per issue, then STOP.

1. **Document what failed**:
   - What you tried
   - Specific error messages
   - Why you think it failed

2. **Research alternatives**:
   - Find 2-3 similar implementations
   - Note different approaches used

3. **Question fundamentals**:
   - Is this the right abstraction level?
   - Can this be split into smaller problems?
   - Is there a simpler approach entirely?

4. **Try different angle**:
   - Different library/framework feature?
   - Different architectural pattern?
   - Remove abstraction instead of adding?

## Technical Standards

### Architecture Principles

- **Composition over inheritance** - Use dependency injection
- **Interfaces over singletons** - Enable testing and flexibility
- **Explicit over implicit** - Clear data flow and dependencies
- **Test-driven when possible** - Never disable tests, fix them

### Code Quality

- **Every commit must**:
  - Compile successfully
  - Pass all existing tests
  - Include tests for new functionality
  - Follow project formatting/linting

- **Before committing**:
  - Run formatters/linters
  - Self-review changes
  - Ensure commit message explains "why"

### Error Handling

- Fail fast with descriptive messages
- Include context for debugging
- Handle errors at appropriate level
- Never silently swallow exceptions

## Decision Framework

When multiple valid approaches exist, choose based on:

1. **Testability** - Can I easily test this?
2. **Readability** - Will someone understand this in 6 months?
3. **Consistency** - Does this match project patterns?
4. **Simplicity** - Is this the simplest solution that works?
5. **Reversibility** - How hard to change later?

## Project Integration

### Learning the Codebase

- Find 3 similar features/components
- Identify common patterns and conventions
- Use same libraries/utilities when possible
- Follow existing test patterns

### Tooling

- Use project's existing build system
- Use project's test framework
- Use project's formatter/linter settings
- Don't introduce new tools without strong justification

## Quality Gates

### Definition of Done

- [ ] Tests written and passing
- [ ] Code follows project conventions
- [ ] No linter/formatter warnings
- [ ] Commit messages are clear
- [ ] Implementation matches plan
- [ ] No TODOs without issue numbers

### Test Guidelines

- Test behavior, not implementation
- One assertion per test when possible
- Clear test names describing scenario
- Use existing test utilities/helpers
- Tests should be deterministic

## Important Reminders

**NEVER**:
- Use `--no-verify` to bypass commit hooks
- Disable tests instead of fixing them
- Commit code that doesn't compile
- Make assumptions - verify with existing code

**ALWAYS**:
- Commit working code incrementally
- Update plan documentation as you go
- Learn from existing implementations
- Stop after 3 failed attempts and reassess
</code></pre><h2 id="projects-written-using-claude-code">Projects written using Claude Code</h2><figure><a href="https://github.com/cdzombak/xrp?ref=dzombak.com"><div><p>GitHub - cdzombak/xrp: HTML/XML aware reverse proxy</p><p>HTML/XML aware reverse proxy. Contribute to cdzombak/xrp development by creating an account on GitHub.</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-24.svg" alt=""><span>cdzombak</span></p></div><p><img src="https://www.dzombak.com/content/images/thumbnail/xrp" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/dzsolarized-vscode?ref=dzombak.com"><div><p>GitHub - cdzombak/dzsolarized-vscode: Solarized variant for VS Code (light + dark modes supported)</p><p>Solarized variant for VS Code (light + dark modes supported) - cdzombak/dzsolarized-vscode</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-25.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://www.dzombak.com/content/images/thumbnail/dzsolarized-vscode-1" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/flickr-rss?ref=dzombak.com"><div><p>GitHub - cdzombak/flickr-rss: Generate an RSS feed of a Flickr photostream or your Friends &amp; Family feed</p><p>Generate an RSS feed of a Flickr photostream or your Friends &amp; Family feed - cdzombak/flickr-rss</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-26.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://www.dzombak.com/content/images/thumbnail/flickr-rss-1" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/lychee-meta-tool?ref=dzombak.com"><div><p>GitHub - cdzombak/lychee-meta-tool: Quickly find &amp; edit untitled photos in your Lychee photo library</p><p>Quickly find &amp; edit untitled photos in your Lychee photo library - cdzombak/lychee-meta-tool</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-27.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://opengraph.githubassets.com/0bc054b50f02fae2565fbd58e0233229978d67ef59f2d3c3a2c7584f3d2250ce/cdzombak/lychee-meta-tool" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/macos-screenlock-mqtt?ref=dzombak.com"><div><p>GitHub - cdzombak/macos-screenlock-mqtt: Report macOS screen lock status to an MQTT broker</p><p>Report macOS screen lock status to an MQTT broker. Contribute to cdzombak/macos-screenlock-mqtt development by creating an account on GitHub.</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-28.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://opengraph.githubassets.com/2eac4061a0425efd3045d26d6ea8c3827670f1d8af1fad2e3a528df8193aebe8/cdzombak/macos-screenlock-mqtt" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/lychee-birb-title?ref=dzombak.com"><div><p>GitHub - cdzombak/lychee-birb-title: Set titles for Bird Buddy photos in your Lychee photo library</p><p>Set titles for Bird Buddy photos in your Lychee photo library - cdzombak/lychee-birb-title</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-29.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://opengraph.githubassets.com/60865f263a542ad9f7bc543702fd15ae3de260090e3e35edca63e6492d97da49/cdzombak/lychee-birb-title" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/lychee-ai-organizer?ref=dzombak.com"><div><p>GitHub - cdzombak/lychee-ai-organizer: Use local LLMs to organize your unsorted photos in Lychee</p><p>Use local LLMs to organize your unsorted photos in Lychee - cdzombak/lychee-ai-organizer</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-30.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://opengraph.githubassets.com/cfd5ea252de7e2e859a8b94d02f2cbaeb1d0ce6c05ad5c5f45b8aed5d3441052/cdzombak/lychee-ai-organizer" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/mac-install?ref=dzombak.com"><div><p>GitHub - cdzombak/mac-install: Idempotent software suite installer for macOS</p><p>Idempotent software suite installer for macOS. Contribute to cdzombak/mac-install development by creating an account on GitHub.</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-31.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://opengraph.githubassets.com/919d1e0000548235c07554e25b7b35eef8822b5989895b6ccef3ca60f542e930/cdzombak/mac-install" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/rss.church?ref=dzombak.com"><div><p>GitHub - cdzombak/rss.church: I Believe in RSS</p><p>I Believe in RSS. Contribute to cdzombak/rss.church development by creating an account on GitHub.</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-32.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://opengraph.githubassets.com/cf321c8e07a66765223c314ac40b5887dd2cf5f4c56150621e4ee3ffb83fa708/cdzombak/rss.church" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/flickr-exporter?ref=dzombak.com"><div><p>GitHub - cdzombak/flickr-exporter: Export all your Flickr photos, or a selected set or collection, preserving title/description/tags and other metadata.</p><p>Export all your Flickr photos, or a selected set or collection, preserving title/description/tags and other metadata. - cdzombak/flickr-exporter</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-33.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://opengraph.githubassets.com/5afb4c8d55872fcdceca3c34a1dfa299f558c4d195991224e95c39e516e76dae/cdzombak/flickr-exporter" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://github.com/cdzombak/gallerygen?ref=dzombak.com"><div><p>GitHub - cdzombak/gallerygen: Generate a static HTML gallery from a directory tree of images</p><p>Generate a static HTML gallery from a directory tree of images - cdzombak/gallerygen</p><p><img src="https://www.dzombak.com/content/images/icon/pinned-octocat-093da3e6fa40-34.svg" alt=""><span>GitHub</span><span>cdzombak</span></p></div><p><img src="https://www.dzombak.com/content/images/thumbnail/gallerygen" alt="" onerror="this.style.display = 'none'"></p></a></figure>

        </div>


        

    </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HorizonDB, a geocoding engine in Rust that replaces Elasticsearch (248 pts)]]></title>
            <link>https://radar.com/blog/high-performance-geocoding-in-rust</link>
            <guid>44836463</guid>
            <pubDate>Fri, 08 Aug 2025 12:57:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://radar.com/blog/high-performance-geocoding-in-rust">https://radar.com/blog/high-performance-geocoding-in-rust</a>, See on <a href="https://news.ycombinator.com/item?id=44836463">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>At Radar, performance is a feature. Our platform processes over 1 billion API calls per day from hundreds of millions of devices worldwide. We provide geolocation infrastructure and solutions, including APIs for:</p><ul role="list"><li><a href="https://radar.com/product/geocoding-api" target="_blank" data-wf-native-id-path="7e9f9f3e-2085-5f55-1e17-1477c9eb89bf" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="7e9f9f3e-2085-5f55-1e17-1477c9eb89bf"><strong>Geocoding</strong></a>: Forward geocoding, reverse geocoding, and IP geocoding APIs with global coverage.</li><li><a href="https://radar.com/product/places-search-api" target="_blank" data-wf-native-id-path="1c432021-7bc9-0814-927e-61b37d5f00fb" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="1c432021-7bc9-0814-927e-61b37d5f00fb"><strong>Search</strong></a><strong>:</strong> Address autocomplete, address validation, and places search APIs.</li><li><a href="https://radar.com/product/routing-api" target="_blank" data-wf-native-id-path="bb8af94d-e2f9-ce26-c9c5-5c50450a9c2f" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="bb8af94d-e2f9-ce26-c9c5-5c50450a9c2f"><strong>Routing</strong></a><strong>: </strong>Distance, matrix, route optimization, route matching, and directions APIs.</li><li><a href="https://radar.com/product/fraud" target="_blank" data-wf-native-id-path="15365177-de4a-9634-7e4a-af0d7cf5f73d" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="15365177-de4a-9634-7e4a-af0d7cf5f73d"><strong>Geolocation compliance</strong></a>: Detect current jurisdiction, distance to border, regulatory exclusion zones, and more.</li></ul><p>But as our products and data scale, so do our engineering challenges.</p><p>To support this growth, we developed <strong>HorizonDB</strong>, a geospatial database written in Rust that consolidates multiple location services into a single, highly performant binary. With HorizonDB, we are able to power all of the above use cases with excellent operational footprint:</p><ul role="list"><li>Handle 1,000 QPS per core.&nbsp;</li><li>Maintain a forward geocoding median latency of 50 ms.</li><li>Maintain a reverse geocoding median latency of &lt;1 ms.</li><li>Scale linearly on commodity hardware.</li></ul><h2>Why we replaced Mongo and Elasticsearch</h2><p>Before HorizonDB, we split geocoding across Elasticsearch and microservices for forward geocoding, and MongoDB for reverse. </p><p>Operating and scaling this stack was costly: Elasticsearch frequently fanned queries to all shards and required service-orchestrated batch updates, while MongoDB lacked true batch ingestion, required overprovisioning, and had no reliable bulk rollback for bad data.</p><h2>The architecture</h2><p>Our goals for this service included:</p><ol role="list"><li>‍<strong>Efficiency:</strong> The service can run on commodity machines, has predictable autoscaling, and is the single source of truth of all our geo entities.</li><li>‍<strong>Operations:</strong> Data assets can be built and processed multiple times a day, changes can be deployed and rolled back trivially, and should be simple to operate.</li><li>‍<strong>Developer experience:</strong> Developers should be able to run the service locally, and changes can be written and tested easily.</li></ol><p>With these goals in mind, we built HorizonDB using <a href="https://rocksdb.org/" target="_blank" data-wf-native-id-path="ef24cc8f-619d-614d-05a8-cb652aa0d38f" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="ef24cc8f-619d-614d-05a8-cb652aa0d38f">RocksDB</a>, <a href="http://s2geometry.io/" target="_blank" data-wf-native-id-path="d4d56947-e884-c6fa-7653-e6def92b7d3f" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="d4d56947-e884-c6fa-7653-e6def92b7d3f">S2</a>, <a href="https://github.com/quickwit-oss/tantivy" data-wf-native-id-path="ef24cc8f-619d-614d-05a8-cb652aa0d393" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="ef24cc8f-619d-614d-05a8-cb652aa0d393">Tantivy</a>, <a href="https://github.com/BurntSushi/fst" target="_blank" data-wf-native-id-path="ef24cc8f-619d-614d-05a8-cb652aa0d397" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="ef24cc8f-619d-614d-05a8-cb652aa0d397">FSTs</a>, <a href="https://github.com/microsoft/LightGBM" target="_blank" data-wf-native-id-path="ef24cc8f-619d-614d-05a8-cb652aa0d39b" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="ef24cc8f-619d-614d-05a8-cb652aa0d39b">LightGBM</a> and <a href="https://fasttext.cc/" target="_blank" data-wf-native-id-path="ef24cc8f-619d-614d-05a8-cb652aa0d39f" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="ef24cc8f-619d-614d-05a8-cb652aa0d39f">FastText</a>. </p><p>Data assets are preprocessed using <a href="https://spark.apache.org/" target="_blank" data-wf-native-id-path="085921f8-2483-8a2b-77c7-6dd0a677c6dc" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="085921f8-2483-8a2b-77c7-6dd0a677c6dc">Apache Spark</a>, ingested in Rust and stored as versioned assets in AWS S3.</p><figure><p><img src="https://cdn.prod.website-files.com/67606084339203323d92a420/689379d4daef27d8453c9535_HorizonDB%20graphic%201%20(2).png" loading="lazy" alt="HorizonDB"></p><figcaption><strong>Figure 1:</strong> The HorizonDB server is a single multi-threaded process that concurrently queries different “layers” and re-ranks the candidates in a uniform manner.</figcaption></figure><p><strong>Rust</strong> <br>‍<a href="https://www.rust-lang.org/" target="_blank" data-wf-native-id-path="61aabe56-ec07-0c19-2b0a-486b8ca52eed" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="61aabe56-ec07-0c19-2b0a-486b8ca52eed">https://www.rust-lang.org/</a></p><p>A compiled language designed by Mozilla meant for systems programming. There are many aspects the team liked about Rust:</p><ul role="list"><li><strong>Compiled and memory safety without garbage collection:</strong> Rust's strong type system and safe and expressive concurrency in the form of Rayon and Tokio lets us write performant code without sacrificing readability. Rust makes it trivial to manage memory without garbage collection, allowing us to manage large indexes of data in memory with predictable latency.<strong>‍</strong></li><li><strong>Higher-order abstractions:</strong> Many of our engineers work with higher-level languages where expressive list operations, null-handling, and pattern matching are a given. Rust has these primitives, so that our team can move fast and express logic cleanly, which is important when dealing with complex logic such as search ranking.<strong>‍</strong></li><li><strong>Multi-threaded not multi-process:</strong> Since HorizonDB needs to fetch hundreds of GB of data from SSDs, having a single process that can leverage the same memory address space is more efficient compared to our API layer language TypeScript deployed to Node.js, which dedicates a new process to every core.</li></ul><p><strong>RocksDB</strong> <br>‍<a href="https://rocksdb.org/" target="_blank" data-wf-native-id-path="f13bf703-a60f-f9d6-e7d2-d7613c43693a" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="f13bf703-a60f-f9d6-e7d2-d7613c43693a">https://rocksdb.org/</a><br></p><p>S2 is Google's spatial indexing library that projects a quadtree onto a sphere, turning O(n) point-in-polygon lookups into cacheable constant time lookups. While writing HorizonDB we wrote Rust bindings for <a href="https://github.com/google/s2geometry" data-wf-native-id-path="11464a29-91e3-5d5c-4e8a-a3d41bdc88da" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="11464a29-91e3-5d5c-4e8a-a3d41bdc88da">Google's C++</a> library that we will open source soon.</p><p><strong>FSTs <br>‍</strong><a href="https://github.com/BurntSushi/fst" target="_blank" data-wf-native-id-path="87bf22db-3a13-367f-f0b2-453ef8622c4e" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="87bf22db-3a13-367f-f0b2-453ef8622c4e">https://github.com/BurntSushi/fst</a> <br></p><p>FSTs are a data structure offering efficient string compression and prefix queries. <a href="https://blog.burntsushi.net/transducers/" target="_blank" data-wf-native-id-path="3160a8d5-54f9-64b1-c6d1-2cf8d2e064df" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="3160a8d5-54f9-64b1-c6d1-2cf8d2e064df">This blog post by Andrew Gallant describes in great detail how this is achieved</a>. We found 80% of our queries were well-formed and wanted an efficient way to cache these “happy-paths”. Using FSTs, we were able to cache millions of these happy-paths on the order of MBs of memory and often returned prefix candidates within single-digit milliseconds.</p><figure><p><img src="https://cdn.prod.website-files.com/67606084339203323d92a420/688bc15b64515589fe4e0a74_HorizonDB%20graphic%203.png" loading="lazy" alt="FST"></p><figcaption><strong>Figure 2:</strong> An FST compactly represents a corpus of words by compressing common &nbsp;prefixes and suffixes. FSTs are also able to encode numeric values which can be retrieved during traversal, allowing us to bitpack metadata such as latitude and longitude for ranking purposes.</figcaption></figure><p><strong>Tantivy</strong> <br>‍<a href="https://github.com/quickwit-oss/tantivy" target="_blank" data-wf-native-id-path="24d9ba27-381c-3294-5233-76da3db48bdc" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="24d9ba27-381c-3294-5233-76da3db48bdc">https://github.com/quickwit-oss/tantivy</a><br></p><p>An in-process inverted index library similar to Lucene.</p><p>We made the decision to use an in-process index over an external service such as Elasticsearch or Meilisearch for a few reasons: <br></p><ul role="list"><li><strong>Search quality:</strong> To improve our recall for use cases like address validation, we often “expand” our search keywords dynamically. This would translate to sending multiple queries over the wire if we used an external service.</li><li><strong>Operational simplicity:</strong> Everything is within the same process, so scaling search servers becomes trivial. Memory mapping gives us a way to efficiently use commodity hardware with large indexes. We found this much simpler than scaling Elasticsearch where tuning JVM params and trying to saturate CPU without increased latency was very difficult.</li></ul><figure><p><img src="https://cdn.prod.website-files.com/67606084339203323d92a420/688bc1919033001b226b4ada_HorizonDB%20graphic%202.png" loading="lazy" alt="Inverted index"></p><figcaption><strong>Figure 3:</strong> An inverted index (as opposed to a typical forward index like in a SQL database) can performantly find relevant documents based on term lookups. Document IDs can be compressed via techniques like delta encoding. The query “Central Park” can be performantly handled by querying and combining the documents returned from the terms “central” and “park”.</figcaption></figure><p><strong>FastText</strong> <br>‍<a href="https://fasttext.cc/" target="_blank" data-wf-native-id-path="b169e495-b11e-cfdf-02c6-06395ebf4ca2" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="b169e495-b11e-cfdf-02c6-06395ebf4ca2">https://fasttext.cc/</a><br></p><p>To improve precision and search quality, we implemented a FastText model trained from a mix of our geocoder corpus and query logs. With FastText, we can semantically represent words in a query in a numeric vector format, suitable for ML applications. FastText is typo-tolerant and handles out-of-vocabulary words with its use of ngrams. “Nearby” vectors represent semantically similar words allowing our ML algorithms to understand semantics of a given word in a search query.</p><p><strong>LightGBM</strong> <br>‍<a href="https://github.com/microsoft/LightGBM" target="_blank" data-wf-native-id-path="a2342c00-ec1f-9dfc-2e0e-f08ac84d9c3d" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="a2342c00-ec1f-9dfc-2e0e-f08ac84d9c3d">https://github.com/microsoft/LightGBM</a><br></p><p>We have trained multiple LightGBM models to classify query intent and tag parts of our query depending on the intent.&nbsp;This allows us to “structure” our queries, improving search performance and precision.&nbsp;For example, a query deemed as a regional query such as “New York” can skip address search, whereas a query like “841 broadway” allows us to skip searching POIs and regions.</p><p><strong>Apache Spark <br>‍</strong><a href="https://spark.apache.org/" target="_blank" data-wf-native-id-path="cacbdb3a-faf4-f5f8-4299-245b11f222d3" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="cacbdb3a-faf4-f5f8-4299-245b11f222d3">https://spark.apache.org/</a></p><p>With Spark, we are able to process hundreds of millions of data points in less than an hour, with near-linear scalability. We often had to tune or refactor jobs to achieve optimal performance when performing joins or aggregations.</p><p>Since our data is written to S3, it becomes trivial to inspect results via Amazon Athena, a hosted deployment of Apache Presto that can read object storage assets using SQL. DuckDB is another lightweight tool that our engineers use to inspect these assets on the fly.</p><h2><strong>Results</strong></h2><p>HorizonDB has transformed both the operational and developmental aspects of our geolocation offerings. We've achieved improvements across the board for cost, performance, and scalability:</p><ul role="list"><li>Our service is now faster, operationally simple, and reliable.</li><li>Our developers are able to move fast with new features and data changes. We are able to ingest and evaluate new data sources within a day.</li><li>We've shut down multiple Mongo clusters, a large Elasticsearch cluster, and several geo microservices, saving us high five-figures in monthly costs.</li></ul><p>We are happy with our design decisions with HorizonDB and are prepared for our scale for the foreseeable future. We will touch on how we designed particular features of the system in future blog posts.</p><p>Many thanks to our hard-working engineers Bradley Schoeneweis, Jason Liu, Jacky Wang, Binh Robles, Greg Sadetsky, David Gurevich, and Felix Li who made this system a reality.</p><h2>Join us</h2><p>Radar is more than just an API&nbsp;layer. Across SDKs, maps, databases, and infrastructure, we're rethinking geolocation from the ground up to offer the fastest, most developer-friendly location stack available.</p><p>If this blog post was interesting to you, we're hiring great engineering talent across the board. </p><p>Check out our <a href="https://radar.com/jobs" target="_blank" data-wf-native-id-path="483df42e-e888-d31f-82e8-8bee205833e4" data-wf-ao-click-engagement-tracking="true" data-wf-element-id="483df42e-e888-d31f-82e8-8bee205833e4">jobs page</a> for more information.</p></div></div>]]></description>
        </item>
    </channel>
</rss>