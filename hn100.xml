<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 28 Sep 2024 17:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Role of Deliberate Practice in the Development of Creativity (2014) (111 pts)]]></title>
            <link>https://repositories.lib.utexas.edu/server/api/core/bitstreams/c8cc4a4f-e641-462b-9a72-654e60f71485/content</link>
            <guid>41680156</guid>
            <pubDate>Sat, 28 Sep 2024 13:35:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://repositories.lib.utexas.edu/server/api/core/bitstreams/c8cc4a4f-e641-462b-9a72-654e60f71485/content">https://repositories.lib.utexas.edu/server/api/core/bitstreams/c8cc4a4f-e641-462b-9a72-654e60f71485/content</a>, See on <a href="https://news.ycombinator.com/item?id=41680156">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Remember That DNA You Gave 23andMe? (124 pts)]]></title>
            <link>https://www.theatlantic.com/health/archive/2024/09/23andme-dna-data-privacy-sale/680057/</link>
            <guid>41679994</guid>
            <pubDate>Sat, 28 Sep 2024 13:05:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/health/archive/2024/09/23andme-dna-data-privacy-sale/680057/">https://www.theatlantic.com/health/archive/2024/09/23andme-dna-data-privacy-sale/680057/</a>, See on <a href="https://news.ycombinator.com/item?id=41679994">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header data-event-module="hero"><div><div><p>The company is in trouble, and anyone who has spit into one of the company’s test tubes should be concerned.</p></div><div><figure><div data-flatplan-lead_figure_media="true"><picture><img alt="Color illustration of a giant double helix in a shopping basket" sizes="(min-width: 976px) 976px, 100vw" srcset="https://cdn.theatlantic.com/thumbor/dwhsnYACFAvfTQqvJU1jM0Wf33E=/0x0:2000x1125/750x422/media/img/mt/2024/09/DNA_worth_final/original.jpg 750w, https://cdn.theatlantic.com/thumbor/MLFiD4XYUzOY1K7S38OlTGj-uiM=/0x0:2000x1125/828x466/media/img/mt/2024/09/DNA_worth_final/original.jpg 828w, https://cdn.theatlantic.com/thumbor/WwkP2L9X1IgqS4qdxVv8Q3N_b40=/0x0:2000x1125/960x540/media/img/mt/2024/09/DNA_worth_final/original.jpg 960w, https://cdn.theatlantic.com/thumbor/M2GrcZbd_Bu2ZtfNk2Dj8vNTPKc=/0x0:2000x1125/976x549/media/img/mt/2024/09/DNA_worth_final/original.jpg 976w, https://cdn.theatlantic.com/thumbor/RhYugbaHCEY3uJ7BVHhLlEx1-i4=/0x0:2000x1125/1952x1098/media/img/mt/2024/09/DNA_worth_final/original.jpg 1952w" src="https://cdn.theatlantic.com/thumbor/WwkP2L9X1IgqS4qdxVv8Q3N_b40=/0x0:2000x1125/960x540/media/img/mt/2024/09/DNA_worth_final/original.jpg" id="article-lead-image" width="960" height="540"></picture></div><figcaption data-flatplan-lead_figure_caption="true">Illustration by Akshita Chandra / The Atlantic. Source: Getty.</figcaption></figure></div></div><div><p><time datetime="2024-09-27T19:38:43Z" data-flatplan-timestamp="true">September 27, 2024, 3:38 PM ET</time> </p></div><gpt-ad format="injector" sizes-at-0="mobile-wide" targeting-pos="injector-article-start" sizes-at-976="desktop-wide"></gpt-ad></header><div data-view-action="view - audio player - start" data-view-label="680057" data-event-module="audio player" data-event-content-type="narrated" data-event-module-state="start" data-event-view="true"><div><p><img alt="Color illustration of a giant double helix in a shopping basket" sizes="80px" srcset="https://cdn.theatlantic.com/thumbor/rwZx92FyPVT1ymZXOiILr4G0yhI=/468x0:1593x1125/80x80/media/img/mt/2024/09/DNA_worth_final/original.jpg 80w, https://cdn.theatlantic.com/thumbor/565G5BewAZ5UlhVeki_2SFXer4s=/468x0:1593x1125/96x96/media/img/mt/2024/09/DNA_worth_final/original.jpg 96w, https://cdn.theatlantic.com/thumbor/Si6PbOzrASQN205HpG0efjDuUFk=/468x0:1593x1125/128x128/media/img/mt/2024/09/DNA_worth_final/original.jpg 128w, https://cdn.theatlantic.com/thumbor/UVKXXysPaBSYqrDpsrqe0iH82HI=/468x0:1593x1125/160x160/media/img/mt/2024/09/DNA_worth_final/original.jpg 160w, https://cdn.theatlantic.com/thumbor/rhx86PEA9BXeL5Iw7gasrTkgSrQ=/468x0:1593x1125/192x192/media/img/mt/2024/09/DNA_worth_final/original.jpg 192w, https://cdn.theatlantic.com/thumbor/JBZed8stnKCET-WqjFwspCXJitE=/468x0:1593x1125/256x256/media/img/mt/2024/09/DNA_worth_final/original.jpg 256w, https://cdn.theatlantic.com/thumbor/c3TQ4Kv3aJ35PrqkiFwJaJptexc=/468x0:1593x1125/384x384/media/img/mt/2024/09/DNA_worth_final/original.jpg 384w, https://cdn.theatlantic.com/thumbor/TMpuUFZOmyFXWaQgM47TfQQY_Wg=/468x0:1593x1125/512x512/media/img/mt/2024/09/DNA_worth_final/original.jpg 512w" src="https://cdn.theatlantic.com/thumbor/rwZx92FyPVT1ymZXOiILr4G0yhI=/468x0:1593x1125/80x80/media/img/mt/2024/09/DNA_worth_final/original.jpg" width="80" height="80"></p></div><p>Produced by ElevenLabs and News Over Audio (NOA) using AI narration.</p></div><section data-event-module="article body" data-flatplan-body="true"><p data-flatplan-paragraph="true">23andMe is <a data-event-element="inline link" href="https://www.bloomberg.com/news/articles/2024-08-08/23andme-s-quarterly-sales-miss-estimates-as-ceo-seeks-control?srnd=undefined">not doing well</a>. Its stock is on the verge of being delisted. It shut down its in-house drug-development unit last month, only the latest in several rounds of layoffs. Last week, the <a data-event-element="inline link" href="https://www.theguardian.com/technology/2024/sep/18/23andme-directors-resign">entire board of directors</a> quit, save for Anne Wojcicki, a co-founder and the company’s CEO. Amid this downward spiral, Wojcicki <a data-event-element="inline link" href="https://www.reuters.com/markets/deals/23andme-ceo-wojcicki-open-third-party-takeover-proposals-firm-filing-shows-2024-09-11/">has said</a> she’ll consider selling 23andMe—which means the DNA of 23andMe’s 15 million customers would be up for sale, too.</p><p data-flatplan-paragraph="true">23andMe’s trove of genetic data might be its most valuable asset. For about two decades now, since human-genome analysis became quick and common, the A’s, C’s, G’s, and T’s of DNA have allowed long-lost relatives to connect, <a data-event-element="inline link" href="https://www.theatlantic.com/health/archive/2024/03/dna-tests-incest/677791/">revealed family secrets</a>, and helped police catch serial killers. Some people’s genomes contain clues to what’s making them sick, or even, <a data-event-element="inline link" href="https://www.bloomberg.com/news/features/2024-05-15/23andme-s-dna-test-drama-shows-limits-of-what-genetics-can-do">occasionally</a>, how their disease should be treated. For most of us, though, consumer tests don’t have much to offer beyond a snapshot of our ancestors’ roots and confirmation of the traits we already know about. (Yes, 23andMe, my eyes <em>are</em> blue.) 23andMe is floundering in part because it hasn’t managed to prove the value of collecting all that sensitive, personal information. And potential buyers may have very different ideas about how to use the company’s DNA data to raise the company’s bottom line. This should concern anyone who has used the service.</p><p data-flatplan-paragraph="true">DNA might contain health information, but unlike a doctor’s office, 23andMe is not bound by the health-privacy law HIPAA. And the company’s <a data-event-element="inline link" href="https://www.23andme.com/legal/privacy/full-version/">privacy policies</a> make clear that in the event of a merger or an acquisition, customer information is a salable asset. 23andMe promises to ask its customers’ permission before using their data for research or targeted advertising, but that doesn’t mean the next boss will do the same. It says so right there in the fine print: The company reserves the right to update its policies at any time. A spokesperson acknowledged to me this week that the company can’t fully guarantee the sanctity of customer data, but said in a statement that “any scenario which impacts our customer's data would need to be carefully considered. We take the privacy and trust of our customers very seriously, and would strive to maintain commitments outlined in our Privacy Statement.”</p><p data-flatplan-paragraph="true"><a data-event-element="inline link" href="https://www.bloomberg.com/news/articles/2023-12-05/the-risks-of-sharing-your-dna-with-genetic-testing-firms-like-23andme?embedded-checkout=true">Certain parties</a> might take an obvious interest in the secrets of Americans’ genomes. Insurers, for example, would probably like to know about any genetic predispositions that might make you more expensive to them. In the United States, a <a data-event-element="inline link" href="http://www.ginahelp.org/">2008 law</a> called the Genetic Information Nondiscrimination Act protects against discrimination by employers and health insurers on the basis of genetic data, but <a data-event-element="inline link" href="https://www.fastcompany.com/3055710/if-you-want-life-insurance-think-twice-before-getting-genetic-testing">gaps in it</a> exempt providers of life, disability, and long-term-care insurance from such restrictions. That means that if you have, say, a genetic marker that can be correlated with a heart condition, a life insurer could find that out and legally deny you a policy—even if you never actually develop that condition. Law-enforcement agencies rely on DNA data to solve many difficult cases, and although 23andMe says it requires a warrant to share data, <a data-event-element="inline link" href="https://www.gedmatch.com/join-the-genetic-witness-program/">some other companies</a> <a data-event-element="inline link" href="https://www.bloomberg.com/news/articles/2019-02-01/major-dna-testing-company-is-sharing-genetic-data-with-the-fbi">have granted</a> broad access to police. You don’t have to commit a crime to be affected: Because we share large chunks of our genome with relatives, your DNA could be used to implicate a close family member or even a third cousin whom you’ve never met. Information about your ethnicity can also be sensitive, and that’s encoded in your genome, too. That’s all part of why, in 2020, the U.S. military <a data-event-element="inline link" href="https://www.army.mil/article/232314/osd_advises_service_members_against_using_dtc_genetic_testing">advised</a> its personnel against using consumer tests.</p><p id="injected-recirculation-link-0" data-view-action="view link - injected link - item 1" data-event-element="injected link" data-event-position="1"><a href="https://www.theatlantic.com/science/archive/2018/07/big-pharma-dna/566240/">Read: Big Pharma would like your DNA</a></p><p data-flatplan-paragraph="true">Spelling out all the potential consequences of an unknown party accessing your DNA is impossible, because scientists’ understanding of the genome is still evolving. Imagine drugmakers trolling your genome to find out what ailments you’re at risk for and then targeting you with ads for drugs to treat them. “There’s a lot of ways that this data might be misused or used in a way that the consumers couldn’t anticipate when they first bought 23andMe,” Suzanne Bernstein, counsel at the Electronic Privacy Information Center, told me. And unlike a password that can be changed after it leaks, once your DNA is out in the wild, it’s out there for good.</p><p data-flatplan-paragraph="true">Some states, <a data-event-element="inline link" href="https://privacyrights.org/resources/genetic-information-privacy-act-california#:~:text=The%20Genetic%20Information%20Privacy%20Act,with%20access%20and%20deletion%20rights.">such as California,</a> give consumers additional genetic-privacy rights and might allow DNA data to be deleted ahead of a sale. The 23andMe spokesperson told me that “customers have the ability to download their data and delete their personal accounts.” Companies are also required to notify customers of any changes to terms of service and give them a chance to opt out, though typically such changes take effect automatically after a certain amount of time, whether or not you’ve read through the fine print.</p><p data-flatplan-paragraph="true">Consumers have assumed this risk without getting much in return. When the first draft of the human genome was unveiled, it was billed as a panacea, hiding within its code secrets that would help each and every one of us unlock a personalized health plan. But <a data-event-element="inline link" href="https://www.bloomberg.com/news/features/2024-05-15/23andme-s-dna-test-drama-shows-limits-of-what-genetics-can-do">most diseases</a>, it turns out, can't be pinned on a single gene. And most people have a boring genome, free of red-flag mutations, which means DNA data just <a data-event-element="inline link" href="https://www.bloomberg.com/news/features/2024-05-15/23andme-s-dna-test-drama-shows-limits-of-what-genetics-can-do">aren’t that useful</a> to them—at least not in this form. And if a DNA test reveals elevated risk for a more common health condition, such as diabetes and heart disease, you probably already know the interventions: eating well, exercising often, getting a solid eight hours of sleep. (To an insurer, though, even a modicum of risk might make someone an unattractive candidate for coverage.) That’s likely a big part of why 23andMe’s sales have slipped. There are only so many people who want to know about their Swedish ancestry, and that, it turns out, is consumer DNA testing’s biggest sell.</p><p id="injected-recirculation-link-1" data-view-action="view link - injected link - item 2" data-event-element="injected link" data-event-position="2"><a href="https://www.theatlantic.com/health/archive/2024/03/dna-tests-incest/677791/">Read: DNA tests are uncovering the true prevalence of incest</a></p><p data-flatplan-paragraph="true">Wojcicki has pulled 23andMe back from the brink before, after the Food and Drug Administration ordered the company to <a data-event-element="inline link" href="https://www.bloomberg.com/news/articles/2013-11-27/do-genetic-tests-need-doctors-fda-defends-its-challenge-to-23andme">stop selling</a> its health tests in 2013 until they could be proved safe and effective. In recent months, Wojcicki has explored a variety of options to save the company, including splitting it to separate the cash-burning drug business from the consumer side. Wojcicki has <a data-event-element="inline link" href="https://investors.23andme.com/node/9936/html">still expressed interest</a> in trying to take the company private herself, but the board rejected her initial offer. 23andMe has until November 4 to raise its shares to at least $1, or be delisted. As that date approaches, a sale looks more and more likely—whether to Wojcicki or someone else.</p><p data-flatplan-paragraph="true">The risk of DNA data being misused has existed since DNA tests first became available. When customers opt in to participate in drug-development research, third parties already get access to their de-identified DNA data, which can in some cases be <a data-event-element="inline link" href="https://www.bloomberg.com/news/articles/2018-06-15/deleting-your-online-dna-data-is-brutally-difficult">linked back to people’s identities</a> after all. Plus, 23andMe has failed to protect its customers’ information in the past—it just agreed to pay <a data-event-element="inline link" href="https://www.reuters.com/technology/cybersecurity/23andme-settles-data-breach-lawsuit-30-million-2024-09-13/">$30 million</a> to settle a lawsuit resulting from an October 2023 data breach. But for nearly two decades, the company had an incentive to keep its customers’ data private: 23andMe is a consumer-facing business, and to sell kits, it also needed to win trust. Whoever buys the company’s data may not operate under the same constraints.</p></section><div data-event-module="footer"><p><h3>About the Author</h3></p></div><gpt-ad format="injector" sizes-at-0="mobile-wide,native,house" targeting-pos="injector-most-popular" sizes-at-976="desktop-wide,native,house"></gpt-ad></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta fined $102M for storing passwords in plain text (149 pts)]]></title>
            <link>https://www.engadget.com/big-tech/meta-fined-102-million-for-storing-passwords-in-plain-text-110049679.html</link>
            <guid>41678840</guid>
            <pubDate>Sat, 28 Sep 2024 08:38:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/big-tech/meta-fined-102-million-for-storing-passwords-in-plain-text-110049679.html">https://www.engadget.com/big-tech/meta-fined-102-million-for-storing-passwords-in-plain-text-110049679.html</a>, See on <a href="https://news.ycombinator.com/item?id=41678840">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The Irish Data Protection Commission (DPC) has slapped Meta with a $101.5 million (€91 million) fine after wrapping up an investigation into a security breach in 2019, wherein the company mistakenly <a data-i13n="cpos:1;pos:1" href="https://www.engadget.com/2019-03-21-facebook-user-passwords-plain-text.html" data-ylk="slk:stored users' passwords in plain text;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas">stored users' passwords in plain text</a>. Meta's original announcement only talked about how it found some user passwords stored in plain text on its servers in January that year. But a month later, it updated its announcement to reveal that <a data-i13n="cpos:2;pos:1" href="https://www.engadget.com/2019-04-18-facebook-stored-instagram-passwords-plain-text.html" data-ylk="slk:millions of Instagram passwords;cpos:2;pos:1;elm:context_link;itc:0;sec:content-canvas">millions of Instagram passwords</a> were also stored in easily readable format.</p><p>While Meta didn't say how many accounts were affected, a senior employee told <a data-i13n="cpos:3;pos:1" href="https://krebsonsecurity.com/2019/03/facebook-stored-hundreds-of-millions-of-user-passwords-in-plain-text-for-years/" rel="nofollow noopener" target="_blank" data-ylk="slk:Krebs on Security;cpos:3;pos:1;elm:context_link;itc:0;sec:content-canvas"><em>Krebs on Security </em></a>back then that the incident involved up to 600 million passwords. Some of the passwords had been stored in easily readable format in the company's servers since 2012. They were also reportedly searchable by over 20,000 Facebook employees, though the DPC has clarified in its decision that they were at least not made available to external parties.</p><p>The DPC found that Meta violated several GDPR rules related to the breach. It determined that the company failed to "notify the DPC of a personal data breach concerning storage of user passwords in plaintext" without undue delay and failed to "document personal data breaches concerning the storage of user passwords in plaintext." It also said that Meta violated the GDPR by not using appropriate technical measures to ensure the security of users' passwords against unauthorized processing.</p><p>"It is widely accepted that user passwords should not be stored in plaintext, considering the risks of abuse that arise from persons accessing such data. It must be borne in mind, that the passwords the subject of consideration in this case, are particularly sensitive, as they would enable access to users’ social media accounts," DPC's Deputy Commissioner, Graham Doyle, said in a statement.</p><p>The DPC has also given the company a reprimand in addition to the penalty. We may know more about what that means for Meta exactly when the commission publishes its full final decision and other related information in the future.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FFT-based ocean-wave rendering, implemented in Godot (470 pts)]]></title>
            <link>https://github.com/2Retr0/GodotOceanWaves</link>
            <guid>41678412</guid>
            <pubDate>Sat, 28 Sep 2024 06:51:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/2Retr0/GodotOceanWaves">https://github.com/2Retr0/GodotOceanWaves</a>, See on <a href="https://news.ycombinator.com/item?id=41678412">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">GodotOceanWaves</h2><a id="user-content-godotoceanwaves" aria-label="Permalink: GodotOceanWaves" href="#godotoceanwaves"></a></p>
<p dir="auto">An open ocean rendering experiment in the Godot Engine utilizing the inverse Fourier transform of directional ocean-wave spectra for wave generation. A concise set of parameters is exposed, allowing for scriptable, real-time modification of wave properties to emulate a wide-variety of ocean-wave environments.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description ocean_demo.mp4">ocean_demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/18603664/371443621-a8083878-a297-4536-a481-9123cea7e7df.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjc1MTk3MDUsIm5iZiI6MTcyNzUxOTQwNSwicGF0aCI6Ii8xODYwMzY2NC8zNzE0NDM2MjEtYTgwODM4NzgtYTI5Ny00NTM2LWE0ODEtOTEyM2NlYTdlN2RmLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTI4VDEwMzAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg1NGI4ODAzMjU0NmQ0YzdjYTg3YTZkYmYxNmUzZmIxOTY5MTE4ODhjN2VmMGEwNTllMjMyYWJhMTMxNGE2MjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.woLyampML5KaxatdZL9Tg1Grfm2FiX_wXjz8ysQSLBA" data-canonical-src="https://private-user-images.githubusercontent.com/18603664/371443621-a8083878-a297-4536-a481-9123cea7e7df.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjc1MTk3MDUsIm5iZiI6MTcyNzUxOTQwNSwicGF0aCI6Ii8xODYwMzY2NC8zNzE0NDM2MjEtYTgwODM4NzgtYTI5Ny00NTM2LWE0ODEtOTEyM2NlYTdlN2RmLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTI4VDEwMzAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg1NGI4ODAzMjU0NmQ0YzdjYTg3YTZkYmYxNmUzZmIxOTY5MTE4ODhjN2VmMGEwNTllMjMyYWJhMTMxNGE2MjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.woLyampML5KaxatdZL9Tg1Grfm2FiX_wXjz8ysQSLBA" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why Fourier Transforms?</h3><a id="user-content-why-fourier-transforms" aria-label="Permalink: Why Fourier Transforms?" href="#why-fourier-transforms"></a></p>
<p dir="auto">A common approach for animating water in video games is by displacing vertices using <em>Gerstner waves</em>. While Gerstner waves work well for modeling the lower-frequency details in calmer waters, they fall short in accurately representing the choppy surfaces in an open ocean. To simulate the latter, a more complex approach simulates waves using the <em>inverse Fourier transform</em> of ocean-wave spectra modeled from empirical data gathered by oceanographers.</p>
<p dir="auto">A benefit of working in frequency space using ocean-wave spectra is the ease of modifying ocean properties (e.g., surface choppiness). When using Gerstner waves, it is unclear how waves (and their parameters) need to be changed to emulate a certain ocean state. In contrast, ocean-wave spectra expose parameters that change waves' properties directly.</p>
<p dir="auto">To compute the Fourier transform, a <em>fast Fourier transform</em> algorithm (FFT) is used specifically. On top of having a lower computational complexity than the classical discrete Fourier transform algorithm (<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$O(N \log N)$</math-renderer> versus <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$O(N^2)$</math-renderer>), the FFT is <em>scalable as a parallel system</em>. This means that it is perfect for running on the GPU. Using Gerstner waves requires each thread to perform <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$N$</math-renderer> computations, one for each wave. In contrast, FFT-based waves only require each thread to perform <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$\log(N)$</math-renderer> equivalent computations. At scale, more waves can be added to the system (at the same performance cost), permitting more accurate surface simulation.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Results</h2><a id="user-content-results" aria-label="Permalink: Results" href="#results"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Wave Shading</h3><a id="user-content-wave-shading" aria-label="Permalink: Wave Shading" href="#wave-shading"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Lighting Model</h4><a id="user-content-lighting-model" aria-label="Permalink: Lighting Model" href="#lighting-model"></a></p>
<p dir="auto">The ocean lighting model largely follows the BSDF described in the 'Atlas' GDC talk. One deviation, however, is the use of the GGX distribution (rather than Beckmann distribution) for the microfacet distribution. This was due to the GGX distribution's 'flatter' and softer highlights providing a more uniform appearance in many of the ocean-wave environments tested.</p>
<p dir="auto">The normal/foam map is sampled with a mix between bicubic and bilinear filtering depending on the world-space pixel density (a value dependent on the normal map texture resolution and texture UV tiling size). This effectively reduces texture aliasing artifacts at lower surface resolutions while maintaining the detail at higher surface resolutions.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Sea Foam</h4><a id="user-content-sea-foam" aria-label="Permalink: Sea Foam" href="#sea-foam"></a></p>
<p dir="auto">Tessendorf notes a method for determining when to generate sea foam by checking where the waves' peaks curl into themselves (i.e., when the Jacobian of the displacement is negative). Foam accumulates linearly and dissipates exponentially on a texture over multiple wave updates, and are controlled by "foam grow rate" and "foam decay rate" parameters respectively.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Sea Spray</h4><a id="user-content-sea-spray" aria-label="Permalink: Sea Spray" href="#sea-spray"></a></p>
<p dir="auto">Sea spray is modeled using particles via Godot's GPUParticles3D node and makes heavy use of a custom particle shader. Particles are distributed evenly across the plane within the GPUParticles3D node's bounding box. Then, they are culled based on the foam amount present at their position. Un-culled particles begin their lifecycle at a random offset.</p>
<p dir="auto">Each sea spray particle uses a billboarded sprite with a single static texture. Over the course of their lifecycle, particles' scales and displacements are modified to emulate a splash's appearance. A dissolve effect in particles' mesh shader fades the sprite in a way that simulates how sea spray atomizes once in the air.</p>
<p dir="auto">One <em>major</em> drawback of this method is that a large increase in particle amount only results in a small increase in sea spray density. This is due to the equal distribution of particles along the bounding box, which results in a majority of the added particles being culled.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/18603664/367495381-c69766e7-711c-4909-a1fa-290bac0d577a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjc1MTk3MDUsIm5iZiI6MTcyNzUxOTQwNSwicGF0aCI6Ii8xODYwMzY2NC8zNjc0OTUzODEtYzY5NzY2ZTctNzExYy00OTA5LWExZmEtMjkwYmFjMGQ1NzdhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTI4VDEwMzAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTdmZThlYjZkYTM1MGZhZTAwNGU1ZTAxY2UzYjgwNTE0ZDZhY2Y5YmNjNTg0MDg4OGY5ZWMxNzRkM2EzZmU4Y2ImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.TsauWxPeyGFgju5z9_6ePtRLKSGiuVHYeXbvtOIx3y4"><img src="https://private-user-images.githubusercontent.com/18603664/367495381-c69766e7-711c-4909-a1fa-290bac0d577a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjc1MTk3MDUsIm5iZiI6MTcyNzUxOTQwNSwicGF0aCI6Ii8xODYwMzY2NC8zNjc0OTUzODEtYzY5NzY2ZTctNzExYy00OTA5LWExZmEtMjkwYmFjMGQ1NzdhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTI4VDEwMzAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTdmZThlYjZkYTM1MGZhZTAwNGU1ZTAxY2UzYjgwNTE0ZDZhY2Y5YmNjNTg0MDg4OGY5ZWMxNzRkM2EzZmU4Y2ImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.TsauWxPeyGFgju5z9_6ePtRLKSGiuVHYeXbvtOIx3y4" alt="shading_demo"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Wave Simulation</h3><a id="user-content-wave-simulation" aria-label="Permalink: Wave Simulation" href="#wave-simulation"></a></p>
<p dir="auto">The method for generating surface waves closely follows Tessendorf. A directional ocean-wave spectrum function is multiplied with Gaussian-distributed random numbers to generate an initial spectral sea state. The initial state is then propagated in time through a "dispersion relation" (relating the frequency of waves and their propagation speed). An inverse Fourier transform can then be applied to the propagated state to generate displacement and normal maps.</p>
<p dir="auto">The methodology Tessendorf describes was implemented through a compute shader pipeline using Godot's RenderingDevice abstraction. The following sections detail more on major aspects of the wave generation system.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Ocean-Wave Spectra</h4><a id="user-content-ocean-wave-spectra" aria-label="Permalink: Ocean-Wave Spectra" href="#ocean-wave-spectra"></a></p>
<p dir="auto">The directional ocean-wave spectrum function, <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$S(\omega, \theta)$</math-renderer>, returns the energy of a wave given its frequency (<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$\omega$</math-renderer>) and direction (<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$\theta$</math-renderer>). It is comprised of a <strong>non-directional spectrum function</strong>, <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$S(\omega)$</math-renderer>, and a <strong>directional spread function</strong>, <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$D(\omega, \theta)$</math-renderer>; the choice of either is entirely independent.</p>
<ul dir="auto">
<li>For the <strong>non-directional spectrum function</strong>, the <em>Texel-Marsen-Arsloe</em> (TMA) spectrum described in Horvath was chosen. Given the wind speed (<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$U$</math-renderer>), depth (<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$D$</math-renderer>), and fetch length (i.e., distance from shoreline) (<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$F$</math-renderer>), the TMA spectrum combines its preceding <em>JONSWAP</em> spectrum with a depth attenuation function and is defined as <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$S_{\text{TMA}}(\omega) = S_{\text{JONSWAP}}(\omega)\Phi(\omega)$</math-renderer> where:</li>
</ul>
<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$$\begin{align*}
S_{\text{JONSWAP}}(\omega) &amp;= \Big[0.076\Big(\tfrac{U^2}{F \cdot 9.81}\Big)^{0.22}\Big]\Big[\tfrac{9.81^2}{\omega^5}\exp\Big({-\tfrac 5 4}\big(\tfrac{\omega_p}{\omega}\big)^4\Big)\Big] \Big[3.3^{\exp\Big(-\tfrac{(\omega - \omega_p)^2}{2(0.07 + 0.02\cdot\mathrm{step}(\omega - \omega_p))^2\omega_p^2}\Big)}\Big]\\\
\Phi(\omega) &amp;\approx \tfrac 1 2 \omega_h^2 + ({-\omega}_h^2+2\omega_h-1)\cdot\mathrm{step}(\omega_h - 1)\\\
\omega_p &amp;= 22\Big(\tfrac{9.81^2}{U F}\Big)\\\
\omega_h &amp;= \omega \sqrt{\tfrac D {9.81}}
\end{align*}$$</math-renderer>
<ul dir="auto">
<li>For the <strong>directional spread function</strong>, a combination of the <em>flat</em> and <em>Hasselmann</em> directional spreadings described in Horvath—mixed by a 'spread' parameter (<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$\mu$</math-renderer>)—was chosen. Horvath also proposes the addition of a 'swell' parameter (<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$\xi$</math-renderer>) to model ocean-wave elongation—this was also incorporated into the spread model. The mixed spread function is defined as <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">${D_{\text{mixed}}(\omega, \theta) = \mathrm{lerp}((2\pi){^{-1}},\ Q(s+s_\xi)\text{|}\cos(\theta \text{/}2)\text{|}^{2(s+s_\xi)},\ \mu)}$</math-renderer> where:</li>
</ul>
<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$$\begin{align*}
&lt;!-- https://www.wolframalpha.com/input?i2d=true&amp;i=taylor+series+Divide%5BPower%5B2%2C%5C%2840%292x-1%5C%2841%29%5D%2C%CF%80%5D*Divide%5BPower%5B%5C%2840%29x%21%5C%2841%29%2C2%5D%2C%5C%2840%292x%5C%2841%29%21%5D+at+x+%3D+0 --&gt;
Q(\sigma) &amp;\approx \begin{cases}
 0.09\sigma^3 + \big(\tfrac{\ln^2 2}{\pi} - \tfrac{\pi}{12}\big)\sigma^2+\big(\tfrac{\ln 2}{\pi}\big)\sigma+\tfrac{1}{2\pi} &amp; \text{if } \sigma \leq 0.4\\\
 \frac{\sqrt \sigma}{2\sqrt \pi} + \frac{1}{16\sqrt{\pi \sigma}} &amp; \text{otherwise.}
\end{cases}\\\
s &amp;= \begin{cases}
 6.97\big(\tfrac \omega {\omega_p}\big){^{4.06}} &amp; \text{if } \omega \leq \omega_p\\\
 9.77\big(\tfrac \omega {\omega_p}\big){^{-2.33 -1.45(\omega_p U\text{/}9.81-1.17)}} &amp; \text{otherwise.}
\end{cases}\\\
s_\xi &amp;= 16 \tanh\big(\tfrac{\omega_p}{\omega}\big)\xi^2
\end{align*}$$</math-renderer>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$Q(\sigma)$</math-renderer> is a normalization factor used to satisfy the condition: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$\int_{-\pi}^\pi D(\omega, \theta)d \theta = 1$</math-renderer>. The Hasselmann directional spread was chosen due to its approximate analytical solution for <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$Q(\sigma)$</math-renderer> (as opposed to e.g., the Donelan-Banner directional spread also described in Horvath).</p>
<p dir="auto">Following a suggestion in Tessendorf, the resultant spectrum function was also multiplied by a small-wave suppression term, <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$\exp({-k}^2(1-\delta)^2)$</math-renderer> (given the magnitude of the wave vector (<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$k$</math-renderer>) and a 'detail' parameter (<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$\delta$</math-renderer>)). Combining the above, our <em>final</em> directional ocean-wave spectrum function used can be denoted as:</p>
<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="3bb48dd0e93aac9c804f5bc9c48749f1">$$S(\omega, \theta) = S_{\text{TMA}}(\omega)D_{\text{mixed}}(\omega, \theta)\exp({-k}^2(1-\delta)^2)$$</math-renderer>
<p dir="auto"><h4 tabindex="-1" dir="auto">Fast Fourier Transform</h4><a id="user-content-fast-fourier-transform" aria-label="Permalink: Fast Fourier Transform" href="#fast-fourier-transform"></a></p>
<p dir="auto">A custom FFT implementation was written for the GPU using compute shaders. The <em>Stockham</em> FFT algorithm was used over the Cooley-Tukey algorithm to avoid the initial bit-reversal permutation. Following Flügge, a 'butterfly' texture is computed, once per spectrum texture resolution change, encoding the dataflow of the FFT.</p>
<p dir="auto">First, the FFT kernel is applied row-wise to perform the 2D FFT on the spectrum texture. The texture is then transposed using a compute shader, allowing the same row-wise FFT kernel to then be reused for—what is effectively—a column-wise FFT. This transposition also improves memory access patterns along with enabling pipeline reuse.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Wave Cascades</h4><a id="user-content-wave-cascades" aria-label="Permalink: Wave Cascades" href="#wave-cascades"></a></p>
<p dir="auto">At large-enough distances—especially with sea foam present—tiling artifacts become very apparent. The wave generation system allows multiple wave cascades to be layered simultaneously to address this. Each cascade has its own tiling size and set of parameters. Cascades can be added/removed from the generation system dynamically in real-time. However, as all cascades use the same compute pipelines, they must have the same spectra texture resolution. Alternatively, blending wave displacements/normals with noise could also reduce tiling artifacts—at a lesser performance cost.</p>
<p dir="auto">Each wave cascades’ parameters and size must be carefully chosen to avoid wave interference when layered. Similarly, the cascades' wave phases should be offset to avoid interference with other cascades. The generation system automatically attempts this by offsetting each cascades’ start times differently (honestly, not sure if it works lol).</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Load Balancing</h4><a id="user-content-load-balancing" aria-label="Permalink: Load Balancing" href="#load-balancing"></a></p>
<p dir="auto">Due to the erratic nature of wave motion, their movement can appear perceptually smooth even without updating their displacements every frame. Thus, an "update rate" parameter was introduced to control how often wave cascades are updated per second. While this reduces the amount of GPU-time spent working on FFT, frames during which the wave generation pipeline runs still stutter.</p>
<p dir="auto">An experiment to asynchronously compute cascade updates using Godot's local RenderingDevices, caused significant performance overhead due to transferring textures between the CPU and GPU. Instead, the wave generation system <em>attempts to load-balance cascades</em>. Whenever the frame time is shorter than the update rate, only one cascade is updated per frame. This reduces stuttering while still benefiting from the lower GPU workload of frame skipping.</p>
<p dir="auto">The displacement, normal, and foam maps generated after running FFT on our directional ocean-wave spectrum function (along with its associated parameters) yield realistic surface motion across a broad range of ocean-wave environments.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description ocean_param_demo.mp4">ocean_param_demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/18603664/367477646-7589758f-1233-4be8-accc-2902a1dd01ec.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjc1MTk3MDUsIm5iZiI6MTcyNzUxOTQwNSwicGF0aCI6Ii8xODYwMzY2NC8zNjc0Nzc2NDYtNzU4OTc1OGYtMTIzMy00YmU4LWFjY2MtMjkwMmExZGQwMWVjLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTI4VDEwMzAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWJmMjM2Njc1OGVhYzFlZmRjNjYwMTQ2OGM0OTA1YTExOTEyNDViMjhjMGIxZTU3MWEwYWNmOWViMWIzNGI1YTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.9ySTzoAP2MM4b3nC3RfbSdJk8v7kVXn0TPmAEyIdTiY" data-canonical-src="https://private-user-images.githubusercontent.com/18603664/367477646-7589758f-1233-4be8-accc-2902a1dd01ec.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjc1MTk3MDUsIm5iZiI6MTcyNzUxOTQwNSwicGF0aCI6Ii8xODYwMzY2NC8zNjc0Nzc2NDYtNzU4OTc1OGYtMTIzMy00YmU4LWFjY2MtMjkwMmExZGQwMWVjLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MjglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTI4VDEwMzAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWJmMjM2Njc1OGVhYzFlZmRjNjYwMTQ2OGM0OTA1YTExOTEyNDViMjhjMGIxZTU3MWEwYWNmOWViMWIzNGI1YTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.9ySTzoAP2MM4b3nC3RfbSdJk8v7kVXn0TPmAEyIdTiY" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">References</h2><a id="user-content-references" aria-label="Permalink: References" href="#references"></a></p>
<p dir="auto"><strong>Flügge, Fynn-Jorin</strong>. <strong><a href="https://tore.tuhh.de/entities/publication/1cd390d3-732b-41c1-aa2b-07b71a64edd2" rel="nofollow">Realtime GPGPU FFT Ocean Water Simulation</a></strong>. Hamburg University of Technology. (2017).<br>
<strong>Gunnell, Garrett</strong>. <strong><a href="https://www.youtube.com/watch?v=yPfagLeUa7k" rel="nofollow">I Tried Simulating The Entire Ocean</a></strong>. (2023).<br>
<strong>Horvath, Christopher J</strong>. <strong><a href="https://dl.acm.org/doi/10.1145/2791261.2791267" rel="nofollow">Empirical Directional Wave Spectra for Computer Graphics</a></strong>. DigiPro. (2015).<br>
<strong>Tessendorf, Jerry</strong>. <strong><a href="https://people.computing.clemson.edu/~jtessen/reports/papers_files/coursenotes2004.pdf" rel="nofollow">Simulating Ocean Water</a></strong>. SIGGRAPH. (2004).<br>
<strong>Matusiak, Robert</strong>. <strong><a href="https://www.ti.com/lit/an/spra291/spra291.pdf" rel="nofollow">Implementing Fast Fourier Transform Algorithms of Real-Valued Sequences</a></strong>. Texas Instruments. (2001).<br>
<strong>Mihelich, Mark</strong>. <strong><a href="https://www.youtube.com/watch?v=Dqld965-Vv0" rel="nofollow">Wakes, Explosions and Lighting: Interactive Water Simulation in 'Atlas'</a></strong>. GDC. (2019).<br>
<strong>Pensionerov, Ivan</strong>. <strong><a href="https://github.com/gasgiant/FFT-Ocean">FFT-Ocean</a></strong>. GitHub. (2020).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Attribution</h2><a id="user-content-attribution" aria-label="Permalink: Attribution" href="#attribution"></a></p>
<p dir="auto"><strong><a href="https://polyhaven.com/a/evening_road_01_puresky" rel="nofollow">Evening Road 01 (Pure Sky)</a></strong> by <strong>Jarod Guest</strong> is used under the <a href="https://creativecommons.org/publicdomain/zero/1.0/" rel="nofollow">CC0 1.0</a> license.<br>
<strong><a href="http://wwwa.pikara.ne.jp/okojisan/otfft-en/stockham3.html" rel="nofollow">OTFFT DIT Stockham Algorithm</a></strong> by <strong>Takuya Okahisa</strong> is used and modified under the <a href="http://wwwa.pikara.ne.jp/okojisan/otfft-en/download.html" rel="nofollow">MIT</a> license.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amusing Ourselves to Death (284 pts)]]></title>
            <link>https://otpok.com/2014/01/03/amusing-ourselves-to-death/</link>
            <guid>41678208</guid>
            <pubDate>Sat, 28 Sep 2024 05:56:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://otpok.com/2014/01/03/amusing-ourselves-to-death/">https://otpok.com/2014/01/03/amusing-ourselves-to-death/</a>, See on <a href="https://news.ycombinator.com/item?id=41678208">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page" role="main">

		
			
<article id="post-1457">
	<!-- .entry-header -->

	<div>
		<p><a href="http://www.diet-specialist.co.uk/pathofknowledge"><img src="https://otpok.com/wp-content/uploads/2014/01/diet-specialist-1.gif?w=646&amp;h=106" alt="" width="646" height="106"></a></p>
<p>This is perhaps one of the most striking passages I have read for a while. It describes the modern world with startling accuracy. In our fear of an increasingly authoritarian rule, we have allowed a far more dangerous vision to come true: <span><strong><em>heedlessness</em></strong></span></p>
<p>Below is the foreward of Neil Postman’s book “<strong>Amusing Ourselves to Death: Public Discourse in the Age of Show Business</strong>“, accompanied by a comic illustration of the two ideas. It gives a concise comparison of the two authors views and what they foresaw society will become. But perhaps the remarkable part of this whole story passage lies beyond its lines with us:</p>
<p><span><em><strong>Most of us will read this and continue living our life exactly the same way as before</strong></em></span></p>
<p><span><strong><em>…wake up</em></strong></span></p>
<p>————————————————————–</p>
<blockquote><p>We were keeping our eye on 1984. When the year came and the prophecy didn’t, thoughtful Americans sang softly in praise of themselves. The roots of liberal democracy had held. Wherever else the terror had happened, we, at least, had not been visited by Orwellian nightmares.</p>
<p>But we had forgotten that alongside Orwell’s dark vision, there was another – slightly older, slightly less well known, equally chilling: Aldous Huxley’s Brave New World. Contrary to common belief even among the educated, Huxley and Orwell did not prophesy the same thing. Orwell warns that we will be overcome by an externally imposed oppression. But in Huxley’s vision, no Big Brother is required to deprive people of their autonomy, maturity and history. As he saw it, people will come to love their oppression, to adore the technologies that undo their capacities to think.</p>
<p>What Orwell feared were those who would ban books. What Huxley feared was that there would be no reason to ban a book, for there would be no one who wanted to read one. Orwell feared those who would deprive us of information. Huxley feared those who would give us so much that we would be reduced to passivity and egoism. Orwell feared that the truth would be concealed from us. Huxley feared the truth would be drowned in a sea of irrelevance. Orwell feared we would become a captive culture. Huxley feared we would become a trivial culture, preoccupied with some equivalent of the feelies, the orgy porgy, and the centrifugal bumblepuppy. As Huxley remarked in Brave New World Revisited, the civil libertarians and rationalists who are ever on the alert to oppose tyranny “failed to take into account man’s almost infinite appetite for distractions”. In 1984, Huxley added, people are controlled by inflicting pain. In Brave New World, they are controlled by inflicting pleasure. In short, Orwell feared that what we hate will ruin us. Huxley feared that what we love will ruin us.</p>
<p>This book is about the possibility that Huxley, not Orwell, was right</p>
<p><strong><em>[Neil Postman – Amusing ourselves to death]</em></strong></p></blockquote>
<p><img src="https://otpok.com/wp-content/uploads/2014/01/screenshot_1.png?w=700" alt=""></p>
<p><img src="https://otpok.com/wp-content/uploads/2014/01/wpid-storageextsdcarddcimcomparison-1.png?w=700" alt=""></p>
<p><img src="https://otpok.com/wp-content/uploads/2014/01/wpid-storageextsdcarddcimcomparison-2.png?w=700" alt=""></p>
<p><img src="https://otpok.com/wp-content/uploads/2014/01/comparison-31.png?w=700" alt=""></p>
<p><img src="https://otpok.com/wp-content/uploads/2014/01/comparison-4.png?w=700" alt=""></p>
<p><img src="https://otpok.com/wp-content/uploads/2014/01/comparison-51.png?w=700" alt=""></p>
<p><img src="https://otpok.com/wp-content/uploads/2014/01/wpid-storageextsdcarddcimcomparison-6.png?w=700" alt=""></p>
<p><img src="https://otpok.com/wp-content/uploads/2014/01/comparison-71.png?w=700" alt=""></p>
<p><img src="https://otpok.com/wp-content/uploads/2014/01/screenshot_16.png?w=700" alt=""></p>
<p><em>[The comic is Stuart McMillen’s interpretation of media theorist Niel Postman’s book Amusing Ourselves to Death (1985), subtitled “Public Discourse in the Age of&nbsp;Show Business”]</em></p>

			
						</div><!-- .entry-content -->

	<!-- .entry-meta -->
</article><!-- #post-## -->
			<div>
	<h3>Related posts</h3>
	
		<article id="post-1449">

						<p><a href="https://otpok.com/2013/12/25/hamster/"><img width="50" height="50" src="https://otpok.com/wp-content/uploads/2013/12/skeleton-wheel.gif?w=50&amp;h=50&amp;crop=1" alt="" decoding="async" data-attachment-id="1450" data-permalink="https://otpok.com/2013/12/25/hamster/skeleton-wheel/" data-orig-file="https://otpok.com/wp-content/uploads/2013/12/skeleton-wheel.gif" data-orig-size="960,544" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="Skeleton Wheel" data-image-description="" data-image-caption="" data-medium-file="https://otpok.com/wp-content/uploads/2013/12/skeleton-wheel.gif?w=300" data-large-file="https://otpok.com/wp-content/uploads/2013/12/skeleton-wheel.gif?w=700" tabindex="0" role="button"></a>
			</p>
			
			<!-- .entry-header -->

		</article>

	
		<article id="post-2847">

						<p><a href="https://otpok.com/2020/03/28/deaths-spectre/"><img width="50" height="50" src="https://otpok.com/wp-content/uploads/2020/03/death-164761.jpg?w=50&amp;h=50&amp;crop=1" alt="" decoding="async" data-attachment-id="2849" data-permalink="https://otpok.com/2020/03/28/deaths-spectre/death-164761/" data-orig-file="https://otpok.com/wp-content/uploads/2020/03/death-164761.jpg" data-orig-size="1920,1271" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="death-164761" data-image-description="" data-image-caption="" data-medium-file="https://otpok.com/wp-content/uploads/2020/03/death-164761.jpg?w=300" data-large-file="https://otpok.com/wp-content/uploads/2020/03/death-164761.jpg?w=700" tabindex="0" role="button"></a>
			</p>
			
			<!-- .entry-header -->

		</article>

	
		<article id="post-498">

			
			<!-- .entry-header -->

		</article>

	</div>

				<!-- #nav-below -->
	
			
	<!-- #comments -->

		
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everything you need to know about Python 3.13 – JIT and GIL went up the hill (263 pts)]]></title>
            <link>https://drew.silcock.dev/blog/everything-you-need-to-know-about-python-3-13/</link>
            <guid>41677131</guid>
            <pubDate>Sat, 28 Sep 2024 01:23:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://drew.silcock.dev/blog/everything-you-need-to-know-about-python-3-13/">https://drew.silcock.dev/blog/everything-you-need-to-know-about-python-3-13/</a>, See on <a href="https://news.ycombinator.com/item?id=41677131">Hacker News</a></p>
<div id="readability-page-1" class="page"><article> <p>On October 2<sup>nd</sup>, 2024, the Python core developers and community will release <a href="https://docs.python.org/3.13/whatsnew/3.13.html">CPython v3.13.0</a> – and it’s a doozy.</p>
<p>So what makes this release different, and why should you care about it?</p>
<p>In short, there are two big changes being made to how Python runs at a core level which have the potential to radically change the performance profile of Python code in the future.</p>
<p>Those changes are:</p>
<ul>
<li>A “free-threaded” version of CPython which allows you to disable the Global Interpreter Lock (GIL), and</li>
<li>Support for experimental Just-in-Time (JIT) compilation.</li>
</ul>
<p>So what are these new features and what impact will they have on you?</p>
<a href="#global-interpreter-lock-gil">#</a><h2 id="global-interpreter-lock-gil">Global Interpreter Lock (GIL)</h2>
<a href="#what-is-the-gil">#</a><h3 id="what-is-the-gil">What is the GIL?</h3>
<p>From the inception of the Python programming language by Guido Van Rossum in a science park in East Amsterdam in the late ’80s, it was designed and implemented as a single-threaded interpreted language. What exactly does this mean?</p>
<p>You’ll commonly hear that there are 2 types of programming languages – interpreted and compiled. So which is Python? The answer is: <strong>yes</strong>.</p>
<p>You will very rarely find a programming language which is purely interpreted from source by an interpreter. For interpreted languages, the human-readable source code is almost always compiled into some kind of intermediary form, called bytecode. The interpreter then looks at the bytecode and executes the instructions one-by-one.</p>
<p>The “interpreter” here is commonly called a “virtual machine”, especially in other languages like Java which does the same thing as Python re. <a href="https://en.wikipedia.org/wiki/Java_bytecode">Java bytecode</a> and <a href="https://en.wikipedia.org/wiki/List_of_Java_virtual_machines">Java VMs</a>. In Java and <a href="https://kotlinlang.org/">friends</a>, it’s much more common to ship the compiled bytecode itself, whereas Python applications are usually distributed as source code (although, having said that, packages are often deployed as <a href="https://packaging.python.org/en/latest/discussions/package-formats/#what-is-a-wheel">wheels</a> as well as <a href="https://packaging.python.org/en/latest/discussions/package-formats/#what-is-a-source-distribution">sdist</a> nowadays).</p>
<p>Virtual machines in this meaning of the word come up in all kinds of unexpected places, like in the PostScript format (PDF files are essentially compiled PostScript) and in font rendering<sup><a href="#user-content-fn-font-rendering" id="user-content-fnref-font-rendering" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>.</p>
<p>If you’ve ever noticed a bunch of <code>*.pyc</code> files in your Python projects, this is the compiled bytecode for your application. You can decompile and explore <code>pyc</code> files in exactly the same way you can find Java class files.</p>
<p><span data-astro-cid-mrmim4ef="">💡</span> <span data-astro-cid-mrmim4ef=""> <p><strong>Python vs CPython</strong></p><p>I can already hear a chorus of pedantic Pythonistas shouting “Python isn’t the same as CPython!”, and they’re right. And this is an important distinction to make.</p><p>Python is the programming language, which is essentially a specification saying what the language should do.</p><p>CPython is the <em>reference implementation</em> of this language specification, and what we’re talking about here is mostly about the CPython implementation. There are other Python implementations, like <a href="https://pypy.org/">PyPy</a> which has always used JIT compilation, <a href="https://www.jython.org/">Jython</a> which runs on the JVM and <a href="https://ironpython.net/">IronPython</a> which runs on the .NET CLR.</p><p>Having said that, pretty much everyone just uses CPython and so I think it’s reasonable to talk about “Python” when we’re really talking about CPython. If you disagree, go ahead and get in the comments or write me a strongly worded email with an aggressive font (maybe <a href="https://www.google.com/search?q=impact+font">Impact</a>; I’ve always thought <a href="https://www.google.com/search?q=comic+sans">Comic Sans</a> has a subtly threatening aura).</p> </span> </p> 
<p>So when we run Python, the <code>python</code> executable will generate the bytecode which is a stream of instructions, then the interpreter will read and execute the instructions one-by-one.</p>
<p>If you try to spin up multiple threads, what happens then? Well, the threads all share the same memory (apart from their local variables) and so they can all access and update the same objects. Each thread will be executing its own bytecode using its own stack and instruction pointer.</p>
<p>What happens if multiple threads try to access / edit the same object at the same time? Imagine one thread is trying to add to a dict while another is trying to read from it. There are two options here:</p>
<ul>
<li>Make the implementation of dict (and all the other objects) thread-safe, which takes a lot of effort and will make it slower for a single-threaded application, or</li>
<li>Create a global mutual exclusion lock (a.k.a. mutex) which allows only one thread to be executing bytecode at any one time.</li>
</ul>
<p>This latter option is the GIL. The former option is what the Python developers are calling “free-threading” mode.</p>
<p>It’s also worth mentioning that the GIL makes garbage collection much simpler and faster. We don’t have time to go into the depths of garbage collection here as it’s a whole big topic in itself, but a simplified version is that Python keeps a count of how many references there are to a particular object, and when that count reaches zero, Python knows that it can safely delete that object. If there are multiple threads concurrently creating and dropping references to different objects, this can lead to race conditions and memory corruptions, so any free-threaded version needs to use atomically counted references for all objects.</p>
<p>The GIL also makes it much easier to develop C extensions for Python (e.g. using the confusingly named <a href="https://cython.org/">Cython</a>) as you can make assumptions about thread safety that make your life much easier, check out the <a href="https://py-free-threading.github.io/porting">py-free-threading guide for porting C extensions</a> for more details on this.</p>
<a href="#why-does-python-have-a-gil">#</a><h3 id="why-does-python-have-a-gil">Why does Python have a GIL?</h3>
<p>Despite having a surge in popularity over the last few years, it’s not a particularly new language – it was conceived in the late ’80s, with the first release on 20<sup>th</sup> February 1991 (making it slightly older than me). Back then, computers looked very different. Most programs were single-threaded and the performance of individual cores was increasing exponentially (see good old <a href="https://en.wikipedia.org/wiki/Moore%27s_law">Moore’s Law</a>). In this environment, it didn’t make much sense to compromise single-threaded performance for thread safety when most programs would not be utilising multiple cores.</p>
<p>Also, implementing thread safety obviously takes a lot of work.</p>
<p>This isn’t to say that you can’t utilise multiple cores in Python. It just means that instead of using threading, you have to utilise multiple processes (i.e. <a href="https://docs.python.org/3/library/multiprocessing.html"><code>multiprocessing</code></a> module).</p>
<p>Multi-processing differs from multi-threading because each process is its own Python interpreter with its own separate memory space. This means that multiple processes can’t access the same objects in memory but instead you have to use special constructs and communication to share data (see <a href="https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes">“Sharing state between processes”</a> and <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue"><code>multiprocessing.Queue</code></a>).</p>
<p>It’s worth mentioning that there is a bit more overhead in using multiple processes as opposed to multiple threads, in addition to it being more difficult to share data.</p>
<p>Using multiple threads is sometimes not as bad as people commonly assume, however. If Python is doing I/O like reading from files or making network calls, it will release the GIL so that other threads can run. This means that if you’re doing lots of I/O, multi-threading will often be as fast as multi-processing. It’s when you are CPU-bound that the GIL becomes a big issue.</p>
<a href="#ok-so-why-are-they-removing-the-gil-now">#</a><h3 id="ok-so-why-are-they-removing-the-gil-now">Ok, so why are they removing the GIL now?</h3>
<p>The removal of the GIL has been something that certain people have been pushing for for several years now, but the main reason it’s not been done is not the amount of work it takes but instead is the corresponding performance degradation that would come with it for single-threaded programs.</p>
<p>Nowadays, the incremental improvements in single-core performance of computers doesn’t change too much from year to year (although big advances are being made with custom processor architectures, e.g. Apple Silicon chips) while the number of cores in a computer continues to increase. This means it’s much more common for programs to utilise multiple cores and hence the inability of Python to properly utilise multi-threading is becoming more and more of an issue.</p>
<p>Fast forward to 2021 and <a href="https://github.com/colesbury">Sam Gross</a> implemented a <a href="https://lwn.net/ml/python-dev/CAGr09bSrMNyVNLTvFq-h6t38kTxqTXfgxJYApmbEWnT71L74-g@mail.gmail.com/">no-GIL Proof of Concept implementation</a> that spurred the <a href="https://github.com/python/steering-council">Python Steering Council</a> to propose a vote on <a href="https://peps.python.org/pep-0703/">PEP 703 – Making the Global Interpreter Lock Optional in CPython</a>. The outcome of the vote was positive, resulting in the Steering Council <a href="https://discuss.python.org/t/a-steering-council-notice-about-pep-703-making-the-global-interpreter-lock-optional-in-cpython/30474">accepting the proposal</a> as part of a <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional-in-cpython-acceptance/37075">gradual rollout</a> in three phases:</p>
<ul>
<li><strong>Phase 1</strong>: Free-threading mode is an experimental build-time option that is not the default.</li>
<li><strong>Phase 2</strong>: Free-threading mode is officially supported but still not the default.</li>
<li><strong>Phase 3</strong>: Free-threading mode is the default.</li>
</ul>
<p>From reading the discussions, there’s a strong desire to not “split” Python into two separate implementations – one with the GIL and one without – with the intention being that eventually after free-threading mode has been the default for a while, the GIL will be removed entirely and the free-threading mode will be the only mode.</p>
<p>While all this GIL vs. no-GIL stuff has been going on the last few years, there has been a parallel effort called the “Faster CPython” project. This has been <a href="https://lwn.net/ml/python-dev/CAGr09bSrMNyVNLTvFq-h6t38kTxqTXfgxJYApmbEWnT71L74-g@mail.gmail.com/">funded by Microsoft</a> and led by <a href="https://us.pycon.org/2023/speaker/profile/81/index.html">Mark Shannon</a> and <a href="https://gvanrossum.github.io/">Guido van Rossum</a> himself, both of whom work at Microsoft.</p>
<p>The effort this team have been making has produced some very impressive results, particularly for <a href="https://docs.python.org/3/whatsnew/3.11.html#faster-cpython">3.11</a> which boasted significant performance boosts over 3.10.</p>
<p>With the combination of community / council support, increasing ubiquity of multi-core processors and the Faster CPython effort, the time was ripe for the beginning of Phase 1 of the GIL adoption plan.</p>
<a href="#what-does-the-performance-look-like">#</a><h3 id="what-does-the-performance-look-like">What does the performance look like?</h3>
<p>I’ve run a few benchmarks on both my machine – MacBook Pro with Apple M3 Pro (CPU has 6 performance cores and 6 efficiency cores) – and on a quiet EC2 instance – t3.2xlarge (8 vCPUs).</p>
<p>The graphs below show a comparison of the runtime performance of a CPU-intensive task (converging Mandelbrot set) between Python 3.12 and Python 3.13 with and without the GIL.</p>
<img src="https://drew.silcock.dev/_astro/bench-comparison-m3-dark.B6MCcmGa_96BpF.webp" alt="Performance comparison for Apple M3 Pro" width="2400" height="800" loading="lazy" decoding="async">
<img src="https://drew.silcock.dev/_astro/bench-comparison-m3-light.CBPn1teC_Z1H6EDr.webp" alt="Performance comparison for Apple M3 Pro" width="2400" height="800" loading="lazy" decoding="async">
<img src="https://drew.silcock.dev/_astro/bench-comparison-ec2-dark.DsCVDY3z_2hOeU1.webp" alt="Performance comparison for t3.2xlarge EC2 instance" width="2400" height="800" loading="lazy" decoding="async">
<img src="https://drew.silcock.dev/_astro/bench-comparison-ec2-light.ojc_JmYg_Z1h2QNH.webp" alt="Performance comparison for t3.2xlarge EC2 instance" width="2400" height="800" loading="lazy" decoding="async">
<p>(These graphs aren’t the most readable, I know – I’ll improve on them when I get some time.)</p>
<p>To explain what these runtimes mean:</p>
<ul>
<li><code>3.12.6</code> – Python version 3.12.6.</li>
<li><code>3.13.0rc2</code> – the default build of Python 3.13.0 release candidate 2 (the latest version at the time of writing).</li>
<li><code>3.13.0rc2t</code> – the Python 3.13.0 release candidate 2 with experimental free-threading enabled at build-time, run without additional arguments (i.e. GIL disabled).</li>
<li><code>3.13.0rc2t-g1</code> – the Python 3.13.0 release candidate 2 with experimental free-threading enabled at build-time, run with the <code>-X gil=1</code> argument, thereby “re-enabling” the GIL at runtime.</li>
</ul>
<p>A few caveats to this:</p>
<ul>
<li>I didn’t use a proper well established benchmark, just a simple iterative algorithm. You can find the code for running the benchmarks and graphing the results at: <a href="https://github.com/drewsilcock/gil-perf">github.com/drewsilcock/gil-perf</a>. Try it out for yourself!</li>
<li>I used <a href="https://github.com/sharkdp/hyperfine">hyperfine</a> to run the benchmarks, which is a really good tool, but these aren’t proper scientific benchmarks running on dedicated hardware. My MacBook is running a whole bunch of things and even the EC2 instance will have stuff going on in the background, although not nearly as much.</li>
<li>These benchmarks are really interesting and fun to talk about, but do bear in mind that in the real world, most libraries that do CPU-intensive work use <a href="https://cython.readthedocs.io/en/latest/src/userguide/nogil.html">Cython</a> or similar&nbsp;– very few people use raw Python for very compute-intensive tasks. Cython has the ability to release the GIL temporarily during execution and has had for a while. These benchmarks aren’t representative of this use case.</li>
</ul>
<p>With that in mind, we can already make a few observations:</p>
<ul>
<li>The performance degradation when Python is built with free-threading support is significant – around 20%.</li>
<li>It doesn’t matter whether you re-enable the GIL via the <code>-X gil=1</code> argument, the performance degradation is the same.</li>
<li>Multi-threading shows a significant boost with GIL disabled, as expected.</li>
<li>Multi-threading with GIL enabled is slower than single-threading, as expected.</li>
<li>Multi-threading with GIL disabled is about the same as multi-processing. Then again, this is a pretty noddy example where you don’t need to do much real work.</li>
<li>Apple Silicon chips really are quite impressive. Single-threaded performance on my M3 Pro is about 4x faster than single-threaded performance on the t3.2xlarge. I mean, I know t3 are designed to be cheap and burstable, but even so! It’s even more impressive if you consider the insane battery life you get out of these things<sup><a href="#user-content-fn-apple-sponsorship" id="user-content-fnref-apple-sponsorship" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>.</li>
</ul>
<a href="#how-do-i-try-out-free-threaded-python">#</a><h3 id="how-do-i-try-out-free-threaded-python">How do I try out free-threaded Python?</h3>
<p>At the time of writing, Python 3.13 is still in release candidate and hasn’t been officially released. Having said that, today is Saturday 28<sup>th</sup> and the release is scheduled for 2<sup>nd</sup> October which is Wednesday, so its not far away.</p>
<p>If you want to try it out ahead of time, you’re out of luck with <a href="https://rye.astral.sh/">rye</a> which only seems to ship deployed versions and <a href="https://docs.astral.sh/uv/">uv</a> which includes the 3.13.0rc2 build but not the 3.13.0rc2t build. Luckily, <a href="https://github.com/pyenv/pyenv">pyenv</a> supports both 3.13.0rc2 and 3.13.0rc2t. To try it out for yourself:</p>
<div><figure><pre data-language="shell"><code><div><div><p>1</p></div><p><span># If you're reading this from the future, rye may have it:</span></p></div><div><div><p>2</p></div><p><span>$</span><span> </span><span>rye</span><span> </span><span>toolchain</span><span> </span><span>list</span><span> </span><span>--include-downloadable</span><span> </span><span>|</span><span> </span><span>rg</span><span> </span><span>-F</span><span> </span><span><a href="https://drew.silcock.dev/cdn-cgi/l/email-protection" data-cfemail="1e7d6e676a7671705e2d302f2d">[email&nbsp;protected]</a></span></p></div><div><p>3</p></div><div><div><p>4</p></div><p><span># uv may also have it</span></p></div><div><div><p>5</p></div><p><span>$</span><span> </span><span>uv</span><span> </span><span>python</span><span> </span><span>list</span><span> </span><span>|</span><span> </span><span>rg</span><span> </span><span>-F</span><span> </span><span>cpython-3.13</span></p></div><div><p>6</p></div><div><div><p>7</p></div><p><span># pyenv should have it, though.</span></p></div><div><div><p>8</p></div><p><span>$</span><span> </span><span>pyenv</span><span> </span><span>install</span><span> </span><span>--list</span><span> </span><span>|</span><span> </span><span>rg</span><span> </span><span>'^\s+3\.13'</span></p></div><div><p>9</p></div><div><div><p>10</p></div><p><span># Take 3.13.0rc2t for a spin</span></p></div><div><div><p>11</p></div><p><span>$</span><span> </span><span>pyenv</span><span> </span><span>install</span><span> </span><span>3.13.0rc2t</span></p></div><div><div><p>12</p></div><p><span>$</span><span> </span><span>pyenv</span><span> </span><span>local</span><span> </span><span>3.13.0rc2t</span></p></div><div><p>13</p></div><div><div><p>14</p></div><p><span>$</span><span> </span><span>python</span><span> </span><span>-VV</span></p></div><div><div><p>15</p></div><p><span>Python</span><span> </span><span>3.13.0rc2</span><span> </span><span>experimental</span><span> </span><span>free-threading</span><span> </span><span>build</span><span> (main, </span><span>Sep</span><span> </span><span>18</span><span> </span><span>2024,</span><span> </span><span>16:41:38</span><span>) [Clang </span><span>15.0.0</span><span> (clang</span><span>-</span><span>1500</span><span>.</span><span>3</span><span>.</span><span>9</span><span>.</span><span>4</span><span>)]</span></p></div><div><p>16</p></div><div><div><p>17</p></div><p><span>$</span><span> </span><span>python</span><span> </span><span>-c</span><span> </span><span>'import sys;print("GIL enabled 🔒" if sys._is_gil_enabled() else "GIL disabled 😎")'</span></p></div><div><div><p>18</p></div><p><span>GIL</span><span> </span><span>disabled</span><span> </span><span>😎</span></p></div><div><p>19</p></div><div><div><p>20</p></div><p><span># GIL can be re-enabled at runtime</span></p></div><div><div><p>21</p></div><p><span>$</span><span> </span><span>python</span><span> </span><span>-X</span><span> </span><span>gil=</span><span>1</span><span> </span><span>-c</span><span> </span><span>'import sys;print("GIL enabled 🔒" if sys._is_gil_enabled() else "GIL disabled 😎")'</span></p></div><div><div><p>22</p></div><p><span>GIL</span><span> </span><span>enabled</span><span> </span><span>🔒</span></p></div></code></pre></figure></div>
<p>Just a heads up if you are trying free-threading Python – if you don’t specify either <code>-X gil=0</code> or <code>-X gil=1</code>, the GIL will be disabled by default but simply importing a module which does not support running without the GIL will cause the GIL to be re-enabled. I found this when running the benchmarks because I imported matplotlib, which results in the GIL being re-enabled and all my benchmarks coming out rubbish. If you manually specify <code>-X gil=0</code>, the GIL will not be sneakily re-enabled, even if a package does not mark itself as supporting GIL-free running.</p>
<a href="#jit-just-in-time-compiler">#</a><h2 id="jit-just-in-time-compiler">JIT (Just-in-Time) Compiler</h2>
<p>It’s not just the GIL that’s a big change in this Python release – there’s also the addition into the Python interpreter of an experimental JIT compiler.</p>
<a href="#what-is-a-jit">#</a><h3 id="what-is-a-jit">What is a JIT?</h3>
<p>JIT stands for Just in Time and is a compilation technique where machine code is produced just in time to execute it, as opposed to ahead of time (AOT) like your traditional C compiler like gcc or clang.</p>
<p>We already talked about bytecode and the interpreter earlier. The important thing is that, before Python 3.13, the interpreter would look at each bytecode instruction one at a time and turn each one into native machine code before executing it. With the introduction of the JIT compiler, it is now possible for bytecode to be “interpreted” into machine code once and updated as necessary, instead of being re-interpreted every time.</p>
<p>It’s important to point out that this kind of JIT that has been <a href="https://github.com/python/cpython/pull/113465">introduced in 3.13</a> is what’s called <a href="https://en.wikipedia.org/wiki/Copy-and-patch">“copy-and-patch” JIT</a>. This is a very recent idea introduced in 2021 in an article called <a href="https://dl.acm.org/doi/10.1145/3485513">“Copy-and-patch compilation: a fast compilation algorithm for high-level languages and bytecode</a>. The core idea behind copy-and-patch as opposed to more advanced JIT compilers is that there is a simple list of pre-generated templates – the JIT compiler will pattern match for bytecode that matches one of the pre-defined templates and if it does, it will patch in pre-generated native machine code.</p>
<p>Traditional JIT compilers will be massively more advanced that this and also massively more memory intensive, especially if you compare it to heavily JIT-compiled languages like Java or C#. (That’s part of the reason Java programs take up so much memory.)</p>
<p>What’s great about JIT compilers is that they can adapt to your code as its running. For instance, as your code runs, the JIT compiler will keep track of how “hot” each piece of code is. JIT compilers can perform incremental optimisations as the code get hotter and hotter and even use information about how the program is actually running to inform the optimisations it is making (like how Profile-Guided Optimisation does for AOT compilers). This means that JIT doesn’t waste time optimising some code which is only running once but the really hot sections of code can have heavy run-time informed optimisations done on them.</p>
<p>Now, the JIT compiler in Python 3.13 is relatively simple and won’t be doing any crazy at this stage, but it’s a really exciting development for the future of Python performance.</p>
<a href="#what-difference-will-the-jit-make-to-me">#</a><h3 id="what-difference-will-the-jit-make-to-me">What difference will the JIT make to me?</h3>
<p>In the short term, the introduction of the JIT is unlikely to make any difference to how you write or run your Python code. But it’s an exciting internal change to the way that the Python interpreter operates that could lead to much more significant performance improvements being made to Python performance in the future.</p>
<p>In particular, it opens up the way for incremental performance improvements to be made over time which could gradually bump up Python’s performance to be more competitive with other languages. Having said that, this is still early stages and the copy-and-patch JIT technique is both new and lightweight, so there’s more big changes needed before we start seeing significant benefits from the JIT compiler.</p>
<a href="#how-do-i-try-out-the-jit">#</a><h3 id="how-do-i-try-out-the-jit">How do I try out the JIT?</h3>
<p>The JIT compilers is “experimental” in 3.13 and isn’t built with support out of the box (at least not when I downloaded 3.13.0rc2 using pyenv). You can enable experimental JIT support by doing:</p>
<div><figure><pre data-language="shell"><code><div><div><p>1</p></div><p><span>$</span><span> </span><span>PYTHON_CONFIGURE_OPTS="--enable-experimental-jit"</span><span> </span><span>pyenv</span><span> </span><span>install</span><span> </span><span>3.13-dev</span></p></div><div><div><p>2</p></div><p><span>python-build:</span><span> </span><span>use</span><span> </span><span>openssl@3</span><span> </span><span>from</span><span> </span><span>homebrew</span></p></div><div><div><p>3</p></div><p><span>python-build:</span><span> </span><span>use</span><span> </span><span>readline</span><span> </span><span>from</span><span> </span><span>homebrew</span></p></div><div><div><p>4</p></div><p><span>Cloning</span><span> </span><span>https://github.com/python/cpython...</span></p></div><div><div><p>5</p></div><p><span>Installing</span><span> </span><span>Python-3.13-dev...</span></p></div><div><div><p>6</p></div><p><span>python-build:</span><span> </span><span>use</span><span> </span><span>tcl-tk</span><span> </span><span>from</span><span> </span><span>homebrew</span></p></div><div><div><p>7</p></div><p><span>python-build:</span><span> </span><span>use</span><span> </span><span>readline</span><span> </span><span>from</span><span> </span><span>homebrew</span></p></div><div><div><p>8</p></div><p><span>python-build:</span><span> </span><span>use</span><span> </span><span>ncurses</span><span> </span><span>from</span><span> </span><span>homebrew</span></p></div><div><div><p>9</p></div><p><span>python-build:</span><span> </span><span>use</span><span> </span><span>zlib</span><span> </span><span>from</span><span> </span><span>xcode</span><span> </span><span>sdk</span></p></div><div><div><p>10</p></div><p><span>Installed</span><span> </span><span>Python-3.13-dev</span><span> </span><span>to</span><span> </span><span>/Users/drew.silcock/.pyenv/versions/3.13-dev</span></p></div><div><div><p>11</p></div><p><span>$</span><span> </span><span>python</span><span> </span><span>-c</span><span> </span><span>'import sysconfig;print("JIT enabled 🚀" if "-D_Py_JIT" in sysconfig.get_config_var("PY_CORE_CFLAGS") else "JIT disabled 😒")'</span></p></div><div><div><p>12</p></div><p><span>JIT</span><span> </span><span>enabled</span><span> </span><span>🚀</span></p></div></code></pre></figure></div>
<p>There are additional configure options which you can read about <a href="https://discuss.python.org/t/pep-744-jit-compilation/50756">on the PEP 744 discussion page</a> (like enabling the JIT but requiring it be enabled by running <code>-X jit=1</code> at runtime, etc.).</p>
<p>The test here only checks for whether the JIT was enabled at built-time, not whether it’s currently running (e.g. has been disabled at runtime). It is possible to check at runtime whether the JIT is enabled, but it’s a bit more tricky. Here’s a script you can use to figure it out (taken from the <a href="https://discuss.python.org/t/pep-744-jit-compilation/50756/53">PEP 744 discussion page</a>)[^jit-deps]:</p>
<div><figure><pre data-language="python"><code><div><div><p>1</p></div><p><span>import</span><span> _opcode</span></p></div><div><div><p>2</p></div><p><span>import</span><span> types</span></p></div><div><p>3</p></div><div><p>4</p></div><div><div><p>5</p></div><p><span>def</span><span> </span><span>is_jitted</span><span>(f: types.FunctionType) -&gt; </span><span>bool</span><span>:</span></p></div><div><div><p>6</p></div><p><span>    </span><span>for</span><span> i </span><span>in</span><span> </span><span>range</span><span>(</span><span>0</span><span>, </span><span>len</span><span>(f.</span><span>__code__</span><span>.co_code), </span><span>2</span><span>):</span></p></div><div><div><p>7</p></div><p><span>        </span><span>try</span><span>:</span></p></div><div><div><p>8</p></div><p><span><span>            </span></span><span>_opcode.get_executor(f.</span><span>__code__</span><span>, i)</span></p></div><div><div><p>9</p></div><p><span>        </span><span>except</span><span> </span><span>RuntimeError</span><span>:</span></p></div><div><div><p>10</p></div><p><span>            </span><span># This isn't a JIT build:</span></p></div><div><div><p>11</p></div><p><span>            </span><span>return</span><span> </span><span>False</span></p></div><div><div><p>12</p></div><p><span>        </span><span>except</span><span> </span><span>ValueError</span><span>:</span></p></div><div><div><p>13</p></div><p><span>            </span><span># No executor found:</span></p></div><div><div><p>14</p></div><p><span>            </span><span>continue</span></p></div><div><div><p>15</p></div><p><span>        </span><span>return</span><span> </span><span>True</span></p></div><div><div><p>16</p></div><p><span>    </span><span>return</span><span> </span><span>False</span></p></div><div><p>17</p></div><div><p>18</p></div><div><div><p>19</p></div><p><span>def</span><span> </span><span>fibonacci</span><span>(n):</span></p></div><div><div><p>20</p></div><p><span><span>    </span></span><span>a, b </span><span>=</span><span> </span><span>0</span><span>, </span><span>1</span></p></div><div><div><p>21</p></div><p><span>    </span><span>for</span><span> _ </span><span>in</span><span> </span><span>range</span><span>(n):</span></p></div><div><div><p>22</p></div><p><span><span>        </span></span><span>a, b </span><span>=</span><span> b, a </span><span>+</span><span> b</span></p></div><div><div><p>23</p></div><p><span>    </span><span>return</span><span> a</span></p></div><div><p>24</p></div><div><p>25</p></div><div><div><p>26</p></div><p><span>def</span><span> </span><span>main</span><span>():</span></p></div><div><div><p>27</p></div><p><span><span>    </span></span><span>fibonacci(</span><span>100</span><span>)</span></p></div><div><div><p>28</p></div><p><span>    </span><span>if</span><span> is_jitted(fibonacci):</span></p></div><div><div><p>29</p></div><p><span>        </span><span>print</span><span>(</span><span>"JIT enabled 🚀"</span><span>)</span></p></div><div><div><p>30</p></div><p><span>    </span><span>else</span><span>:</span></p></div><div><div><p>31</p></div><p><span>        </span><span>print</span><span>(</span><span>"Doesn't look like the JIT is enabled 🥱"</span><span>)</span></p></div><div><p>32</p></div><div><p>33</p></div><div><p>34</p></div><div><div><p>35</p></div><p><span>if</span><span> </span><span>__name__</span><span> </span><span>==</span><span> </span><span>"__main__"</span><span>:</span></p></div><div><div><p>36</p></div><p><span><span>    </span></span><span>main()</span></p></div></code></pre></figure></div>
<p>The PEP 744 discussion has mention of both <code>PYTHON_JIT=0/1</code> and <code>-X jit=0/1</code> – I did not find that the <code>-X</code> option did anything at all, but the environment variable seems to do the trick.</p>
<div><figure><pre data-language="shell"><code><div><div><p>1</p></div><p><span>$</span><span> </span><span>python</span><span> </span><span>is-jit.py</span></p></div><div><div><p>2</p></div><p><span>JIT</span><span> </span><span>enabled</span><span> </span><span>🚀</span></p></div><div><div><p>3</p></div><p><span>$</span><span> </span><span>PYTHON_JIT=</span><span>0</span><span> </span><span>python</span><span> </span><span>is-jit.py</span></p></div><div><div><p>4</p></div><p><span>Doesn</span><span>'t look like the JIT is enabled 🥱</span></p></div></code></pre></figure></div>
<p>[^jit-deps] I also found a few people online talking about how you could use <code>sysconfig.get_config_var("JIT_DEPS")</code> but I did not found that this worked at all for me.</p>
<a href="#conclusion">#</a><h2 id="conclusion">Conclusion</h2>
<p>Python 3.13 is a big release in introducing some exciting new concepts and features to the runtime. It’s unlikely to make any immediate different to how you write and run your Python, but it’s likely that over the next few months and years as both free-threading and JIT become more mature and well established, they’ll begin to have more and more of an impact on the performance profile of Python code, particularly for CPU-bound tasks.</p>
<a href="#further-reading">#</a><h2 id="further-reading">Further reading</h2>
<ul>
<li><a href="https://peps.python.org/pep-0703/">PEP 703 – Making the Global Interpreter Lock Optional in CPython</a></li>
<li><a href="https://py-free-threading.github.io/">py-free-threading</a></li>
<li><a href="https://tonybaloney.github.io/posts/python-gets-a-jit.html">Python 3.13 gets a JIT – Anthony Shaw</a></li>
<li><a href="https://peps.python.org/pep-0744/">PEP 744 – JIT Compilation</a></li>
<li><a href="https://discuss.python.org/t/pep-744-jit-compilation/50756">Discuss – PEP 744: JIT Compilation</a></li>
</ul>
<section data-footnotes=""><a href="#footnote-label">#</a>
<ol>
<li id="user-content-fn-font-rendering">
<p>Font rendering is a fascinating topic and that immensely complex. Trust me, however complicated you think font rendering is, it’s more complicated that that. IIRC most of the complexity actually comes from nicely drawing text at small resolutions. For instance, in TrueType both a whole font and individual glyphs have instructions associated with them which are executed by the FontEngine virtual machine a.k.a. interpreter. If this is something you’re interested in learning more about, I highly recommend Sebastian Lague’s video – <a href="https://www.youtube.com/watch?v=SO83KQuuZvg&amp;pp=ygUOZm9udCByZW5kZXJpbmc%3D">Coding Adventure: Rendering Text</a>. He makes really great videos. The <a href="https://developer.apple.com/fonts/TrueType-Reference-Manual/RM02/Chap2.html#how_works">TrueType reference</a> is also surprisingly readable. <a href="#user-content-fnref-font-rendering" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-apple-sponsorship">
<p>Apple aren’t even paying me to say this stuff, it’s just true. <a href="#user-content-fnref-apple-sponsorship" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
</ol>
</section>   </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[If WordPress is to survive, Matt Mullenweg must be removed (232 pts)]]></title>
            <link>https://joshcollinsworth.com/blog/fire-matt</link>
            <guid>41676653</guid>
            <pubDate>Fri, 27 Sep 2024 23:49:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joshcollinsworth.com/blog/fire-matt">https://joshcollinsworth.com/blog/fire-matt</a>, See on <a href="https://news.ycombinator.com/item?id=41676653">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" tabindex="-1"><article><img src="https://joshcollinsworth.com/images/post_images/beatings.webp" alt="" width="1920" height="1388">

		

		<p><b>Published:</b>
			September 27, 2024
			</p>

		
		<div><p>This post is a little more hasty than some of my others, in the interest of expedience. I hope you’ll bear with the poorly edited jumble of thoughts. It’s being actively edited. I also usually avoid cussing on my blog, but I do a little here because it feels warranted.</p>
<p>Cover image from <a href="https://www.etsy.com/listing/1341940035/the-beatings-will-continue-until-morale" rel="nofollow">this Etsy store</a> (unaffiliated).</p></div>
<p>There are some people who think being right about something gives them the right to do whatever they think should be done about it; a license to act however they see fit in order to correct that wrong.</p>
<p>This, of course, is never the case. Doing the wrong thing for the right reason never makes it the right thing. No matter how bad the original infraction, there are some responses it never justifies. Two wrongs don’t make a right, to be pithy about it. The ends don’t justify…you know how it goes.</p>
<p>Matt Mullenweg appears to be one of those people who believe the ends do indeed justify the means, as he’s effectively blowing up massive swaths of the WordPress community in his fight with some of its landlords.</p>
<p>Matt has, for far too long, enjoyed unchecked powers at the top of WordPress—powers which are all too often a direct and flagrant conflict of interest. And while we’ve seen this power abused before, we’ve never seen it on this scale.</p>
<p>Yes, Matt’s original point <em>might</em> be warranted. But his egregious actions utterly nullify any previous merit.</p>
<p>A line has been crossed, and the entire community is worse for it.</p>


<p>I believe that if WordPress is to survive, let alone thrive, Matt Mullenweg must be removed from all forms of official WordPress leadership, as expediently as possible.</p>

<h2 id="wait-who-are-you-and-why-do-you-care">Wait, who are you and why do you care?</h2>
<p>Let’s get this out of the way right off: I’m not the best person to be talking about this. I haven’t really been involved in WordPress for about five years now. Honestly, I couldn’t tell you the last time I even logged in to a WordPress site.</p>
<p>That said, however: I spent some six or seven years of my life deep in the WordPress world. I built and customized WordPress sites for clients as a designer; I taught a WordPress development course (focused on building custom themes in PHP) for about five years; and I worked in support for Flywheel, a managed WordPress hosting company, for a little over five years. It was there I transitioned to full-time frontend work, building tools to help support WordPress sites.</p>
<p>So while I’ve been out of the WordPress game for a good while now, I still might be considered an expert next to your average Joe. I’d like to think I could still sling some theme templates with the best of ‘em. (Hell, some days I even get a little nostalgic and think about booting up a Local site just for fun.)</p>
<p>You might have spotted the word “Flywheel” up there and realized that company was acquired by WP Engine—the company with which Mullenweg is publicly feuding at the moment—back in 2019. That might reasonably raise questions of my objectivity, so let’s get this out of the way:</p>
<p>Yes, I used to work for WP Engine. I even kinda liked them, for a while (mostly while they just kinda left us alone for the first year or so). But I wouldn’t say my time at the company left a good taste in my mouth.</p>
<p>We don’t need to dredge up a bunch of old and buried stuff that isn’t really important anyway, but suffice to say: I really don’t have any reason to be a WP Engine cheerleader. Most of the people I knew there have left, and I’ve watched from the sidelines as the company has implemented a bunch of scummy policies and shady sales tactics to squeeze money from their customers and make it harder to leave.</p>
<p>On most days, if you wanted to have a conversation about how much WP Engine sucks, frankly, I’d be a happy participant.</p>
<p>So this post might be a lot of things, but I can assure you it’s <em>not</em> me defending my old company just because I used to work for them. I’ve got literally no reason to do that.</p>


<p>To the extent I’m on WP Engine’s side, it’s not because of any sense of loyalty to the company or to the remaining good people I know there; it’s because I believe what Matt’s doing is deeply wrong and foolishly destructive.</p>

<p>I’ll also go on record as saying I got pretty far in the interview process at Automattic once, a few years back. And, since we’re being honest, it was the absolute <em>worst</em> interview process I’ve ever taken part of as a web professional (though the people themselves were lovely). But that alone ain’t gonna get a post out of me. I’m not wasting my time and yours just to gripe about an interview I chose to drop out of over three years ago. Just thought it merited a mention.</p>
<p>I still regarded Matt Mullenweg himself pretty highly after that, up until the last year or so. This post isn’t long enough to get into the details, but Matt had already become a pretty “problemattic” character well before any of this went down.</p>
<p>So in summary: I’m not a big fan of either party, and I don’t have any good reason to side with either one of them.</p>
<p>I <em>am</em>, however, somebody who still cares deeply about WordPress. It’s what gave me my start, and I still recommend it to a lot of people when they ask me what system might best suit their needs.</p>
<p>It’s a wonderful community, all in all, and despite my inactivity, I still feel invested in WordPress, and interested in seeing it continue to be a productive way to democratize the web.</p>
<p>Finally: I am not a lawyer, and since it’s Friday now and this feud had already reached lawyers-involved level by Monday morning, I should be careful to clarify any legal commentary here is expressly my personal, non-expert opinion.</p>
<h2 id="im-sorry-what-happened">I’m sorry, what happened?</h2>
<p>For those of you who haven’t been following the story thus far (read: aren’t chronically online web nerds like me), let’s hit the highlights.</p>
<h3 id="automattic-approaches-wp-engine-to-offer-a-license">Automattic approaches WP Engine to offer a “license”</h3>
<p>Sometime in or around July of this year, <a href="https://automattic.com/" rel="nofollow">Automattic</a> (Matt Mullenweg’s for-profit company, which owns, among other things, <a href="https://wordpress.com/" rel="nofollow">WordPress.com</a>, a major WordPress hosting company) reached out to <a href="https://wpengine.com/" rel="nofollow">WP Engine</a> (also a for-profit company that offers WordPress hosting, and probably Automattic’s largest business rival).</p>
<p>Automattic was offering WP Engine some kind of “licensing,” at a rate of 8% of total business revenue, adding up to the eye-popping sum of several million dollars per year.</p>
<p>WP Engine apparently turned down this offer, presumably because <em>it doesn’t appear they actually need any license</em>. The term “WP” is explicitly not covered by the <a href="https://wordpressfoundation.org/trademark-policy/" rel="nofollow">WordPress trademark policy</a>, and using the term “WordPress” to describe products and services (e.g., calling yourself a “WordPress specialist,” or saying you offer “WordPress hosting”) is fully allowed, according to the policy.</p>
<p>They’ve also been in business for like 15 years now, and somehow none of this has come up before.</p>
<p>Besides, I could name dozens of companies just off the top of my head also using one or both of those terms. So the “you need a license to say this” argument seems highly targeted and extremely dubious.</p>
<h3 id="matts-rejected-so-he-tries-new-strategies">Matt’s rejected, so he tries new strategies</h3>
<p>Immediately following WP Engine’s rejection, <a href="https://wordpressfoundation.org/" rel="nofollow">the WordPress Foundation</a> (the nonprofit that governs WordPress, the open source software, and which Matt Mullenweg <em>also</em> runs, in effect if not nominally) <a href="https://www.searchenginejournal.com/wordpress-files-to-trademark-managed-wordpress-hosted-wordpress/528112/" rel="nofollow">filed to trademark the terms “Managed WordPress” and “Hosted WordPress</a>.”</p>
<p>Neither trademark has been granted at this point, nor should they; they’ve been in use for ages, and are obviously far too generic for any one organization to hold.</p>
<p>Most reasonable and knowledgeable people seem to share this opinion. Companies have been describing themselves as one or both of those terms for around 15 years at this point. (We freely called Flywheel a “managed WordPress hosting company” the entire time I worked there, and we were far from the first. We were also at one point one of WordPress.org’s recommended hosts. So…obviously, not a big deal.)</p>
<p>Anyway, this filing of spurious trademarks makes it appear very much like Matt’s endgame was to extract money from WP Engine, but he just needed more of a foundation to do it (pun intended?). So, following that initial rejection, Matt set the Foundation arm of WordPress working on securing highly dubious trademarks, which, again, I and most reasonable observers think and hope will fail.</p>
<p>Meanwhile, Matt <em>also</em> began sending a series of very apparently extortive messages to WP Engine leadership, essentially demanding they pay up or else. (This is all in <a href="https://wpengine.com/wp-content/uploads/2024/09/Cease-and-Desist-Letter-to-Automattic-and-Request-to-Preserve-Documents-Sent.pdf" rel="nofollow">WP Engine’s letter to Automattic</a>, which I’m getting to, but which comes later in the story.)</p>
<p>All of this was in the run-up to <a href="https://us.wordcamp.org/2024/" rel="nofollow">WordCamp US</a>, the largest WordPress event of the year, at least in North America. (Of note: WP Engine <a href="https://us.wordcamp.org/2024/sponsors/" rel="nofollow">sponsored this event</a> at the highest level, as did WordPress.com.)</p>
<p>Matt let WP Engine leadership know, via private DMs, that he intended to “go nuclear” and “scorched earth” on WP Engine in his keynote at the conference—that is, if WP Engine failed to acquiesce to his monetary demands, i.e., 8% of total revenue, i.e., tens of millions of dollars. It appears he repeated the “just pay up and I’ll make this all go away” offer up to the literal last minute before he went on stage.</p>


<p>Let’s not beat around the bush: words like “threat” and “extortion” very much apply to Matt’s behavior here.</p>

<p>Again: this demand was ostensibly in exchange for a “license” to use terms like “WordPress,” “WordPress hosting,” “WooCommerce,” etc.—none of which appear to be actually necessary.</p>
<p>The only <em>possible</em> exception seems to be “WooCommerce,” which <em>is</em> a trademark (and product/company) owned by Automattic. However, the lines are very blurry on what is and is not permissable when it comes to using the WooCommerce name. WP Engine does indeed call one of its offerings “WooCommerce hosting,” which is explicitly called out in the guidelines. So I don’t know, <em>maybe</em> there’s validity there. <em>Maybe</em>.</p>
<p>However, for one thing, it’s hard to know whether, or how much the trademark guidelines might have changed. Matt made several changes to the WordPress license page in the last week, among other things, to call out WP Engine. That makes me not trust that the WooCommerce license page I’m looking at today is the same as it was last week—which, all on its own, should be setting off raging alarms for even the most casual of observers. It’s extremely bad news when the company you’re doing business with can just decide what the new terms are with no warning or recourse.</p>
<p>Anyway, Matt keeps sending the DMs all the way up until the literal last minute, offering <em>not</em> to excoriate WP Engine onstage during his keynote at the country’s (continent’s? world’s?) largest WordPress event, provided they simply pay up.</p>
<p>Once more: I’m no lawyer, but I’m pretty sure that’s called extortion.</p>
<p>WP Engine says no (actually, they ask for more time, which Matt denies and takes as a no), so he proceeds with operation “scorched earth,” and blasts WP Engine both onstage at WordCamp US, and in several other venues.</p>
<h3 id="waitwhats-matts-actual-deal-why-is-he-doing-this">Wait—what’s Matt’s actual deal? Why is he doing this?</h3>
<p>Aside from the licensing issue, which I covered above (and which seems like a mostly flimsy premise to me), Matt’s got some other complaints with WP Engine. Some have validity, some seem completely made-up. Let’s walk through them.</p>
<h4 id="matt-claims-wp-engine-is-misrepresenting-itself">Matt claims WP Engine is misrepresenting itself</h4>
<p>Among Matt’s complaints: that WP Engine is “misrepresenting” itself as an entity that’s officially affiliated with and/or endorsed by WordPress itself. Matt’s repeatedly used as an example his own mom’s confusion; she apparently thought WP Engine was somehow affiliated with WordPress.com (I guess because they also use the word “WordPress,” and are maybe a vaguely similar shade of blue).</p>


<p>I’m sure it’s frustrating, having taken over half the internet and being worth hundreds of millions of dollars, only to find out your own mom <em>still</em> doesn’t really understand what you do, but: come on, bro.</p>

<p>First, <em>tons</em> of companies use “WP” in their names, and/or the names of their products. Why isn’t Matt going after them?</p>
<p>Second, as many people have already noted: Matt effectively runs both <code>wordpress.com</code> <em>and</em> <code>wordpress.org</code>, which are entirely separate entities that do two completely different things. You wanna tell me <em>that’s</em> clear, but somehow WP Engine and WordPress.com are too similar? Really?</p>
<p>Third, my kindergartner and every kid in his class could tell the difference between the WordPress W and WP Engine’s dumb logo. (WP Engine’s logo has always been a grid of weird, almost-square shapes that’s apparently meant to vaguely resemble an engine, but which makes no sense to pretty much anyone who’s ever seen it, far as I can tell. It’s a bad logo, in my professional opinion as a designer, even the slightly better version they just released recently. But I digress. Point is: it looks literally nothing like any WordPress logo. Also: it’s not the same color. I have color vision deficiency, and even <em>I</em> can tell that.)</p>
<p>Finally, for the whole two years I worked for Shopify, most of my family thought I was at Spotify. Now I’m at Deno, and nobody in my family has any clue what a JavaScript runtime is, and my dad basically thinks I work for Java.</p>
<p>Family members don’t always get tech. That’s not a sign that something is wrong, and it’s most <em>certainly</em> not a sign that any wrongdoing has been committed, let alone deliberately. (Which, I assume, probably wasn’t Matt’s mom’s point to begin with, but that didn’t stop him from running with it.)</p>
<h4 id="matt-claims-wp-engine-is-selling-a-cheap-knock-off-of-wordpress">Matt claims WP Engine is selling a “cheap knock-off” of WordPress</h4>
<p>Matt also claims WP Engine is selling “something that they’ve chopped up, hacked, butchered to look like WordPress.” His reason for this wild claim? Because WP Engine disables revisions (a default feature of WordPress, albeit a pretty small one).</p>
<p>Literally, that’s it. One tiny feature. The whole thing’s been “hacked and butchered” because they just chose to modify one tiny detail.</p>
<p>Of all Matt’s spurious claims, this one might be the one that reeks the most of absolute made-up bullshit. WP Engine will just turn on revisions if you want them to, but that’s beside the point.</p>
<p>First, if I decide to build something with, say, Laravel, but decide there’s one feature I want to turn off, I’m not “hacking and butchering” Laravel. That’s obviously ridiculous.</p>
<p>Second, pretty much all hosts limit revisions in some way or another anyway, because they take up a ton of memory and most people don’t really need them that bad.</p>
<p>And third, <em>it’s open-source software</em>! You don’t get to tell people how they use it!</p>
<p>We could also get into the utter hypocrisy that many of WordPress.com’s plans do far, far, <em>far</em> more invasive modifications of WordPress core (<em>you can’t even install themes and plugins, FFS!</em>), but again, that’s all beside the point. It’s open-source. They can do that. Anyone can. It’s in the license. This claim is clearly total garbage.</p>
<h4 id="matt-says-wp-engine-doesnt-give-back-enough">Matt says WP Engine doesn’t give back enough</h4>


<p>Matt’s other complaint—and I think this is what everything else really boils down to—is: WP Engine doesn’t give back enough to WordPress, in Matt’s estimation.</p>

<p>Matt showed some numbers onscreen at WCUS, comparing Automattic’s contributions to WP Engine’s. But I’m not going to repeat them because I’m certain they’re distorted. Besides, I’m not sure the two companies’ work can, or <em>should</em>, be considered directly comparable in the first place. They do different things in different ways, and there’s no law or license mandating either of them do anything to begin with.</p>
<p>Regardless, Matt seems irked that WP Engine isn’t abiding by the <a href="https://wordpress.org/five-for-the-future" rel="nofollow">“Five for the Future” program, outlined on WordPress.org</a>. Five for the Future asks that if you benefit from WordPress, you give back 5% of your time directly to that open-source project, which I think pretty much everyone can agree is a very noble and admirable aspiration that companies such as these involved <em>should</em> probably be doing.</p>
<p>But it’s not a requirement, or a policy, and enforcing it as such—acting unilaterally as the WordPress police, let alone so suddenly and violently—is extremely questionable and deeply troubling. (Not to mention a likely deterrent for people and organizations who might want to participate in the WordPress space.)</p>
<p>Matt’s claimed he/Automattic have been soliciting WP Engine for increased contributions for “years,” and that they’ve given “$0” to the WordPress foundation. To the best of my knowledge, neither of those claims has been substantiated, but I suppose they don’t really change this discussion much either way, because again: Matt’s taken it upon himself to act as the WP PD to enforce a law that isn’t even a law.</p>
<p>So that’s it; that’s what Matt’s mad about. There’s <em>some</em> substance there, and in a vacuum, I think he’d probably have a lot of people on his side.</p>
<p>But we’re not in a vacuum; there’s a lot of context here. So I’d like to talk about that next.</p>
<h3 id="an-aside-on-motivations-and-justifications">An aside on motivations and justifications</h3>
<p>Having explored Matt’s complaints, I’d like to pause for a moment, because this is where the sides seems to diverge.</p>
<p>The relatively small number of people in the community who appear to remain on Matt’s side (which seems to be mostly made up of his own employees and some people with their own reasons for hating WP Engine) appear to be sticking with him because they agree with <em>this</em> core point, i.e.: WP Engine should be doing more—maybe much more—especially considering that they’re a company owned by private equity and making significant money off WordPress.</p>
<p><strong>On its own, I think that claim seems perfectly fair</strong>. We could disagree about the details, or how much is too much or too little, but I don’t think it’s unreasonable to say a company the size and profitability of WPE probably owes quite a lot to the open-source software it’s built on (ethically, at least; likely not legally).</p>


<p>So it bears mentioning that WP Engine actually <em>does</em> do a pretty good deal for WordPress. You can cherry-pick specific ways it hasn’t contributed much, and you could certainly make a reasonable case they should be doing more. But to say they’ve given “$0” strikes me as pretty deliberately misleading.</p>

<p>WP Engine pays several staff members to contribute work hours to WordPress core (again, maybe the number should be greater than it is, but it’s definitely not zero), on top of the full-time maintenance of plugins, themes, and apps like Advanced Custom Fields, WP Migrate, WP GraphQL, Genesis, Local WP, and many others—all of which used by countless thousands of WordPress users every day.</p>
<p>This is to say nothing of WP Engine sponsoring of WordCamps, creating their own tutorials and educational material, their own events, and so on and so forth.</p>
<p>Point is: WP Engine <em>does</em> do a lot more than zero. You could argue those contributions are not “pure” (Matt does), and that they’re ultimately in service to WP Engine, and not the WordPress community.</p>
<p>But in fairness: sure, they’re all marketing tools in some form or another, but you don’t <em>have</em> to pay for any of them. They all get maintained, they all have tons of users both on and off WP Engine, and they all work no matter what host you choose. (I’m sure they’re all used on WordPress.com. I’d even use some of those things if I had to spin up a WordPress site tomorrow, even if I probably wouldn’t host on WP Engine, personally. I’d probably choose <a href="https://spinupwp.com/" rel="nofollow">SpinupWP</a>, myself, which is another company with “WP” in the name that Matt apparently doesn’t care about.)</p>
<p>Besides, Matt’s company does exactly the same thing with Jetpack, which charges $5–$50 per month, depending on tier, so…not sure where that moral high ground is supposed to be coming from. Is Automattic really gonna claim Jetpack’s paid features are purely for the altruistic benefit of the community? Why do <em>they</em> get a pass on paid features?</p>
<p>I think you could fairly, if crudely, paraphrase Matt’s argument as: “WP Engine is in it for the money, and we are in it for WordPress.”</p>
<p>That’s a really flimsy stance in my view, without even getting into whether we can, or should, have exactly the same expectations of both companies in the first place (which is at least questionable; Automattic has their hands in a lot more things than WP Engine does, including Tumblr, PocketCasts, Longreads, and many others things that may or may not be related to WordPress, along with at least two hosting companies).</p>
<p>Still, once more: there’s probably <em>some</em> validity there. WP Engine is a big company that makes lots of money, and it probably can and should do more.</p>


<p><em>Matt could’ve made that point</em>. I think most people would’ve agreed with him, if he had gone about it properly. We’d probably be lining up with him. There was a way to rally the community around this.</p>

<p><em>If</em> Matt Mullenweg had done this the right way.</p>
<p>But Matt, being Matt, <em>didn’t</em> make that point in a good way.</p>
<p>(Sorry, this post is already too long without me going into all the times in the past he’s stirred up drama and just generally been a toxic jerk to undeserving people in the WordPress community. But if you’re not aware: it’s become increasingly common. He was even adding public snarky comments on WP Engine employees’ posts, ones who had given decades of their life to the project, as recently as yesterday.)</p>


<p>Matt tried extortion, and threats, and petty, childish tantrums, and when none of that worked, he fully exercised his unmatched and unchecked powers in an inconscionable way, in order to extract millions of dollars from WP Engine to put in his own for-profit competitor’s bank account.</p>

<p>But I’m getting ahead of myself.</p>
<p>So that’s the core of this whole thing; Matt thinks private equity is ruining everything and taking too much without giving enough back. It’s an easy home run of a point to make in this economy. Pretty much nobody disagrees with that.</p>
<p>Maybe he thought he’d come off like Robin Hood in this whole deal. I don’t know. But if there was a way to tactfully and gracefully thread that needle, it wasn’t the rampaging hippopotamus approach Matt took.</p>
<p>The split in the community seems to lie in whether that core point <em>justifies</em> Matt’s actions.</p>
<p>It seems to me that most people agree it does <em>not</em>; that Matt’s committed too many flagrant fouls of his own for the original infraction to matter.</p>


<p>Matt had a problem with the landlords, so he carpet bombed the neighborhood. He didn’t like Alderaan’s leaders, and so he fired the Death Star. And now it doesn’t really matter what his original point was; he’s made himself the bad guy.</p>

<p>Anyway, back to the timeline. (Note: I may have the chronology slightly mixed up here on a few of the points, but I don’t think it should really matter.)</p>
<h3 id="the-wordcamp-us-fallout-and-matts-abuse-of-power">The WordCamp US fallout and Matt’s abuse of power</h3>
<p>At some point in this chaos (during his keynote at WCUS, or shortly after), Matt used his sway over every branch of power in the WordPress government to write a blog post called ”<a href="https://wordpress.org/news/2024/09/wp-engine/" rel="nofollow">WP Engine is not WordPress</a>,” (which isn’t something anybody seems to have been confused about, except of course Matt’s mom).</p>
<p>That post, crucially, went up on WordPress.org, which on its own seems questionable. WordPress.org is ostensibly the website for the nonprofit foundation; it’s supposed to exist to <em>prevent</em> any one for-profit company from having too much power over the WordPress ecosystem. It’s supposed to be agnostic.</p>
<p>Not only was that boundary ignored, but since the post was published as WordPress news, it was then <em>syndicated to each and every WordPress admin dashboard in the world</em>.</p>
<p>Forget for a second whether you agree with Matt or not; we’re getting into some of the worst of the conflicts of interest and abuses of power here.</p>
<p>This type of maneuver, plainly, is anti-competitive. It’s a flagrant exploitation of Matt’s many roles and the wild control he has over many branches of WordPress, many with conflicting priorities.</p>
<p>It’s bullying, really; WP Engine doesn’t have any tools to strike back like that. It can’t. (Maybe it <em>wouldn’t</em>, since to date, WP Engine appears to be the company with grown-ups in the room, who know to behave as though their actions will be examined in a courtroom one day.)</p>


<p>This would be like Meta one day deciding it didn’t like how a competitor was using React, and serving every single Facebook user a story on their home feed, brutally disparaging that competitor. It’s <em>clearly</em> a dramatic overreach.</p>

<p>I think Matt <em>thought</em> WP Engine had no retaliation. I think he was <em>counting</em> on this maneuver being yet another push towards their eventual acquiescence. But I guess it doesn’t matter; that’s just my speculation.</p>
<p>In any case, Matt wasn’t done. Matt went on flexing (read: abusing) his power by updating the <a href="https://wordpressfoundation.org/trademark-policy/" rel="nofollow">WordPress trademark policy</a> to <em>retroactively disincentivize the use of the term “WP” in titles of products and companies</em>. (<a href="https://www.reddit.com/r/Wordpress/comments/1foknoq/the_wordpress_foundation_trademark_policy_was/" rel="nofollow">Here’s the source on that change.</a>)</p>
<p>You know why you constantly get notifications saying “we’ve updated our terms”? Because you legally <em>have</em> to do that. To just change the terms without letting people know is shady at best, and actively malicious at worst.</p>
<p>Well, Matt just went in and changed the terms.</p>


<p>Altering the WordPress trademark policy is yet another abuse that should make any remotely impartial observer shudder. Why would anyone want to use a software with an oligarch dictating the terms, and changing them on a whim, with no warning?</p>

<p>It’s around this point in the story Matt is really losing the plot. His whole complaint with WP Engine is that they’re not helping WordPress enough.</p>
<p>But yet…he’s burning WordPress to the ground to make that point.</p>
<h3 id="wp-engines-reaction">WP Engine’s reaction</h3>
<p>Following all this, WP Engine—quite understandably—doesn’t really care for all of their users seeing that negative messaging in their wp-admin. So, WP Engine finds a way to block the news feed on WP Engine sites.</p>
<p>That would be questionable in a vacuum, to be sure. But we’re steeped in context at this point. (A lot of users either turn it off or ignore it on their own anyway, for what it’s worth.)</p>
<p>Following WordCamp US, WP Engine <em>also</em> sent a <a href="https://wpengine.com/wp-content/uploads/2024/09/Cease-and-Desist-Letter-to-Automattic-and-Request-to-Preserve-Documents-Sent.pdf" rel="nofollow">cease-and-desist</a> to Automattic. It’s pretty damning, and does a good job laying out all the points I tried to cover above. (In short: Matt tried to extort money from WP Engine for spurious licensing claims, and used disinformation, or at least heavily slanted data, to do it.)</p>


<p>One of the biggest revelations here is: Matt wanted the money he was trying to get from WP Engine to go <em>to Automattic</em>, which, again, is Matt’s for-profit company.</p>

<p>There are some pretty obvious conflicts of interest here. First and foremost, Automattic (or WordPress.com, at least) is a direct competitor of WP Engine’s.</p>
<p>Second, while Automattic <em>does</em> apparently own the WooCommerce copyright, it does <em>not</em> own the WordPress copyright. That is owned by the WordPress Foundation.</p>
<p>But it gets even murkier from there, as the Foundation is maybe (or maybe not) WordPress.org? And either way, the Foundation is apparently three people, and Matt Mullenweg is not only one of them, he appears to be <em>the only active one</em>!</p>
<p>Of the other two board members, one is a blogger whose company Matt bought out, and who apparently is no longer in the industry. The other is apparently a Partner and Managing Director at—surprise!—<em>a private equity firm</em> (not to mention a twice-failed Republican politician).</p>
<p>Wait…isn’t private equity bad? I guess not if it’s on Matt’s side. (For the record, Matt and his companies are tied up in private equity in other, more substantial ways than this, but that’s not worth getting into. It’s all pretty hypocritical.)</p>


<p>It appears neither of the other two Foundation board members is active, and therefore, Matt is essentially, behind the curtains, the King, Prime Minister, and Pope when it comes to WordPress.</p>

<p>Nobody holds any ability to check his power or challenge him. (That’s very relevant to what happens next.)</p>
<p>Also: Matt apparently kinda sorta owns WordPress.org, too. So he has a dizzying interweaving of conflicts of interest and power abuses here. (<a href="https://www.pluginvulnerabilities.com/2024/09/24/who-is-on-the-wordpress-foundation-board/" rel="nofollow">Source for all that about the foundation here</a>.)</p>


<p>Let’s not leave unspoken the irony that the guy who basically <em>is</em> WordPress.com, <em>and</em> WordPress.org, <strong><em>and</em></strong> the WordPress Foundation, wants you to think the name “WP Engine” is confusing.</p>

<p>Anyway. <a href="https://automattic.com/2024/09/25/open-source-trademarks-wp-engine/" rel="nofollow">Automattic responded</a> by sending its own cease-and-desist to WP Engine, claiming mainly that WP Engine is deliberately confusing people, and that it owes licensing to…someone. Automattic, I guess, though the lines are so blurry it’s clear the separations between WordPress entities were only ever little more than a smokescreen.</p>
<hr>
<p><strong>I should mention: most people believe WordPress.com and WordPress.org/the Foundation are two (three!?) separate entities</strong>. I sure did, before this week. I thought the two had separated many years ago, with the express intent of preventing any one for-profit company from abusing the WordPress name.</p>
<p>I guess they technically are. But when one person apparently enjoys unchecked control over all of them…</p>
<p>[Guitar begins strumming with Alanis Morissette vocalizing]</p>
<h3 id="matt-melts-down">Matt melts down</h3>
<p>Two really weird things happened on Wednesday.</p>
<p>First, <em>out of nowhere</em>, Matt decides to <a href="https://ma.tt/2024/09/charitable-contributions/" rel="nofollow">publish a post on his personal blog</a> outlining his charitable donations. He really frames it as though he’s being victimized and bullied into revealing this information, and I suppose some people were probably (reasonably) asking how much <em>he</em> gives, since he spent the whole week blowing up half the internet over how much WP Engine gives.</p>
<p>In the post, he also spends a lot more time defending himself against claims of being a “mafia boss” than most people who aren’t mafia bosses or acting like mafia bosses ever feel the need to do.</p>
<p>Weird move all around. Especially since the implication seems to be…what? “I’m a good guy so I can’t do bad things”?</p>
<p>I tried my best to look up Matt’s net worth and work out what percentage he’s giving, and by the best figures I could find, we’re likely at or below 5% here. (He’s said to be worth around $400 million, although that figure appears to be a little outdated—especially since he may or may not have <a href="https://www.404media.co/tumblr-and-wordpress-to-sell-users-data-to-train-ai-tools/" rel="nofollow">sold a shitload of user data to AI companies earlier this year</a>.)</p>
<p>Which, fine, that’s still millions of dollars going to charity, and that’s objectively a good thing.</p>
<p>But also: if my wife and I gave that percentage of <em>our</em> income, it wouldn’t even be enough money to get a tax deduction for it. So it’s worth mentioning that just for scale. Contextually, Matt’s donating at below the standard deduction level for somebody of his net worth. (And, most likely, enjoying significant tax benefits for it.)</p>
<p>Anyway, no matter which way you look at it, that’s all weird, but it doesn’t even really matter in the case of this larger discussion. It has major “oh yeah? Well would a bad guy do THIS?” energy.</p>
<p>You know…the sort of thing actual good guys don’t usually have to do.</p>
<p>Almost like Matt was trying to distract from something…</p>
<h3 id="matt-goes-nuclear">Matt goes nuclear</h3>
<p>The next move, and most recent development in this story, is still shocking to me. I think it should be shocking, and deeply disturbing, to <em>any</em> observer.</p>
<p><a href="https://techcrunch.com/2024/09/25/wordpress-org-bans-wp-engine-blocks-it-from-accessing-its-resources/" rel="nofollow">WordPress.org banned WP Engine sites from accessing the plugin repository</a>.</p>
<p>No more doing anything with plugins via the WordPress admin area. No installing, no updating. Not if you’re on WP Engine/Flywheel.</p>
<p>There are many layers to this.</p>
<p>First: again, this is the <code>.org</code> arm of WordPress enforcing this brutal new edict. The Organization, or Foundation, or <em>whatever</em>, is not <em>supposed</em> to be controlled solely by an oligarch who can bend it to their own will, to directly benefit their own interests. It’s <em>supposed</em> to be agnostic.</p>


<p>WordPress.org’s entire reason for existence, as I understood it (and I think as it was pitched to a lot of people), was explicitly to prevent things like this from happening.</p>

<p>Second: <em>not being able to update plugins is a <strong>massive</strong> deal</em>. You could very well be exposing your site to security vulnerabilities if plugins don’t update (to say nothing of bugs). There are nonprofits, charities, government agencies, and public services that host on WP Engine, on top of countless businesses. All of those are just being thrown under the bus to serve one man’s whims.</p>
<p>(<em>Yes,</em> it’s possible to manually update plugins, but nobody’s gonna do that. Certainly not the agencies and freelancers who oversee dozens or hundreds of sites on WP Engine.)</p>


<p>This is bombing civilians. This is putting innocent bystanders in harm’s way. This is firing the Death Star.</p>
<p>What Matt’s done is unforgivable, no matter how right he might have been at the beginning. To unleash harm on actual <em>users</em> of WordPress, indiscriminantly, solely over where they choose to host their sites, is an unconscionable, terroristic abuse of power.</p>

<p>(In the middle of all this, Pressable, a separate host Automattic owns, started offering promos to help people migrate to them from WP Engine. That alone should be majorly headline-grabbing, but Matt’s abuses up to this point are so egregious it barely even registers on the scale.)</p>
<p><em>You don’t hurt users because you’re beefing with their host</em>.</p>
<p>You don’t bomb civilians because they live near a criminal, you don’t shoot at innocent bystanders because a terrorist is hiding behind them, and you don’t fire the Death Star because you disagree with Alderaan’s government.</p>
<p>It no longer matters what this was all about at that point, or whether you were originally right or not. <em>You are irreversibly the bad guy now</em>.</p>
<p>It’s also worth calling out a side effect of this move, which may or may not have been deliberate:</p>


<p>Matt’s actions have ensured <em>his</em> hosting companies are now the <em>only</em> WordPress hosts that can guarantee something like this will never happen to their users.</p>

<p>I mean, he can just flip the switch at any time. He can change the rules whenever he wants to. So what company is safe?</p>
<p>None. Except his.</p>
<p>I hope I don’t need to go into how anti-competitive that is, all on its own, or what an egregious abuse of power it is to have put himself and his company in that position by using WordPress.org to do it.</p>



<p>The weapons Matt Mullenweg has wielded unilaterally in this war shouldn’t even <em>exist</em>, let alone be controlled by one person.</p>

<p>I believe the ability to block an entire hosting provider from accessing the plugins repository is a power that nobody should have. If one could <em>ever</em> be justified in the use of such unthinkably drastic measures, this case most certainly isn’t extreme enough to do that.</p>
<p>Imagine if Microsoft got into a dispute with Apple, and decided to block npm for anyone using a Mac.</p>
<p>Imagine if Apple got into a dispute with Google, and blocked all text messages from Android phones.</p>
<p>Imagine if Google had a dispute with Amazon, and blocked all Amazon communications in Gmail. Or with Walmart, and prevented store locations from showing up on Google Maps.</p>
<p>And imagine if <em>one person</em> at any one of those companies had the power to make that decision, unilaterally and without challenge.</p>
<p><em>This is the scale of thing we’re talking about.</em> This is the collateral damage Matt has unleashed on the WordPress community, and it’s not to <em>anyone’s</em> benefit except maybe Matt’s and his own companies’. (For now, anyway. We’ll see how it all shakes out; it seems pretty inevitable that a class action suit will follow and this all gets dragged into court.)</p>
<p>Virtually no WordPress users are happy about this, no matter how they felt about WP Engine. Certainly, none benefit.</p>


<p>No reasonable person could argue WordPress is in a <em>better</em> place today than it was a week ago, or is on a better path now than it was then.</p>

<p>It’s less secure, less trustworthy, more volatile, and overall just not something <em>anybody</em> is as excited about as they were a week ago. People who spent the majority of their lives working on this software are leaving it. Professionals are looking at new tools to sell their clients. Major sites are considering changing platforms, when they wouldn’t have before.</p>
<p>The neighborhood we all lived in just rocked, by a man who’s enjoyed unchecked power as the head of every branch of the current government, as it were. And he <em>insists</em> he’s doing the right thing by us for blowing up a whole bunch of our homes. (Forgive me, I know the metaphor is beleaguered by this point, but it seems apt.)</p>
<p>Matt’s clearly willing to burn it all down to score a pyrrhic victory, and that’s not a power he or anybody else should ever have over any community, let alone one this size.</p>
<p>Matt has to go.</p>
<p>I don’t expect him to be removed from Automattic leadership (although I think others in leadership absolutely <em>should</em> be considering whether that’s the right move). But in any case:</p>


<p>It’s clear that the blurry lines between WordPress.org and WordPress.com should be turned into unbreachable walls, with no one company on both sides, or able to exercise power over the Foundation and/or Organization.</p>

<p>I don’t care about Automattic giving 5% to WordPress anymore. I want it to give up Matt’s unchecked, unilateral power. Because it’s clearer than ever he can’t be trusted with it.</p>

		

		
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Arch Linux and Valve Collaboration (133 pts)]]></title>
            <link>https://lists.archlinux.org/archives/list/arch-dev-public@lists.archlinux.org/thread/RIZSKIBDSLY4S5J2E2STNP5DH4XZGJMR/</link>
            <guid>41676646</guid>
            <pubDate>Fri, 27 Sep 2024 23:47:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lists.archlinux.org/archives/list/arch-dev-public@lists.archlinux.org/thread/RIZSKIBDSLY4S5J2E2STNP5DH4XZGJMR/">https://lists.archlinux.org/archives/list/arch-dev-public@lists.archlinux.org/thread/RIZSKIBDSLY4S5J2E2STNP5DH4XZGJMR/</a>, See on <a href="https://news.ycombinator.com/item?id=41676646">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="thread-content">

                    <!-- Start first email -->
                    





<div>

     <!-- /email-header: gravatar, author-info, date, peramlink, changed_subject -->
    <p>We are excited to announce that Arch Linux is entering into a direct 
collaboration with Valve. Valve is generously providing backing for two 
critical projects that will have a huge impact on our distribution: a 
build service infrastructure and a secure signing enclave. By supporting 
work on a freelance basis for these topics, Valve enables us to work on 
them without being limited solely by the free time of our volunteers.

This opportunity allows us to address some of the biggest outstanding 
challenges we have been facing for a while. The collaboration will 
speed-up the progress that would otherwise take much longer for us to 
achieve, and will ultimately unblock us from finally pursuing some of 
our planned endeavors. We are incredibly grateful for Valve to make this 
possible and for their explicit commitment to help and support Arch Linux.

These projects will follow our usual development and consensus-building 
workflows. [RFCs] will be created for any wide-ranging changes. 
Discussions on this mailing list as well as issue, milestone and epic 
planning in our GitLab will provide transparency and insight into the 
work. We believe this collaboration will greatly benefit Arch Linux, and 
are looking forward to share further development on this mailing list as 
work progresses.

[RFCs]: <a target="_blank" href="https://rfc.archlinux.page/">https://rfc.archlinux.page/</a></p>

    
    
    

    

</div>

                    <!-- End first email -->

                    <p>
                        
                        <a href="https://lists.archlinux.org/archives/list/arch-dev-public@lists.archlinux.org/thread/RIZSKIBDSLY4S5J2E2STNP5DH4XZGJMR/?sort=date">Show replies by date</a>
                        
                    </p>

                    
                    

                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Trademark Office Cancels Marvel, DC's 'Super Hero' Marks (183 pts)]]></title>
            <link>https://www.reuters.com/legal/litigation/us-trademark-office-cancels-marvel-dcs-super-hero-marks-2024-09-26/</link>
            <guid>41676297</guid>
            <pubDate>Fri, 27 Sep 2024 22:51:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/legal/litigation/us-trademark-office-cancels-marvel-dcs-super-hero-marks-2024-09-26/">https://www.reuters.com/legal/litigation/us-trademark-office-cancels-marvel-dcs-super-hero-marks-2024-09-26/</a>, See on <a href="https://news.ycombinator.com/item?id=41676297">Hacker News</a></p>
Couldn't get https://www.reuters.com/legal/litigation/us-trademark-office-cancels-marvel-dcs-super-hero-marks-2024-09-26/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Lion Cove: Intel's P-Core Roars (113 pts)]]></title>
            <link>https://chipsandcheese.com/2024/09/27/lion-cove-intels-p-core-roars/</link>
            <guid>41675637</guid>
            <pubDate>Fri, 27 Sep 2024 21:18:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2024/09/27/lion-cove-intels-p-core-roars/">https://chipsandcheese.com/2024/09/27/lion-cove-intels-p-core-roars/</a>, See on <a href="https://news.ycombinator.com/item?id=41675637">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Intel’s mobile CPUs have undergone massive changes over the past couple generations as Intel defends its laptop market against AMD, Qualcomm, and to a lesser extent Apple. Meteor Lake adopted aggressive chiplet design with separate compute, GPU, SOC, and IO extender tiles. Lunar Lake switches things up again, putting all compute on one tile while a second “platform controller” tile deals strictly with low speed IO.</p>
<p>Amidst this whirlwind of change, Intel’s performance oriented P-Cores have remained a constant. P-Cores aim to deliver maximum per-thread performance, an important metric in client designs where responsiveness is vital and many programs don’t scale across many cores. Lion Cove is Intel’s latest and greatest high performance architecture, and fills the P-Core role in Lunar Lake. While its goals have remained constant, its design has very much not. Compared to Redwood Cove P-Cores in the previous generation Meteor Lake, Lion Cove has been overhauled with both performance and energy efficiency in mind.</p>
<figure><table><tbody><tr><td>Chip</td><td>P-Core Architecture</td><td>Test System</td><td>Source</td></tr><tr><td>Intel Core Ultra 7 258V (Lunar Lake)</td><td>Lion Cove</td><td>ASUS Zenbook S 14 UX5406SA</td><td>Sampled by ASUS</td></tr><tr><td>Intel Core Ultra 7 155H (Meteor Lake)</td><td>Redwood Cove</td><td>ASUS Zenbook 14 OLED UX3405MA</td><td>Purchased by Clam</td></tr><tr><td>AMD Ryzen AI 9 HX 370</td><td>Zen 5 Mobile</td><td>ASUS ProArt PX13 HN7306WU</td><td>Sampled by ASUS</td></tr></tbody></table></figure>
<p>Here I’ll be looking at Lion Cove as implemented in the Core Ultra 7 258V. </p>
<h2>Acknowledgments </h2>
<p>We’d like to thank Asus for kindly sampling us with a Zenbook S 14 UX5406SA test system. Without their help, a look at Intel’s latest mobile chip wouldn’t have been possible.</p>
<h2>System Architecture</h2>
<p>Lion Cove cores sit on a ring bus interconnect, a familiar design that traces its roots back to Sandy Bridge in 2011. Over time though, ring bus agents have come and gone. P-Cores got to share the ring bus with E-Core clusters in 2021’s Alder Lake. Meteor Lake saw Intel kick the iGPU off the ring bus. Lunar Lake gives E-Cores the boot, leaving just the P-Cores and their associated L3 slices on the ring bus.</p>
<div>
<figure><a href="https://chipsandcheese.com/lioncove_sys-drawio-2/"><img data-recalc-dims="1" decoding="async" width="688" height="282" data-attachment-id="32324" data-permalink="https://chipsandcheese.com/lioncove_sys-drawio-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_sys.drawio-2.png?fit=1201%2C492&amp;ssl=1" data-orig-size="1201,492" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_sys.drawio-2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_sys.drawio-2.png?fit=1201%2C492&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_sys.drawio-2.png?fit=688%2C282&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_sys.drawio-2.png?resize=688%2C282&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_sys.drawio-2.png?w=1201&amp;ssl=1 1201w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_sys.drawio-2.png?resize=768%2C315&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>L3 slices on Lunar Lake continue to have 3 MB of capacity just as they did in Meteor Lake. However, Lunar Lake caps out at four cores and four L3 slices, dropping L3 capacity in half. In exchange, Intel is dealing with a smaller and simpler ring bus. Possibly because of this, Lunar Lake’s L3 latency has dramatically improved compared to Meteor Lake. It’s not as good as AMD, which had a very strong L3 design since early Zen generations. But AMD’s L3 latency advantage isn’t as drastic as it was before.</p>
<div>
<figure><a href="https://chipsandcheese.com/lioncove_latency_ns-1/"><img data-recalc-dims="1" decoding="async" width="688" height="325" data-attachment-id="32328" data-permalink="https://chipsandcheese.com/lioncove_latency_ns-1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_latency_ns-1.png?fit=1151%2C544&amp;ssl=1" data-orig-size="1151,544" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_latency_ns-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_latency_ns-1.png?fit=1151%2C544&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_latency_ns-1.png?fit=688%2C325&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_latency_ns-1.png?resize=688%2C325&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_latency_ns-1.png?w=1151&amp;ssl=1 1151w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_latency_ns-1.png?resize=768%2C363&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>DRAM latency also improves. Lunar Lake’s new chiplet design places the memory controller and CPU cores on the same tile, so memory access no longer have to traverse a cross-die link. The LPDDR5X DRAM chips themselves have been brought on-package too.</p>
<div>
<figure><a href="https://chipsandcheese.com/lioncove_loadedlatency/"><img data-recalc-dims="1" decoding="async" width="688" height="381" data-attachment-id="32426" data-permalink="https://chipsandcheese.com/lioncove_loadedlatency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_loadedlatency.png?fit=734%2C407&amp;ssl=1" data-orig-size="734,407" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_loadedlatency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_loadedlatency.png?fit=734%2C407&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_loadedlatency.png?fit=688%2C381&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_loadedlatency.png?resize=688%2C381&amp;ssl=1" alt=""></a></figure></div>
<p>Like Meteor Lake, Lunar Lake complicates DRAM latency measurements because the memory controller likes to stay in a low power state if there isn’t enough DRAM traffic. A plain memory latency test sees about 131.4 ns of DRAM latency. Creating some artificial bandwidth load drops latency to 112.4 ns. AMD’s Strix Point had 128 ns of DRAM latency for comparison, with margin-of-error differences under moderate bandwidth load. While DRAM latency with LPDDR5(x) will never match desktop DDR5, Lunar Lake’s DRAM latency is good for a mobile platform.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=32333"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="382" data-attachment-id="32333" data-permalink="https://chipsandcheese.com/2024/09/27/lion-cove-intels-p-core-roars/hc2024_lunarlake_msc-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lunarlake_msc-1.jpg?fit=1595%2C885&amp;ssl=1" data-orig-size="1595,885" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="hc2024_lunarlake_msc" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lunarlake_msc-1.jpg?fit=1595%2C885&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lunarlake_msc-1.jpg?fit=688%2C382&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lunarlake_msc-1.jpg?resize=688%2C382&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lunarlake_msc-1.jpg?w=1595&amp;ssl=1 1595w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lunarlake_msc-1.jpg?resize=768%2C426&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lunarlake_msc-1.jpg?resize=1536%2C852&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lunarlake_msc-1.jpg?resize=1200%2C666&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lunarlake_msc-1.jpg?resize=1320%2C732&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lunarlake_msc-1.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>Even with on-package memory, DRAM accesses are power hungry. Lunar Lake tackles this with a 8 MB memory side cache. 8 MB is less capacity than Lion Cove’s 12 MB cache, but that’s not a huge issue because the memory side cache is aimed towards blocks like the NPU or display engine that don’t have large caches of their own. Because improving CPU performance isn’t the main goal, the memory side cache doesn’t have great latency. In fact, estimating its latency is difficult because test sizes contained within the memory side cache will inevitably see a significant number of L3 hits. In those test ranges, latency is roughly 30 ns from a P-Core.</p>
<div>
<figure><a href="https://chipsandcheese.com/lioncove_dram_bw/"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="374" data-attachment-id="32336" data-permalink="https://chipsandcheese.com/lioncove_dram_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_dram_bw.png?fit=736%2C400&amp;ssl=1" data-orig-size="736,400" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_dram_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_dram_bw.png?fit=736%2C400&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_dram_bw.png?fit=688%2C374&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_dram_bw.png?resize=688%2C374&amp;ssl=1" alt=""></a></figure></div>
<p>DRAM bandwidth is quite impressive on Lunar Lake. Going to LPDDR5X-8533 brings noticeable improvements over Meteor Lake’s LPDDR5-7467. Four Lion Cove cores alone can pull more bandwidth than all of Meteor Lake’s cores.</p>
<div>
<figure><a href="https://chipsandcheese.com/lioncove_4c_read/"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="311" data-attachment-id="32339" data-permalink="https://chipsandcheese.com/lioncove_4c_read/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_4c_read.png?fit=1178%2C532&amp;ssl=1" data-orig-size="1178,532" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_4c_read" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_4c_read.png?fit=1178%2C532&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_4c_read.png?fit=688%2C311&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_4c_read.png?resize=688%2C311&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_4c_read.png?w=1178&amp;ssl=1 1178w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_4c_read.png?resize=768%2C347&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>L3 bandwidth is less impressive, though that hasn’t been Intel’s strong point for years. AMD Strix Point’s four high performance Zen 5 cores can pull over 600 GB/s from L3 with reads alone. To Intel’s credit, quad core L3 bandwidth at least doesn’t regress compared to Redwood Cove. But L3 isn’t the focus of recent Intel designs. Rather, the company uses larger L2 caches to make their cores less sensitive to L3 performance. Having a slow cache is fine if you don’t hit it often.</p>
<h2>A Mid-Level Cache For Your Mid-Level Cache</h2>
<p>And Intel cores have trended towards bigger L2 caches. But increasing cache capacity isn’t so simple because larger caches often come with higher latency. Intel’s L2, which the company often refers to a mid-level cache, isn’t immune.</p>
<figure><table><tbody><tr><td>Generation</td><td>L2 Cache</td><td>L3 Cache</td></tr><tr><td>Skylake (Client)</td><td>256 KB, 12 cycle latency</td><td>6 MB, 37 cycle latency (Core i5-6600K)</td></tr><tr><td>Tiger Lake, Willow Cove</td><td>1280 KB, 14 cycle latency</td><td>24 MB, 58 cycle latency (Core i7-11800H)</td></tr><tr><td>Meteor Lake, Redwood Cove</td><td>2 MB, 16 cycle latency</td><td>24 MB, 75 cycle latency (Core Ultra 7 155H)</td></tr><tr><td>Lunar Lake, Lion Cove</td><td>2.5 MB, 17 cycle latency</td><td>12 MB, 51 cycle latency (Core Ultra 7 258V)</td></tr></tbody></table></figure>
<p>Lion Cove counters this by adding a mid-level cache for the mid-level cache so you can avoid mid-level cache latency if you hit in the faster mid-level cache. Intel calls this new mid-level cache a L1, and renames the first level cache to L0. The new “L1” has 192 KB of capacity with 9 cycles of load-to-use latency.</p>
<p>I disagree with Intel’s terminology change. While first level cache latency does drop from five to four cycles, it merely matches Zen 5’s L1D in capacity and latency terms. The 192 KB “L1” has much higher latency than L1D caches in competing cores. You could really stretch things by comparing to a Cortex A72 in Graviton 1. That has 1.75 ns of L1D latency, which is in the same ballpark as the 1.88 ns of measured latency on Lion Cove’s 192 KB L1. But Cortex A72 and Lion Cove don’t have comparable design goals.</p>
<div>
<figure><a href="https://chipsandcheese.com/lioncove_latency_cycles/"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="345" data-attachment-id="32347" data-permalink="https://chipsandcheese.com/lioncove_latency_cycles/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_latency_cycles.png?fit=1084%2C544&amp;ssl=1" data-orig-size="1084,544" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_latency_cycles" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_latency_cycles.png?fit=1084%2C544&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_latency_cycles.png?fit=688%2C345&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_latency_cycles.png?resize=688%2C345&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_latency_cycles.png?w=1084&amp;ssl=1 1084w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_latency_cycles.png?resize=768%2C385&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>I’m going to be stubborn and call the 48 KB first level cache a “L1”, and the 192 KB cache a “L1.5” from now on. Lion Cove’s L1.5 has better bandwidth than Redwood Cove’s L2, though not with a read-only pattern. I needed a read-modify-write pattern to sustain more than 32 bytes per cycle, so I think it’s only a minor improvement. The L1.5’s bandwidth is nowhere near that of the 48 KB L1D on either Lion Cove or Redwood Cove.</p>
<div>
<figure><a href="https://chipsandcheese.com/lioncove_cache_levels-drawio/"><img data-recalc-dims="1" loading="lazy" decoding="async" width="322" height="569" data-attachment-id="32464" data-permalink="https://chipsandcheese.com/lioncove_cache_levels-drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_cache_levels.drawio.jpg?fit=322%2C569&amp;ssl=1" data-orig-size="322,569" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_cache_levels.drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_cache_levels.drawio.jpg?fit=322%2C569&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_cache_levels.drawio.jpg?fit=322%2C569&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_cache_levels.drawio.jpg?resize=322%2C569&amp;ssl=1" alt=""></a><figcaption>That’s a lot of cache levels</figcaption></figure></div>
<p>But I don’t think bandwidth is Intel’s main goal with the new L1.5. Rather, it looks aimed at reducing average L1D miss latency. Catching some L1D misses and servicing them at 9 cycle latency helps of course. Beyond that, the L1.5 lets Intel make a bigger L2, reducing latency for more difficult accesses by servicing more of them within full speed core-private caches. Going from 2 MB on Redwood Cove to 2.5 MB on Lion Cove might not feel like much. However, Intel’s slides show Lion Cove can support up to 3 MB of L2 capacity. That’s as much capacity as last level caches on some old Intel dual core chips like the Core i5-3317U.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=32353"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="324" data-attachment-id="32353" data-permalink="https://chipsandcheese.com/2024/09/27/lion-cove-intels-p-core-roars/lioncove_1t_bw-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_1t_bw-1.png?fit=1173%2C552&amp;ssl=1" data-orig-size="1173,552" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_1t_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_1t_bw-1.png?fit=1173%2C552&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_1t_bw-1.png?fit=688%2C324&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_1t_bw-1.png?resize=688%2C324&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_1t_bw-1.png?w=1173&amp;ssl=1 1173w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_1t_bw-1.png?resize=768%2C361&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>Possibly because L1.5 absorbs a good chunk of L1D miss traffic, Intel didn’t focus as much on L2 bandwidth. Lion Cove’s L2 Bandwidth tops out at 32 bytes per cycle, regardless of whether I’m using a read-only pattern or a read-modify-write one. Intel’s focus is really about giving the L2 more capacity, rather than providing higher L2 bandwidth.</p>
<p>If workloads spill out of L2, L3 bandwidth is quite mediocre. L3 read bandwidth from a single Lion Cove core regresses to just over 10 bytes per cycle, down from 16 bytes per cycle on Redwood Cove. Lion Cove enjoys lower L3 latency and a larger L2 miss queue (80 entries, compared to 64 on Redwood Cove). It’s a combination that should give a single core access to more L3 bandwidth, but that doesn’t show through in testing. A read-modify-write pattern achieves higher bandwidth, at 17-18 bytes per cycle.</p>
<div>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="286" data-attachment-id="32487" data-permalink="https://chipsandcheese.com/2024/09/27/lion-cove-intels-p-core-roars/image-107/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-1.png?fit=2386%2C991&amp;ssl=1" data-orig-size="2386,991" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-1.png?fit=2386%2C991&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-1.png?fit=688%2C286&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-1.png?resize=688%2C286&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-1.png?w=2386&amp;ssl=1 2386w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-1.png?resize=768%2C319&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-1.png?resize=1536%2C638&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-1.png?resize=2048%2C851&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-1.png?resize=1200%2C498&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-1.png?resize=1600%2C665&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-1.png?resize=1320%2C548&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-1.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px"></figure></div>
<p>Neither figure approaches Zen 5, which goes right to the limit of its 32 byte per cycle L2 to L3 interface. A read-modify-write pattern exercises the 32 byte per cycle link in the other direction, bringing total L3 throughput to 64 bytes per cycle. The Ryzen AI 9 HX 370 can run its Zen 5 cores at up to 5.15 GHz compared to 4.8 GHz on the Core Ultra 7 258V. AMD’s clock speed advantage further inflates its L3 bandwidth advantage.</p>
<h2>Out-of-Order Execution Engine</h2>
<p>One of those sweeping changes applies to the schedulers, which have been reorganized with a view towards scalability. Since the Pentium Pro from 1995, Intel has served both integer and FP/vector operations with a unified scheduler. Scaling a large unified scheduler can be difficult, so Intel split the scheduler over time. Skylake put memory address generation ops on a separate scheduler. Sunny Cove split the memory scheduler, and Golden Cove revised the memory scheduler split.</p>
<p>Lion Cove finally splits the unified math scheduler into separate ones for integer and floating point/vector ops. Intel also split register renaming for floating point and integer operations. That’s not visible from software, but it does suggest Intel’s core is now laid out a lot like AMD’s Zen. Both do register renaming separately for integer and vector operations, and use separate schedulers for those operations.</p>
<div>
<figure><a href="https://chipsandcheese.com/lioncove_sched-drawio/"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="309" data-attachment-id="32372" data-permalink="https://chipsandcheese.com/lioncove_sched-drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_sched.drawio.png?fit=1378%2C619&amp;ssl=1" data-orig-size="1378,619" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_sched.drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_sched.drawio.png?fit=1378%2C619&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_sched.drawio.png?fit=688%2C309&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_sched.drawio.png?resize=688%2C309&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_sched.drawio.png?w=1378&amp;ssl=1 1378w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_sched.drawio.png?resize=768%2C345&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_sched.drawio.png?resize=1320%2C593&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>After the split, Lion Cove’s vector and integer schedulers each have similar capacity to Redwood Cove’s unified math scheduler. Combined integer and vector scheduling capacity is simply massive with over 200 available entries. The memory schedulers are no joke either. In every category, Lion Cove’s scheduling capacity beats Zen 5’s.</p>
<figure><table><tbody><tr><td>Category</td><td>Lion Cove Available Scheduler Entries</td><td>Zen 5 Available Scheduler Entries</td></tr><tr><td>Scalar Integer Math</td><td>97</td><td>88</td></tr><tr><td>Floating Point/Vector Math</td><td>114</td><td>76</td></tr><tr><td>Memory Accesses</td><td>62</td><td>58</td></tr></tbody></table></figure>
<p>Besides big schedulers, Intel uses non-scheduling queues to let Lion Cove track more more operations waiting for an execution unit. If a scheduler fills up, the renamer can place micro-ops into an associated non-scheduling queue instead of stalling. Micro-ops sitting in a non-scheduling queue won’t be considered for execution, but can enter a scheduler later when entries become available. AMD’s Zen line and Intel’s own E-Core line have used non-scheduling queues over the years, and it’s great to see the Intel adopt it on P-Cores too.</p>
<p>Lion Cove’s schedulers feed a massive 18 execution ports, up from 12 in Redwood Cove. Much of that execution port count increase comes from moving FP/vector units off the integer scheduler. Execution capacity for common instruction categories hasn’t increased that much. Scalar integer adds get an extra ALU port. A third store address port helps discover memory dependencies faster, though sustained store throughput is limited two per cycle. FP/vector operations are handled by four ports instead of three.</p>
<div>
<figure><a href="https://chipsandcheese.com/lioncove_vecfp-drawio/"><img data-recalc-dims="1" loading="lazy" decoding="async" width="409" height="333" data-attachment-id="32377" data-permalink="https://chipsandcheese.com/lioncove_vecfp-drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_vecfp.drawio.png?fit=409%2C333&amp;ssl=1" data-orig-size="409,333" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_vecfp.drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_vecfp.drawio.png?fit=409%2C333&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_vecfp.drawio.png?fit=409%2C333&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_vecfp.drawio.png?resize=409%2C333&amp;ssl=1" alt=""></a></figure></div>
<p>Lion Cove’s FP/vector execution setup is curious too, because now it very closely resembles AMD’s going back to Zen 1. AMD and Intel now handle FP/vector execution with four pipes. Two deal with floating point multiplies and multiply-adds, while two others handle FP adds. Vector integer adds can go to any of the four ports. Unlike Zen 5, Lion Cove maintains single cycle vector integer add latency even when the schedulers are full and micro-ops are woken up by single cycle ops (other vector integer adds). In AMD’s favor, cores from the Zen line don’t suffer a significant penalty when a floating point multiply with normalized inputs generates a denormal output. Such an event costs 132 cycles on Lion Cove, which is worse than the 124 cycles I saw on Redwood Cove. Skymont behaves like Zen, and doesn’t suffer a significant penalty for denormal results.</p>
<p>Micro-ops leave the schedulers after execution units generate their speculative results. But all the way until they’re retired, micro-ops require entries in various structures like the reorder buffer, register files, and load/store queues. Those queues, buffers, and register files ensure the core can correctly produce results as if instructions were executed in program order. Lion Cove grows those structures, letting the core keep more instructions in-flight. In turn, that makes the core more resilient against long latency events like cache misses. But not every structure got equal treatment.</p>
<figure><table><tbody><tr><td>Structure</td><td>Required if an instruction…</td><td>Lion Cove</td><td>Redwood Cove</td><td>Zen 5</td></tr><tr><td>Reorder Buffer (ROB)</td><td>Exists</td><td>576</td><td>512</td><td>448</td></tr><tr><td>Integer Register File</td><td>Writes to a scalar integer register</td><td>~290</td><td>280</td><td>240</td></tr><tr><td>Floating Point/Vector Register File</td><td>Writes to a floating point/vector register</td><td>~406</td><td>332</td><td>384</td></tr><tr><td>Mask Register File</td><td>Writes to an AVX-512 mask register<br><a href="https://travisdowns.github.io/blog/2020/05/26/kreg2.html">Intel CPUs alias MMX/x87 registers to the same physical register file</a></td><td>~166</td><td>~158</td><td>~146</td></tr><tr><td>Load Queue</td><td>Reads from Memory</td><td>~189</td><td>192</td><td>N/A, ~202 measured</td></tr><tr><td>Store Queue</td><td>Writes to Memory</td><td>120</td><td>114</td><td>104</td></tr><tr><td>Branch Order Buffer</td><td>Affects control flow</td><td>180</td><td>128</td><td>96</td></tr></tbody></table></figure>
<p>Lion Cove’s ROB sees a 12.5% capacity increase. It’s a noticeable improvement, if nowhere near the 45% or 40% ROB size growth that Golden Cove or Zen 5 got over their respective previous generations. However, some of Lion Cove’s supporting resources are much bigger than the corresponding ones in Golden Cove/Redwood Cove. Lion Cove can have over 40% more branches in flight. The floating point register file also sees substantial growth, likely to keep pace with increased floating point scheduling and non-scheduling queue capacity.</p>
<p>Since Skylake, Intel allocates both AVX-512 mask registers and MMX/x87 registers out of the same register file. I can’t test reordering capacity for mask registers because Intel stopped supporting AVX-512 on consumer chips. But testing with MMX registers shows a small increase in rename capacity over Redwood Cove. Intel may still be making AVX-512 oriented improvements, and some of those effects are visible even on client cores.</p>
<p>Improvements elsewhere are minor. The integer register file grew by less than a dozen entries and still doesn’t cover ROB capacity well. Intel added a few store queue entries too. As far as I can tell, the load queue either didn’t get any entries added, or even had a few removed.</p>
<h3>Load/Store Unit</h3>
<p>A CPU’s load/store unit often occupies plenty of die area, and is responsible for ensuring memory accesses appear to execute in program order. That feels challenging with a lot of memory accesses in-flight, because a load’s address has to be checked against all prior store addresses. If they overlap, the load has to read data from the store queue instead of the data cache.</p>

<p>Lion Cove maintains Golden Cove’s zero latency, two-per-cycle store forwarding capability for exact address matches. Latency slightly regresses if the load is contained within a store but addresses don’t match exactly, but that shouldn’t be common unless you’re dealing with network packets or something. Partial overlaps are handled with much higher latency, and are likely handled by blocking the load until the store commits, after which the load can get data from the L1D cache. If so, Zen 5 has a much shorter pipeline from store address generation to retirement.</p>
<figure><table><tbody><tr><td>Case</td><td>Lion Cove</td><td>Golden Cove</td><td>Zen 5</td></tr><tr><td>Exact Address Match</td><td>2 per cycle<br>0 cycle latency</td><td>2 per cycle<br>0 cycle latency</td><td>2 per cycle<br>0 cycle latency</td></tr><tr><td>Load Contained within Store</td><td>8-9 cycle latency</td><td>5-6 cycle latency</td><td>7 cycle latency</td></tr><tr><td>Load/Store Partially Overlap</td><td>19 cycles</td><td>19-20 cycles</td><td>14 cycles</td></tr><tr><td>Independent Misaligned Load</td><td>1 per cycle</td><td>1 per cycle</td><td>4 per 3 cycles</td></tr><tr><td>Independent Misaligned Store</td><td>2 cycles per store</td><td>2 cycles per store</td><td>1 per cycle</td></tr></tbody></table></figure>
<p>Independent accesses can face delays too depending on how they interact with the underlying data cache. Tracking cached data at the byte level would be far too expensive, so caches maintain tags and state at the cache line granularity. That’s typically 64 bytes. Intel’s architectures do worse when an access crosses a 64 byte cache line boundary, taking an extra cycle for a store. Loads do a bit better probably because the data cache has more load ports and can absorb the extra bandwidth needed for a misaligned access. But misaligned loads still aren’t handled as fast as on AMD.</p>
<h4>Address Translation</h4>
<p>Programs operate on virtual addresses, which have to be translated to physical addresses that correspond to locations in DRAM. The load/store unit has to carry out these translations according to page tables set up by the operating system. Page tables are actually multi-level structures, so CPUs cache frequently used address translations in translation lookaside buffers (TLBs). For generations Intel has used a very complex TLB setup with separate TLBs for different page sizes and access types.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=32402"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="579" data-attachment-id="32402" data-permalink="https://chipsandcheese.com/2024/09/27/lion-cove-intels-p-core-roars/lioncove_tlbs/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_tlbs.jpg?fit=785%2C661&amp;ssl=1" data-orig-size="785,661" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_tlbs" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_tlbs.jpg?fit=785%2C661&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_tlbs.jpg?fit=688%2C579&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_tlbs.jpg?resize=688%2C579&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_tlbs.jpg?w=785&amp;ssl=1 785w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_tlbs.jpg?resize=768%2C647&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></a><figcaption>Who uses 1 GB pages anyway</figcaption></figure></div>
<p>Lion Cove brings the 4K page load-only DTLB’s capacity up to 128 entries, from 96 in Redwood Cove. None of the other TLB sizes have changed. That should reduce average memory access latency across a wide variety of client programs, because 4K pages are most commonly used there. However, AMD’s Zen 5 and even Zen 4 can cache far more address translations in a L2 TLB. AMD’s cores therefore have a better chance of avoiding expensive page table walks.</p>
<p>As on Redwood Cove, getting a translation from Lion Cove’s L2 TLB adds 7 extra cycles of latency. That penalty also matches Zen 5.</p>
<h2>Rename and Allocate: Feeding the Backend</h2>
<p>The rename and allocate stage allocates micro-ops into backend structures, while carrying out register renaming and other optimizations to break false dependencies. Register renaming is an inherently serial task because which physical registers correspond to an instruction’s inputs depends on how prior renames have been carried out. Probably for that reason, the renamer is often the narrowest part of a core’s pipeline. AMD and Intel’s latest cores are no exception.</p>
<p>Lion Cove widens the renamer to handle 8 micro-ops per cycle, up from 6 in Redwood Cove. That makes Lion Cove an 8-wide core overall, matching AMD’s Zen 5. Intel’s renamer received some impressive capabilities in Golden Cove, including the ability to execute up to 6 dependent adds with small immediates per cycle. That’s carried forward to Lion Cove, though not widened to match the renamer’s full width.</p>
<figure><table><tbody><tr><td>Test</td><td>Comment</td><td>Lion Cove IPC</td><td>Redwood Cove IPC</td><td>Zen 5 IPC</td></tr><tr><td>XOR r,r</td><td>Commonly used to zero registers. The exclusive-or of two identical values is always zero</td><td>7.31</td><td>5.7</td><td>5.01</td></tr><tr><td>XOR xmm, xmm</td><td>Same as above but for a vector/FP register</td><td>7.31</td><td>5.71</td><td>4.99</td></tr><tr><td>Dependent MOV r,r</td><td>&gt;1 indicates move elimination</td><td>7.02</td><td>5.56</td><td>6.65</td></tr><tr><td>Independent MOV r,r</td><td>Easy</td><td>7.25</td><td>5.71</td><td>5.01</td></tr><tr><td>Dependent increment</td><td>Actual math, normally would create a dependency chain limiting the test to 1 IPC</td><td>5.6</td><td>5.53</td><td>1</td></tr><tr><td>Dependent add immediate</td><td>As above but adding small numbers up to 20 instead of just 1</td><td>4.36</td><td>5.47</td><td>1</td></tr></tbody></table></figure>
<p>Other easier optimizations like move elimination and zeroing idiom recognition can be carried out at or near the renamer’s full width. Zen 5 is no slouch for those, but often can’t carry out those optimizations at eight per cycle. I’m not sure if it makes a big performance difference, but it does show Intel’s focus on building a very powerful rename stage.</p>
<h2>Frontend Fetch and Decode</h2>
<p>The frontend has to feed the rename stage by bringing instructions into the core and decoding them into micro-ops. Lion Cove’s frontend uses a similar strategy to prior P-Cores. A conventional instruction cache feeds a decoder, which both sends micro-ops downstream and fills them into a micro-op cache. Lion Cove widens the decoder to handle eight instructions per cycle, up from six in Redwood Cove. Micro-op cache capacity increases to 5250 micro-ops, up from 4096 on Redwood Cove. Bandwidth from the micro-op cache went up to, from eight to 12 micro-ops per cycle.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=32413"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="293" data-attachment-id="32413" data-permalink="https://chipsandcheese.com/2024/09/27/lion-cove-intels-p-core-roars/lioncove_4b_ifetch/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_4b_ifetch.png?fit=1164%2C496&amp;ssl=1" data-orig-size="1164,496" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_4b_ifetch" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_4b_ifetch.png?fit=1164%2C496&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_4b_ifetch.png?fit=688%2C293&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_4b_ifetch.png?resize=688%2C293&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_4b_ifetch.png?w=1164&amp;ssl=1 1164w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_4b_ifetch.png?resize=768%2C327&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>Unlike AMD Zen 5’s clustered decoder, all eight decode slots on Lion Cove can serve a single thread. Lion Cove can therefore sustain eight instructions per cycle as long as code fits within the 64 KB instruction cache. After that, code fetch throughput from L2 is limited to 16 bytes per cycle. L3 code fetch bandwidth is similar to data-side bandwidth, so Lion Cove’s branch predictor can run very far ahead of fetch to hide even L2 miss latency. The same doesn’t apply to Zen 5, which has lower code fetch throughput from L3.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=32417"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="300" data-attachment-id="32417" data-permalink="https://chipsandcheese.com/2024/09/27/lion-cove-intels-p-core-roars/lioncove_8b_ifetch/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_8b_ifetch.png?fit=1152%2C503&amp;ssl=1" data-orig-size="1152,503" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_8b_ifetch" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_8b_ifetch.png?fit=1152%2C503&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_8b_ifetch.png?fit=688%2C300&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_8b_ifetch.png?resize=688%2C300&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_8b_ifetch.png?w=1152&amp;ssl=1 1152w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_8b_ifetch.png?resize=768%2C335&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>Longer instructions can run into cache bandwidth bottlenecks. With longer 8-byte NOPs, Lion Cove can maintain 8 instructions per cycle as long as code fits within the micro-op cache. Strangely, throughput drops well before the test should spill out of the micro-op cache. The 16 KB data point for example would correspond to 2048 NOPs, which is well within the micro-op cache’s 5250 entry capacity. I saw the same behavior on Redwood Cove.</p>
<p>Once the test spills into the L1 instruction cache, fetch bandwidth drops to just over 32 bytes per cycle. And once it gets into L2, Lion Cove can sustain 16 instruction bytes per cycle.</p>
<h2>Branch Predictor: Directing the Core</h2>
<p>Instruction fetch is steered by the branch predictor, which plays an important role in improving both performance and power efficiency. Everyone tends to improve their branch predictors with every generation, and Lion Cove does so too. A single branch sees little to no penalty (evidence of a mispredicts) even when throwing a 12K long random pattern at it.</p>
<div>
<figure><a href="https://chipsandcheese.com/lioncove_branchhist/"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="369" data-attachment-id="32422" data-permalink="https://chipsandcheese.com/lioncove_branchhist/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_branchhist.png?fit=1174%2C629&amp;ssl=1" data-orig-size="1174,629" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_branchhist" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_branchhist.png?fit=1174%2C629&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_branchhist.png?fit=688%2C369&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_branchhist.png?resize=688%2C369&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_branchhist.png?w=1174&amp;ssl=1 1174w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_branchhist.png?resize=768%2C411&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<div>
<figure><a href="https://chipsandcheese.com/2024/09/22/intels-redwood-cove-baby-steps-are-still-steps/redwood_cove_branchhist/"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="375" data-attachment-id="32172" data-permalink="https://chipsandcheese.com/2024/09/22/intels-redwood-cove-baby-steps-are-still-steps/redwood_cove_branchhist/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_cove_branchhist.png?fit=1106%2C603&amp;ssl=1" data-orig-size="1106,603" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="redwood_cove_branchhist" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_cove_branchhist.png?fit=1106%2C603&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_cove_branchhist.png?fit=688%2C375&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_cove_branchhist.png?resize=688%2C375&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_cove_branchhist.png?w=1106&amp;ssl=1 1106w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_cove_branchhist.png?resize=768%2C419&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>Intel definitely made some changes to direction predictor, but the scope of this change seems to be narrow. Lion Cove performance monitoring events haven’t been documented yet, but Intel does guarantee some architectural performance monitoring events will work across different generations. Events for retired branches and retired mispredicted branches are among those events.</p>
<div>
<figure><a href="https://chipsandcheese.com/lioncove_specint_bpu_accuracy/"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="542" data-attachment-id="32467" data-permalink="https://chipsandcheese.com/lioncove_specint_bpu_accuracy/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specint_bpu_accuracy.png?fit=927%2C730&amp;ssl=1" data-orig-size="927,730" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_specint_bpu_accuracy" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specint_bpu_accuracy.png?fit=927%2C730&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specint_bpu_accuracy.png?fit=688%2C542&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specint_bpu_accuracy.png?resize=688%2C542&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specint_bpu_accuracy.png?w=927&amp;ssl=1 927w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specint_bpu_accuracy.png?resize=768%2C605&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>If I look at the geometric mean of branch prediction accuracy across all SPEC CPU2017 workloads, Redwood Cove and Lion Cove differ by well under 0.1%. Lion Cove has a tweaked branch predictor for sure, but I’m not seeing it move the needle in terms of accuracy. AMD’s Zen 5 still does a bit better overall, and can gain an especially significant edge with difficult workloads like 541.leela and 541.xz. There, AMD’s latest branch predictor sees a 11.4% and 3.84% reduction in mispredicts per instruction compared to Intel’s. Within SPEC CPU2017’s floating point suite, Lion Cove struggles in 526.blender.</p>
<div>
<figure><a href="https://chipsandcheese.com/lioncove_specfp_bpu_accuracy/"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="646" data-attachment-id="32468" data-permalink="https://chipsandcheese.com/lioncove_specfp_bpu_accuracy/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specfp_bpu_accuracy.png?fit=926%2C870&amp;ssl=1" data-orig-size="926,870" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_specfp_bpu_accuracy" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specfp_bpu_accuracy.png?fit=926%2C870&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specfp_bpu_accuracy.png?fit=688%2C646&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specfp_bpu_accuracy.png?resize=688%2C646&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specfp_bpu_accuracy.png?w=926&amp;ssl=1 926w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specfp_bpu_accuracy.png?resize=768%2C722&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>Branch predictor speed matters too, because the point of a branch predictor is to minimize delays from control flow dependencies. Intel continues to use a triple level branch target buffer (BTB) setup to cache frequently used branch targets, but each level has been tweaked compared to Redwood Cove. To start, both architectures can handle two taken branches per cycle likely by unrolling small loops within the micro-op queue. Lion Cove and Redwood Cove both have a 192 entry micro-op queue, but perhaps Lion Cove can’t track as many branches within it.</p>
<div>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="361" data-attachment-id="32428" data-permalink="https://chipsandcheese.com/2024/09/27/lion-cove-intels-p-core-roars/lioncove_btb/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_btb.png?fit=1124%2C590&amp;ssl=1" data-orig-size="1124,590" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_btb" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_btb.png?fit=1124%2C590&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_btb.png?fit=688%2C361&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_btb.png?resize=688%2C361&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_btb.png?w=1124&amp;ssl=1 1124w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_btb.png?resize=768%2C403&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></figure></div>
<p>Next, a L1 BTB is fast enough to do zero bubble branching, which means handling taken branches with just a single cycle of latency. On Lion Cove, the L1 BTB appears to cover 2 KB of code, regardless of how many branches are in it. Redwood Cove can track up to 128 branches in its L1 BTB, mostly independently of branch spacing.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=32429"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="303" data-attachment-id="32429" data-permalink="https://chipsandcheese.com/2024/09/27/lion-cove-intels-p-core-roars/redwood_btb/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_btb.png?fit=1030%2C454&amp;ssl=1" data-orig-size="1030,454" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="redwood_btb" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_btb.png?fit=1030%2C454&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_btb.png?fit=688%2C303&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_btb.png?resize=688%2C303&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_btb.png?w=1030&amp;ssl=1 1030w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_btb.png?resize=768%2C339&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>Then there’s a 6K entry BTB on both cores with 2 cycle latency, followed by a 12K entry BTB. That large last level BTB has 3-4 cycles of latency on Lion Cove, and is difficult to characterize on Redwood Cove.</p>
<p>Returns are predicted via a return stack, which has grown to 24 entries from 20 in Redwood Cove. Prediction latency is better when the tested call depth doesn’t exceed 12, so I suspect this is a two level structure. For comparison AMD has opted for a larger 52 entry return stack on Zen 5, which is duplicated per-thread for a total of 104 entries.</p>
<div>
<figure><a href="https://chipsandcheese.com/lioncove_returnstack_cycles/"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="392" data-attachment-id="32456" data-permalink="https://chipsandcheese.com/lioncove_returnstack_cycles/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_returnstack_cycles.png?fit=776%2C442&amp;ssl=1" data-orig-size="776,442" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_returnstack_cycles" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_returnstack_cycles.png?fit=776%2C442&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_returnstack_cycles.png?fit=688%2C392&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_returnstack_cycles.png?resize=688%2C392&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_returnstack_cycles.png?w=776&amp;ssl=1 776w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_returnstack_cycles.png?resize=768%2C437&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>Capacity isn’t the only factor, and I have to point out how fast Intel’s return prediction is. Lion Cove and Redwood Cove can handle a call+return pair every cycle. AMD’s Zen 5 takes four cycles to do the same, or an average of two cycles per branch. Lion Cove trades some speed for a few extra return stack entries, and averages one branch per cycle up to a call depth of 24. AMD may be faster for direct branches thanks to its giant 1024 entry zero-bubble BTB. But Intel is faster for other categories of branches like calls and returns.</p>
<h2>Core Summary</h2>
<p>All those caches help feed Lion Cove’s core, which has huge upgrades over Redwood Cove. The pipeline is wider, structures are larger, and a reorganized out-of-order engine helps Intel achieve higher scheduling capacity.</p>
<div>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="432" data-attachment-id="32488" data-permalink="https://chipsandcheese.com/2024/09/27/lion-cove-intels-p-core-roars/image-108/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-2.png?fit=1778%2C1116&amp;ssl=1" data-orig-size="1778,1116" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-2.png?fit=1778%2C1116&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-2.png?fit=688%2C432&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-2.png?resize=688%2C432&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-2.png?w=1778&amp;ssl=1 1778w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-2.png?resize=768%2C482&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-2.png?resize=1536%2C964&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-2.png?resize=1200%2C753&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-2.png?resize=1600%2C1004&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-2.png?resize=1320%2C829&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/image-2.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px"></figure></div>
<p>Much like Redwood Cove, Lion Cove is a wide and high clocked out-of-order design. But it’s easily the biggest change to Intel’s performance oriented architecture since Golden Cove. After Redwood Cove’s minor changes over Raptor Cove, and Raptor Cove barely doing anything over Golden Cove, it’s great to see Lion Cove’s sweeping changes.</p>
<div>
<figure><a href="https://chipsandcheese.com/2024/09/22/intels-redwood-cove-baby-steps-are-still-steps/redwood_cove-1/"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="566" data-attachment-id="32167" data-permalink="https://chipsandcheese.com/2024/09/22/intels-redwood-cove-baby-steps-are-still-steps/redwood_cove-1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_cove-1.png?fit=1348%2C1109&amp;ssl=1" data-orig-size="1348,1109" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="redwood_cove-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_cove-1.png?fit=1348%2C1109&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_cove-1.png?fit=688%2C566&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_cove-1.png?resize=688%2C566&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_cove-1.png?w=1348&amp;ssl=1 1348w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_cove-1.png?resize=768%2C632&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_cove-1.png?resize=1200%2C987&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/redwood_cove-1.png?resize=1320%2C1086&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>Intel must have put a lot of effort into Lion Cove’s design. Compared to Redwood Cove, Lion Cove posts 23.2% and 15.8% gains in SPEC CPU2017’s integer and floating point suites, respectively. Against AMD’s Strix Point, single threaded performance in SPEC is well within margin of error. It’s an notable achievement for Intel’s newest P-Core architecture because Lunar Lake feeds its P-Cores with less L3 cache than either Meteor Lake or Strix Point. A desktop CPU like the Ryzen 9 7950X3D only stays 12% and 10.8% ahead in the integer and floating point suites respectively. Getting that close to a desktop core, even a last generation one, is also a good showing.</p>
<div>
<figure><a href="https://chipsandcheese.com/lioncove_spec_total/"><img data-recalc-dims="1" loading="lazy" decoding="async" width="591" height="402" data-attachment-id="32437" data-permalink="https://chipsandcheese.com/lioncove_spec_total/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_spec_total.png?fit=591%2C402&amp;ssl=1" data-orig-size="591,402" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_spec_total" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_spec_total.png?fit=591%2C402&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_spec_total.png?fit=591%2C402&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_spec_total.png?resize=591%2C402&amp;ssl=1" alt=""></a></figure></div>
<p>Results here aren’t comparable to ones in the prior article because I re-ran with <code>-O3 -mtune=native -march=native</code> to let GCC use whatever instruction set extensions the CPU supports. They also aren’t comparable to Intel’s performance estimates, which took a variety of workloads into account at fixed frequencies.</p>
<div>
<figure><a href="https://chipsandcheese.com/hc2024_lioncove_ipc_gain/"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="385" data-attachment-id="32441" data-permalink="https://chipsandcheese.com/hc2024_lioncove_ipc_gain/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lioncove_ipc_gain.jpg?fit=1917%2C1074&amp;ssl=1" data-orig-size="1917,1074" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="hc2024_lioncove_ipc_gain" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lioncove_ipc_gain.jpg?fit=1917%2C1074&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lioncove_ipc_gain.jpg?fit=688%2C385&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lioncove_ipc_gain.jpg?resize=688%2C385&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lioncove_ipc_gain.jpg?w=1917&amp;ssl=1 1917w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lioncove_ipc_gain.jpg?resize=768%2C430&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lioncove_ipc_gain.jpg?resize=1536%2C861&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lioncove_ipc_gain.jpg?resize=1600%2C896&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lioncove_ipc_gain.jpg?resize=1320%2C740&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lioncove_ipc_gain.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>Performance gains will vary across different workloads as SPEC CPU2017 subscores show. But there’s little doubt that Intel succeeded in delivering a generational performance uplift with Lion Cove.</p>
<div>
<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="571" data-attachment-id="32442" data-permalink="https://chipsandcheese.com/lioncove_specint/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specint.png?fit=862%2C716&amp;ssl=1" data-orig-size="862,716" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_specint" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specint.png?fit=862%2C716&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specint.png?fit=688%2C571&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specint.png?resize=688%2C571&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specint.png?w=862&amp;ssl=1 862w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specint.png?resize=768%2C638&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></figure></div>
<div>
<figure><a href="https://chipsandcheese.com/2024/09/27/lion-cove-intels-p-core-roars/lioncove_specfp-2/"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="541" data-attachment-id="32502" data-permalink="https://chipsandcheese.com/2024/09/27/lion-cove-intels-p-core-roars/lioncove_specfp-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specfp-1.png?fit=939%2C738&amp;ssl=1" data-orig-size="939,738" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lioncove_specfp" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specfp-1.png?fit=939%2C738&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specfp-1.png?fit=688%2C541&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specfp-1.png?resize=688%2C541&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specfp-1.png?w=939&amp;ssl=1 939w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/lioncove_specfp-1.png?resize=768%2C604&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<h2>Final Words</h2>
<p>P-Cores have been Intel’s bread and butter long before the company started calling them P-Cores. Progress with Intel’s performance oriented cores hasn’t always been fast. Redwood Cove was only a slight tweak over Golden Cove. Skylake filled out five generations of Intel designs the same architecture. Going back further, Intel used the P6 architecture on the Pentium Pro, Pentium II, and Pentium III with just minor tweaks and clock speed increases in between.</p>
<div>
<figure><a href="https://chipsandcheese.com/hc2024_lnl_perfcores_slide/"><img data-recalc-dims="1" loading="lazy" decoding="async" width="688" height="386" data-attachment-id="32462" data-permalink="https://chipsandcheese.com/hc2024_lnl_perfcores_slide/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lnl_perfcores_slide.jpg?fit=1915%2C1075&amp;ssl=1" data-orig-size="1915,1075" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="hc2024_lnl_perfcores_slide" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lnl_perfcores_slide.jpg?fit=1915%2C1075&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lnl_perfcores_slide.jpg?fit=688%2C386&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lnl_perfcores_slide.jpg?resize=688%2C386&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lnl_perfcores_slide.jpg?w=1915&amp;ssl=1 1915w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lnl_perfcores_slide.jpg?resize=1280%2C720&amp;ssl=1 1280w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lnl_perfcores_slide.jpg?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lnl_perfcores_slide.jpg?resize=1536%2C862&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lnl_perfcores_slide.jpg?resize=1600%2C898&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lnl_perfcores_slide.jpg?resize=1320%2C741&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/09/hc2024_lnl_perfcores_slide.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px"></a></figure></div>
<p>Lion Cove is a much improved architecture compared to Redwood Cove, and shows Intel still has potent engineering muscle despite recent setbacks. Traditionally Intel delivered significant architecture changes during a “tock” in a tick-tock cycle. That reduces risk by separately handling process node and architecture changes. Lunar Lake not only combines a new architecture with a move to a new node, but also drops system level changes on top. At a time when Intel’s facing increased pressure from all sides, a move like Lunar Lake is a sign that Intel can adapt and survive.</p>
<p>Intel’s upcoming Arrow Lake desktop CPU will let Lion Cove stretch its legs with more cache and a larger power budget. Lower latency DDR5 should improve performance even further. After seeing Lion Cove perform well in a mobile form factor, I’m optimistic about what the same architecture can do on desktop. Recently Intel has been sitting on a rather unstable foundation with Raptor Lake, and Arrow Lake’s release will be a great time to put the company’s high performance chips back on stable footing.</p>
<p>Again, we would like to thank ASUS for sending us over a Zenbook S 14 for review and if you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>

<div data-post_id="10949" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-10949 box-instance-id-1">
<p><span>
<ul>
<li>
<div>
<p><img alt="clamchowder" src="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80"> </p>
</div>

</li>
</ul>
</span>
</p></div>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Obsessed with Cuttle: Parametric CAD for prototyping, producing, and procrastin (148 pts)]]></title>
            <link>https://hannahilea.com/blog/cuttle-obsession/</link>
            <guid>41674677</guid>
            <pubDate>Fri, 27 Sep 2024 19:35:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hannahilea.com/blog/cuttle-obsession/">https://hannahilea.com/blog/cuttle-obsession/</a>, See on <a href="https://news.ycombinator.com/item?id=41674677">Hacker News</a></p>
<div id="readability-page-1" class="page">
  <p>
    <nav role="navigation">
      @hannahilea:
      <a href="https://hannahilea.com/">home</a> |
      <a href="https://hannahilea.com/projects">projects</a> |
      <a href="https://hannahilea.com/blog/">blog</a> |
      <a href="mailto:hannah.ilea.robertson+site@gmail.com">contact</a>
    </nav>
    <md-block src="./src.md" hlinks="">
      
    </md-block>
  </p>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD Unveils Its First Small Language Model AMD-135M (270 pts)]]></title>
            <link>https://community.amd.com/t5/ai/amd-unveils-its-first-small-language-model-amd-135m/ba-p/711368</link>
            <guid>41674382</guid>
            <pubDate>Fri, 27 Sep 2024 19:05:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://community.amd.com/t5/ai/amd-unveils-its-first-small-language-model-amd-135m/ba-p/711368">https://community.amd.com/t5/ai/amd-unveils-its-first-small-language-model-amd-135m/ba-p/711368</a>, See on <a href="https://news.ycombinator.com/item?id=41674382">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text" id="bodyDisplay">
			
				
					
					
						<p>In the ever-evolving landscape of artificial intelligence, large language models (LLMs) like GPT-4 and Llama&nbsp;have garnered significant attention for their impressive capabilities in natural language processing and generation. However, small language models (SLMs) are emerging as an essential counterpart in the AI model community offering a unique advantage for specific use cases.&nbsp; AMD is excited to release its very first small language model, AMD-135M with Speculative Decoding. &nbsp;This work demonstrates the commitment to an open approach to AI which will lead to more inclusive, ethical, and innovative technological progress, helping ensure that its benefits are more widely shared, and its challenges more collaboratively addressed.&nbsp;</p>
<p><strong>AMD-135M: First AMD Small Language Model&nbsp;</strong></p>
<p>AMD-135M is the first small language model for Llama family that was trained from scratch on AMD Instinct™ MI250 accelerators&nbsp;utilizing 670B tokens and divided into two models: AMD-Llama-135M and AMD-Llama-135M-code.</p>
<ul>
<li><strong>Pretraining</strong>: The AMD-Llama-135M model was trained from scratch with 670 billion tokens of general data over six days using four MI250 nodes.&nbsp;</li>
<li><strong>Code Finetuning</strong>: The AMD-Llama-135M-code variant was fine-tuned with an additional 20 billion tokens of code data, taking four days on the same hardware.&nbsp;</li>
</ul>
<p>The training code, dataset and weights for this model are open sourced so that developers can reproduce the model and help train other SLMs and LLMs.</p>
<p><strong>Optimization with Speculative Decoding&nbsp;</strong></p>
<p>Large language models typically use an autoregressive approach for inference. However, a major limitation of this approach is that each forward pass can only generate a single token, resulting in low memory access efficiency and affecting overall inference speed.</p>
<p>The advent of speculative decoding has solved this problem. The basic principle involves using a small draft model to generate a set of candidate tokens, which are then verified by the larger target model. This approach allows each forward pass to generate multiple tokens without compromising performance, thereby significantly reducing memory access consumption, and enabling several orders of magnitude speed improvements.</p>
<p><strong>Inference Performance Acceleration</strong></p>
<p>Using AMD-Llama-135M-code as a draft model for CodeLlama-7b, we tested the inference performance with and without speculative decoding on the MI250 accelerator for data center, and Ryzen™&nbsp;AI processor (with NPU) for AI PC. For the particular configurations that we tested using AMD-Llama-135M-code as the draft model, we saw a<span>&nbsp;</span>speedup on the Instinct MI250 accelerator,&nbsp;Ryzen AI CPU<span size="2">[2]</span>, and on&nbsp;Ryzen AI NPU<span size="2">[2]</span><span>&nbsp;</span>versus the inference without speculative decoding.[3]&nbsp;The AMD-135M SLM establishes an end-to-end workflow, encompassing both training and inferencing, on select AMD platforms.</p>

<p><u><strong>Next Steps</strong></u><br>By providing an open-source reference implementation, AMD is not only advancing its AI capabilities but also fostering innovation within the AI community.&nbsp;To learn more about AMD-135M, read the full technical blog<strong>:&nbsp;<a href="https://www.amd.com/en/developer/resources/technical-articles/introducing-amd-first-slm-135m-model-fuels-ai-advancements.html?utm_source=organic&amp;utm_medium=community&amp;utm_campaign=blog&amp;utm_id=amd35" target="_blank" rel="noopener noreferrer">Introducing the First AMD SLM (Small Language Model): AMD-135M Model Fuels AI Advancements</a></strong></p>
<p><strong>Additional Resources&nbsp;</strong></p>
<ul>
<li>For information about the training, inferencing and insights of this model, please visit <a id="menuro5m" title="https://github.com/amd-aig-aima/amd-llm" href="https://github.com/AMD-AIG-AIMA/AMD-LLM" target="_blank" rel="noreferrer noopener nofollow" aria-label="Link AMD Github">AMD Github</a> repository to get access to the code.</li>
<li>Visit Hugging Face <a id="menuro5o" title="https://huggingface.co/amd/amd-llama-135m" href="https://huggingface.co/amd/AMD-Llama-135m" target="_blank" rel="noreferrer noopener nofollow" aria-label="Link Model Card">Model Card</a> to download the model file.</li>
<li>Apply for Instinct accelerator card access on the&nbsp;<a id="menuro5q" title="https://www.amd.com/en/forms/registration/developer-cloud-application.html" href="https://www.amd.com/en/forms/registration/developer-cloud-application.html" target="_blank" rel="noreferrer noopener" aria-label="Link AMD Developer Cloud">AMD Developer Cloud</a>.</li>
<li>For any questions, contact us by email <a id="menuro5s" title="mailto:amd_ai_mkt@amd.com" href="mailto:amd_ai_mkt@amd.com" target="_blank" rel="noreferrer noopener nofollow" aria-label="Link amd_ai_mkt@amd.com">amd_ai_mkt@amd.com</a>.&nbsp;&nbsp;</li>
</ul>
<p>Explore, innovate, and together, let us push the boundaries of AI.&nbsp;</p>

<p><span size="2">Footnotes</span></p>
<p><span size="2"><span>[1] The training code for AMD-135M is based on </span><span>TinyLlama</span><span>, utilizing multi-node distributed training with PyTorch FSDP. </span><span data-ccp-props="{&quot;134233117&quot;:true,&quot;201341983&quot;:0,&quot;335551550&quot;:6,&quot;335551620&quot;:6,&quot;335559739&quot;:200,&quot;335559740&quot;:360}">&nbsp;</span></span></p>
<p><span size="2"><span>[2] Test ran on AMD Ryzen 9 PRO 7940HS with Radeon 780M Graphics. The Ryzen AI APU Architecture includes CPU and NPU kernels. </span><span data-ccp-props="{&quot;134233117&quot;:true,&quot;201341983&quot;:0,&quot;335551550&quot;:6,&quot;335551620&quot;:6,&quot;335559739&quot;:200,&quot;335559740&quot;:360}">&nbsp;</span></span></p>
<p><span size="2"><span>[3] These are the configurations that we tested. You might get different results on other configurations. </span><span data-ccp-props="{&quot;134233117&quot;:true,&quot;201341983&quot;:0,&quot;335551550&quot;:6,&quot;335551620&quot;:6,&quot;335559739&quot;:200,&quot;335559740&quot;:360}">&nbsp;</span></span></p>
<p><span size="2"><span>[4] The performance had been tested on AMD Instinct MI250 + </span><span>ROCm</span><span>TM</span><span> 6.0 using standardized tests with </span><a href="https://github.com/EleutherAI/lm-evaluation-harness%22%20/t%20%22_blank" target="_blank" rel="noopener nofollow noreferrer"><span>lm-evaluation-harness</span></a><span>. Additionally, the model performance tests are independent of the hardware environment. </span><span data-ccp-props="{&quot;134233117&quot;:true,&quot;201341983&quot;:0,&quot;335551550&quot;:6,&quot;335551620&quot;:6,&quot;335559739&quot;:200,&quot;335559740&quot;:360}">&nbsp;</span></span></p>
<p><span size="2"><span>[5] </span><span>Hellaswag</span><span> is dataset and metrics that tests how well that LLMs can reason about physical </span><span>situations;</span><span data-contrast="auto">  </span><span data-ccp-props="{&quot;134233117&quot;:true,&quot;201341983&quot;:0,&quot;335551550&quot;:6,&quot;335551620&quot;:6,&quot;335559739&quot;:200,&quot;335559740&quot;:360}">&nbsp;</span></span></p>
<p><span size="2"><span>WinoGrande</span><span> is a dataset and codebase for evaluating natural language understanding models on a challenging task of Winograd Schema;&nbsp;&nbsp;</span></span></p>
<p><span size="2"><span>SciQ is a dataset of closed-domain question answering tasks with text inputs and outputs;&nbsp;&nbsp;</span></span></p>
<p><span size="2"><span>MMLU is a dataset of multiple-choice questions on abstract algebra topics, such as groups, rings, fields, and polynomials;&nbsp;</span></span></p>
<p><span size="2"><span>ARC-Easy is a dataset of grade-school level science questions for testing advanced question answering systems.&nbsp;</span></span></p>
<p><span size="2"><span>SlimPajama is a deduplicated version of RedPajama and sources from Commoncrawl, C4, GitHub, Books, ArXiv, Wikpedia and StackExchange. We drop the Books data from SlimPajama due to license issues;&nbsp;</span></span></p>
<p><span size="2"><span>[6] Test ran on AMD Ryzen 9 PRO 7940HS w/ Radeon 780M Graphics. The Ryzen AI APU Architecture includes CPU and NPU kernels.&nbsp;</span></span></p>

					
				
			
			
			
				
			
			
			
			
			
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Architecture of London Pubs (1966) (109 pts)]]></title>
            <link>https://thelondonmagazine.org/archive-the-architecture-of-london-pubs-by-stephen-gardiner/</link>
            <guid>41674379</guid>
            <pubDate>Fri, 27 Sep 2024 19:04:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thelondonmagazine.org/archive-the-architecture-of-london-pubs-by-stephen-gardiner/">https://thelondonmagazine.org/archive-the-architecture-of-london-pubs-by-stephen-gardiner/</a>, See on <a href="https://news.ycombinator.com/item?id=41674379">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text"><p><strong>Stephen Gardiner</strong></p>
<div title="Page 1">
<hr>
<div class="page" title="Page 1">

<p><em><span>.</span><br>
In the mid-sixties, architect and writer, Stephen Gardiner, wrote recurrent socio-cultural architectural analysis for The London Magazine. This installment, on the state of that bastion of so-called English cultural activity, the pub, originally appeared in the December 1966 edition of The London Magazine.</em></p>
</div>
</div>
<p><span>.</span></p>
<p><span>.</span></p>
<p><span data-contrast="auto">What’s happened to the pub, that most personal piece of English belongings? The place where you stand up and drink, where there are scrubbed oak benches to sit on, partitions to conceal private conversations, men with pipes and caps, and there is sawdust and beer on wooden floors? What’s happened to those powerful bar tops, the glass flaps over the counter and the bottle-crammed shelves; the complicated cut-glass that fortified you from the fog and the snow, and through which the indecipherable interior form of figures, furniture and lights made impossible shapes from the wet streets outside? What’s happened? – they’re going, or gone, most of them. The great brewers – Watneys, Whitbreads and so on—are disposing of all that rubbish: that’s out now, finished with, they say. That’s dead wood, old hat. We’re living in a Modern Age. Dickens is dead, you know. Did you know that? Well, some people don’t. Anyone would think Victoria was still alive! Good God – there are new materials now. Chromium plate, plastic (marvellous stuff, formica – doesn’t burn). Wonderful new lacquers you can see your face in and, of course, artificial flowers. Last forever. Well, you can’t ​​beat that, can you? – flowers that last forever. The Germans have patented an artificial scent – you just spray it on, each morning – Rose, Gardenia – have to watch you don’t get your sprays mixed up, of course – not that anyone would notice. No, we don’t like bare boards, these days. We have to cater for the Young. Yes, yes, I know what you mean but we don’t encourage that sort of customer any more they’ve got their own pubs to go to this place has changed hands, the street’s ‘turned over’ and that’s all there is to it, we’re afraid. </span></p>
<p><span data-contrast="auto">Drinking’s a business now, not a hobby. It’s double gins not pints of beer we’re after. Why doesn’t someone introduce the treble gin? – it’ll come, sir, it’ll come – amazing how much money there is about – even in the Freeze – just flowing like gin. Yes, we like the place to look friendly – plenty of flowers (</span><i><span data-contrast="auto">and </span></i><span data-contrast="auto">leaves, they make rather a nice dark green variety), and I like pink net formica myself although Market Research generally settles matters of taste. Close-carpeting is best, so warm – avocado, salmon, peach and shrimp are popular colours. And naturally, as you see, sir, we do food now… A little Italian prosciutto? broad beans? a spot of corn? tomatoes? – they’re lovely today, really beautiful. French bread? No?… No, I’m afraid the crabs are just part of the décor. They’re wax.</span><span data-ccp-props="{}">&nbsp;</span></p>
<figure id="attachment_94333" aria-describedby="caption-attachment-94333"><img data-attachment-id="94333" data-permalink="https://thelondonmagazine.org/archive-the-architecture-of-london-pubs-by-stephen-gardiner/untitled-design-7-7/" data-orig-file="https://thelondonmagazine.org/wp-content/uploads/2024/09/Untitled-design-7.png" data-orig-size="1200,630" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Untitled design (7)" data-image-description="" data-image-caption="" data-medium-file="https://thelondonmagazine.org/wp-content/uploads/2024/09/Untitled-design-7-300x158.png" data-large-file="https://thelondonmagazine.org/wp-content/uploads/2024/09/Untitled-design-7-1024x538.png" decoding="async" loading="lazy" src="https://thelondonmagazine.org/wp-content/uploads/2024/09/Untitled-design-7.png" alt="" width="1200" height="630" srcset="https://thelondonmagazine.org/wp-content/uploads/2024/09/Untitled-design-7.png 1200w, https://thelondonmagazine.org/wp-content/uploads/2024/09/Untitled-design-7-300x158.png 300w, https://thelondonmagazine.org/wp-content/uploads/2024/09/Untitled-design-7-1024x538.png 1024w, https://thelondonmagazine.org/wp-content/uploads/2024/09/Untitled-design-7-150x79.png 150w, https://thelondonmagazine.org/wp-content/uploads/2024/09/Untitled-design-7-500x263.png 500w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption id="caption-attachment-94333">Sketches all from the essay and by Stephen Gardiner.</figcaption></figure>
<p><span data-contrast="auto">Pubs, in fact, are going the way of all those other things one cherishes – the countryside, eighteenth-century bits of towns and villages, and the only sort of structure it seems nobody dares touch are churches and this is probably for some bogus superstitious reason. Otherwise the ghastly formula for modernisation is applied wholesale – a formula which is a middle-class conception of what the twentieth-century is all about, a sixth-hand version of contemporary </span><span data-contrast="auto">design, and so far as pubs are concerned the best parallel in the building business is the shoddy reproduction Georgian terrace which contains the latest (and cheapest) thing in aluminium sinks, cookers and the rest. It seems to me that the organisers of these projects are, in general, deliberately out to destroy anything which is unique, unusual or eccentric because they feel such things must conform with their pattern of life – the pattern they understand and which makes them feel safe and secure. Places must be the same. Friends must be the same sort of people, they must wear the same sort of clothes, say the same things. </span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">If they are not the same, then the friend gets struck off some list or other, and while it may possibly be reasonable to file away people into cabinets with smoothly sliding drawers, such things as buildings and pubs come into a different category. Architecture belongs to everyone and ought to be protected by the few who understand it. Of course it isn’t, because there is no one to do the job, or so it seems. How can you, therefore, expect anything but the destruction of pubs on a universal scale? Pubs are a universal English business. A formula has to be found to fit the majority taste. Something which is much the same – like battery hens and cattle factories.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">The first thing to go, when the formula is applied, are the separate compartments and partitions. In this way the variety provided by different bars – public, saloon, private etc. – is eliminated at a stroke. This happened, for example, at two excellent Chelsea pubs – the King’s Head and the Phene – when they were given the treatment two or three years ago. Both were devastated, both look as though a giant vacuum cleaner has passed over them leaving them forlorn, empty, desperately clean, and with just a few fixtures like a structural column or two. Otherwise everything was sucked out of them—all that was good and of real value like the old bar tops, the high backed wooden seats, the grimy mahogany tables with their curvaceous cast-iron legs,</span><span data-ccp-props="{}">&nbsp;</span><span data-contrast="auto">the lovely intricate shelves and polished brass rods and knobs, the dark panelling and even the old men with caps and pipes (imagine them sitting in glittering, close-carpeted no-man’s-land!). How can it happen? – and it is happening everywhere.</span><span data-ccp-props="{}">&nbsp;</span></p>
<figure id="attachment_94335" aria-describedby="caption-attachment-94335"><img data-attachment-id="94335" data-permalink="https://thelondonmagazine.org/archive-the-architecture-of-london-pubs-by-stephen-gardiner/website-image/" data-orig-file="https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image.png" data-orig-size="1200,630" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Website image" data-image-description="" data-image-caption="" data-medium-file="https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-300x158.png" data-large-file="https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-1024x538.png" decoding="async" loading="lazy" src="https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image.png" alt="" width="1200" height="630" srcset="https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image.png 1200w, https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-300x158.png 300w, https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-1024x538.png 1024w, https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-150x79.png 150w, https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-500x263.png 500w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption id="caption-attachment-94335">Credits: Stephen Gardiner.</figcaption></figure>
<p><span data-contrast="auto">Quite shortly the English pub will be extinct, part of history. The trouble is that the wretched brewers, in their hurry to find a modern equivalent of the traditional interior, neither stop to think nor to find proper architects and designers. Pubs shouldn’t be in the hands of hacks in their ‘interior department’; but then it clearly has not filtered through to them that the contents of these old places do have a value, as any antique dealer could tell them. And instead of rushing ahead with the steamroller that sticks down the rule-of-thumb carpets, wallpapers and</span><span data-ccp-props="{}">&nbsp;</span><span data-contrast="auto">plastic tops they should make it their business to discover what the past has to offer in the way of lessons in design and why the kind of plans that were used in the old days were so good, and seem particularly so when set against the dreadful ‘schemes’ that are being done now. It’s no use the customer objecting. (I tried it once, saying angrily, as if I had been tricked in some way: ‘What’s happened here?’ The manager merely said stonily: ‘The customers like it.’) No, it’s up to the brewer. And there are a few obvious arguments that can be put to him. After all, to start with an elementary point, when you are inserting a new inside to a building of quality belonging to a good architectural period the normal and correct procedure is to consider your design in relation to what happens on the outside – the scale, character and proportion of it, the detailing of the windows and so on; and by this I mean the sort of close observation which catches the atmosphere of the design and the period. If instead the inside is considered in isolation – and it is part of an architect’s responsibility to see that the whole of a problem is solved and not pieces of it – then the total result is bound to end as an architectural disaster.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">If you are landed with a Georgian or an early Victorian structure an answer has to be found which is somehow, although by no means literally, in sympathy with it. Should the brewer reject this argument as unimportant he is in fact dismissing something which is fundamental to the making of architecture of any time, and he is not worth talking to. But if he agrees one can go on. And if Henekey’s in the Portobello Road can make a good job of an interior with a few shelves, barrels, and glass-panelled partitions, then surely an architect of today can too – provided he has insight, feeling and some imagination, and is not too conscious of being an architect.</span><span data-ccp-props="{}">&nbsp;</span></p>
<figure id="attachment_94337" aria-describedby="caption-attachment-94337"><img data-attachment-id="94337" data-permalink="https://thelondonmagazine.org/archive-the-architecture-of-london-pubs-by-stephen-gardiner/website-image-2/" data-orig-file="https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-2.png" data-orig-size="1200,630" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Website image (2)" data-image-description="" data-image-caption="" data-medium-file="https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-2-300x158.png" data-large-file="https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-2-1024x538.png" decoding="async" loading="lazy" src="https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-2.png" alt="" width="1200" height="630" srcset="https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-2.png 1200w, https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-2-300x158.png 300w, https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-2-1024x538.png 1024w, https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-2-150x79.png 150w, https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-2-500x263.png 500w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption id="caption-attachment-94337">Credits: Stephen Gardiner.</figcaption></figure>
<p><span data-contrast="auto">The main problem is the plan: it always is, in any architectural problem. Once you’ve got your idea for the plan straight you can settle down to filling in the other gaps – structure, materials, finishes and so on. Well, what is the most successful sort of pub plan? And the only possible point one can start from with any design is one’s own experience – what sort of pub plan does one like best? Some pubs are huge and lavishly decorated with engraved glass like the Salisbury in St Martin’s Lane; others are huge but spare in their finishes and resemble great barns which contain things like dart boards and bar billiards; others are small and lavish like the Red Lion in Duke Street W1, or the Victoria in Sussex Place W8, and don’t have darts; others, again, are small and sparse like the Nag’s Head in Kinnerton Street SW1, or The Raven in Battersea Church Road and this one does have darts. But all the best pubs, the pubs which have the genuine atmosphere which is as solid as the people who inhabit them, do possess one thing in common and this is the position of the bar: it is always sited so that, like an altar in a church, it is the focus of attention. It is either in the centre of the whole space or it is arranged so that it forms clearly defined areas around it, and it is so shaped that it appears to personally control these areas. </span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">From this point on, the plan is developed to emphasise the areas, to increase their privacy and to simplify the control of them. Privacy is an important factor in pubs: the subtle suggestion of a room where there is exclusive service takes one back a bit – to those small inns embedded deep in the country, or within listening distance of the waves breaking, with their high-backed black dining seats – and there is something comforting and a little mysterious about it all, with a vague sense of nostalgia added in, a nostalgia for something remembered from history books and romantic stories and evoked by such details as the barrels at Henekey’s that have the white lettering on them. And this is why these partitions with their engraved glass panels have a real charm. The sense of privacy is complete but you can still see through the smoky glass, enough at any rate to identify a shadowy silhouette beyond. So the ideal plan seems to be, to my mind at least, one where the bar is in the centre (or approximates to this position), and where there are a number of compartments which are separated by partitions which have a somewhat temporary look, but which radiate from the bar.</span><span data-ccp-props="{}">&nbsp;</span></p>
<figure id="attachment_94336" aria-describedby="caption-attachment-94336"><img data-attachment-id="94336" data-permalink="https://thelondonmagazine.org/archive-the-architecture-of-london-pubs-by-stephen-gardiner/website-image-1/" data-orig-file="https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-1.png" data-orig-size="1200,630" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Website image (1)" data-image-description="" data-image-caption="<p>Credits: Stephen Gardiner.</p>
" data-medium-file="https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-1-300x158.png" data-large-file="https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-1-1024x538.png" decoding="async" loading="lazy" src="https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-1.png" alt="" width="1200" height="630" srcset="https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-1.png 1200w, https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-1-300x158.png 300w, https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-1-1024x538.png 1024w, https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-1-150x79.png 150w, https://thelondonmagazine.org/wp-content/uploads/2024/09/Website-image-1-500x263.png 500w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption id="caption-attachment-94336">Credits: Stephen Gardiner.</figcaption></figure>
<p><span data-contrast="auto">We can go on from there. The ideal pub has a low ceiling because this is correct for the scale of the compartments and increases the suspense; you see it carrying on above the top of the partitions which it misses by about a foot, and you wonder what’s going on over the other side. The ideal pub also has some division between the compartment and the bar areas and this is sometimes in the form of glass flaps, as at Henekey’s, or in the form of unaffected but beautifully made shelves such as one finds at the Victoria in Sussex Place, the City Barge at Strand-on-the-Green and the Hansom Cab in the Earl’s Court Road. These shelves are always filled with bottles (and the colour of bottles really does glow, particularly when you think of the different colours of the drinks that fill them) and all kinds of glasses – those huge brandy glasses, for example, which reflect light so admirably. This brings us to the question of display in general. The chief centre for display – in the ideal pub – is right there in the middle of the bar area and that’s where pretty well everything for sale is on show: this is the shop window. This is where labels matter and become, like the lettering and the inn signs outside, the décor. A concentration of labels and calligraphic handwriting can be marvellous. The ideal pub has a genius for the ideal display.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">So it seems that, in the final analysis, the design of the pub comes down to a few known quantities: ceiling heights, partitions, simple country scrubbed furniture or something lavish in leather (whatever the call), shelves and glass, the loving display of a collector and the dominating father-figure, the bar. But there are obviously no cut-and-dried rules. You don’t have to have partitions, and the Hansom Cab, for example, with its irregular, rambling plan manages both suspense and intimacy very well indeed without them. And ceilings can be high provided the total space – like that of the Salisbury – requires and sustains such height.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">And what about the brewer? Well, he might as well know that there are no good modern pubs: the Hoop from Finch’s is absurdly over-elaborate, The Champion is self-conscious, the Hansom Cab is (although well done) a reproduction piece and the Ranelagh in Pimlico is really terrible. For the brewer there are two rules which he must obey if the English pub tradition is to be protected. They are quite simple. If a good pub comes up for redecoration and alterations the best things and the authentic details must be preserved. If a bad pub comes up – and there are a depressingly large number of them – start again from scratch, but with the right architect.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span data-contrast="auto">Lastly, don’t forget the dart board and the bar billiards which the TV set, in some curious, and odious, fashion, seems to have replaced. And no artificial flowers, by request, please.</span><span data-ccp-props="{}">&nbsp;</span></p>
<p><span>.</span></p>
<p><span>.</span></p>
<p><strong>Stephen Gardiner</strong> was a British architect, teacher and writer. Born and raised in Chelsea, he wrote regularly for <em>The London Magazine&nbsp;</em>and&nbsp;<em>The Observer.</em></p>
<hr>
<p><em>To discover more content exclusive to our print and digital editions,&nbsp;<strong><a href="https://www.thelondonmagazine.org/product/recurring-subscription/">subscribe here</a></strong>&nbsp;to receive a copy of The London Magazine to your door every two months, while also enjoying full access to our extensive digital archive of essays, literary journalism, fiction and poetry.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SAML: A Technical Primer (210 pts)]]></title>
            <link>https://ssoready.com/docs/saml/saml-technical-primer</link>
            <guid>41674109</guid>
            <pubDate>Fri, 27 Sep 2024 18:38:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ssoready.com/docs/saml/saml-technical-primer">https://ssoready.com/docs/saml/saml-technical-primer</a>, See on <a href="https://news.ycombinator.com/item?id=41674109">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>If you just want to start integrating SAML right away, check out the <a href="https://ssoready.com/docs/saml/saml-quickstart">SAML
quickstart</a>. You can get a SAML integration
working end-to-end within a few hours.</p><p>This article is for folks who want to understand SAML at a deeper technical
level, or how they could implement SAML without using an open-source library
like SSOReady.</p></div>
<p><a target="_blank" rel="noreferrer" href="https://en.wikipedia.org/wiki/Security_Assertion_Markup_Language">SAML<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>
(“Security Assertion Markup Language”) is a source of a lot of confusion for
developers. This article is a technical primer on some of the most common
questions engineers and other technical folks have about SAML:</p>
<ol>
<li><a href="https://ssoready.com/docs/saml/saml-technical-primer#what-is-the-point-of-saml">Why do businesses want their software vendors to support SAML</a>? In other
words, how does SAML fit into my customer’s business? Why do end users and C-level executives at my customer care about SAML?</li>
<li><a href="https://ssoready.com/docs/saml/saml-technical-primer#fitting-saml-into-your-existing-software">How should I fit SAML into my exiting software</a>? What parts of my software stack
need to be “SAML-aware”? How lightweight can I make my integration? (The answer: quite lightweight. Only a small part of your codebase needs to know about SAML at all.)</li>
<li><a href="https://ssoready.com/docs/saml/saml-technical-primer#saml-at-a-technical-level">At a technical level, how does SAML even work</a>? What does the SAML protocol
even do? What kinds of security guarantees does it give me, or what assumptions can I make about it?</li>
</ol>
<h2 id="what-is-the-point-of-saml" data-state="closed">What is the point of SAML?</h2>
<p>You care about supporting SAML because your customer wants your product to
support SAML. This is sound reasoning on your part. But why does your customer
want SAML support?</p>
<h2 id="one-click-to-login-why-your-users-like-saml" data-state="closed">One click to login: why your users like SAML</h2>
<p>Your users probably don’t know what SAML is. What they do know about is their
company’s <em>identity provider</em>. The most popular one is called
<a target="_blank" rel="noreferrer" href="https://www.okta.com/customer-identity/single-sign-on/">Okta<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>; other common
competitors to Okta include <a target="_blank" rel="noreferrer" href="https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id">Microsoft
Entra<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>
(formerly “Azure Active Directory”) and <a target="_blank" rel="noreferrer" href="https://workspace.google.com/">Google
Workspace<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>. There are dozens more vendors in this
space (big companies often build their own internal alternatives), and they all
use the SAML protocol.</p>
<p>Even though your users don’t know what SAML or what an identity provider is,
they do love what it gives them: one-click login experience for every SaaS tool
they use at work, a so-called <em>Single Sign-On</em> (SSO) experience.</p>
<p>For example, here’s what Okta looks like for your users. When your user opens
their computer at work in the morning, this is what they see:</p>
<figure><figcaption>A screenshot of Okta. Every app they use at work gets a 'tile'. Click on a tile, and you're now logged into it.</figcaption></figure>
<p>At work, your users only need one password: their identity provider password.
They don’t need to set up or remember passwords anywhere. They might find
logging into Okta itself a bit annoying, because their IT team requires
two-factor authentication to log into Okta, but logging into everything else is
a breeze.</p>
<h2 id="one-click-to-fire-why-your-customers-ciso-likes-saml" data-state="closed">One click to fire: why your customer’s CISO likes SAML</h2>
<figure><figcaption>A screenshot of an IT admin deprovisioning an Okta user. CISOs love SAML because it lets them lock down all of an employee's accounts from a single place.</figcaption></figure>
<p>Your customer’s
<a target="_blank" rel="noreferrer" href="https://www.cisco.com/c/en/us/products/security/what-is-ciso.html">CISO<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a> (Chief
Information Security Officer) is in charge of making sure company data is
secure. Concretely, the biggest things they worry about is:</p>
<ul>
<li>Employees accidentally leaking data, because they use the same password
everywhere and that password got breached</li>
<li>Employees intentionally leaking data, because they were fired and want revenge</li>
</ul>
<p>CISOs love vendors that support SAML because they can put those vendor’s apps
inside the corporate identity provider, e.g. Okta. From there:</p>
<ul>
<li>
<p>Employees don’t need to have a password for that vendor. They just log in
using the identity provider. The identity provider uses the SAML protocol to
securely log the employee into the vendor’s app.</p>
</li>
<li>
<p>When the company fires someone, an IT admin doesn’t have to manually go in and delete that employee’s
account from the vendor. Once you remove an employee from Okta, then
Okta will stop letting that employee do SAML-based logins into <em>anything</em> (every
identity provider works like this). The fired employee is locked out of every
work application.</p>
</li>
</ul>
<p>But none of this works if your application doesn’t implement SAML. SAML is the
protocol that powers single-sign on, which lets identity providers like Okta log
employees into your app without using a password.</p>
<p>This is why many CISOs will go as far as to <em>require</em> SAML support out of all
vendors. Many companies have regulatory, contractual, or compliance obligations
to ensure employees don’t use insecure passwords and are properly off-boarded
after being fired. CISOs meet those obligations using SAML.</p>
<h2 id="fitting-saml-into-your-existing-software" data-state="closed">Fitting SAML into your existing software</h2>
<div><p>If you read <a target="_blank" rel="noreferrer" href="https://docs.oasis-open.org/security/saml/v2.0/saml-core-2.0-os.pdf">the SAML
specification<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>,
or look at <a target="_blank" rel="noreferrer" href="https://docs.oasis-open.org/security/saml/Post2.0/sstc-saml-tech-overview-2.0.html">documentation written about SAML<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>
(especially if those docs were written in the early 2000s), it might seem like
SAML is a framework that can subsume or replace all of auth. Don’t do this.</p><p>”SAML solves everything” was a hot idea in 2002, but the industry has
moved away from this. The contemporary consensus is that SAML isn’t a great
protocol. Just use SAML as a way to securely find out what a corporate
user’s email address is.</p></div>
<p>You should think of SAML as a self-contained login method. You probably already
let your users log into your product using things like username+password, email
magic links, “Log in with Google”, etc. Think of SAML as another login method.</p>
<p>SAML is a protocol that lets your customer’s employees securely prove to you
what their email address is, without you having to worry about sending them
confirmation emails, verifying they’re still employed at the company, or that
they belong to the right team at the company.</p>
<p>Roughly speaking, only two parts of your overall system need to know about SAML:</p>
<ol>
<li>
<p>Your login page needs to know that SAML is a login option for a customer.
There are a couple common UI flows for doing this. We cover these in depth in
the <a href="https://ssoready.com/docs/saml/integrating-saml-into-your-login-ui">Integrating SAML with your Login
UI</a> guide.</p>
<p>Ultimately, your login page will, at a technical level, <a href="https://ssoready.com/docs/saml/saml-technical-primer#initiating-a-saml-login">initiate a SAML
login</a>.</p>
</li>
<li>
<p>Your login backend system needs to be able to <a href="https://ssoready.com/docs/saml/saml-technical-primer#handling-a-saml-assertion">handle SAML
assertions</a>. We cover this in depth in the <a href="https://ssoready.com/docs/saml/handling-saml-logins-jit-provisioning">Handling
SAML Logins</a> guide.</p>
<p>Ultimately your backend runs an HTTP endpoint, and your user’s web browsers
will POST SAML payloads there. You verify those payloads, and use your
normal session system (the same one you use for other kinds of logins) to
create a session for the email you securely extracted from the SAML
payload.</p>
</li>
</ol>
<p>If you don’t use an open-source library like SSOReady to help implement SAML,
the lack of structure that SAML imposes on you — as well as historical baggage
from the early days of SAML — can lead you astray in two common
ways:</p>
<ol>
<li>
<p>SAML supports the idea of putting “metadata” on a login session,
configuring “conditional access”, and lots of other fancy functionality. It
might seem like supporting SAML means having your entire system be able to honor
these advanced SAML-specific features.</p>
<p>Without getting into too much detail on what
that functionality was meant to achieve in 2002, suffice it to say that most
modern software systems don’t use this functionality at all.</p>
<p>If you treat SAML as just a way to get a user’s email, you will be in line
with almost all other SAML-supporting software your customer is used to.
CISOs expect, and will be satisfied with, this kind of simple integration.</p>
</li>
<li>
<p>SAML is, unfortunately, much more annoying to configure than any other login
method you already support. The technical details of these SAML settings are
covered later in this article <a href="https://ssoready.com/docs/saml/saml-technical-primer#saml-configuration">here</a>.</p>
<p>You and your customer need to exchange settings about one another before a SAML
login can even begin. But that configuration
happens “offline” — if you’re not using SSOReady, you’ll implement it by
exchanging informal emails with your customer. If you do use SSOReady, you can
have your customer <a href="https://ssoready.com/docs/idp-configuration/enabling-self-service-configuration-for-your-customers">self-serve configure their SAML
setup</a>.</p>
<p>You don’t typically have to write
any UI or backend code related to configuring SAML configuration, beyond having
some way for your engineers to store the SAML settings you got from your
customer. You just need to store three small pieces on your backend (two strings
plus an X.509 certificate), and those settings change very infrequently.</p>
</li>
</ol>
<h2 id="saml-at-a-technical-level" data-state="closed">SAML at a technical level</h2>
<div><p>This section gets quite technical. You don’t need to understand this
material to understand how to use SSOReady. This section is, in a way, a
high-level overview of everything SSOReady abstracts away for you.</p></div>
<p>At the end of the day, SAML is a protocol that lets one of your users tell you
(“assert”) their email address using a payload (an “assertion”) that is
self-contained. When you get a SAML payload, you can securely know:</p>
<ol>
<li>Which of your corporate customers sent you the payload,</li>
<li>What email address, according to that corporate customer, this user has</li>
<li>That the corporate customer wants you to log this user in right away</li>
</ol>
<p>The tricky part about SAML is that you need to watch out for:</p>
<ol>
<li>Forged SAML assertions, wherein an attacker pretends to be one of your
corporate customers</li>
<li>Malicious or misconfigured corporate customers sending assertions about other
company’s employees, e.g. EvilCorp (<code>evilcorp.com</code>) telling you to log someone
in as the CEO of AcmeCorp (<code>ceo@acmecorp.com</code>).</li>
</ol>
<p>If you use SSOReady, these issues are both automatically covered by you.
Otherwise, you’ll typically need to implement (2) yourself, and you may want to
audit your SAML
dependency to make sure they adequately handle (1). Sadly, securely
authenticating SAML is tricky, and many libraries <a target="_blank" rel="noreferrer" href="https://nvd.nist.gov/vuln/detail/CVE-2024-45409">don’t do it
right<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>.</p>
<h2 id="the-saml-flow" data-state="closed">The SAML Flow</h2>
<p>There are three actors involved in a SAML flow:</p>
<ol>
<li>
<p>You are the <strong>service provider</strong> (“SP”). The service provider is software
product being logged into via SAML.</p>
</li>
<li>
<p>Your customer’s Okta/Entra/Google/etc is the <strong>identity provider</strong> (“IDP”).
The identity provider is responsible for knowing whether a user is a real
employee that wants to log into a product, and for telling service providers
about that information using SAML.</p>
</li>
<li>
<p>The <strong>user</strong> is mostly just along for the ride. In SAML, the SP and the IDP will
redirect the user to each other. The user’s browser is responsible for carrying
messages back and forth between the SP and IDP.</p>
</li>
</ol>
<figure><figcaption>A sequence diagram of a successful SAML login flow.</figcaption></figure>
<p>Logging in via SAML has five high-level steps:</p>
<ol>
<li>You and your customer agree, offline, on some settings about how you’re going
to do SAML.</li>
<li>When it’s time to log in via SAML, you have the user POST a SAML
<code>AuthnRequest</code> to your customer’s identity provider. This is called “initiating”
a SAML login.</li>
<li>Your customer’s identity provider handles making sure the user really has
valid corporate credentials. This step is entirely outside your app’s
control.</li>
<li>The identity provider has the user POST a SAML <code>Assertion</code> to your HTTP server.</li>
<li>You authenticate that the assertion is legitimate, and then log the user into
your product.</li>
</ol>
<div><p>If you’re familiar with OAuth, this flow might sound familiar. The biggest
difference between SAML and OAuth is how you verify the user after they get
redirected back to your application.</p><p>In OAuth, your backend server typically takes a <code>code</code> from the user, and
asks the identity provider if this <code>code</code> is legitimate, and what the
underlying user’s details are if it is.</p><p>In SAML, your backend server never talks directly to the identity provider.
You have to look at the assertion and use public-key cryptography to tell if
the identity provider cryptographically signed the message.</p></div>
<p>Step (1) is important conceptually, but doesn’t require any code. That’s covered
in the next section: <a href="https://ssoready.com/docs/saml/saml-technical-primer#saml-configuration">SAML Configuration</a>. Steps (2) and
(5) are the ones you have to write code for. They’re covered in <a href="https://ssoready.com/docs/saml/saml-technical-primer#initiating-a-saml-login">Initiating a
SAML Login</a> and <a href="https://ssoready.com/docs/saml/saml-technical-primer#handling-a-saml-assertion">Handling a SAML
Assertion</a>.</p>
<h3 id="sp--vs-idp-initiated-saml-flows" data-state="closed">SP- vs IDP-initiated SAML flows</h3>
<p>The discussion above illustrates the “SP-initiated” SAML flow, where your
application (the SP) decides to kick off the SAML flow.</p>
<p>SAML also supports “IDP-initiated” flows, where the IDP kicks off the SAML flow,
and just directly sends the user to your ACS URL with an assertion.</p>
<figure><figcaption>A sequence diagram of a successful IDP-initiated flow.</figcaption></figure>
<p>The only difference between an SP- and an IDP-initiated flow is that
IDP-initiated flows won’t require you to <a href="https://ssoready.com/docs/saml/saml-technical-primer#initiating-a-saml-login">initiate
them</a>, and as a result don’t have <a href="https://ssoready.com/docs/saml/saml-technical-primer#including-a-relaystate">a <code>RelayState</code></a>. Both
are widely used in
practice.
When you use SSOReady, you get both SP- and IDP-initiated SAML support
automatically.</p>
<h2 id="saml-configuration" data-state="closed">SAML Configuration</h2>
<p>For each of your customers, you will have five settings associated with the SAML
connection you have with them. These settings are:</p>
<ul>
<li>
<p>An <strong>Assertion Consumer Service (“ACS”) URL</strong>. You assign this value. It’s a
URL where you run an HTTP endpoint that’s ready to <a href="https://ssoready.com/docs/saml/saml-technical-primer#handling-a-saml-assertion">handle SAML
assertions</a>. When the identity provider redirects
the user back to your application, they’ll send the user to the ACS URL.</p>
</li>
<li>
<p>An <strong>SP Entity ID</strong>. You assign this value, and it must be unique for every
customer. It’s a generic string, but
conventionally it’s formatted as a URL. The identity provider will include this SP
Entity ID in the
assertions it sends you, and you’ll use it to ensure the assertion was meant
for <em>you</em> and not some other application.</p>
</li>
<li>
<p>An <strong>IDP Redirect URL</strong>. The IDP assigns this value. When you <a href="https://ssoready.com/docs/saml/saml-technical-primer#initiating-a-saml-login">initiate a SAML
login</a>, this is the URL you redirect the user to.</p>
</li>
<li>
<p>An <strong>IDP Entity ID</strong>. The IDP assigns this value. It’s a generic string, but
conventionally it’s formatted as a URL. When you initiate a SAML
login, you include this value so the IDP knows which application is starting
the login. The IDP will include this IDP Entity ID in the assertions it sends
you, and you’ll use it to make sure the assertion is coming from the right
identity provider.</p>
</li>
<li>
<p>An <strong>IDP Certificate</strong>. The IDP assigns this value. The IDP will use this certificate to
cryptographically sign the assertions it sends you. You will use
this certificate to authenticate that the identity provider really generated the
assertion, and that it wasn’t forged or tampered with.</p>
</li>
</ul>
<p>Once you have all of these settings in place, you can begin doing SAML logins.</p>
<h2 id="initiating-a-saml-login" data-state="closed">Initiating a SAML Login</h2>
<p>Initiating a SAML login concretely consists of having your user’s web browser
send a POST request with a payload that looks like this:</p>

<p>The <code>Issuer</code> needs to be equal to the <a href="https://ssoready.com/docs/saml/saml-technical-primer#saml-configuration">SP Entity ID</a>.</p>
<p>That POST request needs to be pointed at the <a href="https://ssoready.com/docs/saml/saml-technical-primer#saml-configuration">IDP Redirect
URL</a>. The POST request needs to be a standard HTTP form,
with the <code>AuthnRequest.xml</code> being base64-encoded and set as a form field called
<code>SAMLRequest</code>.</p>
<p>You can’t use a normal HTTP redirect to have your user POST a form to another
URL. The typical workaround is to render your user a form that self-submits
using JavaScript:</p>
<div><div><p><span>Self-Submitting SAML Initiation Form</span></p></div><pre tabindex="0"></pre></div>
<h3 id="including-a-relaystate" data-state="closed">Including a <code>RelayState</code></h3>
<p>When initiating a SAML login, you can optionally include a <code>RelayState</code>
parameter. You include this data as an additional parameter in the POST request:</p>
<div><div><p><span>Self-Submitting SAML Initiation Form with a RelayState</span></p></div><pre tabindex="0"></pre></div>
<p>Whatever you put in <code>RelayState</code> will be echoed back to you when you <a href="https://ssoready.com/docs/saml/saml-technical-primer#handling-a-saml-assertion">handle the
SAML assertion</a>. The HTTP POST you receive will
contain, alongside the usual <code>SAMLResponse</code> entry, a <code>RelayState</code> entry.</p>
<p>The typical use-case for <code>RelayState</code> is to keep track of what page your user
was on before forced them to log in with SAML. Then, once they’re done logging
in with SAML, you redirect the user back to the page they were previously on.</p>
<div><p>You can’t trust that the <code>RelayState</code> you get back from an identity provider
is the same as the one you chose when initiating the SAML login. An attacker
can always send you a request with their own <code>RelayState</code> instead.</p><p>The most common security risk associated with <code>RelayState</code> is when you store
a URL in that <code>RelayState</code>, but don’t authenticate its legitimacy. If the
<code>RelayState</code> is allowed to redirect to a URL outside of your web
application, then you have an <a target="_blank" rel="noreferrer" href="https://cwe.mitre.org/data/definitions/601.html">open redirect
vulnerability<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>.</p><p>The safest solution is to cryptographically sign the <code>RelayState</code> value you
include in your request using a secret key. When you use SSOReady, every
<code>RelayState</code> is cryptographically authenticated; you do not need to worry
about the <a href="https://ssoready.com/docs/ssoready-concepts/saml-login-flows#state"><code>state</code>
parameter</a> being tampered
with.</p></div>
<h2 id="handling-a-saml-assertion" data-state="closed">Handling a SAML Assertion</h2>
<p>After you <a href="https://ssoready.com/docs/saml/saml-technical-primer#initiating-a-saml-login">initiate a SAML login</a>, the user is now on
the identity provider’s website. The user then identifies
themselves to the identity provider. Exactly how this works is outside of your
control.</p>
<div><p>Typically, an identity provider will ask for a user’s password, and
then may do multi-factor authentication checks. The point of SAML is that your
customer’s IT admin decides on their corporate security policy, and their
identity provider implements the logic. Your application doesn’t need to worry
about it.</p></div>
<p>If the identity provider decides to not proceed — maybe the user is fired, or
maybe hasn’t been internally authorized to use your application (e.g. your
customer only wants engineers using your app, but the employee works in sales),
then from your perspective, nothing happens. You’ll never hear back from the
login attempt. SAML doesn’t have a “login attempt failed” mechanism.</p>
<p>But if the login succeeds, then your user’s web browser will be redirected back
to your <a href="https://ssoready.com/docs/saml/saml-technical-primer#saml-configuration">SAML ACS URL</a>. The user will POST you a standard
set of HTML form data. That form data will contain up to two values:</p>
<ul>
<li>A <code>SAMLResponse</code> element, containing a base64-encoded XML document. This is
the SAML assertion.</li>
<li>A <code>RelayState</code>. This is only included if you <a href="https://ssoready.com/docs/saml/saml-technical-primer#including-a-relaystate">included a <code>RelayState</code> in your
initiation request</a>.</li>
</ul>
<p>The job of “handling a SAML login” consists of three steps:</p>
<ol>
<li>Authenticating the legitimacy of the SAML payload</li>
<li>Deciding whether you want to honor the SAML request</li>
<li>Logging the user in</li>
</ol>
<p>To do any of this, you first need to parse the SAML assertion, and make sense of
its contents.</p>

<h3 id="anatomy-of-a-saml-assertion" data-state="closed">Anatomy of a SAML assertion</h3>
<p>The previous section discusses how your <a href="https://ssoready.com/docs/saml/saml-technical-primer#saml-configuration">ACS URL</a> will
receive an HTML form with a <code>SAMLResponse</code>. Here’s a real example of such a
<code>SAMLResponse</code>, base64-decoded, that came from Okta:</p>

<p>When we later <a href="https://ssoready.com/docs/saml/saml-technical-primer#cryptographically-authenticating-a-saml-assertion">authenticate the SAML
assertion</a>, this is the
payload we will be authenticating. Whitespace matters.</p>
<p>But for the purposes of human legibility, let’s look at it in a prettier form:</p>
<div><div><p><span>assertion.xml (Pretty-Indented)</span></p></div><pre tabindex="0"></pre></div>
<p>The most important pieces of information are:</p>
<ul>
<li>
<p>The assertion <strong>issuer</strong> lives in <code>&lt;saml2:Issuer&gt;</code></p>

</li>
<li>
<p>The assertion <strong>signature</strong> lives in <code>&lt;ds:Signature&gt;</code> (specifically the one
inside <code>&lt;saml2:Assertion&gt;</code>). The most important parts are the</p>
<ul>
<li>Canonicalization <code>Algorithm</code> on <code>&lt;ds:CanonicalizationMethod&gt;</code></li>
<li>Signature <code>Algorithm</code> on <code>&lt;ds:SignatureMethod&gt;</code></li>
<li>Digest <code>Algorithm</code> on <code>&lt;ds:DigestMethod&gt;</code></li>
<li>The digest hash in <code>&lt;ds:DigestValue&gt;</code></li>
<li>The signature value in <code>&lt;ds:SignatureValue&gt;</code></li>
</ul>

</li>
<li>
<p>The assertion <strong>subject ID</strong> lives in <code>&lt;saml2:NameID&gt;</code></p>

</li>
<li>
<p>The assertion’s <strong>validity window</strong> is specified by the <code>NotBefore</code> and <code>NotOnOrAfter</code> on <code>&lt;saml2:Conditions&gt;</code></p>

</li>
<li>
<p>The assertion’s <strong>audience</strong> lives in <code>&lt;saml2:Audience&gt;</code></p>

</li>
</ul>
<p>Validating the assertion signature is what <a href="https://ssoready.com/docs/saml/saml-technical-primer#cryptographically-authenticating-a-saml-assertion">cryptographically authenticating a
SAML assertion</a> is all
about. Validating all the other pieces of information — the issuer, the subject
ID, the validity window, the audience — happens when you <a href="https://ssoready.com/docs/saml/saml-technical-primer#deciding-whether-to-honor-a-saml-login">decide whether to
honor the login</a>.</p>
<h3 id="cryptographically-authenticating-a-saml-assertion" data-state="closed">Cryptographically authenticating a SAML assertion</h3>
<div><p>Cryptographically authenticating SAML assertions is the most perilous part
of implementing SAML. This is the step where the most security-critical
mistakes happen.</p><p>If you choose to implement this yourself, you’re going to at minimum have to
handle untrusted XML payloads. Make sure your code (and its
dependencies) aren’t susceptible to generic XML vulnerabilities like
<a target="_blank" rel="noreferrer" href="https://cwe.mitre.org/data/definitions/776.html">billion laughs<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a> and <a target="_blank" rel="noreferrer" href="https://owasp.org/www-community/vulnerabilities/XML_External_Entity_(XXE)_Processing">XML
entity expansion
attacks<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>.</p><p>From there, you’ll need to implement <a target="_blank" rel="noreferrer" href="https://www.w3.org/TR/xmldsig-core/">XML Signature (aka
XMLDsig)<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>. This section will call out
many of the more common vulnerabilities with XML Signature implementations.</p></div>
<p>Before you can process a SAML assertion, you need to verify that it was really
sent by your customer’s identity provider. You <strong>must</strong> do this, because the
SAML assertion comes from an untrusted source: a user’s browser. How do you know
the user’s request contains a SAML assertion that was really produced by your
customer’s identity provider?</p>
<div><p>Make sure your SAML implementation can’t be tricked into skipping the
process of cryptographically authenticating SAML assertions.</p><p>Many SAML implementations can have such checks trivially bypassed by, for
example, just removing the <code>&lt;ds:Signature /&gt;</code> elements in an assertion. This
attack works most often when code contains logic that merely asks “are there
any invalid signatures in this XML payload?“. A SAML assertion without any
signatures trivially passes such a check.</p><p><a href="https://ssoready.com/docs/ssoready-concepts/saml-login-flows#unsigned-assertion">SSOReady always requires that SAML assertions be
signed</a>. This
functionality cannot be disabled.</p></div>
<p>SAML implements cryptographic authentication using <a target="_blank" rel="noreferrer" href="https://www.w3.org/TR/xmldsig-core/">XML
Signature<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>, which is a very complicated
standard that tries to anticipate dozens of different ways to sign XML messages.
Thankfully, the SAML specification does restrict what parts of XML Signature can
be used in a SAML assertion:</p>
<blockquote>
<p>5.4 XML Signature Profile</p>
<p>[…] This section details constraints on these facilities so that SAML processors do not
have to deal with the full generality of XML Signature processing.</p>
</blockquote>
<p>The restrictions SAML imposes on XML Signature are:</p>
<ul>
<li>XML Signature supports many different ways for where to put a signature
relative to what it signs. SAML assertions are signed using <em>enveloped</em>
signatures. This means the
<code>&lt;ds:Signature /&gt;</code> elements in a SAML assertion are placed <em>inside</em> the assertion.</li>
</ul>
<div><p>The same section of the specification reads:</p><blockquote>
<p>SAML processors SHOULD support the use of RSA signing and verification for public key
operations in accordance with the algorithm identified by <code>http://www.w3.org/2000/09/xmldsig#rsa-sha1</code>.</p>
</blockquote><p>Do not implement this requirement. Require <code>http://www.w3.org/2001/04/xmldsig-more#rsa-sha256</code> instead.</p><p>SHA1 was still considered secure when SAML 2.0 was drafted, but it is not
considered secure today. In practice, all modern identity providers support
RSA-SHA256 at minimum instead.</p></div>
<ul>
<li>
<p>XML Signature supports many different ways for a signature to indicate what
it’s signing. SAML stipulates that every assertion must have an <code>ID="..."</code>
attribute, and that the signature points at it using <code>URI="#..."</code>.</p>
</li>
<li>
<p>XML Signature supports many <em>canonicalization</em> algorithms (more on these
later). SAML assertions always use <a target="_blank" rel="noreferrer" href="https://www.w3.org/TR/xml-exc-c14n/">Exclusive XML
Canonicalization<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>.</p>
</li>
</ul>
<p>SAML authenticates data in a three-step process: a subset of the SAML assertion
gets <em>canonicalized</em> and then <em>digested</em> (i.e. hashed). The hash is then
<em>signed</em> using RSA.</p>
<p>More concretely, the steps are to:</p>
<ol>
<li><a href="https://ssoready.com/docs/saml/saml-technical-primer#extracting-the-saml-assertion-to-authenticate">Extract out the data that we want to canonicalize</a></li>
<li><a href="https://ssoready.com/docs/saml/saml-technical-primer#canonicalizing-a-saml-assertion">Canonicalize that data</a></li>
<li><a href="https://ssoready.com/docs/saml/saml-technical-primer#verifying-the-digest-of-the-canonicalized-assertion">Verify the digest (i.e. hash) of the canonicalized data</a></li>
<li><a href="https://ssoready.com/docs/saml/saml-technical-primer#extracting-the-signedinfo-to-sign">Extract out the data we want to sign</a></li>
<li><a href="https://ssoready.com/docs/saml/saml-technical-primer#authenticating-the-signedinfo">Verify the RSA signature of that data</a></li>
</ol>

<p>The data to authenticate is the <code>&lt;saml2:Assertion&gt;</code> inside the overall
<code>&lt;saml2p:Response&gt;</code> payload, but with the <code>&lt;ds:Signature&gt;</code> element removed.
However, you may need to copy over namespace declarations from the top-level
<code>&lt;saml2p:Response&gt;</code>; for instance, the identity provider
<a target="_blank" rel="noreferrer" href="https://www.keycloak.org/">Keycloak<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a> shapes its
assertions like so:</p>

<p>You don’t sign <code>&lt;saml:Assertion&gt;...&lt;/saml:Assertion&gt;</code>. You have to copy over all
namespaces “above” the XML assertion that are <a target="_blank" rel="noreferrer" href="https://www.w3.org/TR/xml-exc-c14n/#def-visibly-utilizes">“visibly
utilized”<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>, including
in this case the <code>xmlns:saml</code> declaration:</p>

<p>With this data in hand, you are ready to canonicalize the assertion.</p>
<h4 id="canonicalizing-a-saml-assertion" data-state="closed">Canonicalizing a SAML assertion</h4>
<p>From there, you have to carry out the <a target="_blank" rel="noreferrer" href="https://www.w3.org/TR/xml-exc-c14n/">Exclusive XML
Canonicalization<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a> algorithm on the
assertion. This algorithm is hairy in the details, but at a high level it is
there to make operations like “remove the <code>&lt;Signature&gt;</code> element from the
<code>&lt;Assertion&gt;</code>” be something that two parties can carry out, and still end up
with exactly the same set of bytes. Canonicalization (“c14n”) is an XML-to-bytes
algorithm.</p>
<div><p>Many XML libraries have abstractions that make it impossible to implement
XML canonicalization. You may need to write your own XML parser.</p><p>You need to use a library that exposes where XML namespaces are declared
(i.e. <code>xmlns:</code> attributes), and which lets you see what namespace prefixes
(i.e. the <code>foo</code> in <code>foo:bar</code>, not just what <code>foo</code> resolves to) that elements
and attributes use. These details are often abstracted away, because they
don’t affect message semantics.</p></div>
<p>Exclusive XML Canonicalization builds on top of <a target="_blank" rel="noreferrer" href="https://www.w3.org/TR/xml-c14n11/">Canonical
XML<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>, aka “XML Canonicalization” or just “XML
c14n”.</p>
<p>XML Canonicalization is rather involved, but the basic idea is to make details
that don’t affect message semantics always resolve to the same thing:</p>
<ul>
<li>Empty elements (<code>&lt;foo /&gt;</code>) are converted to start/end pairs (<code>&lt;foo&gt;&lt;/foo&gt;</code>)</li>
<li>Element attributes are sorted by resolved namespace URI, ties broken
alphabetically. Namespace declarations come first.</li>
<li>Whitespace within elements is removed, but whitespace in text nodes is preserved</li>
</ul>
<div><p>The XML canonicalization spec is written to <a target="_blank" rel="noreferrer" href="https://www.w3.org/TR/2001/REC-xml-c14n-20010315#Example-Entities">require support for entity
expansion<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>,
for instance requiring that this document:</p><div><div><p><span>Input.xml (from the XML Canonicalization specification)</span></p></div><pre tabindex="0"></pre></div><p>Canonicalize to:</p><p><strong>Do not honor this requirement.</strong> You will be vulnerable to <a target="_blank" rel="noreferrer" href="https://owasp.org/www-community/vulnerabilities/XML_External_Entity_(XXE)_Processing">XML Entity
Expansion
(“XXE”)<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>
attacks. The specification here is simply inappropriate for systems that
handle untrusted user input, such as SAML. In the real world, no SAML
systems rely on entity expansion. This part of the spec is irrelevant and
actively insecure in practice.</p></div>
<p>What makes XML Exclusive Canonicalization different from ordinary XML
Canonicalization is in how XML namespaces are handled. In particular, XML
Canonicalization stipulates that you only include XML namespaces that are
<a target="_blank" rel="noreferrer" href="https://www.w3.org/TR/xml-exc-c14n/#def-visibly-utilizes"><em>visibly utilized</em><svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>.</p>
<p>In other words, you take every namespace declaration (e.g. a <code>xmlns:foo="bar"</code>
attribute), and you scan through everything “inside” that element. If they use
the declared namespace prefix (e.g. <code>&lt;foo /&gt;</code> or <code>foo:lorem="ipsum"</code>), then you
keep the namespace declaration. Otherwise, you omit it from the output. If a
namespace declaration is “shadowed” (i.e. redeclared by a child element), then
you need to make sure it’s not the child declaration that’s being used. If two
prefixes resolve to the same URI (e.g. <code>&lt;lorem xmlns:a="xxx" xmlns:b="xxx"&gt;</code>),
you need to track the prefixes independently.</p>
<p>XML Exclusive Canonicalization permits for an <a target="_blank" rel="noreferrer" href="https://www.w3.org/TR/xml-exc-c14n/#def-InclusiveNamespaces-PrefixList"><code>InclusiveNamespaces PrefixList</code><svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>
parameter. You need to support this. In SAML, that parameter gets passed in a
<code>InclusiveNamespaces</code> attribute under the <code>ds:Transform</code> element for
canonicalization in the signature:</p>

<p>What this element concretely does is say that any declaration of <code>xs</code> (e.g.
<code>xmlns:xs="..."</code>) is always treated as being visibly used.</p>
<div><p>The XML Exclusive Canonicalization spec has a bunch of discussion about
special-casing <code>xmlns=""</code>. You don’t need to worry about this; it’s written
to make the spec easier to implement using XPath, which has a hard time
“seeing” <code>xmlns=""</code> declarations. But such declarations are never used in
practice in SAML.</p><p>You do, however, need to handle checking whether default (i.e. unprefixed)
namespace declarations are visibly used. Many identity providers send
assertions that declare default namespaces. Not all of these declarations
are always visibly used.</p></div>
<p>When you’re done with this step, you’ve converted the SAML payload into a
precise sequence of bytes, representing a normalized (i.e. canonicalized)
representation of the payload’s <code>&lt;Assertion&gt;</code> with the <code>&lt;Signature&gt;</code> removed.
Now, we can move on to doing cryptography.</p>
<h4 id="verifying-the-digest-of-the-canonicalized-assertion" data-state="closed">Verifying the digest of the canonicalized assertion</h4>
<p>After converting the SAML assertion into a set of canonicalized bytes, SAML
requires that those bytes be put through a digest — i.e. cryptographic hash
— algorithm.</p>
<p>The SAML specification does not put constraints on what digest algorithm be
used, but you can limit yourself to supporting SHA-256; it’s secure and widely
supported by modern identity providers.</p>
<div><p>As with the prior warning regarding RSA-SHA1, we recommend against
implementing SHA1. It is not generally considered secure today.</p><p>SHA1 is a legal digest algorithm for a SAML implementation to use, but its
use has since been formally discouraged by later revisions of the XML
Signature specification:
<a target="_blank" rel="noreferrer" href="https://www.w3.org/TR/xmldsig-core1/#sec-MessageDigests">https://www.w3.org/TR/xmldsig-core1/#sec-MessageDigests<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a></p><p>In practice, all modern identity providers support SHA256 at minimum instead.</p></div>
<p>You will compare the SHA-256 sum of the canonicalized bytes against the
<code>&lt;ds:DigestValue /&gt;</code> element of the signature. If your computed SHA-256 doesn’t
equal the digest value in the assertion, then the message is invalid /
inauthentic.</p>
<p>One quirk here is that <code>&lt;ds:DigestValue /&gt;</code> contains the base64-encoded bytes
from SHA-256, not the more common hex encoding typically used for the output of
SHA-256.</p>
<div><p>Do <strong>not</strong> stop here. All you have done to this point is make sure the
<code>Signature</code> you’re looking at is meant for the assertion you want to
process. You still do not know that the assertion was actually generated by
the identity provider.</p><p>It is trivial for an attacker to generate a correct SHA-256 digest for an
assertion. You have jumped through a hoop SAML introduces, but you have not
yet done any meaningful cryptographic authentication.</p></div>

<p>SAML relies on RSA to cryptographically sign (and authenticate) assertions. The
SHA-256 digest of the assertion, which we verified in the previous section, is
not what gets signed. Instead, an XML element <em>containing</em> the digest is signed.</p>
<p>SAML requires that the RSA-SHA256 signature be over the <code>SignedInfo</code>, an XML
element that contains the digest:</p>
<div><div><p><span>SignedInfo (Pretty-Indented)</span></p></div><pre tabindex="0"></pre></div>
<p>To make matters a bit more complicated, this payload isn’t what gets signed; you
need to copy over all XML namespaces that this <code>SignedInfo</code> payload visibly
uses, so in this case we need to define <code>xmlns:ds</code> on the <code>SignedInfo</code> before
signing:</p>
<div><div><p><span>SignedInfo with namespaces copied in (Pretty-Indented)</span></p></div><pre tabindex="0"></pre></div>
<p>This is the data that you need to verify with RSA-SHA256.</p>
<h4 id="authenticating-the-signedinfo" data-state="closed">Authenticating the <code>SignedInfo</code></h4>
<p>The correct signature is stored in the <code>&lt;SignatureValue&gt;</code> of the <code>&lt;Signature&gt;</code>
element:</p>
<div><div><p><span>SignatureValue (Pretty-Indented)</span></p></div><pre tabindex="0"></pre></div>
<p>As with the digest information, this <code>SignatureValue</code> contains base64 data. You
verify that it is an RSA PKCS #1 v1.5 signature for the XML payload you
extracted in the previous section.</p>
<p>Verifying an RSA signature requires an RSA public key. You <strong>must</strong> use the RSA
public key inside the <a href="https://ssoready.com/docs/saml/saml-technical-primer#saml-configuration">IDP’s X.509 certificate</a> to verify
the signature. Do not use any other key.</p>
<div><p>Every SAML assertion contains a <code>KeyInfo</code> element, which contains an X.509
certificate. <strong>Do not use this key.</strong> An attacker can trivially replace that
<code>KeyInfo</code> with a key they control. From there, they can generate valid
signatures easily.</p><p>Many open-source SAML libraries get this wrong. You should audit this. Any
SAML library that doesn’t take an RSA public key as a required parameter to
verify a SAML assertion is probably vulnerable.</p><p>To determine what the correct IDP certificate is, you must do so
out-of-band. See the section on <a href="https://ssoready.com/docs/saml/saml-technical-primer#saml-configuration">SAML configuration</a> in
this article. If you use SSOReady, you can have your customer securely
upload their IDP certificate using a <a href="https://ssoready.com/docs/idp-configuration/enabling-self-service-configuration-for-your-customers">self-serve configuration
UI</a>
without any work or coding on your part.</p></div>
<div><p>You can optionally check that the contents of that <code>KeyInfo</code> equal the
one-and-only key that you trust. You can use that check to gracefully detect
if your customer has rotated their IDP certificate without first giving you
the new certificate.</p><p>If you use SSOReady, your customers will get such a <a href="https://ssoready.com/docs/ssoready-concepts/saml-login-flows#bad-certificate">graceful warning about
incorrect
certificates</a> out
of the box. This is just a convenience feature for your customer; SSOReady
never trusts the <code>KeyInfo</code> on a user-provided assertion.</p></div>
<p>Once you have verified this signature (using the correct key), you have now
established that your customer’s identity provider really generated this
assertion. It is now your job to decide whether to honor this SAML login.</p>
<h3 id="deciding-whether-to-honor-a-saml-login" data-state="closed">Deciding whether to honor a SAML login</h3>
<div><p>Do <strong>not</strong> skip this step. Just because a SAML assertion was really
generated by your customer’s identity provider doesn’t mean you should honor it.</p><p>An attacker could be  performing a replay attack. You also need to guard
against the possibility of a customer <a href="https://ssoready.com/docs/saml/saml-technical-primer#handling-malicious-identity-providers">maliciously configuring their
identity provider</a> to send you
assertions designed to log in as another one of your customers.</p></div>
<p>After authenticating a SAML login, you now need to take the <a href="https://ssoready.com/docs/saml/saml-technical-primer#canonicalizing-a-saml-assertion">authenticated
payload</a> (<strong>not</strong> the original,
pre-canonicalization assertion) and carry out a few checks on the data.</p>
<div><p>Once you’ve <a href="https://ssoready.com/docs/saml/saml-technical-primer#authenticating-the-signedinfo">authenticated</a> the SAML
assertion, you need to only work with the canonicalized payload from then
on. The XML Signature specification <a target="_blank" rel="noreferrer" href="https://www.w3.org/TR/2002/REC-xmldsig-core-20020212/#sec-See">puts it this
way<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>:</p><blockquote>
<p>automated mechanism that trust the validity of a transformed document on
the basis of a valid signature should operate over the data that was
transformed (including canonicalization) and signed, not the original
pre-transformed data</p>
</blockquote><p>Concretely, the sort of vulnerability you need to worry about is that these
two messages:</p><p>Both canonicalize to the same thing, and so have the same signature. So if
your code does this:</p><p>Then with the second payload (the one with the attacker-inserted comment),
your code will be tricked into thinking the identity provider signed
<code>abraham.lincoln@whitehouse.gov</code>. In both examples, <code>validated_payload</code> are
equal, but in the second example, the pre-canonicalization <code>saml_payload</code> is
represented as:</p><p>The SAML assertion’s signature only testifies to the post-canonicalization
payload, and in this case an attacker found a way to make the semantics of a
payload be affected by something that goes away during canonicalization* (a
comment). The fix is to work with the canonicalized payload.</p><p>If you use SSOReady, the code you write does not need to handle XML payloads
at all. Internally, SSOReady implements controls against
canonicalization-related attacks.</p></div>
<p>You should check that:</p>
<ul>
<li>
<p>The assertion’s <strong>audience</strong> equals the <a href="https://ssoready.com/docs/saml/saml-technical-primer#saml-configuration">SP Entity ID</a>
you assigned.</p>

<div><p>Verifying the assertion’s audience defends against replay attacks.</p><p>An attacker may take a legitimate assertion meant for one application,
and replay it to your application in order to do privilege escalation.</p><p>For example, many universities use the same IDP for all professors and
students. Many large organizations use the same IDP for executives and
for temporary employees.</p><p>The IDP might give out SAML assertions to anyone on staff to access the
internal company documentation hub. What if an employee takes a SAML
assertion meant for the documentation hub’s SP Entity ID, but sends it
to your (much more sensitive) application instead?</p><p>You might assume that checking the X.509 certificate on the assertion
would make sure the payload is meant for you. In fact, many IDPs use the
same certificate for every application.
<a target="_blank" rel="noreferrer" href="https://shibboleth.atlassian.net/wiki/spaces/IDP5/overview">Shibboleth<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a>,
in particular, is popular in higher education and is typically
configured this way.</p><p><a href="https://ssoready.com/docs/ssoready-concepts/saml-login-flows#bad-audience">SSOReady always verifies assertion
audiences</a>. This
functionality cannot
be disabled.</p></div>
</li>
<li>
<p>The assertion’s <strong>validity window</strong> is valid against the current time, i.e.
hasn’t expired:</p>

<div><p>Verifying the assertion’s validity window defends against replay
attacks.</p><p>In the example above, Okta generated a SAML assertion that’s only valid
for 10 minutes. The intention here is that if a victim’s SAML assertion
were somehow leaked to an attacker, the attacker would have less than 10
minutes to carry out an attack. This doesn’t solve every problem, but it
does greatly limit the impact of, for example, leaks of network logs or
other historical data.</p><p>Do not try to implement your own validity window logic on top of SAML
assertions. That way, your customers can choose how tight they want to
make their assertion expirations, depending on their security posture.</p><p>SSOReady always verifies assertion expiration. This functionality cannot
be disabled.</p></div>
</li>
<li>
<p>The assertion <strong>issuer</strong> equals the <a href="https://ssoready.com/docs/saml/saml-technical-primer#saml-configuration">IDP Entity ID</a>:</p>

<div><p>Compared to verifying the assertion’s audience, verifying the issuer is
less critical.</p><p>Authenticating the assertion’s signature using the IDP’s certificate is
what does the heavy lifting of making sure the IDP really issued the
assertion. Checking the issuer is more for helping to debug SAML
misconfiguration.</p><p>Some identity providers don’t use unique IDP entity IDs;
<a target="_blank" rel="noreferrer" href="https://jumpcloud.com/">JumpCloud<svg width="1.5em" height="1.5em" viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="currentColor"><path d="M21 3L15 3M21 3L12 12M21 3V9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path><path d="M21 13V19C21 20.1046 20.1046 21 19 21H5C3.89543 21 3 20.1046 3 19V5C3 3.89543 3.89543 3 5 3H11" stroke="currentColor" stroke-linecap="round"></path></svg></a> defaults to setting the IDP Entity
ID to <code>JumpCloud</code> for all applications.</p><p><a href="https://ssoready.com/docs/ssoready-concepts/saml-login-flows#bad-issuer">SSOReady always verifies assertion
issuers</a>. This
functionality cannot be
disabled.</p></div>
</li>
</ul>
<p>Once you have done these checks, you have now established that:</p>
<ul>
<li>The SAML assertion was really issued by your customer’s identity provider</li>
<li>The SAML assertion was really meant to be consumed by your application</li>
<li>The SAML assertion was recently issued, and hasn’t expired</li>
</ul>
<p>There is now only one major validation you need to worry about: what if the
customer’s identity provider is being malicious?</p>
<h4 id="handling-malicious-identity-providers" data-state="closed">Handling malicious identity providers</h4>
<p>Identity providers will diligently issue any assertion that the relevant IT
admin tells them to. When your customer’s Okta sends you an assertion, the
contents of that assertion are vouched for <em>only</em> by that customer. It’s <strong>not</strong>
vouched for by Okta itself.</p>
<p>To make this concrete: <a href="https://ssoready.com/blog/engineering/abraham-lincoln-and-the-malicious-saml-idp/">it’s trivial to create an Okta account for
<code>abraham.lincoln@whitehouse.gov</code>.</a>,
even if you don’t work at the White House. You don’t have to verify an email or
do anything like that. You can just stick whatever you want in <em>any</em> IDP,
including the popular ones like Okta.</p>
<p>So your final security step in handling SAML is this:</p>
<blockquote>
<p>AcmeCorp just told me to log a user in as <code>bob@acmecorp.com</code>. Is
<code>bob@acmecorp.com</code> a user that AcmeCorp “owns” in my product?</p>
</blockquote>
<p>There are a few ways you can do this, but the simplest way to start is to make
an allowlist of domains, which you (not your customer) control, associated with
every SAML-using customer. Only honor SAML logins for users whose email are in
that allowlist.</p>
<p>Effectively, this approach makes it possible for each of your customers to “own”
domains in your system. Unless they’re marked as owning a domain — something
<em>you</em> control, not your customer — then they can’t do SAML logins into that
domain.</p>

<p>The biggest vulnerability you should worry about is one customer putting another
customer’s email address into their identity provider, and then trying to log in
as them. A whitelist is a simple, reliable way to stop this.</p>
<p>You do not need to worry about an identity provider being “wrong” about its own
employees. The entire point of SAML is to let you delegate one company’s logins
to an identity provider that company controls. It’s beyond your control to
prevent an identity provider from “attacking itself”. It is in your control, and
it is your responsibility, to make sure you don’t accidentally delegate one company’s logins
to another company’s identity provider.</p>
<h3 id="logging-the-user-in" data-state="closed">Logging the user in</h3>
<p>You can now proceed to log the user in. Take whatever system you normally use to
log users in (such as if they logged in via password, or “Log in with Google”,
etc.), and give the user’s web browser a login session in your normal way — be
it a cookie, a JWT bearer token, or anything else.</p>
<div><p>When you were <a href="https://ssoready.com/docs/saml/saml-technical-primer#deciding-whether-to-honor-a-saml-login">deciding whether to honor a SAML
login</a>, you had to validate whether
the SAML assertion is expired. Engineers sometimes think they need to make
their application sessions last just as long as the SAML assertion is valid.
This is not the case.</p><p>You don’t need to make the <code>NotOnOrAfter</code> of a SAML assertion affect how
long your application sessions last. It’s typical for IDPs to make
assertions very short-lived (Okta defaults to 10 minutes), because the goal
is to make it harder for attackers to intercept and replay SAML assertions. But your
application’s sessions can be much longer, because they aren’t communicated
across multiple trust boundaries like SAML assertions are.</p><p>In other words: don’t worry about this. Just give the user a session as
usual, with your usual session duration.</p></div>
<p>You should <strong>strongly</strong> consider implementing an audit log of every SAML
assertion you receive, and whether you decide to honor it or not. You should log
the entire SAML assertion; if one of your XML-related dependencies has a new
vulnerability discovered, you will need these logs to determine if that
vulnerability has been exploited against you.</p>
<div><p>If you use SSOReady, you will have <a href="https://ssoready.com/docs/ssoready-concepts/saml-login-flows">an audit log event for every SAML
login</a>. These include a timestamp,
a complete record of the assertion, and details on any errors that may have
made the SAML assertion invalid.</p></div>
<p>Beyond this, you may want to give your customers a way to disable non-SAML
logins. This doesn’t affect how you handle a SAML login; rather, making SAML
logins mandatory for a customer gives that customer the guarantee that nobody is
going around the <a href="https://ssoready.com/docs/saml/saml-technical-primer#one-click-to-fire-why-your-customers-ciso-likes-saml">benefits of SAML to your customer’s
CISO</a>.</p></div></div>]]></description>
        </item>
    </channel>
</rss>