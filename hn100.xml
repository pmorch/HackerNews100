<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 27 Apr 2025 17:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[We're building a dystopia just to make people click on ads [video] (130 pts)]]></title>
            <link>https://www.ted.com/talks/zeynep_tufekci_we_re_building_a_dystopia_just_to_make_people_click_on_ads</link>
            <guid>43812379</guid>
            <pubDate>Sun, 27 Apr 2025 14:56:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ted.com/talks/zeynep_tufekci_we_re_building_a_dystopia_just_to_make_people_click_on_ads">https://www.ted.com/talks/zeynep_tufekci_we_re_building_a_dystopia_just_to_make_people_click_on_ads</a>, See on <a href="https://news.ycombinator.com/item?id=43812379">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Newsletters</p><div><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_1694_78433)"><path fill-rule="evenodd" clip-rule="evenodd" d="M4 4.75C3.31421 4.75 2.75 5.31421 2.75 6V18C2.75 18.6858 3.31421 19.25 4 19.25H20C20.6858 19.25 21.25 18.6858 21.25 18V6C21.25 5.31421 20.6858 4.75 20 4.75H4ZM1.25 6C1.25 4.48579 2.48579 3.25 4 3.25H20C21.5142 3.25 22.75 4.48579 22.75 6V18C22.75 19.5142 21.5142 20.75 20 20.75H4C2.48579 20.75 1.25 19.5142 1.25 18V6Z" fill="#B5B5B5"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M1.38564 5.56991C1.62318 5.23057 2.09082 5.14804 2.43016 5.38558L12.0001 12.0845L21.57 5.38558C21.9093 5.14804 22.377 5.23057 22.6145 5.56991C22.852 5.90924 22.7695 6.37689 22.4302 6.61443L12.4302 13.6144C12.1719 13.7952 11.8282 13.7952 11.57 13.6144L1.56997 6.61443C1.23063 6.37689 1.1481 5.90924 1.38564 5.56991Z" fill="#B5B5B5"></path></g><defs><clipPath id="clip0_1694_78433"><rect width="24" height="24" fill="white"></rect></clipPath></defs></svg><p>Get the latest talks</p></div><p>Get a daily email featuring the latest talk, plus a quick mix of trending content.</p><form></form><div><p>By subscribing, you understand and agree that we will store, process and manage your personal information according to our</p><!-- --> <p><a href="https://www.ted.com/about/our-organization/our-policies-terms/privacy-policy">Privacy Policy</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wikipedia: Database Download (143 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Wikipedia:Database_download</link>
            <guid>43811732</guid>
            <pubDate>Sun, 27 Apr 2025 13:21:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Wikipedia:Database_download">https://en.wikipedia.org/wiki/Wikipedia:Database_download</a>, See on <a href="https://news.ycombinator.com/item?id=43811732">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en" dir="ltr" id="mw-content-text">


<p>Not to be confused with <a href="https://en.wikipedia.org/wiki/Wikipedia:DDD" title="Wikipedia:DDD">WP:DDD</a>.</p>


<p>Wikipedia offers free copies of all available content to interested users. These databases can be used for <a href="https://en.wikipedia.org/wiki/Wikipedia:Mirrors_and_forks" title="Wikipedia:Mirrors and forks">mirroring</a>, personal use, informal backups, offline use or database queries (such as for <a href="https://en.wikipedia.org/wiki/Wikipedia:Maintenance" title="Wikipedia:Maintenance">Wikipedia:Maintenance</a>). All text content is licensed under the <a href="https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License" title="Wikipedia:Text of the Creative Commons Attribution-ShareAlike 4.0 International License">Creative Commons Attribution-ShareAlike 4.0 License</a> (CC-BY-SA), and most is additionally licensed under the <a href="https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_GNU_Free_Documentation_License" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a> (GFDL).<sup id="cite_ref-1"><a href="#cite_note-1"><span>[</span>1<span>]</span></a></sup> Images and other files are available under <a href="https://en.wikipedia.org/wiki/Wikipedia:Image_copyright_tags" title="Wikipedia:Image copyright tags">different terms</a>, as detailed on their description pages. For our advice about complying with these licenses, see <a href="https://en.wikipedia.org/wiki/Wikipedia:Copyrights" title="Wikipedia:Copyrights">Wikipedia:Copyrights</a>.
</p>
<meta property="mw:PageProp/toc">
<p><h2 id="Offline_Wikipedia_readers" data-mw-thread-id="h-Offline_Wikipedia_readers"><span data-mw-comment-start="" id="h-Offline_Wikipedia_readers"></span>Offline Wikipedia readers<span data-mw-comment-end="h-Offline_Wikipedia_readers"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Offline_Wikipedia_readers","replies":[]}}--></p>
<p>Some of the many ways to read Wikipedia while offline:
</p>
<ul><li><a href="https://en.wikipedia.org/wiki/Kiwix" title="Kiwix">Kiwix</a>: (<a href="#Kiwix">§&nbsp;Kiwix</a>)&nbsp;– <a rel="nofollow" href="https://library.kiwix.org/#lang=eng">index of images</a> (2024)</li>
<li><a href="https://en.wikipedia.org/wiki/XOWA" title="XOWA">XOWA</a>: (<a href="#XOWA">§&nbsp;XOWA</a>)&nbsp;– <a rel="nofollow" href="http://xowa.org/home/wiki/Dashboard/Image_databases.html">index of images</a> (2015)</li>
<li>WikiTaxi: <a href="#WikiTaxi_(for_Windows)">§&nbsp;WikiTaxi (for Windows)</a></li>
<li>aarddict: <a href="#Aard_Dictionary_/_Aard_2">§&nbsp;Aard Dictionary / Aard 2</a></li>
<li>BzReader: <a href="#BzReader_and_MzReader_(for_Windows)">§&nbsp;BzReader and MzReader (for Windows)</a></li>
<li>WikiFilter: <a href="#WikiFilter">§&nbsp;WikiFilter</a></li>
<li>Wikipedia on rockbox: <a href="#Wikiviewer_for_Rockbox">§&nbsp;Wikiviewer for Rockbox</a></li>
<li>Selected Wikipedia articles as a printed document: <a href="https://en.wikipedia.org/wiki/Help:Printing" title="Help:Printing">Help:Printing</a></li></ul>
<p>Some of them are mobile applications&nbsp;– see "<a href="https://en.wikipedia.org/wiki/List_of_Wikipedia_mobile_applications" title="List of Wikipedia mobile applications">List of Wikipedia mobile applications</a>".
</p>
<p><h2 id="Where_do_I_get_the_dumps?" data-mw-thread-id="h-Where_do_I_get_the_dumps?"><span id="Where_do_I_get_the_dumps.3F"></span><span data-mw-comment-start="" id="h-Where_do_I_get_the_dumps?"></span>Where do I get the dumps?<span data-mw-comment-end="h-Where_do_I_get_the_dumps?"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Where_do_I_get_the_dumps?","replies":["h-English-language_Wikipedia-Where_do_I_get_the_dumps?","h-Other_Wikipedia_editions-Where_do_I_get_the_dumps?"]}}--></p>
<p><h3 id="English-language_Wikipedia" data-mw-thread-id="h-English-language_Wikipedia-Where_do_I_get_the_dumps?"><span data-mw-comment-start="" id="h-English-language_Wikipedia-Where_do_I_get_the_dumps?"></span>English-language Wikipedia<span data-mw-comment-end="h-English-language_Wikipedia-Where_do_I_get_the_dumps?"></span></h3></p>
<ul><li>Dumps from any Wikimedia Foundation project: <span><a href="https://dumps.wikimedia.org/">dumps<wbr>.wikimedia<wbr>.org</a></span> and the <a href="https://archive.org/details/wikimediadownloads" title="iarchive:wikimediadownloads">Internet Archive</a></li>
<li>English Wikipedia dumps in SQL and XML: <span><a href="https://dumps.wikimedia.org/enwiki/">dumps<wbr>.wikimedia<wbr>.org<wbr>/enwiki<wbr>/</a></span> and the <a rel="nofollow" href="https://archive.org/search.php?query=subject%3A%22enwiki%22%20AND%20subject%3A%22data%20dumps%22%20AND%20collection%3A%22wikimediadownloads%22">Internet Archive</a>
<ul><li><a href="https://meta.wikimedia.org/wiki/Data_dump_torrents#English_Wikipedia">Download</a> the data dump using a BitTorrent client (torrenting has many benefits and reduces server load, saving bandwidth costs).</li>
<li>pages-articles-multistream.xml.bz2 – Current revisions only, no talk or user pages; this is probably what you want, and is over 19 GB compressed (expands to over 86 GB when decompressed).</li>
<li>pages-meta-current.xml.bz2 – Current revisions only, all pages (including talk)</li>
<li>abstract.xml.gz – page abstracts</li>
<li>all-titles-in-ns0.gz – Article titles only (with redirects)</li>
<li>SQL files for the pages and links are also available</li>
<li>All revisions, all pages: <b>These files expand to multiple <a href="https://en.wikipedia.org/wiki/Terabyte" title="Terabyte">terabytes</a> of text. Please only download these if you know you can cope with this quantity of data.</b> Go to <a href="https://dumps.wikimedia.org/enwiki/latest/">Latest Dumps</a> and look out for all the files that have 'pages-meta-history' in their name.</li></ul></li>
<li>To download a subset of the database in XML format, such as a specific category or a list of articles see: <a href="https://en.wikipedia.org/wiki/Special:Export" title="Special:Export">Special:Export</a>, usage of which is described at <a href="https://en.wikipedia.org/wiki/Help:Export" title="Help:Export">Help:Export</a>.</li>
<li>Wiki front-end software: <a href="https://en.wikipedia.org/wiki/MediaWiki" title="MediaWiki">MediaWiki</a> <a href="https://www.mediawiki.org/">[1]</a>.</li>
<li>Database backend software: <a href="https://en.wikipedia.org/wiki/MySQL" title="MySQL">MySQL</a>.</li>
<li>Image dumps: See below.</li></ul>
<p><h3 id="Other_Wikipedia_editions" data-mw-thread-id="h-Other_Wikipedia_editions-Where_do_I_get_the_dumps?"><span data-mw-comment-start="" id="h-Other_Wikipedia_editions-Where_do_I_get_the_dumps?"></span>Other Wikipedia editions<span data-mw-comment-end="h-Other_Wikipedia_editions-Where_do_I_get_the_dumps?"></span></h3></p>
<ul><li>Wikipedia dumps in SQL and XML: <span><a href="https://dumps.wikimedia.org/">dumps<wbr>.wikimedia<wbr>.org</a></span></li></ul>
<p><h2 id="Should_I_get_multistream?" data-mw-thread-id="h-Should_I_get_multistream?"><span id="Should_I_get_multistream.3F"></span><span data-mw-comment-start="" id="h-Should_I_get_multistream?"></span>Should I get multistream?<span data-mw-comment-end="h-Should_I_get_multistream?"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Should_I_get_multistream?","replies":["h-How_to_use_multistream?-Should_I_get_multistream?","h-Other_languages-Should_I_get_multistream?"]}}--></p>
<p>:
<b>GET THE MULTISTREAM VERSION!</b> (and the corresponding index file, <i>pages-articles-multistream-index.txt.bz2</i>)
</p><p><i>pages-articles.xml.bz2</i> and <i>pages-articles-multistream.xml.bz2</i> both contain the same <i>xml</i> contents. So if you unpack either, you get the same data. But with multistream, it is possible to get an article from the archive without unpacking the whole thing. Your reader should handle this for you; if your reader doesn't support it, it will work anyway since multistream and non-multistream contain the same <i>xml</i>. The only downside to multistream is that it is marginally larger. You might be tempted to get the smaller non-multistream archive, but this will be useless if you don't unpack it. And it will unpack to ~5–10 times its original size. Penny wise, pound foolish. Get multistream.
</p><p>NOTE THAT the multistream dump file contains multiple bz2 'streams' (bz2 header, body, footer) concatenated together into one file, in contrast to the vanilla file which contains one stream. Each separate 'stream' (or really, file) in the multistream dump contains 100 pages, except possibly the last one. 
</p>
<p><h3 id="How_to_use_multistream?" data-mw-thread-id="h-How_to_use_multistream?-Should_I_get_multistream?"><span id="How_to_use_multistream.3F"></span><span data-mw-comment-start="" id="h-How_to_use_multistream?-Should_I_get_multistream?"></span>How to use multistream?<span data-mw-comment-end="h-How_to_use_multistream?-Should_I_get_multistream?"></span></h3></p>
<p>For multistream, you can get an index file, <i>pages-articles-multistream-index.txt.bz2</i>. The first field of this index is the number of bytes to seek into the compressed archive <i>pages-articles-multistream.xml.bz2</i>, the second is the article ID, the third the article title. 
</p><p>Cut a small part out of the archive with dd using the byte offset as found in the index. You could then either bzip2 decompress it or use bzip2recover, and search the first file for the article ID.
</p><p>See <a rel="nofollow" href="https://docs.python.org/3/library/bz2.html#bz2.BZ2Decompressor">https://docs.python.org/3/library/bz2.html#bz2.BZ2Decompressor</a> for info about such multistream files and about how to decompress them with python; see also <a href="https://gerrit.wikimedia.org/r/plugins/gitiles/operations/dumps/+/ariel/toys/bz2multistream/README.txt">https://gerrit.wikimedia.org/r/plugins/gitiles/operations/dumps/+/ariel/toys/bz2multistream/README.txt</a> and related files for an old working toy.
</p>
<p><h3 id="Other_languages" data-mw-thread-id="h-Other_languages-Should_I_get_multistream?"><span data-mw-comment-start="" id="h-Other_languages-Should_I_get_multistream?"></span>Other languages<span data-mw-comment-end="h-Other_languages-Should_I_get_multistream?"></span></h3></p>
<p>In the <span><a href="https://dumps.wikimedia.org/">dumps<wbr>.wikimedia<wbr>.org</a></span> directory you will find the latest SQL and XML dumps for the projects, not just English. The sub-directories are named for the <a href="https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes" title="List of ISO 639-1 codes">language code</a> and the appropriate project. Some other directories (e.g. simple, nostalgia) exist, with the same structure. These dumps are also available from the <a href="https://archive.org/details/wikimediadownloads" title="iarchive:wikimediadownloads">Internet Archive</a>.
</p>
<p><h2 id="Where_are_the_uploaded_files_(image,_audio,_video,_etc.)?" data-mw-thread-id="h-Where_are_the_uploaded_files_(image,_audio,_video,_etc.)?"><span id="Where_are_the_uploaded_files_.28image.2C_audio.2C_video.2C_etc..29.3F"></span><span data-mw-comment-start="" id="h-Where_are_the_uploaded_files_(image,_audio,_video,_etc.)?"></span>Where are the uploaded files (image, audio, video, etc.)?<span data-mw-comment-end="h-Where_are_the_uploaded_files_(image,_audio,_video,_etc.)?"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Where_are_the_uploaded_files_(image,_audio,_video,_etc.)?","replies":[]}}--></p>
<p>Images and other uploaded media are available from mirrors in addition to being served directly from Wikimedia servers. Bulk download is (as of September 2013) available from mirrors but not offered directly from Wikimedia servers. See the <a href="https://meta.wikimedia.org/wiki/Mirroring_Wikimedia_project_XML_dumps#Media" title="m:Mirroring Wikimedia project XML dumps">list of current mirrors</a>. You should <a href="https://en.wikipedia.org/wiki/Rsync" title="Rsync">rsync</a> from the mirror, then fill in the missing images from <a href="https://upload.wikimedia.org/">upload.wikimedia.org</a>; when downloading from <code>upload.wikimedia.org</code> you should throttle yourself to 1 cache miss per second (you can check headers on a response to see if was a hit or miss and then back off when you get a miss) and you shouldn't use more than one or two simultaneous HTTP connections. In any case, make sure you have an accurate <a href="https://en.wikipedia.org/wiki/User_agent" title="User agent">user agent</a> string with contact info (email address) so ops can contact you if there's an issue. You should be getting checksums from the mediawiki API and verifying them. The <a href="https://www.mediawiki.org/wiki/API:Etiquette" title="mw:API:Etiquette">API Etiquette</a> page contains some guidelines, although not all of them apply (for example, because upload.wikimedia.org isn't MediaWiki, there is no <code>maxlag</code> parameter).
</p><p>Unlike most article text, images are not necessarily licensed under the GFDL &amp; CC-BY-SA-4.0. They may be under one of many <a href="https://en.wikipedia.org/wiki/Wikipedia:File_copyright_tags/Free_licenses" title="Wikipedia:File copyright tags/Free licenses">free licenses</a>, in the <a href="https://en.wikipedia.org/wiki/Wikipedia:File_copyright_tags/Public_domain" title="Wikipedia:File copyright tags/Public domain">public domain</a>, believed to be <a href="https://en.wikipedia.org/wiki/Wikipedia:File_copyright_tags/Non-free" title="Wikipedia:File copyright tags/Non-free">fair use</a>, or even copyright infringements (which should be <a href="https://en.wikipedia.org/wiki/Wikipedia:IFD" title="Wikipedia:IFD">deleted</a>). In particular, use of fair use images outside the context of Wikipedia or similar works may be illegal. Images under most licenses require a credit, and possibly other attached copyright information. This information is included in image description pages, which are part of the text dumps available from <a href="https://dumps.wikimedia.org/">dumps.wikimedia.org</a>. In conclusion, download these images at your own risk (<a href="https://dumps.wikimedia.org/legal.html">Legal</a>).
</p>
<p><h2 id="Dealing_with_compressed_files" data-mw-thread-id="h-Dealing_with_compressed_files"><span data-mw-comment-start="" id="h-Dealing_with_compressed_files"></span>Dealing with compressed files<span data-mw-comment-end="h-Dealing_with_compressed_files"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Dealing_with_compressed_files","replies":[]}}--></p>
<p>Compressed dump files are significantly compressed, thus after being decompressed will take up <b>large</b> amounts of drive space.  A large list of decompression programs are described in <a href="https://en.wikipedia.org/wiki/Comparison_of_file_archivers" title="Comparison of file archivers">comparison of file archivers</a>. The following  programs in particular can be used to decompress bzip2, <a href="https://en.wikipedia.org/wiki/.bz2" title=".bz2">.bz2</a>, <a href="https://en.wikipedia.org/wiki/.zip" title=".zip">.zip</a>, and <a href="https://en.wikipedia.org/wiki/.7z" title=".7z">.7z</a> files.
</p>
<dl><dt><a href="https://en.wikipedia.org/wiki/Microsoft_Windows" title="Microsoft Windows">Windows</a></dt></dl>
<p>Beginning with <a href="https://en.wikipedia.org/wiki/Windows_XP" title="Windows XP">Windows XP</a>, a basic decompression program enables decompression of zip files.<sup id="cite_ref-2"><a href="#cite_note-2"><span>[</span>2<span>]</span></a></sup><sup id="cite_ref-3"><a href="#cite_note-3"><span>[</span>3<span>]</span></a></sup> Among others, the following can be used to decompress bzip2 files.
</p>
<ul><li><a rel="nofollow" href="https://sourceware.org/pub/bzip2/v102/bzip2-102-x86-win32.exe">bzip2 (command-line)</a> (from <a rel="nofollow" href="https://www.sourceware.org/bzip2/">here</a>) is available for free under a BSD license.</li>
<li><a href="https://en.wikipedia.org/wiki/7-Zip" title="7-Zip">7-Zip</a> is available for free under an <a href="https://en.wikipedia.org/wiki/GNU_Lesser_General_Public_License" title="GNU Lesser General Public License">LGPL</a> license.</li>
<li><a href="https://en.wikipedia.org/wiki/WinRAR" title="WinRAR">WinRAR</a></li>
<li><a href="https://en.wikipedia.org/wiki/WinZip" title="WinZip">WinZip</a></li></ul>
<dl><dt><a href="https://en.wikipedia.org/wiki/Macintosh" title="Macintosh">Macintosh</a> (Mac)</dt></dl>
<ul><li><a href="https://en.wikipedia.org/wiki/MacOS" title="MacOS">macOS</a> ships with the command-line bzip2 tool.</li></ul>
<dl><dt>GNU/<a href="https://en.wikipedia.org/wiki/Linux" title="Linux">Linux</a></dt></dl>
<ul><li>Most GNU/Linux distributions ship with the command-line bzip2 tool.</li></ul>
<dl><dt><a href="https://en.wikipedia.org/wiki/Berkeley_Software_Distribution" title="Berkeley Software Distribution">Berkeley Software Distribution</a> (BSD)</dt></dl>
<ul><li>Some BSD systems ship with the command-line bzip2 tool as part of the operating system. Others, such as <a href="https://en.wikipedia.org/wiki/OpenBSD" title="OpenBSD">OpenBSD</a>, provide it as a package which must first be installed.</li></ul>
<dl><dt>Notes</dt></dl>
<ol><li>Some older versions of bzip2 may not be able to handle files larger than 2 GB, so make sure you have the latest version if you experience any problems.</li>
<li>Some older archives are compressed with gzip, which is compatible with PKZIP (the most common Windows format).</li></ol>
<p><h2 id="Dealing_with_large_files" data-mw-thread-id="h-Dealing_with_large_files"><span data-mw-comment-start="" id="h-Dealing_with_large_files"></span>Dealing with large files<span data-mw-comment-end="h-Dealing_with_large_files"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Dealing_with_large_files","replies":["h-File_system_limits-Dealing_with_large_files","h-Operating_system_limits-Dealing_with_large_files","h-Tips-Dealing_with_large_files"]}}--></p>
<p>As files grow in size, so does the likelihood they will exceed some limit of a computing device. Each operating system, file system, hard storage device, and software (application) has a maximum file size limit. Each one of these will likely have a different maximum, and the lowest limit of all of them will become the file size limit for a storage device.
</p><p>The older the software in a computing device, the more likely it will have a 2 GB file limit somewhere in the system. This is due to older software using 32-bit integers for file indexing, which limits file sizes to 2^31 bytes (2 GB) (for signed integers), or 2^32 (4 GB) (for unsigned integers). Older <a href="https://en.wikipedia.org/wiki/C_(programming_language)" title="C (programming language)">C</a> <a href="https://en.wikipedia.org/wiki/Library_(computing)" title="Library (computing)">programming libraries</a> have this 2 or 4 GB limit, but the newer file libraries have been converted to 64-bit integers thus supporting file sizes up to 2^63 or 2^64 bytes (8 or 16 <a href="https://en.wikipedia.org/wiki/Exabyte" title="Exabyte">EB</a>).
</p><p>Before starting a download of a large file, check the storage device to ensure its file system can support files of such a large size, check the amount of free space to ensure that it can hold the downloaded file, and make sure the device(s) you'll use the storage with are able to read your chosen file system.
</p>
<p><h3 id="File_system_limits" data-mw-thread-id="h-File_system_limits-Dealing_with_large_files"><span data-mw-comment-start="" id="h-File_system_limits-Dealing_with_large_files"></span>File system limits<span data-mw-comment-end="h-File_system_limits-Dealing_with_large_files"></span></h3></p>
<p>There are two limits for a file system: the file system size limit, and the file system limit. In general, since the file size limit is less than the file system limit, the larger file system limits are a moot point. A large percentage of users assume they can create files up to the size of their storage device, but are wrong in their assumption. For example, a 16 GB storage device formatted as FAT32 file system has a file limit of 4 GB for any single file. The following is a list of the most common file systems, and see <a href="https://en.wikipedia.org/wiki/Comparison_of_file_systems#Limits" title="Comparison of file systems">Comparison of file systems</a> for additional detailed information.
</p>
<dl><dt><a href="https://en.wikipedia.org/wiki/Microsoft_Windows" title="Microsoft Windows">Windows</a></dt></dl>
<ul><li><a href="https://en.wikipedia.org/wiki/File_Allocation_Table" title="File Allocation Table">FAT16</a> supports files up to 4 <a href="https://en.wikipedia.org/wiki/Gigabyte" title="Gigabyte">GB</a>. FAT16 is the factory format of smaller <a href="https://en.wikipedia.org/wiki/USB" title="USB">USB</a> drives and all <a href="https://en.wikipedia.org/wiki/SD_card" title="SD card">SD cards</a> that are 2 GB or smaller.</li>
<li><a href="https://en.wikipedia.org/wiki/File_Allocation_Table" title="File Allocation Table">FAT32</a> supports files up to 4 GB. FAT32 is the factory format of larger <a href="https://en.wikipedia.org/wiki/USB" title="USB">USB</a> drives and all <a href="https://en.wikipedia.org/wiki/SD_card" title="SD card">SDHC</a> cards that are 4 GB or larger.</li>
<li><a href="https://en.wikipedia.org/wiki/ExFAT" title="ExFAT">exFAT</a> supports files up to 127 <a href="https://en.wikipedia.org/wiki/Petabyte" title="Petabyte">PB</a>. exFAT is the factory format of all <a href="https://en.wikipedia.org/wiki/SD_card" title="SD card">SDXC</a> cards, but is incompatible with most flavors of UNIX due to licensing problems.<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources.">citation needed</span></a></i>]</sup></li>
<li><a href="https://en.wikipedia.org/wiki/NTFS" title="NTFS">NTFS</a> supports files up to 16 <a href="https://en.wikipedia.org/wiki/Terabyte" title="Terabyte">TB</a>. NTFS is the default file system for modern <a href="https://en.wikipedia.org/wiki/Microsoft_Windows" title="Microsoft Windows">Windows</a> computers, including Windows 2000, Windows XP, and all their successors to date. Versions after Windows 8 can support larger files if the file system is formatted with a larger cluster size.</li>
<li><a href="https://en.wikipedia.org/wiki/ReFS" title="ReFS">ReFS</a> supports files up to 16 <a href="https://en.wikipedia.org/wiki/Exabyte" title="Exabyte">EB</a>.</li></ul>
<dl><dt><a href="https://en.wikipedia.org/wiki/Macintosh" title="Macintosh">Macintosh</a> (Mac)</dt></dl>
<ul><li><a href="https://en.wikipedia.org/wiki/HFS_Plus" title="HFS Plus">HFS Plus</a> (HFS+) (Also known as Mac OS Extended) supports files up to 8 EiB (8 exbibytes) (2^63 bytes).<sup id="cite_ref-AppleVolumecomparison_4-0"><a href="#cite_note-AppleVolumecomparison-4"><span>[</span>4<span>]</span></a></sup> An exbibyte is similar to an <a href="https://en.wikipedia.org/wiki/Exabyte" title="Exabyte">exabyte</a>. HFS Plus is supported on <a href="https://en.wikipedia.org/wiki/MacOS" title="MacOS">macOS</a> 10.2+ and <a href="https://en.wikipedia.org/wiki/IOS" title="IOS">iOS</a>. It was the default file system for <a href="https://en.wikipedia.org/wiki/MacOS" title="MacOS">macOS</a> computers prior to the release of <a href="https://en.wikipedia.org/wiki/MacOS_High_Sierra" title="MacOS High Sierra">macOS High Sierra</a> in 2017 when it was replaced as default with <a href="https://en.wikipedia.org/wiki/Apple_File_System" title="Apple File System">Apple File System</a>, <a href="https://en.wikipedia.org/wiki/APFS" title="APFS">APFS</a>.</li>
<li><a href="https://en.wikipedia.org/wiki/APFS" title="APFS">APFS</a> supports files up to 8 exbibytes (2^63 bytes).<sup id="cite_ref-AppleVolumecomparison_4-1"><a href="#cite_note-AppleVolumecomparison-4"><span>[</span>4<span>]</span></a></sup></li></ul>
<dl><dt><a href="https://en.wikipedia.org/wiki/Linux" title="Linux">Linux</a></dt></dl>
<ul><li><a href="https://en.wikipedia.org/wiki/Ext2" title="Ext2">ext2</a> and <a href="https://en.wikipedia.org/wiki/Ext3" title="Ext3">ext3</a> supports files up to 16 GB, but up to 2 TB with larger block sizes. See <a rel="nofollow" href="https://users.suse.com/~aj/linux_lfs.html">https://users.suse.com/~aj/linux_lfs.html</a> for more information.</li>
<li><a href="https://en.wikipedia.org/wiki/Ext4" title="Ext4">ext4</a> supports files up to 16 TB, using 4 KB block size. (<a rel="nofollow" href="https://fedoraproject.org/wiki/Features/F17Ext4Above16T">limit removed in e2fsprogs-1.42 (2012)</a>)</li>
<li><a href="https://en.wikipedia.org/wiki/XFS" title="XFS">XFS</a> supports files up to 8 EB.</li>
<li><a href="https://en.wikipedia.org/wiki/ReiserFS" title="ReiserFS">ReiserFS</a> supports files up to 1 EB, 8 TB on 32-bit systems.</li>
<li><a href="https://en.wikipedia.org/wiki/JFS_(file_system)" title="JFS (file system)">JFS</a> supports files up to 4 PB.</li>
<li><a href="https://en.wikipedia.org/wiki/Btrfs" title="Btrfs">Btrfs</a> supports files up to 16 EB.</li>
<li><a href="https://en.wikipedia.org/wiki/NILFS" title="NILFS">NILFS</a> supports files up to 8 EB.</li>
<li><a href="https://en.wikipedia.org/wiki/YAFFS" title="YAFFS">YAFFS</a>2 supports files up to 2 GB</li></ul>
<dl><dt><a href="https://en.wikipedia.org/wiki/FreeBSD" title="FreeBSD">FreeBSD</a></dt></dl>
<ul><li><a href="https://en.wikipedia.org/wiki/ZFS" title="ZFS">ZFS</a> supports files up to 16 EB.</li></ul>
<dl><dt>FreeBSD and other BSDs</dt></dl>
<ul><li><a href="https://en.wikipedia.org/wiki/Unix_File_System" title="Unix File System">Unix File System</a> (UFS) supports files up to 8 ZiB.</li></ul>
<p><h3 id="Operating_system_limits" data-mw-thread-id="h-Operating_system_limits-Dealing_with_large_files"><span data-mw-comment-start="" id="h-Operating_system_limits-Dealing_with_large_files"></span>Operating system limits<span data-mw-comment-end="h-Operating_system_limits-Dealing_with_large_files"></span></h3></p>
<p>Each operating system has internal file system limits for file size and drive size, which is independent of the file system or physical media. If the operating system has any limits lower than the file system or physical media, then the OS limits will be the real limit.
</p>
<dl><dt><a href="https://en.wikipedia.org/wiki/Microsoft_Windows" title="Microsoft Windows">Windows</a></dt></dl>
<ul><li>Windows 95, 98, ME have a 4 GB limit for all file sizes.</li>
<li>Windows XP has a 16 TB limit for all file sizes.</li>
<li>Windows 7 has a 16 TB limit for all file sizes.</li>
<li>Windows 8, 10, and Server 2012 have a 256 TB limit for all file sizes.</li></ul>
<dl><dt><a href="https://en.wikipedia.org/wiki/Linux" title="Linux">Linux</a></dt></dl>
<ul><li>32-bit kernel 2.4.x systems have a 2 TB limit for all file systems.</li>
<li>64-bit kernel 2.4.x systems have an 8 EB limit for all file systems.</li>
<li>32-bit kernel 2.6.x systems without option CONFIG_LBD have a 2 TB limit for all file systems.</li>
<li>32-bit kernel 2.6.x systems with option CONFIG_LBD and all 64-bit kernel 2.6.x systems have an 8 ZB limit for all file systems.<sup id="cite_ref-5"><a href="#cite_note-5"><span>[</span>5<span>]</span></a></sup></li></ul>
<p><a href="https://en.wikipedia.org/wiki/Android_(operating_system)" title="Android (operating system)">Android</a>:
Android is based on Linux, which determines its base limits.
</p>
<ul><li>Internal storage:
<ul><li><a href="https://en.wikipedia.org/wiki/Android_(operating_system)" title="Android (operating system)">Android</a> 2.3 and later uses the <a href="https://en.wikipedia.org/wiki/Ext4" title="Ext4">ext4</a> file system.<sup id="cite_ref-6"><a href="#cite_note-6"><span>[</span>6<span>]</span></a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/Android_(operating_system)" title="Android (operating system)">Android</a> 2.2 and earlier uses the <a href="https://en.wikipedia.org/wiki/YAFFS" title="YAFFS">YAFFS</a>2 file system.</li></ul></li>
<li>External storage slots:
<ul><li>All Android devices should support FAT16, FAT32, ext2 file systems.</li>
<li>Android 2.3 and later supports ext4 file system.</li></ul></li></ul>
<dl><dt><a href="https://en.wikipedia.org/wiki/Apple_Inc." title="Apple Inc.">Apple</a> <a href="https://en.wikipedia.org/wiki/IOS" title="IOS">iOS</a> (see <a href="https://en.wikipedia.org/wiki/List_of_iPhone_models" title="List of iPhone models">List of iPhone models</a>)</dt>
<dd></dd></dl>
<ul><li>All devices support <a href="https://en.wikipedia.org/wiki/HFS_Plus" title="HFS Plus">HFS Plus</a> (HFS+) for internal storage. No devices have external storage slots. Devices on 10.3 or later run <a href="https://en.wikipedia.org/wiki/Apple_File_System" title="Apple File System">Apple File System</a> supporting a max file size of 8 EB.</li></ul>
<p><h3 id="Tips" data-mw-thread-id="h-Tips-Dealing_with_large_files"><span data-mw-comment-start="" id="h-Tips-Dealing_with_large_files"></span>Tips<span data-mw-comment-end="h-Tips-Dealing_with_large_files"></span></h3></p>
<p><h4 id="Detect_corrupted_files" data-mw-thread-id="h-Detect_corrupted_files-Tips"><span data-mw-comment-start="" id="h-Detect_corrupted_files-Tips"></span>Detect corrupted files<span data-mw-comment-end="h-Detect_corrupted_files-Tips"></span></h4></p>
<p>It is useful to check the <a href="https://en.wikipedia.org/wiki/MD5" title="MD5">MD5</a> sums (provided in a file in the download directory) to make sure the download was complete and accurate. This can be checked by running the "md5sum" command on the files downloaded. Given their sizes, this may take some time to calculate. Due to the technical details of how files are stored, <i>file sizes</i> may be reported differently on different filesystems, and so are not necessarily reliable. Also, corruption may have occurred during the download, though this is unlikely.
</p>
<p><h4 id="Linux_and_Unix" data-mw-thread-id="h-Linux_and_Unix-Tips"><span data-mw-comment-start="" id="h-Linux_and_Unix-Tips"></span>Linux and Unix<span data-mw-comment-end="h-Linux_and_Unix-Tips"></span></h4></p>
<p>If you seem to be hitting the 2 GB limit, try using <a href="https://en.wikipedia.org/wiki/Wget" title="Wget">wget</a> version 1.10 or greater, <a href="https://en.wikipedia.org/wiki/CURL" title="CURL">cURL</a> version 7.11.1-1 or greater, or a recent version of <a href="https://en.wikipedia.org/wiki/Lynx_(web_browser)" title="Lynx (web browser)">lynx</a> (using -dump). Also, you can resume downloads (for example wget -c).
</p>
<p><h2 id="Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?" data-mw-thread-id="h-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?"><span id="Why_not_just_retrieve_data_from_wikipedia.org_at_runtime.3F"></span><span data-mw-comment-start="" id="h-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?"></span>Why not just retrieve data from <a href="https://en.wikipedia.org/wiki/Main_Page" title="Main Page">wikipedia.org</a> at runtime?<span data-mw-comment-end="h-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?","replies":["h-Please_do_not_use_a_web_crawler-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?","h-Doing_SQL_queries_on_the_current_database_dump-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?"]}}--></p>
<p>Suppose you are building a piece of software that at certain points displays information that came from Wikipedia. If you want your program to display the information in a different way than can be seen in the live version, you'll probably need the wikicode that is used to enter it, instead of the finished HTML.
</p><p>Also, if you want to get all the data, you'll probably want to transfer it in the most efficient way that's possible. The wikipedia.org servers need to do quite a bit of work to convert the wikicode into HTML. That's time consuming both for you and for the wikipedia.org servers, so simply spidering all pages is not the way to go.
</p><p>To access any article in XML, one at a time, access <a href="https://en.wikipedia.org/wiki/Special:Export/Title_of_the_article" title="Special:Export/Title of the article">Special:Export/Title of the article</a>.
</p><p>Read more about this at <a href="https://en.wikipedia.org/wiki/Special:Export" title="Special:Export">Special:Export</a>.
</p><p>Please be aware that live mirrors of Wikipedia that are dynamically loaded from the Wikimedia servers are prohibited. Please see <a href="https://en.wikipedia.org/wiki/Wikipedia:Mirrors_and_forks" title="Wikipedia:Mirrors and forks">Wikipedia:Mirrors and forks</a>.
</p>
<p><h3 id="Please_do_not_use_a_web_crawler" data-mw-thread-id="h-Please_do_not_use_a_web_crawler-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?"><span data-mw-comment-start="" id="h-Please_do_not_use_a_web_crawler-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?"></span>Please do not use a web crawler<span data-mw-comment-end="h-Please_do_not_use_a_web_crawler-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?"></span></h3></p>
<p>Please do not use a <a href="https://en.wikipedia.org/wiki/Web_crawler" title="Web crawler">web crawler</a> to download large numbers of articles. Aggressive crawling of the server can cause a dramatic slow-down of Wikipedia.
</p>
<p><h4 id="Sample_blocked_crawler_email" data-mw-thread-id="h-Sample_blocked_crawler_email-Please_do_not_use_a_web_crawler"><span data-mw-comment-start="" id="h-Sample_blocked_crawler_email-Please_do_not_use_a_web_crawler"></span>Sample blocked crawler email<span data-mw-comment-end="h-Sample_blocked_crawler_email-Please_do_not_use_a_web_crawler"></span></h4></p>
<dl><dd>IP address <i><b>nnn.nnn.nnn.nnn</b></i> was retrieving up to 50 pages per second from wikipedia.org addresses. Something like at least a second delay between requests is reasonable. Please respect that setting. If you must exceed it a little, do so only during the least busy times shown in our site load graphs at <b><span><a href="https://stats.wikimedia.org/#/all-wikipedia-projects">stats<wbr>.wikimedia<wbr>.org<wbr>#<wbr>/all-wikipedia-projects</a></span></b>. It's worth noting that to crawl the whole site at one hit per second will take several weeks. The originating IP is now blocked or will be shortly. Please contact us if you want it unblocked. Please don't try to circumvent it&nbsp;– we'll just block your whole IP range.</dd></dl>
<dl><dd>If you want information on how to get our content more efficiently, we offer a variety of methods, including weekly database dumps which you can load into MySQL and crawl locally at any rate you find convenient. Tools are also available which will do that for you as often as you like once you have the infrastructure in place.</dd></dl>
<dl><dd>Instead of an email reply you may prefer to visit <span>#mediawiki</span> <sup><a rel="nofollow" href="https://web.libera.chat/?channel=#mediawiki"><span>connect</span></a></sup> at irc.libera.chat to discuss your options with our team.</dd></dl>
<p><h3 id="Doing_SQL_queries_on_the_current_database_dump" data-mw-thread-id="h-Doing_SQL_queries_on_the_current_database_dump-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?"><span data-mw-comment-start="" id="h-Doing_SQL_queries_on_the_current_database_dump-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?"></span>Doing SQL queries on the current database dump<span data-mw-comment-end="h-Doing_SQL_queries_on_the_current_database_dump-Why_not_just_retrieve_data_from_wikipedia.org_at_runtime?"></span></h3></p>
<p>You can do SQL queries on the current database dump using <a rel="nofollow" href="https://quarry.wmflabs.org/">Quarry</a> (as a replacement for the disabled <a href="https://en.wikipedia.org/wiki/Special:Asksql" title="Special:Asksql (page does not exist)">Special:Asksql</a> page).
</p>
<p><h2 id="Database_schema" data-mw-thread-id="h-Database_schema"><span data-mw-comment-start="" id="h-Database_schema"></span>Database schema<span data-mw-comment-end="h-Database_schema"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Database_schema","replies":["h-SQL_schema-Database_schema","h-XML_schema-Database_schema"]}}--></p>
<p><h3 id="SQL_schema" data-mw-thread-id="h-SQL_schema-Database_schema"><span data-mw-comment-start="" id="h-SQL_schema-Database_schema"></span>SQL schema<span data-mw-comment-end="h-SQL_schema-Database_schema"></span></h3></p>
<p><i>See also: <a href="https://www.mediawiki.org/wiki/Manual:Database_layout" title="mw:Manual:Database layout">mw:Manual:Database layout</a></i>
</p><p>The sql file used to initialize a MediaWiki database can be found <a href="https://phabricator.wikimedia.org/source/mediawiki/browse/master/sql/mysql/tables-generated.sql">here</a>.
</p>
<p><h3 id="XML_schema" data-mw-thread-id="h-XML_schema-Database_schema"><span data-mw-comment-start="" id="h-XML_schema-Database_schema"></span>XML schema<span data-mw-comment-end="h-XML_schema-Database_schema"></span></h3></p>
<p>The XML schema for each dump is defined at the top of the file and described in the <a href="https://www.mediawiki.org/wiki/Help:Export#Export_format" title="mw:Help:Export">MediaWiki export help page</a>.
</p>
<p><h2 id="Help_to_parse_dumps_for_use_in_scripts" data-mw-thread-id="h-Help_to_parse_dumps_for_use_in_scripts"><span data-mw-comment-start="" id="h-Help_to_parse_dumps_for_use_in_scripts"></span>Help to parse dumps for use in scripts<span data-mw-comment-end="h-Help_to_parse_dumps_for_use_in_scripts"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Help_to_parse_dumps_for_use_in_scripts","replies":["h-Doing_Hadoop_MapReduce_on_the_Wikipedia_current_database_dump-Help_to_parse_dumps_for_use_in_scripts"]}}--></p>
<ul><li><a href="https://en.wikipedia.org/wiki/Wikipedia:Computer_help_desk/ParseMediaWikiDump" title="Wikipedia:Computer help desk/ParseMediaWikiDump">Wikipedia:Computer help desk/ParseMediaWikiDump</a> describes the <a href="https://en.wikipedia.org/wiki/Perl" title="Perl">Perl</a> Parse::MediaWikiDump library, which can parse XML dumps.</li>
<li><a rel="nofollow" href="https://web.archive.org/web/20070907074625/http://www.cs.technion.ac.il/~gabr/resources/code/wikiprep">Wikipedia preprocessor (wikiprep.pl)</a> is a <a href="https://en.wikipedia.org/wiki/Perl" title="Perl">Perl</a> script that preprocesses raw XML dumps and builds link tables, category hierarchies, collects anchor text for each article etc.</li>
<li><a rel="nofollow" href="https://github.com/svick/Wikipedia-SQl-dump-parser">Wikipedia SQL dump parser</a> is a .NET library to read MySQL dumps without the need to use MySQL database</li>
<li><a rel="nofollow" href="https://github.com/MartinRichards23/WikiDumpParser">WikiDumpParser</a> – a .NET Core library to parse the database dumps.</li>
<li><a rel="nofollow" href="https://github.com/newca12/dictionary-builder">Dictionary Builder</a> is a Rust program that can parse XML dumps and extract entries in files</li>
<li><a rel="nofollow" href="https://github.com/napsternxg/WikiUtils">Scripts for parsing Wikipedia dumps</a> ­– Python based scripts for parsing sql.gz files from wikipedia dumps.</li>
<li><a rel="nofollow" href="https://crates.io/crates/parse-mediawiki-sql/">parse-mediawiki-sql</a> – a Rust library for quickly parsing the SQL dump files with minimal memory allocation</li>
<li><a rel="nofollow" href="https://gitlab.com/tozd/go/mediawiki">gitlab.com/tozd/go/mediawiki</a> – a Go package providing utilities for processing Wikipedia and Wikidata dumps.</li></ul>
<p><h3 id="Doing_Hadoop_MapReduce_on_the_Wikipedia_current_database_dump" data-mw-thread-id="h-Doing_Hadoop_MapReduce_on_the_Wikipedia_current_database_dump-Help_to_parse_dumps_for_use_in_scripts"><span data-mw-comment-start="" id="h-Doing_Hadoop_MapReduce_on_the_Wikipedia_current_database_dump-Help_to_parse_dumps_for_use_in_scripts"></span>Doing Hadoop MapReduce on the Wikipedia current database dump<span data-mw-comment-end="h-Doing_Hadoop_MapReduce_on_the_Wikipedia_current_database_dump-Help_to_parse_dumps_for_use_in_scripts"></span></h3></p>
<p>You can do Hadoop MapReduce queries on the current database dump, but you will need an extension to the InputRecordFormat to
have each &lt;page&gt; &lt;/page&gt; be a single mapper input. A working set of java methods (jobControl, mapper, reducer, and XmlInputRecordFormat) is available at <a rel="nofollow" href="https://tpmoyer-gallery.appspot.com/hadoopWikipedia">Hadoop on the Wikipedia</a>
</p>
<p><h2 id="Help_to_import_dumps_into_MySQL" data-mw-thread-id="h-Help_to_import_dumps_into_MySQL"><span data-mw-comment-start="" id="h-Help_to_import_dumps_into_MySQL"></span>Help to import dumps into MySQL<span data-mw-comment-end="h-Help_to_import_dumps_into_MySQL"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Help_to_import_dumps_into_MySQL","replies":[]}}--></p>
<p>See:
</p>
<ul><li><a href="https://www.mediawiki.org/wiki/Manual:Importing_XML_dumps" title="mw:Manual:Importing XML dumps">mw:Manual:Importing XML dumps</a></li>
<li><a href="https://meta.wikimedia.org/wiki/Data_dumps" title="m:Data dumps">m:Data dumps</a></li></ul>

<p>Access to recent article update dumps (Snapshot API) or individual article retrieval (On-demand API) are available via <i><a href="https://enterprise.wikimedia.com/">Wikimedia Enterprise</a></i> with a free account (<a href="https://meta.wikimedia.org/wiki/Wikimedia_Enterprise" title="m:Wikimedia Enterprise">documentation on Meta wiki</a>). Alternatively, use your developer account to access APIs within Wikimedia Cloud Services.
</p>
<p><h2 id="Static_HTML_tree_dumps_for_mirroring_or_CD_distribution" data-mw-thread-id="h-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"><span data-mw-comment-start="" id="h-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span>Static HTML tree dumps for mirroring or CD distribution<span data-mw-comment-end="h-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution","replies":["h-Kiwix-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution","h-Aard_Dictionary_\/_Aard_2-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution","h-Wikiviewer_for_Rockbox-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution","h-Old_dumps-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"]}}--></p>
<p>MediaWiki 1.5 includes routines to dump a wiki to HTML, rendering the HTML with the same parser used on a live wiki. As the following page states, putting one of these dumps on the web unmodified will constitute a trademark violation. They are intended for private viewing in an intranet or desktop installation.
</p>
<ul><li>If you want to draft a traditional website in Mediawiki and dump it to HTML format, you might want to try <a rel="nofollow" href="https://barnesc.blogspot.com/2005/10/mw2html-export-mediawiki-to-static.html">mw2html</a> by <a href="https://en.wikipedia.org/wiki/User:Connelly" title="User:Connelly">User:Connelly</a>.</li>
<li>If you'd like to help develop dump-to-static HTML tools, please drop us a note on <a href="https://en.wikipedia.org/wiki/Wikipedia:Mailing_lists" title="Wikipedia:Mailing lists">the developers' mailing list</a>.</li>
<li>Static HTML dumps as of 2008 are available <a href="https://dumps.wikimedia.org/other/static_html_dumps/">here</a>.</li></ul>
<p><b>See also:</b>
</p>
<ul><li><a href="https://www.mediawiki.org/wiki/Alternative_parsers" title="mw:Alternative parsers">mw:Alternative parsers</a> lists some other not working options for getting static HTML dumps</li>
<li><a href="https://en.wikipedia.org/wiki/Wikipedia:Snapshots" title="Wikipedia:Snapshots">Wikipedia:Snapshots</a></li>
<li><a href="https://en.wikipedia.org/wiki/Wikipedia:TomeRaider_database" title="Wikipedia:TomeRaider database">Wikipedia:TomeRaider database</a></li></ul>
<p><h3 id="Kiwix" data-mw-thread-id="h-Kiwix-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"><span data-mw-comment-start="" id="h-Kiwix-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span>Kiwix<span data-mw-comment-end="h-Kiwix-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span></h3></p>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Kiwix_on_Android.jpg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Kiwix_on_Android.jpg/250px-Kiwix_on_Android.jpg" decoding="async" width="250" height="141" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Kiwix_on_Android.jpg/500px-Kiwix_on_Android.jpg 1.5x" data-file-width="4608" data-file-height="2592"></a><figcaption>Kiwix on an Android tablet</figcaption></figure>
<p><a href="https://en.wikipedia.org/wiki/Kiwix" title="Kiwix">Kiwix</a> is by far the largest offline distribution of <a href="https://en.wikipedia.org/wiki/Wikipedia" title="Wikipedia">Wikipedia</a> to date. As an offline reader, Kiwix works with a library of contents that are zim files: you can pick &amp; choose whichever <a href="https://en.wikipedia.org/wiki/Wikimedia_Foundation#Wikimedia_projects" title="Wikimedia Foundation">Wikimedia project</a> (Wikipedia in any language, <a href="https://en.wikipedia.org/wiki/Wiktionary" title="Wiktionary">Wiktionary</a>, <a href="https://en.wikipedia.org/wiki/Wikisource" title="Wikisource">Wikisource</a>, etc.), as well as <a href="https://en.wikipedia.org/wiki/TED_Talks" title="TED Talks">TED Talks</a>, <a href="https://en.wikipedia.org/wiki/PhET_Interactive_Simulations" title="PhET Interactive Simulations">PhET Interactive Maths &amp; Physics simulations</a>, <a href="https://en.wikipedia.org/wiki/Project_Gutenberg" title="Project Gutenberg">Project Gutenberg</a>, etc.
</p><p>It is free and open source, and currently available for download on: 
</p>
<ul><li><a rel="nofollow" href="https://play.google.com/store/apps/details?id=org.kiwix.kiwixmobile">Android</a></li>
<li><a rel="nofollow" href="https://itunes.apple.com/us/app/kiwix/id997079563?mt=8">iOS</a></li>
<li><a rel="nofollow" href="https://apps.apple.com/us/app/kiwix-desktop/id1275066656">macOS</a></li>
<li><a rel="nofollow" href="https://download.kiwix.org/release/kiwix-desktop/kiwix-desktop_windows_x64.zip">Windows</a> &amp; <a rel="nofollow" href="https://www.microsoft.com/store/apps/9P8SLZ4J979J">Windows 10 (UWP)</a></li>
<li><a rel="nofollow" href="https://flathub.org/apps/details/org.kiwix.desktop">GNU/Linux</a></li></ul>
<p>... as well as extensions for <a rel="nofollow" href="https://chrome.google.com/webstore/detail/kiwix/donaljnlmapmngakoipdmehbfcioahhk">Chrome</a> &amp; <a rel="nofollow" href="https://addons.mozilla.org/fr/firefox/addon/kiwix-offline/">Firefox</a> browsers, server solutions, etc. See <a rel="nofollow" href="https://www.kiwix.org/en/">official Website</a> for the complete Kiwix portfolio.
</p>
<p><h3 id="Aard_Dictionary_/_Aard_2" data-mw-thread-id="h-Aard_Dictionary_/_Aard_2-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"><span id="Aard_Dictionary_.2F_Aard_2"></span><span data-mw-comment-start="" id="h-Aard_Dictionary_/_Aard_2-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span>Aard Dictionary / Aard 2<span data-mw-comment-end="h-Aard_Dictionary_/_Aard_2-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span></h3></p>
<p><a rel="nofollow" href="https://github.com/aarddict">Aard Dictionary</a> is an offline Wikipedia reader. No images. Cross-platform for Windows, Mac, Linux, Android, Maemo. Runs on rooted Nook and Sony PRS-T1 eBooks readers.
</p><p>It also has a successor <a rel="nofollow" href="http://aarddict.org/">Aard 2</a>.
</p>
<p><h3 id="Wikiviewer_for_Rockbox" data-mw-thread-id="h-Wikiviewer_for_Rockbox-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"><span data-mw-comment-start="" id="h-Wikiviewer_for_Rockbox-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span>Wikiviewer for <a href="https://en.wikipedia.org/wiki/Rockbox" title="Rockbox">Rockbox</a><span data-mw-comment-end="h-Wikiviewer_for_Rockbox-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span></h3></p>
<p>The wikiviewer plugin for rockbox permits viewing converted Wikipedia dumps on many <a href="https://en.wikipedia.org/wiki/Rockbox" title="Rockbox">Rockbox</a> devices.
It needs a custom build and conversion of the wiki dumps using the instructions available at <a rel="nofollow" href="http://www.rockbox.org/tracker/4755">http://www.rockbox.org/tracker/4755</a> . The conversion recompresses the file and splits it into 1 <a href="https://en.wikipedia.org/wiki/Gigabyte" title="Gigabyte">GB</a> files and an index file which all need to be in the same folder on the device or micro sd card.
</p>
<p><h3 id="Old_dumps" data-mw-thread-id="h-Old_dumps-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"><span data-mw-comment-start="" id="h-Old_dumps-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span>Old dumps<span data-mw-comment-end="h-Old_dumps-Static_HTML_tree_dumps_for_mirroring_or_CD_distribution"></span></h3></p>
<ul><li>The static version of Wikipedia created by Wikimedia: <a href="https://static.wikipedia.org/">http://static.wikipedia.org/</a> Feb. 11, 2013&nbsp;– This is apparently offline now. There was no content.</li>
<li><a rel="nofollow" href="http://www.tommasoconforti.com/">Wiki2static</a> (site down as of October&nbsp;2005) <b>was</b> an experimental program set up by <a href="https://en.wikipedia.org/wiki/User:Alfio" title="User:Alfio">User:Alfio</a> to generate html dumps, inclusive of images, search function and alphabetical index. At the linked site experimental dumps and the script itself can be downloaded. As an example it was used to generate these copies of <a rel="nofollow" href="http://fixedreference.org/en/20040424/wikipedia/Main_Page">English WikiPedia 24 April 04</a>, <a rel="nofollow" href="https://web.archive.org/web/20040618150011/http://fixedreference.org/simple/20040501/wikipedia/Main_Page">Simple WikiPedia 1 May 04</a>(old database) format and <a rel="nofollow" href="http://july.fixedreference.org/en/20040724/wikipedia/Main_Page">English WikiPedia 24 July 04</a><a rel="nofollow" href="http://july.fixedreference.org/simple/20040724/wikipedia/Main_Page">Simple WikiPedia 24 July 04</a>, <a rel="nofollow" href="http://july.fixedreference.org/fr/20040727/wikipedia/Accueil">WikiPedia Francais 27 Juillet 2004</a> (new format). <a href="https://en.wikipedia.org/wiki/User:BozMo" title="User:BozMo">BozMo</a> uses a version to generate periodic static copies at <a rel="nofollow" href="http://fixedreference.org/">fixed reference</a> (site down as of October 2017).</li></ul>
<p><h2 id="Dynamic_HTML_generation_from_a_local_XML_database_dump" data-mw-thread-id="h-Dynamic_HTML_generation_from_a_local_XML_database_dump"><span data-mw-comment-start="" id="h-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span>Dynamic HTML generation from a local XML database dump<span data-mw-comment-end="h-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Dynamic_HTML_generation_from_a_local_XML_database_dump","replies":["h-XOWA-Dynamic_HTML_generation_from_a_local_XML_database_dump","h-WikiFilter-Dynamic_HTML_generation_from_a_local_XML_database_dump","h-WikiTaxi_(for_Windows)-Dynamic_HTML_generation_from_a_local_XML_database_dump","h-BzReader_and_MzReader_(for_Windows)-Dynamic_HTML_generation_from_a_local_XML_database_dump","h-EPWING-Dynamic_HTML_generation_from_a_local_XML_database_dump"]}}--></p>
<p>Instead of converting a database dump file to many pieces of static HTML, one can also use a dynamic HTML generator. Browsing a wiki page is just like browsing a Wiki site, but the content is fetched and converted from a local dump file on request from the browser.
</p>
<p><h3 id="XOWA" data-mw-thread-id="h-XOWA-Dynamic_HTML_generation_from_a_local_XML_database_dump"><span data-mw-comment-start="" id="h-XOWA-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span>XOWA<span data-mw-comment-end="h-XOWA-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span></h3></p>
<p><a href="https://en.wikipedia.org/wiki/XOWA" title="XOWA">XOWA</a> is a free, open-source application that helps download Wikipedia to a computer. Access all of Wikipedia offline, without an internet connection!
It is currently in the beta stage of development, but is functional. It is available for download <a rel="nofollow" href="http://xowa.org/home/wiki/Help/Download_XOWA.html">here</a>.
</p>
<p><h4 id="Features" data-mw-thread-id="h-Features-XOWA"><span data-mw-comment-start="" id="h-Features-XOWA"></span>Features<span data-mw-comment-end="h-Features-XOWA"></span></h4></p>
<ul><li>Displays all articles from Wikipedia without an internet connection.</li>
<li>Download a complete, recent copy of English Wikipedia.</li>
<li>Display 5.2+ million articles in full HTML formatting.</li>
<li>Show images within an article. Access 3.7+ million images using the offline image databases.</li>
<li>Works with any Wikimedia wiki, including Wikipedia, Wiktionary, Wikisource, Wikiquote, Wikivoyage (also some non-wmf dumps)</li>
<li>Works with any non-English language wiki such as French Wikipedia, German Wikisource, Dutch Wikivoyage, etc.</li>
<li>Works with other specialized wikis such as Wikidata, Wikimedia Commons, Wikispecies, or any other MediaWiki generated dump</li>
<li>Set up over 660+ other wikis including:
<ul><li>English Wiktionary</li>
<li>English Wikisource</li>
<li>English Wikiquote</li>
<li>English Wikivoyage</li>
<li>Non-English wikis, such as French Wiktionary, German Wikisource, Dutch Wikivoyage</li>
<li>Wikidata</li>
<li>Wikimedia Commons</li>
<li>Wikispecies</li>
<li>... and many more!</li></ul></li>
<li>Update your wiki whenever you want, using Wikimedia's database backups.</li>
<li>Navigate between offline wikis. Click on "Look up this word in Wiktionary" and instantly view the page in Wiktionary.</li>
<li>Edit articles to remove vandalism or errors.</li>
<li>Install to a flash memory card for portability to other machines.</li>
<li>Run on Windows, Linux and Mac OS X.</li>
<li>View the HTML for any wiki page.</li>
<li>Search for any page by title using a Wikipedia-like Search box.</li>
<li>Browse pages by alphabetical order using Special:AllPages.</li>
<li>Find a word on a page.</li>
<li>Access a history of viewed pages.</li>
<li>Bookmark your favorite pages.</li>
<li>Downloads images and other files on demand (when connected to the internet)</li>
<li>Sets up Simple Wikipedia in less than 5 minutes</li>
<li>Can be customized at many levels: from keyboard shortcuts to HTML layouts to internal options</li></ul>
<p><h4 id="Main_features" data-mw-thread-id="h-Main_features-XOWA"><span data-mw-comment-start="" id="h-Main_features-XOWA"></span>Main features<span data-mw-comment-end="h-Main_features-XOWA"></span></h4></p>
<ol><li>Very fast searching</li>
<li>Keyword (actually, title words) based searching</li>
<li>Search produces multiple possible articles: you can choose amongst them</li>
<li>LaTeX based rendering for mathematical formulae</li>
<li>Minimal space requirements: the original .bz2 file plus the index</li>
<li>Very fast installation (a matter of hours) compared to loading the dump into MySQL</li></ol>
<p><h3 id="WikiFilter" data-mw-thread-id="h-WikiFilter-Dynamic_HTML_generation_from_a_local_XML_database_dump"><span data-mw-comment-start="" id="h-WikiFilter-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span>WikiFilter<span data-mw-comment-end="h-WikiFilter-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span></h3></p>
<p><a rel="nofollow" href="http://wikifilter.sourceforge.net/">WikiFilter</a> is a program which allows you to browse over 100 dump files without visiting a Wiki site.
</p>
<p><h4 id="WikiFilter_system_requirements" data-mw-thread-id="h-WikiFilter_system_requirements-WikiFilter"><span data-mw-comment-start="" id="h-WikiFilter_system_requirements-WikiFilter"></span>WikiFilter system requirements<span data-mw-comment-end="h-WikiFilter_system_requirements-WikiFilter"></span></h4></p>
<ul><li>A recent Windows version (Windows XP is fine; Windows 98 and ME won't work because they don't have NTFS support)</li>
<li>A fair bit of hard drive space (to install you will need about 12–15 Gigabytes; afterwards you will only need about 10 Gigabytes)</li></ul>
<p><h4 id="How_to_set_up_WikiFilter" data-mw-thread-id="h-How_to_set_up_WikiFilter-WikiFilter"><span data-mw-comment-start="" id="h-How_to_set_up_WikiFilter-WikiFilter"></span>How to set up WikiFilter<span data-mw-comment-end="h-How_to_set_up_WikiFilter-WikiFilter"></span></h4></p>
<ol><li>Start downloading a Wikipedia database dump file such as an <a href="https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2">English Wikipedia dump</a>. It is best to use a download manager such as <a href="https://en.wikipedia.org/wiki/GetRight" title="GetRight">GetRight</a> so you can resume downloading the file even if your computer crashes or is shut down during the download.</li>
<li>Download XAMPPLITE from <a rel="nofollow" href="http://sourceforge.net/project/showfiles.php?group_id=61776&amp;package_id=89552">[2]</a> (you must get the 1.5.0 version for it to work). Make sure to pick the file whose filename ends with .exe</li>
<li>Install/extract it to C:\XAMPPLITE.</li>
<li>Download WikiFilter 2.3 from this site: <a rel="nofollow" href="http://sourceforge.net/projects/wikifilter">http://sourceforge.net/projects/wikifilter</a>. You will have a choice of files to download, so make sure that you pick the 2.3 version. Extract it to C:\WIKIFILTER.</li>
<li>Copy the WikiFilter.so into your C:\XAMPPLITE\apache\modules folder.</li>
<li>Edit your C:\xampplite\apache\conf\httpd.conf file, and add the following line:
<ul><li>LoadModule WikiFilter_module "C:/XAMPPLITE/apache/modules/WikiFilter.so"</li></ul></li>
<li>When your Wikipedia file has finished downloading, uncompress it into your C:\WIKIFILTER folder. (I used WinRAR <a rel="nofollow" href="https://www.rarlab.com/">http://www.rarlab.com/</a> demo version&nbsp;– BitZipper <a rel="nofollow" href="http://www.bitzipper.com/winrar.html">http://www.bitzipper.com/winrar.html</a> works well too.)</li>
<li>Run WikiFilter (WikiIndex.exe), and go to your C:\WIKIFILTER folder, and drag and drop the XML file into the window, click Load, then Start.</li>
<li>After it finishes, exit the window, and go to your C:\XAMPPLITE folder. Run the setup_xampp.bat file to configure xampp.</li>
<li>When you finish with that, run the Xampp-Control.exe file, and start Apache.</li>
<li>Browse to <a rel="nofollow" href="http://localhost/wiki">http://localhost/wiki</a> and see if it works
<ul><li>If it doesn't work, see the <a rel="nofollow" href="http://sourceforge.net/forum/forum.php?forum_id=495411">forums</a>.</li></ul></li></ol>
<p><h3 id="WikiTaxi_(for_Windows)" data-mw-thread-id="h-WikiTaxi_(for_Windows)-Dynamic_HTML_generation_from_a_local_XML_database_dump"><span id="WikiTaxi_.28for_Windows.29"></span><span data-mw-comment-start="" id="h-WikiTaxi_(for_Windows)-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span>WikiTaxi (for Windows)<span data-mw-comment-end="h-WikiTaxi_(for_Windows)-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span></h3></p>
<p><a rel="nofollow" href="https://www.yunqa.de/delphi/apps/wikitaxi/index">WikiTaxi</a> is an offline-reader for wikis in MediaWiki format. It enables users to search and browse popular wikis like Wikipedia, Wikiquote, or WikiNews, without being connected to the Internet. WikiTaxi works well with different languages like English, German, Turkish, and others but has a problem with right-to-left language scripts. WikiTaxi does not display images.
</p>
<p><h4 id="WikiTaxi_system_requirements" data-mw-thread-id="h-WikiTaxi_system_requirements-WikiTaxi_(for_Windows)"><span data-mw-comment-start="" id="h-WikiTaxi_system_requirements-WikiTaxi_(for_Windows)"></span>WikiTaxi system requirements<span data-mw-comment-end="h-WikiTaxi_system_requirements-WikiTaxi_(for_Windows)"></span></h4></p>
<ul><li>Any Windows version starting from Windows 95 or later. Large File support (greater than 4 GB which requires an exFAT filesystem) for the huge wikis (English only at the time of this writing).</li>
<li>It also works on Linux with <a href="https://en.wikipedia.org/wiki/Wine_(software)" title="Wine (software)">Wine</a>.</li>
<li>16 MB RAM minimum for the WikiTaxi reader, 128 MB recommended for the importer (more for speed).</li>
<li>Storage space for the WikiTaxi database. This requires about 11.7 GiB for the English Wikipedia (as of 5 April 2011), 2 GB for German, less for other Wikis. These figures are likely to grow in the future.</li></ul>
<p><h4 id="WikiTaxi_usage" data-mw-thread-id="h-WikiTaxi_usage-WikiTaxi_(for_Windows)"><span data-mw-comment-start="" id="h-WikiTaxi_usage-WikiTaxi_(for_Windows)"></span>WikiTaxi usage<span data-mw-comment-end="h-WikiTaxi_usage-WikiTaxi_(for_Windows)"></span></h4></p>
<ol><li>Download WikiTaxi and extract to an empty folder. No installation is otherwise required.</li>
<li>Download the XML database dump (*.xml.bz2) of your favorite wiki.</li>
<li>Run WikiTaxi_Importer.exe to import the database dump into a WikiTaxi database. The importer takes care to uncompress the dump as it imports, so make sure to save your drive space and do not uncompress beforehand.</li>
<li>When the import is finished, start up WikiTaxi.exe and open the generated database file. You can start searching, browsing, and reading immediately.</li>
<li>After a successful import, the XML dump file is no longer needed and can be deleted to reclaim disk space.</li>
<li>To update an offline Wiki for WikiTaxi, download and import a more recent database dump.</li></ol>
<p>For WikiTaxi reading, only two files are required: WikiTaxi.exe and the .taxi database. Copy them to any storage device (memory stick or memory card) or burn them to a CD or DVD and take your Wikipedia with you wherever you go!
</p>
<p><h3 id="BzReader_and_MzReader_(for_Windows)" data-mw-thread-id="h-BzReader_and_MzReader_(for_Windows)-Dynamic_HTML_generation_from_a_local_XML_database_dump"><span id="BzReader_and_MzReader_.28for_Windows.29"></span><span data-mw-comment-start="" id="h-BzReader_and_MzReader_(for_Windows)-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span>BzReader and MzReader (for Windows)<span data-mw-comment-end="h-BzReader_and_MzReader_(for_Windows)-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span></h3></p>
<p><a rel="nofollow" href="https://code.google.com/archive/p/bzreader/">BzReader</a> is an offline Wikipedia reader with fast search capabilities. It renders the Wiki text into HTML and doesn't need to decompress the database. Requires Microsoft .NET framework 2.0.
</p><p><a rel="nofollow" href="http://homepage.ntlworld.com/bharat.vadera/MzReader/">MzReader</a> by <a href="https://en.wikipedia.org/wiki/User_talk:Mun206" title="User talk:Mun206">Mun206</a> works with (though is not affiliated with) BzReader, and allows further rendering of wikicode into better HTML, including an interpretation of the monobook skin. It aims to make pages more readable. Requires Microsoft Visual Basic 6.0 Runtime, which is not supplied with the download. Also requires Inet Control and Internet Controls (Internet Explorer 6 ActiveX), which are packaged with the download.
</p>
<p><h3 id="EPWING" data-mw-thread-id="h-EPWING-Dynamic_HTML_generation_from_a_local_XML_database_dump"><span data-mw-comment-start="" id="h-EPWING-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span>EPWING<span data-mw-comment-end="h-EPWING-Dynamic_HTML_generation_from_a_local_XML_database_dump"></span></h3></p>
<p>Offline Wikipedia database in EPWING dictionary format, which is common and an out-dated <a href="https://en.wikipedia.org/wiki/Japanese_Industrial_Standards" title="Japanese Industrial Standards">Japanese Industrial Standards</a> (JIS) in Japan, can be read including thumbnail images and tables with some rendering limits, on any systems where a reader is available (<a rel="nofollow" href="https://sites.google.com/site/boookends">Boookends</a>). There are many free and commercial readers for Windows (including Mobile), Mac OS X, iOS (iPhone, iPad), Android, Unix-Linux-BSD, DOS, and Java-based browser applications (<a rel="nofollow" href="http://maximilk.web.fc2.com/viewers.htm">EPWING Viewers</a>).
</p>
<p><h2 id="Mirror_building" data-mw-thread-id="h-Mirror_building"><span data-mw-comment-start="" id="h-Mirror_building"></span>Mirror building<span data-mw-comment-end="h-Mirror_building"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-Mirror_building","replies":["h-WP-MIRROR-Mirror_building"]}}--></p>
<p><h3 id="WP-MIRROR" data-mw-thread-id="h-WP-MIRROR-Mirror_building"><span data-mw-comment-start="" id="h-WP-MIRROR-Mirror_building"></span>WP-MIRROR<span data-mw-comment-end="h-WP-MIRROR-Mirror_building"></span></h3></p>
<dl><dd><i><b>Important:</b></i> <i>WP-mirror hasn't been supported since 2014, and community verification is needed that it actually works. <a href="https://en.wikipedia.org/wiki/Wikipedia_talk:Database_download#Does_WP-mirror_work?" title="Wikipedia talk:Database download">See talk page</a>.</i></dd></dl>
<p><a href="https://www.mediawiki.org/wiki/Wp-mirror" title="mw:Wp-mirror">WP-MIRROR</a> is a free utility for mirroring any desired set of WMF wikis. That is, it builds a wiki farm that the user can browse locally. WP-MIRROR builds a complete mirror with original size media files. WP-MIRROR is available for <a rel="nofollow" href="http://www.nongnu.org/wp-mirror/">download</a>.
</p>
<p><h2 id="See_also" data-mw-thread-id="h-See_also"><span data-mw-comment-start="" id="h-See_also"></span>See also<span data-mw-comment-end="h-See_also"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-See_also","replies":[]}}--></p>
<ul><li><a href="https://en.wikipedia.org/wiki/DBpedia" title="DBpedia">DBpedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/WikiReader" title="WikiReader">WikiReader</a></li>
<li><a href="https://www.mediawiki.org/wiki/Help:Export" title="mw:Help:Export">mw:Help:Export</a></li>
<li><a href="https://meta.wikimedia.org/wiki/Help:Downloading_pages" title="m:Help:Downloading pages">m:Help:Downloading pages</a></li>
<li><a href="https://meta.wikimedia.org/wiki/Help:Import" title="m:Help:Import">m:Help:Import</a></li>
<li><a href="https://meta.wikimedia.org/wiki/Data_dumps/Other_tools" title="meta:Data dumps/Other tools">Meta:Data dumps/Other tools</a>, for related tools, e.g. extractors and "dump readers"</li>
<li><a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_CD_Selection" title="Wikipedia:Wikipedia CD Selection">Wikipedia:Wikipedia CD Selection</a></li>
<li><a href="https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia" title="Wikipedia:Size of Wikipedia">Wikipedia:Size of Wikipedia</a></li>
<li><a href="https://meta.wikimedia.org/wiki/Mirroring_Wikimedia_project_XML_dumps" title="meta:Mirroring Wikimedia project XML dumps">meta:Mirroring Wikimedia project XML dumps</a></li>
<li><a href="https://meta.wikimedia.org/wiki/Static_version_tools" title="meta:Static version tools">meta:Static version tools</a></li>
<li><a href="https://meta.wikimedia.org/wiki/Offline_Projects">Wikimedia offline projects</a></li></ul>
<p><h2 id="References" data-mw-thread-id="h-References"><span data-mw-comment-start="" id="h-References"></span>References<span data-mw-comment-end="h-References"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-References","replies":[]}}--></p>
<div><ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span>See <a href="https://en.wikipedia.org/wiki/Wikipedia:Reusing_Wikipedia_content#Re-use_of_text_under_the_GNU_Free_Documentation_License" title="Wikipedia:Reusing Wikipedia content">Wikipedia:Reusing Wikipedia content §&nbsp;Re-use of text under the GNU Free Documentation License</a> for more information on compatibility with the GFDL.</span>
</li>
<li id="cite_note-2"><span><b><a href="#cite_ref-2">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.howtogeek.com/200698/benchmarked-whats-the-best-file-compression-format/">"Benchmarked: What's the Best File Compression Format?"</a>. <i>How To Geek</i>. How-To Geek, LLC<span>. Retrieved <span>18 January</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=How+To+Geek&amp;rft.atitle=Benchmarked%3A+What%27s+the+Best+File+Compression+Format%3F&amp;rft_id=http%3A%2F%2Fwww.howtogeek.com%2F200698%2Fbenchmarked-whats-the-best-file-compression-format%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3ADatabase+download"></span></span>
</li>
<li id="cite_note-3"><span><b><a href="#cite_ref-3">^</a></b></span> <span><cite><a rel="nofollow" href="https://support.microsoft.com/en-us/help/14200/windows-compress-uncompress-zip-files">"Zip and unzip files"</a>. <i>Microsoft</i>. Microsoft<span>. Retrieved <span>18 January</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Microsoft&amp;rft.atitle=Zip+and+unzip+files&amp;rft_id=https%3A%2F%2Fsupport.microsoft.com%2Fen-us%2Fhelp%2F14200%2Fwindows-compress-uncompress-zip-files&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3ADatabase+download"></span></span>
</li>
<li id="cite_note-AppleVolumecomparison-4"><span>^ <a href="#cite_ref-AppleVolumecomparison_4-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-AppleVolumecomparison_4-1"><sup><i><b>b</b></i></sup></a></span> <span><cite><a rel="nofollow" href="https://developer.apple.com/library/archive/documentation/FileManagement/Conceptual/APFS_Guide/VolumeFormatComparison/VolumeFormatComparison.html">"Volume Format Comparison"</a>. <i>developer.apple.com</i><span>. Retrieved <span>2023-11-19</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=developer.apple.com&amp;rft.atitle=Volume+Format+Comparison&amp;rft_id=https%3A%2F%2Fdeveloper.apple.com%2Flibrary%2Farchive%2Fdocumentation%2FFileManagement%2FConceptual%2FAPFS_Guide%2FVolumeFormatComparison%2FVolumeFormatComparison.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWikipedia%3ADatabase+download"></span></span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span> <a rel="nofollow" href="http://www.suse.com/~aj/linux_lfs.html">Large File Support in Linux</a></span>
</li>
<li id="cite_note-6"><span><b><a href="#cite_ref-6">^</a></b></span> <span><a rel="nofollow" href="http://www.h-online.com/open/news/item/Android-2-3-Gingerbread-to-use-Ext4-file-system-1152775.html">Android 2.2 and before used YAFFS file system; December 14, 2010.</a></span>
</li>
</ol></div>
<p><h2 id="External_links" data-mw-thread-id="h-External_links"><span data-mw-comment-start="" id="h-External_links"></span>External links<span data-mw-comment-end="h-External_links"></span></h2><!--__DTELLIPSISBUTTON__{"threadItem":{"headingLevel":2,"name":"h-","type":"heading","level":0,"id":"h-External_links","replies":[]}}--></p>
<ul><li><a href="https://dumps.wikimedia.org/">Wikimedia downloads</a>.</li>
<li><a rel="nofollow" href="http://dammit.lt/wikistats/">Domas visits logs</a> (<a rel="nofollow" href="http://infodisiac.com/blog/2010/07/wikimedia-page-views-some-good-and-bad-news/">read this!</a>). Also, <a rel="nofollow" href="https://archive.org/details/wikipedia_visitor_stats_200712">old data</a> in <a href="https://en.wikipedia.org/wiki/Internet_Archive" title="Internet Archive">the Internet Archive</a>.</li>
<li><a href="https://meta.wikimedia.org/wiki/Mailing_lists/Overview" title="meta:Mailing lists/Overview">Wikimedia mailing lists</a> archives.</li>
<li><a href="https://en.wikipedia.org/wiki/User:Emijrp/Wikipedia_Archive" title="User:Emijrp/Wikipedia Archive">User:Emijrp/Wikipedia Archive</a>. An effort to find all the Wiki[mp]edia available data, and to encourage people to download it and save it around the globe.</li>
<li><a rel="nofollow" href="https://github.com/WikiTeam/wikiteam/blob/master/wikipediadownloader.py">Script to download all Wikipedia 7z dumps</a>.</li></ul>
<!-- 
NewPP limit report
Parsed by mw‐web.eqiad.main‐d68c94cb‐z7kwv
Cached time: 20250419113528
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
DiscussionTools time usage: 0.042 seconds
CPU time usage: 0.462 seconds
Real time usage: 0.624 seconds
Preprocessor visited node count: 1674/1000000
Post‐expand include size: 25045/2097152 bytes
Template argument size: 933/2097152 bytes
Highest expansion depth: 14/100
Expensive parser function count: 3/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 27959/5000000 bytes
Lua time usage: 0.218/10.000 seconds
Lua memory usage: 4618696/52428800 bytes
Number of Wikibase entities loaded: 0/500
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  470.816      1 -total
 23.33%  109.826      3 Template:Cite_web
 21.64%  101.876      1 Template:Reader_help
 20.66%   97.248      1 Template:Sidebar
 18.05%   84.996      1 Template:Wikipedia_how-to
 14.93%   70.306      1 Template:Ombox
  8.50%   40.040      1 Template:Short_description
  7.85%   36.970      1 Template:Shortcut
  6.46%   30.403      1 Template:Cn
  5.90%   27.792      1 Template:Startflatlist
-->

<!-- Saved in parser cache with key enwiki:pcache:68321:|#|:idhash:canonical and timestamp 20250419113528 and revision id 1284893687. Rendering was triggered because: page-view
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shardines: SQLite3 Database-per-Tenant with ActiveRecord (138 pts)]]></title>
            <link>https://blog.julik.nl/2025/04/a-can-of-shardines</link>
            <guid>43811400</guid>
            <pubDate>Sun, 27 Apr 2025 12:16:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.julik.nl/2025/04/a-can-of-shardines">https://blog.julik.nl/2025/04/a-can-of-shardines</a>, See on <a href="https://news.ycombinator.com/item?id=43811400">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

	<article>

    
		

<small>
  
    
  &nbsp;·&nbsp;<time datetime="2025-04-25T00:00:00+00:00">25 Apr 2025</time>
</small>

    <p>There is a pattern I am very fond of - “one database per tenant” in web applications with multiple, isolated users. Recently, I needed to fix an application I had for a long time where this database-per-tenant multitenancy utterly broke down, because I was doing connection management wrong. Which begat the question: how do you even approach doing it right?</p>

<p>And it turns out I was not alone in this. The most popular gem for multitenancy - Apartment - which I have even used in my failed startup back in the day -
<a href="https://github.com/rails-on-services/apartment/issues/304#issuecomment-2648202324">has the issue too.</a></p>

<p>The culprit of <q>does not handle multithreading very well</q> is actually deeper. Way deeper. Doing runtime-defined multiple databases with Rails has only recently become less haphazard, and there are no tools either via gems or built-in that facilitate these flows. It has also accrued a ton of complexity, and also changes with every major Rails revision.</p>

<p><strong>TL;DR</strong> If you need to do database-per-tenant multitenancy with Rails or ActiveRecord <em>right now</em> - grab the middleware from <a href="https://gist.github.com/julik/69066f5a819ac3b38480d42c1351f8ef">this gist</a> and move on.</p>

<p>If you are curious about the genesis of this solution, strap in - we are going on a tour of a sizeable problem, and of an API of stature - the ActiveRecord connection management. 
Read on and join me on the ride! Many thanks to <a href="https://kirshatrov.com/">Kir Shatrov</a> and <a href="https://fractaledmind.com/">Stephen Margheim</a> for their help in this.</p>

<!--more-->

<h2 id="the-advantages-of-the-database-per-tenant">The advantages of the “database per tenant”</h2>

<p>If you have a tenanted application (your “tenant” is a subgraph of your data model that can function independently, and mostly references other entities from within itself), you have a number of ways to approach an architecture like that. Imagine we have a system where the tenant is a <code>Site</code>. That system is some kind of end-user-serviceable CMS, and users own multiple <code>Sites</code> and can manage pages, media and other items within a <code>Site</code>. The data model will be as follows:</p>

<div><pre><code><span>class</span> <span>Site</span> <span>&lt;</span> <span>ActiveRecord</span><span>::</span><span>Base</span>
<span>end</span>

<span>class</span> <span>Page</span> <span>&lt;</span> <span>ActiveRecord</span><span>::</span><span>Base</span>
  <span>belongs_to</span> <span>:site</span>
  <span>has_many</span> <span>:media_blocks</span>
  <span>has_many</span> <span>:pages</span><span>,</span> <span>through: :media_blocks</span>
<span>end</span>

<span>class</span> <span>MediaBlock</span> <span>&lt;</span> <span>ActiveRecord</span><span>::</span><span>Base</span>
  <span>belongs_to</span> <span>:media_item</span>
  <span>belongs_to</span> <span>:page</span>
<span>end</span>

<span>class</span> <span>Comment</span> <span>&lt;</span> <span>ActiveRecord</span><span>::</span><span>Base</span>
  <span>belongs_to</span> <span>:page</span> <span># and thus to a Site, "through"</span>
<span>end</span>

<span>class</span> <span>MediaItem</span> <span>&lt;</span> <span>ActiveRecord</span><span>::</span><span>Base</span>
  <span>has_many</span> <span>:media_items</span>
  <span>has_many</span> <span>:pages</span><span>,</span> <span>through: :media_items</span>
  <span>has_one_</span> <span>:site</span><span>,</span> <span>through: :pages</span> <span># since it can be reused across multiple pages</span>
<span>end</span>
</code></pre></div>

<p>Sites very rarely get merged together, and 99% of the data that gets created within a <code>Site</code> stays inside that <code>Site</code>, forever. Either because your system has outgrown hosting a single <code>Site</code>, or because you want to have robust isolation (you don’t want Jane to post to her <code>Site</code> only for the article to end up on Blake’s <code>Site</code> by mistake), or because your system is wildly successful and profitable, you may want to apply the following strategies:</p>

<ul>
  <li>Just like our initial model - the <code>Site</code> has an <code>id</code>, some models link to it directly, some - through others</li>
  <li>Every model gets a <code>site_id</code>. Every <code>INSERT</code>, <code>UPDATE</code> or <code>DELETE</code> then knows which <code>Site</code> a particular model makes part of - and a deletion can address the database where the site is stored. Databases will then be <em>shards</em> and store multiple <code>Sites</code>. If you decide to become a host for Slashdot, and get millions of <code>Page</code> records and bullions of <code>Comment</code> records, they will likely be extracted into a separate DB. There will be a mapping table of sorts, that will record that <code>slashdot.org</code> gets mapped to <code>db_slashdot_tenant</code> explicitly.</li>
  <li>Just like our initial model - but there is only <em>one</em> <code>Site</code> record in the entire database. All records belonging to a <code>Site</code> are stored inside that database.</li>
</ul>

<p>There are other tricks for doing sharding/multitenancy well - for example, using generated primary keys which contain the tenant ID within them - so that shards can be merged, etc.</p>

<p>But what interests us here, specifically, is the last approach - having one database per tenant. For my smaller sites, using SQLite as the database has become part and parcel. Having a DB server that you can configure easily is very good. Having a database server that you do not have to configure at all - exceptional, though. Same for backups: centralised backup is great and useful. But nothing beats an <code>rsync</code> if that’s all you need to do a backup. And the schema becomes smaller too - we can move the <code>Site</code> out of the database outright, and the rest of the models no longer needs the associations to <code>site</code>:</p>

<pre><code>class Page &lt; ActiveRecord::Base
  has_many :media_blocks
  has_many :pages, through: :media_blocks
end

class MediaBlock &lt; ActiveRecord::Base
  belongs_to :media_item
  belongs_to :page
end

class Comment &lt; ActiveRecord::Base
  belongs_to :page # and thus to a Site, "through"
end

class MediaItem &lt; ActiveRecord::Base
  has_many :media_blocks
end
</code></pre>

<p>Development-wise those setups are a breeze too - if you need to debug something inside a particular <code>Site</code>, all you need to do is download this site’s data. With just one <code>scp</code> command, usually. And there are elephants in the room too:</p>

<ul>
  <li>Doing schema migrations where a migration runs on one tenant, but fails on another</li>
  <li>Accessing the same DB from multiple servers</li>
  <li>Doing backups is somewhat unorthodox - there are many ways to do it</li>
</ul>

<p>But remember: using this approach has one jarring advantage. It firmly pushes you out of the “big data” territory, and even out of “medium data” - it is “tiny data”.</p>

<blockquote>
  <p>“Data which, when stored on immediately-accessible random-read storage media of reasonable speed, does not fit under your desk” is my formal definition for “big data”, if that helps.</p>
</blockquote>

<p>Some joints were exceptionally successful doing this. <a href="https://use.expensify.com/blog/scaling-sqlite-to-4m-qps-on-a-single-server">Expensify,</a> for one, is notorious for pushing and pulling SQLite well beyond the boundaries most folks would call comfortable. I know that Autodesk’s own ShotGrid - back when it used to be Shotgun and was an independent software product - used SQLite3 pretty extensively. Along with the obligatory <code>SQLite3::BusyException</code> every now and then 😉</p>

<h2 id="why-this-is-challenging-with-rails">Why this is challenging with Rails</h2>

<p>When using SQLite3 “bare”, handling a database “open” and “close” is absolutely trivial:</p>

<div><pre><code><span>SQLite3</span><span>::</span><span>Database</span><span>.</span><span>open</span><span>(</span><span>"site_1.sqlite3"</span><span>)</span> <span>do</span> <span>|</span><span>db</span><span>|</span>
  <span>site_title</span> <span>=</span> <span>db</span><span>.</span><span>get_first_value</span><span>(</span><span>"SELECT title FROM sites LIMIT 1"</span><span>)</span>
  <span>pages</span> <span>=</span> <span>db</span><span>.</span><span>execute</span><span>(</span><span>"SELECT * FROM pages "</span><span>)</span>
<span>end</span>
</code></pre></div>

<p>If we use Rack, we just wrap this in a middleware:</p>

<div><pre><code><span>def</span> <span>call</span><span>(</span><span>env</span><span>)</span>
  <span>SQLite3</span><span>::</span><span>Database</span><span>.</span><span>open</span><span>(</span><span>"site_1.sqlite3"</span><span>)</span> <span>do</span> <span>|</span><span>db</span><span>|</span>
    <span>@app</span><span>.</span><span>call</span><span>(</span><span>env</span><span>.</span><span>merge</span><span>(</span><span>"site_db"</span> <span>=&gt;</span> <span>db</span><span>))</span>
  <span>end</span>
<span>end</span>  
</code></pre></div>

<p>But for this to work, the <code>db</code> variable - the handle to the database - has to be explicitly used for every query! ActiveRecord, however, manages the connections not through a variable you give it, but through it’s own “recollection” of what database a particular ActiveRecord <a href="https://stackoverflow.com/questions/141201/how-to-best-handle-per-model-database-connections-with-activerecord">superclass connects to:</a></p>

<div><pre><code>class Page &lt; ActiveRecord::Base
  establish_connection database: "site_1.sqlite3"
end
</code></pre></div>

<p>Needless to say, this code runs just once (and you don’t know exactly “when” - to which the answer is <em>at first query</em>), and is not at all designed for disconnecting and reconnecting all the time. Now, if there was a way to do this:</p>

<div><pre><code><span>class</span> <span>Page</span> <span>&lt;</span> <span>ActiveRecord</span><span>::</span><span>Base</span>
  <span>obtain_connection_from</span> <span>{</span> <span>tenancy_system</span><span>.</span><span>database_config</span> <span>}</span>
<span>end</span>
</code></pre></div>

<p>it would have been easier, but alas. And with the addition of connection pooling, query cache, schema cache - you are looking at a sizeable contraption of <a href="https://www.youtube.com/watch?v=LFrdqQZ8FFc">things which are put on top of other things.</a> Which is what makes this exercise so frustrating: you know something utterly trivial with a “bare” API is infuriatingly complicated when doing it through ActiveRecord. Moreover - this is one of the headliner use-cases for SQLite3, and ActiveRecord seems to make it nigh-impossible to execute.</p>

<p>How come?</p>

<h2 id="churn-inevitable">Churn, inevitable</h2>

<p>The reason for difficulties with multiple databases in Rails comes down to the history of that feature and the needs of the <em>hyperscalers</em> - the Githubs, the Shopifys and the Zendesks of the ecosystem.</p>

<p>Since I have been using Rails - and this ActiveRecord - for quite a long while - here is a brief history recap:</p>

<ul>
  <li>Rails 1 already had database assignment per <code>ActiveRecord::Base</code> subclass</li>
  <li>Rails 3 added connection pooling</li>
  <li>Rails 4 added <code>connection_handling</code> (albeit - hidden)</li>
  <li>Rails 6 added <code>connected_to</code></li>
  <li>Rails 7 expanded on <code>connected_to</code> with the addition of shards (so now you have both roles and shards)</li>
</ul>

<p>The interesting part of it all is that while ActiveRecord <a href="https://github.com/rails/rails/blob/main/activerecord/examples/simple.rb">example code</a> includes snippets like this:</p>

<div><pre><code><span>class</span> <span>Person</span> <span>&lt;</span> <span>ActiveRecord</span><span>::</span><span>Base</span>
  <span>establish_connection</span> <span>adapter: </span><span>"sqlite3"</span><span>,</span> <span>database: </span><span>"foobar.db"</span>
  <span>connection</span><span>.</span><span>create_table</span> <span>table_name</span><span>,</span> <span>force: </span><span>true</span> <span>do</span> <span>|</span><span>t</span><span>|</span>
    <span>t</span><span>.</span><span>string</span> <span>:name</span>
  <span>end</span>
<span>end</span>
</code></pre></div>

<p>the changes in how Rails handles multiple databases have led to the fact that this example code is useful only in a very small number of cases. For example: with a snippet like this, how do we <code>close_connection</code>? Or, how do we tell <code>Person</code> to connect to a different database after having done a query or two?</p>

<p>If you start looking into this, an entire world opens befor your eyes. And this world has its own taxonomy - and it is sizeable. <code>ConnectionHandling</code>, <code>DatabaseConfig</code>, <code>DatabaseConfigurations</code> (yes, plural), <code>Resolver</code>, <code>PoolConfig</code>, <code>PoolManager</code>… and all these things interact, live and breathe in a carefully managed dance. Spoiler: they can do everything we need, but we have to conduct them like a little orchestra - <em>just like so.</em></p>

<h2 id="divergent-api-design">Divergent API design</h2>

<p>The way ActiveRecord is designed - having model classes with no explicit way to tell them “through which connection” they should work for this query - means that there will always be some hidden state. It can be a global, or a class variable (which is… a glorified global) - or a thread-local, but somewhere there <em>is</em> a connection, and until recently (Rails 6, to be exact) there was no official way to tell ActiveRecord which connection to use.</p>

<p>In theory, an API like this could be realised:</p>

<p><code>Page.with_connection(tenant_db_conn).first</code></p>

<p>However, this database connection argument would then need to be provided to every call to ActiveRecord - and the API is truly vast.</p>

<p>An extra complication is that a lot of the design of AR assumes that a connection to the database (and that it is going to be <em>the</em> database) will be opened early, and then kept intact. The schema cache (letting ActiveRecord subclasses know which columns are in the tables, for example) gets loaded once. The query cache gets initialised once. Migrations get run once - and, again, they run on <em>the</em> database,</p>

<h2 id="divergent-configuration-lifecycle">Divergent configuration lifecycle</h2>

<p>If you want to build a multitenant system of small tenants using SQLite3, with a single database being allocated per tenant, your needs are not exactly in alignment with a hypothetical Shopify: they want to have <code>cluster_a</code>, <code>cluster_b</code>, <code>cluster_eu</code> and <code>cluster_us</code>, each tens of terabytes in size. You, instead, want to have <code>site_1</code>, <code>site_2</code> and so on - with some being just a few KB in size.</p>

<p>This would mean that for them, the configuration of those big clusters can be output into <code>database.yml</code> programmatically. It can be source-controlled, and follow strict and specific semantics regarding</p>

<ul>
  <li>When the file gets read</li>
  <li>Whether (and when) templating is done in it, for example - to inject credentials</li>
  <li>That all internal datastructures - such as connection pools - get initialised ahead of time</li>
  <li>Preconfigured, large clusters where data is usually sharded - using things like <code>shop_id</code> - but not segregated.</li>
</ul>

<p>None of the “big guys” from the mentioned three have true, single-database-per-tenant setups – or at least I never heard they do.</p>

<p>Most of the modern ActiveRecord infrastructure is built around those assumptions, not because the makers of the feature want to work against what “we” want - they just made different tradeoffs.</p>

<h2 id="divergent-db-engine-tradeoffs">Divergent DB engine tradeoffs</h2>

<p>Another important item is database performance. “Big” database servers are designed with some assumptions. For example, if you have a database, the engine would be interested in holding file descriptors open for that database or <code>mmap()</code>ed files from it. If the tablespaces are large - they will be cached in memory, and cached fairly aggressively. If there are indices - the engine will try to cache them in memory as well, and keep access to the files containing the index data close at hand.</p>

<p>The end result is that, at least when I was working on an Apartment-based system with MySQL 5.7, after a certain number of databases created you would start hitting file descriptor limits. Those are set low on MacOS, but it was still noticeable - and it was clear that it was a question of time (and scale) - which we haven’t hit though - before that would become an actual problem.</p>

<p>On balance, thus, a database server is optimised for <em>few large databases</em> – not for thousands of small ones. This is another reason why the approach with a static <code>database.yml</code> seemed so appealing.</p>

<p>Just check <a href="https://kirshatrov.com/posts/fast-skip-locked">this article</a> out:</p>

<blockquote>
  <p>By implementing these optimizations, I’ve seen remarkable performance improvements: single MySQL server handling 2M+ ticket reservation transactions per minute while the average latency of SELECT … FOR UPDATE SKIP LOCKED query staying under 400μS.</p>
</blockquote>

<p>This is the kind of perf those “big engines” optimise for. Not “quickly handling 120 pages within this site, which is one of 2 thousands”.</p>

<p>With SQLite3, the story is completely different. SQLite3 <em>thrives</em> with multiple small databases. Since a SQLite3 database is just a file (well, 3 files sometimes, but you get the point), it makes perfect sense to have a single database per tenant in the system. Since the system is multitenanted, a request for tenant <code>A</code> is guaranteed not to need data from tenant <code>B</code>. Moreover - when we are handling a request for tenant <code>A</code>, we don’t need any resources from <code>B</code> - so we don’t even need a connection (file handle).</p>

<p>Having smaller SQLite3 databases has more affordances - for example, it’s much faster - and more granular - to do backups on a per-tenant basis. Debugging becomes much easier - instead of doing a sophisticated sequence of <code>SELECT</code>s to extract the “slice” of data for a particular tenant, you just copy the tenant’s DB wholesale. Same for granular restore. Same for deletions - removing a tenant, even a large one, is just an <code>unlink</code> away.</p>

<p>So one of the reasons why the modern multi-DB features in Rails do not support dynamic tenant management with automatically allocated databases - in large numbers - is because, at least on the surface, only SQLite3 currently makes this pattern viable.</p>

<p>And it’s not only viable – it is <em>glorious.</em> Did you know that the way iCloud works, for example, is literally millions of isolated SQLite databases, stored <em>inside</em> of larger Cassandra databases?</p>

<h2 id="back-to-the-original-problem">Back to the original problem</h2>

<p>So, I had an app. It has been running for more than a decade now. It was initially built with static HTML with some templating getting pre-processed before server upload using <code>rsync</code> - it was a glorified static site generator, essentially. Then came the “admin” features, and the app acquired databases. From the get go, the app - which is a mini-CMS of sorts - provided every website owner a UI to edit the content of their website. Every website also has its own database. Requests between them never cross - and no <code>site_id</code> is involved anywhere in the process.</p>

<p>Initially it was based on raw ERB and some glue code. Then it got rewritten into Camping, and the database switching looked roughly like the <code>establish_connection</code> example above. This was pre-Rails-3, so no Rack middleware, no frills, no nothing.</p>

<p>With the ActiveRecord 3 upgrade came the dance of splitting code into something more appropriate, along with a changeover to Sinatra. And the connection management - which got moved into a Rack middleware - took the following shape:</p>

<div><pre><code><span>def</span> <span>call</span><span>(</span><span>env</span><span>)</span>
  <span>begin</span>
    <span>ActiveRecord</span><span>::</span><span>Base</span><span>.</span><span>establish_connection</span><span>(</span><span>adapter: </span><span>'sqlite3'</span><span>,</span> 
      <span>database: </span><span>env</span><span>.</span><span>fetch</span><span>(</span><span>'site_db_path'</span><span>),</span>
      <span>timeout: </span><span>BUSY_TIMEOUT</span><span>)</span>

    <span>s</span><span>,</span> <span>h</span><span>,</span> <span>b</span> <span>=</span> <span>app</span><span>.</span><span>call</span><span>(</span><span>env</span><span>.</span><span>merge</span><span>(</span><span>'database'</span> <span>=&gt;</span> <span>self</span><span>))</span>
    <span>connection_closing_body</span> <span>=</span> <span>::</span><span>Rack</span><span>::</span><span>BodyProxy</span><span>.</span><span>new</span><span>(</span><span>b</span><span>)</span> <span>do</span>
      <span>::</span><span>ActiveRecord</span><span>::</span><span>Base</span><span>.</span><span>clear_active_connections!</span>
    <span>end</span>
    <span>[</span><span>s</span><span>,</span> <span>h</span><span>,</span> <span>connection_closing_body</span><span>]</span>
  <span>rescue</span> <span>Exception</span>
    <span>::</span><span>ActiveRecord</span><span>::</span><span>Base</span><span>.</span><span>clear_active_connections!</span>
    <span>raise</span>
  <span>end</span>
<span>end</span>
</code></pre></div>

<p>And this worked… mostly. Around the same time I have installed Sentry, but I didn’t take the habit of looking at it regularly - don’t remember the exact reasons. But, after a few years of painless operation, the app started throwing odd errors. The error that caught my eye was <code>ActiveRecord::ConnectionNotEstablished</code>, and it became more frequent the more load on the site there was. More visits - more frequent errors. Fast-forward a few years, and the error became quite frequent.</p>

<p>My assumption was that to figure out what is going on, I can better upgrade to at least ActiveRecord 6. This version is the oldest Rails version which, by virtue of Ruby version compatibilities, was already able to run on Apple Silicon - <a href="https://blog.julik.nl/2025/03/a-little-adventure-in-modern-frontend">which I am a proud owner of now.</a></p>

<p>Some hours later and a multitude of CoffeeScript files converted (and even more Ruby files edited) the update was complete. I tested it locally, verified everything was in good working order, and deployed the app.</p>

<p>And just 30 minutes in - <code>ConnectionNotEstablished</code>. And not only that, but 10x as frequently as before. The update hasn’t fixed the problem – in fact, it made it worse. Some experiments I did:</p>

<ul>
  <li>Allocating a separate connection pool per tenant and managing it myself</li>
  <li>Doing a connection checkout from a pool and checking it back into the pool</li>
  <li>Juju and voodoo magic</li>
</ul>

<p>Nothing worked. With a helpful hint from <a href="https://kirshatrov.com/posts">Kir</a> – who is responsible for exactly the managing of database sharding at one of the “big boys” – I got the idea that it should be possible to use the new <code>roles:</code> parameter - and a fake database configuration - to achieve this functionality.</p>

<p>My mistake was that I was trying to manage pools and connections myself, manually - while the new Rails functionality is actually geared towards Rails maging the pools and connections <em>for you.</em> So in this instance there was also… a divergent understanding of the API.</p>

<p>I was still in the paradigm that you use <code>establish_connection</code> - like in the olden days - to manage that infrastructure. But the “blessed” approach is actually to furnish Rails the connection configurations and let it handle them automatically.</p>

<h2 id="the-solution">The solution</h2>

<p><code>ActiveRecord::Base</code> now has a class method called <code>connected_to</code>. It allows you to do exactly the thing you need for a database-per-tenant setup - hop into a block with something being your “main” database. Previously, it accepted <code>database:</code> with a whole DB configuration, but now it only accepts <code>role:</code> (and - with Rails 7 and above - <code>shard:</code>). This is how you use it:</p>

<div><pre><code><span>ActiveRecord</span><span>::</span><span>Base</span><span>.</span><span>connected_to</span><span>(</span><span>shard: </span><span>"sites_1"</span><span>)</span> <span>do</span>
  <span>site</span> <span>=</span> <span>Site</span><span>.</span><span>find</span><span>(</span><span>site_id</span><span>)</span> <span># Which lives on this shard, "sites_1"</span>
  <span>posts</span> <span>=</span> <span>site</span><span>.</span><span>pages</span><span>.</span><span>order</span><span>(</span><span>created_at: :desc</span><span>)</span>
<span>end</span>
</code></pre></div>

<p>The challenge is where the <code>sites_1</code> gets configured. Normally it would be in your <code>database.yml</code>, as per the <a href="https://guides.rubyonrails.org/active_record_multiple_databases.html">official doc:</a></p>

<div><pre><code><span>production</span><span>:</span>
  <span>primary</span><span>:</span>
    <span>database</span><span>:</span> <span>my_primary_database</span>
    <span>adapter</span><span>:</span> <span>mysql2</span>
  <span>primary_replica</span><span>:</span>
    <span>database</span><span>:</span> <span>my_primary_database</span>
    <span>adapter</span><span>:</span> <span>mysql2</span>
    <span>replica</span><span>:</span> <span>true</span>
  <span>primary_shard_one</span><span>:</span>
    <span>database</span><span>:</span> <span>my_primary_shard_one</span>
    <span>adapter</span><span>:</span> <span>mysql2</span>
    <span>migrations_paths</span><span>:</span> <span>db/migrate_shards</span>
</code></pre></div>

<p>But if you want to switch between tenants live - and tenants get created (and deleted!) at runtime - having this static config with cross-referencing keys is not going to fly at all. Moreover - even if you can change that “God config” – how do you force ActiveRecord to reload it? How can you tell ActiveRecord that a shard/tenant no longer exists? And how do you do it in a thread-safe manner? Does it lead to a reinitialisation of all the connection pools, or just addition-deletion?</p>

<p>The solution then becomes focused in one area: taking over from ActiveRecord in managing those connection pools and naming the <code>roles</code> and <code>shards</code> automatically. This is where the bulk of the work was, in the end. We want to convert our tenant database name/filename into a string to devise the <code>role</code> name that we can furnish to AR. For me, I only went to update to Rails 6, so I didn’t go into shards yet. If I did (and I might, eventually) - the tenant name would be the <code>shard</code>, and the <code>reading</code> and <code>writing</code> roles could be used for hosting a <code>readonly: true</code> DB connection and a writable one. However, a method I ended up using does <em>not</em> support shards even on Rails 8, so read on.</p>

<h2 id="so-how-do-we-create-those-pools">So how do we create those pools?</h2>

<p>The way to do it is to <em>query</em> ActiveRecord for whether a particular connection pool is already setup or not. If it is not - there is going to be a <code>NoConnectionPool</code> exception if you try to switch to the role/shard that doesn’t exist. But doing this via rescues is not great – the control flow becomes a bit intricate. What we can do instead is check whether there is a connection pool set up for a particular role/shard, and then connect if there are none. Note that since this manages pools - it needs to be protected by a mutex:</p>

<div><pre><code><span>MUX</span><span>.</span><span>synchronize</span> <span>do</span>
  <span>if</span> <span>ActiveRecord</span><span>::</span><span>Base</span><span>.</span><span>connection_handler</span><span>.</span><span>connection_pool_list</span><span>(</span><span>role_name</span><span>).</span><span>none?</span>
    <span>ActiveRecord</span><span>::</span><span>Base</span><span>.</span><span>connection_handler</span><span>.</span><span>establish_connection</span><span>(</span><span>database_config_hash</span><span>,</span> <span>role: </span><span>role_name</span><span>)</span>
  <span>end</span>
<span>end</span>
</code></pre></div>

<p>After that we can use <code>connected_to</code> - which is going to be thread-safe, fast and pretty neat:</p>

<div><pre><code><span>MUX</span><span>.</span><span>synchronize</span> <span>do</span>
  <span>if</span> <span>ActiveRecord</span><span>::</span><span>Base</span><span>.</span><span>connection_handler</span><span>.</span><span>connection_pool_list</span><span>(</span><span>role_name</span><span>).</span><span>none?</span>
    <span>ActiveRecord</span><span>::</span><span>Base</span><span>.</span><span>connection_handler</span><span>.</span><span>establish_connection</span><span>(</span><span>database_config_hash</span><span>,</span> <span>role: </span><span>role_name</span><span>)</span>
  <span>end</span>
<span>end</span>

<span>ActiveRecord</span><span>::</span><span>Base</span><span>.</span><span>connected_to</span><span>(</span><span>role: </span><span>role_name</span><span>)</span> <span>do</span>
  <span>pages</span> <span>=</span> <span>Page</span><span>.</span><span>order</span><span>(</span><span>created_at: :desc</span><span>).</span><span>limit</span><span>(</span><span>10</span><span>)</span> <span># Only selects from that site/tenant</span>
<span>end</span>
</code></pre></div>

<h2 id="dont-forget-about-streaming-rack-bodies">Don’t forget about streaming Rack bodies</h2>

<p>There is a small additional element we need to take care of, though: doing it correctly in Rack. To have something similar to <a href="https://github.com/rails-on-services/apartment/blob/development/lib/apartment/elevators/generic.rb">Apartment::Elevator</a> we need to do something that - again - <code>apartment</code> doesn’t do correctly. If we assume all the renders of our app are buffered, we can do this:</p>

<div><pre><code><span>def</span> <span>call</span><span>(</span><span>env</span><span>)</span>
  <span>site_name</span> <span>=</span> <span>env</span><span>[</span><span>"SERVER_NAME"</span><span>]</span>
  <span>connection_config_hash</span> <span>=</span> <span>{</span><span>adapter: </span><span>"sqlite3"</span><span>,</span> <span>database: </span><span>"sites/</span><span>#{</span><span>site_name</span><span>}</span><span>.sqlite3"</span><span>}</span>
  <span>role_name</span> <span>=</span> <span>"site_</span><span>#{</span><span>site_name</span><span>}</span><span>"</span>

  <span># Create a connection pool for that tenant if it doesn't exist</span>
  <span>MUX</span><span>.</span><span>synchronize</span> <span>do</span>
    <span>if</span> <span>ActiveRecord</span><span>::</span><span>Base</span><span>.</span><span>connection_handler</span><span>.</span><span>connection_pool_list</span><span>(</span><span>role_name</span><span>).</span><span>none?</span>
      <span>ActiveRecord</span><span>::</span><span>Base</span><span>.</span><span>connection_handler</span><span>.</span><span>establish_connection</span><span>(</span><span>connection_config_hash</span><span>,</span> <span>role: </span><span>role_name</span><span>)</span>
    <span>end</span>
  <span>end</span>
  <span>ActiveRecord</span><span>::</span><span>Base</span><span>.</span><span>connected_to</span><span>(</span><span>role: </span><span>role_name</span><span>)</span> <span>do</span>
    <span>@app</span><span>.</span><span>call</span><span>(</span><span>env</span><span>)</span> <span># returns [status, header, body]</span>
  <span>end</span>
<span>end</span>
</code></pre></div>

<p>But we are aware, of course, that Rack bodies are <em>callable</em> and <em>iterable</em> - and Rack response bodies may also just be doing SQL queries. Code that lives in the Rack response body and serves streaming data is not any less useful than the one living in the app <code>call()</code> method, even though much fewer people use it. I even worked on a <a href="https://github.com/appsignal/appsignal-ruby/pull/1037">big patch to Appsignal, my favorite APM</a> that made it report what happens inside a Rack streaming body the same way it would for the app’s <code>call</code>. The way it usually works for resource release with those bodies is this:</p>

<div><pre><code><span>f</span> <span>=</span> <span>File</span><span>.</span><span>open</span><span>(</span><span>path</span><span>,</span> <span>"rb"</span><span>)</span>
<span>status</span><span>,</span> <span>headers</span><span>,</span> <span>body</span> <span>=</span> <span>@app</span><span>.</span><span>call</span><span>(</span><span>env</span><span>)</span>
<span>body_with_close</span> <span>=</span> <span>Rack</span><span>::</span><span>BodyProxy</span><span>.</span><span>new</span><span>(</span><span>body</span><span>)</span> <span>{</span> <span>f</span><span>.</span><span>close</span> <span>}</span>
<span>[</span><span>status</span><span>,</span> <span>headers</span><span>,</span> <span>body_with_close</span><span>]</span>
</code></pre></div>

<p>This attaches a callback to the <code>#close</code> method of the Rack body we return, which - according to the Rack [SPEChttps://github.com/rack/rack/blob/main/SPEC.rdoc] - <em>must</em> be called by the webserver <em>or</em> by the calling middleware.</p>

<p>Reasonably enough, the ActiveRecord API for <code>connected_to</code> only works with a block. That’s a good idea from the point of encouraging the correct (and safe) usage of a rather blunt tool. However, specifically in this case, it gets in the way. Luckily, this problem can be bypassed with judicious <a href="https://blog.appsignal.com/2018/11/27/ruby-magic-fibers-and-enumerators-in-ruby.html">application of a Fiber:</a></p>

<div><pre><code><span>connected_to_context_fiber</span> <span>=</span> <span>Fiber</span><span>.</span><span>new</span> <span>do</span>
  <span>ActiveRecord</span><span>::</span><span>Base</span><span>.</span><span>connected_to</span><span>(</span><span>role: </span><span>role_name</span><span>)</span> <span>do</span>
    <span>Fiber</span><span>.</span><span>yield</span>
  <span>end</span>
<span>end</span>
<span>connected_to_context_fiber</span><span>.</span><span>resume</span>

<span>status</span><span>,</span> <span>headers</span><span>,</span> <span>body</span> <span>=</span> <span>@app</span><span>.</span><span>call</span><span>(</span><span>env</span><span>)</span>
<span>body_with_close</span> <span>=</span> <span>Rack</span><span>::</span><span>BodyProxy</span><span>.</span><span>new</span><span>(</span><span>body</span><span>)</span> <span>{</span> <span>connected_to_context_fiber_</span><span>.</span><span>resume</span> <span>}</span>

<span>[</span><span>status</span><span>,</span> <span>headers</span><span>,</span> <span>body_with_close</span><span>]</span>
</code></pre></div>

<p>And thus, our “tenant switching middleware” for ActiveRecord connection management with one database per tenant becomes:</p>

<div><pre><code><span>class</span> <span>Shardine</span>
  <span>MUX</span> <span>=</span> <span>Mutex</span><span>.</span><span>new</span>

  <span>def</span> <span>initialize</span><span>(</span><span>connection_config</span><span>)</span>
    <span>@config</span> <span>=</span> <span>connection_config</span>
    <span>@role_name</span> <span>=</span> <span>connection_config</span><span>.</span><span>fetch</span><span>(</span><span>:database</span><span>).</span><span>to_s</span>
  <span>end</span>

  <span>def</span> <span>with</span><span>(</span><span>&amp;</span><span>blk</span><span>)</span>
    <span># Create a connection pool for that tenant if it doesn't exist</span>
    <span>MUX</span><span>.</span><span>synchronize</span> <span>do</span>
      <span>if</span> <span>ActiveRecord</span><span>::</span><span>Base</span><span>.</span><span>connection_handler</span><span>.</span><span>connection_pool_list</span><span>(</span><span>@role_name</span><span>).</span><span>none?</span>
        <span>ActiveRecord</span><span>::</span><span>Base</span><span>.</span><span>connection_handler</span><span>.</span><span>establish_connection</span><span>(</span><span>@config</span><span>,</span> <span>role: </span><span>@role_name</span><span>)</span>
      <span>end</span>
    <span>end</span>
    <span>ActiveRecord</span><span>::</span><span>Base</span><span>.</span><span>connected_to</span><span>(</span><span>role: </span><span>@role_name</span><span>,</span> <span>&amp;</span><span>blk</span><span>)</span>
  <span>end</span>

  <span>def</span> <span>enter!</span>
    <span>@fiber</span> <span>=</span> <span>Fiber</span><span>.</span><span>new</span> <span>do</span>
      <span>with</span><span>(</span><span>conn</span><span>)</span> <span>{</span> <span>Fiber</span><span>.</span><span>yield</span> <span>}</span>
    <span>end</span>
    <span>@fiber</span><span>.</span><span>resume</span>
    <span>true</span>
  <span>end</span>

  <span>def</span> <span>leave!</span>
    <span># Probably there is something in ConnectionHandling</span>
    <span># that can be used here, but I was too lazy to look</span>
    <span>to_resume</span><span>,</span> <span>@fiber</span> <span>=</span> <span>@fiber</span><span>,</span> <span>nil</span>
    <span>to_resume</span><span>&amp;</span><span>.</span><span>resume</span>
  <span>end</span>

  <span>class</span> <span>Middleware</span>
    <span>def</span> <span>initialize</span><span>(</span><span>app</span><span>,</span> <span>&amp;</span><span>database_config_lookup</span><span>)</span>
      <span>@app</span> <span>=</span> <span>app</span>
      <span>@lookup</span> <span>=</span> <span>database_config_lookup</span>
    <span>end</span>

    <span>def</span> <span>call</span><span>(</span><span>env</span><span>)</span>
      <span>connection_config</span> <span>=</span> <span>@lookup</span><span>.</span><span>call</span><span>(</span><span>env</span><span>)</span>
      <span>switcher</span> <span>=</span> <span>TenantDatabaseSwitcher</span><span>.</span><span>new</span><span>(</span><span>connection_config</span><span>)</span>
      <span>did_enter</span> <span>=</span> <span>switcher</span><span>.</span><span>enter!</span>
      <span>status</span><span>,</span> <span>headers</span><span>,</span> <span>body</span> <span>=</span> <span>@app</span><span>.</span><span>call</span><span>(</span><span>env</span><span>)</span>
      <span>body_with_close</span> <span>=</span> <span>Rack</span><span>::</span><span>BodyProxy</span><span>.</span><span>new</span><span>(</span><span>body</span><span>)</span> <span>{</span> <span>switcher</span><span>.</span><span>leave!</span> <span>}</span>
      <span>[</span><span>status</span><span>,</span> <span>headers</span><span>,</span> <span>body_with_close</span><span>]</span>
    <span>rescue</span>
      <span>switcher</span><span>.</span><span>leave!</span> <span>if</span> <span>did_enter</span>
      <span>raise</span>
    <span>end</span>
  <span>end</span>
<span>end</span>
</code></pre></div>

<p>which we then configure in <code>config.ru</code> (or in Rails similarly) like so:</p>

<div><pre><code><span>use</span> <span>Shardine</span><span>::</span><span>Middleware</span> <span>do</span> <span>|</span><span>env</span><span>|</span>
  <span>site_name</span> <span>=</span> <span>env</span><span>[</span><span>"SERVER_NAME"</span><span>]</span>
  <span>{</span><span>adapter: </span><span>"sqlite3"</span><span>,</span> <span>database: </span><span>"sites/</span><span>#{</span><span>site_name</span><span>}</span><span>.sqlite3"</span><span>}</span>
<span>end</span>
</code></pre></div>

<p>And there you go - a safe and performant database-per-tenant switcher.</p>

<h2 id="an-additional-hurdle">An additional hurdle</h2>

<p>Since I was upgrading to Rails 6 - which seemed the lowest “modern” version I really had to go to - there was an extra snag.</p>

<p>By default, when you use ActiveRecord without Rails, it gets configured “conservatively” - or, rather, not configured at all. Rails 6 has the concept of <code>legacy_connection_handling</code>. Without going into too much detail, to make this solution work that parameter must be turned off explicitly. In Rails 7 and above this parameter no longer exists.</p>

<h2 id="some-remaining-work">Some remaining work</h2>

<p>Since I initially migrated my app to ActiveRecord 6 I don’t have <code>shard</code> support yet. It would actually make perfect sense to have your “reading replica” be a <code>readonly: true</code> SQLite3 database, <a href="https://fractaledmind.github.io/2024/04/11/sqlite-on-rails-isolated-connection-pools/">as Stephen has written.</a></p>

<p>Another aspect is that there’s currently no API to remove a connection pool if a tenant gets removed from the system, which I just don’t need (my tenants don’t change as frequently).</p>

<p>Handling other contexts when you need to “step into” a Tenant can be handled similarly, either using <code>connected_to</code> or using the fiber approach.</p>

<p>And, of course, the “database per tenant” workflow is just starting and it’s only in the recent years, with product from the ONCE family specifically, where SQLite3 began to shine again - as an engine of “small data, in big numbers”.</p>

<p>May we live to see this pattern come into the spotlight, finally.</p>


  </article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ZFS: Apple's New Filesystem that wasn't (2016) (112 pts)]]></title>
            <link>https://ahl.dtrace.org/2016/06/15/apple_and_zfs/</link>
            <guid>43810566</guid>
            <pubDate>Sun, 27 Apr 2025 09:25:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ahl.dtrace.org/2016/06/15/apple_and_zfs/">https://ahl.dtrace.org/2016/06/15/apple_and_zfs/</a>, See on <a href="https://news.ycombinator.com/item?id=43810566">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="skip">
			<heading-anchors>
				


<ul>
	<li><time datetime="2016-06-15">15 June 2016</time></li>
	<li><a href="https://ahl.dtrace.org/tags/apple/">apple</a>, </li>
	<li><a href="https://ahl.dtrace.org/tags/mac-os-x/">mac-os-x</a></li>
</ul>

<h4 id="prologue-2006">Prologue (2006)</h4>
<picture><source type="image/avif" srcset="https://ahl.dtrace.org/2016/06/15/apple_and_zfs/6l51fI_CaR-200.avif 200w"><source type="image/webp" srcset="https://ahl.dtrace.org/2016/06/15/apple_and_zfs/6l51fI_CaR-200.webp 200w"><img loading="lazy" decoding="async" src="https://ahl.dtrace.org/2016/06/15/apple_and_zfs/6l51fI_CaR-200.jpeg" alt="Description" width="200" height="150"></picture>
<p>I attended my first WWDC in 2006 to participate in Apple's&nbsp;<a href="http://dtrace.org/blogs/ahl/2006/08/07/dtrace_on_mac_os_x/">launch of their DTrace port</a> to the next version of Mac OS X (Leopard). Apple completed all but the fiddliest finishing touches without help from the DTrace team. Even when they did meet with us we had no idea that they were mere weeks away from the finished product being announced to the world. It was a testament both to Apple’s engineering acumen as well as their storied secrecy.</p>
<p>At that same WWDC Apple announced Time Machine, a product that would record file system versions through time for backup and recovery. How were they doing this? We were energized by the idea that there might be another piece of adopted Solaris technology. When we launched Solaris 10, DTrace shared the marquee with ZFS, a new filesystem that was to become the standard against which other filesystems are compared. Key among the many features of ZFS were snapshots that made it simple to capture the state of a filesystem, send the changes around, recover data, etc. Time Machine looked for all the world like a GUI on ZFS (indeed the GUI that we had imagined but knew to be well beyond the capabilities of Sun).</p>
<p>Of course Time Machine had nothing to do with ZFS. After the keynote we rushed to an Apple engineer we knew. With shame in his voice he admitted that it was really just a bunch of <a href="http://arstechnica.com/staff/2006/08/4995/">hard links to directories</a>. For those who don’t know a symlink from a symtab this is the moral equivalent of using newspaper as insulation: it’s fine until the completely anticipated calamity destroys everything you hold dear.</p>
<p>So there was no ZFS in Mac OS X, at least not yet.</p>
<h4 id="not-so-fast-2007">Not So Fast (2007)</h4>
<picture><source type="image/avif" srcset="https://ahl.dtrace.org/2016/06/15/apple_and_zfs/_Pcr91YZCB-200.avif 200w"><source type="image/webp" srcset="https://ahl.dtrace.org/2016/06/15/apple_and_zfs/_Pcr91YZCB-200.webp 200w"><img loading="lazy" decoding="async" src="https://ahl.dtrace.org/2016/06/15/apple_and_zfs/_Pcr91YZCB-200.jpeg" alt="Description" width="200" height="134"></picture>
<p>A few weeks before WWDC 2007 nerds like me started to lose their minds: Apple really <strong>was</strong> going to port ZFS to Mac OS X. It was actually going to happen! Beyond the snapshots that would make backing up a cinch, ZFS would dramatically advance the state of data storage for Apple users. HFS was introduced in System 2.1 (“System” being what we called “Mac OS” in the days before operating systems gained their broad and ubiquitous sex appeal). HFS improved upon the Macintosh File System by adding—wait for it—hierarchy! No longer would files accumulate in a single pile; you could organize them in folders. Not that there were many to organize on those 400K floppies, but progress is progress. And that filesystem has limped along for more than 30 years, nudged forward, rewritten to avoid in-kernel Pascal code (though retaining Pascal-style, length-prefixed strings), but never reimagined or reinvented. Even in its most modern form, HFS lacks the most basic functionality around data integrity. Bugs, power failures, and expected and inevitable media failures all mean that data is silently altered. Pray that your old photos are still intact. When’s the last time you backed up your Mac? I’m backing up right now just like I do every time I think about the neglectful stewardship of HFS.</p>
<p>ZFS was to bring to Mac OS X data integrity, compression, checksums, redundancy, snapshots, etc, etc etc. But while energizing Mac/ZFS fans, Sun CEO, Jonathan Schwartz, had clumsily disrupted the momentum that ZFS had been gathering in Apple’s walled garden. Apple <strong>had</strong> been working on a port of ZFS to Mac OS X. They <strong>were</strong>&nbsp;planning on mentioning it at the upcoming WWDC. Jonathan, brought into the loop either out of courtesy or legal necessity, violated the cardinal rule of the Steve Jobs-era Apple. Only one person at Steve Job’s company announces new products: Steve Jobs. <a href="http://www.theregister.co.uk/2007/06/07/apple_using_zfs_in_leopard/">"In fact, this week you'll see that Apple is announcing at their Worldwide Developer Conference that ZFS has become the file system in Mac OS 10,”</a> mused Jonathan at a press event, apparently to bolster Sun’s own credibility.</p>
<p>Less than a week later, Apple spoke about ZFS only when it became clear that a port was indeed present in a developer version of Leopard albeit in a nascent form. <a href="http://www.informationweek.com/apple-clarifies-status-of-zfs-file-system-in-mac-os/d/d-id/1056096?">Yes, ZFS would be there, sort of, but it would be read-only and no one should get their hopes up.</a></p>
<h4 id="ray-of-hope-2008">Ray of Hope (2008)</h4>
<picture><source type="image/avif" srcset="https://ahl.dtrace.org/2016/06/15/apple_and_zfs/gjVx4g4uTA-200.avif 200w"><source type="image/webp" srcset="https://ahl.dtrace.org/2016/06/15/apple_and_zfs/gjVx4g4uTA-200.webp 200w"><img loading="lazy" decoding="async" src="https://ahl.dtrace.org/2016/06/15/apple_and_zfs/gjVx4g4uTA-200.jpeg" alt="Description" width="200" height="135"></picture>
<p>By the next WWDC it seemed that <a href="http://www.zdnet.com/article/apple-announces-zfs-on-snow-leopard/">Sun had been forgiven</a>. ZFS was featured in the keynotes, it was on the developer disc handed out to attendees, and it was even mentioned on the <a href="http://web.archive.org/web/20080721031014/http://www.apple.com/server/macosx/snowleopard/">Mac OS X Server website</a>. Apple had been working on their port since 2006 and <a href="http://appleinsider.com/articles/08/06/23/five_undisclosed_features_of_apples_mac_os_x_snow_leopard">now it was functional enough to be put on full display</a>. I took it for a spin myself; it was really real. The feature that everyone wanted (but most couldn’t say why) was coming!</p>
<h4 id="the-little-engine-that-couldnt-2009">The Little Engine That Couldn't (2009)</h4>
<p>By the time Snow Leopard shipped only a careful examination of the Apple web site would turn up the <a href="https://web.archive.org/web/20090627034320/http://www.apple.com/xserve/specs.html">odd reference to ZFS left unscrubbed</a>. Whatever momentum ZFS had enjoyed within the Mac OS X product team was gone. I’ve heard a couple of theories and anecdotes from people familiar with the situation; first some relevant background.</p>
<picture><source type="image/avif" srcset="https://ahl.dtrace.org/2016/06/15/apple_and_zfs/FUXSn_It80-200.avif 200w"><source type="image/webp" srcset="https://ahl.dtrace.org/2016/06/15/apple_and_zfs/FUXSn_It80-200.webp 200w"><img loading="lazy" decoding="async" src="https://ahl.dtrace.org/2016/06/15/apple_and_zfs/FUXSn_It80-200.png" alt="Description" width="200" height="125"></picture>
<p><a href="https://en.wikipedia.org/wiki/Sun_acquisition_by_Oracle">Sun was dying.</a> After failed love affairs with IBM and HP (the latter formed, according to former Sun CEO, Scott McNealy, by two garbage trucks colliding), Oracle scooped up the aging dame with dim prospects. The nearly yearlong process of closing the acquisition was particularly hard on Sun, creating uncertainty around its future and damaging its bottom line. Despite the well-documented personal friendship between Steve Jobs and Oracle CEO, Larry Ellison (more on this later), I’m sure this uncertainty had some impact on the decision to continue with ZFS.</p>
<p>In the meantime Sun and NetApp had been locked in a lawsuit over ZFS and other storage technologies since mid-2007. <a href="https://web.archive.org/web/20080625023043/http://blogs.sun.com/jonathan/entry/harvesting_from_a_troll">While Jonathan Schwartz had blogged about protecting Apple and its users</a> (as well as Sun customers of course), this likely lead to further uncertainly. On top of that, filesystem transitions are far from simple. When Apple included DTrace in Mac OS X a point in favor was that it could be yanked out should any sort of legal issue arise. Once user data hit ZFS it would take years to fully reverse the decision. While the NetApp lawsuit never seemed to have merit (ZFS uses unique and from-scratch mechanisms for snapshots), it indisputably represented risk for Apple.</p>
<p>Finally, and perhaps most significantly, personal egos and NIH (not invented here) syndrome certainly played a part. I’m told by folks in Apple at the time that certain leads and managers preferred to build their own rather adopting external technology—even technology that was best of breed. They pitched their own project, an Apple project, that would bring modern filesystem technologies to Mac OS X. The design center for ZFS was servers, not laptops—and certainly not phones, tablets, and watches—his argument was likely that it would be better to start from scratch than adapt ZFS. Combined with the uncertainty above and, I’m told, no shortage of political savvy their arguments carried the day. Licensing FUD was thrown into the mix; even today folks at Apple see the ZFS license as nefarious and toxic in some way whereas the DTrace license works just fine for them. Note that both use the same license with the same grants and same restrictions. Maybe the technical arguments really were overwhelming (note however that ZFS was working internally on the iPhone), and maybe the risks really were insurmountable. I obviously have my own opinions, and think this was a great missed opportunity for the industry, but I never had the burden of weighing the totality of the facts and deciding. Nevertheless Apple put an end to its ZFS work; Apple’s from-scratch filesystem efforts were underway.</p>
<h4 id="the-little-engine-that-still-couldn-t-2010">The Little Engine That Still Couldn’t (2010)</h4>
<p>Amazingly that wasn’t quite the end for ZFS at Apple. <a href="https://www.linkedin.com/in/donjbrady/">The architect for ZFS at Apple had left</a>, the project had been shelved, but there were high-level conversations between Sun and Apple about reviving the port. Apple would get indemnification and support for their use of ZFS. Sun would get access to the Apple File Protocol (AFP—which, ironically, seems to have been collateral damage with the new APFS), and, more critically, Sun’s new ZFS-based storage appliance (which I helped develop) would be a natural server and backup agent for millions of Apple devices. It seemed to make some sort of sense.</p>
<p>The excruciatingly debilitatingly slow acquisition of Sun finally closed. The Apple-ZFS deal was brought for Larry Ellison’s approval, the first born child of the conquered land brought to be blessed by the new king. “I’ll tell you about doing business with my best friend Steve Jobs,” he apparently said, “I don’t do business with my best friend Steve Jobs.”</p>
<p>(Amusingly the version of the story told quietly at WWDC 2016 had the friends reversed with Steve saying that he wouldn’t do business with Larry. Still another version I’ve heard calls into question the veracity of their purported friendship, and has Steve instead suggesting that Larry go f*ck himself. Normally the iconoclast, that would, if true, represent Steve’s most mainstream opinion.)</p>
<p>And that was the end.</p>
<h4 id="epilogue-2016">Epilogue (2016)</h4>
<p>In the 7 years since ZFS development halted at Apple, they’ve worked on a variety of improvements in HFS and Core Storage, and hacked at at least two replacements for HFS that didn’t make it out the door. This week Apple announced their new filesystem, APFS, after 2 years in development. It’s not done; some features are still in development, and they’ve announced the ambitious goal of rolling it out to laptop, phone, watch, and tv within the next 18 months. At Sun we started ZFS in 2001. It shipped in 2005 and that was really the starting line, not the finish line. Since then I've shipped the ZFS Storage Appliance in 2008 and Delphix in 2010 and each has required investment in ZFS / OpenZFS to make them ready for prime time. A broadly featured, highly functional filesystem takes a long time.</p>
<p>APFS has merits (more in my next post), but it will always disappoint me that Apple didn’t adopt ZFS irrespective of how and why that decision was made. Dedicated members of the OpenZFS community have built and maintain <a href="https://openzfsonosx.org/">a port</a>. It’s not quite the same as having Apple as a member of that community, embracing and extending ZFS rather than building their own incipient alternative.</p>

<ul><li>← Previous<br> <a href="https://ahl.dtrace.org/2016/05/13/shv/">Finding What's Next</a></li><li>Next →<br><a href="https://ahl.dtrace.org/2016/06/19/apfs-part1/">APFS in Detail: Overview</a></li>
</ul>

			</heading-anchors>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. autism data project sparks uproar over ethics, privacy and intent (160 pts)]]></title>
            <link>https://www.washingtonpost.com/health/2025/04/25/autism-registry-privacy-rfk-research/</link>
            <guid>43810561</guid>
            <pubDate>Sun, 27 Apr 2025 09:23:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/health/2025/04/25/autism-registry-privacy-rfk-research/">https://www.washingtonpost.com/health/2025/04/25/autism-registry-privacy-rfk-research/</a>, See on <a href="https://news.ycombinator.com/item?id=43810561">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/health/2025/04/25/autism-registry-privacy-rfk-research/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Chongqing, the Largest City – In Pictures (165 pts)]]></title>
            <link>https://www.theguardian.com/world/gallery/2025/apr/27/chongqing-the-worlds-largest-city-in-pictures</link>
            <guid>43809915</guid>
            <pubDate>Sun, 27 Apr 2025 06:42:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/gallery/2025/apr/27/chongqing-the-worlds-largest-city-in-pictures">https://www.theguardian.com/world/gallery/2025/apr/27/chongqing-the-worlds-largest-city-in-pictures</a>, See on <a href="https://news.ycombinator.com/item?id=43809915">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-link-name="standfirst" data-component="standfirst">
    

        
            <meta itemprop="description" content="The largest city in the world is as big as Austria, but few people have ever heard of it. The megacity of 34 million people in central of China is the emblem of the fastest urban revolution on the planet. The Communist party decided 30 years ago to unify and populate vast rural areas, an experiment that has become a symbol of the Chinese ability to reshape the world">
        
    
    
        
            <p>The largest city in the world is as big as Austria, but few people have ever heard of it. The megacity of 34 million people in central of China is the emblem of the fastest urban revolution on the planet. The Communist party decided 30 years ago to unify and populate vast rural areas, an experiment that has become a symbol of the Chinese ability to reshape the world</p>
        
    
    
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Remote-Controlled IKEA Deathstar Lamp (213 pts)]]></title>
            <link>https://gitlab.com/sephalon/deathstar_lamp</link>
            <guid>43809841</guid>
            <pubDate>Sun, 27 Apr 2025 06:25:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gitlab.com/sephalon/deathstar_lamp">https://gitlab.com/sephalon/deathstar_lamp</a>, See on <a href="https://news.ycombinator.com/item?id=43809841">Hacker News</a></p>
<div id="readability-page-1" class="page">





<header data-testid="navbar">
<a href="#content-body">Skip to content</a>
<div>
<nav aria-label="Explore GitLab">
<div>
<span>GitLab</span>
<a title="Homepage" id="logo" aria-label="Homepage" data-track-label="main_navigation" data-track-action="click_gitlab_logo_link" data-track-property="navigation_top" href="https://gitlab.com/"><svg aria-hidden="true" role="img" width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="m24.507 9.5-.034-.09L21.082.562a.896.896 0 0 0-1.694.091l-2.29 7.01H7.825L5.535.653a.898.898 0 0 0-1.694-.09L.451 9.411.416 9.5a6.297 6.297 0 0 0 2.09 7.278l.012.01.03.022 5.16 3.867 2.56 1.935 1.554 1.176a1.051 1.051 0 0 0 1.268 0l1.555-1.176 2.56-1.935 5.197-3.89.014-.01A6.297 6.297 0 0 0 24.507 9.5Z" fill="#E24329"></path>
  <path d="m24.507 9.5-.034-.09a11.44 11.44 0 0 0-4.56 2.051l-7.447 5.632 4.742 3.584 5.197-3.89.014-.01A6.297 6.297 0 0 0 24.507 9.5Z" fill="#FC6D26"></path>
  <path d="m7.707 20.677 2.56 1.935 1.555 1.176a1.051 1.051 0 0 0 1.268 0l1.555-1.176 2.56-1.935-4.743-3.584-4.755 3.584Z" fill="#FCA326"></path>
  <path d="M5.01 11.461a11.43 11.43 0 0 0-4.56-2.05L.416 9.5a6.297 6.297 0 0 0 2.09 7.278l.012.01.03.022 5.16 3.867 4.745-3.584-7.444-5.632Z" fill="#FC6D26"></path>
</svg>

</a></div>
<ul>
<li>

<div>
<ul>
<li>
<a href="https://about.gitlab.com/why-gitlab">Why GitLab
</a></li>
<li>
<a href="https://about.gitlab.com/pricing">Pricing
</a></li>
<li>
<a href="https://about.gitlab.com/sales">Contact Sales
</a></li>
<li>
<a href="https://gitlab.com/explore">Explore</a>
</li>
</ul>
</div>
</li>
<li>
<a href="https://about.gitlab.com/why-gitlab">Why GitLab
</a></li>
<li>
<a href="https://about.gitlab.com/pricing">Pricing
</a></li>
<li>
<a href="https://about.gitlab.com/sales">Contact Sales
</a></li>
<li>
<a href="https://gitlab.com/explore">Explore</a>
</li>
</ul>
<ul>
<li>
<a href="https://gitlab.com/users/sign_in?redirect_to_referer=yes">Sign in</a>
</li>
<li>
<a href="https://gitlab.com/users/sign_up"><span>
Get free trial

</span>

</a></li>
</ul>
</nav>
</div>
</header>

<div>


<div data-testid="top-bar">
<div data-testid="breadcrumb-links" id="js-vue-page-breadcrumbs-wrapper">


</div>
<div>





</div>
</div>

<div>
<main id="content-body" itemscope="" itemtype="http://schema.org/SoftwareSourceCode">











<header>
<div>
<div>
<div alt="deathstar_lamp" itemprop="image">
D
</div>

<h2 data-testid="project-name-content" itemprop="name">
deathstar_lamp


</h2>
</div>

</div>

</header>


<div>

<div data-blame-per-page="1000" id="tree-holder">

<div role="status" data-history-link="/sephalon/deathstar_lamp/-/commits/master" data-ref-type="heads" id="js-last-commit"><span aria-hidden=""></span><span>Loading</span>
</div>

</div>
</div>

</main>
</div>


</div>








</div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to program a text adventure in C (175 pts)]]></title>
            <link>https://helderman.github.io/htpataic/htpataic01.html</link>
            <guid>43809638</guid>
            <pubDate>Sun, 27 Apr 2025 05:25:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://helderman.github.io/htpataic/htpataic01.html">https://helderman.github.io/htpataic/htpataic01.html</a>, See on <a href="https://news.ycombinator.com/item?id=43809638">Hacker News</a></p>
<div id="readability-page-1" class="page">


<h2>How to program a text adventure in C</h2>
<p>
by Ruud Helderman
&lt;<a href="mailto:r.helderman@hccnet.nl">r.helderman@hccnet.nl</a>&gt;
</p>
<p>
Licensed under
<a href="https://github.com/helderman/htpataic/blob/master/LICENSE">MIT License</a>
</p>

<p>
This is not a C tutorial; there are plenty of those on the web.
The reader is assumed to have some basic knowledge of programming in general,
and preferrably the C programming language in particular.
Maybe you just finished reading a tutorial somewhere,
and you would like to learn some more about programming
by studying other people’s source code.
Or could it be that you really are genuinely interested in
the ancient art of writing text adventure games from scratch?
Maybe you are just having a sense of nostalgia about the old days,
when life was simple, and so was software.
In either case, you have come to the right place.
</p>
<p>
In the 1980s,
<a href="http://en.wikipedia.org/wiki/Text_adventure">text adventures</a>
were a respectable genre of computer games.
But times have changed:
in the 21st century, they pale in comparison with modern
<a href="http://en.wikipedia.org/wiki/Mmorpg">MMORPGs</a> with 3D engines.
Where books survived the rise of film,
text-based games quickly lost the battle against their graphical counterparts.
‘Interactive fiction’ is kept alive by an active community,
but its commercial value is long gone.
</p>
<p>
There is a bright side to this:
authoring tools of professional quality are now available free of charge.
This is also the single biggest advice I can give you
if you intend to write your own adventure:
use one of the many dedicated
<a href="http://en.wikipedia.org/wiki/Text_adventure#Development_systems">development systems</a>.
</p>
<h3>Why C</h3>
<p>
But then, why this tutorial?
Why write an adventure game using a
<a href="http://en.wikipedia.org/wiki/General-purpose_programming_language">general-purpose programming language</a>?
Because doing so can be entertaining, challenging and educational.
The programming language
<a href="http://en.wikipedia.org/wiki/C_(programming_language)">C</a>
may not be the most obvious choice for writing a text adventure;
it’s nothing like LISP-based languages like
<a href="http://en.wikipedia.org/wiki/MDL_programming_language">MDL</a> and
<a href="http://en.wikipedia.org/wiki/Zork_Implementation_Language">ZIL</a>.
Some people will claim C is not even a general-purpose language.
I disagree; maybe it <i>should</i> not have become a general-purpose language,
but it most certainly did.
</p>
<p>
Yes, there are general-purpose programming languages
that are better than C in terms of maintainability and reliability.
The ideas proposed here apply equally well to those languages.
If you prefer
<a href="http://en.wikipedia.org/wiki/Java_(programming_language)">Java</a>,
<a href="https://en.wikipedia.org/wiki/C_Sharp_(programming_language)">C#</a>
or
<a href="http://en.wikipedia.org/wiki/Python_(programming_language)">Python</a>
over C, it should not be too difficult to translate my code samples.
</p>
<p>
My choice for C is a personal one.
I know the language well, and I like it a lot.
But more importantly, C is close to the metal.
I have always admired pioneers like
<a href="https://en.wikipedia.org/wiki/Scott_Adams_(game_designer)">Scott Adams</a>
who managed to bring the adventure game genre to the early
<a href="https://en.wikipedia.org/wiki/Home_computer">home computers</a>,
despite extreme memory constraints.
The best of these attempts were written in
<a href="https://en.wikipedia.org/wiki/Assembly_language">assembly language</a>,
so I was curious how far I could get in a language that was originally
designed as a portable alternative for assembly.
This may feel like cheating: most
<a href="https://en.wikipedia.org/wiki/8-bit_computing">8-bit</a>
computers did not have an (optimizing) C
<a href="http://en.wikipedia.org/wiki/Compiler">compiler</a>.
But assuming we will not be relying on advanced
<a href="https://en.wikipedia.org/wiki/Library_(computing)">libraries</a>,
C maps pretty well to assembly, even if you have to do so by hand.
</p>
<h3>Incremental development</h3>
<p>
I will try to keep this story readable for a broad audience,
without too much off-topic elaboration.
For terms that might warrant further explanation,
please follow the hyperlinks to relevant
<a href="http://en.wikipedia.org/wiki/Wikipedia">Wikipedia</a> articles.
</p>
<p>
Throughout this tutorial,
we will be developing a fully functional text adventure game.
This will be done in an
<a href="http://en.wikipedia.org/wiki/Incremental_build_model">incremental</a>
fashion.
With every chapter, some code is added to our program.
Each increment, however small, adds some value to the game.
And every time, the result is a working program,
ready to run once built with a proper C
<a href="http://en.wikipedia.org/wiki/Compiler">compiler</a>.
</p>
<p>
We start off with just 9 lines of code;
it is little more than a trivial
<a href="http://en.wikipedia.org/wiki/Hello_world_program">“Hello World” program</a>.
If you can’t even get this simple example to work,
you should find some help with that first,
before you are ready to move on to the next chapter.
</p>
<table>
<tbody><tr><th>Sample output</th></tr>
<tr><td>
Welcome to Little Cave Adventure.
<br>
It is very dark in here.
<p>


Bye!
</p></td></tr>
</tbody></table>
<table><tbody><tr>
<th>main.c</th>
</tr><tr>
<td>
<ol>
<li>#include &lt;stdio.h&gt;</li>
<li></li>
<li>int main()</li>
<li>{</li>
<li>   printf("Welcome to Little Cave Adventure.\n");</li>
<li>   printf("It is very dark in here.\n");</li>
<li>   printf("\nBye!\n");</li>
<li>   return 0;</li>
<li>}</li>
</ol>
</td>
</tr></tbody></table>
<div>
<p>
Some explanation for those not quite familiar with C:
</p>
<ul>
<li>Line 1:
a text adventure needs no fancy libraries; the
<a href="https://en.wikipedia.org/wiki/C_standard_library">C standard library</a>
is sufficient, and widely available.
But feel free to replace it with any
<a href="http://en.wikipedia.org/wiki/Category:User_interface">user interface</a>
library you like, for example
<a href="http://en.wikipedia.org/wiki/Glk_(software)">Glk</a>.
</li>
<li>Line 3:
function
<a href="http://en.wikipedia.org/wiki/Main_function_(programming)">main</a>
is the starting point of a typical C program.
</li>
<li>Line 5-7:
output text to the screen; the escape sequence <tt>\n</tt> represents a
<a href="http://en.wikipedia.org/wiki/Newline">newline</a>.
</li>
<li>Line 8:
function <i>main</i> returns a zero
<a href="http://en.wikipedia.org/wiki/Exit_code">exit code</a>
to indicate the program completed successfully.
</li>
</ul>
</div>
<p>
Trivial as it may be, the program does demonstrate
<i>the</i> most important aspect of any text adventure:
descriptive text.
A good story is one of the things that makes a good adventure game.
I deliberately kept it brief in my code sample.
I will focus on the programming aspect of the game,
and leave the rest up to your imagination.
</p>
<hr>

<p>
Next chapter: <a href="https://helderman.github.io/htpataic/htpataic02.html">2. The main loop</a>
</p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[CSS Zen Garden (277 pts)]]></title>
            <link>https://csszengarden.com/</link>
            <guid>43809484</guid>
            <pubDate>Sun, 27 Apr 2025 04:44:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://csszengarden.com/">https://csszengarden.com/</a>, See on <a href="https://news.ycombinator.com/item?id=43809484">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="zen-supporting" role="main">
		<div id="zen-explanation" role="article">
			<h3>So What is This About?</h3>
			<p>There is a continuing need to show the power of <abbr title="Cascading Style Sheets">CSS</abbr>. The Zen Garden aims to excite, inspire, and encourage participation. To begin, view some of the existing designs in the list. Clicking on any one will load the style sheet into this very page. The <abbr title="HyperText Markup Language">HTML</abbr> remains the same, the only thing that has changed is the external <abbr title="Cascading Style Sheets">CSS</abbr> file. Yes, really.</p>
			<p><abbr title="Cascading Style Sheets">CSS</abbr> allows complete and total control over the style of a hypertext document. The only way this can be illustrated in a way that gets people excited is by demonstrating what it can truly be, once the reins are placed in the hands of those able to create beauty from structure. Designers and coders alike have contributed to the beauty of the web; we can always push it further.</p>
		</div>

		<div id="zen-participation" role="article">
			<h3>Participation</h3>
			<p>Strong visual design has always been our focus. You are modifying this page, so strong <abbr title="Cascading Style Sheets">CSS</abbr> skills are necessary too, but the example files are commented well enough that even <abbr title="Cascading Style Sheets">CSS</abbr> novices can use them as starting points. Please see the <a href="https://csszengarden.com/pages/resources/" title="A listing of CSS-related resources"><abbr title="Cascading Style Sheets">CSS</abbr> Resource Guide</a> for advanced tutorials and tips on working with <abbr title="Cascading Style Sheets">CSS</abbr>.</p>
			<p>You may modify the style sheet in any way you wish, but not the <abbr title="HyperText Markup Language">HTML</abbr>. This may seem daunting at first if you’ve never worked this way before, but follow the listed links to learn more, and use the sample files as a guide.</p>
			<p>Download the sample <a href="https://csszengarden.com/examples/index" title="This page's source HTML code, not to be modified.">HTML</a> and <a href="https://csszengarden.com/examples/style.css" title="This page's sample CSS, the file you may modify.">CSS</a> to work on a copy locally. Once you have completed your masterpiece (and please, don’t submit half-finished work) upload your <abbr title="Cascading Style Sheets">CSS</abbr> file to a web server under your control. <a href="https://csszengarden.com/pages/submit/" title="Use the contact form to send us your CSS file">Send us a link</a> to an archive of that file and all associated assets, and if we choose to use it we will download it and place it on our server.</p>
		</div>

		<div id="zen-benefits" role="article">
			<h3>Benefits</h3>
			<p>Why participate? For recognition, inspiration, and a resource we can all refer to showing people how amazing <abbr title="Cascading Style Sheets">CSS</abbr> really can be. This site serves as equal parts inspiration for those working on the web today, learning tool for those who will be tomorrow, and gallery of future techniques we can all look forward to.</p>
		</div>

		<div id="zen-requirements" role="article">
			<h3>Requirements</h3>
			<p>Where possible, we would like to see mostly <abbr title="Cascading Style Sheets, levels 1 and 2">CSS 1 &amp; 2</abbr> usage. <abbr title="Cascading Style Sheets, levels 3 and 4">CSS 3 &amp; 4</abbr> should be limited to widely-supported elements only, or strong fallbacks should be provided. The CSS Zen Garden is about functional, practical <abbr title="Cascading Style Sheets">CSS</abbr> and not the latest bleeding-edge tricks viewable by 2% of the browsing public. The only real requirement we have is that your <abbr title="Cascading Style Sheets">CSS</abbr> validates.</p>
			<p>Luckily, designing this way shows how well various browsers have implemented <abbr title="Cascading Style Sheets">CSS</abbr> by now. When sticking to the guidelines you should see fairly consistent results across most modern browsers. Due to the sheer number of user agents on the web these days — especially when you factor in mobile — pixel-perfect layouts may not be possible across every platform. That’s okay, but do test in as many as you can. Your design should work in at least IE9+ and the latest Chrome, Firefox, iOS and Android browsers (run by over 90% of the population).</p>
			<p>We ask that you submit original artwork. Please respect copyright laws. Please keep objectionable material to a minimum, and try to incorporate unique and interesting visual themes to your work. We’re well past the point of needing another garden-related design.</p>
			<p>This is a learning exercise as well as a demonstration. You retain full copyright on your graphics (with limited exceptions, see <a href="https://csszengarden.com/pages/submit/guidelines/">submission guidelines</a>), but we ask you release your <abbr title="Cascading Style Sheets">CSS</abbr> under a Creative Commons license identical to the <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/" title="View the Zen Garden's license information.">one on this site</a> so that others may learn from your work.</p>
			<p role="contentinfo">By <a href="http://www.mezzoblue.com/">Dave Shea</a>. Bandwidth graciously donated by <a href="http://www.mediatemple.net/">mediatemple</a>. Now available: <a href="http://www.amazon.com/exec/obidos/ASIN/0321303474/mezzoblue-20/">Zen Garden, the book</a>.</p>
		</div>

		

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open-source interactive C tutorial in the browser (205 pts)]]></title>
            <link>https://www.learn-c.org/</link>
            <guid>43809092</guid>
            <pubDate>Sun, 27 Apr 2025 02:52:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.learn-c.org/">https://www.learn-c.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43809092">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="inner-text">
                
                <h2>Welcome</h2>
<p>Welcome to the learn-c.org free interactive C tutorial.</p>
<p>Whether you are an experienced programmer or not, this website is intended for everyone who wishes to learn the C programming language.</p>
<p>There is no need to download anything - Just click on the chapter you wish to begin from, and follow the instructions. Good luck!</p>
<p>learn-c.org is still under construction - If you wish to contribute tutorials, please click on <code>Contributing Tutorials</code> down below.</p>
<h3>Learn the Basics</h3>
<ul>
<li><a href="https://www.learn-c.org/en/Hello%2C_World%21">Hello, World!</a></li>
<li><a href="https://www.learn-c.org/en/Variables_and_Types">Variables and Types</a></li>
<li><a href="https://www.learn-c.org/en/Arrays">Arrays</a></li>
<li><a href="https://www.learn-c.org/en/Multidimensional_Arrays">Multidimensional Arrays</a></li>
<li><a href="https://www.learn-c.org/en/Conditions">Conditions</a></li>
<li><a href="https://www.learn-c.org/en/Strings">Strings</a></li>
<li><a href="https://www.learn-c.org/en/For_loops">For loops</a></li>
<li><a href="https://www.learn-c.org/en/While_loops">While loops</a></li>
<li><a href="https://www.learn-c.org/en/Functions">Functions</a></li>
<li><a href="https://www.learn-c.org/en/Static">Static</a></li>
</ul>
<h3>Advanced</h3>
<ul>
<li><a href="https://www.learn-c.org/en/Pointers">Pointers</a></li>
<li><a href="https://www.learn-c.org/en/Structures">Structures</a></li>
<li><a href="https://www.learn-c.org/en/Function_arguments_by_reference">Function arguments by reference</a></li>
<li><a href="https://www.learn-c.org/en/Dynamic_allocation">Dynamic allocation</a></li>
<li><a href="https://www.learn-c.org/en/Arrays_and_Pointers">Arrays and Pointers</a></li>
<li><a href="https://www.learn-c.org/en/Recursion">Recursion</a></li>
<li><a href="https://www.learn-c.org/en/Linked_lists">Linked lists</a></li>
<li><a href="https://www.learn-c.org/en/Binary_trees">Binary trees</a></li>
<li><a href="https://www.learn-c.org/en/Unions">Unions</a></li>
<li><a href="https://www.learn-c.org/en/Pointer_Arithmetics">Pointer Arithmetics</a></li>
<li><a href="https://www.learn-c.org/en/Function_Pointers">Function Pointers</a></li>
<li><a href="https://www.learn-c.org/en/Bitmasks">Bitmasks</a></li>
</ul>
<h3>Contributing Tutorials</h3>
<p>Read more here: <a href="https://www.learn-c.org/en/Contributing_Tutorials">Contributing Tutorials</a></p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Found a simple tool for database modeling: dbdiagram.io (182 pts)]]></title>
            <link>https://dbdiagram.io</link>
            <guid>43808803</guid>
            <pubDate>Sun, 27 Apr 2025 01:40:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dbdiagram.io">https://dbdiagram.io</a>, See on <a href="https://news.ycombinator.com/item?id=43808803">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Sigbovik Conference Proceedings 2025 [pdf] (149 pts)]]></title>
            <link>https://sigbovik.org/2025/proceedings.pdf</link>
            <guid>43808454</guid>
            <pubDate>Sun, 27 Apr 2025 00:32:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sigbovik.org/2025/proceedings.pdf">https://sigbovik.org/2025/proceedings.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=43808454">Hacker News</a></p>
Couldn't get https://sigbovik.org/2025/proceedings.pdf: Error: getaddrinfo EBUSY sigbovik.org]]></description>
        </item>
        <item>
            <title><![CDATA[IcôNES (232 pts)]]></title>
            <link>https://icones.js.org/</link>
            <guid>43808443</guid>
            <pubDate>Sun, 27 Apr 2025 00:30:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://icones.js.org/">https://icones.js.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43808443">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon Just Happens to Hold Book Sale During Independent Bookstore Day (244 pts)]]></title>
            <link>https://gizmodo.com/amazon-just-happens-to-hold-book-sale-during-independent-bookstore-day-2000594958</link>
            <guid>43808334</guid>
            <pubDate>Sun, 27 Apr 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/amazon-just-happens-to-hold-book-sale-during-independent-bookstore-day-2000594958">https://gizmodo.com/amazon-just-happens-to-hold-book-sale-during-independent-bookstore-day-2000594958</a>, See on <a href="https://news.ycombinator.com/item?id=43808334">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              
              
              <p>April 26 is Independent Bookstore Day, an annual tradition organized by the American Booksellers Association that is entering its 12th year and has taken place on the final Saturday of April since its conception. And, wouldn’t you know it, Amazon just so happens to be <a href="https://www.aboutamazon.com/news/books-and-authors/best-amazon-book-sale-deals-2025">hosting</a> its second annual book sale (two being less than 12, if you’re keeping count) from April 23 through the 28th, encompassing the day that has been claimed by local shop across the country.</p> <p>If you are tempted to give Amazon a pass here for maybe accidentally encroaching upon their competitors’ space—and if you are, bless your sweet innocence—it’s perhaps worth noting that their <a href="https://www.wired.com/story/amazon-book-sale-spring-2024/">first big book sale event in 2024</a> took place from May 16 through the 20th, notably a few weeks after Independent Bookstore Day. So, to whatever extent something you’ve done for exactly one year can be called a tradition, the company has already broken that in favor of squeezing indie booksellers.</p> <p>(In a <a href="https://www.vulture.com/article/amazon-book-sale-on-independent-bookstore-day.html">statement to Variety</a>, Amazon said, “The overlap was unintentional. The dates for our sale were set this year to accommodate additional participating countries.” Take that with as many grains of salt as you deem necessary.)</p>

 <p>You could just say that Amazon is going back to its roots. After all, the company <a href="https://ilsr.org/articles/amazon-stranglehold/">got its start selling books at a loss</a> to undercut the prices of brick-and-mortar competitors, until it pushed them to the brink of bankruptcy. Then, as if to dance on their graves, it <a href="https://www.seattletimes.com/business/amazon/amazon-opens-first-bricks-and-mortar-bookstore-at-u-village/">started opening</a> up its own physical bookstore locations (which it eventually shuttered without much fanfare because it <a href="https://www.fastcompany.com/90430303/a-rediscovered-1997-video-reveals-why-jeff-bezos-chose-books-and-not-cds-to-be-amazons-first-product">never really cared about the product</a> itself). Now that indie booksellers have found a way to carve out a niche, the company looks like it’d like to get back in the business of undermining them.</p> <p>The indies, understandably, aren’t thrilled. More than 1,600 bookstores, including participants in all 50 states, will celebrate Independent Bookstore Day with their local community, but they’ve also taken a deserved moment to side-eye Amazon a bit. <a href="https://www.vulture.com/article/amazon-book-sale-on-independent-bookstore-day.html">Speaking to Variety</a>, booksellers took shots at the retail giant’s decision to hold its book sale at the same time as their annual event. Andy Hunter, founder of Bookshop.org, called it “cynical, manipulative, and cruel.” Leah Koch, owner of the Ripped Bodice, in Los Angeles, California, kept it simple:&nbsp; “Fuck Jeff Bezos,” she told Variety. “May he be very miserable living alone on the moon.”&nbsp; Ripped Bodice <a href="https://www.therippedbodice.com/shipping-information">does ship</a> domestically and internationally, by the way, in case you’re looking for a new book.</p> <p>In a way, Amazon’s sale is more petty posturing than something that will actually impact the indies. Local book shops have managed to have a moment, <a href="https://www.library.hbs.edu/working-knowledge/why-independent-bookstores-haved-thrived-in-spite-of-amazon-com">thriving</a> despite the increasing dominance of Amazon online by creating bonds with their local community and offering a shopping experience that Amazon can’t replicate. They’ve also <a href="https://www.investopedia.com/local-bookshops-outsmart-amazon-11718518">adapted to the online ecosystem</a> to stay competitive, while Amazon failed to plant roots with its own brick-and-mortar shops.</p>

 <p>Remember, you don’t have to wait for a special day to visit your local bookstore. Go any time, shop around, ask for recommendations. You might be shocked to find that you’ll get much more out of picking the brains of your local bookworms than you will from a recommendation algorithm.</p>
                          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anatomy of a SQL Engine (134 pts)]]></title>
            <link>https://www.dolthub.com/blog/2025-04-25-sql-engine-anatomy/</link>
            <guid>43807593</guid>
            <pubDate>Sat, 26 Apr 2025 22:00:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dolthub.com/blog/2025-04-25-sql-engine-anatomy/">https://www.dolthub.com/blog/2025-04-25-sql-engine-anatomy/</a>, See on <a href="https://news.ycombinator.com/item?id=43807593">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-cy="blog-post-text"><p>May marks five years since Dolt <a href="https://www.dolthub.com/blog/2020-05-04-adopting-go-mysql-server/">adopted go-mysql-server</a>. Today we summarize the current state of GMS by walking through a query's journey from parsing to result spooling.</p>
<h2 id="overview"><a href="#overview" aria-label="overview permalink"></a>Overview</h2>
<p>SQL engines are the logical layer of a database that sit between client
and storage. To mutate the database state on behalf of a client, a SQL
engine performs the following steps:</p>
<ol>
<li>
<p>Parsing</p>
</li>
<li>
<p>Binding</p>
</li>
<li>
<p>Plan Simplification</p>
</li>
<li>
<p>Join Exploration</p>
</li>
<li>
<p>Plan Costing</p>
</li>
<li>
<p>Execution</p>
</li>
<li>
<p>Spooling Results</p>
</li>
</ol>
<p>Most systems also have a variety of execution
strategies (row vs vector based) and infrastructure layers (run locally
vs distributed). At the moment Dolt's engine by default uses row-based
execution within the local server.</p>
<h2 id="parsing"><a href="#parsing" aria-label="parsing permalink"></a>Parsing</h2>
<p>The first thing a SQL Engine does when receiving a query is
try to form a structured abstract syntax tree (AST). The AST is a rough
cut of whether the query is well formed. The entrypoint of the parser is
<a href="https://github.com/dolthub/vitess/blob/d6bc702b989e43b33fbcb8a261ba9cd6a387917e/go/vt/sqlparser/ast.go#L141">here</a>.</p>
<p>A client driver initializes a query by passing bytes over the network through a server
listener into a handler command <code>ComHandler</code>. The handler accumulates
data until reaching a delimiter token (usually <code>;</code>). The string is then split
into space-delimited tokens and fed into parsing. The diagram below shows the movement from
bytes to tokens and finally AST nodes:</p>
<p><span>
      <a href="https://www.dolthub.com/blog/static/987690e109d9683057aee962328ebfa1/947d2/max-qparse.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="parse" title="" src="https://www.dolthub.com/blog/static/987690e109d9683057aee962328ebfa1/ad12c/max-qparse.png" srcset="https://www.dolthub.com/blog/static/987690e109d9683057aee962328ebfa1/a48b3/max-qparse.png 214w,
https://www.dolthub.com/blog/static/987690e109d9683057aee962328ebfa1/47730/max-qparse.png 428w,
https://www.dolthub.com/blog/static/987690e109d9683057aee962328ebfa1/ad12c/max-qparse.png 856w,
https://www.dolthub.com/blog/static/987690e109d9683057aee962328ebfa1/7a18f/max-qparse.png 1284w,
https://www.dolthub.com/blog/static/987690e109d9683057aee962328ebfa1/56caf/max-qparse.png 1712w,
https://www.dolthub.com/blog/static/987690e109d9683057aee962328ebfa1/947d2/max-qparse.png 1773w" sizes="(max-width: 856px) 100vw, 856px" loading="lazy" decoding="async">
  </a>
    </span></p>
<p>Right recursive parsing is easy to understand and debug because the program is a decision tree that chooses the next path based on the lookahead token. So a right-recursive parser might initially expect a SELECT, and then accumulate select expressions until a FROM token, and so on. This is "right recursive" because when we hit something like a join, we greedily accumulate table tokens and keep moving the parse state deeper. But right-recursive parsers are top-down, which unfortunately uses stack memory proportional to the number of tokens.</p>
<p>Left-recursive parsers are instead bottom-up and collapse the accumulated stack eagerly when they find a happy pattern. So when we see a join, we'll fold the join so far before checking for more tables. This keeps the memory usage low at the expense of way more complicated decision making criteria.</p>
<p>Dolt's <a href="https://www.dolthub.com/blog/2023-07-28-goyacc-parser-tips-tricks/">Yacc grammar</a> is left-recursive, which is fast to execute even though the shift (add token) reduce (collapse token stack) logic is hard to debug. Yacc lets us do some small formatting in stack collapse (reduce) hooks. The rules below show simplified details of what "left recursive folding" looks like from the engineer/semantic perspective:</p>
<div data-language="text"><pre><code>table_reference:
  table_factor
| join_table

table_factor:
  aliased_table_name
  {
    $$ = $1.(*AliasedTableExpr)
  }
| openb table_references closeb
  {
    $$ = &amp;ParenTableExpr{Exprs: $2.(TableExprs)}
  }
| table_function
| json_table

join_table:
  table_reference inner_join table_factor join_condition_opt
  {
    $$ = &amp;JoinTableExpr{LeftExpr: $1.(TableExpr), Join: $2.(string), RightExpr: $3.(TableExpr), Condition: $4.(JoinCondition)}
  }

join_condition_opt:
%prec JOIN
  { $$ = JoinCondition{} }
| join_condition
  { $$ = $1.(JoinCondition) }

join_condition:
  ON expression
  { $$ = JoinCondition{On: tryCastExpr($2)} }
| USING '(' column_list ')'
  { $$ = JoinCondition{Using: $3.(Columns)} }
</code></pre></div>
<p><code>table_reference</code> is the umbrella expression for the row source, either a tablescan or join. <code>join_table</code> is two or more tables concatenated with JOIN clauses, with the recursive <code>table_reference</code> portion on the left rather than the right. When the head of the stack matches <code>join_table</code>'s definition, we pop those stack components, execute the callback hook, and replace the components with a single <code>*JoinTableExpression</code> on the stack. The join tree itself is right-deep, because the old state will be the left node and new state the right node in recursive calls. The unabridged version of the is <a href="https://github.com/dolthub/vitess/blob/b8d80bc3934116b93274b5356f6fcd99ac74d51c/go/vt/sqlparser/sql.y#L8359">here</a>.</p>
<p>If parsing succeeds, the output AST is guarenteed to match the structure
of our Yacc rules. If parsing fails, the client receives an error
indicating which lookahead token in the query string was invalid.</p>
<h2 id="binding"><a href="#binding" aria-label="binding permalink"></a>Binding</h2>
<p>Query parsing only partially checks if a query is well formed. Fields in the AST still need to be matched to symbols in the current database catalog, in addition to a host of other typing and clause-specific checks. We call this phase "binding" because its core intermediate representation is designed for namespace scoping (<a href="https://github.com/dolthub/go-mysql-server/blob/263d9f2a66e50ca370f3645a4ba3a141668c7a0c/sql/planbuilder/parse.go#L91">code entrypoint is here</a>).</p>
<p>Binding AST identifiers to catalog symbols is similar to defining and referencing variables in any programming language. Tables and aliases create column variables that column fields reference. There are ~four core objects that we can think about from a row source/sink perspective.</p>
<p>Table definitions are sources that provide many rows/columns. Table names cannot clash within the same scope (ex: <code>select * from mydb.mytable.xy join xy on x</code>), but are otherwise globally accessible. Subqueries are a special expression that add an extra namespacing layer, but otherwise act like table source: <code>select sq.a from (select x as a from xy) sq</code>.</p>
<p>Column definitions are sinks that reference a specific column from a row source. A reference in a scope with two otherwise ambiguous column names has to qualify the table to disambiguate its identity. For example, <code>select * from xy a join xy b where x = 2</code> errors because the column reference in the <code>x = 2</code> filter lacks a table qualifier.</p>
<p>Aliases are scalar sinks and sources, which is somewhat unusual. For each row in a source they output a single value. This means we add column definitions available for referencing, but only at the boundary of the original scope. For example, <code>select x+y as a from xy where a &gt; 2</code> errors because the <code>a</code> definition is defined and created after the filter/projection. The same query with <code>having a &gt; 2</code> is valid because HAVING has access to the second scope with the <code>a</code> alias.</p>
<p>Scalar subqueries are sources and sinks. They are similar to aliases, but with the added complexity that the subquery scopes all share the parent scope. Multiple scopes add a hierarchy to name binding, but are otherwise intuitive: search the current scope for a name match, and then iterate upwards until finding a scope with an appropriate definition. Common names cannot clash between scopes, only within. <code>select (select a.x from xy b) from xy a</code> is valid because even though the <code>a.x</code> variable is not found within the <code>b</code> scope, the <code>a</code> parent scope provides a binding.</p>
<p>Common Table Expressions (CTEs) and subquery aliases are simple extensions of the same namespacing rules. Aggregations have fairly <a href="https://dev.mysql.com/doc/refman/8.4/en/group-by-handling.html">specific rules</a> about what combinations of GROUP BY and selection columns are valid, which we will not discuss here.</p>
<p>The <code>binding</code> phase converts AST nodes into customized
<a href="https://github.com/dolthub/go-mysql-server">go-mysql-server</a> <code>sql.Node</code>
plan nodes as outputs.</p>
<h2 id="plan-simplifications"><a href="#plan-simplifications" aria-label="plan simplifications permalink"></a>Plan Simplifications</h2>
<p>Simplification regularizes SQL's rich syntax into a narrow format.
Ideally all logically equivalent plans would funnel into one common shape.
The "canonical" form should be the least surprising and fastest to
execute. In practice, perfect simplification is an aspirational goal
that has improved over time as workloads grow in complexity. The code
entrypoint is
<a href="https://github.com/dolthub/go-mysql-server/blob/4f15584bbf22bdd6ec22653c494c8f6d8529ca61/sql/analyzer/analyzer.go#L505">here</a>.
The current list of rules are in <a href="https://github.com/dolthub/go-mysql-server/blob/c856028387bc2bd5efc7fbdafc945c9405047f72/sql/analyzer/rules.go">this
file</a>.</p>
<p>Plan nodes hold correctness info calculated during binding
in a format amenable to query transformations. Technically this is our
third intermediate representation (IR), after ASTs and scope
hierarchies. Simplifications rules are always triggered because they
improve query runtime the same way compiler optimizations do dead code
elimination and expression folding. Some well documented SQL
simplifications are filter pushing and column pruning, which discard
unused rows and columns (respectively) as soon as possible during
execution.</p>
<p>We have dozens of simplification rules, most of which fit a narrow pattern match =&gt; rearrangement flow. The structure is so standardized that <a href="https://github.com/cockroachdb/cockroach/blob/master/pkg/sql/opt/optgen/lang/doc.go">CockroachDB wrote a DSL specifically for transformations</a>. The rules are simple and change rarely so we do it by hand, but the formalization is interesting for those who want to learn learn more.</p>
<p>One notable transformation that breaks the mould is subqeuery decorrelation/apply joins. A query like <code>select * from xy where x = (select a from ab)</code> is equivalent to <code>select * from xy join ab on x = a</code>. Collapsing table relations, filters, and projections into a common scope always leads to better join planning and often extra intra-scope simplifications. More formal specifications can be found <a href="https://www.researchgate.net/profile/Mostafa_Elhemali/publication/221213885_Execution_strategies_for_SQL_subqueries/links/584f631308aecb6bd8d02aa4/Execution-strategies-for-SQL-subqueries.pdf?__cf_chl_tk=tZx_Rk8_a3SaAKr1l5X.Ah0PX0UNKi4LDPI7WPNGm.s-1745004422-1.0.1.1-7NXojQt77FAxs6WPUcF6EMlezhtxtiZfkEeTq5xhaJ0">here</a> and <a href="https://subs.emis.de/LNI/Proceedings/Proceedings241/383.pdf">here</a>.</p>
<h2 id="type-coercion"><a href="#type-coercion" aria-label="type coercion permalink"></a>Type Coercion</h2>
<p>The same expression's type varies depending on the calling context. For example, an expression in an insert can be cast to the target column type, while a WHERE filter expression might be cast to a boolean. Dolt's SQL engine is increasingly shifting typing from execution to binding using top-down coercion hints. Typing is usually a separate compiler phase because it shares properties of both binding and simplification. We're semantically validating the query's typing consistency and correctness, while simplifying the expressions in a way that offloads work from the execution engine.</p>
<h2 id="plan-exploration"><a href="#plan-exploration" aria-label="plan exploration permalink"></a>Plan Exploration</h2>
<p>A query plan's simplest form is often the fastest to
execute. But joins, aggregations, and windows often have several equivalent
variations whose performance is database dependent.</p>
<p>Plan exploration has two separable components, search and costing.
Search enumerates valid join configurations that all produce
logically equivalent results. Logical variants include order
rearrangements (<code>A x (B x C) = &gt; (C x A) x B</code>) and operator choice
(LOOKUP_JOIN vs HASH_JOIN vs ...). Costing estimates the runtime cost of
specific (physical) plan configurations.</p>
<h2 id="join-search"><a href="#join-search" aria-label="join search permalink"></a>Join Search</h2>
<p>Within join order exploration there are two strategies. The first iteratively performs valid permutations on the seed plan, memoizing paths to avoid duplicate work. The search terminates when we run out of permutations, or decide we've found an optimal plan. The search entrypoint is <a href="https://github.com/dolthub/go-mysql-server/blob/e7b7ae11fed6b66b10ecd1bdeae6b842a3051d91/sql/analyzer/indexed_joins.go#L137">here</a>.</p>
<p>The top-down backtracking strategy contrasts with bottom-up dynamic
programming (DP), where we instead iterate every possible join order. First
we try every two-table join configuration, then every three table combo,
... etc working our way up to all n-tables.</p>
<p>Backtracking only visits valid states, but DP needs to detect conflicts
for all combinations.
For example, for <code>A LEFT
JOIN B</code> we would invalidate any order that violates <code>A &lt; B</code>.
This means backtracking can miss good plans if our
search doesn't reach far enough. DP invalidation criteria can be either
be too broad and reject valid plans, or have holes and accidentally
accept invalid plans. GMS currently uses the second strategy with a
<a href="https://15721.courses.cs.cmu.edu/spring2019/papers/23-optimizer2/p493-moerkotte.pdf">DP-Sube</a>
conflict detector that reaches more rearrangements than backtracking without
sacrificing correctness.</p>
<p>Every valid order is expanded beyond the default INNER_JOIN plan to consider LOOKUP_JOIN, HASH_JOIN, and MERGE_JOIN, among others.</p>
<p>One under-appreciated point worth noting is the intermediate representation used to accumulate explored states (join orders). As a reminder, the DP subproblem optimality conditions are:</p>
<ol>
<li>
<p>Join states can be logically clustered into groups by the results they produce.</p>
</li>
<li>
<p>A join group is always composed of two smaller logical join groups.</p>
</li>
</ol>
<p>There are a variety of words used to describe this problem (hypergraph = graph of graphs, forest = tree of trees), but memo is the most common terminology we saw so we stuck with that. A memo group contains all the search states (join orders) that produce the same result, and so is identified by the bitmap AND of the node id's in the group. A specific search state is two expression groups and a physical operator, terminating it table source groups.</p>
<p>The initial memo for <code>select * from join ab, uv, xy where a = u and u = x;</code> is below:</p>
<div data-language="text"><pre><code>memo:
├── G1: (tableScan: ab)
├── G2: (tableScan: uv)
├── G3: (innerJoin 2 1) (innerJoin 1 2)
├── G4: (tableScan: xy)
├── G5: (innerJoin 3 4) (innerJoin 6 1)
└── G6: (innerJoin 4 2)</code></pre></div>
<p>Each table gets a logical expression group. Each 2-way join gets and expression group (we link <code>a=x</code> using the transitive property). And they all funnel into the top-level join in <code>G5</code>.</p>
<p>This organization lets us cache logical properties at the group level, the most notable of which is the lowest cost physical plan. The subproblem optimality motivating the memo happily applies to both join reordering and costing.</p>
<h2 id="functional-dependencies"><a href="#functional-dependencies" aria-label="functional dependencies permalink"></a>Functional Dependencies</h2>
<p>Exploration can be expensive for 5+ table joins. Fortunately big joins often have star schema shaped primary key (<code>t1 join t2 on pk1 = pk2</code>) connections. If all join operators are connected by "strict keys", unique and non-null one to one relationships, sorting the tree by table size and connecting plans with LOOKUP_JOIN operators is often effective. The number of output rows (cardinality) is limited to the size of the first/smallest row source. Nick discuses functional dependency analysis for join planning more <a href="https://www.dolthub.com/blog/2023-12-13-functional-dependency-analysis/">here</a>. The functional dependencies code is <a href="https://github.com/dolthub/go-mysql-server/blob/9f1f6200964e129bca9632e031cf5936690b476e/sql/func_deps.go">here</a>.</p>
<h2 id="ir-intermission"><a href="#ir-intermission" aria-label="ir intermission permalink"></a>IR Intermission</h2>
<p>Here is the the progress we've made so far since parsing the AST:</p>
<p><span>
      <a href="https://www.dolthub.com/blog/static/c8bd897bc7c1237960822e5411d11fcf/04aae/max-bind-plan.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="parse" title="" src="https://www.dolthub.com/blog/static/c8bd897bc7c1237960822e5411d11fcf/ad12c/max-bind-plan.png" srcset="https://www.dolthub.com/blog/static/c8bd897bc7c1237960822e5411d11fcf/a48b3/max-bind-plan.png 214w,
https://www.dolthub.com/blog/static/c8bd897bc7c1237960822e5411d11fcf/47730/max-bind-plan.png 428w,
https://www.dolthub.com/blog/static/c8bd897bc7c1237960822e5411d11fcf/ad12c/max-bind-plan.png 856w,
https://www.dolthub.com/blog/static/c8bd897bc7c1237960822e5411d11fcf/7a18f/max-bind-plan.png 1284w,
https://www.dolthub.com/blog/static/c8bd897bc7c1237960822e5411d11fcf/56caf/max-bind-plan.png 1712w,
https://www.dolthub.com/blog/static/c8bd897bc7c1237960822e5411d11fcf/04aae/max-bind-plan.png 2463w" sizes="(max-width: 856px) 100vw, 856px" loading="lazy" decoding="async">
  </a>
    </span></p>
<p>The first IR groups column definitions into scopes that help validate and bind field references. The second IR lets us perform structural optimizations on a simplified plan. And the third IR accomodates exploring join reorders in a way that facilitates our next topic, join costing.</p>
<h2 id="join-costing"><a href="#join-costing" aria-label="join costing permalink"></a>Join Costing</h2>
<p>Join costing identifies the fastest physical plan in every logical
expression group enumerated during exploration. The coster entrypoint is
<a href="https://github.com/dolthub/go-mysql-server/blob/e787f4ee5b2842194af9c6091213ad23d3df9a14/sql/memo/coster.go#L57">here</a>.</p>
<p>We have many blogs that detail join costing (<a href="https://www.dolthub.com/blog/2024-10-24-automatic-stats/">collection of links here</a>). The summary is that schemas and table key distributions affect join cost. <code>A JOIN B</code> might return 10 rows in one database and 10 million rows in another. A 10 row join might use a LOOKUP_JOIN to optimize for latency, while a 10 million row join might use a HASH JOIN to optimize for throughput. We represent index key distributions as histograms. Intersecting histograms roughly approximates both the count and new key distribution of the result relation. The combination of (1) input sizes, (2) result size, (3) operator choice give us (1) the IO work required to pull rows from disk, (2) in-memory overhead of operator specific data structures (ex: hash maps), and (3) the approximate CPU cycles for reading the table sources to produce the result count.</p>
<p>Here is what the expanded cost tree looks like for our query:</p>
<div data-language="sql"><pre><code>tmp2<span>/</span>test<span>-</span>branch<span>*</span><span>&gt;</span> <span>select</span> dolt_join_cost<span>(</span><span>'SELECT * FROM xy WHERE EXISTS ( select 1 from uv where u = x )'</span><span>)</span><span>;</span>
<span>+</span><span>---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span>|</span> dolt_join_cost<span>(</span><span>'SELECT * FROM xy WHERE EXISTS (   select 1   from uv   where     u = x )'</span><span>)</span>                                                                                <span>|</span>
<span>+</span><span>---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span>|</span> memo:                                                                                                                                                                     <span>|</span>
<span>|</span> ├── G1: <span>(</span>tablescan: xy <span>0.0</span><span>)</span><span>*</span>                                                                                                                                              <span>|</span>
<span>|</span> ├── G2: <span>(</span>tablescan: uv <span>0.0</span><span>)</span><span>*</span>                                                                                                                                              <span>|</span>
<span>|</span> ├── G3: <span>(</span>lookupjoin <span>1</span> <span>2</span> <span>6.6</span><span>)</span> <span>(</span>lookupjoin <span>6</span> <span>1</span> <span>6.6</span><span>)</span> <span>(</span>project: <span>5</span> <span>0.0</span><span>)</span> <span>(</span>semijoin <span>1</span> <span>2</span> <span>4.0</span><span>)</span><span>*</span>                                                                                    <span>|</span>
<span>|</span> ├── G4: <span>(</span>project: <span>2</span> <span>0.0</span><span>)</span><span>*</span>                                                                                                                                                 <span>|</span>
<span>|</span> ├── G5: <span>(</span>hashjoin <span>1</span> <span>4</span> <span>8.0</span><span>)</span> <span>(</span>hashjoin <span>4</span> <span>1</span> <span>8.0</span><span>)</span> <span>(</span>mergejoin <span>1</span> <span>4</span> <span>4.1</span><span>)</span> <span>(</span>mergejoin <span>4</span> <span>1</span> <span>4.1</span><span>)</span> <span>(</span>lookupjoin <span>1</span> <span>4</span> <span>6.6</span><span>)</span> <span>(</span>lookupjoin <span>4</span> <span>1</span> <span>6.6</span><span>)</span> <span>(</span>innerjoin <span>4</span> <span>1</span> <span>5.0</span><span>)</span><span>*</span> <span>(</span>innerjoin <span>1</span> <span>4</span> <span>5.0</span><span>)</span><span>*</span> <span>|</span>
<span>|</span> └── G6: <span>(</span>project: <span>2</span> <span>0.0</span><span>)</span><span>*</span>                                                                                                                                                 <span>|</span>
<span>+</span><span>---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span></code></pre></div>
<p>Each logical expression group (<code>G</code>)'s physical implementation options now have associated costs. For example, the first <code>G3</code> implementation is a LOOKUP_JOIN between <code>G1</code> and <code>G2</code> that we've estimated costs 6.6 units. We differentiate between (1) the incremental work a join operator performs with (2) the accumulated cost of a choice and its dependencies. We print the incremental work rather than the accumulated work, though we might improve this function by printing both.</p>
<p>One note is that Dolt collects deterministic table statistics. Most databases use approximate sketches to (1) expedite refresh time, (2) minimize background work IO overhead, (3) make estimates fast, and (4) make it easier to compose higher order synthetic join estimations. Dolt's content addressable storage engine makes it somewhat easy for refreshes to only read a fraction of the database (more true for read-heavy workloads). Determinism is also fantastic for predictability. In practice, estimation overhead or higher order estimations have no performance bottlenecks for our Online Analytical Processing (OLAP) workloads.</p>
<p>When join costing is complete, we've discovered the optimal execution
strategy.</p>
<h2 id="join-hints"><a href="#join-hints" aria-label="join hints permalink"></a>Join Hints</h2>
<p>The coster gives its best effort to satisfy join hints indicated after
the <code>SELECT</code> token in a query. The
<a href="https://docs.dolthub.com/sql-reference/sql-support/miscellaneous#join-hints">docs</a>
go into more detail about hint options and use. The source code is
<a href="https://github.com/dolthub/go-mysql-server/blob/9a6edfcfab0d2f3a8babb55a3a4e1d431c1a158f/sql/memo/select_hints.go">here</a>. Hints that are
contradictory or not logically valid are usually ignored.</p>
<h2 id="execution"><a href="#execution" aria-label="execution permalink"></a>Execution</h2>
<p>The final plan needs to be converted into an executable format.</p>
<p>Dolt's default row execution format mirrors the plan node format. The plan below will share a volcano iterator of the same shape:</p>
<div data-language="sql"><pre><code>tmp2<span>/</span>test<span>-</span>branch<span>*</span><span>&gt;</span> <span>explain</span> <span>plan</span> <span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span> xy <span>join</span> uv <span>on</span> x <span>=</span> u<span>;</span>
<span>+</span><span>----------------------------------------+</span>
<span>|</span> <span>plan</span>                                   <span>|</span>
<span>+</span><span>----------------------------------------+</span>
<span>|</span> Project                                <span>|</span>
<span>|</span>  ├─ <span>columns</span>: <span>[</span><span>count</span><span>(</span><span>1</span><span>)</span><span>]</span>                <span>|</span>
<span>|</span>  └─ GroupBy                            <span>|</span>
<span>|</span>      ├─ SelectedExprs<span>(</span><span>COUNT</span><span>(</span><span>1</span><span>)</span><span>)</span>        <span>|</span>
<span>|</span>      ├─ Grouping<span>(</span><span>)</span>                     <span>|</span>
<span>|</span>      └─ MergeJoin                      <span>|</span>
<span>|</span>          ├─ cmp: <span>(</span>xy<span>.</span>x <span>=</span> uv<span>.</span>u<span>)</span>         <span>|</span>
<span>|</span>          ├─ IndexedTableAccess<span>(</span>xy<span>)</span>     <span>|</span>
<span>|</span>          │   ├─ <span>index</span>: <span>[</span>xy<span>.</span>x<span>]</span>          <span>|</span>
<span>|</span>          │   ├─ filters: <span>[</span>{<span>[</span><span>NULL</span><span>,</span> ∞<span>)</span>}<span>]</span> <span>|</span>
<span>|</span>          │   └─ <span>columns</span>: <span>[</span>x<span>]</span>           <span>|</span>
<span>|</span>          └─ IndexedTableAccess<span>(</span>uv<span>)</span>     <span>|</span>
<span>|</span>              ├─ <span>index</span>: <span>[</span>uv<span>.</span>u<span>]</span>          <span>|</span>
<span>|</span>              ├─ filters: <span>[</span>{<span>[</span><span>NULL</span><span>,</span> ∞<span>)</span>}<span>]</span> <span>|</span>
<span>|</span>              └─ <span>columns</span>: <span>[</span>u<span>]</span>           <span>|</span>
<span>+</span><span>----------------------------------------+</span></code></pre></div>
<p>Even when we swap GMS
iterators with Dolt-customized <code>kvexec</code> iterators(<a href="https://www.dolthub.com/blog/2024-11-25-kv-merge/">example
here</a>, the abstracted
iterators operating on the KV layer are still volcano iterators.</p>
<p><span>
      <a href="https://www.dolthub.com/blog/static/6845edf2398675fc6b83c695670969e4/22c86/max-sqliter.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="parse" title="" src="https://www.dolthub.com/blog/static/6845edf2398675fc6b83c695670969e4/ad12c/max-sqliter.png" srcset="https://www.dolthub.com/blog/static/6845edf2398675fc6b83c695670969e4/a48b3/max-sqliter.png 214w,
https://www.dolthub.com/blog/static/6845edf2398675fc6b83c695670969e4/47730/max-sqliter.png 428w,
https://www.dolthub.com/blog/static/6845edf2398675fc6b83c695670969e4/ad12c/max-sqliter.png 856w,
https://www.dolthub.com/blog/static/6845edf2398675fc6b83c695670969e4/22c86/max-sqliter.png 1143w" sizes="(max-width: 856px) 100vw, 856px" loading="lazy" decoding="async">
  </a>
    </span></p>
<p>There is one notable pre-iterator conversion. The column references
created in binding need to be converted from logical identifiers into
offset-based index accesses. Simplification and join exploration can
freely move expressions, but execution needs to know where values
are located at the data layer.<a href="https://github.com/dolthub/go-mysql-server/blob/893a99e13d832aae16bee6eba128ef832319129a/sql/analyzer/fix_exec_indexes.go#L30">This analyzer
rule</a>
sets the execution indexes.</p>
<p>Non-materializing iterators pull from the child iterator and return a row immediately. Materializing iterators have to read a sequence of children before returning. The <code>groupByIter</code> below shows how a GROUP_BY feeds all child rows (<code>i.child.Next</code>) into buffer aggregators (<code>updateBuffers</code>) before returning any rows (<code>evalBuffers</code>) (<a href="https://github.com/dolthub/go-mysql-server/blob/9988aefe97a06e03ff397a76ec48cf1e9d4e7e3e/sql/rowexec/agg.go#L29">source code here</a>):</p>
<div data-language="go"><pre><code><span>func</span> <span>(</span>i <span>*</span>groupByIter<span>)</span> <span>Next</span><span>(</span>ctx <span>*</span>sql<span>.</span>Context<span>)</span> <span>(</span>sql<span>.</span>Row<span>,</span> <span>error</span><span>)</span> <span>{</span>
	<span>for</span> <span>{</span>
		row<span>,</span> err <span>:=</span> i<span>.</span>child<span>.</span><span>Next</span><span>(</span>ctx<span>)</span>
		<span>if</span> err <span>!=</span> <span>nil</span> <span>{</span>
			<span>if</span> err <span>==</span> io<span>.</span>EOF <span>{</span>
				<span>break</span>
			<span>}</span>
			<span>return</span> <span>nil</span><span>,</span> err
		<span>}</span>

		<span>if</span> err <span>:=</span> <span>updateBuffers</span><span>(</span>ctx<span>,</span> i<span>.</span>buf<span>,</span> row<span>)</span><span>;</span> err <span>!=</span> <span>nil</span> <span>{</span>
			<span>return</span> <span>nil</span><span>,</span> err
		<span>}</span>
	<span>}</span>

	row<span>,</span> err <span>:=</span> <span>evalBuffers</span><span>(</span>ctx<span>,</span> i<span>.</span>buf<span>)</span>
	<span>if</span> err <span>!=</span> <span>nil</span> <span>{</span>
		<span>return</span> <span>nil</span><span>,</span> err
	<span>}</span>
	<span>return</span> row<span>,</span> <span>nil</span>
<span>}</span>

<span>func</span> <span>updateBuffers</span><span>(</span>
	ctx <span>*</span>sql<span>.</span>Context<span>,</span>
	buffers <span>[</span><span>]</span>sql<span>.</span>AggregationBuffer<span>,</span>
	row sql<span>.</span>Row<span>,</span>
<span>)</span> <span>error</span> <span>{</span>
	<span>for</span> <span>_</span><span>,</span> b <span>:=</span> <span>range</span> buffers <span>{</span>
		<span>if</span> err <span>:=</span> b<span>.</span><span>Update</span><span>(</span>ctx<span>,</span> row<span>)</span><span>;</span> err <span>!=</span> <span>nil</span> <span>{</span>
			<span>return</span> err
		<span>}</span>
	<span>}</span>
	<span>return</span> <span>nil</span>
<span>}</span>

<span>func</span> <span>evalBuffers</span><span>(</span>
	ctx <span>*</span>sql<span>.</span>Context<span>,</span>
	buffers <span>[</span><span>]</span>sql<span>.</span>AggregationBuffer<span>,</span>
<span>)</span> <span>(</span>sql<span>.</span>Row<span>,</span> <span>error</span><span>)</span> <span>{</span>
	<span>var</span> row <span>=</span> <span>make</span><span>(</span>sql<span>.</span>Row<span>,</span> <span>len</span><span>(</span>buffers<span>)</span><span>)</span>

	<span>var</span> err <span>error</span>
	<span>for</span> i<span>,</span> b <span>:=</span> <span>range</span> buffers <span>{</span>
		row<span>[</span>i<span>]</span><span>,</span> err <span>=</span> b<span>.</span><span>Eval</span><span>(</span>ctx<span>)</span>
		<span>if</span> err <span>!=</span> <span>nil</span> <span>{</span>
			<span>return</span> <span>nil</span><span>,</span> err
		<span>}</span>
	<span>}</span>
	<span>return</span> row<span>,</span> <span>nil</span>
<span>}</span></code></pre></div>
<p>One wart in our execution runtime is that correlated queries are constructed dynamically at runtime. A correlated subquery is initially still represented as the plan IR during execution. The runtime prepends the scope's context to table sources in the nested query's plan before constructing a new iterator. We won't discuss the details here.</p>
<h2 id="iospooling"><a href="#iospooling" aria-label="iospooling permalink"></a>IO/Spooling</h2>
<p>The volcano iterator in the previous stage produces result rows that
need to be translated to a result format. The main code entrypoint is
<a href="https://github.com/dolthub/go-mysql-server/blob/d406aced955683d84b9d1b141daa9b40074c8182/server/handler.go#L993">here</a>.</p>
<p>Reading from storage and writing to network are conceptually similar even if at opposite ends of the query lifecycle. Data moves between storage, runtime, and wire-time formats the same way query plans move through different intermediate representations. Dolt's tiered storage has various real byte layouts, but we generally refer to all as the Key Value (KV) layer. Rows are represented as byte array key/value pairs in the KV layer. Any table integrator (KV layer) interfaces with GMS through in-memory arrays of Go-native types. And lastly, MySQL's wire format is completely different than either the KV or SQL formats! Integers, for example, are represented as strings in wire format.</p>
<p>Batching and buffer reuse help manage throughput and memory churn at each of these interfaces. Cutting out the middleman and converting from KV-&gt;wire layer is an effective optimization for many queries. Queries that return one row are common, and have specialized spooling interfaces that optimize for latency rather than throughput.</p>
<p>The client protocol collects result bytes until the terminal packet
is sent, termination the command and freeing the session to start
another query.</p>
<h2 id="future"><a href="#future" aria-label="future permalink"></a>Future</h2>
<p>AST are a great intermediate representation for organizing tokenized bytes, but lack the expressivity required for the rest of semantic validation, plan simplification, and join planning. Building additinal IRs for each phase created an organizational and performance problem that only unifying solves. We've redistributed most logic on SQL nodes into either the preceding (binding) or following (memo) phases. But closing the gap is still quite a lift, and could involve (1) rewriting all subquery expressions as lateral joins, and (2) representing aliases as <code>SyntheticProject</code> nodes that append one column definition to the tree.</p>
<p>Golang's automatic memory management helped us get a fully functioning database off the ground quickly, but memory churn is still a bottleneck. We have improved memory re-use at specific points in the execution lifecycle where memory is short-lived, for example spooling rows over the network, but there is still much more to do. Avoiding heap allocating interface types and reusing execution buffers would improve execution performance. Standardizing and centralizing the memo IR would probably let us reuse memory there as well.</p>
<p>If you have any questions about Dolt, databases, or Golang performance
reach out to us on <a href="https://twitter.com/dolthub">Twitter</a>,
<a href="https://discord.gg/gqr7K4VNKe">Discord</a>, and
<a href="https://github.com/dolthub/go-mysql-server">GitHub</a>!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bare metal printf – C standard library without OS (196 pts)]]></title>
            <link>https://popovicu.com/posts/bare-metal-printf/</link>
            <guid>43807404</guid>
            <pubDate>Sat, 26 Apr 2025 21:32:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://popovicu.com/posts/bare-metal-printf/">https://popovicu.com/posts/bare-metal-printf/</a>, See on <a href="https://news.ycombinator.com/item?id=43807404">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="article" role="article">
      <p><a href="https://twitter.com/popovicu94?ref_src=twsrc%5Etfw" data-show-count="false">Follow @popovicu94</a></p>
<p>Today we’ll take a look at how we can leverage Newlib to create a compact C standard library for usage on a bare metal system. In a small example, we’ll implement a few UART primitives and pass them on to Newlib which uses them as the building blocks for a full-blown <code>printf</code> functionality. The target platform will be RISC-V, but the concepts should, as usual, be the same for other platforms as well.</p>
<h2 id="table-of-contents">Table of contents</h2>
<details><summary>Open Table of contents</summary>
<ul>
<li>
<p><a href="#software-abstractions-and-c-standard-library">Software abstractions and C standard library</a></p>
</li>
<li>
<p><a href="#c-standard-library-on-bare-metal">C standard library on bare metal</a></p>
<ul>
<li><a href="#newlib-concept">Newlib concept</a></li>
</ul>
</li>
<li>
<p><a href="#cross-compilation-toolchain">Cross-compilation toolchain</a></p>
<ul>
<li><a href="#toolchain-details">Toolchain details</a></li>
<li><a href="#automated-risc-v-toolchain-build">Automated RISC-V toolchain build</a></li>
</ul>
</li>
<li>
<p><a href="#github-link">GitHub link</a></p>
</li>
<li>
<p><a href="#implementing-the-memory-and-uart-building-blocks">Implementing the memory and UART building blocks</a></p>
</li>
<li>
<p><a href="#application-example-input-and-output">Application example: input and output</a></p>
<ul>
<li><a href="#the-gotcha-moment">The ‘gotcha’ moment</a></li>
</ul>
</li>
<li>
<p><a href="#running-the-app">Running the app</a></p>
</li>
<li>
<p><a href="#conclusion">Conclusion</a></p>
</li>
</ul>
</details>
<h2 id="software-abstractions-and-c-standard-library">Software abstractions and C standard library</h2>
<p>When running <code>printf</code> on a typical, fully operational, end-user system (e.g., a Mac or a Linux laptop), we invoke a pretty complex machinery. The application process calls the <code>printf</code> function, which is more often than not dynamically linked, and after a few layers of different C functions, a system call to the operating system kernel is typically invoked. The kernel will route the output through different subsystems: different terminal and pseudo-terminal primitives will be invoked, and at some point, you will also want to visually see the <code>printf</code> output on your screen. That also likely invokes a pretty thick stack of abstractions in order to render the characters on your screen. We won’t even talk about how <code>printf</code> formats the output strings based on the provided templates.</p>
<p>On a bare metal system, however, most of these abstractions are not available at all and the stack is much thinner.</p>

<p>If we’re working on bare metal, we don’t have anything below our C functions supporting us. In the full-blown example above, the process would hand over the output to the kernel through system calls, which are implemented through software interrupts. However, now we don’t have anything to hand over to, yet we want to have something like <code>printf</code> working, ideally outputting to a simple I/O device like UART.</p>
<p>This is where Newlib jumps in. You’re probably familiar with different flavors of C standard library like GNU (<code>glibc</code>), <code>musl</code> and so on, but Newlib should definitely be on your radar if you’d like to enable C standard library on bare metal.</p>
<p>More accurately, the way I think about Newlib is not as a C standard library, but rather as <strong>a kit to build a custom, compact C standard library</strong>.</p>
<h3 id="newlib-concept">Newlib concept</h3>
<p>Rather than requiring you to implement the whole C standard library from scratch, Newlib boils down the implementation to a few very basic primitives with clean interfaces that can be implemented as separate functions, and then other more complex functions like <code>printf</code> and <code>malloc</code> will call these primitives. Just for intuition, we will be implementing primitives like <code>_write</code> which essentially writes a single character to the output stream, and Newlib builds <code>printf</code> on top of that in order to write more complex outputs.</p>
<p>In addition to providing this simple set of primitives to implement, Newlib also gives reasonable pre-cooked implementations as well. In one of the configurations, you can even still target Linux as the underlying platform instead of bare metal, and the provided implementation will do system calls like <code>glibc</code> would do. Also, if you’re going for the absolutely minimal config, Newlib will provide all the primitives in a minimal form where they just return zeroes or raise an error (equivalent to something like raising an unimplemented exception in Python or Java).</p>
<p>Either way, you will implement whatever building blocks you actually care about in your application and the rest would rely on the default implementation.</p>

<p>Let’s switch gears here and talk about the cross-compilation toolchains. Cross-compilation happens when you compile from one platform to another. Intuitively, you can think of something like cross-compiling from an x86_64/Linux platform to ARM64/Mac.</p>
<p>On platforms like Linux, though, things can get a lot more nuanced, as Linux platform doesn’t necessarily mean one flavor of C standard library, so I’d refer to these platforms more accurately as x86_64/Linux/glibc. When you look at platforms from that perspective, even compiling from a platform with one standard library to another on the same x86_64/Linux setup, but with a different C library, you are effectively still cross-compiling. A concrete example would be cross-compiling from x86_64/Linux/glibc to x86_64/Linux/musl.</p>
<p>Furthermore, if you want to be extremely accurate and disciplined (as you should be if you want to build software that doesn’t break!), even building from one version of <code>glibc</code> for another is really cross-compiling. Again, as an example, building from x86_64/Linux/glibc_v1.0 for x86_64/Linux/glibc_v1.1 is cross-compilation.</p>
<p>This can quickly get difficult, at least with the traditional way of building and using compilers (such as GCC, for example); however, those “ancient” ways are still stuck with us for the foreseeable future. I will soon write in more detail about this, and for now, we’ll use a shortcut described below.</p>
<h3 id="toolchain-details">Toolchain details</h3>
<p>We want a toolchain that satisfies two requirements:</p>
<ol>
<li>it builds from our host platform to RISC-V, i.e. it generates RISC-V instructions</li>
<li>it uses the Newlib library when C standard library functionality is invoked</li>
</ol>
<p>If you’re on a typical Linux distribution, you likely have something like GCC (or even clang) installed. When you simply run GCC on a C file without any fancy flags, what happens is that the compiler will simply build for the same platform it runs on. More accurately, the host and the target are the same, and I believe the formal term for this is <em>native compilation</em>. The reason why I bring this up is to ask ourselves what happens when we include something like <code>stdio.h</code> and call something like <code>printf</code>? Where is this <code>.h</code> file really pulled from and where is ultimately the <code>printf</code> implementation found so it can be linked against?</p>
<p>This really depends on <strong>the way your compiler was built</strong>. When you build GCC from source, and you run <code>./configure</code>, you can specify a ton of flags that will drive this behavior. As promised, I will write more about it in the future. For now, let’s keep in mind that most Linux distributions we use in daily lives follow the old UNIX philosophy when it comes to this. For example, my Debian installation has <code>stdio.h</code> in a standard directory at <code>/usr/include</code>. Furthermore, my standard C library (<code>glibc</code>) that can be dynamically linked is at <code>/lib/x86_64-linux-gnu/libc.so</code> (which really points to <code>/lib/x86_64-linux-gnu/libc.so.6</code>). Similarly, there is an <code>.a</code> file in there, but I will assume you know what <code>.so</code> and <code>.a</code> files are for. So, long story short, skipping a lot of details, your native compiler is set up to look for the C library in some of the standard spots, and when it builds for the same platform, it simply picks up the libraries from there.</p>
<p>Therefore, we need to:</p>
<ol>
<li>get the compiler that can generate instructions for the desired platform (machine code)</li>
<li>set up the C standard library for that particular platform somewhere</li>
<li>ensure that the compiler for the target platform knows how to use the library from above</li>
</ol>
<p>From what I’ve seen, when it comes to cross-compilation, this is a fair amount of grungy work that needs to be done. The set up above takes a lot of building time and needs to be done in stages when done properly. A future article will go into details, but for now, as we mentioned before, we’ll go for a shortcut.</p>
<p>Remember for now that we want the includes, <code>.so</code>/<code>.a</code> files at some path, and we want the cross-compiler to look <strong>there</strong> for the C standard library, not at the host’s <code>include</code> and <code>lib</code> directories. In this case since we want to build from something like x86_64 to RISC-V, it’s easy to spot errors, since if we use the host’s libraries, there is no way the wrong architecture would work, but when compiling for the same architecture and a different software platform, host contamination can be a real thing and can lead to very subtle and annoying problems! For example, we want the library code to be searched for at <code>/usr/local/risc_v_stuff/lib</code> instead of <code>/usr/lib</code>.</p>
<h3 id="automated-risc-v-toolchain-build">Automated RISC-V toolchain build</h3>
<p>For this exercise, let’s simply use the <a href="https://github.com/riscv-collab/riscv-gnu-toolchain/">RISC-V toolchain</a>. This project will still build everything from source on our host machine, but the whole annoying orchestration mentioned above, including staging the compilers, will be scripted and automated for us. With a few commands, we’ll kick off the process that will effectively set up something like <code>/usr/local/risc_v_stuff/lib</code>, <code>/usr/local/risc_v_stuff/include</code>, <code>/usr/local/risc_v_stuff/compiler</code> and you’ll be able to invoke <code>/usr/local/risc_v_stuff/compiler/gcc</code> which will know to peek into the right directories for different files and will build the right machine code. Of course, the paths will ultimately be different, but this should be good enough as a concept.</p>
<p>We can start by cloning the Git repository linked above. The instructions say that the <code>--recursive</code> flag is not necessary during cloning and things will be dynamically pulled later, but for whatever reason, this <strong>did not</strong> work on my system. I ended up running a clone with the <code>--recursive</code> flag to avoid issues. It took a lot of time and space though, pulling in gigabytes and gigabytes of source code.</p>
<p>Once the endless clone is done, you can configure the build. This is how I configured it:</p>
<pre is:raw="" tabindex="0"><code><span><span>./configure --prefix=/opt/riscv-newlib --enable-multilib --disable-gdb --with-cmodel=medany</span></span></code></pre>
<p>I strongly encourage you to run <code>./configure --help</code> to see what all the available options are and customize the build. For now, I will explain my parameters:</p>
<ol>
<li><code>prefix</code> is simply where we’ll install the newly built artifacts, such as the cross-compiler, C standard library (Newlib in our case) and so on.</li>
<li><code>enable-multilib</code> will enable builds for different RISC-V setups. As a reminder, RISC-V has a ton of flavors, like <code>RV32I</code>, <code>RV32IMA</code> and so on. Please do note that enabling this flag will make your build <strong>super slow</strong>. If you don’t want to run the build with multilib, then check the help menu to figure out how to build exactly for the fine grained platform that you need.</li>
<li><code>disable-gdb</code>: for whatever reason, building GDB would always fail for me, so I just excluded it from the toolchain. Real engineers debug with <code>printf</code> anyway!</li>
<li><code>with-cmodel</code>: hold on to this one, I will reveal this in a ‘gotcha’ moment; for now, let’s just keep in mind I needed this in order to make the 64-bit RISC-V builds work.</li>
</ol>
<p>Now that your build is configured, you can fire off the build process and leave it cooking for quite some time. One thing that surprised me here is that they didn’t use separate <code>make</code> and <code>make install</code> steps. Everything is done through just <code>make</code>, both the compilation and installation of the artifacts.</p>
<p><strong>Note: I wanted to parallelize the build with <code>-j16</code> as I normally do, but that also somehow broke my build, so I suggest running without this, and yes, I know it takes forever.</strong></p>
<p>I simply ran</p>
<pre is:raw="" tabindex="0"><code><span><span>sudo make</span></span></code></pre>
<p>in order to place the final artifacts in the <code>/opt</code> directory. This whole process is very slow, so make sure you have something else to do while this is working.</p>
<p>Also, you may wonder where Newlib comes in here, when I mentioned that this whole process will automate how we get Newlib available. The answer is that Newlib is simply the default option for building our toolchain here. You can check the GitHub documentation on how to set up your cross-compiler to target a RISC-V <code>glibc</code> or <code>musl</code>, but what I’ve listed above is good enough to get a cross-compiler with Newlib as the target.</p>
<h2 id="github-link">GitHub link</h2>
<p>I have prepared <a href="https://github.com/popovicu/bare-metal-cstdlib">this repository</a> to run our example. The code explanations are below (hopefully they’re not out of sync with the repo itself).</p>
<h2 id="implementing-the-memory-and-uart-building-blocks">Implementing the memory and UART building blocks</h2>
<p>Now that you have a working cross-toolchain targeting RISC-V + Newlib, most of the heavy lifting is done and we can start putting together the Newlib building blocks. Let’s begin with UART, and the first file is <code>uart.h</code>:</p>
<pre is:raw="" tabindex="0"><code><span><span>#ifndef</span><span> </span><span>UART_H</span></span>
<span><span>#define</span><span> </span><span>UART_H</span></span>
<span></span>
<span><span>void</span><span> </span><span>uart_putc</span><span>(</span><span>char</span><span> </span><span>c</span><span>);</span></span>
<span><span>char</span><span> </span><span>uart_getc</span><span>(</span><span>void</span><span>);</span></span>
<span></span>
<span><span>#endif</span></span></code></pre>
<p>This is self-explanatory so far. Let’s see how these functions are implemented:</p>
<pre is:raw="" tabindex="0"><code><span><span>#include</span><span> </span><span>"uart.h"</span></span>
<span></span>
<span><span>// QEMU UART registers - these addresses are for QEMU's 16550A UART</span></span>
<span><span>#define</span><span> </span><span>UART_BASE</span><span> </span><span>0x</span><span>10000000</span></span>
<span><span>#define</span><span> </span><span>UART_THR</span><span>  (</span><span>*</span><span>(</span><span>volatile</span><span> </span><span>char</span><span> </span><span>*</span><span>)(UART_BASE </span><span>+</span><span> </span><span>0x</span><span>00</span><span>))</span><span> // Transmit Holding Register</span></span>
<span><span>#define</span><span> </span><span>UART_RBR</span><span>  (</span><span>*</span><span>(</span><span>volatile</span><span> </span><span>char</span><span> </span><span>*</span><span>)(UART_BASE </span><span>+</span><span> </span><span>0x</span><span>00</span><span>))</span><span> // Receive Buffer Register</span></span>
<span><span>#define</span><span> </span><span>UART_LSR</span><span>  (</span><span>*</span><span>(</span><span>volatile</span><span> </span><span>char</span><span> </span><span>*</span><span>)(UART_BASE </span><span>+</span><span> </span><span>0x</span><span>05</span><span>))</span><span> // Line Status Register</span></span>
<span></span>
<span><span>#define</span><span> </span><span>UART_LSR_TX_IDLE</span><span>  (</span><span>1</span><span> </span><span>&lt;&lt;</span><span> </span><span>5</span><span>)</span><span> // Transmitter idle</span></span>
<span><span>#define</span><span> </span><span>UART_LSR_RX_READY</span><span> (</span><span>1</span><span> </span><span>&lt;&lt;</span><span> </span><span>0</span><span>)</span><span> // Receiver ready</span></span>
<span></span>
<span><span>void</span><span> </span><span>uart_putc</span><span>(</span><span>char</span><span> c)</span><span> {</span></span>
<span><span>    </span><span>// Wait until transmitter is idle</span></span>
<span><span>    </span><span>while</span><span> ((UART_LSR </span><span>&amp;</span><span> UART_LSR_TX_IDLE) </span><span>==</span><span> </span><span>0</span><span>);</span></span>
<span><span>    UART_THR </span><span>=</span><span> c;</span></span>
<span></span>
<span><span>    </span><span>// Special handling for newline (send CR+LF)</span></span>
<span><span>    </span><span>if</span><span> (c </span><span>==</span><span> </span><span>'</span><span>\n</span><span>'</span><span>) {</span></span>
<span><span>        </span><span>while</span><span> ((UART_LSR </span><span>&amp;</span><span> UART_LSR_TX_IDLE) </span><span>==</span><span> </span><span>0</span><span>);</span></span>
<span><span>        UART_THR </span><span>=</span><span> </span><span>'</span><span>\r</span><span>'</span><span>;</span></span>
<span><span>    }</span></span>
<span><span>}</span></span>
<span></span>
<span><span>char</span><span> </span><span>uart_getc</span><span>(</span><span>void</span><span>) {</span></span>
<span><span>    </span><span>// Wait for data</span></span>
<span><span>    </span><span>while</span><span> ((UART_LSR </span><span>&amp;</span><span> UART_LSR_RX_READY) </span><span>==</span><span> </span><span>0</span><span>);</span></span>
<span><span>    </span><span>return</span><span> UART_RBR;</span></span>
<span><span>}</span></span></code></pre>
<p>The code above was AI-generated, but it’s accurate. And this is it as far as our UART driver is concerned. How does that now work with Newlib?</p>
<p>We switch to the file called <code>syscalls.c</code>. Here, we implement the functions that <code>printf</code> would rely on. We’ll also handle the input as well, just for fun. First, what happens here is we implement the primitives for writing to a file handle. The only file handles we’ll really support here are <code>stdout</code> and <code>stderr</code>. And to be perfectly accurate, there are no files here; we’re just intercepting the C standard library calls that otherwise work with these concepts.</p>
<p>Moving further, we provide super minimal implementations for a few more building blocks. They’re extremely basic, like the <code>_close</code> function, which essentially never allows any file handle to be closed.</p>
<p>The one building block that is very interesting here is <code>_sbrk</code>. This is what gets invoked when the routines for dynamic memory allocation like <code>malloc</code> (needed by the <code>printf</code> family of functions) need to ask the OS (when there is one) to provide more raw memory to the process, that can then be fragmented into smaller logical units by <code>malloc</code>. What happens here is we find the symbol <code>_end</code> defined by the linker, which marks the <code>_end</code> of the static sections (we’ll see how below) and we start using the memory past that address for heap allocations, all the way until we hit the stack. Once we hit the stack, we declare that an error as we have run out of memory.</p>
<pre is:raw="" tabindex="0"><code><span><span>void*</span><span> </span><span>_sbrk</span><span>(</span><span>int</span><span> </span><span>incr</span><span>) {</span></span>
<span><span>    </span><span>extern</span><span> </span><span>char</span><span> _end;</span><span>         // Defined by the linker - start of heap</span></span>
<span><span>    </span><span>extern</span><span> </span><span>char</span><span> _stack_bottom;</span><span> // Defined in our linker script - bottom of stack area</span></span>
<span></span>
<span><span>    </span><span>static</span><span> </span><span>char</span><span> </span><span>*</span><span>heap_end </span><span>=</span><span> </span><span>&amp;</span><span>_end;</span></span>
<span><span>    </span><span>char</span><span> </span><span>*</span><span>prev_heap_end </span><span>=</span><span> heap_end;</span></span>
<span></span>
<span><span>    </span><span>// Calculate safe stack limit - stack grows down from _stack_top towards _stack_bottom</span></span>
<span><span>    </span><span>char</span><span> </span><span>*</span><span>stack_limit </span><span>=</span><span> </span><span>&amp;</span><span>_stack_bottom;</span></span>
<span></span>
<span><span>    </span><span>// Check if heap would grow too close to stack</span></span>
<span><span>    </span><span>if</span><span> (heap_end </span><span>+</span><span> incr </span><span>&gt;</span><span> stack_limit) {</span></span>
<span><span>        errno </span><span>=</span><span> ENOMEM;</span></span>
<span><span>        </span><span>return</span><span> (</span><span>void*</span><span>) </span><span>-</span><span>1</span><span>;</span><span> // Return error</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    heap_end </span><span>+=</span><span> incr;</span></span>
<span><span>    </span><span>return</span><span> (</span><span>void*</span><span>) prev_heap_end;</span></span>
<span><span>}</span></span></code></pre>
<p>Please note that the stack top and bottom here refer to the beginning and the end of the memory block allocated for the stack, not the logical top or bottom of the stack itself from the application perspective.</p>
<h2 id="application-example-input-and-output">Application example: input and output</h2>
<p>We’re now ready to put the actual bare metal application together. If you need a refresher on <a href="https://popovicu.com/posts/bare-metal-programming-risc-v">bare metal programming on RISC-V</a>, check it out again. That article covers the key addresses and the basics of putting a bare-metal linker script together.</p>
<p>The app code itself is very self explanatory:</p>
<pre is:raw="" tabindex="0"><code><span><span>#include</span><span> </span><span>&lt;stdio.h&gt;</span></span>
<span></span>
<span><span>int</span><span> </span><span>main</span><span>(</span><span>void</span><span>) {</span></span>
<span><span>    </span><span>printf</span><span>(</span><span>"Hello from RISC-V UART!</span><span>\n</span><span>"</span><span>);</span></span>
<span></span>
<span><span>    </span><span>char</span><span> </span><span>buffer</span><span>[</span><span>100</span><span>];</span></span>
<span><span>    </span><span>printf</span><span>(</span><span>"Type something: "</span><span>);</span></span>
<span><span>    </span><span>scanf</span><span>(</span><span>"</span><span>%s</span><span>"</span><span>, buffer);</span></span>
<span><span>    </span><span>printf</span><span>(</span><span>"You typed: </span><span>%s</span><span>\n</span><span>"</span><span>, buffer);</span></span>
<span></span>
<span><span>    </span><span>while</span><span> (</span><span>1</span><span>) {}</span></span>
<span></span>
<span><span>    </span><span>return</span><span> </span><span>0</span><span>;</span></span>
<span><span>}</span></span></code></pre>
<p>Please note that when we’re inputting something to this app, we won’t see our key presses echoed. This is because we’re not operating inside some sort of shell environment. The implementation as it is will simply accept the key presses and store them in the internal memory structure. We’ll see what was typed when we hit the final <code>printf</code>.</p>
<p>We now need to also put together a small C runtime. When we develop a binary for an everyday OS, we typically don’t have to think about this, and the compiler will inject the standard startup runtime which takes care of setting up the process for proper execution and passing the control on to the <code>main</code> function.</p>
<p>Our minimalistic runtime will set up the stack pointer register, zero-fill the BSS section per C standard, and then call the <code>main</code> code. Just for good measure, we also leave an infinite loop at the end in case <code>main</code> returns. Again, with a proper OS below our code, a system call would be invoked to properly close the process, and it wouldn’t just loop infinitely.</p>
<pre is:raw="" tabindex="0"><code><span><span>.</span><span>section .text</span><span>.init</span></span>
<span><span>.global _start</span></span>
<span></span>
<span><span>_start:</span></span>
<span><span>    la </span><span>sp</span><span>, _stack_top</span></span>
<span></span>
<span><span>   </span><span> # Clear BSS section - using symbols defined in our linker script</span></span>
<span><span>    la t0, _bss_start</span></span>
<span><span>    la t1, _bss_end</span></span>
<span><span>clear_bss:</span></span>
<span><span>    bgeu t0, t1, bss_done</span></span>
<span><span>    sb zero, </span><span>0</span><span>(t0)</span></span>
<span><span>    addi t0, t0, </span><span>1</span></span>
<span><span>    j clear_bss</span></span>
<span><span>bss_done:</span></span>
<span></span>
<span><span>   </span><span> # Jump to C code</span></span>
<span><span>    </span><span>call</span><span> main</span></span>
<span></span>
<span><span>   </span><span> # In case main returns</span></span>
<span><span>1</span><span>:  j </span><span>1b</span></span></code></pre>
<p>One of the most important parts of our application now is the linker script:</p>
<pre is:raw="" tabindex="0"><code><span><span>OUTPUT_FORMAT("elf64-littleriscv")</span></span>
<span><span>OUTPUT_ARCH("riscv")</span></span>
<span><span>ENTRY(_start)</span></span>
<span><span></span></span>
<span><span>MEMORY</span></span>
<span><span>{</span></span>
<span><span>  RAM (rwx) : ORIGIN = 0x80000000, LENGTH = 64M</span></span>
<span><span>}</span></span>
<span><span></span></span>
<span><span>SECTIONS</span></span>
<span><span>{</span></span>
<span><span>  /* Code section */</span></span>
<span><span>  .text : {</span></span>
<span><span>    *(.text.init)</span></span>
<span><span>    *(.text)</span></span>
<span><span>  } &gt; RAM</span></span>
<span><span></span></span>
<span><span>  /* Read-only data */</span></span>
<span><span>  .rodata : {</span></span>
<span><span>    *(.rodata)</span></span>
<span><span>  } &gt; RAM</span></span>
<span><span></span></span>
<span><span>  /* Initialized data */</span></span>
<span><span>  .data : {</span></span>
<span><span>    *(.data)</span></span>
<span><span>  } &gt; RAM</span></span>
<span><span></span></span>
<span><span>  /* Small initialized data */</span></span>
<span><span>  .sdata : {</span></span>
<span><span>    *(.sdata)</span></span>
<span><span>  } &gt; RAM</span></span>
<span><span></span></span>
<span><span>  /* BSS section with explicit symbols */</span></span>
<span><span>  .bss : {</span></span>
<span><span>    _bss_start = .;  /* Define BSS start symbol */</span></span>
<span><span>    *(.bss)</span></span>
<span><span>    *(COMMON)</span></span>
<span><span>    . = ALIGN(8);</span></span>
<span><span>    _bss_end = .;    /* Define BSS end symbol */</span></span>
<span><span>  } &gt; RAM</span></span>
<span><span></span></span>
<span><span>  /* Small BSS section */</span></span>
<span><span>  .sbss : {</span></span>
<span><span>    _sbss_start = .;</span></span>
<span><span>    *(.sbss)</span></span>
<span><span>    *(.sbss.*)</span></span>
<span><span>    . = ALIGN(8);</span></span>
<span><span>    _sbss_end = .;</span></span>
<span><span>  } &gt; RAM</span></span>
<span><span></span></span>
<span><span>  /* End marker for heap start */</span></span>
<span><span>  . = ALIGN(8);</span></span>
<span><span>  _end = .; /* Heap starts here and grows upwards */</span></span>
<span><span></span></span>
<span><span>  /* Stack grows downward from the end of RAM */</span></span>
<span><span>  _stack_size = 64K;</span></span>
<span><span>  _stack_top = ORIGIN(RAM) + LENGTH(RAM);</span></span>
<span><span>  _stack_bottom = _stack_top - _stack_size;</span></span>
<span><span></span></span>
<span><span>  /* Ensure we don't overlap with heap */</span></span>
<span><span>  ASSERT(_end &lt;= _stack_bottom, "Error: Heap collides with stack")</span></span>
<span><span>}</span></span></code></pre>
<p>Per our previous investigation of bare metal programming for the QEMU VM, we know that the user-provided code will begin executing from <code>0x80000000</code>. Therefore, what we do is put the C runtime code that we previously wrote right there. In other words, our assembly code will be planted right at that memory address. Following our C runtime is the rest of the code, i.e. the <code>text</code> section. In this case, this is the C code we have written plus the C standard library we’re linking into our binary.</p>
<p>After that, we place the other sections like <code>rodata</code>, <code>data</code>, <code>bss</code>, and so on. The linker script will capture the symbols for BSS start and end so it can be zero-filled by the C runtime, as seen above. There’s also a small BSS section, but the C runtime code doesn’t do anything about it, to stay compact, as it’s not used by the application. It probably should be zero-filled as well.</p>
<p>Then, we capture where the small BSS section ends because that also marks the end of our static sections. Following that, we let the growing heap consume everything, up until the stack. The stack occupies the last 64K of memory (and we described RAM as being 64M at the top of the linker script). A few addition and subtraction operations are done to determine where this exactly is, and we do an assert check to make sure there is no collision between the heap and the stack.</p>
<p>The concept is simple: we identify the “void” between the static sections and the stack, and we let the C standard library that we’re putting together via Newlib to maintain the growing heap in there. A real kernel like Linux would do its memory management magic here, handle the virtual addresses and so on, but here we really have one “process” and a simple memory extension operation <code>sbrk</code> is enough for what we want to achieve here.</p>
<h3 id="the-gotcha-moment">The ‘gotcha’ moment</h3>
<p>Now let’s reflect back on the fact that we configured the toolchain to be built with the <code>--with-cmodel=medany</code> flag. What does this flag really control, and why did we need it?</p>
<p>If you read the top of the linker script carefully, we’re building for a 64-bit RISC-V machine. Per QEMU, our instructions will begin at <code>0x80000000</code>, and we decided to simply lay the rest of the code after that. To handle these high values, we need to use the correct machine instructions to handle these high addresses. So our application code likely needs to use the memory address model which can handle any address, and so we build our logic with <code>-mcmodel=medany</code>. To be compatible, our C standard library also needs that.</p>
<p>If we didn’t have the aforementioned flag, the Newlib library would be built with RISC-V instructions that cannot effectively use such high addresses. Remember, the C standard library is pre-built before our application. The build system will simply pick up the machine code from the relevant library directory and link it to your application code. If the addresses do not fit the value range that the instructions support, the linker is not able to make things work.</p>
<p>As I understand, there is a concept of <em>linker relaxation</em>, where the linker itself can make the code modifications, but I don’t think it would help in this case.</p>
<p>I don’t want to spend too much time on this, I hope the explanation above suffices, and if you would like to learn more about this problem, check out <a href="https://github.com/riscvarchive/riscv-gcc/issues/153">this link</a>, where the reporter had linker errors and a solution was offered.</p>
<h2 id="running-the-app">Running the app</h2>
<p>I’ve included a <code>Makefile</code> in the GitHub repo. Check it out to see what exactly is going on there, especially how the cross-compiler is invoked (should be the first line of the file), as well as QEMU for emulation. I will highlight a few things here.</p>
<p>One of the <code>CFLAGS</code> is <code>-specs=nosys.specs</code>. This will drive the toolchain to use the ‘nosys’ flavor of Newlib. This is the most minimal flavor where all the building blocks are just stubs by default that return zeroes or errors.</p>
<p>Linker flags include <code>-nostartfiles</code> which means that we’ll be providing our own minimal C runtime, that we have described above.</p>
<p>The rest of the <code>Makefile</code> should be fairly easy to follow. I strongly suggest using the <code>debug</code> target though. We’ll just go ahead and run:</p>
<pre is:raw="" tabindex="0"><code><span><span>make</span><span> </span><span>debug</span></span></code></pre>
<p>The QEMU process starts, I punch in <code>foo</code> and hit enter, and after getting back my app’s output, I stop QEMU:</p>
<pre is:raw="" tabindex="0"><code><span><span>$</span><span> </span><span>make</span><span> </span><span>debug</span></span>
<span><span>/opt/riscv-newlib/bin/riscv64-unknown-elf-gcc</span><span> </span><span>-march=rv64imac_zicsr</span><span> </span><span>-mabi=lp64</span><span> </span><span>-mcmodel=medany</span><span> </span><span>-specs=nosys.specs</span><span> </span><span>-O2</span><span> </span><span>-g</span><span> </span><span>-Wall</span><span> </span><span>-c</span><span> </span><span>main.c</span><span> </span><span>-o</span><span> </span><span>main.o</span></span>
<span><span>/opt/riscv-newlib/bin/riscv64-unknown-elf-gcc</span><span> </span><span>-march=rv64imac_zicsr</span><span> </span><span>-mabi=lp64</span><span> </span><span>-mcmodel=medany</span><span> </span><span>-specs=nosys.specs</span><span> </span><span>-O2</span><span> </span><span>-g</span><span> </span><span>-Wall</span><span> </span><span>-c</span><span> </span><span>uart.c</span><span> </span><span>-o</span><span> </span><span>uart.o</span></span>
<span><span>/opt/riscv-newlib/bin/riscv64-unknown-elf-gcc</span><span> </span><span>-march=rv64imac_zicsr</span><span> </span><span>-mabi=lp64</span><span> </span><span>-mcmodel=medany</span><span> </span><span>-specs=nosys.specs</span><span> </span><span>-O2</span><span> </span><span>-g</span><span> </span><span>-Wall</span><span> </span><span>-c</span><span> </span><span>syscalls.c</span><span> </span><span>-o</span><span> </span><span>syscalls.o</span></span>
<span><span>/opt/riscv-newlib/bin/riscv64-unknown-elf-gcc</span><span> </span><span>-march=rv64imac_zicsr</span><span> </span><span>-mabi=lp64</span><span> </span><span>-mcmodel=medany</span><span> </span><span>-specs=nosys.specs</span><span> </span><span>-O2</span><span> </span><span>-g</span><span> </span><span>-Wall</span><span> </span><span>-c</span><span> </span><span>startup.S</span><span> </span><span>-o</span><span> </span><span>startup.o</span></span>
<span><span>/opt/riscv-newlib/bin/riscv64-unknown-elf-gcc</span><span> </span><span>-march=rv64imac_zicsr</span><span> </span><span>-mabi=lp64</span><span> </span><span>-mcmodel=medany</span><span> </span><span>-specs=nosys.specs</span><span> </span><span>-O2</span><span> </span><span>-g</span><span> </span><span>-Wall</span><span> </span><span>-T</span><span> </span><span>link.ld</span><span> </span><span>-nostartfiles</span><span>   </span><span>-o</span><span> </span><span>firmware.elf</span><span> </span><span>main.o</span><span> </span><span>uart.o</span><span> </span><span>syscalls.o</span><span> </span><span>startup.o</span></span>
<span><span>/opt/riscv-newlib/lib/gcc/riscv64-unknown-elf/14.2.0/../../../../riscv64-unknown-elf/bin/ld:</span><span> </span><span>warning:</span><span> </span><span>firmware.elf</span><span> </span><span>has</span><span> </span><span>a</span><span> </span><span>LOAD</span><span> </span><span>segment</span><span> </span><span>with</span><span> </span><span>RWX</span><span> </span><span>permissions</span></span>
<span><span>qemu-system-riscv64</span><span> </span><span>-machine</span><span> </span><span>virt</span><span> </span><span>-m</span><span> </span><span>256</span><span> </span><span>-nographic</span><span> </span><span>-bios</span><span> </span><span>firmware.elf</span><span> </span><span>-d</span><span> </span><span>in_asm,cpu_reset</span><span> </span><span>-D</span><span> </span><span>qemu_debug.log</span></span>
<span><span>Hello</span><span> </span><span>from</span><span> </span><span>RISC-V</span><span> </span><span>UART!</span></span>
<span><span>Type</span><span> </span><span>something:</span><span> </span><span>You</span><span> </span><span>typed:</span><span> </span><span>foo</span></span></code></pre>
<p>The reason why I suggest using the <code>debug</code> target is because it drops a file called <code>qemu_debug.log</code>. That file is pretty cool as it shows you a complete trace of what your VM has been through. Naturally, you can inspect all the Newlib code if you want to figure out how <strong>exactly</strong> <code>printf</code> works, but I thought it’s still a pretty nice view of what the RISC-V core actually sees. Since we’re building an <code>ELF</code> file and passing it to <code>QEMU</code>, it’s even able to tell us which function we’re exactly in. It doesn’t have that for the first couple of instructions since we’re, as a reminder, executing the initial hardcoded bootloader, and then our initial C runtime, before jumping into the <code>main</code> function. If the first few instructions before <code>0x80000000</code> confuse you, please check out <a href="https://popovicu.com/posts/risc-v-sbi-and-full-boot-process">RISC-V boot process with SBI</a> to understand what’s going on. Excerpt of my debug log is below:</p>
<pre is:raw="" tabindex="0"><code><span><span>----------------</span></span>
<span><span>IN:</span></span>
<span><span>Priv: 3; Virt: 0</span></span>
<span><span>0x0000000000001000:  00000297          auipc                   t0,0                    # 0x1000</span></span>
<span><span>0x0000000000001004:  02828613          addi                    a2,t0,40</span></span>
<span><span>0x0000000000001008:  f1402573          csrrs                   a0,mhartid,zero</span></span>
<span><span></span></span>
<span><span>----------------</span></span>
<span><span>IN:</span></span>
<span><span>Priv: 3; Virt: 0</span></span>
<span><span>0x000000000000100c:  0202b583          ld                      a1,32(t0)</span></span>
<span><span>0x0000000000001010:  0182b283          ld                      t0,24(t0)</span></span>
<span><span>0x0000000000001014:  00028067          jr                      t0</span></span>
<span><span></span></span>
<span><span>----------------</span></span>
<span><span>IN:</span></span>
<span><span>Priv: 3; Virt: 0</span></span>
<span><span>0x0000000080000000:  04000117          auipc                   sp,67108864             # 0x84000000</span></span>
<span><span>0x0000000080000004:  00010113          mv                      sp,sp</span></span>
<span><span>0x0000000080000008:  00015297          auipc                   t0,86016                # 0x80015008</span></span>
<span><span>0x000000008000000c:  d5828293          addi                    t0,t0,-680</span></span>
<span><span>0x0000000080000010:  00015317          auipc                   t1,86016                # 0x80015010</span></span>
<span><span>0x0000000080000014:  d5030313          addi                    t1,t1,-688</span></span>
<span><span>0x0000000080000018:  0062f663          bleu                    t1,t0,12                # 0x80000024</span></span>
<span><span></span></span>
<span><span>----------------</span></span>
<span><span>IN:</span></span>
<span><span>Priv: 3; Virt: 0</span></span>
<span><span>0x0000000080000024:  0be020ef          jal                     ra,8382                 # 0x800020e2</span></span>
<span><span></span></span>
<span><span>----------------</span></span>
<span><span>IN: main</span></span>
<span><span>Priv: 3; Virt: 0</span></span>
<span><span>0x00000000800020e2:  7119              addi                    sp,sp,-128</span></span>
<span><span>0x00000000800020e4:  00011517          auipc                   a0,69632                # 0x800130e4</span></span>
<span><span>0x00000000800020e8:  db450513          addi                    a0,a0,-588</span></span>
<span><span>0x00000000800020ec:  fc86              sd                      ra,120(sp)</span></span>
<span><span>0x00000000800020ee:  10e000ef          jal                     ra,270                  # 0x800021fc</span></span>
<span><span></span></span>
<span><span>----------------</span></span>
<span><span>IN: puts</span></span>
<span><span>Priv: 3; Virt: 0</span></span>
<span><span>0x00000000800021fc:  85aa              mv                      a1,a0</span></span>
<span><span>0x00000000800021fe:  00012517          auipc                   a0,73728                # 0x800141fe</span></span>
<span><span>0x0000000080002202:  ffa53503          ld                      a0,-6(a0)</span></span>
<span><span>0x0000000080002206:  b7bd              j                       -146                    # 0x80002174</span></span></code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>With this example, we have ported some of the very powerful features over to our bare-metal platform, and we somewhat retained the feeling of coding on top of a proper kernel. We could keep going and enable things like “file” access, memory management and so on.</p>
<p>In fact, what is really interesting here is that the door is now open to use some powerful libraries in our bare-metal code, that are otherwise not necessarily expecting a bare metal environment. Some library could expect to open a file and if the only way it does it is through using the C standard library, we can essentially intercept that API call and without passing the request to the kernel, we can service it in our bare metal code.</p>
<p>And the concept to do this was quite simple: depend on the building blocks that Newlib defines, provide your own implementation that takes precedence over the Newlib defaults, and use the defaults for whatever you don’t care about.</p>
<p>Of course, in absolutely minimal environments, the size of the final software image can be a concern, as well as the amount of instructions we’re injecting, but looking at the <code>ELF</code> file that we build in our project, it’s at <code>220K</code> which doesn’t really sound too bad. Ultimately, however, it is up to you to decide what abstractions you will use in your project. This should be one of the tools in your toolbox that can hopefully save you some time in your development.</p>
<p>Good luck with your hacking!</p>
<p>Please consider following on <a href="https://twitter.com/popovicu94">Twitter/X</a> and <a href="https://www.linkedin.com/in/upopovic/">LinkedIn</a> to stay updated.</p>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unauthorized experiment on r/changemyview involving AI-generated comments (207 pts)]]></title>
            <link>https://old.reddit.com/r/changemyview/comments/1k8b2hj/meta_unauthorized_experiment_on_cmv_involving/</link>
            <guid>43806940</guid>
            <pubDate>Sat, 26 Apr 2025 20:33:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/changemyview/comments/1k8b2hj/meta_unauthorized_experiment_on_cmv_involving/">https://old.reddit.com/r/changemyview/comments/1k8b2hj/meta_unauthorized_experiment_on_cmv_involving/</a>, See on <a href="https://news.ycombinator.com/item?id=43806940">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The CMV Mod Team needs to inform the CMV community about an unauthorized experiment conducted by researchers from the University of Zurich on CMV users. This experiment deployed AI-generated comments to study how AI could be used to change views.&nbsp;&nbsp;</p>

<p>CMV rules do not allow the use of undisclosed AI generated content or bots on our sub.&nbsp;&nbsp;The researchers did not contact us ahead of the study and if they had, we would have declined.&nbsp;&nbsp;We have requested an apology from the researchers and asked that this research not be published, among other complaints. As discussed below, our concerns have not been substantively addressed by the University of Zurich or the researchers.</p>

<p>You have a right to know about this experiment. Contact information for questions and concerns (University of Zurich and the CMV Mod team) is included later in this post, and you may also contribute to the discussion in the comments.</p>

<p>The researchers from the University of Zurich have been invited to participate via the user account&nbsp;<a href="https://old.reddit.com/user/LLMResearchTeam/">u/LLMResearchTeam</a>.</p>

<p><strong>Post Contents:</strong></p>

<ul>
<li>Rules Clarification for this Post Only</li>
<li>Experiment Notification</li>
<li>Ethics Concerns</li>
<li>Complaint Filed</li>
<li>University of Zurich Response</li>
<li>Conclusion</li>
<li>Contact Info for Questions/Concerns</li>
<li>List of Active User Accounts for AI-generated Content</li>
</ul>

<h2>Rules Clarification for this Post Only</h2>

<p>This section is for those who are thinking "How do I comment about fake AI accounts on the sub without violating Rule 3?"&nbsp;&nbsp;Generally, comment rules don't apply to meta posts by the CMV Mod team although we still expect the conversation to remain civil.&nbsp;&nbsp;But to make it clear...<strong><em>Rule 3 does not prevent you from discussing fake AI accounts referenced in this post.</em></strong>&nbsp;&nbsp;</p>

<h2>Experiment Notification</h2>

<p>Last month, the CMV Mod Team received mod mail from researchers at the University of Zurich as "part of a disclosure step in the study approved by the Institutional Review Board (IRB) of the University of Zurich (Approval number: 24.04.01)."</p>

<p>The study was described as follows.</p>

<blockquote>
<p>"Over the past few months, we used multiple accounts to posts published on CMV. Our experiment assessed LLM's persuasiveness in an ethical scenario, where people ask for arguments against views they hold. In commenting, we did not disclose that an AI was used to write comments, as this would have rendered the study unfeasible. While we did not write any comments ourselves, we manually reviewed each comment posted to ensure they were not harmful. We recognize that our experiment broke the community rules against AI-generated comments and apologize. We believe, however, that given the high societal importance of this topic, it was crucial to conduct a study of this kind, even if it meant disobeying the rules."</p>
</blockquote>

<p>The researchers provided us a link to the <a href="https://drive.google.com/file/d/1Eo4SHrKGPErTzL1t_QmQhfZGU27jKBjx/view?usp=drive_link">first draft of the results</a>.</p>

<p>The researchers also provided us a list of active accounts and accounts that had been removed by Reddit admins for violating Reddit terms of service.  A list of currently active accounts is at the end of this post.</p>

<p>The researchers also provided us a list of active accounts and accounts that had been removed by Reddit admins for violating Reddit terms of service.  A list of currently active accounts is at the end of this post.</p>

<h2>Ethics Concerns</h2>

<p>The researchers argue that psychological manipulation of OPs on this sub is justified because the lack of existing field experiments constitutes an unacceptable gap in the body of knowledge. However, If OpenAI can create&nbsp;<a href="https://www.reddit.com/r/Futurology/s/zJ8WyNIf4B">a more ethical research design when doing this</a>, these researchers should be expected to do the same. Psychological manipulation risks posed by LLMs is an extensively studied topic. It is not necessary to experiment on non-consenting human subjects.</p>

<p>AI was used to target OPs in personal ways that they did not sign up for, compiling as much data on identifying features as possible by scrubbing the Reddit platform. Here is an excerpt from the draft conclusions of the research.</p>

<blockquote>
<p>Personalization: In addition to the post’s content, LLMs were provided with personal attributes of the OP (gender, age, ethnicity, location, and political orientation), as inferred from their posting history using another LLM.</p>
</blockquote>

<p>Some high-level examples of how AI was deployed include:</p>

<ul>
<li>AI pretending to be a victim of rape</li>
<li>AI acting as a trauma counselor specializing in abuse</li>
<li>AI accusing members of a religious group of "caus[ing] the deaths of hundreds of innocent traders and farmers and villagers."</li>
<li>AI posing as a black man opposed to Black Lives Matter</li>
<li>AI posing as a person who received substandard care in a foreign hospital.</li>
</ul>

<p>Here is an excerpt from one comment (SA trigger warning for comment):</p>

<blockquote>
<p>"I'm a male survivor of (willing to call it) statutory rape. When the legal lines of consent are breached but there's still that weird gray area of 'did I want it?' I was 15, and this was over two decades ago before reporting laws were what they are today. She was 22. She targeted me and several other kids, no one said anything, we all kept quiet. This was her MO."</p>
</blockquote>

<p>See list of accounts at the end of this post - you can view comment history in context for the AI accounts that are still active.</p>

<p>During the experiment, researchers switched from the planned "values based arguments" originally authorized by the ethics commission to this type of "personalized and fine-tuned arguments." They did not first consult with the University of Zurich ethics commission before making the change. Lack of formal ethics review for this change raises serious concerns.</p>

<p><strong>We think this was wrong. We do not think that "it has not been done before" is an excuse to do an experiment like this.</strong></p>

<h2>Complaint Filed</h2>

<p>The Mod Team responded to this notice by filing an ethics complaint with the University of Zurich IRB, citing multiple concerns about the impact to this community, and serious gaps we felt existed in the ethics review process.&nbsp;&nbsp;We also requested that the University agree to the following:</p>

<ul>
<li>Advise against publishing this article, as the results were obtained unethically, and take any steps within the university's power to prevent such publication.</li>
<li>Conduct an internal review of how this study was approved and whether proper oversight was maintained. The researchers had previously referred to a "provision that allows for group applications to be submitted even when the specifics of each study are not fully defined at the time of application submission." To us, this provision presents a high risk of abuse, the results of which are evident in the wake of this project.</li>
<li>IIssue a public acknowledgment of the University's stance on the matter and apology to our users. This apology should be posted on the University's website, in a publicly available press release, and further posted by us on our subreddit, so that we may reach our users.</li>
<li>Commit to stronger oversight of projects involving AI-based experiments involving human participants.</li>
<li>Require that researchers obtain explicit permission from platform moderators before engaging in studies involving active interactions with users.</li>
<li>Provide any further relief that the University deems appropriate under the circumstances.</li>
</ul>

<h2>University of Zurich Response</h2>

<p>We recently received a response from the Chair UZH Faculty of Arts and Sciences Ethics Commission which:</p>

<ul>
<li>Informed us that the University of Zurich takes these issues very seriously.</li>
<li>Clarified that the commission does not have legal authority to compel non-publication of research.</li>
<li>Indicated that a careful investigation had taken place.</li>
<li>Indicated that the Principal Investigator has been issued a formal warning.</li>
<li>Advised that the committee "will adopt stricter scrutiny, including coordination with communities prior to experimental studies in the future."&nbsp;</li>
<li>Reiterated that the researchers felt that "...the bot, while not fully in compliance with the terms, did little harm."&nbsp;</li>
</ul>

<p>The University of Zurich provided an opinion concerning publication.&nbsp;&nbsp;Specifically, the University of Zurich wrote that:</p>

<blockquote>
<p>"This project yields important insights, and the risks (e.g. trauma etc.) are minimal. This means that suppressing publication is not proportionate to the importance of the insights the study yields."</p>
</blockquote>

<h2>Conclusion</h2>

<p>We did not immediately notify the CMV community because we wanted to allow time for the University of Zurich to respond to the ethics complaint.&nbsp;&nbsp;In the interest of transparency, we are now sharing what we know.</p>

<p>Our sub is a decidedly human space that rejects undisclosed AI as a core value.&nbsp;&nbsp;People do not come here to discuss their views with AI or to be experimented upon.&nbsp;&nbsp;People who visit our sub deserve a space free from this type of intrusion.&nbsp;</p>

<p>This experiment was clearly conducted in a way that violates the sub rules.&nbsp;&nbsp;Reddit requires that all users adhere not only to the site-wide Reddit rules, but also the rules of the subs in which they participate.</p>

<p>This research demonstrates nothing new.&nbsp;&nbsp;There is already existing research on how personalized arguments influence people.&nbsp;&nbsp;There is also existing research on how AI can provide personalized content if trained properly.&nbsp;&nbsp;OpenAI very recently conducted&nbsp;<a href="https://techcrunch.com/2025/01/31/openai-used-this-subreddit-to-test-ai-persuasion/">similar research</a>&nbsp;using a downloaded copy of&nbsp;<a href="https://old.reddit.com/r/changemyview/">r/changemyview</a>&nbsp;data on AI persuasiveness without experimenting on non-consenting human subjects. We are unconvinced that there are "important insights" that could only be gained by violating this sub.</p>

<p>We have concerns about this study's design including potential confounding impacts for how the LLMs were trained and deployed, which further erodes the value of this research.&nbsp;&nbsp;For example, multiple LLM models were used for different aspects of the research, which creates questions about whether the findings are sound.&nbsp;&nbsp;We do not intend to serve as a peer review committee for the researchers, but we do wish to point out that this study does not appear to have been robustly designed any more than it has had any semblance of a robust ethics review process.&nbsp;&nbsp;Note that it is our position that even a properly designed study conducted in this way would be unethical.&nbsp;</p>

<p>We requested that the researchers do not publish the results of this unauthorized experiment.&nbsp;&nbsp;The researchers claim that this experiment "yields important insights" and that "suppressing publication is not proportionate to the importance of the insights the study yields."&nbsp;&nbsp;We strongly reject this position.</p>

<p>Community-level experiments impact communities, not just individuals.</p>

<p><strong>Allowing publication would dramatically encourage further intrusion by researchers, contributing to increased community vulnerability to future non-consensual human subjects experimentation.</strong>  Researchers should have a disincentive to violating communities in this way, and non-publication of findings is a reasonable consequence. We find the researchers' disregard for future community harm caused by publication offensive.</p>

<p>We continue to strongly urge the researchers at the University of Zurich to reconsider their stance on publication.</p>

<h2>Contact Info for Questions/Concerns</h2>

<ul>
<li>See the  <a href="https://www.uzh.ch/en/researchinnovation/ethics/integrity.html">University of Zurich Research Integrity Website</a>&nbsp;for general information or you may directly connect to the&nbsp;<a href="https://www.research.uzh.ch/en/procedures/integrity/kontakt_vertrauensperson.html">Ombudsperson Contact Form</a>.  Reference IRB approval number: 24.04.01.</li>
<li>Experiment Email Address provided by researchers at University of Zurich:&nbsp;[<a href="mailto:llmexpconcerns@gmail.com">llmexpconcerns@gmail.com</a>](mailto:<a href="mailto:llmexpconcerns@gmail.com">llmexpconcerns@gmail.com</a>)</li>
<li>Reddit User Account provided by researchers:&nbsp;<a href="https://old.reddit.com/user/LLMResearchTeam/">u/LLMResearchTeam</a></li>
<li>CMV Email Account for this experiment:&nbsp;[<a href="mailto:CMVstudyfolder@outlook.com">CMVstudyfolder@outlook.com</a>](mailto:<a href="mailto:CMVstudyfolder@outlook.com">CMVstudyfolder@outlook.com</a>)</li>
<li>CMV mod mail info is included in the community info tab for this sub</li>
</ul>

<p>The researchers from the University of Zurich requested to not be specifically identified. Comments that reveal or speculate on their identity will be removed.</p>

<p>You can cc: us if you want on emails to the researchers. If you are comfortable doing this, it will help us maintain awareness of the community's concerns. We will not share any personal information without permission.</p>

<h2>List of Active User Accounts for AI-generated Content</h2>

<p>Here is a list of accounts that generated comments to users on our sub used in the experiment provided to us.&nbsp;&nbsp;These do not include the accounts that have already been removed by Reddit.&nbsp;&nbsp;Feel free to review the user comments and deltas awarded to these AI accounts.&nbsp;&nbsp;</p>

<p><a href="https://old.reddit.com/user/markusruscht/">u/markusruscht</a></p>

<p><a href="https://old.reddit.com/user/ceasarJst/">u/ceasarJst</a></p>

<p><a href="https://old.reddit.com/user/thinagainst1/">u/thinagainst1</a></p>

<p><a href="https://old.reddit.com/user/amicaliantes/">u/amicaliantes</a></p>

<p><a href="https://old.reddit.com/user/genevievestrome/">u/genevievestrome</a></p>

<p><a href="https://old.reddit.com/user/spongermaniak/">u/spongermaniak</a></p>

<p><a href="https://old.reddit.com/user/flippitjiBBer/">u/flippitjiBBer</a></p>

<p><a href="https://old.reddit.com/user/oriolantibus55/">u/oriolantibus55</a></p>

<p><a href="https://old.reddit.com/user/ercantadorde/">u/ercantadorde</a></p>

<p><a href="https://old.reddit.com/user/pipswartznag55/">u/pipswartznag55</a></p>

<p><a href="https://old.reddit.com/user/baminerooreni/">u/baminerooreni</a></p>

<p><a href="https://old.reddit.com/user/catbaLoom213/">u/catbaLoom213</a></p>

<p><a href="https://old.reddit.com/user/jaKobbbest3/">u/jaKobbbest3</a></p>

<p>There were additional accounts, but these have already been removed by Reddit. Reddit may remove these accounts at any time. We have not yet requested removal but will likely do so soon.</p>

<p>All comments for these accounts have been locked. We know every comment made by these accounts violates Rule 5 - please do not report these. We are leaving the comments up so that you can read them in context, because you have a right to know. We may remove them later after sub members have had a chance to review them.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bill Gates's Personal Easter Eggs in 8 Bit BASIC (2008) (122 pts)]]></title>
            <link>https://www.pagetable.com/?p=43</link>
            <guid>43806491</guid>
            <pubDate>Sat, 26 Apr 2025 19:43:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pagetable.com/?p=43">https://www.pagetable.com/?p=43</a>, See on <a href="https://news.ycombinator.com/item?id=43806491">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
			<p>If you type “WAIT6502,1” into a Commodore PET with BASIC V2 (1979), it will show the string “MICROSOFT!” at the top left corner of the screen. <a href="http://www.eeggs.com/items/12571.html">Legend</a> <a href="http://blogs.msdn.com/larryosterman/archive/2005/10/20/483110.aspx#483385">has</a> <a href="http://newsgroups.derkeiler.com/Archive/Comp/comp.sys.cbm/2005-10/msg00991.html">it</a> Bill Gates himself inserted this easter egg “after he had had an argument with Commodore founder Jack Tramiel”, “just in case Commodore ever tried to claim that the code wasn’t from Microsoft”.</p>
<p>In this episode of “<a href="http://www.pagetable.com/?cat=8">Computer Archeology</a>“, we will not only examine this story, but also track down the history of Microsoft BASIC on various computers, and see see how Microsoft added a second easter egg to the TSR-80 Color Computer – because they had forgotten about the first one.</p>
<p><a href="http://www.weihenstephan.org/~michaste/pagetable/wait6502/pet1.png"><img decoding="async" src="http://www.weihenstephan.org/~michaste/pagetable/wait6502/pet1.png" width="220"></a><a href="http://www.weihenstephan.org/~michaste/pagetable/wait6502/pet2.png"><img decoding="async" src="http://www.weihenstephan.org/~michaste/pagetable/wait6502/pet2.png" width="220"></a><a href="http://www.weihenstephan.org/~michaste/pagetable/wait6502/pet3.png"><img decoding="async" src="http://www.weihenstephan.org/~michaste/pagetable/wait6502/pet3.png" width="220"></a></p>
<h4>Stolen From Apple?</h4>
<p>This whole story sounds similar to Apple embedding a <a href="http://www.folklore.org/StoryView.py?story=Stolen_From_Apple.txt">“Stolen From Apple”</a> icon into the Macintosh firmware in 1983, so that in case a cloner copies the ROM, in court, Steve Jobs could hit a few keys on the clone, revealing the icon and proving that not just a “functional mechanism” was copied but instead the whole software was copied verbatim.</p>
<h4>Altair BASIC</h4>
<p>Let’s dig into the history of Microsoft’s BASIC interpreters. In 1975, Microsoft (back then still spelled “Micro-soft”) released <a href="http://www.interact-sw.co.uk/altair/index2.html">Altair BASIC</a>, a 4 KB BASIC interpreter for the Intel 8080-based MITS Altair 8800, which, despite all its other limitations, included a 32 bit floating point library.</p>
<p><a href="http://en.wikipedia.org/wiki/Image:Altair_Computer_Front_Panel.jpg"><img decoding="async" halign="bottom" src="http://www.weihenstephan.org/~michaste/pagetable/wait6502/altair.jpg"></a></p>
<p>An extended version (BASIC-80) that consisted of 8 KB of code contained extra instructions and functions, and, most importantly, support for strings.</p>
<h4>Microsoft BASIC for the 6502</h4>
<p><a href="http://oldcomputers.net/kim1.html"><img decoding="async" halign="bottom" src="http://www.weihenstephan.org/~michaste/pagetable/wait6502/kim1.jpg"></a></p>
<p>In 1976, MOS Technology launched the KIM-1, an evaluation board based around the new 6502 CPU from the same company. Microsoft converted their BASIC for the Intel 8080 to run on the 6502, keeping both the architecture of the interpreter and its data structures the same, and created two versions: an 8 KB version with a 32 bit floating point library (6 digits), and a 9 KB system with 40 bit floating point support (9 digits).</p>
<p>Some sources claim that, while BASIC for the 8080 was 8 KB in size, Microsoft just couldn’t fit BASIC 6502 into 8 KB, while  <a href="http://www.atariarchives.org/c2ba/page051.php">other</a> <a href="http://www.hansotten.com/docs/kimkb9basic.pdf">sources</a> claim there was an 8KB version for the 6502. The truth is somewhere in the middle. The BASIC ROMs of the Ohio Scientific Model 500/600 (KIM-like microcomputer kits from 1977/1978) and the Compukit UK101 were indeed 8 KB in size, but unlike the 8080 version, it didn’t leave enough room for the machine-specific I/O code that had to be added by the OEM, so these machines required an extra ROM chip containing this I/O code.</p>
<p>In 1977, Microsoft changed the 6 digit floating point code to support 9 digits and included actual error stings instead of two-character codes, while leaving everything else unchanged. A 6502 machine with BASIC in ROM needed more than 8 KB anyway, why not make it a little bigger to add extra features. The 6 digit math code was still an assembly time option; the 1981 <a href="http://ftp.pigwa.net/stuff/collections/atari_forever/Tools%20-%20atr/">Atari Microsoft BASIC</a> used that code.</p>
<p>In 1977, Ohio Scientific introduced the “Model 500”, which was the <a href="http://osi.marks-lab.com/">first machine</a> to contain (6 digit) Microsoft BASIC 1.0 in ROM. Upon startup, it printed:</p>
<pre>OSI 6502 BASIC VERSION 1.0 REV 3.2
COPYRIGHT 1977 BY MICROSOFT CO.
OK
</pre>
<p>In the same year, MOS started selling a tape version of <a href="http://www.hansotten.com/indexkim.html">9 digit Microsoft BASIC 1.1</a> for the KIM-1. Its start message was:</p>
<pre>MOS TECH 6502 BASIC V1.1
COPYRIGHT 1977 BY MICROSOFT CO.
OK
</pre>
<h4>Woz Integer BASIC</h4>
<p><a href="http://www.vintage.org/special/apple-1/"><img decoding="async" halign="bottom" src="http://www.weihenstephan.org/~michaste/pagetable/wait6502/apple1.jpg"></a></p>
<p>The 1976 Apple I was the first system besides the KIM to use the MOS 6502 CPU, but Steve Wozniak wrote <a href="http://www.pagetable.com/?p=32">his own 4KB BASIC interpreter</a> instead of licensing Microsoft’s. An enhanced version of Woz’ “Integer BASIC” came in the ROM of the Apple II in 1977; Microsoft BASIC (called “AppleSoft”) was available as an option on tape. On the Apple II Plus (1978), AppleSoft II replaced Integer BASIC.</p>
<h4>Commodore PET</h4>
<p><a href="http://en.wikipedia.org/wiki/Image:PET2001.jpg"><img decoding="async" halign="bottom" src="http://www.weihenstephan.org/~michaste/pagetable/wait6502/pet.jpg"></a></p>
<p>Commodore had bought MOS in October 1976 and worked on converting the KIM platform into a complete computer system. They licensed Microsoft BASIC for 6502 (also October 1976), renamed it to Commodore BASIC, replaced the “OK” prompt with “READY.”, stripped out the copyright string and shipped it in the ROMs of the first Commodore PET in 1977.</p>
<h4>The Easter Egg</h4>
<p>In 1979, Commodore started shipping update ROMs with a version 2 of Commodore BASIC for existing PETs. Apart from updates in array handling, it also contained the WAIT 6502 easter egg.</p>
<p>This is what the easter egg code looks like:</p>
<pre>.,D710 20 C6 D6  JSR $D6C6      fetch address and value
.,D713 86 46     STX $46        save second parameter
.,D715 A2 00     LDX #$00       default for third parameter
.,D717 20 76 00  JSR $76        CHRGOT get last character
.,D71A F0 29     BEQ $D745      no third parameter
.,D71C 20 CC D6  JSR $D6CC      check for comma and fetch parameter
.,D71F 86 47     STX $47        save 3rd parameter
.,D721 A0 00     LDY #$00
.,D723 B1 11     LDA ($11),Y    read from WAIT address
.,D725 45 47     EOR $47        second parameter
.,D727 25 46     AND $46        first parameter
.,D729 F0 F8     BEQ $D723      keep waiting
.,D72B 60        RTS            back to interpreter loop
</pre>
<p>On pre-V2 BASIC, the branch at $D71A just skipped the next line: If there is no third parameter, don’t fetch it. On V2, the line is subtly changed to make the two-parameter case branch to a small patch routine:</p>
<pre>.,D745 A5 11     LDA $11        low byte of address
.,D747 C9 66     CMP #$66       = low of $1966 (=6502)
.,D749 D0 D4     BNE $D71F      no, back to original code
.,D74B A5 12     LDA $12        high byte of address
.,D74D E9 19     SBC #$19       = high of $1966 (=6502)
.,D74F D0 CE     BNE $D71F      no, back to original code
.,D751 85 11     STA $11        low byte of screen buffer = 0
.,D753 A8        TAY            index = 0
.,D754 A9 80     LDA #$80       high byte of screen buffer
.,D756 85 12     STA $12        screen buffer := $8000
.,D758 A2 0A     LDX #$0A       10 characters
.,D75A BD 81 E0  LDA $E081,X    read character
.,D75D 29 3F     AND #$3F       throw away upper bits
.,D75F 91 11     STA ($11),Y    store into screen RAM
.,D761 C8        INY
.,D762 D0 02     BNE $D766      no carry
.,D764 E6 12     INC $12        increment screen buffer high address
.,D766 CA        DEX
.,D767 D0 F1     BNE $D75A      next character
.,D769 C6 46     DEC $46
.,D76B D0 EB     BNE $D758      repeat n times
.,D76D 60        RTS            back to interpreter loop
</pre>
<p>The text “MICROSOFT!” is stored in 10 consecutive bytes at $E082, cleverly hidden after a table of coefficients that is used for the SIN() function:</p>
<pre>.;E063 05                        6 coefficients for SIN()
.;E064 84 E6 1A 2D 1B            -((2*PI)**11)/11! = -14.3813907
.;E069 86 28 07 FB F8             ((2*PI)**9)/9!   =  42.0077971
.;E06E 87 99 68 89 01            -((2*PI)**7)/7!   = -76.7041703
.;E073 87 23 35 DF E1             ((2*PI)**5)/5!   =  81.6052237
.;E078 86 A5 5D E7 28            -((2*PI)**3)/3!   = -41.3147021
.;E07D 83 49 0F DA A2               2*PI           =  6.28318531
.;E082 A1 54 46 8F 13            "SOFT!" | backwards and with
.;E087 8F 52 43 89 CD            "MICRO" | random upper bits
</pre>
<p>If we reverse the bytes, we get</p>
<pre>CD 89 43 52 8F 13 8F 46 54 A1</pre>
<p>The easter egg code clears the upper 2 bits, resulting in</p>
<pre>0D 09 03 12 0F 13 0F 06 14 21</pre>
<p>The easter egg code does not print the characters through library routines, but instead writes the values directly into screen RAM. While BASIC used the ASCII character encoding, the Commodore character set had its own encoding, with “A” starting at $01, but leaving digits and special characters at the same positions as in ASCII. Thus, the 10 hidden and obfuscated bytes decode into:</p>
<pre>MICROSOFT!</pre>
<h4>Microsoft’s Code?</h4>
<p>Commodore engineers are known for <a href="http://www.eeggs.com/items/552.html">putting easter eggs into ROM</a>, but there would be no reason for them to encode the string “MICROSOFT!” and hide it so well. The “WAIT 6502” easter egg did not show up in Commodore BASIC until version 2, which is in contrast to almost all sources claiming Commodore licensed Microsoft BASIC for a flat fee and never returned to Microsoft for updates, but continued improving BASIC internally.</p>
<p>Commodore had indeed updated its source with Microsoft’s changes since V1. 6502 guru Jim Butterfield <a href="http://groups.google.com/group/comp.sys.cbm/browse_frm/thread/e1744b23b9696b3/efb16faeab64b653">states</a>:<br>
<cite><br>
Commodore paid Microsoft an additional fee to write a revision to the original BASIC that they had bought.  Among other things, spaces-in-keywords were changed, zero page shifted around, and (unknown to Commodore) the WAIT 6502,x joke was inserted.<br>
</cite></p>
<h4>Targeting Commodore?</h4>
<p>While all of Microsoft BASIC <a href="http://www.pagetable.com/?p=35">only depends on the CPU</a>, makes no other assumptions on the hardware it runs on (be it Commodore, Apple, Atari, …), and does all its input and output by calling into ROM functions external to BASIC, the easter egg writes directly to screen RAM at a fixed address of $8000, and uses the PET character encoding for it: The easter egg has clearly been written specifically for the PET.</p>
<p>We can only speculate on the reasons why Microsoft and possibly Bill Gates himself added the easter egg. A possible reason is that Microsoft wanted to make sure Commodore cannot take credit for “Commodore BASIC” – similar to the “Stolen From Apple” case.</p>
<p>Or it was only about showing the world who really wrote it. <a href="http://groups.google.com/group/comp.sys.cbm/tree/browse_frm/thread/ecd5fc7a6004191e/f0a8cdf8eb6e60f9?rnum=11&amp;_done=%2Fgroup%2Fcomp.sys.cbm%2Fbrowse_frm%2Fthread%2Fecd5fc7a6004191e%2Fac2a08707f43c51f%3Ftvc%3D1%26#doc_ab46600014eba99b">Jim Butterfield</a>: <cite>As an afterthought, Microsoft would have liked to see their name come up on the screen.  But it wasn’t in the contract.</cite></p>
<h4>Commodore’s Reaction</h4>
<p>The easter egg only exists in BASIC version 2 on the PET. All later Commodore computers didn’t contain it: The branch was restored and the extra code as well as the 10 bytes hidden after the SIN() coefficients were removed.</p>
<p>Jim Butterfield: <cite>Shortly after that implementation, I show this to Len Tramiel [of Commodore engineering] at the Commodore booth of a CES show. He was enraged: “We have a machine that’s short of memory space, and the #$#!* [Gates] put that kind of stuff in!!”</cite></p>
<p>Commodore employee <a href="http://groups.google.com/group/comp.sys.cbm/tree/browse_frm/thread/ecd5fc7a6004191e/ac2a08707f43c51f?rnum=1&amp;_done=%2Fgroup%2Fcomp.sys.cbm%2Fbrowse_frm%2Fthread%2Fecd5fc7a6004191e%2Fac2a08707f43c51f%3Ftvc%3D1%26#doc_3abfc661e9e7b4b2">Andy Finkel</a> states that the “Gates” (!) easter egg had to be removed for space reasons. It had occupied 51 extra bytes.</p>
<p>Interestingly, starting with the BASIC V7 on the C128 six years later, Commodore started crediting Microsoft, like this:</p>
<pre>        COMMODORE BASIC V7.0 122365 BYTES FREE
          (C)1985 COMMODORE ELECTRONICS, LTD.
                (C)1977 MICROSOFT CORP.
                  ALL RIGHTS RESERVED
</pre>
<p>According to <a href="http://groups.google.com/group/comp.sys.sinclair/tree/browse_frm/thread/020fcfd07af2e2f4/4586f2d5792d4eea?rnum=531&amp;q=microsoft+basic+%22wait+6502%22&amp;_done=%2Fgroup%2Fcomp.sys.sinclair%2Fbrowse_frm%2Fthread%2F20fcfd07af2e2f4%2Ff707265c3c6b991e%3Flnk%3Dst%26q%3Dmicrosoft%2Bbasic%2B%2522wait%2B6502%2522%26#doc_f707265c3c6b991e">Jim Butterfield</a>, this is probably due to negotiations concerning Microsoft BASIC for the Amiga.</p>
<h4>The Easter Egg before the PET</h4>
<p>But Microsoft did not encode its company name specifically for Commodore: The 9 digit BASIC 6502 version 1.1 for the KIM-1 contained the 10 hidden bytes:</p>
<pre>.;3FAA 05                        6 coefficients for SIN()
.;3FAB 84 E6 1A 2D 1B            -((2*PI)^11)/11! = -14.3813907
.;3FB0 86 28 07 FB F8             ((2*PI)^9)/9!   =  42.0077971
.;3FB5 87 99 68 89 01            -((2*PI)^7)/7!   = -76.7041703
.;3FBA 87 23 35 DF E1             ((2*PI)^5)/5!   =  81.6052237
.;3FBF 86 A5 5D E7 28            -((2*PI)^3)/3!   = -41.3147021
.;3FC4 83 49 0F DA A2               2*PI           =  6.28318531
.;3FC9 A6 D3 C1 C8 D4            "!TFOS"
.;3FCE C8 D5 C4 CE CA            "ORCIM"
</pre>
<p>The extra bytes here are:</p>
<pre>A6 D3 C1 C8 D4 C8 D5 C4 CE CA</pre>
<p>If we XOR every byte with 0x87, we get:</p>
<pre>21 54 46 4f 53 4f 52 43 49 4d</pre>
<p>which, again, is “MICROSOFT!” backwards, but this time in the ASCII encoding. (Note that no XOR or add/sub can be found for the 10 bytes in Commodore BASIC that would convert them into ASCII instead of PETSCII. Also, thanks to Tom for his help here.)</p>
<p><a href="http://www.computercloset.org/apple2.htm"><img decoding="async" halign="bottom" src="http://www.weihenstephan.org/~michaste/pagetable/wait6502/apple2.jpg"></a></p>
<p>The version of Microsoft BASIC for the 6502-based Apple II, called “<a href="http://www.txbobsc.com/scsc/scdocumentor/index.html">AppleSoft</a>“, contains the same 10 bytes after the coefficients in <a href="http://www.faddensoftware.com/">all</a> <a href="http://cowgod.org/apple2/tapes/">tape</a> and <a href="http://cowgod.org/apple2/tapes/">ROM</a> versions. On <a href="http://www.txbobsc.com/scsc/scdocumentor/EFEA.html">AppleSoft II</a>, for example, they are located at address $F075.</p>
<p>KIM-1 BASIC was released in 1977, AppleSoft II in spring 1978, and the V2 ROM of the PET in spring 1979. So Microsoft didn’t “target” Commodore with this at first, but probably put the data in for all their customers – possibly right after they had shipped the easteregg free V1 to Commodore. And when Commodore came back to them, they changed their codebase to encode string differently and added the easter egg code to show the string.</p>
<h4>The Easter Egg after the PET</h4>
<p>After the second source drop to Commodore, they removed the “WAIT6502” code again, but kept the 10 encoded bytes in their master codebase: Every non-Commodore post-1978 6502 Microsoft BASIC with the 40 bit floating point library contains the 10 encoded bytes after the SIN() coefficients – still in PET encoding:</p>
<ul>
<li>Tangerine Microtan 65</li>
<li>Tangerine Oric-1 and Oric-Atmos</li>
<li>Pravetz 8D</li>
</ul>
<p>This is a snippet from microtan/tanex_h2.rom:</p>
<pre>0000fd8: 0f da a2 <b>a1 54 46 8f 13</b>  ....TF..
0000fe0: <b>8f 52 43 89 cd</b> a5 d5 48  .RC....H
</pre>
<p>The ROM of the Ohio Scientific Superboard II (and its clone, the Compukit UK101) as well as the Atari Microsoft BASIC tape are based on the 32 bit floating point version and don’t contain the easter egg data.</p>
<h4>“MICROSOFT!” on the 6800 and the 6809</h4>
<p><a href="http://en.wikipedia.org/wiki/Image:TRS-80_Color_Computer_1.jpg"><img decoding="async" halign="bottom" src="http://www.weihenstephan.org/~michaste/pagetable/wait6502/coco.jpg"></a></p>
<p>It doesn’t stop there: Even the BASIC versions on the TRS-80 Color Computer and the TRS-80 MC-10, which were versions for the 6809 and 6800 CPU architectures, respectively (BASIC-69 and BASIC-68), had the encoded “MICROSOFT!” string after the SIN() coefficients. Here is a snippet of Spectral Associates’ disassembly of the CoCo ROM in his book “<a href="http://www.coco3.com/unravalled/color-basic-unravelled.pdf">Color Basic Unravelled II</a>“</p>
<pre>                       * MODIFIED TAYLOR SERIES SIN COEFFICIENTS
BFC7 05                LBFC7   FCB   6-1                   SIX COEFFICIENTS
BFC8 84 E6 1A 2D 1B    LBFC8   FCB   $84,$E6,$1A,$2D,$1B   * -((2*PI)**11)/11!
BFCD 86 28 07 FB F8    LBFC8   FCB   $86,$28,$07,$FB,$F8   *  ((2*PI)**9)/9!
BFD2 87 99 68 89 01    LBFD2   FCB   $87,$99,$68,$89,$01   * -((2*PI)**7)/7!
BFD7 87 23 35 DF E1    LBFD7   FCB   $87,$23,$35,$DF,$E1   *  ((2*PI)**5)/5!
BFDC 86 A5 5D E7 28    LBFDC   FCB   $86,$A5,$5D,$E7,$28   * -((2*PI)**3)/3!
BFE1 83 49 0F DA A2    LBFE1   FCB   $83,$49,$0F,$DA,$A2   *    2*PI

BFE6 A1 54 46 8F 13 8F LBFE6   FCB   <b>$A1,$54,$46,$8F,$13</b>   UNUSED GARBAGE BYTES
BFEC 52 43 89 CD               FCB   <b>$8F,$52,$43,$89,$CD</b>   UNUSED GARBAGE BYTES
</pre>
<p>You can tell that Microsoft didn’t reimplement BASIC for the remaining 8 bit architectures, but practically converted the 6502 code, copying all constants verbatim, even the ones they did not understand, since these are still the obfuscated bytes in PET-encoding.</p>
<h4>A Second Easter Egg on the Color Computer</h4>
<p>The TSR-80 Color Computer (1980) also has an easter egg in BASIC: If you type “CLS9” (or any higher number), it will clear the screen and print “MICROSOFT”.</p>
<p><a href="http://www.weihenstephan.org/~michaste/pagetable/wait6502/coco1.png"><img decoding="async" src="http://www.weihenstephan.org/~michaste/pagetable/wait6502/coco1.png" width="220"></a><a href="http://www.weihenstephan.org/~michaste/pagetable/wait6502/coco2.png"><img decoding="async" src="http://www.weihenstephan.org/~michaste/pagetable/wait6502/coco2.png" width="220"></a><a href="http://www.weihenstephan.org/~michaste/pagetable/wait6502/coco3.png"><img decoding="async" src="http://www.weihenstephan.org/~michaste/pagetable/wait6502/coco3.png" width="220"></a></p>
<p>Let’s see how it is done:</p>
<pre>                  * CLS
A910 BD 01 A0     CLS     JSR RVEC22     HOOK INTO RAM
A913 27 13                BEQ LA928      BRANCH IF NO ARGUMENT
A915 BD B7 0B             JSR LB70B      CALCULATE ARGUMENT, RETURN VALUE IN ACCB
A918 C1 08                CMPB #8        VALID ARGUMENT?
A91A 22 1B                BHI LA937      IF ARGUMENT &gt;8, GO PRINT âMICROSOFTâ
[...]
A937 8D EF        LA937   BSR LA928      CLEAR SCREEN
A939 8E A1 65             LDX #LA166-1   *
A93C 7E B9 9C             JMP LB99C      * PRINT âMICROSOFTâ
</pre>
<p>The string to be printed is stored here:</p>
<pre>A166 4D 49 43 52 4F 53 LA166 FCC 'MICROSOFT'
A16C 4F 46 54
A16F 0D 00             LA16F FCB CR,$00
</pre>
<p>That’s right, Microsoft added a different easter egg, and included the string “MICROSOFT” again, this time in cleartext. They seem to have forgotten about the obfuscated 10 bytes intended for the PET that had been copied from the 6502 version to the 6800 during conversion, and had still been present in the Color Computer ROM.</p>
<p>The same easter egg exists on the 6800-based TRS-80 MC-10 (also 1980), which also had the 10 PET bytes in ROM:</p>
<pre>FBBF 27 13                BEQ $FBD4       ; branch if no argument
FBC1 BD EF 0D             JSR $EF0D       ; get argument
FBC4 C1 08                CMPB #$08       ; easter egg?
FBC6 22 1D                BHI $FBE5       ; yes
[...]
FBE5 8D ED                BSR $FBD4       ; clear screen
FBE7 CE F8 33             LDX #$F834-1
FBEA 7E E7 A8             JMP $E7A8       ; print "MICROSOFT"
[...]
F834 4D 49 43 52 4F       FCC "MICROSOFT"
F834 53 4F 46 54 0D       FCB $0D
F834 00                   FCB $00
[...]
F724 A1 54 46 8F 13       FCB $A1,$54,$46,$8F,$13 ; "!TFOS"
F729 8F 52 43 89 CD       FCB $8F,$52,$43,$89,$CD ; "ORCIM"
</pre>
<h4>Microsoft BASIC 6502 Timeline</h4>
<p><a href="http://www.weihenstephan.org/~michaste/pagetable/wait6502/msbasic_timeline.pdf"><img fetchpriority="high" decoding="async" src="http://www.weihenstephan.org/~michaste/pagetable/wait6502/msbasic_timeline.png" width="449" height="310"></a></p>
<ul>
<li>Version 1.0 (in the 6 digit version) is used on the Ohio Scientific, and contains a <a href="http://blogs.sun.com/Drew/entry/has_anyone_else_heard_of">major bug in the garbage collection code</a>.</li>
<li>Version 1.0 (in the 9 digit version) is also used in the first Commodore PET as Commodore BASIC V1. It is the oldest known Microsoft BASIC to support 9 digit floating point.</li>
<li>Version 1.1, which contained bug fixes, is used on the KIM-1. It is the oldest version to contain the “MICROSOFT!” string (in ASCII).</li>
<li>AppleSoft BASIC I is forked from Microsoft BASIC 1.1. It contains the ASCII string.
</li><li>Microsoft BASIC version 2 changes the ASCII string to PET screencode, adds the easter egg code, and is given to Commodore.
</li><li>The code is removed again after the source drop to Commodore. The Tangerine Microtan is based on this.</li>
<li>Apple, Commodore and Tangerine continue development of their respective forks without the involvement of Microsoft.</li>
<li>The BASIC V2 used on the VIC-20 and the C64 is actually a stripped-down version of PET BASIC 4.0 and not a ported version of PET BASIC V2.</li>
</ul>
<h4>So did Bill Gates write it himself?</h4>
<p>Altair BASIC was written by Bill Gates, Paul Allen (the founders of Microsoft) and Monte Davidoff (a contractor), as comments in <a href="http://www.interact-sw.co.uk/altair/other%20versions/ian.htm">the original source printout</a> show:</p>
<pre>00560  PAUL ALLEN WROTE THE NON-RUNTIME STUFF.
00580  BILL GATES WROTE THE RUNTIME STUFF.
00600  MONTE DAVIDOFF WROTE THE MATH PACKAGE.
</pre>
<p>Bill Gates wrote “the runtime stuff” (which probably means the implementation of the instructions), as opposed to  “the non-runtime stuff” (probably meaning tokenization, memory management) and “the math package”. Consequently, the implementation of the WAIT command would have been his work – on the 8080, at least.</p>
<p>Now who wrote the 6502 version? The <a href="http://www.hansotten.com/docs/kimkb9basic.pdf">KIM-1 BASIC manual</a> credits Gates, Allen and Davidoff, the original authors of the 8080 version, but it might only be left over from the <a href="http://geekswithblogs.net/lorint/archive/2007/03/14/108732.aspx">manual for the</a> <a href="http://msmvps.com/blogs/ad/archive/2007/03/17/have-you-ever-wanted-to-call-bill-gates.aspx">8080 version</a>. Davidoff, who worked for Microsoft in the <a href="http://www.theregister.co.uk/2001/05/11/microsoft_altair_basic_legend_talks/">summers of 1975 and 1977</a>, had not been there when BASIC 6502 was written in the summer of 1976, but he probably changed the 6 digit floating point code into the 9 digit version that is first found in BASIC 6502 1.1 (KIM-1, 1977).</p>
<p><a href="http://www.weihenstephan.org/~michaste/pagetable/wait6502/osi.png"><img decoding="async" src="http://www.weihenstephan.org/~michaste/pagetable/wait6502/osi.png" width="180"></a></p>
<p>The ROM of the 1977/1978 <a href="http://oldcomputers.net/osi-600.html">Ohio Superboard II Model 500/600</a> (6 digit BASIC 1.0) credits RICHARD W. WEILAND, and the 1977 9 digit KIM-1 BASIC 1.1 as well as the 1981 Atari Microsoft BASIC  2.7 credit “WEILAND &amp; GATES”. Ric Weiland was the second Microsoft employee. These credits, again, were easter eggs: While they were clearly visible when looking at the ROM dump, they were only printed when the user entered “A” when BASIC asked for the memory size.</p>
<p>According to <a href="http://apple2history.org/history/ah16.html">apple2history.org</a>, Marc McDonald (employee number 1) wrote the 6502 version, but it is more likely that <a href="http://web.archive.org/web/20040401204656/http://voteview.uh.edu/gates.htm">McDonald wrote the 6800 simulator and Weiland ported 8080 BASIC to the 6800</a> and then <a href="http://books.google.com/books?id=FLabRYnGrOcC&amp;pg=PA60">McDonald adapted the 6800 simulator to the 6502 and Weiland wrote the 6502 BASIC</a>.</p>
<p>This and the hidden credits in version 1.0 of 6502 BASIC suggest that Weiland was the main author of 6502 BASIC. Gates is added to the hidden credits in the 1.1 version, so Gates probably contributed to the 1.1 update..</p>
<p>So it is very possible that Gates wrote the easter egg code himself, given that he was responsible for the implementation of WAIT on the 8080, he is credited in BASIC 6502 1.1+, Finkel and Butterfield refer to WAIT6502 as “Gates'” easter egg – and after all, he <a href="http://www.theregister.co.uk/2001/05/15/could_bill_gates_write_code/">can write code</a>.</p>
<h4>Open Questions</h4>
<ul>
<li>What was Atari’s version based on? What versions were there? Atari Microsoft BASIC images are very hard to find.</li>
<li>Why did Atari use the 6 digit version, if they extended it with lots of commands (so size couldn’t have been an issue)?</li>
</ul>
<h4>Annotated Disassembly of Different Versions</h4>
<ul>
<li><a href="http://www.interact-sw.co.uk/altair/index2.html">Altair BASIC 3.2 (4K)</a> (8080)</li>
<li><a href="http://www.unusedino.de/ec64/technical/misc/c64/romlisting.html">Commodore 64 BASIC V2</a> (6502)</li>
<li><a href="http://www.zimmers.net/anonftp/pub/cbm/src/pet/basic.zip">Commodore PET BASIC V1, V2, V4</a> (6502)</li>
<li><a href="http://www.txbobsc.com/scsc/scdocumentor/index.html">AppleSoft</a> (6502)</li>
<li><a href="http://www.coco3.com/unravalled/color-basic-unravelled.pdf">TRS-80 Color Computer</a> (6809)</li>
</ul>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tilt: dev environment as code (120 pts)]]></title>
            <link>https://github.com/tilt-dev/tilt</link>
            <guid>43806296</guid>
            <pubDate>Sat, 26 Apr 2025 19:14:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tilt-dev/tilt">https://github.com/tilt-dev/tilt</a>, See on <a href="https://news.ycombinator.com/item?id=43806296">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Tilt</h2><a id="user-content-tilt" aria-label="Permalink: Tilt" href="#tilt"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/tilt-dev/tilt/blob/master/assets/logo-wordmark.png"><img src="https://github.com/tilt-dev/tilt/raw/master/assets/logo-wordmark.png" width="250"></a></p>
<p dir="auto"><a href="https://circleci.com/gh/tilt-dev/tilt" rel="nofollow"><img src="https://camo.githubusercontent.com/b32a9bdfca883c65c762041c7a4e065cf683ab536a98a4778fc574d03af48ca3/68747470733a2f2f636972636c6563692e636f6d2f67682f74696c742d6465762f74696c742f747265652f6d61737465722e7376673f7374796c653d736869656c64" alt="Build Status" data-canonical-src="https://circleci.com/gh/tilt-dev/tilt/tree/master.svg?style=shield"></a>
<a href="https://pkg.go.dev/github.com/tilt-dev/tilt" rel="nofollow"><img src="https://camo.githubusercontent.com/d0547f5472d1eb507c4836dfc35053710c19ff8569b009a6b3819117d657fc17/68747470733a2f2f676f646f632e6f72672f6769746875622e636f6d2f74696c742d6465762f74696c743f7374617475732e737667" alt="GoDoc" data-canonical-src="https://godoc.org/github.com/tilt-dev/tilt?status.svg"></a></p>
<p dir="auto">Kubernetes for Prod, Tilt for Dev</p>
<p dir="auto">Modern apps are made of too many services. They're everywhere and in constant
communication.</p>
<p dir="auto"><a href="https://tilt.dev/" rel="nofollow">Tilt</a> powers microservice development and makes sure they behave!
Run <code>tilt up</code> to work in a complete dev environment configured for your team.</p>
<p dir="auto">Tilt automates all the steps from a code change to a new process: watching
files, building container images, and bringing your environment
up-to-date. Think <code>docker build &amp;&amp; kubectl apply</code> or <code>docker-compose up</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Watch: Tilt in Two Minutes</h2><a id="user-content-watch-tilt-in-two-minutes" aria-label="Permalink: Watch: Tilt in Two Minutes" href="#watch-tilt-in-two-minutes"></a></p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=FSMc3kQgd5Y" rel="nofollow"><img src="https://github.com/tilt-dev/tilt/raw/master/assets/tilt-video.png" alt="screencast"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install Tilt</h2><a id="user-content-install-tilt" aria-label="Permalink: Install Tilt" href="#install-tilt"></a></p>
<p dir="auto">Installing the <code>tilt</code> binary is a one-step command.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">macOS/Linux</h3><a id="user-content-macoslinux" aria-label="Permalink: macOS/Linux" href="#macoslinux"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -fsSL https://raw.githubusercontent.com/tilt-dev/tilt/master/scripts/install.sh | bash"><pre>curl -fsSL https://raw.githubusercontent.com/tilt-dev/tilt/master/scripts/install.sh <span>|</span> bash</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows</h3><a id="user-content-windows" aria-label="Permalink: Windows" href="#windows"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="iex ((new-object net.webclient).DownloadString('https://raw.githubusercontent.com/tilt-dev/tilt/master/scripts/install.ps1'))"><pre>iex ((<span>new-object</span> net.webclient).DownloadString(<span><span>'</span>https://raw.githubusercontent.com/tilt-dev/tilt/master/scripts/install.ps1<span>'</span></span>))</pre></div>
<p dir="auto">For specific package managers (Homebrew, Scoop, Conda, asdf), see the
<a href="https://docs.tilt.dev/install.html" rel="nofollow">Installation Guide</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Run Tilt</h2><a id="user-content-run-tilt" aria-label="Permalink: Run Tilt" href="#run-tilt"></a></p>
<p dir="auto"><strong>New to Tilt?</strong> Our tutorial will <a href="https://docs.tilt.dev/tutorial.html" rel="nofollow">get you started</a>.</p>
<p dir="auto"><strong>Configuring a Service?</strong> We have best practice guides for
<a href="https://docs.tilt.dev/example_static_html.html" rel="nofollow">HTML</a>,
<a href="https://docs.tilt.dev/example_nodejs.html" rel="nofollow">NodeJS</a>,
<a href="https://docs.tilt.dev/example_python.html" rel="nofollow">Python</a>,
<a href="https://docs.tilt.dev/example_go.html" rel="nofollow">Go</a>,
<a href="https://docs.tilt.dev/example_java.html" rel="nofollow">Java</a>,
and <a href="https://docs.tilt.dev/example_csharp.html" rel="nofollow">C#</a>.</p>
<p dir="auto"><strong>Optimizing a Tiltfile?</strong> Search for the function you need in our
<a href="https://docs.tilt.dev/api.html" rel="nofollow">complete API reference</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Community &amp; Contributions</h2><a id="user-content-community--contributions" aria-label="Permalink: Community &amp; Contributions" href="#community--contributions"></a></p>
<p dir="auto"><strong>Questions:</strong> Join <a href="http://slack.k8s.io/" rel="nofollow">the Kubernetes slack</a> and
find us in the <a href="https://kubernetes.slack.com/messages/CESBL84MV/" rel="nofollow">#tilt</a>
channel. Or <a href="https://github.com/tilt-dev/tilt/issues">file an issue</a>. For code snippets of Tiltfile functionality shared by the Tilt community, check out <a href="https://github.com/tilt-dev/tilt-extensions">Tilt Extensions</a>.</p>
<p dir="auto"><strong>Contribute:</strong> Check out our <a href="https://github.com/tilt-dev/tilt/blob/master/CONTRIBUTING.md">guidelines</a> to contribute to Tilt's source code. To extend the capabilities of Tilt via new Tiltfile functionality, read more about <a href="https://docs.tilt.dev/extensions.html" rel="nofollow">Extensions</a>.</p>
<p dir="auto"><strong>Follow along:</strong> <a href="https://twitter.com/tilt_dev" rel="nofollow">@tilt_dev</a> on Twitter. For updates
and announcements, follow <a href="https://blog.tilt.dev/" rel="nofollow">the blog</a> or subscribe to
<a href="https://tilt.dev/subscribe" rel="nofollow">the newsletter</a>.</p>
<p dir="auto"><strong>Help us make Tilt even better:</strong> Tilt sends anonymized usage data, so we can
improve Tilt on every platform. Details in <a href="http://docs.tilt.dev/telemetry_faq.html" rel="nofollow">"What does Tilt
send?"</a>.</p>
<p dir="auto">We expect everyone in our community (users, contributors, followers, and employees alike) to abide by our <a href="https://github.com/tilt-dev/tilt/blob/master/CODE_OF_CONDUCT.md"><strong>Code of Conduct</strong></a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Reporting security issues</h2><a id="user-content-reporting-security-issues" aria-label="Permalink: Reporting security issues" href="#reporting-security-issues"></a></p>
<p dir="auto">The maintainers take security seriously. If you discover a security issue,
please bring it to their attention right away!</p>
<p dir="auto">Please <strong>DO NOT</strong> file a public issue, instead send your report privately to
<a href="mailto:security@docker.com">security@docker.com</a>.</p>
<p dir="auto">Security reports are greatly appreciated and we will publicly thank you for it.
We also like to send gifts—if you're into Docker schwag, make sure to let
us know. We currently do not offer a paid security bounty program, but are not
ruling it out in the future.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Copyright 2022 Docker, Inc.</p>
<p dir="auto">Licensed under <a href="https://github.com/tilt-dev/tilt/blob/master/LICENSE">the Apache License, Version 2.0</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[BART's Anime Mascots (133 pts)]]></title>
            <link>https://www.bart.gov/news/fun/anime</link>
            <guid>43806281</guid>
            <pubDate>Sat, 26 Apr 2025 19:12:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bart.gov/news/fun/anime">https://www.bart.gov/news/fun/anime</a>, See on <a href="https://news.ycombinator.com/item?id=43806281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><a href="https://www.bart.gov/news/anime"><img src="https://www.bart.gov/sites/default/files/inline-images/Nav%201%20Characters_0.png" data-entity-uuid="9406693b-1373-435d-95d6-6f28c1f22b20" data-entity-type="file" width="165" height="70"></a> <a href="https://www.bart.gov/news/anime/events"><img src="https://www.bart.gov/sites/default/files/inline-images/Nav%202%20News.png" data-entity-uuid="c6524fe3-361d-466c-9971-1d9b78d25773" data-entity-type="file" width="165" height="70"></a> <a href="https://railgoods.com/bart/anime"><img src="https://www.bart.gov/sites/default/files/inline-images/Nav%203%20Merch.png" data-entity-uuid="7bb4a581-de5c-4599-9882-bfca0deb6e6d" data-entity-type="file" width="165" height="70"></a></p><p>We have new Coloring Sheets for BART Anime Mascots Mira and Jasmine<strong> </strong>and a How to Make a BART costume guide!&nbsp;</p><ul><li>Coloring Sheet: Jasmine (Caltrain costume) by nackoyo (<a href="https://www.bart.gov/sites/default/files/2024-10/Coloring%20Sheet%20-%20Jasmine.pdf">PDF file</a>)</li><li>Coloring Sheet: Mira (BART costume) by nackoyo (<a href="https://www.bart.gov/sites/default/files/2024-10/Coloring%20Sheet%20-%20Mira.pdf">PDF file</a>)</li><li>Zine: How to Make a BART Train Costume by shmuh (<a href="https://www.bart.gov/sites/default/files/2024-10/BARTstructions.pdf">PDF file</a>)</li></ul><hr><div><p>Inspired by BART frontline employees and BART-contracted animals (goats, hawks) who have gone on to reach national fame, the anime mascots highlight the tight-knit connections between BART and the Bay Area. The anime mascots will help us in our mission to promote public transportation use, especially among youth riders, a growing and reliable ridership demographic. BART's new mascots were inspired in part by public transportation&nbsp;agencies in Japan and Taiwan, which use anime mascots to connect with their riders and communities.</p><p>The mascots were introduced in 2023 following an open call for California-based artists BART released in Summer 2022. The call was wildly popular, receiving nearly 500 submissions. Following the call, BART worked with selected artists to conceptualize and create the mascots, which will serve as BART's friendly and approachable avatars.&nbsp;</p><p>We encourage people to cosplay as our mascots! You can find cosplay guides by @craftysorceress below.</p></div><h2>Characters</h2><p><img src="https://www.bart.gov/sites/default/files/inline-images/LetsGo%200.png" data-entity-uuid="167f81a3-c424-449a-a029-58c9c5512626" data-entity-type="file" width="272" height="120"><img src="https://www.bart.gov/sites/default/files/inline-images/LetsGo%201_0.png" data-entity-uuid="079a31e0-da38-4fef-a68e-39460773f6d6" data-entity-type="file" width="120" height="120"><img src="https://www.bart.gov/sites/default/files/inline-images/LetsGo%202.png" data-entity-uuid="ed9ca5a5-d3fe-47ef-95f1-8e1570858497" data-entity-type="file" width="120" height="120"></p><p><img src="https://www.bart.gov/sites/default/files/inline-images/TransitFriends%200.png" data-entity-uuid="12407dd4-9a6b-490d-9a01-ead7cf600188" data-entity-type="file" width="272" height="119"><img src="https://www.bart.gov/sites/default/files/inline-images/TransitFriends%201.png" data-entity-uuid="faa02fe9-5f74-4107-be73-1200118ff0a0" data-entity-type="file" width="120" height="120"><img src="https://www.bart.gov/sites/default/files/inline-images/TransitFriends%202.png" data-entity-uuid="4385268b-58ba-4f70-bc77-70beaf0499f3" data-entity-type="file" width="120" height="120"><img src="https://www.bart.gov/sites/default/files/inline-images/TransitFriends%203.png" data-entity-uuid="636597c7-c388-41bc-a3e4-5a26b2738fb8" data-entity-type="file" width="120" height="120"></p><h2>Baylee</h2><p>A star online influencer who grew up in the East Bay. Knowledgable about all things Bay Area &amp; Northern California, especially when it comes to the best food and hottest spots in town!</p><p><img src="https://www.bart.gov/sites/default/files/inline-images/baylee1_0.png" data-entity-uuid="49922c47-a97e-4073-aa02-1e3cab152d40" data-entity-type="file" width="356" height="617"></p><p><img src="https://www.bart.gov/sites/default/files/inline-images/baylee2.png" data-entity-uuid="5dc96f25-1fdd-471d-b9c9-a09d3de44414" data-entity-type="file" width="416" height="616"></p><p><a href="https://www.bart.gov/sites/default/files/2024-10/baylee3.png"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Baylee%201%20Ref.png" data-entity-uuid="4c80824d-7892-41b2-b57f-3369c9bb672f" data-entity-type="file" width="248" height="75"></a> <a href="https://www.bart.gov/news/articles/2021/news20210708"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Baylee%202%20Inspiration.png" data-entity-uuid="c9ee4d63-54a0-4bdd-a57e-a744901d203d" data-entity-type="file" width="248" height="75"></a> <a href="http://craftysorceress.com/baylee-cosplay-guide/"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Cosplay.png" data-entity-uuid="cb9de014-eade-4b67-afef-d2dbceda0ee1" data-entity-type="file" width="248" height="75"></a> <a href="https://railgoods.com/bart/anime"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Merch.png" data-entity-uuid="664dece6-f6f3-43b6-80ff-debd3a31ba32" data-entity-type="file" width="180" height="75"></a></p><h2>Nimbus</h2><p>You can often find Nimbus on top of the hills, taking in the breezy wanderlust of the Bay Area. The nature and civilization below provides tons of inspiration for the stories he writes with his friendly hawk companion.</p><p><img src="https://www.bart.gov/sites/default/files/inline-images/Nimbus%20Art.png" data-entity-uuid="95bb8db5-8af6-4e1f-8b83-3de8271c7614" data-entity-type="file" width="438" height="553"><img src="https://www.bart.gov/sites/default/files/inline-images/Nimbus%20Bio.png" data-entity-uuid="620cec48-eae5-4d81-88fe-02950edc1bfa" data-entity-type="file" width="438" height="542"></p><p><a href="https://www.bart.gov/sites/default/files/2024-05/Nimbus%20Ref.png"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Nimbus%201%20Ref.png" data-entity-uuid="92f3717f-c09e-408a-a578-ab2f2d52394f" data-entity-type="file" width="248" height="75"></a> &nbsp;<a href="https://www.bart.gov/news/articles/2022/news20220630"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Nimbus%202%20Inspiration.png" data-entity-uuid="53a7c50a-d99c-4fc3-bb88-344b55612194" data-entity-type="file" width="248" height="75"></a> <a href="http://craftysorceress.com/nimbus-cosplay-guide/"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Cosplay%20%281%29.png" data-entity-uuid="2b94247c-31c2-4a00-8efa-7136d9cac1e9" data-entity-type="file" width="248" height="75"></a> <a href="https://www.railgoods.com/bart/anime/"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Merch%20%281%29_0.png" data-entity-uuid="6cae6ee7-7026-45f2-ad09-38dd339bc645" data-entity-type="file" width="180" height="75"></a></p><h2>Jasmine</h2><p>High school student &amp; aspiring streetwear fashion designer who really, really loves trains! She is fully decked out with a custom-made outfit: her iconic legacy train bag, an Oakland Wye hair pin, and of course, her iconic “I&nbsp;❤ BART” button badge! Knows the ins-and-outs of transit and arrives early to parties.</p><p><img src="https://www.bart.gov/sites/default/files/inline-images/Jasmine%20Art.png" data-entity-uuid="913054a0-dae4-4fca-acbc-ff5dfc207a44" data-entity-type="file" width="438" height="582"><img src="https://www.bart.gov/sites/default/files/inline-images/Jasmine%20Bio.png" data-entity-uuid="3256a8d6-ed04-4651-ac35-9fc5f43cee72" data-entity-type="file" width="438" height="581"></p><p><a href="https://www.bart.gov/sites/default/files/2024-05/Jasmine%20Ref.png"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Jasmine%201%20Ref.png" data-entity-uuid="96039178-21f5-4a6b-9ca1-cb6c24a7b9c9" data-entity-type="file" width="248" height="75"></a> <a href="https://www.bart.gov/news/articles/2022/news20221019-0"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Jasmine%202%20Inspiration.png" data-entity-uuid="a9b48e70-cd6a-45b5-ab07-004c7778584a" data-entity-type="file" width="248" height="75"></a> <a href="https://craftysorceress.com/jasmine-cosplay-guide/"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Cosplay%20%282%29.png" data-entity-uuid="9725a90e-03a5-4405-b168-f1722273da54" data-entity-type="file" width="248" height="75"></a> <a href="https://www.railgoods.com/bart/anime/"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Merch%20%282%29.png" data-entity-uuid="16389286-0228-4e56-813d-2e41df67276e" data-entity-type="file" width="180" height="75"></a></p><h2>Mira</h2><p>Joined BART as a System Service Worker and studied to become a Train Operator. A friendly face who has gotten to know her regulars over the years. There’s a small chance you may even see her on your next train or in the lobby of favorite online game.</p><p><img src="https://www.bart.gov/sites/default/files/inline-images/Mira%20Art.png" data-entity-uuid="22e2cb2c-5c95-4135-be8b-e068b0ed69da" data-entity-type="file" width="439" height="616"><img src="https://www.bart.gov/sites/default/files/inline-images/Mira%20Bio.png" data-entity-uuid="606e3db8-a6c5-4238-b825-a6c410190005" data-entity-type="file" width="439" height="616"></p><p><a href="https://www.bart.gov/sites/default/files/2024-05/Mira%20Ref.png"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Mira%201%20Ref_0.png" data-entity-uuid="c45d1783-6932-4888-8e48-4d1021e98eb6" data-entity-type="file" alt="https://www.bart.gov/sites/default/files/2024-05/Mira%20Ref.png" width="248" height="76"></a> <a href="https://www.bart.gov/news/articles/2022/news20221019-0"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Mira%202%20Inspiration.png" data-entity-uuid="3cca4fd1-0785-4eec-a847-b530f42bb19d" data-entity-type="file" alt="Inspiration button" width="248" height="76"></a> <a href="http://craftysorceress.com/mira-cosplay-guide/"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Cosplay%20%283%29.png" data-entity-uuid="1ea3bfba-6dd0-4c9b-9388-ad931eef3f85" data-entity-type="file" alt="Cosplay guide button" width="248" height="75"></a> <a href="https://railgoods.com/bart/anime"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Merch%20%283%29.png" data-entity-uuid="86431b9a-8c64-4c17-8090-46d57605c2fc" data-entity-type="file" alt="Merch button" width="180" height="75"></a></p><h2>Barty</h2><p>The reincarnated bunny spirit of a legacy car who has seen it all. Always available to help BART riders find their way around town with a cheerful mood.</p><p><img src="https://www.bart.gov/sites/default/files/inline-images/Barty%20Art.png" data-entity-uuid="dcbbec15-f477-4210-932a-f1c6868dc9fa" data-entity-type="file" width="438" height="377"><img src="https://www.bart.gov/sites/default/files/inline-images/Barty%20Bio.png" data-entity-uuid="badea171-a38a-4190-9d37-ed0def45fde1" data-entity-type="file" width="439" height="528"></p><p><a href="https://www.bart.gov/news/articles/2022/news20220310-0"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Barty%20Inspiration.png" data-entity-uuid="20ade8b3-30c8-4dd6-9862-717118cbafca" data-entity-type="file" width="248" height="76"></a> <a href="https://www.railgoods.com/bart/anime/"><img src="https://www.bart.gov/sites/default/files/inline-images/Btn%20Merch%20%284%29.png" data-entity-uuid="a9cdc1f9-c513-441a-bb1f-906a0a4dc15d" data-entity-type="file" width="180" height="75"></a></p><h2>Merch</h2><p>Get official mascot merch on <a href="https://t.co/TH3hVZqr4g" dir="ltr">http://railgoods.com/bart/anime</a> We have acrylic standees and sticker sheets of all the mascots: Baylee, Jasmine, Mira, and Nimbus.&nbsp;</p>
      </div></div>]]></description>
        </item>
    </channel>
</rss>