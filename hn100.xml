<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 28 May 2025 11:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[HNInternal: Show HN: AutoThink – Boosts local LLM performance with adaptive reasoning (274 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44112326</link>
            <guid>44112326</guid>
            <pubDate>Wed, 28 May 2025 02:39:11 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44112326">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I built AutoThink, a technique that makes local LLMs reason more efficiently by adaptively allocating computational resources based on query complexity.</p><p>The core idea: instead of giving every query the same "thinking time," classify queries as HIGH or LOW complexity and allocate thinking tokens accordingly. Complex reasoning gets 70-90% of tokens, simple queries get 20-40%.</p><p>I also implemented steering vectors derived from Pivotal Token Search (originally from Microsoft's Phi-4 paper) that guide the model's reasoning patterns during generation. These vectors encourage behaviors like numerical accuracy, self-correction, and thorough exploration.</p><p>Results on DeepSeek-R1-Distill-Qwen-1.5B:</p><p>- GPQA-Diamond: 31.06% vs 21.72% baseline (+43% relative improvement)</p><p>- MMLU-Pro: 26.38% vs 25.58% baseline</p><p>- Uses fewer tokens than baseline approaches</p><p>Works with any local reasoning model - DeepSeek, Qwen, custom fine-tuned models. No API dependencies.</p><p>The technique builds on two things I developed: an adaptive classification framework that can learn new complexity categories without retraining, and an open source implementation of Pivotal Token Search.</p><p>Technical paper: <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327" rel="nofollow">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327</a></p><p>Code and examples: <a href="https://github.com/codelion/optillm/tree/main/optillm/autothink">https://github.com/codelion/optillm/tree/main/optillm/autoth...</a></p><p>PTS implementation: <a href="https://github.com/codelion/pts">https://github.com/codelion/pts</a></p><p>I'm curious about your thoughts on adaptive resource allocation for AI reasoning. Have you tried similar approaches with your local models?</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Look ma, no bubbles designing a low-latency megakernel for Llama-1B (151 pts)]]></title>
            <link>https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles</link>
            <guid>44111673</guid>
            <pubDate>Wed, 28 May 2025 00:01:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles">https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles</a>, See on <a href="https://news.ycombinator.com/item?id=44111673">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>There are some applications that benefit from running LLMs really, really fast. This low-latency regime encompasses applications like chatbots and human-in-the-loop workflows, where users care a lot about seeing responses come back immediately.</p>
<p>Given the importance of these low-latency workloads, we wanted to explore just how fast we can run open-source models on modern GPUs. To really stress-test existing systems, we consider an aggressive low-latency scenario where we generate a single sequence with Llama-3.2-1B. This workload is strongly memory bound – our performance is dominated by how fast we can load model weights from GPU global memory.</p>
<p>It turns out that popular LLM inference engines – vLLM and SGLang – are only able to use at most 50% of available GPU bandwidth when running this workload on an H100. The root of the problem, which we'll describe more below, is that existing systems break down a model forward pass into around <strong>a hundred separate kernels</strong> that each implement a few operations (e.g. RMS norm, attention, an MLP layer + activation, rotary). Each kernel comes with a setup and teardown period and during this time no useful work gets done – for instance, the all-important task of loading model weights is stalled.</p>
<div><p><img src="https://hazyresearch.stanford.edu/static/posts/2025-05-27-no-bubbles/result.png" alt="Performance comparison graph"></p><p><em>Figure 1: Speed! Results generated with a 32-token prompt and 128 generated tokens, with no speculation</em></p></div>
<p>In this post, we show how we can bypass this problem by merging the entire Llama-1B forward pass into a single "megakernel" that eliminates kernel boundaries altogether. Doing this achieves brr – on an H100, we use 78% of memory bandwidth and outperform existing systems by over 1.5x. (To our knowledge, this is the lowest-latency forward pass for Llama-1B in bfloat16!) In the rest of this post, we'll walk through how and why one would do this. Specifically:</p>
<ul>
<li>First, we'll talk about how small kernels lead to AI systems that underutilize the GPU's full bandwidth.</li>
<li>Second, we'll describe three important points about how we built our megakernel: how we fused lots of kernels together, how we share hardware resources across them to minimize overhead, and how we synchronize them efficiently.</li>
</ul>
<p>If you're interested in learning more of the details or using these ideas yourself, we're <a href="https://github.com/HazyResearch/Megakernels">open-sourcing all of our code here</a>.</p>
<h2>Separate Kernels Kill the Vibe</h2>
<p>In general, the way one runs code on a GPU is by launching a "kernel" – a small program that does a well-defined operation (e.g. RMS norm, MLP). Today, all AI workloads run as long sequences of relatively small kernels. To get an initial sense, let's look at the operations in the Llama-1B transformer block, and some example kernel boundaries of how they might be divided up (Figure 2).</p>
<div><p><img src="https://hazyresearch.stanford.edu/static/posts/2025-05-27-no-bubbles/kernel_boundaries.png" alt="Kernel boundaries diagram"></p><p><em>Figure 2: An example set of kernel boundaries for the Llama-1B transformer block. Red boxes delineate the work done by individual kernels.</em></p></div>
<p>As we described earlier, decoding a single sequence with Llama-1B is a purely memory-bound workload: our performance depends on being able to <strong>always</strong> be loading weights from GPU global memory. So, why are existing approaches so far from using the full bandwidth of the GPU?</p>
<p>When we dug into it, we noticed a key problem was that the current kernel-based approach to running models introduces stalls that prevent us from constantly loading memory:</p>
<ul>
<li>First: GPU kernels are launched with a strict ordering, so that a thread block in one kernel can't start until all thread blocks in previous kernels have completely finished. Consequently, every time we start a kernel, we have to wait for all the straggler thread blocks from the prior one to finish. For example, if a kernel runs 512 thread blocks (like our Llama-1B down projection), but we only have 148 streaming multiprocessors (like on a B200), we end up with 80 empty SM's at the end.</li>
<li>Second, as we've <a href="https://hazyresearch.stanford.edu/blog/2025-03-04-thundermla">previously highlighted</a>, each kernel launch and teardown incurs costs. In principle, NVIDIA's CUDA graphs can help hide costs, but by our measurements they still leave a lot on the table. For a simple dummy kernel (which dumps a start time, sleeps, and dumps an end time) on an H100, we find that running on a CUDA stream incurs a launch cost of about 2.1 microseconds, and with CUDA graphs the launch cost only decreases to around 1.3 microseconds – time spent with the GPU doing no useful work! We'd like to have the GPU spend all of its time doing useful work.</li>
<li>Finally, even after we start the next kernel, we still have to wait to load weights and activations before any compute can start. These latencies leave the GPU sitting idle for thousands of cycles! Ideally, we'd start loading the next weights while the previous computations and stores are happening. NVIDIA has also built a mechanism for this called <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#programmatic-dependent-launch-and-synchronization">Programmatic Dependent Launch</a> (PDL), which allows the next kernel to start preparing while the previous kernel is running, but we found it still introduces unnecessary stalls because the PDL synchronization mechanism (cudaGridDependencySynchronize) is very coarse. For example, it means we have to wait for all queries, keys, and values to complete in order to start attention, as opposed to starting heads as soon as they are ready. We'll later show another specific case of where this is useful in Llama-1B.</li>
</ul>
<p>Taken together, these form the "memory pipeline bubbles" our title references – and they represent a key reason that we're <strong>not always loading from memory</strong>.  For short operations, these pauses add up, wasting a huge chunk of potential bandwidth. In part, this is because Llama-1B (actually 1.24B parameters) in batch size 1 is just so... small: if each operation is really fast, then the time spent in-between them really starts to matter.</p>
<p>To illustrate the magnitude of the problem: for single-sequence generation in 16-bit precision on a single H100, the <strong>memory limit</strong> is 3.35TB/s / 2.48GB = ~1350 forward passes per second. But with 7 kernel launches per layer, and 16 layers, even with an optimistic 5 us of stalling per kernel (counting stragglers, kernel launch, and memory latencies), generation would run at just ~770 forward passes per second. In practice, it's often worse. On low-latency workloads, GPUs spend only a fraction of their time actually doing any useful work!</p>
<p>So while CUDA does provide some existing features (e.g. graphs, streams, PDL) to partially solve these problems, we wanted to see if a different approach could solve all of these problems, where we just fuse the entire model forward pass into a single kernel.</p>
<h2>How to Megakernel</h2>
<p>Next, we'll show you how we fused a whole Llama forward pass into a single kernel, and our methods for resolving three key problems:</p>
<ol>
<li>Fusing dozens of operations is hard to do from scratch. We need a mechanism for executing these operations within the megakernel.</li>
<li>In order to overlap multiple operations on the same hardware, we need to prevent contention over limited resources, such as shared memory.</li>
<li>The GPU synchronizes after each kernel in the traditional kernel model. Without kernels, we have to synchronize the GPU all by ourselves!</li>
</ol>
<p>Let's start with the first issue:</p>
<h4>Issue 1/3: Fusing Lots of Operations</h4>
<p>Traditional kernel fusion generally merges just two or three operations together. In contrast, we need to fuse about a hundred. Consequently, we need to have a sensible abstraction for how we can actually program a megakernel.</p>
<p>Our approach is built on an on-GPU interpreter – essentially a more sophisticated version of our infrastructure underlying <a href="https://hazyresearch.stanford.edu/blog/2025-03-04-thundermla">ThunderMLA</a>. Our interpreter is designed such that each streaming multiprocessor (SM) within the GPU receives a sequence of <strong>instructions</strong> (each implemented using the same CUDA template) and executes them. We <strong>schedule</strong> each SM's instruction sequence ahead of time on the Python side, and notably we can reuse each schedule for hundreds of forward passes!</p>
<p>For our end-to-end Llama forwards pass megakernel, we define the following set of instructions:</p>
<ul>
<li>A fused RMS norm &amp; QKV &amp; RoPE instruction.</li>
<li>An attention computation instruction.</li>
<li>An attention reduction instruction (for ThunderGQA on long sequences).</li>
<li>An O-projection + residual instruction.</li>
<li>A fused RMS norm &amp; up-gate &amp; SiLU instruction.</li>
<li>A down-projection + residual instruction.</li>
<li>An RMS norm &amp; language modeling head instruction, for computing the final token logits.</li>
</ul>
<p>We implement each of these instructions using a common <a href="https://github.com/HazyResearch/Megakernels/blob/main/util/mk_init/sources/src/%7B%7BPROJECT_NAME_LOWER%7D%7D.cu">CUDA template</a> (with load, store, compute boilerplate functions), facilitating interoperability within our interpreter framework.</p>
<h4>Issue 2/3: <span>S</span><span>h</span><span>a</span><span>r</span><span>i</span><span>n</span><span>g</span> Shared Memory to Eliminate Memory Bubbles</h4>
<p>The instruction-and-interpreter structure lets us cleanly organize our megakernel. However, we haven't yet addressed the key issue: making sure that model weights are always being loaded in order to maximize memory bandwidth utilization.</p>
<p>The reason why a megakernel lets us solve this problem is that we can pipeline memory loads across instructions: our interpreter will start loading the model weights for an instruction as soon as it can, even if a previous instruction is still finishing up (e.g. storing out its results to global memory). It's this tight transitioning between instructions that minimizes the memory bubbles that would otherwise appear if we launched multiple kernels.</p>
<p>However, there's a catch: loading the weights from global memory for the next instruction doesn't do you much good if you have no place to put the data you loaded! More precisely, all of our weight matrices are loaded from GPU global memory into our SM's "shared memory" – NVIDIA's term for the fast memory on each SM. Shared memory is a scarce resource on each SM, and we can't start a load for a new instruction if a previous instruction is using all of it. This necessitates a way to keep track of which instruction is using which piece of shared memory and quickly transition shared memory to the next instruction when the current instruction is done with it.</p>
<p>We accomplish this by <strong>paging</strong> shared memory. We first divide the first 213kB of shared memory on an H100 into 13 16KiB pages, and use remaining shared memory for special purposes, like storing instruction parameters. To use one of these pages, instructions have to explicitly request and release them from the interpreter. The interpreter automatically passes released pages to the next instruction, allowing them to start issuing memory loads as early as shared memory becomes available.</p>
<h4>Issue 3/3: Synchronization</h4>
<p><img src="https://hazyresearch.stanford.edu/static/posts/2025-05-27-no-bubbles/thanos.png" alt="Thanos illustration"></p>
<p>While megakernels let us minimize pipeline bubbles, they also introduce a new problem: synchronization. The performance limitation with the normal many-kernel execution model is that no thread blocks in a kernel can start until all thread blocks in previous kernels are finished. However, it's precisely this property that makes it easy to manage data dependencies. When a kernel launches, CUDA guarantees that all of the kernel's input tensors have already been produced and are safe to read from immediately.</p>
<p>With megakernels, we have no such guarantees: when an SM starts to execute a new instruction, its inputs might not be ready! To address this, we explicitly synchronize the instructions inside of our megakernel. We accomplish this with a simple counter system. Before the megakernel launches, we initialize an array of counters (i.e. integers) in GPU global memory with a starting value of zero. Whenever an instruction completes, it increments one of these counters. Similarly, whenever a new instruction starts, it must wait for some of these counters to reach a target value, indicating that all of its dependencies have finished.</p>
<p>One optimization this enables is in the big multi-layer perceptrons (MLPs) in Llama-1B.</p>
<ul>
<li>In a naive implementation using PDL, one must await completing the whole hidden state before beginning the down projection matrix multiply.</li>
<li>We instead produce and consume the intermediate state in four chunks, each with their own counter. This way, an instruction for the down projection only needs to wait for its input chunk to finish.</li>
</ul>
<h2>Putting It All Together</h2>
<p>To our knowledge, our H100 megakernel represents the first time anyone has run the forward pass for a 16-bit 1B+ parameter language model in under one millisecond on a GPU. Our B200 implementation pushes this even further to under 680 microseconds per forward pass!</p>
<p>As shown in Figure 1, our megakernel outperforms vLLM and SGLang baselines (which use CUDA graphs and torch compilation):</p>
<ul>
<li>On an H100, our megakernel runs almost 2.5x faster than vLLM and over 1.5x faster than SGLang.</li>
<li>On a B200, the gap with vLLM rises to over 3.5x, and we remain more than 1.5x faster than SGLang, too.</li>
</ul>
<p>We're still actually quite a ways off from the theoretical limit on a B200, which is around ~3,000 forward passes per second. Part of this gap is because this theoretical limit is based purely on memory bandwidth – but we still have to wait to load activations. And although these activations are small (and don't cost a lot of bandwidth), there are still latencies in loading them that we can't hide. A breakdown of the runtime of our current B200 forward pass (total runtime 600 microseconds):</p>
<ul>
<li>250 microseconds are spent storing activations, awaiting consistency, and loading them. This is about 20% higher than a simple model would suggest: since each instruction has a dependence on the last one, we need to pay two load latencies (check ready, and then load activations) and two store latencies (store activations, then mark ready) per instruction. Using ~500 nanoseconds latency per load / store, this would impose about 200 microseconds of overhead. (We suspect some of the remaining 50 microseconds comes from time spent processing atomics in global memory.)</li>
<li>200 microseconds are spent actually running RMS norm and matrix-vector computations. 95% of this portion is devoted to matrix-vector. On Blackwell, we find that using the tensor cores is marginally helpful for this; on Hopper, we find it better to simply run on the CUDA cores. This difference comes from the fact that both GPUs have relatively similar CUDA core performance, but Blackwell tensor cores are much faster.</li>
<li>30 microseconds are spent awaiting weights from global memory (pipelining works!) Of these, 40% are spent in the LM head, which is the best-pipelined part of the whole megakernel due to its homogeneity and huge size.</li>
<li>40 microseconds are spent on low-level synchronization overhead across warps. A key issue here is that CUDA's asynchronous barriers are relatively slow, even when they're already in the "pass" state, requiring about 60 nanoseconds each time.</li>
<li>80 microseconds are on setup and various other overheads (e.g. passing instruction barriers, marking pages as complete, etc.)</li>
</ul>
<p>We think there's probably more to do on each of these, but that'll have to wait for a future update!</p>
<h2>The Megakernel Cinematic Universe</h2>
<p>In this blog, we focus narrowly on designing a megakernel for low-latency, batch-size one LLM inference. However, we believe that the ability to more precisely control GPU execution with megakernels can more generally be applied to accelerate a much broader set of AI workloads. Stay tuned!</p>
<div><p><img src="https://hazyresearch.stanford.edu/static/posts/2025-05-27-no-bubbles/sonic.png" alt="Sonic illustration"></p><p><strong>The Main Message of this Blog Post</strong></p></div>
<p>If you'd like to learn more, please reach out to Ben or Jordan! Please include a tribute of at least five pictures of kittens in your email.</p>
<ul>
<li>Ben: <a href="mailto:bfs@stanford.edu">bfs@stanford.edu</a></li>
<li>Jordan: <a href="mailto:jbj@stanford.edu">jbj@stanford.edu</a></li>
</ul>
<p>And many, many thanks to Together AI for generously providing us with B200s and H100s to do this work, which would not have been possible without them!</p>
<p>See also: <a href="https://hazyresearch.stanford.edu/blog/2025-03-04-thundermla"><strong>pretty big kernels</strong></a> | <a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk"><strong>regular kernels</strong></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A UEFI app that sends LLDP-MED pkt at boot to negotiate PoE+ power before the OS (154 pts)]]></title>
            <link>https://roderickkhan.com/posts/2025-05-16-poe-uefi-solution</link>
            <guid>44111609</guid>
            <pubDate>Tue, 27 May 2025 23:45:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://roderickkhan.com/posts/2025-05-16-poe-uefi-solution">https://roderickkhan.com/posts/2025-05-16-poe-uefi-solution</a>, See on <a href="https://news.ycombinator.com/item?id=44111609">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Back in 2015, I was working on a project to build PoE-powered embedded x86 computers and digital signage systems. These were full Windows 10 Professional machines running Intel Atom processors, designed to simplify deployment by drawing power directly over Ethernet. Our goal was to eliminate the need to run traditional AC power to these devices, which can be costly and impractical in many deployment scenarios. But unlike typical IoT or low-power devices, these were full-fledged x86 computers that required more power than what the standard PoE (802.3af) could deliver, which maxes out at 15.4W at the PSE (Power Sourcing Equipment), such as a PoE network switch or injector.</p>
<p>Our device required about 23W when fully operational, which pushed us into <strong>802.3at (PoE+)</strong> territory. In most client environments their PoE+ switches provided the power we needed with no problem. But some environments had network switches that would not give us the additional power.</p>
<h3><strong>PoE Standards Overview (IEEE 802.3)</strong></h3>
<table>
<thead>
<tr>
<th>Standard</th>
<th>Max Power at PSE</th>
<th>Max Power at PD</th>
<th>Voltage Range</th>
<th>Pairs Used</th>
<th>Year</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>802.3af (PoE)</strong></td>
<td>15.4 W</td>
<td>12.95 W</td>
<td>44–57 V DC</td>
<td>2 pairs</td>
<td>2003</td>
</tr>
<tr>
<td><strong>802.3at (PoE+)</strong></td>
<td>30 W</td>
<td>25.5 W</td>
<td>50–57 V DC</td>
<td>2 pairs</td>
<td>2009</td>
</tr>
</tbody>
</table>
<p>The problem was that our embedded systems only supported physical‑layer classification which is limited to signaling power requirements through resistive detection and pulsed current signatures during initial PoE handshaking. Only relying on this method can be problematic if the switch is configured to require LLDP for Data Link Layer Classification for devices requiring more than 15.4W. Which is a problem because at minimum our computers required at least 18W in order to boot into the operating system. So our systems would initially start to boot, but then eventually shut off before it got into Windows. We were stuck in a frustrating Catch-22, we needed to send LLDP packets to get more power, but we couldn’t even boot the OS to send them.</p>
<h3><strong>So What Do You Do When the OS Can't Help?</strong></h3>
<p>We did some testing and measured power draw during various phases of the boot cycle. Fortunately, the system's power needs during initial startup (BIOS/UEFI initialization) were low enough to stay under the 15.4W limit. That gave us a brief window to request more power <em>before</em> booting Windows.</p>
<p>So the challenge became: negotiate higher PoE+ power <strong>before</strong> Windows starts. The answer was to handle LLDP negotiation at the BIOS level, or more accurately the UEFI (Unified Extensible Firmware Interface) firmware.  Through our research we discovered that UEFI supports the TCP/IP protocol and has access to the network stack, enabling communication over Ethernet without an OS.</p>
<p>Our first attempt was to work with the motherboard vendor and AMI (the BIOS provider) for a custom firmware build. We signed NDAs and had multiple discussions, but despite our efforts, they ultimately declined to create a custom BIOS for us. After hitting that roadblock and feeling the frustration of stalled progress, I refused to give up. I dug deeper and came across the concept of <strong>UEFI applications</strong>.</p>
<p>A UEFI application is a type of software designed to run in the pre-boot environment of a computer, managed entirely by the UEFI firmware. These applications are different from traditional programs that run once an operating system like Windows or Linux has loaded. Instead, UEFI applications operate with the services and resources provided by the firmware itself, bypassing the need for an OS.</p>
<p>They are typically stored on a dedicated partition called the EFI System Partition (ESP) and launched by the UEFI boot manager during the system's boot process. These apps can access low-level system functionality, including networking, file systems, and input/output devices. In our case, that meant we could build a standalone tool to t ransmit LLDP packets <em>before</em> the OS even initialized. This was the perfect solution, because it required no changes to the BIOS/UEFI firmware itself. I just needed to find someone with the embedded firmware expertise to bring it to life.</p>
<h3><strong>From Warsaw With Code</strong></h3>
<p>After some research, I found <a href="https://www.linkedin.com/in/krolpiotr/">Piotr Król</a>, a former BIOS software engineer at Intel who was doing freelance work out of Poland. He understood the problem immediately. We set up remote serial and IP-KVM access to our development hardware, and Piotr got to work.</p>
<p>There were some challenges along the way including lack of vendor support, incomplete firmware tooling, and remote hardware limitations. Our system didn't include <code>bcfg</code>, which meant we couldn't persistently change the boot order through standard UEFI tools. Piotr identified this early and suggested using <code>startup.nsh</code> as a workaround, a shell script that would automatically run our LLDP application when the EFI shell launched.</p>
<p>Four months later, Piotr delivered <strong>PoePwrNegotiator</strong>: a UEFI application written in C that transmits LLDP-MED (Link Layer Discovery Protocol – Media Endpoint Discovery) packets and requests the higher power levels we needed. No OS required. We deployed this UEFI application on all of our PoE devices in production and it worked flawlessly.</p>
<h3><strong>Sharing the Solution</strong></h3>
<p>This project began as an attempt to solve a very specific challenge we faced nearly a decade ago. I don’t know how many others have tackled this type of problem or taken this approach, but I wanted to share the work in case it helps someone else.</p>
<p>By open-sourcing <strong>PoePwrNegotiator</strong>, my goal is to preserve and document a unique solution to a problem that may still be relevant to those building PoE-powered x86 systems. If someone out there is working on a similar challenge, or even just wants to understand how UEFI applications can be used to control networking behavior at boot, I hope this gives them a useful head start.</p>
<p>PoePwrNegotiator is released under the <strong>MIT License</strong>, one of the most permissive open source licenses available. This means anyone can use, modify, or integrate this code into their own projects, commercial or personal, as long as the original license and copyright notice are included. The goal is to make this as accessible and useful as possible to anyone dealing with power negotiation challenges or looking to learn more about UEFI networking.</p>
<p><strong>GitHub Repo:</strong> <a href="https://github.com/orbitrod/PoePwrNegotiator">https://github.com/orbitrod/PoePwrNegotiator</a></p>
<h3><strong>Special Thanks</strong></h3>
<p><strong>Carlos</strong>, you were instrumental during the testing and the deployment of this application. You were my right hand throughout this project and far beyond it, and your dedication to me and to the work we were doing will never be forgotten. I cannot express enough how much your loyalty and commitment meant to me throughout that entire journey.</p>
<p><strong>Piotr</strong>, thank you for being brilliant, resourceful, and incredibly effective. Your deep expertise in firmware helped us solve a problem others wouldn’t touch. I’m grateful for your expertise and contribution to our project, your work solved the last piece of the puzzle.</p>
<hr>
<blockquote>
<p><em>This project reminded me that innovation often comes from working around limitations, not just within them. PoePwrNegotiator was a solution to a very specific challenge I faced in 2015, but the lessons and approach still feel relevant today. If it sparks ideas, helps someone overcome a similar obstacle, or contributes in any way to future PoE-powered system design, that’s all the reason I need to put it out there.</em></p>
<p><em>— Roderick</em></p>
</blockquote>
<hr>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenTPU: Open-Source Reimplementation of Google Tensor Processing Unit (TPU) (108 pts)]]></title>
            <link>https://github.com/UCSBarchlab/OpenTPU</link>
            <guid>44111452</guid>
            <pubDate>Tue, 27 May 2025 23:10:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/UCSBarchlab/OpenTPU">https://github.com/UCSBarchlab/OpenTPU</a>, See on <a href="https://news.ycombinator.com/item?id=44111452">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">UCSB ArchLab OpenTPU Project</h2><a id="user-content-ucsb-archlab-opentpu-project" aria-label="Permalink: UCSB ArchLab OpenTPU Project" href="#ucsb-archlab-opentpu-project"></a></p>
<p dir="auto">OpenTPU is an open-source re-implementation of Google's Tensor Processing Unit (TPU) by the UC Santa Barbara ArchLab.</p>
<p dir="auto">The TPU is Google's custom ASIC for accelerating the inference phase of neural network computations.</p>
<p dir="auto">Our design is based on details from Google's paper titled "In-Datacentre Performance Analysis of a Tensor Processing Unit" (<a href="https://arxiv.org/abs/1704.04760" rel="nofollow">https://arxiv.org/abs/1704.04760</a>), which is to appear at ISCA2017. However, no formal spec, interface, or ISA has yet been published for the TPU.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">The OpenTPU is powered by PyRTL (<a href="http://ucsbarchlab.github.io/PyRTL/" rel="nofollow">http://ucsbarchlab.github.io/PyRTL/</a>).</h4><a id="user-content-the-opentpu-is-powered-by-pyrtl-httpucsbarchlabgithubiopyrtl" aria-label="Permalink: The OpenTPU is powered by PyRTL (http://ucsbarchlab.github.io/PyRTL/)." href="#the-opentpu-is-powered-by-pyrtl-httpucsbarchlabgithubiopyrtl"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Python 3</li>
<li>PyRTL version &gt;= 0.8.5</li>
<li>numpy</li>
</ul>
<p dir="auto">Both PyRTL and numpy can be installed with pip; e.g., <code>pip install pyrtl</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to Run</h2><a id="user-content-how-to-run" aria-label="Permalink: How to Run" href="#how-to-run"></a></p>
<p dir="auto">To run the simple matrix multiply test in both the hardware and functional simulators:</p>
<p dir="auto">Make sure MATSIZE is set to 8 in config.py, then</p>
<div data-snippet-clipboard-copy-content="python3 assembler.py simplemult.a
python3 runtpu.py simplemult.out simplemult_hostmem.npy simplemult_weights.npy
python3 sim.py simplemult.out simplemult_hostmem.npy simplemult_weights.npy"><pre><code>python3 assembler.py simplemult.a
python3 runtpu.py simplemult.out simplemult_hostmem.npy simplemult_weights.npy
python3 sim.py simplemult.out simplemult_hostmem.npy simplemult_weights.npy
</code></pre></div>
<p dir="auto">To run the Boston housing data regression test in both the hardware and functional simulators:</p>
<p dir="auto">Make sure MATSIZE is set to 16 in config.py, then</p>
<div data-snippet-clipboard-copy-content="python3 assembler.py boston.a
python3 runtpu.py boston.out boston_inputs.npy boston_weights.npy
python3 sim.py boston.out boston_inputs.npy boston_weights.npy"><pre><code>python3 assembler.py boston.a
python3 runtpu.py boston.out boston_inputs.npy boston_weights.npy
python3 sim.py boston.out boston_inputs.npy boston_weights.npy
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hardware Simulation</h3><a id="user-content-hardware-simulation" aria-label="Permalink: Hardware Simulation" href="#hardware-simulation"></a></p>
<p dir="auto">The executable hardware spec can be run using PyRTL's simulation features by running <code>runtpu.py</code>. The simulation expects as inputs a binary program and numpy array files containing the initial host memory and the weights.</p>
<p dir="auto">Be aware that the size of the hardware Matrix Multiply unit is parametrizable --- double check <code>config.py</code> to make sure MATSIZE is what you expect.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Functional Simulation</h3><a id="user-content-functional-simulation" aria-label="Permalink: Functional Simulation" href="#functional-simulation"></a></p>
<p dir="auto">sim.py implements the functional simulator of OpenTPU. It reads in three cmd args: the assembly program, the host memory file, and the weights file. Due to the different quantization mechnisms between high-level applications (written in tensorflow) and OpenTPU, the simulator runs in two modes: 32b float mode and 8b int mode. The downsampling/quantization mechanism is consistent with the HW implementation of OpenTPU. It generates two sets of outputs, one set being 32b-float typed, the other 8b-int typed.</p>
<p dir="auto">Example usage:</p>
<div data-snippet-clipboard-copy-content="python sim.py boston.out boston_input.npy boston_weights.npy"><pre><code>python sim.py boston.out boston_input.npy boston_weights.npy
</code></pre></div>
<p dir="auto">Numpy matrices (.npy files) can be generated by calling <code>numpy.save</code> on a numpy array.</p>
<p dir="auto">checker.py implementes a simple checking function to verify the results from HW, simulator and applications. It checkes the 32b-float application results against 32b-float simulator results and then checks the 8b-int simulator results against 8b-int HW results.</p>
<p dir="auto">Example usage:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">FAQs:</h2><a id="user-content-faqs" aria-label="Permalink: FAQs:" href="#faqs"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How big/efficient/fast is OpenTPU?</h3><a id="user-content-how-bigefficientfast-is-opentpu" aria-label="Permalink: How big/efficient/fast is OpenTPU?" href="#how-bigefficientfast-is-opentpu"></a></p>
<p dir="auto">As of the alpha release, we do not have hard synthesis figures for the full 256x256 OpenTPU.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What can OpenTPU do?</h3><a id="user-content-what-can-opentpu-do" aria-label="Permalink: What can OpenTPU do?" href="#what-can-opentpu-do"></a></p>
<p dir="auto">The hardware prototype can currently handle matrix multiplies and activations for ReLU and sigmoid --- i.e., the inference phase of many neural network computations.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What features are missing?</h3><a id="user-content-what-features-are-missing" aria-label="Permalink: What features are missing?" href="#what-features-are-missing"></a></p>
<p dir="auto">Convolution, pooling, programmable normalization.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Does your design follow that of the TPU?</h3><a id="user-content-does-your-design-follow-that-of-the-tpu" aria-label="Permalink: Does your design follow that of the TPU?" href="#does-your-design-follow-that-of-the-tpu"></a></p>
<p dir="auto">We used high-level design details from the TPU paper to guide our design when possible. Thus, the major components of the chip are the same --- matrix multiply unit, unified buffer, activation unit, accumulator, weight FIFO, etc. Beyond that, the implementations may have many differences.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Does OpenTPU support all the same instructions as TPU?</h3><a id="user-content-does-opentpu-support-all-the-same-instructions-as-tpu" aria-label="Permalink: Does OpenTPU support all the same instructions as TPU?" href="#does-opentpu-support-all-the-same-instructions-as-tpu"></a></p>
<p dir="auto">No. Currently, OpenTPU supports the RHM, WHM, RW, MMC, ACT, NOP, and HLT instructions (see ISA section for details). The purpose, definition, and specification of other TPU instructions is absent from the published paper. Some instructions will likely be added to OpenTPU as we continue development (such as SYNC), but the final ISA will likely feature many differences without a published spec from Google to work off of.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Is OpenTPU binary compatible with the TPU?</h3><a id="user-content-is-opentpu-binary-compatible-with-the-tpu" aria-label="Permalink: Is OpenTPU binary compatible with the TPU?" href="#is-opentpu-binary-compatible-with-the-tpu"></a></p>
<p dir="auto">No. There is no publicly available interface or spec for TPU.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">I'd like to do some analysis/extensions of OpenTPU, but I need Verilog. Do you have a Verilog version?</h3><a id="user-content-id-like-to-do-some-analysisextensions-of-opentpu-but-i-need-verilog-do-you-have-a-verilog-version" aria-label="Permalink: I'd like to do some analysis/extensions of OpenTPU, but I need Verilog. Do you have a Verilog version?" href="#id-like-to-do-some-analysisextensions-of-opentpu-but-i-need-verilog-do-you-have-a-verilog-version"></a></p>
<p dir="auto">PyRTL can can output structural Verilog for the design, using the <code>OutputToVerilog</code> function.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">I have suggestions, criticisms, and/or would like to contribute.</h3><a id="user-content-i-have-suggestions-criticisms-andor-would-like-to-contribute" aria-label="Permalink: I have suggestions, criticisms, and/or would like to contribute." href="#i-have-suggestions-criticisms-andor-would-like-to-contribute"></a></p>
<p dir="auto">That's not a question, but please get in touch! Email Deeksha (<a href="mailto:deeksha@cs.ucsb.edu">deeksha@cs.ucsb.edu</a>) or Joseph (<a href="mailto:jmcmahan@cs.ucsb.edu">jmcmahan@cs.ucsb.edu</a>).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">I'm a Distinguished Hardware Engineer at Google and the Lead Architect of the TPU. I see many inefficiencies in your implementation.</h3><a id="user-content-im-a-distinguished-hardware-engineer-at-google-and-the-lead-architect-of-the-tpu-i-see-many-inefficiencies-in-your-implementation" aria-label="Permalink: I'm a Distinguished Hardware Engineer at Google and the Lead Architect of the TPU. I see many inefficiencies in your implementation." href="#im-a-distinguished-hardware-engineer-at-google-and-the-lead-architect-of-the-tpu-i-see-many-inefficiencies-in-your-implementation"></a></p>
<p dir="auto">Hi Norm! Tim welcomes you to Santa Barbara to talk about all things TPU :)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Software details</h2><a id="user-content-software-details" aria-label="Permalink: Software details" href="#software-details"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">ISA</h3><a id="user-content-isa" aria-label="Permalink: ISA" href="#isa"></a></p>
<ul dir="auto">
<li>RHM src, dst, N
Read Host Memory.
Read <em>N</em> vectors from host memory beginning at address <em>src</em> and save them in the UB (unified buffer) beginning at address <em>dst</em>.</li>
<li>WHM src, dst, N
Write Host Memory.
Write <em>N</em> vectors from the UB beginning at address <em>src</em> to host memory beginning at address <em>dst</em>.</li>
<li>RW addr
Read Weights.
Load the weights tile from the weights DRAM at address <em>addr</em> into the on-chip FIFO.</li>
<li>MMC.{OS} src, dst, N
Matrix Multiply/Convolution.
Perform a matrix multiply operation on the <em>N</em> vectors beginning at UB address <em>src</em>, storing the result in the accumulator buffers beginning at address <em>dst</em>. If the <em>O</em> (overwrite) flag is specified, overwrite the contents of the accumulator buffers at the destination addresses; default behavior is to add to the value there and store the new sum. If the <em>S</em> (switch) flag is specified, switch to using the next tile of weights, which must have already been pre-loaded. The first <code>MMC</code> instruction in a program should always use the <em>S</em> flag.</li>
<li>ACT.{RQ} src, dst, N
Activate.
Perform activation on <em>N</em> vectors in the accumulator buffers starting at address <em>src</em>, storing the results in the UB beginning at address <em>dst</em>. Activation function is specified with a flag: <em>R</em> for ReLU and <em>Q</em> for sigmoid. With no flag, values are passed through without activation. Normalization is programmable at synthesis-time, but not at run-time; by default, after activation the upper 24 bits are dropped from each value, producing an 8-bit integer.</li>
<li>NOP
No op. Do nothing for one cycle.</li>
<li>HLT
Halt. Stop simulation.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Writing a Program</h3><a id="user-content-writing-a-program" aria-label="Permalink: Writing a Program" href="#writing-a-program"></a></p>
<p dir="auto">OpenTPU uses no dynamic scheduling; all execution is fully determinstic* and the hardware relies on the compiler to correctly schedule operations and pad NOPs to handle delays. This OpenTPU release does <br>
not support "repeat" flags on instructions, so many NOPs are required to ensure correct execution.</p>
<p dir="auto">*DRAM is a source of non-deterministic latency, discussed in the Memory Controller section of Microarchitecture.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Generating Data</h3><a id="user-content-generating-data" aria-label="Permalink: Generating Data" href="#generating-data"></a></p>
<p dir="auto"><strong>Application</strong></p>
<ol dir="auto">
<li>Simple one hot 2-layer NN</li>
</ol>
<p dir="auto">gen_one_hot.py generates 8b-int typed random squre matrix as training data and vector as label, example usage:</p>
<div data-snippet-clipboard-copy-content="python gen_one_hot.py --path simple_train --shape 8 8 --range -5 5
python gen_one_hot.py --path simple_train_label --shape 8 1 --range 0 2"><pre><code>python gen_one_hot.py --path simple_train --shape 8 8 --range -5 5
python gen_one_hot.py --path simple_train_label --shape 8 1 --range 0 2
</code></pre></div>
<p dir="auto">simple_nn.py trains a simple 2-layer nn on the given train/label dataset and writes the weights into a file, example usage (run gen_one_hot example first to generate the files):</p>
<div data-snippet-clipboard-copy-content="python simple_nn.py --path simple_train.npy --label simple_train_label.npy"><pre><code>python simple_nn.py --path simple_train.npy --label simple_train_label.npy
</code></pre></div>
<p dir="auto">After running the above command, two files are generated: simple_nn_weight_dram.npy is the 8b-int typed weight dram that the OpenTPU operates on, simple_nn_gt is the pickled ground truth 32b-float resulits and weights. To run with OpenTPU, a test file must also be generated, example usage:</p>
<div data-snippet-clipboard-copy-content="python gen_one_hot.py --path simple_test --shape 100 8 --range 1, 9"><pre><code>python gen_one_hot.py --path simple_test --shape 100 8 --range 1, 9
</code></pre></div>
<p dir="auto">After which simple_test.npy will be generated and it should be used as the host memory by OpenTPU.</p>
<p dir="auto">We also provide simple_nn.a -- the assembly program for this simple nn.</p>
<ol start="2" dir="auto">
<li>Tensorflow DNN regression</li>
</ol>
<p dir="auto">Although applications written in any high-level nn framework can be used, here we use tensorflow as an example.</p>
<p dir="auto">tf_nn.py trains a MLP regressor on the Boston Housing Dataset (<a href="https://archive.ics.uci.edu/ml/datasets/housing" rel="nofollow">https://archive.ics.uci.edu/ml/datasets/housing</a>). Example usage:</p>
<div data-snippet-clipboard-copy-content="python tf_nn.py --N 10 --save-input-path boston_input --save-weight-path boston_weights --save-output-path boston_output
python tf_nn.py --N 10 --save-input-path boston_input --save-weight-path boston_weights --save-output-path boston_output --raw"><pre><code>python tf_nn.py --N 10 --save-input-path boston_input --save-weight-path boston_weights --save-output-path boston_output
python tf_nn.py --N 10 --save-input-path boston_input --save-weight-path boston_weights --save-output-path boston_output --raw
</code></pre></div>
<p dir="auto">After running the above command, four files are generated: gt32.npy holds the ground truth prediction values, boston_input.npy holds the input test cases which is used as the host memeory for OpenTPU, boston_output.npy holds all the intermediate output values, and boston_weights.npy holds the weight matrices which are used as the weight dram for OpenTPU.</p>
<p dir="auto">Adding --raw to the command generates 32b-float typed files instead of 8b ints.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Latencies</h3><a id="user-content-latencies" aria-label="Permalink: Latencies" href="#latencies"></a></p>
<p dir="auto">The following gives the hardware execution latency for each instruction on OpenTPU:</p>
<ul dir="auto">
<li>RHM - <em>M</em> cycles for reading <em>M</em> vectors</li>
<li>WHM - <em>M</em> cycles for writing <em>M</em> vectors</li>
<li>RW - <em>N*N</em>/64 cycles for <em>N_x_N</em> MM Array for DRAM transfer, and up to 3 additional cycles to propagate through the FIFO</li>
<li>MMC - <em>L+2N</em> cycles, for <em>N_x_N</em> MM Array and <em>L</em> vectors multiplied in the instruction</li>
<li>ACT - <em>L+1</em> cycles, for <em>L</em> vectors activated in the instruction</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Microarchitecture</h2><a id="user-content-microarchitecture" aria-label="Permalink: Microarchitecture" href="#microarchitecture"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Matrix Multiply (MM) Unit</h3><a id="user-content-matrix-multiply-mm-unit" aria-label="Permalink: Matrix Multiply (MM) Unit" href="#matrix-multiply-mm-unit"></a></p>
<p dir="auto">The core of the compute of the OpenTPU is the parametrizable array of 8-bit Multiply-Accumulate Units (MACs), each consisting of an 8-bit integer multiplier and an integer adder of between 16 and 32 bits<br>
*. Each MAC has two buffers storing 8-bit weights (the second buffer allows weight programming to happen in parallel). Input vectors enter the array from the left, with values advancing one unit to the r<br>
ight each cycle. Each unit multiplies the input value by the active weight, adds it to the value from the unit above, and passes the result to the unit below. Input vectors are fed diagonally so that val<br>
ues align correctly as partial sums flow down the array.</p>
<p dir="auto">*The multipliers produce 16-bit outputs; as values move down the columns of the array, each add produces 1 extra bit. Width is capped at 32, creating the potential for uncaught overflow.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Accumulator Buffers</h3><a id="user-content-accumulator-buffers" aria-label="Permalink: Accumulator Buffers" href="#accumulator-buffers"></a></p>
<p dir="auto">Result vectors from the MM Array are written to a software-specified address in a set of accumulator buffers. Instructions indicate whether values should be added into the value already at the address or<br>
overwrite it. MM instructions read from the Unified Buffer (UB) and write to the accumulator buffers; activate instructions read from the accumulator buffers and write to the UB.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Weight FIFO</h3><a id="user-content-weight-fifo" aria-label="Permalink: Weight FIFO" href="#weight-fifo"></a></p>
<p dir="auto">At scale (256x256 MACs), a full matrix of weights (a "tile") is 64KB; to avoid stalls while weights are moved from off-chip weight DRAM, a 4-entry FIFO is used to buffer tiles. It is assumed the connecti<br>
on to the weight DRAM is a standard DDR interface moving data in 64-byte chunks (memory controllers are currently emulated with no simulated delay, so one chunk arrives each cycle). When an MM instructio<br>
n carries the "switch" flag, each MAC switches the active weight buffer as first vector of the instruction propagates through the array. Once it reaches the end of the first row, the FIFO begins feeding <br>
new weight values into the free buffers of the array. New weight values are passed down through the array each cycle until each row reaches its destination.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Systolic Setup</h3><a id="user-content-systolic-setup" aria-label="Permalink: Systolic Setup" href="#systolic-setup"></a></p>
<p dir="auto">Vectors are read all at once from the Unified Buffer, but must be fed diagonally into the MM Array. This is accomplished with a set of sequential buffers in a lower triangular configuration. The top valu<br>
e reaches the matrix immediately, the second after one cycle, the third after two, etc., so that each value reaches a MAC at the same time as the corresponding partial sum from the same source vector.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Memory Controllers</h3><a id="user-content-memory-controllers" aria-label="Permalink: Memory Controllers" href="#memory-controllers"></a></p>
<p dir="auto">Currently, memory controllers are emulated and have no delay. The connection to Host Memory is currently the size of one vector. The connection to the Weight DRAM uses a standard width of 64 bytes.</p>
<p dir="auto">Because the emulated controllers can return a new value each cycle, the OpenTPU hardware simulation currently has no non-detministic delay. With a more accurate DRAM interface that may encounter dynamic <br>
delays, programs would need to either take care to schedule for the worst-case memory delay, or make use of another instruction to ensure memory operations complete before the values are required*.</p>
<p dir="auto">*We note that the TPU "SYNC" instruction may fulfill this purpose, but is currently unimplemented on OpenTPU.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">Unified Buffer size, Accumulator Buffer size, and the size of the MM Array can all be specified in config.py. However, the MM Array must always be square, and vectors/weights are always composed of 8-bit integers.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: My LLM CLI tool can run tools now, from Python code or plugins (383 pts)]]></title>
            <link>https://simonwillison.net/2025/May/27/llm-tools/</link>
            <guid>44110584</guid>
            <pubDate>Tue, 27 May 2025 20:53:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/May/27/llm-tools/">https://simonwillison.net/2025/May/27/llm-tools/</a>, See on <a href="https://news.ycombinator.com/item?id=44110584">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-permalink-context="/2025/May/27/llm-tools/">

<p>27th May 2025</p>



<p><strong><a href="https://llm.datasette.io/en/stable/changelog.html#v0-26">LLM 0.26</a></strong> is out with the biggest new feature since I started the project: <a href="https://llm.datasette.io/en/stable/tools.html"><strong>support for tools</strong></a>. You can now use the LLM <a href="https://llm.datasette.io/en/stable/usage.html">CLI tool</a>—and <a href="https://llm.datasette.io/en/stable/python-api.html">Python library</a>—to grant LLMs from OpenAI, Anthropic, Gemini and local models from Ollama with access to any tool that you can represent as a Python function.</p>
<p>LLM also now has <a href="https://llm.datasette.io/en/stable/plugins/directory.html#tools">tool plugins</a>, so you can install a plugin that adds new capabilities to whatever model you are currently using.</p>
<p>There’s a lot to cover here, but here are the highlights:</p>
<ul>
<li>
<strong>LLM can run tools now</strong>! You can <strong>install tools from plugins</strong> and load them by name with <code>--tool/-T name_of_tool</code>.</li>
<li>You can also <strong>pass in Python function code on the command-line</strong> with the <code>--functions</code> option.</li>
<li>The <strong>Python API supports tools too</strong>: <code>llm.get_model("gpt-4.1").chain("show me the locals", tools=[locals]).text()</code>
</li>
<li>Tools work in <strong>both async and sync contexts</strong>.</li>
</ul>
<p>Here’s what’s covered in this post:</p>
<ul>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#trying-it-out">Trying it out</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#more-interesting-tools-from-plugins">More interesting tools from plugins</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#ad-hoc-command-line-tools-with-functions">Ad-hoc command-line tools with --functions</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#tools-in-the-llm-python-api">Tools in the LLM Python API</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#why-did-this-take-me-so-long-">Why did this take me so long?</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#is-this-agents-then-">Is this agents then?</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#what-s-next-for-tools-in-llm-">What’s next for tools in LLM?</a></li>
</ul>


<h4 id="trying-it-out">Trying it out</h4>
<p>First, <a href="https://llm.datasette.io/en/stable/setup.html">install the latest LLM</a>. It may not be on Homebrew yet so I suggest using <code>pip</code> or <code>pipx</code> or <code>uv</code>:</p>

<p>If you have it already, <a href="https://llm.datasette.io/en/stable/setup.html#upgrading-to-the-latest-version">upgrade it</a>.</p>

<p>Tools work with other vendors, but let’s stick with OpenAI for the moment. Give LLM an OpenAI API key</p>
<div><pre>llm keys <span>set</span> openai
<span><span>#</span> Paste key here</span></pre></div>
<p>Now let’s run our first tool:</p>
<div><pre>llm --tool llm_version <span><span>"</span>What version?<span>"</span></span> --td</pre></div>
<p>Here’s what I get:</p>
<p><img src="https://static.simonwillison.net/static/2025/llm-tools.gif" alt="Animated demo. I run that command, LLM shows Tool call: llm_version({}) in yellow, then 0.26a1 in green, then streams out the text The installed version is 0.26a1"></p>
<p><code>llm_version</code> is a very simple demo tool that ships with LLM. Running <code>--tool llm_version</code> exposes that tool to the model—you can specify that multiple times to enable multiple tools, and it has a shorter version of <code>-T</code> to save on typing.</p>
<p>The <code>--td</code> option stands for <code>--tools-debug</code>—it causes LLM to output information about tool calls and their responses so you can peek behind the scenes.</p>
<p>This is using the default LLM model, which is usually <code>gpt-4o-mini</code>. I switched it to <code>gpt-4.1-mini</code> (better but fractionally more expensive) by running:</p>
<div><pre>llm models default gpt-4.1-mini</pre></div>
<p>You can try other models using the <code>-m</code> option. Here’s how to run a similar demo of the <code>llm_time</code> built-in tool using <code>o4-mini</code>:</p>
<div><pre>llm --tool llm_time <span><span>"</span>What time is it?<span>"</span></span> --td -m o4-mini</pre></div>
<p>Outputs:</p>
<blockquote>
<p><code>Tool call: llm_time({})</code></p>
<div><pre>  {
    <span>"utc_time"</span>: <span><span>"</span>2025-05-27 19:15:55 UTC<span>"</span></span>,
    <span>"utc_time_iso"</span>: <span><span>"</span>2025-05-27T19:15:55.288632+00:00<span>"</span></span>,
    <span>"local_timezone"</span>: <span><span>"</span>PDT<span>"</span></span>,
    <span>"local_time"</span>: <span><span>"</span>2025-05-27 12:15:55<span>"</span></span>,
    <span>"timezone_offset"</span>: <span><span>"</span>UTC-7:00<span>"</span></span>,
    <span>"is_dst"</span>: <span>true</span>
  }</pre></div>
<p>The current time is 12:15 PM PDT (UTC−7:00) on May 27, 2025, which corresponds to 7:15 PM UTC.</p>
</blockquote>
<p>Models from (tool supporting) plugins work too. Anthropic’s Claude Sonnet 4:</p>
<div><pre>llm install llm-anthropic -U
llm keys <span>set</span> anthropic
<span><span>#</span> Paste Anthropic key here</span>
llm --tool llm_version <span><span>"</span>What version?<span>"</span></span> --td -m claude-4-sonnet</pre></div>
<p>Or Google’s Gemini 2.5 Flash:</p>
<div><pre>llm install llm-gemini -U
llm keys <span>set</span> gemini
<span><span>#</span> Paste Gemini key here</span>
llm --tool llm_version <span><span>"</span>What version?<span>"</span></span> --td -m gemini-2.5-flash-preview-05-20</pre></div>
<p>You can even run simple tools with Qwen3:4b, a <em>tiny</em> (2.6GB) model that I run using <a href="https://ollama.com/">Ollama</a>:</p>
<div><pre>ollama pull qwen3:4b
llm install <span><span>'</span>llm-ollama&gt;=0.11a0<span>'</span></span>
llm --tool llm_version <span><span>"</span>What version?<span>"</span></span> --td -m qwen3:4b</pre></div>
<p>Qwen 3 calls the tool, thinks about it a bit and then prints out a response:
<img src="https://static.simonwillison.net/static/2025/llm-tools-qwen.jpg" alt="Tool call: llm_version({}) 0.26a1<think> Okay, the user asked, &quot;What version?&quot; I need to respond with the version of the LLM. The tool provided is llm_version, which returns the installed version. I called that function and got the response 0.26a1. Now I should present this information clearly. Let me check if there's any additional context needed, but the user just asked for the version, so a straightforward answer should work. I'll state the version number and maybe mention that it's the installed version. Keep it simple and precise. </think> The installed version of the LLM is 0.26a1."></p>
<h4 id="more-interesting-tools-from-plugins">More interesting tools from plugins</h4>
<p>This demo has been pretty weak so far. Let’s do something a whole lot more interesting.</p>
<p>LLMs are notoriously bad at mathematics. This is deeply surprising to many people: supposedly the most sophisticated computer systems we’ve ever built can’t multiply two large numbers together?</p>
<p>We can fix that with tools.</p>
<p>The <a href="https://github.com/simonw/llm-tools-simpleeval">llm-tools-simpleeval</a> plugin exposes the <a href="https://github.com/danthedeckie/simpleeval">simpleeval</a> “Simple Safe Sandboxed Extensible Expression Evaluator for Python” library by Daniel Fairhead. This provides a robust-enough sandbox for executing simple Python expressions.</p>
<p>Here’s how to run a calculation:</p>
<div><pre>llm install llm-tools-simpleeval
llm -T simpleeval </pre></div>
<p>Trying that out:</p>
<div><pre>llm -T simple_eval <span><span>'</span>Calculate 1234 * 4346 / 32414 and square root it<span>'</span></span> --td</pre></div>
<p>I got back this—it tried <code>sqrt()</code> first, then when that didn’t work switched to <code>** 0.5</code> instead:</p>
<pre><code>Tool call: simple_eval({'expression': '1234 * 4346 / 32414'})
  165.45208860368976


Tool call: simple_eval({'expression': 'sqrt(1234 * 4346 / 32414)'})
  Error: Function 'sqrt' not defined, for expression 'sqrt(1234 * 4346 / 32414)'.


Tool call: simple_eval({'expression': '(1234 * 4346 / 32414) ** 0.5'})
  12.862818066181678

The result of (1234 * 4346 / 32414) is approximately
165.45, and the square root of this value is approximately 12.86.
</code></pre>
<p>I’ve released four tool plugins so far:</p>
<ul>
<li>
<strong><a href="https://github.com/simonw/llm-tools-simpleeval">llm-tools-simpleeval</a></strong>—as shown above, simple expression support for things like mathematics.</li>
<li>
<strong><a href="https://github.com/simonw/llm-tools-quickjs">llm-tools-quickjs</a></strong>—provides access to a sandboxed QuickJS JavaScript interpreter, allowing LLMs to run JavaScript code. The environment persists between calls so the model can set variables and build functions and reuse them later on.</li>
<li>
<strong><a href="https://github.com/simonw/llm-tools-sqlite">llm-tools-sqlite</a></strong>—read-only SQL query access to a local SQLite database.</li>
<li>
<strong><a href="https://github.com/simonw/llm-tools-datasette">llm-tools-datasette</a></strong>—run SQL queries against a remote <a href="https://datasette.io/">Datasette</a> instance!</li>
</ul>
<p>Let’s try that Datasette one now:</p>
<div><pre>llm install llm-tools-datasette
llm -T <span><span>'</span>Datasette("https://datasette.io/content")<span>'</span></span> --td <span><span>"</span>What has the most stars?<span>"</span></span></pre></div>
<p>The syntax here is slightly different: the Datasette plugin is what I’m calling a “toolbox”—a plugin that has multiple tools inside it and can be configured with a constructor.</p>
<p>Specifying <code>--tool</code> as <code>Datasette("https://datasette.io/content")</code> provides the plugin with the URL to the Datasette instance it should use—in this case the <a href="https://datasette.io/content">content database</a> that powers the Datasette website.</p>
<p>Here’s the output, with the schema section truncated for brevity:</p>
<p><img src="https://static.simonwillison.net/static/2025/datasette-tool.jpg" alt="I run that command. It first does a Tool call to Datasette_query with SELECT name, stars, FROM repos ORDER BY stars DESC LIMIT 1. This returns an error message because there is no such column stars. It calls the Datasette_schema() function which returns a whole load of CREATE TABLE statements. Then it executes Datasette_query again this time with SELECT name, stargazers_count FROM repos ORDER BY stargazers_count DESC LIMIT 1. This returns name=datasette a count of 10020, so the model replies and says The repository with the most stars is &quot;datasette&quot; with 10,020 stars."></p>
<p>This question triggered three calls. The model started by guessing the query! It tried <code>SELECT name, stars FROM repos ORDER BY stars DESC LIMIT 1</code>, which failed because the <code>stars</code> column doesn’t exist.</p>
<p>The tool call returned an error, so the model had another go—this time calling the <code>Datasette_schema()</code> tool to get the schema of the database.</p>
<p>Based on that schema it assembled and then executed the correct query, and output its interpretation of the result:</p>
<blockquote>
<p>The repository with the most stars is “datasette” with 10,020 stars.</p>
</blockquote>
<p>Getting to this point was a real <a href="https://www.penny-arcade.com/comic/2010/09/17/mine-all-mine-part-one">Penny Arcade Minecraft moment</a> for me. The possibilities here are <em>limitless</em>. If you can write a Python function for it, you can trigger it from an LLM.</p>
<h4 id="ad-hoc-command-line-tools-with-functions">Ad-hoc command-line tools with <code>--functions</code>
</h4>
<p>I’m looking forward to people building more plugins, but there’s also much less structured and more ad-hoc way to use tools with the LLM CLI tool: the <code>--functions</code> option.</p>
<p>This was inspired by a similar feature <a href="https://sqlite-utils.datasette.io/en/stable/cli.html#defining-custom-sql-functions">I added to sqlite-utils</a> a while ago.</p>
<p>You can pass a block of literal Python code directly to the CLI tool using the <code>--functions</code> option, and any functions defined there will be made available to the model as tools.</p>
<p>Here’s an example that adds the ability to search my blog:</p>
<div><pre>llm --functions <span><span>'</span></span>
<span>import httpx</span>
<span>
<span>def search_blog(q):</span>
<span>    "Search Simon Willison blog"</span>
<span>    return httpx.get("https://simonwillison.net/search/", params={"q": q}).content</span>
<span><span>'</span></span> --td <span><span>'</span>Three features of sqlite-utils<span>'</span></span> -s <span><span>'</span>use Simon search<span>'</span></span></span></pre></div>
<p>This is <em>such a hack</em> of an implementation! I’m literally just hitting <a href="https://simonwillison.net/search/?q=pelicans">my search page</a> and dumping the HTML straight back into tho model.</p>
<p>It totally works though—it helps that the GPT-4.1 series all handle a million tokens now, so crufty HTML is no longer a problem for them.</p>
<p>(I had to add “use Simon search” as the system prompt because without it the model would try to answer the question itself, rather than using the search tool I provided. System prompts for tools are clearly a <em>big topic</em>, Anthropic’s own web search tool has <a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#search-instructions">6,471 tokens of instructions</a>!)</p>
<p>Here’s the output I got just now:</p>
<blockquote>
<p>Three features of sqlite-utils are:</p>
<ol>
<li>It is a combined CLI tool and Python library for manipulating SQLite databases.</li>
<li>It can automatically add columns to a database table if you attempt to insert data that doesn’t quite fit (using the alter=True option).</li>
<li>It supports plugins, allowing the extension of its functionality through third-party or custom plugins.</li>
</ol>
</blockquote>
<p>A better search tool would have more detailed instructions and would return relevant snippets of the results, not just the headline and first paragraph for each result. This is pretty great for just four lines of Python though!</p>
<h4 id="tools-in-the-llm-python-api">Tools in the LLM Python API</h4>
<p>LLM is both a CLI tool and a Python library at the same time (similar to my other project <a href="https://sqlite-utils.datasette.io/">sqlite-utils</a>). The LLM Python library <a href="https://llm.datasette.io/en/stable/python-api.html#tools">grew tool support</a> in LLM 0.26 as well.</p>
<p>Here’s a simple example solving one of the previously hardest problems in LLMs: counting the number of Rs in “strawberry”:</p>
<pre><span>import</span> <span>llm</span>

<span>def</span> <span>count_char_in_text</span>(<span>char</span>: <span>str</span>, <span>text</span>: <span>str</span>) <span>-&gt;</span> <span>int</span>:
    <span>"How many times does char appear in text?"</span>
    <span>return</span> <span>text</span>.<span>count</span>(<span>char</span>)

<span>model</span> <span>=</span> <span>llm</span>.<span>get_model</span>(<span>"gpt-4.1-mini"</span>)
<span>chain_response</span> <span>=</span> <span>model</span>.<span>chain</span>(
    <span>"Rs in strawberry?"</span>,
    <span>tools</span><span>=</span>[<span>count_char_in_text</span>],
    <span>after_call</span><span>=</span><span>print</span>
)
<span>for</span> <span>chunk</span> <span>in</span> <span>chain_response</span>:
    <span>print</span>(<span>chunk</span>, <span>end</span><span>=</span><span>""</span>, <span>flush</span><span>=</span><span>True</span>)</pre>
<p>The <code>after_call=print</code> argument is a way to peek at the tool calls, the Python equivalent of the <code>--td</code> option from earlier.</p>
<p>The <code>model.chain()</code> method is new: it’s similar to <code>model.prompt()</code> but knows how to spot returned tool call requests, execute them and then prompt the model again with the results. A <code>model.chain()</code> could potentially execute dozens of responses on the way to giving you a final answer.</p>
<p>You can iterate over the <code>chain_response</code> to output those tokens as they are returned by the model, even across multiple responses.</p>
<p>I got back this:</p>
<blockquote>
<p><code>Tool(name='count_char_in_text', description='How many times does char appear in text?', input_schema={'properties': {'char': {'type': 'string'}, 'text': {'type': 'string'}}, 'required': ['char', 'text'], 'type': 'object'}, implementation=&lt;function count_char_in_text at 0x109dd4f40&gt;, plugin=None) ToolCall(name='count_char_in_text', arguments={'char': 'r', 'text': 'strawberry'}, tool_call_id='call_DGXcM8b2B26KsbdMyC1uhGUu') ToolResult(name='count_char_in_text', output='3', tool_call_id='call_DGXcM8b2B26KsbdMyC1uhGUu', instance=None, exception=None)</code><br></p>
<p>There are 3 letter “r”s in the word “strawberry”.</p>
</blockquote>
<p>LLM’s Python library also supports <code>asyncio</code>, and tools can be <code>async def</code> functions <a href="https://llm.datasette.io/en/latest/python-api.html#tool-functions-can-be-sync-or-async">as described here</a>. If a model requests multiple async tools at once the library will run them concurrently with <code>asyncio.gather()</code>.</p>
<p>The Toolbox form of tools is supported too: you can pass <code>tools=[Datasette("https://datasette.io/content")]</code> to that <code>chain()</code> method to achieve the same effect as the <code>--tool 'Datasette(...)</code> option from earlier.</p>
<h4 id="why-did-this-take-me-so-long-">Why did this take me so long?</h4>
<p>I’ve been tracking <a href="https://simonwillison.net/tags/llm-tool-use/">llm-tool-use</a> for a while. I first saw the trick described in <a href="https://arxiv.org/abs/2210.03629">the ReAcT paper</a>, first published in October 2022 (a month before the initial release of ChatGPT). I built <a href="https://til.simonwillison.net/llms/python-react-pattern">a simple implementation of that</a> in a few dozen lines of Python. It was clearly a very neat pattern!</p>
<p>Over the past few years it has become <em>very</em> apparent that tool use is the single most effective way to extend the abilities of language models. It’s such a simple trick: you tell the model that there are tools it can use, and have it output special syntax (JSON or XML or <code>tool_name(arguments)</code>, it doesn’t matter which) requesting a tool action, then stop.</p>
<p>Your code parses that output, runs the requested tools and then starts a new prompt to the model with the results.</p>
<p>This works with almost <strong>every model</strong> now. Most of them are specifically trained for tool usage, and there are leaderboards like the <a href="https://gorilla.cs.berkeley.edu/leaderboard.html">Berkeley Function-Calling Leaderboard</a> dedicated to tracking which models do the best job of it.</p>
<p>All of the big model vendors—OpenAI, Anthropic, Google, Mistral, Meta—have a version of this baked into their API, either called tool usage or function calling. It’s all the same underlying pattern.</p>
<p>The models you can run locally are getting good at this too. Ollama <a href="https://ollama.com/blog/tool-support">added tool support</a> last year, and it’s baked into the <a href="https://github.com/ggml-org/llama.cpp/blob/master/docs/function-calling.md">llama.cpp</a> server as well.</p>
<p>It’s been clear for a while that LLM absolutely needed to grow support for tools. I released <a href="https://simonwillison.net/2025/Feb/28/llm-schemas/">LLM schema support</a> back in February as a stepping stone towards this. I’m glad to finally have it over the line.</p>
<p>As always with LLM, the challenge was designing an abstraction layer that could work across as many different models as possible. A year ago I didn’t feel that model tool support was mature enough to figure this out. Today there’s a very definite consensus among vendors about how this should work, which finally gave me the confidence to implement it.</p>
<p>I also presented a workshop at PyCon US two weeks ago about <a href="https://simonwillison.net/2025/May/15/building-on-llms/">Building software on top of Large Language Models</a>, which was exactly the incentive I needed to finally get this working in an alpha! Here’s the <a href="https://building-with-llms-pycon-2025.readthedocs.io/en/latest/tools.html">tools section</a> from that tutorial.</p>
<h4 id="is-this-agents-then-">Is this agents then?</h4>
<p><em>Sigh</em>.</p>
<p>I still <a href="https://simonwillison.net/2024/Dec/31/llms-in-2024/#-agents-still-haven-t-really-happened-yet">don’t like</a> using the term “agents”. I worry that developers will think <a href="https://simonwillison.net/2025/May/22/tools-in-a-loop/">tools in a loop</a>, regular people will think virtual AI assistants <a href="https://en.m.wikipedia.org/wiki/Her_(2013_film)">voiced by Scarlett Johansson</a> and academics will <a href="https://simonwillison.net/2025/Mar/19/worms-and-dogs-and-countries/">grumble about thermostats</a>. But in the LLM world we appear to be converging on “tools in a loop”, and that’s absolutely what this.</p>
<p>So yes, if you want to build “agents” then LLM 0.26 is a great way to do that.</p>
<h4 id="what-s-next-for-tools-in-llm-">What’s next for tools in LLM?</h4>
<p>I already have a <a href="https://github.com/simonw/llm/milestone/13">LLM tools v2 milestone</a> with 13 issues in it, mainly around improvements to how tool execution logs are displayed but with quite a few minor issues I decided shouldn’t block this release. There’s a bunch more stuff in the <a href="https://github.com/simonw/llm/issues?q=is%3Aissue%20state%3Aopen%20label%3Atools">tools label</a>.</p>
<p>I’m most excited about the potential for plugins.</p>
<p>Writing tool plugins is <em>really fun</em>. I have an <a href="https://github.com/simonw/llm-plugin-tools">llm-plugin-tools</a> cookiecutter template that I’ve been using for my own, and I plan to put together a tutorial around that soon.</p>
<p>There’s more work to be done adding tool support to more model plugins. I added <a href="https://llm.datasette.io/en/stable/plugins/advanced-model-plugins.html#supporting-tools">details of this</a> to the advanced plugins documentation. This commit <a href="https://github.com/simonw/llm-gemini/commit/a7f1096cfbb733018eb41c29028a8cc6160be298">adding tool support for Gemini</a> is a useful illustratino of what’s involved.</p>

<p>And yes, <strong>Model Context Protocol</strong> support is clearly on the agenda as well. MCP is emerging as the standard way for models to access tools at a frankly bewildering speed. Two weeks ago it wasn’t directly supported by the APIs of any of the major vendors. In just the past eight days <a href="https://simonwillison.net/2025/May/27/mistral-agents-api/">it’s been added</a> by OpenAI, Anthropic <em>and</em> Mistral! It’s feeling like a lot less of a moving target today.</p>
<p>I want LLM to be able to act as an MCP client, so that any of the MCP servers people are writing can be easily accessed as additional sources of tools for LLM.</p>
<p>If you’re interested in talking more about what comes next for LLM, <a href="https://datasette.io/discord-llm">come and chat to us in our Discord</a>.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why the Original Macintosh Had a Screen Resolution of 512×324 (161 pts)]]></title>
            <link>https://512pixels.net/2025/05/original-macintosh-resolution/</link>
            <guid>44110219</guid>
            <pubDate>Tue, 27 May 2025 20:02:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://512pixels.net/2025/05/original-macintosh-resolution/">https://512pixels.net/2025/05/original-macintosh-resolution/</a>, See on <a href="https://news.ycombinator.com/item?id=44110219">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-32349">
	<!-- .entry-header -->

	<div>
		<p>Many classic Macs came with — or supported — displays running at 512×384 pixels, but many compact Macs, ranging from <a href="https://support.apple.com/en-us/112190">the original 1984 machine</a> up through 1991’s <a href="https://support.apple.com/en-us/112201">Macintosh Classic II</a> had built-in CRTs running at 512×342 pixels. That covers all black-and-white compact Macs with a 9-inch screen. The later Color Classic and Color Classic II used a 10-inch CRT at a full 512×384.</p>
<p>This came up when <a href="https://512pixels.net/2025/05/oh-hey-it-me/">I joined John Gruber on The Talk Show</a>. At one point in the show, I rattled off the original Mac’s resolution as being 512×384.</p>
<p>Except… it wasn’t. The original Mac screen ran at 512×342. I remembered the right number and corrected myself a moment later, but given the name of this website, it was pretty embarrassing. This set me off on a journey to understand why Apple made this decision. Why were the displays on early Macs 42 pixels shorter in height than later ones?</p>
<p>After doing <em>a lot</em> of reading, there are several factors to consider when trying to answer this question.</p>
<p><img decoding="async" src="https://512pixels.net/wp-content/uploads/2025/05/apple1984mac-full.png" alt="Original Macintosh"></p>
<h2>Memory</h2>
<p>The original Mac had a mere 128 <em>kilobytes</em> of memory. The photo of the original Macintosh in this blog post is 604 KB in size, some 4.7x larger than the entire memory footprint of the 1984 machine. Of course, Apple would improve this with later models, but many design decisions made to accommodate the original Mac would forward for years.</p>
<p>Over at Folklore.org, <a href="https://www.folklore.org/Five_Different_Macs.html">Andy Hertzfeld wrote a great post</a> walking through several early versions of what would become the Macintosh, including ones with even <em>less</em> memory:</p>
<blockquote><p>
  In the beginning of 1982, the original 68000 design was more than a year old, and the software was nowhere near finished, so Burrell [Smith] was afraid some of the trade-offs of the original design were no longer current. He used the expansive canvas of a custom chip, where additional logic was almost free, to update the architecture.</p>
<p>  The most important decision was admitting that the software would never fit into 64K of memory and going with a full 16-bit memory bus, requiring 16 RAM chips instead of 8. The extra memory bandwidth allowed him to double the display resolution, going to dimensions of 512 by 342 instead of 384 by 256. He also added bells and whistles like a fancy, DMA-fed sound generator with four independent voices. This was the fourth version of the Macintosh design.
</p></blockquote>
<p>Shipping a computer in the 1980s with a resolution of 384×256 wouldn’t have been too wild. 1982’s Commodore 64 ran at a maximum resolution 320×200. The Apple IIe that shipped in 1983 offered several display modes:</p>
<ul>
<li>40 or 80 columns text, white-on-black, with 24 lines</li>
<li>Low-Resolution: 40×48 (16 colors)</li>
<li>High-Resolution: 280×192 (6 colors)</li>
<li>Double-Low-Resolution: 80×48 (16 colors)</li>
<li>Double-High-Resolution: 560×192 (16 colors)</li>
</ul>
<p>Computers like the C64 and Apple II had to pull off a lot of tricks to pull off getting graphics on the screen. The Macintosh was going to be powered by a full GUI, and 384×256 would have been just too chunky, so thinking about 128 kilobytes of RAM as an <em>upgrade</em> is a fun twist on the normal take of “Wow, the original Mac was so held back!” Really, it’s amazing that it could do what it did.</p>
<p>That feeling takes on new life when you realize the Mac used a portion of its memory to drive the display. At 512×342, the memory needed to draw the screen was a total of 21.8 KB. Had Apple opted for a 4:3 display running at 512×384, the system would have needed 24 KB for the display. Every byte was precious back in the day. Again, <a href="https://folklore.org/Monkey_Lives.html">we turn to Andy Hertzfeld</a>:</p>
<blockquote><p>
  The original Macintosh only had 128K bytes of RAM (that’s one eighth of a megabyte), so dealing with memory management was usually the hardest part of writing both the system and applications. We allocated around 16K bytes for system use, and another 22K bytes for the 512 by 342 black and white screen, so applications were left with only 90K bytes or so. The bigger ones like MacWrite or MacPaint seemed to be bursting at the seams.
</p></blockquote>
<p>It seems that two things are true:</p>
<ul>
<li>The original Macintosh shipped with more memory than earlier designs</li>
<li>Even at 128K, things were extremely tight</li>
</ul>
<p>Daniel Knight pointed this out when writing about the original Mac:</p>
<blockquote><p>
  As&nbsp;<a href="http://www.folklore.org/StoryView.py?project=Macintosh&amp;story=Scrooge_McDuck.txt&amp;topic=Origins&amp;sortOrder=Sort%20by%20Date">Andy Hertzfeld writes</a>, the Mac was only going to have a 256×256 pixel display (a step up from the 280×192 graphics of the Apple II). It wasn’t until&nbsp;<a href="http://www.folklore.org/StoryView.py?project=Macintosh&amp;story=Good_Earth.txt&amp;topic=Origins&amp;sortOrder=Sort%20by%20Date">January 1981</a>&nbsp;that the Mac team decided to give the Motorola 68000 a try. A good thing, too, as the first Mac shipped with a 512×342 pixel display, and that would have consumed over 30% of the 64 KB of memory originally envisioned for the low-cost information appliance.
</p></blockquote>
<h2>CPU Timing</h2>
<p>At the heart of the Macintosh was a Motorola 68000 CPU running at 8 MHz. Just like the 128 kilobytes of RAM, this came with some inherit limitations when paired with the display hardware.</p>
<p>To minimize CRT flicker, Apple worked to achieve a vertical refresh rate of 60 Hz. This meant the CPU spent one-third of its time drawing the display. Just as with the memory, a taller screen would have taken more resources away from running the Mac’s operating system and programs.</p>
<p>If you are familiar with standard TV formats, you probably have already picked up on the fact that this refresh rate/screen size combination meant the Mac was incompatible with NTSC composite video, which the Apple II supported. (It’s also different than PAL systems.) This let Apple balance performance and picture quality in a way the team saw fit, given the hardware that they had, <a href="https://www.folklore.org/Joining_Apple_Computer.html">as Bill Atkinson wrote:</a></p>
<blockquote><p>
  The Apple II displayed white text on a black background. I argued that to do graphics properly we had to switch to a white background like paper. It works fine to invert text when printing, but it would not work for a photo to be printed in negative. The Lisa hardware team complained the screen would flicker too much, and they would need faster refresh with more expensive RAM to prevent smearing when scrolling. Steve listened to all the pros and cons then sided with a white background for the sake of graphics.
</p></blockquote>
<p>Here’s the thing: the original Mac <em>did not</em> run at 8 MHz, but rather 7.83 MHz. This slight tuning meant Apple could time the CPU’s cycles and the CRT’s need for updating more easily.</p>
<h2>Square Pixels</h2>
<p>Running the 9-inch CRT at 512×342 gave the original Mac a pixels density of 72 PPI, but more importantly, the screen size allowed the Mac to have square pixels.</p>
<p>Apple’s first GUI-powered machine, <a href="https://www.macstories.net/mac/the-lisa/">the Lisa</a>, famously had rectangular pixels, <a href="https://folklore.org/Square_Dots.html">as Andy Hertzfeld covered here</a>:</p>
<blockquote><p>
  The Lisa team decided to optimize their display for horizontal resolution, in order to be able to display 80 columns of text in an attractive font. The vertical resolution wasn’t as important, because vertical scrolling works much better for text than horizontal scrolling. The designers decided to endow Lisa with twice as much horizontal resolution as vertical, using a 720 by 360 pixel display, with pixels that were twice as high as they were wide. This was great for text oriented applications like the word processor, but it made things somewhat awkward for the more graphical applications.</p>
<p>  When Burrell redesigned the Macintosh in December 1980 to use the same microprocessor as Lisa, the Motorola 68000, it set off shock waves within Apple. Not only was Burrell’s new design much simpler than Lisa, with less than half the chip count, but it also ran almost twice as fast, using an 8 megahertz clock instead of a 5 megahertz clock. Among other advantages was the fact that the Mac’s 384 by 256 pixel display had the identical horizontal and vertical resolution, a feature that we called “square dots”. Square dots made it easier to write graphical applications, since you didn’t have to worry about the resolution disparity.
</p></blockquote>
<p>Hertzfeld goes on to share that the Mac team tried to get the Lisa team to switch to square pixels, bumping the machine’s resolution to a mind-blowing-for-the-time 768×512 pixels, but it wasn’t in the cards, as the Lisa was well on its way to shipping by the time this meeting took place.</p>
<p><img decoding="async" src="https://512pixels.net/wp-content/uploads/2025/05/apple-lisa.jpg" alt="Apple Lisa"></p>
<p>Of course, the Lisa would end up being a doomed product, and in 1985, Apple rebadged a later revision of the machine as the “Macintosh XL.” It shipped with <a href="https://en.wikipedia.org/wiki/MacWorks_XL">a software shim called “MacWorks XL”</a> that allowed Mac software to run on the Lisa, but the rectangular pixels made the software appear stretched. To solve this, Apple sold a product named the  Macintosh XL Screen Kit, which changed the resolution to 608×432 pixels. This is how the product is described in <a href="https://512pixels.net/wp-content/uploads/2025/05/Lisa-DIY-Guide.pdf">a document outlining DIY upgrades</a> from <a href="https://en.wikipedia.org/wiki/Sun_Remarketing">Sun Remarketing</a>, a company that was focused on keeping Lisa hardware up and running.</p>
<blockquote><p>
  No recently restored Lisa/Mac XL is complete without a Macintosh XL Screen Kit. Unlike the standard 9-inch Macintosh which has square pixels, the stock Lisa/XL has rectangular pixels. With rectangular pixels, circles look like footballs, squares look like spaghetti boxes. The purpose of the Macintosh XL Screen Kit is to square up the pixels. Proportions become exactly the same as on other Macs (1 to 1 ), but the overall display area (608 pixels x 432 pixels) is made roughly the same as a 12-inch Macintosh 11 WYSIWYG monitor (640×480). Standard 9-inch Macs only display 512×342 pixels.</p>
<p>  The complete screen modification kit includes new 3A boot ROMs, a new video ROM and a new yoke coil. (Newer software requires System Update 5.0 and MacWorks Plus as well.) Conscientious installation of the complete screen kit requires one to two hours.
</p></blockquote>
<h2>Mimicking the Real World</h2>
<p>In addition to their square pixels making on-screen graphics look better, the Macintosh team also wanted the computer to be useful for those who needed to print their work. The 72 DPI screen was more than sharp enough for work in applications like MacWrite, MacPaint, and the page layout tools that would follow them. Users could see their work full-sized or at a reduced scale to get a sense of the overall page they were working on. Larger displays would come, but for 1984, 512×342 was plenty.</p>
<h2>Everything in Balance</h2>
<p>In short, there’s no easy answer to explain why early compact Macs ran at a screen resolution of 512×342. Rather, Apple was doing what it does best: designing a product with the right trade-offs for performance, ease of use, and cost. In a few short years, the Mac would grow to support larger displays, but for 1984, the balance was set correctly.</p>
<p>In the very first edition of <em>Macworld</em> magazine, <a href="https://archive.org/details/MacWorld_8404_April_1984_premier">Matthew Douglas wrote</a>:</p>
<blockquote><p>
  Appearances can be deceiving. Most computers display text on one of 24 or 25 “invisible” horizontal lines on the screen. This display is called text mode. To display graphics, the software switches to graphics mode, and the display becomes a field of dots. Each dot, or pixel, is either off (invisible) or on (visible). Of course, a computer may have more than one text mode or two or more graphics modes, or it may be a mixed mode of graphics and text.</p>
<p>  The Mac display has only one mode: graphics. The entire screen is made up of dots: 512 dots horizontally and 342 dots vertically, a total of175,104 dots that combine to display everything you’ll ever see on a Mac screen. (Now you know the secret behind the incredible range of type fonts, attributes, and type sizes.)
</p></blockquote>
<p>In the same edition, David Bunnell interviewed Bill Gates, and he was asked about what made the Mac special. Gates — who had previously said the Mac was a computer he would want his mom to try — replied:</p>
<blockquote><p>
  The Mac was designed as a graphics machine. Apple didn’t put in a ROM character generator or a bunch of video modes. They put in only one video mode, and that’s the pure bit-mapped, 512-by 342-pixel screen. The monitor was designed into the machine so that they could get extremely crisp pictures and have one integrated system. They knew what the aspect ratio was and how the dots would appear. And they also made sure that the mouse would be used and that the 64K ROM would support very rich graphics interaction.</p>
<p>  You can configure a PC with one of the better graphics boards and add a Microsoft mouse and the necessary software, but that’s not the thrust of the machine. The PC is used primarily in its text mode, and to date it’s used mostly without a mouse; you couldn’t get performance or graphics like the Mac’s out of the PC at a comparable price. Although they’re both “turing” machines (that is, they have finite memory), the thrust of the Mac is quite different.</p>
<p>  Of all the personal computers available today, the Mac is unique. It’s the first time somebody said, “We don’t need a lot of the things that other personal computers have, so let’s optimize a few areas and make sure the software is designed around them.”
</p></blockquote>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Running GPT-2 in WebGL: Rediscovering the Lost Art of GPU Shader Programming (130 pts)]]></title>
            <link>https://nathan.rs/posts/gpu-shader-programming/</link>
            <guid>44109257</guid>
            <pubDate>Tue, 27 May 2025 18:02:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nathan.rs/posts/gpu-shader-programming/">https://nathan.rs/posts/gpu-shader-programming/</a>, See on <a href="https://news.ycombinator.com/item?id=44109257">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="content"><article><p><i>May, 24 2025 •
16 min read •
2461 words</i></p><p>Preface: A few weeks back, I implemented GPT-2 using WebGL and shaders (<a href="https://github.com/nathan-barry/gpt2-webgl/tree/main">Github Repo</a>) which made the front page of Hacker News (<a href="https://news.ycombinator.com/item?id=43870998">discussion</a>). By popular demand, here is a short write-up over the main ideas behind GPU shader programming (for general-purpose computing).</p><div><h2>Table Of Contents</h2><hr><div><nav id="TableOfContents"><ol><li><a href="#the-origins-of-general-purpose-gpu-programming">The Origins of General Purpose GPU Programming</a></li><li><a href="#graphics-api-vs-compute-api">Graphics API vs Compute API</a></li><li><a href="#implementing-gpt-2-with-shaders">Implementing GPT-2 with shaders</a><ol><li><a href="#textures--framebuffers-the-data-bus">Textures &amp; Framebuffers: The Data Bus</a></li><li><a href="#fragment-shaders-as-compute-kernels">Fragment Shaders as Compute Kernels</a></li><li><a href="#chaining-passes">Chaining Passes</a></li><li><a href="#limitations">Limitations</a></li></ol></li></ol></nav></div></div><h2 id="the-origins-of-general-purpose-gpu-programming">The Origins of General Purpose GPU Programming</h2><hr><p>In the early 2000s, NVIDIA introduced programmable shaders with the GeForce 3 (2001) and GeForce FX (2003). Instead of being limited to predetermined transformations and effects of earlier GPUs, developers were now given unprecedented control over the rendering pipeline, enabling much more sophisticated visual effects. These programmable shaders laid the foundation for modern GPU computing.</p><p>Researchers soon discovered that certain computations (like linear algebra involving matrices and vectors) could be accelerated by casting them as graphics operations on the GPU.
However, using shader languages like OpenGL’s GLSL for no-graphics tasks was cumbersome. By the mid-2000s, the need for a more straightforward, non-graphics interface to GPUs had become clear, and NVIDIA saw a new opportunity.</p><p>Inspired by the demand for <strong>general-purpose GPU (GPGPU)</strong> programming, in November 2006, NVIDIA released <strong>CUDA</strong>, the <strong>Compute Unified Device Architecture</strong>. CUDA is a parallel computing platform and programming model that gives developers direct access to the GPU’s computational power without the intermediary of a graphics API. With CUDA, one could write C/C++ code to execute on the CPU using straightforward extensions for parallel kernels and managing GPU memory explicitly. This meant that developers could now ignore graphics-specific concepts and dramatically lowered the barrier for general-purpose GPU computing. Following CUDA came OpenCL which expanded general purpose computing beyond the NVIDIA ecosystem.</p><h2 id="graphics-api-vs-compute-api">Graphics API vs Compute API</h2><hr><p>Traditional graphics APIs like OpenGL are centered around a fixed-function pipeline tailored for rendering images. The pipeline consists of stages like vertex processing, rasterization, fragment processing, etc. Each stage can be programmable with shaders, but the overall flow is fixed.
Using OpenGL for computation required a lot of boilerplate. One had to pack data into texture formats, use off-screen framebuffers to capture the results, and often perform multiple render passes to accomplish multi-stage algorithms.</p><p>In contrast, OpenCL and CUDA expose a direct compute model which lets you treat the GPU as one giant SIMD processor:</p><ul><li><strong>Kernels, not shaders</strong>: You write a function and then launch thousands of copies to run in parallel (no notion of vertices or fragments).</li><li><strong>Raw buffers</strong>: Allocate arrays of floats or integers, read/write them directly, and move them back and forth between host and device with explicit copy calls.</li><li><strong>User-driven pipeline</strong>: You define exactly what happens and when instead of using a predefined fixed sequence of rendering stages.</li></ul><p>The result is a far more natural fit for linear algebra, simulations, physics, ML, and any algorithm where you just want to compute independent calculations in bulk.</p><p>In OpenGL, the output of your computation would ultimately be pixels in a framebuffer or values in a texture; in OpenCL, the output can be data in any form (float arrays, computed lists of numbers, etc.) which you then transfer back to the CPU or use in further computations. This makes OpenCL more suitable for general algorithms where you just want the numerical results.</p><h2 id="implementing-gpt-2-with-shaders">Implementing GPT-2 with shaders</h2><hr><p>Below covers textures, framebuffers, vertex and fragment shaders, and other graphics specific concepts I hijacked to get GPT-2 running on a GPU using shaders.</p><h3 id="textures--framebuffers-the-data-bus">Textures &amp; Framebuffers: The Data Bus</h3><p>In traditional graphics rendering, a <strong>texture</strong> is simply a 2D (or 3D) array of pixel data stored in GPU memory. When you map a texture onto a triangle, the fragment shader “samples” it to look up color values. In our compute‐as‐graphics paradigm, we hijack this same mechanism to store and manipulate numerical data instead of colors:</p><ul><li><p><strong>Textures as tensors</strong>:
Each texture is an array of floating‐point values (we use single‐channel R32F formats), where each pixel encodes one element of a matrix or vector. Just like you’d think of an H×W texture as holding H×W RGB pixels for an image, here it holds H×W scalar values for a weight matrix or activation map.</p></li><li><p><strong>Sampling without filtering</strong>:
We use functions like <code>texelFetch</code> to read texture data by exact integer coordinates, bypassing any interpolation. This gives us deterministic, “random access” reads into our weight and activation arrays, akin to indexing into a CPU array by row and column.</p></li></ul><p>A <strong>Framebuffer Object (FBO)</strong> is a lightweight container that lets us redirect rendering output from the screen into one of our textures:</p><ol><li><p><strong>Attach a texture as the render target</strong>:
By binding a texture to an FBO, any draw call you make, normally destined for your monitor, writes into that texture instead. The fragment shader’s <code>out</code> variable becomes a write port into GPU memory.</p></li><li><p><strong>Offscreen and ping-pong rendering</strong>:
Because we can attach different textures in succession, we “ping-pong” between them: one pass writes into <strong>Texture A</strong>, the next pass reads from <strong>Texture A</strong> while writing into <strong>Texture B</strong>, and so on. This avoids ever copying data back to the CPU until the very end.</p></li><li><p><strong>High‐throughput data bus</strong>:
All of this happens entirely on the GPU’s VRAM bus. Binding textures and framebuffers is just pointer swapping on the GPU. Once set up, your fragment shader passes stream through millions of cores in parallel, reading, computing, and writing without ever touching system memory.</p></li></ol><p>Together, textures and FBOs form the <strong>data bus</strong> of our shader‐based compute engine: textures hold the raw bits of your neural network (weights, intermediate activations, and outputs), and framebuffers let you chain shader passes seamlessly, keeping everything on the high-speed GPU pipeline until you explicitly pull the final logits back to the CPU.</p><h3 id="fragment-shaders-as-compute-kernels">Fragment Shaders as Compute Kernels</h3><p>Fragment shaders are where the magic happens. Instead of using fragment shaders to shade pixels for display, we hijack them as compute kernels; each fragment invocation becomes one “thread” that calculates a single output value. The GPU will launch thousands of these in parallel, giving us massive throughput for neural-network operations.</p><p>Below is an example fragment shader for matrix multiplication:</p><div><pre tabindex="0"><code data-lang="glsl"><span><span><span>// Matrix Multiply (C = A × B)</span>
</span></span><span><span><span>#version 300 es</span>
</span></span><span><span><span>precision</span> <span>highp</span> <span>float</span><span>;</span>
</span></span><span><span>
</span></span><span><span><span>uniform</span> <span>sampler2D</span> u_A<span>;</span>    <span>// Texture holding matrix A (M x K)</span>
</span></span><span><span><span>uniform</span> <span>sampler2D</span> u_B<span>;</span>    <span>// Texture holding matrix B (K x N)</span>
</span></span><span><span><span>uniform</span> <span>int</span>        u_K<span>;</span>   <span>// Shared inner dimension</span>
</span></span><span><span><span>out</span> <span>vec4</span>           outColor<span>;</span>
</span></span><span><span>
</span></span><span><span><span>void</span> main<span>()</span> <span>{</span>
</span></span><span><span>  <span>// Determine the output coordinate (i, j) from the fragment’s pixel position</span>
</span></span><span><span>  <span>ivec2</span> coord <span>=</span> <span>ivec2</span><span>(</span>gl_FragCoord<span>.</span>xy<span>);</span>
</span></span><span><span>  <span>float</span> sum <span>=</span> <span>0.0</span><span>;</span>
</span></span><span><span>
</span></span><span><span>  <span>// Perform the dot–product along the K dimension</span>
</span></span><span><span>  <span>for</span> <span>(</span><span>int</span> k <span>=</span> <span>0</span><span>;</span> k <span>&lt;</span> u_K<span>;</span> <span>++</span>k<span>)</span> <span>{</span>
</span></span><span><span>    <span>float</span> a <span>=</span> texelFetch<span>(</span>u_A<span>,</span> <span>ivec2</span><span>(</span>k<span>,</span> coord<span>.</span>y<span>),</span> <span>0</span><span>).</span>r<span>;</span>
</span></span><span><span>    <span>float</span> b <span>=</span> texelFetch<span>(</span>u_B<span>,</span> <span>ivec2</span><span>(</span>coord<span>.</span>x<span>,</span> k<span>),</span> <span>0</span><span>).</span>r<span>;</span>
</span></span><span><span>    sum <span>+=</span> a <span>*</span> b<span>;</span>
</span></span><span><span>  <span>}</span>
</span></span><span><span>
</span></span><span><span>  <span>// Write the result into the single‐channel R component of the output texture</span>
</span></span><span><span>  outColor <span>=</span> <span>vec4</span><span>(</span>sum<span>,</span> <span>0.0</span><span>,</span> <span>0.0</span><span>,</span> <span>1.0</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>Here we have:</p><ul><li><strong>Per-pixel work item</strong>: Each fragment corresponds to one matrix element (i, j). The GPU runs this loop for every (i, j) in parallel across its shader cores.</li><li><strong>Exact indexing</strong>: texelFetch reads a single float by its integer coordinate.</li><li><strong>Write-back</strong>: Assigning to outColor.r writes that computed value directly into the bound FBO’s texture at (i, j).</li></ul><p>Here is another fragment shader but for the GELU activation function:</p><div><pre tabindex="0"><code data-lang="glsl"><span><span><span>// GELU Activation</span>
</span></span><span><span><span>#version 300 es</span>
</span></span><span><span><span>precision</span> <span>highp</span> <span>float</span><span>;</span>
</span></span><span><span><span>uniform</span> <span>sampler2D</span> u_X<span>;</span>
</span></span><span><span><span>out</span> <span>vec4</span> o<span>;</span>
</span></span><span><span>
</span></span><span><span><span>void</span> main<span>()</span> <span>{</span>
</span></span><span><span>  <span>ivec2</span> c <span>=</span> <span>ivec2</span><span>(</span>gl_FragCoord<span>.</span>xy<span>);</span>
</span></span><span><span>  <span>float</span> x <span>=</span> texelFetch<span>(</span>u_X<span>,</span> c<span>,</span> <span>0</span><span>).</span>r<span>;</span>
</span></span><span><span>  <span>float</span> t <span>=</span> <span>0.5</span> <span>*</span> <span>(</span><span>1.0</span> <span>+</span> tanh<span>(</span><span>0.79788456</span> <span>*</span> <span>(</span>x <span>+</span> <span>0.044715</span> <span>*</span> x<span>*</span>x<span>*</span>x<span>)));</span>
</span></span><span><span>  o <span>=</span> <span>vec4</span><span>(</span>x <span>*</span> t<span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><h4 id="shared-vertex-shader">Shared Vertex Shader</h4><p>Every operation gets its own fragment shader since that’s where the math for the operation happens. The vertex shader, on the other hand, is simple and the same for each. All it does is draw two triangles which covers the entire view port.</p><div><pre tabindex="0"><code data-lang="glsl"><span><span><span>// Shared Vertex Shader</span>
</span></span><span><span><span>#version 300 es</span>
</span></span><span><span><span>in</span> <span>vec2</span> a_position<span>;</span>
</span></span><span><span><span>out</span> <span>vec2</span> v_uv<span>;</span>
</span></span><span><span><span>void</span> main<span>()</span> <span>{</span>
</span></span><span><span>  v_uv <span>=</span> a_position <span>*</span> <span>0.5</span> <span>+</span> <span>0.5</span><span>;</span>  <span>// map [-1,+1] to [0,1]</span>
</span></span><span><span>  gl_Position <span>=</span> <span>vec4</span><span>(</span>a_position<span>,</span> <span>0</span><span>,</span> <span>1</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><ul><li><strong>Full-screen quad</strong>: Two triangles cover the viewport. Every pixel in the fragment stage maps to one tensor element.</li><li><strong>Reusable</strong>: Because the vertex work is identical for all operations, we compile it once and reuse it across every matrix multiply, activation, and bias-add pass.</li></ul><p>With this structure in mind, every “shader pass” is really just:</p><ol><li><strong>Vertex shader</strong>: map two triangles to the viewport</li><li><strong>Fragment shader</strong>: perform one tiny piece of your transformer math per pixel</li></ol><h3 id="chaining-passes">Chaining Passes</h3><p>Under the hood, every neural‐network operation, whether it’s a matrix multiply, an activation function, or a bias addition, boils down to four simple GPU steps:</p><ol><li>Bind inputs as textures (weights, activations, or intermediate results).</li><li>Attach a fresh output texture to an offscreen framebuffer (FBO).</li><li>Draw a full‐screen quad using the shared vertex shader.</li><li>Execute the fragment shader, which performs the actual computation per pixel.</li></ol><p>All of the WebGL boilerplate lives in our <code>_runPass</code> helper, so each pass in the GPT-2 forward loop feels like a single, declarative call:</p><div><pre tabindex="0"><code data-lang="typescript"><span><span><span>private</span> <span>_runPass</span><span>(</span>
</span></span><span><span>  <span>name</span>: <span>string</span><span>,</span>
</span></span><span><span>  <span>inputs</span><span>:</span> <span>{</span> <span>[</span><span>u</span>: <span>string</span><span>]</span><span>:</span> <span>WebGLTexture</span> <span>},</span>
</span></span><span><span>  <span>ints</span><span>:</span> <span>{</span> <span>[</span><span>u</span>: <span>string</span><span>]</span><span>:</span> <span>number</span> <span>},</span>
</span></span><span><span>  <span>outTex</span>: <span>WebGLTexture</span><span>,</span>
</span></span><span><span>  <span>W</span>: <span>number</span><span>,</span>
</span></span><span><span>  <span>H</span>: <span>number</span>
</span></span><span><span><span>)</span> <span>{</span>
</span></span><span><span>  <span>// Grab the WebGL2 context and compiled shader program for this pass
</span></span></span><span><span><span></span>  <span>const</span> <span>gl</span> <span>=</span> <span>this</span><span>.</span><span>gl</span><span>;</span>
</span></span><span><span>  <span>const</span> <span>prog</span> <span>=</span> <span>this</span><span>.</span><span>programs</span><span>[</span><span>name</span><span>];</span> <span>// This has our vertex and frag shaders
</span></span></span><span><span><span></span>  <span>gl</span><span>.</span><span>useProgram</span><span>(</span><span>prog</span><span>);</span>
</span></span><span><span>
</span></span><span><span>  <span>// BOILERPLATE: Bind all input textures as uniforms
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// BOILERPLATE: Bind an FBO and attach our empty texture to it.
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// BOILERPLATE: Set up the full-screen quad geometry
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// Draw into our texture
</span></span></span><span><span><span></span>  <span>gl</span><span>.</span><span>viewport</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>W</span><span>,</span> <span>H</span><span>);</span>            <span>// Ensure viewport matches texture size
</span></span></span><span><span><span></span>  <span>gl</span><span>.</span><span>drawArrays</span><span>(</span><span>gl</span><span>.</span><span>TRIANGLES</span><span>,</span> <span>0</span><span>,</span> <span>6</span><span>);</span>  <span>// Runs our shaders
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// Clean up: Unbind FBO
</span></span></span><span><span><span></span>  <span>gl</span><span>.</span><span>bindFramebuffer</span><span>(</span><span>gl</span><span>.</span><span>FRAMEBUFFER</span><span>,</span> <span>null</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><h4 id="forward-pass-layer-by-layer">Forward Pass: Layer by Layer</h4><p>Because each pass leaves its results in VRAM, we never pay the cost of round-trips back to the CPU until the very end. Below is a high-level description of the entire forward pass:</p><ol><li><strong>Upload Embeddings</strong>: Compute the token+position embeddings on the CPU and send them to the GPU as one texture.</li><li><strong>Transformer Layers (12 in total)</strong>:<ul><li><em>Normalize &amp; Project</em>: Apply layer normalization, then run the attention and feed-forward sublayers entirely on the GPU.</li><li><em>Attention</em>: Compute queries, keys, values; calculate attention weights; combine values.</li><li><em>Feed-Forward</em>: Two matrix multiplies with a GELU activation in between.</li><li><em>Residuals</em>: Add the layer’s input back in at each substep.</li></ul></li><li><strong>Final Normalization &amp; Output</strong>: Do one last layer normalization, multiply by the output weight matrix, then read the resulting logits back to the CPU.</li></ol><p>Once logits are back on the CPU, we apply softmax and sample (top-k or top-p) to pick the next token. Then the process starts over again with the new token being appended to the context.</p><p>By chaining these operation passes together, we keep the entire GPT-2 pipeline on the GPU until the final logits. This is how programmable shaders let us treat the graphics pipeline as a general-purpose parallel engine.</p><h3 id="limitations">Limitations</h3><p>While hijacking WebGL allows us to run machine learning models on the GPU, it carries several key limitations:</p><ul><li><strong>No shared/local memory</strong>: Fragment shaders can only read/write global textures. There’s no on-chip scratchpad for blocking or data reuse, so you’re limited to element-wise passes.</li><li><strong>Texture size limits</strong>: GPUs enforce a maximum 2D texture dimension (e.g. 16 K×16 K). Anything larger must be manually split into tiles, adding bookkeeping and extra draw calls.</li><li><strong>No synchronization or atomics</strong>: You can’t barrier or coordinate between fragments in a pass, making reductions, scatter/gather, and other data-dependent patterns difficult or impossible.</li><li><strong>Draw-call and precision overhead</strong>: Every neural-net operation requires binding an FBO, swapping textures, and issuing a draw call (dozens per layer) which incurs CPU overhead. Plus, you’re bound to 16- or 32-bit floats (via <code>EXT_color_buffer_float</code>), with no double precision or integer textures.</li></ul><p>Taken together, these constraints make shader-based compute an interesting educational project but a only a historical novelty for real world use. Compute APIs like CUDA or OpenCL give easier and better tools to achieve the same thing.</p><p>Thanks for reading! You can view the code and run the demo locally at the repo <a href="https://github.com/nathan-barry/gpt2-webgl/tree/main">here</a>. Contact me on <a href="https://x.com/nathanbarrydev">X</a> if you have any suggestions.</p><br></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US pauses new student visa interviews as it mulls expanding social media vetting (123 pts)]]></title>
            <link>https://www.politico.com/news/2025/05/27/trump-team-orders-stop-to-new-student-visa-interviews-as-it-weighs-expanding-social-media-vetting-00370501</link>
            <guid>44109253</guid>
            <pubDate>Tue, 27 May 2025 18:02:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.politico.com/news/2025/05/27/trump-team-orders-stop-to-new-student-visa-interviews-as-it-weighs-expanding-social-media-vetting-00370501">https://www.politico.com/news/2025/05/27/trump-team-orders-stop-to-new-student-visa-interviews-as-it-weighs-expanding-social-media-vetting-00370501</a>, See on <a href="https://news.ycombinator.com/item?id=44109253">Hacker News</a></p>
Couldn't get https://www.politico.com/news/2025/05/27/trump-team-orders-stop-to-new-student-visa-interviews-as-it-weighs-expanding-social-media-vetting-00370501: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[I salvaged $6k of luxury items discarded by Duke students (244 pts)]]></title>
            <link>https://indyweek.com/culture/duke-students-dumpster-diving/</link>
            <guid>44108207</guid>
            <pubDate>Tue, 27 May 2025 15:59:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://indyweek.com/culture/duke-students-dumpster-diving/">https://indyweek.com/culture/duke-students-dumpster-diving/</a>, See on <a href="https://news.ycombinator.com/item?id=44108207">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

					

<article id="post-278832">
	<div>

		
		<p><strong>I</strong> live in an apartment building in downtown Durham that houses more Duke University undergrads than any other category of person—a friend once characterized it as an “adult dorm”—so it wasn’t all that surprising when, last week, I found a cute little table in the trash room on my floor. At the end of the school year, a lot gets thrown away.</p><p>The table was in great condition, amid stacks of linens and unopened boxes of date-nut energy bites. Made from clear acrylic, its edges were tinged a neon lemon-lime color that changed with the light—sometimes appearing to be part of the acrylic itself, other times a reflection dancing along its curves.&nbsp;</p><p>I took it home. When I looked it up online, I discovered it costs $900. (Shipping cost: $199.)</p><p>That was retrieved from the trash room at the end of my hall, where you put things down the chute. The real gold mine is the ground-floor room that the chute empties into, accessible by one of the elevators.</p><p>This is where, around graduation each year, you can find dozens of vacuums, Keurigs, stainless steel trash cans in every size and shape imaginable, mattresses, mirrors, and enough luxury goods to make a reseller weep with joy. The first time I went down there, last week, I noticed something neon in a tote bag and pulled out $395 Balenciaga slides. Nearby were $980 Valentino sneakers—worn, but definitely wearable. More than $1,000 of Lululemon workout clothing tumbled from a bag onto a couch.</p><p>You don’t really have to do any digging—most of the stuff I’ve gotten was sitting on top of discarded furniture. But you do have to rush. After I took the Lululemon haul upstairs, I returned to find city waste workers loading things into a garbage truck, off to a landfill. The volume of discarded clothing seems consistent with generational trends: textile waste in the United States went up by more than 50 percent between 2000 and 2018.&nbsp;</p><div><figure><img decoding="async" width="768" height="1024" onerror="if (typeof newspackHandleImageError === 'function') newspackHandleImageError(this);" src="https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-768x1024.jpeg" alt="" srcset="https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-768x1024.jpeg 768w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-225x300.jpeg 225w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-1152x1536.jpeg 1152w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-1536x2048.jpeg 1536w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-900x1200.jpeg?crop=1 900w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-600x800.jpeg?crop=1 600w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-450x600.jpeg?crop=1 450w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-300x400.jpeg?crop=1 300w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-150x200.jpeg?crop=1 150w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-1200x1600.jpeg 1200w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-2000x2667.jpeg 2000w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-780x1040.jpeg 780w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-400x533.jpeg 400w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-706x941.jpeg 706w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-scaled.jpeg 1920w" sizes="(max-width: 768px) 100vw, 768px"><figcaption>The trash room. Photo by Lena Geller. </figcaption></figure></div><p>Not every treasure is a flashy brand-name item. I also recovered pink satin pajamas that I remember seeing on someone attending a pajama party on my floor, and a ruffled olive-green <em>Top Gun</em> romper from a Halloween event. (Sadly, there was nothing from the risqué Dr. Seuss party. A few months ago, a fire alarm went off, and it became apparent just how much of the building is occupied by Duke students, as nearly everyone except me, my roommate, and a family with two young kids was drunk and dressed in <em>Cat in the Hat</em> costumes.)</p><p>It feels wrong for this much stuff to have been thrown out in the first place, but it also feels mildly wrong to take it. So it was nice to get intermittent reassurance from my building’s maintenance man, Eric. </p><p>The first time, as I was scurrying back to my room, carrying an upholstered kitchen chair that my cats now spend all their time in, I passed Eric in the hallway. He asked me how I was.</p><p>“Just doing some scavenging,” I said. I must have looked guilty, because he said, “That’s OK.”</p><p>A few days later, I was again downstairs in the big trash room when Eric walked in. I moved to leave, feeling awkward about being caught again. “You’re welcome here anytime,” he assured.&nbsp;</p><p>The sheer volume of valuable, usable things being discarded boggles the brain, particularly when it comes to items like clothing with the tags still on and unopened, unexpired food items.&nbsp;</p><p>In trying to make sense of things, I made spreadsheets.</p><p>The first tracks the prices and brands of the items that I kept, donated, or sold. The total value came to around $6,000, not including several items I couldn’t find prices for.</p><div><figure><img decoding="async" width="1024" height="982" onerror="if (typeof newspackHandleImageError === 'function') newspackHandleImageError(this);" src="https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59%E2%80%AFAM-1024x982.png" alt="" srcset="https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-1024x982.png 1024w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-300x288.png 300w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-768x737.png 768w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-1200x1151.png 1200w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-780x748.png 780w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-400x384.png 400w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-706x677.png 706w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM.png 1370w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>A screenshot of one section of the author’s itemized spreadsheet</figcaption></figure></div><p>The second spreadsheet compares Duke’s donation collection data with that at other universities, in an effort to understand whether this college-town phenomenon is universal. I gathered publicly available data from university websites and press releases, supplemented by direct inquiries.</p><p>Duke told me their “Devils Care Donations” initiative collected 32,000 pounds this year through partnerships with TROSA and Goodwill. Ali Harrison, senior associate dean for residence life, says that the university places donation bins in every residence hall on campus, plus off-campus Duke housing like Blue Light and Swift Apartments. Harrison also notes that “Duke students who live off campus in non-Duke housing can schedule a TROSA pickup for large or bulky items and large donations.”</p><p>I emailed six universities, asking about their donation programs and collection data. Most didn’t respond or declined. One directed me to a public web page. Rice University, whose “Give a Hoot! Donate Your Loot!” campaign recently won a statewide award in Texas, sent a detailed response. They reported that they collected around 11,000 pounds of “durable goods” from students this year. (Rice has around 9,000 total students, with roughly half undergrads and half graduate students.)</p><p>Rice’s approach is to implement collections every semester, not just during spring move-out. “By maintaining a consistent presence throughout the academic year,” a spokesperson wrote, “the campaign has become a familiar part of the student experience,” helping students plan ahead to donate rather than discard.</p><p>Looking at the data, Duke’s per-undergraduate donation rate (about 4.9 pounds) is comparable to that at other wealthy private universities like Princeton (7.6 pounds) and Georgetown (6.1 pounds). Duke actually outperforms some schools with similar student demographics like the University of Chicago (0.8 pounds) and Northwestern (0.9 pounds). Most large public universities hover around one pound per student.</p><figure><img decoding="async" width="1024" height="683" onerror="if (typeof newspackHandleImageError === 'function') newspackHandleImageError(this);" src="https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-1024x683.jpg" alt="" srcset="https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-1024x683.jpg 1024w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-300x200.jpg 300w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-768x512.jpg 768w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-1536x1024.jpg 1536w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-2048x1365.jpg 2048w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-1200x800.jpg 1200w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-2000x1333.jpg 2000w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-780x520.jpg 780w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-400x267.jpg 400w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-706x471.jpg 706w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><em>INDY </em>staff writer Lena Geller poses for a picture while wearing Valentino Garavani tennis shoes on Tuesday, May 20, 2025, in Durham. Photo by Angelica Edwards. </figcaption></figure><p><strong>T</strong>he emotional reality of my salvaging week was harder to organize into neat columns. For one, I started feeling like everything I own is shitty. When you’re pulling something out of the trash, it doesn’t feel like it’s going to be a luxury item, so at first, I didn’t think much of a comforter I salvaged and offered it to my boyfriend, who’s always looking for blankets for his dog to lie on. After looking up the cost ($222) and thread count (600), I went back on that offer and replaced my existing comforter with the salvaged one. (The next day, my boyfriend found his own down comforter in the trash.)</p><p>Most items I salvaged were like new, but some needed attention. It felt good to wash, clean, and mend things—removing stains from a blouse, fixing belt loops on black slacks. But then futility would set in. I tried to get the stains out of a pair of muddy Nike high-tops with floral embroidery, using a Solo cup I salvaged as a mixing receptacle to stir together baking soda and hydrogen peroxide into a thick paste, but even after slathering it onto the shoes, the stains persist.&nbsp;</p><p>I also spent some time scrubbing a toaster oven, only to go back to the trash room a few days later and find one that’s cleaner and fancier. Retail value: $400.</p><p>In what would become my final scavenging trip of the year, I tried carrying too many things at once—a handheld vacuum, an air filter, some velvet hangers—and dropped the toaster oven, which splashed water all over me from its steam reservoir.&nbsp;</p><p>Sometimes it’s a spill that does it. I stood there, damp, surrounded by other people’s discards, feeling ridiculous. My apartment was already filled with rescued items. I went home, found that the air filter didn’t fit my unit, and cried.</p><p>The next night, my cat jumped down from the salvaged chair he loves, used his litter box, and then kicked litter everywhere—as per usual. Managing litter has been an ongoing struggle. Various vacuums have proved too weak or too bulky to reach the corners behind the box, so I usually just sweep with a handheld broom and dustpan.</p><p>But as I bent over with my dustpan that night, I remembered the handheld vacuum I’d salvaged just before dropping the toaster oven. I’d found it with its charging cord sitting right next to it, still coiled neatly with a twist tie.</p><p>I grabbed it from my pile of findings and turned it on. It was the most powerful little vacuum I’ve ever seen, its pointed nose perfect for crevices.</p><p><em>Reach Staff Writer Lena Geller at&nbsp;<a href="mailto:lgeller@indyweek.com"><em>lgeller@indyweek.com.</em></a>&nbsp;Comment on this story at&nbsp;<a href="mailto:backtalk@indyweek.com"><em>backtalk@indyweek.com</em></a>.</em><br></p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

	
</article><!-- #post-${ID} -->
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Square Theory (566 pts)]]></title>
            <link>https://aaronson.org/blog/square-theory</link>
            <guid>44107942</guid>
            <pubDate>Tue, 27 May 2025 15:33:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aaronson.org/blog/square-theory">https://aaronson.org/blog/square-theory</a>, See on <a href="https://news.ycombinator.com/item?id=44107942">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>The story starts in <a href="https://discord.com/invite/GPyU97XBGX">Crosscord</a>, the crossword Discord server. Over 5,000 users strong, the server has emerged as a central hub for the online crossword community, a buzzing, sometimes overwhelming, sometimes delightful town square where total noobs, veteran constructors, and champion solvers alike come together to talk about words that cross each other.</p>

<h2 id="square-roots">Square roots</h2>

<p>We direct our attention toward the #etuiposting channel, Crosscord’s designated space for shitposting (so named because ETUI, a sewing case, is a prototypically shitty piece of crosswordese). There, one afternoon in January 2022, crossword constructor and <a href="https://crosswordnexus.com/">Crossword Nexus</a> warden Alex Boisvert posted what seemed at the time to be an innocuous, mildly interesting observation:</p>

<p><img src="https://aaronson.org/assets/images/square-boisvert.png" alt="Alex Boisvert: JET BLACK and JETBLUE have very different meanings, even though they look superficially similar.  Same thing with CATNAP and DOGNAP.  Any other examples of this?"></p>

<p>Suffice to say, the Crosscord hivemind had other examples of this. <a href="http://blog.bewilderinglypuzzles.com/">Will Nediger</a> replied a few minutes later with the clever MULTITOOL and MULTIPLIERS (words with completely unrelated meanings, despite the fact that PLIERS are a TOOL). Several messages later, Alex chimed back in with the elegant PUB QUIZ and BAR EXAM, a pairing that had been used in some form in crosswords by constructors <a href="http://arctanxwords.blogspot.com/2018/04/puzzle-53-i-thought-this-was-speed.html">Christopher Adams</a> (2018) and <a href="https://www.nytimes.com/crosswords/game/daily/2021/01/29">Robyn Weintraub</a> (2021).</p>

<p>Something about this concept—two sets of synonyms (PUB and BAR, QUIZ and EXAM), which when paired together, form phrases that themselves are not synonyms (PUB QUIZ and BAR EXAM)—captured the minds of Crosscord. Suddenly, the floodgates were open.</p>

<p><img src="https://aaronson.org/assets/images/square-discord-posts.png" alt="Will Nediger: UBEREATS / SUPERFOOD; Assorted-Interests: THROW SHADE / PITCH BLACK; Tyler Hinman, Aged Prodigy: With this topic resurrected, it seems nobody posted what I think is the canonical one: BOOTY CALL and BUTT DIAL; jenna lafleur: ROMAN MARS / CLASSICAL RUINS; gppasco: GRAND CANYON / K-HOLE; robinyu: DAD BOD and FATHER FIGURE; kareila: PERMANENT PRESS / FOREVER STAMP; heywhatsupitsbob: FRIENDLY FAVOR / PLATONIC SOLID"></p>

<p>Intermittently over the next <em>year</em>, #etuiposting would be flooded with these pairs of pairs. They became too much even for the shitposting channel, and were ultimately confined to a thread called #double-doubles (a name <a href="https://heywhatsupitsbob.com/">Bob Weisz</a> and I both proposed simultaneously). Today, more than three years after Alex’s original prompt, the thread still remains active, a wordplay oasis of over 3,000 posts.</p>

<p>There’s something going on here. Something more than a shitpost or an ephemeral trend. Double doubles have the proverbial juice, and the juice lies in their structure. Each pair of pairs can be modeled as a square, where the corners are words and the sides are relations between those words:</p>

<p><img src="https://aaronson.org/assets/images/square-booty-call.jpeg" alt="Square showing BOOTY - phrase - CALL connected via synonyms to BUTT - phrase - DIAL"></p>

<p>It’s this square structure that makes each double double feel tight, feel satisfying, feel like a real “find”. This is the essence of what I’ve started calling <strong>square theory</strong>, and it applies to much more than just posts in a Discord server.</p>

<p>Just like it’s satisfying when an essay or a news story comes full circle, or mindblowing when you find an unexpected cycle in your network of friends, it’s inherently compelling when things wrap around and complete the square. Let’s break it down.</p>

<h2 id="the-theory-of-everything">The theory of everything</h2>

<p>Crosscord wasn’t the first to catch onto this kind of formation: Ricki Heicklen has maintained a <a href="https://rickiheicklen.com/unparalleled-misalignments.html">huge list</a> of double doubles (which she calls “Unparalleled Misalignments”—itself a sort of double double) since 2018, and the success of her recent <a href="https://x.com/tradegal_/status/1920189768261828748">Twitter thread</a> about them is another testament to their widespread appeal. Pairs of the same form pop up on a regular basis in the form of crossword clues and Twitter jokes:</p>

<p><img src="https://aaronson.org/assets/images/square-top-gun.png" alt="Crossword clue [Top gun?] for TSHIRTCANNON, with a square showing TOP - phrase - GUN connected via synonyms to TSHIRT - phrase CANNON">
    <img src="https://aaronson.org/assets/images/square-dad-bod.png" alt="Tweet by Steven W Skinner that says 'Why is it called a dad-bod and not a father-figure', with a square showing DAD - phrase - BOD connected via synonyms to FATHER - phrase - FIGURE">
</p>

<p>However, there’s nothing about the square structure that dictates the edges must represent phrases and synonyms. Each edge of the square can be any relation that connects its vertices (but generally, the stronger the relations, the stronger the square). The vertices don’t even necessarily have to be words—they can be any entity or concept.</p>

<p>It evokes the mathematical field of <a href="https://en.wikipedia.org/wiki/Category_theory">category theory</a>, which very abstractly studies mathematical objects and the relations between them. It’s the topology of the square that makes it satisfying, regardless of what the edges and vertices represent.</p>

<p>Members of the #double-doubles thread have already noticed this, consciously or not, with many of the posts interpreting the original prompt more liberally and swapping out the “synonym” relation for something else:</p>

<p><img src="https://aaronson.org/assets/images/square-left-on-read.png" alt="Crosscord post from Joah: LEFT ON READ vs. RIGHT ON RED. Same number of letters too. Maybe I'll make a mini out of it; Square showing LEFT on READ connected via antonym and homophone to RIGHT on RED">
    <img src="https://aaronson.org/assets/images/square-arizona-wildcat.png" alt="Crosscord post from Quiara, Newsletter Economist: ARIZONA WILDCAT / ARIGATO; Square showing ARIZONA phrase WILDCAT connected via abbr. and translation to ARI word GATO">
</p>

<p>Sometimes it feels like the #double-doubles thread has devolved into just #question-mark-clues (crossword clues that are trying to trick you, requiring you to reinterpret them beyond their words’ most likely meaning, or “surface sense”). But that’s no coincidence—abstractly, every question mark clue takes the form of a square.</p>

<p>When brainstorming for question mark clues, crossword constructors experience this on a regular basis. You start with the answer at hand, playing word association with it in search of a combination of words that usually means one thing (the surface sense) but can be interpreted differently (the intended interpretation) to point to the answer, thus completing the square:</p>

<p><img src="https://aaronson.org/assets/images/square-question-mark-clue.png" alt="Square showing word(s) connected to word(s) by surface sense, which are connected by homonyms to word(s) connected to word(s) by intended interpretation, which leads to the answer"></p>

<p>Take <a href="https://www.nytimes.com/2001/04/08/magazine/endpaper-how-to-solve-the-new-york-times-crossword-puzzle.html">Will Shortz’s all-time favorite clue</a> for instance, from a 1995 Martin Ashwood-Smith puzzle: [It turns into a different story] (which deviously didn’t even include the question mark). On the surface, “turns into a different story” typically means something like “develops into another situation.” But the intended interpretation takes the clue’s words to mean “rotates into another floor,” leading to the correct answer of SPIRAL STAIRCASE:</p>

<p><img src="https://aaronson.org/assets/images/square-spiral-staircase.png" alt="Square showing turns (develops) connected to story (situation) by &quot;develops into another situation&quot;, which are connected by homonyms to turns (rotates) connected to story (floor) by &quot;rotates into another floor&quot;, which leads to the answer SPIRAL STAIRCASE"></p>

<p>You might be familiar with this same sort of brainstorming if you’ve ever tried to come up with a clever title for a research paper, or an apt name for a company. There are plenty of names that might make you go “I guess that could work,” but it’s the square-completing ones that make you go “that’s the one,” or “that’s so good!”</p>

<p>One of my favorite examples of this is <a href="https://www.underconsideration.com/brandnew/">Brand New</a>, the blog that catalogues the latest in corporate rebrands. Leave it to a branding blog to have a name this immaculate:</p>

<p><img src="https://aaronson.org/assets/images/square-brand-new.png" alt="Square showing BRAND phrase NEW connected via homonym and synonym to what the blog chronicles, updated brands"></p>

<p>Even a seemingly straightforward brand name like <a href="https://www.grubhub.com/">Grubhub</a> can exemplify the power of square theory. Presumably, Grubhub’s branding team started with a concept (a centralized app for food deliveries) and came up with a name that completes the square. But remove any edge of the square (besides the edge that dictates the app’s purpose), and you’re left with a name that only <em>kinda</em> works:</p>

<p><img src="https://aaronson.org/assets/images/square-grubhub.png" alt="Complete square showing GRUB rhyme HUB connected via synonyms to what the app provides, a central place for food">
    <img src="https://aaronson.org/assets/images/square-grubnexus.png" alt="Incomplete square showing GRUB and NEXUS connected via synonyms to what the app provides, a central place for food">
</p>
<p><img src="https://aaronson.org/assets/images/square-grubcub.png" alt="Incomplete square showing GRUB rhyme CUB connected via only one synonym to what the app provides, a central place for food">
    <img src="https://aaronson.org/assets/images/square-tubhub.png" alt="Incomplete square showing TUB rhyme HUB connected via only one synonym to what the app provides, a central place for food">
</p>

<p>Aside from crossword clues and brand names, squares appear in the wild all the time in the form of jokes. There’s a vast universe of pun-based jokes (often in the form of dad jokes, or Twitter jokes, or <a href="https://www.timeout.com/newyork/clubs/punderdome">Punderdome</a> puns) that can be modeled as a square, where one side of the square is the contrived setup (“what do you call an X with a Y?”) that connects in at least two ways to the punchline (“an algebra problem!”) on the opposite side.</p>

<p>The strength of the joke rests in the strength of the setup, the punchline, and the connections between them—and if every side of the square is strong, you might have created something funny:</p>

<p><img src="https://aaronson.org/assets/images/square-joke-abstract.png" alt="Square showing a setup (contrived) of at least two words, which are connected by synonyms or homonyms, usually, to at least two other words, the punchline (a real word or phrase, or a play on one)">
    <img src="https://aaronson.org/assets/images/square-impasta.png" alt="Square showing FAKE and NOODLE connected by the setup 'What do you call a fake noodle', which connects via synonyms to IMPOSTOR and PASTA, forming the portmanteau punchline 'An impasta!'">
</p>

<h2 id="getting-into-shape">Getting into shape</h2>

<p>You might be thinking: what’s so special about a square? What about triangle theory, or pentagon theory? (Or rectangle theory? Or rhombus theory? Okay, side lengths and angles <a href="https://en.wikipedia.org/wiki/Topology">don’t matter here</a>.)</p>

<p>Well, it’s true that there’s something compelling about any loop-closing property, regardless of side count—a story that comes full circle is still satisfying no matter how many points it hits in between, and it’s still neat to discover a triangle of people who coincidentally know each other.</p>

<p>But here’s what I think makes squares special: a square is the simplest polygon that has non-adjacent sides. In a triangle, each side is adjacent to the other two sides. But in a square, opposite sides have no points in common, which makes any connection between them feel surprising, like a coincidence. In pentagons and beyond, this still holds, but the extra sides add complexity that make them feel slightly less elegant. Nevertheless, other shapes can be interesting too, but I see them as the exception, not the rule.</p>

<p>Remember Alex Boisvert’s original JET BLACK / JETBLUE example? Seems like it could be modeled as a triangle, right? Well, it turns out the “jet” in JET BLACK refers to the gemstone <a href="https://en.wikipedia.org/wiki/Jet_(gemstone)">jet</a>, which is <a href="https://www.etymonline.com/word/jet">etymologically unrelated</a> to JETBLUE’s airplane jet, so it’s actually more properly modeled as a square:</p>

<p><img src="https://aaronson.org/assets/images/square-jet-triangle.png" alt="Triangle showing JET phrase BLACK colors BLUE airline JET">
    <img src="https://aaronson.org/assets/images/square-jet-square.png" alt="Square showing JET homonym JET phrase BLACK colors BLUE airline JET">
</p>

<h2 id="times-square"><em>Times</em> square</h2>

<p>Now that I’ve established that square theory applies to more than just crosswords, it’s time to talk about crosswords again.</p>

<p>It’s typical for American-style crosswords (à la <em>New York Times</em>) to have a theme, which will generally consist of the 4–6 longest Across entries in the grid, often including a “revealer” that ties the theme together. Nowadays, it’s common gospel among crossword constructors that themes should have some sort of wordplay-based connection—that is, a theme like “famous basketball players” or “brands of cereal” is unlikely to elicit a real “aha” moment from solvers, and thus unlikely to be accepted at major crossword outlets.</p>

<p>So what makes for a <em>good</em> crossword theme? Consistency is definitely key, and a notion of “tightness” is important too (the set of possible theme entries shouldn’t be too much bigger than the theme set that appears in the puzzle). But time and time again, I’ve noticed that what makes a theme really pop is—you guessed it—when it completes the square.</p>

<p>Take, for example, the <a href="https://www.xwordinfo.com/Crossword?date=2/17/2025">Monday, February 17, 2025 <em>New York Times</em> crossword</a> by Kate Hawkins and Erica Hsiung Wojcik, which features a great execution of a typical Monday theme type. In this puzzle, the seemingly unrelated theme entries SCRAPBOOK, POPEYES, UNDER PRESSURE, and GIDDY UP are united by the fact that they each end in an affirmative (OK, YES, SURE, YUP).</p>

<p>In a vacuum, this fact wouldn’t be that interesting, but Kate and Erica give the theme a <em>raison d’etre</em> with the revealer YEAH RIGHT, clued as [“Uh-huh, I bet” … or a literal description of what 17-, 24-, 36- and 50-Across all have]—that is, each themer has a synonym for “YEAH” on its “RIGHT” side. The key here is that YEAH RIGHT itself is an idiomatic phrase (meaning “Uh-huh, I bet”), and not just an arbitrary description of the theme mechanic, so it completes the square:</p>

<p><img src="https://aaronson.org/assets/images/square-yeah.png" alt="Square showing what the entries have, an affirmative ending, connected via synonyms to the phrase YEAH RIGHT"></p>

<p>But it doesn’t stop there. Consider the <a href="https://www.xwordinfo.com/Crossword?date=2/18/2019">Monday, February 18, 2019 <em>New York Times</em> crossword</a> by Leslie Young and Andrea Carla Michaels. The theme entries here are NIGHT NIGHT, WHITE WEDDING, and MUSHROOM BALL (you know, like a vegetarian meatball), and the revealer, clued as [Graduation garb … or what the compound answers to 17-, 28- and 44-Across represent?], is CAP AND GOWN. That is, the first part of each themer can precede CAP (e.g. MUSHROOM CAP), and the second part can precede GOWN (e.g. BALL GOWN). This maps pretty squarely onto not one, but three squares, one for each theme entry:</p>

<p><img src="https://aaronson.org/assets/images/square-night-night.png" alt="Three squares, for NIGHT NIGHT, WHITE WEDDING, and MUSHROOM BALL, each showing the phrase connected by two phrases to CAP and GOWN"></p>

<p>And just for fun, we can conjoin the three squares by their CAP AND GOWN edges to form a unified graph that represents the entire theme’s topology:</p>

<p><img src="https://aaronson.org/assets/images/square-cap-and-gown.png" alt="Unified CAP AND GOWN square graph"></p>

<p>The final crossword we’ll look at, and maybe my favorite crossword of all time, is Alina Abidi’s <a href="https://www.xwordinfo.com/Crossword?date=8/18/2021">Wednesday, August 18, 2021 <em>New York Times</em> crossword</a>, with a theme that feels almost impossibly tight.</p>

<p>The puzzle has essentially two theme entries, PIN THE TAIL ON THE DONKEY and WHITE ELEPHANT, with the apt revealer PARTY ANIMAL [Frequent reveler, or a hint to 16-/26- and 36-Across]. That alone is clever, since both themers are party games with animals in their names. But then Alina hits you with the <em>second</em> revealer of THOMAS NAST [Cartoonist suggested by this puzzle’s theme], pointing to the fact that not only are the DONKEY and ELEPHANT animals in party games, but they are also the animals that symbolize the Democratic and Republican <em>parties</em>, as popularized by <a href="https://en.wikipedia.org/wiki/Thomas_Nast">Thomas Nast</a>’s political cartoons.</p>

<p>This is the kind of theme that really sticks with you. Or at least it stuck with me, and I tried for years to understand why it felt so amazing. And then I realized square theory offered an explanation. Squares, as we know, feel tight, satisfying, and clever. But Alina’s theme takes that one step further, creating for each theme entry a square with an <em>extra diagonal</em> through it, reflecting the connection between each animal and a political PARTY:</p>

<p><img src="https://aaronson.org/assets/images/square-democrat.png" alt="Square showing PIN THE TAIL ON THE DONKEY containing DONKEY connected to PARTY ANIMAL by setting and example, with an additional Democrats diagonal connecting PARTY and DONKEY">
    <img src="https://aaronson.org/assets/images/square-republican.png" alt="Square showing WHITE ELEPHANT containing ELEPHANT connected to PARTY ANIMAL by setting and example, with an additional Republican diagonal connecting PARTY and ELEPHANT">
</p>

<p>And again, we can combine these two super-squares into one unified theme graph:</p>

<p><img src="https://aaronson.org/assets/images/square-party-animals.png" alt="Unified PARTY ANIMALS square graph"></p>

<p>Granted, there’s more to a crossword than the structure of its theme, and it can be reductive to distill it into a graph like this. Still, for many puzzles, square theory can serve as an illuminating proxy for the intricacy and tightness of a theme. But that’s not all it can do.</p>

<h2 id="letter-box">Letter box</h2>

<p>Let’s talk about Scrabble, one of the <a href="https://www.nytimes.com/2022/01/25/books/review/seven-games-oliver-roeder.html">seven most important games</a> out there. If you’ve ever played Scrabble (or similar games like Bananagrams), you’d know that every word you play has to intersect another word that’s already on the board.</p>

<p><img src="https://aaronson.org/assets/images/square-scrabble-normal.png" alt="Scrabble play that is boring and the word only intersects one other word"></p>

<p>But occasionally, you’ll think up a play that validly intersects not one, but two words on the board, forming a rectangle of words. Plays like this have a certain panache. They’re satisfying, they make you think, “ooh, nice.” And of course, they can be modeled with square theory:</p>

<p><img src="https://aaronson.org/assets/images/square-scrabble-cool.png" alt="Scrabble play where the word MICE intersects two already-on-the-board words CHASM and SINCE">
    <img src="https://aaronson.org/assets/images/square-scrabble.png" alt="Square showing the Scrabble board words CHASM linking A and M, AVID linking A and I, SINCE linking I and C, and MICE linking M and C">
</p>

<p>You might be thinking that the edge relation here (a word that contains both letters) feels a little flimsy, since not every letter in the word is used. But what if every letter in the word <em>was</em> used? What if we could have a dense network of interlocking squares, where every letter was part of exactly two words? Well, we can, and it’s called an American-style crossword.</p>

<p>In American-style crosswords, every letter is mandatorily “checked” (part of an Across and a Down word), which means <em>every</em> letter is a vertex of a square:</p>

<p><img src="https://aaronson.org/assets/images/square-crossword-grid.png" alt="3x3 crossword grid, and a grid of interconnected squares whose vertices are the letters in the crossword and whose edges are the words that connect those letters"></p>

<p>If you’ve ever tried to construct a crossword, you’ll find that the framing of a crossword grid under square theory <em>feels</em> right. When you’re nearing the end of the grid-filling process, finding valid crossings of words to fill that final corner of a grid, there’s a satisfying “clicking” feeling—a sense of magic—when it all fits together, analogous to the wrapping-around feeling of completing the square.</p>

<p>Taking a step back, that means the clues, the themes, and the very grids of crosswords all share the same abstract fundamental structure, the square:</p>

<p><img src="https://aaronson.org/assets/images/square-crosswords-everything.png" alt="Squares from earlier in the post representing clues, themes, and grids of crosswords"></p>

<p>If you accept the premise that squares are satisfying, square theory offers a unified theory for why crosswords are satisfying too. And if squares are fundamentally compelling, the crossword, in its recursively square structure, starts to look like an equally fundamental art form. Like if you started an English-speaking civilization from scratch, someone, somewhere would inevitably reinvent the crossword. And then someone would start a crossword Discord server, and maybe they’d call it Crosscord.</p>

<p><img src="https://aaronson.org/assets/images/square-crosscord.png" alt="Square showing what the server is, a Discord server for crossword puzzles, connected by keywords to CROSSWORD / DISCORD which are portmanteaued into the server's name, CROSSCORD"></p>

<h2 id="its-hip-to-be-square">It’s hip to be square</h2>

<p>If you’ve read this far, I promise you’ll start to notice squares popping up all over the place in your daily life. I can attest, because I’ve been honing the concept for this post for about two years now, and I often find myself thinking “that’s a square!” whenever I come across a tight joke or title or crossword theme.</p>

<p>If you’re a creative person, square theory is a useful framework to keep in mind. If you’re coming up with a title for a paper or a brand name, try to see if you can think of one that completes the square. If you’re writing puns for a popsicle stick or a Laffy Taffy wrapper, you can use squares to model your setups and punchlines. If you’re constructing a crossword, consider whether your theme or your question mark clues can form squares.</p>

<p>And if you’re writing a story or a news article or a blog post, there’s fundamental value in making it come full circle, or perhaps full square.</p>

        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pyrefly vs. Ty: Comparing Python's Two New Rust-Based Type Checkers (333 pts)]]></title>
            <link>https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/</link>
            <guid>44107655</guid>
            <pubDate>Tue, 27 May 2025 15:01:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/">https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/</a>, See on <a href="https://news.ycombinator.com/item?id=44107655">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
      <blockquote>
<p>Note: I like using em-dashes while writing! Don’t worry, this is not written by AI. <sup><a href="https://medium.com/@brentcsutoras/the-em-dash-dilemma-how-a-punctuation-mark-became-ais-stubborn-signature-684fbcc9f559">(context)</a></sup></p></blockquote>
<p>Earlier this month, two new Rust-based Python type checkers hit the spotlight: <a href="https://github.com/facebook/pyrefly">pyrefly</a> and <a href="https://github.com/astral-sh/ty">ty</a>. Although neither is <em>officially</em> released, they are a welcome change to the Python type checking world, historically dominated by <a href="https://mypy-lang.org/">mypy</a> and <a href="https://pypi.org/project/pylance/">pylance</a>.</p>
<p>While both are open-source and publicly downloadable for quite some time, there have not been any official announcements by Meta nor Astral on their brand new next-generation Python type checkers — <strong>until last week</strong>.</p>
<p>At <a href="https://us.pycon.org/2025/">PyCon 2025</a>, nestled away in a quiet Room 319 at the <a href="https://us.pycon.org/2025/events/typing-summit/">Typing Summit</a>, we had our first official sneak peek into both of these tools — the team behind them, their goals, visions, and ambitions — and their unique approaches to tackling Python’s typing problems.</p>
<figure>
  <img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/ty-introduction.png" alt="ty introduction presentation at PyCon 2025">
  <figcaption>ty team presenting at the typing summit</figcaption>
</figure>
<blockquote>
<p>This blog is a collection of rough notes scribbled during the event, personal conversations with the team, and not-too-thorough experiments that I’ve run myself. As such, some details might be a little blurry.</p></blockquote>
<blockquote>
<p><strong>Also, both of these tools are still in early alpha!</strong></p>
<p>Please do not use this as a definitive judgment as to which one is better and/or worse. This blog is just for fun to see what state the two tools are at now!</p></blockquote>
<blockquote>
<p>The following tests and experiments were performed on the latest versions of pyrefly, ty, mypy, and pyright as of writing this blog:</p>
<ul>
<li><code>pyrefly 0.17.0</code></li>
<li><code>ty 0.0.1-alpha.7 (afb20f6fe 2025-05-26)</code></li>
<li><code>mypy 1.15.0 (compiled: yes)</code></li>
<li><code>pyright 1.1.401</code></li>
</ul></blockquote>
<h2 id="pyrefly">Pyrefly</h2>
<p>Pyrefly is Meta’s new Rust-based Python type checker, replacing <a href="https://pyre-check.org/">Pyre</a> — Meta’s previous Python type checker written in OCaml. The hopes are that Pyrefly should be faster, more portable, and more capable compared to Pyre.</p>
<p>One key thing the Pyrefly team made very clear this year is that they want to be <em><strong>truly open source</strong></em>. Pyre was also <em>technically</em> open source, but it was more of a “we built this for our needs, but here’s the source code if you want it”. In contrast, one of the foundational goals of Pyrefly is to be more engaged with the needs of the open-source community.</p>
<figure>
  <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/ZTSZ1OCUaeQ?si=Rc3-M7a7Yh7SSq-X&amp;start=1405" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
  <figcaption>pyrefly introduction presentation</figcaption>
</figure>
<h2 id="ty">ty</h2>
<p>ty is also a Rust-based Python type checker currently under development by <a href="https://astral.sh/">Astral</a>, the team behind <a href="https://docs.astral.sh/uv/">uv</a> and <a href="https://github.com/astral-sh/ruff">ruff</a>. The project was formerly known as Red-Knot, but now has its official name: ty. Compared to Meta, Astral is a lot more quiet on its announcement: just a soft launch on GitHub, a quick 30-minute presentation, and a couple of blog articles as podcasts here and there.</p>
<figure>
  <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/7uixlNTOY4s?si=qMCrwoIekSkoH3xF&amp;start=3558" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
  <figcaption>ty introduction presentation</figcaption>
</figure>
<h2 id="similarities">Similarities</h2>
<p>Both pyrefly and ty are written in Rust, both are incremental (albeit implemented slightly differently: see details below), and both are powered under the hood by <a href="https://github.com/astral-sh/ruff">Ruff</a> for AST parsing. Also, both have first-class support for command-line type checking and LSP/IDE integration.</p>
<p>However, other than the fact that they are both fast Python type checkers, that’s where the similarities end. In my opinion, there are four categories in which these two tools differ: <strong>in Speed, Goals, Incrementalization, and Capabilities.</strong> That’s what we’ll explore today.</p>
<h2 id="speed">Speed</h2>
<p>Speed seemed like one of the main focuses of Pyrefly, being mentioned multiple times during the intro presentation. According to the team, it’s 35x faster than Pyre and 14x faster than Mypy/Pyright, with support of up to 1.8 million lines of code per second. Fast enough to “type check on every keystroke”.</p>
<p>In comparison, speed was also one of the main design goals for ty, but it felt like less of a focus during the introduction. The only claim was “1-2 orders of magnitude faster than current generation type checkers”. Naturally, I wanted to test performance out for myself.</p>
<h2 id="benchmarking---pytorch">Benchmarking - PyTorch</h2>
<p>For the first test, I cloned and checked out the latest release of PyTorch (<code>v2.7.0</code>) and compared type check times between pyrefly, ty, mypy, and pyright on a MacBook M4. Two tests were run, one on the entire <code>pytorch</code> repository and another on just the <code>torch</code> subdirectory:</p>
<blockquote>
<p>PyTorch on the latest mypy is not supported. Using <code>mypy 1.14.0</code> instead.</p></blockquote>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/fig-pytorch-benchmarks.svg" alt="pytorch benchmarks"></p><details>
  <summary>Commands Run</summary>
  <ul>
<li><strong>pyrefly:</strong> <code>hyperfine --warmup 3 --runs 5 --ignore-failure 'pyrefly check'</code></li>
<li><strong>ty:</strong> <code>hyperfine --warmup 3 --runs 5 --ignore-failure 'ty check'</code></li>
<li><strong>mypy:</strong> <code>hyperfine --warmup 3 --runs 5 --ignore-failure 'mypy --cache-dir=/dev/null .'</code></li>
<li><strong>pyright:</strong> <code>hyperfine --warmup 3 --runs 5 --ignore-failure 'pyright'</code></li>
</ul>
</details>


<details>
  <summary>Raw Data</summary>
  <pre tabindex="0"><code>ty
  Time (mean ± σ):      4.039 s ±  0.234 s    [User: 19.135 s, System: 3.850 s]
  Range (min … max):    3.888 s …  4.455 s    5 runs

pyrefly
  Time (mean ± σ):     13.029 s ±  0.136 s    [User: 60.489 s, System: 6.297 s]
  Range (min … max):   12.916 s … 13.184 s    5 run

mypy
  dnf

pyright
  Time (mean ± σ):     262.742 s ±  4.948 s    [User: 472.717 s, System: 18.898 s]
  Range (min … max):   259.173 s … 270.617 s    5 runs
</code></pre></details>

<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/fig-pytorch-torch-benchmarks.svg" alt="pytorch torch benchmarks"></p><details>
  <summary>Commands Run</summary>
  <ul>
<li><strong>pyrefly:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'pyrefly check torch'</code></li>
<li><strong>ty:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'ty check torch'</code></li>
<li><strong>mypy:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'mypy --cache-dir=/dev/null torch'</code></li>
<li><strong>pyright:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'pyright torch'</code></li>
</ul>
</details>


<details>
  <summary>Raw Data</summary>
  <pre tabindex="0"><code>ty
  Time (mean ± σ):      1.123 s ±  0.022 s    [User: 6.460 s, System: 0.604 s]
  Range (min … max):    1.082 s …  1.167 s    10 runs

pyrefly
  Time (mean ± σ):      2.347 s ±  0.261 s    [User: 15.876 s, System: 0.919 s]
  Range (min … max):    2.089 s …  2.988 s    10 runs
  
mypy
  Time (mean ± σ):     24.731 s ±  0.238 s    [User: 24.144 s, System: 0.519 s]
  Range (min … max):   24.299 s … 25.016 s    10 runs
  
pyright
  Time (mean ± σ):     48.096 s ±  1.705 s    [User: 68.526 s, System: 4.072 s]
  Range (min … max):   46.037 s … 50.488 s    10 runs
</code></pre></details>

<p>Out of the gate, we see that for both <code>pytorch</code> and just <code>torch</code>, ty is about 2-3x faster compared to pyrefly, and both are over 10x-20x faster than mypy and pyright.</p>
<blockquote>
<p>One interesting note is that pyrefly detected more source files than ty: about 8600 for pyrefly and 6500 for ty on <code>pytorch</code> (I’m not sure where the discrepancy comes from).</p></blockquote>
<blockquote>
<p><strong>It’s also important to remember that both pyrefly and ty are still in early alpha, and are not feature complete. This may skew the results!</strong></p></blockquote>
<h2 id="benchmarking---django">Benchmarking - Django</h2>
<p>Next, I ran the same benchmark on Django version 5.2.1.</p>
<blockquote>
<p>Note: mypy errored out during this test.</p></blockquote>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/fig-django-benchmarks.svg" alt="django benchmarks"></p><details>
  <summary>Commands Run</summary>
  <ul>
<li><strong>pyrefly:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'pyrefly check'</code></li>
<li><strong>ty:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'ty check'</code></li>
<li><strong>mypy:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'mypy --cache-dir=/dev/null .'</code></li>
<li><strong>pyright:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'pyright'</code></li>
</ul>
</details>


<details>
  <summary>Raw Data</summary>
  <pre tabindex="0"><code>ty
  Time (mean ± σ):     578.2 ms ±  27.8 ms    [User: 2980.4 ms, System: 546.9 ms]
  Range (min … max):   557.1 ms … 634.0 ms    10 runs

pyrefly
  Time (mean ± σ):     910.7 ms ±  26.2 ms    [User: 3033.0 ms, System: 565.0 ms]
  Range (min … max):   879.6 ms … 963.1 ms    10 runs
  
mypy
  dnf
  
pyright
  Time (mean ± σ):     16.324 s ±  0.476 s    [User: 24.477 s, System: 1.682 s]
  Range (min … max):   15.845 s … 17.182 s    10 runs
</code></pre></details>

<p>We see the same results across the board with ty being the fastest (2,900 files at 0.6s), pyrefly as a close second (3,200 files at 0.9s), and pyright being the slowest (16s).</p>
<h2 id="benchmarking---mypy">Benchmarking - Mypy</h2>
<p>Finally, I ran the benchmark on the <code>mypy</code> repo itself (more specifically the <code>mypyc</code> subdirectory). Similar results here.</p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/fig-mypy-mypyc-benchmarks.svg" alt="mypy mypyc benchmarks"></p><details>
  <summary>Commands Run</summary>
  <ul>
<li><strong>pyrefly:</strong> <code>hyperfine --warmup 3 --runs 20 --ignore-failure 'pyrefly check mypyc'</code></li>
<li><strong>ty:</strong> <code>hyperfine --warmup 3 --runs 20 --ignore-failure 'ty check mypyc'</code></li>
<li><strong>mypy:</strong> <code>hyperfine --warmup 3 --runs 20 --ignore-failure 'mypy --cache-dir=/dev/null mypyc'</code></li>
<li><strong>pyright:</strong> <code>hyperfine --warmup 3 --runs 20 --ignore-failure 'pyright mypyc'</code></li>
</ul>
</details>


<details>
  <summary>Raw Data</summary>
  <pre tabindex="0"><code>ty
  Time (mean ± σ):      74.2 ms ±   1.5 ms    [User: 403.4 ms, System: 41.6 ms]
  Range (min … max):    71.9 ms …  78.1 ms    20 runs

pyrefly
  Time (mean ± σ):     136.0 ms ±   1.5 ms    [User: 728.3 ms, System: 54.5 ms]
  Range (min … max):   133.4 ms … 139.6 ms    20 runs
  
mypy
  Time (mean ± σ):      3.544 s ±  0.099 s    [User: 3.442 s, System: 0.093 s]
  Range (min … max):    3.420 s …  3.774 s    20 runs
  
pyright
  Time (mean ± σ):      2.852 s ±  0.103 s    [User: 4.315 s, System: 0.227 s]
  Range (min … max):    2.704 s …  3.105 s    20 runs
</code></pre></details>

<h2 id="goals">Goals</h2>
<p>The primary goals between pyrefly and ty are where I feel the main difference lies. Pyrefly tries to be as aggressive as possible when typing — inferring as much as possible so that even code with absolutely no explicit types can have some amount of typing guarantees.</p>
<p>ty, on the other hand, follows a different mantra: <strong>the gradual guarantee</strong>. The principal idea is that in a well-typed program, removing a type annotation should not cause a type error. In other words: you shouldn’t need to add new types to working code to resolve type errors.</p>
<figure>
  <img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/gradual-guarantee.png" alt="the gradual guarantee slide from ty presentation">
  <figcaption>the gradual guarantee slide from ty presentation</figcaption>
</figure>
<p>This is shown in this example here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>MyClass</span>:
</span></span><span><span>    attr <span>=</span> <span>None</span>
</span></span><span><span>
</span></span><span><span>foo <span>=</span> MyClass()
</span></span><span><span>
</span></span><span><span><span># ➖ pyrefly | revealed type: None</span>
</span></span><span><span><span># ✅ ty.     | Revealed type: `Unknown | None`</span>
</span></span><span><span><span># ➖ mypy.   | Revealed type is "None"</span>
</span></span><span><span><span># ➖ pyright | Type of "foo.attr" is "None"</span>
</span></span><span><span>reveal_type(foo<span>.</span>attr)
</span></span><span><span>
</span></span><span><span><span># ➖ pyrefly | ERROR: Literal[1] is not assignable to attribute attr with type None</span>
</span></span><span><span><span># ✅ ty.     | &lt; No Error &gt;</span>
</span></span><span><span><span># ➖ mypy.   | ERROR: Incompatible types in assignment (expression has type "int", variable has type "None")</span>
</span></span><span><span><span># ➖ pyright | ERROR: Cannot assign to attribute "attr" for class "MyClass"</span>
</span></span><span><span>foo<span>.</span>attr <span>=</span> <span>1</span>
</span></span></code></pre></div><p>In this example, pyrefly, mypy, and pyright eagerly type <code>foo.attr</code> as <code>None</code> and throw an exception when assigned as <code>1</code> — whereas ty understands that <code>foo.attr = 1</code> should not actually cause a syntax error, and instead types <code>foo.attr</code> as <code>Unknown | None</code> to allow the assignment. (<code>Unknown</code> is a new type added by ty to denote between an <em>explicit</em> <code>Any</code> versus an <em>“unknown”</em> <code>Any</code>.)</p>
<p>As a consequence, this also means that pyrefly can catch some errors that other type checkers cannot. Take this example here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>my_list <span>=</span> [<span>1</span>, <span>"b"</span>, <span>None</span>]
</span></span><span><span>val <span>=</span> my_list<span>.</span>pop(<span>1</span>)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: int | str | None</span>
</span></span><span><span><span># ➖ ty.     | Revealed type: `Unknown`</span>
</span></span><span><span><span># ➖ mypy.   | Revealed type is "builtins.object"</span>
</span></span><span><span><span># ➖ pyright | Type of "val" is "Unknown"</span>
</span></span><span><span>reveal_type(val)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | ERROR: `*` is not supported between `None` and `Literal[2]`</span>
</span></span><span><span><span># ➖ ty.     | &lt; No Error &gt;</span>
</span></span><span><span><span># ➖ mypy.   | ERROR: Unsupported operand types for * ("object" and "int")</span>
</span></span><span><span><span># ➖ pyright | &lt; No Error &gt;</span>
</span></span><span><span>new_val <span>=</span> val <span>*</span> <span>2</span>
</span></span></code></pre></div><blockquote>
<p>mypy <em>technically</em> did throw an error, but for the wrong reasons. For example, setting <code>my_list = [1, "b"]</code> would fix the program, but mypy still reports a mismatch between <code>object</code> and <code>int</code>.</p></blockquote>
<p>Pyrefly implicitly types <code>val</code> as <code>int | str | None</code>, even though neither <code>val</code> nor <code>my_list</code> was explicitly typed. This correctly catches the <code>val * 2</code> error below.</p>
<p>This is just one of many examples, as more will be shown later in the <strong>Capabilities</strong> section.</p>
<h2 id="incrementalism">Incrementalism</h2>
<p>Both pyrefly and ty claim to be incremental — meaning that changing one file would only cause a re-parse on the affected area, and not the entire program. Pyrefly uses a custom incremental engine behind the scenes for its type checker. In constrast, ty uses <a href="https://github.com/salsa-rs/salsa">Salsa</a>, the same incremental framework that powers <a href="https://rust-analyzer.github.io/">Rust Analyzer</a>.</p>
<p>Interestingly, what that means is that ty has fine-grained incrementalization: changing a single function would only cause a re-parse on that function itself (and nothing else), and its dependent functions, and so on. Pyrefly, on the other hand, uses module-level incrementation: changing a single function would cause a re-parse on the entire file/module, and its dependent files/modules, etc.</p>
<p>The reason why pyrefly chose module-level over fine-grained (at least from what I’ve gathered) is that module-level incrementalization is already fast enough in Rust, and fine-grained incrementalization results in a much more complex and harder to maintain codebase with minimal performance improvements.</p>
<h2 id="capabilities">Capabilities</h2>
<p>Both the pyrefly and ty teams make it VERY CLEAR that they are still unfinished and in early alpha, with known issues, bugs, and incomplete features. Despite that, I think it’s cool to go over what each supports <em>as of now</em> as it showcases what each team has focused on and determined to be important so far for their next-generation Python type checkers.</p>
<h2 id="implicit-type-inference">Implicit Type Inference</h2>
<p>Implicit type inference is one of the showcase features of pyrefly. For example, here is a simple case of inferring return types:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>foo</span>(imp: Any):
</span></span><span><span>    <span>return</span> str(imp)
</span></span><span><span>
</span></span><span><span>a <span>=</span> foo(<span>123</span>)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: str</span>
</span></span><span><span><span># ➖ ty.     | Revealed type: `Unknown`</span>
</span></span><span><span><span># ➖ mypy.   | Revealed type is "Any"</span>
</span></span><span><span><span># ✅ pyright | Type of "a" is "str"</span>
</span></span><span><span>reveal_type(a)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | ERROR: `+` is not supported between `str` and `Literal[1]`</span>
</span></span><span><span><span># ➖ ty.     | &lt; No Error &gt;</span>
</span></span><span><span><span># ➖ mypy.   | &lt; No Error &gt;</span>
</span></span><span><span><span># ✅ pyright | ERROR: Operator "+" not supported for types "str" and "Literal[1]"</span>
</span></span><span><span>a <span>+</span> <span>1</span>
</span></span></code></pre></div><p>Here’s another example with inferring types of more complex collection objects (in this case, a <code>dict</code>):</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> typing <span>import</span> reveal_type
</span></span><span><span>
</span></span><span><span>my_dict <span>=</span> {
</span></span><span><span>    key: value <span>*</span> <span>2</span>
</span></span><span><span>    <span>for</span> key, value <span>in</span> {<span>"apple"</span>: <span>2</span>, <span>"banana"</span>: <span>3</span>, <span>"cherry"</span>: <span>1</span>}<span>.</span>items()
</span></span><span><span>    <span>if</span> value <span>&gt;</span> <span>1</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: dict[str, int]</span>
</span></span><span><span><span># ➖ ty.     | Revealed type: `@Todo`</span>
</span></span><span><span><span># ✅ mypy.   | Revealed type is "builtins.dict[builtins.str, builtins.int]"</span>
</span></span><span><span><span># ✅ pyright | Type of "my_dict" is "dict[str, int]"</span>
</span></span><span><span>reveal_type(my_dict)
</span></span></code></pre></div><p><strong>But,</strong> here is where the “gradual guarantee” of ty comes in. Take this example here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>my_list <span>=</span> [<span>1</span>, <span>2</span>, <span>3</span>]
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: list[int]</span>
</span></span><span><span><span># ➖ ty.     | Revealed type: `list[Unknown]`</span>
</span></span><span><span><span># ✅ mypy.   | Revealed type is "builtins.list[builtins.int]"</span>
</span></span><span><span><span># ✅ pyright | Type of "my_list" is "list[int]"</span>
</span></span><span><span>reveal_type(my_list)
</span></span><span><span>
</span></span><span><span><span># ➖ pyrefly | ERROR: Argument `Literal['foo']` is not assignable to parameter with type `int` in function `list.append`</span>
</span></span><span><span><span># ✅ ty.     | &lt; No Error &gt;</span>
</span></span><span><span><span># ➖ mypy.   | ERROR: Argument 1 to "append" of "list" has incompatible type "str"; expected "int" </span>
</span></span><span><span><span># ➖ pyright | ERROR: Argument of type "Literal['foo']" cannot be assigned to parameter "object" of type "int" in function "append"</span>
</span></span><span><span>my_list<span>.</span>append(<span>"foo"</span>)
</span></span></code></pre></div><p>pyrefly, mypy, and pyright all assume that <code>my_list.append("foo")</code> is a typing error, even though it is <em>technically</em> allowed (Python collections can have multiple types of objects!) If this is the intended behavior, ty is the only checker that implicitly allows this without requiring additional explicit typing on <code>my_list</code>.</p>
<h2 id="generics">Generics</h2>
<p>Another thing the pyrefly team mentioned during their talk was that while redesigning pyrefly from the ground up, they focused on the “hard problems first”. This means that a lot of the architecture around pyrefly was built around things like generics, overloads, and wildcard imports.</p>
<p>For example, here are some examples where pyrefly and ty both have correct generic resolution:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># === Simple Case ===</span>
</span></span><span><span><span>class</span> <span>Box</span>[T]:
</span></span><span><span>    <span>def</span> <span>__init__</span>(self, val: T) <span>-&gt;</span> <span>None</span>:
</span></span><span><span>        self<span>.</span>val <span>=</span> val
</span></span><span><span>
</span></span><span><span>b: Box[int] <span>=</span> Box(<span>42</span>)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: int</span>
</span></span><span><span><span># ✅ ty.     | Revealed type: `Unknown | int`</span>
</span></span><span><span><span># ✅ mypy.   | Revealed type is "builtins.int"</span>
</span></span><span><span><span># ✅ pyright | Type of "b.val" is "int"</span>
</span></span><span><span>reveal_type(b<span>.</span>val)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | ERROR: Argument `Literal[100]` is not assignable to parameter `val` with type `str` in function `Box.__init__`</span>
</span></span><span><span><span># ✅ ty.     | ERROR: Object of type `Box[int]` is not assignable to `Box[str]`</span>
</span></span><span><span><span># ✅ mypy.   | ERROR: Argument 1 to "Box" has incompatible type "int"; expected "str"</span>
</span></span><span><span><span># ✅ pyright | ERROR: Type "Box[int]" is not assignable to declared type "Box[str]"</span>
</span></span><span><span>b2: Box[str] <span>=</span> Box(<span>100</span>)
</span></span><span><span>
</span></span><span><span><span># === Bounded Types with Attribute ===</span>
</span></span><span><span><span>class</span> <span>A</span>:
</span></span><span><span>    x: int <span>|</span> str
</span></span><span><span>
</span></span><span><span><span>def</span> <span>f</span>[T: A](x: T) <span>-&gt;</span> T:
</span></span><span><span>    <span># ✅ pyrefly | revealed type: int | str</span>
</span></span><span><span>    <span># ✅ ty.     | Revealed type: `int | str`</span>
</span></span><span><span>    <span># ✅ mypy.   | Revealed type is "Union[builtins.int, builtins.str]"</span>
</span></span><span><span>    <span># ✅ pyright | Type of "x.x" is "int | str"</span>
</span></span><span><span>    reveal_type(x<span>.</span>x)
</span></span><span><span>    <span>return</span> x
</span></span></code></pre></div><p>Whereas here are some examples where pyrefly has better generic resolution compared to ty:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> typing <span>import</span> Callable, TypeVar, assert_type, reveal_type
</span></span><span><span>    
</span></span><span><span><span># === Generic Class Without Explicit Type Param ===</span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>C</span>[T]:
</span></span><span><span>    x: T
</span></span><span><span>
</span></span><span><span>c: C[int] <span>=</span> C()
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: C[int]</span>
</span></span><span><span><span># ➖ ty.     | `C[Unknown]`</span>
</span></span><span><span><span># ✅ pypy.   | Revealed type is "__main__.C[builtins.int]"</span>
</span></span><span><span><span># ✅ pyright | Type of "c" is "C[int]"</span>
</span></span><span><span>reveal_type(c)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: int</span>
</span></span><span><span><span># ➖ ty.     | Revealed type: `Unknown`</span>
</span></span><span><span><span># ✅ pypy.   | Revealed type is "builtins.int"</span>
</span></span><span><span><span># ✅ pyright | Type of "c.x" is "int"</span>
</span></span><span><span>reveal_type(c<span>.</span>x)
</span></span><span><span>
</span></span><span><span><span># === Bounded Types with Callable Attribute ===</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>func</span>[T: Callable[[int], int]](a: T, b: int) <span>-&gt;</span> T:
</span></span><span><span>    <span># ✅ pyrefly | revealed type: int</span>
</span></span><span><span>    <span># ➖ ty.     | ERROR: &lt;Error: Object of type `T` is not callable&gt;</span>
</span></span><span><span>    <span># ✅ pypy.   | Revealed type is "builtins.int"</span>
</span></span><span><span>    <span># ✅ pyright | Type of "a(b)" is "int"</span>
</span></span><span><span>    reveal_type(a(b))
</span></span><span><span>    <span>return</span> a
</span></span></code></pre></div><p>Interestingly enough, both pyrefly and ty seem to struggle with resolving covariance and contravariance relationships. Example here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> __future__ <span>import</span> annotations
</span></span><span><span>
</span></span><span><span><span>class</span> <span>A</span>[X]:
</span></span><span><span>    <span>def</span> <span>f</span>(self) <span>-&gt;</span> B[X]:
</span></span><span><span>        <span>...</span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>B</span>[Y]:
</span></span><span><span>    <span>def</span> <span>h</span>(self) <span>-&gt;</span> B[Y]:
</span></span><span><span>        <span>...</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>cast_a</span>(a: A[bool]) <span>-&gt;</span> A[int]:
</span></span><span><span>    <span># ➖ pyrefly | ERROR: Return type does not match returned value: expected `A[int]`, found `A[bool]`</span>
</span></span><span><span>    <span># ➖ ty.     | ERROR: Returned type `A[bool]` is not assignable to declared return type `A[int]`</span>
</span></span><span><span>    <span># ✅ mypy.   | &lt; No Error &gt;</span>
</span></span><span><span>    <span># ✅ pyright | &lt; No Error &gt;</span>
</span></span><span><span>    <span>return</span> a  <span># Allowed</span>
</span></span></code></pre></div><h2 id="informative-error-messages">Informative Error Messages</h2>
<p>One explicit feature of ty is to have clear and concise error messages.</p>
<p>For example, here is a simple example of a function call with mismatched types:</p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/ty-error-message.png" alt="ty-error-message.png"></p>
<p>Compared to pyrefly, mypy, and pyright:</p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/pyrefly-error-message.png" alt="pyrefly-error-message.png"></p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/mypy-error-message.png" alt="mypy-error-message.png"></p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/pyright-error-message.png" alt="pyright-error-message.png"></p>
<p>Here is another example with mismatched return types:</p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/ty-error-message-2.png" alt="ty-error-message-2.png"></p>
<p>In my opinion, much cleaner! It’s exciting to see new and improved error messages coming to Python.</p>
<h2 id="intersection-and-negation-types">Intersection and Negation Types</h2>
<p>Finally, one really cool feature the Astral team showed off was support for intersection and negation types — which they claim is the only Python type checker to implement. To illustrate this, take a look at this example:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>WithX</span>:
</span></span><span><span>  x: int
</span></span><span><span>
</span></span><span><span><span>@final</span>
</span></span><span><span><span>class</span> <span>Other</span>:
</span></span><span><span>  <span>pass</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>foo</span>(obj: WithX <span>|</span> Other):
</span></span><span><span>  <span>if</span> hasattr(obj, <span>"x"</span>):
</span></span><span><span>    <span># ➖ pyrefly | revealed type: Other | WithX</span>
</span></span><span><span>    <span># ✅ ty.     | Revealed type: `WithX`</span>
</span></span><span><span>    <span># ➖ mypy.   | Revealed type is "Union[__main__.WithX, __main__.Other]"</span>
</span></span><span><span>    <span># ➖ pyright | Type of "obj" is "WithX | Other"</span>
</span></span><span><span>    reveal_type(obj)
</span></span></code></pre></div><blockquote>
<p><code>@final</code> is a new feature in Python 3.12 that prevents a class from being subclassed. This is important for the type checker to know that <code>Other</code> cannot be subclassed with <code>x</code> in the future.</p></blockquote>
<p>Given the constraints that <code>obj</code> is either <code>WithX</code> or final type <code>Other</code>, and <code>obj</code> <em>has</em> to have attribute <code>x</code>, the only resolvable type for <code>obj</code> at <code>reveal_type(obj)</code> is <code>WithX</code>. Breaking down what happens behind the scenes:</p>
<pre tabindex="0"><code>(WithX | Other) &amp; &lt;Protocol with members 'x'&gt;
=&gt; (WithX &amp; &lt;Protocol with members 'x'&gt; | (Other &amp; &lt;Protocol with members 'x'&gt;)
=&gt; WithX | Never
=&gt; WithX
</code></pre><p>Take a look at another example here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>MyClass</span>:
</span></span><span><span>    <span>...</span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>MySubclass</span>(MyClass):
</span></span><span><span>    <span>...</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>bar</span>(obj: MyClass):
</span></span><span><span>    <span>if</span> <span>not</span> isinstance(obj, MySubclass):
</span></span><span><span>        <span># ➖ pyrefly | revealed type: MyClass</span>
</span></span><span><span>        <span># ✅ ty.     | Revealed type: `MyClass &amp; ~MySubclass`</span>
</span></span><span><span>        <span># ➖ mypy.   | Revealed type is "__main__.MyClass"</span>
</span></span><span><span>        <span># ➖ pyright | Type of "obj" is "MyClass"</span>
</span></span><span><span>        reveal_type(obj)
</span></span></code></pre></div><p>ty is the only type checker to resolve <code>obj</code> at <code>reveal_type(obj)</code> to <code>MyClass &amp; ~MySubclass</code>. This means that ty introduces new paradigms to Python types:</p>
<p><strong>intersections and negations!</strong> Neat!</p>
<p>However, this is still in early alpha! For example, this case here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>bar</span>(obj: HasFoo):
</span></span><span><span>    <span>if</span> <span>not</span> hasattr(obj, <span>"bar"</span>):
</span></span><span><span>        reveal_type(obj)
</span></span><span><span>        reveal_type(obj<span>.</span>foo)
</span></span></code></pre></div><p><code>reveal_type(obj)</code> has the correct type of <code>HasFoo &amp; ~&lt;Protocol with members 'bar'&gt;</code>, but <code>reveal_type(obj.foo)</code> resolves to <code>@Todo</code> even though <code>obj.foo</code> should be resolvable to the function <code>foo</code> given the constraints.</p>
<p>As one final fun party trick, here is ty using intersection and negation types to “solve” <a href="https://en.wikipedia.org/wiki/Diophantine_equation">diophantine equations</a>:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># Simply provide a list of all natural numbers here ...</span>
</span></span><span><span>type Nat <span>=</span> Literal[<span>1</span>, <span>2</span>, <span>3</span>, <span>4</span>, <span>5</span>, <span>6</span>, <span>7</span>, <span>8</span>, <span>9</span>, <span>10</span>, <span>11</span>, <span>12</span>, <span>13</span>]
</span></span><span><span>
</span></span><span><span><span>def</span> <span>pythagorean_triples</span>(a: Nat, b: Nat, c: Nat):
</span></span><span><span>    reveal_type(a<span>**</span><span>2</span> <span>+</span> b<span>**</span><span>2</span> <span>==</span> c<span>**</span><span>2</span>)
</span></span><span><span>    <span># reveals 'bool': solutions exist (3² + 4² == 5²)</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>fermats_last_theorem</span>(a: Nat, b: Nat, c: Nat):
</span></span><span><span>    reveal_type(a<span>**</span><span>3</span> <span>+</span> b<span>**</span><span>3</span> <span>==</span> c<span>**</span><span>3</span>)
</span></span><span><span>    <span># reveals 'Literal[False]': no solutions!</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>catalan_conjecture</span>(a: Nat, b: Nat):
</span></span><span><span>    reveal_type(a<span>**</span><span>2</span> <span>-</span> b<span>**</span><span>3</span> <span>==</span> <span>1</span>)
</span></span><span><span>    <span># reveals 'bool': solutions exist (3² - 2³ == 1)</span>
</span></span></code></pre></div><h2 id="final-thoughts">Final Thoughts</h2>
<p>Overall, it’s exciting to have two new faster type checkers in the Python ecosystem! As of right now, pyrefly and ty seem to follow two different systematic goals. Ty takes a gradual approach to typing - given a program that (theoretically) runs flawlessly, running a type checker should not raise any new typing errors - and if it does, it probably indicates an actual flaw somewhere in the code. Pyrefly takes a different approach, one that is similar to many state-of-the-art Python type checkers today - infer as many types as possible, at the cost of possibly introducing typing errors where it shouldn’t.</p>
<p>As mentioned multiple times, both pyrefly and ty are in early alpha. I strongly suspect the features and capabilities of both tools will converge as time goes on, but nevertheless, it is still cool to see where the two type checkers are at now and how they might come into play in different scenarios sometime in the future.</p>
<p><strong>Go try these out for yourself now!</strong></p>
<p>You can try out pyrefly over at <strong><a href="https://pyrefly.org/sandbox">pyrefly.org/sandbox</a></strong>, and ty over at <strong><a href="https://play.ty.dev/">play.ty.dev</a></strong>. Both also have their respective <code>pip install</code> commands and plugins for your editor (VSCode, Cursor, etc).</p>
<p>In the meantime, I heard rumors that Google is planning on open-sourcing their own Go-based Python type checker, so it’ll be very cool to check that out once it comes out 👀 …</p>
<h2 id="appendix">Appendix</h2>
<p>I just wanted to call out that ty’s tests are written in… <strong>MARKDOWN</strong>! How cool is that?</p>
<blockquote>
<p><strong><a href="https://github.com/astral-sh/ruff/tree/main/crates/ty_python_semantic/resources/mdtest">https://github.com/astral-sh/ruff/tree/main/crates/ty_python_semantic/resources/mdtest</a></strong></p></blockquote>
<hr>
<p><em>Thanks for reading!</em></p>
<p><em>If you notice any mistakes, comments, or feedback, please let me know!</em></p>
<p><em>Contact: <a href="mailto:blog@edward-li.com">blog@edward-li.com</a></em></p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Malai – securely share local TCP services (database/SSH) with others (107 pts)]]></title>
            <link>https://malai.sh/hello-tcp/</link>
            <guid>44107393</guid>
            <pubDate>Tue, 27 May 2025 14:34:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://malai.sh/hello-tcp/">https://malai.sh/hello-tcp/</a>, See on <a href="https://news.ycombinator.com/item?id=44107393">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="38"><div data-id="42"><p>Introducing Malai TCP &amp; A Bonus!</p><comment data-id="44"></comment></div><div data-id="45"><p><a href="https://github.com/kulfi-project/kulfi/releases/"><code>malai-0.2.5</code></a> is out now!
It brings a new feature to share your local TCP server with the world!</p><p>
Now you can share any TCP-based service running locally — including your
SSH service, Postgres database, Redis, or even a custom TCP protocol — using the
same seamless workflow that you used with <code>malai http</code>.</p></div><p>Install <code>malai</code> today using:</p><div data-id="47"><comment data-id="48"></comment><pre data-id="57"><code data-id="58">curl -fsSL https://malai.sh/install.sh | sh
</code></pre><comment data-id="59"></comment></div><p>And run:</p><div data-id="67"><comment data-id="68"></comment><pre data-id="77"><code data-id="78">$ malai tcp 5432 --public
Malai: Sharing port 5432
Run malai tcp-bridge &lt;id52&gt; &lt;some-port&gt;
to connect to it from any machine.
</code></pre><comment data-id="79"></comment></div><p>This will share your local TCP server running on port 5432 with the world. You
can connect to it from any machine using the command:</p><div data-id="87"><comment data-id="88"></comment><pre data-id="97"><code data-id="98">$ malai tcp-bridge &lt;id52&gt; 9091
Listening on 127.0.0.1:9091
</code></pre><comment data-id="99"></comment></div><p>Now you can connect to <code>localhost:9091</code> and it'll go through <code>malai</code> and
connect to the exposed service.</p><div data-id="107"><p>Share your SSH server</p><comment data-id="109"></comment><div data-id="110"><p>You can even use <code>malai tcp</code> to expose your local SSH server for remote access — without opening port 22 publicly.</p><p>
First, make sure the OpenSSH server is running:</p></div></div><p>Then, run the following on the machine where the SSH server is running:</p><div data-id="131"><comment data-id="132"></comment><pre data-id="141"><code data-id="142">$ malai tcp 22 --public
Malai: Sharing port 5432
Run malai tcp-bridge &lt;id52&gt; &lt;some-port&gt;
to connect to it from any machine.
</code></pre><comment data-id="143"></comment></div><p>On another machine, use the bridge command:</p><div data-id="151"><comment data-id="152"></comment><pre data-id="161"><code data-id="162">$ malai tcp-bridge &lt;id52&gt; 9090
</code></pre><comment data-id="163"></comment></div><p>Replace <code>&lt;id52&gt;</code> with the ID printed by the <code>malai tcp</code> command. Once the
bridge is running, SSH into your machine like this:</p><div data-id="171"><comment data-id="172"></comment><pre data-id="181"><code data-id="182">ssh -p 9090 user@localhost
</code></pre><comment data-id="183"></comment></div><p>You're connecting to <code>localhost:9090</code>, which is where the <code>tcp-bridge</code> is
listening. It forwards your SSH traffic to the original machine via the Kulfi
network. Make sure to use the correct <code>user</code> that exists on the remote machine.</p><div data-id="191"><p>Use cases</p><comment data-id="193"></comment><div data-id="194"><ul>
<li>Secure your SSH server behind the Kulfi network.</li>
<li>Share a local Postgres or Redis instance with your team.</li>
<li>Demo a multiplayer game server or custom TCP service.</li>
<li>Students can share networked apps or environments with instructors for
real-time help or grading.</li>
</ul></div></div><p>To learn more about <code>malai tcp</code>, check out the <a href="https://malai.sh/tcp/">documentation</a>.</p><div data-id="196"><p>Wait, we have more!</p><comment data-id="198"></comment><p>We've also added a new <code>malai folder</code> command to share a folder with everyone.
This is similar to <code>malai http</code> but it serves your local files and folders.
This is more like a call for testing than launching a new feature. Try it out
and give us feedback!</p></div><div data-id="200"><comment data-id="201"></comment><pre data-id="211"><code data-id="212">$ malai folder ~/projects/fastn/assets/ --public
Serving "/Users/siddhant/projects/fastn/assets" on http://127.0.0.1:59136
Malai: Sharing http://127.0.0.1:59136 at
https://pubqaksutn9im0ncln2bki3i8diekh3sr4vp94o2cg1agjrb8dhg.kulfi.site
To avoid the public proxy, run your own with: malai http-bridge

Or use: malai browse kulfi://pubqaksutn9im0ncln2bki3i8diekh3sr4vp94o2cg1agjrb8dhg
</code></pre><comment data-id="213"></comment></div><p>This spins up a basic HTTP server behind the scenes to serve the provided folder:</p><div data-id="221"><div data-id="222"><p><img data-id="223" src="https://malai.sh/-/malai.sh/assets/malai-folder-browser-view.png"></p><comment data-id="224"></comment><p>Browsing a folder served by <code>malai</code></p></div><comment data-id="226"></comment></div><div data-id="230"><p>We're just getting started, and your support means a lot.</p><p>
If you like what we're building, consider <a href="https://github.com/kulfi-project/kulfi">starring the
repo</a> on GitHub. It helps others
discover the project and keeps us motivated to build more!</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral Agents API (146 pts)]]></title>
            <link>https://mistral.ai/news/agents-api</link>
            <guid>44107187</guid>
            <pubDate>Tue, 27 May 2025 14:09:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/agents-api">https://mistral.ai/news/agents-api</a>, See on <a href="https://news.ycombinator.com/item?id=44107187">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p dir="ltr"><img src="https://cms.mistral.ai/assets/f2a4b295-ff64-4c16-a42a-14f858c65766.png?width=1080&amp;height=457" alt="Cover"></p>
<p dir="ltr">Today we announce our new Agents API, a major step forward in making AI more capable, useful, and an active problem-solver.</p>
<p dir="ltr">Traditional language models excel at generating text but are limited in their ability to perform actions or maintain context. Our new Agents API addresses these limitations by combining Mistral's powerful language models with:</p>
<ul>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Built-in connectors for code execution, web search, image generation, and MCP tools&nbsp;</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Persistent memory across conversations&nbsp;</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Agentic orchestration capabilities</p>
</li>
</ul>
<p dir="ltr">The Agents API complements our <a href="https://docs.mistral.ai/capabilities/completion/" target="_blank" rel="noopener">Chat Completion API</a> by offering a dedicated framework that simplifies implementing agentic use cases. It serves as the backbone of enterprise-grade agentic platforms.</p>
<p dir="ltr">By providing a reliable framework for AI agents to handle complex tasks, maintain context, and coordinate multiple actions, the Agents API enables enterprises to use AI in more practical and impactful ways.</p>
<h2 dir="ltr">Mistral agents in action.</h2>
<p dir="ltr">Explore the diverse applications of Mistral’s Agents API across various sectors:</p>
<ul>
<li id="demo-github" dir="ltr">
<h3>Coding assistant with Github.</h3>
<p dir="ltr">An agentic workflow built with Mistral's agents API where an agent interacts with Github and oversees a developer agent, powered by DevStral to write code. The agent is granted full authority over Github, showcasing automated software development task management.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/1Tt9Fq1pUPQ?si=j4fIT7TqM1RGsyRG" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<a href="https://github.com/mistralai/cookbook/tree/main/mistral/agents/github_agent" target="_blank" rel="noopener">Read our cookbook</a></li>
<li id="demo-linear" dir="ltr">
<h3>Linear tickets assistant.</h3>
<p dir="ltr">An intelligent task coordination assistant powered by our Agents API, using multi-server MCP architecture to transform call transcripts to PRDs to actionable Linear issues and track project deliverables.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/4UPP-JEjcKo?si=gMuPof7qCpuHuc2z" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<a href="https://github.com/mistralai/cookbook/tree/main/mistral/agents/prd_linear_ticket" target="_blank" rel="noopener">Read our cookbook</a></li>
<li id="demo-finance" dir="ltr">
<h3>Financial analyst.</h3>
<p dir="ltr">A financial advisory agent constructed with our Agents API, orchestrating multiple MCP servers to source financial metrics, compile insights, and archive results securely.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/ocxRKz73UJw?si=2xJffa3oIFBViA56" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<a href="https://github.com/mistralai/cookbook/tree/main/mistral/agents/financial_analyst" target="_blank" rel="noopener">Read our cookbook</a></li>
<li id="demo-travel" dir="ltr">
<h3>Travel assistant.</h3>
<p dir="ltr">A powerful AI travel assistant that helps users plan their trips, book accommodations, and manage travel needs.</p>
<iframe title="YouTube video player" src="https://www.youtube.com/embed/DSYlhtG2UNM?si=ZZH4OSd1u3QzhpwF" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe><a href="https://github.com/mistralai/cookbook/tree/main/mistral/agents/travel_assistant" target="_blank" rel="noopener">Read our cookbook</a></li>
<li id="demo-nutrition" dir="ltr">
<h3>Nutrition assistant.</h3>
<p dir="ltr">An AI-powered food diet companion designed to help users establish goals, log meals, receive personalized food suggestions, track their daily achievements, and discover dining options that align with their nutritional targets.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/uEG2z2esl14?si=Ca_PY02gfVeWChgJ" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<a href="https://github.com/mistralai/cookbook/tree/main/mistral/agents/food_diet_companion" target="_blank" rel="noopener">Read our cookbook</a></li>
</ul>
<h2 dir="ltr">Create an agent with built-in connectors and MCP tools.</h2>
<p dir="ltr">Each agent can be equipped with powerful built-in connectors, which are tools that are deployed and ready for Agents to call on demand, and MCP tools:&nbsp;</p>
<ul>
<li dir="ltr" aria-level="1">
<h3><a href="https://docs.mistral.ai/agents/connectors/code_interpreter/" target="_blank" rel="noopener">Code execution</a></h3>
<p dir="ltr" role="presentation">The Agents API can use the code execution connector, empowering developers to create agents that execute Python code in a secure sandboxed environment. This enables agents to tackle a wide range of tasks, including mathematical calculations and analysis, data visualization and plotting, and scientific computing.</p>
</li>
<li dir="ltr" aria-level="1">
<h3><a href="https://docs.mistral.ai/agents/connectors/image_generation/" target="_blank" rel="noopener">Image generation</a></h3>
<p dir="ltr" role="presentation">The image generation connector tool, powered by Black Forest Lab FLUX1.1 [pro] Ultra, enables agents to create images for diverse applications. This feature can be leveraged for various use cases such as generating visual aids for educational content, creating custom graphics for marketing materials, or even producing artistic images.</p>
</li>
<li dir="ltr" aria-level="1">
<h3><a href="https://docs.mistral.ai/agents/connectors/document_library/" target="_blank" rel="noopener">Document library</a></h3>
<p dir="ltr" role="presentation">Document Library is a built-in connector tool that enables agents to access documents from Mistral Cloud. It powers the integrated RAG functionality, strengthening agents’ knowledge by leveraging the content of user-uploaded documents.</p>
</li>
<li dir="ltr" aria-level="1">
<h3><a href="https://docs.mistral.ai/agents/connectors/websearch/" target="_blank" rel="noopener">Web search</a></h3>
<p dir="ltr" role="presentation">The Agents API offers web search as a connector, enabling developers to combine Mistral models with diverse, up-to-date information from web search, reputable news, and other sources. This integration facilitates the delivery of up-to-date, informed, evidence-supported responses.</p>
<p dir="ltr">Agents with web search capabilities show a significant improvement in performance. In the SimpleQA benchmark, Mistral Large and Mistral Medium with web search achieve scores of 75% and 82.32%, respectively, compared to 23% and 22.08% without web search (see figure below).</p>
<h4>SimpleQA Accuracy (Higher is better)</h4>

</li>
<li dir="ltr">
<h3><a href="https://docs.mistral.ai/agents/mcp/" target="_blank" rel="noopener">MCP tools</a></h3>
<p dir="ltr" role="presentation">The Agents API SDK can also leverage tools built on the Model Context Protocol (MCP)—an open, standardized protocol that enables seamless integration between agents and external systems. MCP tools provide a flexible and extensible interface for agents to access real-world context, including APIs, databases, user data, documents, and other dynamic resources. Check out the <a href="#demo-github" rel="noopener">Github</a>, <a href="#demo-finance" rel="noopener">Financial Analyst</a>, and <a href="#demo-linear" rel="noopener">Linear</a> MCP demos to learn how to use MCP tools with Mistral Agents in action.</p>
<img src="https://cms.mistral.ai/assets/5a0eb67b-819c-4a3f-9cc0-7dba190d58d2.svg?width=null&amp;height=null" alt="Mcp Mistral"></li>
</ul>
<h2 dir="ltr">Memory and context with stateful conversations.</h2>
<p dir="ltr">The Agents API provides robust conversation management through a flexible and stateful conversation system. Each conversation retains its context, allowing for seamless and coherent interactions over time.</p>
<ul>
<li>
<h3><a href="https://docs.mistral.ai/agents/agents_basics/#conversations" target="_blank" rel="noopener">Conversation management</a></h3>
<p dir="ltr">There are two ways to start a conversation:</p>
<ol>
<li dir="ltr">With an Agent: Create a conversation with a specific agent_id to leverage its specialized capabilities.</li>
<li dir="ltr">Direct Access: Start a conversation by directly specifying the model and completion parameters, providing quick access to built-in connectors.</li>
</ol>
<p dir="ltr">Each conversation maintains a structured history through conversation entries, ensuring that the context is preserved across interactions.</p>
</li>
<li>
<h3><a href="https://docs.mistral.ai/agents/agents_basics/#continue-a-conversation-working" target="_blank" rel="noopener">Stateful interactions and conversation branching</a></h3>
<p dir="ltr">Developers are no longer required to monitor conversion history; they have the ability to view past conversations. They can always continue any conversation or initiate new conversation paths from any point.&nbsp;</p>
</li>
<li dir="ltr">
<h3><a href="https://docs.mistral.ai/agents/agents_basics/#streaming-output-working" target="_blank" rel="noopener">Streaming output</a></h3>
<p dir="ltr">The API also supports streaming outputs, both when starting a conversation and continuing a previous one. This feature allows for real-time updates and interactions.&nbsp;</p>
</li>
</ul>
<h2 dir="ltr">Agent orchestration.</h2>
<p dir="ltr">The true power of our Agents API lies in its ability to orchestrate multiple agents to solve complex problems. Through dynamic orchestration, agents can be added or removed from a conversation as needed—each one contributing its unique capabilities to tackle different parts of a problem.</p>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/55ca02be-4dfa-4f0e-ba6a-adc7c54dce4c.svg?width=null&amp;height=null" alt="Agents"></p>
<ul>
<li dir="ltr">
<h3><a href="https://docs.mistral.ai/agents/handoffs/#create-an-agentic-workflow" target="_blank" rel="noopener">Creating an agentic workflow</a></h3>
<p dir="ltr">To build a workflow with handoffs, start by creating all necessary agents. You can create as many agents as needed, each with specific tools and models, to form a tailored workflow.</p>
</li>
<li dir="ltr">
<h3><a href="https://docs.mistral.ai/https://docs.mistral.ai/agents/handoffs/#create-an-agentic-workflow" target="_blank" rel="noopener">Agent handoffs</a></h3>
<p dir="ltr">Once agents are created, define which agents can hand off tasks to others. For example, a finance agent might delegate tasks to a web search agent or a calculator agent based on the conversation's needs.</p>
<p dir="ltr">Handoffs enable a seamless chain of actions. A single request can trigger tasks across multiple agents, each handling specific parts of the request. This collaborative approach allows for efficient and effective problem-solving, unlocking powerful possibilities for real-world applications.</p>
</li>
</ul>
<h2 dir="ltr">Get started.</h2>
<p dir="ltr">To get started, check out our <a href="https://docs.mistral.ai/agents/agents_introduction" target="_blank" rel="noopener">docs</a>, create your first agent, and start building!&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Cline Doesn't Index Your Codebase (and Why That's a Good Thing) (160 pts)]]></title>
            <link>https://cline.bot/blog/why-cline-doesnt-index-your-codebase-and-why-thats-a-good-thing</link>
            <guid>44106944</guid>
            <pubDate>Tue, 27 May 2025 13:44:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cline.bot/blog/why-cline-doesnt-index-your-codebase-and-why-thats-a-good-thing">https://cline.bot/blog/why-cline-doesnt-index-your-codebase-and-why-thats-a-good-thing</a>, See on <a href="https://news.ycombinator.com/item?id=44106944">Hacker News</a></p>
<div id="readability-page-1" class="page"><div href="/install?utm_source=website&amp;utm_medium=header"><span><svg fill="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M23.15 2.587L18.21.21a1.494 1.494 0 0 0-1.705.29l-9.46 8.63-4.12-3.128a.999.999 0 0 0-1.276.057L.327 7.261A1 1 0 0 0 .326 8.74L3.899 12 .326 15.26a1 1 0 0 0 .001 1.479L1.65 17.94a.999.999 0 0 0 1.276.057l4.12-3.128 9.46 8.63a1.492 1.492 0 0 0 1.704.29l4.942-2.377A1.5 1.5 0 0 0 24 20.06V3.939a1.5 1.5 0 0 0-.85-1.352zm-5.146 14.861L10.826 12l7.178-5.448v10.896z"></path></svg></span><p><span>Install Cline<!-- --> • <!-- -->1.6M<!-- --> <!-- -->installs</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DuckLake is an integrated data lake and catalog format (242 pts)]]></title>
            <link>https://ducklake.select/</link>
            <guid>44106934</guid>
            <pubDate>Tue, 27 May 2025 13:43:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ducklake.select/">https://ducklake.select/</a>, See on <a href="https://news.ycombinator.com/item?id=44106934">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
		<!-- <div class="searchoverlay">
	<div>
		<form autocomplete="off">
			<div class="autocomplete">
				<div class="empty_input"></div>
				<input id="q" type="text" name="q" placeholder="Search docs & blog">
			</div>
		</form>
		<div id="search_results"></div>
		<div class="shortcuts">
			Search Shortcut <span>cmd</span> + <span>k</span> | <span>ctrl</span> + <span>k</span>
		</div>
	</div>
</div> -->

		

			

<div>
      
    
			<p>DuckLake delivers advanced data&nbsp;lake features without traditional lakehouse complexity by using Parquet files and your SQL database. It's an open, standalone format from the DuckDB team.</p>
			
		</div>



<div>
		<div>
      <h2>
        
        Deployment scenarios
        
      </h2>
    
			<p>DuckLake uses a database system to manage your metadata for the catalog. All you need to run your own data warehouse is a database system and storage for Parquet files.</p>
		</div>
		<div>
					<div>
						<ul>
							
							<li data-tab="arch-tab2" data-iconclass="postgre" data-multi="true">PostgreSQL</li>
							<li data-tab="arch-tab3" data-iconclass="sqlite" data-multi="true">SQLite</li>
							<li data-tab="arch-tab4" data-iconclass="mysql" data-multi="true">MySQL</li>
							<li data-tab="arch-tab1" data-iconclass="duckdb" data-multi="false">DuckDB</li>
						</ul>
						<p>← Choose catalog database</p>
					</div>
					
				
					<div>
						<div>
      <h4>
        
        Client
        
      </h4>
    
							
							
							
							<div>
								<div>
      <h4>
        
        Clients
        
      </h4>
    
									<p>Users can run multiple DuckLake clients and connect concurrently to PostgreSQL, MySQL or SQLite.</p>
								</div>
								<div>
      <h4>
        
        Client
        
      </h4>
    
									<p>DuckDB also works with DuckLake as the catalog database. In this case, you are limited to a single client.</p>
								</div>
							</div>
							
						</div>
						<div>
							<div>
      <h4>
        
        Catalog database
        
      </h4>
    
								
								<div>
									<p><img src="https://ducklake.select/images/deployment_diagram/database.svg" alt="Database Icon"></p>
								</div>
								
								<div>
      <h4>
        
        Catalog database
        
      </h4>
    
										<p>DuckLake can use any SQL system as its catalog database, provided that it supports ACID transactions and primary key constraints.</p>
									</div>
								
								
							</div>
							<div>
      <h4>
        
        Storage
        
      </h4>
    
								
								<div>
									<p><img src="https://ducklake.select/images/deployment_diagram/parquet_folder.svg" alt="Parquet Folder"></p><p>Parquet</p>
								</div>
								
								<div>
      <h4>
        
        Storage
        
      </h4>
    
										<p>DuckLake can store your data on any object storage such as AWS S3.</p>
									</div>
								
								
							</div>
						</div>
					</div>
					
					
				</div>
	</div>


<!--
<section>
	<div class="wrap">
      <h2>
        
        Use cases
        
      </h2>
    
		<div class="cards vertical images">
			<div class="card">
				<div class="image"></div>
				<div class="content">
      <h3>
        
        Multiplayer DuckDB
        
      </h3>
    
					<p>DuckLake unlocks concurrency for multiple DuckDB clients.</p>
					<a href="#" class="textbutton arrow-right">Read more</a>
				</div>
			</div>
			<div class="card">
				<div class="image"></div>
				<div class="content">
      <h3>
        
        Self-hosted data warehouse
        
      </h3>
    
					<p>DuckLake allows you to host your own local data warehouse.</p>
					<a href="#" class="textbutton arrow-right">Read more</a>
				</div>
			</div>
		</div>
	</div>
</section>
-->


<div>
      <h2>
        
        DuckLake’s key features
        
      </h2>
    
		<div>
			<div>
				<p><img src="https://ducklake.select/images/icons/waves.svg" alt="wave icon">
				</p>
				<div>
      <h3>
        
        Data lake operations
        
      </h3>
    
					<p>DuckLake supports snapshots, time travel queries, schema evolution and partitioning.</p>
				</div>
			</div>
			<div>
				<p><img src="https://ducklake.select/images/icons/documents.svg" alt="wave icon">
				</p>
				<div>
      <h3>
        
        Lightweight snapshots
        
      </h3>
    
					<p>You can have as many snapshots as you want without frequent compacting steps!</p>
				</div>
			</div>
			<div>
				<p><img src="https://ducklake.select/images/icons/pipette.svg" alt="wave icon">
				</p>
				<div>
      <h3>
        
        ACID transactions
        
      </h3>
    
					<p>DuckLake allows concurrent access with ACID transactional guarantees over multi-table operations.</p>
				</div>
			</div>
			<div>
				<p><img src="https://ducklake.select/images/icons/clock.svg" alt="wave icon">
				</p>
				<div>
      <h3>
        
        Performance-oriented
        
      </h3>
    
					<p>DuckLake uses statistics for filter pushdown, enabling fast queries even on large datasets.</p>
				</div>
			</div>
		</div>
	</div>

<div>
		<div>
      <h2>
        
        In Conversation: DuckDB Founders on DuckLake
        
      </h2>
    
			<p>Listen to Hannes Mühleisen and Mark Raasveldt walk through the history of data lakes and introduce DuckLake, a new lakehouse format.</p>
		</div>
		<div data-video-id="zeonmOO9jm4">
				<p><img src="https://ducklake.select/images/thumb_introducting-ducklake.png" alt="Thumbnail: Introducing DuckLake"></p>
			</div>
	</div>

<div id="quickinstall">
		<div>
      <h2>
        
        Create your first DuckLake with DuckDB
        
      </h2>
    
			<p>DuckDB provides first-class support for DuckLake through its highly portable extension, running wherever DuckDB does.</p>
			<!--<a href="/docs/installation/" class="button transparent">More installation options</a>-->
		</div>
		<div>
				<div>
					<ul>
						
						<li data-client="duckdb">DuckDB</li>
						<li data-client="sqlite">SQLite</li>
						<li data-client="postgresql">PostgreSQL</li>
						<li data-client="mysql">MySQL</li>
					</ul>
				</div>
				
				<div>
<figure><pre><code data-lang="sql"><span>INSTALL</span><span> ducklake</span><span>;</span>

<span>ATTACH</span> <span>'ducklake:metadata.ducklake'</span> <span>AS</span> <span>my_ducklake</span><span>;</span>
<span>USE</span> <span>my_ducklake</span><span>;</span></code></pre></figure>
				</div>
				
	</div></div>

<div id="quick-installation">

<div data-install="duckdb">
<figure><pre><code data-lang="sql"><span>INSTALL</span><span> ducklake</span><span>;</span>

<span>ATTACH</span> <span>'ducklake:metadata.ducklake'</span> <span>AS</span> <span>my_ducklake</span><span>;</span>
<span>USE</span> <span>my_ducklake</span><span>;</span></code></pre></figure>
</div>

<div data-install="postgresql">
<figure><pre><code data-lang="sql"><span>INSTALL</span><span> ducklake</span><span>;</span>
<span>INSTALL</span><span> postgres</span><span>;</span>

<span>-- Make sure that the database `ducklake_catalog` exists in PostgreSQL.</span>
<span>ATTACH</span> <span>'ducklake:postgres:dbname=ducklake_catalog host=your_postgres_host'</span> <span>AS</span> <span>my_ducklake</span><span>;</span>
<span>USE</span> <span>my_ducklake</span><span>;</span></code></pre></figure>
</div>

<div data-install="sqlite">
<figure><pre><code data-lang="sql"><span>INSTALL</span><span> ducklake</span><span>;</span>
<span>INSTALL</span><span> sqlite</span><span>;</span>

<span>ATTACH</span> <span>'ducklake:sqlite:metadata.sqlite'</span> <span>AS</span> <span>my_ducklake</span><span>;</span>
<span>USE</span> <span>my_ducklake</span><span>;</span></code></pre></figure>
</div>

<div data-install="mysql">
<figure><pre><code data-lang="sql"><span>INSTALL</span><span> ducklake</span><span>;</span>
<span>INSTALL</span><span> mysql</span><span>;</span>

<span>-- Make sure that the database `ducklake_catalog` exists in MySQL</span>
<span>ATTACH</span> <span>'ducklake:mysql:db=ducklake_catalog host=your_mysql_host'</span> <span>AS</span> <span>my_ducklake</span><span>;</span>
<span>USE</span> <span>my_ducklake</span><span>;</span></code></pre></figure>
</div>

</div>


<section>
	<div>
      <h2>
        
        Frequently asked questions
        
      </h2>
    
			<p>Answers to common questions to help you understand and make the most of DuckLake.</p>
		</div>
	
	<div>
		<div>
      <h3>
        
        
				Why should I use DuckLake?
			
        
      </h3>
    
			<div>
				<p>DuckLake provides a lightweight one-stop solution for if you need a data lake and catalog.

				</p><p>You can use DuckLake for a “multiplayer DuckDB” setup with multiple DuckDB instances reading and writing the same dataset –
				a concurrency model <a href="https://duckdb.org/docs/stable/connect/concurrency">not supported by vanilla DuckDB</a>.</p>

				<p>If you only use DuckDB for both your DuckLake entry point and your catalog database, you can still benefit from using DuckLake:
				you can run time travel queries,
				exploit data partitioning,
				and can store your data in multiple files instead of using a single (potentially very large) database file.</p>
			</div>
		</div>

		<div>
      <h3>
        
        
				What is DuckLake?
			
        
      </h3>
    
			<div><p>
				First of all, a catchy name for a DuckDB-originated technology for data lakes and lakehouses.
				More seriously, the term “DuckLake” can refer to three things:

				</p><ol>
					<li>the <i>specification</i> of the DuckLake lakehouse format,</li>
					<li>the <a href="https://duckdb.org/docs/stable/core_extensions/ducklake"><code>ducklake</code> <i>DuckDB extension</i></a>, which supports reading/writing datasets in the DuckLake specification,</li>
					<li>a DuckLake, a <i>dataset</i> stored using the DuckLake lakehouse format.</li>
				</ol>
			</div>
		</div>

		<div>
      <h3>
        
        
				What is the license of DuckLake?
			
        
      </h3>
    
			<p>
				The DuckLake specification and the DuckLake DuckDB extension are released under the MIT license.
			</p>
		</div>

	</div>
	
	
	
</section>


		




	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Art of Fugue – Contrapunctus I (2021) (122 pts)]]></title>
            <link>https://www.ethanhein.com/wp/2021/the-art-of-fugue-contrapunctus-i/</link>
            <guid>44106764</guid>
            <pubDate>Tue, 27 May 2025 13:25:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ethanhein.com/wp/2021/the-art-of-fugue-contrapunctus-i/">https://www.ethanhein.com/wp/2021/the-art-of-fugue-contrapunctus-i/</a>, See on <a href="https://news.ycombinator.com/item?id=44106764">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-22677">
		
	
	<div>
		
<p>JS Bach’s last set of works, collectively titled <em><a href="https://en.wikipedia.org/wiki/The_Art_of_Fugue">The Art of Fugue</a></em>, was published shortly after his death. It was not a big hit. Dense counterpoint was deeply unfashionable at that time, as Western European aristocratic tastes shifted toward singable melodies over block chords. The first published edition of <em>The Art of Fugue</em> only sold about thirty copies, and it wasn’t performed in its entirety until 1922.</p>
<p>Eventually the classical music audience did come to admire Bach’s final fugue collection, but it took <a href="https://en.wikipedia.org/wiki/Johann_Sebastian_Bach#19th_century">almost 100 years after it was written</a>. The fugues still aren’t the easiest listening experience. They were meant to be didactic, to be played and studied rather than to be listened to–though of course you are free to listen to and enjoy them. I’m finding that my own enjoyment is much enhanced by opening up the structure through visualization, so that’s what I’ve done with <a href="https://www.amazon.com/Bach-J-S-Fugue-Angela-Hewitt/dp/B00MX51FHW">Angela Hewitt’s recording of Contrapunctus I</a> using Ableton Live.</p>
<p><iframe title="Bach - The Art of Fugue Contrapunctus I - Ableton Live visualization" width="640" height="480" src="https://www.youtube.com/embed/-yRqKp2rqPk?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>The main thing to listen (and watch) for here is <a href="https://en.wikipedia.org/wiki/The_Art_of_Fugue#Structure">the subject</a>, the little melody that each voice plays as it enters. After the subject, the voices wander off to play other intertwining parts, occasionally returning to the subject as they go. In the subsequent <em>Art of Fugue</em> pieces, Bach does all kinds of twisting and warping of the subject, writing it <a href="https://en.wikipedia.org/wiki/Inversion_(music)#Melodies">upside down</a>, <a href="https://en.wikipedia.org/wiki/Retrograde_(music)">backwards</a>, <a href="https://en.wikipedia.org/wiki/Diminution#Diminution_in_composition">twice as fast</a>, <a href="https://en.wikipedia.org/wiki/Augmentation_(music)">half as fast</a>, <a href="https://en.wikipedia.org/wiki/Stretto">overlaid on top of itself</a>, and so on. In Contrapunctus I, however, he doesn’t do any of these formal games. It sounds more like he’s just riffing around the subject. It’s almost casual, at least by his standards.</p>
<p><span id="more-22677"></span>Here’s Glenn Gould playing Contrapunctus I on organ.</p>
<p><iframe title="Glenn Gould plays Bach &quot;The Art Of Fugue BWV 1080&quot; Organ/Piano" width="640" height="360" src="https://www.youtube.com/embed/GnXHnEz94os?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>And here he is playing it live on piano toward the end of his life, with a lovely slow tempo.</p>
<p><iframe title="Glenn Gould-J.S. Bach-The Art of Fugue (HD)" width="640" height="480" src="https://www.youtube.com/embed/4uX-5HOx2Wc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>Bach published <em>The Art of Fugue</em> in “open score,” meaning that each voice of the counterpoint is on its own line, rather than being grouped together in the usual two-staff notation that we’re used to. Here’s an excerpt of <a href="https://www.youtube.com/watch?v=zQXPoJjfz0I">Contrapunctus VII</a> in open score in Bach’s own handwriting, with some informational color-coding added by Guido Magnano:</p>
<p><a href="https://commons.wikimedia.org/wiki/File:ContrapunctusVII.jpg"><img data-recalc-dims="1" loading="lazy" decoding="async" data-attachment-id="22882" data-permalink="https://www.ethanhein.com/wp/2021/the-art-of-fugue-contrapunctus-i/contrapunctusvii/" data-orig-file="https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?fit=1719%2C1720&amp;ssl=1" data-orig-size="1719,1720" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Bach – Art of Fugue – Contrapunctus VII – color-coded open score" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?fit=300%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?fit=640%2C640&amp;ssl=1" src="https://i0.wp.com/ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII-1024x1024.jpeg?resize=640%2C640&amp;ssl=1" alt="" width="640" height="640" srcset="https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?resize=1024%2C1024&amp;ssl=1 1024w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?resize=1536%2C1536&amp;ssl=1 1536w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?w=1719&amp;ssl=1 1719w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?w=1280&amp;ssl=1 1280w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></p>
<p>Open score was already considered an old-fashioned way to write keyboard music in Bach’s time, and people stopped using it entirely soon afterward. Since the 19th century Bach revival, musicians have taken the open score format as an invitation to play <em>The Art of Fugue</em> on four separate instruments. For example, there have been lots of string quartet recordings. Here’s a good one:</p>
<p><iframe loading="lazy" title="J.S. Bach: The Art Of Fugue, BWV 1080 - Version For String Quartet - Contrapunctus 1" width="640" height="480" src="https://www.youtube.com/embed/3A8iR7cGHHQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>Contrapunctus I also sounds cool on four viols (cousins of the viola and cello, but with frets like a guitar):</p>
<p><iframe loading="lazy" title="Bach-The Art of Fugue- Contrapunctus 1" width="640" height="480" src="https://www.youtube.com/embed/gU8Vu5YEo48?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>It sounds amazing on four saxophones:</p>
<p><iframe loading="lazy" title="J.S. Bach Contrapunctus 1 - Rascher Saxophone Quartet" width="640" height="360" src="https://www.youtube.com/embed/wEJUOUaGlBY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>And it has a nice organ-like quality on four recorders:</p>
<p><iframe loading="lazy" title="Woodpeckers Recorder Quartet - JS Bach - die Kunst der Fuge BWV 1080 - Contrapunctus I &amp; IX" width="640" height="360" src="https://www.youtube.com/embed/aLEL9WcbGLU?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p><a href="https://en.wikipedia.org/wiki/Joseph_Kerman">Joseph Kerman</a> writes about Contrapunctus I in his book <em><a href="https://www.ucpress.edu/book/9780520287631/the-art-of-fugue">Art of Fugue</a></em>:</p>
<blockquote>
<p>[I]n order to set off the technical virtuosity that was the work’s raison d’être, Bach had the extraordinary idea of making its first number a fugue without contrapuntal devices. Contrapunctus 1 has neither strettos, diminutions, and so on, nor even <a href="https://en.wikipedia.org/wiki/Subject_(music)#Countersubject">countersubjects</a> or recurring <a href="https://en.wikipedia.org/wiki/Fugue#Episode">episodes</a>. These devices will be introduced only in the succeeding contrapuncti, one by one. In Contrapunctus 1 <a href="https://www.teoria.com/en/reference/i/invertible-counterpoint.php">invertible counterpoint</a> itself is in very short supply. This elemental fugue never modulates beyond the obligatory dominant and subdominant keys.</p>
<p>In any case, this most basic of fugues is necessarily also one of Bach’s freest and must also be one of his smoothest… The contrapuntal lines, consisting mostly of quarter- and eighth-note patterns, move stepwise or by the smallest leaps, and the expectations of eighteenth-century harmony often go unfulfilled. Strong cadences are shunned. While such generalities only begin to explain the almost mesmeric fluency of Bach’s late style, they may help sensitize us to contrasts where it is abrogated, such as at those episodes featuring larger leaps [bars 29–30, 36–40, 49–53], and at the one really, decisively strong cadence [bar 74].</p>
<p>Eventually the surface does begin to ruffle, when in a new exposition the bass steps in on the heels of its predecessor and enters after three bars rather than four [bar 32]. This entry—it can be heard as a second stab at stretto, after a previous, premature effort in bars 29–30, what is sometimes called a false stretto—moves rather hastily from the dominant around to the subdominant, twisting and turning the subject oddly. Then the tenor entry, as though checked by the low As in the bass, hesitates, accumulating dissonances—sevenths, ninths, and pungent augmented intervals [bars 41, 42, 43].</p>
</blockquote>
<p>These intervals are a lot less “pungent” in <a href="https://www.ethanhein.com/wp/2019/why-cant-you-tune-your-guitar/">12-tone equal temperament</a> than they would have been in <a href="https://www.ethanhein.com/wp/2020/what-does-the-well-tempered-clavier-sound-like-in-actual-well-temperament/">the uneven temperament of Bach’s era</a>.</p>
<blockquote>
<p>The soprano in this group of entries emerges as a sort of ethereal climax, led into by another false stretto. The bass drops out, allowing for heightened activity in the remaining voices, like a beating of wings [bars 48–54].</p>
</blockquote>
<blockquote>
<p>Past the exposition, then, the piece can be seen to grow increasingly complex, though the feeling seems to me not exactly of complexity but of complexities tested out and drifted past, ideas considered and shelved, in a constantly changing improvisational field of a unique kind. Endlessly fertile and quite unstoppable, Bach proceeds spontaneously, almost distractedly, until the piece pulls itself together with one grand gesture, the long dominant pedal in the bass from bar 63 to bar 73.</p>
<p>Literally, of course, the pitch A drops out at bar 66, but in the ear it lasts all the way, so the passage has the effect of a cadenza, an increasingly rhapsodic epilogue during which pitch rises and tension mounts until it is too much to bear—or so we must infer; the buildup is so smooth we had no inkling of impending crisis. This programmatically seamless music literally breaks off, stammers, and finally sinks—truly sinks—to rest.</p>
</blockquote>
<p>Bach doesn’t sound much like jazz, but Kerman identifies qualities in Contrapunctus I that are the things I like about jazz: the not-so-rigid development of themes and interplay of voices, the “complexities tested out and drifted past.” The later fugues are full of complexities that are tested all the way out and then some. There are even a couple of palindrome-like <a href="https://en.wikipedia.org/wiki/Mirror_fugue" target="_blank" rel="noopener">mirror fugues</a>. These are fascinating in that <em><a href="https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach" target="_blank" rel="noopener">Gödel, Escher, Bach</a></em> way, but also exhausting. Sometimes you want to just listen to music without doing a whole Rubik’s cube worth of combinatorial math.</p>
<p>My attention span for this music improves when I hear it quantized over a beat. Here’s Angela Hewitt’s recording over the beat from “<a href="https://www.youtube.com/watch?v=QsZlY0Vz4-o">Empire State of Mind</a>” by Jay-Z and Alicia Keys, inspired by an arrangement by <a href="https://www.notesbyheather.com/">Heather Fortune</a>.</p>

<p>In spite of the jokey title, this remix is not meant to be ironic. (Well, not totally ironic.) The beat helps me stay focused and present, rather than having my mind drift into a, you know, fugue state. That’s what beats are for. This music is supposed to be didactic, right? I learn best when I’m learning to a groove. But I also just like the aesthetic effect, and the suggestion that anything has groove potential.</p>

	</div><!-- .entry-content -->

	<!-- .entry-footer -->

				
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How a hawk learned to use traffic signals to hunt more successfully (398 pts)]]></title>
            <link>https://www.frontiersin.org/news/2025/05/23/street-smarts-hawk-use-traffic-signals-hunting</link>
            <guid>44105965</guid>
            <pubDate>Tue, 27 May 2025 11:46:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.frontiersin.org/news/2025/05/23/street-smarts-hawk-use-traffic-signals-hunting">https://www.frontiersin.org/news/2025/05/23/street-smarts-hawk-use-traffic-signals-hunting</a>, See on <a href="https://news.ycombinator.com/item?id=44105965">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="tts-content"><!--[--><!----><!--]--><figure><picture><!--[--><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/2BngtKIXLF0Ua93HkvlZo0/100e0cb34aa7a8d3085eb572d82ea38b/Screenshot_2025-05-08_092315.png?&amp;w=380&amp;fm=webp&amp;q=80" media="(max-width: 767px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/2BngtKIXLF0Ua93HkvlZo0/100e0cb34aa7a8d3085eb572d82ea38b/Screenshot_2025-05-08_092315.png?&amp;w=642&amp;fm=webp&amp;q=80" media="(min-width: 768px and max-width: 1023px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/2BngtKIXLF0Ua93HkvlZo0/100e0cb34aa7a8d3085eb572d82ea38b/Screenshot_2025-05-08_092315.png?&amp;w=824&amp;fm=webp&amp;q=80" media="(min-width: 1024px and max-width: 1279px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/2BngtKIXLF0Ua93HkvlZo0/100e0cb34aa7a8d3085eb572d82ea38b/Screenshot_2025-05-08_092315.png?&amp;w=644&amp;fm=webp&amp;q=80" media="(min-width: 1280px and max-width: 1439px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/2BngtKIXLF0Ua93HkvlZo0/100e0cb34aa7a8d3085eb572d82ea38b/Screenshot_2025-05-08_092315.png?&amp;w=672&amp;fm=webp&amp;q=80" media="(min-width: 1440px and max-width: 1919px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/2BngtKIXLF0Ua93HkvlZo0/100e0cb34aa7a8d3085eb572d82ea38b/Screenshot_2025-05-08_092315.png?&amp;w=912&amp;fm=webp&amp;q=80" media="(min-width: 1920px)"><!--]--><img src="https://images.ctfassets.net/mrbo2ykgx5lt/2BngtKIXLF0Ua93HkvlZo0/100e0cb34aa7a8d3085eb572d82ea38b/Screenshot_2025-05-08_092315.png?&amp;w=912&amp;fm=webp&amp;q=80" alt="" loading="eager"></picture><figcaption>Adult Cooper’s hawk dispatching a house sparrow. Image: Vladimir Dinets. </figcaption></figure><!--[--><p><strong>Dr Vladimir Dinets, a research assistant professor at the University of Tennessee, is a zoologist who studies animal behavior, ecology, and conservation. As of 2025, he also teaches mathematics at Rudgers University. He is the author of a recently published </strong><a href="https://www.frontiersin.org/journals/ethology/articles/10.3389/fetho.2025.1539103/abstract"><strong><em>Frontiers in Ethology</em></strong><strong> article</strong></a><strong> that documents the impressive adaptation of an avian newcomer to the city. A Cooper’s hawk, a medium-sized raptor native to North America, appears to have learned how to adapt its hunting strategy and strike at a flock of birds precisely when cars at an intersection lined up after traffic lights switched to red, having been alerted by a sound signal that the red phase would last longer than usual. In the following guest editorial, he describes his observations. </strong></p><p>by <a href="https://loop.frontiersin.org/people/2127032/overview">Dr Vladimir Dinets</a></p><p>Many years ago, I got to spend some time in Ngorongoro Crater, a unique place in Africa where immense herds of animals are being watched by equally immense crowds of 4x4-riding tourists, and traffic jams of all kinds are frequent. On my last evening there, a local guide told me at a campfire that some buffalo in the crater had figured out the meaning of car turn signals and used that understanding to get out of the way of turning Jeeps and Land Rovers.</p><p>I never had a chance to return to the crater and still don’t know if that story was true, but it got me interested in animals’ perception of – and interactions with – human-made vehicles. Of course, the most common interaction is the animal becoming a roadkill, but it’s not the whole story. Many animals have learned to use cars for their own benefit, and birds seem to be particularly good at it. Crows drop walnuts, clams, even small vertebrates onto busy roads to have them killed and/or crushed by cars. Carrion-eating birds routinely monitor or patrol busy roads to immediately snatch roadkill. For example, many American highways are partitioned by families of ravens who watch them from dawn till dusk, waiting for meals from under wheels. Songbirds glean dead insects from cars and even nest in moving cars, trains and boats. Small birds use moving cars as mobile shelters from pursuing hawks, while hawks in one Ukrainian city have long been known to use moving cars and streetcars as cover to sneak up on their prey.</p><h2>Hunt at the crosswalk</h2><p>So I’ve been keeping an eye for unusual bird-car play, and that’s why I noticed something interesting going on at a street intersection near my home. The intersection wasn’t particularly busy, and even during morning rush hour, when I was driving my daughter to school, there were usually only a few cars waiting for the green light. But sometimes a pedestrian pressed a button, and that caused the red light to last a lot longer, so the car queue became longer, too, stretching all the way to a small streetside tree with a particularly dense crown. When that happened, the streetlight produced a sound signal, letting blind people know that it was safe to cross.</p><figure><picture><!--[--><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/7EjYAFrSOJtfrqpHsTozaR/89d47415f405e29584ff549eaef532c1/1539103_Figure_1.JPEG?&amp;w=380&amp;fm=webp&amp;q=80" media="(max-width: 767px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/7EjYAFrSOJtfrqpHsTozaR/89d47415f405e29584ff549eaef532c1/1539103_Figure_1.JPEG?&amp;w=642&amp;fm=webp&amp;q=80" media="(min-width: 768px and max-width: 1023px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/7EjYAFrSOJtfrqpHsTozaR/89d47415f405e29584ff549eaef532c1/1539103_Figure_1.JPEG?&amp;w=824&amp;fm=webp&amp;q=80" media="(min-width: 1024px and max-width: 1279px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/7EjYAFrSOJtfrqpHsTozaR/89d47415f405e29584ff549eaef532c1/1539103_Figure_1.JPEG?&amp;w=644&amp;fm=webp&amp;q=80" media="(min-width: 1280px and max-width: 1439px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/7EjYAFrSOJtfrqpHsTozaR/89d47415f405e29584ff549eaef532c1/1539103_Figure_1.JPEG?&amp;w=672&amp;fm=webp&amp;q=80" media="(min-width: 1440px and max-width: 1919px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/7EjYAFrSOJtfrqpHsTozaR/89d47415f405e29584ff549eaef532c1/1539103_Figure_1.JPEG?&amp;w=912&amp;fm=webp&amp;q=80" media="(min-width: 1920px)"><!--]--><img src="https://images.ctfassets.net/mrbo2ykgx5lt/7EjYAFrSOJtfrqpHsTozaR/89d47415f405e29584ff549eaef532c1/1539103_Figure_1.JPEG?&amp;w=912&amp;fm=webp&amp;q=80" alt="" loading="lazy"></picture><figcaption>The study area. The route used by the hawk to attack a flock of birds feeding in front of house #2 is shown with white arrows. The hawk appeared in the tree in front of house #11 as soon as sound signals at the streetlight at the intersection (marked with white asterisks) indicated that red light will be longer than usual, and attacked when the queue of cars reached house #8, making it possible for the hawk to move to the tree in front of house #1 without being visible to potential prey. Credit: Dinets, 2025. </figcaption></figure><p>One winter morning I was in my car waiting for the light to change and suddenly saw a Cooper’s hawk: it emerged from that small tree, flew very low above the sidewalk along the line of cars, made a sharp turn, crossed the street between the cars, and dove onto something near one of the houses.</p><p>A few days later I saw the same thing happen again and decided to investigate. It turned out that the house targeted by the hawk’s attacks was inhabited by a nice big family that liked to eat dinner in the front yard. Next morning their breadcrumbs and other leftovers attracted a small flock of birds – sparrows, doves, and sometimes starlings. That’s what the hawk was after.</p><p>But what was really interesting, and took me much longer to figure out, was that the hawk always attacked when the car queue was long enough to provide cover all the way to the small tree, and that only happened after someone had pressed the pedestrian crossing button. As soon as the sound signal was activated, the raptor would fly from somewhere into the small tree, wait for the cars to line up, and then strike.</p><hr><p><a href="https://www.frontiersin.org/journals/ethology/articles/10.3389/fetho.2025.1539103/full">Download and read original article</a></p><hr><h2>Survival of the smartest?</h2><p>That meant that the hawk understood the connection between the sound and the eventual car queue length. The bird also had to have a good mental map of the place, because when the car queue reached its tree, the raptor could no longer see the place where its prey was and had to get there by memory.</p><p>It was an immature bird. Cooper’s hawks rarely nest in cities in our area but are common winter visitors. So the bird I was watching was almost certainly a migrant, having moved to the city just a few weeks earlier. And it had already figured out how to use traffic signals and patterns. To me it seemed very impressive.</p><figure><picture><!--[--><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/1jzyGvKDMUSCAXO50XsrCt/e48186aea92b2120b4ff6fbb7952dfbe/Fig1.jpg?&amp;w=380&amp;fm=webp&amp;q=80" media="(max-width: 767px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/1jzyGvKDMUSCAXO50XsrCt/e48186aea92b2120b4ff6fbb7952dfbe/Fig1.jpg?&amp;w=642&amp;fm=webp&amp;q=80" media="(min-width: 768px and max-width: 1023px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/1jzyGvKDMUSCAXO50XsrCt/e48186aea92b2120b4ff6fbb7952dfbe/Fig1.jpg?&amp;w=824&amp;fm=webp&amp;q=80" media="(min-width: 1024px and max-width: 1279px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/1jzyGvKDMUSCAXO50XsrCt/e48186aea92b2120b4ff6fbb7952dfbe/Fig1.jpg?&amp;w=644&amp;fm=webp&amp;q=80" media="(min-width: 1280px and max-width: 1439px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/1jzyGvKDMUSCAXO50XsrCt/e48186aea92b2120b4ff6fbb7952dfbe/Fig1.jpg?&amp;w=672&amp;fm=webp&amp;q=80" media="(min-width: 1440px and max-width: 1919px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/1jzyGvKDMUSCAXO50XsrCt/e48186aea92b2120b4ff6fbb7952dfbe/Fig1.jpg?&amp;w=912&amp;fm=webp&amp;q=80" media="(min-width: 1920px)"><!--]--><img src="https://images.ctfassets.net/mrbo2ykgx5lt/1jzyGvKDMUSCAXO50XsrCt/e48186aea92b2120b4ff6fbb7952dfbe/Fig1.jpg?&amp;w=912&amp;fm=webp&amp;q=80" alt="" loading="lazy"></picture><figcaption>Immature Cooper’s hawk in an ambush. Image: Vladimir Dinets.</figcaption></figure><p>Next winter I saw a hawk in adult plumage hunt in exactly the same way, and I’m pretty sure it was the same bird. The following summer, the sound signal at the streetlight stopped working, and the residents of the house moved out, so there were no more bird flocks. I haven’t seen any Cooper’s hawks around here ever since.</p><p>Cooper’s hawk is on a rather short list of bird of prey species that have successfully adapted to life in cities. A city is a difficult and very dangerous habitat for any bird, but particularly for a large raptor specializing in live prey: you have to avoid windows, cars, utility wires, and countless other dangers while catching something to eat every day. I think my observations show that Cooper’s hawks manage to survive and thrive there, at least in part, by being very smart.</p><p><strong>REPUBLISHING GUIDELINES</strong>: Open access and sharing research is part of <a href="https://www.frontiersin.org/about/about-frontiers">Frontiers’ mission</a>. Unless otherwise noted, you can republish articles posted in the Frontiers news site — as long as you include a link back to the original research. Selling the articles is not allowed. </p><!--]--><!----></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Just make it scale: An Aurora DSQL story (124 pts)]]></title>
            <link>https://www.allthingsdistributed.com/2025/05/just-make-it-scale-an-aurora-dsql-story.html</link>
            <guid>44105878</guid>
            <pubDate>Tue, 27 May 2025 11:31:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.allthingsdistributed.com/2025/05/just-make-it-scale-an-aurora-dsql-story.html">https://www.allthingsdistributed.com/2025/05/just-make-it-scale-an-aurora-dsql-story.html</a>, See on <a href="https://news.ycombinator.com/item?id=44105878">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header></header><hr><section><p><time itemprop="datePublished" datetime="2025-05-27">May 27, 2025</time> • 3404 words</p><span itemprop="articleBody"><p><img src="https://www.allthingsdistributed.com/images/aurora-dsql-header.png" alt="Aurora DSQL Team" loading="lazy"></p><p>At re:Invent we announced Aurora DSQL, and since then I’ve had many conversations with builders about what this means for database engineering. What’s particularly interesting isn’t just the technology itself, but the journey that got us here. I’ve been wanting to dive deeper into this story, to share not just the what, but the how and why behind DSQL’s development. Then, a few weeks ago, at our internal developer conference — DevCon — I watched a talk from two of our senior principal engineers (PEs) on building DSQL (a project that started 100% in JVM and finished 100% Rust). After the presentation, I asked <a href="https://www.linkedin.com/in/nicholas-matsakis-615614/">Niko Matsakis</a> and <a href="https://www.linkedin.com/in/marc-bowes-952b5518/">Marc Bowes</a> if they’d be willing to work with me to turn their insights into a deeper exploration of DSQL’s development. They not only agreed, but offered to help explain some of the more technically complex parts of the story.</p><p>In the blog that follows, Niko and Marc provide deep technical insights on Rust and how we’ve used it to build DSQL. It’s an interesting story on the pursuit of engineering efficiency and why it’s so important to question past decisions – even if they’ve worked very well in the past.</p><div><p><strong>Note from the author</strong></p><div><p>Before we get into it, a quick but important note. This was (and continues to be) an ambitious project that requires a tremendous amount of expertise in everything from storage to control plane engineering. Throughout this write-up we've incorporated the learnings and wisdom of many of the Principal and Sr. Principal Engineers that brought DSQL to life. I hope you enjoy reading this as much as I have.</p><p>Special thanks to: Marc Brooker, Marc Bowes, Niko Matsakis, James Morle, Mike Hershey, Zak van der Merwe, Gourav Roy, Matthys Strydom.</p></div></div><h2 id="a-brief-timeline-of-purpose-built-databases-at-aws">A brief timeline of purpose-built databases at AWS <a href="#a-brief-timeline-of-purpose-built-databases-at-aws"></a></h2><p>Since the early days of AWS, the needs of our customers have grown more varied — and in many cases, more urgent. What started with a push to make traditional relational databases easier to manage with the launch of Amazon RDS in 2009 quickly expanded into a portfolio of purpose-built options: DynamoDB for internet-scale NoSQL workloads, Redshift for fast analytical queries over massive datasets, Aurora for those looking to escape the cost and complexity of legacy commercial engines without sacrificing performance. These weren’t just incremental steps—they were answers to real constraints our customers were hitting in production. And time after time, what unlocked the right solution wasn’t a flash of genius, but listening closely and building iteratively, often with the customer in the loop.</p><p>Of course, speed and scale aren’t the only forces at play. In-memory caching with ElastiCache emerged from developers needing to squeeze more from their relational databases. Neptune came later, as graph-based workloads and relationship-heavy applications pushed the limits of traditional database approaches. What’s remarkable looking back isn’t just how the portfolio grew, but how it grew in tandem with new computing patterns—serverless, edge, real-time analytics. Behind each launch was a team willing to experiment, challenge prior assumptions, and work in close collaboration with product teams across Amazon. That’s the part that’s harder to see from the outside: innovation almost never happens overnight. It almost always comes from taking incremental steps forward. Building on successes and learning from (but not fearing) failures.</p><p>While each database service we’ve launched has solved critical problems for our customers, we kept encountering a persistent challenge: how do you build a relational database that requires no infrastructure management and which scales automatically with load? One that combines the familiarity and power of SQL with genuine serverless scalability, seamless multi-region deployment, and zero operational overhead? Our previous attempts had each moved us closer to this goal. Aurora brought cloud-optimized storage and simplified operations, Aurora Serverless automated vertical scaling, but we knew we needed to go further. This wasn’t just about adding features or improving performance - it was about fundamentally rethinking what a cloud database could be.</p><p>Which brings us to Aurora DSQL.</p><h2 id="aurora-dsql">Aurora DSQL <a href="#aurora-dsql"></a></h2><p>The goal with Aurora DSQL’s design is to break up the database into bite-sized chunks with clear interfaces and explicit contracts. Each component follows the Unix mantra—do one thing, and do it well—but working together they are able to offer all the features users expect from a database (transactions, durability, queries, isolation, consistency, recovery, concurrency, performance, logging, and so on).</p><p>At a high-level, this is DSQL’s architecture.</p><img src="https://www.allthingsdistributed.com/images/aurora-dsql-architecture.png" alt="Aurora DSQL Architecture Diagram" width="80%"><p>We had already worked out how to handle reads in 2021—what we didn’t have was a good way to scale writes horizontally. The conventional solution for scaling out writes to a database is <a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol">two-phase commit (2PC)</a>. Each journal would be responsible for a subset of the rows, just like storage. This all works great so long as transactions are only modifying nearby rows. But it gets really complicated when your transaction has to update rows across multiple journals. You end up in a complex dance of checks and locks, followed by an atomic commit. Sure, the happy path works fine in theory, but reality is messier. You have to account for timeouts, maintain liveness, handle rollbacks, and figure out what happens when your coordinator fails — the operational complexity compounds quickly. For DSQL, we felt we needed a new approach – a way to maintain availability and latency even under duress.</p><h2 id="scaling-the-journal-layer">Scaling the Journal layer <a href="#scaling-the-journal-layer"></a></h2><p>Instead of pre-assigning rows to specific journals, we made the architectural decision to write the entire commit into a single journal, no matter how many rows it modifies. This solved both the atomic and durable requirements of <a href="https://en.wikipedia.org/wiki/ACID">ACID</a>. The good news? This made scaling the write path straightforward. The challenge? It made the read path significantly more complex. If you want to know the latest value for a particular row, you now have to check all the journals, because any one of them might have a modification. Storage therefore needed to maintain connections to every journal because updates could come from anywhere. As we added more journals to increase transactions per second, we would inevitably hit network bandwidth limitations.</p><p>The solution was the Crossbar, which separates the scaling of the read path and write path. It offers a subscription API to storage, allowing storage nodes to subscribe to keys in a specific range. When transactions come through, the Crossbar routes the updates to the subscribed nodes. Conceptually, it’s quite simple, but challenging to implement efficiently. Each journal is ordered by transaction time, and the Crossbar has to follow each journal to create the total order.</p><p><img src="https://www.allthingsdistributed.com/images/aurora-dsql-crossbar.png" alt="Aurora DSQL Crossbar Diagram" loading="lazy"></p><p>Adding to the complexity, each layer has to provide a high degree of fan out (we want to be efficient with our hardware), but in the real world, subscribers can fall behind for any number of reasons, so you end up with a bunch of buffering requirements. These problems made us worried about garbage collection, especially GC pauses.</p><p>The reality of distributed systems hit us hard here - when you need to read from every journal to provide total ordering, the probability of any host encountering tail latency events approaches 1 surprisingly quickly – something <a href="https://brooker.co.za/blog/2021/04/19/latency.html">Marc Brooker has spent some time writing about</a>.</p><p>To validate our concerns, we ran simulation testing of the system – specifically modeling how our crossbar architecture would perform when scaling up the number of hosts, while accounting for occasional 1-second stalls. The results were sobering: with 40 hosts, instead of achieving the expected million TPS in the crossbar simulation, we were only hitting about 6,000 TPS. Even worse, our tail latency had exploded from an acceptable 1 second to a catastrophic 10 seconds. This wasn’t just an edge case - it was fundamental to our architecture. Every transaction had to read from multiple hosts, which meant that as we scaled up, the likelihood of encountering at least one GC pause during a transaction approached 100%. In other words, at scale, nearly every transaction would be affected by the worst-case latency of any single host in the system.</p><h2 id="short-term-pain-long-term-gain">Short term pain, long term gain <a href="#short-term-pain-long-term-gain"></a></h2><p>We found ourselves at a crossroads. The concerns about garbage collection, throughput, and stalls weren’t theoretical – they were very real problems we needed to solve. We had options: we could dive deep into JVM optimization and try to minimize garbage creation (a path many of our engineers knew well), we could consider C or C++ (and lose out on memory safety), or we could explore Rust. We chose Rust. The language offered us predictable performance without garbage collection overhead, memory safety without sacrificing control, and zero-cost abstractions that let us write high-level code that compiled down to efficient machine instructions.</p><p>The decision to switch programming languages isn’t something to take lightly. It’s often a <a href="https://www.youtube.com/watch?v=rxsdOQa_QkM">one-way door</a> — once you’ve got a significant codebase, it’s extremely difficult to change course. These decisions can make or break a project. Not only does it impact your immediate team, but it influences how teams collaborate, share best practices, and move between projects.</p><p>Rather than tackle the complex Crossbar implementation, we chose to start with the Adjudicator – a relatively simple component that sits in front of the journal and ensures only one transaction wins when there are conflicts. This was our team’s first foray into Rust, and we picked the Adjudicator for a few reasons: it was less complex than the Crossbar, we already had a Rust client for the journal, and we had an existing JVM (Kotlin) implementation to compare against. This is the kind of pragmatic choice that has served us well for over two decades – start small, learn fast, and adjust course based on data.</p><p>We assigned two engineers to the project. They had never written C, C++, or Rust before. And yes, there were plenty of battles with the compiler. The Rust community has a saying, “<a href="https://nostarch.com/blog/software-engineer-jon-gjengset-gets-nitty-gritty-rust">with Rust you have the hangover first</a>.” We certainly felt that pain. We got used to the compiler telling us “no” a lot.</p><figure><img src="https://www.allthingsdistributed.com/images/aurora-dsql-compiler-no.jpeg" alt="Compiler says “No” image" loading="lazy"><figcaption>(Image by Lee Baillie)</figcaption></figure><p>But after a few weeks, it compiled and the results surprised us. The code was 10x faster than our carefully tuned Kotlin implementation – despite no attempt to make it faster. To put this in perspective, we had spent years incrementally improving the Kotlin version from 2,000 to 3,000 transactions per second (TPS). The Rust version, written by Java developers who were new to the language, clocked 30,000 TPS.</p><p>This was one of those moments that fundamentally shifts your thinking. Suddenly, the couple of weeks spent learning Rust no longer looked like a big deal, when compared with how long it’d have taken us to get the same results on the JVM. We stopped asking, “Should we be using Rust?” and started asking “Where else could Rust help us solve our problems?”</p><p>Our conclusion was to rewrite our data plane entirely in Rust. We decided to keep the control plane in Kotlin. This seemed like the best of both worlds: high-level logic in a high-level, garbage collected language, do the latency sensitive parts in Rust. This logic didn’t turn out to be quite right, but we’ll get to that later in the story.</p><h2 id="its-easier-to-fix-one-hard-problem-then-never-write-a-memory-safety-bug">It’s easier to fix one hard problem then never write a memory safety bug <a href="#its-easier-to-fix-one-hard-problem-then-never-write-a-memory-safety-bug"></a></h2><p>Making the decision to use Rust for the data plane was just the beginning. We had decided, after quite a bit of internal discussion, to build on PostgreSQL (which we’ll just call Postgres from here on). The modularity and extensibility of Postgres allowed us to use it for query processing (i.e., the parser and planner), while replacing replication, concurrency control, durability, storage, the way transaction sessions are managed.</p><p>But now we had to figure out how to go about making changes to a project that started in 1986, with over a million lines of C code, thousands of contributors, and continuous active development. The easy path would have been to hard fork it, but that would have meant missing out on new features and performance improvements. We’d seen this movie before - forks that start with the best intentions but slowly drift into maintenance nightmares.</p><p>Extension points seemed like the obvious answer. Postgres was designed from the beginning to be an extensible database system. These extension points are part of Postgres’ public API, allowing you to modify behavior without changing core code. Our extension code could run in the same process as Postgres but live in separate files and packages, making it much easier to maintain as Postgres evolved. Rather than creating a hard fork that would drift further from upstream with each change, we could build on top of Postgres while still benefiting from its ongoing development and improvements.</p><p>The question was, do we write these extensions in C or Rust? Initially, the team felt C was a better choice. We already had to read and understand C to work with Postgres, and it would offer a lower impedance mismatch. As the work progressed though, we realized a critical flaw in this thinking. The Postgres C code is reliable: it’s been thoroughly battled tested over the years. But our extensions were freshly written, and every new line of C code was a chance to add some kind of memory safety bug, like a use-after-free or buffer overrun. The “a-ha!” moment came during a code review when we found several memory safety issues in a seemingly simple data structure implementation. With Rust, we could have just grabbed a proven, memory-safe implementation from Crates.io.</p><p>Interestingly, the <a href="https://security.googleblog.com/2024/09/eliminating-memory-safety-vulnerabilities-Android.html">Android team published research last September</a> that confirmed our thinking. Their data showed that the vast majority of new bugs come from new code. This reinforced our belief that to prevent memory safety issues, we needed to stop introducing memory-unsafe code altogether.</p><figure><img src="https://www.allthingsdistributed.com/images/aurora-dsql-google-mem-safe-vulns.png" alt="New Memory Unsafe Code and Memory safety Vulns" loading="lazy"><figcaption>(Research from the Android team shows that most new bugs come from new code. So if you pick a memory safe language – you prevent memory safety bugs.)</figcaption></figure><p>We decided to pivot and write the extensions in Rust. Given that the Rust code is interacting closely with Postgres APIs, it may seem like using Rust wouldn’t offer much of a memory safety advantage, but that turned out not to be true. The team was able to create abstractions that enforce safe patterns of memory access. For example, in C code it’s common to have two fields that need to be used together safely, like a <code>char*</code> and a <code>len</code> field. You end up relying on conventions or comments to explain the relationship between these fields and warn programmers not to access the string beyond len. In Rust, this is wrapped up behind a single String type that encapsulates the safety. We found many examples in the Postgres codebase where header files had to explain how to use a struct safely. With our Rust abstractions, we could encode those rules into the type system, making it impossible to break the invariants. Writing these abstractions had to be done very carefully, but the rest of the code could use them to avoid errors.</p><p>It’s a reminder that decisions about scalability, security, and resilience should be prioritized – even when they’re difficult. The investment in learning a new language is minuscule compared to the long-term cost of addressing memory safety vulnerabilities.</p><h2 id="about-the-control-plane">About the control plane <a href="#about-the-control-plane"></a></h2><p>Writing the control plane in Kotlin seemed like the obvious choice when we started. After all, services like Amazon’s Aurora and RDS had proven that JVM languages were a solid choice for control planes. The benefits we saw with Rust in the data plane – throughput, latency, memory safety – weren’t as critical here. We also needed internal libraries that weren’t yet available in Rust, and we had engineers that were already productive in Kotlin. It was a practical decision based on what we knew at the time. It also turned out to be the wrong one.</p><p>At first, things went well. We had both the data and control planes working as expected in isolation. However, once we started integrating them together, we started hitting problems. DSQL’s control plane does a lot more than CRUD operations, it’s the brain behind our hands-free operations and scaling, detecting when clusters get hot and orchestrating topology changes. To make all this work, the control plane has to share some amount of logic with the data plane. Best practice would be to create a shared library to avoid “<a href="https://en.wikipedia.org/wiki/Don%27t_repeat_yourself">repeating ourselves</a>”. But we couldn’t do that, because we were using different languages, which meant that sometimes the Kotlin and Rust versions of the code were slightly different. We also couldn’t share testing platforms, which meant the team had to rely on documentation and whiteboard sessions to stay aligned. And every misunderstanding, even a small one, led to a costly debug-fix-deploy cycles. We had a hard decision to make. Do we spend the time rewriting our <a href="https://brooker.co.za/blog/2022/04/11/simulation.html">simulation tools</a> to work with both Rust and Kotlin? Or do we rewrite the control plane in Rust?</p><p>The decision wasn’t as difficult this time around. A lot had changed in a year. Rust’s 2021 edition had addressed many of the pain points and paper cuts we’d encountered early on. Our internal library support had expanded considerably – in some cases, such as the AWS Authentication Runtime client, the Rust implementations were outperforming their Java counterparts. We’d also moved many integration concerns to API Gateway and Lambda, simplifying our architecture.</p><p>But perhaps most surprising was the team’s response. Rather than resistance to Rust, we saw enthusiasm. Our Kotlin developers weren’t asking “do we have to?” They were asking “when can we start?” They’d watched their colleagues working with Rust and wanted to be part of it.</p><p>A lot of this enthusiasm came from how we approached learning and development. Marc Brooker had written what we now call “The DSQL Book” – an internal guide that walks developers through everything from philosophy to design decisions, including the hard choices we had to defer. The team dedicated time each week to learning sessions on distributed computing, paper reviews, and deep architectural discussions. We brought in Rust experts like Niko who, true to our working backwards approach, helped us think through thorny problems before we wrote a single line of code. These investments didn’t just build technical knowledge – they gave the team confidence that they could tackle complex problems in a new language.</p><p>When we took everything into account, the choice was clear. It was Rust. We needed the control and data planes working together in simulation, and we couldn’t afford to maintain critical business logic in two different languages. We had observed significant throughput performance in the crossbar, and once we had the entire system written in Rust tail latencies were remarkably consistent. Our p99 latencies tracked very close to our p50 medians, meaning even our slowest operations maintained predictable, production-grade performance.</p><h2 id="its-so-much-more-than-just-writing-code">It’s so much more than just writing code <a href="#its-so-much-more-than-just-writing-code"></a></h2><p>Rust turned out to be a great fit for DSQL. It gave us the control we needed to avoid tail latency in the core parts of the system, the flexibility to integrate with a C codebase like Postgres, and the high-level productivity we needed to stand up our control plane. We even wound up using Rust (via WebAssembly) to power our internal ops web page.</p><p>We assumed Rust would be lower productivity than a language like Java, but that turned out to be an illusion. There was definitely a learning curve, but once the team was ramped up, they moved just as fast as they ever had.</p><p>This doesn’t mean that Rust is right for every project. Modern Java implementations like JDK21 offer great performance that is more than enough for many services. The key is to make these decisions the same way you make other architectural choices: based on your specific requirements, your team’s capabilities, and your operational environment. If you’re building a service where tail latency is critical, Rust might be the right choice. But if you’re the only team using Rust in an organization standardized on Java, you need to carefully weigh that isolation cost. What matters is empowering your teams to make these choices thoughtfully, and supporting them as they learn, take risks, and occasionally need to revisit past decisions. That’s how you build for the long term.</p><p>Now, go build!</p><h2 id="recommended-reading">Recommended reading <a href="#recommended-reading"></a></h2><p>If you’d like to learn more about DSQL and the thinking behind it, Marc Brooker has written an in-depth set of posts called DSQL Vignettes:</p><ul><li><a href="https://brooker.co.za/blog/2024/12/03/aurora-dsql.html">Aurora DSQL, and A Personal Story</a></li><li><a href="https://brooker.co.za/blog/2024/12/04/inside-dsql.html">Reads and Compute</a></li><li><a href="https://brooker.co.za/blog/2024/12/05/inside-dsql-writes.html">Transactions and Durability</a></li><li><a href="https://brooker.co.za/blog/2024/12/06/inside-dsql-cap.html">Wait! Isn’t That Impossible?</a></li></ul></span></section><hr></div></div>]]></description>
        </item>
    </channel>
</rss>