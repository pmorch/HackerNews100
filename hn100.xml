<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 14 May 2025 15:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[SMS 2FA is not just insecure, it's also hostile to mountain people (129 pts)]]></title>
            <link>https://blog.stillgreenmoss.net/sms-2fa-is-not-just-insecure-its-also-hostile-to-mountain-people</link>
            <guid>43984297</guid>
            <pubDate>Wed, 14 May 2025 13:28:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.stillgreenmoss.net/sms-2fa-is-not-just-insecure-its-also-hostile-to-mountain-people">https://blog.stillgreenmoss.net/sms-2fa-is-not-just-insecure-its-also-hostile-to-mountain-people</a>, See on <a href="https://news.ycombinator.com/item?id=43984297">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>i have a friend — she's an old lady born and raised here in the western north carolina mountains. she hates computers, yes, but she's been willing to learn a lot and quickly after joining a big signal group chat that our shared local community uses to keep in touch.</p>

<p>aside from memeing with the best of them in the group chat, she also maintains a large fish pond outside the house she built herself. this despite being in her 70s. she's an inspiration.</p>

<p>she has a landline. it works great, and the landline phone hardware works great with her hearing aids. she's had it for years. spectrum has a monoply in our area so the landline and her cable internet service is with spectrum.</p>

<p>she got a cell phone a few years ago. she got a smartphone basically because she had to to do basic life tasks, including joining the big signal group chat. at first she just used it on wifi, but quickly she decided she wanted to be able to use the phone everywhere so she got a cell phone plan from spectrum because they were already her ISP. spectrum mobile uses the verizon network — famed for its good rural coverage.</p>

<p>this is where things started to go haywire.</p>

<p>all her accounts on websites, things like email and bank accounts and health insurance and healthcare providers, they started trying to send her SMS messages in order to let her get into her accounts.</p>

<p>the SMS codes don't work, because they don't come. she doesn't have cell service at her house. it's up in the mountains, sure, but it's not <em>isolated</em>. she lives 20 minutes from downtown asheville and she has lots of neighbors on her road.</p>

<p>she turned on wifi calling on her phone. now she could receive SMS messages from friends and family, but 2FA codes still weren't coming through. i did some digging, and it turns out messages from 5 digit shortcodes often aren't supported over wifi calling. sometimes they are, but in her case they're clearly not. she has a current, stock iphone. she's using the spectrum-provided internet hardware. she knows how to use her phone.</p>

<p>i did more digging — it turns out some ISP-provided landline services support receiving SMS messages to the landline, and then a computer voice reads them out to you. “we don't offer that service” the spectrum chat told us.</p>

<p>some of these accounts can likely be converted to using TOTP 2FA rather than SMS 2FA. this is good, but you have to get in to begin with in order to turn that on. so what my friend has to do is:</p>
<ol><li>make a list, over time, of the websites that she's locked out of because of SMS 2FA</li>
<li>not be able to use those sites at home the whole time she's making the list</li>
<li>schedule a meetup with a friend like me</li>
<li>drive to town to meet the friend</li>
<li>sit down and systematically go through the list of websites and convert them to TOTP</li>
<li>inevitably discover that some of them don't support TOTP</li>
<li>try and contact those companies and explain that they need to turn off SMS 2FA on her account so that she can use their healthcare/banking/email/whatever service from her home</li>
<li>discover that it's not possible to talk to a company anymore in 2025</li></ol>

<p>other options available to her include</p>
<ol><li>port her cellphone number to a VOIP provider that does support receiving SMS from shortcodes over wifi</li>
<li>spend hundreds of dollars setting up a cell tower signal booster outside her house</li>
<li>move</li></ol>

<p>these are all ridiculous options that shouldn't be necessary in order to log in to a website.</p>

<p>if you look at the spectrum mobile coverage map where my friend lives, it shows she has perfect coverage at her house. and all her neighbors do too. all the way up the holler in fact!</p>

<p>this is simply false. she usually doesn't even have service 100 meters down the road.</p>

<p>another friend of mine who also lives out in the county, a millenial, once said that “SMS 2FA is the bane of [her] existence.” the valley she's in isn't even that deep.</p>

<p>and TOTP, the obvious alternative solution, is still pretty sorry. you have to download an app to do it, it's not just a capability that a phone has by default. and then when trying to find an app to use for it, you're presented with a multitude of high-stakes choices, and often pretty technical explanations if you start internet searching about which app to use.</p>

<p>i understand why SMS 2FA is so ubiquitous. when it works, the UX is good, nontechnical users intuitively understand it, and it's usually secure <em>enough</em>.</p>

<p>but there are 1.1 <em>million</em> people in these western north carolina mountains, 25 million in the rest of the appalachians, and many millions more in the mountain west and pacific ranges.</p>

<p>we have internet, but we have F-tier cell service — what are we supposed to do?</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Is HDR, Anyway? (158 pts)]]></title>
            <link>https://www.lux.camera/what-is-hdr/</link>
            <guid>43983871</guid>
            <pubDate>Wed, 14 May 2025 12:46:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lux.camera/what-is-hdr/">https://www.lux.camera/what-is-hdr/</a>, See on <a href="https://news.ycombinator.com/item?id=43983871">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        

  
        <div>
            <p>It's not you. HDR confuses tons of people.</p><p>Last year we announced HDR or "High Dynamic Range" photography was coming to our popular photography app, <a href="https://halide.cam/" rel="noreferrer">Halide</a>. While most customers celebrated, some were confused, and others showed downright <em>concern</em>. That's because HDR can mean two different, but related, things.</p><p>The first HDR is the "HDR mode" introduced to the iPhone camera in 2010.</p><figure><img src="https://www.lux.camera/content/images/2025/04/image-3.png" alt="" loading="lazy" width="960" height="540" srcset="https://www.lux.camera/content/images/size/w600/2025/04/image-3.png 600w, https://www.lux.camera/content/images/2025/04/image-3.png 960w" sizes="(min-width: 720px) 720px"><figcaption><span>September, 2010</span></figcaption></figure><p>The second HDR involves new screens that display more vibrant, detailed images. Shopped for a TV recently? No doubt you've seen stickers like this:</p><figure><img src="https://www.lux.camera/content/images/2025/05/HDR-Sticker.png" alt="" loading="lazy" width="2000" height="1242" srcset="https://www.lux.camera/content/images/size/w600/2025/05/HDR-Sticker.png 600w, https://www.lux.camera/content/images/size/w1000/2025/05/HDR-Sticker.png 1000w, https://www.lux.camera/content/images/size/w1600/2025/05/HDR-Sticker.png 1600w, https://www.lux.camera/content/images/size/w2400/2025/05/HDR-Sticker.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>This post finally explains what HDR <em>actually</em> means, the problems it presents, and three ways to solve them.</p><h2 id="what-is-dynamic-range">What is Dynamic Range?</h2><p>Let's start with a real world problem. Before smart phones, it was impossible to capture great sunsets with point-and-shoot cameras. No matter how you fiddled with the dials, everything came out too bright or too dark.</p><figure><div><p><img src="https://www.lux.camera/content/images/2025/03/new-york-skyline-no-hdr-under-exposed.jpeg" width="2000" height="1500" loading="lazy" alt="" srcset="https://www.lux.camera/content/images/size/w600/2025/03/new-york-skyline-no-hdr-under-exposed.jpeg 600w, https://www.lux.camera/content/images/size/w1000/2025/03/new-york-skyline-no-hdr-under-exposed.jpeg 1000w, https://www.lux.camera/content/images/size/w1600/2025/03/new-york-skyline-no-hdr-under-exposed.jpeg 1600w, https://www.lux.camera/content/images/size/w2400/2025/03/new-york-skyline-no-hdr-under-exposed.jpeg 2400w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.lux.camera/content/images/2025/03/new-york-skyline-no-hdr-over-exposed.jpeg" width="2000" height="1500" loading="lazy" alt="" srcset="https://www.lux.camera/content/images/size/w600/2025/03/new-york-skyline-no-hdr-over-exposed.jpeg 600w, https://www.lux.camera/content/images/size/w1000/2025/03/new-york-skyline-no-hdr-over-exposed.jpeg 1000w, https://www.lux.camera/content/images/size/w1600/2025/03/new-york-skyline-no-hdr-over-exposed.jpeg 1600w, https://www.lux.camera/content/images/size/w2400/2025/03/new-york-skyline-no-hdr-over-exposed.jpeg 2400w" sizes="(min-width: 720px) 720px"></p></div><figcaption><p><span>The result of trying to capture a sunset with an old-school camera.</span></p></figcaption></figure><p>In that photo, the problem has to do with the different light levels coming from the sky and the buildings in shadow, the former emitting<em> thousands</em> of times more light than the latter. Our eyes can see both just fine. Cameras? They can deal with <em>overall</em> bright lighting, or <em>overall</em> dim lighting, but they struggled with scenes contain both really dark and really bright spots.</p><figure><div><p><img src="https://www.lux.camera/content/images/2025/05/HDRI_Sample_Scene_Window_-_05.jpg" width="1600" height="1200" loading="lazy" alt="" srcset="https://www.lux.camera/content/images/size/w600/2025/05/HDRI_Sample_Scene_Window_-_05.jpg 600w, https://www.lux.camera/content/images/size/w1000/2025/05/HDRI_Sample_Scene_Window_-_05.jpg 1000w, https://www.lux.camera/content/images/2025/05/HDRI_Sample_Scene_Window_-_05.jpg 1600w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.lux.camera/content/images/2025/05/HDRI_Sample_Scene_Window_-_08.jpg" width="1600" height="1200" loading="lazy" alt="" srcset="https://www.lux.camera/content/images/size/w600/2025/05/HDRI_Sample_Scene_Window_-_08.jpg 600w, https://www.lux.camera/content/images/size/w1000/2025/05/HDRI_Sample_Scene_Window_-_08.jpg 1000w, https://www.lux.camera/content/images/2025/05/HDRI_Sample_Scene_Window_-_08.jpg 1600w" sizes="(min-width: 720px) 720px"></p></div><figcaption><p><a href="https://en.wikipedia.org/wiki/Multi-exposure_HDR_capture" rel="noreferrer"><span>Via Wikipedia</span></a></p></figcaption></figure><p>Dynamic range simply means, "the difference between the darkest and brightest bits of a scene." For example, this foggy morning is an example of a <em>low</em> dynamic range scene, because everything is sort of gray.</p><figure><img src="https://www.lux.camera/content/images/2025/05/foggy.jpeg" alt="" loading="lazy" width="2000" height="1500" srcset="https://www.lux.camera/content/images/size/w600/2025/05/foggy.jpeg 600w, https://www.lux.camera/content/images/size/w1000/2025/05/foggy.jpeg 1000w, https://www.lux.camera/content/images/size/w1600/2025/05/foggy.jpeg 1600w, https://www.lux.camera/content/images/2025/05/foggy.jpeg 2016w" sizes="(min-width: 720px) 720px"><figcaption><span>Screens have no trouble showing this low-contrast photo. Shot with Halide in Osaka.</span></figcaption></figure><p>Most of our photos aren't as extreme as bright sunsets or foggy mornings. We'll just call those "standard dynamic range" or SDR scenes.</p><p>Before we move on, we need to highlight that the HDR problem isn't limited to cameras. Even if you had a perfect camera that could match human vision, most <em>screens</em> cannot produce enough contrast to match the real world.</p><p>Regardless of your bottleneck, when a scene contains more dynamic range than your camera can capture or your screen can pump out, you lose highlights, shadows, or both.</p><h2 id="solution-1-hdr-mode">Solution 1: "HDR Mode"</h2><p>In the 1990s researchers came up with algorithms to tackle the dynamic range problem. The algorithms started by taking a bunch of photos with different settings to capture more highlights and shadows:</p><figure><div><p><img src="https://www.lux.camera/content/images/2025/05/memorial0062.png" width="512" height="768" loading="lazy" alt=""></p><p><img src="https://www.lux.camera/content/images/2025/05/memorial0065.png" width="512" height="768" loading="lazy" alt=""></p><p><img src="https://www.lux.camera/content/images/2025/05/memorial0068.png" width="512" height="768" loading="lazy" alt=""></p></div><figcaption><p><span>This full sequence has 16 photos. Via </span><a href="https://www.pauldebevec.com/Research/HDR/#radiancemaps" rel="noreferrer"><span>Paul Debevec</span></a><span>.</span></p></figcaption></figure><p>Then the algorithms combined everything into a single "photo" that matches human vision… a photo that was useless, since computer screens couldn't display HDR. So these researchers also came up with algorithms to squeeze HDR values onto an SDR screen, which they called "Tone Mapping."</p><figure><img src="https://www.lux.camera/content/images/2025/05/Reinhard.jpeg" alt="" loading="lazy" width="512" height="768"><figcaption><span>The Reinhard Tone Mapper, invented in 2002. It is </span><a href="https://sites.units.it/ipl/research/details/HDR_ToneMap/memorial_tm.html" rel="noreferrer"><span>one of many</span></a><span>.</span></figcaption></figure><p>These algorithms soon found their way into commercial software for camera nerds.</p><figure><img src="https://www.lux.camera/content/images/2025/04/image.png" alt="" loading="lazy" width="1674" height="1240" srcset="https://www.lux.camera/content/images/size/w600/2025/04/image.png 600w, https://www.lux.camera/content/images/size/w1000/2025/04/image.png 1000w, https://www.lux.camera/content/images/size/w1600/2025/04/image.png 1600w, https://www.lux.camera/content/images/2025/04/image.png 1674w" sizes="(min-width: 720px) 720px"><figcaption><span>Photomatix Circa 2008</span></figcaption></figure><p>Unfortunately, these packages required a lot of fiddling, and too many photographers in the mid-2000s… lacked restraint.</p><figure><img src="https://upload.wikimedia.org/wikipedia/commons/3/3f/Yonge-Dundas_Square_-_2010_%28HDR%29.jpg" alt="undefined" loading="lazy" width="1658" height="1105"><figcaption><span>The Ed Hardy t-shirt of photography. </span><a href="https://en.wikipedia.org/wiki/Tone_mapping" rel="noreferrer"><span>Via Wikipedia</span></a><span>.</span></figcaption></figure><p>Taste aside, average people don't like fiddling with sliders. Most people want to tap a button and get a photo that looks closer to what they see without thinking about it. So Google and Apple went an extra step in their camera apps.</p><p>Your modern phone's camera first captures a series of photos at various brightness levels, like we showed a moment ago. From this burst of photos, the app calculates an HDR image, but unlike that commercial software from earlier, it uses complex logic and AI to make the tone mapping choices for you.</p><figure><img src="https://www.lux.camera/content/images/2025/05/Screenshot-2025-05-08-at-09.13.41.png" alt="" loading="lazy" width="2000" height="1007" srcset="https://www.lux.camera/content/images/size/w600/2025/05/Screenshot-2025-05-08-at-09.13.41.png 600w, https://www.lux.camera/content/images/size/w1000/2025/05/Screenshot-2025-05-08-at-09.13.41.png 1000w, https://www.lux.camera/content/images/size/w1600/2025/05/Screenshot-2025-05-08-at-09.13.41.png 1600w, https://www.lux.camera/content/images/size/w2400/2025/05/Screenshot-2025-05-08-at-09.13.41.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span>Phil Schiller at the iPhone XS introduction showing off a newer Smart HDR </span></figcaption></figure><p>Apple and Google called this stuff "HDR" because "HDR Construction Followed By Automatic Tone Mapping" doesn't exactly roll off the tongue. But just to be clear, the HDR added to the iPhone in 2010<strong> was not HDR</strong>. The final JPEG was an SDR image that tries to replicate what you saw with your eyes. Maybe they should have called it "Fake HDR Mode."</p><p>I know quibbling over names feels as pedantic as going, "Well actually, 'Frankenstein' was the doctor, you're thinking of 'Frankenstein's Monster,'" but if you're going to say you hate HDR, remember that it's <em>bad</em> <em>tone mapping</em> that is the actual monster. That brings us to…</p><h3 id="the-first-hdr-backlash">The First HDR Backlash</h3><p>Over the years, Apple touted better and better algorithms in their camera, like Smart HDR and Deep Fusion. As this happened, we worried that our flagship photography app, Halide, would become irrelevant. Who needs a manual controls when AI can do a better job?</p><p>We were surprised to watch the opposite play out. As phone cameras got smarter, users asked us to turn off these features. One issue was how the algorithms make mistakes, like this weird edge along my son Ethan's face.</p><figure><img src="https://www.lux.camera/content/images/2025/04/image-2.png" alt="" loading="lazy" width="1462" height="1570" srcset="https://www.lux.camera/content/images/size/w600/2025/04/image-2.png 600w, https://www.lux.camera/content/images/size/w1000/2025/04/image-2.png 1000w, https://www.lux.camera/content/images/2025/04/image-2.png 1462w" sizes="(min-width: 720px) 720px"><figcaption><span>When life gives you lemons, you... eat them.</span></figcaption></figure><p>That's because Smart HDR and Deep Fusion require that the iPhone camera capture a burst of photos and stitch them together to preserve the best parts. Sometimes it goofs. Even when the algorithms behave, they come with tradeoffs.</p><p>Consider these photos I took from a boat in the Galapagos: the ProRAW version, which uses multi-photo algorithms, looks smudgier than the single-shot capture I took moments later.</p><figure><div><p><img src="https://www.lux.camera/content/images/2025/05/Untitled-1.jpeg" width="1008" height="1344" loading="lazy" alt="" srcset="https://www.lux.camera/content/images/size/w600/2025/05/Untitled-1.jpeg 600w, https://www.lux.camera/content/images/size/w1000/2025/05/Untitled-1.jpeg 1000w, https://www.lux.camera/content/images/2025/05/Untitled-1.jpeg 1008w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.lux.camera/content/images/2025/05/galapagos-native.jpeg" width="1008" height="1344" loading="lazy" alt="" srcset="https://www.lux.camera/content/images/size/w600/2025/05/galapagos-native.jpeg 600w, https://www.lux.camera/content/images/size/w1000/2025/05/galapagos-native.jpeg 1000w, https://www.lux.camera/content/images/2025/05/galapagos-native.jpeg 1008w" sizes="(min-width: 720px) 720px"></p></div><figcaption><p><span>Left: Merged from Multiple Photos. Right: A single exposure.</span></p></figcaption></figure><p>What's likely happening? When things move in the middle of a burst capture— which always happens when shooting handheld— these algorithms have to nudge pixels around to make things line up. This sacrifices detail.</p><p>Since 2020, we've offered users the option of disabling Smart HDR and Deep Fusion, and it quickly became one of our most popular features.</p><figure><img src="https://www.lux.camera/content/images/2025/05/Disabled_SmarterProcessing.png" alt="" loading="lazy" width="1350" height="718" srcset="https://www.lux.camera/content/images/size/w600/2025/05/Disabled_SmarterProcessing.png 600w, https://www.lux.camera/content/images/size/w1000/2025/05/Disabled_SmarterProcessing.png 1000w, https://www.lux.camera/content/images/2025/05/Disabled_SmarterProcessing.png 1350w" sizes="(min-width: 720px) 720px"></figure><p>This lead us to Process Zero, our completely AI-free camera mode, which we launched last year and became a <a href="https://www.theverge.com/2024/8/18/24222043/halide-process-zero-google-gemini-podcast-installer" rel="noreferrer">smash</a> <a href="https://www.digitaltrends.com/mobile/how-halide-process-zero-changed-my-iphone-camera-forever/" rel="noreferrer">hit</a>. However, without any algorithms, HDR scene end up over and under exposed. Some people actually prefer the look — more on that later — but many were bummed. They just accepted this as a tradeoff for the natural aesthetic of AI-free photos.</p><p>But what if we don't need that tradeoff? What if I told you that analog photographers captured HDR as far back as 1857?</p><figure><img src="https://collectionapi.metmuseum.org/api/collection/v1/iiif/261941/619287/main-image" alt="[The Great Wave, Sète], Gustave Le Gray (French, 1820–1884), Albumen silver print from glass negative " loading="lazy" width="1200" height="974"><figcaption><span>The Great Wave by Gustave Le Gray, </span><a href="https://www.metmuseum.org/art/collection/search/261941" rel="noreferrer"><span>via The Met</span></a></figcaption></figure><p>Ansel Adams, one of the most revered photographers of the 20th century, was a <em>master</em> at capturing dramatic, high dynamic range scenes.</p><figure><img src="https://www.lux.camera/content/images/2025/05/Adams_The_Tetons_and_the_Snake_River.jpg" alt="" loading="lazy" width="1920" height="1537" srcset="https://www.lux.camera/content/images/size/w600/2025/05/Adams_The_Tetons_and_the_Snake_River.jpg 600w, https://www.lux.camera/content/images/size/w1000/2025/05/Adams_The_Tetons_and_the_Snake_River.jpg 1000w, https://www.lux.camera/content/images/size/w1600/2025/05/Adams_The_Tetons_and_the_Snake_River.jpg 1600w, https://www.lux.camera/content/images/2025/05/Adams_The_Tetons_and_the_Snake_River.jpg 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>The Tetons and the Snake River </span><a href="https://en.wikipedia.org/wiki/The_Tetons_and_the_Snake_River" rel="noreferrer"><span>via Wikipedia</span></a></figcaption></figure><p>It's even more incredible that this was done on paper, which has even less dynamic range than computer screens!</p><p>From studying these analog methods, we've arrived at a single-shot process for handling HDR.</p><figure><img src="https://www.lux.camera/content/images/2025/03/new-york-skyline-good-hdr-2.jpeg" alt="" loading="lazy" width="2000" height="1500" srcset="https://www.lux.camera/content/images/size/w600/2025/03/new-york-skyline-good-hdr-2.jpeg 600w, https://www.lux.camera/content/images/size/w1000/2025/03/new-york-skyline-good-hdr-2.jpeg 1000w, https://www.lux.camera/content/images/size/w1600/2025/03/new-york-skyline-good-hdr-2.jpeg 1600w, https://www.lux.camera/content/images/size/w2400/2025/03/new-york-skyline-good-hdr-2.jpeg 2400w" sizes="(min-width: 1200px) 1200px"><figcaption><span>Halide's new, optional tone-mapping.</span></figcaption></figure><p>How do we accomplish this from a single capture? Let's step back in time.</p><h3 id="learning-from-analog">Learning From Analog</h3><p>In the age of film negatives, photography was a three step process. </p><ol><li>Capture a scene on film</li><li>Develop the film in a lab</li><li>Transfer the film to paper</li></ol><p>It's important to break down these steps because— plot twist— film is actually a high dynamic range medium. You just lose the dynamic range when you transfer your photo from a negative to paper. So in the age before Photoshop, master photographers would "dodge and burn" photos to preserve details during the transfer. </p><figure><img src="https://www.lux.camera/content/images/2025/03/image-1.png" alt="" loading="lazy" width="1848" height="1126" srcset="https://www.lux.camera/content/images/size/w600/2025/03/image-1.png 600w, https://www.lux.camera/content/images/size/w1000/2025/03/image-1.png 1000w, https://www.lux.camera/content/images/size/w1600/2025/03/image-1.png 1600w, https://www.lux.camera/content/images/2025/03/image-1.png 1848w" sizes="(min-width: 720px) 720px"><figcaption><span>An excerpt from </span><i><em>The Print</em></i><span>, the Ansel Adams Photography Series 3</span></figcaption></figure><figure><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Clearing_Winter_Storm%2C_Yosemite_Valley.jpg/2880px-Clearing_Winter_Storm%2C_Yosemite_Valley.jpg" alt="undefined" loading="lazy" width="1412" height="1120"><figcaption><span>"Clearing Winter Storm, Yosemite National Park" </span><a href="https://en.wikipedia.org/wiki/Clearing_Winter_Storm,_Yosemite_National_Park" rel="noreferrer"><span>Via Wikipedia</span></a><span>.</span></figcaption></figure><p>Is it a lie to dodge and burn a photo? According to Ansel Adams in <em>The Print</em>:</p><blockquote>When you are making a fine print you are creating, as well as re-creating. The final image you achieve will, to quote Alfred Stieglitz, reveal what you saw and felt.</blockquote><p>I'm inclined to agree. I don't think people reject processing your photos, whether it's dodging-and-burning a print, or fiddling with multi-exposure algorithms. The problem is that <strong>algorithms are not artists</strong>. </p><p>AI cannot read your mind, so it cannot honor your intent. For example, in this shot, I <em>wanted</em> stark contrast between light and dark. AI thought it was doing me a favor by pulling out detail in the shadow, flattening the whole image in the process. Thanks <a href="https://en.wikipedia.org/wiki/Office_Assistant" rel="noreferrer">Clippy</a>.</p><figure><div><p><img src="https://www.lux.camera/content/images/2025/05/463962896_919167870076255_8521404512461066232_n.jpg" width="1440" height="1920" loading="lazy" alt="" srcset="https://www.lux.camera/content/images/size/w600/2025/05/463962896_919167870076255_8521404512461066232_n.jpg 600w, https://www.lux.camera/content/images/size/w1000/2025/05/463962896_919167870076255_8521404512461066232_n.jpg 1000w, https://www.lux.camera/content/images/2025/05/463962896_919167870076255_8521404512461066232_n.jpg 1440w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.lux.camera/content/images/2025/05/464250428_1612875326106791_439563724822943641_n.jpg" width="612" height="917" loading="lazy" alt="" srcset="https://www.lux.camera/content/images/size/w600/2025/05/464250428_1612875326106791_439563724822943641_n.jpg 600w, https://www.lux.camera/content/images/2025/05/464250428_1612875326106791_439563724822943641_n.jpg 612w"></p></div><figcaption><p><span>Same lighting, moments apart. Left: Process Zero. Right: the iPhone camera's automatic tone mapping.</span></p></figcaption></figure><p>Even when tone mapping can help a photo, AI may take things too far, creating hyper-realistic images that exist in an uncanny valley. Machines cannot reason their way to your vision, or even good taste. </p><p>We think there's room for a different approach.</p><h3 id="a-different-approach-opt-in-single-shot-tone-mapping">A Different Approach: Opt-In, Single Shot Tone Mapping</h3><p>After considerable research, experimentation, trial and error, we've arrived on a tone mapper that feels true to the dodging and burning of analog photography. What makes it unique? For starters, it's derived from a single capture, as opposed to the multi-exposure approaches that sacrifice detail. While a single capture can't reach the dynamic range of human vision, good sensors have dynamic range approaching film.</p><p>However, the best feature is that this tone mapping is <strong>off by default. </strong>If you come across a photo that feels like it could use a little highlight or shadow recovery, you can now hop into Halide's updated Image Lab. </p><figure data-kg-thumbnail="https://www.lux.camera/content/media/2025/05/skyline-edit-trimmed_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://www.lux.camera/content/media/2025/05/skyline-edit-trimmed.mp4" poster="https://img.spacergif.org/v1/708x1280/0a/spacer.png" width="708" height="1280" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:08</span>
                        </p>
                        </div>
            </div>
            
        <img src="https://www.lux.camera/content/media/2025/05/skyline-edit-trimmed_thumb.jpg"></figure><p>In the Image Lab we have an exposure slider for adjusting overall brightness just like before. But to its right, we have a single dial that tames or boosts dynamic range. We think it's up to the photographer to decide what feels right.</p><p>To be clear, the tone mapper works different than simply bringing your photo into an editor and dragging the "shadows" and "highlights" sliders. It also does it best to preserve local contrast.</p><figure><div><p><img src="https://www.lux.camera/content/images/2025/05/venice-original.jpeg" width="2000" height="1500" loading="lazy" alt="" srcset="https://www.lux.camera/content/images/size/w600/2025/05/venice-original.jpeg 600w, https://www.lux.camera/content/images/size/w1000/2025/05/venice-original.jpeg 1000w, https://www.lux.camera/content/images/size/w1600/2025/05/venice-original.jpeg 1600w, https://www.lux.camera/content/images/2025/05/venice-original.jpeg 2016w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.lux.camera/content/images/2025/05/venice-exposure-increased.jpeg" width="2000" height="1500" loading="lazy" alt="" srcset="https://www.lux.camera/content/images/size/w600/2025/05/venice-exposure-increased.jpeg 600w, https://www.lux.camera/content/images/size/w1000/2025/05/venice-exposure-increased.jpeg 1000w, https://www.lux.camera/content/images/size/w1600/2025/05/venice-exposure-increased.jpeg 1600w, https://www.lux.camera/content/images/2025/05/venice-exposure-increased.jpeg 2016w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.lux.camera/content/images/2025/05/venice-tone-mapped.jpeg" width="2000" height="1500" loading="lazy" alt="" srcset="https://www.lux.camera/content/images/size/w600/2025/05/venice-tone-mapped.jpeg 600w, https://www.lux.camera/content/images/size/w1000/2025/05/venice-tone-mapped.jpeg 1000w, https://www.lux.camera/content/images/size/w1600/2025/05/venice-tone-mapped.jpeg 1600w, https://www.lux.camera/content/images/2025/05/venice-tone-mapped.jpeg 2016w" sizes="(min-width: 720px) 720px"></p></div><figcaption><p><span>Left and Middle: a shot with simple exposure adjustments. Right: a tone-mapped version.</span></p></figcaption></figure><p>Don’t worry: adjusting this stuff after-the-fact won't sacrifice quality. Since Halide captures DNG or "digital negative" files, it contains all of the information that your screen cannot display. The shadow and highlight details are already in there, and the tone-mapping simply brings it out selectively.</p><h2 id="solution-2-genuine-hdr-displays">Solution 2: Genuine HDR Displays</h2><p>I went to all that trouble explaining the difference between HDR and Tone Mapping because… drum roll please… <strong>today's screens are</strong> <strong>HIGH</strong>er <strong>DYNAMIC</strong> <strong>RANGE</strong>!</p><figure data-kg-thumbnail="https://www.lux.camera/content/media/2025/03/new-york-skyline-hdr_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://www.lux.camera/content/media/2025/03/new-york-skyline-hdr.mp4" poster="https://img.spacergif.org/v1/4032x3024/0a/spacer.png" width="4032" height="3024" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:10</span>
                        </p>
                        </div>
            </div>
            
        <img src="https://www.lux.camera/content/media/2025/03/new-york-skyline-hdr_thumb.jpg"></figure><figure data-kg-thumbnail="https://www.lux.camera/content/media/2025/05/overthrow-neon-hdr_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://www.lux.camera/content/media/2025/05/overthrow-neon-hdr.mp4" poster="https://img.spacergif.org/v1/3024x4032/0a/spacer.png" width="3024" height="4032" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:10</span>
                        </p>
                        </div>
            </div>
            
        <img src="https://www.lux.camera/content/media/2025/05/overthrow-neon-hdr_thumb.jpg"></figure><figure data-kg-thumbnail="https://www.lux.camera/content/media/2025/05/hotel-boston-hdr_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://www.lux.camera/content/media/2025/05/hotel-boston-hdr.mp4" poster="https://img.spacergif.org/v1/3024x4032/0a/spacer.png" width="3024" height="4032" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:10</span>
                        </p>
                        </div>
            </div>
            <figcaption><p><span>The atrium of the Hyatt Centric in Cambridge</span></p></figcaption>
        <img src="https://www.lux.camera/content/media/2025/05/hotel-boston-hdr_thumb.jpg"></figure><p>Ok, today's best screens still can't match the high dynamic range of real life, but they're way higher than the past. Spend a few minutes watching Apple TV's mesmerizing screensavers in HDR, and you get why this feels as big as the move from analog TV to HDTV. So… nine years after the introduction of HDR screens, why hasn't the world moved on?</p><p>A big problem is that it costs the TV, Film, and Photography industries billions of dollars (and a bajillion hours of work) to upgrade their infrastructure. For context, it took well over a decade for HDTV to reach critical mass.</p><p>Another issue is taste. Much like adding a spice to your meal, you don't want HDR to overpower everything. The garishness of bad HDR has left many filmmakers lukewarm on the technology. Just recently, cinematographer Steve Yedlin published <a href="https://www.yedlin.net/DebunkingHDR/" rel="noreferrer">a two hour lecture</a> on the pitfalls of HDR in the real world.</p><p>If you want to see how bad HDR displays can get, look no further than online content creators. At some point these thirsty influencers realized that if you make your videos uncomfortably bright, people will pause while swiping through their Instagram reels. The abuse of brightness has lead to people <a href="https://www.reddit.com/r/iPhone15Pro/comments/17r2sf8/hdr_content_is_painfully_bright_and_is_sadly/" rel="noreferrer">disabling HDR altogether</a>.</p><figure><img src="https://www.lux.camera/content/images/2025/03/image-2.png" alt="" loading="lazy" width="1622" height="734" srcset="https://www.lux.camera/content/images/size/w600/2025/03/image-2.png 600w, https://www.lux.camera/content/images/size/w1000/2025/03/image-2.png 1000w, https://www.lux.camera/content/images/size/w1600/2025/03/image-2.png 1600w, https://www.lux.camera/content/images/2025/03/image-2.png 1622w" sizes="(min-width: 720px) 720px"></figure><p>For all these reasons, I think HDR could end up another dead-end technology of the 2010s, alongside 3D televisions. However, Apple turned out to be HDR's best salesperson, as iPhones have captured and rendered HDR photos for years.</p><p>In fact, after we launched Process Zero last year, quite a few users asked us why their photos aren't as bright as the ones produced by Apple's camera. The answer was compatibility, which Apple improved with iOS 18. So HDR is coming to Process Zero!</p><figure><img src="https://www.lux.camera/content/images/2025/05/p0hdr.jpg" alt="" loading="lazy" width="2000" height="1242" srcset="https://www.lux.camera/content/images/size/w600/2025/05/p0hdr.jpg 600w, https://www.lux.camera/content/images/size/w1000/2025/05/p0hdr.jpg 1000w, https://www.lux.camera/content/images/size/w1600/2025/05/p0hdr.jpg 1600w, https://www.lux.camera/content/images/2025/05/p0hdr.jpg 2000w" sizes="(min-width: 720px) 720px"></figure><p>To handle the taste problem, we're offering three levels of HDR:</p><ul><li><strong>Standard</strong>: increases detail in shadows, and bumps up highlights while giving a tasteful rolloff in highlights</li><li><strong>Max:</strong> HDR that pushes the limits of the iPhone display</li><li><strong>Off:</strong> turns HDR off altogether. </li></ul><h3 id="compatibility-considerations">Compatibility Considerations</h3><p>Once you've got an amazing HDR photo, you're probably wondering where you can view it, today. The good news is that every iPhone that has shipped for the last several years supports HDR. It just isn't always available.</p><p>As we mentioned earlier, some users turn off HDR because the content hurts their eyes, but even if it's on, it isn't <em>always</em> on. Because HDR consumes more power, iOS turns it off in low-power mode. It also turns it off when using your phone in bright sunlight, so it can pump up SDR as bright as it can go.</p><p>An even bigger issue is where you can share it online. Unfortunately, most web browser can't handle HDR photos. Even if you encode HDR into a JPEG, the browser might butcher the image, either reducing the contrast and making everything look flat, or clipping highlights, which is about as ugly as bad digital camera photos from the 1990s.</p><p>But wait… how did I display these HDR examples? If you look carefully those are short HDR videos that I've set to loop. You might need these kinds of silly hacks to get around browser limitations.</p><p>Until recently, the best way to view HDR was with Instagram's native iPhone app. While Instagram is our users' most popular place to share photos… it's Instagram. Fortunately, things are changing.</p><p>iOS 18 adopted Adobe's approach to HDR, which Apple calls "Adaptive HDR." In this system, your photos contain <em>both</em> SDR and HDR information in a single file. If an app doesn't know what to do with the HDR information, or it can't render HDR, there's an SDR fallback. This stuff even works with JPEGs!</p><figure><img src="https://www.lux.camera/content/images/2025/05/image-3.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://www.lux.camera/content/images/size/w600/2025/05/image-3.png 600w, https://www.lux.camera/content/images/size/w1000/2025/05/image-3.png 1000w, https://www.lux.camera/content/images/size/w1600/2025/05/image-3.png 1600w, https://www.lux.camera/content/images/2025/05/image-3.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>From Apple's </span><a href="https://developer.apple.com/videos/play/wwdc2024/10177/" rel="noreferrer"><span>Adaptive HDR Presentation</span></a></figcaption></figure><p>Browser support is halfway there. Google beat Apple to the punch with their own version of Adaptive HDR they call <a href="https://source.android.com/docs/core/camera/ultra-hdr" rel="noreferrer">Ultra HDR</a>, which Chrome 14 now supports. Safari has <a href="https://bugs.webkit.org/show_bug.cgi?id=282299" rel="noreferrer">added HDR support into its developer preview</a>, then it disabled it, due to bugs within iOS. </p><p>Speaking of iOS bugs, there's a reason we aren't launching the Halide HDR update with today's post: HDR photos sometimes render wrong in Apple's own Photos app! Oddly enough, they render just fine in Instagram and other third-party apps. We've filed a bug report with Apple, but due to how Apple releases software, we doubt we'll see a fix until iOS 19.</p><p>Rather than inundate customer support with angry emails about how photos don't look right in Apple's photos app, we've decided to release HDR support in our Technology Preview beta that we're offering to 1,000 Halide subscribers. Why limit it to 1,000? Apple restricts how many people can sign up for TestFlight, so we want to make sure we stay within our limits. This is the start of our preview of some very exciting big features in Halide which are part of our big Mark III update.</p><p>If this stuff excites you and you want to try it out, go to the Members section in Settings <a href="https://apps.apple.com/us/app/halide-mark-ii-pro-camera/id885697368" rel="noreferrer">right now</a>.</p><figure><img src="https://www.lux.camera/content/images/2025/05/IMG_7647.jpg" alt="" loading="lazy" width="1206" height="1700" srcset="https://www.lux.camera/content/images/size/w600/2025/05/IMG_7647.jpg 600w, https://www.lux.camera/content/images/size/w1000/2025/05/IMG_7647.jpg 1000w, https://www.lux.camera/content/images/2025/05/IMG_7647.jpg 1206w" sizes="(min-width: 720px) 720px"></figure><h2 id="solution-3-embrace-sdr">Solution 3: Embrace SDR</h2><p>As mentioned earlier, some users actually prefer SDR. And that’s OK. I think this about more than just the lo-fi aesthetic, and touches on a paradox of photography. Sometimes a less-realistic photo is more engaging.</p><p>But aren't photos about capturing reality? If that were true, we would all use pinhole cameras, ensuring we capture everything in sharp focus.  If photos were about realism, nobody would shoot black and white film.</p><figure><img src="https://www.lux.camera/content/images/2025/05/000272640006.jpg" alt="" loading="lazy" width="2000" height="3017" srcset="https://www.lux.camera/content/images/size/w600/2025/05/000272640006.jpg 600w, https://www.lux.camera/content/images/size/w1000/2025/05/000272640006.jpg 1000w, https://www.lux.camera/content/images/size/w1600/2025/05/000272640006.jpg 1600w, https://www.lux.camera/content/images/2025/05/000272640006.jpg 2075w" sizes="(min-width: 720px) 720px"><figcaption><span>Shot on Ilford HP5, ƒ/1.4</span></figcaption></figure><p>Consider this HDR photo of my dad.</p><figure data-kg-thumbnail="https://www.lux.camera/content/media/2025/05/Marc-ProRAW-hdr_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://www.lux.camera/content/media/2025/05/Marc-ProRAW-hdr.mp4" poster="https://img.spacergif.org/v1/3024x4032/0a/spacer.png" width="3024" height="4032" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:10</span>
                        </p>
                        </div>
            </div>
            <figcaption><p><span>Shot in ProRAW</span></p></figcaption>
        <img src="https://www.lux.camera/content/media/2025/05/Marc-ProRAW-hdr_thumb.jpg"></figure><p>HDR reveals every wrinkle and pore on his face, and the bright whites in his beard draw too much attention. Just as you might use shallow focus to draw attention on your subject, this is one situation where less dynamic range feels better than hyper-realism. Consider the Process Zero version, with HDR disabled.</p><figure><img src="https://www.lux.camera/content/images/2025/05/marc.jpeg" alt="" loading="lazy" width="1128" height="1191" srcset="https://www.lux.camera/content/images/size/w600/2025/05/marc.jpeg 600w, https://www.lux.camera/content/images/size/w1000/2025/05/marc.jpeg 1000w, https://www.lux.camera/content/images/2025/05/marc.jpeg 1128w" sizes="(min-width: 720px) 720px"><figcaption><span>Process Zero, without Tone Mapping</span></figcaption></figure><p>While we have plenty of work before Process Zero achieves all of our ambitions, we think dynamic range is a huge factor in recapturing the beauty of analog photography in the digital age.</p><figure><img src="https://www.lux.camera/content/images/2025/05/image-2.png" alt="" loading="lazy" width="1000" height="1154" srcset="https://www.lux.camera/content/images/size/w600/2025/05/image-2.png 600w, https://www.lux.camera/content/images/2025/05/image-2.png 1000w" sizes="(min-width: 720px) 720px"><figcaption><span>Shot on film.</span></figcaption></figure><p>We think tone mapping is an invaluable tool that dates back hundreds of years. We think HDR displays have amazing potential to create images we've never seen before. We see a future where SDR and HDR live side by side. We want to give you that choice — whether it is tone-mapping, HDR, or any combination thereof. It’s the artists’ choice —&nbsp;and that artist doesn’t have to be an algorithm. </p><p>We think the future of sunsets looks bright.</p>
        </div>
      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Databricks and Neon (164 pts)]]></title>
            <link>https://www.databricks.com/blog/databricks-neon</link>
            <guid>43982777</guid>
            <pubDate>Wed, 14 May 2025 10:10:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.databricks.com/blog/databricks-neon">https://www.databricks.com/blog/databricks-neon</a>, See on <a href="https://news.ycombinator.com/item?id=43982777">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Today, we are excited to announce that we have agreed to acquire <a data-external-link="true" href="https://neon.tech/" target="_blank" rel="noopener noreferrer">Neon</a>, a developer-first, serverless Postgres company. Neon’s co-founders are among the very few people in the world who could re-architect Postgres with true separation of storage and compute, built for modern developers and AI systems. Their world-class team of Postgres hackers and database veterans will join Databricks to deliver serverless Postgres at production scale to developers in an AI-native world.</p>

<h2>A developer-first mission, born from Postgres expertise</h2>

<p>Four years ago, the Neon co-founders joined together with a vision to disrupt the database industry. They observed that the foundations of database technologies were largely designed for the 90s era. Their goal was to build a new database platform that would dramatically improve experiences for developers, in some fundamental ways:</p>

<ol>
	<li>They foresaw that Postgres would become the de facto standard for databases and had the vision to create a serverless Postgres platform.</li>
	<li>They were determined to make it possible to create a new Postgres instance in seconds, so developers wouldn’t need to wait.&nbsp;</li>
	<li>They set out to simplify the operational aspect of database scaling by automating it as load changed, so developers could start very small and not worry about over- or under-provisioning.</li>
	<li>They wanted to enable rapid experimentation and testing by supporting instant forking and branching of databases and creating fully isolated databases, as databases are often one of the most difficult modules to test in end-to-end application testing.</li>
</ol>

<p>A few years in, the Neon team engineered a new, innovative database architecture that decouples storage scaling from compute scaling, which ultimately enabled all of the above goals. When Neon first launched, developers raved about the speed, the simplicity, and the ability to branch and fork their databases like Git does for code.</p>

<h2>The shift toward AI agents</h2>

<p>As Neon became GA last year, they noticed an interesting stat: 30% of the databases were created by AI agents, not humans. When they looked at their stats again recently, the number went from 30% to over 80%. That is, <strong>AI agents were creating 4 times more databases versus humans</strong>.</p>

<p><img alt="database instance creation agent vs human" data-entity-type="" data-entity-uuid="" src="https://www.databricks.com/sites/default/files/inline-images/FINAL.png?v=1747198987" data-ot-ignore="1"></p>

<p>If you think of AI agents as your own massive team of high-speed junior developers (potentially “mentored” by senior developers), it’s then not surprising that the same capabilities the Neon team focused on that made Neon great for developers also made Neon great for these AI agents:</p>

<ol>
	<li><strong>Postgres open source ecosystem</strong>: All frontier LLMs have been trained on the vast amount of public information available about the Postgres open source ecosystem, so all AI agents are experts in using Neon, which is built on Postgres.</li>
	<li><strong>Speed</strong>: Traditional databases were designed for humans to provision and operate. It was OK to take minutes to spin up a database. Given AI agents operate at machine speed, ultra rapid provisioning time becomes critical.</li>
	<li><strong>Elastic scaling and pricing: </strong>The decoupled storage from compute serverless architecture enables extremely low-cost Postgres instances. It’s now possible to launch thousands or even millions of agents with their own databases cost-effectively.</li>
	<li><strong>Branching and forking</strong>: AI agents can be non-deterministic, and “vibes” need to be checked and verified. Neon’s ability to instantly create a full copy of a database, not only for schema but also for the data, allows all these AI agents to be operating on their own isolated database instance in high fidelity for experimentation and validation.</li>
</ol>

<h2>Shared DNA</h2>

<p>We have long known Nikita Shamgunov, Heikki Linnakangas, and Stas Kelvich – the founders of Neon. Before Neon, Nikita started his career working on SQL Server and later became well-known in the database community as a co-founder and CEO of SingleStore, driving the startup’s business to over $100M in annual revenue. Heikki has been a committer on Postgres for two decades - he started his Postgres journey when he was bored on paternity leave and decided to look into Postgres internals for fun. Stas is an ex-physicist who got into Postgres hacking because he needed a better R-tree implementation for his tourism search engine startup.</p>

<p>Databricks and Neon share the same DNA in building hardcore technical innovations at the infrastructure layer. We also share the belief in the importance of open source: we originally started the Apache Spark™ project at UC Berkeley, which also happens to be the original birthplace of Postgres, the open source database project Neon builds on.</p>

<h2>Looking forward</h2>

<p>OLTP databases represent a $100B market dominated by products that were built decades ago. We believe it is time for this market to be disrupted by developers and AI agents. Together with the Neon team, we look forward to building the most developer and AI agent friendly database platform.</p>

<p>What does this mean for existing Neon customers and partners? We are fully committed to the future of the Neon platform. The roadmap remains ambitious, and we intend to continue making Neon the best database platform for developers. After closing, existing customers and partners can expect continued support and innovation – now with the full backing and resources of Databricks. It’s a great moment for existing Neon users, and an exciting one for developers just discovering Neon. Go launch a new Postgres instance on <a data-external-link="true" href="http://neon.tech/" target="_blank" rel="noopener noreferrer">Neon</a> today!</p>

<p>We’re equally excited about what this will mean for Databricks’ enterprise customers. We plan to share much more at the upcoming <a data-external-link="true" href="https://www.databricks.com/dataaisummit" target="_blank" rel="noopener noreferrer">Data + AI Summit</a> in San Francisco, June 9–12.</p>

<p>Check out Neon’s <a data-external-link="true" href="http://neon.tech/blog/neon-and-databricks" target="_blank" rel="noopener noreferrer">blog</a> to hear directly from them about why they’re excited to join forces.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to Build a Smartwatch: Picking a Chip (162 pts)]]></title>
            <link>https://ericmigi.com/blog/how-to-build-a-smartwatch-picking-a-chip/</link>
            <guid>43981680</guid>
            <pubDate>Wed, 14 May 2025 07:02:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ericmigi.com/blog/how-to-build-a-smartwatch-picking-a-chip/">https://ericmigi.com/blog/how-to-build-a-smartwatch-picking-a-chip/</a>, See on <a href="https://news.ycombinator.com/item?id=43981680">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><iframe width="560" height="315" src="https://www.youtube.com/embed/umQ39BhcyMM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><p>Full video of this post is up on my podcast on <a href="https://youtu.be/umQ39BhcyMM">YouTube</a>! Subscribe on <a href="https://pca.st/m2aheg1u">PocketCasts</a>, <a href="https://creators.spotify.com/pod/show/tick-talk-w-eric">Spotify</a>, or <a href="https://podcasts.apple.com/us/podcast/tick-talk-w-eric-migicovsky/id1812362079">Apple Podcasts</a>.  </p><p><strong>TLDR:</strong></p><ul><li>We've selected a chip for Core Time 2 - SF32LB52J. It’s from a company called SiFli, and features an open source SDK.</li><li>I’m writing a series of posts on how to make a smartwatch</li><li>Covering how to pick a chip (today), other hardware and software stuff (later)</li><li>I hope others will use PebbleOS to build interesting new smartwatches and other devices!</li></ul><p>This is the first in a series of posts around the steps you need to take in order to build a smartwatch. I want to show that it's not actually that hard to make a decent one in 2025! Hopefully the work that we're doing to make these <a href="https://store.repebble.com/">new watches</a> will help other folks to build some as well. </p><p>Smartwatches are not and should not be ‘one-size fits all’ devices. I really hope that the open sourcing of PebbleOS might spark someone’s imagination and enable them to build a smartwatch that fits their needs perfectly!</p><p>A smartwatch is a system made up of three main components: </p><ul><li>Watch hardware (this is the actual watch itself)</li><li>Watch software (usually referred to as the firmware or operating system)</li><li>Mobile companion app (iOS and Android app that sends notifications, downloads watchfaces etc)</li></ul><p>As I've mentioned before, designing a consumer electronics product like a smartwatch is an exercise in constraint maximization. You need to identify a target experience goal (”I want a smartwatch with an always-on daylight readable display that lasts for 30 days”), then break that down into features/specifications (”e-paper display”, “Bluetooth LE”, “water resistant”, “$150 retail price”) and then create a design with hardware and software components (”Sharp Memory LCD”, “150mAh lipoly battery”, “FreeRTOS” etc) that meet your specifications and fulfill your goal. </p><p>After almost 20 years of building products, this process comes naturally to me - sometimes unprompted! I can look at any product and picture the exploded view, estimate manufacturing costs, and imagine how its software subsystems fit together. It’s a blessing and a curse!</p><p>You can break down watch hardware into five key systems:</p><ul><li>Microcontroller chip (usually includes Bluetooth radio)</li><li>Display</li><li>Sensors and outputs (tac switches, touch, mics, accelerometer, speakers)</li><li>Other electronic components (chips, passive components, printed circuit board, battery)</li><li>Mechanical structure (watch case, glass, buttons, strap, charge cable)</li></ul><p>Picking components for the latter 3 systems is actually quite straight forward these days. There are lots of good options available at various price points for sensors, batteries, straps, watch cases, mics, etc. Constraint maximization is relatively easy for these, I’ll wrap all these up into one blog post later.</p><p>The two most challenging component decisions that need to be made when designing a smartwatch are selecting:</p><ul><li>a microcontroller and Bluetooth radio (this post)</li><li>a display (future post)</li></ul><h3>The backstory</h3><p>During the first Pebble era, we used STM32F2 microcontrollers (MCU). Why? Because over beers on one Friday evening in 2011 at the <a href="https://hackerdojo.org/">Hacker Dojo</a> a friend of mine, Hugo Fiennes, would NOT stop raving about this relatively new MCU. At the time, I was working on <a href="https://www.tumblr.com/inpulse-blog/222972860/evolution1?source=share">inPulse</a> which used an LPC2103 (8K of RAM!) - he rightly thought that we were in desperate need of an upgrade. </p><p>Honestly, this is 100% typical for me. I think that all the major chip selections I’ve made have come from friends raving about a particular chip. Yes, I have a delightfully eccentric set of friends 😂&nbsp;Thank you Trammell Hudson, TL Lim from Pine64, Peter Barrett and many others for your chats over the years.</p><p>An MCU is the heart of the smartwatch. Think of an MCU as a miniature computer - it contains a CPU, RAM, (usually) flash storage, i/o peripherals and sometimes radios, all within a single tiny integrated circuit. </p><p>You can the specs of the MCUs we used before on this <a href="https://github.com/PebbleA2/wiki/wiki/Watch-Comparison">helpful table</a> - 64-144MHz, 128-256KB RAM. During this era, MCUs didn’t have integrated BT radios, so we used additional chips like TI CC2564 (that model is forever etched in my brain for some reason, probably the pain of finding a decent BT stack to use with it). </p><h3>Why is picking the right MCU so important?</h3><p>Getting back to constraint maximization, the MCU sits at the focal point of the most constrained governing ‘equations’ - software compatibility, power consumption and cost. </p><p>The most interesting and difficult constraint is actually software compatibility. <strong>E</strong>mbedded software is much more fragmented and requirement-specific than computer operating system software. Because computers are not very hard drive space constrained, kernels can be optimized for broader compatibility - Linux contains over 17,000 device drivers. PebbleOS was effectively hardcoded to support MCUs from only one company (STM). Switching to a different brand required writing new peripheral (i2c, SPI, DMA, etc) drivers, adopting a different SDK and (sometimes) build system changes. These changes aren't risky, but they do take time to implement and test. Some MCUs do not easily support FreeRTOS (cough nRF53/54 due to lack of BLE stack).</p><p>Since we are not planning to build hundreds of thousands or millions of watches, software engineering costs cannot be amortized and end up being a significant driver to the total cost of each watch. If a chip is easier to develop software for, one might even consider paying for a higher per-unit cost chip given software development time savings.</p><p>A smartwatch must be connected 24/7 to a phone via Bluetooth, so average power consumption while connected is one of two largest power drains - the other being display.</p><h3>Picking chips for our new watches</h3><p>We decided to use Nordic’s nRF52840 for Core 2 Duo. It’s an older chip, but we were very familiar with it and knew that we could get PebbleOS up and running on it relatively quickly. Initially, we planned to use Nordic’s SoftDevice BLE stack, but thanks to the clever work of Liam (colleague of mine at Pebble and now <a href="http://rebble.io/">Rebble</a> contributor), we switched to using an open source BLE stack called <a href="https://github.com/apache/mynewt-nimble">nimBLE</a>. </p><p>While an nrf52840 is powerful enough for Core 2 Duo, for Core Time 2 we needed an MCU with more RAM and processing power.</p><p>For Core Time 2, we would have liked to stay with Nordic (since we had just spent a bunch of time porting PebbleOS to work with Nordic’s SDK and peripherals) but Nordic’s roadmap in the BLE MCU space was not very inviting. Their new low-end chip, nRF54L15, only has 256k of RAM. The bigger colour display on the Core Time 2 requires more RAM and we wanted some breathing room for new features. Also, it only recently went into mass production, so we don’t have any friends who are using it yet - not too many reviews. Nordic also has a 54H series with 1M of RAM, but the price doubles to $4-5 dollars or more - no 512K RAM option either. CT2 also requires a special interface for its 64 colour MIP display, which 2015-era Pebbles previously used a dedicated FPGA to drive. </p><p>So I was on the hunt for a new chip. I looked at a variety of options like Apollo, BES, and Dialog but couldn’t find anything that fit our needs exactly. One of the biggest stumbling blocks was lack of an open source SDK. One chip from BES looked pretty good but we ran into problems when trying to test it out - there was no open source SDK! No example code. Everything was locked behind an NDA. That wasn’t going to work for us - PebbleOS needs to be open source.</p><p>Luckily, lightning struck. I randomly got an email from the CEO of a smaller Bluetooth chip company called SiFli. We exchanged a few emails over the span of a few hours. It became clear that he was extraordinarily interested in getting his chips to power an open source smartwatch.</p><p>SiFli chips are custom-made to be the primary chip in smartwatches. They power tens of millions of smartwatches already from brands like Redmi, Oppo, Noise and many others. The smallest (!) SiFli chip <a href="https://drive.google.com/file/d/1FV5-n6thM8XkzVQeAap2eJv97sCO_NrG/view?usp=sharing">SF32LB52x</a> has over 512K of SRAM, 16M of PSRAM, has a dedicated MIP peripheral for our display - cutting out the need for a separate FPGA or expensive dedicated display interface chip like Epson.</p><p>It also has a extremely low power consumption profile: ~50uA with BLE connected. Oh and it’s less than $2! They even have several chips with 1 to 2MB SRAM options that we could switch to if necessary.</p><p>Best of all - their SDK is open source on <a href="https://github.com/OpenSiFli">Github</a> and they offered to help port PebbleOS to work on their chips.</p><p>So there you have it. The chip for Core Time 2 will be a SF32LB52J (the 1.8V variant of SF32LB527. That’s going to be etched in my long term memory for sure!</p><p>Next post: how to pick a display…</p><p>SiFli links:</p><ul><li><a href="https://drive.google.com/file/d/1FV5-n6thM8XkzVQeAap2eJv97sCO_NrG/view?usp=sharing">SF32LB52x reference guide</a></li><li><a href="https://github.com/OpenSiFli">https://github.com/OpenSiFli</a></li><li><a href="https://item.taobao.com/item.htm?abbucket=11&amp;detail_redpacket_pop=true&amp;id=913039901152&amp;ltk2=1746962311930er3jpr9z173xe91zr84ps&amp;ns=1&amp;priceTId=undefined&amp;query=%E6%80%9D%E6%BE%88%E7%A7%91%E6%8A%80&amp;skuId=5776328712262&amp;spm=a21n57.1.hoverItem.2&amp;utparam=%7B%22aplus_abtest%22%3A%228935e1d10ee53e64944bb19c4245a174%22%7D&amp;xxc=taobaoSearch">Buy Devkit on Taobao</a> (Aliexpress coming soon)</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wise refuses to let us access our $60k AUD (167 pts)]]></title>
            <link>https://hey.paris/posts/wise/</link>
            <guid>43981582</guid>
            <pubDate>Wed, 14 May 2025 06:46:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hey.paris/posts/wise/">https://hey.paris/posts/wise/</a>, See on <a href="https://news.ycombinator.com/item?id=43981582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://wise.com/">Wise</a> has stolen more than $60,000 AUD from us, and refuses to let us access it.</p><p><a href="https://secretlab.games/">We’ve</a> had a Wise account for around 5 years (since they were called TransferWise). It’s been a really useful way to transact in foreign currencies, and pay for things when we’re travelling for work.</p><p>In early-April 2025, Wise asked us to provide some additional information on our Ultimate Beneficial Owners (UBOs) by uploading a statement of shareholders, and the ID of the owners. Perfectly reasonable stuff for an entity that pretends to be a bank to ask for.</p><p>In response, in early-April 2025, we uploaded a <a href="https://asic.gov.au/online-services/search-asic-registers/search-fees/">paid ASIC Extract</a> for the business, and scans of the passports of the two UBOs. The message requesting additional information on UBOs went away, and the account continued ticking along.</p><p>On 6 May 2025, with no notice, and with no out of the ordinary or large transactions happening, Wise started declining our card on all transactions, and blocking any ACH/wire transfers out of the account.</p><figure><img loading="lazy" src="https://hey.paris/posts/wise/wise1.png"></figure><p>There is a link on each declined transaction that says offers a way to fix it:</p><figure><img loading="lazy" src="https://hey.paris/posts/wise/wise2.png"></figure><p>Alas, it does not, and has never worked:</p><figure><img loading="lazy" src="https://hey.paris/posts/wise/wise3.png"></figure><p>Every time a transaction declines, we also get an email from Wise:</p><figure><img loading="lazy" src="https://hey.paris/posts/wise/wise4.png"></figure><p>But again, alas, the “Unblock your account” button in this email also goes to an unhelpful page:</p><figure><img loading="lazy" src="https://hey.paris/posts/wise/wise5.png"></figure><p>Lodging a support ticket yields an unhelpful response:</p><figure><img loading="lazy" src="https://hey.paris/posts/wise/wise6.png"></figure><p>And replying makes them more confusing:</p><figure><img loading="lazy" src="https://hey.paris/posts/wise/wise7.png"></figure><p>And then, more confusing still, when they ask for the same documents they already had, again:</p><figure><img loading="lazy" src="https://hey.paris/posts/wise/wise8.png"></figure><p>Calling results in someone in their call centre telling us it’s a “bug” and will be “fixed soon”, or that I need to upload more documents. And when asked what documents, they list the same document we already uploaded (an ASIC Extract). And round and round we go.</p><p>So, we’re out $60,000+ AUD, and Wise refuses to help.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The recently lost file upload feature in the Nextcloud app for Android (231 pts)]]></title>
            <link>https://nextcloud.com/blog/nextcloud-android-file-upload-issue-google/</link>
            <guid>43981170</guid>
            <pubDate>Wed, 14 May 2025 05:38:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nextcloud.com/blog/nextcloud-android-file-upload-issue-google/">https://nextcloud.com/blog/nextcloud-android-file-upload-issue-google/</a>, See on <a href="https://news.ycombinator.com/item?id=43981170">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-rocket-location-hash="29aca9a68567175916075bc4aeb63b17">
						
<p>Dear users,</p>



<p>We are aware that for some months, Nextcloud file uploads for Android users have not been working as expected: you cannot upload all data to your Nextcloud, only photos and videos. We have seen your complaints in various forums such as the Nextcloud <a href="https://help.nextcloud.com/t/unable-to-use-android-upload-due-to-new-google-policy/215499" target="_blank" rel="noreferrer noopener">help forum</a>, on <a href="https://github.com/nextcloud/android/issues/14334" target="_blank" rel="noreferrer noopener">GitHub</a>, <a href="https://www.reddit.com/r/NextCloud/comments/1iaewjm/nextcloud_and_new_google_restrictions/?rdt=64069" target="_blank" rel="noreferrer noopener"><u>Reddit</u></a>, or other <a href="https://www.computerbase.de/forum/threads/google-sabotiert-next-cloud-client-ohne-vorankuendigung.2227335/" target="_blank" rel="noreferrer noopener">forums</a>.</p>



<p>As your experience with the Nextcloud Files app for Android has worsened, we wanted to share the background. Google has revoked a critical permission to sync all files. Despite multiple appeals since mid-2024, Google has refused to reinstate it, forcing us to limit file uploads for millions of users.</p>



<p>The Nextcloud file uploads issue means that some media files, such as pictures and videos, can still be uploaded from Android devices to Nextcloud, but all other files cannot. And that is pretty much beating the purpose.</p>



<p>Google is stating security concerns as a reason for revoking the permission. This is hard to believe for us. Nextcloud has had this feature since its inception in 2016, and we have never heard about any security concerns from Google about it. Moreover, several Big Tech apps as well as Google’s own still have this. What we think: Google owning the platform means they can and are giving themselves preferential treatment.</p>



<p>Despite multiple appeals since mid-2024, Google has refused to reinstate the permission, blocking automated Nextcloud file uploads for millions of users.</p>



<p>To make it crystal clear: All of you as users have a worse Nextcloud Files client because Google wanted that. We understand and share your frustration, but there is nothing we can do.</p>



<p>The more tech-savvy of you are certainly able to use the alternative app store, such as <a href="https://f-droid.org/packages/com.nextcloud.client/" target="_blank" rel="noreferrer noopener">F-Droid</a>. But for our user base of roughly one million users on the app store, this will hardly be an option.</p>



<p>For transparency, we have compiled more background below to help you understand the Nextcloud file upload issue and how Google is abusing its gatekeeper position.</p>



<p>Sincerely, the Nextcloud team</p>


<div>
<figure><img decoding="async" width="257" height="576" src="https://nextcloud.com/c/uploads/2025/05/Nextcloud-Google-Playstore-error-message-257x576.png" alt="" srcset="https://nextcloud.com/c/uploads/2025/05/Nextcloud-Google-Playstore-error-message-257x576.png 257w, https://nextcloud.com/c/uploads/2025/05/Nextcloud-Google-Playstore-error-message-134x300.png 134w, https://nextcloud.com/c/uploads/2025/05/Nextcloud-Google-Playstore-error-message-768x1724.png 768w, https://nextcloud.com/c/uploads/2025/05/Nextcloud-Google-Playstore-error-message-684x1536.png 684w, https://nextcloud.com/c/uploads/2025/05/Nextcloud-Google-Playstore-error-message-912x2048.png 912w, https://nextcloud.com/c/uploads/2025/05/Nextcloud-Google-Playstore-error-message.png 1080w" sizes="(max-width: 257px) 100vw, 257px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20257%20576'%3E%3C/svg%3E" data-lazy-srcset="https://nextcloud.com/c/uploads/2025/05/Nextcloud-Google-Playstore-error-message-257x576.png 257w, https://nextcloud.com/c/uploads/2025/05/Nextcloud-Google-Playstore-error-message-134x300.png 134w, https://nextcloud.com/c/uploads/2025/05/Nextcloud-Google-Playstore-error-message-768x1724.png 768w, https://nextcloud.com/c/uploads/2025/05/Nextcloud-Google-Playstore-error-message-684x1536.png 684w, https://nextcloud.com/c/uploads/2025/05/Nextcloud-Google-Playstore-error-message-912x2048.png 912w, https://nextcloud.com/c/uploads/2025/05/Nextcloud-Google-Playstore-error-message.png 1080w" data-lazy-src="https://nextcloud.com/c/uploads/2025/05/Nextcloud-Google-Playstore-error-message-257x576.png"><figcaption>Error message “Changes to auto upload” from Nextcloud app for Android</figcaption></figure></div>


<h2><strong>What happened</strong> with Nextcloud file uploads for Android?</h2>



<p>The permission for read and write access<em> </em>to all file types for the Nextcloud Files app for Android was granted in 2011. In September 2024, an update of the Nextcloud app for Android was refused out of the blue. We have been asked to remove the permission to all files or use “a more privacy aware replacement” like Storage Access Framework (SAF) or MediaStore API.</p>



<p>SAF cannot be used, as it is for sharing/exposing our files to other apps, so the reviewer clearly misunderstood our app workflow. MediaStore API cannot be used as it does not allow access to other files, but only media files.</p>



<p>Despite multiple appeals from our side and sharing additional background, Google is not considering reinstating uploads for all files. Instead of working collaboratively to solve the issue, we only receive the same copy-and-paste answers or links to documentation. With nearly a million users and an 8-year history, it is hard to argue that our Android app has no credibility. So it is very surprising to get treated this way to the disadvantage of our users.</p>



<p>As we needed to release bug fixes to our users and customers, and there was no other way to discuss, we chose to comply with Google’s new regulations. Google finally accepted our newest update, which limits uploads for our users.</p>



<p>The Android app itself still works with the permission, and we released new versions on the external F-Droid store. So the limit is a “purely” Google Play Store-related problem.</p>



<h2>The bigger picture: Big Tech gatekeeping in action</h2>



<p>This might look like a small technical detail, but it is clearly part of a pattern of actions to fight the competition. What we are experiencing is a piece of the script from the Big Tech playbook.</p>



<p>It is a clear example of Big Tech gatekeeping smaller software vendors, making the products of their competitors worse or unable to provide the same services as the giants themselves sell. As they own the platform, they can — and do — give themselves preferential treatment.</p>



<p>A famous example of this in the past was Microsoft, which blocked certain capabilities from Windows to ensure WordPerfect users had a worse experience than people who picked Microsoft Word. Today, Google creates rules in the name of security that make it hard to <a href="https://nextcloud.com/blog/your-microsoft-teams-alternative-nextcloud-talk-munich-launching-live-at-the-nextcloud-summit/" target="_blank" rel="noreferrer noopener">build products that compete with them</a>.</p>



<p>We suppose Google can’t get away with this versus Apple or Microsoft, as those companies would retaliate. But smaller companies, especially those building disruptive technologies like ours, are fair game for them.</p>



<p>Big Tech is scared that small players like Nextcloud will disrupt them, like they once disrupted other companies. So they try to shut the door.</p>



<p>Then they have an army of underpaid and overworked people who have to ‘handle the complaints’. Clearly, they have either been directed to simply remove this permission from Nextcloud and ignore any complaints, or they are entirely incompetent. Either way, it results in companies like ours just giving up, reducing functionality just to avoid getting kicked out of their app store.</p>



<p>This isn’t an isolated issue. With the EU’s lack of enforcement on Microsoft’s bundling of Teams and OneDrive into Windows, it seems Google feels emboldened to follow suit, further stifling competition and innovation.</p>



<p>The issue is that small companies — like ours — have pretty much no recourse. Legal actions are too expensive, and a complaint to the EU takes too long. Together with about 40 other businesses and organizations, we filed a complaint about similar anti-competitive behavior in 2021. We are now four years in, and nothing has happened. What do you think happens to a company that releases no updates to its app in four years?</p>



<p>The current oversight processes are absolutely useless against these billion-dollar companies. Even the <a href="https://ec.europa.eu/commission/presscorner/detail/en/ip_25_1085" target="_blank" rel="noreferrer noopener">fines against Meta and Apple</a> under the Digital Markets Act (DMA) in April 2025 were surprisingly low. Remember, fines can be up to 10% of the company’s total worldwide annual revenue. While fines of €200 million or €500 million, respectively, sound a lot, it could have been in the billions. These Big Tech firms earn that much money in a matter of days, so it amounts to barely a slap on the wrist.</p>



<p>And it took a while. The regulation becomes applicable in May 2023 in the EU to make the markets in the digital sector fairer and more contestable. The first fines were announced almost two years later, which is an immense time span in the digital world, and this is just the first step. The firms will put their lawyers to work and appeal — expect this to take another year or two. The EU really has to step up its game if it is serious about reducing the anti-competitive behavior of Big Tech.</p>


<div data-rocket-location-hash="68e122c6b23b4519e49c5a2a57b5f054" id="summit">
					<p><img decoding="async" src="https://nextcloud.com/c/uploads/2025/01/NC-Summit-logo-animated-once.svg" alt="Nextcloud - Join the Nextcloud Summit 2025" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%200%200'%3E%3C/svg%3E"></p><h2>Join the Nextcloud Summit 2025</h2><p>The digital sovereignty revolution starts here. Join the discussion and hear from industry speakers, thought leaders and key experts. </p>
<p><a href="https://nextcloud.com/summit/?utm_source=hub10post" target="_blank">Register now</a>				</p></div>
						</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bus stops here: Shanghai lets riders design their own routes (336 pts)]]></title>
            <link>https://www.sixthtone.com/news/1017072</link>
            <guid>43980845</guid>
            <pubDate>Wed, 14 May 2025 04:33:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sixthtone.com/news/1017072">https://www.sixthtone.com/news/1017072</a>, See on <a href="https://news.ycombinator.com/item?id=43980845">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>From early-morning school drop-offs to seniors booking rides to the hospital, from suburban commuters seeking a faster link to the metro to families visiting ancestral graves, Shanghai is rolling out a new kind of public bus — one that’s designed by commuters, and launched only when enough riders request it.</p><p>Branded “DZ” for <em>dingzhi</em>, or “customized,” the system invites residents to submit proposed routes through a city-run platform. Others with similar travel needs can opt in or vote, and if demand meets the threshold — typically 15 to 20 passengers per trip — the route goes live.</p><p>More than 220 DZ routes have already launched across all 16 city districts. Through an online platform opened May 8, users enter start and end points, preferred times, and trip frequency. If approved, routes can begin running in as little as three days.</p><p>One of the first test cases was DZ301, a pilot route linking a major metro station with surrounding residential blocks, schools, and office buildings. “The average daily passenger flow is 250 to 260 people — 170 to 180 during the morning peak and 70 to 80 in the evening,” Wu Yongming, deputy manager at Jiushi Bus Company,&nbsp;<a href="https://www.thepaper.cn/newsDetail_forward_30436791" target="_blank">told</a>&nbsp;domestic media.</p><p>The route originated from a resident’s request submitted last December. In response, transit staff conducted on-site research, observing foot traffic, speaking with commuters, and calculating turnover times during peak hours. Drivers then ran trial runs to fine-tune the schedule before the route officially launched.</p><p>Chen Xiaohong, a professor at Tongji University’s School of Transportation, said the system builds on Shanghai’s dense transit network to better match capacity with demand, improving both convenience and resource use during peak travel.</p><p>Proposed routes appear on a “Popular Customization” page, where others can opt in to help reach the launch threshold. Group bookings can also fast-track approval. Fares are market-based, and while they follow basic public transit standards, no discounts are currently offered for students, seniors, or other groups.</p><p>Wang Yixiang, deputy director of the city’s Passenger Transport Department,&nbsp;<a href="http://www.sh.xinhuanet.com/20250509/cef13b1418e547ea93ff0ca0767de033/c.html" target="_blank">said</a>&nbsp;the new platform shortens what was once a slow, bureaucratic process for launching new routes. But he acknowledged early challenges: passenger demand is uneven, public awareness remains low, and planning still relies heavily on manual fieldwork.</p><p>“Going forward, we need to improve route planning, upgrade platform functions, and boost visibility,” Wang said.</p><p><em>(Header image: A “DZ” bus arrives at a residential area in Shanghai, 2025. From Weibo)</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing that changed how I think about programming languages (286 pts)]]></title>
            <link>https://bernsteinbear.com/blog/pl-writing/</link>
            <guid>43980760</guid>
            <pubDate>Wed, 14 May 2025 04:19:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bernsteinbear.com/blog/pl-writing/">https://bernsteinbear.com/blog/pl-writing/</a>, See on <a href="https://news.ycombinator.com/item?id=43980760">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        

        
        
        <p><i>May 13, 2025</i></p>
        
        
        <div>
            <p>Every so often I come across a paper, blog post, or (occasionally) video that
completely changes how I think about a topic in programming languages and
compilers. For some of these posts, I can’t even remember how I thought about
the idea <em>before</em> reading it—it was that impactful.</p>

<p>Here are some of those posts in no particular order:</p>

<ul>
  <li><a href="https://wingolog.org/archives/2022/12/10/a-simple-semi-space-collector">a simple semi-space collector</a> by Andy Wingo brought the concept
of a Cheney/copying/compacting garbage collector from theory to practice for
me. The garbage collector core<sup id="fnref:gc-bug" role="doc-noteref"><a href="#fn:gc-bug" rel="footnote">1</a></sup> in the post is tiny, extensible, and
can be understood in an afternoon.</li>
  <li><a href="https://pypy.org/posts/2022/07/toy-optimizer.html">Implementing a Toy Optimizer</a> by CF Bolz-Tereick changed how
I think about instruction rewrites in an optimizer. No more find-and-replace.
Instead, use a forwarding pointer! I love union-find. The whole toy optimizer
series is fantastic: each post brings something new and interesting to the
table.</li>
  <li><a href="https://pypy.org/posts/2024/08/toy-knownbits.html">A Knownbits Abstract Domain for the Toy Optimizer, Correctly</a> by
CF Bolz-Tereick is a two-for-one. It both introduced me to a new abstract
domain for optimizers and changed how I think about Z3. Before, I vaguely
knew about Z3 as this thing that can check numeric operations, kind of. The
post, however, uses Z3 as a proof engine: if Z3 can’t find a counterexample,
the code is correct<sup id="fnref:correctness" role="doc-noteref"><a href="#fn:correctness" rel="footnote">2</a></sup>. Furthermore, it uses Z3 as a verifier <em>for
Python code</em> by using the same Python code on both Z3 objects and normal
Python objects.</li>
  <li><a href="https://cfallin.org/blog/2021/03/15/cranelift-isel-3/">Cranelift, Part 3: Correctness in Register Allocation</a> by
Chris Fallin also made proofs more accessible, but in a different way.
Instead of proving your register allocator correct on <em>all</em> inputs, prove it
correct on one input: the current code. This means that in production, you
either get a correct register allocation or a meaningful crash. Additionally,
it uses fuzzing as a state space exploration tool that can identify bugs by
making the verifier fail.</li>
  <li><a href="https://swtch.com/~rsc/regexp/regexp2.html">Regular Expression Matching: the Virtual Machine Approach</a> by Russ
Cox made regular expression engines make sense to me. In the blog post is a
small regular expression engine in under 50 lines of readable code. As a side
effect, it also made coroutines/fibers/schedulers more understandable because
its implementation of nondeterminism makes regex “threads” in user-space.</li>
  <li><a href="https://github.com/karpathy/micrograd">micrograd</a> by Andrej Karpathy is a tiny implementation of neural
networks with no libraries (not even NumPy). It’s how I came to understand
machine learning and even write some blog posts about it.</li>
  <li><a href="https://gist.github.com/pizlonator/cf1e72b8600b1437dda8153ea3fdb963">How I implement SSA form</a> by Fil Pizlo changed how I think about
union-find. Instead of either storing an additional pointer inside every
object or having a side-table, add an <code>Identity</code> tag that can store the
pointer. Unlike the other two approaches, this is a destructive rewrite, but
it saves space. It also introduces two new things that I am still mulling
over: Phi/Upsilon form and TBAA-style heap effects.</li>
  <li><a href="https://webkit.org/blog/10308/speculation-in-javascriptcore/">Speculation in JavaScriptCore</a> is another absolute banger
by Fil Pizlo. I learn new things every time I re-read this post, which
describes how JSC’s various optimizers work (and I guess how Fil thinks about
optimizers).</li>
  <li><a href="https://www.youtube.com/watch?v=ZI198eFghJk">Modernizing Compiler Design for Carbon Toolchain</a> is a talk by
Chandler Carruth on the design of the Carbon compiler. About 29 minutes into
it Chandler sets <em>incredibly aggressive</em> compile-time budgets and explains
how the compiler is architected to fit in that time budget. At around 40
minutes, he starts explaining per-layer how this works, starting at the
lexer.</li>
  <li><a href="https://aosabook.org/en/500L/a-python-interpreter-written-in-python.html">A Python Interpreter Written in Python</a> by Allison Kaptur
made bytecode interpreters (and, specifically, how CPython works internally)
click for me.</li>
  <li><a href="https://eli.thegreenplace.net/2012/08/02/parsing-expressions-by-precedence-climbing">Parsing expressions by precedence climbing</a> by Eli
Bendersky presented an understandable and easier-to-develop alternative to
traditional hand-written recursive descent parsers. It both made parsers less
scary to me and also at the same time illustrated how precedence climbing is
the same algorithm as recursive descent but instead of having 10 different
functions with similar structure, it uses one function and a table.</li>
  <li><a href="https://github.com/k0kubun/ruby-jit-challenge">Ruby JIT Challenge</a> by Takashi Kokubun is a great start
to code generation and it is more general than JIT, too. In the post he also
shows off a cool approach to register allocation that I had not seen before:
fold your stack operations at compile-time to operate on a virtual stack of
physical registers.</li>
  <li><a href="https://bernsteinbear.com/assets/img/11-ghuloum.pdf">An Incremental Approach to Compiler Construction</a>
(PDF) by Abdulaziz Ghuloum brought compilers and code generation from a
mystical multi-pass beast down to an understandable single-pass form. I like
how it implements each feature end-to-end, one by one.</li>
  <li><a href="https://borretti.me/article/lessons-writing-compiler">Lessons from Writing a Compiler</a> by Fernando Borretti puts
into words this stripey implementation strategy in the first section, which
is really neat.</li>
  <li><a href="https://dl.acm.org/doi/10.1145/3434304">egg: Fast and extensible equality saturation</a> by [multiple authors]
changed how I think about optimizers and pass ordering. Why <em>not</em> just
generate the compressed hypergraph of all possible versions of an expression
and then pick the “best” one? I mean, there are a couple of reasons, but it
was mind-expanding.</li>
  <li><a href="https://github.com/bytecodealliance/rfcs/blob/main/accepted/cranelift-egraph.md">Cranelift: Using E-Graphs for Verified, Cooperating Middle-End
Optimizations</a> by Chris Fallin showed that e-graphs are
workable and remarkably effective in a production compiler, even if you don’t
do the “full shebang”.</li>
  <li><a href="https://www.philipzucker.com/smart_constructor_aegraph/">Acyclic Egraphs and Smart Constructors</a> by Phil Zucker further
explores acyclic egraphs in the small. This one was a slow burn: I read it
when it came out, didn’t really get it, and then months later it clicked
more. I’m sure that in a few months it’ll click again.</li>
  <li><a href="https://old.reddit.com/r/ProgrammingLanguages/comments/mrifdr/treewalking_interpreters_and_cachelocality/gumsi2v/">This Reddit comment</a> by Bob Nystrom and <a href="https://www.cs.cornell.edu/~asampson/blog/flattening.html">Flattening
ASTs</a> by Adrian Sampson are a double whammy that led to two
things: 1) AST storage can be really compact, almost like bytecode 2) a
really interesting discussion on Mastodon about massively-parallel lock-free
abstract interpretation if IRs were stored like this. This (and some off-hand
comments by Cliff Click about bump-allocation only taking “a few clocks”)
changed how I think about allocating and referring to IR nodes.</li>
</ul>

<p>I hope you enjoy the readings!</p>


        </div>
            
    <!-- Workaround for FB MITM -->
    <!-- Google tag (gtag.js) -->







    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I failed a take-home assignment from Kagi Search (220 pts)]]></title>
            <link>https://bloggeroo.dev/articles/202504031434</link>
            <guid>43980036</guid>
            <pubDate>Wed, 14 May 2025 02:17:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bloggeroo.dev/articles/202504031434">https://bloggeroo.dev/articles/202504031434</a>, See on <a href="https://news.ycombinator.com/item?id=43980036">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="content">
            

<p>2025, May 13</p>
<hr>
<h2>Preface</h2>
<ul>
<li>A "take-home assignment" in the world of Software Developer interviews, is a challenge sent to candidates for them to implement a code solution that solves the challenge.</li>
<li>In this blog post I describe my experience doing a "take-home assignment" for a company that I deemed reputable, and I hope to shine some light on the absurdity of this practice. I will show you what kind of requirements are asked of candidates, for free, without any remorse towards wasting their time.</li>
</ul>
<h2>I am searching for a job, so I sent my CV</h2>
<p>I sent my CV for a role at Kagi Search, the description of the role can be found here:</p>
<ul>
<li><a href="https://archive.md/tRsED">https://archive.md/tRsED</a></li>
</ul>
<p>The summary in that link would be:</p>
<pre><code>Requirements

- Proven experience in building backend systems
- Proficiency in Go
- Strong understanding of scaling and maintaining backend systems
- Ability to collaborate effectively with SREs and other team members.
- Solid understanding of containerization technologies like Docker
</code></pre>
<h2>My application was answered</h2>
<p>So I got an email, saying that I was selected for a take-home assignment. Something like this:</p>
<blockquote>
<p>Thank you for your interest in working at Kagi. We were impressed by your application and would like to move forward with the next stage of our selection process.</p>
<p>As part of our evaluation, we'd like you to complete the Kagi Developer Assessment that will showcase your skills and approach to problem-solving.&nbsp;</p>
<p>It can be found in the following URL:&nbsp; <a href="https://hackmd.io/@antonios/SkeEpQeh1l">https://hackmd.io/@antonios/SkeEpQeh1l</a>
Once you complete it, please reach out and we'll review it and, if successful, invite you for a follow-up interview where we'll discuss your approach and solution in detail.</p>
</blockquote>
<p>The URL for the take-home is archived here: <a href="https://archive.md/A95Ju">https://archive.md/A95Ju</a>. I copy-pasted the requirements into the next paragraph so you don't have to open the link.</p>
<h2>The take-home</h2>
<blockquote>
<p>The project is to build a minimal, terminal-inspired email client, with the following requirements:</p>
<ul>
<li>Email client can either be in the terminal (i.e. a TUI) or a web app</li>
<li>Should have basic email viewing + sending functionality</li>
<li>Can use a fake backend (DB, in-memory, etc) or real IMAP/POP/JMAP/etc backend</li>
<li>Does <strong>not</strong> have to handle rich text messages, just plaintext</li>
</ul>
</blockquote>
<blockquote>
<p>This project tests your ability not only to code, but to deal with ambiguity and open-endedness, which is essential in R&amp;D projects like Kagi Labs.</p>
</blockquote>
<blockquote>
<ul>
<li>Do the project in a way that shows off your skills as a developer</li>
<li>Deliverable is the completed project, in a GitHub repo and deployed anywhere so we can easily test it.</li>
<li>Create a README file with setup instructions</li>
</ul>
</blockquote>
<blockquote>
<p>And don't hesitate to tell us if you have any questions!</p>
</blockquote>
<p>These instructions are extremely broad, and the size of the project is quite large too.</p>
<p>Usually I turn down these kind of unpaid homework, but I had heard about Kagi Search as a reputable company, so I got rid of my inhibitions and bit the bullet.</p>
<h2>Communication with Hiring Manager</h2>
<p>Because the instructions are so broad, I proceeded to deliver some questions over email to the Hiring Manager. I got answers such as:</p>
<blockquote>
<p>We always have a lot of candidates, some they do the basic, some provide a lot of extra features, great documentation and decision making explanations, deployment for demoing, and future thoughts on what would be next. From hiring point of view, we prefer stronger assessments.</p>
</blockquote>
<p>to which I asked:</p>
<blockquote>
<p><strong>Q:</strong> What <strong>kind</strong> of extra feature do you value highly?  <strong>'kind</strong>' meaning something like '<strong>UX improvement</strong>', or '<strong>pretty UI</strong>', or '<strong>privacy related</strong>', or '<strong>something that showcases a lot of complexity</strong>'.</p>
</blockquote>
<p>to which he replied:</p>
<blockquote>
<p>That is part of the assessment itself, see what kind of extra features you can come up if any.</p>
</blockquote>
<p>.... that's very little information to work with, so I came up with a strategy. I would write an implementation plan detailing the entire deliverable before actually writing the code, if my proposal is accepted that means it should be good enough to get a phone interview, maybe even an offer, right? Nope. Despite my efforts to agree on all these terms on email before investing an entire week of full-time effort, it seems none of my efforts meant anything to the Hiring Manager.</p>
<h2>My proposal</h2>
<p>A summary of my proposal is as follows:</p>
<blockquote>
<p>A webapp with golang accessible online, deployed through AWS using ECS Fargate, with SSL (https), integrated with an email sender provider, with authentication through a login screen, with the ability to send emails through a form, and displaying incoming emails in the user interface.</p>
</blockquote>
<p>Below I include the email that I sent to the manager with the detailed proposal, this is a long one, so scroll down to the next header if you prefer:</p>
<hr>
<p>Hi Antonio De Dios,</p>
<p>It is me again :)</p>
<p>I hope it is ok that I keep sending you emails, it's just that I am taking the assessment very seriously.</p>
<p>I have done a little research and I want to offer a proposal of features for the <strong>deliverable</strong>. I also want to ask for a bit more time, as this is the first day in which I found time for the project, and this weekend I will not be available to work on it.</p>
<p>So here is my proposal:</p>
<p><strong>Delivery date: Sunday EOD March 30,</strong></p>
<p>This is two business days later than the two-week mark since you sent me your first email.</p>
<p><strong>Deliverable:</strong></p>
<p>Email client, deployed to "the cloud" (AWS), sends and receives emails through an email provider such as Postmark, written in Golang with a web UI, leveraging APIs and libraries to work efficiently.</p>
<p><strong>Reasoning:</strong></p>
<ul>
<li>Even though this is a backend role, adding a web frontend displays a breadth of knowledge about web technology that remains relevant for backend development. The solution will also include a database, a classic piece of backend roles.</li>
<li>Deploying to AWS with Infra-as-Code tooling, not part of the assessment requirements, but most definitely relevant to the job description due to: IaC tooling, Docker tooling, networking concepts, resiliency. Additionally, this makes it much easier to look at the final result, as opposed to locally building some code. And the infra implementation makes for good interview conversation.</li>
<li>Using an email service such as Postmark simplifies the stack without sacrificing functionality. This kind of implementation remains relevant for many real-world applications for various reasons, such as protecting senders from bounced emails. It is also relevant in the sense that integrating APIs is the bread-and-butter of backend development.</li>
<li>While writing the solution in Golang makes it relevant to the job, I will also pick a combination of Golang libraries and frameworks to deliver the result in an efficient manner.</li>
</ul>
<p><strong>Specs:</strong></p>
<p>These are the implementation details that I have settled on using for this given solution:</p>
<ul>
<li>
<p>For the Golang backend I will be using the following libraries:</p>
<ul>
<li><a href="https://pocketbase.io/">Pocketbase</a>, a backend framework that leverages SQLite as the database.</li>
<li><a href="https://templ.guide/">TEMPL</a>, an HTML templating library for golang. This keeps the work on the backend, which helps with speed and avoids going into too much frontend, which would be more relevant for a frontend role (avoids SPA/JavaScript).</li>
</ul>
</li>
<li>
<p>For the Infra-as-code I will use <a href="https://www.pulumi.com/">Pulumi</a>. For Pulumi I will use the TypeScript SDK because it offers the most brevity in configuration.</p>
</li>
<li>
<p>For the Email Service Provider, I will use <a href="https://postmarkapp.com/">Postmark</a>, this allows me to manage email easily with the usage of Postmark's API and my own database. This circumvents the usage of IMAP/POP, whilst useful standard protocols, a lot of complexity is involved in integrating them properly.</p>
</li>
<li>
<p>The UI will be kept simple, showing pagination for sent and received emails. In addition to the requirements of the assessment, there will be a login screen and two accounts will be provided for the demo.</p>
</li>
</ul>
<p><strong>Stretch goals:</strong>
These are goals which I would like to meet, but I cannot promise due to the time constraint:</p>
<ul>
<li>Backups and resiliency:
<ul>
<li>In the scenario of the service going offline for whatever reason, it would be great if the service is restarted without losing data.</li>
</ul>
</li>
</ul>
<p><strong>CONCLUSION</strong></p>
<p>This is my proposal for the assessment. Coming up with the proposal has taken me some time and I would like to know what kind of response I could expect from Kagi if I drive it to completion. Looking forward to your answer!</p>
<p>Kind regards,</p>
<p>Jose</p>
<hr>
<p>That is where my email ends.</p>
<h2>The Hiring Manager's answer to my proposal</h2>
<p>The answer was so low effort, that it should have given me a clue as to the lack of seriousness from the manager, but I tried to view it in a positive light. I realize now, I am naive. So this was the reply I got:</p>
<blockquote>
<p>Hi Jose,&nbsp;
This is all very exciting. Looking forward to receiving your submission.
Thanks for keeping me updated.</p>
</blockquote>
<p>You will notice that my question went completely ignored. I asked:</p>
<ul>
<li>"I would like to know what kind of response I could expect from Kagi if I drive it to completion."</li>
</ul>
<p>All I got was:</p>
<ul>
<li>"This is all very exciting"</li>
</ul>
<p>Still, this is somewhat of a positive answer, correct? Well, I would have much rather preferred if my proposal had been rejected right there, and my time had been respected.</p>
<p>After all, this is plenty of information to determine whether the solution will be good enough to move me forward into a role.</p>
<h2>My solution</h2>
<p>I delivered on all promises. It took me an entire week of full-time effort. This is an extreme commitment from my side, but I assumed that nailing the take-home would put me very close to a good job.</p>
<p>If you want to look at a demo of the web application, use this youtube link:</p>
<ul>
<li><a href="https://youtu.be/yY1sVXMkP_o">https://youtu.be/yY1sVXMkP_o</a></li>
</ul>
<p>If you want to look at the code for the solution, with very extensive documentation, use this GitHub link:</p>
<ul>
<li><a href="https://github.com/Sleepful/mymail">https://github.com/Sleepful/mymail</a></li>
</ul>
<h2>The rejection email</h2>
<p>After getting an automated rejection email, I asked for feedback, to which I got this email:</p>
<blockquote>
<p>We normally don’t provide feedback at this stage.&nbsp;
We have had other submissions that were simpler and stronger, &nbsp;so we decided to continue forward with these candidates.&nbsp;
We receive a lot of interest and applications for each position at Kagi which makes selection processes is extremely competitive.
We value you as a user and as job candidate and would like to invite you to keep an eye on future positions and continue applying.</p>
</blockquote>
<p>Really?</p>
<ol>
<li>If he wanted a simpler solution, he could have told me on <em><strong>March 18</strong></em>, when I submit my proposal.</li>
<li>If my solution was weak for the role, he could have told me on <em><strong>March 18</strong></em>, also when I submit my proposal. There is no way the judgement changed between the proposal submission and <em><strong>March 31</strong></em>, when I submitted the deliverable deployed for demoing.</li>
<li>It is <em><strong>May 13</strong></em> right now, a month and a half after I got rejected and the job advertisement is still up. I understand that companies are using the same advertisement to fill multiple roles, but clearly this isn't some kind of competition where the winner seats are taken, they are still advertising the seats and it is anyone's guess how many people have gone through the same trick for an imaginary prize.</li>
<li>The original project instructions mentioned that deploying the project for demoing would be nice, but not a required step. After I made my submission, the instructions changed to make it a strict requirement for the project to be deployed. So it is funny that my project is so weak, yet it made them update the guidelines to something stricter. Go figure.</li>
</ol>
<h2>Concluding thoughts</h2>
<p>This is sad for all applicants as a whole, many whom are unemployed and are on a timer towards bankruptcy. Imagine if you were unemployed, your savings are only going to last you so long, and companies demand that you spend the little time you have left on unpaid work. But what choice do we have once we are in a desperate position? Perhaps next time I am asked for one of these unpaid assignments I will link them to this blog, although you have to understand that if someone is going hungry or facing eviction, they don't have much choice in the matter.</p>
<h2>Can we have better selection processes?</h2>
<p>I definitely believe that we can. I also dislike leetcode-style interviews, the programming puzzles that require the candidate to solve a problem during a video-call. This week I failed a "<a href="https://en.wikipedia.org/wiki/Change-making_problem">coin change</a>" problem, well I passed about half of the tests in less than 50 minutes, but I did not have enough time to implement the <a href="https://en.wikipedia.org/wiki/Backtracking">backtracking</a> part of the solution with <a href="https://en.wikipedia.org/wiki/Clojure">Clojure</a>, a programming language that I learned as a hobby and also spent an entire week practicing for this Clojure-based role.</p>
<p>Are there no other options? Of course there are. One such example would be to do a live code review. This could be done asynchronously or synchronously. It could allow actually talking through topics and issues that relate to the challenges in a real software project. This would allow to surface much more of the knowledge in an experienced software engineer. As opposed to the leetcode-style interviews which select for people that memorized some canned leetcode problem well enough to be really efficient at writing the exact code to solve it.</p>
<p>I am not completely against live coding, it is a tool to determine basic coding literacy, but does it make someone a better programmer if they can solve <a href="https://en.wikipedia.org/wiki/Knapsack_problem">knapsack</a> in 50 minutes instead of 2 hours? Heck, I don't think it matters if someone takes longer solving it. Solving these code puzzles under a timer is far removed from the day-to-day responsibilities of an engineer working on an old or new software product.</p>
<p>Lets do better, and please, unless you are in a desperate position, start rejecting roles that ask for this kind of unpaid work. Make it so that they lose all the top talent.</p>
<p>Thanks.</p>
<hr>

<!-- START OF AD -->
<div id="mc_embed_signup">
  <h2>I am looking for a job</h2>
  <p>
    Whether part-time or full-time. You can contact me at <code>hire2025@jose.cloud</code><br> Just keep in mind that I work from my home country Costa Rica.
  </p>
</div>

<!-- START OF MAILCHIMP EMBED -->

<!-- END OF MAILCHIMP EMBED -->
<hr>
<a href="#">Go back to the top</a>
<hr>

          </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Replicube: A puzzle game about writing code to create shapes (105 pts)]]></title>
            <link>https://store.steampowered.com/app/3401490/Replicube/</link>
            <guid>43979916</guid>
            <pubDate>Wed, 14 May 2025 01:55:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://store.steampowered.com/app/3401490/Replicube/">https://store.steampowered.com/app/3401490/Replicube/</a>, See on <a href="https://news.ycombinator.com/item?id=43979916">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						
		
		

		



					<div>
					<p><img src="https://store.cloudflare.steamstatic.com/public/shared/images/responsive/header_menu_hamburger.png" height="100%">
											</p>
					<p><a href="https://store.steampowered.com/?snr=1_5_9__global-responsive-menu">
															<img src="https://store.cloudflare.steamstatic.com/public/shared/images/responsive/header_logo.png" height="36" alt="STEAM">
													</a>
					</p>
				</div>
		
		

		
	
	<div>

		<div role="banner" id="global_header" data-panel="{&quot;flow-children&quot;:&quot;row&quot;}">
		<p><span id="logo_holder">
									<a href="https://store.steampowered.com/?snr=1_5_9__global-header" aria-label="Link to the Steam Homepage">
						<img src="https://store.cloudflare.steamstatic.com/public/shared/images/header/logo_steam.svg?t=962016" width="176" height="44" alt="Link to the Steam Homepage">
					</a>
							</span>
		</p>

			
	

		
			</div>

		<div itemscope="" itemtype="http://schema.org/Product" id="responsive_page_template_content" data-miniprofile-appid="3401490" role="main" data-panel="{&quot;autoFocus&quot;:true}">
		
		<meta itemprop="image" content="https://shared.cloudflare.steamstatic.com/store_item_assets/steam/apps/3401490/52069d1daf91967c3a9dee61c5af1f78c4dc2eaa/capsule_231x87.jpg?t=1746989726">
					
		
		<div data-gpnav="columns">
		                      
         		<div><p><img src="https://cdn.fastly.steamstatic.com/steamcommunity/public/images/apps/3401490/e5e3c799f5a1f836f3831c4e29623920ec5ba6e3.jpg"></p></div>
		<p>Replicube</p>
		

	</div>
		


		

		

				
				<div data-panel="[]">

					
					
					
					
					
					<div id="game_area_purchase">
			
			<h2>
				Download Replicube Demo			</h2>
						
		</div>
					<!-- game_area_purchase -->

					
											
					
					

					
					
																								
					

					
				
								
				
				


									
									<div id="aboutThisGame" data-panel="{&quot;type&quot;:&quot;PanelGroup&quot;}">
							<h2>About This Game</h2>
							<p>Replicube is an open-ended programming puzzle game/toy about writing code to replicate 3D voxel-based objects.</p><h2>SOLVE PUZZLES</h2><p><img src="https://shared.cloudflare.steamstatic.com/store_item_assets/steam/apps/3401490/extras/rc-steam-gif-camera-smaller.gif.gif?t=1746989726"></p><p>The main content of the game involves trying to match a reference object by working out code that will replicate it.&nbsp; There is no "right answer", if your code produces the same object, it's correct!</p><h2>PLAY AROUND</h2><p><img src="https://shared.cloudflare.steamstatic.com/store_item_assets/steam/apps/3401490/extras/rc-steam-gif-tv-smaller.gif.gif?t=1746989726"></p><p>You can always open the voxel tool in "free edit" mode and just play around, making whatever you want.&nbsp; In addition to the main 3D voxel editor, there is also a bonus 2D image editor for writing code to generate 2D images and GIF animations.&nbsp; You can even save your image creation as the background image in the "OS" interface of the game!</p><h2>JOIN THE COMMUNITY</h2><p><img src="https://shared.cloudflare.steamstatic.com/store_item_assets/steam/apps/3401490/extras/rc-steam-gif-histogram.gif?t=1746989726"></p><p>Every puzzle has 2 leaderboards, measuring source code size, and execution efficiency.&nbsp; Often optimizing for one will be at the expense of the other.&nbsp; If you enjoy trying to squeeze a<i> bit more</i>&nbsp;out of your code, the leaderboards are waiting!</p><p><img src="https://shared.cloudflare.steamstatic.com/store_item_assets/steam/apps/3401490/extras/replicube-steam-community.gif.gif?t=1746989726"></p><p>There is also an in-game online forum where players can share their own voxel creations, and even challenge other players to try to recreate them, all presented in an old-school online forum wrapper.</p><h2>EXPORT YOUR CREATIONS</h2><p>Generated 3D voxel objects can be exported into common formats to bringing into other 3D creation tools.</p><p>Generated 2D images and animations can also be exported as png or gif for sharing online.</p>						</div>
				
				
				
					<div>
		<h2>System Requirements</h2>
					<div>
									<p>
						Windows					</p>
									<p>
						macOS					</p>
									<p>
						SteamOS + Linux					</p>
								
			</div>
				<div>
							<div data-os="win">
											<div>
							<ul>
								<strong>Minimum:</strong><br><ul><li>Requires a 64-bit processor and operating system<br></li><li><strong>OS *:</strong> Windows 7 or newer<br></li><li><strong>Processor:</strong> x86_32 CPU with SSE2 instructions, x86_64 CPU<br></li><li><strong>Memory:</strong> 4 GB RAM<br></li><li><strong>Graphics:</strong> integrated graphics with full Vulkan 1.0 support<br></li><li><strong>Storage:</strong> 300 MB available space</li></ul>							</ul>
						</div>
																<div>
							<ul>
								<strong>Recommended:</strong><br><ul><li>Requires a 64-bit processor and operating system</li></ul>							</ul>
						</div>
										

											<p><strong>*</strong>
							Starting January 1st, 2024, the Steam Client will only support Windows 10 and later versions.						</p>
									</div>
							<div data-os="mac">
											<div>
							<ul>
								<strong>Minimum:</strong><br><ul><li><strong>OS:</strong> macOS 10.15 or newer<br></li><li><strong>Processor:</strong> x86_64 or ARM CPU (Apple Silicon)<br></li><li><strong>Memory:</strong> 4 GB RAM<br></li><li><strong>Graphics:</strong> integrated graphics with full Vulkan 1.0 support<br></li><li><strong>Storage:</strong> 300 MB available space</li></ul>							</ul>
						</div>
																<div>
							<ul>
								<strong>Recommended:</strong><br><ul></ul>							</ul>
						</div>
										

									</div>
							<div data-os="linux">
											<div>
							<ul>
								<strong>Minimum:</strong><br><ul><li><strong>OS:</strong> Distribution released after 2016<br></li><li><strong>Processor:</strong> x86_32 CPU with SSE2 instructions, x86_64 CPU<br></li><li><strong>Memory:</strong> 4 GB RAM<br></li><li><strong>Graphics:</strong> integrated graphics with full Vulkan 1.0 support<br></li><li><strong>Storage:</strong> 300 MB available space</li></ul>							</ul>
						</div>
																<div>
							<ul>
								<strong>Recommended:</strong><br><ul></ul>							</ul>
						</div>
										

									</div>
					</div>
	</div>
	

				
								
				</div>

		


		<div id="app_reviews_hash">
							<h2>Customer reviews for Replicube</h2>

		
				<div id="review_histograms_container">
			<canvas id="review_graph_canvas"></canvas>
			<div id="review_histogram_rollup_section">
						<p>Overall Reviews:</p>
						<p><span data-tooltip-html="97% of the 310 user reviews for this game are positive.">Very Positive</span>
													<span>(310 reviews)</span>
												<a data-tooltip-text="This summary uses only reviews written by customers that purchased the game directly from Steam."><img src="https://store.cloudflare.steamstatic.com/public/shared/images/ico/icon_questionmark.png"></a>
					</p></div><!--
			-->
		</div>

		<div id="reviews_filter_options">
			<div>
				<p>Review Type</p>
				<div>
					<p>
						<label for="review_type_all">All&nbsp;<span>(315)</span></label><br>
						
						<label for="review_type_positive">Positive&nbsp;<span>(308)</span></label><br>
						
						<label for="review_type_negative">Negative&nbsp;<span>(7)</span></label>
					</p>
				</div>
			</div>
			<div>
				<p>Purchase Type</p>
				<div>
					<p>
						<label for="purchase_type_all">All&nbsp;<span>(315)</span></label><br>
						
						<label for="purchase_type_steam">Steam Purchasers&nbsp;<span>(310)</span> <a data-tooltip-text="These are reviews written by customers that purchased the game directly from Steam."><img src="https://store.cloudflare.steamstatic.com/public/shared/images/ico/icon_questionmark_dark.png"></a></label><br>
						
						<label for="purchase_type_non_steam">Other&nbsp;<span>(5)</span> <a data-tooltip-text="These are reviews written by customers that did not purchase the game on Steam. (This may include legitimate sources such as other digital stores, retail stores, testing purposes, or press review purposes. Or, from inappropriate sources such as copies given in exchange for reviews.)"><img src="https://store.cloudflare.steamstatic.com/public/shared/images/ico/icon_questionmark_dark.png"></a></label>
					</p>
				</div>
			</div>
						<div>
				<p>Language</p>
				<div>
						<p>
						<label for="review_language_all">All Languages&nbsp;<span>(315)</span></label><br>
						
						<label for="review_language_mine">Your Languages&nbsp;<span>(260)</span> <a data-tooltip-html="Your preferences are currently set to show content authored in these languages: English.<br><br> Click 'customize' below to modify your preferences."><img src="https://store.cloudflare.steamstatic.com/public/shared/images/ico/icon_questionmark_dark.png"></a></label></p>
					</div>
			</div>
						<div id="reviews_date_range_menu">
				<p>Date Range</p>
				<div>
						<div><p>
							To view reviews within a date range, please click and drag a selection on a graph above or click on a specific bar.							</p><p>
							<span onclick="SetReviewsGraphVisibility( true ); "><span>Show graph</span></span></p></div>
						<p>
						<label for="review_date_range_all">Lifetime</label><br>
						
						<label for="review_date_range_histogram">Only Specific Range (Select on graph above)&nbsp;</label><br>
						
						<label for="review_date_range_exclude_histogram">Exclude Specific Range (Select on graph above)&nbsp;</label><br>
					</p></div>
			</div>
						<div>
				<p>Playtime</p>
				<div>
						<p>
							Filter reviews by the user's playtime when the review was written:						</p>

													<p>
							<label for="review_playtime_preset_0">No Minimum</label><br>
														
							<label for="review_playtime_preset_1">Over 1 hour</label><br>
														
							<label for="review_playtime_preset_10">Over 10 hours</label></p><p><span id="app_reviews_playtime_range_text_min">No minimum</span> to <span id="app_reviews_playtime_range_text_max">No maximum</span>
						</p>
						

						<p>
							<label for="review_playtime_type_all">Played across all devices</label><br>
							
							<label for="review_playtime_type_deck">Played mostly on Steam Deck</label>
						</p>

					</div>
			</div>
						<div>
				<p>Display</p>
				<div>
						<p>
							Show reviews in selected display order						</p>

						<p>
						<label for="review_context_summary">Summary</label><br>
						
						<label for="review_context_most_helpful">Most Helpful</label><br>
						
						<label for="review_context_recent">Recent</label><br>
						
						<label for="review_context_funny">Funny</label></p><p><label>&nbsp;Use new helpfulness system. Only applies to Summary and Most Helpful views.</label><br>
							<a href="https://store.steampowered.com/news/app/593110/view/4326355263805583415?snr=1_5_9_">Learn More</a>						</p>
											</div>
			</div>

			

			<p><span id="review_show_graph_button" onclick="SetReviewsGraphVisibility( true ); "><span>Show graph</span> </span>
				<span id="review_hide_graph_button" onclick="SetReviewsGraphVisibility( false ); "><span>Hide graph</span> </span>
			</p>

			
		</div>

		<div id="reviews_active_filters" data-panel="{&quot;focusable&quot;:true,&quot;clickOnActivate&quot;:true}" onclick="ShowReviewSettingsModal();">
				<p>Filters</p>

								
								
				
				
				
				<p>Excluding Off-topic Review Activity</p>
				<p>Playtime: <span id="review_playtime_preset_text"></span></p>
				<p>Played Mostly on Steam Deck</p>
			</div>

		

		
		
		
		
		


		
		
		
		
		
		<div id="Reviews_summary">
						<p>There are no more reviews that match the filters set above</p>
						<p>Adjust the filters above to see other reviews</p>
						
					</div>
		
		

						</div>

					
		

			</div>	<!-- responsive_page_legacy_content -->

		


	</div>	<!-- responsive_page_content -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fingers wrinkle the same way every time they’re in the water too long (156 pts)]]></title>
            <link>https://www.binghamton.edu/news/story/5547/do-your-fingers-wrinkle-the-same-way-every-time-youre-in-the-water-too-long-new-research-says-yes</link>
            <guid>43979063</guid>
            <pubDate>Tue, 13 May 2025 23:34:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.binghamton.edu/news/story/5547/do-your-fingers-wrinkle-the-same-way-every-time-youre-in-the-water-too-long-new-research-says-yes">https://www.binghamton.edu/news/story/5547/do-your-fingers-wrinkle-the-same-way-every-time-youre-in-the-water-too-long-new-research-says-yes</a>, See on <a href="https://news.ycombinator.com/item?id=43979063">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="story-body">
	  
	  
	  <p>Sometimes it takes a kid to ask a question no one has considered before.</p>


			

<p>A couple of years ago, Binghamton University Associate Professor <a href="https://www.binghamton.edu/biomedical-engineering/people/profile.html?id=ggerman">Guy German</a> published research about why human skin wrinkles when you stay in the water too long. Received wisdom held that the water swelled your skin and made your fingers wrinkly, but little to no research had been done to prove that.</p>

<p>What German and his team at the Biological Soft Matter Mechanics Laboratory found is that blood vessels beneath the skin actually contract after prolonged immersion, and that’s where the wrinkles come from.</p>

<p><a href="https://theconversation.com/why-do-fingers-get-wrinkly-after-a-long-bath-or-swim-a-biomedical-engineer-explains-204726" target="_blank">He wrote about the research for The Conversation</a> — a nonprofit news organization that asks academics to share their expertise on current topics — as part of its Curious Kids feature. One of the follow-up questions stumped him, though.</p>

<p>“A student asked, ‘Yeah, but do the wrinkles always form in the same way?’ And I thought: I haven’t the foggiest clue!” said German, a faculty member at the <a href="https://www.binghamton.edu/watson">Thomas J. Watson College of Engineering and Applied Science</a>’s <a href="https://www.binghamton.edu/biomedical-engineering">Department of Biomedical Engineering</a>. “So it led to this research to find out.”</p>

<p>In <a href="https://www.sciencedirect.com/science/article/pii/S1751616125000517?via%3Dihub" target="_blank">a paper recently published in the <i>Journal of the Mechanical Behavior of Biomedical Materials</i></a>, German and Rachel Laytin ’23, MS ’24, show that, yes, the topography patterns remain constant after multiple immersions.</p>

<p>“Blood vessels don’t change their position much — they move around a bit, but in relation to other blood vessels, they’re pretty static,” German said. “That means the wrinkles should form in the same manner, and we proved that they do.”</p>

<p>The research put subjects’ fingers in water for 30 minutes, taking photos and then repeating the immersion under the same conditions at least 24 hours later. By comparing the images, German and Laytin found the same patterns of raised loops and ridges after both immersions.</p>

<p>They also made an interesting side discovery: “We’ve heard that wrinkles don’t form in people who have median nerve damage in their fingers,” German said. “One of my students told us, ‘I’ve got median nerve damage in my fingers.’ So we tested him — no wrinkles!”<b> </b></p>

<p>As much fun as it was to figure out something a child asked, the research also could have real-world applications in forensics, such as fingerprinting at crime scenes and identifying bodies found after prolonged water exposure. German’s father, a retired U.K. police officer, faced some of these challenges during his law enforcement career.<b> </b></p>

<p>“Biometrics and fingerprints are built into my brain,” he said. “I always think about this sort of stuff, because it’s fascinating.”</p>



<p>German is eager to further explore questions about skin immersion with his students: “I feel like a kid in a candy store, because there’s so much science here that I don’t know. We thank the people at The Conversation and the wonderful question they asked us, because it does create cool new science.”</p>	  <!-- 126 <p>Research shows fingers wrinkle the same way with every water immersion</p> -->


  

	  


	  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Flattening Rust’s learning curve (355 pts)]]></title>
            <link>https://corrode.dev/blog/flattening-rusts-learning-curve/</link>
            <guid>43978435</guid>
            <pubDate>Tue, 13 May 2025 22:25:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://corrode.dev/blog/flattening-rusts-learning-curve/">https://corrode.dev/blog/flattening-rusts-learning-curve/</a>, See on <a href="https://news.ycombinator.com/item?id=43978435">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

      <p>I see people make the same mistakes over and over again when learning Rust.
Here are my thoughts (ordered by importance) on how you can ease the learning process.
My goal is to help you save time and frustration.</p>
<h2 id="let-your-guard-down"><a href="#let-your-guard-down" aria-label="Anchor link for: let-your-guard-down">Let Your Guard Down</a></h2>
<p>Stop resisting. That’s the most important lesson.</p>
<p>Accept that learning Rust requires adopting a completely different mental model than what you’re used to.
There are a ton of new concepts to learn like lifetimes, ownership, and the trait system.
And depending on your background, you’ll need to add generics, pattern matching, or macros to the list.</p>
<p>Your learning pace doesn’t have much to do with whether you’re smart or not or if you have a lot of programming experience.
Instead, what matters more is <strong>your attitude toward the language</strong>.</p>
<p>I have seen junior devs excel at Rust with no prior training and senior engineers struggle for weeks/months or even give up entirely. Leave your hubris at home.</p>
<p>Treat the borrow checker as a co-author, not an adversary. This reframes the relationship.
Let the compiler do the teaching: for example, this works great with lifetimes, because the compiler will tell you when a lifetime is ambiguous.
Then just add it but take the time to reason about <em>why</em> the compiler couldn’t figure it out itself.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>longest</span></span><span><span><span>(</span><span>x</span><span>:</span> <span>&amp;</span><span>str</span>, <span>y</span><span>:</span> <span>&amp;</span><span>str</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>&amp;</span><span>str</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>if</span> x<span>.</span><span>len</span><span><span>(</span></span><span><span>)</span></span> <span>&gt;</span> y<span>.</span><span>len</span><span><span>(</span></span><span><span>)</span></span> <span><span>{</span>
</span></span></span></span><span><span><span><span>        x
</span></span></span></span><span><span><span><span>    </span><span><span>}</span></span> <span>else</span> <span><span>{</span>
</span></span></span></span><span><span><span><span>        y
</span></span></span></span><span><span><span><span>    </span><span><span>}</span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>If you try to compile this, the compiler will ask you to add a lifetime parameter.
It provides this helpful suggestion:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span>1</span> <span>|</span> <span><span><span>fn</span> </span><span>longest</span></span><span><span>&lt;</span><span>'a</span><span>&gt;</span></span><span><span><span>(</span><span>x</span><span>:</span> <span>&amp;</span><span>'a</span> <span>str</span>, <span>y</span><span>:</span> <span>&amp;</span><span>'a</span> <span>str</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span>&amp;</span><span>'a</span> <span>str</span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>  <span>|</span>           <span>+</span><span>+</span><span>+</span><span>+</span>     <span>+</span><span>+</span>          <span>+</span><span>+</span>          <span>+</span><span>+</span>
</span></span></span></code></pre>
<p>So you don’t have to guess what the compiler wants and can follow its instructions.
But also sit down and wonder <em>why</em> the compiler couldn’t figure it out itself.</p>
<p>Most of the time when fighting the compiler it is actually exposing a design flaw.
Similarly, if your code gets overly verbose or looks ugly, there’s probably a better way.
Declare defeat and learn to do it the Rust way.</p>
<p>If you come from a dynamic language like Python, you’ll find that Rust is more verbose in general.
Most of it just comes from type annotations, though.
Some people might dismiss Rust as being “unelegant” or “ugly”, but the verbosity actually serves a good purpose and is immensely helpful for building large-scale applications:</p>
<ul>
<li>First off, you will read the code more often than you write it,
which means type annotations will give you more local context to reason with.</li>
<li>Second, it helps immensely with refactoring because the compiler can check if you broke any code while you move things around.
If your code turns out to look very ugly, take a step back and ask if there’s a simpler solution.
Don’t dismiss the language right away.</li>
</ul>
<p>Turn on all <a href="https://doc.rust-lang.org/clippy/">clippy lints</a> on day one – even the pedantic ones.
Run the linter and follow the suggestions religiously.
Don’t skip that step once your program compiles.</p>
<p>Resistance is futile.
The longer you refuse to learn, the longer you will suffer;
but the moment you let your guard down is the moment you’ll start to learn.
Forget what you think you knew about programming and really start to listen to what the compiler, the standard library, and clippy are trying to tell you.</p>
<h2 id="baby-steps"><a href="#baby-steps" aria-label="Anchor link for: baby-steps">Baby Steps</a></h2>
<p>I certainly tried to run before I could walk.
That alone cost me a lot of precious time.</p>
<p>Don’t make it too hard on yourself in the beginning.
Here are some tips:</p>
<ul>
<li>Use <code>String</code> and <code>clone()</code> and <code>unwrap</code> generously; you can always refactor later – and refactoring is the best part about Rust!
I wrote an article on saving yourself time during that phase <a href="https://corrode.dev/blog/prototyping">here</a>.</li>
<li>Use simple if or match statements before starting to learn some of the more idiomatic <code>.and_then</code> etc. combinators</li>
<li>Avoid async Rust in week 1. The additional rules are a tax on people still learning the core ownership model.</li>
</ul>
<p>Don’t introduce too many new concepts at the same time!
Instead, while you learn about a new concept, have an editor open and write out a few examples.
What helped was to just write some code in the <a href="https://play.rust-lang.org/">Rust playground</a> and try to get it to compile. Write super small snippets (e.g., one <code>main.rs</code> for one concept) instead of using one big “tutorial” repo.
Get into the habit of throwing most of your code away.</p>
<p>I still do that and test out ideas in the playground or when I brainstorm with clients.</p>
<p>For instance, here’s one of my favorite code snippets to explain the concept of ownership:</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>my_func</span></span><span><span><span>(</span><span>v</span><span>:</span> String</span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span> do something with v
</span></span></span></span><span><span><span></span><span><span>}</span></span></span>
</span><span>
</span><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>let</span> s <span>=</span> <span>String</span><span><span>::</span></span>from<span><span>(</span><span><span>"</span>hello<span>"</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>    <span>my_func</span><span><span>(</span>s</span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>    <span>my_func</span><span><span>(</span>s</span><span><span>)</span></span><span>;</span> <span> error, but why?
</span></span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>Can you fix it?
Can you explain it?
Ask yourself what would change if <code>v</code> was an <code>i32</code>.</p>
<p>If Rust code looks scary to you, <strong>break it down</strong>.
Write your own, simpler version, then slowly increase the complexity.
Rust is easier to write than to read.
By writing lots of Rust, you will also learn how to read it better as well.</p>
<h2 id="be-accurate"><a href="#be-accurate" aria-label="Anchor link for: be-accurate">Be Accurate</a></h2>
<blockquote>
<p>How you do anything is how you do everything.
– An ancient Rust proverb</p>
</blockquote>
<p>You can be sloppy in other languages, but not in Rust.
That means you have to be accurate while you code or the code just won’t compile.
The expectation is that this approach will save you debugging time in the future.</p>
<p>I found that the people who learn Rust the fastest all have great attention to detail.
If you try to just get things done and move on, you will have a harder time than if you aim to do things right on your first try.
You will have a much better time if you re-read your code to fix stupid typos before pressing “compile.”
Also build a habit of automatically adding <code>&amp;</code> and <code>mut</code> where necessary as you go.</p>
<p>A good example of someone who thinks about these details while coding is
Tsoding. For example, watch <a href="https://www.youtube.com/watch?v=b0KIDIOL_i4">this stream where he builds a search engine in Rust from scratch</a> to see what I mean.
I think you can learn this skill as long as you’re putting in your best effort and give it some time.</p>
<h2 id="don-t-cheat"><a href="#don-t-cheat" aria-label="Anchor link for: don-t-cheat">Don’t Cheat</a></h2>
<p>With today’s tooling it is very easy to offload the bulk of the work to the computer.
Initially, it will feel like you’re making quick progress, but in reality, you just strengthen bad habits in your workflow.
If you can’t explain what you wrote to someone else or if you don’t know about the tradeoffs/assumptions a part of your code makes,
you took it too far.</p>
<p>Often, this approach stems from a fear that you’re not making progress fast enough.
But you don’t have to prove to someone else that you’re clever enough to pick up Rust very quickly.</p>
<h4 id="walk-the-walk"><a href="#walk-the-walk" aria-label="Anchor link for: walk-the-walk">Walk the Walk</a></h4>
<p>To properly learn Rust you actually have to write a lot of code by hand.
Don’t be a lurker on Reddit, reading through other people’s success stories.
Have some skin in the game!
Put in the hours because there is no silver bullet.
Once it works, consider open sourcing your code even if you know it’s not perfect.</p>
<h4 id="don-t-go-on-auto-pilot"><a href="#don-t-go-on-auto-pilot" aria-label="Anchor link for: don-t-go-on-auto-pilot">Don’t Go on Auto-Pilot</a></h4>
<p>LLMs are like driving a car on auto-pilot.
It’s comfortable at first, but you won’t feel in control and slowly, that uneasy feeling will creep in.
Turn off the autopilot while learning.</p>
<p>A quick way to set you up for success is to learn by writing code in the Rust Playground first.
Don’t use LLMs or code completion. Just type it out!
If you can’t, that means you haven’t fully internalized a concept yet.
That’s fine!
Go to the standard library and read the docs.
Take however long it takes and then come back and try again.</p>
<p>Slow is steady and steady is fast.</p>
<h4 id="build-muscle-memory"><a href="#build-muscle-memory" aria-label="Anchor link for: build-muscle-memory">Build Muscle Memory</a></h4>
<p>Muscle memory in programming is highly underrated.
People will tell you that this is what code completion is for, but I believe it’s a requirement to reach a state of flow:
if you constantly blunder over syntax errors or, worse, just wait for the next auto-completion to make progress,
that is a terrible developer experience.</p>
<p>When writing manually, you will make more mistakes.
Embrace them!
These mistakes will help you learn to understand the compiler output.
You will get a “feeling” for how the output looks in different error scenarios.
Don’t gloss over these errors.
Over time you will develop an intuition about what feels “rustic.”</p>
<h4 id="predict-the-output"><a href="#predict-the-output" aria-label="Anchor link for: predict-the-output">Predict The Output</a></h4>
<p>Another thing I like to do is to run “prediction exercises” where I guess if code will compile before running it.
This builds intuition.
Try to make every program free of syntax errors before you run it.
Don’t be sloppy.
Of course, you won’t always succeed, but you will get much better at it over time.</p>
<h4 id="try-to-solve-problems-yourself-only-then-look-up-the-solution"><a href="#try-to-solve-problems-yourself-only-then-look-up-the-solution" aria-label="Anchor link for: try-to-solve-problems-yourself-only-then-look-up-the-solution">Try To Solve Problems Yourself, Only <em>Then</em> Look Up The Solution.</a></h4>
<p>Read lots of other people’s code. I recommend <a href="https://github.com/BurntSushi/ripgrep"><code>ripgrep</code></a>, for example,
which is some of the best Rust code out there.</p>

<p>Don’t be afraid to get your hands dirty.
Which areas of Rust do you avoid?
What do you run away from?
Focus on that.
Tackle your blind spots.
Track your common “escape hatches” (unsafe, clone, etc.) to identify your current weaknesses.
For example, if you are scared of proc macros, write a bunch of them.</p>
<h4 id="break-your-code"><a href="#break-your-code" aria-label="Anchor link for: break-your-code">Break Your Code</a></h4>
<p>After you’re done with an exercise, break it! See what the compiler says.
See if you can explain what happens.</p>
<h4 id="don-t-use-other-people-s-crates-while-learning"><a href="#don-t-use-other-people-s-crates-while-learning" aria-label="Anchor link for: don-t-use-other-people-s-crates-while-learning">Don’t Use Other People’s Crates While Learning</a></h4>
<p>A poor personal version is better than a perfect external crate (at least while learning).
Write some small library code yourself as an exercise.
Notable exceptions are probably <code>serde</code> and <code>anyhow</code>, which can save you time dealing with JSON inputs and setting up error handling that you can spend on other tasks
as long as you know how they work.</p>
<h2 id="build-good-intuitions"><a href="#build-good-intuitions" aria-label="Anchor link for: build-good-intuitions">Build Good Intuitions</a></h2>
<p>Concepts like lifetimes are hard to grasp.
Sometimes it helps to draw how data moves through your system.
Develop a habit to explain concepts to yourself and others through drawing.
I’m not sure, but I think this works best for “visual”/creative people (in comparison to highly analytical people).</p>
<p>I personally use <a href="https://excalidraw.com/">excalidraw</a> for drawing.
It has a “comicy” feel, which takes the edge off a bit.
The implication is that it doesn’t feel highly accurate, but rather serves as a rough sketch.
Many good engineers (as well as great Mathematicians and Physicists) are able to visualize concepts with sketches.</p>
<p>In Rust, sketches can help to visualize lifetimes and ownership of data or for architecture diagrams.</p>
<h2 id="build-on-top-of-what-you-already-know"><a href="#build-on-top-of-what-you-already-know" aria-label="Anchor link for: build-on-top-of-what-you-already-know">Build On Top Of What You Already Know</a></h2>
<p>Earlier I said you should forget everything you know about programming.
How can I claim now that you should build on top of what you already know?</p>
<p>What I meant is that Rust is the most different in familiar areas like control flow handling and value passing.
E.g., mutability is very explicit in Rust and calling a function typically “moves” its arguments.
That’s where you have to accept that Rust is just <em>different</em> and learn from <em>first principles</em>.</p>
<p>However, it is okay to map Rust concepts to other languages you already know.
For instance, “a trait is a bit like an interface” is wrong, but it is a good starting point
to understand the concept.</p>
<p>Here are a few more examples:</p>
<ul>
<li>“A struct is like a class (minus the inheritance)”</li>
<li>“A closure is like a lambda function (but it can capture variables)”</li>
<li>“A module is like a namespace (but more powerful)”</li>
<li>“A borrow is like a pointer (but with single owner).”</li>
</ul>
<p>And if you have a functional background, it might be:</p>
<ul>
<li>“<code>Option</code> is like the <code>Maybe</code> monad”</li>
<li>“Traits are like type-classes”</li>
<li>“Enums are algebraic data types”</li>
</ul>
<p>The idea is that mapping concepts helps fill in the gaps more quickly.</p>
<p>Map what you already know from another language (e.g., Python, TypeScript) to Rust concepts.
As long as you know that there are subtle differences, I think it’s helpful.</p>
<p>I don’t see people mention this a lot, but I believe that <a href="https://rosettacode.org/wiki/Rosetta_Code">Rosetta Code</a> is a great resource for that.
You basically browse their list of tasks, pick one you like and start comparing the Rust solution with the language you’re strongest in.</p>
<p>Also, port code from a language you know to Rust.
This way, you don’t have to learn a new domain at the same time as you learn Rust.
You can build on your existing knowledge and experience.</p>
<ul>
<li>Translate common language idioms from your strongest language to Rust. E.g., how would you convert a list comprehension from Python to Rust?
Try it first, then look for resources, which explain the concept in Rust. For instance, <a href="https://corrode.dev/blog/iterators">I wrote one</a> on this topic specifically.</li>
<li>I know people who have a few standard exercises that they port to every new language they learn.
For example, that could be a ray-tracer, a sorting algorithm, or a small web app.</li>
</ul>
<p>Finally, find other people who come from the same background as you.
Read their blogs where they talk about their experiences learning Rust.
Write down your experiences as well.</p>
<h2 id="don-t-guess"><a href="#don-t-guess" aria-label="Anchor link for: don-t-guess">Don’t Guess</a></h2>
<p>I find that people who tend to <em>guess</em> their way through challenges
often have the hardest time learning Rust.</p>
<p>In Rust, the details are everything.
Don’t gloss over details, because they always reveal some wisdom about the task at hand.
Even if you don’t care about the details, they will come back to bite you later.</p>
<p>For instance, why do you have to call <code>to_string()</code> on a thing that’s already a string?</p>
<pre data-lang="rust"><code data-lang="rust"><span><span>my_func</span><span><span>(</span><span><span>"</span>hello<span>"</span></span><span>.</span><span>to_string</span><span><span>(</span></span><span><span>)</span></span></span><span><span>)</span></span>
</span></code></pre>
<p>Those stumbling blocks are learning opportunities.
It might look like a waste of time to ask these questions and means that it will take longer to finish a task,
but it will pay off in the long run.</p>
<p>Reeeeeally read the error messages the compiler prints.
Everyone thinks they do this, but time and again I see people look confused while the solution is right there in their terminal.
There are <code>hints</code> as well; don’t ignore those.
This alone will save you sooo much time.
Thank me later.</p>
<p>You might say that is true for every language, and you’d be right.
But in Rust, the error messages are actually worth your time.
Some of them are like small meditations: opportunities to think about the problem at a deeper level.</p>
<p>If you get any borrow-checker errors, refuse the urge to guess what’s going on.
Instead of guessing, <em>walk through the data flow by hand</em> (who owns what and when).
Try to think it through for yourself and only try to compile again once you understand the problem.</p>
<h2 id="lean-on-type-driven-development"><a href="#lean-on-type-driven-development" aria-label="Anchor link for: lean-on-type-driven-development">Lean on Type-Driven Development</a></h2>
<p>The key to good Rust code is through its type system.</p>
<p>It’s <strong>all</strong> in the type system.
Everything you need is hidden in plain sight.
But often, people skip too much of the documentation and just look at the examples.</p>
<p>What few people do is <em>read the actual function documentation</em>.
You can even click through the standard library all the way to the source code to read the thing they are using.
There is no magic (and that’s what’s so magical about it).</p>
<p>You can do that in Rust much better than in most other languages.
That’s because Python for example is written in C, which requires you to cross that language boundary to learn what’s going on.
Similarly, the C++ standard library isn’t a single, standardized implementation, but rather has several different implementations maintained by different organizations.
That makes it super hard to know <em>what exactly is going on</em>.
In Rust, the source code is available right inside the documentation. Make good use of that!</p>
<p>Function signatures tell a lot!
The sooner you will embrace this additional information, the quicker you will be off to the races with Rust.
If you have the time, read interesting parts of the standard library docs.
Even after years, I always learn something when I do.</p>
<p>Try to model your own projects with types first.
This is when you start to have way more fun with the language.
It feels like you have a conversation with the compiler about the problem you’re trying to solve.</p>
<p>For example, once you learn how concepts like expressions, iterators and traits fit together,
you can write more concise, readable code.</p>
<p>Once you learn how to encode invariants in types, you can write more correct code
that you don’t have to run to test. Instead, you can’t compile incorrect code in the first place.</p>
<p>Learn Rust through “type-driven development” and let the compiler errors guide your design.</p>
<h2 id="invest-time-in-finding-good-learning-resources"><a href="#invest-time-in-finding-good-learning-resources" aria-label="Anchor link for: invest-time-in-finding-good-learning-resources">Invest Time In Finding Good Learning Resources</a></h2>
<p>Before you start, shop around for resources that fit your personal learning style.
To be honest, there is not that much good stuff out there yet.
On the plus side, it doesn’t take too long to go through the list of resources
<em>before</em> settling on one specific platform/book/course.
The right resource depends on what learner you are.
In the long run, finding the right resource saves you time because you will learn quicker.</p>
<p>I personally don’t like doing toy exercises that others have built out for me.
That’s why I don’t like Rustlings too much; the exercises are not “fun” and too theoretical. I want more practical exercises.
I found that <a href="https://projecteuler.net/">Project Euler</a> or <a href="https://adventofcode.com/">Advent of Code</a> work way better for me.
The question comes up quite often, so I wrote a blog post about <a href="https://corrode.dev/blog/rust-learning-resources-2025">my favorite Rust learning resources</a>.</p>
<h4 id="don-t-just-watch-youtube"><a href="#don-t-just-watch-youtube" aria-label="Anchor link for: don-t-just-watch-youtube">Don’t Just Watch YouTube</a></h4>
<p>I like to watch YouTube, but exclusively for recreational purposes.
In my opinion, watching <a href="https://www.youtube.com/c/theprimeagen">ThePrimeagen</a> is for entertainment only.
He’s an amazing programmer, but trying to learn how to program by watching someone else do it is like trying to learn how to become a great athlete by watching the Olympics.
Similarly, I think we all can agree that <a href="https://www.youtube.com/c/JonGjengset">Jon Gjengset</a> is an exceptional programmer and teacher, but watching him might be overwhelming
if you’re just starting out. (Love the content though!)</p>
<p>Same goes for conference talks or podcasts: they are great for context, and for soft-skills, but not for learning Rust.</p>
<p>Instead, invest in a good book if you can.
Books are not yet outdated and you can read them offline, add personal notes, type out the code yourself and get a
“spatial overview” of the depth of the content by flipping through the pages.</p>
<p>Similarly, if you’re serious about using Rust professionally, buy a course or get your boss to invest in a trainer.
Of course, I’m super biased here as I run a Rust consultancy, but I truly believe that it will save you and your company countless
hours and will set you up for long-term success. Think about it: you will work with this codebase for years to come. Better make that experience a pleasant one.
A good trainer, just like a good teacher, will not go through the Rust book with you, but watch you program Rust in the wild and give you personalized feedback about your weak spots.</p>
<h2 id="find-a-coding-buddy"><a href="#find-a-coding-buddy" aria-label="Anchor link for: find-a-coding-buddy">Find A Coding Buddy</a></h2>
<p>“Shadow” more experienced team members or friends.</p>
<p>Don’t be afraid to ask for a code review on Mastodon or the <a href="https://users.rust-lang.org/">Rust forum</a> and return the favor and do code reviews there yourself.
Take on opportunities for pair programming.</p>
<h4 id="explain-rust-code-to-non-rust-developers"><a href="#explain-rust-code-to-non-rust-developers" aria-label="Anchor link for: explain-rust-code-to-non-rust-developers">Explain Rust Code To Non-Rust Developers</a></h4>
<p>This is such a great way to see if you truly understood a concept.
Don’t be afraid to say “I don’t know.”
Then go and explore the answer together by going straight to the docs.
It’s way more rewarding and honest.</p>
<p>Help out with OSS code that is abandoned.
If you put in a solid effort to fix an unmaintained codebase, you will help others while learning how to work with other people’s Rust code.</p>
<p>Read code out loud and explain it.
There’s no shame in that!
It helps you “serialize” your thoughts and avoid skipping important details.</p>
<p>Take notes.
Write your own little “Rust glossary” that maps Rust terminology to concepts in your business domain.
It doesn’t have to be complete and just has to serve your needs.</p>
<p>Write down things you found hard and things you learned.
If you find a great learning resource, share it!</p>
<h2 id="believe-in-the-long-term-benefit"><a href="#believe-in-the-long-term-benefit" aria-label="Anchor link for: believe-in-the-long-term-benefit">Believe In The Long-Term Benefit</a></h2>
<p>If you learn Rust because you want to put it on your CV, stop.
Learn something else instead.</p>
<p>I think you have to actually <em>like</em> programming (and not just the idea of it) to enjoy Rust.</p>
<p>If you want to be successful with Rust, you have to be in it for the long run.
Set realistic expectations: You won’t be a “Rust grandmaster” in a week but you can achieve a lot in a month of focused effort.
There is no silver bullet, but if you avoid the most common ways to shoot yourself in the foot, you pick up the language much faster.
Rust is a day 2 language. You won’t “feel” as productive as in your first week of Go or Python, but stick it out and it will pay off.
Good luck and have fun!</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Type-constrained code generation with language models (229 pts)]]></title>
            <link>https://arxiv.org/abs/2504.09246</link>
            <guid>43978357</guid>
            <pubDate>Tue, 13 May 2025 22:15:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2504.09246">https://arxiv.org/abs/2504.09246</a>, See on <a href="https://news.ycombinator.com/item?id=43978357">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2504.09246">View PDF</a></p><blockquote>
            <span>Abstract:</span>Large language models (LLMs) have achieved notable success in code generation. However, they still frequently produce uncompilable output because their next-token inference procedure does not model formal aspects of code. Although constrained decoding is a promising approach to alleviate this issue, it has only been applied to handle either domain-specific languages or syntactic features of general-purpose programming languages. However, LLMs frequently generate code with typing errors, which are beyond the domain of syntax and generally hard to adequately constrain. To address this challenge, we introduce a type-constrained decoding approach that leverages type systems to guide code generation. For this purpose, we develop novel prefix automata and a search over inhabitable types, forming a sound approach to enforce well-typedness on LLM-generated code. We formalize our approach on a foundational simply-typed language and extend it to TypeScript to demonstrate practicality. Our evaluation on the HumanEval and MBPP datasets shows that our approach reduces compilation errors by more than half and significantly increases functional correctness in code synthesis, translation, and repair tasks across LLMs of various sizes and model families, including state-of-the-art open-weight models with more than 30B parameters. The results demonstrate the generality and effectiveness of our approach in constraining LLM code generation with formal rules of type systems.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Niels Mündler [<a href="https://arxiv.org/show-email/a5e3666f/2504.09246" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2504.09246v1" rel="nofollow">[v1]</a></strong>
        Sat, 12 Apr 2025 15:03:00 UTC (2,798 KB)<br>
    <strong>[v2]</strong>
        Thu, 8 May 2025 09:33:40 UTC (2,667 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Y Combinator says Google is a monopolist, no comment about its OpenAI ties (123 pts)]]></title>
            <link>https://techcrunch.com/2025/05/13/y-combinator-says-google-is-a-monopolist-that-has-stunted-the-startup-ecosystem/</link>
            <guid>43978224</guid>
            <pubDate>Tue, 13 May 2025 21:57:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/05/13/y-combinator-says-google-is-a-monopolist-that-has-stunted-the-startup-ecosystem/">https://techcrunch.com/2025/05/13/y-combinator-says-google-is-a-monopolist-that-has-stunted-the-startup-ecosystem/</a>, See on <a href="https://news.ycombinator.com/item?id=43978224">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Fabled startup investor and accelerator Y Combinator has some choice words for Google in an amicus brief it just submitted in the U.S.’s monopoly case against the search giant.</p>

<p>In the <a href="https://www.documentcloud.org/documents/25940482-govuscourtsdcd22320513001/" target="_blank" rel="noreferrer noopener nofollow">brief</a>, YC charged that Google is a “monopolist” that has “stunted” the U.S. startup ecosystem by making VC firms like itself hesitate to fund web search and AI startups in what it calls a “kill zone” around Google.</p>







<p>“Google has chilled independent firms like YC from funding and accelerating innovative startups that could otherwise have challenged Google’s dominance,” YC wrote in the filing. “The result is a landscape that has been artificially stunted and stagnant.”</p>

<p>YC’s brief says it’s currently seeking to fund startups developing question-based and agentic AI tools that could transform how people interact with information on the internet. But YC says there’s a “clear risk” that Google will use its monopoly power to slow down the future of those markets.&nbsp;</p>

<p>“Google has effectively frozen the web search and text advertising markets for over a decade,” YC wrote.</p>

<p>The brief, filed May 9, was <a href="https://x.com/pitdesi/status/1921307098543178070" target="_blank" rel="noreferrer noopener nofollow">spotted</a> on X by VC Sheel Mohnot, the general partner of Better Tomorrow Ventures and a prolific social media poster.</p>

<p>But YC isn’t calling for an immediate breakup of Google, as its CEO Garry Tan <a href="https://x.com/garrytan/status/1921308365445296545" target="_blank" rel="noreferrer noopener nofollow">made clear in a reply to Mohnot.</a></p>


<p>Rather, YC is arguing Google should curb practices it considers anti-competitive, like paying Apple billions of dollars to make Google the iPhone’s default search engine. It also wants Google to do things it argues would help startups, like opening up Google’s search index so others can train LLMs on it.</p>

<p>For perspective, Google’s search algorithms have been its highly prized secret since its inception. For YC to ask the government to force Google to open it up to competitive LLMs is almost like demanding the government make Microsoft Windows open source, or forcing Amazon to freely deliver packages for competitors.</p>

<p>If Google doesn’t implement such changes within a five-year time frame, then YC advocates for the government to force Google to divest or spin out parts of itself. YC CEO Tan <a href="https://x.com/garrytan/status/1921308365445296545" target="_blank" rel="noreferrer noopener nofollow">characterized</a> this idea in an X post as a “spinoff hammer” threat.&nbsp;He also posted that “we love Google” but wants “little tech” to succeed, too, in a <a href="https://x.com/garrytan/status/1921319298640892373" target="_blank" rel="noreferrer noopener nofollow">separate X thread</a>.</p>

<figure><div>
<blockquote data-width="500" data-dnt="true"><div lang="en" dir="ltr"><p>YC released an amicus brief re: US v Google yesterday. </p><p>We love Google and what it represents as a paragon of US-led tech and innovation. We also want to make sure the excesses of big tech make way for tomorrow's little tech.</p></div>— Garry Tan (@garrytan) <a rel="nofollow" href="https://twitter.com/garrytan/status/1921319298640892373?ref_src=twsrc%5Etfw">May 10, 2025</a></blockquote>
</div></figure>

<p>To recap, last year <a href="https://techcrunch.com/2024/08/05/google-loses-massive-antitrust-case-over-search/">Google lost a massive antitrust case over its dominance of the search market</a>. While Google appeals the decision, the U.S. government is mulling potential punishments (“remedies”) that Google might be required to implement, <a href="https://techcrunch.com/2024/11/20/doj-google-must-sell-chrome-to-end-monopoly/">such as spinning off Chrome.</a> Those remedies are expected to be <a href="https://www.nytimes.com/2024/09/06/technology/google-search-antitrust-remedies.html" target="_blank" rel="noreferrer noopener nofollow">delivered</a> by August 2025.</p>







<p>YC’s stance may come as a surprise to those who have followed its latest partnerships with Google: Most notably, Google Cloud <a href="https://techcrunch.com/2024/08/01/google-cloud-now-has-a-dedicated-cluster-of-nvidia-gpus-for-y-combinator-startups/">gave YC startups access to a dedicated cluster of Nvidia GPUs</a> last year. Google co-founder Larry Page also <a href="https://techcrunch.com/2025/01/09/y-combinator-scored-a-surprise-win-when-larry-page-came-to-speak/">made a rare in-person appearance</a> to speak at a YC event in December.&nbsp;</p>

<p>Google <a href="https://techcrunch.com/2013/10/02/google-acquires-yc-backed-flutter-a-gesture-recognition-technology-startup/">has also acquired</a> at least two YC-backed startups: Flutter in 2014, and Fridge <a href="https://www.businessinsider.com/google-makes-its-first-aquisition-and-buys-social-group-startup-fridge-2011-7" target="_blank" rel="noreferrer noopener nofollow">in 2011.</a> It also <a href="https://techcrunch.com/2023/07/03/googles-gradient-backs-yc-alum-infisical-to-solve-secret-sprawl/">invested in YC startup Infisical</a> through its Gradient fund in 2023.&nbsp;</p>

<p>However, YC is also closely tied to OpenAI, which is <a href="https://techcrunch.com/2024/10/31/openai-launches-its-google-challenger-chatgpt-search/">now directly competing against Google on search</a>. OpenAI’s CEO Sam Altman used to run YC, while OpenAI was the first group <a href="https://www.ycombinator.com/blog/openai" target="_blank" rel="noreferrer noopener nofollow">affiliated</a> with YC Research.&nbsp;</p>

<p>That’s something Mohnot <a href="https://x.com/pitdesi/status/1921307098543178070" target="_blank" rel="noreferrer noopener nofollow">pointed to</a> on X, writing that the biggest beneficiary of YC’s proposed remedies, by far, would be OpenAI, rather than YC’s famously early-stage startups, while commenting that the amicus brief “paints Google as more powerful than it is.”</p>

<p>TechCrunch asked YC how it would respond to this critique, and whether it has any specific examples of areas that it probably would have funded had it not been for Google. So far, YC hasn’t responded to our comment request.&nbsp;</p>

<p>Google didn’t respond to a request for comment about YC’s amicus brief, either. However, it <a href="https://blog.google/outreach-initiatives/public-policy/doj-search-remedies-framework/" target="_blank" rel="noreferrer noopener nofollow">argued</a> in a blog post last year that the DOJ’s proposals are “radical and sweeping” and would hurt consumers, business, and developers.</p>
</div><div>
	
	
	
	

	
<div>
	<p>
		Charles Rollet is a senior reporter at TechCrunch. His investigative reporting has led to U.S. government sanctions against four tech companies, including China’s largest AI firm. Prior to joining TechCrunch, Charles covered the surveillance industry for IPVM. Charles is based in San Francisco, where he enjoys hiking with his dogs. You can contact Charles securely on Signal at charlesrollet.12 or +1-628-282-2811. 	</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/charles-rollet/" data-event="button" href="https://techcrunch.com/author/charles-rollet/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Starcloud (144 pts)]]></title>
            <link>https://www.ycombinator.com/companies/starcloud</link>
            <guid>43977188</guid>
            <pubDate>Tue, 13 May 2025 20:13:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ycombinator.com/companies/starcloud">https://www.ycombinator.com/companies/starcloud</a>, See on <a href="https://news.ycombinator.com/item?id=43977188">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div><div><p><img src="https://bookface-images.s3.amazonaws.com/small_logos/e9c9b9996bb500ee171134f1e1a0a83f5d48bec3.png" alt=""></p></div><div><h3>Data centers in space</h3><p>(Formerly known as Lumen Orbit) 
Starcloud is building a network of megawatt-scale data centers in space, scaleable to gigawatt capacity, to be able to train large models like GPT6. Falling launch costs give us access to abundant energy, passive cooling, and the ability to rapidly scale in space.

In Partnership with NVIDIA's Inception Program, we are launching our demonstrator satellite in July 2025, which will have 100x more powerful GPUs than have ever been operated in space.</p></div><div><div><p><img src="https://bookface-images.s3.us-west-2.amazonaws.com/avatars/d3394d5d9bde1ea05b6e234ac597803afa51503a.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=ASIAQC4NIECAAMFSIG7X%2F20250513%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20250513T233001Z&amp;X-Amz-Expires=3600&amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjEFAaCXVzLXdlc3QtMiJHMEUCIQDhNHTBQygcUjJeaSozLx%2BFU%2FvYPDAafTUCPOgkxNB%2FswIgCeIitEKD7REEF06xNfaZZi%2BLpIs5kbzuFbwFH6aS29Mq7gMI%2Bf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgwwMDYyMDE4MTEwNzIiDNgw1yZoxR%2FP76N5nirCAymMHwraWml6F7IJUYgdERRXkneGOnvRHE0uTiTkzSt7I5wXvA8867hL348w8SaOh2s%2BIo0hRqrNojkklX57ESJwpTMvkqmIhdGKo4kcutpRiQmyCwMcDv5XYOUYalz1wQSmjuOj5vOCZpY%2BeWWQSoeAktqFRvP4C3VWwl%2FwFkDfROkPoyCeXF8ZPPaFTSKrrkmg0FzjrB7rgjia9sKraK%2FHjl35gTxb0NFc4LNJT2hr%2BEt1nuHwfdlocEU6o%2FPFTdKJrHkRvr8HI6VZkV9V9TiDOc4kUP8VUgE%2BwefzuBNYvZ6XE1vlBFghrVujOmxebR6mR%2By%2FqMgPIb%2BzaWEvq3RkBY5EVI73cStHW6aW1fuqRI%2B9DZyZoFuKU%2FohLDs7wjgHGZXeyOYI6p7CH%2Bz2WfJhc2g3UYIsrhR3E862D7sqHwAKpnrBaN824yKGROrn%2FjtyEdj0FuSFUK901tEWt9OCU1g2rj%2BthlAA1LOvmzfxXTqMk0z6ASBR%2FnRoiPX5n1xJbhK2k5n3HMMRXCQh4uS8DgGdGh%2FDLJ%2B8%2FfuhvMfZQdhQhqn%2FeIDOicGXRhuF%2F4ri7GlowthyVbmJkiuiNuxyFDCYqo%2FBBjqlAavFDnClxx5Ra%2FtgBoCvOE6F%2BFEOKnT2m3NON%2FOroydsTidRjPDWz5e%2BNR3dV0TE7xRYBpSGXklb3ieoKqyTGSp1O6mhj15o6ZNlDZfCpl4VW5p35tUKCtDXMc7aXibbzV8gMm9vuybcpFj9kK9nmpruhBV8xdX4uTxX5BkNUhu9J30EwXmw7lOiFJ1gZLVtQYAjZQ5F0Si3uJmJnalDy0Y5xbsjLg%3D%3D&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=b4dc99f23483480c9e215782267200156b19156dfcf2f9c7fda8bc03be5361cd" alt="Philip Johnston"></p><div><p>Philip Johnston</p><p>Founder/CEO</p><div><p><a href="https://x.com/philipjohnst0n" data-tooltip-id="founder-social-tooltip-1489854" data-tooltip-content="X" aria-label="Twitter account" rel="nofollow" target="_blank"><img src="https://www.ycombinator.com/images/social/x-logo.svg" alt="Twitter account"></a></p><a href="https://linkedin.com/in/philipjohnst" data-tooltip-id="founder-social-tooltip-1489854" data-tooltip-content="LinkedIn" aria-label="LinkedIn profile" rel="nofollow" target="_blank"></a></div></div></div><div><p><img src="https://bookface-images.s3.us-west-2.amazonaws.com/avatars/b6a499a75b563eb52dd9821819b834e5119c29eb.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=ASIAQC4NIECAAMFSIG7X%2F20250513%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20250513T233001Z&amp;X-Amz-Expires=3600&amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjEFAaCXVzLXdlc3QtMiJHMEUCIQDhNHTBQygcUjJeaSozLx%2BFU%2FvYPDAafTUCPOgkxNB%2FswIgCeIitEKD7REEF06xNfaZZi%2BLpIs5kbzuFbwFH6aS29Mq7gMI%2Bf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgwwMDYyMDE4MTEwNzIiDNgw1yZoxR%2FP76N5nirCAymMHwraWml6F7IJUYgdERRXkneGOnvRHE0uTiTkzSt7I5wXvA8867hL348w8SaOh2s%2BIo0hRqrNojkklX57ESJwpTMvkqmIhdGKo4kcutpRiQmyCwMcDv5XYOUYalz1wQSmjuOj5vOCZpY%2BeWWQSoeAktqFRvP4C3VWwl%2FwFkDfROkPoyCeXF8ZPPaFTSKrrkmg0FzjrB7rgjia9sKraK%2FHjl35gTxb0NFc4LNJT2hr%2BEt1nuHwfdlocEU6o%2FPFTdKJrHkRvr8HI6VZkV9V9TiDOc4kUP8VUgE%2BwefzuBNYvZ6XE1vlBFghrVujOmxebR6mR%2By%2FqMgPIb%2BzaWEvq3RkBY5EVI73cStHW6aW1fuqRI%2B9DZyZoFuKU%2FohLDs7wjgHGZXeyOYI6p7CH%2Bz2WfJhc2g3UYIsrhR3E862D7sqHwAKpnrBaN824yKGROrn%2FjtyEdj0FuSFUK901tEWt9OCU1g2rj%2BthlAA1LOvmzfxXTqMk0z6ASBR%2FnRoiPX5n1xJbhK2k5n3HMMRXCQh4uS8DgGdGh%2FDLJ%2B8%2FfuhvMfZQdhQhqn%2FeIDOicGXRhuF%2F4ri7GlowthyVbmJkiuiNuxyFDCYqo%2FBBjqlAavFDnClxx5Ra%2FtgBoCvOE6F%2BFEOKnT2m3NON%2FOroydsTidRjPDWz5e%2BNR3dV0TE7xRYBpSGXklb3ieoKqyTGSp1O6mhj15o6ZNlDZfCpl4VW5p35tUKCtDXMc7aXibbzV8gMm9vuybcpFj9kK9nmpruhBV8xdX4uTxX5BkNUhu9J30EwXmw7lOiFJ1gZLVtQYAjZQ5F0Si3uJmJnalDy0Y5xbsjLg%3D%3D&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=11b0bc3219bf82cb13e0532c755e94dfd70872f9b4cf6f934ce4e6ccf1c1bde2" alt="Ezra Feilden"></p></div><div><p><img src="https://bookface-images.s3.us-west-2.amazonaws.com/avatars/8f6967e1c2ef4027c9198509b6ec69fb6aa3265b.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=ASIAQC4NIECAAMFSIG7X%2F20250513%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20250513T233001Z&amp;X-Amz-Expires=3600&amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjEFAaCXVzLXdlc3QtMiJHMEUCIQDhNHTBQygcUjJeaSozLx%2BFU%2FvYPDAafTUCPOgkxNB%2FswIgCeIitEKD7REEF06xNfaZZi%2BLpIs5kbzuFbwFH6aS29Mq7gMI%2Bf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgwwMDYyMDE4MTEwNzIiDNgw1yZoxR%2FP76N5nirCAymMHwraWml6F7IJUYgdERRXkneGOnvRHE0uTiTkzSt7I5wXvA8867hL348w8SaOh2s%2BIo0hRqrNojkklX57ESJwpTMvkqmIhdGKo4kcutpRiQmyCwMcDv5XYOUYalz1wQSmjuOj5vOCZpY%2BeWWQSoeAktqFRvP4C3VWwl%2FwFkDfROkPoyCeXF8ZPPaFTSKrrkmg0FzjrB7rgjia9sKraK%2FHjl35gTxb0NFc4LNJT2hr%2BEt1nuHwfdlocEU6o%2FPFTdKJrHkRvr8HI6VZkV9V9TiDOc4kUP8VUgE%2BwefzuBNYvZ6XE1vlBFghrVujOmxebR6mR%2By%2FqMgPIb%2BzaWEvq3RkBY5EVI73cStHW6aW1fuqRI%2B9DZyZoFuKU%2FohLDs7wjgHGZXeyOYI6p7CH%2Bz2WfJhc2g3UYIsrhR3E862D7sqHwAKpnrBaN824yKGROrn%2FjtyEdj0FuSFUK901tEWt9OCU1g2rj%2BthlAA1LOvmzfxXTqMk0z6ASBR%2FnRoiPX5n1xJbhK2k5n3HMMRXCQh4uS8DgGdGh%2FDLJ%2B8%2FfuhvMfZQdhQhqn%2FeIDOicGXRhuF%2F4ri7GlowthyVbmJkiuiNuxyFDCYqo%2FBBjqlAavFDnClxx5Ra%2FtgBoCvOE6F%2BFEOKnT2m3NON%2FOroydsTidRjPDWz5e%2BNR3dV0TE7xRYBpSGXklb3ieoKqyTGSp1O6mhj15o6ZNlDZfCpl4VW5p35tUKCtDXMc7aXibbzV8gMm9vuybcpFj9kK9nmpruhBV8xdX4uTxX5BkNUhu9J30EwXmw7lOiFJ1gZLVtQYAjZQ5F0Si3uJmJnalDy0Y5xbsjLg%3D%3D&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=972fb48ad97143ac5e059b84c6c8ecdcf33d0895d249d850241e17831441202b" alt="Adi Oltean"></p></div></div></div><div><p><a href="https://www.ycombinator.com/companies/starcloud" target="_blank" rel="noopener noreferrer"><img src="https://bookface-images.s3.us-west-2.amazonaws.com/logos/3913efef0276a9544b60689f4e43ff4a5a126108.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=ASIAQC4NIECAAMFSIG7X%2F20250513%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20250513T233001Z&amp;X-Amz-Expires=3600&amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjEFAaCXVzLXdlc3QtMiJHMEUCIQDhNHTBQygcUjJeaSozLx%2BFU%2FvYPDAafTUCPOgkxNB%2FswIgCeIitEKD7REEF06xNfaZZi%2BLpIs5kbzuFbwFH6aS29Mq7gMI%2Bf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgwwMDYyMDE4MTEwNzIiDNgw1yZoxR%2FP76N5nirCAymMHwraWml6F7IJUYgdERRXkneGOnvRHE0uTiTkzSt7I5wXvA8867hL348w8SaOh2s%2BIo0hRqrNojkklX57ESJwpTMvkqmIhdGKo4kcutpRiQmyCwMcDv5XYOUYalz1wQSmjuOj5vOCZpY%2BeWWQSoeAktqFRvP4C3VWwl%2FwFkDfROkPoyCeXF8ZPPaFTSKrrkmg0FzjrB7rgjia9sKraK%2FHjl35gTxb0NFc4LNJT2hr%2BEt1nuHwfdlocEU6o%2FPFTdKJrHkRvr8HI6VZkV9V9TiDOc4kUP8VUgE%2BwefzuBNYvZ6XE1vlBFghrVujOmxebR6mR%2By%2FqMgPIb%2BzaWEvq3RkBY5EVI73cStHW6aW1fuqRI%2B9DZyZoFuKU%2FohLDs7wjgHGZXeyOYI6p7CH%2Bz2WfJhc2g3UYIsrhR3E862D7sqHwAKpnrBaN824yKGROrn%2FjtyEdj0FuSFUK901tEWt9OCU1g2rj%2BthlAA1LOvmzfxXTqMk0z6ASBR%2FnRoiPX5n1xJbhK2k5n3HMMRXCQh4uS8DgGdGh%2FDLJ%2B8%2FfuhvMfZQdhQhqn%2FeIDOicGXRhuF%2F4ri7GlowthyVbmJkiuiNuxyFDCYqo%2FBBjqlAavFDnClxx5Ra%2FtgBoCvOE6F%2BFEOKnT2m3NON%2FOroydsTidRjPDWz5e%2BNR3dV0TE7xRYBpSGXklb3ieoKqyTGSp1O6mhj15o6ZNlDZfCpl4VW5p35tUKCtDXMc7aXibbzV8gMm9vuybcpFj9kK9nmpruhBV8xdX4uTxX5BkNUhu9J30EwXmw7lOiFJ1gZLVtQYAjZQ5F0Si3uJmJnalDy0Y5xbsjLg%3D%3D&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=0a00e0ccade096a26d3df0e49685b9bfda6dc498c130721266d505bd4e890f5a" alt="Starcloud"></a></p><div><p><span>Founded:</span><span>2024</span></p><p><span>Batch:</span><span>Summer 2024</span></p><p><span>Team Size:</span><span>4</span></p><p><span>Status:</span><span>Active</span></p><p><span>Location:</span><span>Redmond, WA</span></p><div><a href="https://www.starcloud.com/" data-tooltip-id="social-tooltip-starcloud" data-tooltip-content="https://www.starcloud.com/" aria-label="Company website" target="_blank" rel="nofollow noopener noreferrer"></a><a href="https://www.linkedin.com/company/starcloudinc" data-tooltip-id="social-tooltip-starcloud" data-tooltip-content="LinkedIn" aria-label="LinkedIn profile" target="_blank" rel="nofollow noopener noreferrer"></a><p><a href="https://x.com/starcloud_inc1" data-tooltip-id="social-tooltip-starcloud" data-tooltip-content="X" aria-label="X (Twitter) account" target="_blank" rel="nofollow noopener noreferrer"><img src="https://www.ycombinator.com/images/social/x-logo.svg" alt="X (Twitter) logo"></a></p><a href="https://www.crunchbase.com/organization/lumen-orbit" data-tooltip-id="social-tooltip-starcloud" data-tooltip-content="Crunchbase" aria-label="Crunchbase profile" target="_blank" rel="nofollow noopener noreferrer"></a></div></div></div></div><div><p><img src="https://www.ycombinator.com/media/?type=post&amp;id=83713&amp;key=user_uploads/1489854/7e0ffedf-fdc5-45ce-ade1-c091f6c96ea5"></p>
<h2><strong>TL;DR</strong> - We should train future large AI models in space to make use of abundant solar energy, cooling, and the ability to freely scale up.</h2>
<p>Hey all, we're building data centers in space. We’re launching our first satellite next year, which will have the most powerful GPUs ever put in space by &gt;100x. We will launch a larger iteration each year until we reach gigawatt scale.</p>
<h2 id="the-problem"><strong>❌The Problem</strong></h2>
<p>Future hyperscale data centers will put a huge strain on electricity grids, freshwater distribution, and the Western world’s permitting systems. It will simply not be possible to deploy multi-gigawatt scale data centers rapidly in the way we build data centers today.</p>
<h2 id="our-solution"><strong>✨Our Solution</strong></h2>
<p>We take advantage of falling launch costs to make use of inexpensive solar energy in space and low-cost passive radiative cooling, rapidly scaling up orbital data centers almost indefinitely without the physical or permitting constraints faced on Earth. This will ensure we can continue training ever larger models without destroying the environment.</p>
<h2 id="read-our-white-paper"><strong>⚙️&nbsp;Read our white paper</strong></h2>
<p>Check out our <a href="https://lumenorbit.com/wp"><strong>white paper</strong></a>&nbsp;for more information on why space data centers are the future and how we’re going about making this happen.</p>
<p>See a short video on the design <a href="https://www.youtube.com/watch?v=DIaKDUzFMpk">here</a>.</p><p>
<iframe src="https://www.youtube.com/embed/DIaKDUzFMpk" allowfullscreen=""></iframe>
</p>
<h2 id="what-we-ve-achieved-so-far"><strong>🏆&nbsp;What we’ve achieved so far</strong></h2>
<ul>
<li>Booked our first launch (May 2025) and our second launch (H2 2026)</li>
<li>Set up our payload manufacturing facility in Redmond, WA</li>
<li>Designed and started building and testing our first spacecraft, with the fastest GPUs to ever launch to space by ~100x</li>
<li>Created concept designs for our micro data center (2026 launch) and our Hypercluster data center (launching when Starship-class launch vehicles enter commercial service)</li>
<li>Secured high-value LOIs for H100 compute time in space</li>
</ul>
<h2 id="what-s-next"><strong>🔜&nbsp;What’s next</strong></h2>
<ul>
<li>Launch and complete our demonstrator mission, which will train the first LLM in space!</li>
<li>Prototype our micro-data center design</li>
<li>Secure contracts which incumbent hyper scalers</li>
</ul>
<h2 id="our-team"><strong>👥&nbsp;Our Team</strong></h2>
<p><img src="https://www.ycombinator.com/media/?type=post&amp;id=83713&amp;key=user_uploads/1489854/22431fb2-7f3d-42f8-b2a2-32f684a6cf2b"></p>
<p><a href="https://www.linkedin.com/in/johnstonphilip/"><strong>Philip</strong></a>, CEO, is a second-time founder who has worked at McKinsey &amp; Co. working on satellite projects for national space agencies. Philip has an MPA in National Security &amp; Technology from Harvard University, an MBA from Wharton, an MA in Applied Mathematics &amp; Theoretical Physics from Columbia University, and is a CFA Charter holder.</p>
<p><a href="https://www.linkedin.com/in/ezrafeilden/"><strong>Ezra</strong></a>, CTO, has a decade of experience with satellite design, specializing in deployable solar arrays and large deployable structures. Ezra comes from Airbus Defense &amp; Space (SSTL) and Oxford Space Systems, where he worked on missions including NASA's Lunar Pathfinder. Ezra has a PhD in Materials Engineering from Imperial College London.</p>
<p><a href="https://www.linkedin.com/in/adioltean/"><strong>Adi</strong></a>, Chief Engineer, was previously a Principal Software Engineer at SpaceX, where he was part of the Starlink network team enabling Starlink for in-motion users, including Starship. Before that, he deployed the first LLMs on large GPU production clusters at Microsoft, where he also delivered more than 25 patents in more than two decades. Adi holds degrees in Computer Science and Chemistry from the top two universities in Bucharest.</p>
<h2 id="asks"><strong>🙏 Asks</strong></h2>
<p>Join our team! If you are or know any hardcore engineers with experience in aerospace or data center infrastructure and are interested in working with the Starcloud team, please get in touch!</p>
<p><a href="https://www.lumenorbit.com/careers"><strong>https://www.starcloud.com/careers</strong></a></p>
<p>We are also interested in talking to operators and customers of hyperscale training clusters who are contemplating the scale-up from megawatts to gigawatts.</p>
</div><div><h4><p>Company Photo</p></h4><p><img src="https://bookface-images.s3.us-west-2.amazonaws.com/attachments/065a3eebab8634327ccf3fddba698fec064b402a.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=ASIAQC4NIECAAMFSIG7X%2F20250513%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20250513T233001Z&amp;X-Amz-Expires=3600&amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjEFAaCXVzLXdlc3QtMiJHMEUCIQDhNHTBQygcUjJeaSozLx%2BFU%2FvYPDAafTUCPOgkxNB%2FswIgCeIitEKD7REEF06xNfaZZi%2BLpIs5kbzuFbwFH6aS29Mq7gMI%2Bf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgwwMDYyMDE4MTEwNzIiDNgw1yZoxR%2FP76N5nirCAymMHwraWml6F7IJUYgdERRXkneGOnvRHE0uTiTkzSt7I5wXvA8867hL348w8SaOh2s%2BIo0hRqrNojkklX57ESJwpTMvkqmIhdGKo4kcutpRiQmyCwMcDv5XYOUYalz1wQSmjuOj5vOCZpY%2BeWWQSoeAktqFRvP4C3VWwl%2FwFkDfROkPoyCeXF8ZPPaFTSKrrkmg0FzjrB7rgjia9sKraK%2FHjl35gTxb0NFc4LNJT2hr%2BEt1nuHwfdlocEU6o%2FPFTdKJrHkRvr8HI6VZkV9V9TiDOc4kUP8VUgE%2BwefzuBNYvZ6XE1vlBFghrVujOmxebR6mR%2By%2FqMgPIb%2BzaWEvq3RkBY5EVI73cStHW6aW1fuqRI%2B9DZyZoFuKU%2FohLDs7wjgHGZXeyOYI6p7CH%2Bz2WfJhc2g3UYIsrhR3E862D7sqHwAKpnrBaN824yKGROrn%2FjtyEdj0FuSFUK901tEWt9OCU1g2rj%2BthlAA1LOvmzfxXTqMk0z6ASBR%2FnRoiPX5n1xJbhK2k5n3HMMRXCQh4uS8DgGdGh%2FDLJ%2B8%2FfuhvMfZQdhQhqn%2FeIDOicGXRhuF%2F4ri7GlowthyVbmJkiuiNuxyFDCYqo%2FBBjqlAavFDnClxx5Ra%2FtgBoCvOE6F%2BFEOKnT2m3NON%2FOroydsTidRjPDWz5e%2BNR3dV0TE7xRYBpSGXklb3ieoKqyTGSp1O6mhj15o6ZNlDZfCpl4VW5p35tUKCtDXMc7aXibbzV8gMm9vuybcpFj9kK9nmpruhBV8xdX4uTxX5BkNUhu9J30EwXmw7lOiFJ1gZLVtQYAjZQ5F0Si3uJmJnalDy0Y5xbsjLg%3D%3D&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=67c2c54f66268c3fc9443d60bda73ed99399a87b5bf8a2d1b720f8a8788b553f" alt="Company photo"></p></div><div><p>YC </p><!-- --><p>Summer 2024</p><!-- --><p> Application Video</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Build real-time knowledge graph for documents with LLM (160 pts)]]></title>
            <link>https://cocoindex.io/blogs/knowledge-graph-for-docs/</link>
            <guid>43976895</guid>
            <pubDate>Tue, 13 May 2025 19:48:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cocoindex.io/blogs/knowledge-graph-for-docs/">https://cocoindex.io/blogs/knowledge-graph-for-docs/</a>, See on <a href="https://news.ycombinator.com/item?id=43976895">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__blog-post-container"><p><img decoding="async" loading="lazy" alt="Building Knowledge Graph for Documents with LLM" src="https://cocoindex.io/blogs/assets/images/cover-3369a7567abd2f4d81d81c6f0d153aea.png" width="3069" height="2697"></p>
<p>CocoIndex makes it easy to build and maintain knowledge graphs with continuous source updates. In this blog, we will process a list of documents (using CocoIndex documentation as an example). We will use LLM to extract relationships between the concepts in each document. We will generate two kinds of relationships:</p>
<ol>
<li>Relationships between subjects and objects. E.g., "CocoIndex supports Incremental Processing"</li>
<li>Mentions of entities in a document. E.g., "core/basics.mdx" mentions <code>CocoIndex</code> and <code>Incremental Processing</code>.</li>
</ol>
<p>The source code is available at <a href="https://github.com/cocoindex-io/cocoindex/tree/main/examples/docs_to_knowledge_graph" target="_blank" rel="noopener noreferrer">CocoIndex Examples - docs_to_knowledge_graph</a>.</p>
<p><img decoding="async" loading="lazy" alt="Example knowledge graph showing relationships between concepts in CocoIndex documentation" src="https://cocoindex.io/blogs/assets/images/example-explanation-49f2a253cb339f43a69a6b654728063e.png" width="1731" height="837"></p>
<p>We are constantly improving, and more features and examples are coming soon.
Stay tuned and follow our progress by starring our <a href="https://github.com/cocoindex-io/cocoindex" target="_blank" rel="noopener noreferrer">GitHub repo</a>.</p>
<h2 id="prerequisites">Prerequisites<a href="#prerequisites" aria-label="Direct link to Prerequisites" title="Direct link to Prerequisites">​</a></h2>
<ul>
<li><a href="https://cocoindex.io/docs/getting_started/installation#-install-postgres" target="_blank" rel="noopener noreferrer">Install PostgreSQL</a>. CocoIndex uses PostgreSQL internally for incremental processing.</li>
<li><a href="https://cocoindex.io/docs/ops/storages#Neo4j" target="_blank" rel="noopener noreferrer">Install Neo4j</a>, a graph database.</li>
<li><a href="https://cocoindex.io/docs/ai/llm#openai" target="_blank" rel="noopener noreferrer">Configure your OpenAI API key</a>. Alternatively, you can switch to Ollama, which runs LLM models locally - <a href="https://cocoindex.io/docs/ai/llm#ollama" target="_blank" rel="noopener noreferrer">guide</a>.</li>
</ul>
<h2 id="documentation">Documentation<a href="#documentation" aria-label="Direct link to Documentation" title="Direct link to Documentation">​</a></h2>
<p>You can read the official CocoIndex Documentation for Property Graph Targets <a href="https://cocoindex.io/docs/ops/storages#property-graph-targets" target="_blank" rel="noopener noreferrer">here</a>.</p>
<h2 id="data-flow-to-build-knowledge-graph">Data flow to build knowledge graph<a href="#data-flow-to-build-knowledge-graph" aria-label="Direct link to Data flow to build knowledge graph" title="Direct link to Data flow to build knowledge graph">​</a></h2>
<p><img decoding="async" loading="lazy" alt="Data Flow" src="https://cocoindex.io/blogs/assets/images/data-flow-9acf8fc231ceac4ef3b04dc71eb5ed03.png" width="3069" height="1334"></p>
<h3 id="add-documents-as-source">Add documents as source<a href="#add-documents-as-source" aria-label="Direct link to Add documents as source" title="Direct link to Add documents as source">​</a></h3>
<p>We will process CocoIndex documentation markdown files (<code>.md</code>, <code>.mdx</code>) from the <code>docs/core</code> directory (<a href="https://github.com/cocoindex-io/cocoindex/tree/main/docs/docs/core" target="_blank" rel="noopener noreferrer">markdown files</a>, <a href="https://cocoindex.io/docs/core/basics" target="_blank" rel="noopener noreferrer">deployed docs</a>).</p>
<div><pre tabindex="0"><code><span><span>@cocoindex</span><span>.</span><span>flow_def</span><span>(</span><span>name</span><span>=</span><span>"DocsToKG"</span><span>)</span><span></span><br></span><span><span></span><span>def</span><span> </span><span>docs_to_kg_flow</span><span>(</span><span>flow_builder</span><span>:</span><span> cocoindex</span><span>.</span><span>FlowBuilder</span><span>,</span><span> data_scope</span><span>:</span><span> cocoindex</span><span>.</span><span>DataScope</span><span>)</span><span>:</span><span></span><br></span><span><span>    data_scope</span><span>[</span><span>"documents"</span><span>]</span><span> </span><span>=</span><span> flow_builder</span><span>.</span><span>add_source</span><span>(</span><span></span><br></span><span><span>        cocoindex</span><span>.</span><span>sources</span><span>.</span><span>LocalFile</span><span>(</span><span>path</span><span>=</span><span>"../../docs/docs/core"</span><span>,</span><span></span><br></span><span><span>                                    included_patterns</span><span>=</span><span>[</span><span>"*.md"</span><span>,</span><span> </span><span>"*.mdx"</span><span>]</span><span>)</span><span>)</span><br></span></code></pre></div>
<p>Here <code>flow_builder.add_source</code> creates a <a href="https://cocoindex.io/docs/core/data_types#KTable" target="_blank" rel="noopener noreferrer">KTable</a>.
<code>filename</code> is the key of the KTable.</p>
<p><img decoding="async" loading="lazy" alt="Adding a source to the flow" src="https://cocoindex.io/blogs/assets/images/data-add-source-6f03cf7504f4528d67b9910266752e6f.png" width="1484" height="431"></p>
<h3 id="add-data-collectors">Add data collectors<a href="#add-data-collectors" aria-label="Direct link to Add data collectors" title="Direct link to Add data collectors">​</a></h3>
<p>Add collectors at the root scope:</p>
<div><pre tabindex="0"><code><span><span>document_node </span><span>=</span><span> data_scope</span><span>.</span><span>add_collector</span><span>(</span><span>)</span><span></span><br></span><span><span>entity_relationship </span><span>=</span><span> data_scope</span><span>.</span><span>add_collector</span><span>(</span><span>)</span><span></span><br></span><span><span>entity_mention </span><span>=</span><span> data_scope</span><span>.</span><span>add_collector</span><span>(</span><span>)</span><br></span></code></pre></div>
<ul>
<li><code>document_node</code> collects documents. E.g., <a href="https://cocoindex.io/docs/core/basics" target="_blank" rel="noopener noreferrer"><code>core/basics.mdx</code></a> is a document.</li>
<li><code>entity_relationship</code> collects relationships. E.g., "CocoIndex supports Incremental Processing" indicates a relationship between <code>CocoIndex</code> and <code>Incremental Processing</code>.</li>
<li><code>entity_mention</code> collects mentions of entities in a document. E.g., <a href="https://cocoindex.io/docs/core/basics" target="_blank" rel="noopener noreferrer"><code>core/basics.mdx</code></a> mentions <code>CocoIndex</code> and <code>Incremental Processing</code>.</li>
</ul>
<h3 id="process-each-document-and-extract-summary">Process each document and extract summary<a href="#process-each-document-and-extract-summary" aria-label="Direct link to Process each document and extract summary" title="Direct link to Process each document and extract summary">​</a></h3>
<p>Define a <code>DocumentSummary</code> data class to extract the summary of a document.</p>
<div><pre tabindex="0"><code><span><span>@dataclasses</span><span>.</span><span>dataclass</span><span></span><br></span><span><span></span><span>class</span><span> </span><span>DocumentSummary</span><span>:</span><span></span><br></span><span><span>    title</span><span>:</span><span> </span><span>str</span><span></span><br></span><span><span>    summary</span><span>:</span><span> </span><span>str</span><br></span></code></pre></div>
<p>Within the flow, use <a href="https://cocoindex.io/docs/ops/functions#extractbyllm" target="_blank" rel="noopener noreferrer"><code>cocoindex.functions.ExtractByLlm</code></a> for structured output.</p>
<div><pre tabindex="0"><code><span><span>with</span><span> data_scope</span><span>[</span><span>"documents"</span><span>]</span><span>.</span><span>row</span><span>(</span><span>)</span><span> </span><span>as</span><span> doc</span><span>:</span><span></span><br></span><span><span>    doc</span><span>[</span><span>"summary"</span><span>]</span><span> </span><span>=</span><span> doc</span><span>[</span><span>"content"</span><span>]</span><span>.</span><span>transform</span><span>(</span><span></span><br></span><span><span>            cocoindex</span><span>.</span><span>functions</span><span>.</span><span>ExtractByLlm</span><span>(</span><span></span><br></span><span><span>                llm_spec</span><span>=</span><span>cocoindex</span><span>.</span><span>LlmSpec</span><span>(</span><span></span><br></span><span><span>                    api_type</span><span>=</span><span>cocoindex</span><span>.</span><span>LlmApiType</span><span>.</span><span>OPENAI</span><span>,</span><span> model</span><span>=</span><span>"gpt-4o"</span><span>)</span><span>,</span><span></span><br></span><span><span>                output_type</span><span>=</span><span>DocumentSummary</span><span>,</span><span></span><br></span><span><span>                instruction</span><span>=</span><span>"Please summarize the content of the document."</span><span>)</span><span>)</span><span></span><br></span><span><span></span><br></span><span><span>    document_node</span><span>.</span><span>collect</span><span>(</span><span></span><br></span><span><span>        filename</span><span>=</span><span>doc</span><span>[</span><span>"filename"</span><span>]</span><span>,</span><span> title</span><span>=</span><span>doc</span><span>[</span><span>"summary"</span><span>]</span><span>[</span><span>"title"</span><span>]</span><span>,</span><span></span><br></span><span><span>        summary</span><span>=</span><span>doc</span><span>[</span><span>"summary"</span><span>]</span><span>[</span><span>"summary"</span><span>]</span><span>)</span><br></span></code></pre></div>
<p><code>doc["summary"]</code> adds a new column to the KTable <code>data_scope["documents"]</code>.</p>
<p><img decoding="async" loading="lazy" alt="Adding summary to the each document" src="https://cocoindex.io/blogs/assets/images/data-add-summary-db507681f2cebd1473d99cd6d1e4cf08.png" width="1484" height="431"></p>

<p>Define a data class to represent relationship for the LLM extraction.</p>
<div><pre tabindex="0"><code><span><span>@dataclasses</span><span>.</span><span>dataclass</span><span></span><br></span><span><span></span><span>class</span><span> </span><span>Relationship</span><span>:</span><span></span><br></span><span><span>    </span><span>"""</span><br></span><span><span>    Describe a relationship between two entities.</span><br></span><span><span>    Subject and object should be Core CocoIndex concepts only, should be nouns. For example, `CocoIndex`, `Incremental Processing`, `ETL`,  `Data` etc.</span><br></span><span><span>    """</span><span></span><br></span><span><span>    subject</span><span>:</span><span> </span><span>str</span><span></span><br></span><span><span>    predicate</span><span>:</span><span> </span><span>str</span><span></span><br></span><span><span>    </span><span>object</span><span>:</span><span> </span><span>str</span><br></span></code></pre></div>
<p>The Data class defines a knowledge graph relationship. We recommend putting detailed instructions in the class-level docstring to help the LLM extract relationships correctly.</p>
<ul>
<li><code>subject</code>: Represents the entity the statement is about (e.g., 'CocoIndex').</li>
<li><code>predicate</code>: Describes the type of relationship or property connecting the subject and object (e.g., 'supports').</li>
<li><code>object</code>: Represents the entity or value that the subject is related to via the predicate (e.g., 'Incremental Processing').</li>
</ul>
<p>This structure represents facts like "CocoIndex supports Incremental Processing". Its graph representation is:</p>
<p><img decoding="async" loading="lazy" alt="Presentation of a relationship in a knowledge graph" src="https://cocoindex.io/blogs/assets/images/cocoindex-triple-explanation-6b40f03edfde69f08176572348d6fadc.png" width="1365" height="354"></p>
<p>Next, we will use <code>cocoindex.functions.ExtractByLlm</code> to extract the relationships from the document.</p>
<div><pre tabindex="0"><code><span><span>doc</span><span>[</span><span>"relationships"</span><span>]</span><span> </span><span>=</span><span> doc</span><span>[</span><span>"content"</span><span>]</span><span>.</span><span>transform</span><span>(</span><span></span><br></span><span><span>    cocoindex</span><span>.</span><span>functions</span><span>.</span><span>ExtractByLlm</span><span>(</span><span></span><br></span><span><span>        llm_spec</span><span>=</span><span>cocoindex</span><span>.</span><span>LlmSpec</span><span>(</span><span></span><br></span><span><span>            api_type</span><span>=</span><span>cocoindex</span><span>.</span><span>LlmApiType</span><span>.</span><span>OPENAI</span><span>,</span><span> </span><br></span><span><span>            model</span><span>=</span><span>"gpt-4o"</span><span></span><br></span><span><span>        </span><span>)</span><span>,</span><span></span><br></span><span><span>        output_type</span><span>=</span><span>list</span><span>[</span><span>Relationship</span><span>]</span><span>,</span><span></span><br></span><span><span>        instruction</span><span>=</span><span>(</span><span></span><br></span><span><span>            </span><span>"Please extract relationships from CocoIndex documents. "</span><span></span><br></span><span><span>            </span><span>"Focus on concepts and ignore examples and code. "</span><span></span><br></span><span><span>        </span><span>)</span><span></span><br></span><span><span>    </span><span>)</span><span></span><br></span><span><span></span><span>)</span><br></span></code></pre></div>
<p><code>doc["relationships"]</code> adds a new field <code>relationships</code> to each document. <code>output_type=list[Relationship]</code> specifies that the output of the transformation is a <a href="https://cocoindex.io/docs/core/data_types#LTable" target="_blank" rel="noopener noreferrer">LTable</a>.</p>
<p><img decoding="async" loading="lazy" alt="Adding relationships to each document" src="https://cocoindex.io/blogs/assets/images/data-add-relationship-4ea24f4b9af454350fad48b439614a94.png" width="1484" height="828"></p>
<h3 id="collect-relationships">Collect relationships<a href="#collect-relationships" aria-label="Direct link to Collect relationships" title="Direct link to Collect relationships">​</a></h3>
<div><pre tabindex="0"><code><span><span>with</span><span> doc</span><span>[</span><span>"relationships"</span><span>]</span><span>.</span><span>row</span><span>(</span><span>)</span><span> </span><span>as</span><span> relationship</span><span>:</span><span></span><br></span><span><span>    </span><span># relationship between two entities</span><span></span><br></span><span><span>    entity_relationship</span><span>.</span><span>collect</span><span>(</span><span></span><br></span><span><span>        </span><span>id</span><span>=</span><span>cocoindex</span><span>.</span><span>GeneratedField</span><span>.</span><span>UUID</span><span>,</span><span></span><br></span><span><span>        subject</span><span>=</span><span>relationship</span><span>[</span><span>"subject"</span><span>]</span><span>,</span><span></span><br></span><span><span>        </span><span>object</span><span>=</span><span>relationship</span><span>[</span><span>"object"</span><span>]</span><span>,</span><span></span><br></span><span><span>        predicate</span><span>=</span><span>relationship</span><span>[</span><span>"predicate"</span><span>]</span><span>,</span><span></span><br></span><span><span>    </span><span>)</span><span></span><br></span><span><span>    </span><span># mention of an entity in a document, for subject</span><span></span><br></span><span><span>    entity_mention</span><span>.</span><span>collect</span><span>(</span><span></span><br></span><span><span>        </span><span>id</span><span>=</span><span>cocoindex</span><span>.</span><span>GeneratedField</span><span>.</span><span>UUID</span><span>,</span><span> entity</span><span>=</span><span>relationship</span><span>[</span><span>"subject"</span><span>]</span><span>,</span><span></span><br></span><span><span>        filename</span><span>=</span><span>doc</span><span>[</span><span>"filename"</span><span>]</span><span>,</span><span></span><br></span><span><span>    </span><span>)</span><span></span><br></span><span><span>    </span><span># mention of an entity in a document, for object</span><span></span><br></span><span><span>    entity_mention</span><span>.</span><span>collect</span><span>(</span><span></span><br></span><span><span>        </span><span>id</span><span>=</span><span>cocoindex</span><span>.</span><span>GeneratedField</span><span>.</span><span>UUID</span><span>,</span><span> entity</span><span>=</span><span>relationship</span><span>[</span><span>"object"</span><span>]</span><span>,</span><span></span><br></span><span><span>        filename</span><span>=</span><span>doc</span><span>[</span><span>"filename"</span><span>]</span><span>,</span><span></span><br></span><span><span>    </span><span>)</span><span></span><br></span><span><span></span><br></span></code></pre></div>
<ul>
<li><code>entity_relationship</code> collects relationships between subjects and objects.</li>
<li><code>entity_mention</code> collects mentions of entities (as subjects or objects) in the document separately. For example, <code>core/basics.mdx</code> has a sentence <code>CocoIndex supports Incremental Processing</code>. We want to collect:<!-- -->
<ul>
<li><code>core/basics.mdx</code> mentions <code>CocoIndex</code>.</li>
<li><code>core/basics.mdx</code> mentions <code>Incremental Processing</code>.</li>
</ul>
</li>
</ul>
<h3 id="build-knowledge-graph">Build knowledge graph<a href="#build-knowledge-graph" aria-label="Direct link to Build knowledge graph" title="Direct link to Build knowledge graph">​</a></h3>
<h4 id="basic-concepts">Basic concepts<a href="#basic-concepts" aria-label="Direct link to Basic concepts" title="Direct link to Basic concepts">​</a></h4>
<p>All nodes for Neo4j need two things:</p>
<ol>
<li>Label: The type of the node. E.g., <code>Document</code>, <code>Entity</code>.</li>
<li>Primary key field: The field that uniquely identifies the node. E.g., <code>filename</code> for <code>Document</code> nodes.</li>
</ol>
<p>CocoIndex uses the primary key field to match the nodes and deduplicate them. If you have multiple nodes with the same primary key, CocoIndex keeps only one of them.</p>
<p><img decoding="async" loading="lazy" alt="Deduplication of nodes with the same primary key" src="https://cocoindex.io/blogs/assets/images/deduplicate-6feb8a0f7caa9da640436f7dc18715da.png" width="2750" height="825"></p>
<p>There are two ways to map nodes:</p>
<ol>
<li>When you have a collector just for the node, you can directly export it to Neo4j.</li>
<li>When you have a collector for relationships connecting to the node, you can map nodes from selected fields in the relationship collector. You must declare a node label and primary key field.</li>
</ol>
<h4 id="configure-neo4j-connection">Configure Neo4j connection:<a href="#configure-neo4j-connection" aria-label="Direct link to Configure Neo4j connection:" title="Direct link to Configure Neo4j connection:">​</a></h4>
<div><pre tabindex="0"><code><span><span>conn_spec </span><span>=</span><span> cocoindex</span><span>.</span><span>add_auth_entry</span><span>(</span><span></span><br></span><span><span>    </span><span>"Neo4jConnection"</span><span>,</span><span></span><br></span><span><span>    cocoindex</span><span>.</span><span>storages</span><span>.</span><span>Neo4jConnection</span><span>(</span><span></span><br></span><span><span>        uri</span><span>=</span><span>"bolt://localhost:7687"</span><span>,</span><span></span><br></span><span><span>        user</span><span>=</span><span>"neo4j"</span><span>,</span><span></span><br></span><span><span>        password</span><span>=</span><span>"cocoindex"</span><span>,</span><span></span><br></span><span><span></span><span>)</span><span>)</span><br></span></code></pre></div>
<h4 id="export-document-nodes-to-neo4j">Export <code>Document</code> nodes to Neo4j<a href="#export-document-nodes-to-neo4j" aria-label="Direct link to export-document-nodes-to-neo4j" title="Direct link to export-document-nodes-to-neo4j">​</a></h4>
<p><img decoding="async" loading="lazy" alt="Exporting document nodes to Neo4j" src="https://cocoindex.io/blogs/assets/images/export-document-04623da10bc102aff1d163a14eba7cd4.png" width="1941" height="600"></p>
<div><pre tabindex="0"><code><span><span>document_node</span><span>.</span><span>export</span><span>(</span><span></span><br></span><span><span>    </span><span>"document_node"</span><span>,</span><span></span><br></span><span><span>    cocoindex</span><span>.</span><span>storages</span><span>.</span><span>Neo4j</span><span>(</span><span></span><br></span><span><span>        connection</span><span>=</span><span>conn_spec</span><span>,</span><span></span><br></span><span><span>        mapping</span><span>=</span><span>cocoindex</span><span>.</span><span>storages</span><span>.</span><span>Nodes</span><span>(</span><span>label</span><span>=</span><span>"Document"</span><span>)</span><span>)</span><span>,</span><span></span><br></span><span><span>    primary_key_fields</span><span>=</span><span>[</span><span>"filename"</span><span>]</span><span>,</span><span></span><br></span><span><span></span><span>)</span><br></span></code></pre></div>
<p>This exports Neo4j nodes with label <code>Document</code> from the <code>document_node</code> collector.</p>
<ul>
<li>It declares Neo4j node label <code>Document</code>. It specifies <code>filename</code> as the primary key field.</li>
<li>It carries all the fields from <code>document_node</code> collector to Neo4j nodes with label <code>Document</code>.</li>
</ul>
<h4 id="export-relationship-and-entity-nodes-to-neo4j">Export <code>RELATIONSHIP</code> and <code>Entity</code> nodes to Neo4j<a href="#export-relationship-and-entity-nodes-to-neo4j" aria-label="Direct link to export-relationship-and-entity-nodes-to-neo4j" title="Direct link to export-relationship-and-entity-nodes-to-neo4j">​</a></h4>
<p><img decoding="async" loading="lazy" alt="Exporting relationships and entities to Neo4j" src="https://cocoindex.io/blogs/assets/images/export-relationship-6c28d094bf336190e41cc904b7ca05c4.png" width="1941" height="900"></p>
<p>We don't have explicit collector for <code>Entity</code> nodes.
They are part of the <code>entity_relationship</code> collector and fields are collected during the relationship extraction.</p>
<p>To export them as Neo4j nodes, we need to first declare <code>Entity</code> nodes.</p>
<div><pre tabindex="0"><code><span><span>flow_builder</span><span>.</span><span>declare</span><span>(</span><span></span><br></span><span><span>    cocoindex</span><span>.</span><span>storages</span><span>.</span><span>Neo4jDeclaration</span><span>(</span><span></span><br></span><span><span>        connection</span><span>=</span><span>conn_spec</span><span>,</span><span></span><br></span><span><span>        nodes_label</span><span>=</span><span>"Entity"</span><span>,</span><span></span><br></span><span><span>        primary_key_fields</span><span>=</span><span>[</span><span>"value"</span><span>]</span><span>,</span><span></span><br></span><span><span>    </span><span>)</span><span></span><br></span><span><span></span><span>)</span><br></span></code></pre></div>
<p>Next, export the <code>entity_relationship</code> to Neo4j.</p>
<div><pre tabindex="0"><code><span><span>entity_relationship</span><span>.</span><span>export</span><span>(</span><span></span><br></span><span><span>    </span><span>"entity_relationship"</span><span>,</span><span></span><br></span><span><span>    cocoindex</span><span>.</span><span>storages</span><span>.</span><span>Neo4j</span><span>(</span><span></span><br></span><span><span>        connection</span><span>=</span><span>conn_spec</span><span>,</span><span></span><br></span><span><span>        mapping</span><span>=</span><span>cocoindex</span><span>.</span><span>storages</span><span>.</span><span>Relationships</span><span>(</span><span></span><br></span><span><span>            rel_type</span><span>=</span><span>"RELATIONSHIP"</span><span>,</span><span></span><br></span><span><span>            source</span><span>=</span><span>cocoindex</span><span>.</span><span>storages</span><span>.</span><span>NodeFromFields</span><span>(</span><span></span><br></span><span><span>                label</span><span>=</span><span>"Entity"</span><span>,</span><span></span><br></span><span><span>                fields</span><span>=</span><span>[</span><span></span><br></span><span><span>                    cocoindex</span><span>.</span><span>storages</span><span>.</span><span>TargetFieldMapping</span><span>(</span><span></span><br></span><span><span>                        source</span><span>=</span><span>"subject"</span><span>,</span><span> target</span><span>=</span><span>"value"</span><span>)</span><span>,</span><span></span><br></span><span><span>                </span><span>]</span><span></span><br></span><span><span>            </span><span>)</span><span>,</span><span></span><br></span><span><span>            target</span><span>=</span><span>cocoindex</span><span>.</span><span>storages</span><span>.</span><span>NodeFromFields</span><span>(</span><span></span><br></span><span><span>                label</span><span>=</span><span>"Entity"</span><span>,</span><span></span><br></span><span><span>                fields</span><span>=</span><span>[</span><span></span><br></span><span><span>                    cocoindex</span><span>.</span><span>storages</span><span>.</span><span>TargetFieldMapping</span><span>(</span><span></span><br></span><span><span>                        source</span><span>=</span><span>"object"</span><span>,</span><span> target</span><span>=</span><span>"value"</span><span>)</span><span>,</span><span></span><br></span><span><span>                </span><span>]</span><span></span><br></span><span><span>            </span><span>)</span><span>,</span><span></span><br></span><span><span>        </span><span>)</span><span>,</span><span></span><br></span><span><span>    </span><span>)</span><span>,</span><span></span><br></span><span><span>    primary_key_fields</span><span>=</span><span>[</span><span>"id"</span><span>]</span><span>,</span><span></span><br></span><span><span></span><span>)</span><span></span><br></span><span><span></span><span>)</span><br></span></code></pre></div>
<p>The <code>cocoindex.storages.Relationships</code> declares how to map relationships in Neo4j.</p>
<p>In a relationship, there's:</p>
<ol>
<li>A source node and a target node.</li>
<li>A relationship connecting the source and target.
Note that different relationships may share the same source and target nodes.</li>
</ol>
<p><code>NodeFromFields</code> takes the fields from the <code>entity_relationship</code> collector and creates <code>Entity</code> nodes.</p>
<h4 id="export-the-entity_mention-to-neo4j">Export the <code>entity_mention</code> to Neo4j.<a href="#export-the-entity_mention-to-neo4j" aria-label="Direct link to export-the-entity_mention-to-neo4j" title="Direct link to export-the-entity_mention-to-neo4j">​</a></h4>
<p><img decoding="async" loading="lazy" alt="Exporting entity mention to Neo4j" src="https://cocoindex.io/blogs/assets/images/example-explanation-49f2a253cb339f43a69a6b654728063e.png" width="1731" height="837"></p>
<div><pre tabindex="0"><code><span><span>entity_mention</span><span>.</span><span>export</span><span>(</span><span></span><br></span><span><span>    </span><span>"entity_mention"</span><span>,</span><span></span><br></span><span><span>    cocoindex</span><span>.</span><span>storages</span><span>.</span><span>Neo4j</span><span>(</span><span></span><br></span><span><span>        connection</span><span>=</span><span>conn_spec</span><span>,</span><span></span><br></span><span><span>        mapping</span><span>=</span><span>cocoindex</span><span>.</span><span>storages</span><span>.</span><span>Relationships</span><span>(</span><span></span><br></span><span><span>            rel_type</span><span>=</span><span>"MENTION"</span><span>,</span><span></span><br></span><span><span>            source</span><span>=</span><span>cocoindex</span><span>.</span><span>storages</span><span>.</span><span>NodesFromFields</span><span>(</span><span></span><br></span><span><span>                label</span><span>=</span><span>"Document"</span><span>,</span><span></span><br></span><span><span>                fields</span><span>=</span><span>[</span><span>cocoindex</span><span>.</span><span>storages</span><span>.</span><span>TargetFieldMapping</span><span>(</span><span>"filename"</span><span>)</span><span>]</span><span>,</span><span></span><br></span><span><span>            </span><span>)</span><span>,</span><span></span><br></span><span><span>            target</span><span>=</span><span>cocoindex</span><span>.</span><span>storages</span><span>.</span><span>NodesFromFields</span><span>(</span><span></span><br></span><span><span>                label</span><span>=</span><span>"Entity"</span><span>,</span><span></span><br></span><span><span>                fields</span><span>=</span><span>[</span><span>cocoindex</span><span>.</span><span>storages</span><span>.</span><span>TargetFieldMapping</span><span>(</span><span></span><br></span><span><span>                    source</span><span>=</span><span>"entity"</span><span>,</span><span> target</span><span>=</span><span>"value"</span><span>)</span><span>]</span><span>,</span><span></span><br></span><span><span>            </span><span>)</span><span>,</span><span></span><br></span><span><span>        </span><span>)</span><span>,</span><span></span><br></span><span><span>    </span><span>)</span><span>,</span><span></span><br></span><span><span>    primary_key_fields</span><span>=</span><span>[</span><span>"id"</span><span>]</span><span>,</span><span></span><br></span><span><span></span><span>)</span><br></span></code></pre></div>
<p>Similarly here, we export <code>entity_mention</code> to Neo4j Relationships using <code>cocoindex.storages.Relationships</code>.
It creates relationships by:</p>
<ul>
<li>Creating <code>Document</code> nodes and <code>Entity</code> nodes from the <code>entity_mention</code> collector.</li>
<li>Connecting <code>Document</code> nodes and <code>Entity</code> nodes with relationship <code>MENTION</code>.</li>
</ul>
<h3 id="main-function">Main function<a href="#main-function" aria-label="Direct link to Main function" title="Direct link to Main function">​</a></h3>
<p>Finally, the main function for the flow initializes the CocoIndex flow and runs it.</p>
<div><pre tabindex="0"><code><span><span> </span><span>@cocoindex</span><span>.</span><span>main_fn</span><span>(</span><span>)</span><span></span><br></span><span><span></span><span>def</span><span> </span><span>_run</span><span>(</span><span>)</span><span>:</span><span></span><br></span><span><span>    </span><span>pass</span><span></span><br></span><span><span></span><br></span><span><span></span><span>if</span><span> __name__ </span><span>==</span><span> </span><span>"__main__"</span><span>:</span><span></span><br></span><span><span>    load_dotenv</span><span>(</span><span>override</span><span>=</span><span>True</span><span>)</span><span></span><br></span><span><span>    _run</span><span>(</span><span>)</span><br></span></code></pre></div>
<h2 id="query-and-test-your-index">Query and test your index<a href="#query-and-test-your-index" aria-label="Direct link to Query and test your index" title="Direct link to Query and test your index">​</a></h2>
<p>🎉 Now you are all set!</p>
<ol>
<li>
<p>Install the dependencies:</p>

</li>
<li>
<p>Run following commands to setup and update the index.</p>
<div><pre tabindex="0"><code><span><span>python main.py cocoindex setup</span><br></span><span><span>python main.py cocoindex update</span><br></span></code></pre></div>
<p>You'll see the index updates state in the terminal. For example, you'll see the following output:</p>
<div><pre tabindex="0"><code><span><span>documents: 7 added, 0 removed, 0 updated</span><br></span></code></pre></div>
</li>
<li>
<p>(Optional) I used CocoInsight to troubleshoot the index generation and understand the data lineage of the pipeline.
It is in free beta now, you can give it a try. Run following command to start CocoInsight:</p>
<div><pre tabindex="0"><code><span><span>python3 main.py cocoindex server -c https://cocoindex.io</span><br></span></code></pre></div>
<p>And then open the url <a href="https://cocoindex.io/cocoinsight" target="_blank" rel="noopener noreferrer">https://cocoindex.io/cocoinsight</a>.  It just connects to your local CocoIndex server, with Zero pipeline data retention.</p>
<p><img decoding="async" loading="lazy" alt="CocoInsight" src="https://cocoindex.io/blogs/assets/images/cocoinsight-48cd25b497e0e5ac6bb8a7927be44eba.png" width="2860" height="1216"></p>
</li>
</ol>
<h3 id="browse-the-knowledge-graph">Browse the knowledge graph<a href="#browse-the-knowledge-graph" aria-label="Direct link to Browse the knowledge graph" title="Direct link to Browse the knowledge graph">​</a></h3>
<p>After the knowledge graph is built, you can explore the knowledge graph you built in Neo4j Browser.</p>
<p>For the dev environment, you can connect to Neo4j browser using credentials:</p>
<ul>
<li>username: <code>Neo4j</code></li>
<li>password: <code>cocoindex</code>
which is pre-configured in our docker compose <a href="https://raw.githubusercontent.com/cocoindex-io/cocoindex/refs/heads/main/dev/Neo4j.yaml" target="_blank" rel="noopener noreferrer">config.yaml</a>.</li>
</ul>
<p>You can open it at <a href="http://localhost:7474/" target="_blank" rel="noopener noreferrer">http://localhost:7474</a>, and run the following Cypher query to get all relationships:</p>

<p><img decoding="async" loading="lazy" alt="Neo4j Browser for CocoIndex Docs" src="https://cocoindex.io/blogs/assets/images/neo4j-for-coco-docs-333f04ecd9b74ffc70b1bd89415a57e5.png" width="2732" height="1274"></p>
<h2 id="support-us">Support us<a href="#support-us" aria-label="Direct link to Support us" title="Direct link to Support us">​</a></h2>
<p>We are constantly improving, and more features and examples are coming soon.
If you love this article, please give us a star ⭐ at <a href="https://github.com/cocoindex-io/cocoindex" target="_blank" rel="noopener noreferrer">GitHub repo</a> to help us grow.</p>
<p>Thanks for reading!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dusk OS (177 pts)]]></title>
            <link>https://duskos.org/</link>
            <guid>43976862</guid>
            <pubDate>Tue, 13 May 2025 19:44:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://duskos.org/">https://duskos.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43976862">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p>Dusk OS is a 32-bit Forth and big brother to <a href="http://collapseos.org/">Collapse OS</a>. Its
<a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/design/purpose.txt">primary purpose</a> is to be maximally useful during the <a href="http://collapseos.org/why.html">first
stage of civilizational collapse</a>, that is, when we can't produce
modern computers anymore but that there's still many modern computers around.</p>
<p>It does so by aggressively prioritizing <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/design/simple.txt">simplicity</a> at the
cost of <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/design/limits.txt">unorthodox constraints</a>, while also aiming to make
<a href="#operator">operators</a> happy.</p>
<p>Dusk OS innovates by having an <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/comp/c.txt">"almost C" compiler</a> allowing it to
piggy-back on UNIX C code, through a modest <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/design/port.txt">porting effort</a>, to
reach its goals and stay true to its design constraints with a minimal effort.</p>
<p>The end result is a system that has a very high "power density", high enough to
issue a <a href="#challenge">challenge to the conventional software culture</a>.</p>
<p>You can read on for more details, but the impatient among you might prefer
taking the <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/tour.txt">Dusk Tour</a>.</p>
<h2>Status</h2>
<ul>
<li>Can run on:<ul>
<li>Bare metal on those CPUs:<ul>
<li>i386 (BIOS or EFI)</li>
<li>amd64 (EFI)</li>
<li>ARM</li>
<li>RISC-V</li>
<li>m68k</li>
<li><a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/deploy/README.md">List of supported targets</a></li>
</ul>
</li>
<li><a href="https://git.sr.ht/~vdupras/dusk-wasm">WebAssembly</a></li>
<li><a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/usermode/README.md">At native speed on top of "regular" OSes</a></li>
</ul>
</li>
<li>Can read, write, create and boot from FAT12/FAT16 (no FAT32 for now) volumes.</li>
<li>Has an <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/comp/c.txt">"almost C" compiler</a> which, despite its "almost" qualifier,
  is capable of compiling quite complex "real world" code.</li>
<li>Very small footprint. Designed to run smoothly on 30 years old machines.
  Almost anything that has a 32-bit CPU in fact.</li>
<li>Simple and terse. For example, the total lines of code involved in having a
  fully booted bare metal i386 PC Dusk system running on a FAT16 with a C
  compiler, i386 assembler and a Grid text editor is less than 6000.</li>
<li>It completely self-hosts on all its targets. That is, a machine running Dusk
  OS has all the tools necessary to either improve itself or produce a media to
  run Dusk OS on another machine.</li>
<li>In order to bootstrap itself from something else than itself, it also has a
  <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/posix/README.md">POSIX-compatible VM</a> written in C that is able to generate images
  for all its targets.</li>
<li>Excluding the <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/oberon.txt">Oberon system (WIP)</a>, documented externally, has more
  documentation than code.</li>
<li>Licensed under CC0, effectively placing it in the public domain.</li>
</ul>
<p>List of ported codebases:</p>
<ul>
<li>The <code>puff()</code> algorithm from <a href="https://github.com/madler/zlib">zlib</a></li>
<li>DWC USB controller driver from Plan 9</li>
<li>Michael Schierl's RISC5 emulator allowing Dusk to run
  <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/oberon/README.md">Project Oberon</a>.</li>
</ul>
<p>List of homegrown applications:</p>
<ul>
<li><a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/comp/c.txt">C Compiler</a></li>
<li><a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/comp/oberon.txt">Oberon Compiler (WIP)</a></li>
<li><a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/comp/lisp.txt">Some sort of a Lisp</a></li>
<li><a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/text/ed.txt">Text editor</a></li>
<li><a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/bin/gbe.txt">Binary editor</a></li>
<li><a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/emul/uxn.txt">uxn</a> and <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/emul/varvara.txt">varvara</a></li>
<li><a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/asm/x86.txt">i386/amd64 assembler</a> and <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/asm/x86d.txt">disassembler (i386-only for now)</a></li>
<li><a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/asm/arm.txt">ARM assembler</a> and <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/asm/armd.txt">disassembler</a></li>
<li><a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/asm/riscv.txt">RISC-V assembler</a></li>
<li><a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/emul/6502.txt">6502 emulator</a></li>
</ul>
<h2>How terse is that thing anyway?</h2>
<pre><code>$ git checkout v19
$ make dusk
$ ./dusk -f fs/home/codesz.fs
Kilobytes of code in Dusk OS
Everything except /doc /data /tests     728
Documentation                           714
Boot payload minus HAL                  50
C compiler                              49
Oberon compiler                         43
Oberon system                           98
Lisp                                    16
Text Editor                             13
All USB drivers                         62
EFI drivers                             14
This script                             3

CPU-specific... i386  amd64 arm   riscv m68k 
Assembler       11    same  5     14    5
Kernel          10    9     13    14    9
HAL             6     6     7     8     7
EFI interface   4     4
</code></pre>
<p>As a reference, running <code>wc -c *.c *.h lib/* include/*</code> on <a href="https://www.bellard.org/tcc/">TinyCC</a>
0.9.27's source code yields 1420 kilobytes of code.</p>
<h2>Getting Dusk</h2>
<p>Dusk OS's git repository is <a href="https://git.sr.ht/~vdupras/duskos">hosted on sourcehut</a>. The regular way of
getting it will be to clone the repository from there. You can try to run it
directly from the <code>master</code> branch or you can target one of its
<a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/CHANGELOG.md">releases</a> using the release tags (for example <code>v11</code>).</p>
<p>More information about how to build and run Dusk OS is available in the
<a href="https://git.sr.ht/~vdupras/duskos">root README</a> as well as in its <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc">documentation</a>.</p>
<p>Becoming a Dusk <a href="#operator">operator</a> is an involving process. It's possible
that you're interested in tasting a bit of Dusk's power before you dive into
this wonderful adventure of learning Forth and Dusk. You're in luck, there's
the <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/tour.txt">Dusk Tour</a> which doesn't require prior Forth knowledge and allows
you to dip your toes in it.</p>
<p>For deployments to actual machines, there's also the
<a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/deploy/README.md">Dusk OS Deployments collection</a> that can be of use.</p>
<p>There is also the option of building <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/usermode/README.md">Dusk Packages</a> on top of other
OSes. You can look at <a href="https://git.sr.ht/~vdupras/dusk-examples">Dusk Packages examples</a> for a quick start.</p>
<h2>Discuss Dusk</h2>
<p><a href="https://duskos.org/discuss.html">Details here</a></p>
<h2>Funding</h2>
<p>I began a sabbatical from working on modern technology in 2023, which I hope to
extend indefinitely. To that end, I've put up a <a href="https://duskos.org/services.html">service offering</a>
that I hope might help me towards that goal. On that front, I have good reasons
to be optimistic as the <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/SPONSORS">sponsors</a> ball is already rolling!</p>
<p>Moreover, I believe that my work on Dusk OS and Collapse OS could be
significant enough to some people that it could end up being <a href="https://duskos.org/funding.html">philanthropically
funded</a>. If you're rich and inspired by this work, please consider it.</p>
<h2>Why build this OS?</h2>
<p>Most modern operating systems can do whatever we want them to do. Why do we
need another one? Simplicity.</p>
<p>It's difficult to predict post-collapse conditions, but we can suppose that
many <a href="#operator">operators</a> will need to use their machines in novel and
creative ways. Hackability of the operating system then becomes paramount. Open
source modern operating systems all can be modified to fit its user's needs,
but their complexity limits the likelihood that the user is able to do so. A
simpler OS increases this likelihood.</p>
<p>But we can't have our cake and eat it too, right? Either you have a simple toy
OS or a complex one. Well, maybe not?</p>
<p>Its instigator (and, I guess, some of its <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/AUTHORS">authors</a> and <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/SPONSORS">sponsors</a> too)
believes that in the history of computing, Forth has been under-explored. Its
approach to simplicity is, I think, revolutionary. It has significant
shortcomings when system specifications become more complex (Forth hates
complexity and doesn't manage it well), but I believe it possible to elegantly
marry it with languages that like complexity better.</p>
<p>This mix could provide the operator with computing powers rarely seen with
other approaches. We've got to try it.</p>

<h3>A challenge to the conventional software culture</h3>
<p>While the primary purpose of this operating system is related to civilizational
collapse, it's also in big part a reaction to the modern software stack. That
stack is disgustingly complicated. It's a product of the compounded effect of a
software culture that breeds complexity and had the opportunity to build upon
itself over decades, unchecked and unchallenged, oozing its inscrutable pus at
every corner.</p>
<p>The further we let that culture creep out, the harder it is to get out of it.
Hardware follows it dutifully -- and in the same spirit of spurious complexity
-- making clean slate approaches more and more out of reach... but not
impossible yet!</p>
<p>Dusk OS has an amazing "power density", that is, it packs a lot of power in a
very small package. This density allows it to do many things that the modern
stack can do at a fraction of the complexity cost.</p>
<p>It is partly intended as a wake up call to software developers: the regular way
to develop software today is stupid and wasteful and has been for the last 30
years. This stupidity and wastefulness feeds itself and makes us design bigger
and stupider<sup id="fnref:1"><a href="#fn:1">1</a></sup> hardware to cater to bigger and stupider software.</p>
<p>We don't need this. All this sweat, tears and blood (because yes, blood is
involved in having a global supply chain enabling these crazily complex
machines we produce) is futile because the conventional approach to computing is
an evolutionary dead end.</p>
<p>There are other ways and we urgently need to explore them. Forth is such a way.
It had a hard time adapting to bigger machines because of its affinity for
simplicity which makes it a bad fit to spurious hardware complexity.</p>
<p>Dusk OS aims to bridge that gap and be a Forth that, yes, is a bit more complex
than your run-of-the-mill Forth, but is careful to keep that very high "power
density" making Forth revolutionary and use that spurious power that modern
hardware gives us, thus competing better, feature-wise, with the modern software
stack.</p>
<h2>Features making Dusk OS special</h2>
<h3>A whole OS built from source on boot</h3>
<p>One thing that makes Dusk OS special is that it boots from a tiny kernel
weighting less than 4 <strong>kilobytes</strong>. From this tiny core, on boot, it
builds its way up, <em>from source code</em>, to a system that has a functional C
compiler, which then allows it to bootstrap itself some more, from source
code.</p>
<p>This peculiarity of Dusk OS has interesting properties. The nicest one, in my
humble opinion, is that this allows us to sidestep the <em>entire</em> problems of
binary compatibility and relocation and only deal with source compatibility.
So, no ELF, no binutils, only code that is designed to run from where it was
generated in the first place. This is so much simpler!</p>
<p>Object files? Global symbols? Nah. C functions are simple Forth words.</p>
<h3>Harmonized Assembly Layer</h3>
<p>Dusk features what we call the <a href="https://git.sr.ht/~vdupras/duskos/tree/master/item/fs/doc/hal.txt">Harmonized Assembly Layer</a> (HAL for short).
This is a cross-CPU assembler, on which the C compiler relies, which prioritizes
implementation and usage simplicity, but is also designed to generate efficient
native code.</p>
<h3>Shortest path to self-hosting for an "almost C" compiler</h3>
<p>Dusk OS self-hosts in about 500 lines of assembly and a few hundred lines of
Forth (the exact number depends on the target machine). From there, it
bootstraps to DuskCC, which is roughly 1200 lines of Forth code. To my
knowledge, Dusk OS is unique in that regard.</p>
<p>You can pick any C compiler that requires POSIX and it will automatically
require orders of magnitude more lines of code to bootstrap because you need
that POSIX system in addition to the C compiler. So even if you pick a small C
compiler such as tcc, you still need a POSIX system to build it, which is
usually in the millions of LOCs.</p>
<p>To be fair, Dusk OS is not the first project thinking of optimizing that path.
<a href="https://github.com/fosslinux/live-bootstrap">Efforts at making our modern software world bootstrappable</a>
lead to an "almost C", <a href="https://git.sr.ht/~oriansj/M2-Planet">M2-Planet</a> with a feature set comparable to
DuskCC with very few lines of code. M2-Planet itself is about 5K lines of code
and the various stages that lead to it are generally a few hundred lines each.
The project initially ran on top of regular kernels (as in "fat kernels with
lots of code"), but some bare metal stages (<a href="https://github.com/ironmeld/builder-hex0">1</a>,
<a href="https://git.stikonas.eu/andrius/stage0-uefi">2</a>) were created and now this little chain end up being
comparable to Dusk in terms of lines of code. Still more than Dusk, but in the
same ballpark.</p>
<p>Although this path is short and technically leads you to an "almost C"
compiler, you can hardly use it because it has no "real kernel" (those bare
metal stages mentioned above are enough to compile M2-Planet, but really not
much else, they're extremely limited) and no shell. You'll need those if you
want to use your shiny compiler.</p>
<p>One of your best picks, should you try this path, would be <a href="https://www.fiwix.org/">Fiwix</a>, a
minimal POSIX i386 kernel weighting less than 50K lines of C+asm. But then,
M2-Planet is not enough. You need to compile tcc (which M2-Planet can compile
after having applied a few patches) which weights 80K. Userspace is worse.
Bash+coreutils are 400K, even busybox is 190K. We still end up with a pretty
minimal and simple system, but it's still a lot more code than Dusk.</p>
<p>So, unless someone tells me about some option I don't know about, DuskCC is
quite innovative on the aspect of self-hosting path length.</p>
<h3>Dusk OS is pretty fast</h3>
<p>The code generated by Dusk OS holds pretty well to modern compilers with fancy
optimizations and millions of lines of code.</p>
<p>In a <a href="https://duskos.org/sieve.html">Byte Sieve benchmark done on a i386 NetBSD system</a>,
the DuskCC version of the sieve is almost as fast as GCC's unoptimized build and
the HAL's translation of the Sieve algorithm blows past GCC's unoptimized build
to nearly reach the speed of <code>GCC -O2</code>!</p>
<p>And it gets better! HAL's design allows efficiency to naturally "bubble up" to
higher level code with impressive results. For example, there's the
<a href="https://git.sr.ht/~vdupras/dusk-examples">"charcount" example</a> which pits Dusk's <code>rfind</code> against POSIX's
<code>regex(3)</code>. For this particular and simple use case (count occurrences of
characters range in a big file), Dusk is 15% faster than Debian bookworm amd64's
<code>regex(3)</code> implementation.</p>

<h2>Who is Dusk for?</h2>
<p>Dusk OS doesn't have users, but <em>operators</em>. What's the difference? Control.
You <em>use</em> a phone, you <em>use</em> a coffee machine, hell you even <em>use</em> a car these
days. But you <em>operate</em> a bulldozer, you <em>operate</em> a crane, you <em>operate</em> a
plane.</p>
<p>You <em>use</em> Linux, you <em>use</em> Windows. You <em>operate</em> Dusk OS.</p>
<p>Can you <em>operate</em> Linux? Sure, if you're some kind of god<sup id="fnref:2"><a href="#fn:2">2</a></sup>, in the same way
that you can <em>operate</em> a Tesla if you're a top Tesla engineer. But you're much
more likely to be able to <em>operate</em> a landmower than a Tesla.</p>
<p>When you <em>operate</em> a system, there is no problem that can arise that will make
you powerless. Sure, you can have a hardware failure that hopelessly breaks your
system, but at least you'll be able to identify that failure and know for sure
that there is no software solution or workaround. That's control.</p>
<p>The Dusk operator is someone who's <a href="http://collapseos.org/why.html#creative">creative</a>, close to hardware, can
read a datasheet. Dusk shines when one wants to poke around the hardware
without limit.</p>
<p>It compares favorably to other more complete OSes because there's no concurrent
process to mess with your poking and the driver structure is more approachable,
hackable due to its stricter scope and savvier target audience. Also, there is
no "userland". Every tool that Dusk provides or that the operator builds can be
directly used on system memory. No middleman.</p>
<p>Let's use an example. Let's say you're on a notebook that runs on a chipset of
Intel's ICHn family. You read the datasheet and see "oh, nice, there's an SPI
interface in there. Maybe that it's not hooked to anything on the notebook,
let's play with it."</p>
<p>Now, that chipset is very, very central to the computer. There are good chances,
on a BSD or Linux system, that if you begin poking around its registers, you'll
step on someone else toes and crash the system because, for example, of some
other process that needed to read from disk at the same time.</p>
<p>In Dusk, you could completely break the SATA controller, you'll still be golden
as long as you don't access mass storage. Because Dusk doesn't have
concurrency, you have tight control over what happen or doesn't happen on the
machine, so all you need to do is to avoid words that access mass storage. That
gives you ample wiggling space for your hacking session.</p>
<p>To be clear: this is also possible with a custom made BSD or Linux, but you're
going to have to strip a lot of pieces from your distro before you get there
and some of those pieces might be useful debugging tools which will be
difficult to retrofit because they need a wider system. You'll also need a
higher cognitive space to fit BSD/Linux wider abstractions in your mind.</p>
<h2>Linking to this website</h2>
<p>If you want to link to this website, please use <code>http://</code> links rather than
<code>https://</code> ones. While <code>http://</code> links are trivially "upgradable" to HTTPS, the
opposite is not.</p>
<p>Right now, this website is hosted on a service (sourcehut pages) that doesn't
offer the option of not using SSL, but eventually, it's possible that this
website ends up being served by a machine running Dusk OS. In that case, it
will not be served under SSL and all <code>https://</code> links will be broken.</p>
<h2>Related links</h2>
<ul>
<li><a href="http://tumbleforth.hardcoded.net/">Tumble Forth, a Forth vulgarization blog</a></li>
<li><a href="https://wiki.osdev.org/">OSDev wiki</a></li>
<li><a href="http://www.osdever.net/FreeVGA/home.htm">FreeVGA</a></li>
<li><a href="http://www.cs.cmu.edu/~ralf/files.html">Ralf Brown's x86 interrupt list</a></li>
<li><a href="https://www.versalogic.com/wp-content/themes/vsl-new/assets/resources/support/pdf/69030BG.pdf">VGA BIOS OEM Reference Guide</a></li>
</ul>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Airbnb is in midlife crisis mode (206 pts)]]></title>
            <link>https://www.wired.com/story/airbnb-is-in-midlife-crisis-mode-reinvention-app-services/</link>
            <guid>43976557</guid>
            <pubDate>Tue, 13 May 2025 19:17:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/airbnb-is-in-midlife-crisis-mode-reinvention-app-services/">https://www.wired.com/story/airbnb-is-in-midlife-crisis-mode-reinvention-app-services/</a>, See on <a href="https://news.ycombinator.com/item?id=43976557">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content" tabindex="-1"><article lang="en-US"><div><header data-testid="SplitScreenContentHeaderWrapper"><div><div><p>CEO Brian Chesky is spending hundreds of millions to relaunch his travel company as an everything app. Fitness! Food! Microdermabrasion? A WIRED exclusive.</p></div><div><p><span><picture><source media="(max-width: 767px)" srcset="https://media.wired.com/photos/6822476247f5e6d08106fda0/2:3/w_120,c_limit/WIRED_2025_05_07_Chesky_-109x.jpg 120w, https://media.wired.com/photos/6822476247f5e6d08106fda0/2:3/w_240,c_limit/WIRED_2025_05_07_Chesky_-109x.jpg 240w, https://media.wired.com/photos/6822476247f5e6d08106fda0/2:3/w_320,c_limit/WIRED_2025_05_07_Chesky_-109x.jpg 320w, https://media.wired.com/photos/6822476247f5e6d08106fda0/2:3/w_640,c_limit/WIRED_2025_05_07_Chesky_-109x.jpg 640w, https://media.wired.com/photos/6822476247f5e6d08106fda0/2:3/w_960,c_limit/WIRED_2025_05_07_Chesky_-109x.jpg 960w" sizes="100vw"><source media="(min-width: 768px)" srcset="https://media.wired.com/photos/6822476247f5e6d08106fda0/2:3/w_120,c_limit/WIRED_2025_05_07_Chesky_-109x.jpg 120w, https://media.wired.com/photos/6822476247f5e6d08106fda0/2:3/w_240,c_limit/WIRED_2025_05_07_Chesky_-109x.jpg 240w, https://media.wired.com/photos/6822476247f5e6d08106fda0/2:3/w_320,c_limit/WIRED_2025_05_07_Chesky_-109x.jpg 320w, https://media.wired.com/photos/6822476247f5e6d08106fda0/2:3/w_640,c_limit/WIRED_2025_05_07_Chesky_-109x.jpg 640w, https://media.wired.com/photos/6822476247f5e6d08106fda0/2:3/w_960,c_limit/WIRED_2025_05_07_Chesky_-109x.jpg 960w, https://media.wired.com/photos/6822476247f5e6d08106fda0/2:3/w_1280,c_limit/WIRED_2025_05_07_Chesky_-109x.jpg 1280w, https://media.wired.com/photos/6822476247f5e6d08106fda0/2:3/w_1600,c_limit/WIRED_2025_05_07_Chesky_-109x.jpg 1600w, https://media.wired.com/photos/6822476247f5e6d08106fda0/2:3/w_1920,c_limit/WIRED_2025_05_07_Chesky_-109x.jpg 1920w, https://media.wired.com/photos/6822476247f5e6d08106fda0/2:3/w_2240,c_limit/WIRED_2025_05_07_Chesky_-109x.jpg 2240w" sizes="100vw"><img alt="Airbnb CEO Brian Chesky sits in the middle of a chaotic scene" src="https://media.wired.com/photos/6822476247f5e6d08106fda0/2:3/w_2560%2Cc_limit/WIRED_2025_05_07_Chesky_-109x.jpg"></picture></span></p></div></div><div><p><span>PHOTOGRAPH: GABRIELA HASBUN</span></p></div></header></div><div data-testid="ArticlePageChunks" data-attribute-verso-pattern="article-body"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>As Brian Chesky</span> tells it, the reinvention of Airbnb started with the coup at OpenAI. On November 17, 2023, the board of OpenAI <a href="https://www.wired.com/story/sam-altman-firing-openai-future/">fired</a> company CEO Sam Altman. His friend Chesky leapt into action—publicly defending his pal on X, getting on the phone with Microsoft’s CEO, and throwing himself into the thick of Altman’s battle to retake OpenAI. Five days later Altman <a href="https://www.wired.com/story/sam-altman-officially-returns-to-openai-board-seat-microsoft/">prevailed</a>, and Chesky—“I was so jacked up,” he says—turned his buzzing mind to his own company, <a href="https://www.wired.com/tag/airbnb/">Airbnb</a>.</p><p>Thanksgiving weekend was beginning. The Chesky extended family had already held their turkey get-together a week earlier, and the Airbnb CEO had no holiday plan. He was completely alone in his sprawling San Francisco apartment except for Sophie, his golden retriever.</p><p>Still wired out of his mind from the cathartic corporate rescue, Chesky began to write. He wanted to bust the company he’d cofounded out of its pigeonhole of short-term home rentals. Amazon, he was fond of pointing out, was first an online bookstore before it became the everything store. Chesky had long believed that Airbnb should expand in a similar way. But things kept getting in the way—dealing with safety <a href="https://www.wired.com/story/airbnb-party-ban-shootings-police/">issues</a>, <a href="https://www.wired.com/story/new-york-city-airbnb-law-one-year-results/">fighting regulation</a>, coping with the existential crisis of a global pandemic. The company was in danger of being tagged with the word that ambitious entrepreneurs dread like the plague: <em>mature</em>.</p><p>Now Chesky was emboldened to lay out his vision. Home rentals are simply a service, so why stop there? Airbnb could be <em>the</em> platform for booking all sorts of services. While other apps cover specific sectors—food delivery, home maintenance, car rides—Chesky figured that Airbnb’s experience in attractively displaying homes, vetting hosts, and responding to crises could make it more trustworthy than competitors and therefore the go-to option for virtually anything.</p><p>In a frantic typing spree at the dining room table, on the couch, the bed, and at times in his office, Chesky specced out how he would redesign the Airbnb app. Its users—now at 2 billion—would open up the app not only at vacation time but whenever they needed to find a portrait photographer, a personal trainer, or someone to cook their meals. Chesky reasoned that Airbnb would need to significantly strengthen its identity verification. He even thought he could get people to use the app as a credential, something as respected as a government-issued ID. If he could transform Airbnb into a storefront for real-world services, Chesky thought, he’d catapult his company from a nearly $10-billion-a-year business into one that boasted membership in tech’s pantheon.</p><p>Over the next few days, Chesky spilled these thoughts into an Evernote document. “I was basically going from room to room just pouring out this stream-of-consciousness manifesto, like Jack Kerouac writing <em>On the Road</em>,” he says, referring to the frenetically produced single roll of teletype paper that catalyzed the beat movement. “I dusted off all my ideas from 2012 to 2016,” Chesky tells me. “I basically said, ‘We’re not just a vacation app—we’re going to be a platform, a community.’” By Friday he had around 10,000 words, “incomprehensible to anyone but me.” He began to refine it, and by the time the weekend was over, Chesky had distilled his document down to 1,500 words.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><figure><p><span>PHOTOGRAPH: GABRIELA HASBUN</span></p></figure><p>After the holiday, Chesky gathered his leadership team into a conference room. He handed the team copies of his memo à la Jeff Bezos and waited as his direct reports took it in. “Usually when I share ideas, people aren’t bought in,” he says. “But this time, there wasn’t a lot of feedback. People were really excited. And two years later, that document will now be executed with an exacting detail to what I wrote.”</p><p>This month, Airbnb will launch the first stage of its more than $200 million reinvention: a panoply of more than 10,000 vendors peddling a swath of services in 260 cities in 30 countries. It is also revitalizing an unsuccessful experiment the company began in 2016: offering bespoke local activities, or what it calls “experiences.” The next stage, launch date unspecified, involves making your profile on Airbnb so robust that it’s “almost like a passport,” as Chesky puts it. After that comes a deep immersion into AI: Inspired by his relationship with Altman, Chesky hopes to build the ultimate agent, a super-concierge who starts off handling customer service and eventually knows you well enough to plan your travel and maybe the rest of your life.</p><p>“Brian’s been badly underrated as a tech CEO,” Altman says of his friend. “He's not usually mentioned in the same breath as Larry Page or Bill Gates, but I think he is on a path to build as big of a company.”</p><p>That’s a stretch—Airbnb is nowhere near the size of those oligarchic powers. But Chesky was feeling the need for big changes; While impressive, Airbnb’s growth rate doesn’t suggest that the company will soon reach the trillion-dollar heights of Google and Microsoft. “I’m 43 and at a crossroads, where I can either be almost done or just getting started,” he tells me. “There's a scenario where I'm basically done. Airbnb is very profitable. We've kind of, mostly, nailed vacation rentals. But we can do more.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>In early April,</span> I visited Chesky at the company’s lavish San Francisco headquarters. The relaunch was five weeks away. The second floor—where signs warn employees not to bring visitors—had become a sprawling eyes-only command center. The walls were covered with dozens of large poster boards, each one featuring a city, that read as if a group of McKinsey consultants had tackled a fourth-grade geography assignment. Austin, Texas, was written up as “a funky come-as-you-are kind of place” with a handful of “first principles,” one of which was “Outlaw of Texas,” with pointers to food trucks and vintage markets. Another so-called principle was “Live and Alive,” referring to music venues and bat watching; a third was “Dam Lakes,” referring to various water sports. Other blindingly obvious notations included barbeque, tacos, and the two-step. The Paris poster painted a “revolutionary” city marked by slow living and enduring culture.</p><p>Chesky strode up and greeted me enthusiastically. Dressed in a slim T-shirt that exposed his swole physique, he bounced on his heels with a jittery energy that reminded me of the first time I met him, in January 2009. He had just joined Y Combinator’s famous program for startups, and he and his classmates were at a party at the home of YC cofounder Paul Graham. (Graham told me then that he thought Airbnb’s business plan was crazy but was impressed by their determination.) I mentioned to Chesky that I was headed to Washington, DC, for Barack Obama’s inauguration, and he and his cofounders immediately tried to convince me to use their service to sleep on someone’s couch. I declined, but somehow over the next 15 years they managed to sell the idea to 2 billion people, including me, and now the company has a market cap worth more than Marriott.</p><p>Chesky ushers me into a conference room to get a preview of the new Airbnb app. His engineers and designers have rebuilt the app from scratch, and he waves around a stick of lip balm as a talisman as he talks me through the redesign. Also in the room is his product marketing head, Jud Coplan, while his vice president of design, Teo Connor, Zooms in from London. While customers likely think of Airbnb as a travel company, its leaders view the operation as an achievement in design. Which makes sense; both Chesky and his cofounder Joe Gebbia were students at the Rhode Island School of Design.</p><figure><p><span>Airbnb's new user interface featuring experiences and services.</span><span>COURTESY OF AIRBNB</span></p></figure></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Chesky explains that historically, people used Airbnb only once or twice a year, so its design had to be exceptionally simple. Now the company is retooling for more frequent access. Open the app, and you see a trio of icons that act as gateways to the expanded functions. Within minutes Chesky and his lieutenants are applauding the cheery, retro style of the icons—a house for traditional rentals, a hotel bell for services, and a Jules Verne-ish hot-air balloon representing activities. “We really thought deeply about the metaphor—what was the right visual to express an experience?” says Connor. Once they decided on the balloon, they drilled into how much fire should belch from the basket. The icons were drawn by a former Apple designer whose name Chesky would not divulge. “He’s a bit of a secret weapon,” he says.</p><p>A less-secret weapon is Chesky’s collaboration with the iconic, also ex-Apple, industrial designer Jony Ive. Chesky’s north star, it should be said, is Apple. “Steve Jobs, to me, is like Michelangelo or da Vinci,” he says. Despite never meeting Jobs, “I feel like I know him deeply, professionally, in a way that few people ever did, in a way that you only possibly could by starting a tech company as a creative person and going on a rocket ship,” Chesky says. By hiring Ive’s LoveFrom company and working with Jobs’ key collaborator, Chesky gets a taste of the famous Jobs/Ive dynamic. Ive himself doesn’t make that comparison, but he does praise Chesky’s design chops. “There are certain tactical things where I hope that sometimes I'm of use to Brian, just as as a fellow designer,” Ive says. “But the majority of our work has been around ideas and the way we frame problems and understand opportunities.”</p><p>Another key part of the app is the profile page. “You need trust,” Chesky says—meaning a verifiable identity. Airbnb has been vetting the new vendors, which it calls “service hosts.” For months, Chesky says, an army of background researchers has been scrutinizing the résumés, licenses, and recommendations of chefs, photographers, manicurists, masseuses, hair stylists, makeup artists, personal trainers, and aestheticians who provide spa treatments such as facials and microdermabrasions. They’re all being professionally photographed.</p><div data-testid="GenericCallout"><figure><p><span>Airbnb's new guest profile interface.</span><span>COURTESY OF AIRBNB</span></p></figure></div><p>For the next phase—turning Airbnb’s user profiles into a primary internet ID—Connor and her team have engaged in some far-out experimentation. She rattles off a list of technologies they’ve been exploring, including biometrics, holograms, and the reactive inks used to deter counterfeiting on official ID cards. But it’s far from easy to become a private identity utility (hello, Facebook), and even Chesky notes that getting governments to accept an Airbnb credential to verify identity is “a stretch goal.”</p><p>Now that a whole slew of people will have new reasons to chat with each other and coordinate plans, Airbnb has also enhanced its messaging functions. Fellow travelers who share experiences can form communities, stay in touch, even share videos and photos. “I don’t know if I want to call it a social network, because of the stigma associated with it,” says Ari Balogh, Airbnb’s CTO. So they employ a fuzzier term. “We think of it as a connection platform,” he says. “You’re going to see us build a lot more stuff on top of it, although we’re not an advertising system, thank goodness.” (My own observation is that any for-profit company that can host advertising will, but whatever.)</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>This brings us to the services—the heart and soul of this reinvention. Those now on offer seem designed to augment an Airbnb stay with all the stuff that drives up your bill at a luxury resort, like a DIY White Lotus. It will be interesting to see how the company handles the inevitable cases of food poisoning or bad haircuts (and skeezy customers), but Airbnb can draw on its 17 years of experience with dirty sheets, all-night discos on the ground floor, or a host who is literally terrorizing you. Eventually, Chesky says, Airbnb will offer “hundreds” of services, perhaps as far-ranging as plumbing, cleaning, car repair, guitar lessons, and tutoring, and then take its 15 percent fee.</p></div><div data-testid="RowWrapper"><figure><p><span>Crafted cuts by Bryan, Chicago, Illinois</span><span>COURTESY OF AIRBNB; LYNDON FRENCH</span></p></figure><figure><p><span>Train with Steve Jordan, Trainer to the Stars, Los Angeles, California</span><span>COURTESY OF AIRBNB; JACKIE BEALE</span></p></figure></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>The other key feature of the company’s reinvention, of course, is Experiences. If the idea sounds familiar, that’s because Airbnb launched a service by that name almost a decade ago, with pretty much the same pitch: special activities for travelers, like architects leading tours of buildings or chefs showing people how to fold dumplings.</p><p>It flopped, although Airbnb never formally pulled the plug. Chesky’s excuses include tactical errors: After a big initial splash, the company didn’t follow up with more marketing, and it didn’t establish a strong flow of new experiences. But the big reason, he says, was that it was too early. Now the company has five times as many customers and an ecosystem to support the effort. “It was like our Newton,” says Chesky, referring to Apple’s handheld device that predated the iPhone. (Another Apple reference, for those keeping score.)</p><p>Chesky’s crew has arranged for more than 22,000 experiences in 650 cities, including a smattering of so-called “originals,” with people at the top of their field—star athletes, Michelin chefs, famous celebrities. In the pipeline is Conan O’Brien selling a perch behind a mic in his podcast studio. (Don’t expect it to air.) Taking a lesson from his earlier flop, Chesky has planned a steady cadence of these short-term promotional stunts, which, of course, is what the Conan experience ultimately is. “We’re going to have thousands of originals and maybe one day hundreds of thousands—a regular drumbeat of some of the biggest iconic celebrities,” Chesky says.</p><p>He shows me how someone could take a trip to, say, Mexico City and book experiences instantly. “Fun fact—I’ve always dreamed of being a professional wrestler in Mexico. I want to be a <em>luchador</em>!” he tells me, then immediately regrets it. Regardless: In an Airbnb experience, he says, you can meet a real luchador, get in the ring with him, and learn some moves. Can you keep the mask?</p><p>“Probably,” says Chesky. In any case, you’d share the photos with others in your group. (But don’t call it a social network.)</p></div><div data-testid="RowWrapper"><figure><p><span>Megan Thee Stallion in Los Angeles, California</span><span>COURTESY OF AIRBNB; ADRIENNE RAQUEL</span></p></figure><figure><p><span>Horseback riding through four hidden temples of the Inkas, Cusco, Peru</span><span>COURTESY OF AIRBNB; PAZ OLIVARES-DROGUETT</span></p></figure></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>Airbnb’s planned transformation</span> tracks with another reinvention: that of its leader.</p><p>Chesky had originally taken the title of CEO over his two pretty-much equal cofounders because his personality was more forward facing—it wasn’t even formalized until 2010. But then, in 2011, the company had its first real crisis when a host publicly shared a horror story about how an Airbnb guest from deep, deep hell pillaged and trashed her home. What wasn’t stolen—the customer broke into a locked closet to grab a passport, cash, and heirloom jewelry—was ravaged and burned in the fireplace. “The death-like smell from the bathroom was frightening,” wrote the host. The story threatened to destroy the cheerful person-to-person vibe the company had cultivated. It didn’t help that Airbnb’s initial response was clueless and weak.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Chesky stepped up to become the face of the company and instituted overdue safety protocols. Over the next few years, Chesky cemented his alpha status. In 2018 his cofounder Joe Gebbia stepped down from daily duties, though he still sits on the board. (Recently Gebbia has been in the news for his very public participation in DOGE’s remaking of the US government. When asked about it at a Q and A session with employees, Chesky said that Gebbia was free to have his own opinions, but they are not the company’s. Chesky did not attend Trump’s inauguration.) The third cofounder, Nathan Blecharczyk, is still with the company, though it’s notable that as I sat in meetings with over a dozen executives, the only time his name came up was when I mentioned it.</p><p>Chesky was totally in charge during the pandemic, when Airbnb lost 80 percent of its business in eight weeks. He laid off a quarter of the staff. Now that bookings surpass pre-2020 levels, he thinks the company is stronger. And he learned a big lesson: “The pandemic was the turning point of the company,” he says. “My first principle became ‘Don’t apologize for how you want to run your company.’ Most of all you should not apologize for being in the details. The number one thing people want to do is keep you out of the details.”</p><p>When Chesky shared some of these views at a Y Combinator event in 2024, Paul Graham was inspired to <a data-offer-url="https://www.paulgraham.com/foundermode.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.paulgraham.com/foundermode.html&quot;}" href="https://www.paulgraham.com/foundermode.html" rel="nofollow noopener" target="_blank">write an essay</a> called “<a href="https://www.wired.com/story/plaintext-want-to-get-into-founder-mode-you-should-be-so-lucky/">Founder Mode</a>.” Graham used Chesky’s story to argue that only the person who created a company knows what is best, and the worst mistake is to listen to management types who haven’t built their own. The essay struck a nerve; people were stopping Chesky on the street and yelling “Founder mode!” Someone dropped off a baseball hat for him with those words; it now sits on a shelf in his conference room.</p><p>Chesky, meanwhile, has been deep in the details, especially on this reinvention, itself kind of a classic founder move. “I did review work before the pandemic, but people kind of hated it. There were negative associations to a CEO reviewing everything; it’s considered micromanaging.” Also, his idol Steve Jobs was famous—infamous?—for his unsparing criticism. Chesky contends that once he went all-in on dishing out criticism, with no sheepishness, people seemed happier. But even if they weren’t, he’d do it anyway. Curious to see how this worked, I arranged to attend a Chesky review.</p><p>Gathered in a conference room, the design and engineering teams presented near-final app tweaks affecting how hosts set up their services. Chesky seemed fairly pleased with what he was seeing—so much so that he apologized to me afterward that I didn’t get to see him go animal with his underlings. Nonetheless, even during this lovefest of a product review, Chesky babbled a constant stream of minor corrections. <em>The cursor is oddly centered … Those visual cues are a little confusing … We need a subtle drop shadow here … The next line doesn’t seem centered vertically … That address input is pretty awkward … That button looks oddly short, is it supposed to be that short? … That shimmer, do we think we need it? Get rid of it … That top module doesn’t make sense … We need to rewrite all the copy on this page … I think we need a better empty state … That title’s not clear …</em></p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>The group shuffles out satisfied and a bit stunned that they got away so easy. But when I meet Chesky a day later to sum things up, he tells me that I’d just missed a spicier product review. Then he gets serious, explaining what the reinvention means to him. “I felt a little bit like the vacation rental guy,” he says. “Like we as a company are a little underestimated.” He brings up Apple again, saying that both companies embody the idea that a business relationship can generate emotion. “My ambition is kind of like the ambition of an artist and designer,” he says.</p><p>At that point Chesky gets a little woo. “Magic, in hindsight, is not technology,” he says as he reflects on Apple’s wizardry. What he’s realized is that magic lies in forging connections with those who offer you a bed, a microdermabrasion, a sparring match in a lucha libre ring. “The magic that is timeless is, like, the stuff you remember at the end of your life.”</p><p>He lets that sit for a minute. Then he puts a cap on that insight, sounding less like a CEO than a life coach. “I’ve never had a dream with a device in it,” he says. Leave it to the subconscious to highlight what matters. That said, his <em>day</em> dreams certainly involve a new kind of device. In his off hours he’s helping with a secret project headed by his friends Altman and Ive to create a device that Altman says is the next step beyond computers. (“This is not theoretical memo-swapping,” Altman tells me. “We’re hard at work on it, prototyping.”)</p><p>But that’s somewhere off in the future. In the realm of products that actually exist in the world, Chesky will have to face competition from dozens of domain leaders including Yelp, Instacart, DoorDash, Ticketmaster, Hotels.com, Tinder, OpenTable, and Craigslist, to name but a few. You can probably add Apple, Meta, and Microsoft, since Chesky wants Airbnb to be a universal credential and what certainly looks like a social network. Even Steve Jobs might have blinked at taking on that crowd all at once.</p><hr><p><em>Images styled by Jillian Knox.</em><br><em>Featuring: Liv Skinner, Liv Well and Francesca Lopez, Zinnia Wildflower Bakehouse</em></p><p><em>Let us know what you think about this article. Submit a letter to the editor at</em> <em><a href="mailto:mail@wired.com">mail@wired.com</a> or comment below.</em></p></div></div></article><div><div data-testid="RowWrapper"><ul><li data-testid="LinkStackBullet"></li><li data-testid="LinkStackBullet"></li><li data-testid="LinkStackBullet"></li><li data-testid="LinkStackBullet"><p><a href="https://www.wired.com/story/questions-answered-by-yuval-noah-harari-for-wired-ai-artificial-intelligence-singularity/" target="_blank">Yuval Noah Harari</a>: “Prepare to share the planet with AI superintelligence”</p></li><li data-testid="LinkStackBullet"></li></ul></div><div data-testid="RowWrapper"><p><a href="https://www.wired.com/author/steven-levy/"><span><picture><source media="(max-width: 767px)" srcset="https://media.wired.com/photos/65e83761c9d1003a19b2989e/1:1/w_120,c_limit/undefined 120w, https://media.wired.com/photos/65e83761c9d1003a19b2989e/1:1/w_240,c_limit/undefined 240w" sizes="100vw"><source media="(min-width: 768px)" srcset="https://media.wired.com/photos/65e83761c9d1003a19b2989e/1:1/w_120,c_limit/undefined 120w, https://media.wired.com/photos/65e83761c9d1003a19b2989e/1:1/w_240,c_limit/undefined 240w" sizes="100vw"><img alt="" src="https://media.wired.com/photos/65e83761c9d1003a19b2989e/1:1/w_270%2Cc_limit/undefined"></picture></span></a></p><div><p><a href="https://www.wired.com/author/steven-levy/">Steven Levy</a> covers the gamut of tech subjects for WIRED, in print and online, and has been contributing to the magazine since its inception. His weekly column, <a href="https://www.wired.com/newsletter/plaintext?sourceCode=AuthorBio">Plaintext</a>, is exclusive to subscribers online but the newsletter version is open to all—<a href="https://www.wired.com/newsletter/plaintext?sourceCode=AuthorBio"><strong>sign up here</strong></a>. He has been writing about technology for ... <a href="https://www.wired.com/author/steven-levy">Read more</a></p></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why are banks still getting authentication so wrong? (268 pts)]]></title>
            <link>https://jamal.haba.sh/its-2025-why-are-banks-still-getting-authentication-so-wrong/</link>
            <guid>43976359</guid>
            <pubDate>Tue, 13 May 2025 18:56:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jamal.haba.sh/its-2025-why-are-banks-still-getting-authentication-so-wrong/">https://jamal.haba.sh/its-2025-why-are-banks-still-getting-authentication-so-wrong/</a>, See on <a href="https://news.ycombinator.com/item?id=43976359">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-05-13T16:00Z">
                    13 May, 2025
                </time>
            </i>
        </p>
    

    <p>While recently traveling to the U.S., I was completely locked out of my TD Personal Banking account.</p>
<p>TD relies heavily on SMS-based two-factor authentication (2FA) for customer logins. I had, quite reasonably, disabled my Canadian SIM to avoid the usual price gouging and roaming charges.</p>
<p>Luckily, I had their proprietary “TD Authenticate” app installed, thinking it would serve as a viable alternative. But when I opened TD Authenticate, I had been logged out, and logging back in required, you guessed it, an SMS message to my now-inaccessible Canadian number.</p>
<p>I had the authentication app. I had my credentials. But the system’s design created an inescapable catch-22.</p>
<p>This is a textbook case of security punishing the user instead of protecting them.</p>
<p>TD doesn’t offer TOTP support. No passkeys. No fallback email verification. Just a fragile, closed loop with a single point of failure, and one that failed entirely in a very foreseeable scenario.</p>
<p>Despite years of progress in digital identity and authentication standards, many Canadian financial institutions remain stuck with brittle, outdated authentication flows that fail from both a security and usability perspective.</p>
<p>In the case of SMS-based 2FA, it isn’t just inconvenient, it’s actively harmful.</p>
<h2 id="the-problem-with-sms-based-2fa">The Problem With SMS-Based 2FA</h2><p>SMS has long been discredited as a secure second factor.</p>
<p>As far back as 2017, <a href="https://www.nist.gov/">NIST</a> explicitly <a href="https://pages.nist.gov/800-63-3-Implementation-Resources/63B/Authenticators/">discouraged the use of SMS for delivering one-time codes</a>. With <a href="https://www.cisa.gov/">CISA</a> describing it as a <a href="https://www.cisa.gov/sites/default/files/2023-01/fact-sheet-implementing-phishing-resistant-mfa-508c.pdf">“last resort MFA option”</a> and <a href="https://www.cisa.gov/sites/default/files/2023-01/fact-sheet-implementing-phishing-resistant-mfa-508c.pdf">“temporary solution while organizations transition to a stronger MFA implementation.”</a></p>
<p>The problem is that SMS-Based 2FA leaves users vulnerable to cyber-attacks, as threat actors can exploit protocol vulnerabilities and use social-engineering to:</p>
<ul>
<li>Intercept 2FA codes sent via text messages</li>
<li>Take control of a user’s phone number with a SIM Swap.</li>
<li>Trick the user into revealing their 2FA code with phishing.</li>
</ul>
<p>In 2023, the <a href="https://www.cyber.gc.ca/en">Canadian Centre for Cyber Security</a> reiterated exactly this message, saying:</p>
<blockquote>
<p>“Only consider short message service (SMS) codes as an authentication factor for low-risk logins. SMS is insecure as codes are sent in unencrypted form. An increasing number of cyber attacks involves threat actors intercepting SMS codes through SIM swapping, phishing or other social engineering attacks.”</p>
<p><a href="https://www.cyber.gc.ca/en/guidance/steps-effectively-deploying-multi-factor-authentication-mfa-itsap00105">https://www.cyber.gc.ca/en/guidance/steps-effectively-deploying-multi-factor-authentication-mfa-itsap00105</a></p>
</blockquote>
<p>I don’t think anyone considers a bank account “low-risk.” Yet here we are, still relying on SMS as the default, and sometimes only, 2FA option</p>
<h2 id="proprietary-otp-apps-a-marginal-improvement-at-best">Proprietary OTP Apps: A Marginal Improvement at Best</h2><p>In an effort to move beyond SMS, some banks, TD included, have rolled out their own proprietary OTP apps rather than adopting the open <a href="https://datatracker.ietf.org/doc/html/rfc6238">TOTP standard (RFC 6238)</a>.</p>
<p>The result? Slightly better security. Significantly worse usability.</p>
<p>These apps often:</p>
<ul>
<li>Don’t integrate with password managers or platform authenticators.</li>
<li>Require a login before you can generate a code, which defeats the purpose of an authenticator app.</li>
<li>Offer no support for hardware tokens or modern passkeys.</li>
</ul>
<p>Even worse, these apps often become excuses, a reason to avoid implementing the open, interoperable standards that actually make a difference.</p>
<h2 id="what-good-authentication-looks-like-in-2025">What Good Authentication Looks Like In 2025</h2><p>A modern authentication flow in 2025 should be built around strong, user-friendly, standards-based mechanisms:</p>
<ul>
<li>Passkeys (FIDO2/WebAuthn): Phishing-resistant, device-based login using biometrics. Excellent UX and security.</li>
<li>TOTP Support: Let users use <em>any</em> standard authenticator (Authy, Google Authenticator, Microsoft Authenticator, 1Password, etc.).</li>
<li>Hardware Security Keys: FIDO2 keys like YubiKey for users who want maximum assurance.</li>
<li>Secure Recovery Paths: Trusted devices, or recovery codes, not SMS.</li>
<li>Password Manager Compatibility: Seamless autofill and passkey support across trusted password managers and OS keychains.</li>
</ul>
<h2 id="security-shouldnt-punish-the-user">Security Shouldn't Punish The User</h2><p>Authentication flows too often feel like they were designed in a vacuum, engineered by siloed security teams and product managers with no regard users.
If a system breaks in common scenarios, like international travel, it’s not a secure system. It’s a hostile one.</p>
<h2 id="banks-must-do-better">Banks Must Do Better</h2><p>TD isn’t the only offender, but it’s a glaring example of how not to do authentication in 2025.
The refusal to support basic standards like passkeys or TOTP isn’t just an inconvenience, it’s a security liability that actively harms users and undermines trust.
There’s no excuse anymore. The standards exist. The risks are well-documented.
If your authentication flow still relies on SMS and a brittle proprietary app, it’s long past time for a serious overhaul.
Security and usability are not mutually exclusive. Achieving both requires systems designed with competence, foresight, and actual consideration for the user experience.</p>
<h3 id="postscript-three-years-later-still-broken">Postscript: Three Years Later, Still Broken</h3><p>By the way, that trip to the U.S. happened three years ago. Nothing has changed since.</p>
<p>I’ve emailed TD about this issue (basically saying what I said in this blog post), we’ll see what they say.</p>
<p>And let’s be honest, this whole conversation is just touching the surface. The real progress starts when we talk about <a href="https://en.wikipedia.org/wiki/Self-sovereign_identity">Self-Sovereign Identity</a>, <a href="https://www.w3.org/TR/did-1.0/">DIDs</a>, and decentralized auth infrastructure.</p>
<p>But that’s a blog post for another day.</p>
<p>And don’t even get me started on logging into accounts at the Canada Revenue Agency.</p>


    

    
        
            <p>
                
                    <a href="https://jamal.haba.sh/posts/?q=writing">#writing</a>
                
            </p>
        

        
            


        
    


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: HelixDB – Open-source vector-graph database for AI applications (Rust) (203 pts)]]></title>
            <link>https://github.com/HelixDB/helix-db/</link>
            <guid>43975423</guid>
            <pubDate>Tue, 13 May 2025 17:26:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/HelixDB/helix-db/">https://github.com/HelixDB/helix-db/</a>, See on <a href="https://news.ycombinator.com/item?id=43975423">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<themed-picture data-catalyst-inline="true"><picture>
  <img src="https://github.com/HelixDB/helix-db/raw/main/docs/icon-1.png" alt="HelixDB Logo" width="200" height="200">
</picture></themed-picture>
<p dir="auto"><b>HelixDB</b>: an open-source graph-vector database written in Rust built for RAG and AI applications.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">
  <a href="https://helix-db.com/" rel="nofollow">Homepage</a> |
  <a href="https://docs.helix-db.com/" rel="nofollow">Docs</a> |
  <a href="https://discord.gg/2stgMPr5BD" rel="nofollow">Discord</a> |
  <a href="https://x.com/hlx_db" rel="nofollow">X</a>
</h3><a id="user-content---homepage---docs---discord---x" aria-label="Permalink: Homepage |
  Docs |
  Discord |
  X" href="#--homepage---docs---discord---x"></a></p>
<p dir="auto"><a href="https://github.com/HelixDB/helix-db/stargazers"><img src="https://camo.githubusercontent.com/cf19c2d23418c2bf2d2db81dac4a8aa274c24028206fba9baf9426df80b8482d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f48656c697844422f68656c69782d6462" alt="GitHub Repo stars" data-canonical-src="https://img.shields.io/github/stars/HelixDB/helix-db"></a></p>

</div>
<hr>
<p dir="auto">HelixDB is a high-performance graph-vector database  designed with a focus on developer experience and performance. Built in Rust and powered by LMDB as its storage engine, it combines the reliability of a proven storage layer with modern features tailored for AI and vector-based applications.</p>
<p dir="auto">We are currently using LMDB via Heed3, a rust wrapper built by the amazing team over at <a href="https://github.com/meilisearch/heed">Meilisearch</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key Features</h2><a id="user-content-key-features" aria-label="Permalink: Key Features" href="#key-features"></a></p>
<ul dir="auto">
<li><strong>Fast &amp; Efficient</strong>: Built for performance we're currently 1000x faster than Neo4j, 100x faster than TigerGraph and on par with Qdrant for vectors.</li>
<li><strong>RAG-First</strong>: Native support for graph and vector data types, making it ideal for RAG (Retrieval Augmented Generation) and AI applications</li>
<li><strong>Graph-Vector</strong>: Easiest database for storing relationships between nodes, vectors, or nodes AND vectors.</li>
<li><strong>Reliable Storage</strong>: Powered by LMDB (Lightning Memory-Mapped Database) for robust and efficient data persistence</li>
<li><strong>ACID Compliant</strong>: Ensures data integrity and consistency</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Helix CLI</h4><a id="user-content-helix-cli" aria-label="Permalink: Helix CLI" href="#helix-cli"></a></p>
<p dir="auto">The Helix CLI tool can be used to check, compile and deploy Helix locally.</p>
<ol dir="auto">
<li>
<p dir="auto">Install CLI</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -sSL &quot;https://install.helix-db.com&quot; | bash"><pre>curl -sSL <span><span>"</span>https://install.helix-db.com<span>"</span></span> <span>|</span> bash</pre></div>
</li>
<li>
<p dir="auto">Install Helix</p>

</li>
<li>
<p dir="auto">Setup</p>
<div dir="auto" data-snippet-clipboard-copy-content="helix init --path <path-to-project>"><pre>helix init --path <span>&lt;</span>path-to-project<span>&gt;</span></pre></div>
</li>
<li>
<p dir="auto">Write queries</p>
<p dir="auto">Open your newly created <code>.hx</code> files and start writing your schema and queries.
Head over to <a href="https://docs.helix-db.com/introduction/cookbook/basic" rel="nofollow">our docs</a> for more information about writing queries</p>
<div dir="auto" data-snippet-clipboard-copy-content="QUERY addUser(name: String, age: Integer) =>
   user <- AddN<User({name: name, age: age})
   RETURN user

QUERY getUser(user_name: String) =>
   user <- N<User::WHERE(_::{name}::EQ(user_name))
   RETURN user"><pre><span>QUERY</span> <span>addUser</span><span>(</span><span>name</span>: <span>String</span><span>,</span> <span>age</span>: <span>Integer</span><span>)</span> <span>=</span><span>&gt;</span>
   <span>user</span> <span>&lt;</span><span>-</span> <span>AddN</span><span>&lt;</span><span>User</span><span>(</span><span>{</span><span>name</span>: <span>name</span><span>,</span> <span>age</span>: <span>age</span><span>}</span><span>)</span>
   <span>RETURN</span> <span>user</span>

<span>QUERY</span> <span>getUser</span><span>(</span><span>user_name</span>: <span>String</span><span>)</span> <span>=</span><span>&gt;</span>
   <span>user</span> <span>&lt;</span><span>-</span> <span>N</span><span>&lt;</span><span>User</span>::<span>WHERE</span><span>(</span><span>_</span>::<span>{</span>name<span>}</span>::<span>EQ</span><span>(</span><span>user_name</span><span>)</span><span>)</span>
   <span>RETURN</span> <span>user</span></pre></div>
</li>
<li>
<p dir="auto">Check your queries compile before building them into API endpoints (optional)</p>
<div dir="auto" data-snippet-clipboard-copy-content="# in ./<path-to-project>
helix check"><pre><span><span>#</span> in ./&lt;path-to-project&gt;</span>
helix check</pre></div>
</li>
<li>
<p dir="auto">Deploy your queries</p>
<div dir="auto" data-snippet-clipboard-copy-content="# in ./<path-to-project>
helix deploy --local"><pre><span><span>#</span> in ./&lt;path-to-project&gt;</span>
helix deploy --local</pre></div>
</li>
<li>
<p dir="auto">Start calling them using our <a href="https://github.com/HelixDB/helix-ts">TypeScript SDK</a> or <a href="https://github.com/HelixDB/helix-py">Python SDK</a>. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import HelixDB from &quot;helix-ts&quot;;

// Create a new HelixDB client
// The default port is 6969
const client = new HelixDB();

// Query the database
await client.query(&quot;addUser&quot;, {
   name: &quot;John&quot;,
   age: 20
});

// Get the created user
const user = await client.query(&quot;getUser&quot;, {
   user_name: &quot;John&quot;
});

console.log(user);"><pre><span>import</span> <span>HelixDB</span> <span>from</span> <span>"helix-ts"</span><span>;</span>

<span>// Create a new HelixDB client</span>
<span>// The default port is 6969</span>
<span>const</span> <span>client</span> <span>=</span> <span>new</span> <span>HelixDB</span><span>(</span><span>)</span><span>;</span>

<span>// Query the database</span>
<span>await</span> <span>client</span><span>.</span><span>query</span><span>(</span><span>"addUser"</span><span>,</span> <span>{</span>
   <span>name</span>: <span>"John"</span><span>,</span>
   <span>age</span>: <span>20</span>
<span>}</span><span>)</span><span>;</span>

<span>// Get the created user</span>
<span>const</span> <span>user</span> <span>=</span> <span>await</span> <span>client</span><span>.</span><span>query</span><span>(</span><span>"getUser"</span><span>,</span> <span>{</span>
   <span>user_name</span>: <span>"John"</span>
<span>}</span><span>)</span><span>;</span>

<span>console</span><span>.</span><span>log</span><span>(</span><span>user</span><span>)</span><span>;</span></pre></div>
</li>
</ol>
<p dir="auto">Other commands:</p>
<ul dir="auto">
<li><code>helix instances</code> to see all your local instances.</li>
<li><code>helix stop &lt;instance-id&gt;</code> to stop your local instance with specified id.</li>
<li><code>helix stop --all</code> to stop all your local instances.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<p dir="auto">Our current focus areas include:</p>
<ul dir="auto">
<li>Expanding vector data type capabilities for RAG applications</li>
<li>Enhancing the query language with more robust type checking</li>
<li>Implementing a test suite to enable end-to-end testing of queries before deployment</li>
<li>Building a Deterministic Simulation Testing engine enabling us to robustly iterate faster</li>
<li>Binary quantisation for even better performance</li>
</ul>
<p dir="auto">Long term projects:</p>
<ul dir="auto">
<li>In-house graph-vector storage engine (to replace LMDB)</li>
<li>In-house network protocol &amp; serdes libraries (similar to protobufs/gRPC)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">HelixDB is licensed under the The AGPL (Affero General Public License).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Commercial Support</h2><a id="user-content-commercial-support" aria-label="Permalink: Commercial Support" href="#commercial-support"></a></p>
<p dir="auto">HelixDB is available as a managed service for selected users, if you're interested in using Helix's managed service or want enterprise support, <a href="mailto:founders@helix-db.com">contact</a> us for more information and deployment options.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GOP sneaks decade-long AI regulation ban into spending bill (110 pts)]]></title>
            <link>https://arstechnica.com/ai/2025/05/gop-sneaks-decade-long-ai-regulation-ban-into-spending-bill/</link>
            <guid>43975254</guid>
            <pubDate>Tue, 13 May 2025 17:12:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/ai/2025/05/gop-sneaks-decade-long-ai-regulation-ban-into-spending-bill/">https://arstechnica.com/ai/2025/05/gop-sneaks-decade-long-ai-regulation-ban-into-spending-bill/</a>, See on <a href="https://news.ycombinator.com/item?id=43975254">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>The reconciliation bill primarily focuses on cuts to Medicaid access and increased health care fees for millions of Americans. The AI provision appears as an addition to these broader health care changes, potentially limiting debate on the technology's policy implications.</p>
<p>The move is already inspiring backlash. On Monday, tech safety groups and at least one Democrat criticized the proposal, reports <a href="https://thehill.com/policy/technology/5296022-schakowsky-criticizes-republicans-ai-ban/">The Hill</a>. Rep. Jan Schakowsky (D-Ill.), the ranking member on the Commerce, Manufacturing and Trade Subcommittee, called the proposal a "giant gift to Big Tech," while nonprofit groups like the Tech Oversight Project and Consumer Reports warned it would leave consumers unprotected from AI harms like deepfakes and bias.</p>
<h2>Big Tech’s White House connections</h2>
<p>President Trump has already <a href="https://arstechnica.com/tech-policy/2025/01/trumps-day-one-orders-saving-tiktok-creating-doge-and-ending-censorship/">reversed</a> several Biden-era executive orders on AI safety and risk mitigation. The push to prevent state-level AI regulation represents an escalation in the administration's industry-friendly approach to AI policy.</p>
<p>Perhaps it's no surprise, as the AI industry has cultivated close ties with the Trump administration since before the president took office. For example, Tesla CEO Elon Musk <a href="https://arstechnica.com/tech-policy/2025/05/bill-gates-accuses-elon-musk-of-killing-children-with-doge-led-usaid-cuts/">serves</a> in the Department of Government Efficiency (DOGE), while entrepreneur David Sacks acts as <a href="https://www.reuters.com/world/us/trump-appoints-former-paypal-coo-david-sacks-ai-crypto-czar-2024-12-06/">"AI czar,"</a> and venture capitalist Marc Andreessen reportedly <a href="https://www.thefp.com/p/marc-andreessen-on-ai-tech-censorship-trump-democrats">advises</a> the administration. OpenAI CEO Sam Altman <a href="https://arstechnica.com/ai/2025/01/trump-announces-500b-stargate-ai-infrastructure-project-with-agi-aims/">appeared with Trump</a> in an AI datacenter development plan announcement in January.</p>
<p>By limiting states' authority over AI regulation, the provision could prevent state governments from using federal funds to develop AI oversight programs or support initiatives that diverge from the administration's <a href="https://arstechnica.com/ai/2024/11/trump-victory-signals-major-shakeup-for-us-ai-regulations/">deregulatory stance</a>. This restriction would extend beyond enforcement to potentially affect how states design and fund their own AI governance frameworks.</p>


          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ETH Zurich researchers discover new security vulnerability in Intel processors (394 pts)]]></title>
            <link>https://ethz.ch/en/news-and-events/eth-news/news/2025/05/eth-zurich-researchers-discover-new-security-vulnerability-in-intel-processors.html</link>
            <guid>43974891</guid>
            <pubDate>Tue, 13 May 2025 16:44:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ethz.ch/en/news-and-events/eth-news/news/2025/05/eth-zurich-researchers-discover-new-security-vulnerability-in-intel-processors.html">https://ethz.ch/en/news-and-events/eth-news/news/2025/05/eth-zurich-researchers-discover-new-security-vulnerability-in-intel-processors.html</a>, See on <a href="https://news.ycombinator.com/item?id=43974891">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content">
                
                    
    
    <!-- Panorama header -->
    

    <!-- Blog header -->
    
    <!-- Tags when not blog -->
    
        
    

    <!-- Articleheader -->
    <div>
    <p>
            Computer scientists at ETH Zurich discover new class of vulnerabilities in Intel processors, allowing them to break down barriers between different users of a processor using carefully crafted instruction sequences. Entire processor memory can be read by employing quick, repeated attacks.
        </p>
    

</div>

    <!-- Nav & News images -->
    

    <!-- Details -->
    

    <!-- ArticleLeadImage -->
    
        <div>
            <figure>
            <img alt="  The image shows an example Intel server system." src="https://ethz.ch/en/news-and-events/eth-news/news/2025/05/eth-zurich-researchers-discover-new-security-vulnerability-in-intel-processors/_jcr_content/articleLeadImage/image.imageformat.carousel.926172841.jpg">
			<figcaption>
			    <p>
			      All Intel processors since 2018 are affected by Branch Privilege Injection. The image shows an example of an Intel server system.&nbsp;(Image: ETH Zurich / Computer Security Group, Corporate Communications)</p>
			  </figcaption>
			</figure>
    
          </div>
    

    <!-- Parsys 1 -->
    <div>

<div>
                
                	<h2><b>In brief</b></h2>
                
                
                <ul> 
 <li>The new class of vulnerabilities in Intel processors arises from speculative technologies that anticipate individual computing steps.</li> 
 <li>Openings enable gradual reading of entire privilege memory contents of shared processor (CPU).</li> 
 <li>All Intel processors from the last 6 years are affected, from PCs to servers in data centres.</li> 
</ul>
            </div>
<div>
                
                
                <p>Anyone who speculates on likely events ahead of time and prepares accordingly can react quicker to new developments. What practically every person does every day, consciously or unconsciously, is also used by modern computer processors to speed up the execution of programs. They have so-called speculative technologies which allow them to execute instructions on reserve that experience suggests are likely to come next. Anticipating individual computing steps accelerates the overall processing of information.</p> 
<p>However, what boosts computer performance in normal operation can also open up a backdoor for hackers, as recent research by computer scientists from the Computer Security Group (COMSEC) at the Department of Information Technology and Electrical Engineering at ETH Zurich shows. The computer scientists have discovered a new class of vulnerabilities that can be exploited to misuse the prediction calculations of the CPU (central processing unit) in order to gain unauthorised access to information from other processor users.</p> 
<h2><b>PC, laptop and server processors all affected</b></h2> 
<p>“The security vulnerability affects all Intel processors,” emphasises Kaveh Razavi, head of COMSEC. “We can use the vulnerability to read the entire contents of the processor’s buffer memory (cache) and the working memory (RAM) of another user of the same CPU.” The CPU uses the RAM (random access memory) and cache to temporarily store calculation steps and information that is likely to be needed next.</p> 
<p>This vulnerability fundamentally undermines data security, particularly in the cloud environment where many users share the same hardware resources. It affects the processors of the world’s largest CPU manufacturer, which are used in PCs and laptops, as well as those used in data centre servers.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p> 
<h2><b>Nanosecond gap in authority check</b></h2> 
<p>The so-called BPRC (Branch Predictor Race Conditions) emerge during a brief period of a few nanoseconds when the processor switches between prediction calculations for two users with different permissions, explains Sandro Rüegge, who has been examining the vulnerability in detail over the past few months. </p>
            </div>
<div>
                        <figure>
            <img alt="  In the illustration, a hacker manages to overcome the protective measures (privileges) at step 3." src="https://ethz.ch/en/news-and-events/eth-news/news/2025/05/eth-zurich-researchers-discover-new-security-vulnerability-in-intel-processors/_jcr_content/wide_content/image/image.imageformat.1286.1020709684.png">
			<figcaption>
			    <p>
			      To compute faster, a predictor in the computer processor anticipates certain calculation steps. Hackers can exploit these anticipations to bypass security barriers and access confidential information. In the illustration, a hacker manages to overcome the protective measures (privileges) at step 3.&nbsp;(Illustration: ETH Zurich / COMSEC, HK)</p>
			  </figcaption>
			</figure>
    
                    </div>
<div>
                
                
                <p>Breaking through the built-in protective barriers between users, known as privileges, is possible because the permissions for individual activities are not stored at the same time as the calculations. With special inputs, it is now possible to cause ambiguity in the sequence of events when changing users, resulting in incorrect assignment of privileges. An attacker could exploit this in order to read an information byte (a unit consisting of eight binary 0/1 pieces of information).</p> 
<h2><b>Unlocking entire contents of memory byte by byte</b></h2> 
<p>The disclosure of a single byte would be negligible. However, the attack can be repeated in quick succession, allowing the contents of the entire memory to be read over time, explains Rüegge. “We can trigger the error repeatedly and achieve a readout speed of over 5000 bytes per second.” In the event of an attack, therefore, it is only a matter of time before the information in the entire CPU memory falls into the wrong hands. &nbsp;</p> 
<h2><b>Part of a series of security vulnerabilities</b></h2> 
<p>The vulnerability that the ETH Zurich researchers have now identified is not the first to be discovered in the speculative CPU technologies introduced in the mid-1990s. In 2017, Spectre and Meltdown were the first two vulnerabilities of this kind to hit the headlines, and new variants have been appearing regularly ever since. Johannes Wikner, a former PhD student in Razavi's group, already identified a vulnerability known as Retbleed back in 2022. He exploited traces of speculatively executed instructions in the CPU’s cache to access information from other users.</p> 
<h2><b>Suspicious signal reveals vulnerability</b></h2> 
<p>The starting point for the discovery of the new vulnerability class was work that followed on from the Retbleed investigations. “I examined the functions of the protective measures that Intel had introduced to patch up the Retbleed vulnerability,” says Johannes Wikner.</p> 
<p>In doing so, he discovered an unusual signal from the cache memory that appeared regardless of whether the protective measures were enabled or disabled. Rüegge then took over detailed analysis of the cause of the signal and, based on this work, was able to uncover the new attack vector.</p> 
<h2><b>Fundamental architectural problem</b></h2> 
<p>The vulnerability was discovered back in September 2024. Since then, Intel has implemented protective measures to secure its processors. Nevertheless, there are many indications that the problem is more serious. “The series of newly discovered vulnerabilities in speculative technologies is an indication of fundamental flaws in the architecture,” Razavi points out. “The gaps have to be found one by one and then closed.”</p> 
<p>Closing these sorts of gaps requires a special update to the processor’s microcode. This can be done via a BIOS or operating system update and should therefore be installed on our PCs in one of the latest cumulative updates from Windows.</p>
            </div>
<div>
                
                	<h2><b>Reference</b></h2>
                
                
                <p>Rüegge S, Wikner, J, Razavi, K. Branch Privilege Injection: Compromising Spectre v2 Hardware Mitigations by Exploiting Branch Predictor Race Conditions. In: 34<sup>th</sup> USENIX Security Symposium, 2025.</p>
<p>CVE-Number: CVE-2024-45332</p>
            </div>

</div>
    <!-- Socialsharing (display only) -->
    
    
        
            
            
            
        
        
    


    <!-- Parsys 2 -->
    

    <!-- Rightside Parsys -->
    

    <!-- Taglist -->
    
        
            
        
    

    <!-- Comments -->
    
    	
	    
    

                
                
            </section></div>]]></description>
        </item>
    </channel>
</rss>