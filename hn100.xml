<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 14 Mar 2025 15:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Migrating from AWS to a European Cloud – How We Cut Costs by 62% (118 pts)]]></title>
            <link>https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62</link>
            <guid>43361366</guid>
            <pubDate>Fri, 14 Mar 2025 10:56:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62">https://www.hopsworks.ai/post/migrating-from-aws-to-a-european-cloud-how-we-cut-costs-by-62</a>, See on <a href="https://news.ycombinator.com/item?id=43361366">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="editorInbox" fs-codehighlight-element="code" fs-codehighlight-theme="base16/gruvbox-light-hard"><p><h2 id="hops">Hopsworks</h2></p><p>Hopsworks is an open platform for developing and operating AI systems at scale that can be deployed on any Kubernetes clusters from public clouds to sovereign air-gapped data centers. Hopsworks can be considered an alternative to <a target="_blank" href="https://www.hopsworks.ai/dictionary/mlops-platform">MLOps platforms</a> such as AWS Sagemaker, GCP Vertex, and Databricks (for AI), but it has higher performance real-time AI, better Python integration to the Lakehouse, as shown in our <a target="_blank" href="https://dl.acm.org/doi/10.1145/3626246.3653389">peer-reviewed SIGMOD’24 research paper</a>, and is the <a target="_blank" href="https://www.hopsworks.ai/post/feature-store-the-missing-data-layer-in-ml-pipelines">OG feature store for ML</a>.</p><p>Hopsworks provides both data and compute support, see Figure 1. There is a Lakehouse layer (Delta Lake, Apache Hudi, Iceberg coming soon) to store large amounts of historical <a target="_blank" href="https://www.hopsworks.ai/dictionary/feature-data">feature data</a> for training AI models and for batch inference. There is also a low latency database, <a target="_blank" href="https://www.rondb.com/">RonDB</a>, that we develop for real-time AI workloads (including specialized query support for <a target="_blank" href="https://www.hopsworks.ai/post/rondb-a-real-time-database-for-real-time-ai-systems">low-latency query support</a> and <a target="_blank" href="https://www.hopsworks.ai/post/the-journey-from-star-schema-to-snowflake-schema-in-the-feature-store">snowflake schema data models</a>). Hopsworks also supports compute (Jobs, Notebooks) on Kubernetes, supporting Python, Spark, and Ray and GPU sharing/optimization at scale. However, you can also provide your own compute, in which case you use Hopsworks primarily as a data layer to integrate your <a target="_blank" href="https://www.hopsworks.ai/dictionary/ai-pipelines">AI pipelines</a>. For this, Hopsworks also provides a <a target="_blank" href="https://www.hopsworks.ai/dictionary/model-registry">model registry</a> and supports deploying models on <a target="_blank" href="https://www.hopsworks.ai/dictionary/kserve">KServe</a>/<a target="_blank" href="https://www.hopsworks.ai/dictionary/vllm">vLLM</a>.</p><figure><p><img src="https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67d00f6dd7b4afb64bb4b2d3_Figure%201%20AI%20Lakehouse_lightbox.png" loading="lazy" alt=""></p><figcaption><strong><em>Figure 1. </em></strong><em>The Hopsworks stack deploys on a Kubernetes Cluster and requires a S3 storage bucket. Hopsworks also has several internal services that can be replaced by external services, including a Container Registry.</em></figcaption></figure><p>Hopsworks serverless is a freemium version of Hopsworks where you can store up to 50GB of Lakehouse data, 100MB of low-latency feature data in RonDB, up to 100 models in the model registry, and serve 2 <a target="_blank" href="https://www.hopsworks.ai/dictionary/model-deployment">model deployments</a>. Hopsworks freemium is widely used to run <a target="_blank" href="https://serverless-ml.carrd.co/">serverless AI applications</a>.&nbsp;</p><p>Hopsworks serverless, in effect, provides mostly free storage to users, but no free compute, which kept our hosting costs down to $8k/month on AWS. We could scale from 8k to 20k users without much additional cost (an extra TB of s3 storage only costs around $25/month). However, egress costs on AWS were a significant risk. In 2024, we released the Hopsworks Query Service, which provides high throughput read access to feature data from Python clients (<a target="_blank" href="https://www.hopsworks.ai/post/python-centric-feature-service-with-arrowflight-and-duckdb">using Arrow and DuckDB</a>). Suddenly, clients could read 100s of MB or even GBs easily into Pandas DataFrames. We looked nervously at the growth in data egress and projected cost increases and decided to act. We knew OVH, given our shared European roots, and found out they provided as managed services all we needed to run Hopsworks - managed Kubernetes, a managed container registry, and S3 compatible object storage. Installing Hopsworks on OVH with Helm charts worked great, so we decided to migrate Hopsworks Serverless to OVHCloud - but we kept it in North America where most of our users are to maintain existing latency to those users.&nbsp;</p><p><h2 id="aws-ovh">Moving from AWS Services and to OVHCloud Services</h2></p><p>We really only had Kubernetes and S3 as dependencies. Hopsworks has its own observability stack, based on OpenSearch and OpenSearch Dashboards, as well as its own metrics stack, based on Prometheus/Graphana. We have always been careful to not use cloud-specific services in Hopsworks.&nbsp; Here is a comparison of the main services we considered.</p><p><h3 id="kubernetes">Managed Kubernetes</h3></p><p>Both AWS and OVH offer managed K8s services. AWS wins on maturity, but OVH wins on pricing (free!).</p><figure><p><img src="https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67d0101c85f05749d547f403_Table%201%20Managed%20Kubernetes_lightbox.png" loading="lazy" alt=""></p></figure><p><h3 id="network">Network Egress</h3></p><p>OVH has long been known for not charging for public egress, while AWS is infamous for its predatory public egress costs. <a target="_blank" href="https://getdeploying.com/reference/data-egress">Cloud egress costs are documented on this site</a>. OVH charges for egress at some (newer?) regions, albeit at 1/8th the cost of AWS. Many cloud regions do not charge for egress today (March 2025).</p><figure><p><img src="https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67d010447e0f85d807c7f367_Table%202%20Network%20Egress_lightbox.png" loading="lazy" alt=""></p></figure><p><h3 id="s3-storage">S3 Storage</h3></p><p>AWS S3 is the premier cloud-based object storage service. It may have higher availability, but it’s three times more expensive than OVH’s S3 storage.</p><figure><p><img src="https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67d0105ac437a11bad00dbe3_868b577a-2328-4bab-94c5-68953437623f.png" loading="lazy" alt=""></p></figure><p><h3 id="container">Container Registry</h3></p><p>AWS’ Elastic Container Registry (ECR) is a mature, configurable, scalable managed service. OVH’s Harbor is available at only 3 different sizes for a fixed monthly cost, providing potential scalability concerns for those with &gt;5 TB storage requirements or high concurrency requirements.</p><figure><p><img src="https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67d010c958c9e01b22b9af00_Table%204%20%20Container%20Registry_lightbox.png" loading="lazy" alt=""></p></figure><p><h3 id="inter">Inter Availability Zone Data Transfers</h3></p><p>Hopsworks is tolerant to Availability Zone failures by replicated services over instances in different availability zones. This results in network traffic between instances on different availability zones.</p><figure><p><img src="https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67d010f1c7d76a0907889af4_Table%205%20Inter%20Availability%20Zone%20data%20transfer_lightbox.png" loading="lazy" alt=""></p></figure><p><h3 id="ebs">EBS Instances</h3></p><p>Some Hopsworks services require persistent volumes that are provided by AWS Elastic Block Storage and OVH Block Storage (implemented using Ceph). Hopsworks also has instances that use local (NVMe) disks. OVH has higher throughput NVMe disks available for lower storage capacities (1-4 TBs) compared to AWS. We use these instances for our database, RonDB, but they are not included in the table below.</p><figure><p><img src="https://cdn.prod.website-files.com/618399cd49d125734c8dec95/67d011197e553ea6af33a7f4_Table%206%20EBS%20Instances_lightbox.png" loading="lazy" alt=""></p></figure><p><h2 id="migration">The Actual Migration</h2></p><p>We notified our users of a maintenance window of 24 hours, on November 26th 2024. We backed up the Hopsworks cluster to a AWS S3 bucket on that day and then migrated that bucket to a S3 bucket in OVHCloud. This made migration with some downtime relatively painless. The Hopsworks cluster on OVH was deployed with helm charts, and had a testing process that we followed before re-opening Hopsworks for logins. No users contacted us about anything untoward in their accounts as a result of the migration.</p><p><h2 id="summary">Summary</h2></p><p>In Q4 2024, we completed the migration from AWS, seamlessly transitioning thousands of users to a resilient Kubernetes-based infrastructure on OVHCloud. Although OVH and Hopsworks are technologies built in Europe, Hopsworks serverless service is located in North America, where most Hopsworks users are located and OVH also provides cloud capacity. In Europe, Hopsworks and OVH have since become partners to provide a sovereign AI platform for developing and operating AI systems at scale. We like OVH’s simple pricing and lower pricing - not just network egress but most services are lower cost and the quality is generally good.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla Cybertruck deliveries on hold as trims are flying off 'bulletproof' truck (110 pts)]]></title>
            <link>https://electrek.co/2025/03/13/tesla-cybertruck-deliveries-are-on-hold-as-trims-are-flying-off-the-bulletproof-truck/</link>
            <guid>43361038</guid>
            <pubDate>Fri, 14 Mar 2025 09:58:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2025/03/13/tesla-cybertruck-deliveries-are-on-hold-as-trims-are-flying-off-the-bulletproof-truck/">https://electrek.co/2025/03/13/tesla-cybertruck-deliveries-are-on-hold-as-trims-are-flying-off-the-bulletproof-truck/</a>, See on <a href="https://news.ycombinator.com/item?id=43361038">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1600" height="849" src="https://electrek.co/wp-content/uploads/sites/3/2025/03/Tesla-cantrail-trim.png?w=1600" alt="Tesla cantrail trim" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/03/Tesla-cantrail-trim.png?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/03/Tesla-cantrail-trim.png?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/03/Tesla-cantrail-trim.png?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/03/Tesla-cantrail-trim.png?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high"></figure>

<p>According to Tesla delivery agents, Cybertruck deliveries are on hold. There’s a containment hold as many owners are reporting trims flying off the supposedly ‘bulletproof’ electric truck.</p>



<p>Many Tesla Cybertruck buyers are taking to forums and social media to report that their deliveries are being pushed.</p>



<p>There are 8 Cybertrucks buyers waiting for deliveries who reported the delay on the <a href="https://www.cybertruckownersclub.com/forum/threads/delayed-delivery-due-to-inspection.37854/">Cybertruck Owners Club</a> forum and many more on X and Facebook.</p>



<p>Most are being told by Tesla delivery specialists that there’s a “containment hold” on all Cybertruc deliveries.</p>	
	



<p>A containment hold generally occurs when an automaker finds something wrong with newly produced vehicles and wants to hold deliveries to fix the issue so that it can avoid recalling vehicles in customers’ hands.</p>



<p>In this case, most Cybertruck buyers are unaware of the issue. However, one buyer shared an image from Tesla’s service app where a Tesla rep said that the issue was due to the Cybertruck’s “cantrail trim”:</p>



<figure><img decoding="async" width="1320" height="1364" src="https://electrek.co/wp-content/uploads/sites/3/2025/03/483955633_3913467258865093_6700888003189534068_n.jpg?quality=82&amp;strip=all&amp;w=991" alt="" srcset="https://electrek.co/wp-content/uploads/sites/3/2025/03/483955633_3913467258865093_6700888003189534068_n.jpg 1320w, https://electrek.co/wp-content/uploads/sites/3/2025/03/483955633_3913467258865093_6700888003189534068_n.jpg?resize=145,150 145w, https://electrek.co/wp-content/uploads/sites/3/2025/03/483955633_3913467258865093_6700888003189534068_n.jpg?resize=290,300 290w, https://electrek.co/wp-content/uploads/sites/3/2025/03/483955633_3913467258865093_6700888003189534068_n.jpg?resize=768,794 768w, https://electrek.co/wp-content/uploads/sites/3/2025/03/483955633_3913467258865093_6700888003189534068_n.jpg?resize=991,1024 991w, https://electrek.co/wp-content/uploads/sites/3/2025/03/483955633_3913467258865093_6700888003189534068_n.jpg?resize=339,350 339w, https://electrek.co/wp-content/uploads/sites/3/2025/03/483955633_3913467258865093_6700888003189534068_n.jpg?resize=140,145 140w, https://electrek.co/wp-content/uploads/sites/3/2025/03/483955633_3913467258865093_6700888003189534068_n.jpg?resize=968,1000 968w, https://electrek.co/wp-content/uploads/sites/3/2025/03/483955633_3913467258865093_6700888003189534068_n.jpg?resize=150,155 150w" sizes="(max-width: 1320px) 100vw, 1320px"></figure>



<p>The cantrail trim is decorative trim that covers the roof ledge of a vehicle. For the Cybertruck, it consists of the highlighted section below:</p>



<figure><img decoding="async" src="https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-13-at-1.54.32%E2%80%AFPM.png?w=1024" alt=""></figure>



<p>Despite the fact that Tesla has claimed that the Cybertruck is “bulletproof” and made out of an “exoskeleton”, the electric vehicle’s build is actually much closer to a traditional unibody system rather than “exoskeleton.” Most of the visible body parts, which would be part of the chassis in a exoskeleton build, are actually trims attached to the body.</p>



<p>In some cases, they are extremely flimsy trims.</p>



<p>We previously reported on <a href="https://electrek.co/2024/06/25/tesla-recalls-over-11000-cybertrucks-over-trim-detaching-wiper-issue/" target="_blank" rel="noreferrer noopener">Tesla recalling 11,000 Cybertrucks due to some trims detaching while driving</a>. That recall was in June of last year, but is looks like the problem might be coming back.</p>



<p>Images of the Cybertruck’s front plate being held by glue has recently been circulating on social media:</p>



<figure><img loading="lazy" decoding="async" width="720" height="1253" src="https://electrek.co/wp-content/uploads/sites/3/2025/03/Gl4wwoCX0AEHgTt.jpeg?quality=82&amp;strip=all&amp;w=588" alt="" srcset="https://electrek.co/wp-content/uploads/sites/3/2025/03/Gl4wwoCX0AEHgTt.jpeg 720w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Gl4wwoCX0AEHgTt.jpeg?resize=86,150 86w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Gl4wwoCX0AEHgTt.jpeg?resize=172,300 172w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Gl4wwoCX0AEHgTt.jpeg?resize=588,1024 588w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Gl4wwoCX0AEHgTt.jpeg?resize=201,350 201w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Gl4wwoCX0AEHgTt.jpeg?resize=140,244 140w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Gl4wwoCX0AEHgTt.jpeg?resize=575,1000 575w, https://electrek.co/wp-content/uploads/sites/3/2025/03/Gl4wwoCX0AEHgTt.jpeg?resize=150,261 150w" sizes="auto, (max-width: 720px) 100vw, 720px"></figure>



<p>The previous recall also mentioned a problem with adhesive resulting in trims detaching.</p>



<p>In the case of the Cybertruck’s cantrail trim, Tesla’s service manual points to brackets holding the top part of the trims:</p>



<figure><img decoding="async" src="https://electrek.co/wp-content/uploads/sites/3/2025/03/Screenshot-2025-03-13-at-1.45.24%E2%80%AFPM.png?w=1024" alt=""></figure>



<p>The hold on Cybertruck deliveries appears to have started last weekend as some deliveries appoitments have been canceled and buyers have yet to be told when they will be able to pick up their vehicles.</p>




	<p>Here’s a good explanation of the issues with Tesla’s cantrail trim:</p>



<figure><p>
<iframe id="post-youtube-video-1" title="Cybertruck Stainless Panels are falling off in Freezing Temperatures- THIS needs to be fixed" width="500" height="281" data-src="https://www.youtube.com/embed/3WldSl3HGr8?start=2&amp;feature=oembed&amp;enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>



<p>Tesla has yet to issue a public service bulletin on the issue. The last service bulletin for the Cybertruck is dated February 26, and it is about replacing the left body controller’s PCB on certain vehicles:</p>



<blockquote>
<p>On certain Cybertruck vehicles, the left body controller’s printed circuit board was misaligned with its heatsink, which may cause the board to crack when the enclosure is closed. This may lead to a graceful power-off event. Replace the left body controller.</p>
</blockquote>



<p>We have previously reported on Tesla getting around service bulletin and potentially even recalls on the Cybertruck.</p>



<p>Based on internal information, we reported on <a href="https://electrek.co/2024/12/20/tesla-finds-cell-dent-issues-in-cybertrucks-starts-replacing-battery-packs/">Tesla having a fairly widespread “cell dent” issue in some Cybertruck battery packs in December</a>.</p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I-cant-believe-its-not-webusb: Hacking around lack of WebUSB support in Firefox (197 pts)]]></title>
            <link>https://github.com/ArcaneNibble/i-cant-believe-its-not-webusb</link>
            <guid>43360642</guid>
            <pubDate>Fri, 14 Mar 2025 08:36:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ArcaneNibble/i-cant-believe-its-not-webusb">https://github.com/ArcaneNibble/i-cant-believe-its-not-webusb</a>, See on <a href="https://news.ycombinator.com/item?id=43360642">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">We don't need no stinkin' WebUSB!</h2><a id="user-content-we-dont-need-no-stinkin-webusb" aria-label="Permalink: We don't need no stinkin' WebUSB!" href="#we-dont-need-no-stinkin-webusb"></a></p>
<p dir="auto">It turns out that there is a way for a web page to access USB devices <em>without</em> requiring WebUSB and its associated political disagreements! Not only that, a device can intentionally design itself to bypass all of the user consent requirements.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ArcaneNibble/i-cant-believe-its-not-webusb/blob/main/demo.gif"><img src="https://github.com/ArcaneNibble/i-cant-believe-its-not-webusb/raw/main/demo.gif" alt="demo video" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick demo</h2><a id="user-content-quick-demo" aria-label="Permalink: Quick demo" href="#quick-demo"></a></p>
<p dir="auto">Load <a href="https://github.com/ArcaneNibble/i-cant-believe-its-not-webusb/blob/main/u2f-hax.uf2">u2f-hax.uf2</a> onto a Raspberry Pi Pico (RP2040 version), and then load <a href="https://github.com/ArcaneNibble/i-cant-believe-its-not-webusb/blob/main/index.html">index.html</a> from either localhost or another secure context.</p>
<p dir="auto">The "On!" and "Off!" buttons will toggle the LED, and the state of pin <code>GP22</code> will be regularly updated on the page (you can conveniently short it to the adjacent GND pad with a piece of wire or metal).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How?!</h2><a id="user-content-how" aria-label="Permalink: How?!" href="#how"></a></p>
<p dir="auto">The Pico is programmed to emulate a <a href="https://en.wikipedia.org/wiki/Universal_2nd_Factor" rel="nofollow">U2F</a> dongle (i.e. a physical two-factor security key). However, instead of performing any security functions, arbitrary data is smuggled in the "key handle" and signature of <code>U2F_AUTHENTICATE</code> messages. As long as the key handle starts with 0xfeedface, the Pico instantly "confirms" user presence and returns data.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why is this possible?</h2><a id="user-content-why-is-this-possible" aria-label="Permalink: Why is this possible?" href="#why-is-this-possible"></a></p>
<p dir="auto">By design, the U2F key handle is an opaque blob of data which is conceptually "owned by" the security dongle. It is supposed to be returned by the dongle as a result of a registration, stored as-is by the relying party, and then given as-is back to the security dongle when authenticating.</p>
<p dir="auto">One reason this key handle functionality exists is to enable an unlimited number of websites to be associated with a particular  low-cost dongle with very limited memory. This hypothetical dongle stores a unique "master" encryption key internally. When a new registration is created, it creates a new public/private key pair, returns the public key, encrypts the private key with the "master" key, and <em>returns the encrypted private key as the key handle</em>. No matter how many registrations are created, the dongle does not have to be responsible for storing the keys associated with them. When the key handle is passed back to the dongle during an authentication, the dongle just unwraps the private key using its master key.</p>
<p dir="auto">In order to <em>allow for</em> all of these low-cost designs without <em>mandating</em> any particular internal algorithms, the key handle is treated as opaque, and so we can abuse it to smuggle arbitrary data.</p>
<p dir="auto">In order to <em>return</em> data, we need to somehow smuggle it as an ECDSA signature. An ECDSA signature is a tuple of two numbers <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="9d860057dfcc06c6c609f835f8fee0ec">$(r, s)$</math-renderer>, where each of the numbers is calculated <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="9d860057dfcc06c6c609f835f8fee0ec">$\mod n$</math-renderer>, where <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="9d860057dfcc06c6c609f835f8fee0ec">$n$</math-renderer> is the <em>order</em> of the elliptic curve base point. This basically means any value from 0 up to 0xffffffff00000000ffffffffffffffffbce6faada7179e84f3b9cac2fc632551 (the order of the secp256r1 base point). These numbers are then packed into some ASN.1.</p>
<p dir="auto">Although it is <em>sometimes</em> possible to tell whether an ECDSA signature was actually calculated "properly" rather than being some numbers we just made up (see Issue #1), there isn't a good reason for anybody other than the relying party to perform anything beyond basic validity checks. Chrome appears to check whether the numbers in the signature are actually in the range from 0 to <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="9d860057dfcc06c6c609f835f8fee0ec">$n$</math-renderer>, but Firefox doesn't check even that.</p>
<p dir="auto">As a result, we can just generate some dummy ASN.1 and then put the data we actually want to send inside of it. In order to reliably get around Chrome's basic validity checks, we just waste the first byte of each number with the value 0x7f. This will result in numbers which are always positive and less than <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="9d860057dfcc06c6c609f835f8fee0ec">$n$</math-renderer>. The entire software stack up to browser JavaScript will pass these "valid-enough" numbers straight through.</p>
<p dir="auto">Finally, because "access to USB devices" is politically contentious but "make users more secure" has very broad political support across the entire browser industry, this capability is widely supported without requiring extraneous setup, configuration, nor prompting.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Is this a security vulnerability?</h2><a id="user-content-is-this-a-security-vulnerability" aria-label="Permalink: Is this a security vulnerability?" href="#is-this-a-security-vulnerability"></a></p>
<p dir="auto">No.</p>
<p dir="auto">This <em>cannot</em> be used to access arbitrary USB devices. It only works with devices which are <em>intentionally</em> breaking the rules. In essence, this is an intentionally vulnerable device.</p>
<p dir="auto"><em>However</em>, it is known that the security model around USB devices is generally... questionable on most platforms. Plugging in a malicious USB device allows it to do anything that you yourself can do with devices such as a keyboard or a mouse.</p>
<p dir="auto">Do not plug arbitrary unknown devices into your computer (or your phone, etc.).</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ex-Facebook director's new book paints brutal image of Mark Zuckerberg (378 pts)]]></title>
            <link>https://www.sfgate.com/tech/article/ex-facebook-director-book-brutal-image-zuckerberg-20220239.php</link>
            <guid>43360024</guid>
            <pubDate>Fri, 14 Mar 2025 06:07:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sfgate.com/tech/article/ex-facebook-director-book-brutal-image-zuckerberg-20220239.php">https://www.sfgate.com/tech/article/ex-facebook-director-book-brutal-image-zuckerberg-20220239.php</a>, See on <a href="https://news.ycombinator.com/item?id=43360024">Hacker News</a></p>
Couldn't get https://www.sfgate.com/tech/article/ex-facebook-director-book-brutal-image-zuckerberg-20220239.php: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[TinyKVM: Fast sandbox that runs on top of Varnish (245 pts)]]></title>
            <link>https://info.varnish-software.com/blog/tinykvm-the-fastest-sandbox</link>
            <guid>43358980</guid>
            <pubDate>Fri, 14 Mar 2025 02:12:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://info.varnish-software.com/blog/tinykvm-the-fastest-sandbox">https://info.varnish-software.com/blog/tinykvm-the-fastest-sandbox</a>, See on <a href="https://news.ycombinator.com/item?id=43358980">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <article>
        <div>
          
          
          <p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><div><p>An introduction to a KVM-based single-process sandbox</p><p>Hey All. In between working on my PhD, <a href="https://github.com/libriscv/libriscv" rel="noopener">libriscv</a> and an untitled game (it’s too much I know), I also have been working on a <a href="https://github.com/varnish/tinykvm" rel="noopener">KVM sandbox</a> for single programs. A so-called userspace emulator. I wanted to make the worlds fastest sandbox using hardware virtualization, or at least I had an idea of what I wanted to do.</p><p>I wrote a blog post about sandboxing each and every request in Varnish back in 2021, titled <a href="https://medium.com/@fwsgonzo/virtual-machines-for-multi-tenancy-in-varnish-1c619ea3276" rel="noopener">Virtual Machines for Multi-tenancy in Varnish</a>. In it, I wrote that I would look into using KVM for sandboxing instead of using a RISC-V emulator. And so… I went ahead and wrote TinyKVM.</p><p>So, what is <a href="https://github.com/varnish/tinykvm" rel="noopener">TinyKVM</a> and what does it bring to the table?</p></div>
<!--more--><p><img src="https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%201.png?width=1093&amp;height=933&amp;name=tinykvm%20image%201.png" width="1093" height="933" loading="lazy" alt="tinykvm image 1" srcset="https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%201.png?width=547&amp;height=467&amp;name=tinykvm%20image%201.png 547w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%201.png?width=1093&amp;height=933&amp;name=tinykvm%20image%201.png 1093w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%201.png?width=1640&amp;height=1400&amp;name=tinykvm%20image%201.png 1640w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%201.png?width=2186&amp;height=1866&amp;name=tinykvm%20image%201.png 2186w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%201.png?width=2733&amp;height=2333&amp;name=tinykvm%20image%201.png 2733w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%201.png?width=3279&amp;height=2799&amp;name=tinykvm%20image%201.png 3279w" sizes="(max-width: 1093px) 100vw, 1093px"></p>
<p><span><em>TinyKVM executes regular Linux programs with the same results as native execution.</em></span></p>
<p>TinyKVM can be used to sandbox regular Linux programs or programs with specialized APIs embedded into your servers.</p>
<h2>TinyKVM’s design</h2>
<p>In order to explain just what TinyKVM is, I’m just going to list explicit features that are currently implemented and working as intended:</p>
<p>TinyKVM runs static <em>Linux ELF programs</em>. It can also be extended with an API made by you to give it access to eg. an outer HTTP server or cache. I’ll also be adding dynamic executable support eventually. ⏳ It currently runs on AMD64 (x86_64), and I will port it to AArch64 (64-bit ARM) at some later point in time.</p>
<p>TinyKVM creates hugepages where possible for guest pages. It can also use hugepages on the host in addition. The result is often (if not always) higher performance than a vanilla native program. Just to hammer this a bit in: <a href="https://easyperf.net/blog/2022/09/01/Utilizing-Huge-Pages-For-Code" rel="noopener">https://easyperf.net/blog/2022/09/01/Utilizing-Huge-Pages-For-Code</a> found that just allocating 2MB pages for the execute segment gave a 5% compilation boost for the LLVM codebase.</p>
<p><img src="https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%202.png?width=1093&amp;height=933&amp;name=tinykvm%20image%202.png" width="1093" height="933" loading="lazy" alt="tinykvm image 2" srcset="https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%202.png?width=547&amp;height=467&amp;name=tinykvm%20image%202.png 547w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%202.png?width=1093&amp;height=933&amp;name=tinykvm%20image%202.png 1093w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%202.png?width=1640&amp;height=1400&amp;name=tinykvm%20image%202.png 1640w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%202.png?width=2186&amp;height=1866&amp;name=tinykvm%20image%202.png 2186w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%202.png?width=2733&amp;height=2333&amp;name=tinykvm%20image%202.png 2733w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%202.png?width=3279&amp;height=2799&amp;name=tinykvm%20image%202.png 3279w" sizes="(max-width: 1093px) 100vw, 1093px"></p>
<p><em><span>I quickly allocated some hugepages and ran TinyKVM w/STREAM, and yes it’s quite a bit faster.</span></em></p>
<div><p>TinyKVM has only 2us overhead when calling a function in the guest. This may seem like much compared to my <a href="https://libriscv.no/docs/performance/latency" rel="noopener">RISC-V emulators 3ns</a>, however we are entering another process, and we get to use all of our CPU features.</p><p>TinyKVM can halt execution after a given time without any thread or signal setup during the call. This is unavailable to regular Linux programs. With no execution timeout the call overhead is 1.2us, as we don’t need a timer anymore.</p><p>TinyKVM can be remotely debugged with GDB. The program can be resumed after debugging, and I’ve actually used that to live-debug a request in Varnish and see it complete normally afterwards. Cool stuff, if I may say so.</p><p>TinyKVM can fork itself into copies that use <span>copy-on-write</span> to allow for huge workloads like LLMs to share most memory. As an example, 6GB weights required only 260MB working memory per instance, making it highly scalable.</p><p>TinyKVM forks can reset themselves in record time to a previous state using mechanisms unavailable to regular Linux programs. If security is important, VMs can be made ephemeral by resetting them after every request. Thus removing the possibility of seeing traces of previous requests and many classes of attacks are made impossible as any form of persistence is no longer possible. A TinyKVM instance can also be reset to another VM it was not forked from at a performance penalty, as the pagetables have to change.</p><p>TinyKVM uses only a fraction of the KVM API, which itself is around 42k LOC. The total lines of code covered by TinyKVM at run-time is unknown, but it is probably less than 40k LOC in total due to not using any devices, or any drivers. This can be compared to eg. 350k wasmtime and 165k FireCracker, which are both large enough to also ideally be run with a process jail on top. FireCracker “ships” with a jailer. For TinyKVM, specifically, KVM codebase is around 7k + 37k for base + x86, and TinyKVM is around 9k LOC.</p><p>TinyKVM creates static pagetables during initialization, in a way which works even for strange run-times like Go. The only pages that may be changed afterwards are <span>copy-on-write</span> pages. We can say that the pagetables are largely static, which is a security benefit. They may not be modified after start, and there are run-time sanity checks in place during KVM exits.</p><p>A KVM guest has separate PCID/ASID similar to a process, and so for the purposes of current and future speculation bugs, it will likely be immune.</p><p>The TinyKVM guest has a tiny kernel which cannot be modified. SMEP and SMAP is enabled as well as CR0.WP and page protections. The whole kernel uses 7x 4k pages in total. We also enter in usermode and try to avoid kernel mode except where impossible. Did you know that you can handle CPU exceptions in usermode on x86?</p></div>
<h2>System calls</h2>
<p>An API toward the host is created by defining and hooking up system calls in the emulator. In the guest they are implemented both by the SYSCALL/SYSRET and you can also use an OUT instruction directly from usermode, the latter being the lowest latency. This then causes a VM exit, and registers are inspected to find that a system call is being performed. All in all, the round-trip is a bit less than ~1 microsecond, which is quite different from libriscv’s 2ns. But, you can design a smaller API with larger inputs and outputs, instead of many smaller calls (which makes sense when its cheaper).</p>
<h2>Benchmarks</h2>
<p><img src="https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%203.png?width=600&amp;height=371&amp;name=tinykvm%20image%203.png" width="600" height="371" loading="lazy" alt="tinykvm image 3" srcset="https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%203.png?width=300&amp;height=186&amp;name=tinykvm%20image%203.png 300w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%203.png?width=600&amp;height=371&amp;name=tinykvm%20image%203.png 600w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%203.png?width=900&amp;height=557&amp;name=tinykvm%20image%203.png 900w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%203.png?width=1200&amp;height=742&amp;name=tinykvm%20image%203.png 1200w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%203.png?width=1500&amp;height=928&amp;name=tinykvm%20image%203.png 1500w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%203.png?width=1800&amp;height=1113&amp;name=tinykvm%20image%203.png 1800w" sizes="(max-width: 600px) 100vw, 600px"></p>
<p><em><span>The overhead of resetting the VM + calling into the VM guest.</span></em></p>
<p>For call overhead, we measure the cost of resetting the VM as tail latency, because it can be paid while (for example) a HTTP response is already underway. If you don’t need to reset, simply ignore the red. Reset time scales with working memory usage.</p>
<p><img src="https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%204.png?width=600&amp;height=371&amp;name=tinykvm%20image%204.png" width="600" height="371" loading="lazy" alt="tinykvm image 4" srcset="https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%204.png?width=300&amp;height=186&amp;name=tinykvm%20image%204.png 300w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%204.png?width=600&amp;height=371&amp;name=tinykvm%20image%204.png 600w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%204.png?width=900&amp;height=557&amp;name=tinykvm%20image%204.png 900w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%204.png?width=1200&amp;height=742&amp;name=tinykvm%20image%204.png 1200w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%204.png?width=1500&amp;height=928&amp;name=tinykvm%20image%204.png 1500w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%204.png?width=1800&amp;height=1113&amp;name=tinykvm%20image%204.png 1800w" sizes="(max-width: 600px) 100vw, 600px"></p>
<p><img src="https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%205.png?width=600&amp;height=371&amp;name=tinykvm%20image%205.png" width="600" height="371" loading="lazy" alt="tinykvm image 5" srcset="https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%205.png?width=300&amp;height=186&amp;name=tinykvm%20image%205.png 300w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%205.png?width=600&amp;height=371&amp;name=tinykvm%20image%205.png 600w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%205.png?width=900&amp;height=557&amp;name=tinykvm%20image%205.png 900w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%205.png?width=1200&amp;height=742&amp;name=tinykvm%20image%205.png 1200w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%205.png?width=1500&amp;height=928&amp;name=tinykvm%20image%205.png 1500w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%205.png?width=1800&amp;height=1113&amp;name=tinykvm%20image%205.png 1800w" sizes="(max-width: 600px) 100vw, 600px"></p>
<p>Memory benchmarks are done to see if there’s anything obviously wrong, and there is not. One of my favorite benchmarks was encoding 1500 AVIF images per second in a HTTP benchmark (AVIF encoder as a service):</p>
<p><img src="https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%206.png?width=600&amp;height=371&amp;name=tinykvm%20image%206.png" width="600" height="371" loading="lazy" alt="tinykvm image 6" srcset="https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%206.png?width=300&amp;height=186&amp;name=tinykvm%20image%206.png 300w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%206.png?width=600&amp;height=371&amp;name=tinykvm%20image%206.png 600w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%206.png?width=900&amp;height=557&amp;name=tinykvm%20image%206.png 900w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%206.png?width=1200&amp;height=742&amp;name=tinykvm%20image%206.png 1200w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%206.png?width=1500&amp;height=928&amp;name=tinykvm%20image%206.png 1500w, https://info.varnish-software.com/hs-fs/hubfs/tinykvm%20image%206.png?width=1800&amp;height=1113&amp;name=tinykvm%20image%206.png 1800w" sizes="(max-width: 600px) 100vw, 600px"></p>
<p><em><span>AVIFs can be transcoded from JPEGs quite fast with the right methods.</span></em></p>
<p>So we can see that 16x98.88 = 1582 image transcodings per second. Of course the transcoder settings and image size matters, but I am more interested in the scaling aspect. Does doubling concurrency increase our overall performance up to a point? Yes, it does.</p>
<p>Fun fact: Did you know that you can transcode JPEG to AVIF without going through (lossy?) RGB conversion? YUV planes work the same way in both formats!</p>
<h3>Fast sandboxing</h3>
<div><p>So, what exactly makes this the worlds fastest sandbox? Well, for one it doesn’t use any I/O, doesn’t use any drivers and no virtual devices so it shakes off a common problem with other KVM solutions: Virtualized I/O reduces performance somewhat.</p><p>TinyKVM also focuses heavily on hugepages, even if they are not available in the host, and it will still benefit from the fewer page walks. When backed by real hugepages, we immediately see large gains. I have measured a large CPU-based LLM workload against one running inside TinyKVM, with the same seed and settings, and correcting for setup I found that TinyKVM ran at 99.7% native speed. I don’t know if that means that virtualization has a minimum overhead of 0.3% or if it is just a statistical difference when running for so long. But I’m happy that the difference is small enough not to matter.</p><p>It also completes function calls into the guest (VM calls) fairly fast, meaning we’re not really adding any overheads anywhere. We’re really just running on the CPU, which is what we want. So, provided we have zero-copy solutions for accessing data, we’re just processing it and passing it back in a safe way.</p><p>In short, we’re just processing data at the speed of the native CPU, and that’s exactly what we want. To avoid the overheads related to classical full-system virtualization and enjoy our new lane without any at all.</p></div>
<h2>Drawbacks</h2>
<div><p>It’s not possible to reduce vCPU count after increasing it in the KVM API, and because of this I consider multi-processing something that can be better achieved by running more VMs concurrently and just using/abusing the automatic memory sharing. TinyKVM does have experimental multi-processing support but you can’t wind down after, so it’s not a great choice for long-running processes like Varnish.</p><p>There are multiple ways to work around this, which I won’t go into detail about here, but I’ll just mention that you can re-use a VM by resetting it to another, different VM. Costly, but opens the door for cool solutions like a shared pool of VMs.</p></div>
<h2>Future work</h2>
<ul>
<li>Intel TDX/AMD SEV support would be nice to have.</li>
<li>AArch64 port.</li>
<li>There is a KVM feature to lock down memory in a way where not even kernel mode in the guest can change it called KVM_MEM_READONLY which I hope can be a part of locking down the guest even more. A potential drawback is increased usage of memory mappings.</li>
<li>The user-facing API in TinyKVM needs work to become friendly.</li>
<li>Move much of the system call emulation that I’ve written for a Varnish integration into TinyKVM proper, which paves the way further for dynamic linker loading. ld.so uses mmap() with fd’s to map in files that it is loading. So, if you load ld.so into your own emulator, and then you add your intended program as the first argv argument, and its arguments after that, ld.so will load your program for you, provided you have that mmap() implementation. This is what <em>libriscv</em> does!</li>
</ul>
<h2>Conclusion</h2>
<p>TinyKVM perhaps surprisingly places itself among the smallest serious sandboxing solutions out there, and may also be the fastest. It takes security seriously and tries to avoid complex guest features and kernel mode in general. TinyKVM has a minimal attack surface and no ambition to grow further in complexity, outside of nicer user-facing APIs and ports to other architectures.</p>
<p>Have a look at the <a href="https://github.com/varnish/tinykvm" rel="noopener">code repository for TinyKVM</a>, if you’re interested. Documentation and user-facing API needs a lot of work still, but feel free to build the project and run some simple Linux programs with it. As long as they are static and don’t need file or network access, they might just run out-of-the box.</p>
<p>Not too long from now we will <a href="https://github.com/varnish/libvmod-tinykvm" rel="noopener">open-source a VMOD in Varnish</a> that lets you transform data safely using TinyKVM. It works on both open-source Varnish and Varnish Enterprise! If you'd like to learn more, <a href="https://www.varnish-software.com/contact-us/" rel="noopener">reach out to us today</a>!</p>
<hr>
<p>Blog originally posted <a href="https://fwsgonzo.medium.com/tinykvm-the-fastest-sandbox-564a1c5e9b42" rel="nofollow noopener">here</a>.</p></span></p>

          
        </div>
      </article>

      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Church FAQ (233 pts)]]></title>
            <link>https://whatever.scalzi.com/2025/03/13/the-church-faq/</link>
            <guid>43358947</guid>
            <pubDate>Fri, 14 Mar 2025 02:00:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://whatever.scalzi.com/2025/03/13/the-church-faq/">https://whatever.scalzi.com/2025/03/13/the-church-faq/</a>, See on <a href="https://news.ycombinator.com/item?id=43358947">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

	
	

	
		<p>
		<span>
			<i></i>
			Posted on			<a href="https://whatever.scalzi.com/2025/03/13/the-church-faq/" rel="bookmark">March 13, 2025</a>
			&nbsp;&nbsp;
			Posted by										&nbsp;&nbsp;
				 &nbsp;<a href="https://whatever.scalzi.com/2025/03/13/the-church-faq/#comments">
				14 Comments</a>		</span>
	</p>
	
<figure><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="639" height="849" data-attachment-id="54796" data-permalink="https://whatever.scalzi.com/2025/03/13/the-church-faq/pxl_20230505_171319102/" data-orig-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20230505_171319102.jpg?fit=3072%2C4080&amp;ssl=1" data-orig-size="3072,4080" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.85&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Pixel 7 Pro&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1683292399&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;6.81&quot;,&quot;iso&quot;:&quot;41&quot;,&quot;shutter_speed&quot;:&quot;0.000451&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="PXL_20230505_171319102" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20230505_171319102.jpg?fit=226%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20230505_171319102.jpg?fit=3072%2C4080&amp;ssl=1" src="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20230505_171319102.jpg?resize=639%2C849&amp;ssl=1" alt="" srcset="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20230505_171319102.jpg?w=3072&amp;ssl=1 3072w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20230505_171319102.jpg?resize=226%2C300&amp;ssl=1 226w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20230505_171319102.jpg?resize=768%2C1020&amp;ssl=1 768w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20230505_171319102.jpg?resize=1157%2C1536&amp;ssl=1 1157w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20230505_171319102.jpg?resize=1542%2C2048&amp;ssl=1 1542w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20230505_171319102.jpg?resize=150%2C200&amp;ssl=1 150w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20230505_171319102.jpg?resize=1200%2C1594&amp;ssl=1 1200w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20230505_171319102.jpg?w=1278&amp;ssl=1 1278w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20230505_171319102.jpg?w=1917&amp;ssl=1 1917w" sizes="(max-width: 639px) 100vw, 639px"></figure>


<div>
<figure><img data-recalc-dims="1" decoding="async" width="225" height="338" data-attachment-id="48641" data-permalink="https://whatever.scalzi.com/2023/08/09/post-mortem-on-ohio-issue-1/whsjohns2/" data-orig-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/08/WHSJohnS2.jpg?fit=225%2C338&amp;ssl=1" data-orig-size="225,338" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="WHSJohnS2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/08/WHSJohnS2.jpg?fit=200%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/08/WHSJohnS2.jpg?fit=225%2C338&amp;ssl=1" src="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/08/WHSJohnS2.jpg?resize=225%2C338&amp;ssl=1" alt="" srcset="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/08/WHSJohnS2.jpg?w=225&amp;ssl=1 225w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/08/WHSJohnS2.jpg?resize=200%2C300&amp;ssl=1 200w" sizes="(max-width: 225px) 100vw, 225px"></figure></div>


<p><strong>A few years ago, we bought a church building.</strong> Since then, every time I mention it online and/or on social media, someone always responds, “wait, you bought a church, <em>what</em>” and then asks some standard questions. At this point it makes good sense to offer up a Church FAQ to answer some of those most common questions. Let’s begin!</p>



<p><strong>Wait, you bought a church, <em>what?</em> </strong></p>



<p>Indeed, we bought a church.</p>



<p><strong>Where? </strong></p>



<p>In our town of Bradford, Ohio. </p>



<p><strong>What denomination used to be there?</strong></p>



<p>It’s the former home of Bradford’s Methodist congregation. The church building itself dates back to at least 1919 (that being the year of a calendar we have that features a picture of the building). There was a congregation there until at least 2016. So they got about 100 years of use out of the building. </p>



<p><strong>Why did they stop using it? </strong></p>



<p>The congregation shrank over time, a not uncommon occurrence for mainline protestant churches these days. As I understand it the congregation merged with another congregation down the road, which has services at a different church building. I believe the West Ohio Conference of the Methodist Church (which previously owned the building) may have rented the building for a bit after the congregation left, but when we acquired the building it was not being used, which is probably why the Methodists decided to sell it.</p>



<figure><img decoding="async" src="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2021/11/Scalzi-cbg-2.jpg" alt=""></figure>



<p><strong>So do you live there?</strong></p>



<p>No, we have an actual house to live in. I know old churches are frequently turned into funky residences, but reconfiguring a church to be an actual livable space on a daily basis takes a lot of effort. Our house is designed to be a residence for humans; we prefer to live in that house.</p>



<p><strong>Are you going to use the building as a church and/or start a cult?</strong></p>



<p>No and no. None of the Scalzis are particularly religious, especially in an organized fashion, and despite the actions of certain science fiction authors in the past offering precedent, I have no desire to start a cult. It seems like a lot of work and my ego does not run in the direction of needing acolytes.</p>



<p><strong>Coffee shop and/or bookstore and/or brewpub and/or some other retail business?</strong></p>



<p>I have worked hard all my life <em>not </em>to work in retail and don’t intend to start now, thank you. </p>



<p><strong>Then why did you buy it?</strong></p>



<p>Because we wanted office space. For a number of years Krissy and I talked about starting a company to develop creative projects that were not my novels, and also to handle the licensing and merchandising of the properties that I already had that were not already under option. That company would eventually become Scalzi Enterprises.  Although I write my novels at home, we wanted to have office space elsewhere.</p>



<p><strong>Why?</strong></p>



<p>Because if we eventually hired other people to help us, we wanted them to have some place to work that was not our actual house. And in a general sense it would be useful to have extra space; our house is already full with a quarter-century of us living in it.</p>



<p><strong>Why not get actual office space rather than a church building? </strong></p>



<p>We tried, but we live in a small town without a lot of commercial real estate. We looked at a couple of buildings in town that went up for sale, but weren’t happy with their state of repair. We didn’t want to look outside of Bradford because then there would be a commute. We wanted something within a couple of miles of our house. Eventually it looked like to get what we wanted, we would have to buy a plot of land and then build on it. I went online to look at real estate websites to see what land was available, and as it happens a couple of hours prior, the Methodists put the church building up for sale. We saw the listing, made an appointment to see it that afternoon, and put in an offer when we got home from the viewing. We closed on the building in December of 2021. </p>



<p><strong>What made you offer on the building?</strong></p>



<p>It had everything we wanted — ample space for offices and an excellent location — and above and beyond what we already knew we wanted, when we viewed the space we saw that it offered other opportunities as well. I always wanted an extensive library, for example, and the building had a balcony area which would be perfect for one of those. The basement area would be perfect for having gatherings, and the sanctuary area was, of course, a natural place for concerts or readings or whatever else we might want to do there. And then there was the price.</p>



<p><strong>How much was it?</strong></p>



<p>$75,000. </p>



<p><strong><em>WHAT</em></strong></p>



<p>One of my favorite things to do is show the building to people who live on the coasts, ask them how much they think it cost, and watch them get angry every time I tell them to go lower. But more seriously, we knew that we wouldn’t find a better building anywhere close to us at anywhere near that price. It made <em>absolute </em>economic sense to get the building.</p>



<p><strong>Usually when you get a building like this for that amount of money, it’s on the verge of falling down. Was it? </strong></p>



<p>Thankfully, no. Krissy’s former job was as a insurance claims adjuster; she has certificates attesting to her ability to evaluate the soundness of structures. When we had our visit to the property, she literally climbed through the walls to see for herself what shape the building was in. Her determination: The building would need significant renovation, but fundamentally it was sound. We would need to put in money, but if the renovations were done right it wouldn’t be a money pit.</p>



<figure><img decoding="async" src="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2021/11/PXL_20211108_151419967.jpg" alt=""></figure>



<p><strong>What renovations did you do?</strong></p>



<p>A whole new roof, to start; now the building has a 50-year roof, which means it will almost certainly outlive me. The electricity was knob and tube and had to be redone. There was an outside retaining wall that had to be torn out and redone. The aforementioned balcony was actually not safe to be on; it was cantilevered out into space with no support and had a shin-high barrier that wouldn’t stop anyone from going over the side. That was fixed, and new floors and custom bookcases by a local artisan built in so I could have my library. The basement floor was redone; the kitchen space down there gutted and remodeled. We pulled up high-traffic industrial carpet glued to the sanctuary floor and reconditioned the hardwood floors underneath. New HVAC, and improved drainage for the maintenance room. The office and Sunday school room in the basement was turned into a guest suite. The structure was sealed against moisture and the walls were all replastered  and repainted.</p>



<p>And so on. None of that was cheap, nor was it done quickly; the renovations took two years. Both the time and cost were affected by the work being done during the pandemic, but no matter what it would have been a laborious and expensive process. It was worth it.</p>



<p><strong>Did you do any of that work yourself?</strong></p>



<p>Oh, <em>hell </em>no; I’m not competent to do anything but sign checks. We had contractors do everything, and Krissy, who had 20 years of dealing with contractors in her previous job, managed the renovation on our end. She <em>terrified </em>them. </p>



<p><strong>Are the renovations complete?</strong></p>



<p>The major ones, yes. There are a few things to do but they are second order tasks. I want to recondition the old pastor’s study, get the organ functional again, and we want to make the sanctuary level more easily accessible via ramps and such. But all of that can be dealt with over time. At this point, most of what we wanted and needed to do is done, and we are able to use the building how we intended.</p>



<figure><img data-recalc-dims="1" decoding="async" width="639" height="481" data-attachment-id="54816" data-permalink="https://whatever.scalzi.com/2025/03/13/the-church-faq/pxl_20250210_194814354-2/" data-orig-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20250210_194814354-1.jpg?fit=4080%2C3072&amp;ssl=1" data-orig-size="4080,3072" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.68&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Pixel 9 Pro&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1739216894&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;6.9&quot;,&quot;iso&quot;:&quot;23&quot;,&quot;shutter_speed&quot;:&quot;0.008471&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="PXL_20250210_194814354" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20250210_194814354-1.jpg?fit=300%2C226&amp;ssl=1" data-large-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20250210_194814354-1.jpg?fit=4080%2C3072&amp;ssl=1" src="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20250210_194814354-1.jpg?resize=639%2C481&amp;ssl=1" alt="" srcset="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20250210_194814354-1.jpg?w=4080&amp;ssl=1 4080w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20250210_194814354-1.jpg?resize=300%2C226&amp;ssl=1 300w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20250210_194814354-1.jpg?resize=768%2C578&amp;ssl=1 768w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20250210_194814354-1.jpg?resize=1536%2C1157&amp;ssl=1 1536w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20250210_194814354-1.jpg?resize=2048%2C1542&amp;ssl=1 2048w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20250210_194814354-1.jpg?resize=400%2C300&amp;ssl=1 400w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20250210_194814354-1.jpg?resize=200%2C150&amp;ssl=1 200w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20250210_194814354-1.jpg?resize=1200%2C904&amp;ssl=1 1200w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20250210_194814354-1.jpg?w=1278&amp;ssl=1 1278w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20250210_194814354-1.jpg?w=1917&amp;ssl=1 1917w" sizes="(max-width: 639px) 100vw, 639px"></figure>



<p><strong>What did the people of Bradford think about you buying the church? </strong></p>



<p>By and large the response was positive. We’ve lived in town since 2001 so we weren’t an unknown quantity; everyone here knows us. There was some concern that someone might buy the church for the land underneath it, tear it down and then put up, like, a check-cashing store or a vape shop. So when we bought it and stated our intention to renovate and maintain the building, there was some measure of communal relief. When the renovations were done we held an open house for the community so they could see what we’d done with the place. Most people seemed happy with it.</p>



<p>Likewise, we have the intent of keeping the space a part of the community, and not just as our office space. From time to time we plan to have events there (concerts, readings) that will be free and open to the town, sponsored by the Scalzi Family Foundation (yes, we have a foundation; it’s easier to do a lot of charitable things that way). The building will still be part of the civic life of Bradford.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="639" height="481" data-attachment-id="54823" data-permalink="https://whatever.scalzi.com/2025/03/13/the-church-faq/pxl_20240717_150659567/" data-orig-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20240717_150659567.jpg?fit=4080%2C3072&amp;ssl=1" data-orig-size="4080,3072" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.68&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Pixel 8 Pro&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1721228819&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;6.9&quot;,&quot;iso&quot;:&quot;19&quot;,&quot;shutter_speed&quot;:&quot;0.038147&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="PXL_20240717_150659567" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20240717_150659567.jpg?fit=300%2C226&amp;ssl=1" data-large-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20240717_150659567.jpg?fit=4080%2C3072&amp;ssl=1" src="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20240717_150659567.jpg?resize=639%2C481&amp;ssl=1" alt="" srcset="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20240717_150659567.jpg?w=4080&amp;ssl=1 4080w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20240717_150659567.jpg?resize=300%2C226&amp;ssl=1 300w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20240717_150659567.jpg?resize=768%2C578&amp;ssl=1 768w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20240717_150659567.jpg?resize=1536%2C1157&amp;ssl=1 1536w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20240717_150659567.jpg?resize=2048%2C1542&amp;ssl=1 2048w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20240717_150659567.jpg?resize=400%2C300&amp;ssl=1 400w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20240717_150659567.jpg?resize=200%2C150&amp;ssl=1 200w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20240717_150659567.jpg?resize=1200%2C904&amp;ssl=1 1200w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20240717_150659567.jpg?w=1278&amp;ssl=1 1278w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20240717_150659567.jpg?w=1917&amp;ssl=1 1917w" sizes="auto, (max-width: 639px) 100vw, 639px"></figure>



<p><strong>Does this mean you are going to make the building available for event rentals?</strong> </p>



<p>Probably not. It’s one thing to offer private events, funded by our foundation, that are open to the townsfolk. It’s another thing to offer the space up as a commercial venue. One, that’s a lot more work for us, and two, we would have to make sure the building was up to code as rental space, which would entail more renovations and cost. We occasionally get inquiries and we’ve politely turned them down and are likely to continue to. There are other event spaces in town, from the community center to the local winery, and we encourage people to give them their money. </p>



<p><strong>But you <em>have </em>used it for gatherings, yes?</strong></p>



<p>Sure. My wife threw me a surprise party there for this blog’s 25th anniversary, and when we held an eclipse party in 2024, we had the pre- and post- parties at the church. The last couple of family reunions have been held at the church, and we hold Thanksgiving and Christmas parties there as well. Having a gathering at the church is much less stressful than having it at our house. People aren’t in our personal space, the pets don’t freak out, and people with allergies to cats and dogs don’t have to worry about sneezing. It works out great. Also, when people come to visit, they have the option of staying in the guest suite at the church instead of our more crowded (and cat hair-laden) guest room. So that’s a plus too.  </p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="639" height="481" data-attachment-id="54820" data-permalink="https://whatever.scalzi.com/2025/03/13/the-church-faq/pxl_20231202_172250290-mp/" data-orig-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20231202_172250290.MP_.jpg?fit=4080%2C3072&amp;ssl=1" data-orig-size="4080,3072" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="PXL_20231202_172250290.MP" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20231202_172250290.MP_.jpg?fit=300%2C226&amp;ssl=1" data-large-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20231202_172250290.MP_.jpg?fit=4080%2C3072&amp;ssl=1" src="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20231202_172250290.MP_.jpg?resize=639%2C481&amp;ssl=1" alt="" srcset="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20231202_172250290.MP_.jpg?w=4080&amp;ssl=1 4080w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20231202_172250290.MP_.jpg?resize=300%2C226&amp;ssl=1 300w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20231202_172250290.MP_.jpg?resize=768%2C578&amp;ssl=1 768w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20231202_172250290.MP_.jpg?resize=1536%2C1157&amp;ssl=1 1536w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20231202_172250290.MP_.jpg?resize=2048%2C1542&amp;ssl=1 2048w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20231202_172250290.MP_.jpg?resize=400%2C300&amp;ssl=1 400w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20231202_172250290.MP_.jpg?resize=200%2C150&amp;ssl=1 200w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20231202_172250290.MP_.jpg?resize=1200%2C904&amp;ssl=1 1200w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20231202_172250290.MP_.jpg?w=1278&amp;ssl=1 1278w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20231202_172250290.MP_.jpg?w=1917&amp;ssl=1 1917w" sizes="auto, (max-width: 639px) 100vw, 639px"></figure>



<p><strong>You said something about getting the organ functional again. Do you have, like, a pipe organ?</strong></p>



<p>We do, sort of. The pipes are there, but the organ hasn’t been attached to it for years, possibly decades. The organ itself (which played through a speaker) is also not functioning, and I need to get in touch with someone to repair it. Actually reattaching it to the pipes and making the whole thing work again would be an extremely expensive endeavor and would probably cost as much — if not more — than it cost us to buy the church. Pipe organs are an expensive hobby, basically. I’m not sure I’m ready to commit to that. </p>



<p><strong>Does the church have a bell? And do you ring it?</strong></p>



<p>It does have a bell, and we ring only very occasionally. We don’t want to annoy the neighbors. </p>



<p><strong>Is the church haunted?</strong></p>



<p>We have been told by former parishioners that it is, but I have not met the local ghosts yet. Perhaps they are waiting to see if I am worthy. </p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="639" height="479" data-attachment-id="54818" data-permalink="https://whatever.scalzi.com/2025/03/13/the-church-faq/pxl_20241224_213739431/" data-orig-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20241224_213739431.jpg?fit=4032%2C3024&amp;ssl=1" data-orig-size="4032,3024" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.73&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Pixel 5&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1735076259&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.38&quot;,&quot;iso&quot;:&quot;97&quot;,&quot;shutter_speed&quot;:&quot;0.041674&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="PXL_20241224_213739431" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20241224_213739431.jpg?fit=300%2C225&amp;ssl=1" data-large-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20241224_213739431.jpg?fit=4032%2C3024&amp;ssl=1" src="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20241224_213739431.jpg?resize=639%2C479&amp;ssl=1" alt="" srcset="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20241224_213739431.jpg?w=4032&amp;ssl=1 4032w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20241224_213739431.jpg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20241224_213739431.jpg?resize=768%2C576&amp;ssl=1 768w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20241224_213739431.jpg?resize=1536%2C1152&amp;ssl=1 1536w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20241224_213739431.jpg?resize=2048%2C1536&amp;ssl=1 2048w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20241224_213739431.jpg?resize=1200%2C900&amp;ssl=1 1200w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20241224_213739431.jpg?resize=800%2C600&amp;ssl=1 800w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20241224_213739431.jpg?resize=600%2C450&amp;ssl=1 600w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20241224_213739431.jpg?resize=400%2C300&amp;ssl=1 400w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20241224_213739431.jpg?resize=200%2C150&amp;ssl=1 200w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20241224_213739431.jpg?w=1278&amp;ssl=1 1278w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2025/03/PXL_20241224_213739431.jpg?w=1917&amp;ssl=1 1917w" sizes="auto, (max-width: 639px) 100vw, 639px"></figure>



<p><strong>Isn’t it a little… <em>quirky </em>to own a church? </strong></p>



<p>I mean, yes. I’ve noted before that now I’ve become a bit of a cliche, that cliche being the eccentric writer who owns a folly. Some own theaters and railways, some own Masonic temples, some own islands. I own a church. In my defense, I had a functional reason to own it, noted above, and I didn’t spend a genuinely silly amount of money for it, also noted above. As a folly, it is both practical and affordable. </p>



<p><strong>What do you call the church now?</strong></p>



<p><em>Not</em> “Church of the Scalzi,” which is actually the name of a church in Venice, Italy. Its formal name now is “The Old Church.” But for day-to-day use we just say “the church.” </p>



<p><strong>Hey, have you ever heard of that song, “Alice’s Restaurant”?</strong></p>



<p>Yes, I have, and everyone thinks they are being terribly clever when they reference it to me. After the first thousand times it wears a smidge thin. </p>



<p><strong>Is it true that your six-necked guitar now resides at the church?</strong></p>



<p>It is true: <a href="https://whatever.scalzi.com/2021/03/10/today-in-life-choices-that-are-either-questionable-or-awesome-or-both/">The Beast</a>, as it is called, and was called long before I owned it, currently resides on the altar. It surprises people every time they see it.</p>



<p><strong>Are you <em>sure </em>you’re not starting a cult?</strong></p>



<p>I’m sure. Besides, who would want to worship me? Krissy, maybe. Me, nah. </p>



<p>And that’s it for now. If there are more questions I think need to be in the FAQ, I’ll add them as I go along. </p>



<p>— JS</p>

	
	<!-- BEGIN .postmeta -->
	

	<!-- .post-navigation -->

	
<!-- #comments -->

	

	
<!-- END .postarea -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Profit-Enhancing Middlemen' Fuel $200B Health-Care Chaos (111 pts)]]></title>
            <link>https://www.bloomberg.com/news/features/2025-03-13/middlemen-in-us-health-care-are-driving-up-costs-frustrating-patients</link>
            <guid>43358872</guid>
            <pubDate>Fri, 14 Mar 2025 01:43:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/features/2025-03-13/middlemen-in-us-health-care-are-driving-up-costs-frustrating-patients">https://www.bloomberg.com/news/features/2025-03-13/middlemen-in-us-health-care-are-driving-up-costs-frustrating-patients</a>, See on <a href="https://news.ycombinator.com/item?id=43358872">Hacker News</a></p>
Couldn't get https://www.bloomberg.com/news/features/2025-03-13/middlemen-in-us-health-care-are-driving-up-costs-frustrating-patients: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[C Plus Prolog (151 pts)]]></title>
            <link>https://github.com/needleful/c_plus_prolog</link>
            <guid>43357955</guid>
            <pubDate>Thu, 13 Mar 2025 22:48:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/needleful/c_plus_prolog">https://github.com/needleful/c_plus_prolog</a>, See on <a href="https://news.ycombinator.com/item?id=43357955">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">C Plus Prolog</h2><a id="user-content-c-plus-prolog" aria-label="Permalink: C Plus Prolog" href="#c-plus-prolog"></a></p>
<p dir="auto">Prolog is the only good programming language. I should know, <a href="https://needleful.net/" rel="nofollow">my website</a> is <a href="https://github.com/needleful/nng">written in Prolog</a>.</p>
<p dir="auto">Unfortunately, C is the only useful programming language.</p>
<p dir="auto">Scientists have been trying to find an answer to this problem for nearly 50 years. Some make their <a href="https://doc.rust-lang.org/book/ch19-06-macros.html" rel="nofollow">C more like Prolog</a>. Others make their <a href="https://prescheme.org/" rel="nofollow">Prolog more like C</a>.</p>
<p dir="auto">I offer a new solution: simply add Prolog and C together. I call it, “C Plus Prolog”, or “C+P” for short.</p>
<div dir="auto" data-snippet-clipboard-copy-content=":- include(stdio).

int func main
{ 
	puts(&quot;Hello, world!&quot;);
	return 0
}."><pre>:- include(<span>stdio</span>)<span>.</span>

<span>int</span> <span>func</span> <span>main</span>
{ 
	puts(<span>"Hello, world!"</span>);
	<span>return</span> <span>0</span>
}<span>.</span></pre></div>
<p dir="auto">If you're familiar with C, you'll notice this is some sort of weird, bad C. You're mistaken, however. This is valid Prolog, using some <a href="https://www.swi-prolog.org/pldoc/man?section=ext-blockop" rel="nofollow">non-standard features of SWI-Prolog</a> for the curly braces.</p>
<p dir="auto">This Prolog is read as a list of terms, and converted to valid C code. So this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="int func main(int var argc, char:ptr:array var argv)"><pre><span>int</span> <span>func</span> main(<span>int</span> <span>var</span> <span>argc</span>, <span>char</span>:<span>ptr</span>:<span>array</span> <span>var</span> <span>argv</span>)</pre></div>
<p dir="auto">Translates to this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="int main(int argc, char*( argv[]))"><pre><span>int</span> <span>main</span>(<span>int</span> <span>argc</span>, <span>char</span><span>*</span>( <span>argv</span>[]))</pre></div>
<p dir="auto">Beyond some obvious changes like <code>var</code> and <code>func</code> operators, which make the syntax expressible as Prolog, there are some unexpected quirks:</p>
<ul dir="auto">
<li>Symbols starting with capital letters or underscores are for use with Prolog, and won't translate to C unless wrapped in single quotes:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="int:ptr var string = 'NULL';
'_Bool' var v = true;"><pre><span>int</span>:<span>ptr</span> <span>var</span> <span>string</span> = <span>'NULL'</span>;
<span>'_Bool'</span> <span>var</span> <span>v</span> = <span>true</span>;</pre></div>
<ul dir="auto">
<li>As a result of the above, character literals start with <code>#</code>:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="char var c = #c;    % quotes not required for lowercase letters
char var c2 = #'C'; 
char var nl = #'\n';"><pre><span>char</span> <span>var</span> <span>c</span> = #<span>c</span>;    % <span>quotes</span> <span>not</span> <span>required</span> <span>for</span> <span>lowercase</span> <span>letters</span>
<span>char</span> <span>var</span> <span>c2</span> = #<span>'C'</span>; 
<span>char</span> <span>var</span> <span>nl</span> = #<span>'\n'</span>;</pre></div>
<ul dir="auto">
<li>Operator precedence was maintained for existing Prolog operators. For example, <code>=</code> and comparison operators like <code>==</code>, <code>&lt;</code>, and <code>&gt;</code> have the same precedence, so you'll need parentheses where you wouldn't in C:</li>
</ul>

<p dir="auto">This is also why the arrow operator <code>-&gt;</code> was replaced with <code>@</code>, since it's used in a completely different way in Prolog and has a completely different precedence.</p>
<p dir="auto">C+P:</p>

<p dir="auto">C:</p>

<ul dir="auto">
<li>Complex type declarations are different. C-style declarations are perfectly expressible in Prolog, I just don't like them.</li>
</ul>

<p dir="auto">Becomes</p>

<p dir="auto">It may also help you remember the difference between <code>const char *</code> and <code>char const *</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const(char):ptr,
char:const(ptr)"><pre><span>const</span>(<span>char</span>):<span>ptr</span>,
<span>char</span>:const(<span>ptr</span>)</pre></div>
<p dir="auto">The examples provide a more complete picture of the syntax.
In the end, it's C but more verbose and particular about semicolons. Not exactly a silver bullet.</p>
<p dir="auto">Let's introduce the <code>*=&gt;</code> operator.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The <code>*=&gt;</code> Operator</h2><a id="user-content-the--operator" aria-label="Permalink: The *=> Operator" href="#the--operator"></a></p>
<p dir="auto">Take this snippet from example 04:</p>
<div dir="auto" data-snippet-clipboard-copy-content="max(A, B) *=> A > B then A else B."><pre><span>max</span>(<span>A</span>, <span>B</span>) *=&gt; <span>A</span> &gt; <span>B</span> <span>then</span> <span>A</span> <span>else</span> <span>B</span><span>.</span></pre></div>
<p dir="auto">I said C+P does no processing beyond translating terms to C, but it does one small step. The compiler will gather all the terms defined with <code>*=&gt;</code>, and then substitute the left-hand for the right-hand in the rest of the code until no more rules apply.</p>
<p dir="auto">Because this operates on Prolog terms, not text, we don't need any extra parentheses in our <code>max(A, B)</code> macro to prevent mishaps with operator precedence. It's inserted into code as a single term, and looks exactly like a function call in use:</p>
<div dir="auto" data-snippet-clipboard-copy-content="float var f = max(12.3, 0) + 20;
printf(&quot;Should be 32.3: %f\n&quot;, f);`"><pre><span>float</span> <span>var</span> <span>f</span> = max(<span>12</span>.<span>3</span>, <span>0</span>) + <span>20</span>;
printf(<span>"Should be 32.3: %f\n"</span>, <span>f</span>);`</pre></div>
<p dir="auto">It's converted to the following C:</p>
<div dir="auto" data-snippet-clipboard-copy-content="float f=(((12.3>0) ? 12.3 : 0)+20);
printf(&quot;Should be 32.3: %f\n&quot;, f);"><pre><span>float</span> <span>f</span><span>=</span>(((<span>12.3</span><span>&gt;</span><span>0</span>) ? <span>12.3</span> : <span>0</span>)<span>+</span><span>20</span>);
<span>printf</span>(<span>"Should be 32.3: %f\n"</span>, <span>f</span>);</pre></div>
<p dir="auto">Also, I'm tired of adding <code>\n</code> at the end of all my <code>printf</code> statements. Let's defined another macro, <code>println</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="PrintLn *=> PrintF :-
	PrintLn =.. [println,Format| Args],
	string_concat(Format, &quot;\n&quot;, Format2),
	PrintF =.. [printf,Format2| Args]."><pre>PrintLn *=&gt; PrintF :-
	<span>PrintLn</span> <span>=</span><span>.</span>. [println,Format| Args],
	<span>string_concat</span>(<span>Format</span>, <span>"\n"</span>, <span>Format2</span>),
	<span>PrintF</span> =<span>.</span>. [printf,Format2| Args].</pre></div>
<p dir="auto">We have full access to Prolog at compile time using <code>:-</code>, allowing us to do just about anything.
This macro gets any instance of <code>println(Format, Args...)</code> with a string literal <code>Format</code>, and converts it to <code>printf</code> with a newline appended to <code>Format</code>.</p>
<p dir="auto">Simple enough. Let's implement poor man's generics.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Poor Man's Generics in C Plus Prolog</h2><a id="user-content-poor-mans-generics-in-c-plus-prolog" aria-label="Permalink: Poor Man's Generics in C Plus Prolog" href="#poor-mans-generics-in-c-plus-prolog"></a></p>
<p dir="auto">Example 05 defines a generic type, <code>list[T]</code>, using the following syntax:</p>
<div dir="auto" data-snippet-clipboard-copy-content="list[T]
{
	struct list[T] {
		T:ptr:ptr var items
	};

	% Let's also define syntax for type-specific functions, in this case “list[T]:capacity”
	size_t:ptr func list[T]:capacity(list[T] var this)
	{
		...
	};
	...
}."><pre><span>list</span>[<span>T</span>]
{
	<span>struct</span> <span>list</span>[<span>T</span>] {
		<span>T</span>:<span>ptr</span>:<span>ptr</span> <span>var</span> <span>items</span>
	};

	% <span>Let</span>'<span>s</span> <span>also</span> <span>define</span> <span>syntax</span> <span>for</span> <span>type</span>-<span>specific</span> <span>functions</span>, <span>in</span> <span>this</span> <span>case</span> “<span>list</span>[<span>T</span>]:<span>capacity</span>”
	<span>size_t</span>:<span>ptr</span> <span>func</span> <span>list</span>[<span>T</span>]:capacity(<span>list</span>[<span>T</span>] <span>var</span> <span>this</span>)
	{
		<span>.</span>..
	};
	...
}.</pre></div>
<p dir="auto">This will work similarly to C++ templates.
For simplicity of implementation, we'll require the user to instantiate the template.</p>
<div dir="auto" data-snippet-clipboard-copy-content="declare(list[int]).
declare(list[const(char):ptr])."><pre><span>declare</span>(<span>list</span>[<span>int</span>])<span>.</span>
<span>declare</span>(<span>list</span>[const(<span>char</span>):<span>ptr</span>])<span>.</span></pre></div>
<p dir="auto">We could probably do this automatically by scanning the code for any usage of <code>list[T]</code> and instantiating the template right above it, but I'll leave that as an exercise for the reader.</p>
<p dir="auto">We also have some syntax to get a function name with <code>list[T]:Method</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="int func main {
	list[int] var my_ints = list[int]:new(17);

	size_t var size = *(list[int]:capacity(my_ints));
	for(int var i = 0; i < size; i += 1)
	{
		list[int]:append(my_ints, i*i)
	};
	for(int var i = 0; i < size; i+= 1)
	{
		printf(&quot;%d squared = %d.\n&quot;, i, list[int]:get(my_ints, i))
	};
	return 0
}."><pre><span>int</span> <span>func</span> <span>main</span> {
	<span>list</span>[<span>int</span>] <span>var</span> <span>my_ints</span> = <span>list</span>[<span>int</span>]:new(<span>17</span>);

	<span>size_t</span> <span>var</span> <span>size</span> = *(<span>list</span>[<span>int</span>]:capacity(<span>my_ints</span>));
	for(<span>int</span> <span>var</span> <span>i</span> = <span>0</span>; <span>i</span> &lt; <span>size</span>; <span>i</span> += <span>1</span>)
	{
		<span>list</span>[<span>int</span>]:append(<span>my_ints</span>, <span>i</span>*<span>i</span>)
	};
	for(<span>int</span> <span>var</span> <span>i</span> = <span>0</span>; <span>i</span> &lt; <span>size</span>; <span>i</span>+= <span>1</span>)
	{
		printf(<span>"%d squared = %d.\n"</span>, <span>i</span>, <span>list</span>[<span>int</span>]:get(<span>my_ints</span>, <span>i</span>))
	};
	<span>return</span> <span>0</span>
}<span>.</span></pre></div>
<p dir="auto">Not exactly C++, but it keeps the namespaces clear.</p>
<p dir="auto">Let's read the macro.</p>
<p dir="auto">It matches on our template as you might have expected:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Name[T] {Body} *=> Comment :-
	\+ground(T), atom(Name),
	atom_concat('//Defined template type: ', Name, Comment),"><pre>Name[T] {Body} *=&gt; Comment :-
	<span>\+</span>ground(<span>T</span>), atom(<span>Name</span>),
	atom_concat(<span>'//Defined template type: '</span>, <span>Name</span>, <span>Comment</span>),</pre></div>
<p dir="auto">We have a template with a name <code>Name</code>, type parameterss <code>T</code>, and the body <code>Body</code>.
The macro removes this code and inserts a comment. Everything else is handled in the Prolog world.</p>
<div dir="auto" data-snippet-clipboard-copy-content="	assertz(
		declare(Name[Z]) *=> NewBody"><pre>	<span>assertz</span>(
		declare(<span>Name</span>[<span>Z</span>]) *=&gt; <span>NewBody</span></pre></div>
<p dir="auto">By God. Our macro <code>assert</code>s another macro, <code>declare(Name[Z])</code>. It also has conditions:</p>
<div dir="auto" data-snippet-clipboard-copy-content="		:- (
			ground(Z),
			T=Z,
			Body=NewBody,"><pre>		:- (
			ground(<span>Z</span>),
			<span>T</span><span>=</span><span>Z</span>,
			<span>Body</span><span>=</span><span>NewBody</span>,</pre></div>
<p dir="auto">Those three lines are the bulk of the macro. It unifies the template type <code>T</code> with the real (ground) type <code>Z</code>, then returns the body of the template. This is what turns <code>declare(list[int])</code> into the code for the type.</p>
<p dir="auto">But that's not all it does, the macro we're asserting itself asserts more macros:</p>
<div dir="auto" data-snippet-clipboard-copy-content="			('*mangled_name'(Name[Z]) *=> ZName),
			assertz(Name[Z] *=> ZName),
			assertz((ZName:Method *=> ZMethod :-
				% Crudely hacking some shorthand for method names
				Method \= ptr, Method \= array,
				(	atom(Method)
				->	MName = Method,
					ZArgs = []
				;	Method =.. [MName|ZArgs]
				),
				('*mangled_name'(ZName:MName) *=> MZ_Name),
				ZMethod =.. [MZ_Name|ZArgs]
			))
		))."><pre>			('*mangled_name'(Name[Z]) *=&gt; ZName),
			<span>assertz</span>(<span>Name</span>[<span>Z</span>] *=&gt; <span>ZName</span>),
			assertz((<span>ZName</span>:<span>Method</span> *=&gt; <span>ZMethod</span> :-
				% <span>Crudely</span> <span>hacking</span> <span>some</span> <span>shorthand</span> <span>for</span> <span>method</span> <span>names</span>
				<span>Method</span> \= <span>ptr</span>, <span>Method</span> \= <span>array</span>,
				(	atom(<span>Method</span>)
				-&gt;	<span>MName</span> = <span>Method</span>,
					<span>ZArgs</span> = <span>[]</span>
				;	<span>Method</span> =<span>.</span>. [MName|ZArgs]
				),
				('*mangled_name'(ZName:MName) *=&gt; MZ_Name),
				<span>ZMethod</span> =<span>.</span>. [MZ_Name|ZArgs]
			))
		)).</pre></div>
<p dir="auto">This generates the C names for things like <code>list[int]</code> and <code>list[int]:function</code>. <code>'*mangled_name'</code> is just another macro, but this example is long enough and it's all in example 05. The <code>*</code> and single quotes are nothing special, they just prevent this macro from accidentally colliding with a user-defined <code>mangled_name</code> function.</p>
<p dir="auto">C+P provides no type information we could use for method syntax, to do something like <code>my_list.append(...)</code>, instead of <code>list[int]:append(my_list, ...)</code>.</p>
<p dir="auto">We could, of course, use macros to gather the types of every variable in a function body and swap out method calls for the appropriate function, but at a certain point I'm just writing a full-on XLang-to-C compiler in the macro system, which is an interesting idea, but I'm employed.</p>
<p dir="auto">I've provided several other examples of the wonders of C Plus Prolog:</p>
<ul dir="auto">
<li>Example 06 overloads the <code>struct</code> keyword to add compile-time reflection.</li>
<li>Example 08a compiles different code if the target file ends in <code>.h</code> or <code>.c</code>, to declare a typical header and implementation in a single file.</li>
</ul>
<p dir="auto">I find it strangely compelling to write in C+P, adding features that sound impossible. It's like a puzzle game, which is why I reach for Prolog so frequently.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation and Usage</h2><a id="user-content-installation-and-usage" aria-label="Permalink: Installation and Usage" href="#installation-and-usage"></a></p>
<p dir="auto">C Plus Prolog is easy to install. All you need is a C compiler, SWI-Prolog, and this repository. You can figure it out.</p>
<p dir="auto">Then you can run <code>cpp.pl</code> from this repository with the following syntax:
<code>swipl -s cpp.pl -- &lt;input file&gt; &lt;output file&gt;</code></p>
<p dir="auto">By convention, C Plus Prolog files end in the extension <code>.c+p</code>.</p>
<p dir="auto">Check <code>test.sh</code> and <code>test.ps1</code> for more example usage, plus a fast way to run all the tests (which are probably not rootkits).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is the point of this?</h2><a id="user-content-what-is-the-point-of-this" aria-label="Permalink: What is the point of this?" href="#what-is-the-point-of-this"></a></p>
<p dir="auto">C Plus Prolog is a half-serious exploration of macros in a systems programming language.
In the process of making this, it became clear I prefer the compile-time evaluation and reflection offered by languages like D and Zig over syntactic macros.</p>
<p dir="auto">Sure, with enough work you can do everything and more with the macro system, but what does it actually <em>add</em> over a separate code generator, or a DSL? A nicer interface, maybe.</p>
<p dir="auto">In Common Lisp, you have the full codebase at compile time, but the real value is recompiling code at runtime, not manipulating a list of lists that sort of looks like code, if you squint through all the backquotes and commas.</p>
<p dir="auto">Rust's procedural macros save maybe 40 lines of code, since you don't have to write your own tokenizer, but everything else is up to you and whatever libraries you want to bring in.</p>
<p dir="auto">Most of my metaprogramming wants involve reflecting on information the compiler already knows, like the types and annotations in the code, not the raw syntax tree.  For a language-within-a-language, which syntactic macros are best at, I'd usually rather go the extra mile and make an entirely separate DSL with a purpose-built syntax, rather than contort the problem to, say, S expressions or Prolog terms.</p>
<p dir="auto">Despite that, C+P is dangerously close to being useful.
The biggest advantage it has is the fact it generates plain C by default, allowing for performant cross-platform builds where less popular languages don't have support, or have to generate much more complicated C due to different abstractions.
Prolog's been around for 50 years, as well. If the SWI-Prolog extensions were removed, swapping <code>func F {Body}</code> with <code>func F =&gt; Body</code>, for example, it could be built on a huge swath of hardware.</p>
<p dir="auto">But I wouldn't want to use it. There's absolutely no validation or error messaging, so any mistakes lead to broken C or silent failures. I thought using <code>var</code> as an operator would be nice, but it's a lot of visual noise.  And if you thought C's semicolons were annoying, in C Plus Prolog, semicolons are operators. Having two in a row breaks things. Having one at the start breaks things. Having one at the <em>end</em> breaks things.</p>
<p dir="auto">If someone wanted to use C+P, there are many better alternatives.</p>
<ul dir="auto">
<li>The <a href="https://nim-lang.org/" rel="nofollow">Nim</a> and <a href="https://haxe.org/" rel="nofollow">Haxe</a> languages can compile to C and/or C++, and they offer a good user experience out of the box, though I can't say how directly their code translates to C.</li>
<li><a href="https://github.com/eudoxia0/cmacro">cmacro</a> offers a similar level of syntax manipulation to C+P in a nicer format, powered by Common Lisp.</li>
<li><a href="https://dlang.org/" rel="nofollow">The D language</a> Has compilers backed by GCC and LLVM, so it should work in most places C will.</li>
</ul>
<p dir="auto">I don't know what the conclusion is.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Y Combinator urges the White House to support Europe's Digital Markets Act (353 pts)]]></title>
            <link>https://techcrunch.com/2025/03/13/y-combinator-urges-the-white-house-to-support-europes-digital-markets-act/</link>
            <guid>43357739</guid>
            <pubDate>Thu, 13 Mar 2025 22:12:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/03/13/y-combinator-urges-the-white-house-to-support-europes-digital-markets-act/">https://techcrunch.com/2025/03/13/y-combinator-urges-the-white-house-to-support-europes-digital-markets-act/</a>, See on <a href="https://news.ycombinator.com/item?id=43357739">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Y Combinator, one of the world’s most prolific startup accelerators, sent a letter on Wednesday urging the Trump administration to openly support Europe’s Digital Markets Act (DMA), a wide-ranging piece of legislation that <a href="https://techcrunch.com/2024/03/07/europes-dma-rules-for-big-tech-explained/">aims to crack open Big Tech’s market power.</a></p>

<p>The DMA designates six tech companies as “gatekeepers” to the internet — Alphabet, Amazon, Apple, ByteDance, Meta, and Microsoft — and limits these technology kingpins from engaging in anticompetitive tactics on their platforms, in favor of interoperability. The law became applicable in May 2023, and it’s <a href="https://techcrunch.com/2025/03/05/eu-must-fully-apply-its-market-fairness-rulebook-on-google-search-rivals-urge/">already had a major impact on American tech companies.</a></p>







<p>In a letter to the White House <a href="https://x.com/lutherlowe/status/1900244370881667376" target="_blank" rel="noreferrer noopener nofollow">posted on X</a> by YC’s head of Public Policy, Luther Lowe, the startup accelerator argued that the DMA shouldn’t be lumped in with other European tech legislation, which U.S. officials often criticize as being overbearing.</p>

<p>Instead, YC argues in the letter that the spirit of Europe’s DMA is in line with values that promote — not hinder — American innovation.</p>

<p>“[W]e respectfully urge the White House to recalibrate its stance toward Europe’s digital regulation, drawing a clear line between measures that hamper innovation and those that foster it,” states YC’s letter, which was also signed by YC-backed startups, independent tech companies, and trade associations.</p>

<p>It’s not entirely surprising that YC would come out in explicit public support of the DMA. After all, the accelerator markets itself as a champion of “Little Tech” — an American venture-backed ecosystem of technology startups.</p>

<p>YC argues in the letter that the DMA opens up key avenues to create opportunities for American startups in AI, search, and consumer apps, and prevents Big Tech companies from boxing out smaller ventures.</p>


<p>Specifically, YC in its letter points to Apple <a href="https://techcrunch.com/2025/03/02/apple-might-not-release-a-truly-modernized-siri-until-2027/">reportedly delaying its LLM-powered version of Siri until 2027</a>, years after competitors brought generative AI voice assistants to market. YC argues this represents a lack of competitive pressure, noting that third-party developers of AI voice assistants are unable to integrate their services into Apple’s operating systems</p>

<p>YC might take Big Tech to task for its reported anticompetitive behavior and take shots at companies like Apple, which it argues harms the venture-backed startup ecosystem. But YC and other supposedly Little Tech-aligned VCs are actually becoming quite influential in Washington.</p>

<p>Andreessen Horowitz (a16z), which published a “Little Tech Agenda” last year, spends millions of dollars trying to influence policy battles at the federal and local levels. According to data from <a href="https://www.opensecrets.org/orgs/andreessen-horowitz/summary?id=D000047147" target="_blank" rel="noreferrer noopener nofollow">Open Secrets</a>, a16z’s contributions during the 2024 U.S. election cycle totaled $89 million. YC, still a smaller player in American politics, <a href="https://www.opensecrets.org/orgs/y-combinator/summary?id=D000069477" target="_blank" rel="noreferrer noopener nofollow">contributed around $2 million</a>.</p>







<p>What’s less clear here is how the Trump administration will respond to the DMA in the long run — and YC’s endorsement of it.</p>

<p>President Trump signaled in January that he would <a href="https://finance.yahoo.com/news/trump-opens-a-divide-between-us-and-eu-over-big-tech-213413056.html" target="_blank" rel="noreferrer noopener">protect American tech companies from overzealous European regulators</a>. However, Trump has also <a href="https://www.cnn.com/2024/08/31/politics/video/smr-trump-zuckerberg" target="_blank" rel="noreferrer noopener nofollow">historically been tough on Big Tech firms</a> like Apple, Google, and Meta.</p>

<p>During the Paris AI Action Summit in February, Vice President J.D. Vance criticized a few of the EU’s laws against tech companies, including the Digital Services Act and General Data Protection Regulation. However, Vance didn’t mention the DMA, which more narrowly targets anticompetitive tech industry practices.</p>

<p>Lowe told TechCrunch last year <a href="https://techcrunch.com/2024/06/11/dcs-political-class-doesnt-know-y-combinator-exists-but-its-trying-to-change-that/">during a StrictlyVC event</a> that the DMA is “not perfect, but at least they’re taking a stab at figuring out how do we curb the most egregious forms of self-preferencing by these large firms.”</p>

<p>Lowe did not immediately respond to TechCrunch’s request for comment.</p>
</div><div>
	
	
	
	

	
<div>
	<p>
		Maxwell Zeff is a senior reporter at TechCrunch specializing in AI and emerging technologies. Previously with Gizmodo, Bloomberg, and MSNBC, Zeff has covered the rise of AI and the Silicon Valley Bank crisis. He is based in San Francisco. When not reporting, he can be found hiking, biking, and exploring the Bay Area’s food scene.	</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/maxwell-zeff/" data-event="button" href="https://techcrunch.com/author/maxwell-zeff/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Recursion kills: The story behind CVE-2024-8176 in libexpat (126 pts)]]></title>
            <link>https://blog.hartwork.org/posts/expat-2-7-0-released/</link>
            <guid>43357687</guid>
            <pubDate>Thu, 13 Mar 2025 22:05:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.hartwork.org/posts/expat-2-7-0-released/">https://blog.hartwork.org/posts/expat-2-7-0-released/</a>, See on <a href="https://news.ycombinator.com/item?id=43357687">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody text">
    <p>For readers new to Expat:</p>
<blockquote>
<p><a href="https://libexpat.github.io/">libexpat</a> is a fast streaming XML parser.
Alongside libxml2, Expat is one of the
<a href="https://libexpat.github.io/doc/users/">most widely used</a>
software libre XML parsers written in C, specifically C99.
It is cross-platform and licensed under
<a href="https://opensource.org/licenses/MIT">the MIT license</a>.</p>
</blockquote>
<p><a href="https://github.com/libexpat/libexpat/releases/tag/R_2_7_0">Expat 2.7.0</a> has been released earlier today.
I will make this a more detailed post than usual
because in many ways there is more to tell about this release
than the average libexpat release: there is a story this time.</p>
<h2>What is in release 2.7.0?</h2>
<p>The key motivation for cutting a release now is to get the fix to
a long-standing vulnerability out to users: I will get to that vulnerability
— <code>CVE-2024-8176</code> — in detail in a moment.  First, what else is in this release?</p>
<p>There are also fixes to the two official build systems as usual,
as well as improvements to the documentation.</p>
<p>There is a <a href="https://github.com/libexpat/libexpat/pull/950">new fuzzer <code>xml_lpm_fuzzer</code></a>
by <a href="https://github.com/c01db33f">Mark Brand</a>
that OSS-Fuzz has already started to include with their daily
<a href="https://introspector.oss-fuzz.com/project-profile?project=expat">continuous fuzzing</a>;
the fuzzer is based on
Clang's <a href="https://llvm.org/docs/LibFuzzer.html">libFuzzer</a> and
Google's <a href="https://github.com/google/libprotobuf-mutator">libprotobuf-mutator (LPM)</a> that
applies a variant of
<a href="https://google.github.io/clusterfuzz/reference/coverage-guided-vs-blackbox/">coverage-guided fuzzing</a> called
<a href="https://github.com/google/fuzzing/blob/master/docs/structure-aware-fuzzing.md">structured fuzzing</a>.
A side job of integrating that new fuzzer was making dependency libprotobuf-mutator support
the versions of Protobuf that are shipped by Ubuntu 24.04, 22.04 and 20.04:
<a href="https://github.com/google/libprotobuf-mutator/pulls?q=is%3Amerged+is%3Apr+author%3Ahartwork">my related work upstream</a>
is available to everyone.</p>
<p>Another interesting sideshow of this release is the (harmless)
<a href="https://en.wikipedia.org/wiki/Time-of-check_to_time-of-use">TOCTTOU issue</a>
that was uncovered by static analysis
in a benchmarking helper tool shipped next to core libexpat.  If you have not heard
of that class of race condition vulnerability but are curious,
the <a href="https://github.com/libexpat/libexpat/pull/959">related pull request</a>
could be of interest: it is textbook TOCTTOU in a real-world example.</p>
<p>One other thing that is new in this release is that Windows binaries are now built
by GitHub Actions rather than AppVeyor and not just 32bit but also 64bit.
I have added 64bit binaries post-release to the
<a href="https://github.com/libexpat/libexpat/releases/tag/R_2_6_4">previous release Expat 2.6.4</a>
already on January 21st, but only now it is becoming a regular part of the release process.</p>
<h2>The vulnerability report</h2>
<p>So what is that long-standing vulnerability about?
In July 2022 — roughly two and a half years ago —
<a href="https://thejh.net/">Jann Horn</a> of <a href="https://googleprojectzero.blogspot.com/">Google Project Zero</a>
and <a href="https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html">Spectre/Meltdown</a> fame
reached out to me via e-mail with a finding
in libexpat, including an idea for a fix.</p>
<p>What he found can be thought of as
"the linear version of <a href="https://en.wikipedia.org/wiki/Billion_laughs_attack">billion laughs</a>"
— a linear chain
(of so-called <a href="https://www.w3.org/TR/2006/REC-xml-20060816/#sec-physical-struct">general entities</a>)
rather than a tree — like this:</p>
<div><pre><span>&lt;!DOCTYPE doc [</span>
<span>  &lt;!ENTITY g0 ''&gt;</span>
<span>  </span><span>&lt;!ENTITY g1 '&amp;g0;'&gt;</span>
<span>  </span><span>&lt;!ENTITY g2 '&amp;g1;'&gt;</span>
]&gt;
<span>&lt;doc&gt;</span><span>&amp;g2;</span><span>&lt;/doc&gt;</span>
</pre></div>

<p>Except not with two (or three) levels, but thousands.
Why would a chain of thousands of entity references be a problem to libexpat?
Because of recursion, because of recursive C function calls:
each call to a function increases the stack, and
if functions are calling each other recursively, and
attacker-controlled input can influence the number of recursive calls,
then with the right input, attackers can force the stack to overflow
into the heap: stack overflow, segmentation fault,
<a href="https://en.wikipedia.org/wiki/Denial-of-service_attack">denial of service</a>.
It depends on the stack size of the target machine how many levels of nesting
it takes for this to hit: 23,000 levels of nesting would be enough
to hit on one machine, but not another.</p>
<p>The education that introduces or leads people towards recursion should come
with a warning; recursion is not just beautiful, a thinking tool and allowing
for often simpler solutions — it also has a dark side to it:
a big inherent security problem.
The article
<a href="https://www.researchgate.net/publication/220477862_The_Power_of_10_Rules_for_Developing_Safety-Critical_Code">The Power of 10: Rules for Developing Safety-Critical Code</a>
warned about the use of recursion in 2006, but Expat development already started in 1997.</p>
<p>Already in that initial e-mail, Jann shared what he considered the fix —
avoiding (or resolving) recursion — and there was a proof-of-concept patch
attached of how that could be done in general.
Unlike other Project Zero findings, there would be
no <a href="https://googleprojectzero.blogspot.com/p/vulnerability-disclosure-policy.html">90-days-deadline</a>
for this issue,
because
— while <a href="https://www.qualys.com/2017/06/19/stack-clash/stack-clash.txt">stack clashing</a>
was considered and is a theoretical possibility —
denial of service was considered to be the realistic impact.
It should be noted that this risk assessment comes without any guarantees.</p>
<h2>The vulnerability process</h2>
<p>Two things became apparent to me:</p>
<ol>
<li>It seemed likely that this vulnerability had multiple "faces" or variants,
   and that the only true fix would indeed be to effectively remove <em>all</em> remaining
   recursion from Expat.
   It is not the first time that recursion has been an issue in C software, or even
   libexpat in particular: <a href="https://github.com/ferivoz">Samanta Navarro</a>
   <a href="https://github.com/libexpat/libexpat/pull/558">resolved vulnerable recursion</a>
   in a different place in libexpat code in February 2022 already.
   Thanks again!</li>
<li>That it would be a pile of work,
   not a good match to my unpaid voluntary role in Expat as an addition to my
   unrelated-to-Expat day job,
   and not without risk without a partner at detail level on the topic.
   My prior work on
   <a href="https://blog.hartwork.org/posts/cve-2013-0340-billion-laughs-fixed-in-expat-2-4-0/">fixing billion laughs for Expat 2.4.0</a>
   made me expect this to be similar, but bigger.</li>
</ol>
<p>And with that expectation, the issue started aging without a fix,
and in some sense, I felt paralyzed about the topic and kept procrastinating
about it for a long time.
Every now and the topic came up with my friend,
journalist and security researcher <a href="https://hboeck.de/">Hanno Böck</a>
whom I had shared the issue with.
He was arguing that even without a fix, the issue should be made public
at some point.</p>
<p>One reason why I was objecting to publication without a fix was that it was clear
that in lack of a cheap clean fix, vendors and distributions would start
applying quick hacks that would produce false positives
(i.e. rejecting well-formed benign XML misclassified as an attack),
leave half of the issue
unfixed, and leave the ecosystem with a potentially heterogeneous state
of downstream patches where — say — in openSUSE a file would be rejected but
in Debian it would parse fine — or the other way around: a great mess.</p>
<p>I eventually concluded that the vulnerability could not
keep sitting in my inbox unfixed for another year,
that it needed a fix before publication to not cause a mess, and
that I had to take action.</p>
<h2>Reaching out to companies for help</h2>
<p>In early 2024, I started considering ways of finding help more, and
<a href="https://github.com/libexpat/libexpat/commit/8548bc03fdb887c8720f01e95440f1406bd15ffa">added a call for help banner</a>
to the change log that was included with Expat 2.6.2.
I started drafting an e-mail that I would send out to companies known to
use libexpat in hardware.
I had started maintaining a (by no means complete)
<a href="https://libexpat.github.io/doc/users/#hardware">public list of companies using Expat in hardware</a>
that now came in handy.</p>
<p>On April 14th, 2024 I started finding looking for security
contacts for companies on <a href="https://libexpat.github.io/doc/users/#hardware">that list</a>.
For some, it was easy to find and for others, I gave up eventually;
for some, I am still not sure whether I got the right address
or whether they are ghosting me as part of an <a href="https://en.wikipedia.org/wiki/Ostrich_policy">ostrich policy</a>.
I wish more companies would <a href="https://securitytxt.org/">start serving <code>/.well-known/security.txt</code></a>;
finding vulnerability report contacts is still actual work in 2025 and should not be.</p>
<p>So then I mailed to circa 40 companies using a template, like this:</p>
<div><pre>Hello ${company},


this e-mail is about ${company} product IT security.
Are you the right contact for that?  If not please forward
it to the responsible contact within ${company} — thank you!

On the security matter:

It has come to my attention that ${company} products and
business rely on libexpat or the "Expat" XML parser library,
e.g. product ${product} is using libexpat according to
document [1].  I am contacting you as the maintainer of
libexpat and its most active contributor for the last 8
years, as can be seen at [2]; I am reaching out to you today
to raise awareness that:

- All but the latest release of libexpat (2.6.2) have
  security issues known to the public, so every product
  using older versions of libexpat can be attacked through
  vulnerable versions of libexpat.

- Both automated fuzzing [3] and reports from security
  researchers keep uncovering vulnerabilities in libexpat,
  so it needs a process of updating the copy of libexpat
  that you bundle and ship with your products, if not
  already present.

- My time on libexpat is unfunded and limited, and there is
  no one but me to constantly work on libexpat security and
  to also progress on bigger lower priority tasks in
  libexpat.

- There is a non-public complex-to-fix security issue in
  libexpat that I have not been able to fix alone in my
  spare time for months now, that some attackers may have
  managed to find themselves and be actively exploiting
  today.

I need partners in fixing that vulnerability.
Can ${company} be a partner in fixing that vulnerability,
so that your products using libexpat will be secure to use
in the future?

I am looking forward to your reply, best



Sebastian Pipping

Maintainer of libexpat


[1] ${product_open_source_copyright_pdf_url}
[2] https://github.com/libexpat/libexpat/graphs/contributors
[3] https://en.wikipedia.org/wiki/Fuzzing
</pre></div>

<h2>Replies are coming in</h2>
<p>The responses I got from companies were all over the map:</p>
<ul>
<li>
<p>My "favorite" reply was "We cannot understand what you want from us"
  when everyone else had understood me just fine.  Nice!</p>
</li>
<li>
<p>Though that competes with the reply "A patch will be released after the fall."
  when they had not received any details from me.  Okay!</p>
</li>
<li>
<p>There was arguing that the example product that I had mentioned was no
  longer receiving updates
  (rather than addressing their affected other products
  that are not end-of-life and continue to use libexpat).</p>
</li>
<li>
<p>I was asked to prove a concrete attack on the company's products
  (which would not scale, need access to the actual product, etc).</p>
</li>
<li>
<p>That they "do not have sufficient resources to assist you on
  this matter even if libexpat is employed in some of .....'s products"
  came back a few times.</p>
</li>
</ul>
<p>It was interesting and fun in some sense, and not fun in another.</p>
<h2>Next stop: confidentiality</h2>
<p>What came next was that I asked companies to sign a simple freeform
<a href="https://en.wikipedia.org/wiki/Non-disclosure_agreement">NDA</a> with me.
Companies were not prepared for that.  Why was I asking for an NDA and
<a href="https://en.wikipedia.org/wiki/Traffic_Light_Protocol"><code>TLP:RED</code></a>?
To (1) make sure that who got the details would need to collaborate
on a true fix and not just monkey-patch their own setups and (2)
to avoid the scenario of heterogeneous trouble fixes
that I mentioned before
that would have been likely in case of a leak before there was a true fix.</p>
<p>Some discussions failed at NDA stage already, while others survived
and continued to video calls with me explaining Jann's findings in detail.</p>
<p>It is worth noting that I knew going in that many vulnerability reward
programs exclude the whole class of denial of service and so
I tied sharing the expected impact to signing an NDA to reduce the chances
of everyone discarding it "Oh 'just' denial of service, we'll pass".</p>
<h2>The eventual team and security work</h2>
<p>Simplifying a bit, I found two main partner companies in this:
<a href="https://www.siemens.com/">Siemens</a> and
<em>a company that would not like to be named</em>, let's call them "Unnamed Company".
Siemens started development towards a candidate fix, and Unnamed Company
started evaluating options of what other companies they could pay to help for them,
which got <a href="https://www.linutronix.de/">Linutronix</a>
and also <a href="https://www.redhat.com/">Red Hat</a> involved.</p>
<p>Siemens took the builder role
while Linutronix, Red Hat and I provided quality assurance of various kinds.
While we did not work day and night, it is fair to say that we have been working
on the issue since May 2024 — <em>for about 10 months</em>.</p>
<h2>The three faces of the vulnerability</h2>
<p>It did indeed turn out that the vulnerability has multiple — three — faces:</p>
<h3>1. General entities in character data</h3>
<div><pre><span>&lt;!DOCTYPE doc [</span>
<span>  &lt;!ENTITY g0 ''&gt;</span>
<span>  </span><span>&lt;!ENTITY g1 '&amp;g0;'&gt;</span>
<span>  </span><span>&lt;!ENTITY g2 '&amp;g1;'&gt;</span>
]&gt;
<span>&lt;doc&gt;</span><span>&amp;g2;</span><span>&lt;/doc&gt;</span>
</pre></div>

<h3>2. General entities in attribute values</h3>
<div><pre><span>&lt;!DOCTYPE doc [</span>
<span>  &lt;!ENTITY g0 ''&gt;</span>
<span>  </span><span>&lt;!ENTITY g1 '&amp;g0;'&gt;</span>
<span>  </span><span>&lt;!ENTITY g2 '&amp;g1;'&gt;</span>
]&gt;
<span>&lt;doc</span><span> </span><span>key=</span><span>'&amp;g2;'</span><span>/&gt;</span>
</pre></div>

<h3>3. Parameter entities</h3>
<div><pre><span>&lt;!DOCTYPE doc [</span>
<span>  &lt;!ENTITY % p0 ''&gt;</span>
<span>  </span><span>&lt;!ENTITY % p1 '&amp;#37;p0;'&gt;</span>
<span>  </span><span>&lt;!ENTITY % p2 '&amp;#37;p1;'&gt;</span>
<span>  </span><span>&lt;!ENTITY % define_g0 "&lt;!ENTITY g0 '&amp;#37;p2;'&gt;</span>"&gt;
<span>  </span>%define_g0;
]&gt;
<span>&lt;doc/&gt;</span>
</pre></div>

<p>The third variant "Parameter entities" reuses ideas from my 2013 exploit for vulnerability
<a href="https://blog.hartwork.org/posts/cve-2021-3541-parameter-laughs-fixed-in-libxml2-2-9-11/">Parameter Laughs (CVE-2021-3541)</a>:
It used the same mechanism of delayed interpretation.</p>
<h2>Conclusions and gratitude</h2>
<p>It is no overstatement to say that without
<a href="https://github.com/berkayurun">Berkay Eren Ürün</a>
— the main author of the fix —
and his manager
<a href="https://www.linkedin.com/in/thomas-pr%C3%B6ll-5a8b8519b/">Dr. Thomas Pröll</a>
at Siemens there would be no fix today: a big and personal "thank you!" from me.</p>
<p>Thanks to Unnamed Company, to Linutronix, to Red Hat for your help making this plane fly!</p>
<p>Thanks to Jann Horn for his whitehat research and the demo patch that lead the path to a fix!</p>
<p>Thanks to everyone who contributed to this release of Expat!</p>
<p>And please tell your friends:</p>
<blockquote>
<p>Please leave recursion to math and keep it out of (in particular C) software:
it kills and will kill again.<br>
Kind regards from libexpat,
see <a href="https://github.com/libexpat/libexpat/pull/558">CVE-2022-25313</a>
and <a href="https://github.com/libexpat/libexpat/issues/893">CVE-2024-8176</a> for proof.</p>
</blockquote>
<p>For more details about this release, please
<a href="https://github.com/libexpat/libexpat/blob/R_2_7_0/expat/Changes">check out the change log</a>.</p>
<p>If <em>you</em> maintain Expat packaging
or
a bundled copy of Expat
or
a pinned version of Expat somewhere,
please update to 2.7.0.  Thank you!</p>
<p>Sebastian Pipping</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["Normal" engineers are the key to great teams (485 pts)]]></title>
            <link>https://spectrum.ieee.org/10x-engineer</link>
            <guid>43356995</guid>
            <pubDate>Thu, 13 Mar 2025 20:35:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/10x-engineer">https://spectrum.ieee.org/10x-engineer</a>, See on <a href="https://news.ycombinator.com/item?id=43356995">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="In Praise of “Normal” Engineers"><p><em>A version of this post <a href="https://refactoring.fm/p/in-praise-of-normal-engineers" target="_blank">originally appeared</a> in <a href="https://refactoring.fm/about" target="_blank">Refactoring</a>, a Substack offering advice for <a href="https://spectrum.ieee.org/tag/software-engineers">software engineers</a>. </em><br></p><p>Most of us have encountered a few software engineers who seem practically magician-like, a class apart from the rest of us in their ability to reason about complex mental models, leap to nonobvious yet elegant solutions, or emit waves of high-quality code at unreal velocity. </p><p>I have run into many of these incredible beings over the course of my career. I think their existence is what explains the curious durability of the notion of a “10x engineer,” someone who is 10 times as productive or skilled as their peers. The idea<span>—</span>which has <a href="https://knowyourmeme.com/memes/10x-engineer" target="_blank">become a meme</a><span><span>—is</span> based on </span><a href="https://dl.acm.org/doi/10.1145/362851.362858" target="_blank">flimsy, shoddy research</a><span>, and the claims people have made to defend it have often been risible (for example, 10x engineers have dark backgrounds, are rarely seen doing user-interface work, and are poor mentors and interviewers) or blatantly double down on stereotypes (“we look for young dudes in hoodies who remind us of Mark Zuckerberg”). But damn if it doesn’t resonate with experience. It just feels true.</span></p><p>I don’t have a problem with the idea that there are engineers who are 10 times as productive as other engineers. The problems I do have are twofold.</p><h2>Measuring productivity is fraught and imperfect</h2><p>First, how are you measuring productivity? I have a problem with the implication that there is One True Metric of productivity that you can standardize and sort people by. Consider the magnitude of skills and experiences at play: </p><ul><li>Are you working on <a href="https://spectrum.ieee.org/tag/microprocessors">microprocessors</a>, <a href="https://spectrum.ieee.org/tag/iot">IoT</a>, database internals, Web services, <a href="https://spectrum.ieee.org/tag/user-experience">user experience</a>, mobile apps<span>—</span>what? </li><li>Are you using <a href="https://spectrum.ieee.org/top-programming-languages-2024" target="_blank">Golang, Python, Cobol, or Lisp</a>? Which version, <a href="https://spectrum.ieee.org/tag/libraries">libraries</a>, and frameworks? What other software must you have mastered? </li><li>What adjacent skills, market segments, and product subject matter expertise are you drawing upon? Design, security, compliance, data visualization, marketing, finance? </li><li>What stage of development? What scale of usage? Are you writing for a <a href="https://spectrum.ieee.org/tag/mars">Mars</a> rover, or shrink-wrapped software you can never change? </li></ul><p><span>Also, people and their skills and abilities are not static. At one point, I was a pretty good database reliability engineer. Maybe I was even a 10x database engineer then, but certainly not now. I haven’t debugged a query plan in <em>years</em>.</span></p><p><span>“10x engineer” makes it sound like productivity is an immutable characteristic of a person. But someone who is a 10x engineer in a particular skill set is still going to have infinitely more areas where they are average (or below average). I know a lot of world-class engineers, but I’ve never met anyone who is 10 times better than everyone else across the board, in every situation.</span></p><h2>Engineers don’t own software, teams own software</h2><p><span>Second, and even more importantly: So what? Individual engineers don’t own <a href="https://spectrum.ieee.org/tag/software-engineering">software; engineering</a> teams own software. It doesn’t matter how fast an individual engineer can write software. What matters is how fast the team can collectively write, test, review, ship, maintain, refactor, extend, architect, and revise the software that they own. </span></p><p><span>Everyone uses the same software delivery pipeline. If it takes the slowest engineer at your company five hours to ship a single line of code, it’s going to take the fastest engineer at your company five hours to ship a single line of code. The time spent writing code is typically dwarfed by the time spent on <a href="https://spectrum.ieee.org/twitters-tips-for-making-software-engineers-more-efficient" target="_blank">every other part</a> of the <a href="https://spectrum.ieee.org/tag/software-development">software development</a> lifecycle. </span></p><p><span>If you have services or software components that are owned by a single engineer, that person is a single point of failure. </span></p><p><span>I’m not saying this should never happen. It’s quite normal at <a href="https://spectrum.ieee.org/tag/startups">startups</a> to have individuals owning software, because the biggest existential risk that you face is not moving fast enough and going out of business. But as you start to grow as a company, ownership needs to get handed over to a team. Individual engineers get sick, go on vacation, and leave the company, and the business has to be resilient to that. </span></p><p><span>When a team owns the software, then the key job of any engineering leader is to craft a high-performing engineering team. If you must 10x something, build 10x engineering teams.</span></p><h2>The best engineering organizations are the ones where normal engineers can do great work</h2><p><span>When people talk about world-class engineering organizations, they often have in mind teams that are top-heavy with staff and principal engineers, or that recruit heavily from the ranks of former <a href="https://spectrum.ieee.org/tag/big-tech">Big Tech</a> employees and top universities. But I would argue that a truly great engineering org is one where you don’t have to be one of the “best” or most pedigreed engineers to have a lot of impact on the business. I think it’s actually the other way around. A truly great engineering organization is one where perfectly normal, workaday software engineers, with decent skills and an ordinary amount of expertise, can consistently move fast, ship code, respond to users, understand the systems they’ve built, and move the business forward a little bit more, day by day, week by week.</span></p><p><span>Anyone can build an org where the most experienced, brilliant engineers in the world can <a href="https://spectrum.ieee.org/product-management-career" target="_blank">create products</a> and make progress. That’s not hard. And putting all the spotlight on individual ability has a way of letting your leaders off the hook from doing their jobs. It is a huge competitive advantage if you can build systems where less experienced engineers can convert their effort and energy into product and business momentum. And the only meaningful measure of productivity is whether or not you are moving the business materially forward. </span></p><p><span>A truly great engineering org also happens to be one that mints world-class software engineers. But I’m getting ahead of myself here. </span></p><h2>Let’s talk about “normal” engineers</h2><p><span>A lot of technical people got really attached to our identities as smart kids. The software industry tends to reflect and reinforce this preoccupation at every turn, as seen in Netflix’s claim that “we look for the top 10 percent of global talent” or Coinbase’s desire to “hire the top 0.1 percent.” I would like to challenge us to set that baggage to the side and think about ourselves as</span><span></span><em>normal</em><span> people.</span></p><p><span>It can be humbling to think of yourself as a normal person. But most of us are, and there is <em>nothing wrong with that</em>. Even those of us who are certified geniuses on certain criteria are likely quite normal in other ways—kinesthetic, emotional, spatial, musical, linguistic, and so on. </span></p><p><span>Software engineering both selects for and develops certain types of intelligence, particularly around abstract reasoning, but <em>nobody</em> is born a great software engineer. Great engineers are made, not born. </span></p><h2>Build sociotechnical systems with “normal people” in mind </h2><p><span>When it comes to hiring talent and building teams, yes, absolutely, we should focus on identifying the ways people are exceptional. But when it comes to building sociotechnical systems for software delivery, we should focus on all the ways people are <em>normal</em>. </span></p><p><span>Normal people have <a href="https://spectrum.ieee.org/engineering-bias-out-of-ai" target="_blank">cognitive biases</a>—confirmation bias, recency bias, hindsight bias. We work hard, we care, and we do our best; but we also forget things, get impatient, and zone out. Our eyes are inexorably drawn to the color red (unless we are colorblind). We develop habits and resist changing them. When we see the same text block repeatedly, we stop reading it.</span></p><p><span>We are embodied beings who can get overwhelmed and fatigued. If an alert wakes us up at 3 a.m., we are much more likely to make mistakes while responding to that alert than if we tried to do the same thing at 3 p.m. Our emotional state can affect the quality of our work. </span></p><p><span>When your systems are designed to be used by normal engineers, all that excess brilliance they have can get poured into the product itself, instead of wasting it on navigating the system.</span></p><h2>Great engineering orgs mint world-class engineers </h2><p><span>A great engineering organization is one where you don’t have to be one of the best engineers in the world to have a lot of impact. But—rather ironically—great engineering orgs mint world-class engineers like nobody’s business. </span></p><p><span>The best engineering orgs are not the ones with the smartest, most experienced people in the world. They’re the ones where normal software engineers can consistently make progress, deliver value to users, and move the business forward. </span><span>Places where engineers can </span><span>have a large impact are a magnet for top performers. Nothing makes engineers happier than building things, solving problems, and making progress.</span></p><p><span>If you’re lucky enough to have world-class engineers in your organization, good for you! Your role as a leader is to leverage their brilliance for the good of your customers and your other engineers, without coming to <em>depend</em> on their brilliance. After all, these people don’t belong to you. They may walk out the door at any moment, and that has to be okay. </span></p><p><span>These people can be phenomenal assets, assuming they can be team players and keep their egos in check. That’s probably why so many tech companies seem to obsess over identifying and hiring them, especially in <a href="https://spectrum.ieee.org/tag/silicon-valley">Silicon Valley</a>.</span></p><p><span>But companies attach too much importance to finding these people after they’ve already been minted, which ends up reinforcing and replicating all the prejudices and inequities of the world at large. Talent may be evenly distributed across populations, but opportunity is not. </span></p><h2>Don’t hire the “best” people. Hire the right people</h2><p><span>We place too much emphasis on individual agency and characteristics, and not enough on the systems that shape us and inform our behaviors. </span></p><p><span>I believe a whole slew of issues (candidates self-selecting out of the interview process, diversity of applicants, and more) would be improved simply by shifting the focus of hiring away from this inordinate emphasis on hiring the <em>best </em>people and realigning around the more reasonable and accurate <em>right </em>people. </span></p><p><span>It’s a competitive advantage to build an environment where people can be hired for their unique strengths, not their lack of weaknesses; where the emphasis is on composing teams; where inclusivity is a given both for ethical reasons and because it raises the bar for performance for everyone. Inclusive culture is what meritocracy depends on. </span></p><p><span>This is the kind of place that engineering talent is drawn to like a moth to a flame. It feels good to move the business forward, sharpen your skills, and improve your craft. It’s the kind of place that people go when they want to become world-class engineers. And it tends to be the kind of place where world-class engineers want to stick around and train the next generation.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Lost Art of Logarithms (483 pts)]]></title>
            <link>https://www.lostartoflogarithms.com/</link>
            <guid>43356314</guid>
            <pubDate>Thu, 13 Mar 2025 19:05:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lostartoflogarithms.com/">https://www.lostartoflogarithms.com/</a>, See on <a href="https://news.ycombinator.com/item?id=43356314">Hacker News</a></p>
<div id="readability-page-1" class="page">
        <header>
            

            <img src="https://www.lostartoflogarithms.com/images/AuthorWithSlideRule.jpg" alt="Charles Petzold holding massive slide rule" title="Charles Petzold holding massive slide rule" width="674" height="286">

            <h2>
                An online book-in-progress by<br>
                <span>Charles Petzold</span><br>
                wherein is explored the utility, history,<br>
                and ubiquity of that marvelous invention,<br>
                <span>logarithms</span><br>
                including what the hell they are;<br>
                <!-- span class="fancy">&</span><br / -->
                with some demonstrations of their<br>
                primary historical application in<br>
                plane and spherical trigonometry.
            </h2>
        </header>

        <div>
            <ul>
                <li>This is a work in progress and nowhere close to completion</li>
            </ul>

            <p>
                Some paragraphs are coherent; others are not. 
                Sometimes paragraphs are only a phrase or a note to myself.
                Nothing has been professionally edited. 
            </p>

            <ul>
                <li>It is best viewed on a desktop or laptop computer</li>
            </ul>

            <p>
                I've been developing the pages in Edge using Visual Studio Code 
                running under Windows 11 on a Microsoft Surface Pro 9.
                I've also been testing the pages in Chrome on that machine, and   
                in Safari on a Mac Mini running Sequoia, and 
                in Chrome (version 126, it says) on an Asus Chromebook. 
            </p>
            <p>
                However, my iPad Mini running iOS 12.5.7 has several 
                problems with these webpages, 
                and the pages often become quite awkward on phones. 
            </p>
        </div>

        <!-- table style="margin: 0 auto; border: 2px solid blue; background: #D0D0FF;">
            <tbody>
                <tr>
                    <td style="vertical-align:top;">
                        <ul>
                            <li style="list-style-type: none">Featuring:</li>
                        </ul>
                    </td>
                    <td style="vertical-align:top; padding-right: 32px">
                        <ul>
                            <li><b>No</b> paywall!</li>
                            <li><b>No</b> cookies!</li>
                            <li><b>No</b> popups!</li>
                            <li><b>No</b> ads! (except for the author’s own books)</li>
                            <li>Guaranteed 100% human-intelligence generated!</li>
                        </ul>
                    </td>
                </tr>
            </tbody>
        </table -->

        <p>
            <h2>
                Preface. <a href="https://www.lostartoflogarithms.com/preface">What I'm Trying to do Here</a>
            </h2>

            <h2>
                Part I. The Book of Vlacq
            </h2>

            <h2>
                Chapter 1. <a href="https://www.lostartoflogarithms.com/chapter01">Logarithms? Are Those Like Algorithms?</a>
            </h2>

            <h2>
                Chapter 2. <a href="https://www.lostartoflogarithms.com/chapter02">The Magic Demystified</a>
            </h2>

            <h2>
                Part II. In the Service of Trigonometry
            </h2>

            <h2>
                Chapter 3. <a href="https://www.lostartoflogarithms.com/chapter03">The Trigonometric Connection</a>
            </h2>

            <h2>
                Chapter 4. <a href="https://www.lostartoflogarithms.com/chapter04">Beyond the Straight Right Triangle</a>
            </h2>

            <h2>
                Chapter 5. <a href="https://www.lostartoflogarithms.com/chapter05">The Ubiquitous Sinusoid</a>
            </h2>

            <h2>
                Chapter 6. <a href="https://www.lostartoflogarithms.com/chapter06">Mapping Out the Earth</a>
            </h2>

            <h2>
                Chapter 7 <a href="https://www.lostartoflogarithms.com/chapter07">Reaching for the Stars</a>
            </h2>

            <h2>
                Chapter 8. Calculating Manhattanhenge
            </h2>

            <h2>
                Part III. Mathematicians at Work
            </h2>

            <h2>
                Chapter 9. <a href="https://www.lostartoflogarithms.com/chapter09">Napier’s Life and Reformatory Times</a>
            </h2>

            <h2>
                Chapter 10. Countdown to the Apocalypse
            </h2>

            <h2>
                Chapter 11. Conceiving the Logarithm
            </h2>

            <h2>
                Chapter 12. Napier’s Handoff to Briggs
            </h2>

            <h2>
                Chapter 13. e, Naturally
            </h2>

            <h2>
                Chapter 14. <a href="https://www.lostartoflogarithms.com/chapter14">Logarithms at Your Fingertips</a>
            </h2>

            <h2>
                Chapter 15. <a href="https://www.lostartoflogarithms.com/chapter15">Peter Mark Roget and the Log-Log Scale</a>
            </h2>

            <h2>
                Part IV. Logarithms Everywhere
            </h2>

            <h2>
                Chapter 16. Log and Log-Log Phenomena
            </h2>

            <h2>
                Chapter 17. Time and Space
            </h2>

            <h2>
                Chapter 18. Sound and Music
            </h2>

            <h2>
                · · ·
            </h2>
            
            <h2>
                · · ·
            </h2>

            <h2>
                Back Matter
            </h2>

            <h2>
                <a href="https://www.lostartoflogarithms.com/author">About the Author</a>
            </h2>
        </p>

        <hr>
        
        
    

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Bubbles, a vanilla JavaScript web game (186 pts)]]></title>
            <link>https://ehmorris.com/bubbles/</link>
            <guid>43355658</guid>
            <pubDate>Thu, 13 Mar 2025 17:48:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ehmorris.com/bubbles/">https://ehmorris.com/bubbles/</a>, See on <a href="https://news.ycombinator.com/item?id=43355658">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[History of Maths for Beginners (141 pts)]]></title>
            <link>https://thonyc.wordpress.com/2025/03/13/history-of-maths-for-beginners/</link>
            <guid>43355542</guid>
            <pubDate>Thu, 13 Mar 2025 17:35:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thonyc.wordpress.com/2025/03/13/history-of-maths-for-beginners/">https://thonyc.wordpress.com/2025/03/13/history-of-maths-for-beginners/</a>, See on <a href="https://news.ycombinator.com/item?id=43355542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>In the comments on <a href="https://thonyc.wordpress.com/2025/02/12/books-on-the-history-of-mathematics-some-thoughts/">my recent post on books on the history of maths </a>Fernando Q. Gouvêa jumped in to draw attention to the book&nbsp;<em>Math Through the Ages</em>:&nbsp;<em>A Gentle History for Teachers and Others</em>, which he coauthored with William P. Berlinghoff. I had not come across this book before, as I noted in my post I gave up reading general histories of maths long ago, so, I went looking and came across the following glowing recommendation:</p>



<blockquote>
<p>“<em>Math&nbsp;Through&nbsp;the&nbsp;Ages</em>&nbsp;is a treasure, one of the best history of&nbsp;math&nbsp;books at its level ever written. Somehow, it manages to stay true to a surprisingly sophisticated story, while respecting the needs of its audience. Its overview of the subject captures most of what one needs to know, and the 30 sketches are small gems of exposition that stimulate further exploration.” — Glen Van Brummelen, Quest University&nbsp;</p>
</blockquote>



<p>Glen Van Brummelen is an excellent historian of maths and regular readers will already know that I’m a very big fan of his books on the history of trigonometry, <a href="https://thonyc.wordpress.com/2021/12/08/ohms-or-everything-you-wanted-to-know-about-the-history-of-trigonometry-and-didnt-know-who-to-ask/">which I reviewed here.</a> Intrigued I decided to acquire a copy and see if it lives up to Glen’s description. The book that I acquired is the paperback Dover reprint from 2019 of the second edition from Oxton House Publishers, Farmington, Maine, it has a recommended price in the USA of $18, which is certainly affordable for its intended audience, school, college and university students. The cover blurb says, “Designed for students just beginning their study of the discipline…”</p>



<figure><a href="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11250148_04.jpg"><img data-attachment-id="13656" data-permalink="https://thonyc.wordpress.com/2025/03/13/history-of-maths-for-beginners/kepler-conics20250313_11250148_04/" data-orig-file="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11250148_04.jpg" data-orig-size="2411,3268" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="Kepler Conics20250313_11250148_04" data-image-description="" data-image-caption="" data-medium-file="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11250148_04.jpg?w=221" data-large-file="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11250148_04.jpg?w=500" width="755" height="1023" src="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11250148_04.jpg?w=755" alt="" srcset="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11250148_04.jpg?w=755 755w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11250148_04.jpg?w=1510 1510w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11250148_04.jpg?w=111 111w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11250148_04.jpg?w=221 221w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11250148_04.jpg?w=768 768w" sizes="(max-width: 755px) 100vw, 755px"></a></figure>



<p>The book distinguishes itself by not being a long continuous narrative covering the chronological progress of the historical development of mathematics, the usual format of one volume histories. Instead, it opens with a comparatively brief such survey, just fifty-eight pages entitled,&nbsp;<em>The History of Mathematics in a Large Nutshell</em>, before presenting the thirty sketches mentioned by Van Brummelen in his recommendation. These are brief expositions of single topics from the history of maths, each one conceived and presented as a complete, independent pedagogical unit. The sketches however contain references to other units where a term or concept used in the current unit is explained more fully. Normally when reviewing books, I usually leave the discussion of the bibliography till the end of my review but in this case the bibliography is an integral part of the unit presentation. The bibliography contains one hundred and eighty numbered&nbsp;&nbsp;titles listed alphabetically by the authors’ names. At the end of each sketch there is a list of numbers of the titles where the reader can find more information on the topic of the sketch. Within the sketches themselves this recommendation system is also applied to single themes. The interaction between sketches and bibliography is a very central aspect of the books pedagogical structure.&nbsp;&nbsp;</p>



<p>Given its very obvious limitations, my own far from complete survey of the history of mathematics now occupies more than two metres of bookshelf space and that is not counting the numerous books on the histories of astronomy, physics, navigation, cartography, technology and so forth, which I own, that contain elements on the history of mathematics, the&nbsp;<em>Large Nutshell</em>&nbsp;is actually reasonably good. It sketches the development of mathematics in Mesopotamia, Egypt, China, Ancient Greece, India, and the Islamic Empires, before leaping to Europe in the fifteenth and sixteenth centuries and from there is fairly large steps down to the present day. Naturally, a lot gets left out that other authors might have considered important but it still manages to give a general feeling of the depth and breadth of the history of mathematics. The real work begins with the&nbsp;<em>Sketches</em>.</p>



<p>Sketch No.1 is naturally&nbsp;<em>Keeping Count</em>:&nbsp;<em>Writing Whole Numbers</em>. Our authors deliver competent but brief accounts of the Egyptian, Mesopotamian, Mayan, Roman and Hindu-Arabic number systems but don’t consider the Greek system worth mentioning. They of course emphasise the difficulty of calculating with Roman numerals, as everybody does, but at least mention the calculation was actually done with a counting board or abacus, without seeming to realise that abacus is the Greek name for a counting board, an error that leads them to a stupid statement in another sketch. They also don’t mention that everybody, Mesopotamians, Greeks, Egyptians etc used counting boards to do calculations. They of course mention this in order to emphasise the advantages of the Hindu-Arabic numerals for doing calculations on paper, which leads to another error later.</p>



<p>Sketch No.2&nbsp;<em>Reading and Writing Arithmetic</em>:&nbsp;<em>The Basic Symbols</em>&nbsp;now makes a big leap to the introduction of symbolic arithmetic in the Renaissance to replace the previous rhetorical arithmetic. This precedes chronologically through the various innovations, including, unfortunately, the myth that Robert Recorde was the first to introduce the equal’s sign. Is OK, but they admit that their brief sketch skips over many, many symbols that were used from time to time.&nbsp;</p>



<p>Almost predictably, Sketch No.3 is&nbsp;<em>Nothing Becomes a Number</em>:&nbsp;<em>The Story of Zero</em>. We start with the Babylonians, who had a place value number system, and after a long time without one, introduced a place holder zero. We then spring to India and from them straight to the Arabs and the logistic origins of the word zero before being served up the first major error in the book. Our authors tell us:</p>



<blockquote>
<p>By the 9<sup>th</sup>&nbsp;century A.D., the Indians had made a conceptual leap that ranks as one of the most important mathematical events of all time. They had begun to recognise&nbsp;<em>sunya</em>, the absence of quantity, as a quantity in its own right! That is, they had begun to treat zero as a number. For instance, the mathematician Mahāvīra wrote that a number multiplied by zero results&nbsp;&nbsp;in zero, and that zero subtracted from a number leaves the number unchanged. He also claimed that a number divided by zero remain unchanged. A couple of centuries later. Bhāskara declared, a number divided by zero to be an infinite quantity.</p>
</blockquote>



<p>This is quite frankly bizarre! Already in the 628 CE, Brahmagupta (c. 598–c. 668) had in Chater 12 of his&nbsp;<em>Brāhma-sphu</em><em>ṭ</em><em>a-siddhānta</em>&nbsp;given the rules for addition, subtraction, multiplication, and division with positive and negative numbers and zero in the same way as they can be found in any elementary arithmetic textbook today, with the exception that he defined division by zero. His work was well known in India. In fact,&nbsp;Bhāskara II, mentioned by our authors, based his account on that of Brahmagupta. His work was also translated into Arabic in the eight century and provided the basis for al-Khwārimī’s account, which has not survived in Arabic but was translated into Latin in the twelfth century as&nbsp;<em>Dixit Algorizmi</em>, which was the first introduction of the Hindu-Arabic number system in Europe, as is noted by our authors. Our authors then over emphasise the reluctance of some European mathematicians to accept zero as a number, whilst introducing Thomas Harriot’s method for solving polynomials and zero as a defining property of rings and fields.</p>



<p>Sketch No.4&nbsp;<em>Broken Numbers</em>:<em>&nbsp;Writing Fractions</em>&nbsp;is a quite good account of their history considering its brevity.</p>



<p>Sketch No.5&nbsp;<em>Less than Nothing</em>:&nbsp;<em>Negative Numbers</em>&nbsp;is also a reasonably good account of the very problematic history of accepting the existence of negative numbers. Somewhat bizarrely, having totally ignored him on the topic of zero, our authors now state that:</p>



<blockquote>
<p>A prominent Indian mathematician, Brahmagupta, recognised and worked with negative quantities to some extent as early as the 7<sup>th</sup>&nbsp;century. He treated positive numbers as possessions and negative numbers as debts, and also stated rules for adding, subtracting, multiplying, and dividing with negative numbers.&nbsp;</p>
</blockquote>



<p>It’s the same section of his book where he deals with zero as a number!</p>



<p>Sketch No. 6&nbsp;<em>By Ten and Tenths</em>:&nbsp;<em>Metric Measurement</em>&nbsp;is a good brief account of the French introduction of the metric system and its subsequent modernisation. I would have appreciated a brief nod for the English, polymath John Wilkins (1614–1672), who advocated for a metric system already in the seventeenth century.&nbsp;</p>



<p>Sketch No. 7&nbsp;<em>Measuring the Circle</em>:&nbsp;<em>The Story of&nbsp;</em><em>π</em>&nbsp;a simple brief account. An interesting addition is a table showing the size of the error in calculating the circumference of a circular lake with a diameter of one kilometre using different historical values for&nbsp;π.</p>



<p>Sketch No. 8&nbsp;<em>The Cossic Art</em>:&nbsp;<em>Writing Algebra with Symbols</em>&nbsp;is a competent sketch of the gradual but zig zag transition from rhetorical to symbolic algebra.&nbsp;</p>



<p>Sketch No. 9&nbsp;<em>Linear Thinking</em>:&nbsp;<em>Solving First Degree Equations</em>&nbsp;starts with the Egyptians mentions the Chinese but completely ignores the Babylonians. It then wanders off into an account of the false proposition method of solution.</p>



<p>Sketch No. 10&nbsp;<em>A Square and Things</em>:&nbsp;<em>Quadratic Equations</em>&nbsp;after one dimension we move onto two dimensions. This is devoted to al- Khwārimī’s discussion of methods of solving quadratic equations completely ignoring the fact that the Babylonians had the general solution of the quadratic equation a thousand years earlier, albeit in a different form to the one we teach schoolchildren and, of course ignoring negative solutions. Every worse ignoring&nbsp;&nbsp;the fact that Brahmagupta had the general solution in the form we use today including negative solutions!</p>



<p>Sketch No. 11&nbsp;<em>Intrigue in Renaissance Italy</em>:&nbsp;<em>Solving Cubic Equations</em>&nbsp;and now onto three dimensions. We start with the Ancient Geek problem of trisecting an angle and then surprisingly, considering that this is supposed to be “for students just beginning their study of the subject,” we suddenly get a piece of fairly advanced trigonometry blasted into the text:</p>



<p>cos(3𝛂) = 4 cos<sup>3</sup>(𝛂) – 3 cos(𝛂)</p>



<p>to point out that the solution can be found with a cubic equation!</p>



<p>We move on to Omar Khayyám and his solution of cubic equations using conic sections. We then get a mangled version of the Antonio Fiore/Tartaglia challenge. Out authors claim that Tartaglia bragged that he could solve cubic equations and so Fiore, who had learnt the solution from his teacher&nbsp;Scipione del Ferro, challenged him to a mathematical contest. In fact, Tartaglia didn’t know how to solve them when challenged by Fiore and realising he was onto a hiding to nothing sat down and discovered how to solve them. End result victory for Tartaglia. They get the rest of the story right. How Cardano persuaded Tartaglia to reveal his solution after promising not to publish it before Tartaglia did. Then discovering that Scipione del Ferro had discovered the solution before Tartaglia, and having extended Tartaglia’s solution, went on and published anyway, although giving full credit to Tartaglia for his work. You can read the whole story here. Our authors, however, contradict themselves. In their&nbsp;<em>Large Nutshell</em>&nbsp;they gave a brief version of the story and wrote:</p>



<blockquote>
<p>Once he knew Tartaglia’s method for solving some cubic equations. Cardano was able to generalise it to a way of solving any cubic equation. Felling he had made a contribution of his own, Cardano decided he was no longer bound by his promise of secrecy.</p>
</blockquote>



<p>Now in Sketch 11 they write:</p>



<blockquote>
<p>At this point, Cardano knew that he had made a real contribution to mathematics. But how could he publish it without breaking his promise? He found a way. He discovered that del Ferro had found the solution of a crucial case before Tartaglia had. Since he had the not promised to keep&nbsp;<em>del Ferro’s</em>&nbsp;solution secret, he felt he could publish it, even though it was identical to the one he had learnt from Tartaglia.</p>
</blockquote>



<p>We then get Cardano’s discovery of conjugate pairs of complex numbers sometimes in the solutions of cubic equations and a somewhat confused explanation of his reaction to them. Our authors tell us that Cardano wrote to Tartaglia and asked him about the problem and Tartaglia simply suggest that Cardano had not understood how to solve such problems. This is true, quoting MacTutor:</p>



<blockquote>
<p>One of the first problems that Cardan hit was that the formula sometimes involved square roots of negative numbers even though the answer was a ‘proper’ number. On&nbsp;4&nbsp;August&nbsp;1539&nbsp;Cardan wrote to Tartaglia:-</p>



<p><em>I have sent to enquire after the solution to various problems for which you have given me no answer, one of which concerns the cube equal to an unknown plus a number. I have certainly grasped this rule, but when the cube of one-third of the coefficient of the unknown is greater in value than the square of one-half of the number, then, it appears, I cannot make it fit into the equation.</em></p>



<p>Indeed, Cardan gives precisely the conditions here for the formula to involve square roots of negative numbers. Tartaglia, by this time, greatly regretted telling Cardan the method and tried to confuse him with his reply&nbsp;(although in fact Tartaglia, like Cardan, would not have understood the complex numbers now entering into mathematics):-</p>



<p><em>… and thus, I say in reply that you have not mastered the true way of solving problems of this kind, and indeed I would say that your methods are totally false.</em></p>
</blockquote>



<p>Our authors simply move on to state, It fell to Rafael Bombelli to resolve the issue.”&nbsp;</p>



<p>This is selling Cardano short, firstly he was well aware that if one multiplies complex conjugate pairs together the imaginary term disappears as he demonstrated in Ars Magna, he simply thought that it didn’t make sense, mathematically.&nbsp;</p>



<blockquote>
<p><em>Dismissing mental tortures, and multiplying&nbsp;</em>5<em>&nbsp;</em><em>+ √-</em>15<em>&nbsp;</em><em>by&nbsp;</em>5<em>&nbsp;</em><em>– √-</em>15<em>, we obtain&nbsp;</em>25<em>&nbsp;</em><em>–&nbsp;</em>(<em>–</em>15)<em>. Therefore the product is&nbsp;</em>40<em>. …. and thus far does arithmetical subtlety go, of which this, the extreme, is, as I have said, so subtle that it is useless.</em>&nbsp;(MacTutor)</p>
</blockquote>



<p>Secondly, Bombelli used Cardano’s work as the starting point for his own work.&nbsp;</p>



<p>Sketch No. 12&nbsp;<em>A Cheerful Fact</em>:&nbsp;<em>The Pythagorean Theorem</em>&nbsp;This is a reasonable overview of some of the history of what is probably the most well-known of all mathematical theorems. The first half of the title is a Gilbert and Sullivan quote!</p>



<p>Sketch No. 13&nbsp;<em>A Marvellous Proof</em>:&nbsp;<em>Fermat’s Last Theorem</em>&nbsp;Once again a reasonably synopsis of the history of another possible candidate for the most well-known of all mathematical theorems.</p>



<p>Sketch No. 14&nbsp;<em>On Beauty Bare</em>:&nbsp;<em>Euclid’s Plane Geometry</em>&nbsp;Another reasonably synopsis of the world’s most famous and biggest selling maths textbook.</p>



<p>Sketch No. 15&nbsp;<em>In Perfect Shape</em>:&nbsp;<em>The Platonic Solids</em>&nbsp;&nbsp;A brief, compact and largely accurate introduction to the history of the Platonic regular solids and the Archimedean semi-regular solids. I would have wished for more detail on the Renaissance rediscovery of the Archimedean solids, which was actually carried out by artists rather than mathematicians, a fact that our authors seem not to be aware of, as part of the explorations into the then new linear perspective.&nbsp;</p>



<p>Sketch No. 16 Shapes by the&nbsp;<em>Numbers</em>:&nbsp;<em>Coordinate Geometry</em>&nbsp;Another good synopsis covering both Fermat and Descartes and includes the important fact that the Cartesian system was actually popularised by the expanded Latin edition of&nbsp;<em>La Géométrie</em>&nbsp;put together by Frans van Schooten jr. Although our authors miss the junior. Having acknowledge his contribution they then miss one of his biggest contributions. Our authors write:</p>



<blockquote>
<p>The two-axis rectangular coordinate system that we commonly attribute to Descarte seems instead to have evolved gradually during the century and a half after he published&nbsp;<em>La Géométrie</em>.</p>
</blockquote>



<p>Rectangular Cartesian coordinates were introduced by Frans van Schooten jr. in his Latin edition.</p>



<p>Sketch No. 17&nbsp;<em>Impossible, Imaginary, Useful</em>:&nbsp;<em>Complex Numbers</em>&nbsp;Having, in their discussion of cubic equations, failed to acknowledge the fact that Cardano actually showed how to eliminate conjugate pairs of complex numbers in his&nbsp;<em>Ars Magna</em>, even if doing so offended his feelings for mathematics, our authors now bring the relevant quote to this effect. Having opened with Cardano we then move onto Bombelli’s acceptance and Descartes’ rejection before our authors attribute an implicit anticipation of Euler’s Formula in the work of Abraham De Moivre, whilst ignoring the explicit formulation in the work of Roger Cotes. Dealing comparatively extensively with Euler our authors then move onto Argand and Gauss and the geometrical representation of complex numbers, whilst completely ignoring Casper Wessel.&nbsp;&nbsp;</p>



<p>Sketch No. 18&nbsp;<em>Half is Better</em>:&nbsp;<em>Sine and Cosine</em>&nbsp;We now get introduced to the history of trigonometry. Once again a reasonable short presentation of the main points of the history down to the seventeenth century, with an all too brief comment about the development of the trigonometrical ratios as functions.</p>



<p>Sketch No. 19&nbsp;<em>Strange New Worlds</em>:&nbsp;<em>The Non-Euclidian Geometries</em>&nbsp;Once again nothing here to criticise.</p>



<p>Sketch No. 20&nbsp;<em>In the Eye of the Beholder</em>:&nbsp;<em>Projective Geometry</em>&nbsp;Starts with some brief comments about the discovery of linear perspective during the Renaissance, then moves on to the development of projective geometry out of that and ends with Pascal’s Theorem. Once again no complaints on my part.</p>



<p>Sketch No. 21&nbsp;<em>What’s in a Game?</em>:&nbsp;<em>The Start of Probability Theory</em>&nbsp;Having had the teenage Pascal on conics we now have the mature Pascal on divvying up the stakes in an interrupted game of chance. This is followed by&nbsp;&nbsp;standard story of the evolution of probability theory.&nbsp;</p>



<p>Sketch No. 22&nbsp;<em>Making Sense of Data</em>:&nbsp;<em>Statistics Becomes a Science</em>&nbsp;Once again presents a standard account without any significant errors.&nbsp;</p>



<p>Sketch No. 23&nbsp;<em>Machines that Think?</em>:&nbsp;<em>Electronic Computers</em>&nbsp;Having presented twenty-two sketches with only occasional historical error, our authors now come completely of the rails: To start:&nbsp;</p>



<blockquote>
<p>Some would say that the story begins 5000 years ago with the abacus, a calculating device of beads and rods that is still used today.</p>
</blockquote>



<p>The Greek term abacus refers to a counting board. The calculating device of beads and rods, which is also referred to as an abacus, was developed in Asia and didn’t become known in Europe until the end of the seventeenth beginning of the eighteenth centuries, via Russia, well after the counting board had ceased to be used in Europe.</p>



<p>We get no mention of the astrolabe, which is universally described as an analogue computer,</p>



<p>We then get Napier’s Bones and Oughtred’s slide rule as calculating devices implying that they are somehow related. They aren’t the slide rule is based on logarithms and although Napier invented logarithms his Bones aren’t.</p>



<p>Next up is the&nbsp;<em>Pascaline</em>&nbsp;with, quotes correctly the information that they were difficult to manufacture and so were a flop, although they don’t put it that bluntly. No mention of Wilhelm Schickard, whose calculator was earlier that the&nbsp;<em>Pascaline</em>. Of course, if we have Pascal’s calculator then we must have Leibniz’s, and we get the following hammer statement:</p>



<blockquote>
<p>Leibniz’s machine, the&nbsp;<em>Stepped Reckoner</em>, represented a major theoretical advance over the Pascaline in that&nbsp;<strong>its calculations were done in binary (base-two) arithmetic&nbsp;</strong>(my emphasis), the basis of all modern computer architecture.</p>
</blockquote>



<p>When I read that I had a genuine what the fuck moment. There are three possibilities, either our authors are using a truly crappy source on the history of reckoning machines, or they are simply making shit up, or they created a truly bad syllogistic&nbsp;&nbsp;argument.</p>



<p>1) Leibniz was one of the first European mathematicians to develop binary arithmetic</p>



<p>2) Leibniz created a reckoning machine</p>



<p>Therefore: Leibniz’s reckoning machine must have used binary arithmetic</p>



<p>Of course, the conclusion does not follow from the premises and Leibniz’s&nbsp;<em>Stepped Reckoner</em>&nbsp;used decimal not binary numbers.&nbsp;</p>



<p>A fourth possibility is a truly cataclysmic typo but our authors repeat the claim at the beginning of the next sketch of that is definitively not the case.</p>



<p>We get a brief respite with the description of a successful stepped reckoner in the nineteenth century, then we move onto Charles Babbage and a total train wreck. Our authors tell us:</p>



<blockquote>
<p>Early in the 19<sup>th</sup>&nbsp;century, Cambridge mathematics professor Charles Babbage began work on a machine for generating accurate logarithmic and astronomical tables.</p>



<p>[…]</p>



<p>By 1822, Babbage was literally cranking out tables with six-figure accuracy on a small machine he called a&nbsp;<em>Difference Engine</em>.</p>
</blockquote>



<p>I think our authors live in an alternative universe, only this could explain how Babbage was “cranking out tables with six-figure accuracy” on a machine that was never built! They appear to be confusing the small working model he produced to demonstrate the principles on which the engine was to function with the full engine which was never constructed. They even include a picture of that small working model labelled, Babbage’s Difference Engine.</p>



<figure><a href="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11372636_05.jpg"><img data-attachment-id="13659" data-permalink="https://thonyc.wordpress.com/2025/03/13/history-of-maths-for-beginners/kepler-conics20250313_11372636_05/" data-orig-file="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11372636_05.jpg" data-orig-size="879,1362" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Kepler Conics20250313_11372636_05" data-image-description="" data-image-caption="" data-medium-file="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11372636_05.jpg?w=194" data-large-file="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11372636_05.jpg?w=500" width="661" height="1024" src="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11372636_05.jpg?w=661" alt="" srcset="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11372636_05.jpg?w=661 661w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11372636_05.jpg?w=97 97w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11372636_05.jpg?w=194 194w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11372636_05.jpg?w=768 768w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250313_11372636_05.jpg 879w" sizes="(max-width: 661px) 100vw, 661px"></a></figure>



<blockquote>
<p>In 1832, Babbage and Joseph Clement&nbsp;produced a small working model (one-seventh of the plan),&nbsp;&nbsp;which operated on 6-digit numbers by second-order differences. Lady Byron described seeing the working prototype in 1833: “We both went to see the thinking machine (or so it seems) last Monday. It raised several Nos. to the 2nd and 3rd powers, and extracted the root of a Quadratic equation.”&nbsp;Work on the larger engine was suspended in 1833. (Wikipedia)</p>
</blockquote>



<p>1822 was the date that Babbage began work on the Difference Engine.&nbsp;&nbsp;As to being small:</p>



<blockquote>
<p>This first difference engine would have been composed of around 25,000 parts, weighed fifteen short tons&nbsp;(13,600&nbsp;kg), and would have been 8&nbsp;ft (2.4&nbsp;m) tall. (Wikipedia)</p>
</blockquote>



<p>Not exactly a desktop computer.&nbsp;</p>



<p>Our authors continue with the ahistorical fantasies:</p>



<blockquote>
<p>In 1801, Joseph-Marie Jacquard had designed a loom that wove complex patterns guided by a series of cards with holes punched in them. Its success in turning out “pre-programmed” patterns led Babbage to try to make a calculating machine that would accept instructions and data from punch cards.&nbsp;&nbsp;He called the proposed device an&nbsp;<em>Analytical Engine</em>.</p>
</blockquote>



<p>In 1801 Jacquard exhibited an earlier loom at the&nbsp;Exposition des produits de l’industrie française&nbsp;and was awarded a bronze medal. He started developing the punch card loom in 1804. The claim about Babbage is truly putting the cart before the horse. Following the death of his wife in 1827, Babbage undertook an extended journey throughout continental Europe, studying and researching&nbsp;all sorts of industrial plant to study and analyse their uses of mechanisation and automation. Something he had already done in the 1820s in Britain. He published the results of this research in his&nbsp;<em>On the Economy of Machinery and Manufactures</em>&nbsp;in 1832. To quote myself, “It would be safe to say that in 1832 Babbage knew more about mechanisation and automation that almost anybody else on the entire planet and what it was capable of doing and which activities could be mechanised and/or automated. It was in this situation that Babbage decided to transfer his main interest from the Difference Engine to developing the concept of the Analytical Engine conceived from the very beginning as a general-purpose computer capable of carrying out everything that could be accomplished by such a machine, far more than just a super number cruncher.” The Jacquard loom was just one of the machines he had studied on his odyssey and he incorporated its concept of punch card programming into the plans for his new computer.</p>



<p>Of course, our authors can’t resit repeating the Ada Lovelace myths and hagiography:</p>



<blockquote>
<p>Babbage’s assistant in this undertaking was Augusta Ada Lovelace […] Lovelace translated, clarified and extended a French description of Babbage’s project, adding a large amount of original commentary. She expanded on the idea of “programming” the machine with punched-card instructions and wrote what is considered to be the first significant computer program…</p>
</blockquote>



<p>Ada was in no way ever Babbage’s assistant. The notes added to the&nbsp;French description of Babbage’s project were cowritten with Babbage. They did not expand on the idea of “programming” the machine with punched-card instructions. The computer program included in Note G was written by Babbage and not Lovelace and wasn’t the first significant computer program. Babbage had already written several before the Lovelace translation.</p>



<figure><a href="https://thonyc.wordpress.com/wp-content/uploads/2025/03/babbage-01.jpg"><img data-attachment-id="13661" data-permalink="https://thonyc.wordpress.com/2025/03/13/history-of-maths-for-beginners/babbage-01-2/" data-orig-file="https://thonyc.wordpress.com/wp-content/uploads/2025/03/babbage-01.jpg" data-orig-size="1630,1195" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="babbage-01" data-image-description="" data-image-caption="" data-medium-file="https://thonyc.wordpress.com/wp-content/uploads/2025/03/babbage-01.jpg?w=300" data-large-file="https://thonyc.wordpress.com/wp-content/uploads/2025/03/babbage-01.jpg?w=500" width="1024" height="750" src="https://thonyc.wordpress.com/wp-content/uploads/2025/03/babbage-01.jpg?w=1024" alt="" srcset="https://thonyc.wordpress.com/wp-content/uploads/2025/03/babbage-01.jpg?w=1024 1024w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/babbage-01.jpg?w=150 150w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/babbage-01.jpg?w=300 300w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/babbage-01.jpg?w=768 768w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/babbage-01.jpg 1630w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Babbage in his autobiography <em>Passages from the Life of a Philosopher</em>&nbsp;(Longmans, 1864)</figcaption></figure>



<p>We now turn to George Boole and the advent of Boolean algebraic logic, which is dealt with extremely briefly but contains the following rather strange comment:</p>



<blockquote>
<p>Although Boole reportedly thought his system would never have any practical applications…</p>
</blockquote>



<p>I spent several years at university researching the life and work of George Boole and never came across any such claim. In fact, Boole himself applied his logical algebra to probability theory in&nbsp;<em>Laws of Thought</em>, as is stated quite clearly in its full title&nbsp;<em>An Investigation of the Laws of Thought: on Which are Founded the Mathematical Theories of Logic and Probabilities</em>.</p>



<p>After a very brief account of Herman Hollerith’s use of punch cards to accelerate the counting of 1880 US census, we take a big leap to Claude Shannon.&nbsp;</p>



<blockquote>
<p>The pieces started to come together in 1937, when Claude Shannon, in his master’s thesis at M.I.T., combined Boolean algebra with electrical relays and switching circuits to show how machines can “do” mathematical logic.</p>
</blockquote>



<p>We get no mention of the fact that Shannon was working on the circuitry of Vannevar Bush’s Differential Analyser, an analogue computer. The Differential Analyser played a highly significant role in the history of the computer as all three, US, war time computers, the ABC, the Harvard Mark I, and the ENIAC, all mentioned by our authors,&nbsp;&nbsp;were all projects set in motion to create an improved version of the Differential Analyser.</p>



<p>We move onto WWII:</p>



<blockquote>
<p>Alan Turing, the mathematician who spearheaded the successful British attempt to break the German U-boat command’s so-called “Enigma code,” designed several electronic machines to help in the crypto analysis.</p>
</blockquote>



<p>Turing designed one machine the Bombe based on the existing Polish Bomba, his design was improved by Gordon Welchman and the machine was actually designed and constructed by Harold Keen. We get a brief not totally accurate account of Max Newman, Tommy Flowers and the Colossus, before moving to Germany and Konrad Zuse. Our authors tell us:</p>



<blockquote>
<p>Meanwhile in Germany, Konrad Zuse had also built a programmable electronic computer. His work begun in the late 1930s, resulted in a functioning electro-mechanical machine by sometime in the early 1940s, giving him some historical claim to the title of inventory of electronic computers.&nbsp;</p>
</blockquote>



<p>This contains a central contradiction an electro-mechanical computer is not an electronic computer. Also, we have a precise time line for the development of Zuse’s computers. His Z1, a purely mechanical computer was finished in 1938, his Z2, an electro-mechanical computer in 1939. The Z3, which our authors are talking about, another electro-mechanical computer, an improvement on the Z2, was finished in 1941. Our authors erroneously claimed that Leibniz’s steeped reckoner used binary arithmetic but didn’t think it necessary to point out that Zuse was the first to use binary rather than decimal in his computers beginning with the Z1. They also state:</p>



<blockquote>
<p>However, wartime secrecy kept his work hidden as well.</p>
</blockquote>



<p>Zuse already set up his computer company during the war and went straight into development and production after the war. Although he couldn’t complete construction of his Z4, begun in 1944, until 1949, delivering the finished product to the ETH in Zurich in 1950.&nbsp;</p>



<p>The&nbsp;<strong>Z4</strong>&nbsp;was arguably the world’s first commercial&nbsp;digital computer, and is the oldest surviving programmable computer. (Wikipedia).</p>



<p>Our authors now deliver surprisingly brief accounts of the ABC, the Harvard Mark I, and ENIAC, before delivering the John von Neumann myth.</p>



<blockquote>
<p>John von Neuman is generally credited with devising a way to store programs inside a computer.</p>
</blockquote>



<p>The stored program computer in question is the EDVAC devised by John Mauchly and J. Presper Eckert, the inventors of ENIAC. Von Neuman merely described their design, without attribution, in his&nbsp;<em>First Draft of a Report on the EDVAC</em>, thereby stealing the credit.&nbsp;</p>



<p>The sketch ends with a very rapid gallop through the first stored program computers, EDSAC in Britain and UNIVAC I in the US, the introduction of first transistors and then integrated circuitry.&nbsp;</p>



<p>There is so much good, detailed history of the development of computers that I simply don’t understand how our authors could get so much wrong.</p>



<p>Sketch No. 24&nbsp;<em>The Arithmetic of Reasoning</em>:&nbsp;<em>Boolean Algebra</em>&nbsp;A brief nod to Aristotelian logic and then the repeat of the false claim that Leibniz’s steeped reckoner used binary numbers leads us into Leibniz’s attempts to create a calculus of logic, which however our authors admit remained unpublished and unknown until the twentieth century. All of this is merely a lead in to Augustus De Morgan and George Boole. We get brief, pathetic biographies of both of them emphasising their struggles in life. This leads into a presentation of Boole’s logic, presented in the form of truth tables which didn’t exist till much later! We then get an extraordinary ahistorical statement:</p>



<blockquote>
<p>De Morgan, too, was an influential, persuasive proponent of the algebraic treatment of logic. His publications helped to refine, extend, and popularise the system started by Boole.</p>
</blockquote>



<p>De Morgan was an outspoken supporter of Boole’s work but , his publications did not help to refine, extend, and popularise the system started by Boole. We do however get De Morgan’s Laws and his logic of relations. The latter gives our authors the chance to wax lyrical about Charles Saunders Pierce.&nbsp;</p>



<p>Towards the end of this very short sketch, we get the following, “But it was the work of Boole, De Morgan, C. S. Pierce&nbsp;<strong>and others…”&nbsp;</strong>(my emphasis) That “and others” covers a multitude of omissions, probably the most important being the work of William Stanley Jevons (1835–1882). An oft repeated truisms is that Boole’s algebraic logic is not Boolean algebra! It was first Jevons, who modifying Boole system turned it into the Boolean algebra used by computers today.&nbsp;</p>



<p>Sketch No. 25&nbsp;<em>Beyond Counting</em>:&nbsp;<em>Infinity and the Theory of Sets</em>&nbsp;Enter stage right George Cantor. We get a reasonable introduction to Cantorian set theory, Kronecker’s objections and the problem of set theory paradoxes. We then get a long digression on nineteenth century neo-Thomism, metaphysics, infinite sets and the Mind of God. Sorry but this has no place in a very short, supposedly elementary introduction to the history of mathematics, “designed for students just beginning their study of the discipline.”</p>



<p>Sketch No. 26&nbsp;<em>Out of the Shadows</em>:&nbsp;<em>The Tangent Function</em>&nbsp;is a reasonable account of the gradual inclusion of the tangent function into the trigonometrical canon. My only criticism is although they correctly state that Regiomontanus introduced the tangent function in his&nbsp;<em>Tabulae directionum profectionumque</em>&nbsp;written in 1467. Our authors only give the first two words of the title and translate it as&nbsp;<em>Table of Directions</em>. This is not incorrect but left so probably leads to misconceptions. Directions here does not refer to finding ones way geographical but to a method used in astrology to determine from a birth horoscope the major events, including death, in the subject life. This require complex spherical trigonometrical calculation transferring points from one system of celestial coordinates to another system. Hence the need for tangents.</p>



<p>Sketch No. 27&nbsp;<em>Counting Ratios</em>:&nbsp;<em>Logarithms</em>&nbsp;A detailed introduction to the history of the invention of logarithms. In general, OK but a bit off the rails on the story of Jost Bürgi.&nbsp;</p>



<blockquote>
<p>As Napier and Briggs worked in Scotland, the Thirty Years’ War was spreading misery on the European continent. One of its side effects was the loss of most copies of a 1620 publication by Joost Bürgi, a Swiss clockmaker. Bürgi had discovered the basic principles of logarithms while assisting the astronomer Johannes Kepler in Prague in 1588, some years before Napier, but his book of tables was not published until six years after Napier’s&nbsp;<em>Descriptio</em>&nbsp;appeared. When most of the copies disappeared, Bürgi’s work faded into obscurity.</p>
</blockquote>



<p>The failure of Bürgi’s work to make an impact almost certainly had to do with the facts that only a very small number were ever printed and it only consisted of a table of what we now call anti-logarithms with no explanation of how they were created or how to use them.&nbsp;&nbsp;Also, in 1588 Kepler was still living and working in Graz and Bürgi was living and working in Kassel. Kepler first moved to Prague in 1600 and Bürgi in 1604. The reference to 1588 in Bürgi’s work is almost certainly to&nbsp;prosthaphaeresis, a method of using trigonometrical formulars to turn difficult multiplications and divisions into addition and subtractions which Bürgi had learnt from Paul Wittich, and not to logarithms. His work on logarithms almost certainly started later than that of Napier but was independent. Interestingly it is thought that Napier was led to logarithms after being taught&nbsp;&nbsp;prosthaphaeresis by John Craig, who had also learnt it from Wittich</p>



<p>Sketch No. 28&nbsp;<em>Anyway You Slice It</em>:&nbsp;<em>Conic Sections</em>&nbsp;What is actually quite a good section on the history of conic sections is spoilt by one minor error and a cluster fuck concerning Kepler. The small error concerns Witelo (c.1230–after 1280), our authors write:</p>



<blockquote>
<p>There is some evidence that Apollonius’s&nbsp;<em>Conics</em>&nbsp;was known in Europe as far back as the 113<sup>th</sup>&nbsp;century, when Erazmus Witelo used conics in his book on optics and perspective.</p>
</blockquote>



<p>Witelo’s book is titled&nbsp;<em>De Perspectiva</em>&nbsp;and is purely a book on optics not perspective. Perspectivist optics is a school of optical theory derived from the optics of Ibn al-Haytham, whose book&nbsp;<em>Kitāb al-Manāẓir</em>&nbsp;is titled&nbsp;<em>De Perspectiva</em>&nbsp;in Latin translation. On to Kepler:</p>



<figure><a href="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14432970_01-1.jpg"><img data-attachment-id="13669" data-permalink="https://thonyc.wordpress.com/2025/03/13/history-of-maths-for-beginners/kepler-conics20250312_14432970_01-2/" data-orig-file="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14432970_01-1.jpg" data-orig-size="2116,2258" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="Kepler Conics20250312_14432970_01" data-image-description="" data-image-caption="" data-medium-file="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14432970_01-1.jpg?w=281" data-large-file="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14432970_01-1.jpg?w=500" loading="lazy" width="960" height="1024" src="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14432970_01-1.jpg?w=960" alt="" srcset="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14432970_01-1.jpg?w=960 960w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14432970_01-1.jpg?w=1920 1920w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14432970_01-1.jpg?w=141 141w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14432970_01-1.jpg?w=281 281w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14432970_01-1.jpg?w=768 768w" sizes="(max-width: 960px) 100vw, 960px"></a></figure>



<figure><a href="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14451773_02-1.jpg"><img data-attachment-id="13671" data-permalink="https://thonyc.wordpress.com/2025/03/13/history-of-maths-for-beginners/kepler-conics20250312_14451773_02-2/" data-orig-file="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14451773_02-1.jpg" data-orig-size="2033,389" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Kepler Conics20250312_14451773_02" data-image-description="" data-image-caption="" data-medium-file="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14451773_02-1.jpg?w=300" data-large-file="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14451773_02-1.jpg?w=500" loading="lazy" width="1024" height="195" src="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14451773_02-1.jpg?w=1024" alt="" srcset="https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14451773_02-1.jpg?w=1024 1024w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14451773_02-1.jpg?w=1019 1019w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14451773_02-1.jpg?w=150 150w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14451773_02-1.jpg?w=300 300w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14451773_02-1.jpg?w=768 768w, https://thonyc.wordpress.com/wp-content/uploads/2025/03/kepler-conics20250312_14451773_02-1.jpg 2033w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>Sorry about the scans but I couldn’t be arsed to type out the whole section.</p>



<p>The camera obscura that Kepler assembled on the market place in Graz in the summer of 1600 was a small tent and not “a large wooden structure.” It was a pin hole camera and not a kind of large-size one.&nbsp;</p>



<p>The pin hole camera problem, the image created is larger than it should be, had been known since the Middle Ages, was widely discussed in the literature, and Kepler had been expecting it before he began his observations.&nbsp;&nbsp;His attention had been drawn to the problem by Tycho when he was in Prague at the beginning of 1600 to negotiate his move there to work with Tycho. Tycho had also drawn his attention to the problem of atmospheric refraction in the astronomy. These were the triggers that motivated his studies in optics. As noted he moved to Prague to work with Tycho but there was no Imperial Observatory in Prague.&nbsp;</p>



<p>Kepler inherited Tycho’s position of Imperial Mathematicus but not his observation, which were inherited by his daughter. This led to several years of difficult negotiations before he obtained permission to publish his work based on those observations. Tycho had set Kepler on the problem of calculating the orbit of Mars, the work that led to his first two planetary laws, before his death.</p>



<p>The book that Kepler used for his optical studies and on which he based his own work was Friedrich Risner’s&nbsp;“<em>Opticae thesaurus: Alhazeni Arabis libri septem, nuncprimum editi; Eiusdem liber De Crepusculis et nubium ascensionibus, Item Vitellonis Thuringopoloni libri X</em>” (Optical Treasure: Seven books of Alhazen the Arab, published for the first time; His book On Twilight and the Rising of Clouds, Also of Vitello Thuringopoloni book X), which was published in 1572.</p>



<p>“Of course he thought of the conic sections!” Actually, when Kepler first realised that the orbit of Mars was some sort of oval, he didn’t think of the conic section at all. Something for which he criticised himself in his&nbsp;<em>Astronomia Nova,</em>&nbsp;when he finally realised, after much more wasted effort, that the orbit was actually an ellipse. A realisation not based on more measurements. The&nbsp;<em>Astronomia Nova</em>&nbsp;from 1609 only contains the first two planetary laws. The third was first published in his&nbsp;<em>Harmonice Mundi</em>&nbsp;in 1619.</p>



<p><em>&nbsp;</em>Sketch No. 29&nbsp;<em>Beyond the Pale</em>:&nbsp;<em>Irrational Numbers</em>&nbsp;Once again an acceptable historical account.&nbsp;</p>



<p>Sketch No. 30&nbsp;<em>Barely Touching</em>:&nbsp;<em>From Tangents to Derivatives</em>&nbsp;Not perfect but more than reasonable</p>



<p>The book ends with a&nbsp;<em>What to Read Next</em>&nbsp;which begins with&nbsp;<em>The Reference Shelf</em>. Here they recommend several one volume histories of mathematics starting with Victor J Katz and then moving onto Howard Eves’s&nbsp;<em>An Introduction to the History of Mathematics&nbsp;</em>and David M . Burton’s&nbsp;<em>The History of Mathematics</em>&nbsp;which they admit are both showing their age. They move on to lot more recommendations but studiously ignore, what was certainly the market leader before Katz,&nbsp;Carl B. Boyer,&nbsp;<em>A History of</em>&nbsp;<em>Mathematics</em>&nbsp;in the third edition edited by Uta Merzbach. I seriously wonder what they have against Boyer whose books on the history of mathematics are excellent.&nbsp;</p>



<p>Having covered a fairly wide range of reference books, including a lot of Grattan-Guinness we now move onto&nbsp;<em>Twelve Historical Books You Ought to Read</em>. Of course, any such recommendation is subjective but some of their choices are more that questionable. They start, for example, with Tobias Dantzig’s&nbsp;<em>Number the Language of Science</em>. This was first published in 1930 and is definitively severely dated.&nbsp;</p>



<p>Our authors also recommend Reviel Netz &amp; William Noel,&nbsp;<em>The Archimedes Codex</em>&nbsp;(2007). This book was the object of an incredible media hype when it first appeared. Whilst the imaging techniques used to expose the Archimedean manuscript on the palimpsest are truly fascinating the claims made by the authors about the newly won knowledge about Archimedes works are extremely hyperbolic and contain more than a little bullshit. Should this really be on a list of twelve history of maths books one ought to read?</p>



<p>I did a double take when I saw that they recommend Eric Templer&nbsp;<em>Bell’s Men of</em>&nbsp;<em>Mathematics</em>. They themselves say:</p>



<blockquote>
<p>The book has lost some of its original popularity, not (or at least not primarily) because of its politically incorrect, but rather because Bell takes too many liberties with his sources. (Some critics would say “because he makes things up.”) The book is fun to read, but don’t rely solely on Bell for facts.</p>
</blockquote>



<p>My take don’t waste your time reading this crap book. I read it when I was sixteen and spent at least twenty years unlearning all the straight forward lies that Bell spews out!</p>



<p>Another disaster recommendation is Dava Sobel’s&nbsp;<em>Longitude</em>, which our authors say, “provides a good picture of the interactions among mathematics, astronomy, and navigation in the 18<sup>th</sup>&nbsp;century.” Sobel’s much hyped book give an extremely distorted “picture of the interactions among mathematics, astronomy, and navigation in the 18<sup>th</sup>&nbsp;century,” which at times is closer to a fantasy novel than a serios history book. If you really want to learn the true facts about&nbsp;&nbsp;those interactions then read Richard Dunn &amp; Rebekah Higgitt,&nbsp;<em>Finding Longitude</em>:&nbsp;<em>How ships, clocks and stars helped solve the longitude problem</em>&nbsp;(Collins, 2014) and Katy Barrett,&nbsp;<em>Looking for Longitude</em>:&nbsp;<em>A Cultural</em>&nbsp;<em>History</em>&nbsp;(Liverpool University Press), <a href="https://thonyc.wordpress.com/2023/09/20/she-sought-it-here-she-sought-it-there-she-found-elusive-longitude-everywhere/"><strong>both volumes are written by professional historians, who spent several years researching the topic in a major research project.&nbsp;</strong></a></p>



<p>The section closes with&nbsp;<em>History Online</em>, which can’t be any good because it doesn’t include The Renaissance Mathematicus!&nbsp;🙃</p>



<p>The apparatus begins with a useful&nbsp;<em>When They Lived</em>, which simply lists lots of the most well-known mathematicians alphabetically with their birth and death dates. This is followed by the bibliography, already mentioned above, and a good index. There are numerous, small, black and white illustration scattered throughout the pages.</p>



<p>Despite my negative comment about some points in the book, I would actually recommend it as a reasonably priced, mostly accurate, introduction to the history of mathematics. A small criticism is it does at times display a US American bias, but it was written primarily for American students. A good jumping off point for somebody developing an interest in the discipline. In any case considerably better that my jumping off point, E. T. Bell!</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Honey Bunnies (180 pts)]]></title>
            <link>https://mameson.com/experiment/glsl/fro_9/fro_9.html</link>
            <guid>43355521</guid>
            <pubDate>Thu, 13 Mar 2025 17:33:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mameson.com/experiment/glsl/fro_9/fro_9.html">https://mameson.com/experiment/glsl/fro_9/fro_9.html</a>, See on <a href="https://news.ycombinator.com/item?id=43355521">Hacker News</a></p>
<div id="readability-page-1" class="page">



<p><span>"fro 9"</span><br>
©2024 Maeda Mameo
</p>








</div>]]></description>
        </item>
        <item>
            <title><![CDATA[IO Devices and Latency (371 pts)]]></title>
            <link>https://planetscale.com/blog/io-devices-and-latency</link>
            <guid>43355031</guid>
            <pubDate>Thu, 13 Mar 2025 16:46:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://planetscale.com/blog/io-devices-and-latency">https://planetscale.com/blog/io-devices-and-latency</a>, See on <a href="https://news.ycombinator.com/item?id=43355031">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>By <!-- -->Benjamin Dicken<!-- --> | <time datetime="2025-03-13">March 13, 2025</time></p><p>Non-volatile storage is a cornerstone of modern computer systems.<!-- --> <!-- -->Every modern photo, email, bank balance, medical record, and other critical pieces of data are kept on digital storage devices, often replicated many times over for added durability.</p><p><strong>Non-volatile</strong> storage, or colloquially just "<strong>disk</strong>", can store binary data even when the computer it is attached to is powered off.<!-- --> <!-- -->Computers have other forms of <strong>volatile</strong> storage such as CPU registers, CPU cache, and random-access memory, all of which are faster but require continuous power to function.</p><p>Here, we're going to cover the history, functionality, and performance of non-volatile storage devices over the history of computing, all using fun and interactive visual elements.<!-- --> <!-- -->This blog is written in celebration of our latest product release: <a href="https://planetscale.com/blog/announcing-metal">PlanetScale Metal</a>.<!-- --> <!-- -->Metal uses locally attached NVMe drives to run your cloud database, as opposed to the slower and less consistent network-attached storage used by most cloud database providers.<!-- --> <!-- -->This results in a blazing fast queries, low latency, and unlimited IOPS.<!-- --> <!-- -->Check out <a href="https://planetscale.com/docs/metal">the docs</a> to learn more.</p><h2 id="tape-storage"><a href="#tape-storage">Tape Storage</a></h2><p>As early as the 1950s, computers were using <strong>tape drives</strong> for non-volatile digital storage.<!-- --> <!-- -->Tape storage systems have been produced in many form factors over the years, ranging from ones that take up an entire room to small drives that can fit in your pocket, such as the iconic Sony Walkman.<!-- --> <!-- -->A tape <strong>Reader</strong> is a box containing hardware specifically designed for reading <strong>tape cartridges</strong>.<!-- --> <!-- -->Tape cartridges are inserted and then unwound which causes the tape to move over the <strong>IO Head</strong>, which can <span>read</span> and <span>write</span> data.</p><p>Though tape started being used to store digital information over 70 years ago, it is still in use today for certain applications.<!-- --> <!-- -->A standard LTO tape cartridge has several-hundred meters of 0.5 inch wide tape.<!-- --> <!-- -->The tape has several tracks running along its length, each track being further divided up into many small cells.<!-- --> <!-- -->A single tape cartridge contains many trillions of cells.</p><p>Each cell can have its magnetic polarization set to up or down, corresponding to a binary 0 or 1.<!-- --> <!-- -->Technically, the magnetic field created by the <em>transition</em> between two cells is what makes the 1 or zero.<!-- --> <!-- -->A long sequence of bits on a tape forms a page of data.<!-- --> <!-- -->In the visualization of the tape reader, we simplify this by showing the tape as a simple sequence of data pages, rather than showing individual bits.</p><p>When a tape needs to be read, it is loaded into a reader, sometimes by hand and sometimes by robot.<!-- --> <!-- -->The reader then spins the cartridge with its motor and uses the <strong>reader head</strong> to read off the binary values as the tape passes underneath.</p><p>Give this a try with the (greatly slowed down) interactive visualization below.<!-- --> <!-- -->You can control the <span>speed of the tape</span> if you'd like it faster or slower.<!-- --> <!-- -->You can also <span>issue read requests</span> and <span>write requests</span> and then monitor how long these take.<!-- --> <!-- -->You'll also be able to see the queue of pending IO operations pop up in the top-left corner.<!-- --> <!-- -->Try issuing a few requests to get a feel for how tape storage works:</p><p>If you spend enough time with this, you will notice that:</p><ol><li>If you read/write to a cell "near" the read head, it's fast.</li><li>If you read/write to a cell "far" from the read head, it's slow.</li></ol><p>Even with modern tape systems, reading data that is far away on a tape can take 10s of seconds, because it may need to spin the tape by hundreds of meters to reach the desired data.<!-- --> <!-- -->Let's compare two more specific, interactive examples to illustrate this further.</p><p>Say we need to <span>read</span> a total of 4 pages and <span>write</span> an additional 4 pages worth of data.<!-- --> <!-- -->In the first scenario, all 4 pages we need to read are in a neat sequence, and the 4 to write to are immediately after the reads.<!-- --> <!-- -->You can see the IO operations queued up in the white container on the top-left.<!-- --> <!-- -->Go ahead and click the <span>Time IO button</span> to see this in action, and observe the time it takes to complete.</p><p>As you can see, it takes somewhere around 3-4 seconds.<!-- --> <!-- -->On a real system, with an IO head that can operate much faster and motors that can drive the spools more quickly, it would be much faster.</p><p>Now consider another scenario where we need to read and write the same number of pages.<!-- --> <!-- -->However, these reads and writes are spread out throughout the tape.<!-- --> <!-- -->Go ahead and click the <span>Time IO button</span> again.</p><p>That took ~7x longer for the same total number of reads and writes!<!-- --> <!-- -->Imagine if this system was being used to load your social media feed or your email inbox.<!-- --> <!-- -->It might take 10s of seconds or even a full minute to display.<!-- --> <!-- -->This would be totally unacceptable.</p><p>Though the latency for random reads and writes is poor, tape systems operate quite well when reading or writing data in long sequences.<!-- --> <!-- -->In fact, tape storage still has many such use cases today in the modern tech world.<!-- --> <!-- -->Tape is particularly well-suited for situations where there is a need for <em>massive</em> amounts of storage that does not need to be read frequently, but needs to be safely stored.<!-- --> <!-- -->This is because tape is both cheaper per-gigabyte and has a longer shelf-life than its competition: solid state drives and hard disk drives.<!-- --> <!-- -->For example, CERN has a tape storage data warehouse with <a href="https://home.cern/science/computing/data-preservation">over 400 petabytes of data</a> under management.<!-- --> <!-- -->AWS also offers <a href="https://aws.amazon.com/storagegateway/vtl/">tape archiving</a> as a service.</p><p>What tape is not well suited for is high-traffic transactional databases.<!-- --> <!-- -->For these and many other high-performance tasks, other storage mediums are needed.</p><h2 id="hard-disk-drives"><a href="#hard-disk-drives">Hard Disk Drives</a></h2><p>The next major breakthrough in storage technology was the <strong>hard disk drive</strong>.</p><p>Instead of storing binary data on a tape, we store them on a small circular metal disk known as the <strong>Platter</strong>.<!-- --> <!-- -->This disk is placed inside of an <strong>enclosure</strong> with a special <strong>read/write head</strong>, and spins very fast (7200 RPM is common, for example).<!-- --> <!-- -->Like the tape, this disk is also divided into tracks.<!-- --> <!-- -->However, the tracks are <em>circular</em>, and a single disk will often have well over 100,000 tracks.<!-- --> <!-- -->Each track contains hundreds of thousands of pages, and each page containing 4k (or so) of data.</p><p>An HDD requires a mechanical spinning motion of both the reader and the platter to bring the data to the correct location for reading.<!-- --> <!-- -->One advantage of HDD over tape is that the entire surface area of the bits is available 100% of the time.<!-- --> <!-- -->It still takes time to move the needle + spin the disk to the correct location for a read or write, but it does not need to be "uncovered" like it needs to be for a tape.<!-- --> <!-- -->This combined with the fact that there are two different things that can spin, means data can be read and written with much lower latency.<!-- --> <!-- -->A typical random read can be performed in 1-3 milliseconds.</p><p>Below is an interactive hard drive.<!-- --> <!-- -->You can control the <span>speed of the platter</span> if you'd like it faster or slower.<!-- --> <!-- -->You can request that the hard drive <span>read a page</span> and <span>write to a nearby available page</span>.<!-- --> <!-- -->If you request a read or write before the previous one is complete, a queue will be built up, and the disk will process the requests in the order it receives them.<!-- --> <!-- -->As before, you'll also be able to see the queue of pending IO operations in the white IO queue box.</p><p>As with the tape, the speed of the platter spin has been slowed down by orders of magnitude to make it easier to see what's going on.<!-- --> <!-- -->In real disks, there would also be many more tracks and sectors, enough to store multiple terabytes of data in some cases.</p><p>Let's again consider a few specific scenarios to see how the order of reads and writes affects latency.</p><p>Say we need to write a total of three pages of data and then read 3 pages afterward.<!-- --> <!-- -->The three writes will happen on nearby available pages, and the reads will be from tracks 1, 4, and 3.<!-- --> <!-- -->Go ahead and click the <span>Time IO button</span>.<!-- --> <!-- -->You'll see the requests hit the queue, the reads and writes get fulfilled, and then the total time at the end.</p><p>Due to the sequential nature of most of these operations, all the tasks were able to complete quickly.</p><p>Now consider the same set of 6 reads and writes, but with them being interleaved in a different order.<!-- --> <!-- -->Go ahead and click the <span>Time IO button</span> again.</p><p>If you had the patience to wait until the end, you should notice how the same total number of reads and writes took much longer.<!-- --> <!-- -->A lot of time was spent waiting for the platter to spin into the correct place under the read head.</p><p>Magnetic disks have supported command queueing directly on the disks for a long time (80s with SCSI, 2000s with SATA).<!-- --> <!-- -->Because of this, the OS can issue multiple commands that run in parallel and potentially out-of-order, similar to SSDs.<!-- --> <!-- -->Magnetic disks also improve their performance if they can build up a queue of operations that the disk controller can then schedule reads and writes to optimize for the geometry of the disk.</p><p>Here's a visualization to help us see the difference between the latency of a <span>random tape read</span> compared to a <span>random disk read</span>.<!-- --> <!-- -->A random tape read will often take multiple seconds (I put 1 second here to be generous) and a disk head seek takes closer to 2 milliseconds (one thousandth of a second)</p><p>Even though HDDs are an improvement over tape, they are still "slow" in some scenarios, especially random reads and writes.<!-- --> <!-- -->The next big breakthrough, and currently the most common storage format for transactional databases, are SSDs.</p><h2 id="solid-state-drives"><a href="#solid-state-drives">Solid State Drives</a></h2><p>Solid State Storage, or "flash" storage, was invented in the 1980s.<!-- --> <!-- -->It was around even while tape and hard disk drives dominated the commercial and consumer storage spaces.<!-- --> <!-- -->It didn't become mainstream for consumer storage until the 2000s due to technological limitations and cost.</p><p>The advantage of SSDs over both tape and disk is that they do not rely on any <em>mechanical</em> components to read data.<!-- --> <!-- -->All data is read, written, and erased electronically using a special type of non-volatile <em>transistor</em> known as NAND flash.<!-- --> <!-- -->This means that each 1 or 0 can be read or written without the need to move any physical components, but 100% through electrical signaling.</p><p>SSDs are organized into one or more <strong>targets</strong>, each of which contains many <strong>blocks</strong> which each contain some number of <strong>pages</strong>.<!-- --> <!-- -->SSDs read and write data at the page level, meaning they can only read or write full pages at a time.<!-- --> <!-- -->In the SSD below, you can see reads and writes happening via the <strong>lines</strong> between the controller and targets (also called "traces").</p><p>The removal of mechanical components reduces the latency between when a request is made and when the drive can fulfill the request.<!-- --> <!-- -->There is no more waiting around for something to spin.</p><p>We're showing small examples in the visual to make it easier to follow along, but a single SSD is capable of storing multiple terabytes of data.<!-- --> <!-- -->For example, say each page holds 4096 bits of data (4k).<!-- --> <!-- -->Now, say each block stores 16k pages, each target stores 16k blocks, and our device has 8 targets.<!-- --> <!-- -->This comes out to <code>4k * 16k * 16k * 8 = 8,796,093,022,208</code> bits, or 8 terabytes.<!-- --> <!-- -->We could increase the capacity of this drive by adding more targets or packing more pages in per block.</p><p>Here's a visualization to help us see the difference between the latency of a random read on an <span>HDD</span> vs <span>SSD</span>.<!-- --> <!-- -->A random read on an SSD varies by model, but can execute as fast as 16μs (μs = microsecond, which is one millionth of a second).</p><p>It would be tempting to think that with the removal of mechanical parts, the organization of data on an SSD no longer matters.<!-- --> <!-- -->Since we don't have to wait for things to spin, we can access any data at any location with perfect speed, right?</p><p>Not quite.</p><p>There are other factors that impact the performance of IO operations on an SSD.<!-- --> <!-- -->We won't cover them all here, but two that we will discuss are <strong>parallelism</strong> and <strong>garbage collection</strong>.</p><h3 id="ssd-parallelism"><a href="#ssd-parallelism">SSD Parallelism</a></h3><p>Typically, each <strong>target</strong> has a dedicated <strong>line</strong> going from the control unit to the target.<!-- --> <!-- -->This line is what processes reads and writes, and only one page can be communicated by each line at a time.<!-- --> <!-- -->Pages can be communicated on these lines <em>really fast</em>, but it still does take a small slice of time.<!-- --> <!-- -->The organization of data and sequence of reads and writes has a significant impact on how efficiently these lines can be used.</p><p>In the interactive SSD below, we have 4 targets and a set of 8 write operations queued up.<!-- --> <!-- -->You can click the <span>Time IO button</span> to see what happens when we can use the lines in parallel to get these pages written.</p><p>In this case, we wrote 8 pages spread across the 4 targets.<!-- --> <!-- -->Because they were spread out, we were able to leverage parallelism to write 4 at a time in two time slices.</p><p>Compare that with another sequence where the SSD writes all 8 pages to the same target.<!-- --> <!-- -->The SSD can only utilize a single data line for the writes.<!-- --> <!-- -->Again, hit the <span>Time IO button</span> to see the timing.</p><p>Notice how only one line was used and it needed to write sequentially.<!-- --> <!-- -->All the other lines sat dormant.</p><p>This demonstrates that the order in which we read and write data matters for performance.<!-- --> <!-- -->Many software engineers don't have to think about this on a day-to-day basis, but those designing software like MySQL need to pay careful attention to what <a href="https://planetscale.com/blog/btrees-and-database-indexes">structures data is being stored in</a> and how data is laid out on disk.</p><h3 id="ssd-garbage-collection"><a href="#ssd-garbage-collection">SSD Garbage Collection</a></h3><p>The minimum "chunk" of data that can be read from or written to an SSD is the size of a page.<!-- --> <!-- -->Even if you only need a subset of the data within, that is the unit that requests to the drive must be made in.</p><p>Data can be read from a page any number of times.<!-- --> <!-- -->However, writes are a bit different.<!-- --> <!-- -->After a page is written to, it cannot be overwritten with new data until the old data has been explicitly <strong>erased</strong>.<!-- --> <!-- -->The tricky part is, individual pages cannot be erased.<!-- --> <!-- -->When you need to erase data, the entire block must be erased, and afterwards all of the pages within it can be reused.</p><p>Each SSD needs to have an internal algorithm for managing which pages are empty, which are in use, and which are <strong>dirty</strong>.<!-- --> <!-- -->A <strong>dirty</strong> page is one that has been written to but the data is no longer needed and ready to be erased.<!-- --> <!-- -->Data also sometimes needs to be re-organized to allow for new write traffic.<!-- --> <!-- -->The algorithm that manages this is called the <strong>garbage collector</strong>.</p><p>Let's see how this can have an impact by looking at another visualization.<!-- --> <!-- -->In the below SSD, all four of the targets are storing data.<!-- --> <!-- -->Some of the data is <span>dirty, indicated by red text</span>.<!-- --> <!-- -->We want to <span>write</span> 5 pages worth of data to this SSD.<!-- --> <!-- -->If we <span>time this sequence of writes</span>, the SSD can happily write them to free pages with no need for extra garbage collection.<!-- --> <!-- -->There are sufficient unused pages in the first target.</p><p>Now say we have a drive with different data already on it, but we want to <span>write</span> those same 5 pages of data to it.<!-- --> <!-- -->In this drive, we only have 2 pages that are unused, but a number of <span>dirty pages</span>.<!-- --> <!-- -->In order to write 5 pages of data, the SSD will need to spend some time doing garbage collection to make room for the new data.<!-- --> <!-- -->When attempting to <span>time another sequence of writes</span>, some garbage collection will take place to make room for the data, slowing down the write.</p><p>In this case, the drive had to move the two non-dirty pages from the top-left target to new locations.<!-- --> <!-- -->By doing this, it was able to make all of the pages on the top-left target dirty, making it safe to erase that data.<!-- --> <!-- -->This made room for the 5 new pages of data to be written.<!-- --> <!-- -->These additional steps significantly slowed down the performance of the write.</p><p>This shows how the organization of data on the drive can have an impact on performance.<!-- --> <!-- -->When SSDs have a lot of reads, writes, and deletes, we can end up with SSDs that have degraded performance due to garbage collection.<!-- --> <!-- -->Though you may not be aware, busy SSDs do garbage collection tasks regularly, which can slow down other operations.</p><p>These are just two of many reasons why the arrangement of data on a SSD affects its performance.</p><h2 id="storage-in-the-cloud"><a href="#storage-in-the-cloud">Storage in the cloud</a></h2><p>The shift from tape, to disk, to solid state has allowed durable IO performance to accelerate dramatically over the past several decades.<!-- --> <!-- -->However, there is another phenomenon that has caused an additional shift in IO performance: moving to the cloud.</p><p>Though there were companies offering cloud compute services before this, the mass move to cloud gained significant traction when Amazon AWS launched in 2006.<!-- --> <!-- -->Since that time, tens of thousands of companies have moved their app servers and database systems to their cloud and other similar services from Google, Microsoft, and others.</p><p>Though there are many upsides to this trend, there are several downsides.<!-- --> <!-- -->One of these is that servers tend to have less permanence.<!-- --> <!-- -->Users rent (virtualised) servers on arbitrary hardware within gigantic data centers.<!-- --> <!-- -->These servers can get shut down at any time for a variety of reasons - hardware failure, hardware replacement, network disconnects, etc.<!-- --> <!-- -->When building platforms on rented cloud infrastructure, computer systems need to be able to tolerate more frequent failures at any moment.<!-- --> <!-- -->This, along with many engineers' desire for dynamically-scaleable storage volumes has led to a new sub-phenomenon: Separation of <strong>storage</strong> and <strong>compute</strong>.</p><h2 id="separating-storage-from-compute"><a href="#separating-storage-from-compute">Separating storage from compute</a></h2><p>Traditionally, most servers, desktops, laptops, phones and other computing devices have their non-volatile storage directly attached.<!-- --> <!-- -->These are attached with SATA cables, PCIe interfaces, or even built directly into the same SOC as the RAM, CPU, and other components.<!-- --> <!-- -->This is great for speed, but provides the following challenges:</p><ol><li>If the server goes down, the data goes down with it.</li><li>The storage is of a fixed size.</li></ol><p>For application servers, 1. and 2. are typically not a big deal since they work well in ephemeral environments by design.<!-- --> <!-- -->If one goes down, just spin up a new one.<!-- --> <!-- -->They also don't typically need much storage, as most of what they do happens in-memory.</p><p>Databases are a different story.<!-- --> <!-- -->If a server goes down, we don't want to lose our data, and data size grows quickly, meaning we may hit storage limits.<!-- --> <em>Partly</em> due to this, many cloud providers allow you to spin up compute instances with a separately-configurable storage system attached over the network.<!-- --> <!-- -->In other words, using network-attached storage as the default.</p><p>When you create a new server in EC2, the default is typically to attach an EBS network storage volume.<!-- --> <!-- -->Many database services including Amazon RDS, Amazon Aurora, Google Cloud SQL, and PlanetScale rely on these types of storage systems that have compute separated from storage over the network.<!-- --> <!-- -->This provides a nice advantage in the that the storage volume can be dynamically resized as data grows and shrinks.<!-- --> <!-- -->It also means that if a server goes down, the data is still safe, and can be re-attached to a different server.<!-- --> <!-- -->This simplicity has come at a cost, however.</p><h2 id="local-vs-network-storage"><a href="#local-vs-network-storage">Local vs network storage</a></h2><p>Consider the following simple configuration.<!-- --> <!-- -->In it, we have a server with a CPU, RAM, and direct-attached NVMe SSD.<!-- --> <!-- -->NVMe SSDs are a type of solid state disk that use the non-volatile memory host controller interface specification for blazing-fast IO speed and great bandwidth.<!-- --> <!-- -->In such a setup, the <span>round trip from CPU to memory (RAM)</span> takes about 100 nanoseconds (a nanosecond is 1 billionth of a second).<!-- --> <span>A round trip from the CPU to a locally-attached NVMe SSD</span> takes about 50,000 nanoseconds (50 microseconds).</p><p>This makes it pretty clear that it's best to keep as much data in memory as possible for faster IO times.<!-- --> <!-- -->However, we still need disk because (A) memory is more expensive and (B) we need to store our data somewhere permanent.<!-- --> <!-- -->As slow as it may seem here, a locally-attached NVMe SSD is about as fast as it gets for modern storage.</p><p>Let's compare this to the <span>speed of a network-attached storage volume</span>, such as EBS.<!-- --> <!-- -->Read and write requires a short network round trip within a data center.<!-- --> <!-- -->The round trip time is significantly worse, taking about 250,000 nanoseconds (250 microseconds, or 0.25 milliseconds).</p><p>Using the same cutting-edge SSD now takes an <em>order of magnitude</em> longer to fulfill individual read and write requests.<!-- --> <!-- -->When we have large amounts of sequential IO, the negative impact of this can be reduced, but not eliminated.<!-- --> <!-- -->We have introduced significant <em>latency</em> deterioration for every time we need to hit our storage system.</p><p>Another issue with network-attached storage in the cloud comes in the form of limiting IOPS.<!-- --> <!-- -->Many cloud providers that use this model, including AWS and Google Cloud, limit the amount of IO operations you can send over the wire.<!-- --> <!-- -->By default, a GP3 EBS instance on Amazon allows you to send 3000 IOPS per-second, with an additional pool that can be built up to allow for occasional bursts.<!-- --> <!-- -->The following visual shows how this works.<!-- --> <!-- -->Note that the burst balance size is smaller here than in reality to make it easier to see.</p><p>If instead you have your storage attached directly to your compute instance, there are no artificial limits placed on IO operations.<!-- --> <!-- -->You can read and write as fast as the hardware will allow for.</p><p>For as many steps as we've taken forward in IO performance over the years, this seems like a step in the wrong direction.<!-- --> <!-- -->This separation buys some nice conveniences, but at what cost to performance?</p><p>How do we overcome issue 1 (data durability) and 2 (drive scalability) while keeping good IOPS performance?</p><p>Issue 1 can be overcome with replication.<!-- --> <!-- -->Instead of relying on a single server to store all data, we can replicate it onto several computers.<!-- --> <!-- -->One common way of doing this is to have one server act as the primary, which will receive all write requests.<!-- --> <!-- -->Then 2 or more additional servers get all the data replicated to them.<!-- --> <!-- -->With the data in three places, the likelihood of losing data becomes very small.</p><p>Let's look at concrete numbers.<!-- --> <!-- -->As a made up value, say in a given month, there is a 1% chance of a server failing.<!-- --> <!-- -->With a single server, this means we have a 1% chance of losing our data each month.<!-- --> <!-- -->This is an unacceptable for any serious business purpose.<!-- --> <!-- -->However, with three servers, this goes down to 1% × 1% × 1% = 0.0001% chance (1 in one million).<!-- --> <!-- -->At PlanetScale the protection is actually far stronger than even this, as we automatically detect and replace failed nodes in your cluster.<!-- --> <!-- -->We take frequent and reliable backups of the data in your database for added protection.</p><p>Problem 2. can be solved, though it takes a bit more manual intervention when working with directly-attached SSDs.<!-- --> <!-- -->We need to ensure that we monitor and get alerted when our disk approaches capacity limits, and then have tools to easily increase capacity when needed.<!-- --> <!-- -->With such a feature, we can have data permanence, scalability, and blazing fast performance. This is exactly what PlanetScale has built with Metal.</p><p>Planetscale just announced <strong><a href="https://planetscale.com/blog/announcing-metal">Metal</a></strong>, an industry-leading solution to this problem.</p><p>With Metal, you get a full-fledged Vitess+MySQL cluster set up, with each MySQL instance running with a direct-attached NVMe SSD drive.<!-- --> <!-- -->Each Metal cluster comes with a primary and two replicas by default for extremely durable data.<!-- --> <!-- -->We allow you to resize your servers with larger drives with just a few clicks of a button when you run up against storage limits.<!-- --> <!-- -->Behind the scenes, we handle spinning up new nodes and migrating your data from your old instances to the new ones with zero downtime.</p><p>Perhaps most importantly, with a Metal database, there is no artificial cap on IOPS.<!-- --> <!-- -->You can perform IO operations with minimal latency, and hammer it as hard as you want without being throttled or paying for expensive IOPS classes on your favorite cloud provider.</p><p>If you want the ultimate in performance and scalability, <strong><a href="https://planetscale.com/metal">try Metal today</a></strong>.</p></article></div>]]></description>
        </item>
    </channel>
</rss>