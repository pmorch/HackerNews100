<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 17 Nov 2024 22:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Humans have caused 1.5 °C of long-term global warming according to new estimates (350 pts)]]></title>
            <link>https://www.lancaster.ac.uk/news/humans-have-already-caused-15-c-of-long-term-global-warming-according-to-new-estimates</link>
            <guid>42166030</guid>
            <pubDate>Sun, 17 Nov 2024 18:49:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lancaster.ac.uk/news/humans-have-already-caused-15-c-of-long-term-global-warming-according-to-new-estimates">https://www.lancaster.ac.uk/news/humans-have-already-caused-15-c-of-long-term-global-warming-according-to-new-estimates</a>, See on <a href="https://news.ycombinator.com/item?id=42166030">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                    <img src="https://cisweb.lancaster.ac.uk/img/cwip/cisweb.lancaster.ac.uk/EventsMedia/earth-adobestock242647178-smaller-638669192826825152.jpg?mode=crop&amp;width=874&amp;height=492&amp;center=0.50%2c0.50" alt="Earth from space" loading="lazy">
                                        <p>A new study published today in Nature Geoscience by Dr Andrew Jarvis at Lancaster University and Professor Piers Forster at the University of Leeds shows that humans may have already caused 1.5 °C of global warming when measured from a time genuinely before the industrial revolution and the start of large-scale carbon emissions.</p><p>The Paris Climate Agreement from 2016 established a long-term temperature goal of “limiting global temperature increase well below 2 degrees Celsius, while pursuing efforts to limit the increase to 1.5 degrees.” The 1.5 °C of warming figure has since become the yardstick to judge progress, or the lack of it, on climate change.</p><p>The human-induced contribution to global warming is currently put at 1.31 °C, but with an uncertainty range of 1.10 to 1.60 °C, according to the Intergovernmental Panel on Climate Change (IPCC’s) preferred methods. This means it is unclear, from the IPCC’s adopted estimates, whether the 1.5 °C boundary has been breached or not. </p><p>Crucially, the IPCC’s preferred methods use temperature records from 1850-1900 as their ‘pre-industrial’ baseline for their calculations. They do this because this is when the first temperature records were taken, although the exact way to measure global temperature increases has never been defined within the climate negotiations. </p><p>Using this same 1850-1900 baseline, Dr Jarvis and Professor Forster’s method more than halves the uncertainty in the current human-caused warming estimate, thereby showing human-caused global warming currently remains below 1.5 °C if measured this way. On this measure, crossing the 1.5 °C Paris guardrail is under 10 years away at current warming rates. </p><p>However, Dr Jarvis and Professor Forster go further. Their method makes a more accurate estimate of the true long-term human contribution to global warming by pushing the base period from which the global temperature change is measured back to before 1700.</p><p>The authors find that when measured from this earlier more accurate definition of pre-industrial time, the long-term human contribution to warming was 1.49 °C ± 0.11 °C in 2023 and is now above 1.5 °C. This reveals that there is almost 0.2 °C of warming within the 1850-1900 baseline currently being used to define the warming.</p><p>Dr Jarvis, lead author of the study, said: “Measuring human-caused global warming is a difficult task because it forces us to compare today’s temperature with what it was in pre-industrial times – we call this the pre-industrial baseline. The closest we come to pre-industrial global temperature measurements are from the middle of the 1800’s, and unsurprisingly, these data are somewhat patchy and the Industrial Revolution was well underway by then. So using these early temperature data as a baseline as previous methods do not only ignores the warming that was already underway, it also bakes significant uncertainty into warming estimates.”</p><p>This new study instead uses CO<sub>2</sub> records from air bubbles trapped in ice-cores to establish a pre-1700 baseline for temperature. These records stretch back thousands of years, well before the Industrial Revolution and the effects of human-derived carbon emissions. The scientists are able to use the CO<sub>2</sub> record to anchor global warming estimates because of what they say is an overlooked relationship between the two.</p><p>“If you plot global temperatures against the concentration of CO<sub>2</sub> in the atmosphere, they both fall on a remarkably straight line, much straighter than current theory would predict,” said Dr Jarvis. “That line tells you not only how much the Earth has warmed since pre-industrial times, but also how much of that warming can be blamed on human activity.</p><p>“The climate is unimaginably complex, so perhaps it isn’t so surprising that such a direct method for accurately measuring the warming humans are responsible for has been overlooked,” Dr Jarvis added.</p><p>The scientists believe their new method is a strong candidate for measuring progress against the 1.5 and 2.0 degree Paris yardsticks.</p><p>Dr Jarvis said: “Our method has a number of strengths. Firstly, it directly tackles the problem of how to establish a robust pre-industrial baseline, although it functions equally well with the 1850-1900 baseline. Secondly, it produces estimates of human-caused warming that are at least 30 percent more certain than current methods. Finally, it is easy and quick to apply, meaning that we can produce warming estimates as soon as the CO<sub>2</sub> and temperature data become available without having to re-run complex climate models. This also means the results are transparent, making them far easier to communicate to non-specialists.”</p><p>Professor Forster, co-author of the study, said: “Our study shows that human societies have caused more than 1.5 degrees of long-term global warming. However, this does not necessarily mean that the Paris Agreement’s 1.5 temperature guardrail is breached, as we find that 0.18 °C of warming happened before global temperature records began, and this baked-in warming would not have been factored into the Paris Agreement.</p><p>“Policy makers set the Paris temperature goal to limit the devasting climate impacts many around the world are already experiencing. It was set to push countries to higher national ambition. It is clear we need to do more. The ambition set out in the goal for “pursuing efforts to limit the increase to 1.5 degrees” was not vainglorious when set, rather countries did not match it with their efforts. Urgent actions can slow warming rates and push back the time of breaching the Paris 1.5 degree limit. Although breaching the limit is now inevitable, delivering action commensurate with the noble Paris goal is more important than ever.” he added.</p><p>Although useful for measuring current levels of human-induced global warming, the researchers caution against using their method for making predictions on future warming.</p><p>Dr Jarvis, said: “Although atmospheric CO<sub>2</sub> is responsible for the bulk of human-induced warming so far, it is not solely responsible, and we know other factors such as methane could become increasingly important in the future, especially if we encounter climate tipping points. This means we need to keep an eye on our analysis. Fortunately, departures from the current linear regime could provide valuable early warning of such a change.”</p><p>The study is outlined in the paper ‘<a href="https://www.nature.com/articles/s41561-024-01580-5" rel="noopener noreferrer" target="_blank">Estimated human-induced warming from a linear temperature and atmospheric CO<sub>2</sub> relationship</a>’ published in Nature Geoscience.</p><p>DOI:<a href="https://www.nature.com/articles/s41561-024-01580-5" rel="noopener noreferrer" target="_blank">10.1038/s41561-024-01580-5</a></p>          <a href="https://www.lancaster.ac.uk/news/">Back to News</a>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Good Software Development Habits (196 pts)]]></title>
            <link>https://zarar.dev/good-software-development-habits/</link>
            <guid>42165057</guid>
            <pubDate>Sun, 17 Nov 2024 16:34:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zarar.dev/good-software-development-habits/">https://zarar.dev/good-software-development-habits/</a>, See on <a href="https://news.ycombinator.com/item?id=42165057">Hacker News</a></p>
<div id="readability-page-1" class="page">
  
  <header>
    <a href="https://zarar.dev/">
      <h2>
        Zarar's blog
      </h2>
    </a>
    <nav>
      <p><a href="https://zarar.dev/">Home</a>
<a href="https://zarar.dev/me/">Me</a>
<a href="https://zarar.dev/subscribe/">Subscribe</a></p>

    </nav>
  </header>
  <main>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2024-09-05T13:00Z">
                    05 Sep, 2024
                </time>
            </i>
        </p>
    

    <p>This post is not advice, it's what's working for me.</p>
<p>It's easy to pick up bad habits and hard to create good ones. Writing down what's working for me helps me maintain any good habits I've worked hard to develop. Here's an unordered list of 10 things that have helped me increase speed and maintain a respectable level of quality in the product I'm currently developing.</p>
<ol>
<li><p>Keep commits small enough that you wonder if you're taking this "keep commits small" thing a little too far. You just never know when you have to revert a particular change and there's a sense of bliss knowing where you introduced a bug six days ago and only reverting that commit without going through the savagery of merge conflicts. My rule of thumb: compiling software should be commitable.</p>
</li>
<li><p>Live Kent Beck's <a href="https://x.com/KentBeck/status/250733358307500032?lang=en">holy words of wisdom</a>: "for each desired change, make the change easy (warning: this may be hard), then make the easy change". Aim for at least half of all commits to be refactorings. Continuous refactoring is thinking of changes I can make in under 10 minutes that improve something. Doing this pays off whenever a bigger requirement comes in and you find yourself making a small change to satisfy it only because of those smaller improvements. Big refactorings are a bad idea.</p>
</li>
<li><p>All code is a liability. Undeployed code is the grim reaper of liabilities. I need to know if it works or at least doesn't break anything. Tests give you confidence, production gives you approval. The hosting costs might rack up a little with so many deploys but it's a small price to pay for knowing the last thing you did was a true sign of progression. <em>Working software is the primary measure of progress</em>, says one of the <a href="https://agilemanifesto.org/principles.html">agile principles</a>. Working and progress are doing a lot of heavy lifting in that sentence, so I've defined them for myself. Working is something being working enough to be deployed, and if it's code that's contributing to a capability, that's progress.</p>
</li>
<li><p>Know when you're testing the framework's capability. If you are, don't do it. The framework is already tested by people who know a lot more than you, and you have to trust them that the <code>useState()</code> hook does what it's supposed to do. If you keep components small, then you reduce the need for a lot of tests as the framework will be doing most of the heavy lifting in the component. If the component is big, then you introduce more complexity and now you need to write a lot of tests.</p>
</li>
<li><p>If a particular function doesn't fit anywhere, create a new module (or class or component) for it and you'll find a home for it later. It's better to create a new independent construct than to jam it into an existing module where you know deep down it doesn't make sense. Worst comes to worst, it lives as an independent module which isn't too bad anyway.</p>
</li>
<li><p>If you don't know what an API should look like, write the tests first as it'll force you to think of the "customer" which in this case is you. You'll invariably discover cases that you would not have thought of if you had just written the code first and tests after. You don't have to be religious about TDD and it's OK to work in larger batches (e.g., write more than just a couple lines of code before making it pass).  The amount of code to write in a red/failing state doesn't always have to be small. You know what you're doing, don't let dogma get in the way of productivity.</p>
</li>
<li><p>Copy-paste is OK once. The second time you're introducing duplication (i.e., three copies), don't. You should have enough data points to create a good enough abstraction. The risk of diverging implementations of the same thing is too high at this point, and consolidation is needed.  It's better to have some wonky parameterization than it is to have multiple implementations of nearly the same thing. Improving the parameters will be easier than to consolidate four different implementations if this situation comes up again.</p>
</li>
<li><p>Designs get stale. You can slow the rate at which they get stale by refactoring, but ultimately you'll need to change how things work. Don't feel too bad about moving away from something that was dear to you a while ago and something you felt proud about at the time.  You did the right thing then and shouldn't beat yourself up for not getting it right enough that you wouldn't need to change anything. Most of the time writing software is changing software. Just accept it and move on. There's no such thing as the perfect design, and change is at the core of software development. How good you are at changing things is how good you are at software development.</p>
</li>
<li><p>Technical debt can be classified into three main types: 1) things that are preventing you from doing stuff now, 2) things that will prevent you from doing stuff later, and 3) things that <em>might</em> prevent you from doing stuff later. Every other classification is a subset of these three. Minimize having lots of stuff in #1 and try to focus on #2. Ignore #3.</p>
</li>
<li><p>Testability is correlated with good design. Something not being easily testable hints that the design needs to be changed. Sometimes that design is your test design. As an example, if you find yourself finding it difficult to mock <code>em.getRepository(User).findOneOrFail({id})</code>, then chances are you either need to put that call into its own function that can be mocked, or write a test utility which allows for easier mocking of the entity manager methods. Tests go unwritten when it's hard to test, not because you don't want to test.</p>
</li>
</ol>
<p>There's probably a lot more, but 10 is a nice number.</p>


    

    
        

        
            


        
    


  </main>
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everything Is Just Functions: Mind-Blowing Insights from SICP and David Beazley (234 pts)]]></title>
            <link>https://ezzeriesa.notion.site/1-week-with-David-Beazley-and-SICP-4c440389cf1e43f48fe67c969967f655#58ee6b0435b24e26bd624b33ffed94df</link>
            <guid>42164541</guid>
            <pubDate>Sun, 17 Nov 2024 15:07:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ezzeriesa.notion.site/1-week-with-David-Beazley-and-SICP-4c440389cf1e43f48fe67c969967f655#58ee6b0435b24e26bd624b33ffed94df">https://ezzeriesa.notion.site/1-week-with-David-Beazley-and-SICP-4c440389cf1e43f48fe67c969967f655#58ee6b0435b24e26bd624b33ffed94df</a>, See on <a href="https://news.ycombinator.com/item?id=42164541">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The myth that you can’t build interactive web apps except as single page app (156 pts)]]></title>
            <link>https://htmx.org/essays/you-cant/</link>
            <guid>42164154</guid>
            <pubDate>Sun, 17 Nov 2024 13:44:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://htmx.org/essays/you-cant/">https://htmx.org/essays/you-cant/</a>, See on <a href="https://news.ycombinator.com/item?id=42164154">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

  
  
    <address>Tony Alaribe</address>
    <p><time>September 20, 2024</time></p><h3 id="an-ode-to-browser-advancements"><a href="#an-ode-to-browser-advancements" aria-label="Anchor link for: an-ode-to-browser-advancements">#</a><strong>An Ode to Browser Advancements.</strong></h3>
<p>I often encounter discussions on Reddit and YCombinator where newer developers seek tech stack advice. Inevitably,
someone claims it’s impossible to build a high-quality application without using a single-page application (SPA)
framework like React or AngularJS. This strikes me as odd because, even before the SPA revolution, many popular
multi-page web applications offered excellent user experiences.</p>
<p>Two years ago, I set out to build an <a rel="noopener" target="_blank" href="https://apitoolkit.io/">observability platform</a> and chose to experiment with a
multi-page application (MPA) approach using HTMX. I wondered: Would a server-rendered MPA be inadequate for a data-heavy
application, considering that most observability platforms are built on ReactJS?</p>
<p>What I discovered is that you can create outstanding server-rendered applications if you pay attention to certain
details.</p>
<p><strong>Here are some common MPA myths and what I’ve learned about them.</strong></p>
<h2 id="myth-1-mpa-page-transitions-are-slow-because-javascript-and-css-are-downloaded-on-every-page-navigation"><a href="#myth-1-mpa-page-transitions-are-slow-because-javascript-and-css-are-downloaded-on-every-page-navigation" aria-label="Anchor link for: myth-1-mpa-page-transitions-are-slow-because-javascript-and-css-are-downloaded-on-every-page-navigation">#</a>Myth 1:  MPA Page Transitions are slow because JavaScript and CSS are downloaded on every page navigation</h2>
<p>The perception that MPA page transitions are slow is widespread—and not entirely unfounded—since this is the default
behavior of browsers. However, browsers have made significant improvements over the past decade to mitigate this issue.</p>
<p>To illustrate, in the video below, a full page reload with the cache disabled takes 2.90 seconds until the
DOMContentLoaded event fires. I recorded this at a café with poor Wi-Fi, but let’s use this as a reference point. Keep
that number in mind.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/log-exp-cache.mp4">
</video>
<p>It is common to reduce load times in MPAs using libraries such as <strong>PJAX, Turbolinks, and even HTMX Boost</strong>. These
libraries hijack the page reload using Javascript and swap out only the HTML body element between transitions. That way,
most of the page’s head section assets don’t need to be reloaded or re-downloaded.</p>
<p>But there’s a lesser known way of reducing how much assets are re-downloaded or evaluated during page transitions.</p>
<h3 id="client-side-caching-via-service-workers"><a href="#client-side-caching-via-service-workers" aria-label="Anchor link for: client-side-caching-via-service-workers">#</a>Client-side Caching via Service workers</h3>
<p>Frontend developers who have built Progressive Web Applications (PWA) with SPA frameworks might know about service
workers.</p>
<p>For those of us who are not frontend or PWA developers, service workers are a built-in feature of browsers. They let you
write Javascript code that sits between your users and the network, intercepting requests and deciding how the browser
handles them.</p>
<p><img src="https://htmx.org/img/you-cant/service-worker-chart.png" alt="service-worker-chart.png"></p>
<p>Due to its association with the PWA trend, service workers are only ordinary among SPA developers, and developers need
to realize that this technology can also be used for regular Multi-Page Applications.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/log_exp_with_cache.mp4">
</video>
<p>In the video demonstration, we enable a service worker to cache and refresh the current page. You’ll notice that there’s
no flicker when clicking the link to reload the page, resulting in a smoother user experience.</p>
<p>Moreover, instead of transmitting over 2 MB of static assets as before, the browser now only fetches 84 KB of HTML
content—the actual page data. This optimization reduces the <code>DOMContentLoaded</code> event time from 2.9 seconds to under 500
milliseconds. Impressively, this improvement is achieved <strong>without</strong> using HTMX Boost, PJAX, or Turbolinks.</p>
<h3 id="how-to-implement-service-workers-in-your-multi-page-application"><a href="#how-to-implement-service-workers-in-your-multi-page-application" aria-label="Anchor link for: how-to-implement-service-workers-in-your-multi-page-application">#</a>How to Implement Service workers in Your Multi-Page Application</h3>
<p>You might be wondering how to replicate these performance gains in your own MPA. Here’s a simple guide:</p>
<ol>
<li><strong>Create a <code>sw.js</code> File</strong>: This is your service worker script that will manage caching and network requests.</li>
<li><strong>List Files to Cache</strong>: Within the service worker, specify all the assets (HTML, CSS, JavaScript, images) that
should be cached.</li>
<li><strong>Define Caching Strategies</strong>: Indicate how each type of asset should be cached—for example, whether they should be
cached permanently or refreshed periodically.</li>
</ol>
<p>By implementing a service worker, you effectively tell the browser how to handle network requests and caching, leading
to faster load times and a more seamless user experience.</p>
<h3 id="use-workbox-to-generate-service-workers"><a href="#use-workbox-to-generate-service-workers" aria-label="Anchor link for: use-workbox-to-generate-service-workers">#</a>Use Workbox to generate service workers</h3>
<p>While it’s possible to write service workers by hand—and there are excellent resources
like <a rel="noopener" target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/Service_Worker_API/Using_Service_Workers">this MDN article</a> to
help you—I prefer using Google’s <a rel="noopener" target="_blank" href="https://developer.chrome.com/docs/workbox">Workbox</a> library to automate the process.</p>
<h3 id="steps-to-use-workbox"><a href="#steps-to-use-workbox" aria-label="Anchor link for: steps-to-use-workbox">#</a>Steps to Use Workbox:</h3>
<ol>
<li>
<p><strong>Install Workbox</strong>: Install Workbox via npm or your preferred package manager:</p>
<pre data-lang="bash"><code data-lang="bash"><span>npm</span><span> install workbox-cli</span><span> --global
</span></code></pre>
</li>
<li>
<p>Generate a Workbox Configuration file: Run the following command to create a configuration file:</p>
<pre data-lang="bash"><code data-lang="bash"><span>workbox</span><span> wizard
</span></code></pre>
</li>
<li>
<p><strong>Configure Asset Handling</strong>: In the generated <code>workbox-config.js</code> file, define how different assets should be
cached. Use the <code>urlPattern</code> property—a regular expression—to match specific HTTP requests. For each matching
request, specify a caching strategy, such as <code>CacheFirst</code> or <code>NetworkFirst</code>.</p>
<p><img src="https://htmx.org/img/you-cant/workbox-cfg.png" alt="workbox-cfg.png"></p>
</li>
<li>
<p><strong>Build the Service Worker</strong>: Run the Workbox build command to generate the <code>sw.js</code> file based on your configuration:</p>
<pre data-lang="bash"><code data-lang="bash"><span>workbox</span><span> generateSW workbox-config.js
</span></code></pre>
</li>
<li>
<p><strong>Register the Service Worker in Your Application</strong>: Add the following script to your HTML pages to register the
service worker:</p>
<pre data-lang="html"><code data-lang="html"><span>&lt;</span><span>script</span><span>&gt;
</span><span>  </span><span>if </span><span>(</span><span>'serviceWorker' </span><span>in navigator) {
</span><span>    window.</span><span>addEventListener</span><span>(</span><span>'load'</span><span>, </span><span>function</span><span>() {
</span><span>      navigator.serviceWorker.</span><span>register</span><span>(</span><span>'/sw.js'</span><span>).</span><span>then</span><span>(</span><span>function</span><span>(registration) {
</span><span>        console.</span><span>log</span><span>(</span><span>'ServiceWorker registration successful with scope: '</span><span>, </span><span>registration</span><span>.scope);
</span><span>      }, </span><span>function</span><span>(err) {
</span><span>        console.</span><span>log</span><span>(</span><span>'ServiceWorker registration failed: '</span><span>, </span><span>err</span><span>);
</span><span>      });
</span><span>    });
</span><span>  }
</span><span>&lt;/</span><span>script</span><span>&gt;
</span></code></pre>
</li>
</ol>
<p>By following these steps, you instruct the browser to serve cached assets whenever possible, drastically reducing load
times and improving the overall performance of your multi-page application.</p>
<p><img src="https://htmx.org/img/you-cant/service-worker.png" alt="Image showing the registered service worker from the chrome browser console."></p>
<p>Image showing the registered service worker from the chrome browser console.</p>
<h3 id="speculation-rules-api-prerender-pages-for-instant-page-navigation"><a href="#speculation-rules-api-prerender-pages-for-instant-page-navigation" aria-label="Anchor link for: speculation-rules-api-prerender-pages-for-instant-page-navigation">#</a><a rel="noopener" target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/Speculation_Rules_API"><code>Speculation Rules API</code></a>: Prerender pages for instant page navigation.</h3>
<p>If you have used <strong>htmx-preload</strong> or <strong>instantpage.js,</strong> you’re familiar with prerendering and the problem
the <a rel="noopener" target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/Speculation_Rules_API">“Speculation Rules API”</a> aims to solve. The
Speculation Rules API is designed to improve performance for future navigations. It has an expressive syntax for
specifying which links should be prefetched or prerendered on the current page.</p>
<p><img src="https://htmx.org/img/you-cant/speculation-rules.png" alt="Speculation rules configuration example"></p>
<p>Speculation rules configuration example</p>
<p>The script above is an example of how speculation rules are configured. It is a Javascript object, and without going
into detail, you can see that it uses keywords such as “where,” “and,” “not,” etc. to describe what elements should
either be prefetched or prerendered.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/prerender-vid.mp4">
</video>
<p><a rel="noopener" target="_blank" href="https://developer.chrome.com/docs/web-platform/prerender-pages">Example impact of prerendering (Chrome Team)</a></p>
<h2 id="myth-2-mpas-can-t-operate-offline-and-save-updates-to-retry-when-there-s-network"><a href="#myth-2-mpas-can-t-operate-offline-and-save-updates-to-retry-when-there-s-network" aria-label="Anchor link for: myth-2-mpas-can-t-operate-offline-and-save-updates-to-retry-when-there-s-network">#</a>Myth 2: MPAs can’t operate offline and save updates to retry when there’s network</h2>
<p>From the last sections, you know that service workers can cache everything and make our apps operate entirely offline.
But what if we want to save offline POST requests and retry them when there is internet?</p>
<p><img src="https://htmx.org/img/you-cant/workbox-offline-cfg.png" alt="workbox-offline-cfg.png"></p>
<p>The configuration javascript file above shows how to configure Workbox to support two common offline scenarios. Here,
you see background Sync, where we ask the service worker to cache any failed requests due to the internet and retry it
for up to 24 hours.</p>
<p>Below, we define an offline catch Handler, triggered when a request is made offline. We can return a template partial
with HTML or a JSON response or dynamically build a response based on the request input. The sky is the limit here.</p>
<h2 id="myth-3-mpas-always-flash-white-during-page-transitions"><a href="#myth-3-mpas-always-flash-white-during-page-transitions" aria-label="Anchor link for: myth-3-mpas-always-flash-white-during-page-transitions">#</a>Myth 3: MPAs always flash white during page Transitions</h2>
<p>In the service worker videos, we already saw that this will not happen if we configure caching and prerendering.
However, this myth was not generally true until 2019. Since 2019, most browsers withhold painting the next screen until
all the required assets for the next page are available or a timeout is reached, resulting in no flash of white while
transitioning between both pages. This only works when navigating within the same origin/domain.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/paint-holding.mp4">
</video>
<p><a rel="noopener" target="_blank" href="https://developer.chrome.com/blog/paint-holding">Paint holding documentation on chrome.com</a>.</p>
<h2 id="myth-4-fancy-cross-document-page-transitions-are-not-possible-with-mpas"><a href="#myth-4-fancy-cross-document-page-transitions-are-not-possible-with-mpas" aria-label="Anchor link for: myth-4-fancy-cross-document-page-transitions-are-not-possible-with-mpas">#</a>Myth 4: Fancy Cross-document page transitions are not possible with MPAs.</h2>
<p>The advent of single-page application frameworks made custom transitions between pages more popular. The allure of
different navigation styles comes from completely taking control of page navigation from the browsers. In practice, such
transitions have mostly been popular within the demos at web dev conference talks.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/page-transitions.mp4">
</video>
<p><a rel="noopener" target="_blank" href="https://developer.chrome.com/docs/web-platform/view-transitions">Cross Document Transitions documentation on chrome.com</a>.</p>
<p>This remains a common argument for single-page applications, especially on Reddit and Hacker News comment sections.
However, browsers have been working towards solving this problem natively for the last couple of years. Chrome 126
rolled out cross-document view transitions. This means we can build our MPAs to include those fancy animations and
transitions between pages using CSS only or CSS and Javascript.</p>
<p>My favorite bit is that we might be able to create lovely cross-document transitions with CSS only:</p>
<p><img src="https://htmx.org/img/you-cant/cross-doc-transitions-css.png" alt="cross-doc-transitions-css.png"></p>
<p>You can quickly learn more on
the <a rel="noopener" target="_blank" href="https://developer.chrome.com/docs/web-platform/view-transitions">Google Chrome announcement page</a></p>
<p>This link hosts a <a rel="noopener" target="_blank" href="https://view-transitions.netlify.app/stack-navigator/mpa-prerender/">multi-page application demo</a>,
where you can play around with a rudimentary server-rendered application using the cross-document view transitions API
to simulate a stack-based animation.</p>
<h2 id="myth-5-with-htmx-or-mpas-every-user-action-must-happen-on-the-server"><a href="#myth-5-with-htmx-or-mpas-every-user-action-must-happen-on-the-server" aria-label="Anchor link for: myth-5-with-htmx-or-mpas-every-user-action-must-happen-on-the-server">#</a>Myth 5: With htmx or MPAs, every user action must happen on the server.</h2>
<p>I’ve heard this a lot when HTMX is discussed. So, there might be some confusion caused by the HTMX positioning. But you
don’t have to do everything server-side. Many HTMX and regular MPA users continue to use Javascript, Alpine, or
Hyperscript where appropriate.</p>
<p>In situations where robust interactivity is helpful, you can lean into the component islands architecture using
WebComponents or any javascript framework (react, angular, etc) of your choice. That way, instead of your entire
application being an SPA, you can leverage those frameworks specifically for the bits of your application that need that
interactivity.</p>
<p>The example above shows a very interactive search component in the <a rel="noopener" target="_blank" href="https://apitoolkit.io/">APItoolkit</a>. It’s a web
component implemented with lit-element, a zero-compile-step library for writing web components. So, the entire web
component event fits in a Javascript file.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/webcomponents-filter-element2.mp4">
</video>
<h2 id="myth-6-operating-directly-on-the-dom-is-slow-therefore-it-would-be-best-to-use-react-virtual-dom"><a href="#myth-6-operating-directly-on-the-dom-is-slow-therefore-it-would-be-best-to-use-react-virtual-dom" aria-label="Anchor link for: myth-6-operating-directly-on-the-dom-is-slow-therefore-it-would-be-best-to-use-react-virtual-dom">#</a>Myth 6: Operating directly on the DOM is slow. Therefore, it would be best to use React/Virtual DOM.</h2>
<p>The speed of direct DOM operations was a major motivation for building ReactJS on and popularizing the virtual DOM
technology. While virtual DOM operations can be faster than direct DOM operations, this is only true for applications
that perform many complex operations and refresh in milliseconds, where that performance might be noticeable. But most
of us are not building such software.</p>
<p>The Svelte team wrote an excellent article
titled <a rel="noopener" target="_blank" href="https://svelte.dev/blog/virtual-dom-is-pure-overhead">“Virtual DOM is pure Overhead.”</a> I recommend reading it,
as it better explains why Virtual DOM doesn’t matter for most applications.</p>
<h2 id="myth-7-you-still-need-to-write-javascript-for-every-minor-interactivity"><a href="#myth-7-you-still-need-to-write-javascript-for-every-minor-interactivity" aria-label="Anchor link for: myth-7-you-still-need-to-write-javascript-for-every-minor-interactivity">#</a>Myth 7: You still need to write JavaScript for every minor interactivity.</h2>
<p>With the advancements in browser tech, you can avoid writing a lot of client-side Javascript in the first place. For
example, a standard action on the web is to show and hide things based on a button click or toggle. These days, you can
show and hide elements with only CSS and HTML, for example, by using an HTML input checkbox to track state. We can style
an HTML label as a button and give it a <code>for="checkboxID</code>“ attribute, so clicking the label toggles the checkbox.</p>
<pre data-lang="jsx"><code data-lang="jsx"><span>&lt;input id="published" class="hidden peer" type="checkbox"/&gt;
</span><span>&lt;label for="published" class="btn"&gt;toggle content&lt;/label&gt;
</span><span>
</span><span>&lt;div class="hidden peer-checked:block"&gt;
</span><span>    Content to be toggled when label/btn is clicked
</span><span>&lt;/div&gt;
</span></code></pre>
<p>We can combine such a checkbox with HTMX intersect to fetch content from an endpoint when the button is clicked.</p>
<pre data-lang="html"><code data-lang="html"><span>&lt;</span><span>input </span><span>id</span><span>=</span><span>"published" </span><span>class</span><span>=</span><span>"peer" </span><span>type</span><span>=</span><span>"checkbox" </span><span>name</span><span>=</span><span>"status"</span><span>/&gt;
</span><span>&lt;</span><span>div
</span><span>        </span><span>class</span><span>=</span><span>"hidden peer-checked:block"
</span><span>        </span><span>hx-trigger</span><span>=</span><span>"intersect once"
</span><span>        </span><span>hx-get</span><span>=</span><span>"/log-item"
</span><span>&gt;Shell/Loading text etc
</span><span>&lt;/</span><span>div</span><span>&gt;
</span></code></pre>
<p>All the classes above are vanilla <a rel="noopener" target="_blank" href="https://tailwindcss.com/">Tailwind CSS</a> classes, but you can also write the CSS by
hand. Below is a video of that code being used to hide or reveal log items in the log explorer.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/expanding-log-item.mp4">
</video>
<h2 id="final-myth-without-a-proper-frontend-framework-your-client-side-javascript-will-be-spaghetti-and-unmaintainable"><a href="#final-myth-without-a-proper-frontend-framework-your-client-side-javascript-will-be-spaghetti-and-unmaintainable" aria-label="Anchor link for: final-myth-without-a-proper-frontend-framework-your-client-side-javascript-will-be-spaghetti-and-unmaintainable">#</a>Final Myth: Without a <em>“Proper”</em> frontend framework, your Client-side Javascript will be <a rel="noopener" target="_blank" href="https://www.reddit.com/r/webdev/comments/bkk0gl/avoiding_the_vanillajs_spaghetticode/">Spaghetti and Unmaintainable</a>.</h2>
<p>This may or may not be true.</p>
<h3 id="who-cares-i-love-spaghetti"><a href="#who-cares-i-love-spaghetti" aria-label="Anchor link for: who-cares-i-love-spaghetti">#</a>Who cares? I love Spaghetti.</h3>
<p>I like to argue that some of the most productive days of the web were the PHP and JQuery spaghetti days. A lot of
software was built at that time, including many of the popular internet brands we know today. Most of them were built as
so-called spaghetti codes, which helped them ship their products early and survive long enough to refactor and not be
spaghetti.</p>
<h2 id="conclusion"><a href="#conclusion" aria-label="Anchor link for: conclusion">#</a>Conclusion</h2>
<p>The entire point of this talk is to show you that a lot is possible with browsers in 2024. While we were not looking,
browsers have closed the gap and borrowed the best ideas from the single-page application revolution. For example,
WebComponents exist thanks to the lessons we learned from single-page applications.</p>
<p>So now, we can build very interactive, even offline web applications using mostly browser tools—HTML, CSS, maybe some
Javascript—and still not sacrifice much in terms of user experience.</p>
<h3>The browser has come a long way. Give it a chance!</h3>

  <p>
    &lt;/&gt;
  </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude AI built me a React app to compare maps side by side (167 pts)]]></title>
            <link>https://github.com/veloplanner/map-matrix</link>
            <guid>42164141</guid>
            <pubDate>Sun, 17 Nov 2024 13:39:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/veloplanner/map-matrix">https://github.com/veloplanner/map-matrix</a>, See on <a href="https://news.ycombinator.com/item?id=42164141">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Map Matrix</h2><a id="user-content-map-matrix" aria-label="Permalink: Map Matrix" href="#map-matrix"></a></p>
<p dir="auto">Compare multiple maps side by side. <a href="https://veloplanner.github.io/map-matrix/" rel="nofollow">Live demo</a></p>
<p dir="auto"><strong>This project was mostly generated by Claude AI.</strong></p>
<p dir="auto">I wanted to develop a simple tool that I needed for <a href="https://veloplanner.com/" rel="nofollow">veloplanner.com</a>. I thought about using this opportunity to try out Claude AI for coding a project from scratch. It worked surprisingly well! I was able to explain my idea and get a working prototype in a few hours. Most of the time I was just copying code from Claude and pasting it into the editor. Later, I started using Cursor AI (with claude-3.5-sonnet model) which improved the experience a lot.</p>
<p dir="auto">You can add custom map source by clicking the "Add Custom Source" button in the navbar.
Configuration is stored in the browser's local storage.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/veloplanner/map-matrix/blob/main/screenshot.png"><img src="https://github.com/veloplanner/map-matrix/raw/main/screenshot.png" alt="screenshot"></a></p>
<p dir="auto">Example of Cursor AI flow:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/veloplanner/map-matrix/blob/main/screenshot-cursor-ai.png"><img src="https://github.com/veloplanner/map-matrix/raw/main/screenshot-cursor-ai.png" alt="screenshot-cursor-ai.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>

</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cloudflare.com's Robots.txt (120 pts)]]></title>
            <link>https://www.cloudflare.com/robots.txt</link>
            <guid>42163883</guid>
            <pubDate>Sun, 17 Nov 2024 12:39:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cloudflare.com/robots.txt">https://www.cloudflare.com/robots.txt</a>, See on <a href="https://news.ycombinator.com/item?id=42163883">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Bpftune uses BPF to auto-tune Linux systems (182 pts)]]></title>
            <link>https://github.com/oracle/bpftune</link>
            <guid>42163597</guid>
            <pubDate>Sun, 17 Nov 2024 11:38:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/oracle/bpftune">https://github.com/oracle/bpftune</a>, See on <a href="https://news.ycombinator.com/item?id=42163597">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">bpftune - BPF driven auto-tuning</h2><a id="user-content-bpftune---bpf-driven-auto-tuning" aria-label="Permalink: bpftune - BPF driven auto-tuning" href="#bpftune---bpf-driven-auto-tuning"></a></p>
<p dir="auto">bpftune aims to provide lightweight, always-on auto-tuning of system
behaviour.  The key benefit it provides are</p>
<ul dir="auto">
<li>by using BPF observability features, we can continuously monitor
and adjust system behaviour</li>
<li>because we can observe system behaviour at a fine grain (rather
than using coarse system-wide stats), we can tune at a finer grain
too (individual socket policies, individual device policies etc)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">The problem</h2><a id="user-content-the-problem" aria-label="Permalink: The problem" href="#the-problem"></a></p>
<p dir="auto">The Linux kernel contains a large number of tunables; these
often take the form of sysctl(8) parameters, and are usually
introduced for situations where there is no one "right" answer
for a configuration choice.  The number of tunables available
is quite daunting.  On a 6.2 kernel we see</p>
<div data-snippet-clipboard-copy-content="# sysctl --all 2>/dev/null|wc -l
1624"><pre><code># sysctl --all 2&gt;/dev/null|wc -l
1624
</code></pre></div>
<p dir="auto"><a href="https://github.com/leandromoreira/linux-network-performance-parameters">See here for an excellent writeup on network-related tunables.</a>.</p>
<p dir="auto">At the same time, individual systems get a lot less care
and adminstrator attention than they used to; phrases like
"cattle not pets" exemplify this.  Given the modern cloud
architectures used for most deployments, most systems never
have any human adminstrator interaction after initial
provisioning; in fact given the scale requirements, this
is often an explicit design goal - "no ssh'ing in!".</p>
<p dir="auto">These two observations are not unrelated; in an earlier
era of fewer, larger systems, tuning by administrators was
more feasible.</p>
<p dir="auto">These trends - system complexity combined with minimal
admin interaction suggest a rethink in terms of tunable
management.</p>
<p dir="auto">A lot of lore accumulates around these tunables, and to help
clarify why we developed bpftune, we will use a straw-man
version of the approach taken with tunables:</p>
<p dir="auto">"find the set of magic numbers that will work for the
system forever"</p>
<p dir="auto">This is obviously a caricature of how administrators
approach the problem, but it does highlight a critical
implicit assumption - that systems are static.</p>
<p dir="auto">And that gets to the "BPF" in bpftune; BPF provides means
to carry out low-overhead observability of systems. So
not only can we observe the system and tune appropriately,
we can also observe the effect of that tuning and re-tune
if necessary.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key design principles</h2><a id="user-content-key-design-principles" aria-label="Permalink: Key design principles" href="#key-design-principles"></a></p>
<ul dir="auto">
<li>Minimize overhead.  Use observability features sparingly; do not
trace very high frequency events.</li>
<li>Be explicit about policy changes providing both a "what" - what
change was made - and a "why" - how does it help? syslog logging
makes policy actions explicit with explanations</li>
<li>Get out of the way of the administrator.  We can use BPF
observability to see if the admin sets tunable values that we
are auto-tuning; if they do, we need to get out of the way and
disable auto-tuning of the related feature set.</li>
<li>Don't replace tunables with more tunables! bpftune is designed to
be zero configuration; there are no options, and we try to avoid
magic numbers where possible.</li>
<li>Use push-pull approaches. For example, with tcp buffer sizing,
we often want to get out of the way of applications and bump
up tcp sndbuf and rcvbuf, but at a certain point we run the
risk of exhausting TCP memory.  We can however monitor if we
are approaching TCP memory pressure and if so we can tune down
values that we've tuned up.  In this way, we can let the system
find a balance between providing resources and exhausting them.
In some cases, we won't need to tune up values; they may be fine
as they are. But in other cases these limits block optimal performance,
and if they are raised safely - with awareness of global memory
limits - we can get out the way of improved performance.  Another
concern is that increasing buffer size leads to latency - to
handle that, we correlate buffer size changes and TCP smoothed
round-trip time; if the correlation between these exceeds a
threshold (0.7) we stop increasing buffer size.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Concepts</h2><a id="user-content-concepts" aria-label="Permalink: Concepts" href="#concepts"></a></p>
<p dir="auto">The key components are</p>
<ul dir="auto">
<li>
<p dir="auto">tuners: each tuner manages tunables and handles events sent
from BPF programs to userspace via the shared ring buffer.
Each tuner has an associated set of tunables that it manages.</p>
</li>
<li>
<p dir="auto">optional strategies: a tuner can specify multiple strategies;
after running for a while a strategy times out and we assess
if a better strategy is available.  Each strategy specifies a</p>
<ul dir="auto">
<li>name</li>
<li>description</li>
<li>timeout</li>
<li>evaluation function</li>
<li>set of BPF program names in tuner associated with strategy</li>
</ul>
<p dir="auto">Strategies are optional and should be set in the tuner init()
method via bpftune_strategies_add().  See test/strategy
for a coded example.  When a strategy times out, the various
evaluation functions are called and the highest-value evaluation
dictates the next stratgey.</p>
<p dir="auto">Strategies provide a way of providing multiple schemes for
auto-tuning the same set of tunables, where the choice is
guided by an evaluation of the effectiveness of the strategies.</p>
</li>
<li>
<p dir="auto">events specify a</p>
<ul dir="auto">
<li>tuner id: which tuner the event is destined for</li>
<li>a scenario: what happened</li>
<li>an associated netns (if supported)</li>
<li>information about the event (IP address etc)</li>
</ul>
</li>
<li>
<p dir="auto">the tuner then responds to the event guided by the active strategy;
increase or decrease a tunable value, etc.  Describing the event
in the log is key; this allows an admin to understand what
changed and why.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<ul dir="auto">
<li>bpftune is a daemon which manages a set of .so plugin tuners;
each of these is a shared object that is loaded on start-up.</li>
<li>tuners can be enabled or disabled; a tuner is automatically
disabled if the admin changes associated tunables manually.</li>
<li>tuners share a global BPF ring buffer which allows posting of
events from BPF programs to userspace.  For example, if the
sysctl tuner sees a systl being set, it posts an event.</li>
<li>each tuner has an associated id (set when it is loaded),
and events posted contain the tuner id.</li>
<li>each tuner has a BPF component (built using a BPF skeleton)
and a userspace component.  The latter has init(), fini()
and event_handler() entrypoints.  When an event is
received, the tuner id is used to identify the appropriate
event handler and its event_handler() callback function is run.</li>
<li>init, fini and event_handler functions are loaded from the
tuner .so object.</li>
<li>BPF components should include bpftune.bpf.h; it contains
the common map definitions (ringbuf, etc) and shared variables
such as learning rate and tuner ids that each tuner needs.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported tuners</h2><a id="user-content-supported-tuners" aria-label="Permalink: Supported tuners" href="#supported-tuners"></a></p>
<ul dir="auto">
<li>TCP connection tuner: auto-tune choice of congestion control algorithm.
See bpftune-tcp-conn (8).</li>
<li>neighbour table tuner: auto-tune neighbour table sizes by growing
tables when approaching full. See bpftune-neigh (8).</li>
<li>route table tuner: auto-tune route table size by growing tables
when approaching full.  See bpftune-route (8).</li>
<li>sysctl tuner: monitor sysctl setting and if it collides with an
auto-tuned sysctl value, disable the associated tuner.  See
bpftune-sysctl (8).</li>
<li>TCP buffer tuner: auto-tune max and initial buffer sizes.  See
bpftune-tcp-buffer (8).</li>
<li>net buffer tuner: auto-tune tunables related to core networking.
See bpftune-net-buffer (8).</li>
<li>netns tuner: notices addition and removal of network namespaces,
which helps power namespace awareness for bpftune as a whole.
Namespace awareness is important as we want to be able to auto-tune
containers also.  See bpftune-netns (8).</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Code organization</h2><a id="user-content-code-organization" aria-label="Permalink: Code organization" href="#code-organization"></a></p>
<p dir="auto">Both core bpftune.c and individual tuners use the libbpftune library.
It handles logging, tuner init/fini, and BPF init/fini.</p>
<p dir="auto">Each tuner shared object defines an init(), fini() and event_handler()
function. These respectively set up and clean up BPF and handle events
that originate from the BPF code.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">If building the repository manually, simply run</p>
<div data-snippet-clipboard-copy-content="$ make ; sudo make install"><pre><code>$ make ; sudo make install
</code></pre></div>
<p dir="auto">at the top-level of the repository.  bpftune also supports a</p>

<p dir="auto">target, which will make a bpftune RPM.  See ./buildrpm/bpftune.spec</p>
<p dir="auto">We can also build with non-standard libdir for distros which do not
use /usr/lib64 like CachyOS; in this case to install to /usr/lib
instead</p>
<div data-snippet-clipboard-copy-content="$ make libdir=lib
$ sudo make install libdir=lib"><pre><code>$ make libdir=lib
$ sudo make install libdir=lib
</code></pre></div>
<p dir="auto">To build the following packages are needed (names may vary by distro);</p>
<ul dir="auto">
<li>libbpf, libbpf-devel &gt;= 0.6</li>
<li>libcap-devel</li>
<li>bpftool &gt;= 4.18</li>
<li>libnl3-devel</li>
<li>clang &gt;= 11</li>
<li>llvm &gt;= 11</li>
<li>python3-docutils</li>
</ul>
<p dir="auto">From the kernel side, the kernel needs to support BPF ring buffer
(around the 5.6 kernel, though 5.4 is supported on Oracle Linux
as ring buffer support was backported), and kernel BTF is
required (CONFIG_DEBUG_INFO_BTF=y).  Verify /sys/kernel/btf/vmlinux
is present.</p>
<p dir="auto">To enable bpftune as a service</p>
<div data-snippet-clipboard-copy-content="$ sudo service bpftune start"><pre><code>$ sudo service bpftune start
</code></pre></div>
<p dir="auto">...and to enable it by default</p>
<div data-snippet-clipboard-copy-content="$ sudo systemctl enable bpftune"><pre><code>$ sudo systemctl enable bpftune
</code></pre></div>
<p dir="auto">bpftune logs to syslog so /var/log/messages will contain details
of any tuning carried out.</p>
<p dir="auto">bpftune can also be run in the foreground as a program; to redirect
output to stdout/stderr, run</p>

<p dir="auto">On exit, bpftune will summarize any tuning done.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tests</h2><a id="user-content-tests" aria-label="Permalink: Tests" href="#tests"></a></p>
<p dir="auto">Tests are supplied for each tuner in the tests/ subdirectory.
"make test" runs all the tests.  Tests use network namespaces
to simulate interactions with remote hosts. See ./TESTING.md
for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Does my system support bpftune?</h2><a id="user-content-does-my-system-support-bpftune" aria-label="Permalink: Does my system support bpftune?" href="#does-my-system-support-bpftune"></a></p>
<p dir="auto">Simply run "bpftune -S" to see:</p>
<div data-snippet-clipboard-copy-content="$ bpftune -S
bpftune works fully
bpftune supports per-netns policy (via netns cookie)"><pre><code>$ bpftune -S
bpftune works fully
bpftune supports per-netns policy (via netns cookie)
</code></pre></div>
<p dir="auto">Two aspects are important here</p>
<ul dir="auto">
<li>does the system support fentry/fexit etc? If so full support
is likely.</li>
<li>does the system support network namespace cookies? If so
per-network-namespace policy is supported.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<p dir="auto">Simply starting bpftune and observing changes made via /var/log/messages
can be instructive.  For example, on a standard VM with sysctl defaults,
I ran</p>

<p dir="auto">...and went about normal development activities such as cloning git
trees from upstream, building kernels, etc.  From the log we see
some of the adjustments bpftune made to accommodate these activities</p>
<div data-snippet-clipboard-copy-content="$ sudo grep bpftune /var/log/messages
...
Apr 19 16:14:59 bpftest bpftune[2778]: bpftune works fully
Apr 19 16:14:59 bpftest bpftune[2778]: bpftune supports per-netns policy (via netns cookie)
Apr 19 16:18:40 bpftest bpftune[2778]: Scenario 'specify bbr congestion control' occurred for tunable 'TCP congestion control' in global ns. Because loss rate has exceeded 1 percent for a connection, use bbr congestion control algorithm instead of default
Apr 19 16:18:40 bpftest bpftune[2778]: due to loss events for 145.40.68.75, specify 'bbr' congestion control algorithm
Apr 19 16:26:53 bpftest bpftune[2778]: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
Apr 19 16:26:53 bpftest bpftune[2778]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 6291456) -> (4096 131072 7864320)
Apr 19 16:26:53 bpftest bpftune[2778]: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
Apr 19 16:26:53 bpftest bpftune[2778]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 7864320) -> (4096 131072 9830400)
Apr 19 16:29:04 bpftest bpftune[2778]: Scenario 'specify bbr congestion control' occurred for tunable 'TCP congestion control' in global ns. Because loss rate has exceeded 1 percent for a connection, use bbr congestion control algorithm instead of default
Apr 19 16:29:04 bpftest bpftune[2778]: due to loss events for 140.91.12.81, specify 'bbr' congestion control algorithm"><pre><code>$ sudo grep bpftune /var/log/messages
...
Apr 19 16:14:59 bpftest bpftune[2778]: bpftune works fully
Apr 19 16:14:59 bpftest bpftune[2778]: bpftune supports per-netns policy (via netns cookie)
Apr 19 16:18:40 bpftest bpftune[2778]: Scenario 'specify bbr congestion control' occurred for tunable 'TCP congestion control' in global ns. Because loss rate has exceeded 1 percent for a connection, use bbr congestion control algorithm instead of default
Apr 19 16:18:40 bpftest bpftune[2778]: due to loss events for 145.40.68.75, specify 'bbr' congestion control algorithm
Apr 19 16:26:53 bpftest bpftune[2778]: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
Apr 19 16:26:53 bpftest bpftune[2778]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 6291456) -&gt; (4096 131072 7864320)
Apr 19 16:26:53 bpftest bpftune[2778]: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
Apr 19 16:26:53 bpftest bpftune[2778]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 7864320) -&gt; (4096 131072 9830400)
Apr 19 16:29:04 bpftest bpftune[2778]: Scenario 'specify bbr congestion control' occurred for tunable 'TCP congestion control' in global ns. Because loss rate has exceeded 1 percent for a connection, use bbr congestion control algorithm instead of default
Apr 19 16:29:04 bpftest bpftune[2778]: due to loss events for 140.91.12.81, specify 'bbr' congestion control algorithm
</code></pre></div>
<p dir="auto">To deterministically trigger bpftune behaviour, one approach we can
take is to download a large file with inappropriate settings.</p>
<p dir="auto">In one window, set tcp rmem max to a too-low value, and run bpftune
as a program logging to stdout/stderr (-s):</p>
<div data-snippet-clipboard-copy-content="$ sudo sysctl -w net.ipv4.tcp_rmem=&quot;4096 131072 1310720&quot;
net.ipv4.tcp_rmem = 4096 131072 1310720
$ sudo bpftune -s"><pre><code>$ sudo sysctl -w net.ipv4.tcp_rmem="4096 131072 1310720"
net.ipv4.tcp_rmem = 4096 131072 1310720
$ sudo bpftune -s
</code></pre></div>
<p dir="auto">In another window, wget a large file:</p>
<div data-snippet-clipboard-copy-content="$ wget https://yum.oracle.com/ISOS/OracleLinux/OL8/u7/x86_64/OracleLinux-R8-U7-x86_64-dvd.iso"><pre><code>$ wget https://yum.oracle.com/ISOS/OracleLinux/OL8/u7/x86_64/OracleLinux-R8-U7-x86_64-dvd.iso
</code></pre></div>
<p dir="auto">In the first window, we see bpftune tuning up rmem:</p>
<div data-snippet-clipboard-copy-content="bpftune: bpftune works in legacy mode
bpftune: bpftune does not support per-netns policy (via netns cookie)
bpftune: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
bpftune: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 1310720) -> (4096 131072 1638400)"><pre><code>bpftune: bpftune works in legacy mode
bpftune: bpftune does not support per-netns policy (via netns cookie)
bpftune: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
bpftune: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 1310720) -&gt; (4096 131072 1638400)
</code></pre></div>
<p dir="auto">This occurs multiple times, and on exit (Ctrl+C) we see
the summary of changes made:</p>
<div data-snippet-clipboard-copy-content="bpftune: Summary: scenario 'need to increase TCP buffer size(s)' occurred 9 times for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
bpftune: sysctl 'net.ipv4.tcp_rmem' changed from (4096 131072 1310720 ) -> (4096 131072 9765625 )"><pre><code>bpftune: Summary: scenario 'need to increase TCP buffer size(s)' occurred 9 times for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
bpftune: sysctl 'net.ipv4.tcp_rmem' changed from (4096 131072 1310720 ) -&gt; (4096 131072 9765625 )
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">For more info</h2><a id="user-content-for-more-info" aria-label="Permalink: For more info" href="#for-more-info"></a></p>
<p dir="auto">See the docs/ subdirectory for manual pages covering bpftune
and associated tuners.</p>
<p dir="auto">bpftune was presented at the eBPF summit; <a href="https://www.youtube.com/watch?v=X0TvfH8hrQE&amp;t=420s" rel="nofollow">video here</a>.</p>
<p dir="auto">bpftune <a href="https://www.youtube.com/watch?v=3ylmGE6sW8w" rel="nofollow">was also discussed on Liz Rice's excellent eCHO eBPF podcast</a>, specifically in the context of using reinforcement learning in BPF</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">This project welcomes contributions from the community. Before submitting a pull request, please <a href="https://github.com/oracle/bpftune/blob/main/CONTRIBUTING.md">review our contribution guide</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security</h2><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<p dir="auto">Please consult the <a href="https://github.com/oracle/bpftune/blob/main/SECURITY.md">security guide</a> for our responsible security vulnerability disclosure process</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Copyright (c) 2023 Oracle and/or its affiliates.</p>
<p dir="auto">This software is available to you under</p>
<p dir="auto">SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note</p>
<p dir="auto">Being under the terms of the GNU General Public License version 2.</p>
<p dir="auto">SPDX-URL: <a href="https://spdx.org/licenses/GPL-2.0.html" rel="nofollow">https://spdx.org/licenses/GPL-2.0.html</a></p>
<p dir="auto">See <a href="https://github.com/oracle/bpftune/blob/main/LICENSE.txt">the license file</a> for more details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Garak, LLM Vulnerability Scanner (113 pts)]]></title>
            <link>https://github.com/NVIDIA/garak</link>
            <guid>42163591</guid>
            <pubDate>Sun, 17 Nov 2024 11:37:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/NVIDIA/garak">https://github.com/NVIDIA/garak</a>, See on <a href="https://news.ycombinator.com/item?id=42163591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">garak, LLM vulnerability scanner</h2><a id="user-content-garak-llm-vulnerability-scanner" aria-label="Permalink: garak, LLM vulnerability scanner" href="#garak-llm-vulnerability-scanner"></a></p>
<p dir="auto"><em>Generative AI Red-teaming &amp; Assessment Kit</em></p>
<p dir="auto"><code>garak</code> checks if an LLM can be made to fail in a way we don't want. <code>garak</code> probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know <code>nmap</code>, it's <code>nmap</code> for LLMs.</p>
<p dir="auto"><code>garak</code> focuses on ways of making an LLM or dialog system fail. It combines static, dyanmic, and adaptive probes to explore this.</p>
<p dir="auto"><code>garak</code>'s a free tool. We love developing it and are always interested in adding functionality to support applications.</p>
<p dir="auto"><a href="https://opensource.org/licenses/Apache-2.0" rel="nofollow"><img src="https://camo.githubusercontent.com/5ce2e21e84680df1ab24807babebc3417d27d66e0826a350eb04ab57f4c8f3e5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368655f322e302d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache_2.0-blue.svg"></a>
<a href="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml"><img src="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml/badge.svg" alt="Tests/Linux"></a>
<a href="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml"><img src="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml/badge.svg" alt="Tests/Windows"></a>
<a href="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml"><img src="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml/badge.svg" alt="Tests/OSX"></a>
<a href="http://garak.readthedocs.io/en/latest/?badge=latest" rel="nofollow"><img src="https://camo.githubusercontent.com/ec7dff6db1b623f10238aaa176f6070b8dfee2ba106479e9ac7a66fbe8f3e778/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f676172616b2f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/garak/badge/?version=latest"></a>
<a href="https://discord.gg/uVch4puUCs" rel="nofollow"><img src="https://camo.githubusercontent.com/3dfa2e5918dc7c5299e3f3e8383c6d7fc9e5a26de70d29c5144a166075db153b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d6f6e253230646973636f72642d79656c6c6f772e737667" alt="discord-img" data-canonical-src="https://img.shields.io/badge/chat-on%20discord-yellow.svg"></a>
<a href="https://github.com/psf/black"><img src="https://camo.githubusercontent.com/5bf9e9fa18966df7cb5fac7715bef6b72df15e01a6efa9d616c83f9fcb527fe2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg"></a>
<a href="https://pypi.org/project/garak" rel="nofollow"><img src="https://camo.githubusercontent.com/2e89cf4e24191c2b133e3cbc641eb89ec3d1c6e61bfec492ddec940757b871f3/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f676172616b" alt="PyPI - Python Version" data-canonical-src="https://img.shields.io/pypi/pyversions/garak"></a>
<a href="https://badge.fury.io/py/garak" rel="nofollow"><img src="https://camo.githubusercontent.com/635b443d53cf2ab927beabd3255f7895db7b74af3dbf5a7777c61fcc96792bbd/68747470733a2f2f62616467652e667572792e696f2f70792f676172616b2e737667" alt="PyPI" data-canonical-src="https://badge.fury.io/py/garak.svg"></a>
<a href="https://pepy.tech/project/garak" rel="nofollow"><img src="https://camo.githubusercontent.com/b38370d22779880d75968e06cf8ade053d44a0e95f94649a8354756b5c24a4ba/68747470733a2f2f706570792e746563682f62616467652f676172616b" alt="Downloads" data-canonical-src="https://pepy.tech/badge/garak"></a>
<a href="https://pepy.tech/project/garak" rel="nofollow"><img src="https://camo.githubusercontent.com/3d929a651aaa7c9b44b580d1537bbb7954b796081bfeb1125fc3d1fac4f78585/68747470733a2f2f706570792e746563682f62616467652f676172616b2f6d6f6e7468" alt="Downloads" data-canonical-src="https://pepy.tech/badge/garak/month"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get started</h2><a id="user-content-get-started" aria-label="Permalink: Get started" href="#get-started"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">&gt; See our user guide! <a href="https://docs.garak.ai/" rel="nofollow">docs.garak.ai</a></h3><a id="user-content--see-our-user-guide-docsgarakai" aria-label="Permalink: > See our user guide! docs.garak.ai" href="#-see-our-user-guide-docsgarakai"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">&gt; Join our <a href="https://discord.gg/uVch4puUCs" rel="nofollow">Discord</a>!</h3><a id="user-content--join-our-discord" aria-label="Permalink: > Join our Discord!" href="#-join-our-discord"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">&gt; Project links &amp; home: <a href="https://garak.ai/" rel="nofollow">garak.ai</a></h3><a id="user-content--project-links--home-garakai" aria-label="Permalink: > Project links &amp; home: garak.ai" href="#-project-links--home-garakai"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">&gt; Twitter: <a href="https://twitter.com/garak_llm" rel="nofollow">@garak_llm</a></h3><a id="user-content--twitter-garak_llm" aria-label="Permalink: > Twitter: @garak_llm" href="#-twitter-garak_llm"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">&gt; DEF CON <a href="https://garak.ai/garak_aiv_slides.pdf" rel="nofollow">slides</a>!</h3><a id="user-content--def-con-slides" aria-label="Permalink: > DEF CON slides!" href="#-def-con-slides"></a></p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">LLM support</h2><a id="user-content-llm-support" aria-label="Permalink: LLM support" href="#llm-support"></a></p>
<p dir="auto">currently supports:</p>
<ul dir="auto">
<li><a href="https://huggingface.co/models" rel="nofollow">hugging face hub</a> generative models</li>
<li><a href="https://replicate.com/" rel="nofollow">replicate</a> text models</li>
<li><a href="https://platform.openai.com/docs/introduction" rel="nofollow">openai api</a> chat &amp; continuation models</li>
<li><a href="https://www.litellm.ai/" rel="nofollow">litellm</a></li>
<li>pretty much anything accessible via REST</li>
<li>gguf models like <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> version &gt;= 1046</li>
<li>.. and many more LLMs!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install:</h2><a id="user-content-install" aria-label="Permalink: Install:" href="#install"></a></p>
<p dir="auto"><code>garak</code> is a command-line tool. It's developed in Linux and OSX.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Standard install with <code>pip</code></h3><a id="user-content-standard-install-with-pip" aria-label="Permalink: Standard install with pip" href="#standard-install-with-pip"></a></p>
<p dir="auto">Just grab it from PyPI and you should be good to go:</p>
<div data-snippet-clipboard-copy-content="python -m pip install -U garak"><pre><code>python -m pip install -U garak
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Install development version with <code>pip</code></h3><a id="user-content-install-development-version-with-pip" aria-label="Permalink: Install development version with pip" href="#install-development-version-with-pip"></a></p>
<p dir="auto">The standard pip version of <code>garak</code> is updated periodically. To get a fresher version, from GitHub, try:</p>
<div data-snippet-clipboard-copy-content="python -m pip install -U git+https://github.com/NVIDIA/garak.git@main"><pre><code>python -m pip install -U git+https://github.com/NVIDIA/garak.git@main
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Clone from source</h3><a id="user-content-clone-from-source" aria-label="Permalink: Clone from source" href="#clone-from-source"></a></p>
<p dir="auto"><code>garak</code> has its own dependencies. You can to install <code>garak</code> in its own Conda environment:</p>
<div data-snippet-clipboard-copy-content="conda create --name garak &quot;python>=3.10,<=3.12&quot;
conda activate garak
gh repo clone NVIDIA/garak
cd garak
python -m pip install -e ."><pre><code>conda create --name garak "python&gt;=3.10,&lt;=3.12"
conda activate garak
gh repo clone NVIDIA/garak
cd garak
python -m pip install -e .
</code></pre></div>
<p dir="auto">OK, if that went fine, you're probably good to go!</p>
<p dir="auto"><strong>Note</strong>: if you cloned before the move to the <code>NVIDIA</code> GitHub organisation, but you're reading this at the <code>github.com/NVIDIA</code> URI, please update your remotes as follows:</p>
<div data-snippet-clipboard-copy-content="git remote set-url origin https://github.com/NVIDIA/garak.git"><pre><code>git remote set-url origin https://github.com/NVIDIA/garak.git
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto">The general syntax is:</p>
<p dir="auto"><code>garak &lt;options&gt;</code></p>
<p dir="auto"><code>garak</code> needs to know what model to scan, and by default, it'll try all the probes it knows on that model, using the vulnerability detectors recommended by each probe. You can see a list of probes using:</p>
<p dir="auto"><code>garak --list_probes</code></p>
<p dir="auto">To specify a generator, use the <code>--model_type</code> and, optionally, the <code>--model_name</code> options. Model type specifies a model family/interface; model name specifies the exact model to be used. The "Intro to generators" section below describes some of the generators supported. A straightforward generator family is Hugging Face models; to load one of these, set <code>--model_type</code> to <code>huggingface</code> and <code>--model_name</code> to the model's name on Hub (e.g. <code>"RWKV/rwkv-4-169m-pile"</code>). Some generators might need an API key to be set as an environment variable, and they'll let you know if they need that.</p>
<p dir="auto"><code>garak</code> runs all the probes by default, but you can be specific about that too. <code>--probes promptinject</code> will use only the <a href="https://github.com/agencyenterprise/promptinject">PromptInject</a> framework's methods, for example. You can also specify one specific plugin instead of a plugin family by adding the plugin name after a <code>.</code>; for example, <code>--probes lmrc.SlurUsage</code> will use an implementation of checking for models generating slurs based on the <a href="https://arxiv.org/abs/2303.18190" rel="nofollow">Language Model Risk Cards</a> framework.</p>
<p dir="auto">For help &amp; inspiration, find us on <a href="https://twitter.com/garak_llm" rel="nofollow">twitter</a> or <a href="https://discord.gg/uVch4puUCs" rel="nofollow">discord</a>!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">Probe ChatGPT for encoding-based prompt injection (OSX/*nix) (replace example value with a real OpenAI API key)</p>
<div data-snippet-clipboard-copy-content="export OPENAI_API_KEY=&quot;sk-123XXXXXXXXXXXX&quot;
python3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding"><pre><code>export OPENAI_API_KEY="sk-123XXXXXXXXXXXX"
python3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding
</code></pre></div>
<p dir="auto">See if the Hugging Face version of GPT2 is vulnerable to DAN 11.0</p>
<div data-snippet-clipboard-copy-content="python3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0"><pre><code>python3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Reading the results</h2><a id="user-content-reading-the-results" aria-label="Permalink: Reading the results" href="#reading-the-results"></a></p>
<p dir="auto">For each probe loaded, garak will print a progress bar as it generates. Once generation is complete, a row evaluating that probe's results on each detector is given. If any of the prompt attempts yielded an undesirable behavior, the response will be marked as FAIL, and the failure rate given.</p>
<p dir="auto">Here are the results with the <code>encoding</code> module on a GPT-3 variant:
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3c772412f9310195163d6092ba995d436ebad8d7e430d89a8484c3a92b5ec972/68747470733a2f2f692e696d6775722e636f6d2f3844786634354e2e706e67"><img src="https://camo.githubusercontent.com/3c772412f9310195163d6092ba995d436ebad8d7e430d89a8484c3a92b5ec972/68747470733a2f2f692e696d6775722e636f6d2f3844786634354e2e706e67" alt="alt text" data-canonical-src="https://i.imgur.com/8Dxf45N.png"></a></p>
<p dir="auto">And the same results for ChatGPT:
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5fc7c4aee43ab989750750ea912245acd367d41dee20f0532d19d8bcce9d3a5e/68747470733a2f2f692e696d6775722e636f6d2f564b41463569662e706e67"><img src="https://camo.githubusercontent.com/5fc7c4aee43ab989750750ea912245acd367d41dee20f0532d19d8bcce9d3a5e/68747470733a2f2f692e696d6775722e636f6d2f564b41463569662e706e67" alt="alt text" data-canonical-src="https://i.imgur.com/VKAF5if.png"></a></p>
<p dir="auto">We can see that the more recent model is much more susceptible to encoding-based injection attacks, where text-babbage-001 was only found to be vulnerable to quoted-printable and MIME encoding injections.  The figures at the end of each row, e.g. 840/840, indicate the number of text generations total and then how many of these seemed to behave OK. The figure can be quite high because more than one generation is made per prompt - by default, 10.</p>
<p dir="auto">Errors go in <code>garak.log</code>; the run is logged in detail in a <code>.jsonl</code> file specified at analysis start &amp; end. There's a basic analysis script in <code>analyse/analyse_log.py</code> which will output the probes and prompts that led to the most hits.</p>
<p dir="auto">Send PRs &amp; open issues. Happy hunting!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Intro to generators</h2><a id="user-content-intro-to-generators" aria-label="Permalink: Intro to generators" href="#intro-to-generators"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hugging Face</h3><a id="user-content-hugging-face" aria-label="Permalink: Hugging Face" href="#hugging-face"></a></p>
<p dir="auto">Using the Pipeline API:</p>
<ul dir="auto">
<li><code>--model_type huggingface</code> (for transformers models to run locally)</li>
<li><code>--model_name</code> - use the model name from Hub. Only generative models will work. If it fails and shouldn't, please open an issue and paste in the command you tried + the exception!</li>
</ul>
<p dir="auto">Using the Inference API:</p>
<ul dir="auto">
<li><code>--model_type huggingface.InferenceAPI</code> (for API-based model access)</li>
<li><code>--model_name</code> - the model name from Hub, e.g. <code>"mosaicml/mpt-7b-instruct"</code></li>
</ul>
<p dir="auto">Using private endpoints:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>--model_type huggingface.InferenceEndpoint</code> (for private endpoints)</p>
</li>
<li>
<p dir="auto"><code>--model_name</code> - the endpoint URL, e.g. <code>https://xxx.us-east-1.aws.endpoints.huggingface.cloud</code></p>
</li>
<li>
<p dir="auto">(optional) set the <code>HF_INFERENCE_TOKEN</code> environment variable to a Hugging Face API token with the "read" role; see <a href="https://huggingface.co/settings/tokens" rel="nofollow">https://huggingface.co/settings/tokens</a> when logged in</p>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">OpenAI</h3><a id="user-content-openai" aria-label="Permalink: OpenAI" href="#openai"></a></p>
<ul dir="auto">
<li><code>--model_type openai</code></li>
<li><code>--model_name</code> - the OpenAI model you'd like to use. <code>gpt-3.5-turbo-0125</code> is fast and fine for testing.</li>
<li>set the <code>OPENAI_API_KEY</code> environment variable to your OpenAI API key (e.g. "sk-19763ASDF87q6657"); see <a href="https://platform.openai.com/account/api-keys" rel="nofollow">https://platform.openai.com/account/api-keys</a> when logged in</li>
</ul>
<p dir="auto">Recognised model types are whitelisted, because the plugin needs to know which sub-API to use. Completion or ChatCompletion models are OK. If you'd like to use a model not supported, you should get an informative error message, and please send a PR / open an issue.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Replicate</h3><a id="user-content-replicate" aria-label="Permalink: Replicate" href="#replicate"></a></p>
<ul dir="auto">
<li>set the <code>REPLICATE_API_TOKEN</code> environment variable to your Replicate API token, e.g. "r8-123XXXXXXXXXXXX"; see <a href="https://replicate.com/account/api-tokens" rel="nofollow">https://replicate.com/account/api-tokens</a> when logged in</li>
</ul>
<p dir="auto">Public Replicate models:</p>
<ul dir="auto">
<li><code>--model_type replicate</code></li>
<li><code>--model_name</code> - the Replicate model name and hash, e.g. <code>"stability-ai/stablelm-tuned-alpha-7b:c49dae36"</code></li>
</ul>
<p dir="auto">Private Replicate endpoints:</p>
<ul dir="auto">
<li><code>--model_type replicate.InferenceEndpoint</code> (for private endpoints)</li>
<li><code>--model_name</code> - username/model-name slug from the deployed endpoint, e.g. <code>elim/elims-llama2-7b</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Cohere</h3><a id="user-content-cohere" aria-label="Permalink: Cohere" href="#cohere"></a></p>
<ul dir="auto">
<li><code>--model_type cohere</code></li>
<li><code>--model_name</code> (optional, <code>command</code> by default) - The specific Cohere model you'd like to test</li>
<li>set the <code>COHERE_API_KEY</code> environment variable to your Cohere API key, e.g. "aBcDeFgHiJ123456789"; see <a href="https://dashboard.cohere.ai/api-keys" rel="nofollow">https://dashboard.cohere.ai/api-keys</a> when logged in</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Groq</h3><a id="user-content-groq" aria-label="Permalink: Groq" href="#groq"></a></p>
<ul dir="auto">
<li><code>--model_type groq</code></li>
<li><code>--model_name</code> - The name of the model to access via the Groq API</li>
<li>set the <code>GROQ_API_KEY</code> environment variable to your Groq API key, see <a href="https://console.groq.com/docs/quickstart" rel="nofollow">https://console.groq.com/docs/quickstart</a> for details on creating an API key</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">ggml</h3><a id="user-content-ggml" aria-label="Permalink: ggml" href="#ggml"></a></p>
<ul dir="auto">
<li><code>--model_type ggml</code></li>
<li><code>--model_name</code> - The path to the ggml model you'd like to load, e.g. <code>/home/leon/llama.cpp/models/7B/ggml-model-q4_0.bin</code></li>
<li>set the <code>GGML_MAIN_PATH</code> environment variable to the path to your ggml <code>main</code> executable</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">REST</h3><a id="user-content-rest" aria-label="Permalink: REST" href="#rest"></a></p>
<p dir="auto"><code>rest.RestGenerator</code> is highly flexible and can connect to any REST endpoint that returns plaintext or JSON. It does need some brief config, which will typically result a short YAML file describing your endpoint. See <a href="https://reference.garak.ai/en/latest/garak.generators.rest.html" rel="nofollow">https://reference.garak.ai/en/latest/garak.generators.rest.html</a> for examples.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">NIM</h3><a id="user-content-nim" aria-label="Permalink: NIM" href="#nim"></a></p>
<p dir="auto">Use models from <a href="https://build.nvidia.com/" rel="nofollow">https://build.nvidia.com/</a> or other NIM endpoints.</p>
<ul dir="auto">
<li>set the <code>NIM_API_KEY</code> environment variable to your authentication API token, or specify it in the config YAML</li>
</ul>
<p dir="auto">For chat models:</p>
<ul dir="auto">
<li><code>--model_type nim</code></li>
<li><code>--model_name</code> - the NIM <code>model</code> name, e.g. <code>meta/llama-3.1-8b-instruct</code></li>
</ul>
<p dir="auto">For completion models:</p>
<ul dir="auto">
<li><code>--model_type nim.NVOpenAICompletion</code></li>
<li><code>--model_name</code> - the NIM <code>model</code> name, e.g. <code>bigcode/starcoder2-15b</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">OctoAI</h3><a id="user-content-octoai" aria-label="Permalink: OctoAI" href="#octoai"></a></p>
<ul dir="auto">
<li>set the <code>OCTO_API_TOKEN</code> environment variable to your Replicate API token, e.g. "r8-123XXXXXXXXXXXX"; see <a href="https://replicate.com/account/api-tokens" rel="nofollow">https://replicate.com/account/api-tokens</a> when logged in</li>
</ul>
<p dir="auto">Octo public endpoint:</p>
<ul dir="auto">
<li><code>--model_type octo</code></li>
<li><code>--model_name</code> - the OctoAI public endpoint for the model, e.g. <code>mistral-7b-instruct-fp16</code></li>
</ul>
<p dir="auto">Octo private endpoint:</p>
<ul dir="auto">
<li><code>--model_type octo.InferenceEndpoint</code> (for private endpoints)</li>
<li><code>--model_name</code> - the deployed endpoint URL, e.g. <code>https://llama-2-70b-chat-xxx.octoai.run/v1/chat/completions</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Test</h3><a id="user-content-test" aria-label="Permalink: Test" href="#test"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><code>--model_type test</code></p>
</li>
<li>
<p dir="auto">(alternatively) <code>--model_name test.Blank</code>
For testing. This always generates the empty string, using the <code>test.Blank</code> generator.  Will be marked as failing for any tests that <em>require</em> an output, e.g. those that make contentious claims and expect the model to refute them in order to pass.</p>
</li>
<li>
<p dir="auto"><code>--model_type test.Repeat</code>
For testing. This generator repeats back the prompt it received.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Intro to probes</h2><a id="user-content-intro-to-probes" aria-label="Permalink: Intro to probes" href="#intro-to-probes"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Probe</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>blank</td>
<td>A simple probe that always sends an empty prompt.</td>
</tr>
<tr>
<td>atkgen</td>
<td>Automated Attack Generation. A red-teaming LLM probes the target and reacts to it in an attempt to get toxic output. Prototype, mostly stateless, for now uses a simple GPT-2 <a href="https://huggingface.co/garak-llm/artgpt2tox" rel="nofollow">fine-tuned</a> on the subset of hhrlhf attempts that yielded detectable toxicity (the only target currently supported for now).</td>
</tr>
<tr>
<td>av_spam_scanning</td>
<td>Probes that attempt to make the model output malicious content signatures</td>
</tr>
<tr>
<td>continuation</td>
<td>Probes that test if the model will continue a probably undesirable word</td>
</tr>
<tr>
<td>dan</td>
<td>Various <a href="https://adguard.com/en/blog/chatgpt-dan-prompt-abuse.html" rel="nofollow">DAN</a> and DAN-like attacks</td>
</tr>
<tr>
<td>donotanswer</td>
<td>Prompts to which responsible language models should not answer.</td>
</tr>
<tr>
<td>encoding</td>
<td>Prompt injection through text encoding</td>
</tr>
<tr>
<td>gcg</td>
<td>Disrupt a system prompt by appending an adversarial suffix.</td>
</tr>
<tr>
<td>glitch</td>
<td>Probe model for glitch tokens that provoke unusual behavior.</td>
</tr>
<tr>
<td>grandma</td>
<td>Appeal to be reminded of one's grandmother.</td>
</tr>
<tr>
<td>goodside</td>
<td>Implementations of Riley Goodside attacks.</td>
</tr>
<tr>
<td>leakerplay</td>
<td>Evaluate if a model will replay training data.</td>
</tr>
<tr>
<td>lmrc</td>
<td>Subsample of the <a href="https://arxiv.org/abs/2303.18190" rel="nofollow">Language Model Risk Cards</a> probes</td>
</tr>
<tr>
<td>malwaregen</td>
<td>Attempts to have the model generate code for building malware</td>
</tr>
<tr>
<td>misleading</td>
<td>Attempts to make a model support misleading and false claims</td>
</tr>
<tr>
<td>packagehallucination</td>
<td>Trying to get code generations that specify non-existent (and therefore insecure) packages.</td>
</tr>
<tr>
<td>promptinject</td>
<td>Implementation of the Agency Enterprise <a href="https://github.com/agencyenterprise/PromptInject/tree/main/promptinject">PromptInject</a> work (best paper awards @ NeurIPS ML Safety Workshop 2022)</td>
</tr>
<tr>
<td>realtoxicityprompts</td>
<td>Subset of the RealToxicityPrompts work (data constrained because the full test will take so long to run)</td>
</tr>
<tr>
<td>snowball</td>
<td><a href="https://ofir.io/snowballed_hallucination.pdf" rel="nofollow">Snowballed Hallucination</a> probes designed to make a model give a wrong answer to questions too complex for it to process</td>
</tr>
<tr>
<td>xss</td>
<td>Look for vulnerabilities the permit or enact cross-site attacks, such as private data exfiltration.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Logging</h2><a id="user-content-logging" aria-label="Permalink: Logging" href="#logging"></a></p>
<p dir="auto"><code>garak</code> generates multiple kinds of log:</p>
<ul dir="auto">
<li>A log file, <code>garak.log</code>. This includes debugging information from <code>garak</code> and its plugins, and is continued across runs.</li>
<li>A report of the current run, structured as JSONL. A new report file is created every time <code>garak</code> runs. The name of this file is output at the beginning and, if successful, also the end of the run. In the report, an entry is made for each probing attempt both as the generations are received, and again when they are evaluated; the entry's <code>status</code> attribute takes a constant from <code>garak.attempts</code> to describe what stage it was made at.</li>
<li>A hit log, detailing attempts that yielded a vulnerability (a 'hit')</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">How is the code structured?</h2><a id="user-content-how-is-the-code-structured" aria-label="Permalink: How is the code structured?" href="#how-is-the-code-structured"></a></p>
<p dir="auto">Check out the <a href="https://reference.garak.ai/" rel="nofollow">reference docs</a> for an authoritative guide to <code>garak</code> code structure.</p>
<p dir="auto">In a typical run, <code>garak</code> will read a model type (and optionally model name) from the command line, then determine which <code>probe</code>s and <code>detector</code>s to run, start up a <code>generator</code>, and then pass these to a <code>harness</code> to do the probing; an <code>evaluator</code> deals with the results. There are many modules in each of these categories, and each module provides a number of classes that act as individual plugins.</p>
<ul dir="auto">
<li><code>garak/probes/</code> - classes for generating interactions with LLMs</li>
<li><code>garak/detectors/</code> - classes for detecting an LLM is exhibiting a given failure mode</li>
<li><code>garak/evaluators/</code> - assessment reporting schemes</li>
<li><code>garak/generators/</code> - plugins for LLMs to be probed</li>
<li><code>garak/harnesses/</code> - classes for structuring testing</li>
<li><code>resources/</code> - ancillary items required by plugins</li>
</ul>
<p dir="auto">The default operating mode is to use the <code>probewise</code> harness. Given a list of probe module names and probe plugin names, the <code>probewise</code> harness instantiates each probe, then for each probe reads its <code>recommended_detectors</code> attribute to get a list of <code>detector</code>s to run on the output.</p>
<p dir="auto">Each plugin category (<code>probes</code>, <code>detectors</code>, <code>evaluators</code>, <code>generators</code>, <code>harnesses</code>) includes a <code>base.py</code> which defines the base classes usable by plugins in that category. Each plugin module defines plugin classes that inherit from one of the base classes. For example, <code>garak.generators.openai.OpenAIGenerator</code> descends from <code>garak.generators.base.Generator</code>.</p>
<p dir="auto">Larger artefacts, like model files and bigger corpora, are kept out of the repository; they can be stored on e.g. Hugging Face Hub and loaded locally by clients using <code>garak</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Developing your own plugin</h2><a id="user-content-developing-your-own-plugin" aria-label="Permalink: Developing your own plugin" href="#developing-your-own-plugin"></a></p>
<ul dir="auto">
<li>Take a look at how other plugins do it</li>
<li>Inherit from one of the base classes, e.g. <code>garak.probes.base.TextProbe</code></li>
<li>Override as little as possible</li>
<li>You can test the new code in at least two ways:
<ul dir="auto">
<li>Start an interactive Python session
<ul dir="auto">
<li>Import the model, e.g. <code>import garak.probes.mymodule</code></li>
<li>Instantiate the plugin, e.g. <code>p = garak.probes.mymodule.MyProbe()</code></li>
</ul>
</li>
<li>Run a scan with test plugins
<ul dir="auto">
<li>For probes, try a blank generator and always.Pass detector: <code>python3 -m garak -m test.Blank -p mymodule -d always.Pass</code></li>
<li>For detectors, try a blank generator and a blank probe: <code>python3 -m garak -m test.Blank -p test.Blank -d mymodule</code></li>
<li>For generators, try a blank probe and always.Pass detector: <code>python3 -m garak -m mymodule -p test.Blank -d always.Pass</code></li>
</ul>
</li>
<li>Get <code>garak</code> to list all the plugins of the type you're writing, with <code>--list_probes</code>, <code>--list_detectors</code>, or <code>--list_generators</code></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto">We have an FAQ <a href="https://github.com/NVIDIA/garak/blob/main/FAQ.md">here</a>. Reach out if you have any more questions! <a href="mailto:leon@garak.ai">leon@garak.ai</a></p>
<p dir="auto">Code reference documentation is at <a href="https://garak.readthedocs.io/en/latest/" rel="nofollow">garak.readthedocs.io</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citing garak</h2><a id="user-content-citing-garak" aria-label="Permalink: Citing garak" href="#citing-garak"></a></p>
<p dir="auto">You can read the <a href="https://github.com/NVIDIA/garak/blob/main/garak-paper.pdf">garak preprint paper</a>. If you use garak, please cite us.</p>
<div data-snippet-clipboard-copy-content="@article{garak,
  title={{garak: A Framework for Security Probing Large Language Models}},
  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},
  year={2024},
  howpublished={\url{https://garak.ai}}
}"><pre><code>@article{garak,
  title={{garak: A Framework for Security Probing Large Language Models}},
  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},
  year={2024},
  howpublished={\url{https://garak.ai}}
}
</code></pre></div>
<hr>
<p dir="auto"><em>"Lying is a skill like any other, and if you wish to maintain a level of excellence you have to practice constantly"</em> - Elim</p>
<p dir="auto">For updates and news see <a href="https://twitter.com/garak_llm" rel="nofollow">@garak_llm</a></p>
<p dir="auto">© 2023- Leon Derczynski; Apache license v2, see <a href="https://github.com/NVIDIA/garak/blob/main/LICENSE">LICENSE</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Constraints in Go (145 pts)]]></title>
            <link>https://bitfieldconsulting.com/posts/constraints</link>
            <guid>42162878</guid>
            <pubDate>Sun, 17 Nov 2024 08:44:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bitfieldconsulting.com/posts/constraints">https://bitfieldconsulting.com/posts/constraints</a>, See on <a href="https://news.ycombinator.com/item?id=42162878">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="23" id="block-yui_3_17_2_1_1710022019086_133076"><blockquote>
<p><em>Design is the beauty of turning constraints into
advantages.</em><br>
—<a href="https://talks.ui-patterns.com/videos/design-is-the-beauty-of-turning-constraints-into-advantages-aza-raskin">Aza
Raskin</a> </p>
</blockquote>
<p>This is the fourth in a four-part series of tutorials on generics in
Go.</p>
<ol type="1">
<li><a href="https://bitfieldconsulting.com/posts/generics">Generics</a></li>
<li><a href="https://bitfieldconsulting.com/posts/type-parameters">Type parameters</a></li>
<li><a href="https://bitfieldconsulting.com/posts/generic-types">Generic types</a></li>
<li><strong>Constraints</strong></li>
</ol>
<hr>
<p>In my book <a href="https://bitfieldconsulting.com/books/generics">Know Go</a>, and in the previous
tutorials in this series, you’ll learn all about generic programming in
Go and the new universe of programs it opens up to us. Ironically, one
of the new features of Go that gives us the most freedom is
<em>constraints</em>. Let’s talk about that, and explain the
paradox.</p>
<p>We saw in the <a href="https://bitfieldconsulting.com/posts/generic-types">previous tutorial</a>
that when we’re writing generic functions that take any type, the range
of things we can <em>do</em> with values of that type is necessarily
rather limited. For example, we can’t add them together. For that, we’d
need to be able to prove to Go that they’re one of the types that
support the <code>+</code> operator.</p>
<h2 id="method-set-constraints">Method set constraints</h2>
<p>It’s the same with interfaces, as we discussed in the <a href="https://bitfieldconsulting.com/posts/generics">first post</a> in this series. The empty
interface, <code>any</code>, is implemented by every type, and so
knowing that something implements <code>any</code> tells you nothing
distinctive about it.</p>
<h3 id="limitations-of-the-any-constraint">Limitations of the
<code>any</code> constraint</h3>
<p>Similarly, in a generic function parameterised by some type T,
constraining T to <code>any</code> doesn’t give Go any information about
it. So it has no way to guarantee that a given operator, such as
<code>+</code>, will work with values of T.</p>
<p>A Go proverb says:</p>
<blockquote>
<p><em>The bigger the interface, the weaker the abstraction.</em><br>
—<a href="https://go-proverbs.github.io/">https://go-proverbs.github.io/</a></p>
</blockquote>
<p>And the same is true of constraints. The broader the constraint, and
thus the more types it allows, the less we can guarantee about what
operations we can do on them.</p>
<p>There <em>are</em> a few things we can do with <code>any</code>
values, as you already know, because we’ve done them. For example, we
can declare variables of that type, we can assign values to them, we can
return them from functions, and so on.</p>
<p>But we can’t really do a whole lot of <em>computation</em> with them,
because we can’t use operators like <code>+</code> or <code>-</code>. So
in order to be able to do something useful with values of T, such as
adding them, we need more restrictive constraints.</p>
<p>What kinds of constraints <em>could</em> there be on T? Let’s examine
the possibilities.</p>
<h3 id="basic-interfaces">Basic interfaces</h3>
<p>One kind of constraint that we’re already familiar with in Go is an
<em>interface</em>. In fact, all constraints are interfaces of a kind,
but let’s use the term <em>basic</em> interface here to avoid any
confusion. A basic interface, we’ll say, is one that contains only
method elements.</p>
<p>For example, the <code>fmt.Stringer</code> interface we saw in the <a href="https://bitfieldconsulting.com/posts/generics">first tutorial</a>:</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>type</span> Stringer <span>interface</span> <span>{</span></span>
<span id="cb1-2">    String<span>()</span> <span>string</span></span>
<span id="cb1-3"><span>}</span></span></code></pre></div>
<p>We’ve seen that we can write an ordinary, non-generic function that
takes a parameter of type <code>Stringer</code>. And we can also use
this interface as a type constraint for a generic function.</p>
<p>For example, we could write a generic function parameterised by some
type T, but this time T can’t be just any type. Instead, we’ll say that
whatever T turns out to be, it must implement the
<code>fmt.Stringer</code> interface:</p>
<div id="cb2"><pre><code><span id="cb2-1"><span>func</span> Stringify<span>[</span>T fmt<span>.</span>Stringer<span>](</span>s T<span>)</span> <span>string</span> <span>{</span></span>
<span id="cb2-2">    <span>return</span> s<span>.</span>String<span>()</span></span>
<span id="cb2-3"><span>}</span></span></code></pre></div>
<p>This is clear enough, and it works the same way as the generic
functions we’ve already written. The only new thing is that we used the
constraint <code>Stringer</code> instead of <code>any</code>. Now when
we actually call this function in a program, we’re only allowed to pass
it arguments that implement <code>Stringer</code>.</p>
<p>What would happen, then, if we tried to call <code>Stringify</code>
with an argument that <em>doesn’t</em> implement <code>Stringer</code>?
We feel instinctively that this shouldn’t work, and it doesn’t:</p>
<div id="cb3"><pre><code><span id="cb3-1">fmt<span>.</span>Println<span>(</span>Stringify<span>(</span><span>1</span><span>))</span></span>
<span id="cb3-2"><span>// int does not implement Stringer (missing method String)</span></span></code></pre></div>
<p>That makes sense. It’s just the same as if we wrote an ordinary,
non-generic function that took a parameter of type
<code>Stringer</code>, as we did in the <a href="https://bitfieldconsulting.com/posts/generics">first
tutorial</a>.</p>
<p>There’s no advantage to writing a generic function in this case,
since we can use this interface type directly in an ordinary function.
All the same, a basic interface—one defined by a set of methods—is a
valid constraint for type parameters, and we can use it that way if we
want to.</p>
<h3 id="exercise-stringy-beans">Exercise: Stringy beans</h3>
<p>Flex your generics muscles a little now, by writing a generic
function constrained by <code>fmt.Stringer</code> to solve the <a href="https://github.com/bitfield/know-go/tree/main/exercises/stringy"><code>stringy</code></a>
exercise.</p>
<div id="cb4"><pre><code><span id="cb4-1"><span>type</span> greeting <span>struct</span><span>{}</span></span>
<span id="cb4-2"></span>
<span id="cb4-3"><span>func</span> <span>(</span>greeting<span>)</span> String<span>()</span> <span>string</span> <span>{</span></span>
<span id="cb4-4">    <span>return</span> <span>"Howdy!"</span></span>
<span id="cb4-5"><span>}</span></span>
<span id="cb4-6"></span>
<span id="cb4-7"><span>func</span> TestStringifyTo_PrintsToSuppliedWriter<span>(</span>t <span>*</span>testing<span>.</span>T<span>)</span> <span>{</span></span>
<span id="cb4-8">    t<span>.</span>Parallel<span>()</span></span>
<span id="cb4-9">    buf <span>:=</span> <span>&amp;</span>bytes<span>.</span>Buffer<span>{}</span></span>
<span id="cb4-10">    stringy<span>.</span>StringifyTo<span>[</span>greeting<span>](</span>buf<span>,</span> greeting<span>{})</span></span>
<span id="cb4-11">    want <span>:=</span> <span>"Howdy!</span><span>\n</span><span>"</span></span>
<span id="cb4-12">    got <span>:=</span> buf<span>.</span>String<span>()</span></span>
<span id="cb4-13">    <span>if</span> want <span>!=</span> got <span>{</span></span>
<span id="cb4-14">        t<span>.</span>Errorf<span>(</span><span>"want %q, got %q"</span><span>,</span> want<span>,</span> got<span>)</span></span>
<span id="cb4-15">    <span>}</span></span>
<span id="cb4-16"><span>}</span></span></code></pre></div>
<p>(<a href="https://github.com/bitfield/know-go/blob/main/exercises/stringy/stringy_test.go">Listing
<code>exercises/stringy</code></a>)</p>
<p><strong>GOAL:</strong> Your job here is to write a generic function
<code>StringifyTo[T]</code> that takes an <code>io.Writer</code> and a
value of some arbitrary type constrained by <code>fmt.Stringer</code>,
and prints the value to the writer.</p>
<hr>
<p><strong>HINT:</strong> This is a bit like the
<code>PrintAnything</code> function we saw before, isn’t it? Actually,
it’s a “print anything stringable” function. We already know what the
constraint is (<code>fmt.Stringer</code>), and the rest is
straightforward.</p>
<hr>
<p><strong>SOLUTION:</strong> Here’s a version that would work, for
example:</p>
<div id="cb5"><pre><code><span id="cb5-1"><span>func</span> StringifyTo<span>[</span>T fmt<span>.</span>Stringer<span>](</span>w io<span>.</span>Writer<span>,</span> p T<span>)</span> <span>{</span></span>
<span id="cb5-2">    fmt<span>.</span>Fprintln<span>(</span>w<span>,</span> p<span>.</span>String<span>())</span></span>
<span id="cb5-3"><span>}</span></span></code></pre></div>
<p>(<a href="https://github.com/bitfield/know-go/blob/main/solutions/stringy/stringy.go">Listing
<code>solutions/stringy</code></a>)</p>
<p>Strictly speaking, of course, we don’t really need to call the
<code>String</code> method: <code>fmt</code> already knows how to do
that automagically. But if we just passed <code>p</code> directly, we
wouldn’t need the <code>Stringer</code> constraint, and we could use
<code>any</code>… but what would be the fun in that?</p>
<h2 id="type-set-constraints">Type set constraints</h2>
<p>We’ve seen that one way an interface can specify an allowed range of
types is by including a <em>method element</em>, such as
<code>String() string</code>. That would be a basic interface, but now
let’s introduce another kind of interface. Instead of listing methods
that the type must have, it directly specifies a set of types that are
allowed.</p>
<h3 id="type-elements">Type elements</h3>
<p>For example, suppose we wanted to write some generic function
<code>Double</code> that multiplies a number by two, and we want a type
constraint that allows only values of type <code>int</code>. We know
that <code>int</code> has no methods, so we can’t use any basic
interface as a constraint. How can we write it, then?</p>
<p>Well, here’s how:</p>
<div id="cb6"><pre><code><span id="cb6-1"><span>type</span> OnlyInt <span>interface</span> <span>{</span></span>
<span id="cb6-2">    <span>int</span></span>
<span id="cb6-3"><span>}</span></span></code></pre></div>
<p>Very straightforward! It looks just like a regular interface
definition, except that instead of method elements, it contains a single
<em>type element</em>, consisting of a named type. In this case, the
named type is <code>int</code>.</p>
<h3 id="using-a-type-set-constraint">Using a type set constraint</h3>
<p>How would we use a constraint like this? Let’s write
<code>Double</code>, then:</p>
<div id="cb7"><pre><code><span id="cb7-1"><span>func</span> Double<span>[</span>T OnlyInt<span>](</span>v T<span>)</span> T <span>{</span></span>
<span id="cb7-2">    <span>return</span> v <span>*</span> <span>2</span></span>
<span id="cb7-3"><span>}</span></span></code></pre></div>
<p>In other words, for some T that satisfies the constraint
<code>OnlyInt</code>, <code>Double</code> takes a T parameter and
returns a T result.</p>
<p>Note that we now have one answer to the sort of problem we
encountered with <code>AddAnything</code>: how to enable the
<code>*</code> operator (or any other arithmetic operator) in a
parameterised function. Since T can only be <code>int</code> (thanks to
the <code>OnlyInt</code> constraint), Go can guarantee that the
<code>*</code> operator will work with T values.</p>
<p>It’s not the complete answer, though, since there are other types
that support <code>*</code> that <em>wouldn’t</em> be allowed by this
constraint. And in any case, if we were only going to support
<code>int</code>, we could have just written an ordinary function that
took an <code>int</code> parameter.</p>
<p>So we’ll need to be able to expand the range of types allowed by our
constraint a little, but not beyond the types that support
<code>*</code>. How can we do that?</p>
<h3 id="unions">Unions</h3>
<p>What types <em>can</em> satisfy the constraint <code>OnlyInt</code>?
Well, only <code>int</code>! To broaden this range, we can create a
constraint specifying more than one named type:</p>
<div id="cb8"><pre><code><span id="cb8-1"><span>type</span> Integer <span>interface</span> <span>{</span></span>
<span id="cb8-2">    <span>int</span> <span>|</span> <span>int8</span> <span>|</span> <span>int16</span> <span>|</span> <span>int32</span> <span>|</span> <span>int64</span></span>
<span id="cb8-3"><span>}</span></span></code></pre></div>
<p>The types are separated by the pipe character, <code>|</code>. You
can think of this as representing “or”. In other words, a type will
satisfy this constraint if it is <code>int</code> <em>or</em>
<code>int8</code> <em>or</em>… you get the idea.</p>
<p>This kind of interface element is called a <em>union</em>. The type
elements in a union can include any Go types, including interface
types.</p>
<p>It can even include other constraints. In other words, we can
<em>compose</em> new constraints from existing ones, like this:</p>
<div id="cb9"><pre><code><span id="cb9-1"><span>type</span> Float <span>interface</span> <span>{</span></span>
<span id="cb9-2">    <span>float32</span> <span>|</span> <span>float64</span></span>
<span id="cb9-3"><span>}</span></span>
<span id="cb9-4"></span>
<span id="cb9-5"><span>type</span> Complex <span>interface</span> <span>{</span></span>
<span id="cb9-6">    <span>complex64</span> <span>|</span> <span>complex128</span></span>
<span id="cb9-7"><span>}</span></span>
<span id="cb9-8"></span>
<span id="cb9-9"><span>type</span> Number <span>interface</span> <span>{</span></span>
<span id="cb9-10">    Integer <span>|</span> Float <span>|</span> Complex</span>
<span id="cb9-11"><span>}</span></span></code></pre></div>
<p>We’re saying that <code>Integer</code>, <code>Float</code>, and
<code>Complex</code> are all unions of different built-in numeric types,
but we’re also creating a new constraint <code>Number</code>, which is a
union of those three <em>interface</em> types we just defined. If it’s
an integer, a float, or a complex number, then it’s a number!</p>
<h3 id="the-set-of-all-allowed-types">The set of all allowed types</h3>
<p>The <em>type set</em> of a constraint is the set of all types that
satisfy it. The type set of the empty interface (<code>any</code>) is
the set of all types, as you’d expect.</p>
<p>The type set of a union element (such as <code>Float</code> in the
previous example) is the union of the type sets of all its terms.</p>
<p>In the <code>Float</code> example, which is the union of
<code>float32 | float64</code>, its type set contains
<code>float32</code>, <code>float64</code>, and no other types.</p>
<h3 id="intersections">Intersections</h3>
<p>You probably know that with a basic interface, a type must have
<em>all</em> of the methods listed in order to implement the interface.
And if the interface contains other interfaces, a type must implement
<em>all</em> of those interfaces, not just one of them.</p>
<p>For example:</p>
<div id="cb10"><pre><code><span id="cb10-1"><span>type</span> ReaderStringer <span>interface</span> <span>{</span></span>
<span id="cb10-2">    io<span>.</span>Reader</span>
<span id="cb10-3">    fmt<span>.</span>Stringer</span>
<span id="cb10-4"><span>}</span></span></code></pre></div>
<p>If we were to write this as an <em>interface literal</em>, we would
separate the methods with a semicolon instead of a newline, but the
meaning is the same:</p>
<div id="cb11"><pre><code><span id="cb11-1"><span>interface</span> <span>{</span> io<span>.</span>Reader<span>;</span> fmt<span>.</span>Stringer <span>}</span></span></code></pre></div>
<p>To implement this interface, a type has to implement <em>both</em>
<code>io.Reader</code> <em>and</em> <code>fmt.Stringer</code>. Just one
or the other isn’t good enough.</p>
<p>Each line of an interface definition like this, then, is treated as a
distinct type element. The type set of the interface as a whole is the
<em>intersection</em> of the type sets of all its elements. That is,
only those types that all the elements have in common.</p>
<p>So putting interface elements on different lines has the effect of
requiring a type to implement <em>all</em> those elements. We don’t need
this kind of interface very often, but we can imagine cases where it
might be necessary.</p>
<h3 id="empty-type-sets">Empty type sets</h3>
<p>You might be wondering about what happens if we define an interface
whose type set is completely empty. That is, if there are no types that
can satisfy the constraint.</p>
<p>Well, that could happen with an intersection of two type sets that
have <em>no</em> elements in common. For example:</p>
<div id="cb12"><pre><code><span id="cb12-1"><span>type</span> Unpossible <span>interface</span> <span>{</span></span>
<span id="cb12-2">    <span>int</span></span>
<span id="cb12-3">    <span>string</span></span>
<span id="cb12-4"><span>}</span></span></code></pre></div>
<p>Clearly no type can be both <code>int</code> and <code>string</code>
at the same time! Or, to put it another way, this interface’s type set
is empty.</p>
<p>If we try to instantiate a function constrained by
<code>Unpossible</code>, we’ll find, naturally enough, that it can’t be
done:</p>
<pre><code>cannot implement Unpossible (empty type set)</code></pre>
<p>We probably wouldn’t do this on purpose, since an unsatisfiable
constraint doesn’t seem that useful. But with more sophisticated
interfaces, we might accidentally reduce the allowed type set to zero,
and it’s helpful to know what this error message means so that we can
fix the problem.</p>
<h2 id="composite-type-literals">Composite type literals</h2>
<p>A <em>composite</em> type is one that’s built up from other types. We
saw some composite types in the <a href="https://bitfieldconsulting.com/posts/generic-types">previous
tutorial</a>, such as <code>[]E</code>, which is a slice of some element
type E.</p>
<p>But we’re not restricted to defined types with names. We can also
construct new types on the fly, using a <em>type literal</em>: that is,
literally writing out the type definition as part of the interface.</p>
<h3 id="a-struct-type-literal">A struct type literal</h3>
<p>For example, this interface specifies a <em>struct</em> type
literal:</p>
<div id="cb14"><pre><code><span id="cb14-1"><span>type</span> Pointish <span>interface</span> <span>{</span></span>
<span id="cb14-2">    <span>struct</span><span>{</span> X<span>,</span> Y <span>int</span> <span>}</span></span>
<span id="cb14-3"><span>}</span></span></code></pre></div>
<p>A type parameter with this constraint would allow any instance of
such a struct. In other words, its type set contains exactly one type:
<code>struct{ X, Y int }</code>.</p>
<h3 id="access-to-struct-fields">Access to struct fields</h3>
<p>While we can write a generic function constrained by some struct type
such as <code>Pointish</code>, there are limitations on what that
function can do with that type. One is that it can’t access the struct’s
<em>fields</em>:</p>
<div id="cb15"><pre><code><span id="cb15-1"><span>func</span> GetX<span>[</span>T Pointish<span>](</span>p T<span>)</span> <span>int</span> <span>{</span></span>
<span id="cb15-2">    <span>return</span> p<span>.</span>X</span>
<span id="cb15-3"><span>}</span></span>
<span id="cb15-4"><span>// p.X undefined (type T has no field or method X)</span></span></code></pre></div>
<p>In other words, we can’t refer to a field on <code>p</code>, even
though the function’s constraint explicitly says that any <code>p</code>
is guaranteed to be a struct with at least the field <code>X</code>.
This is a limitation of the Go compiler that has not yet been overcome.
Sorry about that.</p>
<h2 id="some-limitations-of-type-sets">Some limitations of type
sets</h2>
<p>An interface containing type elements can <em>only</em> be used as a
constraint on a type parameter. It can’t be used as the type of a
variable or parameter declaration, like a basic interface can. That too
is something that might change in the future, but this is where we are
today.</p>
<h3 id="constraints-versus-basic-interfaces">Constraints versus basic
interfaces</h3>
<p>What exactly stops us from doing that, though? We already know that
we can write functions that take ordinary parameters of some basic
interface type such as <code>Stringer</code>. So what happens if we try
to do the same with an interface containing type elements, such as
<code>Number</code>?</p>
<p>Let’s see:</p>
<div id="cb16"><pre><code><span id="cb16-1"><span>func</span> Double<span>(</span>p Number<span>)</span> Number <span>{</span></span>
<span id="cb16-2"><span>// interface contains type constraints</span></span></code></pre></div>
<p>This doesn’t compile, for the reasons we’ve discussed. Some potential
confusion arises from the fact that a basic interface can be used as
both a regular interface type <em>and</em> a constraint on type
parameters. But interfaces that contain type elements can only be used
as constraints.</p>
<h3 id="constraints-are-not-classes">Constraints are not classes</h3>
<p>If you have some experience with languages that have <em>classes</em>
(hierarchies of types), then there’s another thing that might trip you
up with Go generics: constraints are not classes, and you can’t
instantiate a generic function or type on a constraint interface.</p>
<p>To illustrate, suppose we have some concrete types <code>Cow</code>
and <code>Chicken</code>:</p>
<div id="cb17"><pre><code><span id="cb17-1"><span>type</span> Cow <span>struct</span><span>{</span> moo <span>string</span> <span>}</span></span>
<span id="cb17-2"></span>
<span id="cb17-3"><span>type</span> Chicken <span>struct</span><span>{</span> cluck <span>string</span> <span>}</span></span></code></pre></div>
<p>And suppose we define some interface <code>Animal</code> whose type
set consists of <code>Cow</code> and <code>Chicken</code>:</p>
<div id="cb18"><pre><code><span id="cb18-1"><span>type</span> Animal <span>interface</span> <span>{</span></span>
<span id="cb18-2">    Cow <span>|</span> Chicken</span>
<span id="cb18-3"><span>}</span></span></code></pre></div>
<p>So far, so good, and suppose we now define a generic type
<code>Farm</code> as a slice of <code>T Animal</code>:</p>

<p>Since we know the type set of <code>Animal</code> contains exactly
<code>Cow</code> and <code>Chicken</code>, then either of those types
can be used to instantiate <code>Farm</code>:</p>
<div id="cb20"><pre><code><span id="cb20-1">dairy <span>:=</span> Farm<span>[</span>Cow<span>]{}</span></span>
<span id="cb20-2">poultry <span>:=</span> Farm<span>[</span>Chicken<span>]{}</span></span></code></pre></div>
<p>What about <code>Animal</code> itself? Could we create a
<code>Farm[Animal]</code>? No, because there’s no such type as
<code>Animal</code>. It’s a type <em>constraint</em>, not a type, so
this gives an error:</p>
<div id="cb21"><pre><code><span id="cb21-1">mixed <span>:=</span> Farm<span>[</span>Animal<span>]{}</span></span>
<span id="cb21-2"><span>// interface contains type constraints</span></span></code></pre></div>
<p>And, as we’ve seen, we also couldn’t use <code>Animal</code> as the
type of some variable, or ordinary function parameter. Only basic
interfaces can be used this way, not interfaces containing type
elements.</p>
<h2 id="approximations">Approximations</h2>
<p>Let’s return to our earlier definition of an interface
<code>Integer</code>, consisting of a union of named types.
Specifically, the built-in signed integer types:</p>
<div id="cb22"><pre><code><span id="cb22-1"><span>type</span> Integer <span>interface</span> <span>{</span></span>
<span id="cb22-2">    <span>int</span> <span>|</span> <span>int8</span> <span>|</span> <span>int16</span> <span>|</span> <span>int32</span> <span>|</span> <span>int64</span></span>
<span id="cb22-3"><span>}</span></span></code></pre></div>
<p>We know that the type set of this interface contains all the types
we’ve named. But what about defined types whose <em>underlying</em> type
is one of the built-in types?</p>
<h3 id="limitations-of-named-types">Limitations of named types</h3>
<p>For example:</p>

<p>Is <code>MyInt</code> also in the type set of <code>Integer</code>?
Let’s find out. Suppose we write a generic function that uses this
constraint:</p>
<div id="cb24"><pre><code><span id="cb24-1"><span>func</span> Double<span>[</span>T Integer<span>](</span>v T<span>)</span> T <span>{</span></span>
<span id="cb24-2">    <span>return</span> v <span>*</span> <span>2</span></span>
<span id="cb24-3"><span>}</span></span></code></pre></div>
<p>Can we pass it a <code>MyInt</code> value? We’ll soon know:</p>
<div id="cb25"><pre><code><span id="cb25-1">fmt<span>.</span>Println<span>(</span>Double<span>(</span>MyInt<span>(</span><span>1</span><span>)))</span></span>
<span id="cb25-2"><span>// MyInt does not implement Integer</span></span></code></pre></div>
<p>No.&nbsp;That makes sense, because <code>Integer</code> is a list of named
types, and we can see that <code>MyInt</code> isn’t one of them.</p>
<p>How can we write an interface that allows not only a set of specific
named types, but also any other types <em>derived</em> from them?</p>
<h3 id="type-approximations">Type approximations</h3>
<p>We need a new kind of type element: a <em>type approximation</em>. We
write it using the tilde (<code>~</code>) character:</p>
<div id="cb26"><pre><code><span id="cb26-1"><span>type</span> ApproximatelyInt <span>interface</span> <span>{</span></span>
<span id="cb26-2">    <span>~</span><span>int</span></span>
<span id="cb26-3"><span>}</span></span></code></pre></div>
<p>The type set of <code>~int</code> includes <code>int</code> itself,
but also any type whose underlying type is <code>int</code> (for
example, <code>MyInt</code>).</p>
<p>If we rewrite <code>Double</code> to use this constraint, we can pass
it a <code>MyInt</code>, which is good. Even better, it will accept
<em>any</em> type, now or in the future, whose underlying type is
<code>int</code>.</p>
<h3 id="derived-types">Derived types</h3>
<p>Approximations are especially useful with struct type elements.
Remember our <code>Pointish</code> interface?</p>
<div id="cb27"><pre><code><span id="cb27-1"><span>type</span> Pointish <span>interface</span> <span>{</span></span>
<span id="cb27-2">    <span>struct</span><span>{</span> x<span>,</span> y <span>int</span> <span>}</span></span>
<span id="cb27-3"><span>}</span></span></code></pre></div>
<p>Let’s write a generic function with this constraint:</p>
<div id="cb28"><pre><code><span id="cb28-1"><span>func</span> Plot<span>[</span>T Pointish<span>](</span>p T<span>)</span> <span>{</span></span></code></pre></div>
<p>We can pass it values of type <code>struct{ x, y int }</code>, as
you’d expect:</p>
<div id="cb29"><pre><code><span id="cb29-1">p <span>:=</span> <span>struct</span><span>{</span> x<span>,</span> y <span>int</span> <span>}{</span><span>1</span><span>,</span> <span>2</span><span>}</span></span>
<span id="cb29-2">Plot<span>(</span>p<span>)</span></span></code></pre></div>
<p>But now comes a problem: we can’t pass values of any <em>named</em>
struct type, even if the struct definition itself matches the constraint
perfectly:</p>
<div id="cb30"><pre><code><span id="cb30-1"><span>type</span> Point <span>struct</span> <span>{</span></span>
<span id="cb30-2">    x<span>,</span> y <span>int</span></span>
<span id="cb30-3"><span>}</span></span>
<span id="cb30-4">p <span>:=</span> Point<span>{</span><span>1</span><span>,</span> <span>2</span><span>}</span></span>
<span id="cb30-5">Plot<span>(</span>p<span>)</span></span>
<span id="cb30-6"><span>// Point does not implement Pointish (possibly missing ~ for</span></span>
<span id="cb30-7"><span>// struct{x int; y int} in constraint Pointish)</span></span></code></pre></div>
<p>What’s the problem here? Our constraint allows
<code>struct{ x, y int }</code>, but <code>Point</code> is <em>not that
type</em>. It’s a type <em>derived</em> from it. And, just as with
<code>MyInt</code>, a derived type is distinct from its underlying
type.</p>
<p>You know now how to solve this problem: use a type approximation! And
Go is telling us the same thing: “Hint, hint: I think you meant to write
a <code>~</code> in your constraint.”</p>
<p>If we add that approximation, the type set of our interface expands
to encompass all types derived from the specified struct, including
<code>Point</code>:</p>
<div id="cb31"><pre><code><span id="cb31-1"><span>type</span> Pointish <span>interface</span> <span>{</span></span>
<span id="cb31-2">    <span>~</span><span>struct</span><span>{</span> x<span>,</span> y <span>int</span> <span>}</span></span>
<span id="cb31-3"><span>}</span></span></code></pre></div>
<h3 id="exercise-a-first-approximation">Exercise: A first
approximation</h3>
<p>Can you use what you’ve just learned to solve the <a href="https://github.com/bitfield/know-go/tree/main/exercises/intish"><code>intish</code></a>
challenge?</p>
<p>Here you’re provided with a function <code>IsPositive</code>, which
determines whether a given value is greater than zero:</p>
<div id="cb32"><pre><code><span id="cb32-1"><span>func</span> IsPositive<span>[</span>T Intish<span>](</span>v T<span>)</span> <span>bool</span> <span>{</span></span>
<span id="cb32-2">    <span>return</span> v <span>&gt;</span> <span>0</span></span>
<span id="cb32-3"><span>}</span></span></code></pre></div>
<p>(<a href="https://github.com/bitfield/know-go/blob/main/exercises/intish/intish.go">Listing
<code>exercises/intish</code></a>)</p>
<p>And there’s a set of accompanying tests that instantiate this
function on some derived type <code>MyInt</code>:</p>
<div id="cb33"><pre><code><span id="cb33-1"><span>type</span> MyInt <span>int</span></span>
<span id="cb33-2"></span>
<span id="cb33-3"><span>func</span> TestIsPositive_IsTrueFor1<span>(</span>t <span>*</span>testing<span>.</span>T<span>)</span> <span>{</span></span>
<span id="cb33-4">    t<span>.</span>Parallel<span>()</span></span>
<span id="cb33-5">    input <span>:=</span> MyInt<span>(</span><span>1</span><span>)</span></span>
<span id="cb33-6">    <span>if</span> <span>!</span>intish<span>.</span>IsPositive<span>(</span>input<span>)</span> <span>{</span></span>
<span id="cb33-7">        t<span>.</span>Errorf<span>(</span><span>"IsPositive(1): want true, got false"</span><span>)</span></span>
<span id="cb33-8">    <span>}</span></span>
<span id="cb33-9"><span>}</span></span>
<span id="cb33-10"></span>
<span id="cb33-11"><span>func</span> TestIsPositive_IsFalseForNegative1<span>(</span>t <span>*</span>testing<span>.</span>T<span>)</span> <span>{</span></span>
<span id="cb33-12">    t<span>.</span>Parallel<span>()</span></span>
<span id="cb33-13">    input <span>:=</span> MyInt<span>(-</span><span>1</span><span>)</span></span>
<span id="cb33-14">    <span>if</span> intish<span>.</span>IsPositive<span>(</span>input<span>)</span> <span>{</span></span>
<span id="cb33-15">        t<span>.</span>Errorf<span>(</span><span>"IsPositive(-1): want false, got true"</span><span>)</span></span>
<span id="cb33-16">    <span>}</span></span>
<span id="cb33-17"><span>}</span></span>
<span id="cb33-18"></span>
<span id="cb33-19"><span>func</span> TestIsPositive_IsFalseForZero<span>(</span>t <span>*</span>testing<span>.</span>T<span>)</span> <span>{</span></span>
<span id="cb33-20">    t<span>.</span>Parallel<span>()</span></span>
<span id="cb33-21">    input <span>:=</span> MyInt<span>(</span><span>0</span><span>)</span></span>
<span id="cb33-22">    <span>if</span> intish<span>.</span>IsPositive<span>(</span>input<span>)</span> <span>{</span></span>
<span id="cb33-23">        t<span>.</span>Errorf<span>(</span><span>"IsPositive(0): want false, got true"</span><span>)</span></span>
<span id="cb33-24">    <span>}</span></span>
<span id="cb33-25"><span>}</span></span></code></pre></div>
<p>(<a href="https://github.com/bitfield/know-go/blob/main/exercises/intish/intish_test.go">Listing
<code>exercises/intish</code></a>)</p>
<p><strong>GOAL:</strong> Your task here is to define the
<code>Intish</code> interface.</p>
<hr>
<p><strong>HINT:</strong> A method set won’t work here, because the
<code>int</code> type <em>has</em> no methods! On the other hand, the
type literal <code>int</code> won’t work either, because
<code>MyInt</code> is not <code>int</code>, it’s a new type derived from
it.</p>
<p>What kind of constraint could you use instead? I think you know where
this is going, don’t you? If not, have another look at the previous
section on type approximations.</p>
<hr>
<p><strong>SOLUTION:</strong> It’s not complicated, once you know that a
type approximation is required:</p>
<div id="cb34"><pre><code><span id="cb34-1"><span>type</span> Intish <span>interface</span> <span>{</span></span>
<span id="cb34-2">    <span>~</span><span>int</span></span>
<span id="cb34-3"><span>}</span></span></code></pre></div>
<p>(<a href="https://github.com/bitfield/know-go/blob/main/solutions/intish/intish.go">Listing
<code>solutions/intish</code></a>)</p>
<h2 id="interface-literals">Interface literals</h2>
<p>Up to now, we’ve always used type parameters with a <em>named</em>
constraint, such as <code>Integer</code> (or even just
<code>any</code>). And we know that those constraints are defined as
interfaces. So could we use an <em>interface literal</em> as a type
constraint?</p>
<h3 id="syntax-of-an-interface-literal">Syntax of an interface
literal</h3>
<p>An interface literal, as you probably know, consists of the keyword
<code>interface</code> followed by curly braces containing (optionally)
some interface elements.</p>
<p>For example, the simplest interface literal is the empty interface,
<code>interface{}</code>, which is common enough to have its own
predeclared name, <code>any</code>.</p>
<p>We should be able to write this empty interface literal wherever
<code>any</code> is allowed as a type constraint, then:</p>
<div id="cb35"><pre><code><span id="cb35-1"><span>func</span> Identity<span>[</span>T <span>interface</span><span>{}](</span>v T<span>)</span> T <span>{</span></span></code></pre></div>
<p>And so we can. But we’re not restricted to only <em>empty</em>
interface literals. We could write an interface literal that contains a
method element, for example:</p>
<div id="cb36"><pre><code><span id="cb36-1"><span>func</span> Stringify<span>[</span>T <span>interface</span><span>{</span> String<span>()</span> <span>string</span> <span>}](</span>s T<span>)</span> <span>string</span> <span>{</span></span>
<span id="cb36-2">    <span>return</span> s<span>.</span>String<span>()</span></span>
<span id="cb36-3"><span>}</span></span></code></pre></div>
<p>This is a little hard to read at first, perhaps. But we’ve already
seen this exact function before, only in that case it had a
<em>named</em> constraint <code>Stringer</code>. We’ve simply replaced
that name with the corresponding interface literal:</p>
<div id="cb37"><pre><code><span id="cb37-1"><span>interface</span><span>{</span> String<span>()</span> <span>string</span> <span>}</span></span></code></pre></div>
<p>That is, the set of types that have a <code>String</code> method. We
don’t need to name this interface in order to use it as a constraint,
and sometimes it’s clearer to write it as a literal.</p>
<h3 id="omitting-the-interface-keyword">Omitting the
<code>interface</code> keyword</h3>
<p>And we’re not limited to just method elements in interface literals
used as constraints. We can use type elements too:</p>

<p>Conveniently, in this case we can omit the enclosing
<code>interface { ... }</code>, and write simply <code>~int</code> as
the constraint:</p>

<p>For example, we could write some function <code>Increment</code>
constrained to types derived from <code>int</code>:</p>
<div id="cb40"><pre><code><span id="cb40-1"><span>func</span> Increment<span>[</span>T <span>~</span><span>int</span><span>](</span>v T<span>)</span> T <span>{</span></span>
<span id="cb40-2">    <span>return</span> v <span>+</span> <span>1</span></span>
<span id="cb40-3"><span>}</span></span></code></pre></div>
<p>However, we can only omit the <code>interface</code> keyword when the
constraint contains exactly one type element. Multiple elements wouldn’t
be allowed, so this doesn’t work:</p>
<div id="cb41"><pre><code><span id="cb41-1"><span>func</span> Increment<span>[</span>T <span>~</span><span>int</span><span>;</span> <span>~</span><span>float64</span><span>](</span>v T<span>)</span> T <span>{</span></span>
<span id="cb41-2"><span>// syntax error: unexpected semicolon in parameter list; possibly </span></span>
<span id="cb41-3"><span>// missing comma or ]</span></span></code></pre></div>
<p>And we can’t omit <code>interface</code> with method elements
either:</p>
<div id="cb42"><pre><code><span id="cb42-1"><span>func</span> Increment<span>[</span>T String<span>()</span> <span>string</span><span>](</span>v T<span>)</span> T <span>{</span></span>
<span id="cb42-2"><span>// syntax error: unexpected ( in parameter list; possibly </span></span>
<span id="cb42-3"><span>// missing comma or ]</span></span></code></pre></div>
<p>And we can only omit <code>interface</code> in a constraint
<em>literal</em>. We can’t omit it when defining a named constraint. So
this doesn’t work, for example:</p>
<div id="cb43"><pre><code><span id="cb43-1"><span>type</span> Intish <span>~</span><span>int</span></span>
<span id="cb43-2"><span>// syntax error: unexpected ~ in type declaration</span></span></code></pre></div>
<h3 id="referring-to-type-parameters">Referring to type parameters</h3>
<p>We’ve seen that in certain cases, instead of having to define it
separately, we can write a constraint directly as an interface literal.
So you might be wondering: can we refer to T inside the interface
literal itself? Yes, we can.</p>
<p>To see why we might need to do that, suppose we wanted to write a
generic function <code>Contains[T]</code>, that takes a slice of T and
tells you whether or not it contains a given value.</p>
<p>And suppose that we’ll determine this, for any particular element of
the slice, by calling some <code>Equal</code> method on the element.
That means we must constrain the function to only types that have a
suitable <code>Equal</code> method.</p>
<p>So the constraint for T is going to be an interface containing the
method <code>Equal(T) bool</code>, let’s say.</p>
<p>Can we do this? Let’s try:</p>
<div id="cb44"><pre><code><span id="cb44-1"><span>func</span> Contains<span>[</span>T <span>interface</span><span>{</span> Equal<span>(</span>T<span>)</span> <span>bool</span> <span>}](</span>s <span>[]</span>T<span>,</span>  v T<span>)</span> <span>bool</span> <span>{</span></span></code></pre></div>
<p>Yes, this is fine. In fact, using an interface literal is the
<em>only</em> way to write this constraint. We couldn’t have created
some <em>named</em> interface type to do the same thing. Why not?</p>
<p>Let’s see what happens if we try:</p>
<div id="cb45"><pre><code><span id="cb45-1"><span>type</span> Equaler <span>interface</span> <span>{</span></span>
<span id="cb45-2">    Equal<span>(???)</span> <span>bool</span> <span>// we can't say 'T' here</span></span>
<span id="cb45-3"><span>}</span></span></code></pre></div>
<p>Because the type parameter T is part of the <code>Equal</code> method
signature, and we don’t <em>have</em> T here. The only way to refer to T
is in an interface literal inside a type constraint:</p>
<div id="cb46"><pre><code><span id="cb46-1"><span>[</span>T <span>interface</span><span>{</span> Equal<span>(</span>T<span>)</span> <span>bool</span> <span>}]</span></span></code></pre></div>
<p>At least, we can’t write a <em>specific</em> interface that mentions
T in its method set. What we’d need here, in fact, is a <em>generic</em>
interface, and you’ll learn how to define and use these in my book, <a href="https://bitfieldconsulting.com/posts/generics">Know Go</a>. If these tutorials have given you an
appetite for generic programming in Go, I think you’ll really enjoy the
book—check it out!</p>
<h3 id="exercise-greater-love">Exercise: Greater love</h3>
<p>Your turn now to see if you can solve the <a href="https://github.com/bitfield/know-go/tree/main/exercises/greater"><code>greater</code></a>
exercise.</p>
<p>You’ve been given the following (incomplete) function:</p>
<div id="cb47"><pre><code><span id="cb47-1"><span>func</span> IsGreater<span>[</span>T <span>/* Your constraint here! */</span><span>](</span>x<span>,</span> y T<span>)</span> <span>bool</span> <span>{</span></span>
<span id="cb47-2">    <span>return</span> x<span>.</span>Greater<span>(</span>y<span>)</span></span>
<span id="cb47-3"><span>}</span></span></code></pre></div>
<p>(<a href="https://github.com/bitfield/know-go/blob/main/exercises/greater/greater.go">Listing
<code>exercises/greater</code></a>)</p>
<p>This takes two values of some arbitrary type, and compares them by
calling the <code>Greater</code> method on the first value, passing it
the second value.</p>
<p>The tests exercise this function by calling it with two values of a
defined type <code>MyInt</code>, which has the required
<code>Greater</code> method.</p>
<div id="cb48"><pre><code><span id="cb48-1"><span>type</span> MyInt <span>int</span></span>
<span id="cb48-2"></span>
<span id="cb48-3"><span>func</span> <span>(</span>m MyInt<span>)</span> Greater<span>(</span>v MyInt<span>)</span> <span>bool</span> <span>{</span></span>
<span id="cb48-4">    <span>return</span> m <span>&gt;</span> v</span>
<span id="cb48-5"><span>}</span></span>
<span id="cb48-6"></span>
<span id="cb48-7"><span>func</span> TestIsGreater_IsTrueFor2And1<span>(</span>t <span>*</span>testing<span>.</span>T<span>)</span> <span>{</span></span>
<span id="cb48-8">    t<span>.</span>Parallel<span>()</span></span>
<span id="cb48-9">    <span>if</span> <span>!</span>greater<span>.</span>IsGreater<span>(</span>MyInt<span>(</span><span>2</span><span>),</span> MyInt<span>(</span><span>1</span><span>))</span> <span>{</span></span>
<span id="cb48-10">        t<span>.</span>Fatalf<span>(</span><span>"IsGreater(2, 1): want true, got false"</span><span>)</span></span>
<span id="cb48-11">    <span>}</span></span>
<span id="cb48-12"><span>}</span></span>
<span id="cb48-13"></span>
<span id="cb48-14"><span>func</span> TestIsGreater_IsFalseFor1And2<span>(</span>t <span>*</span>testing<span>.</span>T<span>)</span> <span>{</span></span>
<span id="cb48-15">    t<span>.</span>Parallel<span>()</span></span>
<span id="cb48-16">    <span>if</span> greater<span>.</span>IsGreater<span>(</span>MyInt<span>(</span><span>1</span><span>),</span> MyInt<span>(</span><span>2</span><span>))</span> <span>{</span></span>
<span id="cb48-17">        t<span>.</span>Fatalf<span>(</span><span>"IsGreater(1, 2): want false, got true"</span><span>)</span></span>
<span id="cb48-18">    <span>}</span></span>
<span id="cb48-19"><span>}</span></span></code></pre></div>
<p>(<a href="https://github.com/bitfield/know-go/blob/main/exercises/greater/greater_test.go">Listing
<code>exercises/greater</code></a>)</p>
<p><strong>GOAL:</strong> To make these tests pass, you’ll need to write
an appropriate type constraint for <code>IsGreater</code>. Can you see
what to do?</p>
<hr>
<p><strong>HINT:</strong> Remember, we got here by talking about
constraints as interface literals, and in particular, interface literals
that refer to the type parameter.</p>
<p>If you try to define some <em>named</em> interface with the method
set containing <code>Greater</code>, for example, that won’t work. We
can’t do it for the same reason that we couldn’t define a named
interface with the method set <code>Equal</code>: we don’t know what
type of argument that method takes.</p>
<p>Just like <code>Equal</code>, <code>Greater</code> takes arguments of
some arbitrary type T, so we need an interface literal that can
<em>refer</em> to T in its definition. Does that help?</p>
<hr>
<p><strong>SOLUTION:</strong> Here’s one way to do it:</p>
<div id="cb49"><pre><code><span id="cb49-1"><span>func</span> IsGreater<span>[</span>T <span>interface</span><span>{</span> Greater<span>(</span>T<span>)</span> <span>bool</span> <span>}](</span>x<span>,</span> y T<span>)</span> <span>bool</span> <span>{</span></span>
<span id="cb49-2">    <span>return</span> x<span>.</span>Greater<span>(</span>y<span>)</span></span>
<span id="cb49-3"><span>}</span></span></code></pre></div>
<p>(<a href="https://github.com/bitfield/know-go/blob/main/solutions/greater/greater.go">Listing
<code>solutions/greater</code></a>)</p>
<p>Like most things, it’s delightfully simple once you know. For a type
parameter T, the required interface is:</p>

<p>And that’s how we do that.</p>
<p>Well, I hope you enjoyed this tutorial series, and if so, why not
treat yourself to a copy of <a href="https://bitfieldconsulting.com/books/generics">Know Go</a>?
There’s much more to explore, so I’d love you to come along with me for
the ride.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[All-in-one embedding model for interleaved text, images, and screenshots (211 pts)]]></title>
            <link>https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/</link>
            <guid>42162622</guid>
            <pubDate>Sun, 17 Nov 2024 07:42:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/">https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/</a>, See on <a href="https://news.ycombinator.com/item?id=42162622">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>




<p>TL;DR — We are excited to announce <code>voyage-multimodal-3</code>, a new state-of-the-art for multimodal embeddings and a big step forward towards seamless RAG and semantic search for documents rich with both visuals and text. Unlike existing multimodal embedding models, <code>voyage-multimodal-3</code> is capable of vectorizing interleaved texts + images and capturing key visual features from screenshots of PDFs, slides, tables, figures, and more, thereby eliminating the need for complex document parsing. <code>voyage-multimodal-3</code> improves retrieval accuracy by an average of 19.63% over the next best-performing multimodal embedding model when evaluated across 3 multimodal retrieval tasks (20 total datasets).</p>



<p>Two months ago, we released the <a href="https://blog.voyageai.com/2024/09/18/voyage-3/" rel="nofollow" target="_blank"><code>voyage-3</code> and <code>voyage-3-lite</code></a> series of multilingual text embedding models, providing best-in-class performance across a variety of datasets. Today, we’re excited to introduce <code>voyage-multimodal-3</code>, our first multimodal embedding model and a big step toward RAG and semantic search for knowledge bases rich with both visuals and text.</p>



<p><code>voyage-multimodal-3</code> supports text and content-rich images such as screenshots of texts, figures, tables, PDFs, slide decks, and more. The resultant vectors capture critical textual and visual features such as font size, text location, whitespace, etc. This eliminates the need for heuristic-based document parsing, which often struggles with accuracy when layouts are complex or interspersed with figures and photos. Unlike existing multimodal embedding models that handle either a single text or image input, <code>voyage-multimodal-3</code> allows for interleaved texts and images for maximum flexibility. Our <a href="https://colab.research.google.com/drive/12aFvstG8YFAWXyw-Bx5IXtaOqOzliGt9" rel="nofollow" target="_blank">sample notebook</a> demonstrates all of these features.</p>



<p><code>voyage-multimodal-3</code> has an architecture that is similar to that of modern vision-language transformers. This makes it a significant departure from existing multimodal embedding models, including, but not limited to, OpenAI CLIP large (<code>clip-vit-large-patch14-336</code>) and Cohere multimodal v3 (<code>embed-multimodal-v3.0</code>).</p>



<figure><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="1440" height="800" data-attachment-id="1166" data-permalink="https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/slide-2/" data-orig-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?fit=1440%2C800&amp;quality=80&amp;ssl=1" data-orig-size="1440,800" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Slide 2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?fit=300%2C167&amp;quality=80&amp;ssl=1" data-large-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?fit=1024%2C569&amp;quality=80&amp;ssl=1" src="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?resize=1440%2C800&amp;quality=80&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?w=1440&amp;quality=80&amp;ssl=1 1440w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?resize=300%2C167&amp;quality=80&amp;ssl=1 300w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?resize=1024%2C569&amp;quality=80&amp;ssl=1 1024w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?resize=768%2C427&amp;quality=80&amp;ssl=1 768w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?resize=1200%2C667&amp;quality=80&amp;ssl=1 1200w" sizes="(max-width: 1000px) 100vw, 1000px"></figure>



<p>In a set of evaluations across 20 multimodal retrieval datasets and 34 text retrieval datasets, we found that <code>voyage-multimodal-3</code>:</p>



<ol>
<li>Outperforms OpenAI CLIP large and Cohere multimodal v3 by an average of 41.44% (a 2.1x improvement) and 43.37% (a 2.2x improvement) on table/figure retrieval, 26.54% and 25.84% on document screenshot retrieval, and 6.55% and 5.86% on text-to-photo retrieval, respectively.</li>



<li>Outperforms OpenAI v3 large and Cohere multimodal/English<sup>1</sup> v3 by 5.13% and 13.70% on text-only datasets, respectively.</li>
</ol>



<h3>Support for Interleaved Text &amp; Images</h3>



<p>All existing commonly used multimodal embedding models (such as Amazon Titan Multimodal G1, Google Vertex AI multimodal, and Cohere multimodal v3) are based on OpenAI’s CLIP, which processes different modalities of data through independent networks. In other words, images <em>must</em> be vectorized through the vision tower, while text <em>must</em> be vectorized through the text tower, preventing these models from being able to processing interleaved data.</p>



<figure><img data-recalc-dims="1" decoding="async" width="1440" height="800" data-attachment-id="1167" data-permalink="https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/slide-1/" data-orig-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?fit=1440%2C800&amp;quality=80&amp;ssl=1" data-orig-size="1440,800" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Slide 1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?fit=300%2C167&amp;quality=80&amp;ssl=1" data-large-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?fit=1024%2C569&amp;quality=80&amp;ssl=1" src="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?resize=1440%2C800&amp;quality=80&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?w=1440&amp;quality=80&amp;ssl=1 1440w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?resize=300%2C167&amp;quality=80&amp;ssl=1 300w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?resize=1024%2C569&amp;quality=80&amp;ssl=1 1024w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?resize=768%2C427&amp;quality=80&amp;ssl=1 768w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?resize=1200%2C667&amp;quality=80&amp;ssl=1 1200w" sizes="(max-width: 1000px) 100vw, 1000px"></figure>



<p>In contrast, <code>voyage-multimodal-3</code> vectorizes both modalities of data directly within the same transformer encoder, ensuring that both text and visual features are treated as part of a unified representation rather than distinct components. This mimics the model architecture of the latest vision-language models, only for vectorization rather than generation. As a result, interleaved texts and images, document screenshots, PDFs with complex layouts, annotated images, etc can be vectorized in a way that preserves the contextual relationship between visual and textual information.</p>



<h3>Mixed Modality Search with Screenshots</h3>



<p>All CLIP-like models perform poorly on mixed-modality search due to a phenomenon known as the <a href="https://arxiv.org/abs/2203.02053" rel="nofollow" target="_blank">modality gap</a>. As illustrated in the figure below, the closest vector to the snippet “I address you, members of the Seventy-Seventh Congress…” is not its screenshot, but other texts. This leads to search results that are skewed towards items of the same modality; in other words, text vectors will be closer to irrelevant texts than relevant images in the embedding space.</p>



<figure><img data-recalc-dims="1" decoding="async" width="692" height="660" data-attachment-id="1171" data-permalink="https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/slide-3-v3/" data-orig-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-3-v3.png?fit=692%2C660&amp;quality=80&amp;ssl=1" data-orig-size="692,660" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Slide 3 v3" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-3-v3.png?fit=300%2C286&amp;quality=80&amp;ssl=1" data-large-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-3-v3.png?fit=692%2C660&amp;quality=80&amp;ssl=1" src="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-3-v3.png?resize=692%2C660&amp;quality=80&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-3-v3.png?w=692&amp;quality=80&amp;ssl=1 692w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-3-v3.png?resize=300%2C286&amp;quality=80&amp;ssl=1 300w" sizes="(max-width: 692px) 100vw, 692px"></figure>



<p>To illustrate this issue quantitatively, we conducted an experiment involving mixed-modality data. We created two sets of PyTorch documentation with identical content: one set as plain text (strings) and and the other set as screenshots. By combining a subset of text-based documentation with screenshots of remaining subset, we created a series of mixed-modality datasets. Each dataset represented a different proportion of text and screenshots, ranging from 0% to 100% screenshots. We then evaluated the retrieval accuracy of various multimodal models on these datasets, reporting the <a href="http://www.evidentlyai.com/ranking-metrics/ndcg-metric#:~:text=Normalized%20Discounted%20Cumulative%20Gain%20(NDCG)%20is%20a%20ranking%20quality%20metric,DCG%20representing%20a%20perfect%20ranking." rel="nofollow" target="_blank">normalized discounted cumulative gain</a>&nbsp;(NDCG@10) for each model across different screenshot ratios.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1920" height="1080" data-attachment-id="1172" data-permalink="https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/voyage-multimodal-3_results-001/" data-orig-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?fit=1920%2C1080&amp;quality=80&amp;ssl=1" data-orig-size="1920,1080" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="voyage-multimodal-3_results.001" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?fit=300%2C169&amp;quality=80&amp;ssl=1" data-large-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?fit=1024%2C576&amp;quality=80&amp;ssl=1" src="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?resize=1920%2C1080&amp;quality=80&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?w=1920&amp;quality=80&amp;ssl=1 1920w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?resize=300%2C169&amp;quality=80&amp;ssl=1 300w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?resize=1024%2C576&amp;quality=80&amp;ssl=1 1024w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?resize=768%2C432&amp;quality=80&amp;ssl=1 768w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?resize=1536%2C864&amp;quality=80&amp;ssl=1 1536w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?resize=1200%2C675&amp;quality=80&amp;ssl=1 1200w" sizes="auto, (max-width: 1000px) 100vw, 1000px"></figure>



<p>As shown above, CLIP-based models experience a decline in retrieval quality as the proportion of screenshots increases up to 90%, highlighting a retrieval bias influenced by modality. Moreover, these models perform poorly when all text is converted to images.</p>



<p>In contrast, <code>voyage-multimodal-3</code> is not only the most performant for all ratios, but also has little-to-no performance drop across the board, indicating that the vectors truly capture the semantic content contained in the screenshots. This robustness is due to the model’s unique approach of processing all input modalities through the same backbone.</p>



<p>With <code>voyage-multimodal-3</code>, there is no longer a need for screen parsing models, layout analysis, or any other complex text extraction pipelines; you can easily vectorize a knowledge base containing both pure-text documents as well unstructured data (such as PDFs/slides/webpages/etc) — screenshots are all you need.</p>



<h3>Evaluation Details</h3>



<p><strong>Datasets.</strong> We evaluate <code>voyage-multimodal-3</code> across 20 multimodal datasets spanning three different tasks: table/figure retrieval, document screenshot retrieval, and text-to-photo retrieval. We also evaluate <code>voyage-multimodal-3</code> on a standard text retrieval task spanning 34 datasets in 6 domains (law, finance, conversation, code, web, and tech).</p>



<p>For all datasets, the query is text, while the document could be a figure, photo, text, document screenshot, or a combination of these. For each task, we use prior top-performing models as the baseline. Alongside task names, we provide each task’s corresponding description and datasets used in the table below:</p>



<figure><table><thead><tr><th>Task</th><th>Description</th><th><strong>Datasets</strong></th></tr></thead><tbody><tr><td>Table/figure retrieval</td><td>Table/figure retrieval measures the strength of a model’s ability to match an image containing a table or figure (charts, graphs, etc) with descriptions, captions, or other textual queries which reference the figure.</td><td>charxiv, mmtab-test, ChartQA, Chartve, FintabnetQA, PlotQA,</td></tr><tr><td>Document screenshot retrieval</td><td>In this category, models are used to match queries with scans or screenshots of documents containing both text and charts.</td><td>Energy, Healthcare Industry, Artificial Intelligence, Government Report, InfoVQA, DocVQA, ArxivQA, TabFQuad, TAT-DQA, Shift Project</td></tr><tr><td>Text-to-photo retrieval</td><td>This is the typical text-to-image matching used by CLIP and other CLIP-like models, where queries are associated with the most semantically relevant photos.</td><td>meme-cap, mm-imdb, winoground, docci</td></tr><tr><td>Standard text retrieval</td><td>Standard text retrieval retrieves relevant documents by matching query strings with document strings.</td><td>LeCaRDv2, LegalQuAD legal_summarization, AILA_casedocs, AILA_statutes, rag-benchmark-finance-apple-10K-2022, financebench, TAT-QA, finance-alpaca-csv fiqa-personal-finance-dataset, finance-financialmodelingprep-stock-news-sentiments-rss-feed, ConvFinQA, finqa, hc3_finance, dialogsum, QAConv, HQA-data, LeetCodeCpp-new, LeetCodeJava-new, LeetCodePython-new, humaneval, mbpp, ds1000-referenceonly, ds1000, apps_5doc, Huffpostsports, Huffpostscience, Doordash, Healthforcalifornia, Cohere, 5GEdge, OneSignal, Langchain, PyTorch1024</td></tr></tbody></table></figure>



<p>Note that the standard text retrieval task encompasses all datasets used to evaluate <code>voyage-3</code> and <code>voyage-3-lite</code> except long context and multilingual datasets. See our <a href="https://blog.voyageai.com/2024/09/18/voyage-3/" rel="nofollow" target="_blank">previous blog post</a> for more information.</p>



<p><strong>Models</strong>. For the three multimodal tasks, we evaluate&nbsp;<code>voyage-multimodal-3</code>&nbsp;alongside four alternative multimodal embedding models: <a href="https://huggingface.co/openai/clip-vit-large-patch14-336" rel="nofollow" target="_blank">OpenAI CLIP large</a> (<code>clip-vit-large-patch14-336</code>), <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html" rel="nofollow" target="_blank">Amazon Titan Multimodal Embeddings G1</a> (<code>amazon.titan-embed-image-v1</code>), <a href="https://docs.cohere.com/reference/embed" rel="nofollow" target="_blank">Cohere multimodal v3</a> (<code>embed-multimodal-v3.0</code>), and <a href="https://huggingface.co/google/siglip-so400m-patch14-384" rel="nofollow" target="_blank">SigLIP So400M</a> (<code>siglip-so400m-patch14-384</code>). We also evaluate <a href="https://huggingface.co/vidore/colqwen2-v0.1" rel="nofollow" target="_blank">ColQwen2 v0.1</a> (<code>colqwen-v0.1</code>), a late interaction model that outputs many embeddings per document.</p>



<p>For the standard text retrieval task, we evaluate <code>voyage-multimodal-3</code> alongside <a href="https://platform.openai.com/docs/guides/embeddings" rel="nofollow" target="_blank">OpenAI v3 large</a> (<code>text-embeddings-3-large</code>), Cohere multimodal/English<sup>1</sup> v3, and <code>voyage-3</code>.</p>



<p><strong>Metrics.</strong> Given a query, we retrieve the top 10 results by cosine similarity and report the&nbsp;NDCG@10.</p>



<h3>Results</h3>



<p><strong>Multimodal retrieval</strong>. As shown in the figure below, <code>voyage-multimodal-3</code> outperforms OpenAI CLIP large, Amazon Titan Multimodal G1, Cohere multimodal v3, SigLIP So400M, and ColQwen2 v0.1 by:</p>



<ul>
<li>41.44%, 45.00%, 43.37%, 20.66%, and 6.14% on table/figure retrieval, respectively</li>



<li>26.54%, 37.68%, 25.84%, 35.62%, and 0.98% on document screenshot retrieval, respectively</li>



<li>6.55%, 5.16%, 5.86%, 3.42%, and 10.34% on text-to-photo retrieval, respectively</li>
</ul>



<p><strong>Standard text retrieval</strong>. As shown in the figure below, <code>voyage-multimodal-3</code> outperforms OpenAI v3 large and Cohere multimodal/English<sup>1</sup> v3 by 5.13% and 13.70%, respectively. The performance of <code>voyage-multimodal-3</code> is 0.05% better than that of <code>voyage-3</code>, making the two comparable in terms of retrieval accuracy for pure text documents.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1920" height="1080" data-attachment-id="1226" data-permalink="https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/voyage-multimodal-3_results-002-4/" data-orig-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?fit=1920%2C1080&amp;quality=80&amp;ssl=1" data-orig-size="1920,1080" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="voyage-multimodal-3_results.002" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?fit=300%2C169&amp;quality=80&amp;ssl=1" data-large-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?fit=1024%2C576&amp;quality=80&amp;ssl=1" src="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?resize=1920%2C1080&amp;quality=80&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?w=1920&amp;quality=80&amp;ssl=1 1920w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?resize=300%2C169&amp;quality=80&amp;ssl=1 300w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?resize=1024%2C576&amp;quality=80&amp;ssl=1 1024w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?resize=768%2C432&amp;quality=80&amp;ssl=1 768w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?resize=1536%2C864&amp;quality=80&amp;ssl=1 1536w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?resize=1200%2C675&amp;quality=80&amp;ssl=1 1200w" sizes="auto, (max-width: 1000px) 100vw, 1000px"></figure>



<p>All evaluation results are available in <a href="https://docs.google.com/spreadsheets/d/1LAbDBkzO--LGR9y4FWzqhFsImO6ic5NITxgPWGmMIP4/edit?gid=222347087#gid=222347087" rel="nofollow" target="_blank">this spreadsheet</a>.</p>



<h3>Try voyage-multimodal-3 now!</h3>



<p><code>voyage-multimodal-3</code> is available today! The first 200 million tokens are free. To get started, check out our <a href="https://colab.research.google.com/drive/12aFvstG8YFAWXyw-Bx5IXtaOqOzliGt9" rel="nofollow" target="_blank">sample notebook</a>, or head over to our&nbsp;<a href="https://docs.voyageai.com/docs/multimodal-embeddings" rel="nofollow" target="_blank">docs</a>&nbsp;to learn more.</p>



<p>If you’re also interested in fine-tuned embedding models, we’d love to hear from you—please email us at&nbsp;<a href="mailto:contact@voyageai.com">contact@voyageai.com</a>. Follow us on&nbsp;<a href="https://x.com/VoyageAI" rel="nofollow" target="_blank">X (Twitter)</a>&nbsp;and&nbsp;<a href="https://www.linkedin.com/company/voyageai/" rel="nofollow" target="_blank">LinkedIn</a>,&nbsp;and join our&nbsp;<a href="https://discord.gg/zAU7GQEmvT" rel="nofollow" target="_blank">Discord</a>&nbsp;for more updates.</p>



<hr>



<p><sup>1 </sup>Cohere multimodal v3 uses Cohere English v3 (<code>embed-english-v3.0</code>) for the text tower, which makes the both models’ vectors identical on pure text. To minimize confusion, we use “Cohere multimodal v3” as the only label in the charts.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CSS gets a new logo and it uses the color `rebeccapurple` (724 pts)]]></title>
            <link>https://michaelcharl.es/aubrey/en/code/new-rebeccapurple-css-logo</link>
            <guid>42161919</guid>
            <pubDate>Sun, 17 Nov 2024 04:18:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://michaelcharl.es/aubrey/en/code/new-rebeccapurple-css-logo">https://michaelcharl.es/aubrey/en/code/new-rebeccapurple-css-logo</a>, See on <a href="https://news.ycombinator.com/item?id=42161919">Hacker News</a></p>
Couldn't get https://michaelcharl.es/aubrey/en/code/new-rebeccapurple-css-logo: Error: connect ECONNREFUSED 2400:8902::f03c:92ff:fe5d:e614:443]]></description>
        </item>
        <item>
            <title><![CDATA[Two Nobel Prize winners want to cancel their own CRISPR patents in Europe (131 pts)]]></title>
            <link>https://www.technologyreview.com/2024/09/25/1104475/nobel-prize-winners-cancel-crispr-patents-europe/</link>
            <guid>42161664</guid>
            <pubDate>Sun, 17 Nov 2024 03:03:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.technologyreview.com/2024/09/25/1104475/nobel-prize-winners-cancel-crispr-patents-europe/">https://www.technologyreview.com/2024/09/25/1104475/nobel-prize-winners-cancel-crispr-patents-europe/</a>, See on <a href="https://news.ycombinator.com/item?id=42161664">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div> <p>In the decade-long fight to control CRISPR, the super-tool for modifying DNA, it’s been common for lawyers to try to overturn patents held by competitors by pointing out errors or inconsistencies.</p>  <p>But now, in a surprise twist, the team that earned the Nobel Prize in chemistry for developing CRISPR is asking to cancel two of their own seminal patents, <em>MIT Technology Review</em> has learned. The decision could affect who gets to collect the lucrative licensing fees on using the technology.</p> </div><div> <p>­­The request to withdraw the pair of European patents, by lawyers for Nobelists Emmanuelle Charpentier and Jennifer Doudna, comes after a damaging August opinion from a European technical appeals board, which ruled that the duo’s earliest patent filing didn’t explain CRISPR well enough for other scientists to use it and doesn’t count as a proper invention.</p>  <p>The Nobel laureates’ lawyers say the decision is so wrong and unfair that they have no choice but to preemptively cancel their patents, a scorched-earth tactic whose aim is to prevent the unfavorable legal finding from being recorded as the reason.&nbsp;</p> 
 <p>“They are trying to avoid the decision by running away from it,” says Christoph Then, founder of <a href="https://www.testbiotech.org/en/">Testbiotech</a>, a German nonprofit that is among those opposing the patents, who provided a copy of the technical opinion and response letter to <em>MIT Technology Review</em>. “We think these are some of the earliest patents and the basis of their licenses.”</p>  <h3>Discovery of the century</h3>  <p>CRISPR has been called the <a href="https://www.technologyreview.com/2014/12/04/170211/who-owns-the-biggest-biotech-discovery-of-the-century/">biggest biotech discovery of the century, </a>and the battle to control its commercial applications—such as gene-altered plants, modified mice, and new medical treatments—has raged for a decade.</p> 
</div><div> <p>The dispute primarily pits Charpentier and Doudna, who were honored with the Nobel Prize in 2020 for developing the method of genome editing, against Feng Zhang, a researcher at the Broad Institute of MIT and Harvard, who claimed to have invented the tool first on his own.</p>  <p>Back in 2014, the Broad Institute carried out a coup de main when it managed to win, and <a href="https://www.technologyreview.com/2017/02/15/153986/patent-office-hands-win-in-crispr-battle-to-broad-institute/">later defend</a>, the controlling US patent on CRISPR’s main uses. But the Nobel pair<strong> </strong>could, and often did, point to their European patents as bright points in their fight. In 2017, the University of California, Berkeley, where Doudna works, <a href="https://news.berkeley.edu/2017/03/28/european-patent-office-to-grant-uc-a-broad-patent-on-crispr-cas9/">touted its first European patent as exciting, “broad,” and “precedent” setting</a>.</p>  <p>After all, a region representing more than 30 countries had not only recognized the pair’s pioneering discovery; it had set a standard for other patent offices around the world. It also made the US Patent Office look like<a href="https://ipwatchdog.com/2024/05/06/crispr-battle-lens-international-patent-harmonization/id=176202/"> an outlier</a> whose decisions favoring the Broad Institute might not hold up long term. A further appeal challenging the US decisions is pending in federal court.</p>  <h3>Long-running saga</h3>  <p>But now the European Patent Office is also saying—for different reasons—that Doudna and Charpentier can’t claim their basic invention. And that’s a finding their attorneys think is so damaging, and reached in such an unjust way, that they have no choice but to sacrifice their own patents. “The Patentees cannot be expected to expose the Nobel-prize winning invention … to the repercussions of a decision handed down under such circumstances,” says the 76<em>-</em>page letter sent by German attorneys on their behalf on September 20.</p> </div><div> <p>The chief intellectual-property attorney at the University of California, Randi Jenkins, confirmed the plan to revoke the two patents but downplayed their importance.&nbsp;</p>  <p>“These two European patents are just another chapter in this long-running saga involving CRISPR-Cas9,” Jenkins said. “We will continue pursuing claims in Europe, and we expect those ongoing claims to have meaningful breadth and depth of coverage.”</p>  <p>The patents being voluntarily disavowed are EP2800811, granted in 2017, and EP3401400, granted in 2019. Jenkins added the Nobelists still share one issued CRISPR patent in Europe, EP3597749, and one that is pending. That tally doesn’t include a thicket of patent claims covering more recent research from Doudna’s Berkeley lab that were filed separately.</p>  <h3>Freedom to operate</h3>  <p>The cancellation of the European patents will affect a broad network of biotech companies that have bought and sold rights as they seek to achieve either commercial exclusivity to new medical treatments or what’s called “freedom to operate”—the right to pursue gene-slicing research unmolested by doubts over who really owns the technique.&nbsp;</p> 

 <p>These companies include Editas Medicine, allied with the Broad Institute; Caribou Biosciences and Intellia Therapeutics in the US, both cofounded by Doudna; and Charpentier’s companies, CRISPR Therapeutics and ERS Genomics.</p>  <p>ERS Genomics, which is based in Dublin and calls itself “the CRISPR licensing company,” was set up in Europe specifically to collect fees from others using CRISPR. It claims to have sold nonexclusive access to its “foundational patents” to more than 150 companies, universities, and organizations who use CRISPR in their labs, manufacturing, or research products.</p>  <p>For example,<a href="https://ersgenomics.com/ers-genomics-and-stemsight-sign-crispr-cas9-license-agreement/"> earlier this year</a> Laura Koivusalo, founder of a small Finnish biotech company, StemSight, agreed to a “standard fee” because her company is researching an eye treatment using stem cells that were previously edited using CRISPR.</p>  <p>Although not every biotech company thinks it’s necessary to pay for patent rights long before it even has a product to sell, Koivusalo decided it would be the right thing to do. “The reason we got the license was the Nordic mentality of being super honest. We asked them if we needed a license to do research, and they said yes, we did,” she says.</p> </div><div> <p>A slide deck from ERS<a href="https://www.slideshare.net/slideshow/ers-licensing-deck-for-bio-crispr-cas9pdf/266623574"> available online</a> lists the fee for small startups like hers at $15,000 a year. Koivusalo says she agreed to buy a license to the same two patents that are now being canceled. She adds: “I was not aware they were revoked. I would have expected them to give a heads-up.”&nbsp;</p>  <p>A spokesperson for ERS Genomics said its customers still have coverage in Europe based on the Nobelists’ remaining CRISPR patent and pending application.</p> </div><div><p>In the US, the Broad Institute has also been selling licenses to use CRISPR. And the fees can get big if there’s an actual product involved. That was the case last year, when Vertex Pharmaceuticals won approval to sell<a href="https://www.technologyreview.com/2024/01/08/1085101/crispr-gene-editing-sickle-cell-disease-breakthrough-technologies/"> the first CRISPR-based treatment</a>, for sickle-cell disease. To acquire rights under the Broad Institute’s CRISPR patents, Vertex<a href="https://www.technologyreview.com/2023/12/13/1085209/vertex-license-controversial-crispr-patent-editas/"> agreed to pay</a> $50 million on the barrelhead—and millions more in the future.</p>  <h3>PAM problem</h3>  <p>There’s no doubt that Charpentier and Doudna were first to publish, in a 2012 paper, how CRISPR can function as a “programmable” means of editing DNA. And their patents in Europe withstood an initial round of formal oppositions filed by lawyers.</p> 
 <p>But this August, in a separate analysis, a technical body decided that Berkeley had omitted a key detail from its earliest patent application, making it so that “the skilled person could not carry out the claimed method,” according to the finding. That is, it said, the invention wasn’t fully described or enabled.</p>  <p>The omission relates to a feature of DNA molecules called “protospacer adjacent motifs,” or PAMs. These features, a bit like runway landing lights, determine at what general locations in a genome the CRISPR gene scissors are able to land and make cuts, and where they can’t.</p>  <p>In the 76-page reply letter sent by lawyers for the Nobelists, they argue there wasn’t really any need to mention these sites, which they say were so obvious that “even undergraduate students” would have known they were needed.&nbsp;</p>  <p>The lengthy letter leaves no doubt the Nobel team feels they’ve been wronged. In addition to disavowing the patents, the text runs on because it seeks to “make of public record the reasons for which we strongly disagree with [the] assessment on all points” and to “clearly show the incorrectness” of the decision, which, they say, “fails to recognize the nature and origin of the invention, misinterprets the common general knowledge, and additionally applies incorrect legal standards.” </p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Xogot – Godot for iPad (136 pts)]]></title>
            <link>https://xogot.com/</link>
            <guid>42161223</guid>
            <pubDate>Sun, 17 Nov 2024 01:32:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xogot.com/">https://xogot.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42161223">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="cookieSettings"><h2>Cookie Settings</h2><p>We use cookies to improve user experience. Choose what cookie categories you allow us to use. You can read more
about our Cookie Policy by clicking on Cookie Policy below.</p><div><div><p><label><span>Essential (required)</span></label></p><p>These cookies enable strictly necessary cookies for security, language support and verification of identity.
These cookies can’t be disabled.</p></div><div id="checkbox_functionality"><p><label><span>Functionality</span></label></p><p>These cookies collect data to remember choices users make to improve and give a better
user experience. Disabling can cause some parts of the site to not work properly.</p></div><div id="checkbox_performance"><p><label><span>Performance &amp; Analytics</span></label></p><p>These cookies help us to understand how visitors interact with our website, help us measure and analyze traffic to improve our service.</p></div><div id="checkbox_targeting"><p><label><span>Targeting &amp; Advertising</span></label></p><p>These cookies help us to better deliver marketing content and customized ads.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pentagon fails 7th audit in a row but says progress made (111 pts)]]></title>
            <link>https://thehill.com/policy/defense/4992913-pentagon-fails-7th-audit-in-a-row-but-says-progress-made/</link>
            <guid>42160768</guid>
            <pubDate>Sun, 17 Nov 2024 00:12:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thehill.com/policy/defense/4992913-pentagon-fails-7th-audit-in-a-row-but-says-progress-made/">https://thehill.com/policy/defense/4992913-pentagon-fails-7th-audit-in-a-row-but-says-progress-made/</a>, See on <a href="https://news.ycombinator.com/item?id=42160768">Hacker News</a></p>
Couldn't get https://thehill.com/policy/defense/4992913-pentagon-fails-7th-audit-in-a-row-but-says-progress-made/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Effect of a giant meteorite impact on Paleoarchean environment and life (130 pts)]]></title>
            <link>https://www.chemistryworld.com/news/meteorite-200-times-larger-than-one-that-killed-dinosaurs-reset-early-life/4020391.article</link>
            <guid>42160716</guid>
            <pubDate>Sun, 17 Nov 2024 00:02:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chemistryworld.com/news/meteorite-200-times-larger-than-one-that-killed-dinosaurs-reset-early-life/4020391.article">https://www.chemistryworld.com/news/meteorite-200-times-larger-than-one-that-killed-dinosaurs-reset-early-life/4020391.article</a>, See on <a href="https://news.ycombinator.com/item?id=42160716">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A giant meteorite that slammed into Earth over 3 billion years ago devastated early microbial life in the oceans, but also freed up a nutrient bonanza.</p>
<div data-attachment="537343" data-sequence="2">
<p><img alt="Meteorite and Earth artist impression" src="https://d2cbg94ubxgsnp.cloudfront.net/Pictures/480xany/3/4/3/537343_gettyimages460713851_476389_crop.jpg" sizes="(max-width: 1023px) 100vw, 780px" srcset="https://d2cbg94ubxgsnp.cloudfront.net/Pictures/480xany/3/4/3/537343_gettyimages460713851_476389_crop.jpg 480w,https://d2cbg94ubxgsnp.cloudfront.net/Pictures/600xany/3/4/3/537343_gettyimages460713851_476389_crop.jpg 600w,https://d2cbg94ubxgsnp.cloudfront.net/Pictures/780xany/3/4/3/537343_gettyimages460713851_476389_crop.jpg 780w" loading="eager" width="4827" height="3218"></p>


</div>
<p>This meteorite was far larger than the infamous Cretaceous era ending one. ‘We’re looking at a bolide that was 500 to 200 times bigger than the one that killed off the dinosaurs,’ says <a href="https://eps.harvard.edu/people/nadja-drabon">Nadja Drabon</a>, a geologist at Harvard University.</p>
<p>The Archean eon 2.5–4 billion years ago suffered at least 16 major impacts by meteorites upwards of 10km across. Each would have vaporised enough rock to darken the ancient skies for years.</p>
<p>Drabon’s group say the impact 3.26 billion years ago triggered a giant tsunami, as well as clouding the oceans and darkening the skies for years to decades. The impact also evaporated tens of metres of seawater.</p>
<p>Yet there was a silver lining: the churning of the seas brought bioavailable iron up from the ocean depths to its depleted surface and allowed some microbes to flourish, while the meteorite also brought phosphorus vital for life.</p>
<p>Drabon and her colleagues went in search of evidence of ancient major impacts in a remote area south of Kruger National Park in South Africa. There they sought out rocky outcrops containing a layer of spherules – molten droplets formed following a major meteorite impact that rained down over huge swathes of the planet. There are eight such spherule bands in this area, each preserving an ancient impact event.</p>
<p>While the impact crater itself is long gone, analysis of rocks from 3.26 billion years ago tells a tale of planetary devastation. The layer of spherules from this huge impact was 15 to 20cm thick in places, compared with less than a centimetre for the famed dinosaur-killing meteorite, says Drabon.</p>
<p>These ancient droplets contain spikes in iridium and chromium isotope anomalies, revealing their extraterrestrial origins. The glassy spherules were mixed into ripped-up seafloor materials, thought to be the result of gigantic tsunamis.</p>
<div data-attachment="537342" data-sequence="1">
<p><img alt="Figure" src="https://d2cbg94ubxgsnp.cloudfront.net/Pictures/480xAny/3/4/2/537342_giant_asteroid_pumelled_archaen_life14_637911.jpg" srcset="https://d2cbg94ubxgsnp.cloudfront.net/Pictures/480xAny/3/4/2/537342_giant_asteroid_pumelled_archaen_life14_637911.jpg 480w" loading="lazy" width="1347" height="1882">m</p>


</div>
<p>Above this layer are younger iron-rich rocks and minerals such as siderite and pyrite. ‘After the impact, we suddenly see a spike in iron in the sediment,’ says Drabon. This influx likely came from the deep ocean. The geologists also found a phosphorus spike after impact, calculating that the meteorite could have delivered 363 billion tonnes in a reduced and, therefore, bioavailable state.</p>
<p>Tellingly, the group also saw a change in the carbon isotope signature above the impact layer that they interpret as a rise in microbes with metabolism geared towards efficient iron cycling.&nbsp;This indicates a rebound of microbial life, perhaps within a few years to decades, the investigators conclude.</p>
<p>The group reason that, following the impact, the iron-metabolising microbes would have consumed the newly available Fe<sup>2+</sup> in the shallow seas, transforming it to Fe<sup>3+</sup>, which would sink to the ocean floor as iron hydroxides.</p>
<p>‘Their argument is very compelling, with many different lines of evidence,’ says <a href="https://cst.temple.edu/about/faculty-staff/alexandra-davatzes">Alexandra Davatzes</a>, a geologist at Temple University in Philadelphia, who was not part of the study. ‘We’re in a totally microbial world in the Archean and the potential for recovery would be much, much faster than today.’</p>
<p>This bounce back could have happened repeatedly. ‘Initially large bolide impacts might have killed off a lot of microbial activity on Earth’s surface and in shallow waters, but then have been good for microbial activity, especially in shallow water environments,’ says <a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/geowissenschaften/arbeitsgruppen/geo-und-umweltnaturwissenschaften/geo-und-umweltnaturwissenschaften/isotopengeochemie/arbeitsgruppe/staff/ronny-schoenberg/">Ronny Schönberg</a>, a geochemist at the University of Tübingen, Germany who was not part of the research.</p>
<p>It might even shift how we view meteorite hits on early Earth. ‘Everyone associates big impacts with the extinction of the dinosaurs and thinks of them as disastrous,’ says Drabon. ‘But they also carried a lot of transient benefits for the early and evolving biosphere.’</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[James Gleick's Chaos: The Software (237 pts)]]></title>
            <link>https://github.com/rudyrucker/chaos</link>
            <guid>42160647</guid>
            <pubDate>Sat, 16 Nov 2024 23:52:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/rudyrucker/chaos">https://github.com/rudyrucker/chaos</a>, See on <a href="https://news.ycombinator.com/item?id=42160647">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">James Gleick's CHAOS: The Software</h2><a id="user-content-james-gleicks-chaos-the-software" aria-label="Permalink: James Gleick's CHAOS: The Software" href="#james-gleicks-chaos-the-software"></a></p>
<p dir="auto">This is a free release of the source, manual, and executables of a 1991 Autodesk DOS program that was called  "James Gleick's CHAOS: The Software." The software was written by Josh Gordon, Rudy Rucker and John Walker. Rucker wrote most of the algorithms, except for the Fractal Landscapes algorithms, which are by John Walker.  Josh Gordon did the interface, and much of the implementation of the algorithm code. The program was written in consultation with James Gleick about his brilliant book, <a href="https://around.com/books/" rel="nofollow"> <i>Chaos: Making a New Science</i> </a>. This release is under a Gnu license.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a27c746a91d0e022b7fe8edbacff12cc6b446f5a113cb7ae30c952fbd26d68c1/687474703a2f2f7777772e727564797275636b65722e636f6d2f626c6f672f696d61676573372f6368616f73636f7665722e6a7067"><img src="https://camo.githubusercontent.com/a27c746a91d0e022b7fe8edbacff12cc6b446f5a113cb7ae30c952fbd26d68c1/687474703a2f2f7777772e727564797275636b65722e636f6d2f626c6f672f696d61676573372f6368616f73636f7665722e6a7067" height="400" alt="Cover of Chaos Package" data-canonical-src="http://www.rudyrucker.com/blog/images7/chaoscover.jpg"></a></p>
<p dir="auto">Downloads for the Release 1.1</p>
<p dir="auto"><a href="https://github.com/rudyrucker/chaos/releases/download/1.1-chaos/chaos_executable_v1_1.zip">The CHAOS executables and parameter files.</a></p>
<p dir="auto"><a href="https://github.com/rudyrucker/chaos/releases/download/1.1-chaos/chaos_manual.pdf">The CHAOS User manual</a>.</p>
<p dir="auto"><a href="https://github.com/rudyrucker/chaos/archive/1.1-chaos.zip">The CHAOS source code.</a></p>
<p dir="auto">It's possible to run the Chaos program on any virtually any platform, inside a DOS shell called DOSBox. Details on the <a href="https://github.com/rudyrucker/chaos/releases">Releases page</a>.</p>
<p dir="auto">You are free to alter the Chaos code and upload new versions. Or use our algorithms to spin off smaller programs. See our Chaos <a href="https://github.com/rudyrucker/chaos/"> GitHub repository</a> for the Chaos code online.</p>
<p dir="auto">The biggest outstanding upgrades for Chaos might be: (1) Increase the resolution or pixel size of the display. (2) Elmimate our use of the old DOS TSR or "terminate and stay resident" program metashel.exe, made by MetaGraphics Software Corporation. Chaos uses metashel calls for its graphics.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What's In Chaos</h2><a id="user-content-whats-in-chaos" aria-label="Permalink: What's In Chaos" href="#whats-in-chaos"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c0feed1a87cedc9140dbae255b2ffdcd64478f51959d3ed46728f74e3ae32ffa/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f73746172746d656e752e474946"><img src="https://camo.githubusercontent.com/c0feed1a87cedc9140dbae255b2ffdcd64478f51959d3ed46728f74e3ae32ffa/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f73746172746d656e752e474946" width="400" alt="" data-animated-image="" data-canonical-src="http://www.rudyrucker.com/chaos/startmenu.GIF"></a></p>
<p dir="auto">CHAOS has six modules.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4b5652f733b4ebd4023692e7397a7f19c6ead5d321bde7bf3ad73798371cad9b/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f6d616e64656c68656172742e474946"><img src="https://camo.githubusercontent.com/4b5652f733b4ebd4023692e7397a7f19c6ead5d321bde7bf3ad73798371cad9b/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f6d616e64656c68656172742e474946" width="400" alt="" data-animated-image="" data-canonical-src="http://www.rudyrucker.com/chaos/mandelheart.GIF"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/98c58f2ce117ddcdf10fadaedc2a20a31c686be5f486da899fd185c8a06457b5/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f6d616e64656c726f61722e474946"><img src="https://camo.githubusercontent.com/98c58f2ce117ddcdf10fadaedc2a20a31c686be5f486da899fd185c8a06457b5/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f6d616e64656c726f61722e474946" width="400" alt="" data-animated-image="" data-canonical-src="http://www.rudyrucker.com/chaos/mandelroar.GIF"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/92b3f916e37b3e98b319b32ee6beefc00d95d394f483a8961780736d53acd651/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f72686f7273652e474946"><img src="https://camo.githubusercontent.com/92b3f916e37b3e98b319b32ee6beefc00d95d394f483a8961780736d53acd651/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f72686f7273652e474946" width="400" alt="" data-animated-image="" data-canonical-src="http://www.rudyrucker.com/chaos/rhorse.GIF"></a></p>
<p dir="auto">MANDEL. A <i>Mandelbrot Set</i> program, incorporating:  quadratic and cubic Julia sets, quadratic and cubic Mandelbrot sets, and a gnarly cubic connectedness map called the <a href="http://tinyurl.com/rudyfractals" rel="nofollow">Rudy  set</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d3b6800813a798cc15fee180493a506e730173614eb3b91ee530323e371bfa78/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f6d61676e65742e474946"><img src="https://camo.githubusercontent.com/d3b6800813a798cc15fee180493a506e730173614eb3b91ee530323e371bfa78/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f6d61676e65742e474946" width="400" alt="" data-animated-image="" data-canonical-src="http://www.rudyrucker.com/chaos/magnet.GIF"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/cb8d57198ec70fb198195925a5060afb8baad5023fe296f7c772e7550df30eb0/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f6d61676e6574626173696e732e474946"><img src="https://camo.githubusercontent.com/cb8d57198ec70fb198195925a5060afb8baad5023fe296f7c772e7550df30eb0/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f6d61676e6574626173696e732e474946" width="400" alt="" data-animated-image="" data-canonical-src="http://www.rudyrucker.com/chaos/magnetbasins.GIF"></a></p>
<p dir="auto">MAGNETS. A <i>Pendulum and Magnets</i> program showing chaotic physical motion and fractal basins of attraction.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e65bc77a805582db4e422b58371b2ace45c97ed411aff878595504a70b02efa2/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f6174747261637468656e6f6e2e474946"><img src="https://camo.githubusercontent.com/e65bc77a805582db4e422b58371b2ace45c97ed411aff878595504a70b02efa2/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f6174747261637468656e6f6e2e474946" width="400" alt="" data-animated-image="" data-canonical-src="http://www.rudyrucker.com/chaos/attracthenon.GIF"></a> <a target="_blank" rel="noopener noreferrer" href=""><img src="" width="400" alt=""></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/08d65630637a0561c4b15940963ae3e8aae4817f30b9f080c6623d3b8d811654/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f61747472616374796f726b652e474946"><img src="https://camo.githubusercontent.com/08d65630637a0561c4b15940963ae3e8aae4817f30b9f080c6623d3b8d811654/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f61747472616374796f726b652e474946" width="400" alt="" data-animated-image="" data-canonical-src="http://www.rudyrucker.com/chaos/attractyorke.GIF"></a></p>
<p dir="auto">ATTRACT. A <i>Strange Attractors</i> program showing the Lorenz Attractor, the Logistic Map, the Yorke Attractors, and the Henon Attractors.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/50fe72efa595dbb08d41183631f3c40d60ee7e7b6e536e27ea9c2e419644f19d/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f6261726e736c6579666c6f776572732e474946"><img src="https://camo.githubusercontent.com/50fe72efa595dbb08d41183631f3c40d60ee7e7b6e536e27ea9c2e419644f19d/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f6261726e736c6579666c6f776572732e474946" width="400" alt="" data-animated-image="" data-canonical-src="http://www.rudyrucker.com/chaos/barnsleyflowers.GIF"></a></p>
<p dir="auto">GAME. A <i>Barnsley Fractals</i> program showing Iterated Function System fractals such as the famous fractal fern.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7bbef17ab76cb78801a34e58a3104ea9013ae1c7bfa1c709b377b4b98352e8ad/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f666f7267656d656e752e474946"><img src="https://camo.githubusercontent.com/7bbef17ab76cb78801a34e58a3104ea9013ae1c7bfa1c709b377b4b98352e8ad/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f666f7267656d656e752e474946" width="400" alt="" data-animated-image="" data-canonical-src="http://www.rudyrucker.com/chaos/forgemenu.GIF"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1d9d497effd57e32b2e0ed357dc8d133a2ecc707ad07890051dac48ce40cb60b/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f666f726765706c616e65742e474946"><img src="https://camo.githubusercontent.com/1d9d497effd57e32b2e0ed357dc8d133a2ecc707ad07890051dac48ce40cb60b/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f666f726765706c616e65742e474946" width="400" alt="" data-animated-image="" data-canonical-src="http://www.rudyrucker.com/chaos/forgeplanet.GIF"></a></p>
<p dir="auto">FORGE. A <i>Fractal Forgeries</i> program that shows clouds, maps, mountain ranges, and planets based on random fractals.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/216e41f9f199cc1fb1fb72d8851d685c94311c7c1ad62600f6addb34ab88e603/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f686f6467652e474946"><img src="https://camo.githubusercontent.com/216e41f9f199cc1fb1fb72d8851d685c94311c7c1ad62600f6addb34ab88e603/687474703a2f2f7777772e727564797275636b65722e636f6d2f6368616f732f686f6467652e474946" width="400" alt="" data-animated-image="" data-canonical-src="http://www.rudyrucker.com/chaos/hodge.GIF"></a></p> 
<p dir="auto">TOY. A Toy Universes program that shows <i>cellular automata</i>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stop Making Me Memorize the Borrow Checker (170 pts)]]></title>
            <link>https://erikmcclure.com/blog/stop-making-me-memorize-borrow-checker/</link>
            <guid>42160501</guid>
            <pubDate>Sat, 16 Nov 2024 23:29:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://erikmcclure.com/blog/stop-making-me-memorize-borrow-checker/">https://erikmcclure.com/blog/stop-making-me-memorize-borrow-checker/</a>, See on <a href="https://news.ycombinator.com/item?id=42160501">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I started learning Rust about 3 or 4 years ago. I am now knee-deep in several very complex Rust projects that keep slamming into the limitations of the Rust compiler. One of the most common and obnoxious problems is hitting a situation the borrow-checker can’t deal with and realizing that I need to completely re-architect how my program works, because lifetimes are “contagious” the same way async is. Naturally, Rust has both!</p><p>Despite how obviously useful the borrow-checker is in writing correct code, in practice it is horrendous to work with. This is because the borrow checker cannot run until an entire function compiles. Sometimes it seems to refuse to run until my entire file compiles. Because an explicit lifetime must come from somewhere, they have a habit of “floating up” through the stack, from the point of usage to the point of origin, infecting everything in-between with another explicit generic lifetime parameter. If you end up not needing it, you need to go through and delete every instance of this lifetime, which can sometimes be 30 or more generic statements that end up needing to be modified.</p><p>In the worst cases, your entire architecture simply cannot work with the borrow checker, and at minimum you’ll need to wrap things in an Rc&lt;&gt;, which again will requiring upwards of 30 or more statements depending on the complexity of your architecture. Other times you realize you need a split borrow, and have to then modify <em>every single function under the split borrow check</em> to take specific field references instead of the original type. These constant refactors have been a major detractor for the language for years, although some improvements, like <code>impl</code>, have reduced the need for refactoring in some narrow cases.</p><p>This means, to be a highly productive Rust programmer, you basically have to memorize the borrow checker rules, so you get it right the first time. This is stupid, because <em>the whole point</em> of having a type system or a borrow checker is to tell you when you get it wrong, so you <em>don’t</em> have to memorize how the borrow rules work. I don’t need to memorize how all the types work, because these errors get caught almost immediately, and rarely require massive refactors because the whole architecture doesn’t need to exist before it can identify problems.</p><p>This is painful because I am an experienced C++ programmer, and C++ has this exact problem except worse: undefined behavior. In the worst case, C++ simply doesn’t check anything, compiles your code wrong, and then does inexplicable and impossible things at runtime for no discernable reason (or it just deletes your entire function). If you run <code>ubsan</code> (undefined behavior sanitizer), it will at least explode at runtime with an error message. Unfortunately, it can only catch undefined behavior that actually happens, so if your test suite doesn’t cover all your code branches you might have undefined behavior lurking in the code somewhere. Even worse, the very existence of undefined behavior sometimes creates a new branch you couldn’t possibly think of testing without knowing about the undefined behavior in the first place!</p><p>This means that in order to write C++, you effectively have to memorize the undefined behavior rules, which sucks. Sound familiar? This is both stupid and strictly worse than Rust, because there is no compile-time error at all, only a runtime error if you get it wrong (and you are running <code>ubsan</code>). However, because it’s a runtime error, correcting it usually requires less total refactoring… usually.</p><p>At this point, C++ can’t fix it’s undefined behavior problem because C++ uses undefined behavior to drive optimization, so now it’s just stuck like this forever. Rust can’t really fix borrow checking either, because borrow checking is embedded so deeply into the compiler at this point. All Rust can do is make the borrow checker more powerful (probably by introducing partial borrows, which seems stuck in eternal bikeshedding hell) or introduce more powerful IDE tooling that can make refactors less painful and more automatic, like automatically removing a generic parameter from everywhere it was used.</p><p>Problems like these are unfortunate, because it drives people towards using C for it’s “simplicity”, when in reality they are simply deferring logic errors until runtime. I think Rust manages to “get away” with it’s excessive verbosity because “safe C++” is even more horrendously verbose and arcane, and safe C++ is what Rust is really competing against right now. I just think Rust needs more competition.</p><p>Any prospective Rust competitor, however, needs to be very cognizant of the tradeoffs they force programmers to make in exchange for correctness. It is not sufficient to invent a language that makes it possible to write provably correct kernel-level code, it has to be <em>easy to use</em> as well, and we really need to get away from indirectly forcing programmers to anticipate what the compiler will do simply to be productive. It’s not the 1970s anymore, writing a program shouldn’t feel like taking a stack of punchcards to the mainframe to see if it works or not. Rust is not the answer, it is simply a step towards the answer.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Teach yourself to echolocate (218 pts)]]></title>
            <link>https://www.atlasobscura.com/articles/how-to-echolocate</link>
            <guid>42160071</guid>
            <pubDate>Sat, 16 Nov 2024 22:43:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.atlasobscura.com/articles/how-to-echolocate">https://www.atlasobscura.com/articles/how-to-echolocate</a>, See on <a href="https://news.ycombinator.com/item?id=42160071">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="article-body">

            <p><span>Daniel Kish navigates the world </span>like a bat does—and he does so without ever leaving the ground.</p>
<p>After losing his vision as an infant, Kish taught himself to move around with the help of echolocation. Like bats, Kish uses his mouth to produce a series of short, crisp clicking sounds, and then listens to how those sounds bounce off the surrounding landscape. (Our winged neighbors tend to emit these clicks at <a href="https://www.atlasobscura.com/articles/what-does-echolocation-sound-like">frequencies humans can’t hear</a>, but Kish’s clicks are perfectly audible to human ears.) From there, Kish makes a mental map of his environment, considering everything from broad contours—like walls and doors—down to textural details.</p>
<p>Kish now <a href="https://visioneers.org/">teaches echolocation</a>, mostly to students who are blind. For these students, Kish believes that an echolocation practice can buoy confidence and independence. Kish’s own experience is persuasive—he famously <a href="https://www.youtube.com/watch?v=xATIyq3uZM4">bikes along hilly, car-lined streets</a>—and a <a href="https://www.ncbi.nlm.nih.gov/pubmed/23538130">growing body</a> of <a href="http://www.ingentaconnect.com/content/dav/aaua/2009/00000095/00000002/art00013?token=004d187194fb161639412f415d763f256f45504a6c4273516f2530482972715a614f6d4e227ac">scholarly research</a> <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005670#sec015">has begun to unpack exactly how</a> expert echolocaters do their thing. This research has also backed up the idea that this skill <a href="https://www.wired.com/2009/06/echolocation/">is highly learnable</a>. When <a href="https://whitneylab.berkeley.edu/publications.html">researchers at the University of California, Berkeley</a>, asked novice echolocators to use tongue clicks to determine which of the two objects in front of them was larger, the newbies were <a href="https://www.npr.org/sections/13.7/2013/01/28/170355712/be-like-a-bat-sound-can-show-you-the-way">soon able to do so</a> in a way that the scientists couldn’t attribute to chance.</p>
<p>Whatever your sightedness, there’s something to be said for learning to listen more attentively to sonic scenery. Kish believes that vision has a way of blunting the other senses unless people work to really flex them. Deft echolocators, he says, are able to perceive fine differences—distinguishing, say, between an oleander bush (“a million sharp returns”) and an evergreen (“wisps closely packed together, which sound like a bit like a sponge or a curtain”). They’re discovering sonic wonder wherever they go. We asked Kish to tailor a lesson for first-timers just learning to listen to the landscape.</p>
<h2><strong>1) Practice tuning in </strong></h2>
<p>Before you begin producing your own sounds, just practice noticing the ways that sounds change around you. Try this exercise next time you’re in a car (assuming you’re not in the driver’s seat).</p>
<p>Crack open the window and close your eyes. This is a good chance to pass through a varied landscape pretty quickly, and begin to differentiate between sounds. “On a residential street, you should hear the sound of the car jump in and out as you pass other parked cars, possibly trees, posts, mailboxes, or houses near the curb,” Kish says. “Everything we pass reflects the sound of our car differently.” Prime yourself to pay attention to incidental soundtracks.</p>
<h2><strong>2) Pick your supplies</strong></h2>
<p>If you are a sighted person, you’ll want a blindfold. “It’s very, very difficult to discern these kinds of subtleties if your eyes are working at the same time,” Kish says. Occluding one sense gives the less-dominant ones room to stretch their legs.</p>
<p>Now is also a good time to stock up on what you’ll need for your practice sessions. First, you’ll need a metal tray or a bowl, so make sure you’ve got one on hand. Once you start moving through space later on, it will also help to have a trekking pole or a cane, or at least a partner you trust to shout if you wander too far off base.</p>
<figure><img src="https://img.atlasobscura.com/ymF4lUil1TXhLztpByFsXPg3kxyyO6hxZZsn83Q-Cm0/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9mYjcxZTkxNWEw/ZDFjZjcxYzNfRWNo/b2xvY2F0aW9uXzMu/anBn.jpg" alt="For beginners, the best clicks are ones that you can make cleanly and reliably." width="auto" data-kind="article-image" id="article-image-59920" loading="lazy"><figcaption>For beginners, the best clicks are ones that you can make cleanly and reliably.</figcaption></figure>
<h2><strong>3) Choose an environment</strong></h2>
<p>Expert echolocators like Kish can get a bit fancier with their choices, and try to hear the character of a room. Tin decor, buttresses, or other accoutrements that might make a realtor swoon will also give Kish reason to perk up his ears. “It will sound more alive,” he says. “It will sing to you.”</p>
<p>For beginners, picking the right place is a bit of a Goldilocks situation: You don’t want a flat field, where there’s nothing for sound to bounce off. Then again, you ought to steer clear of spots where your hearing will be impeded by, say, a sea of carpet. “Probably the best is a fairly quiet, open space without a lot of clutter, maybe a non-reverberant room,” Kish says.</p>
<h2><strong>4) Practice your clicks</strong></h2>
<p>Clicks are not created equal, and some of them will work against you. “The most commonly produced rubbish click is a ‘cluck,’” Kish says. A cluck sounds something like two clicks on top of each other, which masks the returning sound. A good click can’t be sloppy, and it must be possible to reliably reproduce.</p>
<p>For beginners, Kish says that a dental click fits the bill (this is a <em>tsk-tsk</em> sound, Kish says, “like you’re disappointed”). Another contender is the sound you might use to prompt a horse to giddy-up; a “ch” sound, as in “check” or “church,” is another option.</p>
<p>The key is finding the option that’s comfortable for you. “You settle on whatever click you can do, and stick to it,” Kish says.</p>
<figure><img src="https://img.atlasobscura.com/Wfh1-5xglNkC0CwxYJnlEnvcErFO4nsY0DJuCWQ-IzI/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9mYjcxZTkxNWEw/ZDFjZjcxYzNfRWNo/b2xvY2F0aW9uXzIu/anBn.jpg" alt="Tune in!" width="auto" data-kind="article-image" id="article-image-59921" loading="lazy"><figcaption>Tune in!</figcaption></figure>
<h2><strong>5) Start simple </strong></h2>
<p>The goal with clicking is to take stock of three things. The first is presence/absence (is something there?). Then, the location (what direction is it in?). Finally, distance (how far away is it?).</p>
<p>To teach these skills, Kish often starts with this exercise: Students pair up with a partner who holds a bowl or flat paddle somewhere above their head. The student clicks, turns their head, and tries to gauge where the bowl is—straight ahead, or off to the side?</p>
<p>Kish doesn’t click all the time—only when he needs to refresh the mental map he’s working from. For beginning students, though, it’s helpful to practice the physical mechanics of clicking, in order to learn how to listen to bouncing sounds.</p>
<h2><strong>6) Get moving</strong></h2>
<p>The next step is to do all of this while in motion. Walk along a hallway and try to listen for differences in sounds that might indicate corners or open doors.</p>
<p>At first, you’ll shuffle and fumble through this exercise, and it’s bound to be frustrating. Go ahead and ask your partner whether or not you’re on the right track—but, if you’re using a blindfold, keep it on. “The temptation is very strong to pop the blindfold off and on,” Kish says. “I resist that because there is an adaptation process that has to happen here. You disrupt it entirely when you pull off the blindfold. I wouldn’t use vision to spot-check an experience; I would try to avoid that.”</p>
<h2><strong>7) Stop when you need to</strong></h2>
<p>Moving through the world in a new way can be both thrilling and thoroughly disorienting. Kish has found that people who are sighted, and are unaccustomed to not being able to rely on their vision, need to take breaks every 30-45 minutes. His blind students, for whom non-visual navigation is routine, can hang in longer.</p>
<p>Echolocation takes patience and practice. Kish cautions that it’s hard to get good at this—it took him years. But trying it out can open your ears to the world.</p>






          </section></div>]]></description>
        </item>
    </channel>
</rss>