<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 05 Aug 2025 05:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: I've been building an ERP for manufacturing for the last 3 years (174 pts)]]></title>
            <link>https://github.com/crbnos/carbon</link>
            <guid>44792005</guid>
            <pubDate>Mon, 04 Aug 2025 22:24:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/crbnos/carbon">https://github.com/crbnos/carbon</a>, See on <a href="https://news.ycombinator.com/item?id=44792005">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
   <a href="https://carbon.ms/" rel="nofollow">
      <img width="auto" height="100" alt="Carbon Logo" src="https://private-user-images.githubusercontent.com/64510427/465005356-86a5e583-adac-4bf9-8192-508a0adf2308.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQzNjEzMDIsIm5iZiI6MTc1NDM2MTAwMiwicGF0aCI6Ii82NDUxMDQyNy80NjUwMDUzNTYtODZhNWU1ODMtYWRhYy00YmY5LTgxOTItNTA4YTBhZGYyMzA4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODA1VDAyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWFiNjZkMTZlYTBjMmE1Y2Y2ODM1ZGMxNjU0ODRlMzEwN2QyOGZkZmM5NWM2Zjk3YTk5YTc2ZGQ4Y2NlNzI1YWYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.qGm2vI_fgIS_CX3858gOiYnZevpLmnJQaPk-14G2y50" secured-asset-link="">
   </a>
</p>


<p dir="auto">
  <a href="https://go.midday.ai/K7GwMoQ" rel="nofollow">
    <img src="https://camo.githubusercontent.com/0d69bd5f3e56aabad0d1a0869a2bbf559648e8fde5478d9fa6331f424a097618/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53757061626173652d3345434638453f7374796c653d666f722d7468652d6261646765266c6f676f3d7375706162617365266c6f676f436f6c6f723d7768697465" alt="Supabase" data-canonical-src="https://img.shields.io/badge/Supabase-3ECF8E?style=for-the-badge&amp;logo=supabase&amp;logoColor=white">
  </a>
</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/64510427/465004791-2e09b891-d5e2-4f68-b924-a1c8ea42d24d.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQzNjEzMDIsIm5iZiI6MTc1NDM2MTAwMiwicGF0aCI6Ii82NDUxMDQyNy80NjUwMDQ3OTEtMmUwOWI4OTEtZDVlMi00ZjY4LWI5MjQtYTFjOGVhNDJkMjRkLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODA1VDAyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWUxODI0ZDhiOGQxZGFjNGJlYzlhYTY2MThlZWU0YTM4YmIwNzU4NGY5ZDQ5MGFjNDIyZTg3N2NjM2EwMzk5NmQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.iXA9k7MJN4NDQx4-7ujQxmuXYLommM6cNM59cW9kcjo"><img src="https://private-user-images.githubusercontent.com/64510427/465004791-2e09b891-d5e2-4f68-b924-a1c8ea42d24d.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQzNjEzMDIsIm5iZiI6MTc1NDM2MTAwMiwicGF0aCI6Ii82NDUxMDQyNy80NjUwMDQ3OTEtMmUwOWI4OTEtZDVlMi00ZjY4LWI5MjQtYTFjOGVhNDJkMjRkLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODA1VDAyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWUxODI0ZDhiOGQxZGFjNGJlYzlhYTY2MThlZWU0YTM4YmIwNzU4NGY5ZDQ5MGFjNDIyZTg3N2NjM2EwMzk5NmQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.iXA9k7MJN4NDQx4-7ujQxmuXYLommM6cNM59cW9kcjo" alt="ERP Screenshot"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/64510427/465004825-b04f3644-91aa-4f74-af8d-6f3e12116a6b.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQzNjEzMDIsIm5iZiI6MTc1NDM2MTAwMiwicGF0aCI6Ii82NDUxMDQyNy80NjUwMDQ4MjUtYjA0ZjM2NDQtOTFhYS00Zjc0LWFmOGQtNmYzZTEyMTE2YTZiLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODA1VDAyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ1NmVhYzg5MmY2ODhkYTUwZDQ1YzJhNGExNWQ2N2Q2NGJlNDIzYTdhN2Y4NzA1ZWUzZDgxMDYxZDQ2ZTAxMzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.9v_tr9MxKAgyv7W8sNm5RmA7LO3JlQ_9-la1xIVVyoo"><img src="https://private-user-images.githubusercontent.com/64510427/465004825-b04f3644-91aa-4f74-af8d-6f3e12116a6b.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQzNjEzMDIsIm5iZiI6MTc1NDM2MTAwMiwicGF0aCI6Ii82NDUxMDQyNy80NjUwMDQ4MjUtYjA0ZjM2NDQtOTFhYS00Zjc0LWFmOGQtNmYzZTEyMTE2YTZiLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODA1VDAyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ1NmVhYzg5MmY2ODhkYTUwZDQ1YzJhNGExNWQ2N2Q2NGJlNDIzYTdhN2Y4NzA1ZWUzZDgxMDYxZDQ2ZTAxMzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.9v_tr9MxKAgyv7W8sNm5RmA7LO3JlQ_9-la1xIVVyoo" alt="MES Screenshot"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Does the world need another ERP?</h2><a id="user-content-does-the-world-need-another-erp" aria-label="Permalink: Does the world need another ERP?" href="#does-the-world-need-another-erp"></a></p>
<p dir="auto">We built Carbon after years of building end-to-end manufacturing systems with off-the-shelf solutions. We realized that:</p>
<ul dir="auto">
<li>Modern, API-first tooling didn't exist</li>
<li>Vendor lock-in bordered on extortion</li>
<li>There is no "perfect ERP" because each company is unique</li>
</ul>
<p dir="auto">We built Carbon to solve these problems ☝️.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<p dir="auto">Carbon is designed to make it easy for you to extend the platform by building your own apps through our API. We provide some examples to get you started in the <a href="https://github.com/crbnos/carbon/blob/main/examples">examples</a> folder.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/64510427/445149654-ed6dc66b-e9cb-435e-b5a9-9daf933f4a1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQzNjEzMDIsIm5iZiI6MTc1NDM2MTAwMiwicGF0aCI6Ii82NDUxMDQyNy80NDUxNDk2NTQtZWQ2ZGM2NmItZTljYi00MzVlLWI1YTktOWRhZjkzM2Y0YTFkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODA1VDAyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTgyMjYzOTE5OTg4Yjc0MThkMjI4Yzg0ZDNkY2ExOWVlYTM3ZTAxMjk3NTc4MDU3YmM2M2JkZGIwZTY3NjU1MmYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.7r7Af605IQ1B4mUp-fSqs-VojI1soLKXUZD3FYkaZ2c"><img src="https://private-user-images.githubusercontent.com/64510427/445149654-ed6dc66b-e9cb-435e-b5a9-9daf933f4a1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQzNjEzMDIsIm5iZiI6MTc1NDM2MTAwMiwicGF0aCI6Ii82NDUxMDQyNy80NDUxNDk2NTQtZWQ2ZGM2NmItZTljYi00MzVlLWI1YTktOWRhZjkzM2Y0YTFkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODA1VDAyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTgyMjYzOTE5OTg4Yjc0MThkMjI4Yzg0ZDNkY2ExOWVlYTM3ZTAxMjk3NTc4MDU3YmM2M2JkZGIwZTY3NjU1MmYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.7r7Af605IQ1B4mUp-fSqs-VojI1soLKXUZD3FYkaZ2c" alt="Carbon Architecture"></a></p>
<p dir="auto">Features:</p>
<ul>
<li> ERP</li>
<li> MES</li>
<li> QMS</li>
<li> Custom Fields</li>
<li> Nested BoM</li>
<li> Traceability</li>
<li> MRP</li>
<li> Configurator</li>
<li> MCP Client/Server</li>
<li> API</li>
<li> Webhooks</li>
<li> Accounting</li>
<li> Capacity Planning</li>
<li> Simulation</li>
</ul>
<p dir="auto">Technical highlights:</p>
<ul>
<li> Unified auth and permissions across apps</li>
<li> Full-stack type safety (Database → UI)</li>
<li> Realtime database subscriptions</li>
<li> Attribute-based access control (ABAC)</li>
<li> Role-based access control (Customer, Supplier, Employee)</li>
<li> Row-level security (RLS)</li>
<li> Composable user groups</li>
<li> Dependency graph for operations</li>
<li> Third-party integrations</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Techstack</h2><a id="user-content-techstack" aria-label="Permalink: Techstack" href="#techstack"></a></p>
<ul dir="auto">
<li><a href="https://remix.run/" rel="nofollow">Remix</a> – framework</li>
<li><a href="https://www.typescriptlang.org/" rel="nofollow">Typescript</a> – language</li>
<li><a href="https://tailwindcss.com/" rel="nofollow">Tailwind</a> – styling</li>
<li><a href="https://radix-ui.com/" rel="nofollow">Radix UI</a> - behavior</li>
<li><a href="https://supabase.com/" rel="nofollow">Supabase</a> - database</li>
<li><a href="https://supabase.com/" rel="nofollow">Supabase</a> – auth</li>
<li><a href="https://upstash.com/" rel="nofollow">Upstash</a> - cache</li>
<li><a href="https://trigger.dev/" rel="nofollow">Trigger</a> - jobs</li>
<li><a href="https://resend.com/" rel="nofollow">Resend</a> – email</li>
<li><a href="https://novu.co/" rel="nofollow">Novu</a> – notifications</li>
<li><a href="https://vercel.com/" rel="nofollow">Vercel</a> – hosting</li>
<li><a href="https://stripe.com/" rel="nofollow">Stripe</a> - billing</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Codebase</h2><a id="user-content-codebase" aria-label="Permalink: Codebase" href="#codebase"></a></p>
<p dir="auto">The monorepo follows the Turborepo convention of grouping packages into one of two folders.</p>
<ol dir="auto">
<li><code>/apps</code> for applications</li>
<li><code>/packages</code> for shared code</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>/apps</code></h3><a id="user-content-apps" aria-label="Permalink: /apps" href="#apps"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Package Name</th>
<th>Description</th>
<th>Local Command</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>erp</code></td>
<td>ERP Application</td>
<td><code>npm run dev</code></td>
</tr>
<tr>
<td><code>mes</code></td>
<td>MES</td>
<td><code>npm run dev:mes</code></td>
</tr>
<tr>
<td><code>academy</code></td>
<td>Academy</td>
<td><code>npm run dev:academy</code></td>
</tr>
<tr>
<td><code>starter</code></td>
<td>Starter</td>
<td><code>npm run dev:starter</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>/packages</code></h3><a id="user-content-packages" aria-label="Permalink: /packages" href="#packages"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Package Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>eslint-config-carbon</code></td>
<td>Shared, extendable eslint configuration for apps and packages</td>
</tr>
<tr>
<td><code>@carbon/database</code></td>
<td>Database schema, migrations and types</td>
</tr>
<tr>
<td><code>@carbon/documents</code></td>
<td>Transactional PDFs and email templates</td>
</tr>
<tr>
<td><code>@carbon/integrations</code></td>
<td>Integration definitions and configurations</td>
</tr>
<tr>
<td><code>@carbon/jest</code></td>
<td>Jest preset configuration shared across apps and packages</td>
</tr>
<tr>
<td><code>@carbon/jobs</code></td>
<td>Background jobs and workers</td>
</tr>
<tr>
<td><code>@carbon/logger</code></td>
<td>Shared logger used across apps</td>
</tr>
<tr>
<td><code>@carbon/react</code></td>
<td>Shared web-based UI components</td>
</tr>
<tr>
<td><code>@carbon/kv</code></td>
<td>Redis cache client</td>
</tr>
<tr>
<td><code>@carbon/lib</code></td>
<td>Third-party client libraries (slack, resend)</td>
</tr>
<tr>
<td><code>@carbon/stripe</code></td>
<td>Stripe integration</td>
</tr>
<tr>
<td><code>@carbon/tsconfig</code></td>
<td>Shared, extendable tsconfig configuration used across apps and packages</td>
</tr>
<tr>
<td><code>@carbon/utils</code></td>
<td>Shared utility functions used across apps and packages</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Setup</h3><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Clone the repo into a public GitHub repository (or fork <a href="https://github.com/crbnos/carbon/fork">https://github.com/crbnos/carbon/fork</a>). If you plan to distribute the code, keep the source code public to comply with <a href="https://github.com/crbnos/carbon/blob/main/LICENSE">AGPLv3</a>. To clone in a private repository, <a href="https://carbon.ms/sales" rel="nofollow">acquire a commercial license</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/crbnos/carbon.git"><pre>git clone https://github.com/crbnos/carbon.git</pre></div>
</li>
<li>
<p dir="auto">Go to the project folder</p>

</li>
</ol>
<p dir="auto">Make sure that you have <a href="https://docs.docker.com/desktop/install/mac-install/" rel="nofollow">Docker installed</a> on your system since this monorepo uses the Docker for local development.</p>
<p dir="auto">In addition you must configure the following external services:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Service</th>
<th>Purpose</th>
<th>URL</th>
</tr>
</thead>
<tbody>
<tr>
<td>Upstash</td>
<td>Serverless Redis</td>
<td><a href="https://console.upstash.com/login" rel="nofollow">https://console.upstash.com/login</a></td>
</tr>
<tr>
<td>Trigger.dev</td>
<td>Job runner</td>
<td><a href="https://cloud.trigger.dev/login" rel="nofollow">https://cloud.trigger.dev/login</a></td>
</tr>
<tr>
<td>Posthog</td>
<td>Product analytics platform</td>
<td><a href="https://us.posthog.com/signup" rel="nofollow">https://us.posthog.com/signup</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Each of these services has a free tier which should be plenty to support local development. If you're self hosting, and you don't want to use Upstash or Posthog, it's pretty easy to replace upstash with a redis container in <code>@carbon/kv</code> and remove the Posthog analytics.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">First download and initialize the repository dependencies.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ nvm use           # use node v20
$ npm install       # install dependencies
$ npm run db:start  # pull and run the containers"><pre>$ nvm use           <span><span>#</span> use node v20</span>
$ npm install       <span><span>#</span> install dependencies</span>
$ npm run db:start  <span><span>#</span> pull and run the containers</span></pre></div>
<p dir="auto">Create an <code>.env</code> file and copy the contents of <code>.env.example</code> file into it</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ cp ./.env.example ./.env"><pre>$ cp ./.env.example ./.env</pre></div>
<ol dir="auto">
<li>Use the output of <code>npm run db:start</code> to set the supabase entries:</li>
</ol>
<ul dir="auto">
<li><code>SUPABASE_SERVICE_ROLE_KEY=[service_role key]</code></li>
<li><code>SUPABASE_ANON_KEY=[anon key]</code></li>
</ul>
<ol start="2" dir="auto">
<li><a href="https://console.upstash.com/redis" rel="nofollow">Create a Redis database in upstash</a> and copy the following from the <code>REST API</code> section:</li>
</ol>
<ul dir="auto">
<li><code>UPSTASH_REDIS_REST_URL=[UPSTASH_REDIS_REST_URL]</code></li>
<li><code>UPSTASH_REDIS_REST_TOKEN=[UPSTASH_REDIS_REST_TOKEN]</code></li>
</ul>
<ol start="3" dir="auto">
<li>Navigate to the project you created in <a href="https://github.com/crbnos/carbon/blob/main/Trigger.dev">https://cloud.trigger.dev/</a> and copy the following from the <code>Environments &amp; API Keys</code> section:</li>
</ol>
<ul dir="auto">
<li><code>TRIGGER_PUBLIC_API_KEY=[Public 'dev' API Key, starting 'pk_dev*']</code></li>
<li><code>TRIGGER_API_KEY=[Server 'dev' API Key, starting 'tr_dev*']</code></li>
</ul>
<ol start="4" dir="auto">
<li>In Posthog go to <a href="https://%5Bregion%5D.posthog.com/project/%5Bproject-id%5D/settings/project-details" rel="nofollow">https://[region].posthog.com/project/[project-id]/settings/project-details</a> to find your Project ID and Project API key:</li>
</ol>
<ul dir="auto">
<li><code>POSTHOG_API_HOST=[https://[region].posthog.com]</code></li>
<li><code>POSTHOG_PROJECT_PUBLIC_KEY=[Project API Key starting 'phc*']</code></li>
</ul>
<ol start="5" dir="auto">
<li>Add a <code>STRIPE_SECRET_KEY</code> from the Stripe admin interface, and then run <code>npm run -w @carbon/stripe register:stripe</code> to get a <code>STRIP_WEBHOOK_SECRET</code></li>
</ol>
<ul dir="auto">
<li><code>STRIPE_SECRET_KEY="sk_test_*************"</code></li>
<li><code>STRIP_WEBHOOK_SECRET="whsec_************"</code></li>
</ul>
<p dir="auto">Then you can run the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ npm run db:build     # run db migrations and seed script
$ npm run build        # build the packages"><pre>$ npm run db:build     <span><span>#</span> run db migrations and seed script</span>
$ npm run build        <span><span>#</span> build the packages</span></pre></div>
<p dir="auto">Finally, start the apps and packages:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ npm run dev
$ npm run dev:mes        # npm run dev in all apps &amp; packages"><pre>$ npm run dev
$ npm run dev:mes        <span><span>#</span> npm run dev in all apps &amp; packages</span></pre></div>
<p dir="auto">You can now sign in with:</p>
<p dir="auto">username: <a href="mailto:your-email@address.com">your-email@address.com</a>
password: carbon</p>
<p dir="auto">After installation you should be able run the apps locally.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Application</th>
<th>URL</th>
</tr>
</thead>
<tbody>
<tr>
<td>ERP</td>
<td><a href="http://localhost:3000/" rel="nofollow">http://localhost:3000</a></td>
</tr>
<tr>
<td>MES</td>
<td><a href="http://localhost:3001/" rel="nofollow">http://localhost:3001</a></td>
</tr>
<tr>
<td>Academy</td>
<td><a href="http://localhost:4111/" rel="nofollow">http://localhost:4111</a></td>
</tr>
<tr>
<td>Starter</td>
<td><a href="http://localhost:4000/" rel="nofollow">http://localhost:4000</a></td>
</tr>
<tr>
<td>Postgres</td>
<td>postgresql://postgres:postgres@localhost:54322/postgres</td>
</tr>
<tr>
<td>Supabase Studio</td>
<td><a href="http://localhost:54323/project/default" rel="nofollow">http://localhost:54323/project/default</a></td>
</tr>
<tr>
<td>Mailpit</td>
<td><a href="http://localhost:54324/" rel="nofollow">http://localhost:54324</a></td>
</tr>
<tr>
<td>Edge Functions</td>
<td><a href="http://localhost:54321/functions/v1/%3Cfunction-name%3E" rel="nofollow">http://localhost:54321/functions/v1/</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Notes</h3><a id="user-content-notes" aria-label="Permalink: Notes" href="#notes"></a></p>
<p dir="auto">To kill the database containers in a non-recoverable way, you can run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ npm run db:kill   # stop and delete all database containers"><pre>$ npm run db:kill   <span><span>#</span> stop and delete all database containers</span></pre></div>
<p dir="auto">To restart and reseed the database, you can run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ npm run db:build # runs db:kill, db:start, and setup"><pre>$ npm run db:build <span><span>#</span> runs db:kill, db:start, and setup</span></pre></div>
<p dir="auto">To run a particular application, use the <code>-w workspace</code> flag.</p>
<p dir="auto">For example, to run test command in the <code>@carbon/react</code> package you can run:</p>
<div data-snippet-clipboard-copy-content="$ npm run test -w @carbon/react"><pre><code>$ npm run test -w @carbon/react
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">API</h2><a id="user-content-api" aria-label="Permalink: API" href="#api"></a></p>
<p dir="auto">The API documentation is located in the ERP app at <code>${ERP}/x/api/js/intro</code>. It is auto-generated based on changes to the database.</p>
<p dir="auto">There are two ways to use the API:</p>
<ol dir="auto">
<li>From another codebase using a supabase client library:</li>
</ol>
<ul dir="auto">
<li><a href="https://supabase.com/docs/reference/javascript/introduction" rel="nofollow">Javascript</a></li>
<li><a href="https://supabase.com/docs/reference/dart/introduction" rel="nofollow">Flutter</a></li>
<li><a href="https://supabase.com/docs/reference/python/introduction" rel="nofollow">Python</a></li>
<li><a href="https://supabase.com/docs/reference/csharp/introduction" rel="nofollow">C#</a></li>
<li><a href="https://supabase.com/docs/reference/swift/introduction" rel="nofollow">Swift</a></li>
<li><a href="https://supabase.com/docs/reference/kotlin/introduction" rel="nofollow">Kotlin</a></li>
</ul>
<ol start="2" dir="auto">
<li>From within the codebase using our packages.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">From another Codebase</h3><a id="user-content-from-another-codebase" aria-label="Permalink: From another Codebase" href="#from-another-codebase"></a></p>
<p dir="auto">Navigate to settings in the ERP to generate an API key. If you're self-hosting you can also use the supabase service key instead of the public key for root access. In that case you don't needto include the <code>carbon-key</code> header.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import { Database } from &quot;@carbon/database&quot;;
import { createClient } from &quot;@supabase/supabase-js&quot;;

const apiKey = process.env.CARBON_API_KEY;
const apiUrl = process.env.CARBON_API_URL;
const publicKey = process.env.CARBON_PUBLIC_KEY;

const carbon = createClient<Database>(apiUrl, publicKey, {
  global: {
    headers: {
      &quot;carbon-key&quot;: apiKey,
    },
  },
});

// returns items from the company associated with the api key
const { data, error } = await carbon.from(&quot;item&quot;).select(&quot;*&quot;);"><pre><span>import</span> <span>{</span> <span>Database</span> <span>}</span> <span>from</span> <span>"@carbon/database"</span><span>;</span>
<span>import</span> <span>{</span> <span>createClient</span> <span>}</span> <span>from</span> <span>"@supabase/supabase-js"</span><span>;</span>

<span>const</span> <span>apiKey</span> <span>=</span> <span>process</span><span>.</span><span>env</span><span>.</span><span>CARBON_API_KEY</span><span>;</span>
<span>const</span> <span>apiUrl</span> <span>=</span> <span>process</span><span>.</span><span>env</span><span>.</span><span>CARBON_API_URL</span><span>;</span>
<span>const</span> <span>publicKey</span> <span>=</span> <span>process</span><span>.</span><span>env</span><span>.</span><span>CARBON_PUBLIC_KEY</span><span>;</span>

<span>const</span> <span>carbon</span> <span>=</span> <span>createClient</span><span>&lt;</span><span>Database</span><span>&gt;</span><span>(</span><span>apiUrl</span><span>,</span> <span>publicKey</span><span>,</span> <span>{</span>
  <span>global</span>: <span>{</span>
    <span>headers</span>: <span>{</span>
      <span>"carbon-key"</span>: <span>apiKey</span><span>,</span>
    <span>}</span><span>,</span>
  <span>}</span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>// returns items from the company associated with the api key</span>
<span>const</span> <span>{</span> data<span>,</span> error <span>}</span> <span>=</span> <span>await</span> <span>carbon</span><span>.</span><span>from</span><span>(</span><span>"item"</span><span>)</span><span>.</span><span>select</span><span>(</span><span>"*"</span><span>)</span><span>;</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">From the Monorepo</h3><a id="user-content-from-the-monorepo" aria-label="Permalink: From the Monorepo" href="#from-the-monorepo"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import { getCarbonServiceRole } from &quot;@carbon/auth&quot;;
const carbon = getCarbonServiceRole();

// returns all items across companies
const { data, error } = await carbon.from(&quot;item&quot;).select(&quot;*&quot;);

// returns items from a specific company
const companyId = &quot;xyz&quot;;
const { data, error } = await carbon
  .from(&quot;item&quot;)
  .select(&quot;*&quot;)
  .eq(&quot;companyId&quot;, companyId);"><pre><span>import</span> <span>{</span> <span>getCarbonServiceRole</span> <span>}</span> <span>from</span> <span>"@carbon/auth"</span><span>;</span>
<span>const</span> <span>carbon</span> <span>=</span> <span>getCarbonServiceRole</span><span>(</span><span>)</span><span>;</span>

<span>// returns all items across companies</span>
<span>const</span> <span>{</span> data<span>,</span> error <span>}</span> <span>=</span> <span>await</span> <span>carbon</span><span>.</span><span>from</span><span>(</span><span>"item"</span><span>)</span><span>.</span><span>select</span><span>(</span><span>"*"</span><span>)</span><span>;</span>

<span>// returns items from a specific company</span>
<span>const</span> <span>companyId</span> <span>=</span> <span>"xyz"</span><span>;</span>
<span>const</span> <span>{</span> data<span>,</span> error <span>}</span> <span>=</span> <span>await</span> <span>carbon</span>
  <span>.</span><span>from</span><span>(</span><span>"item"</span><span>)</span>
  <span>.</span><span>select</span><span>(</span><span>"*"</span><span>)</span>
  <span>.</span><span>eq</span><span>(</span><span>"companyId"</span><span>,</span> <span>companyId</span><span>)</span><span>;</span></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Projects evaluated to see if they're as free and open source as advertised (139 pts)]]></title>
            <link>https://isitreallyfoss.com/</link>
            <guid>44791554</guid>
            <pubDate>Mon, 04 Aug 2025 21:26:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://isitreallyfoss.com/">https://isitreallyfoss.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44791554">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<header id="top">
				<a href="https://isitreallyfoss.com/">is <strong>it</strong> <em>really</em> <strong>FOSS</strong>?</a>
				
			</header>
			<main>
	<div>

		<h2>
			Where Projects are Evaluated <br>
			<span>To see if they're as free and open source as advertised</span>
		</h2>

		<p>
			The software rights of users are continously (and often opaquely) being eroded by the desire of growth.
			<br>
			This website aims to push back against that by bringing transparency to <a href="https://isitreallyfoss.com/about/foss">FOSS</a> software users.
		</p>

		

		<div>
			<h3>Latest Projects Added</h3>
			
		</div>

		

	</div>
</main>
			
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Once a death sentence, cardiac amyloidosis is finally treatable (123 pts)]]></title>
            <link>https://www.nytimes.com/2025/08/04/well/cardiac-amyloidosis.html</link>
            <guid>44790944</guid>
            <pubDate>Mon, 04 Aug 2025 20:23:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/08/04/well/cardiac-amyloidosis.html">https://www.nytimes.com/2025/08/04/well/cardiac-amyloidosis.html</a>, See on <a href="https://news.ycombinator.com/item?id=44790944">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/08/04/well/cardiac-amyloidosis.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Content-Aware Spaced Repetition (115 pts)]]></title>
            <link>https://www.giacomoran.com/blog/content-aware-sr/</link>
            <guid>44790422</guid>
            <pubDate>Mon, 04 Aug 2025 19:32:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.giacomoran.com/blog/content-aware-sr/">https://www.giacomoran.com/blog/content-aware-sr/</a>, See on <a href="https://news.ycombinator.com/item?id=44790422">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>  <p>Spaced repetition systems are powerful, but they have a fundamental blind spot: they don’t understand what your flashcards are&nbsp;<em>about</em>.</p>
<p>To your SRS, a card asking “what’s the capital of Italy?” and another asking “what country is Rome the capital of?” are treated independently, each with its own isolated review history. It has no concept that reviewing related material should reinforce your memory of the whole topic.</p>
<p>At the heart of every SRS is a&nbsp;<strong>memory model</strong> which predicts how long you’ll remember each card based on your past performance. Most of today’s models ignore the <em>content</em> of the cards entirely. This is where <strong>content-aware memory models</strong> come in: they account for the semantic meaning of your cards, not just your review ratings.</p>
<p>This is more than a minor tweak for scheduling accuracy. It’s a foundational change that makes it practical to build the fluid, intelligent learning tools many have envisioned: from&nbsp;<a href="https://notes.andymatuschak.org/z7wCFe7MP9VeCVApcBLC7SN">idea-centric memory systems</a>&nbsp;that test understanding from multiple angles to truly&nbsp;<a href="https://davidbieber.com/snippets/2024-03-04-conversational-spaced-repetition/">conversational spaced repetition</a>&nbsp;with a voice-enabled AI agent as tutor.</p>
<p>This post explores what content-aware memory models are, and the new kinds of learning experiences they make possible.</p>
<h2 id="contents">Contents</h2>
<ul>
<li><a href="#schedulers-and-memory-models">Schedulers and memory models</a></li>
<li><a href="#content-aware-memory-models">Content-aware memory models</a>
<ul>
<li><a href="#karl">KARL</a></li>
<li><a href="#small-experiments-on-rember-data">Small experiments on Rember data</a></li>
<li><a href="#other-considerations">Other considerations</a></li>
</ul>
</li>
<li><a href="#ux-unlocks">UX unlocks</a></li>
<li><a href="#data-problem">Data problem</a></li>
</ul>
<h2 id="schedulers-and-memory-models">Schedulers and memory models</h2>
<p>I find it useful to distinguish between schedulers and memory models. This distinction wasn’t immediately obvious to me when I first approached the topic, but I’ve found it essential for thinking clearly about spaced repetition systems (SRS). Here, I’ll introduce both concept and I’ll make the case that separating schedulers from memory models enables independent innovation and simplifies the development of each component by isolating user experience (UX) concerns within the scheduler. In the literature, the scheduler is sometimes called the “teacher model” since it decides what to teach, while the memory model is the “student model” since it represents what the student knows.</p>
<p>In Anki and other spaced repetition systems, the <strong>scheduler</strong> is an algorithm that picks the next card to review today, answering the question <em>“Given the review history of every card in the student’s collection, which cards should the student review in this session?”</em>. In practice, when building a spaced repetition system, this is the question you actually care about. The scheduler’s core job is deciding&nbsp;<em>which cards to show today</em>, not just&nbsp;<em>when each card should ideally be reviewed</em>. A card might be “due” for review, but the scheduler might skip it due to daily review limits, prioritize other overdue cards, or defer it based on other goals.</p>
<p>For a long time, the only scheduler available in Anki was <a href="https://faqs.ankiweb.net/what-spaced-repetition-algorithm">a variant of SuperMemo’s SM-2 scheduler</a>, which dates back to <a href="https://www.supermemo.com/en/blog/the-true-history-of-spaced-repetition#1987">1987</a>. It’s remarkably simple yet effective. SuperMemo has since advanced its scheduler, now at version <a href="https://supermemo.guru/wiki/Algorithm_SM-18">SM-18</a>. The latest iterations of the algorithm achieve the same retention levels with fewer reviews, and are more robust when reviews deviate from optimal intervals, for example when a student returns from a break of several weeks. While the SuperMemo schedulers are closed-source, an explanation of how they work is <a href="https://supermemo.guru/wiki/Algorithm_SM-17">available</a>. <a href="https://github.com/open-spaced-repetition/fsrs4anki">FSRS</a> is an open-source scheduler by Jarrett Ye built on similar principles to modern SuperMemo algorithms and can be used in Anki.</p>
<p>A <strong>memory model</strong> predicts forgetting curves, answering the question <em>“Given the review history of every card in the student’s collection, what are the chances the student remembers a specific card at any given moment?”</em></p>
<p>We define <strong>retrievability</strong> as the probability that a student remembers a card at a particular time. In practice, we often model this as a binary outcome where a student either remembers or forgets a card. Some spaced repetition systems allow for more nuanced grading. For example, Anki has four options: “Again”, “Hard”, “Good”, “Easy”. This additional information from the review history can be used to make memory models more accurate.</p>
<p>The forgetting curve plots this retrievability over time. A memory model’s job is to compute these curves based on a student’s review history. Retrievability generally decreases over time, as it becomes more likely that the student will forget the card. For example, here are the forgetting curves estimated by different memory models from the same review history (the figure comes from my <a href="https://www.politesi.polimi.it/handle/10589/186407">master’s thesis</a>):</p>
<p><img alt="Plot of forgetting curves estimated by different memory models from the same review history" loading="lazy" decoding="async" fetchpriority="auto" width="1500" height="900" src="https://www.giacomoran.com/_astro/fc_comparison.DEARgvNj_2owL0b.webp"></p>
<p>The relationship between schedulers and memory models varies across different systems. A scheduler may or may not use a memory model. For example, the <a href="https://en.wikipedia.org/wiki/Leitner_system">Leitner system</a> and SM-2 do not rely on a memory model to schedule reviews, they are based on simple mechanical rules. Modern schedulers like SM-18 and FSRS instead include a memory model (stability and difficulty are used to compute retrievability). I recommend taking a look at Fernando Borretti’s articles on the implementing <a href="https://borretti.me/article/implementing-sm2-in-rust">SM-2</a> and <a href="https://borretti.me/article/implementing-fsrs-in-100-lines">FSRS</a>. Note that FSRS ties together the scheduler and the memory model at the implementation level, but they can be separated quite easily to unlock more flexibility in designing the system. There are also <a href="https://siddharth.io/files/deep-tutor.pdf">schedulers based on model-free reinforcement-learning</a>, that learn a scheduling policy directly from user interactions, without building an explicit, human-interpretable model of forgetting like the ones we’re discussing.</p>
<p>Once you have a memory model that estimates how likely the student is to remember each card, you can build different schedulers with various strategies:</p>
<ul>
<li>You can schedule a card for review when retrievability drops below 90%. This is the most common strategy in SRS as far as I can tell. This is more or less what FSRS does, you can also adjust the <em>desired retention</em> to other values.</li>
<li>You can randomly select cards for review, with probability proportional to how likely they have been forgotten (one minus retrievability). <a href="https://www.nature.com/articles/s41539-021-00105-8">Example</a>.</li>
<li>You can override the default scheduler behavior for new cards or after a failed review by implementing custom&nbsp;<em>learning</em> and&nbsp;<em>relearning steps</em>. This is also included in most FSRS implementations.</li>
<li>You can fuzz the intervals (adjust them randomly by small amounts) or apply load balancing to smooth out the amount of reviews scheduled for the student over a few days.</li>
<li>You can implement different strategies for when the student takes a break of weeks or months. For example, in Rember we mark cards overdue for a week as stale and the student can limit the amount of stale cards in each review session. This is an idea we took from <a href="https://www.remnote.com/">RemNote</a>.</li>
<li>You can build an exam feature, where the student sets the date for the exam and you schedule reviews in order to achieve high retrievability on that date for cards related to the exam. <a href="https://arxiv.org/abs/1805.08322">Example</a>.</li>
<li>You can account for goals complementary to retention, for example the student workload, the amount of new cards the student is adding, or the estimated answering time for each card. <a href="https://www.nature.com/articles/s41539-020-00074-4">Example</a>.</li>
<li>You can optimize for long-term outcomes, not just today’s retrievability. For example, if a student fails a card today, reviewing it again tomorrow might boost its future stability more than waiting a week. This forward-looking approach considers how today’s review outcome affects the card’s entire future learning trajectory. Some of the examples above account for similar ramifications. Nate Meyvis has explored <a href="https://www.natemeyvis.com/notes-on-spaced-repetition-scheduling.html">similar ideas</a>.</li>
</ul>
<p>As these examples illustrate, a single, accurate memory model can act as a foundation for a diverse ecosystem of schedulers, each optimized for different goals and user experiences.</p>
<p>The distinction between memory models and schedulers offers two key advantages:</p>
<ol>
<li><strong>Independent innovation cycles.</strong> We can innovate on schedulers independently of memory models, and vice versa. Scheduler research can treat memory models as a black box to obtain retrievability predictions.</li>
<li><strong>Separation from UX concerns.</strong> We can focus on building better memory models independently of product or UX considerations. The design of schedulers is deeply intertwined with product and UX considerations. For example, a scheduler might ensure a student re-attempts a failed card before the session ends. The primary benefit here might be psychological, providing the student with the assurance of getting it right, rather than a decision optimized purely for long-term retention. Another example is load balancing, the goal of which is primarily improving the student’s experience.</li>
</ol>
<p>This architectural separation isn’t just theoretical. It enables practical benefits: you can A/B test different scheduling strategies using the same underlying memory model, swap in improved memory models without rebuilding your entire system, and allow users to choose scheduling approaches that fit their learning style while maintaining consistent forgetting predictions underneath.</p>
<p>Most current SRS implementations conflate these concerns. The next generation of systems will likely benefit from treating them as distinct, composable components.</p>
<h2 id="content-aware-memory-models">Content-aware memory models</h2>
<p>This section explores how leveraging the textual content and semantic relationships between cards can improve memory models.</p>
<p>To the best of my knowledge, most memory models in real-world spaced repetition systems treat each card in isolation. SM-2, SM-18, and FSRS rely solely on each card’s individual review history to predict its forgetting curve. Ignoring the following factors means we are leaving useful information on the table:</p>
<ol>
<li><strong>The review histories of related cards</strong>. Card semantics allow us to identify related cards. This enables memory models to account for the review histories of&nbsp;<em>all</em> relevant cards when estimating a specific card’s retrievability.</li>
<li><strong>The card’s textual content</strong>. Beyond identifying related cards, the semantic content itself can directly inform the memory model. A model could, for example, estimate the inherent difficulty of a card based on its question or answer text, even before any reviews have taken place.</li>
</ol>
<p>It’s important to note that “card semantics” encompasses more than simple textual similarity. We’re looking to capture the card’s quality: how effectively the question can activate the memory pathways that lead the student to remember the concept. For example, we need to detect slight ambiguities in the question, which would make it difficult to answer. Moreover, we will likely need to examine the semantics of groups of cards, not just cards in isolation. For example, to memorize a list of three items, you might want a card for each item, plus an integrative card for the entire list. The card for the entire list in isolation would be quite poor in terms of card quality and would be very difficult to remember without the other three cards supporting it.</p>
<p>Informally, this direction proposes shifting the memory model’s focus from:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>retrievability(t) = f(t; single_card_history)</span></span></code></pre>
<p>To leveraging a richer context:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>retrievability(t) = f(t; all_cards_history, all_cards_content)</span></span></code></pre>
<p>Where <code>t</code> is the time since the last review, and each history (whether <code>single_card_history</code> or <code>all_cards_history</code>) is a chronological sequence of review events, typically comprising a timestamp and the student’s rating for that review.</p>
<p>Note that current memory models treat all new cards (cards with no reviews) as the same. Considering the textual content would allow us to obtain more informed initial estimates of a card’s inherent difficulty, leading to better scheduling for cards with no or few reviews compared to uniform defaults.</p>
<p>For example, consider the following collection of cards:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>Q: In reinforcement learning, broadly, what is the Bellman equation?</span></span>
<span><span>A: An equation that describes the fundamental relationship between the value of a state and the values of its successor states</span></span>
<span><span></span></span>
<span><span>Q: In reinforcement learning, what equation describes the fundamental relationship between the value of a state and the values of its successor states?</span></span>
<span><span>A: The Bellman equation</span></span>
<span><span></span></span>
<span><span>Q: In reinforcement learning, what's the formula of the Bellman equation?</span></span>
<span><span>A: $$ v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_{\pi}(s')] $$</span></span></code></pre>
<p>All three cards are semantically related, but the first two, being conceptual inversions of each other, share a much stronger connection than with the third, which focuses on the formula.
Reviewing either of the first two cards would likely make recalling the other significantly easier. The third card might have a similar effect, but considerably weaker. (Alternatively, if a student successfully reviews either of the first two cards, we should increase our confidence that they’ll recall the other). The third card is also inherently more challenging due to its “less atomic” nature: a student must recall multiple components of the formula, increasing the likelihood of forgetting a single element and marking the card as forgotten.</p>
<h3 id="karl">KARL</h3>
<p>This idea has been explored in the literature by <a href="https://arxiv.org/abs/2402.12291">Shu et al., 2024 - KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students</a>, where they introduce the term <strong>content-aware scheduling</strong>. The memory model in KARL encodes the textual content of the cards with BERT embeddings.</p>
<p>These embeddings play a dual role:</p>
<ol>
<li>They facilitate the&nbsp;retrieval of the top-k semantically similar cards&nbsp;from the user’s study history, whose review histories are then passed to the memory model</li>
<li>The&nbsp;embeddings of the current card and these top-k similar cards&nbsp;are themselves fed into the memory model</li>
</ol>
<p>The KARL scheduler was evaluated on a dataset consisting of 123,143 study logs on diverse trivia questions, collected from 543 users within a custom flashcard app. It slightly outperformed FSRSv4, which has improved since then, being now at version 6. This result is remarkable because KARL does not model the memory dynamics explicitly, like FSRS does by estimating difficulty and stability, and using a power-law forgetting curve. I wonder how the performance would be for a model that is good at both capturing the memory dynamics, like FSRSv6, and at accounting for card semantics, like KARL.</p>
<h3 id="small-experiments-on-rember-data">Small experiments on Rember data</h3>
<p>I ran a few small experiments myself that provide additional evidence that this direction is promising. In <a href="https://rember.com/">Rember</a> we group cards around small notes; at the time of the experiment, my account had 4,447 reviews for 940 cards, grouped in 317 notes. You can think of notes as grouping together semantically similar cards.</p>
<p>I ran a couple of experiments on top of FSRS:</p>
<ul>
<li><code>exp_1</code>: when the card has no reviews, set the initial stability to the average stability of other cards in the note.</li>
<li><code>exp_2</code>: multiply stability by a constant factor when other cards from the note have been reviewed between now and the card’s last review (the constant factor is optimized using grid search). This simulates a “priming” effect where recent exposure to related concepts reinforces the current card</li>
</ul>
<p>Stability represents how long the card will last in memory, it’s defined as the interval at which the card’s retrievability drops to 90%.</p>
<p>I compared the following memory models:</p>
<ul>
<li><code>random</code>: random retrievability predictions in <code>[0,1]</code></li>
<li><code>fsrs</code>: FSRSv5 with default parameters</li>
<li><code>fsrs_optimized</code>: FSRSv5 with parameters optimized on the collection</li>
<li><code>fsrs_exp_1</code>: <code>fsrs</code> + <code>exp_1</code></li>
<li><code>fsrs_exp_2</code>: <code>fsrs</code> + <code>exp_2</code></li>
<li><code>fsrs_optimized_exp_1</code>: <code>fsrs_optimized</code> + <code>exp_1</code></li>
<li><code>fsrs_optimized_exp_2</code>: <code>fsrs_optimized</code> + <code>exp_2</code></li>
</ul>
<p>I performed 4-fold cross-validation splitting by note IDs, and compared the models on the same evaluation metrics from my <a href="https://www.politesi.polimi.it/handle/10589/186407">master’s thesis</a>: <em>AUC</em> (measuring discrimination, the highest the better), <em>ICI</em> (measuring the average calibration error, the lower the better), <em>E_max</em> (measuring the maximum calibration error, the lower the better).</p>
<p>The results are summarized in the following table:</p>
<div>




















































<table><thead><tr><th>Model</th><th>AUC</th><th>ICI</th><th>E_max</th></tr></thead><tbody><tr><td><code>random</code></td><td>0.4887±0.0156</td><td>0.4235±0.0055</td><td>0.9193±0.0099</td></tr><tr><td><code>fsrs</code></td><td>0.5708±0.0172</td><td>0.0364±0.0120</td><td>0.2720±0.0912</td></tr><tr><td><code>fsrs_optimized</code></td><td>0.6294±0.0115</td><td>0.0108±0.0041</td><td>0.1904±0.0689</td></tr><tr><td><code>fsrs_exp_1</code></td><td>0.5883±0.0248</td><td>0.0281±0.0086</td><td>0.2204±0.0640</td></tr><tr><td><code>fsrs_exp_2</code></td><td>0.5716±0.0159</td><td>0.0307±0.0079</td><td>0.2355±0.0867</td></tr><tr><td><code>fsrs_optimized_exp_1</code></td><td>0.6148±0.0128</td><td><strong>0.0070±0.0021</strong></td><td>0.1383±0.0356</td></tr><tr><td><code>fsrs_optimized_exp_2</code></td><td><strong>0.6386±0.0185</strong></td><td>0.0075±0.0031</td><td><strong>0.1285±0.0522</strong></td></tr></tbody></table></div>
<p>The experimental results, though based on a small dataset, indicate a clear trend:</p>
<ul>
<li>Incorporating note-level information into FSRS, even with default parameters (<code>fsrs_exp_1</code> and <code>fsrs_exp_2</code>), generally outperforms <code>fsrs</code> across all metrics.</li>
<li>The optimal performance is achieved when FSRS is first optimized on the collection and then combined with note-level information.</li>
</ul>
<p>These results, while preliminary given the dataset size, support the hypothesis that integrating semantic context enhances memory models.</p>
<h3 id="other-considerations">Other considerations</h3>
<p><a href="https://www.mathacademy.com/">MathAcademy</a> includes an interesting spaced repetition feature, which accounts for the card semantics. They manually create a tree of concepts, with dependencies between them. The spaced repetition scheduler accounts for reviews of prerequisite concepts when scheduling a card. You can read more about how their scheduler works <a href="https://www.mathacademy.com/how-our-ai-works">here</a>. Here’s a quote:</p>
<blockquote>
<p>Existing spaced repetition algorithms are limited to the context of independent flashcards - but this is not appropriate for a hierarchical body of knowledge like mathematics. For instance, if a student practices adding two-digit numbers, then they are effectively practicing adding one-digit numbers as well! In general, repetitions on advanced topics should “trickle down” the knowledge graph to update the repetition schedules of simpler topics that are implicitly practiced.</p>
</blockquote>
<p>This is amazing work, but the scheduler is limited to MathAcademy’s tree of concepts; we need memory models that account for card semantics and apply more generally.</p>
<p>A general caveat might be the computational cost of the scheduler. For example, FSRS can schedule thousands of reviews in a few milliseconds on my M2 MacBook Pro, and therefore it can easily run on-device in a SRS. A memory model that accounts for the review history of all cards and the textual content of the cards will likely be more computationally intensive and might not be able to run on-device without excessive delays or battery drain. KARL addresses this computational challenge by considering only the top-k most semantically similar cards rather than all cards in the collection.</p>
<p>This kind of models will likely be trained on data from many spaced repetition students, which has a few implications:</p>
<ul>
<li>Even if a student has no reviews in the system, the model will still be able to estimate the initial difficulty for their cards.</li>
<li>We are making the implicit assumption that card semantics influence reviews in the same way across students, this might not always be true. For example, a card difficult for one person might be easy for another due to differing familiarity with the topic. However, if a card is hard to remember for most students, it is reasonable to assume it will be hard for others. Potential solutions include: focusing on how the question relates to the underlying topic, rather than the topic itself, or somehow modeling the student’s ability.</li>
</ul>
<h2 id="ux-unlocks">UX unlocks</h2>
<p>While accuracy improvements are valuable, the bigger impact comes from the UX opportunities unlocked by card semantics. By removing the rigid coupling between cards and their review histories, content-aware memory models give designers of spaced repetition systems much more freedom.</p>
<p>Here’s a concrete example of this constraint in action. When we started working on Rember (in the pre-LLM era), we considered building a fully markdown-based SRS. The main reason we dropped the idea is that you need an ID for each card, to link a card to its review history. Keeping IDs in the markdown quickly gets messy. You end up with something like:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>[card:abcxyz]</span></span>
<span><span>Q: What's the capital of Italy?</span></span>
<span><span>A: Rome</span></span></code></pre>
<p>Which is far from ideal, it’s fragile since the user might accidentally edit the ID, or end up with duplicate IDs by copy-pasting cards.</p>
<p>Solutions that get rid of the IDs were all dead-ends. You cannot rely on exact textual matches because the user might edit a card, and you don’t want to reset the review history if the user fixes a typo. You cannot rely on the relative position of the card in the document, as the user might move cards around. Forcing a GUI on top of the markdown that takes care of the IDs is viable, but kinda defeats the point of markdown.</p>
<p>The key insight is decoupling cards from their specific review histories. Instead of linking reviews to card IDs, the memory model considers only a universal history of content-based reviews, as triples: <code>(rating, timestamp, card's textual content)</code>. This eliminates the need for card consistency over time. There’s no “card’s review history” anymore, a single “review history” spans the entire student’s collection.</p>
<p>The scheduler can then assess the retrievability of all current cards in a student’s collection, including newly added ones, without relying on past review IDs. Students can edit cards freely without disrupting the scheduler. The memory model, using semantic understanding, differentiates between minor stylistic edits (like typos, which maintain the core meaning) and substantive changes to the card’s content (like replacing an answer, which would alter its semantic meaning sufficiently to be treated as a distinct card).</p>
<p>This decoupling also simplifies systems that dynamically generate prompts. Andy Matuschak explored bringing <em>ideas</em> rather than cards into the system. Prompts are dynamically generated during each review session to test the ideas from multiple angles, and evolve over time as you get more familiar with the ideas (see his Patreon post&nbsp;<a href="https://www.patreon.com/posts/fluid-practice-83882597">Fluid practice for fluid understanding</a>&nbsp;or his public&nbsp;<a href="https://notes.andymatuschak.org/z7wCFe7MP9VeCVApcBLC7SN">notes</a>). Content-aware memory models make this approach much more tractable.&nbsp;Current schedulers assume a review history per card. For dynamically generated prompts, this forces an awkward choice: either treat each unique prompt variation as a new card, losing its connection to the core idea, or group them coarsely at the idea level, which likely leads to under-reviewing the individual prompts.&nbsp;Content-aware memory models, however, naturally handle this gray area.</p>
<p>Taking this further, we could design a SRS where the flashcard-based review session is replaced with a conversation with a voice-enabled AI agent that asks questions or engages in open-ended discussion. This is part of the idea of <a href="https://davidbieber.com/snippets/2024-03-04-conversational-spaced-repetition/">conversational spaced repetition</a> explored by David Bieber. The AI agent could track the key ideas or concept the student goes over during the conversation, somehow judging whether the user could remember them or not. The content-aware memory model should be able account for those less structured reviews, even if they don’t directly map to Q&amp;A cards.</p>
<p>Additional benefits include:</p>
<ul>
<li>Having duplicates cards in the system is less disruptive to the review practice, since the scheduler will consider them as one and the same (even though you might still want to implement ways to detect and remove them from the system).</li>
<li>Reduced migration costs between SRS. Current importers must meticulously map both cards and review histories. With content-aware memory models, importing reviews could leverage the prior system’s textual card representation, enabling the new model to “understand” review histories from&nbsp;<em>any</em>&nbsp;system, regardless of perfect content mapping.</li>
</ul>
<p>While this approach reduces some direct user control over individual card histories, such as manually resetting a specific card’s schedule, I believe we can mitigate the problem and that the benefits of the approach far outweigh the costs.</p>
<p>In summary, I predict that content-aware memory models will make it much easier to design and build new interfaces for memory systems. They remove annoying hurdles that occupy the minds of SRS developers.</p>
<h2 id="data-problem">Data problem</h2>
<p>The main challenge in building content-aware memory models is lack of data. To my knowledge, no publicly available dataset exists that contains real-world usage data with both card textual content and review histories.</p>
<p><a href="https://arxiv.org/abs/2402.12291">KARL</a>, mentioned above, is trained on a dataset collected by paying users to review trivia flashcards on a custom app. I would hesitate to rely on a memory model trained solely on artificial data for my own spaced repetition practice. FSRS is trained on <a href="https://huggingface.co/datasets/open-spaced-repetition/anki-revlogs-10k">anki-revlogs-10k</a>, a large dataset consisting of more than 200M reviews from 10k Anki collections. The dataset includes only card, note, and deck IDs, omitting their textual content due to Anki’s <a href="https://ankiweb.net/account/privacy">Privacy Policy</a>, which states:</p>
<blockquote>
<p>In the interests of research and product development, we may use your review history and options (but not the text or media on cards) for statistical purposes, such as calculating the average pass rate across all users.</p>
</blockquote>
<p>While other review datasets exist, a crucial missing piece is a large dataset that:</p>
<ol>
<li>Is non-commercial and can be used for research purposes</li>
<li>Includes review histories</li>
<li>Includes cards’ textual content</li>
<li>Covers a wide range of topics (e.g. not just language learning data)</li>
</ol>
<p>Building on <a href="https://www.natemeyvis.com/notes-on-spaced-repetition-scheduling.html">Nate Meyvis’s insights</a>, I’ll add another requirement (I’ll discuss below why this is important):</p>
<ol start="5">
<li>A small fraction of the reviews are scheduled at random to provide unbiased data points</li>
</ol>
<p>Some of the challenges with data coming from spaced repetition systems:</p>
<ul>
<li><strong>The data is sparse.</strong> Limited to a time-stamped binary sequence of review ratings, spaced repetition data offers only a faint and insufficient signal to fully reconstruct the complex, dynamic state of a student’s memory.</li>
<li><strong>The data is incomplete.</strong> The limited data captured by spaced repetition systems fails to account for crucial out-of-system interactions that significantly shape a student’s memory. Students interact with material outside the SRS: through reading, conversation, or practical application. These interactions, important for memory, are not captured by the system. Furthermore, each review significantly alters the card’s future schedule. Consequently, unrecorded external recall events can have a substantial but uncaptured effect on the memory state assumed by the system.</li>
<li><strong>The data is biased.</strong> Spaced repetition data is inherently biased by the memory model that schedules reviews, creating a “chicken or the egg” problem where the data used to train the model is influenced by the model itself, potentially hindering further optimization (see <a href="https://ceur-ws.org/Vol-1432/sl_pap3.pdf">this paper</a>). This is why scheduling a small fraction of reviews at random in a real-world SRS could significantly improve the accuracy of memory models.</li>
<li><strong>The data is self-reported.</strong>&nbsp;We assume students provide ratings that truly reflect their inner memory state, but they may mark cards as “remembered” when they’ve actually forgotten them, perhaps to avoid the discomfort of perceived failure.</li>
</ul>
<p>The strong results achieved by current-generation schedulers like FSRS and SM-18 provide compelling evidence that a valuable signal indeed exists and can be separated from noise, despite the challenges described above.</p>
<p>One potential path forward is an open-source, community-contributed dataset where users voluntarily share their Anki and other SRS data, complete with tools to filter out sensitive content and eventually standardized benchmarks for evaluating memory models. If you’re interested in contributing data, have experience building community-driven datasets, or have thoughts on this approach, I’d love to <a href="https://www.giacomoran.com/cdn-cgi/l/email-protection#31565850525e5c5e43505f71565c50585d1f525e5c">hear from you</a>.</p>
<hr>
<p><em>We’re looking for testers for a new workflow for generating flashcards with AI, if you might be interested sign up for the waitlist at <a href="https://rember.com/">rember.com</a>. The best way to get updates on my work is <a href="https://x.com/giacomo_ran">x dot com</a>.</em></p> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Passkeys are just passwords that require a password manager (127 pts)]]></title>
            <link>https://danfabulich.medium.com/passkeys-are-just-passwords-that-require-a-password-manager-ebb7f2fdcadf</link>
            <guid>44790385</guid>
            <pubDate>Mon, 04 Aug 2025 19:29:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danfabulich.medium.com/passkeys-are-just-passwords-that-require-a-password-manager-ebb7f2fdcadf">https://danfabulich.medium.com/passkeys-are-just-passwords-that-require-a-password-manager-ebb7f2fdcadf</a>, See on <a href="https://news.ycombinator.com/item?id=44790385">Hacker News</a></p>
Couldn't get https://danfabulich.medium.com/passkeys-are-just-passwords-that-require-a-password-manager-ebb7f2fdcadf: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[NASA's Curiosity picks up new skills (113 pts)]]></title>
            <link>https://www.jpl.nasa.gov/news/marking-13-years-on-mars-nasas-curiosity-picks-up-new-skills/</link>
            <guid>44790271</guid>
            <pubDate>Mon, 04 Aug 2025 19:20:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jpl.nasa.gov/news/marking-13-years-on-mars-nasas-curiosity-picks-up-new-skills/">https://www.jpl.nasa.gov/news/marking-13-years-on-mars-nasas-curiosity-picks-up-new-skills/</a>, See on <a href="https://news.ycombinator.com/item?id=44790271">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p data-block-key="k29rj">Thirteen years since Curiosity landed on Mars, engineers are finding ways to make the NASA rover even more productive. The six-wheeled robot has been given more autonomy and the ability to multitask — improvements designed to make the most of Curiosity’s energy source, a multi-mission radioisotope thermoelectric generator (MMRTG). Increased efficiency means the rover has ample power as it continues to decipher how the ancient Martian climate changed, transforming a world of lakes and rivers into the chilly desert it is today.</p><p data-block-key="7hhum">Curiosity recently rolled into a region filled with <a href="https://www.nasa.gov/missions/mars-science-laboratory/curiosity-rover/nasas-curiosity-mars-rover-starts-unpacking-boxwork-formations/">boxwork formations</a>. These hardened ridges are believed to have been created by underground water billions of years ago. Stretching for miles on this part of <a href="https://science.nasa.gov/resource/curiositys-proposed-path-up-mount-sharp/">Mount Sharp</a>, a 3-mile-tall (5-kilometer-tall) mountain, the formations might reveal whether microbial life could have survived in the Martian subsurface eons ago, extending the period of habitability farther into when the planet was drying out.</p></div><div><p data-block-key="3uu6m">Instead, Curiosity and its younger sibling <a href="https://science.nasa.gov/mission/mars-2020-perseverance/">Perseverance</a> each use their <a href="https://www.jpl.nasa.gov/news/press_kits/mars_2020/launch/mission/spacecraft/power/">MMRTG</a> nuclear power source, which relies on decaying plutonium pellets to create energy and recharge the rover’s batteries. Providing ample power for the rovers’ many science instruments, MMRTGs are known for their longevity (the twin <a href="https://science.nasa.gov/mission/voyager/">Voyager</a> spacecraft have relied on <a href="https://science.nasa.gov/mission/voyager/spacecraft/#h-radioisotope-power-system-rps">RTGs</a> since 1977). But as the plutonium decays over time, it takes longer to recharge Curiosity’s batteries, leaving less energy for science each day.</p><p data-block-key="fq71o">The team carefully manages the rover’s daily power budget, factoring in every device that draws on the batteries. While these components were all tested extensively before launch, they are part of complex systems that reveal their quirks only after years in the extreme Martian environment. Dust, radiation, and sharp temperature swings bring out edge cases that engineers couldn’t have expected.</p><p data-block-key="1nfva">“We were more like cautious parents earlier in the mission,” said Reidar Larsen of NASA’s Jet Propulsion Laboratory in Southern California, which built and operates the rover. Larsen led a group of engineers who developed the new capabilities. “It’s as if our teenage rover is maturing, and we’re trusting it to take on more responsibility. As a kid, you might do one thing at a time, but as you become an adult, you learn to multitask.”</p><h3 data-block-key="e6csg"><b>More Efficient Science</b></h3><p data-block-key="2ho0n">Generally, JPL engineers send Curiosity a list of tasks to complete one by one before the rover ends its day with a nap to recharge. In 2021, the team began studying whether two or three rover tasks could be safely combined, reducing the amount of time Curiosity is active.</p><p data-block-key="cfof9">For example, Curiosity’s radio regularly sends data and images to a passing orbiter, which <a href="https://www.nasa.gov/centers-and-facilities/jpl/the-mars-relay-network-connects-us-to-nasas-martian-explorers/">relays them to Earth</a>. Could the rover talk to an orbiter while driving, moving its robotic arm, or snapping images? Consolidating tasks could shorten each day’s plan, requiring less time with heaters on and instruments in a ready-to-use state, reducing the energy used. Testing showed Curiosity safely could, and all of these have now been successfully demonstrated on Mars.</p></div><div><p data-block-key="436fb">Another trick involves letting Curiosity decide to nap if it finishes its tasks early. Engineers always pad their estimates for how long a day’s activity will take just in case hiccups arise. Now, if Curiosity completes those activities ahead of the time allotted, it will go to sleep early.</p><p data-block-key="b26s7">By letting the rover manage when it naps, there is less recharging to do before the next day’s plan. Even actions that trim just 10 or 20 minutes from a single activity add up over the long haul, maximizing the life of the MMRTG for more science and exploration down the road.</p><h3 data-block-key="f5j4r"><b>Miles to Go</b></h3><p data-block-key="ai5jd">In fact, the team has been implementing other new capabilities on Curiosity for years. Several mechanical issues required a rework of how the robotic arm’s <a href="https://www.jpl.nasa.gov/news/drilling-success-curiosity-is-collecting-mars-rocks/">rock-pulverizing drill</a> collects samples, and <a href="https://www.nasa.gov/missions/mars-science-laboratory/curiosity-rover/nasas-curiosity-mars-rover-gets-a-major-software-upgrade/">driving capabilities</a> have been enhanced with software updates. When a color filter wheel stopped turning on one of the two cameras mounted on Mastcam, Curiosity’s swiveling “head,” the team <a href="https://www.nasa.gov/missions/mars-science-laboratory/curiosity-rover/nasas-curiosity-rover-clocks-4000-days-on-mars/">developed a workaround</a> allowing them to capture the same beautiful panoramas.</p><p data-block-key="5sjdo">JPL also developed an <a href="https://www.jpl.nasa.gov/news/an-algorithm-helps-protect-mars-curiositys-wheels/">algorithm to reduce wear</a> on Curiosity’s rock-battered wheels. And while engineers closely monitor any new damage, they aren’t worried: After 22 miles (35 kilometers) and extensive research, it’s clear that, despite some punctures, the wheels have years’ worth of travel in them. (And in a worst-case scenario, Curiosity could remove the damaged part of the wheel’s “tread” and still drive on the remaining part.)</p><p data-block-key="fi860">Together, these measures are doing their job to keep Curiosity as busy as ever.</p><h3 data-block-key="5hk3l"><b>More About Curiosity</b></h3><p data-block-key="c18ov">Curiosity was built by NASA’s Jet Propulsion Laboratory, which is managed by Caltech in Pasadena, California. JPL leads the mission on behalf of NASA’s Science Mission Directorate in Washington as part of NASA’s Mars Exploration Program portfolio. Malin Space Science Systems in San Diego built and operates Mastcam.</p><p data-block-key="2s4lj">For more about Curiosity, visit:</p><p data-block-key="fmd7f"><a href="https://science.nasa.gov/mission/msl-curiosity"><b>science.nasa.gov/mission/msl-curiosity</b></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Offline.kids – Screen-free activities for kids (128 pts)]]></title>
            <link>https://offline.kids/</link>
            <guid>44789192</guid>
            <pubDate>Mon, 04 Aug 2025 17:50:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://offline.kids/">https://offline.kids/</a>, See on <a href="https://news.ycombinator.com/item?id=44789192">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><ul><li>
<figure><a href="https://offline.kids/teaching-kids-to-lose-gracefully/" target="_self"><img loading="lazy" decoding="async" width="1536" height="1024" src="https://offline.kids/wp-content/uploads/2025/06/Teaching-Kids-to-Lose-Gracefully.png" alt="Teaching Kids to Lose Gracefully" srcset="https://offline.kids/wp-content/uploads/2025/06/Teaching-Kids-to-Lose-Gracefully.png 1536w, https://offline.kids/wp-content/uploads/2025/06/Teaching-Kids-to-Lose-Gracefully-300x200.png 300w" sizes="auto, (max-width: 1536px) 100vw, 1536px"></a></figure>

<p><time datetime="2025-06-05T08:26:49+00:00">June 5, 2025</time></p>

<h2><a href="https://offline.kids/teaching-kids-to-lose-gracefully/" target="_self">Teaching Kids to Lose Gracefully</a></h2>

<div><p>Losing a game can feel devastating for kids — but it’s also an important opportunity to help them build resilience and empathy. In this gentle guide, we share practical tips for parents and carers to support children in learning how to handle setbacks with confidence and grace.</p><p><a href="https://offline.kids/teaching-kids-to-lose-gracefully/">View post</a></p></div>
</li><li>
<figure><a href="https://offline.kids/solo-play-ideas-for-kids-when-you-need-a-moment/" target="_self"><img loading="lazy" decoding="async" width="1536" height="1024" src="https://offline.kids/wp-content/uploads/2025/06/Solo-Play-Activities.png" alt="Solo Play Ideas for Kids (When You Need a Moment)" srcset="https://offline.kids/wp-content/uploads/2025/06/Solo-Play-Activities.png 1536w, https://offline.kids/wp-content/uploads/2025/06/Solo-Play-Activities-300x200.png 300w" sizes="auto, (max-width: 1536px) 100vw, 1536px"></a></figure>

<p><time datetime="2025-05-29T11:40:34+00:00">May 29, 2025</time></p>

<h2><a href="https://offline.kids/solo-play-ideas-for-kids-when-you-need-a-moment/" target="_self">Solo Play Ideas for Kids (When You Need a Moment)</a></h2>

<div><p>Sometimes kids need something they can do on their own — while you’re on a work call, cooking dinner, or just taking a breath. Here’s a list of solo activities you can try with your child, grouped by age. Some are linked to full activity guides on Offline.Kids, and others are simple ideas you can…</p><p><a href="https://offline.kids/solo-play-ideas-for-kids-when-you-need-a-moment/">View post</a></p></div>
</li><li>
<figure><a href="https://offline.kids/wed-love-your-feedback-help-shape-offline-kids/" target="_self"><img loading="lazy" decoding="async" width="1536" height="1024" src="https://offline.kids/wp-content/uploads/2025/05/Feedback-Invitation-Illustration.png" alt="We’d Love Your Feedback – Help Shape Offline.Kids" srcset="https://offline.kids/wp-content/uploads/2025/05/Feedback-Invitation-Illustration.png 1536w, https://offline.kids/wp-content/uploads/2025/05/Feedback-Invitation-Illustration-300x200.png 300w, https://offline.kids/wp-content/uploads/2025/05/Feedback-Invitation-Illustration-1024x683.png 1024w, https://offline.kids/wp-content/uploads/2025/05/Feedback-Invitation-Illustration-768x512.png 768w" sizes="auto, (max-width: 1536px) 100vw, 1536px"></a></figure>

<p><time datetime="2025-05-22T13:36:59+00:00">May 22, 2025</time></p>

<h2><a href="https://offline.kids/wed-love-your-feedback-help-shape-offline-kids/" target="_self">We’d Love Your Feedback – Help Shape Offline.Kids</a></h2>

<div><p>Hi there 👋 Offline.Kids is a passion project — started by a tired parent (me!) looking for simple, low-effort ways to spend meaningful time with my daughter. If you’ve ever found yourself Googling “easy activities for kids” with one eye on the clock and the other on your coffee, you’re in the right place. Now…</p><p><a href="https://offline.kids/wed-love-your-feedback-help-shape-offline-kids/">View post</a></p></div>
</li><li>
<figure><a href="https://offline.kids/smartphone-free-childhood-create-powerful-new-video/" target="_self"><img loading="lazy" decoding="async" width="1280" height="720" src="https://offline.kids/wp-content/uploads/2025/05/SFC-video-thumb.jpg" alt="Smartphone Free Childhood create powerful new video" srcset="https://offline.kids/wp-content/uploads/2025/05/SFC-video-thumb.jpg 1280w, https://offline.kids/wp-content/uploads/2025/05/SFC-video-thumb-300x169.jpg 300w, https://offline.kids/wp-content/uploads/2025/05/SFC-video-thumb-1024x576.jpg 1024w, https://offline.kids/wp-content/uploads/2025/05/SFC-video-thumb-768x432.jpg 768w" sizes="auto, (max-width: 1280px) 100vw, 1280px"></a></figure>

<p><time datetime="2025-05-22T10:53:34+00:00">May 22, 2025</time></p>

<h2><a href="https://offline.kids/smartphone-free-childhood-create-powerful-new-video/" target="_self">Smartphone Free Childhood create powerful new video</a></h2>

<div><p>Discover how the Smartphone Free Childhood movement is empowering parents and carers to protect kids from smartphone harm — with community, courage, and hope.</p><p><a href="https://offline.kids/smartphone-free-childhood-create-powerful-new-video/">View post</a></p></div>
</li></ul>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: What trick of the trade took you too long to learn? (124 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44789068</link>
            <guid>44789068</guid>
            <pubDate>Mon, 04 Aug 2025 17:39:59 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44789068">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><tbody><tr id="44794030"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44794030" href="https://news.ycombinator.com/vote?id=44794030&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Excellence in anything is a byproduct of having fun. Fun is a byproduct of understanding. Understanding is a byproduct of going slow. Going slow is a byproduct of curiosity. Curiosity is a byproduct of saying "I don’t know," of shunning beliefs and attending to what is in front, with zero baggage or impositions of your own—shunning the ego in the moment, moment by moment. Excellence comes when each piece is as equal as any other, when preference is shunned, when space is created to allow what is in the moment, without resistance, without insistence.</p></div></td></tr></tbody></table></td></tr><tr id="44794160"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44794160" href="https://news.ycombinator.com/vote?id=44794160&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>This is relatable. Once one gets over the frustration of failing and making mistakes (thousands of times in some cases), it becomes fun and easier to stay curious.</p></div></td></tr></tbody></table></td></tr><tr id="44790699"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44790699" href="https://news.ycombinator.com/vote?id=44790699&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Start as early as possible in investing (in index funds) and otherwise being financially savvy. It is very beneficial to realise early on that growing your hard earned money and spending it wisely is way more important as it will in the future lead to some unexpected benefits. Freedom of thought and action!</p></div></td></tr></tbody></table></td></tr><tr id="44793713"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793713" href="https://news.ycombinator.com/vote?id=44793713&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>My mistake was not starting early, because the numbers were small and it didn’t seem worth the time. The habit and systems are important to build, so that when the numbers do get bigger it goes to the right place.</p></div></td></tr></tbody></table></td></tr><tr id="44793768"><td></td></tr><tr id="44793855"><td></td></tr><tr id="44794155"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_44794155" href="https://news.ycombinator.com/vote?id=44794155&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I also graduated in 2008.</p><p>Starting my adult life with the markets in freefall made it hard to get in the habit of investing.  All through school, people told me "most people don't start investing until their 30s, and end up in a worse than they ought to because the time-value of money compounds a bunch if you add a few more years."  I thought I knew better than to be one of those people.</p><p>Instead, I ended up keeping a lot of savings in cash during years when the interest rates were approximately 0.  I've tried to get better at putting money into the markets over the past few years, but my financials look very different than they might have.</p><p>&gt; In other words, 2008 had no meaningful impact on you then.</p><p>People who graduated in the late 00s might not have accrued big financial losses, but it had a very meaningful impact on my comfort investing.</p></div></td></tr></tbody></table></td></tr><tr id="44794254"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_44794254" href="https://news.ycombinator.com/vote?id=44794254&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I don't follow. The markets were low, great time to invest. Great time to buy a home. 2008 was bad for people who owned homes or were already investing.</p></div></td></tr></tbody></table></td></tr><tr id="44793501"><td></td></tr><tr id="44794177"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44794177" href="https://news.ycombinator.com/vote?id=44794177&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Excellent resource. The videos link to a YouTube channel with a treasure trove of content in multiple disciplines. Thanks for sharing!</p></div></td></tr></tbody></table></td></tr><tr id="44790860"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44790860" href="https://news.ycombinator.com/vote?id=44790860&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Call Vanguard: 877-662-7447</p><p>An investing prof at Chicago puts this on the whiteboard at the start of semester, saying this is really all most people need to know and this class is unlikely to learn anything in his or any class that will let them, personally, do better.</p></div></td></tr></tbody></table></td></tr><tr id="44791587"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44791587" href="https://news.ycombinator.com/vote?id=44791587&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>That’s good advice for a layman but most high earners can do much better if they care and are motivated. Most are neither though lol.</p><p>Mostly on the tax side. Some specific examples:</p><p>- after maxing out your 401k what should you do next? IRA? Mega backdoor roth? Something else?</p><p>- If you have kids, how to best save for future education expenses? Hint: consider 529 plan.</p><p>- HSA is technically the best tax advantaged account, most high earners don’t realize it and “waste” the HSA funds to reimburse typical medical bills. HSA has triple tax benefits: contributions are tax-free, growth is tax-free, and withdrawals are also tax-free after age 65 for any reason, not just medical expenses. So basically investing without any tax obligation. You can also withdraw tax free before 65, but for medical expenses only.</p><p>i could go on…investing is great, but reducing your tax obligation is an even more powerful technique if you want to grow your net worth.</p></div></td></tr></tbody></table></td></tr><tr id="44793631"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793631" href="https://news.ycombinator.com/vote?id=44793631&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Looked into HSA recently at work but legally you need a high deductible health care plan to be eligible. Looking at the options, just nothing looked good compared to my current $0 deductible/$0 co-pay plan. Hard to know for sure but just seemed like I would be paying a lot more out of pocket every year.</p></div></td></tr></tbody></table></td></tr><tr id="44793910"><td></td></tr><tr id="44793345"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793345" href="https://news.ycombinator.com/vote?id=44793345&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>The HSA thing intrigued me and so I did some digging. It appears that post-65, you still have to pay income tax on non-medical withdrawals from an HSA? That is, besides the tax-free-for-medical-expenses part it reverts to a traditional IRA?</p><p>One additional trick though is that it looks like you can pay for any HSA-eligible medical expenses (incurred after you created the HSA) out-of-pocket now, and reimburse your bills at any point in the future? Thus you can still earn interest on the cash before withdrawing it at any point in the future (treating it as tax-free liquidity).</p><p>(I don't fully understand this so these are questions not statements, but hopefully I'm correct!)</p></div></td></tr></tbody></table></td></tr><tr id="44793621"><td></td></tr><tr id="44793544"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_44793544" href="https://news.ycombinator.com/vote?id=44793544&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p><i>&gt; The HSA thing intrigued me and so I did some digging. It appears that post-65, you still have to pay income tax on non-medical withdrawals from an HSA?</i></p><p>That's what I understood too. That claim that you can completely skip taxes looks wrong.</p></div></td></tr></tbody></table></td></tr><tr id="44793565"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_44793565" href="https://news.ycombinator.com/vote?id=44793565&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>You can completely escape taxes for any amounts that you have receipts for medical expenses incurred after the HSA was opened.</p><p>You can pay cash for a qualified medical expense in 2025 and take out that
amount of money from the HSA decades later.</p></div></td></tr></tbody></table></td></tr><tr id="44793591"><td></td></tr><tr id="44793725"><td><table><tbody><tr><td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td><center><a id="up_44793725" href="https://news.ycombinator.com/vote?id=44793725&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>One thing to note!!!! Make sure to also keep records on the HSA start date and any transfers you may engage in, e.g., after leaving an employer or just changing providers. It is worth having that on record in case 20, 30, 40 years later your claims are rejected because the original opening date was lost in some transfer at some point.</p></div></td></tr></tbody></table></td></tr><tr id="44793595"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_44793595" href="https://news.ycombinator.com/vote?id=44793595&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>i’m the parent. You are correct, my mistake! It’s too late to edit at this point.  :(</p><p>What I should have said is that after 65 you can spend it on non medical stuff without penalty. BUT if you do so you’ll owe tax that year (withdrawal).</p><p>So to summarize, you can avoid all tax if it’s spent on medical stuff. For non medical (post 65) it’s still good, but not as good.</p><p>Still an amazing deal because old people tend to spend a lot more on healthcare.</p></div></td></tr></tbody></table></td></tr><tr id="44793516"><td></td></tr><tr id="44791842"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44791842" href="https://news.ycombinator.com/vote?id=44791842&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>What you have stated is almost the same as call Vanguard, all those options are the same in the sense that they all involve investing, leaving it alone for a long time. Its just the vehicle thats slightly different and tax advantages.</p><p>I wouldn’t consider those options needing much motivation or research. The key with all of them is investing early and leaving it alone.</p></div></td></tr></tbody></table></td></tr><tr id="44792069"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_44792069" href="https://news.ycombinator.com/vote?id=44792069&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>If one is clueless with investing and taxes in general, and they call vanguard, their eyes will glaze over. It would be like me explaining software development to my 85 year old father.</p><p>I do agree people should call vanguard. But just blindly following steps they give you is unlikely to be productive if you don’t understand why you’re doing those steps. Furthermore, those people who don’t understand _why_ will freak out every time there’s a huge market correction. They get scared - because they don’t understand any of it.</p><p>I’m also curious, do they offer financial advice for your accounts outside vanguard? Genuinely curious since i’m unsure.</p></div></td></tr></tbody></table></td></tr><tr id="44792388"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_44792388" href="https://news.ycombinator.com/vote?id=44792388&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I’ve think the “call” part is a bit from the past, should be a setup account online with Vanguard, put it in VOO or VTSAX or the equivalent low fee market index fund and leave it alone.</p><p>Replace Vanguard with any other firm but the key is picking low fee ETFs and leaving them alone. Vanguard tends to have a reputation for the lowest fees.</p></div></td></tr></tbody></table></td></tr><tr id="44793037"><td></td></tr><tr id="44791702"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44791702" href="https://news.ycombinator.com/vote?id=44791702&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>At that point get someone to do it for you for x% of what they're getting you back and live blissfully unaware of arcane tax code specifics.</p></div></td></tr></tbody></table></td></tr><tr id="44791769"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_44791769" href="https://news.ycombinator.com/vote?id=44791769&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>At least the 401k, IRA, and HSA don't require knowing anything particularly arcane. Money goes in, don't touch until 59 1/2 (401k, IRA) or 65 (HSA).</p><p>529 plans can get a bit more complicated because you'll  want one from your state (if your state has an income tax) and they may offer several, but then it's less about knowing tax code specifics than about what the differences are between their offerings.</p></div></td></tr></tbody></table></td></tr><tr id="44792135"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_44792135" href="https://news.ycombinator.com/vote?id=44792135&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>right, it’s not that arcane at all. I discovered all this over a couple weekends in my 20s. Started on reddit, then moved on to more official guides and books. I spent maybe 5 weekends in total doing this learning.</p><p>It’s really not that hard and i don’t understand why more people aren’t interested. Let’s reframe for a minute…if i said a high earner could retire a year earlier, or maybe even a few years earlier just by learning some semi-advanced tax strategies. Should they do so? Yeah. They’d be crazy not to lol.</p></div></td></tr></tbody></table></td></tr><tr id="44793575"><td></td></tr><tr id="44792477"><td></td></tr><tr id="44793648"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793648" href="https://news.ycombinator.com/vote?id=44793648&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>i tried that hack one year. Cigna just emptied my account on the first try with a provider charging 2 emergency room visits when I was there only for a xray.</p><p>Cigna refused to lift a finger unless i sued them both.</p><p>yeah, i wouldn't recommend that.</p></div></td></tr></tbody></table></td></tr><tr id="44793967"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44793967" href="https://news.ycombinator.com/vote?id=44793967&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>An important part that could have been beneficial would have been to add, "today".</p><p>I had a class where the teacher did something similar, but she showed that if you started a ROTH today and contributed only the 4 years you were in college and then stopped forever. You would have nearly the same amount of money as someone who started 1 year after they completed college and invested every year until retirement.</p><p>Ultimately she was encouraging us to take out student loans and invest it or use any excess scholarship money to max out a ROTH IRA. She even advocated for investing all student loan money and opening credit cards tp actually pay for college, making minimum payments until graduation. Then moving away to a LCOL country and learn the language for 8-9 years while remaining in school taking 1 online class a year and travelling the world on student loans and not to worry about starting a career until 30 and start paying once you are back and start a job.</p></div></td></tr></tbody></table></td></tr><tr id="44794132"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44794132" href="https://news.ycombinator.com/vote?id=44794132&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>ROTH IRA contributions have to be from earned income, though. The rest of the advice is of the same quality, imho. Beware!</p></div></td></tr></tbody></table></td></tr><tr id="44793511"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44793511" href="https://news.ycombinator.com/vote?id=44793511&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>This is generally not applicable to most people.</p><p>1. Invest enough to get the company match in an S&amp;P 500.  It probably isn’t Vanguard that your company uses</p><p>2. Pay off all of your debt except your house (and maybe your car)</p><p>3. Max out your HSA - if you are married it’s - $8550</p><p>4.  Max out your 401K - again that’s probably not through Vanguard - $23500</p><p>5. Step 5 - then call Vanguard and depending on your income just do a Roth up to $8000 (?).</p><p>(Unless you are over 50 then do catch  up contributions as 4.5)</p><p>If you are under 50, you can do $40,500 tax advantaged and over 50 $48050</p></div></td></tr></tbody></table></td></tr><tr id="44793884"><td></td></tr><tr id="44794124"><td></td></tr><tr id="44793965"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793965" href="https://news.ycombinator.com/vote?id=44793965&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Step 6 is the mega-backdoor Roth. Do after tax contributions to your 401k and have your 401k servicer do an in-plan conversion to Roth 401k.</p><p>Also, on step 4, you may need to do a backdoor Roth IRA if you’re over the Roth IRA income limits.</p></div></td></tr></tbody></table></td></tr><tr id="44794057"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_44794057" href="https://news.ycombinator.com/vote?id=44794057&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I’ve only had one employer that allowed after tax contributions (which for other people reading this is not the same as Roth 401K).</p><p>The issue is that most companies don’t allow it because of compliance reasons and rules regarding highly compensated employees.  Of course the one company that did allow it was BigTech.</p><p>Not that I’m missing much.  I doubt I will be in a higher tax bracket at retirement than I am now and I live in a state tax free state.</p></div></td></tr></tbody></table></td></tr><tr id="44794109"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_44794109" href="https://news.ycombinator.com/vote?id=44794109&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>To me the primary benefit is when it comes to RMDs. Having a mix of traditional and Roth allows me to draw down the traditional in lower tax years to avoid RMD and have the Roth to fill in those later years or pass on to children. Plus, if you’ve maxed the $23,500ish of the traditional 401k and still have extra to save, it’s more tax advantaged in a Roth vs a taxable brokerage account.</p><p>These are obviously champagne problems but if you’re a high earning W2 it’s worth considering.</p></div></td></tr></tbody></table></td></tr><tr id="44794228"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44794228" href="https://news.ycombinator.com/vote?id=44794228&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>For those of us very late to the investing game, what is the best strategy to try to catch up at least somewhat?</p></div></td></tr></tbody></table></td></tr><tr id="44792340"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44792340" href="https://news.ycombinator.com/vote?id=44792340&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>My millionaire, step-father-in-law, gave this advice to my brother when graduated.</p><p>I was lucky, my physics department administrator told me the same thing when I was graduating.</p><p>The 2ND best piece of advice is to rollover your 401k when you move to a new company -&gt; this cost me at least 500k because they effectively stagnate when your company isn't paying the maintenance cost (AIUI).</p></div></td></tr></tbody></table></td></tr><tr id="44792607"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44792607" href="https://news.ycombinator.com/vote?id=44792607&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>&gt; this cost me at least 500k because they effectively stagnate when your company isn't paying the maintenance cost</p><p>Is this true? My understanding is that the fees come out of the account itself. There's other good reasons to roll over (primarily investment flexibility) but I have not heard of something like this.</p></div></td></tr></tbody></table></td></tr><tr id="44793312"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793312" href="https://news.ycombinator.com/vote?id=44793312&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I don't think the point about 401k stagnation is true. At most fee structures and optionality of funds change. How did that cost you 500k exactly?</p></div></td></tr></tbody></table></td></tr><tr id="44793307"><td></td></tr><tr id="44793420"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793420" href="https://news.ycombinator.com/vote?id=44793420&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Vanguard is one of the cooler-structured brokerages. Vanguard (the management firm) is owned by the mutual funds themselves, of which you are an investor of. So their shareholder obligation is genuinely towards "clients" of the individual mutual funds. As far as I'm aware, this is the only mutually-owned mutual fund firm.</p><p>It's definitely got a solid track record and good fees, but these are things I'd feel weird about advertising it on HN for.</p></div></td></tr></tbody></table></td></tr><tr id="44793333"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793333" href="https://news.ycombinator.com/vote?id=44793333&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>vanguards very large and they're a mutual fund, so they have a lower profit motive than most brokers for it. they're basically the standard retirement fund company now</p></div></td></tr></tbody></table></td></tr><tr id="44791219"><td></td></tr><tr id="44790852"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44790852" href="https://news.ycombinator.com/vote?id=44790852&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Most problems (including analytically intractible ones) can be modeled with a relatively simple monte-carlo simulation. Most simple monte-carlo simulations can be fully implemented in a spreadsheet.</p><p>Using timing coincidences in particle physics experiments is incredibly powerful. If multiple products from the same reaction can be measured at once, it's usually worth looking into.</p><p>Circular saws using wood cutting blades with carbide teeth can cut aluminum plates.</p><p>You can handle and attach atomically thin metal foils to things by floating them on water.</p><p>Use library search tools and academic databases. They are entirely superior to web search and AI.</p></div></td></tr></tbody></table></td></tr><tr id="44790911"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44790911" href="https://news.ycombinator.com/vote?id=44790911&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I feel like the Monte-Carlo simulation modeling trick is one I picked up intuitively but only recently heard formalized. Do you (or anyone else) have a list of example problems that are solved in this way? Like a compendium of case studies on how to apply this trick to real world problems?</p></div></td></tr></tbody></table></td></tr><tr id="44791753"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44791753" href="https://news.ycombinator.com/vote?id=44791753&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>_How to Measure Anything_ by Douglas Hubbard includes a chapter on Monte Carlo simulations and comes with downloadable Excel examples: <a href="https://www.howtomeasureanything.com/3rd-edition/" rel="nofollow">https://www.howtomeasureanything.com/3rd-edition/</a> (scroll down to Ch. 6)</p><p>The main example is, you're considering leasing new equipment that might save you money. What's the risk that it will actually cost more, considering various ranges of potential numbers (and distributions)?</p><p>I think it's harder to apply to software since there are more unknowns (or the unknowns are fatter-tailed) but I still liked the book just for the philosophical framing at the beginning: you want to the measure things because they help you make decisions; you don't need perfect measurements since reducing the range of uncertainty is often enough to make the decision.</p></div></td></tr></tbody></table></td></tr><tr id="44791827"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44791827" href="https://news.ycombinator.com/vote?id=44791827&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>A simple example that highlights the strength of this method: a 137Cs point source is at the origin. A detector consisting of a right cylinder at arbitrary distance and orientation is nearby. What is the solid angle?</p><p>There may exist an analytical solution for this, but I wouldn't trust myself to derive it correctly. It would certainly be a huge mess.</p><p>If we add that the source is also a right cylinder instead of point source, and we want to add first order attenuation of emitted gammas by the source itself, the spreadsheet becomes only a bit more complex, but there will not be a pen and paper equation solution.</p><p>In this example every row of the spreadsheet would represent a hypothetical ray. One could randomly choose a location in the source, a random trajectory, and check if the photon intersects the detector. An alternative approach would be randomly choosing points in both target and detector, then doing additional math.</p><p>The results are recovered by making histograms and computing stats on the outputs of all the rows. You probably need a few thousand for most things at least. Remember roughly speaking 10k hits gets you ~1% statistics.</p></div></td></tr></tbody></table></td></tr><tr id="44793863"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793863" href="https://news.ycombinator.com/vote?id=44793863&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>What annoys me with Monte Carlo methods is trying to get self-consistent statistics at multiple parameter values. E.g., in your example, what is the derivative of the solid angle as the detector is moved? Or more generally, if we have some 'net goodness' measure for a system depending on parameters, how can we efficiently maximize it, when simulations are noisy and basins are shallow?</p><p>My understanding is that these sorts of questions come up in ML, and there are ways of dealing with it, but they can't converge nearly as fast as simple iterations like Newton's method. Even if I have to take a series approximation instead of a simple formula, I'll be able to use autodiff (or at worst, symbolic differentiation) to get quick and precise answers to these questions.</p></div></td></tr></tbody></table></td></tr><tr id="44793432"><td></td></tr><tr id="44791231"><td></td></tr><tr id="44793142"><td></td></tr><tr id="44793567"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44793567" href="https://news.ycombinator.com/vote?id=44793567&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I've used various commercial databases over the years. Some popular commercial databases relevant to HN readers include Web of Science, Scopus, and Engineering Village. When I worked at the USPTO, I used the less popular database Dialog, which I preferred. To my knowledge, none of these are available direct to consumers. I've only been able to get access from places with subscriptions. Some university libraries allow visitors where you can use these databases for free on-site.</p><p>I would call these databases complementary, not "entirely superior". There are two main advantages. One is that these databases will contain many things that you can't find on Google. The second advantage of these databases is that they are designed for advanced searchers and have more powerful query languages. Google on the other hand is dumbed down and will try to guess what you want, often doing a poor job. You can get very specific on these databases in ways that you can't with Google.</p><p>Related: I'm somewhat fascinated by more specialized bibliographic databases because they often contain things that can't be found on Google or the major commercial databases I listed above. I started keeping a list of them. <a href="https://github.com/btrettel/specialized-bibs" rel="nofollow">https://github.com/btrettel/specialized-bibs</a></p></div></td></tr></tbody></table></td></tr><tr id="44794329"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44794329" href="https://news.ycombinator.com/vote?id=44794329&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>On a related note, if you studied and got a degree at a university, check if they have an alumni program. I pay a small yearly fee that lets me access the university's academic databases and their VPN, so I get some other perks as it looks like I'm connected through eduroam.</p></div></td></tr></tbody></table></td></tr><tr id="44793656"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793656" href="https://news.ycombinator.com/vote?id=44793656&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>even wood tipped circular saw can cut Al plate...</p><p>and academic/library search is indeed so underrated!</p></div></td></tr></tbody></table></td></tr><tr id="44793964"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793964" href="https://news.ycombinator.com/vote?id=44793964&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Most people really cannot tell you what they want in any reasonable way. So expecting good specs for software without a very laborious interview and review process is pure wishful thinking. People "know what they like when they see it", so spend time rapid prototyping.</p><p>Smaller and more recent: iTerm has deep tmux support. Just do `tmux -CC` to start your session or `tmux -CC a` to attach to it and you don't have to memorise all the tmux commands.</p></div></td></tr></tbody></table></td></tr><tr id="44793892"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793892" href="https://news.ycombinator.com/vote?id=44793892&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Understanding the concept of Opportunity Cost and how it applies to everything in life.</p><p>- Buying a house: Is this the best return I can get on a downpayment. (Spoiler: It is not).</p><p>- Accepting a specific job offer: Is this the best way to spend 8 hours a day?</p><p>- Not making a successful trade, is as much as a loss than losing money explicitely on a trade.</p><p>- If you own something, you should consider what could you do with it's cash value if you sold it instead.</p><p>- If you have a paid off home, could you sell it and get a better ROI with the cash equivalent and rent instead? (The answer is yes and you should do it).</p></div></td></tr></tbody></table></td></tr><tr id="44794201"><td></td></tr><tr id="44794235"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44794235" href="https://news.ycombinator.com/vote?id=44794235&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>As usual, I'd like to bring up that it is until you have enough profit for it to no longer be only about that.</p><p>Also regardless of the quality of the examples, it may be the case that the point (opportunity cost is important to consider) is still valid.</p></div></td></tr></tbody></table></td></tr><tr id="44794251"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44794251" href="https://news.ycombinator.com/vote?id=44794251&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Thanks for clarifying. Being mindful of opportunity cost allows people to retire early, to spend more time with their loved ones, do better financial decisions.</p><p>I have seen too many of my loved ones work until their 70s because they didn't think about those concepts enough. That is what is sad.</p></div></td></tr></tbody></table></td></tr><tr id="44794236"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44794236" href="https://news.ycombinator.com/vote?id=44794236&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>So It's really sad to think about the value of time or things? Don't you implicitly do that every time you chose an activity over another? When you chose a job over another? when you chose to spend your time with a friend or with your wife?</p></div></td></tr></tbody></table></td></tr><tr id="44791033"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44791033" href="https://news.ycombinator.com/vote?id=44791033&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>The big one, for keeping my focus on the power of repeated, consistent action, and prioritizing my "future selves":</p><p><i>What is likely to happen if I do (or don't do) this thing one thousand days (or times) in a row?</i></p><p>Examples:</p><p>- exercising 2h per day and eating right --&gt; I'm going to look and feel great and my health will be far better than that of my peers</p><p>- Should I buy these cookies along with the rest of my groceries? If I do that 1,000 grocery trips in a row …</p><p>- spending 30+ minutes per day reading the highest quality material I can find; taking notes; and figuring out ways to implement the knowledge and ideas I gain --&gt; …</p></div></td></tr></tbody></table></td></tr><tr id="44793663"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793663" href="https://news.ycombinator.com/vote?id=44793663&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>This is a good one. What's gonna happen if I respond to comments on HN for 1000 days in a row? Uh oh.</p></div></td></tr></tbody></table></td></tr><tr id="44791987"><td></td></tr><tr id="44791609"><td></td></tr><tr id="44793610"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793610" href="https://news.ycombinator.com/vote?id=44793610&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Whenever people try to argue that small and meaningful commits do not matter, I introduce them to git bisect. I’ve yet to meet someone familiar with it that does not agree.</p></div></td></tr></tbody></table></td></tr><tr id="44793447"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793447" href="https://news.ycombinator.com/vote?id=44793447&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Similar feelings about git worktree. Being able to check out multiple branches at once without having to deal with stash is a game changer.</p></div></td></tr></tbody></table></td></tr><tr id="44793335"><td></td></tr><tr id="44793086"><td></td></tr><tr id="44794262"><td></td></tr><tr id="44793347"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44793347" href="https://news.ycombinator.com/vote?id=44793347&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Agreed with interactive rebase, but what of reflog? I've checked that for commit hashes when things have gone very wrong but I don't know many commands.</p></div></td></tr></tbody></table></td></tr><tr id="44793792"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793792" href="https://news.ycombinator.com/vote?id=44793792&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>It allows you to observe and interact with the history of refs like HEAD, so superficially destructive operations like amending a commit can be recovered, reverted, etc. It's kind of like meta-git, allowing version control operations on the version control system. It's a lifesaver when you botch a merge or something and allows you to do "destructive" operations fearlessly (up to a point, you can munge a git repo pretty bad if you try hard enough).</p></div></td></tr></tbody></table></td></tr><tr id="44794139"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44794139" href="https://news.ycombinator.com/vote?id=44794139&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>duplicate code is not that bad. reduce duplication over time as you find the common patterns/abstractions, instead of trying to build abstractions that don't fit the use cases</p></div></td></tr></tbody></table></td></tr><tr id="44794214"><td></td></tr><tr id="44789137"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44789137" href="https://news.ycombinator.com/vote?id=44789137&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I only learned this in the last five years: do less, automate less, do more by hand, and use the limited capability of the manual method to really choose projects that are worthwile, rather than aim for maximum efficiency.</p></div></td></tr></tbody></table></td></tr><tr id="44790847"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44790847" href="https://news.ycombinator.com/vote?id=44790847&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Similar to this: if you want to optimize your productivity*, do so on a timescale of at least weeks if not months or years.</p><p>Simple example: Can you get more done working 12 hours a day than 8? Sure, for the first day. Second day maybe. But after weeks, you're worse off in one way or another.</p><p>It's easy to chase imaginary gains like automating repetitive tasks that don't actually materialize, but some basics like sleep, nutrition, happiness, etc are 100% going to affect you going forward.</p><p>* I actually hate that word, and prefer saying "effectiveness". Productivity implies the only objective is more, more, more, endlessly. Effectiveness opens up the possibility that you achieve better results with less.</p></div></td></tr></tbody></table></td></tr><tr id="44794014"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44794014" href="https://news.ycombinator.com/vote?id=44794014&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Realizing that buying a house is absolutely not a good investment and that the whole society is on a narrative to convince more and more people to blindly buy (realtors, lenders, other homeowners, the governments, parents,...)</p><p>Doing the real math is the trick of the trade. The math for owning has been made so that it looks like a good deal while in reality it is not at all. Most people will literally compare mortgage to their rent, or "I sold my house I bought for 500k for 1M$, therefore I made 500k$".</p><p>Treat owning as a luxury item. The same way you would own a sport car or travel on a private jet. Do the (real) math and realize Owning is costing you money.</p><p>Also don't let yourself get emotional about buying a house. Society has made it look as if buying a house was a proof of success. A lot of research shows that once people buy they lose flexibility, feel more stuck, cannot access higher paying job in a different places etc. Renting has a ton of advantages.</p><p>This calculator get most things right. As an exercise, you can try to retroactively put the numbers for the house you bought and the rent equivalent. The results might surprise you:
www.nytimes.com/interactive/2024/upshot/buy-rent-calculator.html</p></div></td></tr></tbody></table></td></tr><tr id="44794118"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44794118" href="https://news.ycombinator.com/vote?id=44794118&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>As I grew older (and hopefully wiser), I have internalized investment returns is a lot more than sanitized numbers. I did the numbers and as a result did not buy much real estate when I was yonger. It was pretty clear, if I put the same cash outflow into stocks, I would come out ahead... Way ahead!</p><p>The things is, I didn't put the same cash outflow in stocks. For various reasons. But mainly, it was just not psychologically palatable to max out my investments on equity by streching my finances. With hindsight, given the buffer in most real estate investments (left with tangible assets, banks shouldering some of the risk etc.) it would have been a lot more psychologically palatable to investment my max in real estate.</p><p>My point is not that real estate is "best". But merely, there is a lot more to returns than cold numbers. If you think you already understand it, and are under 30, you are likely underestimating the significance of that. I know I did.</p></div></td></tr></tbody></table></td></tr><tr id="44794161"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44794161" href="https://news.ycombinator.com/vote?id=44794161&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>This is the argument that buying a house serves as a subpar, forced investment for most people.</p><p>But I don't know if I agree with you, I understood in my twenties that housing was generally a bad investment. We are now 10 years later and I invested everything in the stock market instead and came way ... way ahead than if I bough one or more houses.</p><p>I personally find it way way more scary to buy an average 2M$ house that could crash and lose half of its value overnight than the whole American economy that is relatively diversified.
What happens if there is a fire in your neighborhood? Or if your city is the next Detroit? 
We have been made to believe a house is a safe investment. I personally think it's the scariest least diversified investment you could ever make.</p></div></td></tr></tbody></table></td></tr><tr id="44794250"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44794250" href="https://news.ycombinator.com/vote?id=44794250&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>You know I hear this all the time but it never quite makes sense to me. REA is inherently a better asset class than the vast majority of assets, simply because you can leverage up without a risk of being stopped out.</p><p>Doing the numbers on (most) developed economies, buying freehold housing is typically a worse investment than stocks before leverage, but after, RE almost always comes ahead. Nobody is going to let you mortgage a basket of stocks, but almost any bank will let you do that with a house.</p><p>That said - I hate this idea because I think this kind of thinking is what has lead to many of societies problems in the developed world at the moment. But, from a rational point of view the numbers make sense.</p></div></td></tr></tbody></table></td></tr><tr id="44794268"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44794268" href="https://news.ycombinator.com/vote?id=44794268&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>It's worse than that. Buying a house without leverage is a terrible investment.</p><p>The leverage is absolutely required because all the fees that come with real estate (Realtors, Interest, HOA, Taxes, Insurances, Closing costs, Downpayment opportunity cost,...). The leverage makes it an OK investment, but still historically not on the level of a diversified basket of stocks.</p><p>The leverage also works in both ways and essentially makes your house an even more risky asset. It supposes that it only ever goes up which has not always been the case historically.</p></div></td></tr></tbody></table></td></tr><tr id="44794094"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44794094" href="https://news.ycombinator.com/vote?id=44794094&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>*&gt; Most people will literally compare mortgage to their rent, or "I sold my house I bought for 500k for 1M$, therefore I made 500k$".</p><p>I’ll bite: what’s the problem? If anything mortgage is even more favorable than rent because part of it goes to paying off the principal. Which is still your money, just in a different asset. But I guess you include this and still reach the opposite result?</p><p>Are you talking about taxes or maintenance or something ? Which country / locality? What time span? How many places are there where you would’ve been better off renting than buying in recent memory… not where I’ve lived but you never know!</p></div></td></tr></tbody></table></td></tr><tr id="44794131"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44794131" href="https://news.ycombinator.com/vote?id=44794131&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Because the mortgage and the rent are completely different numbers.</p><p>The rent is the maximum you will ever pay for that period.
The mortgage is the minimum you will pay and doesn't include the downpayment, repairs, HOA, Insurance and tax increases. It also doesn't include the closing costs, realtor costs, all the fees.</p><p>This is why the math is so complex. People easily forget all those fees and costs to make the math look simple.</p><p>This also takes into account that you invest into the US market and equity with all the cash that you would have had to spend on your house.</p><p>I'm talking about the US, but this holds true in multiple countries.</p><p>To answer your last question: In almost all HCOL (SF, Seattle, ...) you would have been way better renting than buying, except if you time it perfectly.
If you look at today's market you would almost definitely be better renting than buying (But nobody knows the future...)</p></div></td></tr></tbody></table></td></tr><tr id="44794299"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44794299" href="https://news.ycombinator.com/vote?id=44794299&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>This is just plain wrong. Let's break it down:</p><p>* Insurance and tax increases: real and you have to deal with them, but they don't go up as fast as rent does. Advantage: owning</p><p>* HOA fees: don't buy an HOA home, problem solved. You should do that anyway just because HOAs suck ass.</p><p>* Down payment: not a real cost, that money is yours in the form of equity</p><p>* Closing costs and other fees: real but completely negligible when amortized over the life of the home.</p><p>* Repairs: real, and they suck. This is the only place renting can stack up superior, but you can do a lot to mitigate this by doing your due diligence up front and not buying a dud house. Overall, unless you get unlucky you should come out ahead, but you can lose the die roll.</p><p>* Realtor costs: greatly depends. When we bought our house we paid zero, because the seller pays the buyer's costs. This can be a serious cost if you're changing houses a lot, but... just don't do that (for multiple reasons).</p><p>Overall, after buying my house 7 years ago I'm coming out ahead month over month (thanks to rent going up like clockwork every year, while my mortgage has stayed the same). By the time I die, I will be <i>significantly</i> ahead, and that's not even taking into account unrealized gains in the value of the property. Which I don't count for much, because my home is a place to live and not an investment vehicle. But maybe someday it'll do me good, or my heirs.</p><p>Owning a home <i>really is</i> a great deal for the <i>vast</i> majority of people in the US. It's not some society wide conspiracy, it's not an ad to get others into the market so you can benefit, it's the truth.</p></div></td></tr></tbody></table></td></tr><tr id="44794073"><td></td></tr><tr id="44794087"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44794087" href="https://news.ycombinator.com/vote?id=44794087&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>You are exactly right and I will add this to my post.</p><p>This calculator get most things right. As an exercise, you can try to retroactively put the numbers for the house you bought and the rent equivalent. The results might surprise you:
www.nytimes.com/interactive/2024/upshot/buy-rent-calculator.html</p></div></td></tr></tbody></table></td></tr><tr id="44794121"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44794121" href="https://news.ycombinator.com/vote?id=44794121&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>This is of course true if your house is not income producing. If you rent out rooms of your house, the calculus changes considerably.</p><p>…and now having viewed their calculator, I find it disingenuous that they haven’t added this as an additional configurable step, given how many bells and whistles their tool has. The takeaway from the tool is that you should never buy in HCOL areas, but renting out rooms in your unit is potentially a way to make it work out financially.</p></div></td></tr></tbody></table></td></tr><tr id="44794190"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44794190" href="https://news.ycombinator.com/vote?id=44794190&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I don't know if I agree. On average renting a room out of your place is the same as buying a place as a rental investment with the difference you can claim you live there (which gives you some small tax advantage).</p><p>In general, real estate rental investment are terrible subpar investment. How would this be different?</p></div></td></tr></tbody></table></td></tr><tr id="44792099"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44792099" href="https://news.ycombinator.com/vote?id=44792099&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>&gt; What Trick of the Trade took you too long to learn?</p><p>"Everything worth doing is worth doing badly"</p><p>And as a corollary, every complex system that works came from a simple system that works.</p><p>I learned this in programming, but now I apply it on everything from motorcycle maintenance, home appliance repair to parenting.</p><p>--</p><p>Often the easier way to fix a complex system is to pretend that it could be simpler and then reintroduce the complexity-inducing requirements.</p><p>I had a professor who taught debugging as a whole another skill from programming and used to say "Most of programming is starting from an empty editor and debugging until your code works".</p><p>The debugging "lab" in Java course (in the year 2000) was one of my transformational after-school classes - where I got a java program which fits within 2-3 pages of print code with a bug and was told to go find it in print for ~20 minutes, then given 40 minutes with a debugger instead.</p></div></td></tr></tbody></table></td></tr><tr id="44792563"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44792563" href="https://news.ycombinator.com/vote?id=44792563&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>&gt; And as a corollary, every complex system that works came from a simple system that works.</p><p>"Do the simplest thing that works" is one of the few core architecture principles I stick my neck out for time and time again. Why write a simple function when you can spend a week accounting for every imagined corner case and implement modular expansion capabilities! Please... stop...</p><p>I empathize with the debugger story. If you're super deep in a language it makes sense to know the debugger inside and out. But stdout is universal and I've never been a specific language developer rather than being able to jump into whatever is needed.</p></div></td></tr></tbody></table></td></tr><tr id="44793050"><td></td></tr><tr id="44790171"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44790171" href="https://news.ycombinator.com/vote?id=44790171&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Writing tests first is a good way to end up with testable code. If you skip that, retrofitting tests is incredibly difficult.</p></div></td></tr></tbody></table></td></tr><tr id="44790762"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44790762" href="https://news.ycombinator.com/vote?id=44790762&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>While it can be a useful forcing function, I find it also just fits into a higher velocity workflow in general, one I would describe like so:</p><p>1. Make PRs small, but not as small as you can possibly make them.</p><p>2. Intend to write at least one nice-ish test suite that tests like 50-80% of the LOC in the PR. Don't try to unit test the entire thing, that's not a unit test. And if something is intrinsically hard to test - requires extensive mocking etc - let it go instead of writing a really brittle test.</p><p>3. Tests only do two things: help you make the code correct right now, or help the company keep the code right long term. If your test doesn't do either, don't write it.</p><p>4. Ok - now code. And just keep in mind you're the poor sod who's gonna have to test it, so try to factor most of the interesting stuff to not require extensive mocking or shit tons of state.</p><p>I've found this workflow works almost anywhere and on almost any project or code. It doesn't require any dogmatic beliefs in PR sizes or test coverages. And it helps prevent the evils that dogmatic beliefs often lead you into. It just asks you to keep your eyes open and don't paint yourself into a corner, short term or long term.</p></div></td></tr></tbody></table></td></tr><tr id="44790930"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44790930" href="https://news.ycombinator.com/vote?id=44790930&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>One more piece - if the test would be hard to write, use that to drive you to clean up the architecture of that piece of code.</p></div></td></tr></tbody></table></td></tr><tr id="44790728"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44790728" href="https://news.ycombinator.com/vote?id=44790728&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Even if you aren't going to do the complete test code just writing down the expected checks for each test makes things so much easier</p></div></td></tr></tbody></table></td></tr><tr id="44790915"><td></td></tr><tr id="44793555"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793555" href="https://news.ycombinator.com/vote?id=44793555&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>When I stopped trying to be right and I started trying to be friends my career finally took off.</p></div></td></tr></tbody></table></td></tr><tr id="44793643"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793643" href="https://news.ycombinator.com/vote?id=44793643&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>When you buy a house and get a mortgage, you are going to be paying MUCH more in interest (than expected). Over the course of the mortgage, you are going to be paying MUCH more than the sticker price. Between closing costs and taxes and fees maintenance, you will need more cash than you think.</p><p>My advice is look at the numbers very carefully and choose something that is (below) or fits your budget. Sudden financial issues like the loss of a job or new vehicle purchase can put a big strain on all this.</p></div></td></tr></tbody></table></td></tr><tr id="44793748"><td></td></tr><tr id="44793875"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44793875" href="https://news.ycombinator.com/vote?id=44793875&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>it's called renting.</p><p>Americans are obsessed with owning a home at all cost. This means that you are effectively bidding against people that do not even do the math. They are ready to spend Millions of dollar on something that is comparatively cheap to rent.</p><p>The fact that absolutely everyone wants to buy pushed prices through the roof. The good news is that you can take the other side of this bet. it's called renting.</p><p>Currently in most places in the US you will save literally millions over the 30 year mortgage by renting and investing in the market instead.</p><p>Reminder that renting and owning is functionally almost exactly the same thing.</p><p>Never trust your realtor and never trust other homeowner that most of the time never did the math (We all know those people: "I bought my home for 500k 15 years ago. It is now worth 1M$, therefore I made 500k$").</p><p>In other words, let other people take the irrational side of this bet and take the rational side by renting. It's an arbitrage opportunity.</p></div></td></tr></tbody></table></td></tr><tr id="44793997"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793997" href="https://news.ycombinator.com/vote?id=44793997&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>An important fact that this doesn't account for is that, in the United States, living in housing that you own is highly tax-advantaged, at least if you can get a mortgage on it. For example, mortgage interest is tax-deductible for owner-occupied housing (whereas landlords usually can't deduct interest on their mortgages and so those taxes are passed on to renters), and mortgage rates for owner-occupied housing are far below market due to government subsidies and guarantees (whereas landlords have to pay higher rates that, again, they pass on to renters). This isn't good policy, but as long as it's the case, buying a single-family home is a smarter financial choice for most Americans than renting one.</p></div></td></tr></tbody></table></td></tr><tr id="44794047"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_44794047" href="https://news.ycombinator.com/vote?id=44794047&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I live in the US and I'm aware of this. Those tax deductible interest should be calculated in the complex buy vs rent equation.</p><p>In fact, even when taking them into account, it still doesn't make sense for most Americans to own. (In today's market).</p><p>Renting and investing is still the way to go for at least 75% of Americans (this is slightly more nuanced for low cost of living areas, but hold true for any MCOL or HCOL areas).</p><p>Eventually the math could make sense again, but right now owning is a huge luxury that will cost you millions in the long run.</p><p>I invite you to play with this calculator:
www.nytimes.com/interactive/2024/upshot/buy-rent-calculator.html</p></div></td></tr></tbody></table></td></tr><tr id="44794178"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_44794178" href="https://news.ycombinator.com/vote?id=44794178&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I am a renter so I don't have a horse in this race, but renting is many times the financially worse choice even in HCOL.</p><p>Why? Because rent inflates like crazy over here! In the Bay Area 7%+ a year is completely expected, and 10% is not unusual. I have been all over the Bay Area for more than a decade (San Francisco proper, East Bay, South Bay) and know this well. It's been nuts.</p><p>Random example: the 1 bedroom apartment that I lived in 2012 and was then going for $1,500 a month, is now going for $3,800 in the exact same building (with no/minimal renovations it seems, I just looked it up). An ~8% YoY increase. That will do it to any buy vs rent calculator, very easy to break even in under 5 years, and that's excluding the speculative ability to refinance if interest rates go down from the current 7%, in which case it becomes a huge boost.</p><p>Renting as a long term choice just works in European countries where normal people can lock in 5+ year leases with no or minimal rent increases. America is too profit-seeking and greedy for that.</p><p>I still rent for flexibility reasons, but I definitely see it as a luxury lifestyle choice, the most financially responsible thing would be to buy, even in HCOL.</p><p>All this in my opinion and personal experience, totally fine if people see it differently.</p></div></td></tr></tbody></table></td></tr><tr id="44794199"><td><table><tbody><tr><td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td><center><a id="up_44794199" href="https://news.ycombinator.com/vote?id=44794199&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>SF is the poster child of a HCOL where buying makes absolutely no sense.</p><p>Even if rent increases a lot, the buy to rent ratio is so horrible that it could continue to increase for MANY more years before buying could make sense.</p><p>I invite you to use the NYT Rent or buy calculator, It is clear as day: 
www.nytimes.com/interactive/2024/upshot/buy-rent-calculator.html</p></div></td></tr></tbody></table></td></tr><tr id="44794226"><td><table><tbody><tr><td indent="7"><img src="https://news.ycombinator.com/s.gif" height="1" width="280"></td><td><center><a id="up_44794226" href="https://news.ycombinator.com/vote?id=44794226&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I just did, picturing exactly the situation I'm in right now:</p><p>- Rent: $3,500</p><p>- Home price: $700,000 (a similar unit just sold for this price a few months ago in my building)</p><p>- Rent increase: 8%</p><p>- All other parameters left as default, which seem reasonable (and as I said, there might be chances of refinancing over the next 10 years, which would drastically skew the picture, but I'm leaving that assumption out)</p><p>The ratio of 0.5% monthly rent/price is common for non-luxury "dated" condos all over the city, so I think my situation reflects well the typical renter.</p><p>Once again, in my personal experience, guided by a decade+ of living here, what people miss is the crazy rate of rent inflation. There is always a massive rent increase right around the corner, and God forbid if you are forced to move (because the landlord wants you to, it happened a couple times), then you take a gigantic hit at market rate. Once you factor in these occasional resets and the standard yearly increase, you get very close to 10% rent increase.</p></div></td></tr></tbody></table></td></tr><tr id="44794167"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_44794167" href="https://news.ycombinator.com/vote?id=44794167&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>It's not all numbers, though.  Both have a lot of intangibles that can and should affect your decision.</p><p>Owning can feel suffocating at times, and like a ball and chain at others. You can't just decide you don't like it or the area anymore and go.  Maintenance is also no joke.</p><p>Renting feels ephemeral.  Getting kicked out at lease end sucks, it's hard to uproot everything and start over.  Having inane rules and a landlord constantly drive by can make you feel both infantile and spied on.</p><p>I've done both off and on and those are my own thoughts on the two.</p><p>Financially only it's easy to pick a winner.  But for some, one of these factors may be worth the extra however much money the difference is.</p></div></td></tr></tbody></table></td></tr><tr id="44794210"><td><table><tbody><tr><td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td><center><a id="up_44794210" href="https://news.ycombinator.com/vote?id=44794210&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>The best way to calculate those intangible is to associate a value to them, Most people love to say that owning is so good because they can decide on their own house improvement.</p><p>Ok, but how much do you really value this over? Is it worth 2M$ over 30 years? Because in a lot of cases this is what you leave on the table by deciding to own.</p></div></td></tr></tbody></table></td></tr><tr id="44794205"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_44794205" href="https://news.ycombinator.com/vote?id=44794205&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>But mortgage interest is an itemized deduction, which means it only becomes a tax benefit when your interest + other deductions exceed the standard deduction. And if you do take the deduction, only the delta between it the standard deduction is a benefit.</p></div></td></tr></tbody></table></td></tr><tr id="44793718"><td></td></tr><tr id="44794023"><td></td></tr><tr id="44793708"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793708" href="https://news.ycombinator.com/vote?id=44793708&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>You can memorize the correct opening moves in chess. For maybe my first year playing chess, I just YOLO'd the opening moves. My judgement there was probably not much worse than the rest of my play, but with other players playing engine moves in the opening, I was probably in a losing position early on in most of my games. I gained about, I think, 100 ELO after learning some 3 or 4 move opening combinations.</p></div></td></tr></tbody></table></td></tr><tr id="44794039"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44794039" href="https://news.ycombinator.com/vote?id=44794039&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Claude Code recently showcased how powerful it can be when you don’t have to memorize commands. My AI agent works similarly. It finds the right CLI commands instead of relying on Playwright or an MCP server to perform tasks. What’s interesting is that even the agent doesn’t know many commands upfront; it simply uses the help option to discover what’s available.</p></div></td></tr></tbody></table></td></tr><tr id="44793618"><td></td></tr><tr id="44793962"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793962" href="https://news.ycombinator.com/vote?id=44793962&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Similarly, it's `XXX=123`, and cannot be `XXX = 123` or `$XXX=123`.</p><p>Shellcheck (and Wooledge) are crucial!</p></div></td></tr></tbody></table></td></tr><tr id="44793933"><td></td></tr><tr id="44793993"><td></td></tr><tr id="44789428"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44789428" href="https://news.ycombinator.com/vote?id=44789428&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>How do you maintain tests, in order for LLM edits to not keep breaking things?</p><pre><code>  - As a formal test suite in the program's own language?
  - Or using a .md natural language "tests" collection that must pass, which an LLM can understand?

</code></pre><p>
To answer the OP, I learned use different models for reasoning vs. coding.</p></div></td></tr></tbody></table></td></tr><tr id="44793743"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793743" href="https://news.ycombinator.com/vote?id=44793743&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Always fix errors in reverse order, from the bottom of the file to the top. That way the line numbers don't change as you go.</p></div></td></tr></tbody></table></td></tr><tr id="44793047"><td></td></tr><tr id="44790844"><td></td></tr><tr id="44793093"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793093" href="https://news.ycombinator.com/vote?id=44793093&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>But don’t fall into the trap of that’s all you do. This can lead to procrastination of the thing you’re trying to do.</p></div></td></tr></tbody></table></td></tr><tr id="44790879"><td></td></tr><tr id="44793448"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793448" href="https://news.ycombinator.com/vote?id=44793448&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Easy transfers between different people's iThings with Airdrop. I got my CompSci degree over 30 years ago and yet my 74yo aunt taught me this - the shame!</p></div></td></tr></tbody></table></td></tr><tr id="44793506"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793506" href="https://news.ycombinator.com/vote?id=44793506&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>One other benefit, if it happens to matter to you, is that Airdropped files like photos or videos retain their original quality as opposed to taking a slight hit to quality when being transferred via text or email.</p></div></td></tr></tbody></table></td></tr><tr id="44793534"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44793534" href="https://news.ycombinator.com/vote?id=44793534&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Also good to know is that if you crop a picture in your iOS photos app and then airdrop it to someone, they can undo the crop in their photos app. It is a non-obviously non-destructive operation.</p></div></td></tr></tbody></table></td></tr><tr id="44790848"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44790848" href="https://news.ycombinator.com/vote?id=44790848&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>That macros ruin everything they touch. I used them extensively for maybe 15 years, stopped adding them, and then a bit later removed them all from my code. My C/C++ code was a lot nicer without them.</p></div></td></tr></tbody></table></td></tr><tr id="44791011"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44791011" href="https://news.ycombinator.com/vote?id=44791011&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>My early C code would have been awfully slow without them.  We needed enums and good inlining before we could ditch (most) macros.  When did Zorland/Zortech C become good enough?</p><p>There are still a few special cases where macros are useful, such as the multiple #include trick where a macro #defined before the #include determines what the macro invocations in the include file does -- really helpful for building certain kinds of tables.</p></div></td></tr></tbody></table></td></tr><tr id="44791313"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44791313" href="https://news.ycombinator.com/vote?id=44791313&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Zortech had an inliner even before it was Zortech.</p><p>The #include trick is called the "X Macro". I used it extensively, and eventually just removed it.</p></div></td></tr></tbody></table></td></tr><tr id="44791757"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44791757" href="https://news.ycombinator.com/vote?id=44791757&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Was it reliable enough?</p><p>(I have never played with it -- I saw the ads in Byte but I never met anybody who had tried it.  It seemed so ridiculously cheap that I felt it had to be a scam ;) )</p></div></td></tr></tbody></table></td></tr><tr id="44792862"><td></td></tr><tr id="44794172"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44794172" href="https://news.ycombinator.com/vote?id=44794172&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Learn how to stab your colleagues in the back and play petty mean spirited office politics games. That's most of what being employed in this industry actually is, and if you accept that now and optimize yourself around it you can save decades wasted trying to actually get good at doing tangible (albeit economically useless) things.</p></div></td></tr></tbody></table></td></tr><tr id="44793384"><td></td></tr><tr id="44793825"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793825" href="https://news.ycombinator.com/vote?id=44793825&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>If you're selling something physical, always have free shipping. Include the price of shipping in the price of the product.</p><p>I guess people now expect free shipping via Amazon and boy does this make things sell faster.</p></div></td></tr></tbody></table></td></tr><tr id="44793158"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793158" href="https://news.ycombinator.com/vote?id=44793158&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>read something new every day before going to bed</p><p>journal before you start your day</p><p>buy some sort of electric kettle</p></div></td></tr></tbody></table></td></tr><tr id="44793112"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793112" href="https://news.ycombinator.com/vote?id=44793112&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Environment variables can be expanded at the command line, just like files.  Believe I started before it was a thing and never thought to check until twenty years later(?), when I hit the tab key by mistake. :-D</p></div></td></tr></tbody></table></td></tr><tr id="44794037"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44794037" href="https://news.ycombinator.com/vote?id=44794037&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>You should never look at code and say "this should work". If it "should" work it would work. If it doesn't work, you definitely made a mistake. Poke every assumption one by one. Preferably with an interactive debugger.</p><p>This may seem obvious but when I was younger I used to spin out in frustration at bugs.</p></div></td></tr></tbody></table></td></tr><tr id="44790820"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44790820" href="https://news.ycombinator.com/vote?id=44790820&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>When I realized that a) `screen` exists and b) what it does, I felt like an utter fool for having gone for years—<i>YEARS</i>—without benefiting from it.</p></div></td></tr></tbody></table></td></tr><tr id="44793398"><td></td></tr><tr id="44793550"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44793550" href="https://news.ycombinator.com/vote?id=44793550&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>It is a terminal multiplexer. You will be able to find youtube videos. The gp is talking about a tool called gnu screen. If you need a more distinct token to search on try “tmux”.</p></div></td></tr></tbody></table></td></tr><tr id="44793883"><td></td></tr><tr id="44793551"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44793551" href="https://news.ycombinator.com/vote?id=44793551&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Pretty sure they mean GNU screen, a terminal multiplexer. Similar in functionality to tmux or zellj (a newer alternative) if you have heard of those.</p></div></td></tr></tbody></table></td></tr><tr id="44793355"><td></td></tr><tr id="44793336"><td></td></tr><tr id="44791973"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44791973" href="https://news.ycombinator.com/vote?id=44791973&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Measure everything. There are two benefits to this.</p><p>1. Discarding the bullshit. A consistent practice of weighting assumptions and conclusions on evidence/numbers helps identify biases and motives from other people.</p><p>2. Measures allow for value identification and performance. Most people just guess at this. Guessing is wrong more than 80% of the time and often wrong by multiple orders of magnitude.</p><p>Most people don’t think like this and find this line of thinking completely foreign, so I often just keep my conclusions to myself. To see a movie about this watch Money Ball.</p></div></td></tr></tbody></table></td></tr><tr id="44792060"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44792060" href="https://news.ycombinator.com/vote?id=44792060&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>This is interesting. I’ve often thought about building a dashboard for my personal life, comparable to the dashboards I build for growth teams.</p><p>And then iterating on it over time, and I find what’s valuable and what’s not.</p></div></td></tr></tbody></table></td></tr><tr id="44793439"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793439" href="https://news.ycombinator.com/vote?id=44793439&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Learning that it is more important to know how to communicate, how to promote myself and the art of persuasion to get ahead than any technical skills I could know.</p></div></td></tr></tbody></table></td></tr><tr id="44793105"><td></td></tr><tr id="44790861"><td></td></tr><tr id="44793481"><td></td></tr><tr id="44793857"><td></td></tr></tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I asked four former friends why we stopped speaking (2023) (144 pts)]]></title>
            <link>https://www.vogue.com/article/reconnecting-with-ex-friends</link>
            <guid>44788783</guid>
            <pubDate>Mon, 04 Aug 2025 17:18:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vogue.com/article/reconnecting-with-ex-friends">https://www.vogue.com/article/reconnecting-with-ex-friends</a>, See on <a href="https://news.ycombinator.com/item?id=44788783">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content" tabindex="-1"><article lang="en-US"><div><header><div data-testid="ContentHeaderContainer"><figure><div><p><span><div data-testid="aspect-ratio-container"><picture><source media="(max-width: 767px)" srcset="https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_120,c_limit/00-story%20(2).jpg 120w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_240,c_limit/00-story%20(2).jpg 240w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_320,c_limit/00-story%20(2).jpg 320w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_640,c_limit/00-story%20(2).jpg 640w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_960,c_limit/00-story%20(2).jpg 960w" sizes="100vw"><source media="(min-width: 768px)" srcset="https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_120,c_limit/00-story%20(2).jpg 120w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_240,c_limit/00-story%20(2).jpg 240w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_320,c_limit/00-story%20(2).jpg 320w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_640,c_limit/00-story%20(2).jpg 640w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_960,c_limit/00-story%20(2).jpg 960w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_1280,c_limit/00-story%20(2).jpg 1280w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_1600,c_limit/00-story%20(2).jpg 1600w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_1920,c_limit/00-story%20(2).jpg 1920w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_2240,c_limit/00-story%20(2).jpg 2240w" sizes="100vw"><img alt="Women at beach with tear running through it" src="https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_2560%2Cc_limit/00-story%2520(2).jpg"></picture></div></span></p><p><span>Photo: Getty Images</span></p></div></figure></div></header></div><div data-testid="ArticlePageChunks" data-attribute-verso-pattern="article-body"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>On a warm July evening, I dove into bed and grabbed my phone, giddy and anxious. As I scrolled through TikTok, attempting to calm my nerves, a Google Calendar notification flashed on the screen: “VIDEO CALL WITH SIMONE.”</p><p>Before I could swipe the reminder away, Simone FaceTimed me. I attempted to rehearse my greeting as the call buffered: <em>Should I keep it cool with a, “Hey, what’s good?” No, that sounds cold. What about a Keke Palmer-esque, “Girl!” No, that’s doing too much. “Good evening?” No, it’s not evening her time, that doesn’t even make sen—</em></p><p>“Girl!” Simone said with a chuckle.</p><p>I couldn’t help but crack a smile. As I’d learned over the course of our six-year friendship, her warmth never failed to replace my anxiety with joy.</p><p>“Damn, it’s been a <em>minute</em>.” she added.</p><p>She was right. Though Simone is my closest friend, we don’t see or talk to each other often. Both are my fault. In 2020, after months holed up in my tiny Washington, D.C. apartment, I decided to wait out the winter at my mother’s cottage in Kenya. It was just what the doctor ordered, and a few months later, I decided to move to Nairobi permanently.</p><p>My move changed our friendship—it changed <em>all</em> of my friendships, actually. I tried to stay in touch with my friends stateside for a while, but as time went on, FaceTime dates became harder to plan, and fewer voice notes were exchanged via WhatsApp. Now, I don’t know if I can call any of them friends anymore—and my relationship with Simone felt like it was hanging by a thread.</p><p>Things in Kenya aren’t much better. Though I’m Kenyan by ethnicity, I grew up abroad, in the US and UK, and I’ve found that my foreign accent and perspective other me, even within my family. These days, my social life tends to begin and end with nights on the couch, re-watching <em>Shameless</em> with my boyfriend. I’m ashamed and terrified about that reality; it feels dangerous to rely on only him for human connection.</p><p>After all, friends are witnesses to your life. They enrich the living experience. Not having that makes me feel like that tree that falls in the forest alone: Can anybody hear me? Do I matter?</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Marriage and family therapist Shontel Cargill promises me that these feelings are normal. She says that friendship loss in one’s mid-to-late 20s is common for several reasons: life transitions, romantic relationships, evolution of priorities, and more. And while it doesn’t happen to everyone, for some, friendship loss “can lead to psychological distress,” sparking issues with anxiety, depression, trust, and self-esteem. <em>Check, check, check, and check.</em></p><p>Cargill says that talking about your struggles with others can help the healing process, but I’ll be honest—that hasn’t worked for me. Most people I’ve spoken to about my predicament don’t get it, which only makes me feel worse. I tried to bring it up on my aforementioned call with Simone, but her empathetic smile and pitying eyes said it all: She couldn’t relate. Lucky her.</p><p>I needed answers. Concrete ones—not those generic suggestions that I “put myself out there” or “just give it time.” Everyone around me had managed to hold on to friends throughout their lives; everyone seemed to be on girls’ trips and boozy brunches; seemed to have a tribe of confidants ready to drop everything for them. And here I was, a lonely, overworked 28-year-old who spent way too much time in her apartment, wondering why she didn’t have any of that.</p><p>So, like a good journalist, I decided to investigate. After speaking to Simone, I determined that I’d reach out to some of my former friends directly, and see if we could have a conversation about why we “broke up.” Many declined, and understandably so. But to my surprise, a few agreed to participate in my crazy scheme.&nbsp;</p><p>Here are those conversations—and their revelations. Their names have been changed.</p><p>Celine</p><p>Circumstance brought Celine and me together. We were both new freshmen at an international school in Nairobi, and our shared fear proved the perfect BFF elixir. Celine was sweet and reserved, with a quiet confidence that I admired—even more so when I got older. But she wanted to do her own thing and I, a not-so-confident 14-year-old, wanted to fit in. I had a hunger for popularity, and when I realized that Celine didn’t share that, I neglected the friendship. Soon, it evaporated.</p><p>Celine remembered things similarly.</p><p>“Once school started, we new kids were initially welcomed into the group of ‘misfits’ that every high school has,” she wrote to me via Facebook. “But eventually, we broke away—you, to join the funny kids, a group of hilarious and friendly people who could match your unparalleled wit and high-octane energy and me, to join the kids at the back of the bus, literally and figuratively.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>I thought that by the end of high school, we were strangers. But Celine reminded me that we had our special moments, and that there was always love between the two of us. “By senior year, we were moving in completely different circles, even in our somewhat tiny school. But, to my mind, there wasn’t an acrimonious end to our friendship, and we could always share a funny moment here and there. We just evolved in different directions,” she wrote.</p><p>Celine’s kindness surprised me—the pain of friendship breakups past had colored the way I thought about <em>all</em> of my former chums. I forgot that people can grow apart and still love each other from afar. Celine lives in Europe, and the chances of us revitalizing our former bond are slim. But I feel a sense of peace, knowing that we’ll always be rooting for each other.</p><p>Steve</p><p>My cousin Steve and I have always had a love-hate relationship. I despised him when we were young—during one squabble, I was so consumed with fury that I took our shared Game Boy Advance and threw it down the stairs, destroying it—but, peppered throughout my memories of us going toe-to-toe are flashes of roaring laughter. The more joyful side of our relationship really developed when I started at the international school. He had already been a student there for a few years, and to my surprise, he took me under his wing. Those were some of the best years of my life—we partied (arguably too much), we cried, we learned. We were free. And when my boyfriend died in a tragic accident during our senior year, Steve became my fiercest protector. He allowed me to grieve, however I chose to. When he held my hand it felt like he’d never let go.</p><p>But he did, because he had to. It was time to go to college, and for us to have our own adventures. I tried to stay connected to him, but it didn’t seem like he was interested in pursuing an adult friendship. Texts would go unanswered, calls missed, and after a while, my bond with Steve felt as lost as my youth.</p><p>When I first floated the idea of this article to Steve, he didn’t think our relationship qualified. “We’re family,” he explained on WhatsApp. “And the thing about family is, relationships can wax and wane and friends drift apart, but, you know, families still have to come back together.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>That is true. Steve and I are bonded by blood, so he’ll always be a part of my life in some capacity. But being family is not synonymous with being friends, and I think the understanding that we’re irrevocably tied may be part of why we aren’t close anymore. Why try, when I know I’ll see you at some cousin’s wedding or brother’s baby shower?</p><p>So, we had another conversation. Steve was hesitant—it took weeks for him to get back to me—and he didn’t say much, but he did change the way I looked at our estrangement.</p><p>He described what he considered a defining moment in our friendship’s collapse. In 2017, Steve and I both found ourselves living at home in Kenya, depressed and unemployed. Our college years were tough, and we needed a break to figure things out. Looking back, I remember my own pain being my top priority. I barely noticed that Steve was struggling too, and it goes without saying that I wasn’t there for him. We were in Kenya for months, but only spent one, disastrous night together, when we barely spoke. (I, for one, was too busy making out with my cousin’s friend.)</p><p>“I don’t know if you remember that night, but I remember I took you home and spent all night talking to you and consoling you,” Steve told me. He went on to explain that at the time, we were in exactly the same place emotionally, but we weren’t there for each other. “We were both Kenyan-Americans who had this lovely upbringing, and we both faced trials and adversities when it comes to the United States,” he recounted. “We almost had to come back home to recuperate, and to find some sort of moral guide. We were going through something so similar and there were so many anecdotes and so much support that we could have given to one another, but we really didn't.”</p><p>If anything, Steve found our relationship one-sided, feeling that the support he showed me was never reciprocated. But that, I argued, wasn’t <em>entirely</em> fair. I’ve tried to be there for Steve over the years, but he’s evasive and holds his emotions very close to his chest. How can I show up for someone who doesn’t let me in?</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>This, he could understand. “I’m usually the one who’s more distant,” he admitted.</p><p>Yet Steve asked an interesting question toward the end of the conversation—one that I can’t stop thinking about. “Can we boil down our lack of a relationship to a series of instances?” He asked. “Or is it more that at some point, neither of us felt lonely enough to put in the work to maintain the relationship?”</p><p>Our conversation ended with Steve suggesting that closeness comes solely from in-person interactions. Because he lives stateside and I’m in Kenya, the likelihood of us having that time is slim to none. I don’t know where my relationship with Steve stands now, but I do know that I feel defeated and misunderstood by him. Maybe that will change one day, but if not, I’ll just have to be content with the friendship we had—or, the one <em>I</em> thought we had, anyway.</p><p>Matt</p><p>Matt, I met in college. He was a year older, and worked behind the front desk of my dorm. It wasn’t long before his polite smiles as I entered the building graduated to conversations about classes, crushes, and Greek life. Soon after that, we became proper friends.</p><p>Matt was the first person who made college feel like home to me. He made the <em>US</em> feel like home. I hadn’t lived stateside in years, and to my surprise, I was out of the loop with American culture. I often felt out of place—except, that is, when I was with Matt. A white Texan-Californian whose family runs a 5K every Thanksgiving, he was, to my surprise, made of everything I was made of.</p><p>It’s possible that even then, our connection wasn’t the healthiest. I remember being jealous of his other relationships, particularly with our mutual friend Madison. As they grew closer, I felt left out, and like I had to fight for his love and attention. I sensed that Matt knew what was happening, and that he didn’t like what he saw.</p><p>Years passed, and Matt and I remained close, even after we both graduated from college. But then, he decided to move back to Texas.</p><p>I don’t know why Matt and I didn’t try harder to stay in touch. I wanted to visit him, but my minimum-wage salary was not going to cover travel costs. He seemed to have little interest in texts or FaceTimes, but I would still try to reach out every now and again to see how he was doing. He was nice enough during those virtual interactions, but it was clear he had moved on. I found myself wondering if our relationship meant more to me than it did to him.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Those insecurities came to a head a few years ago. My boyfriend and I were in a really bad place, and I found myself alone and devastated. I needed a friend, so I reached out to Matt—and boy, did I trauma dump on him. He was kind and listened patiently, but I didn’t hear from him for months after that. Then, when he finally resurfaced, he leveled with me, explaining that what I had gone through was a lot to be confronted with, especially after we hadn’t spoken in a while. As a somewhat overly emotional person with deep abandonment issues, that was all I needed to hear. I got it, but I was crushed. I’m still crushed.</p><p>“I think you’re really hard on yourself,” Matt said in a voice message recently, when I rehashed all of this. It wasn’t that I had driven him away, he urged, but that he (like Steve) had a different communication style, and was more reticent. “I’m hyper-focused on whatever environment I’m in at the moment, and I know that seems really annoying to say, but that hyper-awareness stops me from reaching out to people,” he continued. “l think about you every single day—like, you are one of my best friends in my entire life—but I’m so bad at reminding you and other people I love of that.”</p><p>As I wiped away tears, Matt went on to open up about how he’s changed over the years, and how it’s shifted the way he looks at the Boyfriend Incident. He explained that back then, he’d thought that relationships were simple: If you and your beau weren’t getting along, you should leave him. That mentality affected the way that he responded to my woes. Besides, he’d been going through troubles of his own. “I don’t think either of us were in a good place,” he confessed.</p><p>Matt said a lot of wonderful things about me and our friendship during our conversation, but one thing meant the most. “In my mid 20s, I was really selfish,” he said. “But I’m currently at a point where I don’t really care about things for myself. Now that I’m almost 30, my loved ones and my friendship are all that really matter.”</p><p>I was so inspired by Matt’s introspection. Not only did it give me hope for our future as friends, but it also felt like proof that these conversations, however hard and emotional, were worth it.</p><p>Dominique</p><p>When I first met Dominique, I was sure we would be friends forever. It was sorority rush, and amid the sea of women I spoke to that hellish week, Dominique stood out. That wasn’t only because we were two of the handful of Black women participating in the Greek process; Dominique was also fabulous and accessible, she was effortlessly warm and hilarious, and she had a glow that reminded you not to take life too seriously.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>I learned to love everything about Dominique—not just her star quality, but also her vulnerability, her darkness. She quickly became my most treasured friend. There wasn’t anything we couldn’t do or talk about. We could party together, we could drink wine at home together, we could cry together, we could gently call each other out. We could save each other.</p><p>I didn’t want to do anything without her by my side.</p><p>I can’t pinpoint when things changed in our friendship. I had to leave college for a semester due to medical issues, and during that period I was disengaged from everyone close to me. It cost me a lot of friends, including Dominique, to an extent. When I returned, she was distant. She had graduated and was moving on from our college life, yes, but the rift felt deeper than that. She wasn’t there when I needed her most, but I also wasn’t telling her what I needed.</p><p>I held onto that resentment, and Dominique and I continued to grow apart. She found herself in a dangerously toxic relationship, and instead of helping her, I just worried from afar.</p><p>Out of all of these daunting conversations with my former friends, I was most nervous to talk to Dominique. I knew I’d failed her as a friend, and I wasn’t sure if I was prepared for her to not-so-gently call me out on it.</p><p>Yet she did the opposite. She couldn’t have been kinder or more gracious about what happened between us. “I am in my maturity now, [and] I have come to an understanding and a realization that I am not a person that’s good at maintaining friendships,” she confessed to me. I was shocked. Beautiful, brilliant, lovely Dominique, not good with friendships? 2015 me wouldn’t have believed it.</p><p>It turns out that Dominique felt the same way I did. She’d thought I’d shut her out, and instead of talking to me about it, she’d taken a step back. <em>Maybe we’re not as close as I think we are</em>, she’d mused, adding that she felt “out of the loop” when I was struggling with my health. She’d become “comfortable with the idea [that] there were other people that were closer to you than me.” All the while, I’d thought that <em>I</em> wasn’t as important to her as she was to me. We both agreed that nothing concrete had happened; neglected feelings had just led us to stop communicating.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>I can’t believe Dominique and I waited so many years to have this conversation. I’d harbored so much guilt, confusion, and pain over our friendship. It had haunted me, and played a big role in the way I saw myself as a friend. And all this time, Dominique had thought the same of herself.</p><p>I don’t know if Dominique and I will ever be friends like we used to be, but the olive branch has been extended. And, for the first time, I feel hopeful.</p><p>At first, my motivation for talking to my former friends about why we fell out of touch was a little masochistic. I thought I was a bad friend, and my loneliness was a product of my own self-centeredness, my stubbornness, my tendency to either vent or withhold. I thought I deserved to be punished by the people I’d wronged.</p><p>I’m not walking away from these conversations with the conviction that I’m a <em>good</em> friend, or even a good person. However, talking with my ex-friends did remind me that loving people—even platonically—isn’t easy. Sometimes you hurt your friends, sometimes they hurt you, and sometimes there’s no hurt at all, but they still fade away like a memory. Life is short, but it’s long, too. If you’re lucky, people will come in and out of your life and, for however long they’re there, you’ll feel loved.</p><p>So, should I tackle my ex-boyfriends next?</p></div></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I spent 6 years building a ridiculous wooden pixel display (846 pts)]]></title>
            <link>https://benholmen.com/blog/kilopixel/</link>
            <guid>44787902</guid>
            <pubDate>Mon, 04 Aug 2025 16:16:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://benholmen.com/blog/kilopixel/">https://benholmen.com/blog/kilopixel/</a>, See on <a href="https://news.ycombinator.com/item?id=44787902">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h2>TL,DR: I built the world's most impractical 1000-pixel display and anyone in the world can draw on it</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/4OUF7sfAuHA?si=f_ypc5pkT7tevWDZ&amp;controls=0&amp;modestbranding=1&amp;rel=0&amp;showinfo=0&amp;autoplay=1&amp;mute=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>If you just want to play with it, goto <a href="https://kilopx.com/?ref=blog">kilopx.com</a>.</p>

<h3>The backstory</h3>

<p>Six years ago I had an idea to build a large, inefficient display with a web interface that anyone could interact with. I've enjoyed <a href="https://en.wikipedia.org/wiki/Danny_Rozin">Danny Rozin's unconvenional mirrors</a> over the years and was inspired by an <a href="https://github.com/TomWhitwell/SlowMovie">eInk movie player that played at 24 frames per <em>hour</em></a> that got me thinking about a laborious display that could slowly assemble an image.</p>

<p>I landed on the idea of a 40×25 pixel grid of pixels, turned one by one by a single mechanism. Compared to our modern displays with millions of pixels changing 60 times a second, a wooden display that changes a single pixel 10 times a <em>minute</em> is an incredibly inefficient way to create an image. Conveniently, 40×25 = 1,000 pixels, leading to the name <em>Kilopixel</em> and the six-letter domain name <a href="https://kilopx.com/?ref=blog">kilopx.com</a>. How do you back down from that? That's the best domain name I've ever owned.</p>

<p>So I got to work. This project has everything: a web app, a physical controller, a custom CNC build, generated gcode, tons of fabrication, 3d modeling, 3d printing, material sourcing - so much to get lost in. It's the most ambitious project I've ever built.</p>

<h3>The first prototype: 21×3 pixels</h3>

<p>My first thought was to use a wooden gantry that would ride on some sort of track. Since I'm most comfortable working with wood, it's my default prototyping medium. However, I quickly pivoted to extruded aluminum and the excellent <a href="https://openbuildspartstore.com/v-slot-linear-rail-1/">hardware kits from Openbuilds</a> that include pulleys, gantry parts, extruded aluminum, and timing belts. It's very similar to the materials used in 3D printer frames, and connects very easily with off the shelf stepper motors. This allowed me to build a gantry with X and Y, essentially a wall-mounted XY plotter. I built the first prototype with two stepper motors, a Raspberry Pi, a CNC controller, and a beefy power supply. It allowed me to generating and sending instructions to the CNC controller to move to a particular pixel, turning that pixel, and reading values from sensors. It also revealed quite a few problems with my pixel choices and pixel manipulation mechanisms.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/prototype-1-1.jpg" alt="The first prototype" width="1440" height="1440" loading="lazy"></p>

<h3>1,000 of anything is <em>expensive</em></h3>

<p>Picking pixels was a real adventure. I've tried ping pong balls, styrofoam balls, bouncy balls, wooden balls, 3d printed balls, golf balls, foam balls...anything approximately spherical and about 1-1.5in in diameter. The problems I encountered were largely cost (even a 50¢ ball is $500 of balls), weight (again, a thousand of these things), and availability. For a long time I thought ping pong balls were my best bet, so I purchased a few hundred of them, 3d printed painting jigs, and spray painted them. I used a hot nail to melt two opposite holes on each ball so they could be strung up on the display.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/ping-pong-pixels-1.jpg" alt="Painting ping pong pixels" width="1080" height="1080" loading="lazy">
<img src="https://benholmen.com/assets/images/kilopixel/ping-pong-pixels-2.jpg" alt="Painting ping pong pixels" width="1080" height="1080" loading="lazy">
</p>

<h3>Ping pong balls are basically soda cans</h3>

<p>You can stand on a soda can, <em>as long as it's not open</em>. Open the can, and it crushes easily. Ping pong balls are the same way. They're relatively strong until you melt two holes in them. Then they can be deformed, which is fatal to any spray paint you've put on them. And not only are they fragile, but the cheap ones are inconsistently sized, and a half millimeter here and there adds up when you have a row of 40 balls. Ping pong balls were a no-go.</p>

<h3>Nerfed</h3>

<p>My next attempt at a cheap, spherical pixel was foam Nerf balls - much smaller than ping pong balls, and only available in bright colors. They accepted spray paint OK but the paint deteriorated over time.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/prototype-2-pixel-1.jpg" alt="Nerf balls covered in black paint" width="1440" height="1920" loading="lazy">
<img src="https://benholmen.com/assets/images/kilopixel/prototype-2-pixel-2.jpg" alt="Nerf balls half painted in black paint" width="1440" height="1920" loading="lazy">
<img src="https://benholmen.com/assets/images/kilopixel/prototype-2.jpg" alt="The second prototype" width="1440" height="1080" loading="lazy">
</p>

<p>It was difficult to consistently bore a hole through the nerf balls, and they really liked to grab the wire and were hard to turn. I struggled to consistently turn them and I wasn't thrilled with the bright colors.</p>

<p>I also tried bouncy balls (hard to drill a hole, hard to paint, inconsistent sizes, heavy), wooden balls (not very round, hard to paint a crisp line, heavy), and styrofoam balls (hard to paint with acrylic paint, and they melt with spray paint).</p>

<h3>Turning balls</h3>

<p>I had the idea to use a small, slow motor to rotate a LEGO wheel against the ping pong ball. I'd use a reflectivity sensor to detect if it was showing black or white, and stop once the pixel was rotated properly. I modeled and printed a custom hub for a LEGO wheel, a few different mechanisms to move the wheel in and out of contact of the sphere, and an interface for the gantry. I tried using a solenoid to push the motor into the ball, which was underpowered, and a servo. Neither approach worked great and ultimately I decided this ball turning approach was a dead end.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/gantry-with-servo.png" alt="Gantry mechanism with servo" width="906" height="970" loading="lazy">
<img src="https://benholmen.com/assets/images/kilopixel/lego-wheel-hub.png" alt="LEGO wheel hub" width="906" height="970" loading="lazy">
<img src="https://benholmen.com/assets/images/kilopixel/prototype-2-wheel.jpg" alt="Wheel turning mechanism" width="1440" height="1080" loading="lazy">
</p>

<h3>Pivoting to non-spherical pixels</h3>

<p>In 2024 I had a couple of productive conversations with <a href="https://sideprojectpodcast.com/episodes/kilopixel-with-ben-holmen">Joe Tannenbaum on the Side Project podcast</a> and <a href="https://overengineered.fm/episodes/the-art-of-pairing-with-strangers-w-ben-holmen">Chris Morrell on the Over Engineered podcast</a>. Those conversations helped me consider that maybe balls were not the way to go - I thought about flaps and illuminated buttons, then settled on a cubic wooden pixel. I also decided to manufacture the pixels myself because I'm very comfortable in my wood shop. This decision cost me a huge amount of time because doing things one thousand times takes <em>forever</em>, but I was really pleased with how it operated and looked.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/painting-timelapse.gif" alt="Painting a thousand pixels timelapse" width="480" height="270" loading="lazy">
<img src="https://benholmen.com/assets/images/kilopixel/finished-pixel.jpg" alt="Finished pixel" width="1440" height="1080" loading="lazy">
<img src="https://benholmen.com/assets/images/kilopixel/many-pixels.jpg" alt="Hundreds of pixels" width="1440" height="1080" loading="lazy">
</p>

<h3>Building the grid</h3>

<p>I'd learned from earlier prototypes that I needed to strictly define a grid and not depend on the pixels themselves for spacing. That 40mm pixel might be 39.5mm, or 41mm. And that variation adds up across 40 pixels - you might be 10mm off by the end of the row. So for my (hopefully final) build I created 25 thin shelves, drilled 40 holes in each one using a jig to enforce consistent spacing, and threaded pixels on 40 metal wires. This was painstaking and time consuming - I broke it down into multiple sessions over several weeks. But it did create a very predictable grid of pixels and guaranteed that each pixel moved completely independently of the surrounding pixels.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/pixel-assembly.jpg" alt="Dozens of pixels being assembled" width="1440" height="1080" loading="lazy">
<img src="https://benholmen.com/assets/images/kilopixel/assembling-timelapse.gif" alt="Assembling the pixels timelapse" width="480" height="270" loading="lazy">
</p>

<p>Finally, I had my first thousand-pixel display and it seemed promising! I could stop here and have some interesting wall art - and it feels amazing to swipe across with your hand. But we're not stopping! In Wisconsin, we say <em>Forward!</em></p>

<h3>A CNC machine in my office</h3>

<p>I've used a hobby CNC machine in my wood shop for many years, so I was familiar with the basics of CNC and the possibilities for this project. Generally speaking, a CNC machine is something that takes very specific movement instructions written in a language called gcode, and uses those instructions to move to a certain position and do something like drill a hole, cut a groove, or burn with a laser. Stepper motors are typically used because they move very predictably when they receive electrical signals from a CNC controller. Common hobby CNC machines include laser engravers (Glowforge), milling machines (X-Carve), and 3D printers. They all use movement instructions to move X, Y, and Z axes very precisely and do things at those coordinates.</p>

<p>It's easy to find a basic CNC controller that can be used for a CNC mill, a laser engraver, or plotter. These CNC controllers accept gcode over USB/serial, and turn stepper motors to put the machine in the correct position. They typically run <a href="https://github.com/gnea/grbl">grbl</a>, an open source gcode parser that runs on Arduinos.</p>

<p>The Kilopixel is essentially a 2-axis machine that uses the third axis for the pixel poking mechanism.</p>

<p>I connected a Raspberry Pi to the CNC controller and use it for two purposes: querying my API to get the next pixel, writing the appropriate gcode to get there, activating the pixel poker, and then reading a light sensor to determine the physical state of the pixel. It then returns that state to the API and continues the loop. This is run with a Python script and depends on <a href="https://github.com/joan2937/pigpio">pigpio</a> to read the light sensor over GPIO pins.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/completed-display.jpg" alt="Completed Kilopixel display" width="1440" height="1117" loading="lazy"></p>

<h3>Poking pixels</h3>

<p>The pixels rotate and have a notch that registers every 90° to encourage them to align properly. To turn them, I created a reciprocating poking mechanism that uses a flexible glue stick to push on the edge of the pixel. As the pixel turns, the poker moves to the right and lifts up slightly, then moves out of the way and retracts. This is all controlled by gcode and is a rather finicky part of the whole machine.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/poker-1.gif" alt="Prototype stepper motor poker" loading="lazy" width="480" height="270">
<img src="https://benholmen.com/assets/images/kilopixel/poker-2.gif" alt="Pixel poker poking the pixels" loading="lazy" width="480" height="480">
</p>

<h3>We're changing pixels. What should we draw?</h3>

<p>At this point, I have a thousand pixel display that listens to an API and changes pixels one-by-one. So what does the API say?</p>

<p>The API is controlled by a <a href="https://kilopx.com/?ref=blog">web app</a> that is the source of truth for what should be on the display. It has a few modes:</p>

<ul>
<li>User-submitted: anyone can submit a 40×25 image to be drawn, and the most popular submission will be drawn next. Loop forever.</li>
<li>Real-time collaboration: there's a single picture being drawn, and anyone can change any pixel in real time. This doesn't work great with many participants, but is a solid choice if I install the Kilopixel in a coffee shop or something.</li>
<li>Idle modes: I wrote a few algorithms to generate shapes and patterns, but my favorite mode is a clock that can barely keep up with drawing itself.</li>
</ul>

<p>For the public launch of the Kilopixel I chose the user-submitted mode, and you can <a href="https://kilopx.com/draw?ref=blog">submit your own right now</a>. Or <a href="https://kilopx.com/submissions?ref=blog">vote for the submissions</a> you want to see drawn next.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/kilopx.com-submissions.png" alt="Screenshot of kilopx.com showing user submissions" loading="lazy" width="1496" height="1340">
<img src="https://benholmen.com/assets/images/kilopixel/kilopx.com-draw.png" alt="Screenshot of kilopx.com showing user submissions" loading="lazy" width="1496" height="1340">
</p>

<p>I tinkered with a few stacks for the web app over the years, using it as an excuse to try new things. At first it was a node/Socket.IO app, then a Laravel + Livewire app, and finally a Laravel + InertiaJS + VueJS app. It's hosted on a modest DigitalOcean VPS. It also runs locally on my laptop to record and upload video.</p>

<h3>Putting it out there</h3>

<p>Since the inception of the project, I really wanted this to be something that I shared with at least a few people. It's neat to have in my office, but if it was just for my own enjoyment, it wouldn't be worth all this effort.</p>

<p>I originally planned to hang this in my friend's coffee shop and let a few people at a time interact with it. I still love this idea! And I might do it.</p>

<p>But what I'm really excited about it putting this on the internet for <em>everyone</em> and that means recording and streaming the display in my office. Here's the setup:</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/streaming-setup.jpg" alt="" width="1440" height="930" loading="lazy"></p>

<p>There are two webcams involved: one mounted directly on the pixel poker for a closeup, and one wide shot. The two cameras are combined in OBS where I can stream to YouTube, and the wide shot is also recorded continuously using ffmpeg. Streaming to YouTube provides a live view of the physical device alongside the digital queue of submissions. The camera, USB hub, and light are hung from the ceiling with a respectful amount of jank for the streaming phase of this project.</p>

<p>Besides streaming, the laptop is running a scheduled job that queries the API to see if a submission has recently finished drawing. If it does, it generates a rather complex ffmpeg command to generate a one minute timelapse of the submission being drawn. The timelapse is uploaded to kilopx.com and posted to Bluesky where it can be shared by the creator of the artwork - <a href="https://bsky.app/profile/kilopx.com/post/3lutmwu7kls2v">for example, pixel art by Matt Stauffer</a></p>

<h3>Something physical, in my office, controlled by the internet. What could go wrong?</h3>

<p>I've built some defensive features into the web app so I can mitigate common abuse patterns if they become a problem. I've decided to not lock it down prematurely - I think it might be fun to see what people can do with this thing! Voting is open to anyone with a few basic session checks, submission of artwork requires a Bluesky OAuth login, and I have a mechanism to quickly delete problem submissions.</p>

<p>I'll see what the internet does and adapt accordingly!</p>

<h3>What next?</h3>

<p>I'm sincerely hoping the internet has fun with this project for a bit! Once it winds down, I've considered turning control of the display over to an internet friend - after all, it just hits an API, why not yours? If you're interested, <a href="https://benholmen.com/cdn-cgi/l/email-protection#ceacaba08eacaba0a6a1a2a3aba0e0ada1a3">email me</a>.</p>

<p>And then, the final destination will be behind me on my webcam - I'll let anyone on a video call monkey with my background to their heart's content. What could go wrong?</p>

<p>In the meantime, please <a href="https://kilopx.com/draw?ref=blog">submit something</a> or just <a href="https://kilopx.com/?ref=blog">follow along</a>!</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla withheld data, lied, misdirected police to avoid blame in Autopilot crash (459 pts)]]></title>
            <link>https://electrek.co/2025/08/04/tesla-withheld-data-lied-misdirected-police-plaintiffs-avoid-blame-autopilot-crash/</link>
            <guid>44787780</guid>
            <pubDate>Mon, 04 Aug 2025 16:07:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2025/08/04/tesla-withheld-data-lied-misdirected-police-plaintiffs-avoid-blame-autopilot-crash/">https://electrek.co/2025/08/04/tesla-withheld-data-lied-misdirected-police-plaintiffs-avoid-blame-autopilot-crash/</a>, See on <a href="https://news.ycombinator.com/item?id=44787780">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1600" height="841" src="https://electrek.co/wp-content/uploads/sites/3/2025/08/Tesla-Autopilot-crash-data-lies-Elon-Musk.png?w=1600" alt="" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/08/Tesla-Autopilot-crash-data-lies-Elon-Musk.png?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/08/Tesla-Autopilot-crash-data-lies-Elon-Musk.png?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/08/Tesla-Autopilot-crash-data-lies-Elon-Musk.png?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/08/Tesla-Autopilot-crash-data-lies-Elon-Musk.png?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high"></figure>

<p>Tesla was caught withholding data, lying about it, and misdirecting authorities in the wrongful death case involving Autopilot that it lost this week.</p>



<p>The automaker was undeniably covering up for Autopilot.</p>



<p>Last week, a jury found Tesla partially liable for a wrongful death involving a crash on Autopilot. I explained the case in the verdict in <a href="https://electrek.co/2025/08/01/tesla-tsla-is-found-liable-in-fatal-autopilot-crash-has-to-pay-329-million/" target="_blank" rel="noreferrer noopener">this article </a>and <a href="https://www.youtube.com/watch?v=fzV1i7LeRrw">video</a>.</p>



<p>But we now have access to the trial transcripts, which confirm that Tesla was extremely misleading in its attempt to place all the blame on the driver.</p>	
	



<p>The company went as far as to actively withhold critical evidence that explained Autopilot’s performance around the crash.</p>



<h3 id="h-tesla-withheld-the-crash-snapshot-data-that-its-own-server-received-within-minutes-of-the-collision">Tesla <strong>withheld the crash‑snapshot data that its own server received within minutes of the collision</strong></h3>



<p>Within about three minutes of the crash, the Model S uploaded a “collision snapshot”—video, CAN‑bus streams, EDR data, etc.—to Tesla’s servers, the “Mothership”, and received an acknowledgement. The vehicle then deleted its local copy, resulting in Tesla being the only entity having access.</p>



<p>What ensued were years of battle to get Tesla to acknowledge that this collision snapshot exists and is relevant to the case.</p>



<p>The police repeatedly attempted to obtain the data from the collision snapshot, but Tesla led the authorities and the plaintiffs on a lengthy journey of deception and misdirection that spanned years.</p>



<p>Here, in chronological order, is what happened based on all the evidence in the trial transcript:</p>



<h3><strong>1 | 25 Apr 2019 – The crash and an instant upload Tesla pretended never happened</strong></h3>



<p><span>Within ~3 minutes of the cras</span>h, the Model S packaged sensor video, CAN‑bus, EDR, and other streams into a single&nbsp;“snapshot_collision_airbag-deployment.tar”&nbsp;file and pushed it to Tesla’s server, then deleted its local copy.</p>



<p>We know that now, thanks to forensic evidence extracted from the onboard computer.</p>



<p>The plaintiffs hired Alan Moore, a mechanical engineer who specializes in accident reconstruction, to forensically recover data from the Autopilot ECU (computer).</p>



<p>Based on the data, Moore was able to confirm that Tesla had this “collision snapshot” all along, but “unlinked” it from the vehicle:</p>



<blockquote>
<p><em>“That tells me within minutes of this crash Tesla had all of this data … the car received an acknowledgement … then said ‘OK, I’m done, I’m going to unlink it.’”</em></p>
</blockquote>



<p>The plaintiffs tried to obtain this data, but Tesla told them that it didn’t exist.</p>



<p>Tesla’s written discovery responses were shown during the trial to prove that the company acted as if this data were not available.</p>



<hr>



<h3><strong>2 | 23 May 2019 – Tesla’s lawyer scripts the homicide investigator’s evidence request</strong></h3>



<p>Corporal Riso, a homicide investigator with the Florida Highway Patrol (FHP), sought Tesla’s help in retrieving telemetry data to aid in reconstructing the crash.</p>



<p>He was put in contact with Tesla attorney Ryan McCarthy and asked if he needed to subpoena Tesla to get the crash data.</p>



<p>Riso said of McCarthy during the trial:</p>



<blockquote>
<p><em>“He said it’s not necessary. <strong>‘Write me a letter and I’ll tell you what to put in the letter.’</strong>”</em></p>
</blockquote>



<p>At the time, he didn’t see Tesla as an adversary in this case and thought that McCarthy would facilitate the retrieval of the data without having to go through a formal process. However, the lawyer crafted the letter to avoid sending the police the full crash data.</p>



<p>Riso followed the instructions verbatim. He said during the trial:</p>



<blockquote>
<p>“I specifically wrote down what the attorney at Tesla told me to write down in the letter.”</p>
</blockquote>



<p>But McCarthy specifically crafted the letter to ommit sharing the colllision snapshot, which includes bundled video, EDR, CAN bus, and Autopilot data.</p>



<p>Instead, Tesla provided the police with infotainment data with call logs, a copy of the Owner’s Manual, but not the actual crash telemetry from the Autopilot ECU.</p>



<p>Tesla never said that it already had this data for more than a month by now.</p>



<hr>



<h3><strong>3 | June 2019 – A staged “co‑operation” that corrupts evidence</strong></h3>



<p>Tesla got even more deceptice when the police specifically tried to collect the data directly from the Autopilot computer.</p>



<p>On June 19, 2019, Riso physically removed the MCU and Autopilot ECU from the Tesla.</p>



<p>Again, the investigator thought that Tesla was being collaborative with the investigation at the time so he asked the company how to get the data out of the computer. He said at the trial:</p>



<blockquote>
<p>I had contacted Mr. McCarthy and asked him how I can get this data off of the computer components. He said that he would coordinate me meeting with a technician at their service center, the Tesla service center in Coral Gables.</p>
</blockquote>



<p>Tesla arranged for Riso to meet Michael Calafell, a Tesla technician, at the local service center in in Coral Gables with the Autopilot ECU and the Model S’ MCU, the two main onboard computers.</p>



<p>To be clear, Tesla already had all this data in its servers and could have just sent it to Riso, but instead, they lured him into its service center with the piece of evidence in his custody.</p>



<p>What ensued was pure cinema.</p>



<p>Michael Calafell, who testified never having been tasked with extracting data from an Autopilot ECU before, connected both computers to a Model S in the shop to be able to access them, but he then claimed that the data was “corrupted” and couldn’t be access.</p>



<p>Riso said during his testimony:</p>



<blockquote>
<p><strong>I brought the center tablet [MCU] and the flat silver box [Autopilot ECU] with multicolored connectors to the Tesla service center.”</strong></p>
</blockquote>



<blockquote>
<p><strong>“I watched Mr. Calafell the whole time. The evidence was in my custody. I did not let it out of my sight.”</strong></p>
</blockquote>



<p>However, the situation got a lot more confusing as Calafell swore in an affidavit that he didn’t actually power the ECU, only the MCU, on that day, June 19.</p>



<p>Only years later, when Alan Moore, the forensic engineer hired by the plaintiff, managed to get access to the Autopilot ECU, we learned that Tesla undeniably powered up the computer on June 19 and the data was accessible.</p>



<hr>



<h3><strong>4 | 2019 – 2024 – Repeated denials and discovery stonewalling</strong></h3>



<p>Through years of communications with the police, the plaintiffs and the court through the investigation and later the discovery process for the lawsuit, Tesla never mentioned that it had all the data that explained how Autopilot saw the crash, which everyone was seeking, sitting on its servers for years.</p>



<p>The facts are:</p>



<ul>
<li>Tesla had the data on its servers within minutes of the crash</li>



<li>When the police sought the data, Tesla redirected them toward other data</li>



<li>When the police sought Tesla’s help in extracting it from the computer, Tesla falsely claimed it was “corrupted”</li>



<li>Tesla invented an “auto-delete” feature that didn’t exist to try explain why it couldn’t originally find the data in the computer</li>



<li>When the plaintiffs asked for the data, Tesla said that it didn’t exist</li>



<li>Tesla only admitted to the existence of the data once presented with forensic evidence that it was created and transfered to its servers.</li>
</ul>



<hr>



<h3 id="h-5-late-2024-court-orders-a-bit-for-bit-nand-flash-image"><strong>5 | Late 2024 – Court orders a bit‑for‑bit NAND‑flash image</strong></h3>



<p>By late 2024, the court allowed the plantiffs to have a third-party expert access the Autopilot ECU to try to acccess the data that Tesla claimed was now corrupted.</p>



<p>The court allowed the forensic engineers to do a bit-for-bit NAND flash image, which consists of a complete, sector-by-sector copy of the data stored on a NAND flash memory chip, including all data, metadata, and error correction code (ECC) information.</p>



<p>The engineers quickly found that all the data was there despite Tesla’s previous claims.</p>



<p>Moore, the forensic engineer hired by the plaintiffs, said:</p>



<blockquote>
<p>“Tesla engineers said this couldn’t be done… yet it was done by people outside Tesla.”</p>
</blockquote>



<p>Now, the plaintiffs had access to everything.</p>



<hr>



<h3><strong>6 | Feb‑Mar 2025 – The forensic “treasure‑trove” reveals the file name &amp; checksum</strong></h3>



<p>Moore was astonished by all the data found through cloning the Autopilot ECU:</p>



<blockquote>
<p>“For an engineer like me, the data out of those computers was a treasure‑trove of how this crash happened.”</p>
</blockquote>



<p>The data that Tesla had provided was not as easily searchable, the videos were grainy, and it was missing key alerts and timestamps about Autopilot and its decision-making leading up to the crash.</p>



<p>On top of all the data being so much more helpful, Moore found unallocated space and metadata for ‘snapshot_collision_airbag‑deployment.tar’, including its SHA‑1 checksum and the exact server path.</p>



<hr>



<h3><strong>7 | May 2025 – Subpoenaed server logs corner Tesla</strong></h3>



<p>Armed with the the newly found metadata, plaintiffs were able to subpoenaed Tesla’s AWS logs. </p>



<p>Tesla still fought them, but facing a sanctions hearing, Tesla finally produced the untouched TAR file plus access logs showing it had been stored <strong>since 18:16 PDT on 25 Apr 2019</strong>—the same three‑minute timestamp Moore had highlighted.</p>



<p>The automaker had to admit to have the data all along.</p>



<p>During the trial, Mr. Schreiber, attorney for the plaintiffs, claimed that Tesla used the data for its own internal analysis of the crash:</p>



<blockquote>
<p>They not only had the snapshot — they used it in their own analysis. It shows Autopilot was engaged. It shows the acceleration and speed. It shows McGhee’s hands off the wheel.</p>
</blockquote>



<p>Yet, it didn’t give access to the police nor the family of the victim who have been trying to understand what happened to their daughter.</p>



<hr>



<h3><strong>8 | July 2025 Trial – The puzzle laid bare for the jury</strong></h3>



<p>Finally, this entire situation was laid bare in front of the jury last month and certainly influenced the jury in their verdict.</p>



<p>The jury was confronted with clear evidence of Tesla trying to hide data about the crash, and then, they were shown what that data revealed.</p>



<p>The data recovered made a few things clear:</p>



<ul>
<li>Autopilot was active</li>



<li>Autosteer was controlling the vehicle</li>



<li>No manual braking or steering override was detected from the driver</li>



<li>There was <strong>no record of a “Take Over Immediately” alert</strong>, despite approaching a T-intersection with a stationary vehicle in its path.</li>



<li>Moore found logs showing <strong>Tesla systems were capable of issuing such warnings</strong>, but <strong>did not</strong> in this case.</li>



<li>Map and vision data from the ECU revealed:
<ul>
<li>Map data from the Autopilot ECU included a flag that the area was a <strong>“restricted Autosteer zone.”</strong></li>



<li>Despite this, the system <strong>allowed Autopilot to remain engaged</strong> at full speed.</li>
</ul>
</li>
</ul>



<p>Moore commented on the last point:</p>



<blockquote>
<p>“Tesla had the map flag. The car knew it was in a restricted zone, yet Autopilot did not disengage or issue a warning.”</p>
</blockquote>



<p>This was critical to the case as one of the arguments was that Tesla dangerously let owners use Autopilot on roads it was not designed to operate on as it was specifically trained for highways.</p>



<p>The National Transportation Safety Board (NTSB) had even worn Tesla about it and the automaker didn’t geofenced the system:.</p>



<p>The NTSB had wrote Tesla:</p>



<blockquote>
<p>“Incorporate system safeguards that limit the use of automated vehicle control systems to those conditions for which they were designed (the vehicle’s operational design domain).”</p>
</blockquote>



<p>The driver was responsible for the crash and he admitted as such. He admitted to not using Autopilot properly and not paying attention during the crash.</p>



<p>However, the main goal of the plaintiffs in this case was to assign part of the blame for the crash to Tesla for not preventing such abuse of the system despite the clear risk.</p>



<p>The logic is that if Tesla had implemted geofencing and better driver monitoring, the driver, McGee, would have never been able to use Autopilot in this case, which could have potentially avoidded putting himself in the situation that led to the crash.</p>



<p>That’s on top of Autopilot failing at what Tesla has repeatedly claim it could do: stop those crashes from happening in the first place.</p>



<h2 id="h-electrek-s-take">Electrek’s Take</h2>



<p>Tesla fans need to do a quick exercise in empathy right now. The way they are discussing this case, such as claiming the plaintiffs are just looking for a payout, is truly appalling.</p>



<p>You should put yourself in the family’s shoes. If your daughter died in a car crash, you’d want to know exactly what happened, identify all contributing factors, and try to eliminate them  to give some meaning to this tragic loss and prevent it from happening to someone else.</p>



<p>It’s an entirely normal human reaction. And to make this happen in the US, you must go through the courts.</p>



<p>Secondly, Tesla fans need to do a quick exercise in humbleness. They act like they know exactly what this case is about and assume that it will “just be thrown out in appeal.”</p>



<p>The truth is that unless you read the entire transcripts and saw all the evidence, you don’t know more about it than the 12 jurors who unanimously decided to assign 33% of the blame for the crash to Tesla.</p>



<p>And that’s the core of the issue here. They want to put all the blame on the driver, and what the plaintiffs were trying to do was just assign part of the blame on Tesla, and the jurors agreed.</p>



<p>The two sides are not that far off from each other. They both agreed that most of the blame goes to the driver, and even the driver appears to agree with that. He admitted to being distracted and he quickly settled with the plaintiffs.</p>




	<p>This case was only meant to explore how Tesla’s marketing and deployment of Autopilot might have contributed to the crash, and after looking at all the evidence, the jury agreed that it did.</p>



<p>There’s no doubt that the driver should bare most of the responsability and there’s no doubt that he didn’t use Autopilot properly.</p>



<p>However, there’s also no doubt that Autopilot was active, didn’t prevent the crash despite Tesla claiming it is safer than humans, and Tesla was warned to use better geo-fencing and driver monitoring to prevent abuse of the system like that.</p>



<p>I think a 33% blame in this case is more than fair.</p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<div><p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p><p><a href="https://bit.ly/450t9Bz"><img src="https://electrek.co/wp-content/uploads/sites/3/2025/07/NativeBanner_ElecktrekDisplayAds_BeaumontRev2.jpg?quality=82&amp;strip=all" alt="" width="750" height="150"></a></p></div>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Qwen-Image: Crafting with native text rendering (367 pts)]]></title>
            <link>https://qwenlm.github.io/blog/qwen-image/</link>
            <guid>44787631</guid>
            <pubDate>Mon, 04 Aug 2025 15:56:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qwenlm.github.io/blog/qwen-image/">https://qwenlm.github.io/blog/qwen-image/</a>, See on <a href="https://news.ycombinator.com/item?id=44787631">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-Image/merge3.jpg#center" width="100%"></figure><p><a href="https://github.com/QwenLM/Qwen-Image" target="_blank">GITHUB</a>
<a href="https://huggingface.co/Qwen/Qwen-Image" target="_blank">HUGGING FACE</a>
<a href="https://modelscope.cn/models/Qwen/Qwen-Image" target="_blank">MODELSCOPE</a>
<a href="https://modelscope.cn/aigc/imageGeneration?tab=advanced" target="_blank">DEMO</a>
<a href="https://discord.gg/yPEP2vHTu4" target="_blank">DISCORD</a></p><p>We are thrilled to release <strong>Qwen-Image</strong>, a 20B MMDiT image foundation model that achieves significant advances in complex text rendering and precise image editing. To try the latest model, feel free to visit <a href="https://chat.qwenlm.ai/">Qwen Chat</a> and choose “Image Generation”.</p><p>The key features include:</p><ul><li><strong>Superior Text Rendering</strong>: Qwen-Image excels at complex text rendering, including multi-line layouts, paragraph-level semantics, and fine-grained details. It supports both alphabetic languages (e.g., English) and logographic languages (e.g., Chinese) with high fidelity.</li><li><strong>Consistent Image Editing</strong>: Through our enhanced multi-task training paradigm, Qwen-Image achieves exceptional performance in preserving both semantic meaning and visual realism during editing operations.</li><li><strong>Strong Cross-Benchmark Performance</strong>: Evaluated on multiple public benchmarks, Qwen-Image consistently outperforms existing models across diverse generation and editing tasks, establishing a strong foundation model for image generation.</li></ul><h2 id="performance">Performance</h2><p>We present a comprehensive evaluation of Qwen-Image across multiple public benchmarks, including GenEval, DPG, and OneIG-Bench for general image generation, as well as GEdit, ImgEdit, and GSO for image editing. Qwen-Image achieves state-of-the-art performance on all benchmarks, demonstrating its strong capabilities in both image generation and editing. Furthermore, results on LongText-Bench, ChineseWord, and TextCraft show that it excels in text rendering—particularly in Chinese text generation—outperforming existing state-of-the-art models by a significant margin. This highlights Qwen-Image’s unique position as a leading image generation model that combines broad general capability with exceptional text rendering precision.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bench.png#center" width="100%"></figure><h2 id="demo">Demo</h2><p>One of Qwen-Image’s outstanding capabilities is its ability to achieve high-fidelity text rendering in different scenarios. Let’s take a look at the following Chinese rendering case:</p><blockquote><p>宫崎骏的动漫风格。平视角拍摄，阳光下的古街热闹非凡。一个穿着青衫、手里拿着写着“阿里云”卡片的逍遥派弟子站在中间。旁边两个小孩惊讶的看着他。左边有一家店铺挂着“云存储”的牌子，里面摆放着发光的服务器机箱，门口两个侍卫守护者。右边有两家店铺，其中一家挂着“云计算”的牌子，一个穿着旗袍的美丽女子正看着里面闪闪发光的电脑屏幕；另一家店铺挂着“云模型”的牌子，门口放着一个大酒缸，上面写着“千问”，一位老板娘正在往里面倒发光的代码溶液。</p></blockquote><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/aliyun.png#center%20" width="100%"></figure><p>The model not only accurately captures Miyazaki’s anime style, but also features shop signs like “云存储” “云计算” and “云模型” as well as the “千问” on the wine jars, all rendered realistically and accurately with the depth of field. The poses and expressions of the characters are also perfectly preserved.</p><p>Let’s look at another example of Chinese rendering:</p><blockquote><p>一副典雅庄重的对联悬挂于厅堂之中，房间是个安静古典的中式布置，桌子上放着一些青花瓷，对联上左书“义本生知人机同道善思新”，右书“通云赋智乾坤启数高志远”， 横批“智启通义”，字体飘逸，中间挂在一着一副中国风的画作，内容是岳阳楼。</p></blockquote><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/tongyi.png#center" width="100%"></figure><p>The model accurately drew the left and right couplets and the horizontal scroll, applied calligraphy effects, and accurately generated the Yueyang Tower in the middle. The blue and white porcelain on the table also looked very realistic.</p><p>So, how does the model perform on English?
Let’s look at an English rendering example:</p><blockquote><p>Bookstore window display. A sign displays “New Arrivals This Week”. Below, a shelf tag with the text “Best-Selling Novels Here”. To the side, a colorful poster advertises “Author Meet And Greet on Saturday” with a central portrait of the author. There are four books on the bookshelf, namely “The light between worlds” “When stars are scattered” “The slient patient” “The night circus”</p></blockquote><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/book.png#center%20" width="100%"></figure><p>In this example, the model not only accurately outputs “New Arrivals This Week”, but also accurately generates the cover text of four books: “The light between worlds”, “When stars are scattered”, “The slient patient”, and “The night circus”.</p><p>Let’s look at a more complex case of English rendering:</p><blockquote><p>A slide featuring artistic, decorative shapes framing neatly arranged textual information styled as an elegant infographic. At the very center, the title “Habits for Emotional Wellbeing” appears clearly, surrounded by a symmetrical floral pattern. On the left upper section, “Practice Mindfulness” appears next to a minimalist lotus flower icon, with the short sentence, “Be present, observe without judging, accept without resisting”. Next, moving downward, “Cultivate Gratitude” is written near an open hand illustration, along with the line, “Appreciate simple joys and acknowledge positivity daily”. Further down, towards bottom-left, “Stay Connected” accompanied by a minimalistic chat bubble icon reads “Build and maintain meaningful relationships to sustain emotional energy”. At bottom right corner, “Prioritize Sleep” is depicted next to a crescent moon illustration, accompanied by the text “Quality sleep benefits both body and mind”. Moving upward along the right side, “Regular Physical Activity” is near a jogging runner icon, stating: “Exercise boosts mood and relieves anxiety”. Finally, at the top right side, appears “Continuous Learning” paired with a book icon, stating “Engage in new skill and knowledge for growth”. The slide layout beautifully balances clarity and artistry, guiding the viewers naturally along each text segment.</p></blockquote><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/six.png#center%20" width="100%"></figure><p>In this case, the model needs to generate 6 submodules, each with its own icon, title, and corresponding introductory text. Qwen-Image has completed the layout.</p><p>What about smaller text? Let us test it:</p><blockquote><p>A man in a suit is standing in front of the window, looking at the bright moon outside the window. The man is holding a yellowed paper with handwritten words on it: “A lantern moon climbs through the silver night, Unfurling quiet dreams across the sky, Each star a whispered promise wrapped in light, That dawn will bloom, though darkness wanders by.” There is a cute cat on the windowsill.</p></blockquote><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/small.png#center%20" width="100%"></figure><p>In this case, the paper is less than one-tenth of the entire image, and the paragraph of text is relatively long, but the model still accurately generates the text on the paper.</p><p>What if there are more words? Let’s try a harder case:</p><blockquote><p>一个穿着"QWEN"标志的T恤的中国美女正拿着黑色的马克笔面相镜头微笑。她身后的玻璃板上手写体写着 “一、Qwen-Image的技术路线： 探索视觉生成基础模型的极限，开创理解与生成一体化的未来。二、Qwen-Image的模型特色：1、复杂文字渲染。支持中英渲染、自动布局； 2、精准图像编辑。支持文字编辑、物体增减、风格变换。三、Qwen-Image的未来愿景：赋能专业内容创作、助力生成式AI发展。”</p></blockquote><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/long.png#center%20" width="100%"></figure><p>You can see that the model has completely generated a complete handwritten paragraph on the glass plate.</p><p>What if it’s bilingual? For the same scenario, let’s try this prompt:</p><blockquote><p>一个穿着"QWEN"标志的T恤的中国美女正拿着黑色的马克笔面相镜头微笑。她身后的玻璃板上手写体写着 “Meet Qwen-Image – a powerful image foundation model capable of complex text rendering and precise image editing. 欢迎了解Qwen-Image, 一款强大的图像基础模型，擅长复杂文本渲染与精准图像编辑”</p></blockquote><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bi.png#center%20" width="50%"></figure><p>As you can see, the model can switch between two languages at any time when rendering text.</p><p>Qwen-Image’s text capabilities make it easy to create posters, such as:</p><blockquote><p>A movie poster. The first row is the movie title, which reads “Imagination Unleashed”. The second row is the movie subtitle, which reads “Enter a world beyond your imagination”. The third row reads “Cast: Qwen-Image”. The fourth row reads “Director: The Collective Imagination of Humanity”. The central visual features a sleek, futuristic computer from which radiant colors, whimsical creatures, and dynamic, swirling patterns explosively emerge, filling the composition with energy, motion, and surreal creativity. The background transitions from dark, cosmic tones into a luminous, dreamlike expanse, evoking a digital fantasy realm. At the bottom edge, the text “Launching in the Cloud, August 2025” appears in bold, modern sans-serif font with a glowing, slightly transparent effect, evoking a high-tech, cinematic aesthetic. The overall style blends sci-fi surrealism with graphic design flair—sharp contrasts, vivid color grading, and layered visual depth—reminiscent of visionary concept art and digital matte painting, 32K resolution, ultra-detailed.</p></blockquote><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/poster.png#center%20" width="50%"></figure><p>Since we can make posters, of course we can also make PPTs directly. Let’s look at a case of making PPTs in Chinese:</p><blockquote><p>一张企业级高质量PPT页面图像，整体采用科技感十足的星空蓝为主色调，背景融合流动的发光科技线条与微光粒子特效，营造出专业、现代且富有信任感的品牌氛围；页面顶部左侧清晰展示橘红色Alibaba标志，色彩鲜明、辨识度高。主标题位于画面中央偏上位置，使用大号加粗白色或浅蓝色字体写着“通义千问视觉基础模型”，字体现代简洁，突出技术感；主标题下方紧接一行楷体中文文字：“原生中文·复杂场景·自动布局”，字体柔和优雅，形成科技与人文的融合。下方居中排布展示了四张与图片，分别是：一幅写实与水墨风格结合的梅花特写，枝干苍劲、花瓣清雅，背景融入淡墨晕染与飘雪效果，体现坚韧不拔的精神气质；上方写着黑色的楷体"梅傲"。一株生长于山涧石缝中的兰花，叶片修长、花朵素净，搭配晨雾缭绕的自然环境，展现清逸脱俗的文人风骨；上方写着黑色的楷体"兰幽"。一组迎风而立的翠竹，竹叶随风摇曳，光影交错，背景为青灰色山岩与流水，呈现刚柔并济、虚怀若谷的文化意象；上方写着黑色的楷体"竹清"。一片盛开于秋日庭院的菊花丛，花色丰富、层次分明，配以落叶与古亭剪影，传递恬然自适的生活哲学；上方写着黑色的楷体"菊淡"。所有图片采用统一尺寸与边框样式，呈横向排列。页面底部中央用楷体小字写明“2025年8月，敬请期待”，排版工整、结构清晰，整体风格统一且细节丰富，极具视觉冲击力与品牌调性。</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/ppt.png#center%20" width="100%"></figure></blockquote><p>In fact, beyond text processing, Qwen-Image also excels at general image generation, supporting a wide range of artistic styles. From photorealistic scenes to impressionistic paintings, from anime styles to minimalist designs, the model flexibly responds to a wide range of creative prompts, becoming a versatile tool for artists, designers, and storytellers. We will describe these in detail in our technical report.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/s2.jpg#center%20" width="100%"></figure><p>In terms of image editing, Qwen-Image supports a variety of operations, including style transfer, additions, deletions, detail enhancement, text editing, and character pose adjustment. This allows even ordinary users to easily achieve professional-level image editing. We will describe these in detail in our technical report.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/s3.jpg#center%20" width="100%"></figure><p>In summary, we hope that Qwen-Image can further promote the development of image generation, lower the technical barriers to visual content creation, and inspire more innovative applications. At the same time, we also look forward to the active participation and feedback of the community to jointly build an open, transparent, and sustainable generative AI ecosystem.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Customizing tmux (118 pts)]]></title>
            <link>https://evgeniipendragon.com/posts/customizing-tmux-and-making-it-less-dreadful/</link>
            <guid>44787374</guid>
            <pubDate>Mon, 04 Aug 2025 15:41:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://evgeniipendragon.com/posts/customizing-tmux-and-making-it-less-dreadful/">https://evgeniipendragon.com/posts/customizing-tmux-and-making-it-less-dreadful/</a>, See on <a href="https://news.ycombinator.com/item?id=44787374">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><p>I have been exploring some new tools here and there. When I started watching <a href="https://www.youtube.com/c/theprimeagen">Primeagen</a>, I took a note of several tools that he was using and advocating for. One of them was tmux.</p><h2 id="what-is-tmux">What is tmux?</h2><p><a href="https://github.com/tmux/tmux/wiki">tmux</a> is a terminal multiplexer. What that means is you can have many terminals in one. According to tmux wiki:</p><blockquote><p>tmux is a program which runs in a terminal and allows multiple other terminal programs to be run inside it. Each program inside tmux gets its own terminal managed by tmux, which can be accessed from the single terminal where tmux is running - this called multiplexing and tmux is a terminal multiplexer.</p></blockquote><p>tmux goes further by allowing you to have a lot of control over how the terminals are displayed, how they function, and how are they styled. You can have many separate windows as tabs. You can have many panes within one window. Are you working on several different projects? Then why don’t you have multiple sessions - each of them containing sets of windows and panes - all giving you access to multiple terminals. With a tool like this you no longer need to arrange multiple terminal instance windows on your desktop or using multiple workspaces for those instances - you can have it all in one place.</p><p>One additional feature that tmux boasts in is the ability to retain the terminal session upon its closure. Imagine that you have a long running process like a local server or a persistent connection like ssh session. If you were to close that window, you would lose that session and the process that was in it. Tmux has the ability to detach and attach to multiple sessions making them easier to manage and not to worry about losing it.</p><p>tmux wiki goes into detail talking about several other features of tmux:</p><blockquote><p>The main uses of tmux are to:</p><ul><li><p>Protect running programs on a remote server from connection drops by running them inside tmux.</p></li><li><p>Allow programs running on a remote server to be accessed from multiple different local computers.</p></li><li><p>Work with multiple programs and shells together in one terminal, a bit like a window manager.</p></li></ul><p>For example:</p><ul><li><p>A user connects to a remote server using ssh(1) from an xterm(1) on their work computer and run several programs. perhaps an editor, a compiler and a few shells.</p></li><li><p>They work with these programs interactively, perhaps start compiling, then close the xterm(1) with tmux and go home for the day.</p></li><li><p>They are then able to connect to the same remote server from home, attach to tmux, and continue from where they were previously.</p></li></ul></blockquote><h2 id="my-first-tmux-experience">My first tmux experience</h2><figure><img loading="lazy" src="https://github.com/tmux/tmux/wiki/images/tmux_default.png" alt="The first impression of tmux didn’t leave me with much excitement of exploring what is truly possible :/ Source: https://github.com/tmux/tmux/wiki/Getting-Started"><figcaption><p>The first impression of tmux didn’t leave me with much excitement of exploring what is truly possible :/ Source: <a href="https://github.com/tmux/tmux/wiki/Getting-Started">https://github.com/tmux/tmux/wiki/Getting-Started</a></p></figcaption></figure><p>When I first started using tmux, I felt overwhelmed by the very poor default UI that it offers and the amount of options and shortcuts that I needed to know to operate it well and be productive. It took me several tries before I started to feel more or less comfortable with it.</p><p>One of the things that stood out to me right away was how dreadful UI looked. Not only it felt uninviting, but it almost felt “gatekeepy” - if you don’t know how to use it in this configuration, then might as well forget about it. But one thing that kept me going was the promise of a highly customizable terminal that I could make my own at some point.</p><figure><img loading="lazy" src="https://github.com/tmux/tmux/wiki/images/tmux_with_panes.png" alt="While promising all of these amazing features, the dreadful tmux UI didn’t necessarily inspire :( Source: https://github.com/tmux/tmux/wiki/Getting-Started"><figcaption><p>While promising all of these amazing features, the dreadful tmux UI didn’t necessarily inspire :( Source: <a href="https://github.com/tmux/tmux/wiki/Getting-Started">https://github.com/tmux/tmux/wiki/Getting-Started</a></p></figcaption></figure><h2 id="customization-journey">Customization journey</h2><p>To preface, I must mention that I am using <a href="https://iterm2.com/">iterm2</a> with <a href="https://github.com/romkatv/powerlevel10k">Powerline10k</a> and <a href="https://github.com/ryanoasis/nerd-fonts">NerdFonts</a>. They heavily affect the final look of the tmux due to colors and icons being inherited from the iterm2 configuration. With that said, let’s get into it.</p><p>The file that is responsible for customizing tmux - its function and its look - is <code>.tmux.conf</code>. This file should be located in user’s Home folder a.k.a. <code>~/</code>. That is where tmux will be looking for it.</p><p>There are quite a few options when it comes to customizing tmux. One of the first things that I wanted to address was the <strong>prefix</strong> key binding. This prefix is a key combination that you would use to enter the tmux shortcut or command mode. By default, the prefix is <code>C-b</code> a.k.a. <code>Ctrl-b</code>. One of the common remaps that most people go with is to switch it to <code>C-a</code>, which is a lot easier to use. To do that, we need to add the following lines to the configuration file:</p><div><pre tabindex="0"><code data-lang="shell"><span><span><span># remap prefix from 'C-b' to 'C-a'</span>
</span></span><span><span>unbind C-b
</span></span><span><span>set-option -g prefix C-a
</span></span><span><span>bind-key C-a send-prefix
</span></span></code></pre></div><p>While researching ways to customize tmux, I stumbled upon several more useful key remaps, which I decided to use:</p><div><pre tabindex="0"><code data-lang="shell"><span><span><span># split panes using | and -</span>
</span></span><span><span><span>bind</span> <span>|</span> split-window -h
</span></span><span><span><span>bind</span> - split-window -v
</span></span><span><span>unbind <span>'"'</span>
</span></span><span><span>unbind %
</span></span><span><span>
</span></span><span><span><span># reload config file (change file location to your the tmux.conf you want to use)</span>
</span></span><span><span><span>bind</span> r source-file ~/.tmux.conf<span>;</span> display-message <span>"~/.tmux.conf reloaded."</span>
</span></span><span><span>
</span></span><span><span><span># switch panes using Alt-arrow without prefix</span>
</span></span><span><span><span>bind</span> -n M-Left <span>select</span>-pane -L
</span></span><span><span><span>bind</span> -n M-Right <span>select</span>-pane -R
</span></span><span><span><span>bind</span> -n M-Up <span>select</span>-pane -U
</span></span><span><span><span>bind</span> -n M-Down <span>select</span>-pane -D
</span></span><span><span>
</span></span><span><span><span># switch windows using Shift-arrow without prefix</span>
</span></span><span><span><span>bind</span> -n S-Left previous-window
</span></span><span><span><span>bind</span> -n S-Right next-window
</span></span></code></pre></div><p>Here we are changing function to split window from <code>C-a "</code> to <code>C-a |</code> for vertical splits and from <code>C-a %</code> to <code>C-a -</code> for horizontal splits, which is a lot more intuitive. Next we are creating several useful keybinds: <code>C-a r</code> to source the configuration file, <code>M-Left</code>, <code>M-Right</code>, <code>M-Up</code>, <code>M-Down</code> for faster navigation between the panes (<code>M</code> being either <code>Alt</code> or <code>Option</code>), and <code>S-Left</code> and <code>S-Right</code> for better navigation between the windows (<code>S</code> being <code>Shift</code>). These changes alone have already made my tmux experience 10x better than before.</p><p>Next, I wanted to improve my experience when scrolling the terminal output. By default, if you were to attempt to scroll it up or down, tmux would iterate over the history of the commands that you have executed. I would like to be able to look over the previous output and be able to operate on it. By default it is possible by entering the copy mode using <code>C-a [</code>. Once you’re there, it gives you very little to work with - the default terminal output retention is quite small. To fix all of these issues, I used the following configuration:</p><div><pre tabindex="0"><code data-lang="shell"><span><span><span># Increase the scrollback buffer to a higher limit than the default 2000 lines</span>
</span></span><span><span><span>set</span> -g history-limit <span>9999999</span>
</span></span><span><span>
</span></span><span><span><span># Enable mouse control (clickable windows, panes, resizable panes)</span>
</span></span><span><span><span>set</span> -g mouse on
</span></span><span><span>
</span></span><span><span><span># Enable vim keybinds to be used in copy mode</span>
</span></span><span><span>set-window-option -g mode-keys vi
</span></span></code></pre></div><p>These settings enable tmux retain 9999999 lines of terminal output (more than enough for me), enable using mouse, which removes the necessity to enter the copy mode to browse terminal output, and lastly enable Vim key bindings when in copy mode to quickly search and navigate through it.</p><p>And that brings us to styling tmux. As I mentioned, I didn’t like the original UI much, so I felt strongly about changing that. To do that, I used the following configuration:</p><div><pre tabindex="0"><code data-lang="shell"><span><span><span># clock mode</span>
</span></span><span><span>setw -g clock-mode-colour yellow
</span></span><span><span>
</span></span><span><span><span># copy mode</span>
</span></span><span><span>setw -g mode-style <span>'fg=black bg=yellow bold'</span>
</span></span><span><span>
</span></span><span><span><span># panes</span>
</span></span><span><span><span>set</span> -g pane-border-style <span>'fg=yellow'</span>
</span></span><span><span><span>set</span> -g pane-active-border-style <span>'fg=green'</span>
</span></span><span><span>
</span></span><span><span><span># statusbar</span>
</span></span><span><span><span>set</span> -g status-position top
</span></span><span><span><span>set</span> -g status-justify left
</span></span><span><span><span>set</span> -g status-style <span>'fg=green'</span>
</span></span><span><span>
</span></span><span><span><span>set</span> -g status-left <span>''</span>
</span></span><span><span><span>set</span> -g status-left-length <span>10</span>
</span></span><span><span>
</span></span><span><span><span>set</span> -g status-right <span>'#[fg=green,bg=default,bright]#(tmux-mem-cpu-load) #[fg=red,dim,bg=default]#(uptime | cut -f 4-5 -d " " | cut -f 1 -d ",") #[fg=white,bg=default]%a%l:%M:%S %p#[default] #[fg=blue]%Y-%m-%d'</span>
</span></span><span><span>
</span></span><span><span>setw -g window-status-current-style <span>'fg=black bg=green'</span>
</span></span><span><span>setw -g window-status-current-format <span>' #I #W #F '</span>
</span></span><span><span>
</span></span><span><span>setw -g window-status-style <span>'fg=green bg=black'</span>
</span></span><span><span>setw -g window-status-format <span>' #I #[fg=white]#W #[fg=yellow]#F '</span>
</span></span><span><span>
</span></span><span><span>setw -g window-status-bell-style <span>'fg=black bg=yellow bold'</span>
</span></span><span><span>
</span></span><span><span><span># messages</span>
</span></span><span><span><span>set</span> -g message-style <span>'fg=black bg=yellow bold'</span>
</span></span></code></pre></div><p>These lines attempt to create a (subjectively) better looking UI that is easier on the eyes and more intuitive. It uses standard color names that would fit the color scheme that your terminal is already using so that it doesn’t stick out too much. Although it is very much possible to use whatever colors you want here - tmux is your oyster. Here we are also calibrating the positioning of the elements like status bar, look and function of parts of the status bar, and lastly colors for messages and copy mode.</p><p>Finally, let’s add tmux plugin manager. That’s right - tmux has a whole library of community maintained plugins. If you want to you can find entire plugins that will change the look and feel or your tmux in an instant. The reason I decided to go with my own configuration is so that I could learn it better and own it. Any new theme plugin would require some learning to know how it functions and where everything is. Regardless, configuring tmux plugin manager can be done in just a few lines:</p><div><pre tabindex="0"><code data-lang="shell"><span><span><span># List of plugins</span>
</span></span><span><span><span>set</span> -g @plugin <span>'tmux-plugins/tpm'</span>
</span></span><span><span><span>set</span> -g @plugin <span>'tmux-plugins/tmux-sensible'</span>
</span></span><span><span>
</span></span><span><span><span># Other examples:</span>
</span></span><span><span><span># set -g @plugin 'github_username/plugin_name'</span>
</span></span><span><span><span># set -g @plugin 'github_username/plugin_name#branch'</span>
</span></span><span><span><span># set -g @plugin 'git@github.com:user/plugin'</span>
</span></span><span><span><span># set -g @plugin 'git@bitbucket.com:user/plugin'</span>
</span></span><span><span>
</span></span><span><span><span># Initialize TMUX plugin manager (keep this line at the very bottom of tmux.conf)</span>
</span></span><span><span>run <span>'~/.tmux/plugins/tpm/tpm'</span>
</span></span></code></pre></div><p>One note to make here is that you would need to clone the repository for tmux plugin manager into <code>~/.tmux/plugins/tpm</code> for it to work. You can do that by following instructions on the <a href="https://github.com/tmux-plugins/tpm">tmux plugin manager</a> GitHub repository.</p><p>I haven’t installed many plugins just yet because I am still exploring which plugins I would want to have installed. I have several that I’m interested in exploring and giving a go:</p><ul><li><a href="https://github.com/tmux-plugins/tmux-battery">tmux-battery</a></li><li><a href="https://github.com/b0o/tmux-autoreload">tmux-autoreload</a></li><li><a href="https://github.com/ofirgall/tmux-browser">tmux-browser</a></li><li><a href="https://github.com/lost-melody/tmux-command-palette">tmux-command-palette</a></li><li><a href="https://github.com/wfxr/tmux-fzf-url">tmux-fzf-url</a></li><li><a href="https://github.com/jaclu/tmux-menus">tmux-menus</a></li><li><a href="https://github.com/tmux-plugins/tmux-resurrect">tmux-ressurect</a></li></ul><h2 id="full-configuration">Full configuration</h2><p>Here is a full configuration file. It includes a couple of other features that I haven’t mentioned because they are less significant than the ones that I have highlighted.</p><div><pre tabindex="0"><code data-lang="shell"><span><span><span># remap prefix from 'C-b' to 'C-a'</span>
</span></span><span><span>unbind C-b
</span></span><span><span>set-option -g prefix C-a
</span></span><span><span>bind-key C-a send-prefix
</span></span><span><span>
</span></span><span><span><span># split panes using | and -</span>
</span></span><span><span><span>bind</span> <span>|</span> split-window -h
</span></span><span><span><span>bind</span> - split-window -v
</span></span><span><span>unbind <span>'"'</span>
</span></span><span><span>unbind %
</span></span><span><span>
</span></span><span><span><span># reload config file (change file location to your the tmux.conf you want to use)</span>
</span></span><span><span><span>bind</span> r source-file ~/.tmux.conf<span>;</span> display-message <span>"~/.tmux.conf reloaded."</span>
</span></span><span><span>
</span></span><span><span><span># switch panes using Alt-arrow without prefix</span>
</span></span><span><span><span>bind</span> -n M-Left <span>select</span>-pane -L
</span></span><span><span><span>bind</span> -n M-Right <span>select</span>-pane -R
</span></span><span><span><span>bind</span> -n M-Up <span>select</span>-pane -U
</span></span><span><span><span>bind</span> -n M-Down <span>select</span>-pane -D
</span></span><span><span>
</span></span><span><span><span># switch windows using Shift-arrow without prefix</span>
</span></span><span><span><span>bind</span> -n S-Left previous-window
</span></span><span><span><span>bind</span> -n S-Right next-window
</span></span><span><span>
</span></span><span><span><span># Increase the scrollback buffer to a higher limit than the default 2000 lines</span>
</span></span><span><span><span>set</span> -g history-limit <span>9999999</span>
</span></span><span><span>
</span></span><span><span><span># Enable mouse control (clickable windows, panes, resizable panes)</span>
</span></span><span><span><span>set</span> -g mouse on
</span></span><span><span>
</span></span><span><span><span># Enable vim keybinds to be used in copy mode</span>
</span></span><span><span>set-window-option -g mode-keys vi
</span></span><span><span>
</span></span><span><span><span># don't rename windows automatically</span>
</span></span><span><span><span># set-option -g allow-rename off</span>
</span></span><span><span>
</span></span><span><span><span># rename window to reflect current program</span>
</span></span><span><span>setw -g automatic-rename on
</span></span><span><span><span># renumber windows when a window is closed</span>
</span></span><span><span><span>set</span> -g renumber-windows on
</span></span><span><span>
</span></span><span><span><span># don't do anything when a 'bell' rings</span>
</span></span><span><span><span>set</span> -g visual-activity off
</span></span><span><span><span>set</span> -g visual-bell off
</span></span><span><span><span>set</span> -g visual-silence off
</span></span><span><span>setw -g monitor-activity off
</span></span><span><span><span>set</span> -g bell-action none
</span></span><span><span>
</span></span><span><span><span># clock mode</span>
</span></span><span><span>setw -g clock-mode-colour yellow
</span></span><span><span>
</span></span><span><span><span># copy mode</span>
</span></span><span><span>setw -g mode-style <span>'fg=black bg=yellow bold'</span>
</span></span><span><span>
</span></span><span><span><span># panes</span>
</span></span><span><span><span>set</span> -g pane-border-style <span>'fg=yellow'</span>
</span></span><span><span><span>set</span> -g pane-active-border-style <span>'fg=green'</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span># Allows for faster key repetition</span>
</span></span><span><span><span># set -s escape-time 50</span>
</span></span><span><span>
</span></span><span><span><span># statusbar</span>
</span></span><span><span><span>set</span> -g status-position top
</span></span><span><span><span>set</span> -g status-justify left
</span></span><span><span><span>set</span> -g status-style <span>'fg=green'</span>
</span></span><span><span>
</span></span><span><span><span>set</span> -g status-left <span>''</span>
</span></span><span><span><span>set</span> -g status-left-length <span>10</span>
</span></span><span><span>
</span></span><span><span><span>set</span> -g status-right <span>'#[fg=green,bg=default,bright]#(tmux-mem-cpu-load) #[fg=red,dim,bg=default]#(uptime | cut -f 4-5 -d " " | cut -f 1 -d ",") #[fg=white,bg=default]%a%l:%M:%S %p#[default] #[fg=blue]%Y-%m-%d'</span>
</span></span><span><span>
</span></span><span><span>setw -g window-status-current-style <span>'fg=black bg=green'</span>
</span></span><span><span>setw -g window-status-current-format <span>' #I #W #F '</span>
</span></span><span><span>
</span></span><span><span>setw -g window-status-style <span>'fg=green bg=black'</span>
</span></span><span><span>setw -g window-status-format <span>' #I #[fg=white]#W #[fg=yellow]#F '</span>
</span></span><span><span>
</span></span><span><span>setw -g window-status-bell-style <span>'fg=black bg=yellow bold'</span>
</span></span><span><span>
</span></span><span><span><span># messages</span>
</span></span><span><span><span>set</span> -g message-style <span>'fg=black bg=yellow bold'</span>
</span></span><span><span>
</span></span><span><span><span># List of plugins</span>
</span></span><span><span><span>set</span> -g @plugin <span>'tmux-plugins/tpm'</span>
</span></span><span><span><span>set</span> -g @plugin <span>'tmux-plugins/tmux-sensible'</span>
</span></span><span><span>
</span></span><span><span><span># Other examples:</span>
</span></span><span><span><span># set -g @plugin 'github_username/plugin_name'</span>
</span></span><span><span><span># set -g @plugin 'github_username/plugin_name#branch'</span>
</span></span><span><span><span># set -g @plugin 'git@github.com:user/plugin'</span>
</span></span><span><span><span># set -g @plugin 'git@bitbucket.com:user/plugin'</span>
</span></span><span><span>
</span></span><span><span><span># Initialize TMUX plugin manager (keep this line at the very bottom of tmux.conf)</span>
</span></span><span><span>run <span>'~/.tmux/plugins/tpm/tpm'</span>
</span></span></code></pre></div><p>This file could also be found in my GitHub repo: <a href="https://github.com/EvgeniiKlepilin/config-files/blob/main/.tmux.conf">https://github.com/EvgeniiKlepilin/config-files/blob/main/.tmux.conf</a> . I will keep updating it as I continue my customization efforts, so stay tuned.</p><h2 id="final-look">Final look</h2><p>In the end, here is what my tmux looks like now:</p><figure><img loading="lazy" src="https://evgeniipendragon.com/posts/customizing-tmux-and-making-it-less-dreadful/tmux_is_awesome.png" alt="Much better! 😎"><figcaption><p>Much better! 😎</p></figcaption></figure><p>I hope you will find this useful and maybe even inspiring to create something custom of your own!</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A deep dive into Rust and C memory interoperability (138 pts)]]></title>
            <link>https://notashes.me/blog/part-1-memory-management/</link>
            <guid>44786962</guid>
            <pubDate>Mon, 04 Aug 2025 15:12:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://notashes.me/blog/part-1-memory-management/">https://notashes.me/blog/part-1-memory-management/</a>, See on <a href="https://news.ycombinator.com/item?id=44786962">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">   <blockquote>
<p><strong>“Memory oppresses me.” - Severian, The Book of the New Sun</strong></p>
<p><strong>Interviewer</strong>: “What happens if you allocate memory with C’s malloc and try to free it with Rust’s dealloc, if you get a pointer to the memory from C?”</p>
<p><strong>Me</strong>: “If we do it via FFI then there’s a possibility the program may continue working (because the underlying structs share the same memory layout? right? …right?)”</p>
<p><em>Now if you have any experience working with memory management, you know that this is a dangerous answer. But I didn’t know it at the time. I was just trying to get through the interview.</em></p>
<p>But I realized at that moment that I had been treating memory allocators like black boxes. I knew the rules - never mix allocators - but I didn’t truly understand <em>why</em>. So here’s my attempt at de-mystifying memory management, starting with the fundamentals and building a testing laboratory to explore what happens when different memory worlds collide.</p>
</blockquote>
<h2 id="prerequisites">Prerequisites<a href="#prerequisites">#</a></h2>
<p>To get the most from this article, you should be familiar with:</p>
<ul>
<li>Basic Rust and C programming</li>
<li>Pointers and memory management concepts</li>
<li>Command line tools (bash, gcc, cargo)</li>
<li>Basic understanding of stack vs heap</li>
</ul>
<p>Don’t worry if you’re not an expert, I’m not one either - but I’ll explain concepts as best I can!</p>
<h2 id="table-of-contents">Table of Contents<a href="#table-of-contents">#</a></h2>
<ol>
<li><a href="#the-interview-question-that-started-everything">The Interview Question That Started Everything</a></li>
<li><a href="#why-memory-allocators-dont-mix">Why Memory Allocators Don’t Mix</a></li>
<li><a href="#memory-fundamentals-building-our-mental-model">Memory Fundamentals: Building Our Mental Model</a></li>
<li><a href="#building-a-memory-testing-laboratory">Building a Memory Testing Laboratory</a></li>
<li><a href="#first-experiments-surprising-results">First Experiments: Surprising Results</a></li>
<li><a href="#key-takeaways-and-whats-next">Key Takeaways and What’s Next</a></li>
</ol>
<h2 id="the-interview-question-that-started-everything">The Interview Question That Started Everything<a href="#the-interview-question-that-started-everything">#</a></h2>
<p>It was Friday afternoon when I had an interview for an amazing startup which focuses on building very high performance systems. The interview experience was intense while being highly rewarding. We touched upon topics async runtimes, memory management, rust FFI etc.</p>
<p>The intention wasn’t to test my language specific knowledge but being able to reason about how these systems work at a level closer to the machine.</p>
<p>It caught me a little offguard. It’s not something I had prepared for. However, to be a good systems engineer, It is essential to develop a knack for the fundamentals - understanding how things work all the way down to the metal. Whether it’s the intricacies of the CPU cache hierarchy, memory alignment, or the behavior of allocators under concurrency, these low-level details can have profound impacts on system performance and correctness.</p>
<p>That experience prompted me to reflect on my own gaps and sparked a sort of yearning to dig deeper into the topic. Hence, I decided to do this and start a journey to understand memory management better, starting with the basics and building a comprehensive testing framework to explore the interactions between Rust and C memory allocators.</p>
<h2 id="why-memory-allocators-dont-mix">Why Memory Allocators Don’t Mix<a href="#why-memory-allocators-dont-mix">#</a></h2>
<p>Before diving into the technical details, let’s understand the fundamental problem. But first, we need to establish what different exit codes mean when testing memory operations:</p>
<h3 id="understanding-exit-codes-in-memory-testing">Understanding Exit Codes in Memory Testing<a href="#understanding-exit-codes-in-memory-testing">#</a></h3>
<p>When experimenting with memory allocators, the exit code tells us exactly what happened:</p>





























<table><thead><tr><th>Exit Code</th><th>Signal</th><th>Meaning</th><th>Safety</th></tr></thead><tbody><tr><td>0</td><td>None</td><td>Process completed “successfully”</td><td>⚠️ <strong>DANGEROUS</strong> - Silent corruption</td></tr><tr><td>-11 or 139</td><td>SIGSEGV</td><td>Segmentation fault - invalid memory access</td><td>✅ Safe - OS detected bad access</td></tr><tr><td>-6 or 134</td><td>SIGABRT</td><td>Program aborted - allocator detected corruption</td><td>✅ Safe - Allocator safety checks worked</td></tr></tbody></table>
<blockquote>
<p>⚠️ <strong>The Hidden Danger of Exit Code 0</strong></p>
<p>When mixing allocators, exit code 0 is the worst possible outcome. It means memory corruption occurred but went undetected. Your program continues running with a corrupted heap - a time bomb that will explode unpredictably later. A crash (SIGSEGV or SIGABRT) is actually the safe outcome because it prevents further corruption.</p>
</blockquote>
<p>Now, when you write:</p>
<div tabindex="0" data-language="rust"><pre><code><span><span>// dangerous.rs</span></span>
<span><span>let</span><span> ptr </span><span>=</span><span> unsafe</span><span> { </span><span>libc</span><span>::</span><span>malloc</span><span>(</span><span>64</span><span>) };</span></span></code></pre><p><span>rust</span></p></div>
<p>You’re not just getting 64 bytes of memory. You’re entering into a complex contract with a specific allocator implementation. That allocator needs to track:</p>
<ul>
<li>How much memory you requested</li>
<li>Whether this chunk is free or allocated</li>
<li>Where the next and previous chunks are</li>
<li>Thread ownership information</li>
<li>Debugging metadata (in debug builds)</li>
</ul>
<p>Different allocators store this information differently. When you later call:</p>
<div tabindex="0" data-language="rust"><pre><code><span><span>// dangerous.rs</span></span>
<span><span>unsafe</span><span> { </span><span>std</span><span>::</span><span>alloc</span><span>::</span><span>dealloc</span><span>(ptr </span><span>as</span><span> *mut</span><span> u8</span><span>, layout) };</span></span></code></pre><p><span>rust</span></p></div>
<blockquote>
<p>⚠️ <strong>The Metadata Mismatch</strong></p>
<p>Rust’s allocator looks for its metadata format at specific offsets from your pointer. If it finds glibc’s metadata instead, the best case is an immediate crash. The worst case? Silent corruption that manifests as mysterious bugs hours later.</p>
</blockquote>
<h2 id="memory-fundamentals-building-our-mental-model">Memory Fundamentals: Building Our Mental Model<a href="#memory-fundamentals-building-our-mental-model">#</a></h2>
<p>To understand why allocators clash, we need to build a mental model of how memory actually works in modern systems.</p>
<h3 id="virtual-memory-the-grand-illusion">Virtual Memory: The Grand Illusion<a href="#virtual-memory-the-grand-illusion">#</a></h3>
<p>Every process on a modern operating system lives in its own virtual address space. On a 64-bit Linux system, your process sees:
<img src="https://notashes.me/_astro/memory-layout-of-a-process.CPUe2S_j_Zy5Jpx.webp" alt="virtual address space layout" width="1280" height="720" loading="lazy" decoding="async"></p>
<p>This is all an illusion. These addresses don’t correspond directly to physical RAM. Instead, the CPU and operating system work together to translate virtual addresses to physical addresses on every memory access. Understanding this translation is crucial because it affects everything from allocator design to the performance impact of memory access patterns.</p>
<h3 id="the-true-cost-of-memory-access">The True Cost of Memory Access<a href="#the-true-cost-of-memory-access">#</a></h3>
<p>To understand memory access costs, let’s trace what happens when our test program accesses a typical heap address. During our experiments, malloc returned addresses like <code>0x00007fab8c3d2150</code>. This isn’t random - addresses starting with <code>0x00007f</code> are in the standard heap region on 64-bit Linux systems.</p>
<p>Here’s how the CPU translates this virtual address to physical RAM:</p>
<div tabindex="0" data-language="plaintext"><pre><code><span><span>Virtual Address Translation (x86_64 with 4-level paging)</span></span>
<span><span></span></span>
<span><span>Virtual Address: 0x00007fab8c3d2150 (from our malloc experiment)</span></span>
<span><span></span></span>
<span><span>Bit Layout:</span></span>
<span><span>┌─────────┬─────────┬─────────┬─────────┬────────────┐</span></span>
<span><span>│  PML4   │   PDP   │   PD    │   PT    │   Offset   │</span></span>
<span><span>│ [47:39] │ [38:30] │ [29:21] │ [20:12] │   [11:0]   │</span></span>
<span><span>├─────────┼─────────┼─────────┼─────────┼────────────┤</span></span>
<span><span>│  0x0FE  │  0x1AE  │  0x118  │  0x1D2  │   0x150    │</span></span>
<span><span>└─────────┴─────────┴─────────┴─────────┴────────────┘</span></span>
<span><span></span></span>
<span><span>Where:</span></span>
<span><span>- PML4 = Page Map Level 4 (top-level page table)</span></span>
<span><span>- PDP = Page Directory Pointer</span></span>
<span><span>- PD = Page Directory  </span></span>
<span><span>- PT = Page Table</span></span>
<span><span>- Offset = Position within the 4KB page</span></span>
<span><span></span></span>
<span><span>Translation Steps:</span></span>
<span><span>1. CR3 register + (PML4 index × 8) → PML4 entry → PDP base address</span></span>
<span><span>2. PDP base + (PDP index × 8) → PDP entry → PD base address  </span></span>
<span><span>3. PD base + (PD index × 8) → PD entry → PT base address</span></span>
<span><span>4. PT base + (PT index × 8) → PT entry → Physical page base</span></span>
<span><span>5. Physical page base + offset (0x150) → Final physical address</span></span>
<span><span></span></span>
<span><span>Cost: 4 memory accesses without TLB hit</span></span>
<span><span>      ~1 cycle with TLB hit (typical case)</span></span></code></pre><p><span>plaintext</span></p></div>
<p>The Translation Lookaside Buffer (TLB) is a specialized cache that stores recent virtual-to-physical address mappings. When you access memory sequentially (like iterating through an array), the TLB hit rate approaches 100%, making translation nearly free. But random access patterns can cause TLB misses, adding ~100 cycles per access - which is why memory access patterns matter so much for performance.</p>
<h3 id="the-heap-where-dynamic-memory-lives">The Heap: Where Dynamic Memory Lives<a href="#the-heap-where-dynamic-memory-lives">#</a></h3>
<p>When you call <code>malloc(64)</code>, you’re asking the allocator to find 64 bytes of free memory on the heap. But this simple request triggers a complex chain of events:</p>
<ol>
<li><strong>Thread-Local Cache Check</strong>: Modern allocators first check thread-local caches to avoid lock contention</li>
<li><strong>Central Cache Search</strong>: If the thread cache is empty, check central free lists</li>
<li><strong>Free List Management</strong>: Search through free lists organized by size classes</li>
<li><strong>Heap Expansion</strong>: If no suitable chunk exists, request more memory from the OS</li>
</ol>
<p>The allocator must also deal with fragmentation:</p>
<div tabindex="0" data-language="plaintext"><pre><code><span><span>Heap State After Various Allocations/Deallocations:</span></span>
<span><span></span></span>
<span><span>[Used:16][Free:32][Used:64][Free:16][Used:32][Free:64]</span></span>
<span><span></span></span>
<span><span>Request for 48 bytes:</span></span>
<span><span>- First free chunk (32 bytes): Too small ✗</span></span>
<span><span>- Second free chunk (16 bytes): Too small ✗  </span></span>
<span><span>- Third free chunk (64 bytes): Success ✓</span></span>
<span><span></span></span>
<span><span>Even though we have 112 bytes free total, they're not contiguous!</span></span></code></pre><p><span>plaintext</span></p></div>
<h3 id="cpu-cache-architecture-the-hidden-performance-layer">CPU Cache Architecture: The Hidden Performance Layer<a href="#cpu-cache-architecture-the-hidden-performance-layer">#</a></h3>
<p>Modern CPUs have multiple cache levels to bridge the massive speed gap between CPU and RAM:</p>
<div tabindex="0" data-language="plaintext"><pre><code><span><span>CPU Cache Hierarchy (typical Intel/AMD x86_64)</span></span>
<span><span>━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span></span>
<span><span></span></span>
<span><span>CPU Core</span></span>
<span><span>├─ Registers (16-32, ~0 cycles)</span></span>
<span><span>├─ L1 Cache (32-64KB, ~4-5 cycles)</span></span>
<span><span>├─ L2 Cache (256KB-1MB, ~12-15 cycles)</span></span>
<span><span>└─ L3 Cache (8-32MB shared, ~40-60 cycles)</span></span>
<span><span>    │</span></span>
<span><span>    └─── Main Memory (~100-300 cycles)</span></span>
<span><span></span></span>
<span><span>Cache Line Size: 64 bytes (x86_64)</span></span></code></pre><p><span>plaintext</span></p></div>
<p><em>Note: These are typical values - actual latencies vary by CPU model and generation</em></p>
<blockquote>
<p>💡 <strong>False Sharing: The Hidden Performance Killer</strong></p>
<p>This architecture has profound implications. Consider false sharing:</p>
<div tabindex="0" data-language="c"><pre><code><span><span>struct</span><span> thread_stats {</span></span>
<span><span>    int</span><span> thread1_counter;</span><span>  // Offset 0-3</span></span>
<span><span>    int</span><span> thread2_counter;</span><span>  // Offset 4-7  </span></span>
<span><span>    // Both in same 64-byte cache line!</span></span>
<span><span>};</span></span></code></pre><p><span>c</span></p></div>
<p>When thread 1 updates its counter, it invalidates the entire cache line on other cores. Thread 2 must wait for exclusive access to update its counter, even though they’re touching different variables. In our experiments, this caused an <strong>8.67x performance penalty</strong> - from 359.7M ops/sec down to 41.4M ops/sec!</p>
<p><strong>How we measured this</strong>: Using <code>perf stat -e L1-dcache-loads,L1-dcache-load-misses ./false_sharing_test</code>, we observed 891M L1 cache misses with false sharing vs only 12M without - a 74x increase in cache misses!</p>
</blockquote>
<h2 id="building-a-memory-testing-laboratory">Building a Memory Testing Laboratory<a href="#building-a-memory-testing-laboratory">#</a></h2>
<p>Understanding theory is one thing. Seeing it explode in practice is another. Armed with knowledge about virtual memory, heap structure, and cache architecture, I needed to build a comprehensive testing framework that could safely explore what happens when different memory worlds collide.</p>
<p>The framework needed to:</p>
<ol>
<li>Test multiple allocator implementations</li>
<li>Safely handle (and analyze) crashes</li>
<li>Measure performance without affecting results</li>
<li>Provide detailed debugging information</li>
</ol>
<blockquote>
<p>📊 <strong>Testing Infrastructure Overview:</strong></p>
<p><strong>Key Components:</strong></p>
<ul>
<li><strong>Subprocess isolation</strong>: Each test runs in its own process via <code>Command::new()</code></li>
<li><strong>C library loading</strong>: <code>export LD_LIBRARY_PATH=../c-lib:$LD_LIBRARY_PATH</code></li>
<li><strong>Exit code analysis</strong>: Maps signals to meaningful results</li>
<li><strong>Performance tools</strong>: <code>perf stat</code>, custom timing, cache analysis</li>
</ul>
<p><strong>Repository Structure:</strong></p>
<div tabindex="0" data-language="plaintext"><pre><code><span><span>rust-c-memory-interop/</span></span>
<span><span>├── c-lib/           # Custom allocator implementations</span></span>
<span><span>├── rust-ffi/        # Rust test binaries and FFI bindings</span></span>
<span><span>├── tools/           # Analysis scripts (bash)</span></span>
<span><span>│   ├── run_crash_tests.sh  # Runs crash tests in subprocesses</span></span>
<span><span>│   ├── perf_analysis.sh    # Generates performance analysis code</span></span>
<span><span>│   └── deep_analysis.sh    # Generates memory analysis code</span></span>
<span><span>└── test_results/    # Output from experiments</span></span></code></pre><p><span>plaintext</span></p></div>
<p><strong>Note</strong>: The bash scripts in <code>tools/</code> dynamically generate Rust code for specialized analysis. This keeps the main codebase clean while allowing complex experiments.</p>
</blockquote>
<p>Here’s the framework I built:</p>
<div tabindex="0" data-language="rust"><pre><code><span><span>// rust-ffi/src/comprehensive_tests.rs</span></span>
<span><span>use</span><span> std</span><span>::</span><span>collections</span><span>::</span><span>HashMap</span><span>;</span></span>
<span><span>use</span><span> std</span><span>::</span><span>time</span><span>::</span><span>Instant</span><span>;</span></span>
<span></span>
<span><span>#[derive(</span><span>Debug</span><span>, </span><span>Clone</span><span>)]</span></span>
<span><span>pub</span><span> struct</span><span> TestResult</span><span> {</span></span>
<span><span>    pub</span><span> test_name</span><span>:</span><span> String</span><span>,</span></span>
<span><span>    pub</span><span> allocator</span><span>:</span><span> String</span><span>,</span></span>
<span><span>    pub</span><span> success</span><span>:</span><span> bool</span><span>,</span></span>
<span><span>    pub</span><span> duration</span><span>:</span><span> std</span><span>::</span><span>time</span><span>::</span><span>Duration</span><span>,</span></span>
<span><span>    pub</span><span> metrics</span><span>:</span><span> HashMap</span><span>&lt;</span><span>String</span><span>, </span><span>f64</span><span>&gt;,</span></span>
<span><span>    pub</span><span> notes</span><span>:</span><span> Vec</span><span>&lt;</span><span>String</span><span>&gt;,</span></span>
<span><span>}</span></span>
<span></span>
<span><span>pub</span><span> struct</span><span> ComprehensiveTestSuite</span><span> {</span></span>
<span><span>    results</span><span>:</span><span> Vec</span><span>&lt;</span><span>TestResult</span><span>&gt;,</span></span>
<span><span>}</span></span>
<span></span>
<span><span>impl</span><span> ComprehensiveTestSuite</span><span> {</span></span>
<span><span>    pub</span><span> fn</span><span> new</span><span>() </span><span>-&gt;</span><span> Self</span><span> {</span></span>
<span><span>        Self</span><span> {</span></span>
<span><span>            results</span><span>:</span><span> Vec</span><span>::</span><span>new</span><span>(),</span></span>
<span><span>        }</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    pub</span><span> fn</span><span> run_all_tests</span><span>(</span><span>&amp;mut</span><span> self</span><span>) {</span></span>
<span><span>        println!</span><span>(</span><span>"=== Comprehensive Memory Allocator Test Suite ===</span><span>\n</span><span>"</span><span>);</span></span>
<span></span>
<span><span>        // Basic functionality tests</span></span>
<span><span>        self</span><span>.</span><span>test_basic_allocation</span><span>();</span></span>
<span><span>        self</span><span>.</span><span>test_alignment_requirements</span><span>();</span></span>
<span><span>        self</span><span>.</span><span>test_size_classes</span><span>();</span></span>
<span><span>        </span></span>
<span><span>        // Performance tests</span></span>
<span><span>        self</span><span>.</span><span>test_allocation_performance</span><span>();</span></span>
<span><span>        self</span><span>.</span><span>test_fragmentation_behavior</span><span>();</span></span>
<span><span>        self</span><span>.</span><span>test_cache_efficiency</span><span>();</span></span>
<span><span>        </span></span>
<span><span>        // Safety tests</span></span>
<span><span>        self</span><span>.</span><span>test_metadata_corruption</span><span>();</span></span>
<span><span>        self</span><span>.</span><span>test_allocator_mixing</span><span>();</span></span>
<span><span>        </span></span>
<span><span>        // Generate report</span></span>
<span><span>        self</span><span>.</span><span>generate_report</span><span>();</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre><p><span>rust</span></p></div>
<h3 id="implementing-multiple-allocators">Implementing Multiple Allocators<a href="#implementing-multiple-allocators">#</a></h3>
<p>To test allocator interactions, I implemented four different allocators in C, each with distinct characteristics and use cases:</p>
<p><strong>1. Standard malloc wrapper</strong> - <em>A thin pass-through to glibc’s malloc</em>:</p>
<blockquote>
<p><strong>Use case</strong>: General-purpose allocation, the default for most C programs <br>
<strong>Pros</strong>: Fast, well-tested, handles fragmentation well <br>
<strong>Cons</strong>: No built-in debugging, metadata can be corrupted</p>
</blockquote>
<div tabindex="0" data-language="c"><pre><code><span><span>// allocators.c - Just forwards to system malloc/free</span></span>
<span><span>void*</span><span> standard_malloc</span><span>(</span><span>size_t</span><span> size</span><span>) {</span></span>
<span><span>    void*</span><span> ptr </span><span>=</span><span> malloc</span><span>(size);</span></span>
<span><span>    printf</span><span>(</span><span>"[C] standard_malloc(</span><span>%zu</span><span>) = </span><span>%p\n</span><span>"</span><span>, size, ptr);</span></span>
<span><span>    return</span><span> ptr;</span></span>
<span><span>}</span></span>
<span></span>
<span><span>void</span><span> standard_free</span><span>(</span><span>void*</span><span> ptr</span><span>) {</span></span>
<span><span>    free</span><span>(ptr);</span></span>
<span><span>    printf</span><span>(</span><span>"[C] standard_free(</span><span>%p</span><span>)</span><span>\n</span><span>"</span><span>, ptr);</span></span>
<span><span>}</span></span></code></pre><p><span>c</span></p></div>
<p><strong>2. Debug allocator</strong> - <em>Adds magic values before and after user data to detect buffer overflows and corruption</em>:</p>
<blockquote>
<p><strong>Use case</strong>: Development and debugging, catching memory corruption early <br>
<strong>Pros</strong>: Detects buffer overflows, use-after-free, double-free <br>
<strong>Cons</strong>: ~20 bytes overhead per allocation, slower than standard malloc</p>
</blockquote>
<div tabindex="0" data-language="c"><pre><code><span><span>// debug_allocator.c</span></span>
<span><span>#define</span><span> MALLOC_MAGIC_HEADER</span><span> 0x</span><span>DEADBEEF</span><span>  // Classic magic number for "dead beef"</span></span>
<span><span>#define</span><span> MALLOC_MAGIC_FOOTER</span><span> 0x</span><span>CAFEBABE</span><span>  // Java's magic number, means "cafe babe"</span></span>
<span></span>
<span><span>typedef</span><span> struct</span><span> alloc_header {</span></span>
<span><span>    uint32_t</span><span> magic;</span></span>
<span><span>    size_t</span><span> size;</span></span>
<span><span>    uint32_t</span><span> flags;</span></span>
<span><span>    void*</span><span> debug_info;</span></span>
<span><span>} </span><span>alloc_header_t</span><span>;</span></span>
<span></span>
<span><span>void*</span><span> debug_malloc</span><span>(</span><span>size_t</span><span> size</span><span>) {</span></span>
<span><span>    size_t</span><span> total_size </span><span>=</span><span> sizeof</span><span>(</span><span>alloc_header_t</span><span>) </span><span>+</span><span> size </span><span>+</span><span> sizeof</span><span>(</span><span>uint32_t</span><span>);</span></span>
<span><span>    void*</span><span> raw_ptr </span><span>=</span><span> malloc</span><span>(total_size);</span></span>
<span><span>    </span></span>
<span><span>    if</span><span> (</span><span>!</span><span>raw_ptr) </span><span>return</span><span> NULL</span><span>;</span></span>
<span><span>    </span></span>
<span><span>    alloc_header_t</span><span>*</span><span> header </span><span>=</span><span> (</span><span>alloc_header_t</span><span>*</span><span>)raw_ptr;</span></span>
<span><span>    header-&gt;magic </span><span>=</span><span> MALLOC_MAGIC_HEADER;</span></span>
<span><span>    header-&gt;size </span><span>=</span><span> size;</span></span>
<span><span>    header-&gt;flags </span><span>=</span><span> 0</span><span>;</span></span>
<span><span>    </span></span>
<span><span>    // User pointer starts after header</span></span>
<span><span>    void*</span><span> user_ptr </span><span>=</span><span> (</span><span>char*</span><span>)raw_ptr </span><span>+</span><span> sizeof</span><span>(</span><span>alloc_header_t</span><span>);</span></span>
<span><span>    </span></span>
<span><span>    // Footer at the end</span></span>
<span><span>    uint32_t*</span><span> footer </span><span>=</span><span> (</span><span>uint32_t*</span><span>)((</span><span>char*</span><span>)user_ptr </span><span>+</span><span> size);</span></span>
<span><span>    *</span><span>footer </span><span>=</span><span> MALLOC_MAGIC_FOOTER;</span></span>
<span><span>    </span></span>
<span><span>    return</span><span> user_ptr;</span></span>
<span><span>}</span></span></code></pre><p><span>c</span></p></div>
<p>Memory layout for debug allocator:</p>
<div tabindex="0" data-language="plaintext"><pre><code><span><span>[HEADER: 16 bytes] [USER DATA: requested size] [FOOTER: 4 bytes]</span></span>
<span><span>├─ Magic (4B)      ├─ Your actual data        └─ Magic (4B)</span></span>
<span><span>├─ Size (8B)       │                              0xCAFEBABE</span></span>
<span><span>├─ Flags (4B)      │</span></span>
<span><span>└─ 0xDEADBEEF      └─ Returned pointer points here</span></span></code></pre><p><span>plaintext</span></p></div>
<p><strong>3. Direct mmap allocator</strong> - <em>Bypasses the heap entirely, requesting memory pages directly from the OS</em>:</p>
<blockquote>
<p><strong>Use case</strong>: Large allocations, security-sensitive code, custom memory management <br>
<strong>Pros</strong>: Isolated from heap corruption, guaranteed zeroed memory, can be marked read-only <br>
<strong>Cons</strong>: Minimum allocation is 4KB (page size), slow for small allocations</p>
</blockquote>
<div tabindex="0" data-language="c"><pre><code><span><span>// mmap_allocator.c</span></span>
<span><span>void*</span><span> mmap_malloc</span><span>(</span><span>size_t</span><span> size</span><span>) {</span></span>
<span><span>    size_t</span><span> page_size </span><span>=</span><span> sysconf</span><span>(_SC_PAGESIZE);</span></span>
<span><span>    size_t</span><span> alloc_size </span><span>=</span><span> ((size </span><span>+</span><span> page_size </span><span>-</span><span> 1</span><span>) </span><span>/</span><span> page_size) </span><span>*</span><span> page_size;</span></span>
<span><span>    </span></span>
<span><span>    void*</span><span> ptr </span><span>=</span><span> mmap</span><span>(</span><span>NULL</span><span>, alloc_size, PROT_READ </span><span>|</span><span> PROT_WRITE, </span></span>
<span><span>                     MAP_PRIVATE </span><span>|</span><span> MAP_ANONYMOUS, </span><span>-</span><span>1</span><span>, </span><span>0</span><span>);</span></span>
<span><span>    </span></span>
<span><span>    if</span><span> (ptr </span><span>==</span><span> MAP_FAILED) </span><span>return</span><span> NULL</span><span>;</span></span>
<span><span>    </span></span>
<span><span>    // Store size in first 8 bytes</span></span>
<span><span>    *</span><span>((</span><span>size_t*</span><span>)ptr) </span><span>=</span><span> alloc_size;</span></span>
<span><span>    return</span><span> (</span><span>char*</span><span>)ptr </span><span>+</span><span> sizeof</span><span>(</span><span>size_t</span><span>);</span></span>
<span><span>}</span></span></code></pre><p><span>c</span></p></div>
<p><strong>4. Arena allocator</strong> - <em>A bump allocator that allocates from a large pool and frees everything at once</em>:</p>
<blockquote>
<p><strong>Use case</strong>: Temporary allocations, parsing, per-request memory in servers <br>
<strong>Pros</strong>: Extremely fast allocation (just pointer bump), no fragmentation, cache-friendly <br>
<strong>Cons</strong>: Can’t free individual allocations, may waste memory</p>
</blockquote>
<div tabindex="0" data-language="c"><pre><code><span><span>// arena_allocator.c</span></span>
<span><span>typedef</span><span> struct</span><span> arena {</span></span>
<span><span>    void*</span><span> memory;</span></span>
<span><span>    size_t</span><span> size;</span></span>
<span><span>    size_t</span><span> used;</span></span>
<span><span>    struct</span><span> arena</span><span>*</span><span> next;</span></span>
<span><span>} </span><span>arena_t</span><span>;</span></span>
<span></span>
<span><span>void*</span><span> arena_malloc</span><span>(</span><span>size_t</span><span> size</span><span>) {</span></span>
<span><span>    // Align to 8 bytes - required for 64-bit pointers and doubles</span></span>
<span><span>    // Formula: (size + 7) &amp; ~7 rounds up to next multiple of 8</span></span>
<span><span>    size </span><span>=</span><span> (size </span><span>+</span><span> 7</span><span>) </span><span>&amp;</span><span> ~</span><span>7</span><span>;</span></span>
<span><span>    </span></span>
<span><span>    if</span><span> (</span><span>!</span><span>g_arena </span><span>||</span><span> g_arena-&gt;used </span><span>+</span><span> size </span><span>&gt;</span><span> g_arena-&gt;size) {</span></span>
<span><span>        // Need new arena</span></span>
<span><span>        arena_t</span><span>*</span><span> new_arena </span><span>=</span><span> malloc</span><span>(</span><span>sizeof</span><span>(</span><span>arena_t</span><span>));</span></span>
<span><span>        new_arena-&gt;memory </span><span>=</span><span> malloc</span><span>(</span><span>1024</span><span> *</span><span> 1024</span><span>);</span><span> // 1MB chunks</span></span>
<span><span>        new_arena-&gt;size </span><span>=</span><span> 1024</span><span> *</span><span> 1024</span><span>;</span></span>
<span><span>        new_arena-&gt;used </span><span>=</span><span> 0</span><span>;</span></span>
<span><span>        new_arena-&gt;next </span><span>=</span><span> g_arena;</span></span>
<span><span>        g_arena </span><span>=</span><span> new_arena;</span></span>
<span><span>    }</span></span>
<span><span>    </span></span>
<span><span>    void*</span><span> ptr </span><span>=</span><span> (</span><span>char*</span><span>)g_arena-&gt;memory </span><span>+</span><span> g_arena-&gt;used;</span></span>
<span><span>    g_arena-&gt;used </span><span>+=</span><span> size;</span></span>
<span><span>    return</span><span> ptr;</span></span>
<span><span>}</span></span></code></pre><p><span>c</span></p></div>
<h3 id="creating-safe-crash-tests">Creating Safe Crash Tests<a href="#creating-safe-crash-tests">#</a></h3>
<p>The most challenging part was creating tests that could crash safely and provide useful diagnostics. Since mixing allocators can cause segmentation faults, I needed to isolate each test in a subprocess:</p>
<blockquote>
<p>📊 <strong>Why Subprocess Isolation?</strong></p>
<ul>
<li><strong>Main process safety</strong>: Crashes in subprocess don’t kill the test harness</li>
<li><strong>Exit code capture</strong>: Can detect SIGSEGV (-11) vs SIGABRT (-6) vs success (0)</li>
<li><strong>Output collection</strong>: Capture stdout/stderr even when process crashes</li>
<li><strong>Timeout protection</strong>: Prevent infinite loops with <code>timeout</code> command</li>
</ul>
</blockquote>
<div tabindex="0" data-language="rust"><pre><code><span><span>// crash_tests.rs</span></span>
<span><span>use</span><span> std</span><span>::</span><span>process</span><span>::</span><span>{</span><span>Command</span><span>, </span><span>Stdio</span><span>};</span></span>
<span><span>use</span><span> std</span><span>::</span><span>io</span><span>::</span><span>Write</span><span>;</span></span>
<span></span>
<span><span>// Note: Crash test subprocess management is handled by tools/run_crash_tests.sh</span></span>
<span><span>// This bash script approach provides better isolation and exit code handling.</span></span>
<span></span>
<span><span>// The actual crash tests are implemented in crash_tests.rs:</span></span>
<span><span>fn</span><span> test_rust_free_c_malloc</span><span>() {</span></span>
<span><span>    println!</span><span>(</span><span>"=== Test: Rust dealloc on C malloc ==="</span><span>);</span></span>
<span><span>    </span></span>
<span><span>    unsafe</span><span> {</span></span>
<span><span>        let</span><span> ptr </span><span>=</span><span> standard_malloc</span><span>(</span><span>64</span><span>);</span></span>
<span><span>        println!</span><span>(</span><span>"C malloc returned: {:p}"</span><span>, ptr);</span></span>
<span><span>        </span></span>
<span><span>        // This is UNDEFINED BEHAVIOR - mixing allocators!</span></span>
<span><span>        let</span><span> layout </span><span>=</span><span> Layout</span><span>::</span><span>from_size_align</span><span>(</span><span>64</span><span>, </span><span>8</span><span>)</span><span>.</span><span>unwrap</span><span>();</span></span>
<span><span>        println!</span><span>(</span><span>"Attempting Rust dealloc with layout: {:?}"</span><span>, layout);</span></span>
<span><span>        std</span><span>::</span><span>alloc</span><span>::</span><span>dealloc</span><span>(ptr </span><span>as</span><span> *mut</span><span> u8</span><span>, layout);</span></span>
<span><span>        </span></span>
<span><span>        println!</span><span>(</span><span>"If you see this, it didn't crash immediately..."</span><span>);</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre><p><span>rust</span></p></div>
<p>The crash test script (<code>tools/run_crash_tests.sh</code>) runs each test with timeout protection:</p>
<div tabindex="0" data-language="bash"><pre><code><span><span>timeout</span><span> 5</span><span> ./target/release/crash_tests</span><span> $test </span><span>&gt;&gt;</span><span> $OUTPUT_FILE </span><span>2&gt;&amp;1</span></span>
<span><span>EXIT_CODE</span><span>=</span><span>$?</span></span>
<span></span>
<span><span>case</span><span> $EXIT_CODE </span><span>in</span></span>
<span><span>    0</span><span>)</span></span>
<span><span>        echo</span><span> "Result: NO CRASH (dangerous - undefined behavior likely)"</span></span>
<span><span>        ;;</span></span>
<span><span>    134</span><span>)</span></span>
<span><span>        echo</span><span> "Result: SIGABRT (allocator detected corruption)"</span></span>
<span><span>        ;;</span></span>
<span><span>    139</span><span>)</span></span>
<span><span>        echo</span><span> "Result: SIGSEGV (segmentation fault)"</span></span>
<span><span>        ;;</span></span>
<span><span>esac</span></span></code></pre><p><span>bash</span></p></div>
<h2 id="first-experiments-surprising-results">First Experiments: Surprising Results<a href="#first-experiments-surprising-results">#</a></h2>
<p>With the laboratory built, it was time to start experimenting. My first test was the obvious one - what happens when you mix allocators?</p>
<h3 id="experiment-1-the-basic-mix">Experiment 1: The Basic Mix<a href="#experiment-1-the-basic-mix">#</a></h3>
<p>To test allocator mixing safely, I ran each test in a subprocess to catch crashes:</p>
<div tabindex="0" data-language="rust"><pre><code><span><span>// From our test harness</span></span>
<span><span>fn</span><span> test_allocator_mixing</span><span>() {</span></span>
<span><span>    let</span><span> child </span><span>=</span><span> Command</span><span>::</span><span>new</span><span>(</span><span>"./test_binary"</span><span>)</span></span>
<span><span>        .</span><span>arg</span><span>(</span><span>"mix_allocators"</span><span>)</span></span>
<span><span>        .</span><span>output</span><span>()</span></span>
<span><span>        .</span><span>expect</span><span>(</span><span>"Failed to execute test"</span><span>);</span></span>
<span><span>    </span></span>
<span><span>    // In the subprocess:</span></span>
<span><span>    unsafe</span><span> fn</span><span> mix_allocators</span><span>() {</span></span>
<span><span>        let</span><span> c_ptr </span><span>=</span><span> libc</span><span>::</span><span>malloc</span><span>(</span><span>64</span><span>);</span></span>
<span><span>        println!</span><span>(</span><span>"C malloc returned: {:p}"</span><span>, c_ptr);</span></span>
<span><span>        </span></span>
<span><span>        let</span><span> layout </span><span>=</span><span> Layout</span><span>::</span><span>from_size_align</span><span>(</span><span>64</span><span>, </span><span>8</span><span>)</span><span>.</span><span>unwrap</span><span>();</span></span>
<span><span>        std</span><span>::</span><span>alloc</span><span>::</span><span>dealloc</span><span>(c_ptr </span><span>as</span><span> *mut</span><span> u8</span><span>, layout);</span></span>
<span><span>        </span></span>
<span><span>        println!</span><span>(</span><span>"If you see this, we got lucky..."</span><span>);</span></span>
<span><span>    }</span></span>
<span><span>    </span></span>
<span><span>    let</span><span> exit_code </span><span>=</span><span> child</span><span>.</span><span>status</span><span>.</span><span>code</span><span>()</span><span>.</span><span>unwrap_or</span><span>(</span><span>-</span><span>1</span><span>);</span></span>
<span><span>}</span></span></code></pre><p><span>rust</span></p></div>
<p>I expected an immediate crash. What I got surprised me:</p>
<div tabindex="0" data-language="plaintext"><pre><code><span><span>C malloc returned: 0x55cd332f5be0</span></span>
<span><span>Attempting Rust dealloc with layout: Layout { size: 64, align: 8 }</span></span>
<span><span>If you see this, it didn't crash immediately...</span></span>
<span><span></span></span>
<span><span>Exit code: 0</span></span></code></pre><p><span>plaintext</span></p></div>
<p>Remember our exit code table? Exit code 0 is the <strong>worst possible outcome</strong>. The program continued with corrupted heap metadata - a silent time bomb.</p>
<blockquote>
<p>🔥 <strong>DANGER: Exit Code 0 with Memory Corruption</strong></p>
<p>This is a nightmare scenario:</p>
<ul>
<li>✅ Your tests pass</li>
<li>✅ Your program runs “normally”</li>
<li>❌ Heap metadata is silently corrupted</li>
<li>❌ Random crashes will occur later</li>
<li>❌ Data corruption is unpredictable</li>
<li>❌ Security vulnerabilities are introduced</li>
</ul>
<p>A crash (SIGSEGV/SIGABRT) is actually the <strong>safe</strong> outcome!</p>
</blockquote>
<p>Let’s understand why this happened instead of crashing immediately.</p>
<h3 id="experiment-2-understanding-the-non-crash">Experiment 2: Understanding the Non-Crash<a href="#experiment-2-understanding-the-non-crash">#</a></h3>
<p>Why didn’t it crash? Time for some detective work. I needed to peek at the raw memory around our allocation to understand glibc’s metadata structure.</p>
<blockquote>
<p>📊 <strong>Tools Used for Memory Inspection:</strong></p>
<ul>
<li><strong>Memory access</strong>: <code>std::slice::from_raw_parts</code> - Rust’s way to view raw memory as a byte slice</li>
<li><strong>Offset calculation</strong>: <code>pointer.offset(-16)</code> - Look 16 bytes before the returned pointer</li>
<li><strong>Why -16?</strong>: glibc stores chunk metadata in the 8-16 bytes before user data</li>
<li><strong>Run command</strong>: <code>./tools/deep_analysis.sh</code> (dynamically generates and runs analysis code)</li>
</ul>
</blockquote>
<div tabindex="0" data-language="rust"><pre><code><span><span>// deep_analysis.sh dynamically generates this analysis code:</span></span>
<span><span>fn</span><span> analyze_glibc_malloc_internals</span><span>() {</span></span>
<span><span>    unsafe</span><span> {</span></span>
<span><span>        // Allocate different sizes to trigger different paths</span></span>
<span><span>        let</span><span> small </span><span>=</span><span> libc</span><span>::</span><span>malloc</span><span>(</span><span>24</span><span>);      </span><span>// Fastbin</span></span>
<span><span>        let</span><span> medium </span><span>=</span><span> libc</span><span>::</span><span>malloc</span><span>(</span><span>512</span><span>);    </span><span>// Smallbin  </span></span>
<span><span>        let</span><span> large </span><span>=</span><span> libc</span><span>::</span><span>malloc</span><span>(</span><span>131072</span><span>);  </span><span>// Large bin or mmap</span></span>
<span><span>        </span></span>
<span><span>        // Peek at malloc chunk headers (glibc specific)</span></span>
<span><span>        // Chunk format: size | flags in lowest 3 bits</span></span>
<span><span>        if</span><span> !</span><span>small</span><span>.</span><span>is_null</span><span>() {</span></span>
<span><span>            let</span><span> chunk_ptr </span><span>=</span><span> (small </span><span>as</span><span> *mut</span><span> usize</span><span>)</span><span>.</span><span>offset</span><span>(</span><span>-</span><span>1</span><span>);</span></span>
<span><span>            let</span><span> chunk_size </span><span>=</span><span> *</span><span>chunk_ptr </span><span>&amp;</span><span> !</span><span>0x7</span><span>;</span></span>
<span><span>            let</span><span> flags </span><span>=</span><span> *</span><span>chunk_ptr </span><span>&amp;</span><span> 0x7</span><span>;</span></span>
<span><span>            </span></span>
<span><span>            println!</span><span>(</span><span>"Small chunk header:"</span><span>);</span></span>
<span><span>            println!</span><span>(</span><span>"  Size: {} (0x{:x})"</span><span>, chunk_size, chunk_size);</span></span>
<span><span>            println!</span><span>(</span><span>"  Flags: 0x{:x}"</span><span>, flags);</span></span>
<span><span>            println!</span><span>(</span><span>"    PREV_INUSE: {}"</span><span>, flags </span><span>&amp;</span><span> 0x1</span><span> !=</span><span> 0</span><span>);</span></span>
<span><span>            println!</span><span>(</span><span>"    IS_MMAPPED: {}"</span><span>, flags </span><span>&amp;</span><span> 0x2</span><span> !=</span><span> 0</span><span>);</span></span>
<span><span>        }</span></span>
<span><span>        </span></span>
<span><span>        libc</span><span>::</span><span>free</span><span>(small);</span></span>
<span><span>        libc</span><span>::</span><span>free</span><span>(medium);</span></span>
<span><span>        libc</span><span>::</span><span>free</span><span>(large);</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre><p><span>rust</span></p></div>
<p>To run this analysis:</p>
<div tabindex="0" data-language="bash"><pre><code><span><span>cd</span><span> rust-ffi</span></span>
<span><span>export</span><span> LD_LIBRARY_PATH</span><span>=</span><span>../c-lib:$LD_LIBRARY_PATH</span></span>
<span><span>cargo</span><span> run</span><span> --release</span><span> --bin</span><span> deep_analysis</span></span></code></pre><p><span>bash</span></p></div>
<p>This revealed glibc’s metadata structure:</p>
<div tabindex="0" data-language="plaintext"><pre><code><span><span>Memory layout around allocation:</span></span>
<span><span>Offset -16 to -1 (before user ptr):</span></span>
<span><span>00 00 00 00 00 00 00 00 51 00 00 00 00 00 00 00</span></span>
<span><span>Offset 0 to 15 (user data):</span></span>
<span><span>00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</span></span></code></pre><p><span>plaintext</span></p></div>
<p>That <code>0x51</code> at offset -8 is the key. Let me break it down:</p>
<ul>
<li>Bottom 3 bits are flags:
<ul>
<li>Bit 0 (0x1): PREV_INUSE - previous chunk is allocated</li>
<li>Bit 1 (0x2): IS_MMAPPED - chunk from mmap (not set here)</li>
<li>Bit 2 (0x4): NON_MAIN_ARENA - from thread arena (not set)</li>
</ul>
</li>
<li>Upper bits: 0x50 = 80 bytes total chunk size</li>
</ul>
<p>So: User requested 64 bytes, glibc allocated an 80-byte chunk (16 bytes metadata overhead).</p>
<p>When Rust’s allocator looked for its metadata at a different offset, it found zeros - which by pure chance didn’t trigger an immediate crash. But the heap is now corrupted, and any subsequent allocation could fail catastrophically.</p>
<h3 id="experiment-3-the-allocator-matrix">Experiment 3: The Allocator Matrix<a href="#experiment-3-the-allocator-matrix">#</a></h3>
<p>I systematically tested every combination:</p>
<div tabindex="0" data-language="rust"><pre><code><span><span>// allocator_matrix.rs</span></span>
<span><span>fn</span><span> test_allocator_mixing</span><span>() {</span></span>
<span><span>    let</span><span> allocators </span><span>=</span><span> vec!</span><span>[</span><span>"standard"</span><span>, </span><span>"debug"</span><span>, </span><span>"mmap"</span><span>, </span><span>"arena"</span><span>];</span></span>
<span><span>    let</span><span> mut</span><span> results </span><span>=</span><span> Vec</span><span>::</span><span>new</span><span>();</span></span>
<span><span>    </span></span>
<span><span>    for</span><span> alloc </span><span>in</span><span> &amp;</span><span>allocators {</span></span>
<span><span>        for</span><span> dealloc </span><span>in</span><span> &amp;</span><span>allocators {</span></span>
<span><span>            if</span><span> alloc </span><span>!=</span><span> dealloc {</span></span>
<span><span>                let</span><span> result </span><span>=</span><span> test_mix</span><span>(alloc, dealloc);</span></span>
<span><span>                results</span><span>.</span><span>push</span><span>(result);</span></span>
<span><span>            }</span></span>
<span><span>        }</span></span>
<span><span>    }</span></span>
<span><span>    </span></span>
<span><span>    // Print results matrix</span></span>
<span><span>    println!</span><span>(</span><span>"</span><span>\n</span><span>Allocator Mixing Results:"</span><span>);</span></span>
<span><span>    println!</span><span>(</span><span>"Alloc with → Free with = Result"</span><span>);</span></span>
<span><span>    println!</span><span>(</span><span>"─────────────────────────────────"</span><span>);</span></span>
<span><span>    </span></span>
<span><span>    for</span><span> result </span><span>in</span><span> results {</span></span>
<span><span>        println!</span><span>(</span><span>"{:10} → {:10} = {:?}"</span><span>, </span></span>
<span><span>                 result</span><span>.</span><span>allocator, </span></span>
<span><span>                 result</span><span>.</span><span>deallocator, </span></span>
<span><span>                 result</span><span>.</span><span>outcome);</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre><p><span>rust</span></p></div>
<p>The results painted a clear picture:</p>
<div tabindex="0" data-language="plaintext"><pre><code><span><span>Allocator Mixing Results:</span></span>
<span><span>Alloc with → Free with = Result</span></span>
<span><span>─────────────────────────────────</span></span>
<span><span>standard   → debug      = CRASH (Abort: invalid magic number)</span></span>
<span><span>standard   → mmap       = CRASH (Segfault: munmap on malloc'd memory)</span></span>
<span><span>standard   → arena      = NO-OP (arena doesn't free individual chunks)</span></span>
<span><span>debug      → standard   = CRASH (Segfault: bad metadata offset)</span></span>
<span><span>debug      → mmap       = CRASH (Segfault: munmap on malloc'd memory)</span></span>
<span><span>debug      → arena      = NO-OP</span></span>
<span><span>mmap       → standard   = CRASH (Abort: free on mmap'd memory)</span></span>
<span><span>mmap       → debug      = CRASH (Abort: bad magic number)</span></span>
<span><span>mmap       → arena      = NO-OP</span></span>
<span><span>arena      → standard   = CRASH (double free when arena resets)</span></span>
<span><span>arena      → debug      = CRASH (Abort: bad magic number)</span></span>
<span><span>arena      → mmap       = CRASH (Segfault: munmap on malloc'd memory)</span></span></code></pre><p><span>plaintext</span></p></div>
<blockquote>
<p><strong>Update</strong>: Our actual crash tests revealed a more nuanced reality:</p>
<ul>
<li><strong>Rust/C mixing often doesn’t crash immediately</strong> (Exit code 0)</li>
<li><strong>Only certain combinations trigger immediate detection</strong> (like double_free)</li>
<li><strong>Silent corruption is the most common outcome</strong> - far more dangerous than crashes</li>
</ul>
</blockquote>
<p>Key insights:</p>
<ul>
<li>Debug allocator’s magic number checks catch corruption fastest (SIGABRT)</li>
<li>Standard/mmap mixing fails at the syscall level (SIGSEGV)</li>
<li>Arena allocator’s NO-OP behavior creates memory leaks</li>
<li>Every non-matching combination eventually fails - it’s just a matter of when</li>
</ul>
<h3 id="experiment-4-size-class-discovery">Experiment 4: Size Class Discovery<a href="#experiment-4-size-class-discovery">#</a></h3>
<blockquote>
<p><strong>What are size classes?</strong> Memory allocators don’t allocate exact byte amounts. Instead, they round up to predefined “size classes” to reduce fragmentation and improve performance. For example, if you request 20 bytes, you might actually get 24 bytes. This standardization allows the allocator to efficiently reuse freed chunks and maintain free lists for common sizes.</p>
</blockquote>
<p>One fascinating discovery was how allocators organize memory into these size classes. I used glibc’s <code>malloc_usable_size()</code> function to discover the actual allocated sizes:</p>
<blockquote>
<p>📊 <strong>Tools for Size Class Discovery:</strong></p>
<ul>
<li><strong>Function</strong>: <code>libc::malloc_usable_size()</code> - Returns actual allocated size</li>
<li><strong>Platform</strong>: Linux-specific (requires <code>#[cfg(target_os = "linux")]</code>)</li>
<li><strong>Method</strong>: Allocate every size from 1-256 bytes, track when actual size changes</li>
<li><strong>Purpose</strong>: Understand memory overhead and fragmentation</li>
</ul>
</blockquote>
<div tabindex="0" data-language="rust"><pre><code><span><span>// size_classes.rs - Part of comprehensive_tests</span></span>
<span><span>fn</span><span> discover_size_classes</span><span>() {</span></span>
<span><span>    println!</span><span>(</span><span>"Discovering allocator size classes...</span><span>\n</span><span>"</span><span>);</span></span>
<span><span>    </span></span>
<span><span>    let</span><span> mut</span><span> size_to_actual </span><span>=</span><span> HashMap</span><span>::</span><span>new</span><span>();</span></span>
<span><span>    </span></span>
<span><span>    for</span><span> size </span><span>in</span><span> 1</span><span>..=</span><span>256</span><span> {</span></span>
<span><span>        unsafe</span><span> {</span></span>
<span><span>            let</span><span> ptr </span><span>=</span><span> libc</span><span>::</span><span>malloc</span><span>(size);</span></span>
<span><span>            </span></span>
<span><span>            #[cfg(target_os </span><span>=</span><span> "linux"</span><span>)]</span></span>
<span><span>            {</span></span>
<span><span>                // This function reveals the actual chunk size</span></span>
<span><span>                let</span><span> actual </span><span>=</span><span> libc</span><span>::</span><span>malloc_usable_size</span><span>(ptr) </span><span>as</span><span> usize</span><span>;</span></span>
<span><span>                size_to_actual</span><span>.</span><span>insert</span><span>(size, actual);</span></span>
<span><span>            }</span></span>
<span><span>            </span></span>
<span><span>            libc</span><span>::</span><span>free</span><span>(ptr);</span></span>
<span><span>        }</span></span>
<span><span>    }</span></span>
<span><span>    </span></span>
<span><span>    // Find size class boundaries</span></span>
<span><span>    let</span><span> mut</span><span> current_class </span><span>=</span><span> 0</span><span>;</span></span>
<span><span>    for</span><span> size </span><span>in</span><span> 1</span><span>..=</span><span>256</span><span> {</span></span>
<span><span>        let</span><span> actual </span><span>=</span><span> size_to_actual[</span><span>&amp;</span><span>size];</span></span>
<span><span>        if</span><span> actual </span><span>!=</span><span> current_class {</span></span>
<span><span>            println!</span><span>(</span><span>"Size class boundary at {} bytes → {} bytes actual"</span><span>, </span></span>
<span><span>                     size, actual);</span></span>
<span><span>            current_class </span><span>=</span><span> actual;</span></span>
<span><span>        }</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre><p><span>rust</span></p></div>
<p>To run this analysis:</p>
<div tabindex="0" data-language="bash"><pre><code><span><span>./target/release/comprehensive_tests</span><span> |</span><span> grep</span><span> "Size class"</span></span></code></pre><p><span>bash</span></p></div>
<p>Results showed glibc’s size class optimization:</p>
<div tabindex="0" data-language="plaintext"><pre><code><span><span>Size class boundary at 1 bytes → 24 bytes actual</span></span>
<span><span>Size class boundary at 25 bytes → 40 bytes actual</span></span>
<span><span>Size class boundary at 41 bytes → 56 bytes actual</span></span>
<span><span>Size class boundary at 57 bytes → 72 bytes actual</span></span>
<span><span>Size class boundary at 73 bytes → 88 bytes actual</span></span>
<span><span>...</span></span></code></pre><p><span>plaintext</span></p></div>
<blockquote>
<p>⚠️ <strong>The 2300% Overhead</strong></p>
<p>The minimum allocation is 24 bytes - even for a single byte! This 2300% overhead for tiny allocations explains why pooling small objects is so important.</p>
</blockquote>
<h3 id="hidden-danger-use-after-free-data-persistence">Hidden Danger: Use-After-Free Data Persistence<a href="#hidden-danger-use-after-free-data-persistence">#</a></h3>
<p>One of the most surprising discoveries was how much data survives after <code>free()</code>. I tested this by filling memory with a pattern, freeing it, then immediately reallocating to see what remained:</p>
<blockquote>
<p>📊 <strong>Use-After-Free Analysis Method:</strong></p>
<ul>
<li><strong>Pattern</strong>: Fill with incrementing bytes (0x00, 0x01, 0x02…)</li>
<li><strong>Test</strong>: Free the memory, immediately allocate same size</li>
<li><strong>Detection</strong>: Compare byte-by-byte to see what survived</li>
<li><strong>Tool</strong>: Part of <code>deep_analysis</code> binary, see Experiment 2.3 in EXPERIMENTS.md</li>
</ul>
</blockquote>
<div tabindex="0" data-language="c"><pre><code><span><span>// From EXPERIMENTS.md - Experiment 2.3</span></span>
<span><span>void</span><span> analyze_use_after_free</span><span>() {</span></span>
<span><span>    uint8_t*</span><span> ptr </span><span>=</span><span> malloc</span><span>(</span><span>64</span><span>);</span></span>
<span><span>    </span></span>
<span><span>    // Fill with recognizable pattern</span></span>
<span><span>    for</span><span> (</span><span>size_t</span><span> i </span><span>=</span><span> 0</span><span>; i </span><span>&lt;</span><span> 64</span><span>; i</span><span>++</span><span>) {</span></span>
<span><span>        ptr</span><span>[i] </span><span>=</span><span> (</span><span>uint8_t</span><span>)(i </span><span>&amp;</span><span> 0x</span><span>FF</span><span>);</span></span>
<span><span>    }</span></span>
<span><span>    </span></span>
<span><span>    free</span><span>(ptr);</span></span>
<span><span>    </span></span>
<span><span>    // Immediately allocate same size</span></span>
<span><span>    uint8_t*</span><span> new_ptr </span><span>=</span><span> malloc</span><span>(</span><span>64</span><span>);</span></span>
<span><span>    </span></span>
<span><span>    if</span><span> (new_ptr </span><span>==</span><span> ptr) {</span><span>  // Often get same address back</span></span>
<span><span>        // Count surviving bytes...</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre><p><span>c</span></p></div>
<p>In our tests:</p>
<div tabindex="0" data-language="plaintext"><pre><code><span><span>Size 64: 48/64 bytes survived (75.0%)</span></span>
<span><span>First 32 bytes after free:</span></span>
<span><span>00 00 00 00 00 00 00 00 20 6e 56 3f fc 7f 00 00  &lt;- Free list pointers</span></span>
<span><span>10 11 12 13 14 15 16 17 18 19 1a 1b 1c 1d 1e 1f  &lt;- Original data intact!</span></span></code></pre><p><span>plaintext</span></p></div>
<p>Only the first 16 bytes get overwritten with free list management pointers. The rest of your data remains in memory, readable by any subsequent allocation that reuses this chunk. This is a severe security risk - sensitive data like passwords or keys can persist long after being “freed”.</p>
<h3 id="experiment-5-performance-baselines">Experiment 5: Performance Baselines<a href="#experiment-5-performance-baselines">#</a></h3>
<p>Before diving into complex performance analysis (coming in Part 3), I established baselines using our performance analysis tools:</p>
<blockquote>
<p>📊 <strong>Performance Measurement Tools:</strong></p>
<ul>
<li><strong>Timing</strong>: <code>std::time::Instant</code> for high-resolution timing</li>
<li><strong>Warmup</strong>: 1000 allocations to prime the allocator caches</li>
<li><strong>Statistical method</strong>: 100,000 iterations, take median of 5 runs</li>
<li><strong>CPU isolation</strong>: Disabled frequency scaling, pinned to specific cores</li>
<li><strong>Script</strong>: <code>tools/perf_analysis.sh</code> automates the full benchmark</li>
</ul>
</blockquote>
<div tabindex="0" data-language="rust"><pre><code><span><span>// perf_analysis.sh dynamically generates performance benchmarking code:</span></span>
<span><span>fn</span><span> benchmark_allocator</span><span>&lt;</span><span>F</span><span>, </span><span>G</span><span>&gt;(name</span><span>:</span><span> &amp;</span><span>str</span><span>, alloc_fn</span><span>:</span><span> F</span><span>, free_fn</span><span>:</span><span> G</span><span>, size</span><span>:</span><span> usize</span><span>)</span></span>
<span><span>where</span></span>
<span><span>    F</span><span>:</span><span> Fn</span><span>(</span><span>usize</span><span>) </span><span>-&gt;</span><span> *mut</span><span> c_void,</span></span>
<span><span>    G</span><span>:</span><span> Fn</span><span>(</span><span>*mut</span><span> c_void),</span></span>
<span><span>{</span></span>
<span><span>    const</span><span> ITERATIONS</span><span>:</span><span> usize</span><span> =</span><span> 100_000</span><span>;</span></span>
<span><span>    </span></span>
<span><span>    // Warmup</span></span>
<span><span>    for</span><span> _ </span><span>in</span><span> 0</span><span>..</span><span>1000</span><span> {</span></span>
<span><span>        let</span><span> ptr </span><span>=</span><span> alloc_fn</span><span>(size);</span></span>
<span><span>        if</span><span> !</span><span>ptr</span><span>.</span><span>is_null</span><span>() {</span></span>
<span><span>            free_fn</span><span>(ptr);</span></span>
<span><span>        }</span></span>
<span><span>    }</span></span>
<span><span>    </span></span>
<span><span>    // Actual benchmark</span></span>
<span><span>    let</span><span> start </span><span>=</span><span> Instant</span><span>::</span><span>now</span><span>();</span></span>
<span><span>        </span></span>
<span><span>        let</span><span> mut</span><span> pointers </span><span>=</span><span> Vec</span><span>::</span><span>with_capacity</span><span>(iterations);</span></span>
<span><span>        for</span><span> _ </span><span>in</span><span> 0</span><span>..</span><span>iterations {</span></span>
<span><span>            unsafe</span><span> {</span></span>
<span><span>                let</span><span> ptr </span><span>=</span><span> libc</span><span>::</span><span>malloc</span><span>(size);</span></span>
<span><span>                pointers</span><span>.</span><span>push</span><span>(ptr);</span></span>
<span><span>            }</span></span>
<span><span>        }</span></span>
<span><span>        </span></span>
<span><span>        let</span><span> alloc_time </span><span>=</span><span> start</span><span>.</span><span>elapsed</span><span>();</span></span>
<span><span>        let</span><span> alloc_rate </span><span>=</span><span> iterations </span><span>as</span><span> f64</span><span> /</span><span> alloc_time</span><span>.</span><span>as_secs_f64</span><span>();</span></span>
<span><span>        </span></span>
<span><span>        let</span><span> start </span><span>=</span><span> Instant</span><span>::</span><span>now</span><span>();</span></span>
<span><span>        for</span><span> ptr </span><span>in</span><span> pointers {</span></span>
<span><span>            unsafe</span><span> {</span></span>
<span><span>                libc</span><span>::</span><span>free</span><span>(ptr);</span></span>
<span><span>            }</span></span>
<span><span>        }</span></span>
<span><span>        </span></span>
<span><span>        let</span><span> free_time </span><span>=</span><span> start</span><span>.</span><span>elapsed</span><span>();</span></span>
<span><span>        let</span><span> free_rate </span><span>=</span><span> iterations </span><span>as</span><span> f64</span><span> /</span><span> free_time</span><span>.</span><span>as_secs_f64</span><span>();</span></span>
<span><span>        </span></span>
<span><span>        println!</span><span>(</span><span>"Size {:5}: {:7.1}M allocs/sec, {:7.1}M frees/sec"</span><span>,</span></span>
<span><span>                 size, alloc_rate </span><span>/</span><span> 1_000_000.0</span><span>, free_rate </span><span>/</span><span> 1_000_000.0</span><span>);</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre><p><span>rust</span></p></div>
<p>To reproduce these measurements:</p>
<div tabindex="0" data-language="bash"><pre><code><span><span>cd</span><span> rust-ffi</span></span>
<span><span>cargo</span><span> build</span><span> --release</span><span> --bin</span><span> perf_test</span></span>
<span><span>export</span><span> LD_LIBRARY_PATH</span><span>=</span><span>../c-lib:$LD_LIBRARY_PATH</span></span>
<span><span>./target/release/perf_test</span></span></code></pre><p><span>bash</span></p></div>
<p>Initial results from our testing:</p>
<div tabindex="0" data-language="plaintext"><pre><code><span><span>Size    16: 17.1M allocs/sec, 32.3M frees/sec (58.3ns alloc, 31.0ns free)</span></span>
<span><span>Size    64: 12.8M allocs/sec, 31.9M frees/sec (78.0ns alloc, 31.3ns free)</span></span>
<span><span>Size   256:  5.6M allocs/sec,  9.3M frees/sec (177ns alloc, 107ns free)</span></span>
<span><span>Size  1024:  2.0M allocs/sec,  5.3M frees/sec (490ns alloc, 188ns free)</span></span>
<span><span>Size  4096:  0.5M allocs/sec,  2.3M frees/sec (1.9μs alloc, 428ns free)</span></span></code></pre><p><span>plaintext</span></p></div>
<p>Key observations:</p>
<ul>
<li>Small allocations are incredibly fast due to thread-local caching (tcache)</li>
<li>Free is consistently 2-6x faster than allocation</li>
<li>Performance degrades with size due to cache misses and syscalls for large allocations</li>
</ul>
<blockquote>
<p><strong>What is tcache?</strong> Thread-local cache (tcache) is glibc’s optimization that gives each thread its own small cache of recently freed chunks. This avoids lock contention and makes small allocations extremely fast - no need to access the global heap. Chunks up to 1032 bytes (64 chunks × 7 size classes) can be cached per thread.</p>
</blockquote>
<ul>
<li>But beware: these are best-case numbers with perfect cache conditions!</li>
</ul>
<h2 id="key-takeaways-and-whats-next">Key Takeaways and What’s Next<a href="#key-takeaways-and-whats-next">#</a></h2>
<p>This first part of our journey revealed several critical insights:</p>
<blockquote>
<p>💡 <strong>Key Insights from Our Experiments</strong></p>
<ol>
<li>
<p><strong>Exit Code 0 is the enemy</strong> - Our tests showed that mixing allocators often doesn’t crash immediately (exit code 0), creating silent corruption that’s far more dangerous than an immediate segfault</p>
</li>
<li>
<p><strong>Metadata tells the story</strong> - That <code>0x51</code> value revealed glibc stores size (0x50) + flags (0x1) before each allocation. Different allocators expect metadata at different offsets, causing the mixing failures</p>
</li>
<li>
<p><strong>Memory overhead is shocking</strong> - A 1-byte allocation consumes 24 bytes (2300% overhead!). Understanding size classes is crucial for efficient memory use</p>
</li>
<li>
<p><strong>Data persists after free</strong> - 75% of freed memory remains intact, creating serious security risks. Only the first 16 bytes get overwritten with free list pointers</p>
</li>
<li>
<p><strong>Cache effects dominate performance</strong> - False sharing caused an 8.67x slowdown in our tests. Memory layout matters as much as algorithm choice</p>
</li>
<li>
<p><strong>Every allocator combination fails differently</strong> - Our matrix showed debug allocators catch errors fastest (SIGABRT), while arena allocators silently leak memory</p>
</li>
</ol>
</blockquote>
<p>Going back to the interview question: “What happens if you allocate with malloc and free with Rust?”</p>
<p>Now we know: You’ll get exit code 0 (the dangerous silent corruption), followed by unpredictable crashes later. The only safe answer is “never do this.”</p>
<p>In <strong>Part 2</strong>, we’ll dive deeper with core dump analysis, explore how attackers exploit these vulnerabilities, and see what actually happens at the moment of crash. We’ll use gdb to trace through the exact instruction where things go wrong.</p>
<blockquote>
<p>🔍 <strong>Preview of Debugging Tools in Part 2:</strong></p>
<ul>
<li><strong>Core dumps</strong>: <code>ulimit -c unlimited</code> and analyzing with <code>gdb</code></li>
<li><strong>Memory inspection</strong>: <code>x/32gx $rsp</code> to examine stack contents</li>
<li><strong>Backtrace analysis</strong>: <code>bt full</code> to see the exact crash location</li>
<li><strong>LD_PRELOAD hooks</strong>: Intercept malloc/free to trace allocations</li>
</ul>
</blockquote>
<p>Stay tuned for <strong>Part 2</strong>, where things get really interesting - we’ll trigger crashes on purpose, analyze core dumps, and see what actually happens when allocators collide. Spoiler: it’s even messier than you might think.</p>
<hr>
<blockquote>
<p>📝 <strong>Repository &amp; Testing Environment</strong></p>
<p>All code from this series is available at <a href="https://github.com/notashes/rust-c-memory-interop" rel="nofollow noopener noreferrer" target="_blank">https://github.com/notashes/rust-c-memory-interop<span> ↗</span></a>.</p>
<p>Tests were conducted on:</p>
<ul>
<li>Linux 6.5</li>
<li>glibc 2.39</li>
<li>Rust 1.75</li>
<li>Intel Core i7</li>
</ul>
<p>Your crashes may vary, but the principles remain constant.</p>
</blockquote>
<h3 id="debugging-tips-when-things-go-wrong">Debugging Tips: When Things Go Wrong<a href="#debugging-tips-when-things-go-wrong">#</a></h3>
<p>When working with FFI and memory allocators, here are essential debugging techniques:</p>
<p><strong>1. Enable Address Sanitizer (ASan)</strong>:</p>
<div tabindex="0" data-language="bash"><pre><code><span><span># For C code</span></span>
<span><span>gcc</span><span> -fsanitize=address</span><span> -g</span><span> your_code.c</span></span>
<span></span>
<span><span># For Rust (in Cargo.toml)</span></span>
<span><span>[profile.dev]</span></span>
<span><span>opt-level</span><span> =</span><span> 0</span></span>
<span><span>debug</span><span> =</span><span> true</span></span></code></pre><p><span>bash</span></p></div>
<p><strong>2. Use Valgrind for memory leak detection</strong>:</p>
<div tabindex="0" data-language="bash"><pre><code><span><span>valgrind</span><span> --leak-check=full</span><span> --show-leak-kinds=all</span><span> ./your_program</span></span></code></pre><p><span>bash</span></p></div>
<p><strong>3. Core dump analysis</strong>:</p>
<div tabindex="0" data-language="bash"><pre><code><span><span># Enable core dumps</span></span>
<span><span>ulimit</span><span> -c</span><span> unlimited</span></span>
<span></span>
<span><span># After crash, analyze with gdb</span></span>
<span><span>gdb</span><span> ./your_program</span><span> core</span></span>
<span><span>(</span><span>gdb</span><span>) </span><span>bt</span><span> full</span><span>  # Full backtrace</span></span>
<span><span>(</span><span>gdb</span><span>) </span><span>info</span><span> registers</span></span>
<span><span>(</span><span>gdb</span><span>) </span><span>x/32xg</span><span> $rsp  </span><span># Examine stack</span></span></code></pre><p><span>bash</span></p></div>
<p><strong>4. Common FFI pitfalls to watch for</strong>:</p>
<ul>
<li><strong>Ownership confusion</strong>: Document who owns each pointer</li>
<li><strong>Lifetime mismatches</strong>: Rust may drop memory C still references</li>
<li><strong>ABI mismatches</strong>: Ensure calling conventions match</li>
<li><strong>Null checks</strong>: C functions may return NULL, Rust expects Option</li>
</ul>
<p><strong>5. Red flags in crash output</strong>:</p>
<ul>
<li><code>free(): invalid pointer</code> - Wrong allocator or corrupted metadata</li>
<li><code>double free or corruption</code> - Classic use-after-free</li>
<li><code>malloc(): memory corruption</code> - Heap metadata damaged</li>
<li>Exit code 0 with corruption - The worst case, silent failure</li>
</ul>
<h3 id="how-to-reproduce-these-experiments">How to Reproduce These Experiments<a href="#how-to-reproduce-these-experiments">#</a></h3>
<p>Want to see these crashes yourself? Here’s how to run the key experiments:</p>
<div tabindex="0" data-language="bash"><pre><code><span><span># Clone the repository</span></span>
<span><span>git</span><span> clone</span><span> https://github.com/notashes/rust-c-memory-interop</span></span>
<span><span>cd</span><span> rust-c-memory-interop</span></span>
<span></span>
<span><span># Build the C library</span></span>
<span><span>cd</span><span> c-lib</span></span>
<span><span>make</span></span>
<span></span>
<span><span># Build Rust binaries</span></span>
<span><span>cd</span><span> ../rust-ffi</span></span>
<span><span>cargo</span><span> build</span><span> --release</span></span>
<span></span>
<span><span># Run crash tests (safely in subprocesses)</span></span>
<span><span>cd</span><span> ..</span></span>
<span><span>./tools/run_crash_tests.sh</span></span>
<span></span>
<span><span># Run dynamic analysis tools</span></span>
<span><span>./tools/deep_analysis.sh</span><span>    # Generates and runs memory analysis</span></span>
<span><span>./tools/perf_analysis.sh</span><span>    # Generates and runs performance benchmarks</span></span>
<span></span>
<span><span># View results</span></span>
<span><span>cat</span><span> test_results/crash_test_results_detailed.txt</span></span></code></pre><p><span>bash</span></p></div>
<p><strong>Key Tools You’ll Need:</strong></p>
<ul>
<li><code>gcc</code> and <code>make</code> for C library</li>
<li><code>cargo</code> for Rust</li>
<li><code>perf</code> for performance analysis (optional)</li>
<li><code>gdb</code> for debugging crashes (optional)</li>
<li>Linux system (for glibc-specific features)</li>
</ul>     </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI promised efficiency. Instead, it's making us work harder (164 pts)]]></title>
            <link>https://afterburnout.co/p/ai-promised-to-make-us-more-efficient</link>
            <guid>44786790</guid>
            <pubDate>Mon, 04 Aug 2025 15:01:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://afterburnout.co/p/ai-promised-to-make-us-more-efficient">https://afterburnout.co/p/ai-promised-to-make-us-more-efficient</a>, See on <a href="https://news.ycombinator.com/item?id=44786790">Hacker News</a></p>
Couldn't get https://afterburnout.co/p/ai-promised-to-make-us-more-efficient: Error: getaddrinfo ENOTFOUND afterburnout.co]]></description>
        </item>
        <item>
            <title><![CDATA[Objects should shut the fuck up (344 pts)]]></title>
            <link>https://dustri.org/b/objects-should-shut-the-fuck-up.html</link>
            <guid>44786367</guid>
            <pubDate>Mon, 04 Aug 2025 14:33:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dustri.org/b/objects-should-shut-the-fuck-up.html">https://dustri.org/b/objects-should-shut-the-fuck-up.html</a>, See on <a href="https://news.ycombinator.com/item?id=44786367">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content" role="main">
        <article>
          <header>
            <h2>
              <a href="https://dustri.org/b/objects-should-shut-the-fuck-up.html" rel="bookmark" title="Permalink to Objects should shut the fuck up">Objects should shut the fuck up</a>
               
            </h2>
          </header>
          <div>
            <p>I have a small car, and it has a dual-tank: gas and
<a href="https://en.wikipedia.org/wiki/Liquefied_petroleum_gas">LPG</a>, which is a great
way to reduce my car-related budget, as LPG is way cheaper. Unfortunately, when
the tank is starting to get depleted, the car will emit strident and loud beeps
to notify me about it. And every single time time, it startles me like Hell,
which isn't something I appreciate very much, especially when I'm steering a 1
ton metal box to overtake a trailer at <a href="https://en.wikipedia.org/wiki/Autoroutes_of_France#Motorway_speed_limits">130km/h on the
highway</a>.
To make things even double-plus-good, a full-screen "LPG tank almost depleted!"
message will cover the whole dashboard. The very same dashboard that constantly
have at its bottom the current level of said LPG tank.</p>
<p>The only time I want my car to maybe make some noise is when some critical shit
is happening, like the oil level decreasing at an alarming rate. Having it yell
at me that I'm only able to drive around 100 more kilometers on LPG, while I
have a <strong>full gasoline tank</strong> affording me a total autonomy of like 1000
kilometres is anything but. I hate this, I hate that it's not configurable, I
hate that it's waking everyone asleep in the car (especially toddlers), and I
fucking hate that my car is actively lowering the safety of its passengers by
scaring its driver. And of course, the notification will repeat a couple of
times, at seemingly random intervals, likely because the complete bellend who
designed this terrible scheme is taking the ever-lowering worldwide car-crash
statistics as a personal affront, and is on a mission from God to fix this.</p>
<p>This major papercut made me think about how making noise for anything but the
direst emergency should be an off-by-default privilege that only the user can
explicitly grant, instead if being the default for all electricity-powered
objects. I'm a CyberSecurity Professional™, earning a living breaking into
computer and computer-adjacent systems, so I own the bare minimum in terms of
Smart Devices™, and yet, yet I realised that I still have a non-zero amount of
infernal noise-making crap.</p>
<p>For example, my washing machine has an obnoxious alarm when it completes a
cycle, that can fortunately be disabled via a (hidden!) menu. Now, I get it, it
might be useful to have some kind of alarm so that you don't forget your
soaking clothes and have them rot into a new variant of whatever medieval
disease. But, the alarm, while exceedingly loud, only lasts a handful of
seconds, making it close to useless. Moreover, if you forget your laundry long
enough for it to decay, you kind of deserve to have a Nurgle cult growing in
your basement. Even assuming for the sake of the argument that this
anti-feature could somehow be useful, this isn't the only yapping caused of
this fiendish machine: the fucking "beep" happening every time one turns the
knob or presses a button can't be deactivated. And don't you fucking dare talk
to me about accessibility: all buttons are touch-sensitive, so useless to
visual-impaired users, and all tones are exactly the same, making them
absolutely useless for anything but waking up/startling the sleeping and
unsuspecting house inhabitants. But this isn't everything, of course there is
more: it has a "cute" tone when it starts, for no fucking reason at all. Why is
this a thing? Who the fuck thought it would be a good idea? What's the
reasoning behind this? It's a washing machine, there is no complicated fragile
boot sequence, no hazardous warm-up shenanigans: I press the power-on button,
it starts. Imagine having this ludicrous behaviour on all your objects: Opening
your faucet? "DUDUDUDUUUUUUUU!" Unlocking your front-door?
"LULULUUUUULULUUUUUU!" Turning on the cooktop hood?
"LALALAAAAAAALAAAAAALAAAAA!" Maybe turning your lights on in your living room?
The Ride of the Valkyries starts blasting at full volume.</p>
<p>As electricity is green-ish and cheap-ish where I'm living, I have the luxury
of having a drier as well, forming an unholy honky duo with the washing
machine: while it thankfully doesn't have a startup sound, the interaction-beep
can't be disabled there as well, nor can the alarm. You know, the alarm telling
me that my clothes are dry… There is no reasons, let alone urgency, that I
should get any form of audio notification about this. I could spent 6 months in
the hospital after a car crash because of the aforementioned LPG <a href="https://en.wikipedia.org/wiki/Seven_trumpets">seven
trumpets</a>, come back to my place,
and find my cloths still impeccably dry.</p>
<p>The kitchen isn't spared either: my hotplates will try their very best to make
everyone deaf should something like a dishcloth, or perish the though, some
water be present on more than two touch-sensitive buttons at once. I fail to
conceptualize what the issue is here, and why this warrants all this racket:
doing nothing would be perfectly acceptable. Heck, if something really has to
be done for whatever regulation bullshit or whatnot, how about blinking the
lights used to convey that the plates are hot? Or, if you really need to take
action, just turn them off automatically. Really, anything but something that
would shame a car alarm in every way.</p>
<p>But the very best, my complete favourite, "the world class, maybe even the
world champion" (kudos if you know where this is coming from), is a fucking
baby-phone that beeps when you turn it on. The use-case is "monitoring a
sleeping child", and the loud beep tends to wake the aforementioned kid up. May
the gormless cock-up who created this absolute potato of a device spend his
entire life walking on legos.</p>
<p>Fortunately, everything isn't complete garbage in this world, and I even have a
couple of examples at hand:</p>
<ul>
<li>My dishwasher: no sound whatsoever, it simply opens up when it's done.</li>
<li>My fridge: should I improperly close its door, it'll emit some faint noise
  for like 30 seconds. It's a totally valid important notification, as nobody
      wants to burn electricity to cold the room while the food goes bad.</li>
<li>My ebook reader: it's physically unable to produce any sound.</li>
</ul>
<p>The world is dire enough as it is without having me adding to my shopping
criteria "does it shut the fuck up?" to an already long list. If you're
designing objects, please take some time to test their notification mechanism
near an asleep toddler and/or a sleep-deprived lunatic, instead of making
piling more noisy interruptions to our already notification-saturated reality.</p>
          </div>
        </article>
      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How we made JSON.stringify more than twice as fast (268 pts)]]></title>
            <link>https://v8.dev/blog/json-stringify</link>
            <guid>44786005</guid>
            <pubDate>Mon, 04 Aug 2025 14:09:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://v8.dev/blog/json-stringify">https://v8.dev/blog/json-stringify</a>, See on <a href="https://news.ycombinator.com/item?id=44786005">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody"><p><code>JSON.stringify</code> is a core JavaScript function for serializing data. Its performance directly affects common operations across the web, from serializing data for a network request to saving data to <code>localStorage</code>. A faster <code>JSON.stringify</code> translates to quicker page interactions and more responsive applications. That’s why we’re excited to share that a recent engineering effort has made <code>JSON.stringify</code> in V8 <strong>more than twice as fast</strong>. This post breaks down the technical optimizations that made this improvement possible.</p><h2 id="a-side-effect-free-fast-path" tabindex="-1">A Side-Effect-Free Fast Path <a href="#a-side-effect-free-fast-path">#</a></h2><p>The foundation of this optimization is a new fast path built on a simple premise: if we can guarantee that serializing an object will not trigger any side effects, we can use a much faster, specialized implementation. A "side effect" in this context is anything that breaks the simple, streamlined traversal of an object.<br>This includes not only the obvious cases like executing user-defined code during serialization, but also more subtle internal operations that might trigger a garbage collection cycle. For more details on what exactly can cause side effects and how you can avoid them, see <a href="#limitations">Limitations</a>.</p><p>As long as V8 can determine that serialization will be free from these effects, it can stay on this highly-optimized path. This allows it to bypass many expensive checks and defensive logic required by the general-purpose serializer, resulting in a significant speedup for the most common types of JavaScript objects that represent plain data.</p><p>Furthermore, the new fast path is iterative, in contrast to the recursive general-purpose serializer. This architectural choice not only eliminates the need for stack overflow checks and allows us to quickly resume after <a href="#handling-different-string-representations">encoding changes</a>, but also allows developers to serialize significantly deeper nested object graphs than was previously possible.</p><h2 id="handling-different-string-representations" tabindex="-1">Handling different String Representations <a href="#handling-different-string-representations">#</a></h2><p>Strings in V8 can be represented with either one-byte or two-byte characters. If a string contains only ASCII characters, they are stored as a one-byte string in V8 that uses 1 byte per character. However if a string contains just a single character outside of the ASCII range, all characters of the string use a 2 byte representation, essentially doubling the memory utilization.</p><p>To avoid the constant branching and type checks of a unified implementation, the entire stringifier is now templatized on the character type. This means we compile two distinct, specialized versions of the serializer: one completely optimized for one-byte strings and another for two-byte strings. This has an impact on binary size, but we think the increased performance is definitely worth it.</p><p>The implementation handles mixed encodings efficiently. During serialization, we must already inspect each string's instance type to detect representations we can’t handle on the fast path (like <a href="https://crsrc.org/c/v8/src/objects/string.h;drc=9768251f3e8f598d82420259a940d2057ed56b42;l=1013"><code>ConsString</code></a>, which might trigger a GC during flattening) that require a fallback to the slow path. This necessary check also reveals whether a string uses one-byte or two-byte encoding.</p><p>Because of this, the decision to switch from our optimistic one-byte stringifier to the two-byte version is essentially free. When this existing check reveals a two-byte string, a new two-byte stringifier is created, inheriting the current state. At the end, the final result is constructed by simply concatenating the output from the initial one-byte stringifier with the output from the two-byte one. This strategy ensures we stay on a highly-optimized path for the common case, while the transition to handling two-byte characters is lightweight and efficient.</p><h2 id="optimizing-string-serialization-with-simd" tabindex="-1">Optimizing String Serialization with SIMD <a href="#optimizing-string-serialization-with-simd">#</a></h2><p>Any string in JavaScript can contain characters that require escaping when serializing to JSON (e.g. <code>"</code> or <code>\</code>). A traditional character-by-character loop to find them is slow.</p><p>To accelerate this, we employ a two-level strategy based on the string's length:</p><ul><li>For longer strings, we switch to dedicated hardware SIMD instructions (e.g., ARM64 Neon). This allows us to load a much larger chunk of the string into a wide SIMD register and check multiple bytes for any escapable characters at once in just a few instructions. (<a href="https://crsrc.org/c/v8/src/json/json-stringifier.cc;drc=1645281bbd1b183a252835d376166bd210135bbe;l=3369">source</a>)</li><li>For shorter strings, where the setup cost of hardware instructions would be too high, we use a technique called SWAR (SIMD Within A Register). This approach uses clever bitwise logic on standard general-purpose registers to process multiple characters at once with very low overhead. (<a href="https://crsrc.org/c/v8/src/json/json-stringifier.cc;drc=1645281bbd1b183a252835d376166bd210135bbe;l=3353">source</a>)</li></ul><p>Regardless of the method, the process is highly efficient: we rapidly scan through the string chunk by chunk. If no chunk contains any special characters (the common case), we can simply copy the whole string.</p><h2 id="the-express-lane-on-the-fast-path" tabindex="-1">The Express Lane on the Fast Path <a href="#the-express-lane-on-the-fast-path">#</a></h2><p>Even within the main fast path, we found an opportunity for another, even faster 'express lane'. By default, the fast path must still iterate over an object's properties and, for each key, perform a series of checks: confirm the key is not a <code>Symbol</code>, ensure it's enumerable, and finally, scan the string for characters that require escaping (e.g. <code>"</code> or <code>\</code>).</p><p>To eliminate this, we introduce a flag on an object's <a href="https://v8.dev/docs/hidden-classes">hidden class</a>. Once we have serialized all properties of an object, we mark its hidden class as fast-json-iterable if no property key is a <code>Symbol</code>, all properties are enumerable, and no property key contains characters that require escaping.</p><p>When we serialize an object that has the same hidden class as an object we serialized before (which is quite common, e.g. an array of objects which all have the same shape) and it is fast-json-iterable, we can simply copy all the keys to the string buffer without any further checks.</p><p>We also added this optimization to <code>JSON.parse</code>, where we can utilize it for fast key comparisons while parsing an array, assuming that objects in the array often have the same hidden classes.</p><h2 id="a-faster-double-to-string-algorithm" tabindex="-1">A faster double-to-string algorithm <a href="#a-faster-double-to-string-algorithm">#</a></h2><p>Converting numbers to their string representation is a surprisingly complex and performance-critical task. As part of our work on <code>JSON.stringify</code>, we identified an opportunity to significantly speed up this process by upgrading our core <code>DoubleToString</code> algorithm. We have now replaced the long-serving Grisu3 algorithm with <a href="https://github.com/jk-jeon/dragonbox">Dragonbox</a> for shortest length number to string conversions.</p><p>While this optimization was driven by our <code>JSON.stringify</code> profiling, the new Dragonbox implementation benefits <strong>all</strong> calls to <code>Number.prototype.toString()</code> throughout V8. This means any code that converts numbers to strings, not just JSON serialization, will see this performance boost for free.</p><h2 id="optimizing-the-underlying-temporary-buffer" tabindex="-1">Optimizing the underlying temporary buffer <a href="#optimizing-the-underlying-temporary-buffer">#</a></h2><p>A significant source of overhead in any string-building operation is how memory is managed. Previously, our stringifier built the output in a single, contiguous buffer on the C++ heap. While simple, this approach has a significant drawback: whenever the buffer ran out of space, we had to allocate a larger one and copy the entire existing content over. For large JSON objects, this cycle of re-allocation and copying created major performance overhead.</p><p>The crucial insight was that forcing this temporary buffer to be contiguous offered no real benefit, as the final result is assembled into a single string only at the very end.</p><p>With this in mind, we replaced the old system with a segmented buffer. Instead of one large, growing block of memory, we now use a list of smaller buffers (or "segments"), allocated in V8's Zone memory. When a segment is full, we simply allocate a new one and continue writing there, completely eliminating the expensive copy operations.</p><h2 id="limitations" tabindex="-1">Limitations <a href="#limitations">#</a></h2><p>The new fast path achieves its speed by specializing for common, simple cases. If the data being serialized doesn't meet these criteria, V8 falls back to the general-purpose serializer to ensure correctness. To get the full performance benefit, the <code>JSON.stringify</code> call must adhere to the following conditions.</p><ul><li>No <code>replacer</code> or <code>space</code> arguments: Providing a <code>replacer</code> function or a <code>space</code>/<code>gap</code> argument for pretty-printing are features handled exclusively by the general-purpose path. The fast path is designed for compact, non-transformed serialization.</li><li>Plain data objects and arrays: The objects being serialized should be simple data containers. This means they, and their prototypes, must not have a custom <code>.toJSON()</code> method. The fast path assumes standard prototypes (like <code>Object.prototype</code> or <code>Array.prototype</code>) that don't have custom serialization logic.</li><li>No indexed properties on objects: The fast path is optimized for objects with regular, string-based keys. If an object contains array-like indexed properties (e.g., <code>'0', '1', ...</code>), it will be handled by the slower, more general serializer.</li><li>Simple string types: Some internal V8 string representations (like <code>ConsString</code>) can require memory allocation to be flattened before they can be serialized. The fast path avoids any operation that might trigger such allocations and works best with simple, sequential strings. This is something that’s hard to influence as a web developer. But don’t worry, it should just work in most cases.</li></ul><p>For the vast majority of use cases, such as serializing data for API responses or caching configuration objects, these conditions are naturally met, allowing developers to benefit from the performance improvements automatically.</p><h2 id="conclusion" tabindex="-1">Conclusion <a href="#conclusion">#</a></h2><p>By rethinking <code>JSON.stringify</code> from the ground up, from its high-level logic down to its core memory and character-handling operations, we've delivered a more than 2x performance improvement measured on the JetStream2 json-stringify-inspector benchmark. See the figure below for results on different platforms. These optimizations are available in V8 starting with version 13.8 (Chrome 138).</p><figure><img alt="" height="371" loading="lazy" src="https://v8.dev/_img/json-stringify/results-jetstream2.svg" width="600"><figcaption>JetStream2 Results</figcaption></figure></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Toyota Corolla of programming (167 pts)]]></title>
            <link>https://deprogrammaticaipsum.com/the-toyota-corolla-of-programming/</link>
            <guid>44785759</guid>
            <pubDate>Mon, 04 Aug 2025 13:49:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deprogrammaticaipsum.com/the-toyota-corolla-of-programming/">https://deprogrammaticaipsum.com/the-toyota-corolla-of-programming/</a>, See on <a href="https://news.ycombinator.com/item?id=44785759">Hacker News</a></p>
<div id="readability-page-1" class="page"><article role="document"><div><p><span><span>I</span>n</span> 1995, an otherwise unknown software developer released the first version of a new scripting language whose explicit aim was to make applications for this new platform called “The World Wide Web”. After starting as a small project, and thanks to the crazy dot-com years, it grew dramatically to become one of the most widely used programming languages of all time. After some stumbling first steps, it eventually got some sort of standardization in 1997, even reluctantly including some OOP features to please community and pundits alike.</p><p>However, no matter how hard it tried, this language and its users were mocked for decades by so-called “serious” programmers, who derided its “WTF”-level syntax, the quirks of its runtime model, its ever-increasing amount of security issues, or the gazillion frameworks that sprang around it. Despite the backlash, this language and its community prevailed, eventually getting a huge second act, including some explicit support from Big Tech themselves. Nowadays, as the language reaches the glorious age of 30, a new project drives its future evolution thanks to the strengths of the Go programming language.</p><p>Interestingly enough, the description spread across the two paragraphs above fits not just one but two programming languages: on one side, PHP, heavily inspired by Perl and <a href="https://groups.google.com/g/comp.infosystems.www.authoring.cgi/c/PyJ25gZ6z7A/m/M9FkTUVDfcwJ?lidx=2&amp;wpid=363176&amp;pli=1">released</a> by Rasmus Lerdorf in June 1995 with the name “Personal Home Page Tools”; and on the other hand JavaScript, designed by Brendan Eich and <a href="https://web.archive.org/web/20060111090514/http://wp.netscape.com/newsref/pr/newsrelease67.html">released</a> December of that same year by Netscape.</p><p>But the parallels between both languages do not stop there. In 1997, they got both some level of standardization, thanks to <a href="https://www.php.net/manual/phpfi2.php">PHP/FI 2</a> and <a href="https://www.ecma-international.org/wp-content/uploads/ECMA-262_1st_edition_june_1997.pdf">ECMA-262</a>. During the 2010s, each language got major support from Facebook on one side, and Google and Microsoft on the other side. And in 2025, they both got a major revamp effort based on the <a href="https://deprogrammaticaipsum.com/the-age-of-concurrency/">Go programming language</a>: a new PHP runtime called <a href="https://frankenphp.dev/">FrankenPHP</a>, and the recently announced <a href="https://devblogs.microsoft.com/typescript/typescript-native-port/">TypeScript compiler</a>.</p><p>PHP and JavaScript represent two faces of the same coin: web programming, both server-side and client-side. The growth of the <a href="https://deprogrammaticaipsum.com/from-hypertext-to-spas-to-hypertext/">World Wide Web</a> transformed them into major players, despite their (let us be honest) quite jarring initial design flaws, their slow committee-based evolution, and the seemingly endless series of security flaws that plagued their respective ecosystems.</p><p>An oft-quoted <a href="https://www.stroustrup.com/quotes.html">smirk</a> by Bjarne Stroustrup, the creator of C++, states that</p><blockquote><p>There are only two kinds of languages: the ones people complain about and the ones nobody uses.</p></blockquote><p>Two years ago we published an issue about <a href="https://deprogrammaticaipsum.com/programming-the-liberal-arts/">BASIC</a>, the programming language that all developers love to hate but which single-handedly defined a whole era in our industry. It is only fair that we dedicate some words to PHP, the one everyone also complains about, the one everyone <a href="https://php-ceo.medium.com/">laughs</a> about, yet <a href="https://kinsta.com/php-market-share/">apparently</a> powers between 70 and 80 percent of the world’s websites; needless to say, an impressive number, no matter how hard you look at it, and no matter how much “serious” programmers laugh about it.</p><p>Throughout history, programming languages have received interesting, if revealing, epithets. C is said to be “portable assembly”. <a href="https://deprogrammaticaipsum.com/write-anywhere-run-once/">Java</a> is good for “write once, debug anywhere”. <a href="https://deprogrammaticaipsum.com/the-state-of-python-in-2021/">Python</a> is usually referred to as “executable pseudocode”. <a href="https://deprogrammaticaipsum.com/innovationscript/">JavaScript</a> “was created in 10 days, and it shows”. Perl is the <a href="https://www.wired.com/story/programmers-arent-humble-anymore-nobody-codes-in-perl/">“duct tape of the Internet”</a>.</p><p>And, well, PHP is either a “fractal of bad design” (seriously, people?) or an acronym meaning “Pretty Horrific Programming”. Ouch.</p><p>Here is how I see things: PHP is the <em>lingua franca</em> of affordable <del>cloud</del> web hosting options; or, in other terms, the <a href="https://en.wikipedia.org/wiki/Toyota_Corolla">Toyota Corolla</a> of programming languages: boring, solid, easy, and affordable. You can find, almost anywhere in the world, an affordable web hosting option offering the saint quadrinity of LAMP: Linux, Apache, MySQL, and PHP; an operating system, a web server, a database server, and a scripting language, in an inexpensive package, enabling the masses to go further. Paraphrasing George Clooney, what else?</p><p>These days, PHP features many traits of a modern programming language; let us enumerate some, beginning with its fully <a href="https://github.com/php/php-src">open-source</a> nature. It has advanced <a href="https://deprogrammaticaipsum.com/the-hype-cycle-of-oop/">OOP</a> features like <a href="https://www.php.net/manual/en/language.oop5.traits.php">traits</a>, <a href="https://www.php.net/manual/en/language.oop5.property-hooks.php">property hooks</a>, <a href="https://www.php.net/manual/en/language.namespaces.php">namespaces</a>, <a href="https://www.php.net/manual/en/language.attributes.overview.php">attributes</a>, and <a href="https://www.php.net/manual/en/language.types.enumerations.php">enums</a>. It includes functional programming constructs, like <a href="https://www.php.net/manual/en/functions.anonymous.php">closures</a> with capture lists and <a href="https://www.php.net/manual/en/functions.arrow.php">arrow functions</a>, and a “pipe” operator <a href="https://thephp.foundation/blog/2025/07/11/php-85-adds-pipe-operator/">coming next November</a> (rejoice, OCaml and F# developers!). PHP has associative arrays, and a rudimentary yet fast, useful, and growing type checking system (remember: <code>declare(strict_types=1);</code> is your friend) including <a href="https://www.php.net/manual/en/language.types.never.php"><code>never</code></a> and <a href="https://www.php.net/manual/en/language.types.declarations.php">nullable types</a>. It bundles a full library of algorithms ready to use, following the “batteries included” mantra, and if all else fails, there is a powerful and open-source <a href="https://getcomposer.org/">package manager</a> with <a href="https://packagist.org/">enough packages</a> to make <code>npm</code> blush. There is excellent support for <a href="https://phpunit.de/index.html">unit testing</a>. As scripting programming languages go, it is quite fast to compile and execute. It has its own ecosystem of conferences, <a href="https://afieldguidetoelephpants.net/">a mascot</a>, and even a powerful <a href="https://www.jetbrains.com/phpstorm/">IDE</a> made by none other than JetBrains.</p><p>But, alas, it does feature <a href="https://www.php.net/manual/en/control-structures.goto.php">a <code>goto</code> operator</a>. Oh, là, là! What would Dijkstra say! Not to mention those pesky variables prefixed with a stupid dollar sign, and a cringeworthy <code>foreach</code> statement. The ignominy!</p><p>Most so-called “serious” programmers would be wise to step down from their ivory tower and take a look at PHP in 2025 before an LLM kicks them out of their job. The language has seen major, steady, and consistent revamps for the past decade, including a stable release rhythm once per year, every November, plus a dedicated team that has been <a href="https://www.cvedetails.com/vendor/74/PHP.html">consistently</a> removing security vulnerabilities, and removing obsolete APIs with newer and safer ones.</p><p>I know they should take another look at PHP, because I myself had to step down from my own ivory tower and do that. It is only fair to state my <em>mea culpa</em>: back in 2009 I participated in a (admittedly useless) community effort called “I Hate PHP” (of which the Internet Archive has duly kept <a href="https://web.archive.org/web/20071213013127/https://www.ihatephp.net/">a copy</a>) where my name appeared prominently on the front page. I plead guilty, your honor.</p><p>In my defense, I will argue that those were the somewhat darker ages of PHP. Those were the times of <a href="https://www.oreilly.com/library/view/beyond-java/0596100949/">Bruce Tate’s “Beyond Java”</a> and its long diatribes against PHP spread in the pages therein.</p><p>Remember the long-gone <a href="https://web.archive.org/web/20050210035857/http://phpsec.org/">PHP Security Consortium</a>? Are the days of <a href="https://bobby-tables.com/">Little Bobby Tables</a> over? Let us be honest; not really. You can still release code with SQL injection vulnerabilities if you want (hint: avoid the <code>.</code> operator as much as you can. For all things that are no database queries, <a href="https://www.php.net/manual/en/language.types.string.php#language.types.string.parsing">string interpolation</a> is your friend).</p><p>The <a href="https://deprogrammaticaipsum.com/pastor-manul-laphroaig/">Rt. Rvd. Pastor Manul Laphroaig</a>, in his <a href="https://archive.org/details/Pocorgtfo01/page/n13/mode/2up">sermon</a> to the Beloved Congregation of the First United Church of the Weird Machines, claimed that there is divinity in every programming language… including PHP:</p><blockquote><p>If a language like PHP introduces so many people to pwnage, then that is its divinity. It provides a first step for children to learn how program execution goes astray, with control and data so easy to mangle.</p></blockquote><p>Amen.</p><p>Nowadays, what I see now is a healthy, thriving <a href="https://deprogrammaticaipsum.com/the-tragedy-of-the-common-enemy/">community</a> around PHP. One that, with the exception from JetBrains, is not encumbered by the whims of a FAANG or any other “Big Tech” firm. And the <a href="https://thephp.foundation/">PHP Foundation</a> is driving the evolution of its <a href="https://github.com/php-fig/fig-standards/">standards</a> towards the future, hopefully navigating that unstable space between money and people.</p><p>Take these numbers with a grain of salt, but if we look at language ratings, there is a lot of terrain to reclaim back: on TIOBE, even if PHP was named <a href="https://web.archive.org/web/20050113041221/http://www.tiobe.com/tpci.htm">language of the year 2004</a>, it presently appears at the <a href="https://www.tiobe.com/tiobe-index/php/">15th position</a> and going down: it used to be 3rd in 2010. In the IEEE ranking, it appears in the <a href="https://spectrum.ieee.org/top-programming-languages-2024">13th position</a>, and on the <a href="https://pypl.github.io/PYPL.html">7th position</a> at PYPL, which is quite a drop in the past 20 years. But hey! It appears <a href="https://redmonk.com/sogrady/2025/06/18/language-rankings-1-25/">4th</a> at the RedMonk ranking, and the <a href="https://redmonk.com/rstephens/files/2025/06/redmonk-language-rankings-jan-2025-1.png">graph</a> shows that the fall from 2013 to now has not been <em>that</em> strong. Not all is lost!</p><p>Jokes aside, the most important developments in the history of PHP had to do with the engines used to power it. For decades, the <a href="https://www.zend.com/">Zend engine</a> served as the reference point for the evolution of the language; designed for the needs of the World Wide Web of 1999 (a world of LAMP stacks) and as good as it was for its time, it could not evolve gracefully into a world of DevOps, containers, and <a href="https://deprogrammaticaipsum.com/antonomasia/">Kubernetes</a> orchestrators.</p><p>(Raise your hand if you have ever tried to put together a <code>Dockerfile</code> with Nginx, <a href="https://www.php.net/manual/en/install.fpm.php">FPM</a>, and <a href="https://supervisord.org/">Supervisor</a>, to run some forgotten PHP 7.1 application. I feel your pain, my friend.)</p><p>Thankfully, we can move away from Zend now. Thanks to the unpredictable power of its community, there is this thing called <a href="https://frankenphp.dev/">FrankenPHP</a>. This project has been recently <a href="https://thephp.foundation/blog/2025/06/08/php-30/">adopted</a> by the PHP Foundation as an official runtime, and it ticks all the boxes that could propel PHP into another orbit.</p><p>FrankenPHP not only dramatically simplifies the creation of containers, but it also provides new execution models for PHP code, all while offering 100% compatibility with the massive existing codebase. We talk more about FrankenPHP <a href="https://deprogrammaticaipsum.com/k%C3%A9vin-dunglas/">in the Vidéothèque section</a> this month.</p><p>As nice as FrankenPHP is, I do not count on the end of the backlash against PHP anytime soon. The language will continue to suffer the stigma of its humble beginnings. <a href="https://en.wikipedia.org/wiki/Rasmus_Lerdorf">Rasmus Lerdorf</a> did not get an interview in <a href="https://www.oreilly.com/library/view/masterminds-of-programming/9780596801670/">“Masterminds of Programming”</a>, because PHP is the quintessential language built in a bazaar, as the fruit of an accidental <del>affair</del> design. Nor will Rasmus be invited to the next <a href="https://deprogrammaticaipsum.com/jean-sammet/">HOPL conference</a>, despite the unreasonable popularity of the language, nor will PHP be used for PhD dissertations (other than those related to security, that is). To add insult to injury, PHP is not even mentioned in the <a href="https://deprogrammaticaipsum.com/geoffrey-james/">Tao of Programming</a> (OK, OK, it was published in 1987, I give you that).</p><p>But thankfully IEEE’s “Computer Magazine” did pay attention to PHP, and followed suit with an interview of Rasmus in both <a href="https://www.computer.org/csdl/magazine/co/2012/11/mco2012110006/13rRUILLkze">written</a> and <a href="https://www.youtube.com/watch?v=YIGRXEzjE6c">video</a> formats, published in 2012.</p><p>At least that. In that interview, Rasmus states the core philosophy behind the bazaar:</p><blockquote><p>I learned a bit along the way that, for this to grow, I had to give up control of PHP—I had to let other people have some control. (…) It’s not just them contributing to my project—it becomes our project, and that really changed the nature of PHP.</p></blockquote><p>Cover photo by <a href="https://unsplash.com/@kobuagency?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">KOBU Agency</a> on <a href="https://unsplash.com/photos/hello-world-text-67L18R4tW_w?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Unsplash</a>.</p></div></article><p>Continue reading <a href="https://deprogrammaticaipsum.com/k%C3%A9vin-dunglas/">Kévin Dunglas</a> or go back to
<a href="https://deprogrammaticaipsum.com/issue/issue-083-php">Issue 083: PHP</a>.<br>Download this issue as a <a href="https://deprogrammaticaipsum.com/pdf/issue-083-php.pdf">PDF</a> or <a href="https://deprogrammaticaipsum.com/epub/issue-083-php.epub">EPUB</a> file and read it on your preferred device.<br>Did you like this article? Consider <a href="https://deprogrammaticaipsum.com/newsletter/">subscribing</a> to our newsletter or <a href="https://deprogrammaticaipsum.com/contribute/">contributing</a> to the sustainability of this magazine. Thanks!</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Perplexity is using stealth, undeclared crawlers to evade no-crawl directives (1037 pts)]]></title>
            <link>https://blog.cloudflare.com/perplexity-is-using-stealth-undeclared-crawlers-to-evade-website-no-crawl-directives/</link>
            <guid>44785636</guid>
            <pubDate>Mon, 04 Aug 2025 13:39:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/perplexity-is-using-stealth-undeclared-crawlers-to-evade-website-no-crawl-directives/">https://blog.cloudflare.com/perplexity-is-using-stealth-undeclared-crawlers-to-evade-website-no-crawl-directives/</a>, See on <a href="https://news.ycombinator.com/item?id=44785636">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post"><article><p>2025-08-04</p><section><p>5 min read</p><img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5MASs8Vb8NUgKEKxHQEBiz/a6983d0188bf0279b82af7134905da6c/image6.png" alt=""><div><p>We are observing stealth crawling behavior from Perplexity, an AI-powered answer engine. Although Perplexity initially crawls from their declared user agent, when they are presented with a network block, they appear to obscure their crawling identity in an attempt to circumvent the website’s preferences. We see continued evidence that Perplexity is repeatedly modifying their user agent and changing their source <a href="https://www.cloudflare.com/learning/network-layer/what-is-an-autonomous-system/"><u>ASNs</u></a> to hide their crawling activity, as well as ignoring — or sometimes failing to even fetch — <a href="https://www.cloudflare.com/learning/bots/what-is-robots-txt/"><u>robots.txt</u> </a>files.</p><p>The Internet as we have known it for the past three decades is <a href="https://blog.cloudflare.com/content-independence-day-no-ai-crawl-without-compensation/"><u>rapidly changing</u></a>, but one thing remains constant: it is built on trust. There are clear preferences that crawlers should be transparent, serve a clear purpose, perform a specific activity, and, most importantly, follow website directives and preferences. Based on Perplexity’s observed behavior, which is incompatible with those preferences, we have de-listed them as a verified bot and added heuristics to our managed rules that block this stealth crawling.</p>
    <p>
      <h3 id="how-we-tested">How we tested</h3>
      
    </p>
    <p>We received complaints from customers who had both disallowed Perplexity crawling activity in their <code>robots.txt</code> files and also created WAF rules to specifically block both of Perplexity’s <a href="https://docs.perplexity.ai/guides/bots"><u>declared crawlers</u></a>: <code>PerplexityBot</code> and <code>Perplexity-User</code>. These customers told us that Perplexity was still able to access their content even when they saw its bots successfully blocked. We confirmed that Perplexity’s crawlers were in fact being blocked on the specific pages in question, and then performed several targeted tests to confirm what exact behavior we could observe.</p><p>We created multiple brand-new domains, similar to <code>testexample.com</code> and <code>secretexample.com</code>. These domains were newly purchased and had not yet been indexed by any search engine nor made publicly accessible in any discoverable way. We implemented a <code>robots.txt</code> file with directives to stop any respectful bots from accessing any part of a website: &nbsp;</p>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/66QyzKuX9DQqQYPvCZpw4m/78e7bbd4ff79dd2f1523e70ef54dab9e/BLOG-2879_-_2.png" alt="robots.txt file on our text website" width="1456" height="254" loading="lazy">
          </figure><p>We conducted an experiment by querying Perplexity AI with questions about these domains, and discovered Perplexity was still providing detailed information regarding the exact content hosted on each of these restricted domains. This response was unexpected, as we had taken all necessary precautions to prevent this data from being retrievable by their <a href="https://www.cloudflare.com/learning/bots/what-is-a-web-crawler/"><u>crawlers</u></a>.</p>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/08ZLg0OE7vX8x35f9rDeg/a3086959793ac565b329fbbab5e52d1e/BLOG-2879_-_3.png" alt="Perplexity answering questions about our test website that should have not been accessible by Perplexity" width="855" height="453" loading="lazy">
          </figure>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5uHc0gooXlr98LB56KBb3g/b7dae5987a64f2442d1f89cf21e974ba/BLOG-2879_-_4.png" alt="Perplexity not checking for the presence of a robots.txt file" width="1574" height="616" loading="lazy">
          </figure>
    <p>
      <h3 id="obfuscating-behavior-observed">Obfuscating behavior observed</h3>
      
    </p>
    <p><b>Bypassing Robots.txt and undisclosed IPs/User Agents</b></p><p>Our multiple test domains explicitly prohibited all automated access by specifying in robots.txt and had specific WAF rules that blocked crawling from <a href="https://docs.perplexity.ai/guides/bots"><u>Perplexity’s public crawlers</u></a>.&nbsp;We observed that Perplexity uses not only their declared user-agent, but also a generic browser intended to impersonate Google Chrome on macOS when their declared crawler was blocked. </p><table><tbody><tr><td><p>Declared</p></td><td><p>Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; Perplexity-User/1.0; +https://perplexity.ai/perplexity-user)</p></td><td><p>20-25m daily requests</p></td></tr><tr><td><p>Stealth</p></td><td><p>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36</p></td><td><p>3-6m daily requests</p></td></tr></tbody></table><p>Both their declared and undeclared crawlers were attempting to access the content for scraping contrary to the web crawling norms as outlined in RFC <a href="https://datatracker.ietf.org/doc/html/rfc9309"><u>9309</u></a>.</p><p>This undeclared crawler utilized multiple IPs not listed in <a href="https://docs.perplexity.ai/guides/bots"><u>Perplexity’s official IP range</u></a>, and would rotate through these IPs in response to the restrictive robots.txt policy and block from Cloudflare. In addition to rotating IPs, we observed requests coming from different ASNs in attempts to further evade website blocks. This activity was observed across tens of thousands of domains and millions of requests per day. We were able to fingerprint this crawler using a combination of machine learning and network signals.</p><p>An example:&nbsp;</p>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/4UKtFs1UPddDh9OCtMuwzC/bcdabf5fdd9b0d029581b14a90714d91/unnamed.png" alt="Perplexity crawling workflow based on observations" width="1600" height="973" loading="lazy">
          </figure><p>Of note: when the stealth crawler was successfully blocked, we observed that Perplexity uses other data sources — including other websites — to try to create an answer. However, these answers were less specific and lacked details from the original content, reflecting the fact that the block had been successful.&nbsp;</p>
    <p>
      <h2 id="how-well-meaning-bot-operators-respect-website-preferences">How well-meaning bot operators respect website preferences</h2>
      
    </p>
    <p>In contrast to the behavior described above, the Internet has expressed clear preferences on how good crawlers should behave. All well-intentioned crawlers acting in good faith should:</p><p><b>Be transparent</b>. Identify themselves honestly, using a unique user-agent, a declared list of IP ranges or <a href="https://developers.cloudflare.com/bots/concepts/bot/verified-bots/web-bot-auth/"><u>Web Bot Auth</u></a> integration, and provide contact information if something goes wrong.</p><p><b>Be well-behaved netizens</b>. Don’t flood sites with excessive traffic, <a href="https://www.cloudflare.com/learning/bots/what-is-data-scraping/"><u>scrape</u></a> sensitive data, or use stealth tactics to try and dodge detection.</p><p><b>Serve a clear purpose</b>. Whether it’s powering a voice assistant, checking product prices, or making a website more accessible, every bot has a reason to be there. The purpose should be clearly and precisely defined and easy for site owners to look up publicly.</p><p><b>Separate bots for separate activities</b>. Perform each activity from a unique bot. This makes it easy for site owners to decide which activities they want to allow. Don’t force site owners to make an all-or-nothing decision. </p><p><b>Follow the rules</b>. That means checking for and respecting website signals like <code>robots.txt</code>, staying within rate limits, and never bypassing security protections.</p><p>More details are outlined in our official <a href="https://developers.cloudflare.com/bots/concepts/bot/verified-bots/policy/"><u>Verified Bots Policy Developer Docs</u></a>.</p><p>OpenAI is an example of a leading AI company that follows these best practices. They clearly <a href="https://platform.openai.com/docs/bots"><u>outline their crawlers</u> and </a>give detailed explanations for each crawler’s purpose. They respect robots.txt and do not try to evade either a robots.txt directive or a network level block. And <a href="https://openai.com/index/introducing-chatgpt-agent/"><u>ChatGPT Agent</u></a> is signing http requests using the newly proposed open standard <a href="https://developers.cloudflare.com/bots/concepts/bot/verified-bots/web-bot-auth/"><u>Web Bot Auth</u></a>.</p><p>When we ran the same test as outlined above with ChatGPT, we found that ChatGPT-User fetched the robots file and stopped crawling when it was disallowed. We did not observe follow-up crawls from any other user agents or third party bots. When we removed the disallow directive from the robots entry, but presented ChatGPT with a block page, they again stopped crawling, and we saw no additional crawl attempts from other user agents. Both of these demonstrate the appropriate response to website owner preferences.</p>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/HMJjS7DRmu4octZ99HX8K/753966a88476f80d7a981b1c135fd251/BLOG-2879_-_6.png" alt="BLOG-2879 - 6" width="1200" height="265" loading="lazy">
          </figure>
    <p>
      <h2 id="how-can-you-protect-yourself">How can you protect yourself?</h2>
      
    </p>
    <p>All the undeclared crawling activity that we observed from Perplexity’s hidden User Agent was scored by our bot management system as a bot and was unable to pass managed challenges. Any bot management customer who has an existing block rule in place is already protected. Customers who don’t want to block traffic can set up rules to <a href="https://developers.cloudflare.com/waf/custom-rules/use-cases/challenge-bad-bots/"><u>challenge requests</u></a>, giving real humans an opportunity to proceed. Customers with existing challenge rules are already protected. Lastly, we added signature matches for the stealth crawler into our <a href="https://developers.cloudflare.com/bots/concepts/bot/#ai-bots"><u>managed rule</u></a> that <a href="https://developers.cloudflare.com/bots/additional-configurations/block-ai-bots/"><u>blocks AI crawling activity</u></a>. This rule is available to all customers, including our free customers.&nbsp;&nbsp;</p>
    <p>
      <h2 id="whats-next">What’s next?</h2>
      
    </p>
    <p>We announced <a href="https://blog.cloudflare.com/content-independence-day-no-ai-crawl-without-compensation/"><u>Content Independence Day</u></a> almost one month ago, giving content creators and publishers more control over how their content is accessed. Today, over two and a half million websites have chosen to completely disallow AI training through our managed robots.txt feature or our <a href="https://developers.cloudflare.com/bots/concepts/bot/#ai-bots"><u>managed rule blocking AI Crawlers</u></a>. Every Cloudflare customer is now able to selectively decide which declared AI crawlers are able to access their content in accordance with their business objectives.</p><p>We expected a change in bot and crawler behavior based on these new features, and we expect that the techniques bot operators use to evade detection will continue to evolve. Once this post is live the behavior we saw will almost certainly change, and the methods we use to stop them will keep evolving as well.&nbsp;</p><p>Cloudflare is actively working with technical and policy experts around the world, like the IETF efforts to standardize <a href="https://ietf-wg-aipref.github.io/drafts/draft-ietf-aipref-vocab.html?cf_target_id=_blank"><u>extensions to robots.txt</u></a>, to establish clear and measurable principles that well-meaning bot operators should abide by. We think this is an important next step in this quickly evolving space.</p>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/25VWBDa33UWxDOtqEVEx5o/41eb4ddc262551b83179c1c23a9cb1e6/BLOG-2879_-_7.png" alt="BLOG-2879 - 7" width="1200" height="262" loading="lazy">
          </figure></div></section><div><p>Cloudflare's connectivity cloud protects <a target="_blank" href="https://www.cloudflare.com/network-services/" rel="noreferrer">entire corporate networks</a>, helps customers build <a target="_blank" href="https://workers.cloudflare.com/" rel="noreferrer">Internet-scale applications efficiently</a>, accelerates any <a target="_blank" href="https://www.cloudflare.com/performance/accelerate-internet-applications/" rel="noreferrer">website or Internet application</a>, <a target="_blank" href="https://www.cloudflare.com/ddos/" rel="noreferrer">wards off DDoS attacks</a>, keeps <a target="_blank" href="https://www.cloudflare.com/application-security/" rel="noreferrer">hackers at bay</a>, and can help you on <a target="_blank" href="https://www.cloudflare.com/products/zero-trust/" rel="noreferrer">your journey to Zero Trust</a>.</p><p>Visit <a target="_blank" href="https://one.one.one.one/" rel="noreferrer">1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.</p><p>To learn more about our mission to help build a better Internet, <a target="_blank" href="https://www.cloudflare.com/learning/what-is-cloudflare/" rel="noreferrer">start here</a>. If you're looking for a new career direction, check out <a target="_blank" href="https://www.cloudflare.com/careers" rel="noreferrer">our open positions</a>.</p></div><a href="https://blog.cloudflare.com/tag/cloudforce-one/">Cloudforce One</a><a href="https://blog.cloudflare.com/tag/threat-intelligence/">Threat Intelligence</a><a href="https://blog.cloudflare.com/tag/ai-bots/">AI Bots</a><a href="https://blog.cloudflare.com/tag/bots/">Bots</a><a href="https://blog.cloudflare.com/tag/ai/">AI</a><a href="https://blog.cloudflare.com/tag/bot-management/">Bot Management</a><a href="https://blog.cloudflare.com/tag/security/">Security</a><a href="https://blog.cloudflare.com/tag/generative-ai/">Generative AI</a></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Read your code (166 pts)]]></title>
            <link>https://etsd.tech/posts/rtfc/</link>
            <guid>44785562</guid>
            <pubDate>Mon, 04 Aug 2025 13:33:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://etsd.tech/posts/rtfc/">https://etsd.tech/posts/rtfc/</a>, See on <a href="https://news.ycombinator.com/item?id=44785562">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="article"> <p>If you’d told me 2–3 years ago that in 2025, one of my top pieces of advice for the new generation of developers would be “<em>read your code</em>” (we’re not even talking about re-reading it)… I’m not sure I would’ve believed you.</p>
<h2 id="what-this-article-covers">What This Article Covers</h2>
<p>I’m not here to lecture anyone, but <strong>if you’re aiming to build serious projects</strong> these days, it might be worth learning how to approach AI coding tools the right way.</p>
<p><strong>This post covers:</strong></p>
<ul>
<li>Three critical risks of poor <em>vibe-coding</em> practices</li>
<li>Two effective approaches for production-grade AI-assisted development</li>
<li>Practical tips to maintain code quality while leveraging AI speed</li>
</ul>
<p><strong>Vibe-Coding Refresh</strong></p>
<p>Maybe it’s time we take a fresh look at what vibe-coding actually is. It’s more than just hobby prompting to get code — it’s a practice that serious developers should learn to master. What it means to me:
<mark>Vibe-Coding is a dialogue-based coding process between a human and an AI where the human guides and the AI implements.</mark></p>
<h2 id="its-possible-to-ship-code-without-ever-reading-it">It’s possible to ship code without ever reading it.</h2>
<p>Since Claude Code and Windsurf arrived, it’s now totally possible to get working results without reading a single line of code. You can vibe-code without ever leaving your chat window and just operate based on outcomes - I’ve tried that, out of curiosity.</p>
<p>Even if it doesn’t work on the first try (though it often does), you just explain what’s wrong, and voilà — working result incoming.</p>
<h2 id="but-this-comes-with-three-critical-issues">But This Comes With Three Critical Issues</h2>
<h3 id="1-a-weakened-architecture">1. A Weakened Architecture</h3>
<p>Not reviewing AI-generated code <strong>will</strong> lead to serious problems.</p>
<p>First up: the slow but sure breakdown of your architecture… assuming you even took the time to plan one in the first place.</p>
<p>From experience, even with well-crafted prompts and clearly defined plans for a new feature, Claude Code (which I love, by the way) still sometimes goes off-script. Make sure to properly configure your @CLAUDE.md files to avoid this as much as possible.</p>
<p><strong>Example of architectural drift:</strong></p>
<pre tabindex="0" data-language="javascript"><code><span><span>// Your established pattern: services handle business logic</span></span>
<span><span>class</span><span> UserService</span><span> {</span></span>
<span><span>  async</span><span> getUserProfile</span><span>(</span><span>userId</span><span>)</span><span> {</span></span>
<span><span>    const</span><span> user</span><span> =</span><span> await</span><span> db</span><span>.</span><span>users</span><span>.</span><span>findById</span><span>(</span><span>userId</span><span>);</span></span>
<span><span>    return</span><span> this</span><span>.</span><span>formatUserData</span><span>(</span><span>user</span><span>);</span></span>
<span><span>  }</span></span>
<span><span>}</span></span>
<span></span>
<span><span>// What Claude might generate if unchecked:</span></span>
<span><span>// Business logic creeping into controllers</span></span>
<span><span>app</span><span>.</span><span>get</span><span>(</span><span>'</span><span>/profile/:id</span><span>'</span><span>,</span><span> async</span><span> (</span><span>req</span><span>,</span><span> res</span><span>)</span><span> =&gt;</span><span> {</span></span>
<span><span>  const</span><span> user</span><span> =</span><span> await</span><span> db</span><span>.</span><span>users</span><span>.</span><span>findById</span><span>(</span><span>req</span><span>.</span><span>params</span><span>.</span><span>id</span><span>);</span></span>
<span><span>  // Formatting logic that belongs in service layer</span></span>
<span><span>  const</span><span> formattedUser</span><span> =</span><span> {</span></span>
<span><span>    name</span><span>:</span><span> user</span><span>.</span><span>firstName</span><span> +</span><span> '</span><span> '</span><span> +</span><span> user</span><span>.</span><span>lastName</span><span>,</span></span>
<span><span>    memberSince</span><span>:</span><span> new</span><span> Date</span><span>(</span><span>user</span><span>.</span><span>createdAt</span><span>)</span><span>.</span><span>getFullYear</span><span>()</span></span>
<span><span>  }</span><span>;</span></span>
<span><span>  res</span><span>.</span><span>json</span><span>(</span><span>formattedUser</span><span>);</span></span>
<span><span>});</span></span></code></pre>
<p>If you don’t catch it early, those small inconsistencies become part of the codebase—and your favorite assistant will be tempted to follow those bad examples in the future.</p>
<p><mark>You’re still the architect!</mark></p>
<p>Everyone keeps saying it these days: treat your AI like a (brilliant) new junior dev<sup><a href="#user-content-fn-brillant-junior-dev" id="user-content-fnref-brillant-junior-dev" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>. And what does a junior dev do before starting a new feature? They read through the existing codebase. Which means any weak or messy design choices are likely to get repeated.</p>
<p>And by the way, you’d never push junior dev code without reviewing it.</p>
<p>Now more than ever, we’re all responsible for the architecture. We spend more time guiding the AI on how to code than writing the actual code ourselves.</p>
<h3 id="2-loss-of-implementation-knowledge">2. Loss of Implementation Knowledge</h3>
<blockquote>
<p>You cannot delegate the act of thinking.</p>
</blockquote>
<p><em>Alain (French philosopher, 19th century)</em></p>
<p><strong>If you’re only focused on the end result,</strong> you’ll soon know as little as your users about how things actually work. You may be the most advanced user of your own app — but <strong>you won’t own your domain anymore</strong>.</p>
<p><strong>Why does that matter?</strong></p>
<p>In every experience I’ve had — especially when building my latest startup<sup><a href="#user-content-fn-from-0-to-exit-in-2-years" id="user-content-fnref-from-0-to-exit-in-2-years" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup> — I’ve learned that apps and features don’t take shape at implementation time. They’re designed upstream: business rules, tech and infrastructure decisions all take form before you touch the keyboard. They come to you while commuting, while chatting, or—often—while in the shower.</p>
<p>Depending on your level of responsibility, you may or may not be involved outside of work. But you’ll probably agree that your best ‘Aha!’ moments didn’t happen in front of VS Code.</p>
<p>If you don’t have the structure of your domain — its concepts and abstractions — constantly simmering somewhere in the back of your mind, you won’t be able to fully leverage the creative potential of modern tech.</p>
<p><mark>If you really think you don’t need this knowledge, your business might not be all that “Tech” to begin with.</mark> In that case, use a well-structured Notion doc, or a no-code tool—you’ll save a ton of time and money.</p>
<p>And don’t hesitate to leave your code editor and chat with an AI that doesn’t have access to your codebase—our good old rubber duck didn’t either, and that’s precisely why it worked.</p>
<h3 id="3-security-vulnerabilities">3. Security Vulnerabilities</h3>
<p>Are you working on a production app? Then you must care about security. Most web security issues are avoided through knowledge and experience. But a lax implementation or fuzzy access scopes, and you’re in serious trouble.</p>
<p><strong>Example that happened to me last week:</strong></p>
<pre tabindex="0" data-language="javascript"><code><span><span>// What I asked for: "List user's projects"</span></span>
<span><span>// What Claude generated:</span></span>
<span><span>app</span><span>.</span><span>get</span><span>(</span><span>'</span><span>/api/projects/:id</span><span>'</span><span>,</span><span> async</span><span> (</span><span>req</span><span>,</span><span> res</span><span>)</span><span> =&gt;</span><span> {</span></span>
<span><span>  const</span><span> projectId</span><span> =</span><span> req</span><span>.</span><span>body</span><span>.</span><span>id</span><span>; </span><span>// From the client</span></span>
<span><span>  const</span><span> project</span><span> =</span><span> await</span><span> db</span><span>.</span><span>projects</span><span>.</span><span>find</span><span>(</span><span>{</span><span> id</span><span>:</span><span> projectId</span><span> }</span><span>);</span></span>
<span><span>  res</span><span>.</span><span>json</span><span>(</span><span>project</span><span>);</span></span>
<span><span>});</span></span>
<span></span>
<span><span>// What it should have been:</span></span>
<span><span>app</span><span>.</span><span>get</span><span>(</span><span>'</span><span>/api/projects/:id</span><span>'</span><span>,</span><span> async</span><span> (</span><span>req</span><span>,</span><span> res</span><span>)</span><span> =&gt;</span><span> {</span></span>
<span><span>  const</span><span> projectId</span><span> =</span><span> req</span><span>.</span><span>body</span><span>.</span><span>id</span><span>;</span></span>
<span><span>  const</span><span> userId</span><span> =</span><span> req</span><span>.</span><span>authenticatedUser</span><span>.</span><span>id</span><span>; </span><span>// From auth middleware</span></span>
<span><span>  const</span><span> project</span><span> =</span><span> await</span><span> db</span><span>.</span><span>projects</span><span>.</span><span>find</span><span>(</span><span>{</span></span>
<span><span>    id</span><span>:</span><span> projectId</span><span>,</span></span>
<span><span>    where</span><span>:</span><span> {</span><span> userId</span><span>:</span><span> userId</span><span> }</span></span>
<span><span>  }</span><span>);</span></span>
<span><span>  res</span><span>.</span><span>json</span><span>(</span><span>project</span><span>);</span></span>
<span><span>});</span></span></code></pre>
<p>The AI, focused on the end goal, implemented exactly what I asked for… except that it never once verified whether the resource actually belonged to the current user. <strong>Classic mistake.</strong></p>
<p>Sure, you can tell me: “always include access control in your prompt”, but some flaws only become obvious during implementation. Ever had a security insight pop into your head while coding a feature?</p>
<p>You can’t be too careful. A misworded prompt, a misunderstood intention, an unreviewed commit—and bam, you’ve got a breach. I fear this will become more common with hastily vibe-coded projects.</p>
<p><mark>Security has always needed to be part of the implementation process. Why should that change now?</mark></p>
<h2 id="two-ways-to-vibe-code-responsibly">Two Ways to Vibe-Code Responsibly</h2>
<p>As stated in <a href="https://www-cdn.anthropic.com/58284b19e702b49db9302d5b6f135ad8871e7658.pdf">this great ressource</a> by Anthropic, there are two viable ways to vibe-code a production-ready project in 2025:</p>
<blockquote>
<p>Learn to distinguish between tasks that work well asynchronously (peripheral features, prototyping) versus those needing synchronous supervision (core business logic, critical fixes). Abstract tasks on the product’s edges can be handled with “auto-accept mode,” while core functionality requires closer oversight.</p>
</blockquote>
<h3 id="1-fast-prototyping-with-auto-accept-mode">1. Fast Prototyping with Auto-Accept Mode</h3>
<p>Here, you use the AI in auto-pilot mode. Describe the expected output, provide specs, and let it run. Before ending the session, <strong>you review what’s been done</strong>, and adjust as needed.</p>
<p>This works well when:</p>
<ul>
<li>You’re working on a topic you’re not familiar with</li>
<li>Generating test scaffolding (but still review the tests — a test that doesn’t test anything meaningful is just a green checkmark)</li>
<li>Exploring new libraries or frameworks</li>
</ul>
<h3 id="2-synchronous-coding-for-core-features">2. Synchronous Coding for Core Features</h3>
<p>This is where the real innovation is happening in our field. <strong>Pair-vibe-coding, without auto-accept</strong>, is the most effective way to ship quality features. Every suggestion is a chance to either accept or iterate.</p>
<p>And that changes everything: at every small step, you can correct direction before things drift. <mark>It’s always easier to straighten a sapling than a grown tree.</mark> The earlier you lock down good concepts and interfaces in your architecture, the better future suggestions from the AI will be.</p>
<p><strong>Plan your session:</strong> When you start a session, begin with a clear plan. Read it carefully, regardless of your approach, and don’t validate it unless you fully agree with it. The plan is to the session what the seed is to the tree: bad seed, bad soil, no fruit.</p>
<h2 id="the-vibe-coding-checklist">The Vibe-Coding Checklist</h2>
<p>Before pushing any AI-generated code:</p>
<ul>
<li> <strong>Architecture Check</strong>: Does this follow our established patterns?</li>
<li> <strong>Security Review</strong>: Are all resources properly scoped to users?</li>
<li> <strong>Tests</strong>: Do they actually test meaningful behavior?</li>
</ul>
<p>But also, do not forget to check:</p>
<ul>
<li> <strong>Documentation</strong>: Will <em>you</em> understand this in 6 months?</li>
<li> <strong>Error Handling</strong>: Are edge cases covered?</li>
<li> <strong>Performance</strong>: Any obvious N+1 queries or inefficiencies?</li>
</ul>
<p>And above all, make sure to:</p>
<ul>
<li> <mark>Leave with some <strong>knowledge</strong> of the new code</mark>.</li>
</ul>
<h2 id="to-wrap-it-up">To Wrap It Up</h2>
<p>AI coding assistants are powerful tools, but they’re amplifiers of your expertise, not replacements for it. The day you stop understanding your codebase is the day you stop being its architect.</p>
<p><strong>Teams:</strong> don’t cancel code reviews thinking Claude Code acts as a second dev alongside the one assigned to the feature. Bugs aren’t the biggest threat—losing mastery of your domain and architecture is. That’s the real roadblock to innovation.</p>
<p><strong>Engineers:</strong> you can usually let the AI <em>RTFM</em> but you: <mark>Read That F*cking Code</mark>!</p>
<section data-footnotes="">
<ol>
<li id="user-content-fn-brillant-junior-dev">
<p><a href="https://youtu.be/w4rG5GY9IlA?si=vZDXi8NtyPmxILMB&amp;t=217">Learning Software Engineering During the Era of AI | Raymond Fu | TEDxCSTU</a> <a href="#user-content-fnref-brillant-junior-dev" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-from-0-to-exit-in-2-years">
<p><a href="https://www.linkedin.com/pulse/from-0-exit-within-2-years-bootstrapped-philippe-vanderstigel/">From 0 to exit within 2 years | Philippe Vanderstigel</a> <a href="#user-content-fnref-from-0-to-exit-in-2-years" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
</ol>
</section> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My Ideal Array Language (119 pts)]]></title>
            <link>https://www.ashermancinelli.com/csblog/2025-7-20-Ideal-Array-Language.html</link>
            <guid>44785224</guid>
            <pubDate>Mon, 04 Aug 2025 13:05:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ashermancinelli.com/csblog/2025-7-20-Ideal-Array-Language.html">https://www.ashermancinelli.com/csblog/2025-7-20-Ideal-Array-Language.html</a>, See on <a href="https://news.ycombinator.com/item?id=44785224">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div id="mdbook-help-container">
            <h2>Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        

        <!-- Set the theme before any content is loaded, prevents flash -->
        

        

        <!-- Hide / unhide sidebar before it is displayed -->
        

        <nav id="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox></mdbook-sidebar-scrollbox>
            
            
        </nav>

        <div id="page-wrapper">

            <div class="page">
                
                <div id="menu-bar">
                    

                    <h2>Asher's Blog</h2>

                    
                </div>

                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                

                <div id="content">
                    <main>
                        <h2 id="my-ideal-array-language"><a href="#my-ideal-array-language">My Ideal Array Language</a></h2>
<p><em>2025-07-20</em></p>
<p>What do I think the ideal array language should look like?</p>
<ul>
<li><a href="#my-ideal-array-language">My Ideal Array Language</a></li>
<li><a href="#why-does-this-matter">Why does this matter?</a></li>
<li><a href="#user-extensible-rank-polymorphism">User-Extensible Rank Polymorphism</a>
<ul>
<li><a href="#user-extensibility-in-mojo">User Extensibility in Mojo</a></li>
</ul>
</li>
<li><a href="#value-semantics-and-automatic-bufferization">Value Semantics and Automatic Bufferization</a>
<ul>
<li><a href="#fortrans-array-semantics">Fortran’s Array Semantics</a></li>
<li><a href="#comparison-with-mlir-types-and-concepts">Comparison with MLIR Types and Concepts</a></li>
<li><a href="#fortran-array-semantics-in-mlir">Fortran Array Semantics in MLIR</a></li>
<li><a href="#aside-dependent-types-in-fortran">Aside: Dependent Types in Fortran</a></li>
</ul>
</li>
<li><a href="#compilation-step">Compilation Step</a>
<ul>
<li><a href="#offline-vs-online-compilation">Offline vs Online Compilation</a></li>
<li><a href="#compiler-transparency-and-inspectability">Compiler Transparency and Inspectability</a>
<ul>
<li><a href="#example-nvhpcs-user-facing-optimization-reporting">Example: NVHPC’s User-Facing Optimization Reporting</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#simt-and-automatic-parallelization">SIMT and Automatic Parallelization</a>
<ul>
<li><a href="#simt-vs-simd">SIMT vs SIMD</a></li>
<li><a href="#default-modes-of-parallelism">Default Modes of Parallelism</a></li>
</ul>
</li>
<li><a href="#array-aware-type-system">Array-Aware Type System</a></li>
<li><a href="#syntax">Syntax</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#bonus-comparing-parallel-functional-array-languages">Bonus: <em>Comparing Parallel Functional Array Languages</em></a></li>
</ul>
<h2 id="why-does-this-matter"><a href="#why-does-this-matter">Why does this matter?</a></h2>
<p>The fundamental units of computation available to users today are not the same as they were 20 years ago.
When users had at most a few cores on a single CPU, it made complete sense that every program was written with the assumption that it would only run on a single core.</p>
<p>Even in a high-performance computing (HPC) context, the default mode of parallelism was (for a long time) the Message Passing Interface (MPI), which is a <em>descriptive</em> model of multi-core and multi-node parallelism.
Most code was still basically written with the same assumtions: all units of computation were assumed to be uniform.</p>
<p>Hardware has trended towards heterogeneity in several ways:</p>
<ul>
<li>More cores per node</li>
<li>More nodes per system</li>
<li>More kinds of subsystems (GPUs, FPGAs, etc.)</li>
<li>More kinds of computational units on a single subsystem
<ul>
<li>CPUs have lots of vector units and specialized instructions</li>
<li>NVIDIA GPUs have lots of tensor cores specialized for matrix operations</li>
</ul>
</li>
<li>New paradigms at the assembly level
<ul>
<li>Scalable Vector Extensions (SVE) and Scalable Matrix Extensions (SMEs) on Arm</li>
</ul>
</li>
<li>Tight hardware release schedules, meaning less and less time in between changes in hardware and more and more rewrites required for hand-written code at the lowest level</li>
</ul>
<p>The old assumptions do not hold true anymore, and programming languages need to be aware of these changes and able to optimize around them.</p>
<p>Imagine the units of computation available in 2025 as a spectrum from SIMD units, to tensor cores, to CUDA cores, to small power-efficient Arm CPU cores, to large beefy AMD CPUs.
It is easy to imagine this spectrum filling in with more specialized hardware pushing the upper and lower boundaries and filling in the gaps.
One day it might be as natural to share work between nodes as it is between individual SIMD lanes on a single CPU core.
This level of heterogeneity is not something that can be ignored by a programming language or a programming language ecosystem.
I believe languages and compilers that do not consider the trajectory of hardware development will be left behind to some degree.</p>
<h2 id="user-extensible-rank-polymorphism"><a href="#user-extensible-rank-polymorphism">User-Extensible Rank Polymorphism</a></h2>
<p>IMO this is what makes something an array language.
No language can be an array language without rank polymorphism.</p>
<p>Numpy provides <em>some</em> rank polymorphism, but it’s not a first-class <em>language</em> feature.
Numpy also needs to be paired with a JIT compiler to make python a real array language, so NUMBA or another kernel language is required for Python to make the list.
Otherwise, users would not be able to write their own polymorphic kernels (<code>ufunc</code>s).</p>
<p>Similarly, the JAX and XLA compilers provide an array language base, but without a kernel language like Pallas or Triton, it’s not extensible enough for every use case they care about.</p>
<div id="admonition-default" role="note">
<p>A proper array language will have an array language <em>base</em> with the ability to write <em>kernels</em> directly, either by exposing lower-level details the user can opt-in to, or by allowing primitives to be composed in a way the compiler can reason about.</p>
</div>
<h2 id="user-extensibility-in-mojo"><a href="#user-extensibility-in-mojo">User Extensibility in Mojo</a></h2>
<div id="admonition-todo-unfinished-section" role="note" aria-labelledby="admonition-todo-unfinished-section-title">
<ul>
<li>MLIR primitives exposed in the language</li>
<li>Pushing details out of the compiler and into the language</li>
<li>Helps extensibility; fewer uses of compiler builtins</li>
<li>Standard library contains lots of language features that are usually implemented in the compiler</li>
<li><a href="https://youtu.be/5gPG7SXoBag?si=kLnJhKjxxWo5udp2">recent talk on <em>GPUMODE</em></a></li>
</ul>
</div>
<h2 id="value-semantics-and-automatic-bufferization"><a href="#value-semantics-and-automatic-bufferization">Value Semantics and Automatic Bufferization</a></h2>
<p>Most of the major ML frameworks have value semantics for arrays by default, and it gives the compiler much more leeway when it comes to memory.
Not only is manual memory management a huge pain and a source of bugs, if the ownership semantics are not sufficiently represented in the language or the language defaults are not ammenable to optimization, the compiler will have a much harder time generating performant code.</p>
<p>My understanding of the Rust borrow checker is that its purpose is to handle the intersection of manual memory management and strict ownership.
Users choose between value and reference semantics, and the compiler helps you keep track of ownership.
Summarized as <em>“aliasing xor mutability”</em>.</p>
<p><a href="https://bsky.app/profile/steveklabnik.com/post/3lusthgcwcs2r"><em>Thanks Steve, for helping clarify the Rust bits!</em></a></p>
<h2 id="fortrans-array-semantics"><a href="#fortrans-array-semantics">Fortran’s Array Semantics</a></h2>
<p>In a way, value semantics and automatic bufferization of arrays is part of why Fortran compilers are able to generate such performant code.</p>
<p>When you use an array in Fortran, you are not just getting a pointer to a contiguous block of memory.
If you access an array from C FFI, you get access to an <em>array descriptor</em> or a <em>dopevector</em> which contains not only the memory backing the array, but also rich shape, stride, and bounds information.</p>
<p>This is not always how the array is represented in the final program however; because the low level representation of the array is only revealed to the user if they ask for it, the compiler is able to optimize around it in a way that is not possible with C.</p>
<p>In C, the compiler <em>must assume arrays alias each other</em> unless the user provides explicit aliasing information.
In Fortran, it is exactly the opposite: unless the user informs the compiler that they formed a pointer to an array, the compiler can assume that the array has exclusive ownership.
The language rules dictate that function arguments may not alias unless the user explicitly declares them to be aliasing.
This means the compiler can optimize around array operations in a way that is only possible in C with lots of extra effort from the user.</p>
<p>Additionally, Fortran represents rank polymorphism natively.
Just like Numpy universal functions (<code>ufunc</code>s), Fortran has the concept of <code>ELEMENTAL</code> procedures which can be called on arrays of any rank.</p>
<h2 id="comparison-with-mlir-types-and-concepts"><a href="#comparison-with-mlir-types-and-concepts">Comparison with MLIR Types and Concepts</a></h2>
<p><a href="https://mlir.llvm.org/">MLIR</a> is the compiler infrastructure backing many of the major ML frameworks.
I won’t go too deep into MLIR here, but many of the components of MLIR are designed to handle array descriptors and their associated shape, stride, and bounds information in various intermediate representations.</p>

<p>MLIR has a few array representations, most prominently the <code>memref</code> and <code>tensor</code> types, <code>tensor</code> being the more abstract, unbufferized version of a <code>memref</code>.
The process of <em>bufferizing</em>, or converting ops with tensor semantics to ops with memref semantics (<a href="https://mlir.llvm.org/docs/Bufferization/">described here</a>) is how abstract operations on arrays are converted to a more concrete representation with explicit memory backing.</p>
<p>One might consider the <code>tensor</code> dialect to be a purely functional, un-bufferized array programming language that is primarily <em>internal</em> to the compiler.
In fact, <a href="https://pldb.io/blog/chrisLattner.html">see this quote from Chris Lattner</a>:</p>
<div id="admonition-_chris-lattner_" role="note" aria-labelledby="admonition-_chris-lattner_-title">
<p><em>What languages changed the way you think?</em></p>
<p>I would put in some of the classics like Prolog and APL. APL and Prolog are like a completely different way of looking at problems and thinking about them.</p>
<p>I love that, even though it’s less practical in certain ways. Though all the ML compilers today are basically reinventing APL.</p>
</div>
<p>Array languages lend themselves to compilation for a few reasons:</p>
<ul>
<li>The lack of bufferization is a great match for compiler optimizations; if the buffers do not exist in the user’s program, the compiler can optimize them and sometimes get rid of them entirely.</li>
<li>Functional array langauges are a closer match to the internal representation of array programs in many compilers, particularly MLIR-based ones.
<ul>
<li>The buffers are left entirely up to the compiler</li>
<li>Most modern compilers use <a href="https://en.wikipedia.org/wiki/Static_single-assignment_form">SSA form for their internal representation</a>, meaning every “value” (virtual register) is assigned to exactly once.</li>
<li>For example, to set an element in <code>tensor</code>-land, one must create an entirely new tensor. Which operations actually result in a new buffer being created is left up to the compiler.</li>
</ul>
</li>
</ul>
<p>Take this example from the MLIR docs:</p>
<pre><code>%r = tensor.insert %f into %t[%idx] : tensor&lt;5xf32&gt;
</code></pre>
<p>The the tensor with <code>%f</code> inserted into the <code>%t</code> tensor at index <code>%idx</code> is <code>%r</code>: an entirely new tensor.
Compare this with the bufferized version in the <code>memref</code> dialect:</p>
<pre><code>memref.store %f, %t[%idx] : memref&lt;5xf32&gt;
</code></pre>
<p>The <code>memref.store</code> operation has <em>side effects</em> and the backing memory is modified in place.
This is a lower-level representation, and typically harder to reason about.
The compiler may have to consider if the memory escaped the current scope before modifying it in place, for example.
The higher-level <code>tensor</code> dialect is much closer to function array programming languages, so a functional array language is a great match for an optimizing compiler.</p>
<!-- ~~~admonish todo
- lattner apl mlir https://pldb.io/chrisLattner.html
- An interview with Chris Lattner
- Why compilers lend themselves to functional? Bc ssa format, can only deal with values, but then have special language for loads/stores, sorta like refs in ocaml where you got special language for interacting with memory in that way
- Why pointer chasing so much more expensive?
~~~ -->
<h2 id="fortran-array-semantics-in-mlir"><a href="#fortran-array-semantics-in-mlir">Fortran Array Semantics in MLIR</a></h2>
<p>The LLVM Flang Fortran compiler was one of the first users of MLIR in the world outside of ML frameworks.
Flang has two primary MLIR dialects for representing Fortran programs: <code>hlfir</code> and <code>fir</code>, or high-level Fortran intermediate representation and Fortran intermediate representation, respectively.</p>
<p><code>hlfir</code> initially represents Fortran programs that are <em>not bufferized</em>, meaning the compiler can optimize around array operations without considering the details of memory management, except when required by the user’s program.</p>
<p>Take this Fortran program:</p>
<pre><code>subroutine axpy(a, b, c, n)
  implicit none
  integer, intent(in)    :: n
  real,    intent(in)    :: a(n,n), b(n,n)
  real,    intent(inout)   :: c(n,n)
  c = a * b + c
end subroutine
</code></pre>
<p><a href="https://godbolt.org/z/sMWvMoo5M">At this godbolt link</a>, we can see the IR for this program after every compiler pass in the bottom-right panel, which is MLIR in the <code>hlfir</code> and <code>fir</code> dialects.</p>
<p>A savy reader might notice that the <code>hlfir</code> dialect is not bufferized, meaning the memory backing the arrays is not represented in the IR.
The language semantics give the compiler this freedom.</p>
<p>After each pass, the IR is dumped with this message: <code>IR Dump After &lt;pass name&gt;</code>.
If you search for <code>IR Dump After BufferizeHLFIR</code>, you can see the IR after the compiler pass that introduces memory to back a temporary array used to calculate the result.</p>
<p>If you then turn the optimization level up to <code>-O3</code> by changing the compiler flags in the top-right, you can search for the pass <code>OptimizedBufferization</code> which leverages the language semantics to reduce and reuse memory in the program, and you’ll notice that the temporary array is no longer present in the IR.</p>
<p>An ideal array language should be able to represent this kind of dynamic shape information in the type system and leave space for the compiler to perform these sorts of optimizations.</p>
<h2 id="aside-dependent-types-in-fortran"><a href="#aside-dependent-types-in-fortran">Aside: Dependent Types in Fortran</a></h2>
<p>You may have also noticed that the shapes of the matrices passed to the function are dynamic - the parameter <code>n</code> is used to determine the shape of the arrays.</p>
<p>In the IR dumps, you can see that the shapes are used to determine the types of the arrays <em>at runtime</em>; the types of the arrays depends on the parameter passed into the function.</p>
<p>This is represented in the IR like this:</p>
<pre><code>func.func @_QPaxpy(
    %arg0: !fir.ref&lt;!fir.array&lt;?x?xf32&gt;&gt; {fir.bindc_name = "a"}, // ...
    ) {
    // ...
    %12 = fir.shape %6, %11 : (index, index) -&gt; !fir.shape&lt;2&gt;
    %13:2 = hlfir.declare %arg0(%12)
                {fortran_attrs = #fir.var_attrs&lt;intent_in&gt;, uniq_name = "_QFaxpyEa"}
                : (!fir.ref&lt;!fir.array&lt;?x?xf32&gt;&gt;, !fir.shape&lt;2&gt;, !fir.dscope)
                -&gt; (!fir.box&lt;!fir.array&lt;?x?xf32&gt;&gt;, !fir.ref&lt;!fir.array&lt;?x?xf32&gt;&gt;)
</code></pre>
<h2 id="compilation-step"><a href="#compilation-step">Compilation Step</a></h2>
<p>Whether offline or online compilation, there needs to be a compilation step.
Part of the beauty of array languages is the language semantics, but the real power comes from the <em>ability to optimize</em> around those semantics.</p>
<p>If a user adds two arrays together, it’s imperative that a compiler is able to see the high-level information in the user’s program and optimize around it.</p>
<h2 id="offline-vs-online-compilation"><a href="#offline-vs-online-compilation">Offline vs Online Compilation</a></h2>
<p>One might argue for either offline compilation (like Fortran, with an explicit compilation step) or online compilation (like Python, where the compiler is invoked at runtime).
For workloads with very heavy compute, it is likely that the process driving the computation can outpace the hardware, meaning it is not very costly to have the compiler invoked while the program is running.</p>
<p>It can be quite a downside to have the compiler on the hotpath, especially for smaller programs where it might become a bottleneck.
Compilers built for offline compilation can often get away with suboptimal performance.
As long as users can lauch <code>make -j</code> and get their program back after grabbing a coffee, it’s not usually a big deal.
Online compilation introduces an entirely new set of challanges, but the lower barrier to entry for users may be worth it.</p>
<p>All major ML frameworks driven by Python make this tradeoff, for example.</p>
<!-- ```admonish todo
- Offline v online compilation. As long as you can drive your heavy units of compute, it doesnt matter which model you use, but you do get the downside of putting the compiler on the hotpath with online compilation. Can sorta get away with a lot in offline. Users will make and forget unless its egregious.
``` -->

<h2 id="compiler-transparency-and-inspectability"><a href="#compiler-transparency-and-inspectability">Compiler Transparency and Inspectability</a></h2>
<p>Compiler optimizations are notoriously unreliable.
If there were a library that was as unreliable and opaque as most compilers, I do not believe users would be willing to adopt it.
In addition, most compiler reporting is built for compiler engineers, not users.
For a user to have any understanding of why their program is slow, they need to be able to understand the optimizations that the compiler is <em>not</em> performing, and be able to inspect compiler logs.</p>
<p>A good example of this is the LLVM remarks framework - I find this framework indispensible for debugging performance issues, especially when paired with a profiling tool like linux <code>perf</code>.
Pairing up the hot paths in the profile with the remarks from the compiler gives a good indication of what the compiler is <em>not</em> optimizing and why - but again this is built for compiler engineers, not users.</p>
<p>If a user finds that their C program is slow, they might look at Clang’s optimization remarks and find that a function in their hot loop was not inlined because of the cost of the stack space taken up by the a function in the inner loop, or that dependence analysis failed because the user did not provide enough aliasing information to the compiler.
Even if they manage to dump the logs and use LLVM’s remarks-to-html tool and generate a readable report of their program, they may still have problems finding actionable information in that report.
<em><strong>User-facing optimization reports and hints are a must.</strong></em></p>
<h3 id="example-nvhpcs-user-facing-optimization-reporting"><a href="#example-nvhpcs-user-facing-optimization-reporting">Example: NVHPC’s User-Facing Optimization Reporting</a></h3>
<p>This is one of my favorite features of the NVHPC compilers - they all have a user-facing optimization reporting framework.
Adding <code>-Minfo=all</code> and <code>-Mneginfo=all</code> to the command line gives a detailed report of the optimizations that the compiler is performing, optimizations that were missed, and why.</p>
<p><a href="https://godbolt.org/z/b4ePMK7zo">Take this C code for example</a>:</p>
<pre><code>void add_float_arrays(const float *a,
                      const float *b,
                            float *c,
                      size_t n)
{
    for (size_t i = 0; i &lt; n; ++i) {
        c[i] = a[i] + b[i];
    }
}

// -Minfo output:
// add_float_arrays:
//       8, Loop versioned for possible aliasing
//          Generated vector simd code for the loop
//          Vectorized loop was interleaved
//          Loop unrolled 4 times
</code></pre>
<p>It doesn’t take too savy of a user to see the <code>Loop versioned for possible aliasing</code> remark and wonder <em>“Well, how do I tell the compiler that these arrays are not aliasing?”</em></p>
<p>Of course, annotating the arrays with <code>restrict</code> gives the compiler this information:</p>
<pre><code>void add_float_arrays(const float *restrict a,
                      const float *restrict b,
                            float *restrict c,
                      size_t                n)
{
    for (size_t i = 0; i &lt; n; ++i) {
        c[i] = a[i] + b[i];
    }
}

// -Minfo output:
// add_float_arrays_restrict:
//      20, Generated vector simd code for the loop
//          Vectorized loop was interleaved
</code></pre>
<p>Of course, the language semantics should be enough to tell the compiler that arrays in a function like this do not alias, but this is an example of what friendly user-facing compiler reporting looks like, in my opinion.</p>
<h2 id="simt-and-automatic-parallelization"><a href="#simt-and-automatic-parallelization">SIMT and Automatic Parallelization</a></h2>
<h2 id="simt-vs-simd"><a href="#simt-vs-simd">SIMT vs SIMD</a></h2>
<p>SIMT is a programming model that allows for parallel execution of the same instruction on multiple threads.
Users typically write a function which recieves a thread identifier, performs operations on data, and writes to an output parameter.
It is nearly impossible to <em>not</em> achieve parallelism with SIMT; once you have described your function in this way, the compiler has to do not other work in order to achieve <strong>parallelism</strong>.
SIMT kernels often operate in <em>lockstep</em>, meaning that every instruction in a SIMT kernel is executed <em>by every thread</em>, but instructions in a thread that is not <em>active</em> are not committed to memory.</p>
<p>In this example, <em>every thread</em> executes <em>both</em> the <code>if</code> and the <code>else</code> branches, but only threads that are active in either region will actually write to <code>pointer</code>.</p>
<pre><code>if thread_id &lt; 2:           # [1, 1, 1, 1] - all threads active
    pointer[thread_id] = 1  # [1, 1, 0, 0] - 2 active threads
else:
    pointer[thread_id] = 2  # [0, 0, 1, 1] - 2 active threads
</code></pre>
<p>So, the user may write a <em>divergent</em> (meaning lots of <code>if</code>/<code>else</code> branches that differ between threads) or otherwise suboptimal program, but they do not have to worry about whether parallelism was achieved or not.</p>
<p>SIMD is a programming model that allows for parallel execution of the <em>S</em>ame <em>I</em>nstruction on <em>M</em>ultiple <em>D</em>ata elements.</p>
<p>Unless users achieve SIMD by writing platform-specific intrinsics directly by specifying they want to use a particular assembly instruction to add two vectors of 8 32-bit floats together, they are relying on the compiler to generate the SIMD code for them.</p>
<p>The lack of trust in compiler to generate near-optimal SIMD code was a major hurdle to adoption.
Users savy enough to write their own assembly were always able to take advantage of the latest hardware, but this basically necessitates rewriting their code each time they want to leverage a new hardware target.</p>
<p>I believe SIMT was a part of the success of the CUDA programming model in part because of how reliably it achieves parallelism (<a href="https://youtu.be/GmNkYayuaA4?si=7ZDmisps4eHxMgr3">Stephen Jones discusses this in his 2025 GTC talk</a>, <a href="https://www.youtube.com/watch?v=139UPjoq7Kw">and this talk on scaling ML workloads had some interesting points too</a>).
With CUDA, users described their programs in terms of SIMT kernels, functions which execute in parallel.</p>
<p>With that in mind, in my ideal array language, users must be able to <em>opt in to SIMT programming</em>, but <em>achieve</em> SIMT programming through automatic parallelization.</p>
<h2 id="default-modes-of-parallelism"><a href="#default-modes-of-parallelism">Default Modes of Parallelism</a></h2>
<p>In this ideal language, a <em>descriptive</em> paradigm for parallelism should be the default while allowing users to opt in to a more <em>prescriptive</em> paradigm if they desire.
A descriptive model should be the default because it gives the compiler a huge amount of flexibility without putting a large burden on the user.</p>
<p>Users should be able to write SIMT kernels with really specific information about how the compiler should map the code to the hardware, while relying on automatic parallelization for most cases.</p>
<h2 id="array-aware-type-system"><a href="#array-aware-type-system">Array-Aware Type System</a></h2>
<p>The type system should be able to represent the shape, stride, and bounds of an array with automatic type inference.
I think the flexibility of OCaml’s type system would be a nice match.
Type annotations are not needed, but they are available if users or library authors would like to opt in to stricter constraints.
Some Hindley-Milner style type inference giving users the ability to opt in to type declarations while allowing the compiler to infer types and optimizer around array information when it’s available would be ideal.
This may be paired with a high-level LTO compilation system that allows the compiler to perform whole-program optimizations and infer more array information in the type system may allow for more aggressive optimizations.</p>
<p><a href="https://www.youtube.com/watch?v=3Lbs0pJ_OHI">This PLDI 2025 talk on representing array information in the type system</a> was really interesting - I don’t know if it’s my ideal, but it was fun to watch someone explore the space.</p>
<!-- Opt-out features:
- Automatic parallelization
- Automatic bufferization

Opt-in features:
- SIMT parallelism
- Manual memory management -->
<h2 id="syntax"><a href="#syntax">Syntax</a></h2>
<p>I purposefully do not have a strong opinion here.
I think the syntax is too dependent on the target audience and I don’t think my ideal language would succeed without a strong community of users.
That being said, the core algorithms of array programming must be representable in a <em>consistent</em> way.</p>
<p>Take numpy for example.
Coming to the language, one might expect a uniform way to perform generic algorithms like <code>reduce</code> and <code>map</code>.
You end up needing to define your own <code>ufunc</code> and call it like <code>&lt;ufunc&gt;.reduce()</code>.
Contrast this with a language like BQN, where a sum reduction is just <code>+´</code>.
I recognize APL-like languages are not approachable or maintainable for everyone, but the flexibility and uniformity of APL-like languages ought to be considered.</p>
<h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2>
<p>Hardware is getting more and more heterogeneous and strange, and programming languages need to be ready.
I believe that modelling programs as functional, unbufferized array programs is the most effective and flexible way to map a large domain of programs to the hardware of today and tomorrow.
As compiler engineers and folks generally working in the space of programming languages, the onus is on us to build programming languages and programming language facilities that are prepared for the hardware of the future.</p>
<h2 id="bonus-comparing-parallel-functional-array-languages"><a href="#bonus-comparing-parallel-functional-array-languages">Bonus: <em>Comparing Parallel Functional Array Languages</em></a></h2>
<p>After writing this post, my friend and colleague sent me the paper <a href="https://arxiv.org/abs/2505.08906"><em>Comparing Parallel Functional Array Languages: Programming and Performance</em></a>, which I found highly relevant.
These are the thoughts I jotted down while reading it.</p>
<p>I was unsure of this statement about APL:</p>
<div id="admonition-_25-the-apl-language_" role="note" aria-labelledby="admonition-_25-the-apl-language_-title">
<p>Almost all APL primitives have data parallel semantics making them a natural fit for SIMD architectures.
There are, however, two challenges: APL does not guarantee the associativity and referential transparency required for the safe parallel execution of operations like scan (\ or ⍀);
and, the high-level expression of parallel array operations may not necessarily map effectively to a specific SIMD architecture.</p>
</div>
<p>In my understanding, APL does not allow users to obtain reference to arrays directly, so every object has value semantics, and if the compiler elides buffers that are not needed, the user ought never notice.</p>
<p>I loved section <em>3.2. Array Representation</em>, and tables 1, 2, and 3.
The comparison of type systems in various array languages was extremely informative, and I left wanting to learn more about Futhark.</p>
<p>Section <em>3.3. Parallel Computation Paradigms</em> made note of the restrictions on higher-ordered functions in array programming languages, which I think is helpful to note for usual functional programmers who are perhaps used to higher-ordered functions all over the place.
One must not use higher-ordered functions too liberally in array languages for performance reasons; although functional programming <em>can</em> lend itself to highly performant parallel code, it is not due to thier support for higher-ordered functions.</p>
<p>If I could write another paper with these authors, I would title it <em>The Unreasonable Optimizability of Functional, Unbufferized Array Languages</em>.
I believe it captures the essence of what I care about when it comes to array languages.
While the paper was endlessly fascinating, I am less interested in the syntax of the languages and more interested in the semantics and the optimizations made available to the compiler by virtue of those semantics.
The lines-of-code comparisons were interesting and the ease with which the user interacts with the language is important, but the <em>optimizability</em> of languages which are capable of representing their programs as functional, unbufferized and array-oriented is what I find most compelling.</p>
<p>This is why I am still so interested in Fortran even though it is largely a procedural language - the Flang compiler is able to represent programs in an unbufferized array-oriented SSA intermediate representation, which lends itself so well to optimization.</p>

                    </main>

                    <nav aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="https://www.ashermancinelli.com/csblog/2025-7-21-SMT-Solver-Interview.html" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i></i>
                            </a>

                            <a rel="next prefetch" href="https://www.ashermancinelli.com/csblog/values.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i></i>
                            </a>

                        
                    </nav>
                </div>
            </div>

            <nav aria-label="Page navigation">
                    <a rel="prev" href="https://www.ashermancinelli.com/csblog/2025-7-21-SMT-Solver-Interview.html" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i></i>
                    </a>

                    <a rel="next prefetch" href="https://www.ashermancinelli.com/csblog/values.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>
            </nav>

        </div>




        


        
        
        

        
        
        

        <!-- Custom JS scripts -->
        
        



    </div>
    

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Century-old stone “tsunami stones” dot Japan's coastline (2015) (135 pts)]]></title>
            <link>https://www.smithsonianmag.com/smart-news/century-old-warnings-against-tsunamis-dot-japans-coastline-180956448/</link>
            <guid>44785107</guid>
            <pubDate>Mon, 04 Aug 2025 12:51:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.smithsonianmag.com/smart-news/century-old-warnings-against-tsunamis-dot-japans-coastline-180956448/">https://www.smithsonianmag.com/smart-news/century-old-warnings-against-tsunamis-dot-japans-coastline-180956448/</a>, See on <a href="https://news.ycombinator.com/item?id=44785107">Hacker News</a></p>
Couldn't get https://www.smithsonianmag.com/smart-news/century-old-warnings-against-tsunamis-dot-japans-coastline-180956448/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[DrawAFish.com Postmortem (271 pts)]]></title>
            <link>https://aldenhallak.com/blog/posts/draw-a-fish-postmortem.html</link>
            <guid>44784743</guid>
            <pubDate>Mon, 04 Aug 2025 12:10:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aldenhallak.com/blog/posts/draw-a-fish-postmortem.html">https://aldenhallak.com/blog/posts/draw-a-fish-postmortem.html</a>, See on <a href="https://news.ycombinator.com/item?id=44784743">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <div>
                <p><img src="https://aldenhallak.com/blog/images/drawafish-hero.jpg" alt="DrawAFish.com website screenshot - 100 hand drawn fish swimming in a tank."></p><p>DrawAFish.com</p>
            </div>

            <h2>TL;DR:</h2>
            <ul>
                <li><strong>Incident Duration:</strong> ~6 hours (2AM–8AM EST)</li>
                <li><strong>Impact:</strong>
                    <ul>
                        <li>Username vandalism (slurs)</li>
                        <li>Offensive fish approved / safe fish removed</li>
                    </ul>
                </li>
                <li><strong>Root Causes:</strong>
                    <ol>
                        <li>Legacy 6-digit admin password exposed in past data breach</li>
                        <li>Username update API lacked authentication</li>
                        <li>JWT not tied to specific user</li>
                    </ol>
                </li>
                <li><strong>Mitigation:</strong> Manual reversal of mod actions, fixed authorization logic, backups reviewed</li>
                <li><strong>Takeaway:</strong> hwoopsy daisy 🙂</li>
            </ul>

            <div>
                <p><img src="https://aldenhallak.com/blog/images/hackernews-pointing.jpg" alt="Post 'Show HN: Draw A Fish and watch it swim with the others' on the #1 spot of hacker news, highlighted, circled, lots of red arrows pointing to it."></p><p>Did you see? Did you see it? What it says? What it says on top of the website?
                </p>
            </div>

            <p>If you were on HackerNews on <a href="https://news.ycombinator.com/front?day=2025-08-01" target="_blank">Aug 1 2025</a>, you may have seen <a href="https://drawafish.com/">DrawAFish.com</a>.
                Because it was in the number 1 spot. You also may have seen it if you follow me on <a href="https://instagram.com/verybigandstrong" target="_blank">instagram</a>. You also probably saw
                that I was in the #1 spot on Hackernews there too. Because I posted about it a lot. And also if you
                talked to me in person you probably heard about it. And then you probably heard a lot of quotes from
                <i>The Social Network</i> (2010) where I replaced various words with "Fish."</p>

            <blockquote>
                "A million fish isn't cool. You know what's cool? <a href="https://www.youtube.com/watch?v=4e0n7vTLz1U" target="_blank">A billion fish</a>."
            </blockquote>

            <p>If you read the post on Hackernews, you saw that the website was an exercise in
                vibe-coding. I used Copilot to implement features, and I used it to implement features <i>fast</i>.</p>

            <p> In my career,<sup><a href="#footnote-0" id="ref-0">[0]</a></sup> I have learned the art of "Blameless
                Postmortems." The problem with a Blameless
                Postmortem is that it doesn't really work when you're the sole contributor. So this is a blameful
                postmortem. And I blame me. (Not the LLM, sorry).</p>

            <p>If you saw the website only on August 1, you probably do not understand why I need to write a postmortem
                at
                all. Everything went swimmingly (ha, ha). But if you had the displeasure of viewing my website between
                the hours of 2AM (20 minutes after I went to sleep) and 8AM (when I woke up)<sup><a href="#footnote-1" id="ref-1">[1]</a></sup> EST on Aug 3, then you would have seen chaos. Every single username was
                transformed to a heinous slur, many unsavory fish had made it into the fishtank, and many beautiful fish
                were gone. How did this happen?</p>

            <h2>The Vulnerabilities</h2>

            <div>
                <p><img src="https://aldenhallak.com/blog/images/hacked.jpg" alt="Screenshot showing a hacked user profile with username changed to 'Stinky Fart Man'"></p><p>This is not what they changed my username to. I'll let you use your
                    imagination.</p>
            </div>

            <ol>
                <li>
                    <p><strong>Legacy admin password exposed in data breach:</strong> When creating the website, I used
                        my childhood username and my childhood 6-digit password for testing. I think the first time it
                        leaked was on Neopets.com.<sup><a href="#footnote-2" id="ref-2">[2]</a></sup> I then set up
                        Google Auth. From then on, I only logged in with Google Auth. But the password remained. I
                        simply forgot. This allowed some of the most intelligent and brilliant minds on the internet to
                        find my password on the Neopets data leak paste, log in as an admin, and approve and disapprove
                        some really disgusting and horrible fish.</p>

                    <p>On the plus side, the brilliant minds (while attempting to ban every single user) managed to
                        accidentally ban themselves. So that vulnerability was only an active issue for an hour or so.
                    </p>
                </li>

                <li>
                    <p><strong>Username update API lacked authentication:</strong> I vibe coded the profile backend,
                        which allows users to modify their username. I added this feature last minute, figured I'd
                        review it later, and then didn't. The username modification did not perform any auth logic
                        whatsoever. whoopsy</p>
                </li>

                <li>
                    <p><strong>JWT not tied to specific user:</strong> There was another security vulnerability - I
                        used the JWT to authorize login, but never confirmed that the JWT token belonged to the userId /
                        email associated with it in the admin actions. So you could log in with my username and password,
                        grab the JWT, and then send that along with your request. While this was a mistake, it ended up
                        saving me. Fortunately, hackernews user @<a href="https://news.ycombinator.com/user?id=iceweaselfan44" target="_blank">iceweaselfan44</a> used this vulnerability to authorize as an admin and
                        delete the really bad fish. He was awake, on the other side of the world, removing fish and
                        helping out.<sup><a href="#footnote-3" id="ref-3">[3]</a></sup></p>
                </li>
            </ol>

            <h2>The Recovery</h2>

            <div>
                <p><img src="https://aldenhallak.com/blog/images/moderation-logs.jpg" alt="A moderation log that shows a moderation action done at 3AM and undone in the morning."></p><p>Moderation logs. There are so many of these.</p>
            </div>

            <p>I woke up at around 7:45 am, saw a couple pings and messages, and immediately rushed to my desktop.<sup><a href="#footnote-4" id="ref-4">[4]</a></sup> Fortunately, I had set up firebase backups.
                Unfortunately, I had set them up wrong. I spent a good hour or so diagnosing the errors and then quickly
                pushed changes to require authentication.</p>

            <p>I had a mod log set up, so undoing the changes was as simple as writing an annoying script that just
                undid all the mod actions. Did I learn my lesson about vibe coding? Maybe. I vibe coded the script,
                looked over the code, and did a dry run first.</p>

            <p>I banned the other mod account ran by IceWeasel<sup><a href="#footnote-5" id="ref-5">[5]</a></sup> and
                patched the JWT bug - at which point he reached out to me and explained how he created it. We then
                hopped on a call and he took a look at the codebase, where he expertly suggested a refactor that would
                be more idiomatic with current security practices. And then when I blindly pushed it and it broke
                everything, he expertly committed some more changes that fixed it.</p>

            <h2>Reflection</h2>

            <p>At this point, you're probably thinking: geeze man. Wow. youve gotta be stupid.</p>

            <p>And I'd like to get on my hands and knees and beg for your suspension of belief when I say: yes I am but
                not when it comes to my job. It is really fun to just have high velocity, and it is really fun to not do
                code reviews and to just push stuff. Sometimes it feels like all I do at work is review code and write
                docs - and the code I write at work is lately deeply within legacy systems and makes my brain hurt
                sometimes. I have good reviews! Just ask my coworkers! The ones that hate me hate me because I leave TOO
                many comments and am TOO thorough (or they are jealous of my handsome good looks and devilish charm).
            </p>

            <p>So when I decided to learn how to build on GCP and vibe code a small app that I figured a handful of
                people would see, I took it easy on the code reviews. I let Copilot do all the work, I wrote no tests,
                and instead of writing TODOs and Documentation I simply said "I'll remember to change my password / add
                auth / understand this code later." And then I didn't do that. Whoops!</p>

            <p>It would be very nice for my ego to blame the LLM here. But LLMs are a tool. They let you generate a lot
                of code really fast, and sometimes that is good. Sometimes it is not. And it is up to you to review it,
                and decide what code gets committed.</p>

            <!-- Lessons learned image suggestion: Meme or infographic about vibe coding vs proper development -->
            <div>
                <p><img src="https://aldenhallak.com/blog/images/llms-dont-commit.jpg" alt="Very crudely draw MS-Paint sticker for 'LLMs don't commit code, people do'. It's drawn over a sticker that says 'Guns don't kill people, people do.'"></p><p>I am selling these stickers for 100 dollars each. Please reach out to purchase
                    them.</p>
            </div>

            <hr>

            <p><small><sup id="footnote-0">[0]</sup> Nearly 5 years at the same company. <a href="#ref-0">↩</a></small>
            </p>

            <p><small><sup id="footnote-1">[1]</sup> I usually try to sleep more than this. I want to sleep more than
                    this. But my fancy "smart" ikea roller blinds that keep it dark at night and bright when I wake up
                    fell down because I taped them to the wall very poorly. <a href="#ref-1">↩</a></small></p>

            <p><small><sup id="footnote-2">[2]</sup> I remember being a kid and forgetting my password and thinking "wow
                    it's nice how they just email you your password, instead of resetting it. More websites should do
                    that." <a href="#ref-2">↩</a></small></p>

            <p><small><sup id="footnote-3">[3]</sup> The JWT token issue was the only vuln that really required knowing
                    how anything worked. I don't think it's a coincidence that there was a correlation between being a
                    studious / diligent person and actively being a helpful force. <a href="#ref-3">↩</a></small></p>

            <p><small><sup id="footnote-4">[4]</sup> Which is next to my bed. I live in New York.<sup><a href="#footnote-4a" id="ref-4a">[4a]</a></sup><br>
                    <sup id="footnote-4a">[4a]</sup> You may have heard differently, especially if you saw my <a href="https://imgur.com/WDqhtIu.jpg" target="_blank">doxxing</a><sup><a href="#footnote-4b" id="ref-4b">[4b]</a></sup> on the unsavory website. Fortunately, my publicly listed
                    information appears to be a little out of date. <a href="https://x.com/MCBananaPeelZ/status/1774139530041303368" target="_blank">The first time I
                        was doxxed</a> was much scarier. <a href="#ref-4a">↩</a><br>
                    <sup id="footnote-4b">[4b]</sup> To be fair, I deserved the doxxing for removing the weird and
                    offensive fish and slurs. <a href="#ref-4b">↩</a> <a href="#ref-4">↩</a></small></p>

            <p><small><sup id="footnote-5">[5]</sup> Only because it had no identifying information at all - just didn't
                    know who this person was. He reached out later and we chatted over discord and now we're cool. <a href="#ref-5">↩</a></small></p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Palantir Is Extending Its Reach Even Further into Government (233 pts)]]></title>
            <link>https://www.wired.com/story/palantir-government-contracting-push/</link>
            <guid>44784498</guid>
            <pubDate>Mon, 04 Aug 2025 11:44:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/palantir-government-contracting-push/">https://www.wired.com/story/palantir-government-contracting-push/</a>, See on <a href="https://news.ycombinator.com/item?id=44784498">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>President Donald Trump’s</span> administration has dramatically expanded its work with Palantir, elevating the company cofounded by Trump ally Peter Thiel as the government’s go-to software developer. Following massive contract terminations for consulting giants and government contractors like Accenture, Booz Allen, and Deloitte, Palantir has emerged ahead. Now the data analytics firm is partnering with those companies—offering them a lifeline while consolidating its own power.</p><p>Palantir has become one of the few winners in the Trump administration’s cost-cutting efforts, <a data-offer-url="https://www.nytimes.com/2025/05/30/technology/trump-palantir-data-americans.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.nytimes.com/2025/05/30/technology/trump-palantir-data-americans.html&quot;}" href="https://www.nytimes.com/2025/05/30/technology/trump-palantir-data-americans.html" rel="nofollow noopener" target="_blank">receiving more than $113 million</a> in federal spending since the beginning of the year, according to The New York Times. Palantir’s US government revenue has grown by more than $ 370 million compared to this time last year, according to the company’s <a data-offer-url="https://investors.palantir.com/news-details/2025/Palantir-Reports-Q1-2025-Revenue-Growth-of-39-YY-U-S--Revenue-Growth-of-55-YY-Raises-FY-2025-Revenue-Guidance-to-36-YY-Growth-and-U-S--Comm-Revenue-Guidance-to-68-YY-Crushing-Consensus-Expectations/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://investors.palantir.com/news-details/2025/Palantir-Reports-Q1-2025-Revenue-Growth-of-39-YY-U-S--Revenue-Growth-of-55-YY-Raises-FY-2025-Revenue-Guidance-to-36-YY-Growth-and-U-S--Comm-Revenue-Guidance-to-68-YY-Crushing-Consensus-Expectations/&quot;}" href="https://investors.palantir.com/news-details/2025/Palantir-Reports-Q1-2025-Revenue-Growth-of-39-YY-U-S--Revenue-Growth-of-55-YY-Raises-FY-2025-Revenue-Guidance-to-36-YY-Growth-and-U-S--Comm-Revenue-Guidance-to-68-YY-Crushing-Consensus-Expectations/" rel="nofollow noopener" target="_blank">most recent quarterly earnings report</a>. Before making remarks at last week’s AI Summit in DC, Trump thanked a variety of cabinet secretaries and tech leaders, <a href="https://www.youtube.com/live/tBNX9x5GgPE?feature=shared&amp;t=922">including Palantir chief technology officer Shyam Sankar</a>. “We buy a lot of things from Palantir,” Trump said. “Are we paying our bills? I think so.”</p><p>Instead of replacing these more traditional contractors, Palantir’s software is becoming the core tool deployed by them in government systems, placing Palantir in a newly central role.</p><p>The White House itself is thrilled by this partnership: “The Trump Administration has high-standard [sic] when spending American’s hard-earned tax dollars—which is why agencies have partnered with Palantir, a top-tier American company renowned for their longstanding history of innovation, results, and increasing government efficiency,” says White House spokesperson Taylor Rogers.</p><p>Palantir did not immediately respond to requests for comment.</p><p>In April, <a href="https://www.wired.com/story/palantir-doge-irs-mega-api-data/">WIRED reported that Palantir was working alongside IRS engineers</a> to build what sources called a “mega API,” which would unify all data across the agency. An API, or application programming interface, enables applications and databases to exchange data and possibly compare it against other interoperable datasets. Once completed, this mega API could become the “read center of all IRS systems.” Immigration and Customs Enforcement <a href="https://www.wired.com/story/ice-palantir-immigrationos/">contracted Palantir for $30 million</a> to track self-deportations in April. The company has also won federal contracts more recently, like a $795 million award from the Pentagon in May to expand its Maven Smart System program. The total contract ceiling for the Army’s Maven program is now $1.3 billion.</p><p>This growth comes as some of the companies Palantir has chosen to partner with have lost billions in government contract cuts. In April, defense secretary Pete Hegseth announced plans to cut $5.1 billion in IT consulting contracts with companies including Accenture, Booz Allen, and Deloitte. <a href="https://media.defense.gov/2025/Apr/10/2003687449/-1/-1/1/SECRETARY-OF-DEFENSE-PETE-HEGSETH-UPDATE-ON-CONTINUING-ELIMINATION-OF-WASTEFUL-SPENDING-AT-THE-DOD.PDF">In a memo announcing the cuts</a>, Hegseth said that the Pentagon would be forced to bring more of its IT work in-house.</p><p>“These contracts represent non-essential spending on third party consultants to perform services more efficiently performed by the highly skilled members of our DoD workforce using existing resources,” Hegseth wrote.</p><p>Palantir’s partnerships with these companies vary, but each one makes it easier for Palantir to extend the reach of its software and AI technology across the federal government. With Accenture’s government branch, Palantir will train and certify at least 1,000 Accenture workers on its Foundry software as well as its AI tech, <a data-offer-url="https://newsroom.accenture.com/news/2025/palantir-and-accenture-federal-services-join-forces-to-help-federal-government-agencies-reinvent-operations-with-ai" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://newsroom.accenture.com/news/2025/palantir-and-accenture-federal-services-join-forces-to-help-federal-government-agencies-reinvent-operations-with-ai&quot;}" href="https://newsroom.accenture.com/news/2025/palantir-and-accenture-federal-services-join-forces-to-help-federal-government-agencies-reinvent-operations-with-ai" rel="nofollow noopener" target="_blank">according to an Accenture press release</a> The companies also said that together they could create “a 360-degree view” of government agency budgets, something the so-called Department of Government Efficiency (DOGE) has sought to build and use to review federal spending. (Palantir <a data-offer-url="https://t.co/5mcFHsKEje" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://t.co/5mcFHsKEje&quot;}" href="https://t.co/5mcFHsKEje" rel="nofollow noopener" target="_blank">partnered with Accenture before in 2022</a>, but this is the first partnership to focus on US government clients.)</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>“We are teaming up with Accenture Federal Services to accelerate AI across the U.S. Government, working to address federal agencies’ highest-priority operational challenges,” Palantir posted to X last month.</p><p>"What makes this partnership so uniquely powerful is Accenture’s expertise working with the federal government and our ability to bring commercial capabilities to government solutions, combined with Palantir’s deep experience in government software," Julie Sweet, chair and CEO of Accenture, said in a press release. “Together, we will harness the ever-growing power of AI to help the federal government succeed in its critical mission to modernize and reinvent its operations—with stronger data flows, transparency and resilience—to better serve warfighters, citizens and all its stakeholders.”</p><p>Accenture did not immediately respond to a request for comment.</p><p>While Palantir has become a major government contractor in its own right, partnering with contracting giants could enable the software company to scale at a much faster rate, leveraging long-standing relationships these larger contractors have with virtually every federal agency. “It's actually a pretty savvy business decision on the part of both Palantir, then also what you would call a traditional, more legacy-oriented, like defense or just government contractors,” says Jessica Tillipman, associate dean for government procurement law at George Washington University. “If they’re newer to certain areas and others have that footprint, that’s how it would benefit Palantir.”</p><p>Last week, Palantir and Deloitte announced a partnership that includes what they call the “Enterprise Operating System” (EOS) to unify data across organizations. At government agencies like the Internal Revenue Service and reportedly at the Social Security Administration (SSA), Palantir is already working to combine agency datasets, allowing what were previously disparate datasets to communicate with one another more easily.</p><p>"Deloitte shares Palantir's commitment to decisive action and a dedication to delivering meaningful, lasting results for commercial and government clients," said Jason Girzadas, Deloitte US CEO, said in a press release announcing the partnership. "Expanding our preferred relationship at this pivotal moment provides our clients with Palantir's latest advances in AI, combined with Deloitte's engineering scale and deep sector experience."</p><p>Deloitte did not immediately respond to a request for comment.</p><p>Palantir struck some of these deals prior to Trump taking office as well. In December of last year, <a data-offer-url="https://www.boozallen.com/menu/media-center/q3-2025/partnership-to-accelerate-defense-mission-innovation.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.boozallen.com/menu/media-center/q3-2025/partnership-to-accelerate-defense-mission-innovation.html&quot;}" href="https://www.boozallen.com/menu/media-center/q3-2025/partnership-to-accelerate-defense-mission-innovation.html" rel="nofollow noopener" target="_blank">Booz Allen partnered with Palantir</a> specifically, working together on building out defense IT infrastructure.</p><p>“To have one company monopolize and become the gatekeeper of software in the government, to become an ‘app factory,’ for the government, in a sense, where they're in every agency, they're part of the defense complex and the intelligence complex, brings huge concerns regarding fairness, regarding competition, and puts Palantir in a very unique position that maybe has never existed,” says Juan Sebastián Pinto, a former Palantir employee and critic of the company.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[KDE Plasma prepares crackdown on focus-stealing window behavior under Wayland (128 pts)]]></title>
            <link>https://www.neowin.net/news/kde-plasma-prepares-crackdown-on-focus-stealing-window-behavior-under-wayland/</link>
            <guid>44784312</guid>
            <pubDate>Mon, 04 Aug 2025 11:22:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.neowin.net/news/kde-plasma-prepares-crackdown-on-focus-stealing-window-behavior-under-wayland/">https://www.neowin.net/news/kde-plasma-prepares-crackdown-on-focus-stealing-window-behavior-under-wayland/</a>, See on <a href="https://news.ycombinator.com/item?id=44784312">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                                    <span>When you purchase through links on our site, we may earn an affiliate commission. <a href="https://www.neowin.net/terms">Here’s how it works</a>.</span>
                        
                        
                        <p>
    
    <time datetime="Aug 4, 2025 07:20 EDT" pubdate="pubdate">
    Aug 4, 2025 07:20 EDT
    </time>
         · <span>Hot!</span>    
    
    
    </p>

                    </div><div itemprop="articleBody">
                                                                        <p><img alt="KDE Plasma" src="https://cdn.neowin.com/news/images/uploaded/2025/06/1750516218_kde-plasma-2.webp"></p>

<p>One of the most <a href="https://www.neowin.net/news/kde-is-fixing-blurry-screens-by-snapping-almost-1x-scale-factors-back-to-1x-on-wayland/">interesting things about Wayland</a> is how it handles window focus, unlike X11, where focus stealing can be frustrating and even a security risk. Its main advantage is a mechanism that prevents focus stealing. The protocol that plays a role in this is known as "XDG Activation."</p>

<p>Here's how it works: Say you double-click a PDF file in your file manager. The file manager first asks the Wayland compositor for a special, single-use "activation token". This request is tied directly to your click, proving a human wanted this to happen.</p>

<p>The file manager then launches your PDF viewer and hands it the token. The PDF viewer, upon starting, shows this token to the compositor and asks to be activated. The compositor checks if the token is legit and, if so, gives the PDF viewer focus.</p>

                            <!-- PLACE THIS SECTION INSIDE OF YOUR BODY WHERE YOU WANT THE VIDEO PLAYER TO RENDER -->
            <p>If the token is missing, old, or otherwise invalid, the compositor says no. The viewer window will not get focus, and instead, its icon in the taskbar will start flashing to grab your attention.</p>

<figure><a href="https://cdn.neowin.com/news/images/uploaded/2025/08/1754304760_bildschirmfoto_20250731_143103-1024x740.webp"><img alt="KWrite text editor window window has no focus colors are softened Task bar with a couple of apps KWrite icon has an orange background behind it indicating KWrite is demanding attention" height="740" src="https://cdn.neowin.com/news/images/uploaded/2025/08/1754304760_bildschirmfoto_20250731_143103-1024x740.webp" width="1024"></a>

<figcaption>Flashing icon in the taskbar | Image: <a href="https://blog.broulik.de/2025/08/on-window-activation/">Kai-Uwe Broulik</a></figcaption></figure><p>Kai-Uwe Broulik, a KDE developer, recently wrote about the plan to "switch on KWin’s focus stealing on Wayland at a low level". This means KWin, the window manager for KDE Plasma, will begin enforcing this properly.</p>

<p>Under X11, new or dialog windows can only grab focus if their application was most recently active, a check often based on a timestamp called <code>_NET_WM_USER_TIME</code>. It was a flimsy system at best. For example, Kai cited how the prevention logic on X11 would sometimes stop the Adobe Flash Player fullscreen window from showing on top of a YouTube video. On X11, an application could just call <strong><code>XSetInputFocus</code></strong> on another app's window, and while KWin would try to undo it, focus did flicker for a moment.</p>

<p>Over on Wayland, things are much better with XDG Activation, but some apps still violate the protocol through improper usage. In situations like that, KWin would, by default, just focus any new window that opened. This is changing.</p>

<p>A new "Extreme" setting for "Focus Stealing Prevention" in the Window Management settings will force KWin to activate a window if and only if it requests activation with a valid token.</p>

<figure><a href="https://cdn.neowin.com/news/images/uploaded/2025/08/1754304876_screenshot_20250802_100046-1024x658.webp"><img alt="Window Behavior configuration dialog various window-related tabs and options mouse cursor pointing at a combo box Focus stealing prevention whose current item is Extreme" height="658" src="https://cdn.neowin.com/news/images/uploaded/2025/08/1754304876_screenshot_20250802_100046-1024x658.webp" width="1024"></a>

<figcaption>Image: <a href="https://blog.broulik.de/2025/08/on-window-activation/">Kai-Uwe Broulik</a></figcaption></figure><p>Using this stricter mode, developers Xaver Hugl and Kai-Uwe Broulik have already fixed a ton of issues, introducing several key changes:</p>

<ul>
<li>Dolphin no longer discards its activation token when launching a new instance.</li>
	<li>KRunner, Kickoff, and other Plasmoid popups now correctly request activation.</li>
	<li>LayerShell-Qt now requests activation on show and properly reads the <strong><code>XDG_ACTIVATION_TOKEN</code></strong> from the environment.</li>
	<li>Allowing privileged clients like Plasma to request tokens correctly.</li>
	<li>Ignoring modifier key presses for focus prevention logic, since they are often part of a global shortcut.</li>
</ul>
<p>The work extends to the backend as well, with the DBusRunner specification now gaining a <code>SetActivationToken</code> method that is called just before an action runs. Baloo, the desktop search runner, now uses this to ensure that opening files in an existing application window works correctly.</p>

<p>You can learn more on <a href="https://blog.broulik.de/2025/08/on-window-activation/">Kai's blog post.</a></p>
                        
                        
                                                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GHz spiking neuromorphic photonic chip with in-situ training (111 pts)]]></title>
            <link>https://arxiv.org/abs/2506.14272</link>
            <guid>44784297</guid>
            <pubDate>Mon, 04 Aug 2025 11:21:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2506.14272">https://arxiv.org/abs/2506.14272</a>, See on <a href="https://news.ycombinator.com/item?id=44784297">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2506.14272">View PDF</a></p><blockquote>
            <span>Abstract:</span>Neuromorphic photonic computing represents a paradigm shift for next-generation machine intelligence, yet critical gaps persist in emulating the brain's event-driven, asynchronous dynamics,a fundamental barrier to unlocking its full potential. Here, we report a milestone advancement of a photonic spiking neural network (PSNN) chip, the first to achieve full-stack brain-inspired computing on a complementary metal oxide semiconductor-compatible silicon platform. The PSNN features transformative innovations of gigahertz-scale nonlinear spiking dynamics,in situ learning capacity with supervised synaptic plasticity, and informative event representations with retina-inspired spike encoding, resolving the long-standing challenges in spatiotemporal data integration and energy-efficient dynamic processing. By leveraging its frame-free, event-driven working manner,the neuromorphic optoelectronic system achieves 80% accuracy on the KTH video recognition dataset while operating at ~100x faster processing speeds than conventional frame-based approaches. This work represents a leap for neuromorphic computing in a scalable photonic platform with low latency and high throughput, paving the way for advanced applications in real-time dynamic vision processing and adaptive decision-making, such as autonomous vehicles and robotic navigation.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Xuhan Guo [<a href="https://arxiv.org/show-email/1e050960/2506.14272" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Tue, 17 Jun 2025 07:37:25 UTC (4,396 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mastercard deflects blame for NSFW games being taken down (527 pts)]]></title>
            <link>https://www.pcgamer.com/games/mastercard-deflects-blame-for-nsfw-games-being-taken-down-but-valve-says-payment-processors-specifically-cited-a-mastercard-rule-about-damaging-the-brand/</link>
            <guid>44783566</guid>
            <pubDate>Mon, 04 Aug 2025 09:27:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pcgamer.com/games/mastercard-deflects-blame-for-nsfw-games-being-taken-down-but-valve-says-payment-processors-specifically-cited-a-mastercard-rule-about-damaging-the-brand/">https://www.pcgamer.com/games/mastercard-deflects-blame-for-nsfw-games-being-taken-down-but-valve-says-payment-processors-specifically-cited-a-mastercard-rule-about-damaging-the-brand/</a>, See on <a href="https://news.ycombinator.com/item?id=44783566">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN.jpg" alt="KIEV, UKRAINE - 2020/01/24: In this photo illustration the Bank cards mastercard on computer keyboard. (Photo Illustration by Igor Golovniov/SOPA Images/LightRocket via Getty Images)" srcset="https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/3KG3rFXYAbaPno99GQK8VN.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: Getty Images)</span>
</figcaption>
</div>

<div id="article-body">
<p>Two weeks after Valve confirmed it had <a data-analytics-id="inline-link" href="https://www.pcgamer.com/software/platforms/valve-confirms-credit-card-companies-pressured-it-to-delist-certain-adult-games-from-steam/" data-before-rewrite-localise="https://www.pcgamer.com/software/platforms/valve-confirms-credit-card-companies-pressured-it-to-delist-certain-adult-games-from-steam/">removed a pile of NSFW games from Steam</a> because of pressure from credit card companies—and one week after <a data-analytics-id="inline-link" href="https://www.pcgamer.com/gaming-industry/itch-io-latest-in-platforms-pressured-by-credit-card-companies-as-well-as-activist-group-collective-shout-which-has-successfully-caught-an-award-winning-indie-and-more-in-the-crossfire/" data-before-rewrite-localise="https://www.pcgamer.com/gaming-industry/itch-io-latest-in-platforms-pressured-by-credit-card-companies-as-well-as-activist-group-collective-shout-which-has-successfully-caught-an-award-winning-indie-and-more-in-the-crossfire/">Itch.io followed suit</a>—Mastercard has <a data-analytics-id="inline-link" href="https://www.mastercard.com/us/en/news-and-trends/press/2025/august/clarifying-recent-headlines-on-gaming-content.html" target="_blank" data-url="https://www.mastercard.com/us/en/news-and-trends/press/2025/august/clarifying-recent-headlines-on-gaming-content.html" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">released a statement</a> denying responsibility for the takedowns, saying it "allows all lawful purchases" to be processed through its systems.</p><p>"Mastercard has not evaluated any game or required restrictions of any activity on game creator sites and platforms, contrary to media reports and allegations," the company says<em>.</em></p><p>"Our payment network follows standards based on the rule of law. Put simply, we allow all lawful purchases on our network. At the same time, we require merchants to have appropriate controls to ensure Mastercard cards cannot be used for unlawful purchases, including illegal adult content."</p><p>It's an odd statement at first glance, because while the content removed by Steam and Itch.io may violate laws in some countries, it's fully legal in the US—objectionable and gross as hell in some cases, sure, but still within the boundaries of the law. Apart from that, both Valve and Itch.io explicitly stated that payment processors are the reason games were deindexed or removed from sale entirely.</p><p>Mastercard's 'out' here seems to be found in the structure of its operations. The company's website says it is "neither an issuer [a merchant bank] nor an acquirer [a bank, credit union, or other entity that provides debit cards or lines of credit to consumers]," but rather that it "<a data-analytics-id="inline-link" href="https://sea.mastercard.com/en-region-sea/business/merchants/start-accepting/payment-process.html" target="_blank" data-url="https://sea.mastercard.com/en-region-sea/business/merchants/start-accepting/payment-process.html" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">provide[s] the technology and the network that power transactions</a>." In other words, Mastercard does not process payments, it facilitates the systems that do.</p><p>In the case of Itch.io, the platform specifies in its "<a data-analytics-id="inline-link" href="https://itch.io/updates/update-on-nsfw-content" target="_blank" data-url="https://itch.io/updates/update-on-nsfw-content" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">update on NSFW content</a>" that its payment processors are Paypal and Stripe—and Stripe, which supports a <a data-analytics-id="inline-link" href="https://stripe.com/ae/payments/payment-methods" target="_blank" data-url="https://stripe.com/ae/payments/payment-methods" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">range of payment methods</a> including Visa and Mastercards, was the one <a data-analytics-id="inline-link" href="http://itch.io/" target="_blank" data-url="http://itch.io" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Itch.io</a> "suspended the ability to pay with for 18+ content for the foreseeable future."</p><p>Similarly, both <a data-analytics-id="inline-link" href="https://www.verotel.com/en/index.html?lang=en" target="_blank" data-url="https://www.verotel.com/en/index.html?lang=en" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Verotel</a> and <a data-analytics-id="inline-link" href="https://ccbill.com/" target="_blank" data-url="https://ccbill.com/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">CCBill</a>, which were recently recommended by the IGDA as "alternatives to overly risk-averse financial partners," accept major credit cards as payment methods.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-tMB45j7AvDWp2uiMV2GJFb"><section><p>Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team.</p></section></div><p>In a statement provided to PC Gamer, Valve said that it had tried to work things out with Mastercard directly prior to removing the games, and suggested that Mastercard did have at least an indirect influence on the outcome.</p><p>"Mastercard did not communicate with Valve directly, despite our request to do so," a Valve representative said. "Mastercard communicated with payment processors and their acquiring banks. Payment processors communicated this with Valve, and we replied by outlining Steam’s policy since 2018 of attempting to distribute games that are legal for distribution.</p><p>"Payment processors rejected this, and specifically cited Mastercard’s Rule 5.12.7 and risk to the Mastercard brand."</p><figure><blockquote><p>"Mastercard did not communicate with Valve directly, despite our request to do so."</p><figcaption><cite>Valve representative</cite></figcaption></blockquote></figure><p><a data-analytics-id="inline-link" href="https://www.mastercard.us/content/dam/public/mastercardcom/na/global-site/documents/mastercard-rules.pdf" target="_blank" data-url="https://www.mastercard.us/content/dam/public/mastercardcom/na/global-site/documents/mastercard-rules.pdf" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Mastercard's Rule 5.12.7</a> relates to "illegal or brand-damaging transactions," and states:</p><p><em>A Merchant must not submit to its Acquirer, and a Customer must not submit to the Interchange System, any Transaction that is illegal, or in the sole discretion of the Corporation, may damage the goodwill of the Corporation or reflect negatively on the Marks.</em></p><p>That includes, according to the rules, any product or services that "is patently offensive and lacks serious artistic value (such as, by way of example and not limitation, images of nonconsensual sexual behavior, sexual exploitation of a minor, nonconsensual mutilation of a person or body part, and bestiality), or any other material that the Corporation deems unacceptable to sell in connection with a Mark."</p><p>Acquirers—the financial institutions that provide the cards and lines of credit—who fail to take action in response to complaints are subject to significant penalties, monetary and otherwise.</p><p>Regardless of the role Mastercard played in all of this, directly or indirectly, its "clarification" sure makes it seem like the pressure is being felt. Which from a gamer perspective, at least, is a good thing: As others have mentioned, a public pressure campaign certainly seemed to work for Collective Shout, the Australian anti-porn crusaders who <a data-analytics-id="inline-link" href="https://www.pcgamer.com/gaming-industry/australian-anti-porn-group-claims-responsibility-for-steams-new-censorship-rules-in-victory-against-porn-sick-brain-rotted-pedo-gamer-fetishists-and-things-only-get-weirder-from-there/" data-before-rewrite-localise="https://www.pcgamer.com/gaming-industry/australian-anti-porn-group-claims-responsibility-for-steams-new-censorship-rules-in-victory-against-porn-sick-brain-rotted-pedo-gamer-fetishists-and-things-only-get-weirder-from-there/">started this whole thing</a>—regardless of how the gears turn behind the curtain, there's no reason to think it can't work the other way too.</p>
</div>


<div id="slice-container-authorBio-tMB45j7AvDWp2uiMV2GJFb"><p>Andy has been gaming on PCs from the very beginning, starting as a youngster with text adventures and primitive action games on a cassette-based TRS80. From there he graduated to the glory days of Sierra Online adventures and Microprose sims, ran a local BBS, learned how to build PCs, and developed a longstanding love of RPGs, immersive sims, and shooters. He began writing videogame news in 2007 for The Escapist and somehow managed to avoid getting fired until 2014, when he joined the storied ranks of PC Gamer. He covers all aspects of the industry, from new game announcements and patch notes to legal disputes, Twitch beefs, esports, and Henry Cavill. Lots of Henry Cavill.</p></div>
</section>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HTMX is hard, so let's get it right (125 pts)]]></title>
            <link>https://github.com/BookOfCooks/blog/blob/master/htmx-is-hard-so-lets-get-it-right.md</link>
            <guid>44783266</guid>
            <pubDate>Mon, 04 Aug 2025 08:30:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/BookOfCooks/blog/blob/master/htmx-is-hard-so-lets-get-it-right.md">https://github.com/BookOfCooks/blog/blob/master/htmx-is-hard-so-lets-get-it-right.md</a>, See on <a href="https://news.ycombinator.com/item?id=44783266">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>


                <li>
      

      <div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
          <p>
            GitHub Copilot
          </p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_spark&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_spark_link_product_navbar&quot;}" href="https://github.com/features/spark">
      
      <div>
          <p>
            GitHub Spark
              <span>
                New
              </span>
          </p><p>
        Build and deploy intelligent apps
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_product_navbar&quot;}" href="https://github.com/features/models">
      
      <div>
          <p>
            GitHub Models
              <span>
                New
              </span>
          </p><p>
        Manage and compare prompts
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_product_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
          <p>
            GitHub Advanced Security
          </p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
          <p>
            Actions
          </p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    
                </ul>
              </div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
          <p>
            Codespaces
          </p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
          <p>
            Issues
          </p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
          <p>
            Code Review
          </p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
          <p>
            Discussions
          </p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
          <p>
            Code Search
          </p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
          

      </div>
</li>


                <li>
      

      
</li>


                <li>
      

      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      Events &amp; Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      

      <div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
          <p>
            GitHub Sponsors
          </p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
          <p>
            The ReadME Project
          </p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      

      <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
          <p>
            Enterprise platform
          </p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:BookOfCooks/blog" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="WPAnYliyY1FgoOA8Va2qojQcgE4Z6D0ngRZ_ocMapCiw8P8hwCSETk0Gq9pKOzyy1sg4y5mC3UTOxjHJzymdKg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="BookOfCooks/blog" data-current-org="" data-current-owner="BookOfCooks" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=BookOfCooks%2Fblog" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/BookOfCooks/blog/blob/master/htmx-is-hard-so-lets-get-it-right.md&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="a0e1293e471f60ad624a289d0e30d910c8c92e00d39582f686e5d00a39e54016" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-d67da881-3dd5-4124-9b48-94824db9f093" for="icon-button-c7ce36fd-9a1c-48af-bbba-ecfabb410fa9" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.260d30274859410b0337.module.css">
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.76259b61ecc822265749.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>

      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Job-seekers are dodging AI interviewers (455 pts)]]></title>
            <link>https://fortune.com/2025/08/03/ai-interviewers-job-seekers-unemployment-hiring-hr-teams/</link>
            <guid>44783155</guid>
            <pubDate>Mon, 04 Aug 2025 08:04:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fortune.com/2025/08/03/ai-interviewers-job-seekers-unemployment-hiring-hr-teams/">https://fortune.com/2025/08/03/ai-interviewers-job-seekers-unemployment-hiring-hr-teams/</a>, See on <a href="https://news.ycombinator.com/item?id=44783155">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>The next time you get buttoned-up and sit down for a long-awaited job interview, you might not find a human on the other end of the call. Instead, job-hunters are now joining <a href="https://fortune.com/company/zoom/" target="_blank" aria-label="Go to https://fortune.com/company/zoom/">Zoom</a> meetings only to <a href="https://www.tiktok.com/@leohumpsalot/video/7501016832850103583" target="_blank" rel="noopener" aria-label="Go to https://www.tiktok.com/@leohumpsalot/video/7501016832850103583">be greeted by</a> AI interviewers. Candidates tell <em>Fortune </em>they’re either confused, intrigued, or straight-up dejected when the robotic, faceless bots join the calls.&nbsp;</p><div>



<p>“Looking for a job right now is so demoralizing and soul-sucking, that to submit yourself to that added indignity is just a step too far,” Debra Borchardt, a seasoned writer and editor who has been on the job-hunt for three months, tells <em>Fortune. </em>“Within minutes, I was like, ‘I don’t like this. This is awful.’ It started out normal…Then it went into the actual process of the interview, and that’s when it got a little weird.”



</p><p>AI interviewers are only the newest <a href="https://fortune.com/2024/09/05/ai-changing-job-hunting-recruiting/" target="_self" aria-label="Go to https://fortune.com/2024/09/05/ai-changing-job-hunting-recruiting/">change to the</a> hiring process that has <a href="https://fortune.com/2025/06/11/ai-hiring-process-employee-skills-candidate/" target="_self" aria-label="Go to https://fortune.com/2025/06/11/ai-hiring-process-employee-skills-candidate/">been upended</a> by the advanced technology. With HR teams dwindling and hiring managers tasked to review <a href="https://fortune.com/2025/05/25/insurance-giant-progressive-is-hiring-12000-workers-this-year-and-its-using-ai-to-parse-through-hundreds-of-thousands-of-applications/" target="_self" aria-label="Go to https://fortune.com/2025/05/25/insurance-giant-progressive-is-hiring-12000-workers-this-year-and-its-using-ai-to-parse-through-hundreds-of-thousands-of-applications/">thousands of</a> applicants for a single role, they’re <a href="https://fortune.com/2024/09/24/two-page-resume-new-normal-ai-job-seekers/" target="_self" aria-label="Go to https://fortune.com/2024/09/24/two-page-resume-new-normal-ai-job-seekers/">optimizing their</a> jobs <a href="https://fortune.com/2024/09/04/how-ai-changing-future-recruiting-job-search/" target="_self" aria-label="Go to https://fortune.com/2024/09/04/how-ai-changing-future-recruiting-job-search/">by using AI to</a> filter top applicants, schedule candidate interviews, and automate correspondence about next steps in the process. AI interviewers may be a god-send for middle-managers, but job-seekers see them as only another hurdle in the <a href="https://fortune.com/2025/07/14/gen-z-job-hunting-harder-millions-unemployed-millennial-gen-x-careers-ai-entry-level-work/" target="_self" aria-label="Go to https://fortune.com/2025/07/14/gen-z-job-hunting-harder-millions-unemployed-millennial-gen-x-careers-ai-entry-level-work/">intense hunt for</a> work.&nbsp;



</p><p>The experience for some job-hunters has been so poor that they’re swearing off interviews conducted by AI altogether. Candidates tell <em>Fortune </em>that AI interviewers make them feel unappreciated to the point where they’d rather skip out on potential job opportunities, reasoning the company’s culture can’t be great if human bosses won’t make the time to interview them. But HR experts argue the opposite; since AI interviewers can help hiring managers save time in first-round calls, the humans have more time to have more meaningful conversations with applicants down the line.&nbsp;



</p><p>Job-seekers and HR are starkly divided on how they feel about the tech, but one thing is fact—AI interviewers aren’t going anywhere.&nbsp;</p><p>“The truth is, if you want a job, you’re gonna go through this thing,” Adam Jackson, CEO and founder of Braintrust, a company that distributes AI interviewers, tells <em>Fortune. </em>“If there were a large portion of the job-seeking community that were wholesale rejecting this, our clients wouldn’t find the tool useful… This thing would be chronically underperforming for our clients. And we’re just not seeing that—we’re seeing the opposite.”



</p><h2>Job-seekers are dodging AI interviewers&nbsp;</h2>



<p>Social media has been <a href="https://www.tiktok.com/@petobsessed777/video/7499996920622992682?q=AI%20interviewer&amp;t=1753974873967" target="_blank" rel="noopener" aria-label="Go to https://www.tiktok.com/@petobsessed777/video/7499996920622992682?q=AI%20interviewer&amp;t=1753974873967">exploding with</a> job-seekers detailing their AI interviewer <a href="https://www.tiktok.com/@meghantheeyogi/video/7532986697219362079?q=AI%20interviewer&amp;t=1753974873967" target="_blank" rel="noopener" aria-label="Go to https://www.tiktok.com/@meghantheeyogi/video/7532986697219362079?q=AI%20interviewer&amp;t=1753974873967">experiences</a>: describing bots hallucinating and <a href="https://www.tiktok.com/@loeybugxo/video/7500355242530245930?q=AI%20interviewer&amp;t=1753974873967" target="_blank" rel="noopener" aria-label="Go to https://www.tiktok.com/@loeybugxo/video/7500355242530245930?q=AI%20interviewer&amp;t=1753974873967">repeating questions</a> on end, calling the robotic <a href="https://www.tiktok.com/@sebwhatseb/video/7501713243182746902?q=AI%20interviewer&amp;t=1753974873967" target="_blank" rel="noopener" aria-label="Go to https://www.tiktok.com/@sebwhatseb/video/7501713243182746902?q=AI%20interviewer&amp;t=1753974873967">conversations awkward</a>, or saying it’s less nerve-wracking than talking to a human. Despite how much hiring managers love AI interviewers, job-seekers aren’t sold on the idea just yet.&nbsp;



</p><p>Allen Rausch, a 56-year-old technical writer who has worked at <a href="https://fortune.com/company/amazon-com/" target="_self" aria-label="Go to https://fortune.com/company/amazon-com/">Amazon</a> and <a href="https://fortune.com/company/electronic-arts/" target="_self" aria-label="Go to https://fortune.com/company/electronic-arts/">Electronic Arts</a>, has been on the job hunt for two months since getting laid off from his previous role at InvestCloud. In looking for new opportunities, he was “startled” to run into AI interviewers for the first time—let alone on three occasions for separate jobs. All of the meetings would last up to 25 minutes, and featured woman-like cartoons with female voices. It asked basic career questions, running through his resume and details about the job opening, but couldn’t answer any of his questions on the company or culture. 



</p><p>Rausch says he’s only open to doing more AI interviews if they don’t test his writing skills, and if human connection is guaranteed at some point later in the process.



</p><p>“Given the percentage of responses that I’m getting to just basic applications, I think a lot of AI interviews are wasting my time,” he tells <em>Fortune.</em> “I would probably want some sort of a guarantee that, ‘Hey, we’re doing this just to gather initial information, and we are going to interview you with a human being [later].’”</p><p>While Rausch withstood multiple AI interviews, Borchardt couldn’t even sit through a single one. The 64-year-old editorial professional says things went downhill when the robotic interviewer simply ran through her resume, asking her to repeat all of her work experiences at each company listed. The call was impersonal, irritating, and to Borchardt, quite lazy. She ended the interview in less than 10 minutes.&nbsp;



</p><p>“After about the third question, I was like, ‘I’m done.’ I just clicked exit,” she says. “I’m not going to sit here for 30 minutes and talk to a machine… I don’t want to work for a company if the HR person can’t even spend the time to talk to me.”



</p><p>Alex Cobb, a professional now working at U.K. energy company Murphy Group, also encountered an AI interviewer several months ago searching for a new role. While he’s sympathetic towards how many applications HR has to sift through, he finds AI interviewers to be “weird” and ultimately ineffective in fully assessing human applicants. The experience put a bad taste in his mouth, to the point where Cobb won’t pursue any AI-proctored interviews in the foreseeable future.&nbsp;



</p><p>“If I know from looking at company reviews or the hiring process that I will be using AI interviewing, I will just not waste my time, because I feel like it’s a cost-saving exercise more than anything,” Cobb tells <em>Fortune.</em> “It makes me feel like they don’t value my learning and development. It makes me question the culture of the company—are they going to cut jobs in the future because they’ve learned robots can already recruit people? What else will they outsource that to do?”



</p><h2>AI interviewers are a god-send for squeezed hiring managers&nbsp;</h2>



<p>While many job-seekers are backing away from taking AI interviews, hiring managers are accepting the technology with open arms. A large part of it comes from necessity.&nbsp;</p><p>“They’re becoming more common in early-stage screening because they can streamline high-volume hiring,” Priya Rathod, workplace trends editor at Indeed, tells <em>Fortune.</em> “You’re seeing them all over. But for high-volume hiring like customer service or retail or entry-level tech roles, we’re just seeing this more and more… It’s doing that first-stage work that a lot of employers need in order to be more efficient and save time.”



</p><p>It should be noted that not all AI interviewers are created equal—there’s a wide range of AI interviewers entering the market. Job-seekers who spoke with <em>Fortune </em>described monotonous, robotic-voiced bots with pictures of strange feminized avatars. But some AI interviewers, like the one created by Braintrust, distribute a faceless bot with a more natural sounding voice. Its CEO says applicants using the tech are overall happy with their experience—and its hiring manager clientele are enthusiastic, too.&nbsp;



</p><p>However, Jackson admits AI interviewers still have their limitations, despite how revolutionary they are for HR teams.



</p><p>“It does 100 interviews, and it’s going to hand back the best 10 to the hiring manager, and then the human takes over,” he says. “AI is good at objective skill assessment—I would say even better than humans. But [when it comes to] cultural fit, I wouldn’t even try to have AI do that.”
</p></div><p><strong>Introducing the 2025 Fortune 500</strong>, the definitive ranking of the biggest companies in America.&nbsp;<a href="https://fortune.com/ranking/fortune500/?&amp;itm_source=fortune&amp;itm_medium=article_tout&amp;itm_campaign=plea_text" target="_self" aria-label="Go to https://fortune.com/ranking/fortune500/?&amp;itm_source=fortune&amp;itm_medium=article_tout&amp;itm_campaign=plea_text">Explore this year's list.</a></p></div>]]></description>
        </item>
    </channel>
</rss>