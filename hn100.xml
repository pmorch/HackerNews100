<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 11 Jul 2023 13:00:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Privatisation has been a costly failure in Britain (313 pts)]]></title>
            <link>https://www.economist.com/by-invitation/2023/07/10/mathew-lawrence-on-why-privatisation-has-been-a-costly-failure-in-britain</link>
            <guid>36678375</guid>
            <pubDate>Tue, 11 Jul 2023 09:20:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/by-invitation/2023/07/10/mathew-lawrence-on-why-privatisation-has-been-a-costly-failure-in-britain">https://www.economist.com/by-invitation/2023/07/10/mathew-lawrence-on-why-privatisation-has-been-a-costly-failure-in-britain</a>, See on <a href="https://news.ycombinator.com/item?id=36678375">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><h2>The energy transition further strengthens the argument for state ownership, says the think-tank head</h2></section><div data-body-id="cp1"><p data-caps="initial"><span data-caps="initial">T</span><small>HE BRITISH</small> economy has been subject to a giant experiment: privatisation on a scale more extensive than in almost any other <small>OECD</small> country. Perhaps most strikingly, following the lead of Augusto Pinochet’s Chile, in 1989 the Conservative government privatised the water industry in England and Wales. This outlier status remains to this day: the majority of water infrastructure in other countries is held and managed by the public. To see the disastrous effects of this experiment, one need only look at England’s crisis-ridden water companies—or brave a swim in an English river flooded with sewage.</p><p>Emblematic of these failures is Thames Water, England’s largest water company. Having accumulated debts of £14bn ($18bn), in part the legacy of a leveraged buy-out by Macquarie, an Australian investment group, it is now precariously exposed to higher interest rates. But the problem extends beyond one company. </p><p>At the time of privatisation, the water industry as a whole carried no debt, partly because £15bn had been paid off by the government beforehand. Since then the sector has accumulated £53bn in debt while distributing £72bn to shareholders, the majority of whom are international investors. English water-bill payers, who have no choice but to use their local monopoly provider, are essentially taxed without representation for the benefit of using this essential service. </p><p>The practice of loading a company with debt to distribute cash to shareholders and managers is especially pronounced among private-equity owners in the water industry. However, the tension between generating strong returns for investors in uncompetitive monopoly conditions and providing high-quality, affordable infrastructure to the public is as real for publicly listed companies as it is for those owned by private equity. </p><p>The effects of this tension are clear. For example, South East Water—thousands of whose customers were left without running water this summer—spent more on dividends and servicing its debt than on infrastructure in the two years to March 2022. Water bills for Britain as a whole have increased by around 360%, more than double the rate of inflation, since privatisation. Over that time, annual capital investment by the ten largest water and sewage companies has fallen by some 15%, according to research by the <i>Financial Times </i>(<i>FT</i>). Bills are lower in Scotland, whose water company is government-owned, even though it has invested 35% more per household per year than English firms.</p><p>The case for privatisation rested on two claims, reiterated by Michael Howard, <a href="https://www.economist.com/by-invitation/2023/07/06/thames-water-may-be-troubled-but-privatisation-has-served-britain-well-argues-michael-howard">writing recently</a> for <i>The Economist</i>. First, the bracing force of competition would improve utility management and hence overall outcomes. Second, it would unlock access to private capital to fund much-needed investment that would otherwise be held back by short-termism and scarcity—supposedly endemic to public ownership. </p><p>Neither claim stands up to scrutiny. Private management and weak regulation have delivered a crisis-ridden sector and chronic underinvestment. Water companies have made clear that investment will be funded mainly by increased bills, undermining the idea that private capital is essential for investment.</p><p>Dogmatic adherence to privatisation in the face of its sustained failure suggests ideology, not pragmatism, was the motivation. Whether key infrastructure is publicly or privately owned, investment in its construction and maintenance will always ultimately be borne by taxpayers, bondholders and water customers. The question is whether we want to add to those costs the weight of shareholder payouts and the extra interest that private owners have to pay over sovereign borrowers. If your answer is, like mine, no, then the solution is obvious: the public ownership that prevails in most other water systems around the world. </p><p>Rather than defending a failing status quo, a better approach would be to use existing law to hold responsible those who have wrung the industry dry, and then to change the law’s purpose and water companies’ governance and ownership to put the public interest first—treating water as a human right and organising water networks as a critical public service. </p><p>The case for public ownership should rest on a sober assessment of how best to deliver fundamental public goods and services. Public ownership—which can operate at multiple levels, from the national to the municipal—is clearly not appropriate for every sector. But it allows the pursuit of a broad range of objectives beyond profit-maximisation. </p><p>If the argument for public ownership is strong in water, it is even more powerful when it comes to the biggest socio-economic challenge confronting humanity: the energy transition. In the span of a decade we must undertake an unprecedented investment and divestment sprint to deliver a clean electricity system, which private ownership and market co-ordination are ill-equipped to deliver. As Derek Brower put it in a recent column in the <i>FT</i>: “The sheer scale of the physical infrastructure that must be revamped, demolished or replaced is almost beyond comprehension. Governments, not BlackRock, will have to lead this new Marshall Plan.” </p><p>From bringing new renewable-energy generation online, to dramatically expanding a backlogged grid and storage, to phasing out fossil-fuel assets, the discrete elements of grid decarbonisation aren’t discrete at all: they must be synchronised to limit physical bottlenecks and inflationary pressures. Private investment, hindered by the profit imperative and, in many quarters, a preference for liquid assets over longer-term capital investments, cannot deliver a smooth wholesale transformation of the electricity system. Public ownership offers greater affordability, more effective co-ordination and greater democratic oversight. The argument for it is built on necessity, not nostalgia. </p><p>In his recent book, “The Death of Consensus”, Phil Tinline traced how the power of political nightmares has driven the transformation of Britain’s economy. The case for private ownership drew strength from a spectre that, advocates argued, haunted British life: sclerotic service under nationalised utilities that stood as both cause and symptom of wider national decline. Yet the age of privatisation has now produced its own nightmare: of raw sewage gushing into rivers as international shareholders profit, utility bills rising relentlessly as infrastructure creaks and, now, the potential collapse of leading water and energy companies under the weight of their own fragile, extractive business models. It is time we woke up and exorcised the ghost.<span data-ornament="ufinish">■</span></p><p><i>Mathew Lawrence is the founder and director of Common Wealth, a think-tank, and the co-author of “Owning the Future” (2022). </i></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SUSE is forking RHEL (299 pts)]]></title>
            <link>https://www.suse.com/news/SUSE-Preserves-Choice-in-Enterprise-Linux/</link>
            <guid>36678079</guid>
            <pubDate>Tue, 11 Jul 2023 08:41:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.suse.com/news/SUSE-Preserves-Choice-in-Enterprise-Linux/">https://www.suse.com/news/SUSE-Preserves-Choice-in-Enterprise-Linux/</a>, See on <a href="https://news.ycombinator.com/item?id=36678079">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div>
<h4>LUXEMBOURG</h4>

<p>Today <a href="http://www.suse.com/">SUSE</a>, the company behind Rancher, NeuVector, and SUSE Linux Enterprise (SLE) and a global leader in enterprise open source solutions, announced it is forking publicly available Red Hat Enterprise Linux (RHEL) and will develop and maintain a RHEL-compatible distribution available to all without restrictions. Over the next few years, SUSE plans to invest more than $10 million into this project.&nbsp;&nbsp;</p>
</div>

<p>Dirk-Peter van Leeuwen, CEO of SUSE, said, “For decades, collaboration and shared success have been the building blocks of our open source community. We have a responsibility to defend these values. This investment will preserve the flow of innovation for years to come and ensures that customers and community alike are not subjected to vendor lock-in and have genuine choice tomorrow as well as today.”&nbsp;<span>&nbsp;</span></p>

<p>SUSE remains fully committed to investing in its highly regarded Linux solutions such as SLE and openSUSE that countless satisfied enterprise customers and the community rely on. At the same time, it acknowledges that enterprises and the open source community deserve choice and freedom from vendor lock-in. SUSE has a long history in empowering and supporting users with mixed Linux environments.&nbsp;&nbsp;</p>

<p>SUSE is committed to working with the open source community to develop a long-term, enduring compatible alternative for RHEL and CentOS users. SUSE plans to contribute this project to an open source foundation, which will provide ongoing free access to alternative source code.&nbsp;&nbsp;</p>

<p>“This collaborative effort demonstrates SUSE’s deep-rooted commitment to fostering innovation and nurturing community-driven development, and it reinforces the fundamental values of open source software. We invite the community to actively engage and collaborate in shaping the future of this essential software,” said Dr. Thomas Di Giacomo, Chief Technology and Product Officer, SUSE. “We firmly believe this new RHEL-compatible Linux distribution, together with SUSE’s portfolio, will help the community and customers navigate unprecedented advancements in enterprise Linux, cloud computing, containerization, edge, AI/ML and other emerging technologies.”&nbsp;&nbsp;</p>

<p>“The enterprise Linux community requires standardization, stability, and consistency,” said Gregory Kurtzer, CEO of <a href="https://www.ciq.com/?utm_source=web&amp;utm_medium=press-release&amp;utm_campaign=suse&amp;utm_content=rhel-fork">CIQ</a> and Founder of <a href="https://www.rockylinux.org/">Rocky Linux</a>. “CIQ is bringing stability to our partners, customers, and community, by building a broad coalition of like-minded companies, organizations, and individuals. SUSE has embodied the core principles and spirit of open source; CIQ is thrilled to collaborate with SUSE on advancing an open enterprise Linux standard.”&nbsp;&nbsp;</p>

<div>
<p>To read more about how this strengthens Liberty Linux and how to get involved, go <a href="https://www.suse.com/c/at-suse-we-make-choice-happen/">here</a>.&nbsp;&nbsp;&nbsp;</p>



<p><strong>About SUSE&nbsp;</strong></p>
</div>

<p>SUSE is a global leader in innovative, reliable and secure enterprise-grade open source solutions, relied upon by more than 60% of the Fortune 500 to power their mission-critical workloads. We specialize in Business-critical Linux, Enterprise Container Management and Edge solutions, and collaborate with partners and communities to empower our customers to innovate everywhere – from the data center to the cloud, to the edge and beyond.&nbsp;</p>

<p>SUSE puts the “open” back in open source, giving customers the ability to tackle innovation challenges today and the freedom to evolve their strategy and solutions tomorrow. The company employs more than 2,000 people globally. SUSE is listed on the Frankfurt Stock Exchange.&nbsp;</p>

<div>


<p><strong>Forward-Looking Statements&nbsp;</strong></p>
</div>

<p>Any statements in this press release about future expectations, plans and prospects for the company, including statements containing the words “aims,” “targets,” “will,” “believes,” “anticipates,” “plans,” “expects,” and similar expressions, may constitute forward-looking statements and should be read with caution. Actual results may differ materially from those indicated by such forward-looking statements as a result of various important factors, including competitive landscape, development of customer deals, reliance upon customer relationships, management of growth and acquisitions, the possibility of undetected software issues, the risks of impacts of the Covid-19 pandemic and economic downturns, pricing pressures and the viability of the Internet. In addition, any forward-looking statements included herein represent views as of the date of this press release and these views could change. The Company does not have any obligation to update its forward-looking statements. These forward-looking statements are subject to change and should not be relied upon as representing the Company’s views as of any date other than the date of this press release.&nbsp;</p>

<div>
<p><img alt="RHEL Response " data-align="center" data-entity-type="" data-entity-uuid="" height="306" src="https://links.imagerelay.com/cdn/3404/ql/0211f0a7329140f5a410e96767dec868/AdobeStock_156220876.jpeg"></p>

<p>Copyright 2023 SUSE LLC. All rights reserved. SUSE and the SUSE logo are registered trademarks of SUSE LLC in the United States and other countries. All third-party trademarks are the property of their respective owners.&nbsp;</p>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-Prompt-Engineer (187 pts)]]></title>
            <link>https://github.com/mshumer/gpt-prompt-engineer</link>
            <guid>36677034</guid>
            <pubDate>Tue, 11 Jul 2023 06:16:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mshumer/gpt-prompt-engineer">https://github.com/mshumer/gpt-prompt-engineer</a>, See on <a href="https://news.ycombinator.com/item?id=36677034">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">gpt-prompt-engineer</h2>
<p dir="auto"><a href="https://twitter.com/mattshumer_" rel="nofollow"><img src="https://camo.githubusercontent.com/3f2ef3da38c79ef4c01ff068c1062e0c06200161a767b4015ce0372de4180d96/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f6d6174747368756d65725f3f7374796c653d736f6369616c" alt="Twitter Follow" data-canonical-src="https://img.shields.io/twitter/follow/mattshumer_?style=social"></a> <a href="https://colab.research.google.com/github/mshumer/gpt-prompt-engineer/blob/main/gpt_prompt_engineer.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open Main Version In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a> <a href="https://colab.research.google.com/drive/16NLMjqyuUWxcokE_NF6RwHD8grwEeoaJ?usp=sharing" rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open Classification Version In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></p>
<h2 tabindex="-1" dir="auto">Overview</h2>
<p dir="auto">Prompt engineering is kind of like alchemy. There's no clear way to predict what will work best. It's all about experimenting until you find the right prompt. <code>gpt-prompt-engineer</code> is a tool that takes this experimentation to a whole new level.</p>
<p dir="auto"><strong>Simply input a description of your task and some test cases, and the system will generate, test, and rank a multitude of prompts to find the ones that perform the best.</strong></p>
<h2 tabindex="-1" dir="auto">Features</h2>
<ul dir="auto">
<li>
<p dir="auto"><strong>Prompt Generation</strong>: Using GPT-4 and GPT-3.5-Turbo, <code>gpt-prompt-engineer</code> can generate a variety of possible prompts based on a provided use-case and test cases.</p>
</li>
<li>
<p dir="auto"><strong>Prompt Testing</strong>: The real magic happens after the generation. The system tests each prompt against all the test cases, comparing their performance and ranking them using an ELO rating system.</p>
</li>
</ul>
<a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/41550495/250947004-f8171cff-1703-40ca-b9fd-f0aa24d07110.png"><img width="1563" alt="Screen Shot 2023-07-04 at 11 41 54 AM" src="https://user-images.githubusercontent.com/41550495/250947004-f8171cff-1703-40ca-b9fd-f0aa24d07110.png"></a>
<ul dir="auto">
<li>
<p dir="auto"><strong>ELO Rating System</strong>: Each prompt starts with an ELO rating of 1200. As they compete against each other in generating responses to the test cases, their ELO ratings change based on their performance. This way, you can easily see which prompts are the most effective.</p>
</li>
<li>
<p dir="auto"><strong>Classification Version</strong>: The <code>gpt-prompt-engineer -- Classification Version</code> notebook is designed to handle classification tasks. It evaluates the correctness of a test case by matching it to the expected output ('true' or 'false') and provides a table with scores for each prompt.</p>
</li>
</ul>
<a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/41550495/252494291-d5c9f2a8-97fa-445d-9c38-dec744f77854.png"><img width="1607" alt="Screen Shot 2023-07-10 at 5 22 24 PM" src="https://user-images.githubusercontent.com/41550495/252494291-d5c9f2a8-97fa-445d-9c38-dec744f77854.png"></a>
<ul dir="auto">
<li><strong>Weights &amp; Biases Logging</strong>: Optional logging to <a href="https://wandb.ai/site" rel="nofollow">Weights &amp; Biases</a> of your configs such as temperature and max tokens, the system and user prompts for each part, the test cases used and the final ranked ELO rating for each candidate prompt. Set <code>use_wandb</code> to <code>True</code> to use. Only available in the main <code>gpt-prompt-engineer</code> notebook for now.</li>
</ul>
<h2 tabindex="-1" dir="auto">Setup</h2>
<ol dir="auto">
<li>
<p dir="auto"><a href="https://colab.research.google.com/github/mshumer/gpt-prompt-engineer/blob/main/gpt_prompt_engineer.ipynb" rel="nofollow">Open the notebook in Google Colab</a> or in a local Jupyter notebook. For classification, use <a href="https://colab.research.google.com/drive/16NLMjqyuUWxcokE_NF6RwHD8grwEeoaJ?usp=sharing" rel="nofollow">this one.</a></p>
</li>
<li>
<p dir="auto">Add your OpenAI API key to the line <code>openai.api_key = "ADD YOUR KEY HERE"</code>.</p>
</li>
<li>
<p dir="auto">If you have GPT-4 access, you're ready to move on. If not, change <code>CANDIDATE_MODEL='gpt-4'</code> to <code>CANDIDATE_MODEL='gpt-3.5-turbo'</code>. If you're using the classification version, and don't have GPT-4 access, change <code>model='gpt-4'</code> in the second cell to `model='gpt-3.5-turbo'.</p>
</li>
</ol>
<h2 tabindex="-1" dir="auto">How to Use</h2>
<ol dir="auto">
<li>Define your use-case and test cases. The use-case is a description of what you want the AI to do. Test cases are specific prompts that you would like the AI to respond to. For example:</li>
</ol>
<div data-snippet-clipboard-copy-content="description = &quot;Given a prompt, generate a landing page headline.&quot; # this style of description tends to work well

test_cases = [
    {
        'prompt': 'Promoting an innovative new fitness app, Smartly',
    },
    {
        'prompt': 'Why a vegan diet is beneficial for your health',
    },
    {
        'prompt': 'Introducing a new online course on digital marketing',
    },
    {
        'prompt': 'Launching a new line of eco-friendly clothing',
    },
    {
        'prompt': 'Promoting a new travel blog focusing on budget travel',
    },
    {
        'prompt': 'Advertising a new software for efficient project management',
    },
    {
        'prompt': 'Introducing a new book on mastering Python programming',
    },
    {
        'prompt': 'Promoting a new online platform for learning languages',
    },
    {
        'prompt': 'Advertising a new service for personalized meal plans',
    },
    {
        'prompt': 'Launching a new app for mental health and mindfulness',
    }
]"><pre><code>description = "Given a prompt, generate a landing page headline." # this style of description tends to work well

test_cases = [
    {
        'prompt': 'Promoting an innovative new fitness app, Smartly',
    },
    {
        'prompt': 'Why a vegan diet is beneficial for your health',
    },
    {
        'prompt': 'Introducing a new online course on digital marketing',
    },
    {
        'prompt': 'Launching a new line of eco-friendly clothing',
    },
    {
        'prompt': 'Promoting a new travel blog focusing on budget travel',
    },
    {
        'prompt': 'Advertising a new software for efficient project management',
    },
    {
        'prompt': 'Introducing a new book on mastering Python programming',
    },
    {
        'prompt': 'Promoting a new online platform for learning languages',
    },
    {
        'prompt': 'Advertising a new service for personalized meal plans',
    },
    {
        'prompt': 'Launching a new app for mental health and mindfulness',
    }
]
</code></pre></div>
<p dir="auto">For the classification version, your test cases should be in the format:</p>
<div data-snippet-clipboard-copy-content="test_cases = [
    {
        'prompt': 'I had a great day!',
        'output': 'true'
    },
    {
        'prompt': 'I am feeling gloomy.',
        'output': 'false'
    },
    // add more test cases here
]"><pre><code>test_cases = [
    {
        'prompt': 'I had a great day!',
        'output': 'true'
    },
    {
        'prompt': 'I am feeling gloomy.',
        'output': 'false'
    },
    // add more test cases here
]
</code></pre></div>
<ol start="3" dir="auto">
<li>
<p dir="auto">Choose how many prompts to generate. Keep in mind, this can get expensive if you generate many prompts. 10 is a good starting point.</p>
</li>
<li>
<p dir="auto">Call <code>generate_optimal_prompt(description, test_cases, number_of_prompts)</code> to generate a list of potential prompts, and test and rate their performance. For the classification version, just run the last cell.</p>
</li>
<li>
<p dir="auto">The final ELO ratings will be printed in a table, sorted in descending order. The higher the rating, the better the prompt.</p>
</li>
</ol>
<a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/41550495/250948262-324f90b8-c0ee-45fd-b219-6c44d9aa281b.png"><img width="1074" alt="Screen Shot 2023-07-04 at 11 48 45 AM" src="https://user-images.githubusercontent.com/41550495/250948262-324f90b8-c0ee-45fd-b219-6c44d9aa281b.png"></a>
<p dir="auto">For the classification version, the scores for each prompt will be printed in a table (see the image above).</p>
<h2 tabindex="-1" dir="auto">Contributions are welcome! Some ideas:</h2>
<ul dir="auto">
<li>have a number of different system prompt generators that create different styles of prompts, to cover more ground (ex. examples, verbose, short, markdown, etc.)</li>
<li>automatically generate the test cases</li>
<li>expand the classification version to support more than two classes using tiktoken</li>
</ul>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">This project is <a href="https://github.com/your_username/your_repository/blob/master/LICENSE">MIT</a> licensed.</p>
<h2 tabindex="-1" dir="auto">Contact</h2>
<p dir="auto">Matt Shumer - <a href="https://twitter.com/mattshumer_" rel="nofollow">@mattshumer_</a></p>
<p dir="auto">Project Link: <a href="https://github.com/mshumer/gpt-prompt-engineer/blob/main/url">https://github.com/mshumer/gpt-prompt-engineer</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Laser, a new game played on a chess board (134 pts)]]></title>
            <link>https://playlaser.xyz</link>
            <guid>36676415</guid>
            <pubDate>Tue, 11 Jul 2023 04:27:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://playlaser.xyz">https://playlaser.xyz</a>, See on <a href="https://news.ycombinator.com/item?id=36676415">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header><span data-svelte-h="svelte-ee3vl7">LASER</span> </header> <div><p><span data-svelte-h="svelte-pxsjkp">RULES</span></p> <p><span>Movement</span></p><div data-svelte-h="svelte-h8xg07"><p><span>Laser&nbsp;&nbsp;</span><span>Moves horizontal and vertical, shoots laser diagonally until the edge of the board or a Wall, destroying all pieces in its path. Cannot capture pieces.</span></p></div> <div data-svelte-h="svelte-1raf1hq"><p><span>Wall&nbsp;&nbsp;</span><span>Same movement as a rook in chess, stops the Laser and cannot be destroyed by it</span></p></div> <div data-svelte-h="svelte-t1meyh"><p><span>King&nbsp;&nbsp;</span><span>Same movement as chess</span></p></div> <div data-svelte-h="svelte-23b521"><p><span>Knight&nbsp;&nbsp;</span><span>Same movement as chess</span></p></div> <div data-svelte-h="svelte-ca1shm"><p><span>Pawn&nbsp;&nbsp;</span><span>Moves diagonally, takes horizontally and vertically</span></p></div> <p><span>Winning</span></p><p><span>Take your opponent's King</span> or <span>move one of your pawns to the seven squares in the opposite corner</span>, for black that's a1, a2, a3, a4, b1, c1, d1 and for white that's h8, h7, h6, h5, g8, f8, e8.</p> <p><span>Gameplay</span></p><p>No checks or checkmates, the game continues until one of the win conditions. The game is a draw if both kings are lasered at the same time. Black moves first.
</p></div>   </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[After 30 Years, Linux Finally Hits 3% Market Share (472 pts)]]></title>
            <link>https://linuxiac.com/linux-hits-3-percent-market-share/</link>
            <guid>36676103</guid>
            <pubDate>Tue, 11 Jul 2023 03:29:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://linuxiac.com/linux-hits-3-percent-market-share/">https://linuxiac.com/linux-hits-3-percent-market-share/</a>, See on <a href="https://news.ycombinator.com/item?id=36676103">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p><strong><strong>Linux enthusiasts rejoice! After a long journey, according to StatCounter’s data, by June 2023, Linux has achieved a 3% desktop market share.</strong></strong></p>



<p>Linux has a long history that dates back <a href="https://linuxiac.com/linux-birthday/">more than 30 years</a>. However, it has never been as popular among regular computer users as other operating systems such as Microsft’s Windows or Apple’s macOS.</p>



<p>Of course, for many years, Linux has emerged as a dominant force in the realm of server operating systems. Due to its exceptional performance, stability, reliability, and security, it has been widely adopted in server/cloud/IoT environments.</p>



<p>However, these days, Linux is no longer limited to these environments alone; it is rapidly gaining momentum as an operating system of choice for many desktop users, especially developers.</p>



<p>And the most recent figures confirm this, giving all advocates of Linux and open source in general reason to rejoice.</p>





<p><a href="https://gs.statcounter.com/os-market-share/desktop/worldwide" target="_blank" rel="noreferrer noopener">According to StatCounter</a>, a web analytics company, by June 2023, Linux has reached a 3% market share in the desktop segment. This is a remarkable achievement considering its fierce competition from other operating systems.</p>



<figure><a href="https://linuxiac.b-cdn.net/wp-content/uploads/2023/07/lunux-marketshare-statcounter.jpg"><img decoding="async" width="1024" height="838" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMTIwIDkxNyIgd2lkdGg9IjExMjAiIGhlaWdodD0iOTE3IiBkYXRhLXU9Imh0dHBzJTNBJTJGJTJGbGludXhpYWMuY29tJTJGd3AtY29udGVudCUyRnVwbG9hZHMlMkYyMDIzJTJGMDclMkZsdW51eC1tYXJrZXRzaGFyZS1zdGF0Y291bnRlci5qcGciIGRhdGEtdz0iMTEyMCIgZGF0YS1oPSI5MTciIGRhdGEtYmlwPSIiPjwvc3ZnPg==" data-spai="1" alt="Desktop Operating System Market Share Worldwide" srcset=" " sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Desktop Operating System Market Share Worldwide</figcaption></figure>



<p>While someone may seem the figure modest, it signifies a growing acceptance and recognition of the power and versatility of Linux.</p>



<p>In any case, the achievement of a 3% market share by Linux is undoubtedly a cause for celebration among its dedicated community. It reflects the growing recognition of Linux’s strengths and the efforts to overcome its historical barriers.</p>



<p>Moreover, with the continued development and innovation within the Linux ecosystem, its market share will continue growing in the coming years.</p>



<p>The growing importance of cloud computing and the rise of server infrastructure have also contributed to Linux’s success. Still, the main reason for reaching this figure is the operating system’s growing popularity among desktop users.</p>



<p>With exceptionally easy-to-use and entirely user-centric <a href="https://linuxiac.com/best-7-linux-distro-releases-for-desktop-in-2022/">Linux desktop distributions</a>, the operating system is no longer what it was 20 years ago – a complex equation available only to highly technically enlightened hackers.</p>


<h2 id="h-linux-growing-popularity-among-desktop-users">Linux Growing Popularity among Desktop Users</h2>


<figure><a href="https://linuxiac.b-cdn.net/wp-content/uploads/2023/07/linux-desktop.jpg"><img decoding="async" width="1024" height="640" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxNjgwIDEwNTAiIHdpZHRoPSIxNjgwIiBoZWlnaHQ9IjEwNTAiIGRhdGEtdT0iaHR0cHMlM0ElMkYlMkZsaW51eGlhYy5jb20lMkZ3cC1jb250ZW50JTJGdXBsb2FkcyUyRjIwMjMlMkYwNyUyRmxpbnV4LWRlc2t0b3AuanBnIiBkYXRhLXc9IjE2ODAiIGRhdGEtaD0iMTA1MCIgZGF0YS1iaXA9IiI+PC9zdmc+" data-spai="1" alt="Linux with GNOME Desktop." srcset=" " sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Linux with GNOME Desktop.</figcaption></figure>



<p>Yes, I know. Over the last 10+ years, each one has often been heralded as “Linux on the Desktop,” although it turns out that’s not quite the case. But still, we’re close to that point now. And for good reasons.</p>


<h3 id="free-lightweight-amp-customizable">Free, Lightweight &amp; Customizable</h3>


<p>The main appealing aspect of Linux for desktop users is its lightweight nature, free from corporate bloatware, and especially the limitless customization options.</p>



<p>It allows users to tailor their desktop environment to suit their preferences and workflow. With a vast selection of desktop environments like GNOME, KDE, Xfce, and many others, users can choose the one that best aligns with their needs.</p>


<h3 id="valuing-user-privacy">Valuing User Privacy</h3>


<p>Another important factor driving Linux’s growing popularity among desktop users is privacy. Compared to other mainstream operating systems, Linux generally collects no user data.</p>



<p>While <a href="https://linuxiac.com/fedora-40-plans-to-use-telemetry/">some distributions may try to collect basic telemetry data</a> for improvement purposes, the level of data collection is typically minimal and can be disabled or opted out of entirely. This aspect appeals to privacy-conscious individuals who prefer more control over their personal information.</p>


<h3 id="linux-is-a-developers-dream-come-true">Linux is a Developer’s Dream Come True</h3>


<p>Linux has long been the operating system of choice for developers worldwide, and its allure continues to grow.</p>



<p>First and foremost, Linux’s open-source nature empowers developers with unparalleled freedom. They can access and modify the source code, customize their environments, and contribute to the community, fostering collaboration and innovation.</p>



<p>Furthermore, performance is also a crucial factor. Linux’s efficiency, scalability, and ability to run on diverse hardware architectures make it ideal for resource-intensive tasks.</p>



<p>Lastly, its command-line interface and powerful scripting capabilities offer flexibility and automation, streamlining development workflows.</p>


<h2 id="bottom-line">Bottom Line</h2>


<p>So, as Linux enthusiasts rejoice, it is essential to remember that the journey does not end here. Linux has proven its worth, and its rise to a 3% desktop market share is a testament to its resilience and adaptability in the desktop field.</p>



<p>With ongoing advancements and increased support from the Open Source community and businesses, Linux is poised to become an even more formidable player in the world of operating systems.</p>




		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-4 details leaked? (386 pts)]]></title>
            <link>https://threadreaderapp.com/thread/1678545170508267522.html</link>
            <guid>36675934</guid>
            <pubDate>Tue, 11 Jul 2023 03:00:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://threadreaderapp.com/thread/1678545170508267522.html">https://threadreaderapp.com/thread/1678545170508267522.html</a>, See on <a href="https://news.ycombinator.com/item?id=36675934">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-controller="mentions">

<div>
<p><a href="https://threadreaderapp.com/user/Yampeleg"><img src="https://pbs.twimg.com/profile_images/1505912788031623170/GC7LMHNp_bigger.jpg" alt="Yam Peleg Profile picture" data-controller="twitter-profile" data-twtrid="634339745" data-action="error->twitter-profile#error"></a>
</p>

</div> 
<div id="tweet_1" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545170508267522" dir="auto"><p>
GPT-4's details are leaked. </p><p>

It is over.</p><p>

Everything is here: <a data-preview="true" href="https://twitter.com/i/web/status/1678545170508267522">twitter.com/i/web/status/1…</a>
<sup><i></i></sup></p></div>
<div id="tweet_2" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545175210074112" dir="auto"><p>
Parameters count:</p><p>

GPT-4 is more than 10x the size of GPT-3. We believe it has a total of ~1.8 trillion parameters across 120 layers.
<sup><i></i></sup></p></div>
<div id="tweet_3" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545177282052098" dir="auto"><p>
Mixture Of Experts - Confirmed.</p><p>

OpenAI was able to keep costs reasonable by utilizing a mixture of experts (MoE) model.<br>
They utilizes 16 experts within their model, each is about ~111B parameters for MLP. 2 of these experts are routed to per forward pass.
<sup><i></i></sup></p></div>
<div id="tweet_4" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545179840589836" dir="auto"><p>
MoE Routing:</p><p>

While the literature talks a lot about advanced routing algorithms for choosing which experts to route each token to, OpenAI’s is allegedly quite simple, for the current GPT-4 model.</p><p>

There roughly ~55B shared parameters for attention.
<sup><i></i></sup></p></div>
<div id="tweet_5" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545182990512129" dir="auto"><p>
Inference:</p><p>

Each forward pass inference (generation of 1 token) only utilizes ~280B parameters and ~560 TFLOPs. This contrasts with the ~1.8 trillion parameters and ~3,700 TFLOP that would be required per forward pass of a purely dense model.
<sup><i></i></sup></p></div>
<div id="tweet_6" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545185108631552" dir="auto"><p>
Dataset:</p><p>

GPT-4 is trained on ~13T tokens.</p><p>

These are not unique tokens, they count the epochs as more tokens as well.</p><p>

Epoch number: 2 epochs for text-based data and 4 for code-based data.</p><p>

There is millions of rows of instruction fine-tuning data from ScaleAI &amp; internally.
<sup><i></i></sup></p></div>
<div id="tweet_7" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545188363329539" dir="auto"><p>
GPT-4 32K</p><p>

There was an 8k context length (seqlen) for the pre-training phase. The 32k seqlen version of GPT-4 is based on fine-tuning of the 8k after the pre-training.
<sup><i></i></sup></p></div>
<div id="tweet_8" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545190892470272" dir="auto"><p>
Batch Size:</p><p>

The batch size was gradually ramped up over a number of days on the cluster, but by the end, OpenAI was using a batch size of 60 million! This, of course, is “only” a batch size of 7.5 million tokens per expert due to not every expert seeing all tokens.
<sup><i></i></sup></p></div>
<p>
For the real batch size:<br>
Divide this number by the seq len to get the real batch size. just stop with this misleading numbers already.
<sup><i></i></sup>
</p>
<div id="tweet_10" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545195506233344" dir="auto"><p>
Parallelism Strategies</p><p>

To parallelize across all their A100s GPUs They utilized 8-way tensor parallelism as that is the limit for NVLink.</p><p>

Beyond that, they are using 15-way pipeline parallelism.</p><p>

(likely used ZeRo Stage 1. It is possible they used block-level FSDP)
<sup><i></i></sup></p></div>
<div id="tweet_11" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545197792206851" dir="auto"><p>
Training Cost</p><p>

OpenAI’s training FLOPS for GPT-4 is ~2.15e25, on ~25,000 A100s for 90 to 100 days at about 32% to 36% MFU.</p><p>

Part of this extremely low utilization is due to an absurd number of failures requiring checkpoints that needed to be restarted from.
<sup><i></i></sup></p></div>
<div id="tweet_12" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545200325558272" dir="auto"><p>
If their cost in the cloud was about $1 per A100 hour, the training costs for this run alone would be about $63 million.</p><p>

(Today, the pre-training could be done with ~8,192 H100 in ~55 days for $21.5 million at $2 per H100 hour.)
<sup><i></i></sup></p></div>
<div id="tweet_13" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545202477146113" dir="auto"><p>
Mixture of Expert Tradeoffs</p><p>

There are multiple MoE tradeoffs taken: For example, MoE is incredibly difficult to deal with on inference because not every part of the model is utilized on every token generation.
<sup><i></i></sup></p></div>
<div id="tweet_14" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545205442621440" dir="auto"><p>
This means parts may sit dormant when other parts are being used. When serving users, this really hurts utilization rates.</p><p>

Researchers have shown that using 64 to 128 experts achieves better loss than 16 experts, but that’s purely research.
<sup><i></i></sup></p></div>
<p>
There are multiple reasons to go with fewer experts. One reason for OpenAI choosing 16 experts is because more experts are difficult to generalize at many tasks. More experts can also be more difficult to achieve convergence with.
<sup><i></i></sup>
</p>
<p>
With such a large training run, OpenAI instead chose to be more conservative on the number of experts.
<sup><i></i></sup>
</p>
<div id="tweet_17" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545212581265409" dir="auto"><p>
GPT-4 Inference Cost</p><p>

GPT-4 costs 3x that of the 175B parameter Davinchi.<br>
This is largely due to the larger clusters required for GPT-4 and much lower utilization achieved.
<sup><i></i></sup></p></div>
<p>
AN estimate of it's costs is $0.0049 cents per 1k tokens for 128 A100s to inference GPT-4 8k seqlen and $0.0021 cents per 1k tokens for 128 H100’s to inference GPT-4 8k seqlen. It should be noted, we assume decent high utilization, and keeping batch sizes high.
<sup><i></i></sup>
</p>
<div id="tweet_19" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545217631272960" dir="auto"><p>
Multi-Query Attention</p><p>

OpenAI are using MQA just like everybody else.<br>
Because of that only 1 head is needed and memory capacity can be significantly reduced for the KV cache. Even then, the 32k seqlen GPT-4 definitely cannot run on 40GB A100s, and the 8k is capped on max bsz.
<sup><i></i></sup></p></div>
<div id="tweet_20" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545219774562304" dir="auto"><p>
Continuous batching</p><p>

OpenAI implements both variable batch sizes and continuous batching. This is so as to allow some level of maximum latency as well optimizing the inference costs.
<sup><i></i></sup></p></div>
<div id="tweet_21" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545222815424512" dir="auto"><p>
Vision Multi-Modal</p><p>

It is a separate vision encoder from the text encoder, with cross-attention. The architecture is similar to Flamingo. This adds more parameters on top of the 1.8T of GPT-4. It is fine-tuned with another ~2 trillion tokens, after the text only pre-training.
<sup><i></i></sup></p></div>
<p>
On the vision model, OpenAI wanted to train it from scratch, but it wasn’t mature enough, so they wanted to derisk it by starting with text.
<sup><i></i></sup>
</p>
<p>
One of the primary purposes of this vision capability is for autonomous agents able to read web pages and transcribe what’s in images and video.
<sup><i></i></sup>
</p>
<div id="tweet_24" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678545231300403200" dir="auto"><p>
Some of the data they train on is joint data (rendered LaTeX/text), screen shots of web page, youtube videos: sampling frames, and run Whisper around it to get transcript.</p><p>

[Dont want to say "I told you so" but..]
<sup><i></i></sup></p></div>
<div id="tweet_25" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678546324105330689" dir="auto"><p>
Speculative Decoding</p><p>

OpenAI might be using speculative decoding on GPT-4's inference. (not sure 100%)</p><p>

The idea is to use a smaller faster model to decode several tokens in advance, and then feeds them into a large oracle model as a single batch.
<sup><i></i></sup></p></div>
<p>
If the small model was right about its predictions – the larger model agrees and we can decode several tokens in a single batch.
<sup><i></i></sup>
</p>
<p>
But if the larger model rejects the tokens predicted by the draft model then the rest of the batch is discarded. And we continue with the larger model.
<sup><i></i></sup>
</p>
<p>
The conspiracy theory that the new GPT-4 quality had been deteriorated might be simply because they are letting the oracle model accept lower probability sequences from the speculative decoding model.
<sup><i></i></sup>
</p>
<div id="tweet_29" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678547812177330180" dir="auto"><p>
Inference Architecture</p><p>

The inference runs on a cluster of 128 GPUs.</p><p>

There are multiple of these clusters in multiple datacenters in different locations.</p><p>

It is done in 8-way tensor parallelism and 16-way pipeline parallelism.</p><p>

Each node of 8 GPUs has only ~130B parameters, or… <a data-preview="true" href="https://twitter.com/i/web/status/1678547812177330180">twitter.com/i/web/status/1…</a>
<sup><i></i></sup></p></div>
<p>
The model has 120, so it fits in 15 different nodes.<br>
[Possibly the there are less layers on the first node since it needs to also compute the embeddings]
<sup><i></i></sup>
</p>
<div id="tweet_31" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678549234612674560" dir="auto"><p>
According to these numbers: OpenAI should have trained on 2x the tokens if they were trying to go by chinchilla's optimal.</p><p>

[let alone surpass it like we do]</p><p>

This goes to show that they are struggling to get high quality data.
<sup><i></i></sup></p></div>
<div id="tweet_32" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678550338075336706" dir="auto"><p>
Why no FSDP?</p><p>

A possible reason for this could be that some of the hardware infra they secured is of an older generation. </p><p>

This is pretty common at local compute clusters as the organisation usually upgrade the infra in several "waves" to avoid a complete pause of operation.… <a data-preview="true" href="https://twitter.com/i/web/status/1678550338075336706">twitter.com/i/web/status/1…</a>
<sup><i></i></sup></p></div>
<div id="tweet_33" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678553044219224064" dir="auto"><p>
Dataset Mixture</p><p>

They trained on 13T tokens.<br>
CommonCrawl &amp; RefinedWeb are both 5T.</p><p>

Remove the duplication of tokens from multiple epochs and we get to a much reasonable number of "unaccounted for" tokens: The "secret" data.
<sup><i></i></sup></p></div>
<div id="tweet_34" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678553048501633024" dir="auto"><p>
Which by this point we already get rumors that parts of it came from twitter, reddit &amp; youtube.</p><p>

[Rumors that start to become lawsuits] </p><p>

Some speculations are:<br>
- LibGen (4M+ books)<br>
- Sci-Hub (80M+ papers)<br>
- All of GitHub
<sup><i></i></sup></p></div>
<div id="tweet_35" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678553050296856577" dir="auto"><p>
My own opinion:</p><p>

The missing dataset it a custom dataset of college textbooks collected by hand for as much courses as possible.</p><p>

This is very easy to convert to txt file and than with self-instruct into instruction form.
<sup><i></i></sup></p></div>
<div id="tweet_36" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678553055673872387" dir="auto"><p>
This creates the "illusion" that GPT-4 "is smart" no matter who use it.</p><p>

Computer scientist? sure! it can help you with your questions about P!=NP<br>
Philosophy major? It can totally talk to you about epistemology.</p><p>

Don't you see?<br>
It was trained on the textbooks. It is so obvious.
<sup><i></i></sup></p></div>
<div id="tweet_37" data-controller="thread" data-action="click->thread#showTweet" data-screenname="Yampeleg" data-tweet="1678553507236835328" dir="auto"><p>
There are also papers that try to extract by force memorized parts of books from GPT-4 to understand what it trained on. </p><p>

There are some books it knows so well that it had seen them for sure.</p><p>

Moreover, If i remember correctly: It even know the unique ids of project Euler exes.
<sup><i></i></sup></p></div>
<p>• • •</p>
<p><span>
Missing some Tweet in this thread? You can try to
<a id="force-click" href="#" data-category="refresh" data-action="1678545170508267522">force a refresh</a>
</span>
</p>
　
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Easy HTTPS for your private networks (207 pts)]]></title>
            <link>https://www.getlocalcert.net/</link>
            <guid>36674224</guid>
            <pubDate>Mon, 10 Jul 2023 23:05:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.getlocalcert.net/">https://www.getlocalcert.net/</a>, See on <a href="https://news.ycombinator.com/item?id=36674224">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <h2>ACME Client Configuration</h2>

<div>
  <div>
    <h3>Using Caddy</h3>

    <p>
    Modern, all-in-one web server
    </p>

    <div><p>
    Edit Caddyfile:
    </p><!-- https://github.com/caddy-dns/acmedns -->
    <pre><code>
<yoursubdomain>.localhostcert.net {
  tls {
    dns acmedns credentials.json
  }
  respond "Hello"
}
</yoursubdomain></code></pre>

    <p><a href="https://docs.getlocalcert.net/acme-clients/caddy/">See full example</a>

    </p></div>
  </div>

  <div>
    <h3>Using traefik</h3>
    <p>
    Cloud-native application proxy
    </p>
    <div><p>
    Edit configuration:
    </p><pre><code>
certificatesResolvers:
  myresolver:
    acme:
      dnsChallenge:
        provider: acme-dns</code></pre>

    <p><a href="https://docs.getlocalcert.net/acme-clients/traefik/">See full example</a>
    </p></div>
  </div>

</div>
<div>

  <div>
    <h3>Using cert-manager</h3>

    <p>
    Certificate automation for Kubernetes
    </p>

    <div><p>
    Edit configuration:
    </p><pre><code>
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: example-issuer
spec:
  acme:
    solvers:
    - dns01:
        acmeDNS:
          host: https://api.getlocalcert.net/api/v1/acme-dns-compat
          accountSecretRef:
            name: acme-dns
            key: credentials.json</code></pre>

    <p><a href="https://docs.getlocalcert.net/acme-clients/cert-manager/">See full example</a>

    </p></div>
  </div>

  <div>
    <h3>Using acme.sh</h3>

    <p>
    Get started quickly
    </p>

    <div>
    <pre><code>
$ export ACMEDNS_BASE_URL=https://api.getlocalcert.net/api/v1/acme-dns-compat
$ export ACMEDNS_USERNAME=yourApiKeyId
$ export ACMEDNS_PASSWORD=yourApiKeySecret
$ export ACMEDNS_SUBDOMAIN=yoursubdomain
$ ./acme.sh --issue \
            --dns dns_acmedns \
            -d yoursubdomain.localhostcert.net</code></pre>
    <p><a href="https://docs.getlocalcert.net/acme-clients/acme-sh/">See full example</a>
    </p></div>
  </div>



</div>

<div>

  <div>
    <h3>Using LEGO</h3>

    <p>
    A client build by Let's Encrypt
    </p>

    <div>
    <pre><code>
$ export ACME_DNS_API_BASE=https://api.getlocalcert.net/api/v1/acme-dns-compat
$ export ACME_DNS_STORAGE_PATH=credentials.json
$ lego --email you@example.com \
       --dns acme-dns \
       --domains yoursubdomain.localhostcert.net \
       run</code></pre>
    <p><a href="https://docs.getlocalcert.net/acme-clients/lego/">See full example</a>
    </p></div>
    
  </div>

  <div>
    <h3>Certify The Web</h3>

    <p>
    Manage certificates on Windows and IIS
    </p>

    <div>
      <pre><code>
1. Select acme-dns as the DNS update method.
2. Enter https://api.getlocalcert.net/api/v1/acme-dns-compat as the server
3. Click Request Certificate
4. Skip the CNAME step, you won't need it
      </code></pre>
      <p><a href="https://docs.getlocalcert.net/acme-clients/certify-the-web/">See full example</a>
    </p></div>
  </div>

</div>

    <section>
      <h2>
        Ready to set up your free domain name?
      </h2>
      
      <p>
      <a href="https://console.getlocalcert.net/">Get Started</a>
      </p>
    </section>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brian Eno albums available in Dolby Atmos and Spatial (117 pts)]]></title>
            <link>https://www.udiscovermusic.com/news/brian-eno-albums-dolby-atmos-first-time/</link>
            <guid>36674069</guid>
            <pubDate>Mon, 10 Jul 2023 22:49:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.udiscovermusic.com/news/brian-eno-albums-dolby-atmos-first-time/">https://www.udiscovermusic.com/news/brian-eno-albums-dolby-atmos-first-time/</a>, See on <a href="https://news.ycombinator.com/item?id=36674069">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mvp-main-body-wrap">
<!--hide-mobile-newmenu-tabs--><article id="mvp-article-wrap" itemscope="" itemtype="http://schema.org/NewsArticle">
			<meta itemscope="" itemprop="mainEntityOfPage" itemtype="https://schema.org/WebPage" itemid="https://www.udiscovermusic.com/news/brian-eno-albums-dolby-atmos-first-time/">
        	
			<div>
					
					<header id="mvp-post-head">
                      
												  <span><p>The titles from Eno’s catalog now available include ‘Taking Tiger Mountain (By Strategy)’ and ‘Another Green World.’</p>
</span>
																			<!--mvp-author-info-wrap-->
											</header>
					
					<div id="mvp-post-content-top">								
					
					
						<div id="mvp-post-feat-img" itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
																				
												<p><img width="1000" height="600" src="https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy-1000x600.jpg" alt="Brian-Eno-Albums-Dolby-Atmos" srcset="https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy-1000x600.jpg 1000w, https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy-300x180.jpg 300w, https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy-1024x614.jpg 1024w, https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy-768x461.jpg 768w, https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy-590x354.jpg 590w, https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy-400x240.jpg 400w, https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy.jpg 1440w" sizes="(max-width: 1000px) 100vw, 1000px"></p><meta itemprop="url" content="https://www.udiscovermusic.com/wp-content/uploads/2023/03/Brian-Eno-Cecily-Eno-2023-copy-1000x600.jpg">
													<meta itemprop="width" content="1000">
													<meta itemprop="height" content="600">
											
																							
                                            </div><!--mvp-post-feat-img-->
                                        					
										<p>Brian Eno - Photo: Cecily Eno</p>
					
					

					<div id="mvp-content-main">
	                    
<p>Seen classic titles from <a href="https://www.udiscovermusic.com/artist/brian-eno/"><strong>Brian Eno’s</strong></a> catalog are now available in Dolby Atmos for the first time, These new releases range from his early solo work, <a href="https://www.udiscovermusic.com/stories/rediscover-eno-taking-tiger-mountain/"><strong><em>Taking Tiger Mountain</em></strong></a> and <a href="https://www.udiscovermusic.com/stories/brian-eno-another-green-world-new-sound/"><strong><em>Another Green World</em></strong></a> to some of the artist’s most renowned output of recent times including <em>Small Craft On A Milk Sea</em> and <em>The Ship</em>, which will be presented for the first time as a live orchestral arrangement in concert this coming October.</p>
<p><strong><a href="https://shop.udiscovermusic.com/collections/brian-eno" target="_blank" rel="noopener">Shop the best of Brian Eno’s discography on vinyl and more</a>.</strong></p>
<p>Brian Eno says, “What’s interesting about 3D music is the possibility of making an immersive space which is capable of sustaining much more detail than a 2 dimensional space (such as that presented by stereo listening). Our ears aren’t as directional as our eyes, but they are still capable of locating sounds in space, and listening in 3 dimensions this becomes a whole new compositional possibility. It allows a listener the experience of ‘exploring’ the musical space in much more intricate ways. It’s the step from 2-dimensional sound-painting to 3-dimensional sound-sculpting.”</p>
		
<p>Additionally, Brian Eno’s most recent album, the critically acclaimed vocal album <a href="https://www.udiscovermusic.com/news/brian-eno-foreverandevernomore-out-now/"><em><strong>FOREVERANDEVERNOMORE</strong></em></a> and the album’s instrumental accompaniment, <a href="https://www.udiscovermusic.com/news/brian-eno-forever-voiceless/"><em><strong>Forever Voiceless</strong></em></a> will also be available in Dolby Atmos and Spatial audio.</p>
<p>In June, Eno announced details of his<a href="https://www.udiscovermusic.com/news/brian-eno-solo-tour-ships/"><strong> first-ever solo tour</strong></a>, which will take him across Europe later this year. The electronic and ambient music legend has previously toured as part of <a href="https://www.udiscovermusic.com/artist/roxy-music/"><strong>Roxy Music</strong></a> and with other artists – including most recently alongside his brother in 2021 – as well as at festivals and his own one-off shows.</p>
<p>His first solo tour will kick off with two shows at the Venice Biennale Musica on October 21 before heading to Berlin, Paris, and Utrecht. It will wrap up on October 30 with another pair of shows at London’s Royal Festival Hall. <a href="https://linktr.ee/brianeno?utm_source=Email&amp;utm_medium=Email&amp;utm_campaign=BrianEnoBrianEnoalbumsavailableinDolbyAtmosandSpatialforthefirsttime070723&amp;utm_content=UMGUK42415-1012847&amp;vvsa_consumer_id=35467988&amp;vvsa_tracking=_vvsa_tyiMfxQa525130" target="_blank" rel="noopener"><strong>Tickets are still available to purchase</strong></a>, though the shows in Venice and Utrecht have now sold out.</p>
<p>Eno’s new live show, Ships, will be based around his 2016 album<em> The Ship</em> and was originally commissioned by La Biennale di Venezia. He will perform together with the Baltic Sea Philharmonic, orchestrated and conducted by Kristin Järvi. The performance will also feature actor Peter Serafinowicz and Eno’s long-time collaborators Leo Abrahams (guitar) and Peter Chilvers (programming/keyboards).</p>
<p><strong><a href="https://brianeno.lnk.to/StreamEM!UMGUK42415-1012847?vvsa_consumer_id=35467988&amp;vvsa_tracking=_vvsa_JWfcz8cD525129" target="_blank" rel="noopener">Listen to Brian Eno in Dolby Atmos and Spatial audio</a>.</strong></p>
		
						
					<!--US / UK English-->
                      
						
	

					</div><!--mvp-content-main-->
						
					</div><!--mvp-post-content-top-->
					
					<!--mvp-post-content-bottom-->

	</div><!--mvp-main-box-->
	
	
</article><!--mvp-article-wrap-->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MicroVM by QEMU (119 pts)]]></title>
            <link>https://qemu.readthedocs.io/en/latest/system/i386/microvm.html</link>
            <guid>36673945</guid>
            <pubDate>Mon, 10 Jul 2023 22:35:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qemu.readthedocs.io/en/latest/system/i386/microvm.html">https://qemu.readthedocs.io/en/latest/system/i386/microvm.html</a>, See on <a href="https://news.ycombinator.com/item?id=36673945">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <nav data-toggle="wy-nav-shift">
      
    </nav>

    <section data-toggle="wy-nav-shift">

      
      <nav aria-label="top navigation">
        
          <i data-toggle="wy-nav-top"></i>
          <a href="https://qemu.readthedocs.io/en/latest/index.html">QEMU</a>
        
      </nav>


      <div id="microvm-virtual-platform-microvm" itemprop="articleBody" role="main" itemscope="itemscope" itemtype="http://schema.org/Article">

<p><code><span>microvm</span></code> is a machine type inspired by <code><span>Firecracker</span></code> and
constructed after its machine model.</p>
<p>It’s a minimalist machine type without <code><span>PCI</span></code> nor <code><span>ACPI</span></code> support,
designed for short-lived guests. microvm also establishes a baseline
for benchmarking and optimizing both QEMU and guest operating systems,
since it is optimized for both boot time and footprint.</p>
<div id="supported-devices">
<h2>Supported devices<a href="#supported-devices" title="Permalink to this headline">¶</a></h2>
<p>The microvm machine type supports the following devices:</p>
<ul>
<li>ISA bus</li>
<li>i8259 PIC (optional)</li>
<li>i8254 PIT (optional)</li>
<li>MC146818 RTC (optional)</li>
<li>One ISA serial port (optional)</li>
<li>LAPIC</li>
<li>IOAPIC (with kernel-irqchip=split by default)</li>
<li>kvmclock (if using KVM)</li>
<li>fw_cfg</li>
<li>Up to eight virtio-mmio devices (configured by the user)</li>
</ul>
</div>
<div id="limitations">
<h2>Limitations<a href="#limitations" title="Permalink to this headline">¶</a></h2>
<p>Currently, microvm does <em>not</em> support the following features:</p>
<ul>
<li>PCI-only devices.</li>
<li>Hotplug of any kind.</li>
<li>Live migration across QEMU versions.</li>
</ul>
</div>
<div id="using-the-microvm-machine-type">
<h2>Using the microvm machine type<a href="#using-the-microvm-machine-type" title="Permalink to this headline">¶</a></h2>
<div id="machine-specific-options">
<h3>Machine-specific options<a href="#machine-specific-options" title="Permalink to this headline">¶</a></h3>
<p>It supports the following machine-specific options:</p>
<ul>
<li>microvm.x-option-roms=bool (Set off to disable loading option ROMs)</li>
<li>microvm.pit=OnOffAuto (Enable i8254 PIT)</li>
<li>microvm.isa-serial=bool (Set off to disable the instantiation an ISA serial port)</li>
<li>microvm.pic=OnOffAuto (Enable i8259 PIC)</li>
<li>microvm.rtc=OnOffAuto (Enable MC146818 RTC)</li>
<li>microvm.auto-kernel-cmdline=bool (Set off to disable adding virtio-mmio devices to the kernel cmdline)</li>
</ul>
</div>
<div id="boot-options">
<h3>Boot options<a href="#boot-options" title="Permalink to this headline">¶</a></h3>
<p>By default, microvm uses <code><span>qboot</span></code> as its BIOS, to obtain better boot
times, but it’s also compatible with <code><span>SeaBIOS</span></code>.</p>
<p>As no current FW is able to boot from a block device using
<code><span>virtio-mmio</span></code> as its transport, a microvm-based VM needs to be run
using a host-side kernel and, optionally, an initrd image.</p>
</div>
<div id="running-a-microvm-based-vm">
<h3>Running a microvm-based VM<a href="#running-a-microvm-based-vm" title="Permalink to this headline">¶</a></h3>
<p>By default, microvm aims for maximum compatibility, enabling both
legacy and non-legacy devices. In this example, a VM is created
without passing any additional machine-specific option, using the
legacy <code><span>ISA</span> <span>serial</span></code> device as console:</p>
<div><pre><span></span>$ qemu-system-x86_64 -M microvm \
   -enable-kvm -cpu host -m 512m -smp 2 \
   -kernel vmlinux -append "earlyprintk=ttyS0 console=ttyS0 root=/dev/vda" \
   -nodefaults -no-user-config -nographic \
   -serial stdio \
   -drive id=test,file=test.img,format=raw,if=none \
   -device virtio-blk-device,drive=test \
   -netdev tap,id=tap0,script=no,downscript=no \
   -device virtio-net-device,netdev=tap0
</pre></div>
<p>While the example above works, you might be interested in reducing the
footprint further by disabling some legacy devices. If you’re using
<code><span>KVM</span></code>, you can disable the <code><span>RTC</span></code>, making the Guest rely on
<code><span>kvmclock</span></code> exclusively. Additionally, if your host’s CPUs have the
<code><span>TSC_DEADLINE</span></code> feature, you can also disable both the i8259 PIC and
the i8254 PIT (make sure you’re also emulating a CPU with such feature
in the guest).</p>
<p>This is an example of a VM with all optional legacy features
disabled:</p>
<div><pre><span></span>$ qemu-system-x86_64 \
   -M microvm,x-option-roms=off,pit=off,pic=off,isa-serial=off,rtc=off \
   -enable-kvm -cpu host -m 512m -smp 2 \
   -kernel vmlinux -append "console=hvc0 root=/dev/vda" \
   -nodefaults -no-user-config -nographic \
   -chardev stdio,id=virtiocon0 \
   -device virtio-serial-device \
   -device virtconsole,chardev=virtiocon0 \
   -drive id=test,file=test.img,format=raw,if=none \
   -device virtio-blk-device,drive=test \
   -netdev tap,id=tap0,script=no,downscript=no \
   -device virtio-net-device,netdev=tap0
</pre></div>
</div>
<div id="triggering-a-guest-initiated-shut-down">
<h3>Triggering a guest-initiated shut down<a href="#triggering-a-guest-initiated-shut-down" title="Permalink to this headline">¶</a></h3>
<p>As the microvm machine type includes just a small set of system
devices, some x86 mechanisms for rebooting or shutting down the
system, like sending a key sequence to the keyboard or writing to an
ACPI register, doesn’t have any effect in the VM.</p>
<p>The recommended way to trigger a guest-initiated shut down is by
generating a <code><span>triple-fault</span></code>, which will cause the VM to initiate a
reboot. Additionally, if the <code><span>-no-reboot</span></code> argument is present in the
command line, QEMU will detect this event and terminate its own
execution gracefully.</p>
<p>Linux does support this mechanism, but by default will only be used
after other options have been tried and failed, causing the reboot to
be delayed by a small number of seconds. It’s possible to instruct it
to try the triple-fault mechanism first, by adding <code><span>reboot=t</span></code> to the
kernel’s command line.</p>
</div>
</div>
</div>

    </section>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shortening the Let's Encrypt chain of trust (455 pts)]]></title>
            <link>https://letsencrypt.org/2023/07/10/cross-sign-expiration.html</link>
            <guid>36673793</guid>
            <pubDate>Mon, 10 Jul 2023 22:19:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://letsencrypt.org/2023/07/10/cross-sign-expiration.html">https://letsencrypt.org/2023/07/10/cross-sign-expiration.html</a>, See on <a href="https://news.ycombinator.com/item?id=36673793">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<article>
		<p>When Let’s Encrypt first launched, we needed to ensure that our certificates were widely trusted. To that end, we arranged to have our intermediate certificates <a href="https://letsencrypt.org/2015/10/19/lets-encrypt-is-trusted.html">cross-signed by IdenTrust’s DST Root CA X3</a>. This meant that all certificates issued by those intermediates would be trusted, even while our own ISRG Root X1 wasn’t yet. During subsequent years, our Root X1 became <a href="https://letsencrypt.org/docs/certificate-compatibility/">widely trusted</a> on its own.&nbsp;</p>
<p>Come late 2021, our cross-signed intermediates and DST Root CA X3 itself were expiring. And while all up-to-date browsers at that time trusted our root, <a href="https://letsencrypt.org/2020/11/06/own-two-feet.html">over a third of Android devices</a> were still running old versions of the OS which would suddenly stop trusting websites using our certificates. That breakage would have been too widespread, so we arranged for a new cross-sign – this time <a href="https://letsencrypt.org/2020/12/21/extending-android-compatibility.html">directly onto our root</a> rather than our intermediates – which would outlive DST Root CA X3 itself. This stopgap allowed those old Android devices to continue trusting our certificates for three more years.</p>
<p>On September 30th, 2024, that cross-sign too will expire.</p>
<p>In the last three years, the percentage of Android devices which trust our ISRG Root X1 has risen from 66% to 93.9%. That percentage will increase further over the next year, especially as Android releases version 14, which has the ability to <a href="https://www.xda-developers.com/android-14-root-certificates-updatable">update its trust store without a full OS update</a>. In addition, dropping the cross-sign will reduce the number of certificate bytes sent in a TLS handshake by over 40%. Finally, it will significantly reduce our operating costs, allowing us to focus our funding on continuing to improve your privacy and security.</p>
<p>For these reasons, we will not be getting a new cross-sign to extend compatibility any further.</p>
<p>The transition will roll out as follows:</p>
<ul>
<li>
<p>On <strong>Thursday, Feb 8th, 2024</strong>, we will stop providing the cross-sign by default in requests made to our /acme/certificate API endpoint. For most Subscribers, this means that your ACME client will configure a chain which terminates at ISRG Root X1, and your webserver will begin providing this shorter chain in all TLS handshakes. The longer chain, terminating at the soon-to-expire cross-sign, will still be available as an alternate chain which you can configure your client to request.</p>
</li>
<li>
<p>On <strong>Thursday, June 6th, 2024</strong>, we will stop providing the longer cross-signed chain entirely. This is just over <a href="https://letsencrypt.org/2015/11/09/why-90-days.html">90 days</a> (the lifetime of one certificate) before the cross-sign expires, and we need to make sure subscribers have had at least one full issuance cycle to migrate off of the cross-signed chain.</p>
</li>
<li>
<p>On <strong>Monday, September 30th, 2024</strong>, the cross-signed certificate will expire. This should be a non-event for most people, as any client breakages should have occurred over the preceding six months.</p>
</li>
</ul>
<p><img alt="Infographic of the distribution of installed Android versions, showing that 93.9% of the population is running Android 7.1 or above." src="https://letsencrypt.org/images/2023.07.10-android-version-distribution.png">
</p>
<p><strong>If you use Android 7.0 or earlier</strong>, you may need to take action to ensure you can still access websites secured by Let’s Encrypt certificates. We recommend installing and using <a href="https://www.mozilla.org/en-US/firefox/browsers/mobile/android/">Firefox Mobile</a>, which uses its own trust store instead of the Android OS trust store, and therefore trusts ISRG Root X1.</p>
<p><strong>If you are a site operator</strong>, you should keep an eye on your website usage statistics and active user-agent strings during Q2 and Q3 of 2024. If you see a sudden drop in visits from Android, it is likely because you have a significant population of users on Android 7.0 or earlier. We encourage you to provide the same advice to them as we provided above.</p>
<p><strong>If you are an ACME client author</strong>, please make sure that your client correctly downloads and installs the certificate chain provided by our API during every certificate issuance, including renewals. Failure modes we have seen in the past include a) never downloading the chain at all and only serving the end-entity certificate; b) never downloading the chain and instead serving a hard-coded chain; and c) only downloading the chain at first issuance and not re-downloading during renewals. Please ensure that your client does not fall into any of these buckets.</p>
<p>We appreciate your understanding and support, both now and in the years to come as we provide safe and secure communication to everyone who uses the web. If you have any questions about this transition or any of the other work we do, please ask on our <a href="https://community.letsencrypt.org/">community forum</a>.</p>
<p>We’d like to thank IdenTrust for their years of partnership. They played an important role in helping Let’s Encrypt get to where we are today and their willingness to arrange a stopgap cross sign in 2021 demonstrated a true commitment to creating a secure Web.&nbsp;</p>
<p>We depend on contributions from our supporters in order to provide our services. If your company or organization can help our work by becoming a <a href="https://www.abetterinternet.org/sponsor/">sponsor</a> of Let’s Encrypt please email us at <a href="mailto:sponsor@letsencrypt.org">sponsor@letsencrypt.org</a>. We ask that you make an <a href="https://letsencrypt.org/donate/">individual contribution</a> if it is within your means.</p>

	</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Self-hosted photo and video backups directly from your mobile phone (643 pts)]]></title>
            <link>https://github.com/immich-app/immich</link>
            <guid>36673224</guid>
            <pubDate>Mon, 10 Jul 2023 21:28:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/immich-app/immich">https://github.com/immich-app/immich</a>, See on <a href="https://news.ycombinator.com/item?id=36673224">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><div dir="auto"> 
    
  <p><a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/7d08b7e3dec312341d94a4aff5add03658295489c509aa6a7af1d911a7b93e33/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e7376673f636f6c6f723d334635314235267374796c653d666f722d7468652d6261646765266c6162656c3d4c6963656e7365266c6f676f436f6c6f723d303030303030266c6162656c436f6c6f723d656365636563" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/license-MIT-green.svg?color=3F51B5&amp;style=for-the-badge&amp;label=License&amp;logoColor=000000&amp;labelColor=ececec"></a>
  <a href="https://discord.gg/D8JsnBEuKb" rel="nofollow">
    <img src="https://camo.githubusercontent.com/9951ac4f9dbf9adbd35d470c4668d875f6ffa8769bdb3f97d5b353ec154f5dac/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3937393131363632333837393336383735352e7376673f6c6162656c3d446973636f7264266c6f676f3d446973636f7264267374796c653d666f722d7468652d6261646765266c6f676f436f6c6f723d303030303030266c6162656c436f6c6f723d656365636563" data-canonical-src="https://img.shields.io/discord/979116623879368755.svg?label=Discord&amp;logo=Discord&amp;style=for-the-badge&amp;logoColor=000000&amp;labelColor=ececec">
  </a></p></div>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/immich-app/immich/blob/main/design/immich-logo.svg"><img src="https://github.com/immich-app/immich/raw/main/design/immich-logo.svg" width="150" title="Login With Custom URL"></a>
</p>
<h3 tabindex="-1" dir="auto">Immich - High performance self-hosted photo and video backup solution</h3>
<br>
<a href="https://immich.app/" rel="nofollow">
<img src="https://github.com/immich-app/immich/raw/main/design/immich-screenshots.png" title="Main Screenshot">
</a>

<p dir="auto">
  <a href="https://github.com/immich-app/immich/blob/main/README_zh_CN.md">中文</a>
  <a href="https://github.com/immich-app/immich/blob/main/README_tr_TR.md">Türkçe</a>
  <a href="https://github.com/immich-app/immich/blob/main/README_ca_ES.md">Català</a>
</p>
<h2 tabindex="-1" dir="auto">Disclaimer</h2>
<ul dir="auto">
<li><g-emoji alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png">⚠️</g-emoji> The project is under <strong>very active</strong> development.</li>
<li><g-emoji alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png">⚠️</g-emoji> Expect bugs and breaking changes.</li>
<li><g-emoji alias="warning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png">⚠️</g-emoji> <strong>Do not use the app as the only way to store your photos and videos!</strong></li>
</ul>
<h2 tabindex="-1" dir="auto">Content</h2>
<ul dir="auto">
<li><a href="https://immich.app/docs" rel="nofollow">Official Documentation</a></li>
<li><a href="https://github.com/orgs/immich-app/projects/1">Roadmap</a></li>
<li><a href="#demo">Demo</a></li>
<li><a href="#features">Features</a></li>
<li><a href="https://immich.app/docs/overview/introduction" rel="nofollow">Introduction</a></li>
<li><a href="https://immich.app/docs/install/requirements" rel="nofollow">Installation</a></li>
<li><a href="https://immich.app/docs/overview/support-the-project" rel="nofollow">Contribution Guidelines</a></li>
<li><a href="#support-the-project">Support The Project</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto">You can find the main documentation, including installation guides, at <a href="https://immich.app/" rel="nofollow">https://immich.app/</a>.</p>
<h2 tabindex="-1" dir="auto">Demo</h2>
<p dir="auto">You can access the web demo at <a href="https://demo.immich.app/" rel="nofollow">https://demo.immich.app</a></p>
<p dir="auto">For the mobile app, you can use <code>https://demo.immich.app/api</code> for the <code>Server Endpoint URL</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="The credential
email: demo@immich.app
password: demo"><pre>The credential
email: demo@immich.app
password: demo</pre></div>
<div data-snippet-clipboard-copy-content="Spec: Free-tier Oracle VM - Amsterdam - 2.4Ghz quad-core ARM64 CPU, 24GB RAM"><pre><code>Spec: Free-tier Oracle VM - Amsterdam - 2.4Ghz quad-core ARM64 CPU, 24GB RAM
</code></pre></div>
<h2 tabindex="-1" dir="auto">Features</h2>
<table>
<thead>
<tr>
<th>Features</th>
<th>Mobile</th>
<th>Web</th>
</tr>
</thead>
<tbody>
<tr>
<td>Upload and view videos and photos</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Auto backup when the app is opened</td>
<td>Yes</td>
<td>N/A</td>
</tr>
<tr>
<td>Selective album(s) for backup</td>
<td>Yes</td>
<td>N/A</td>
</tr>
<tr>
<td>Download photos and videos to local device</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Multi-user support</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Album and Shared albums</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Scrubbable/draggable scrollbar</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Support raw formats</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Metadata view (EXIF, map)</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Search by metadata, objects, faces, and CLIP</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Administrative functions (user management)</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Background backup</td>
<td>Yes</td>
<td>N/A</td>
</tr>
<tr>
<td>Virtual scroll</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>OAuth support</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>API Keys</td>
<td>N/A</td>
<td>Yes</td>
</tr>
<tr>
<td>LivePhoto backup and playback</td>
<td>iOS</td>
<td>Yes</td>
</tr>
<tr>
<td>User-defined storage structure</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Public Sharing</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Archive and Favorites</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Global Map</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Partner Sharing</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Facial recognition and clustering</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Memories (x years ago)</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Offline support</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Read-only gallery</td>
<td>Yes</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto">Support the project</h2>
<p dir="auto">I've committed to this project, and I will not stop. I will keep updating the docs, adding new features, and fixing bugs. But I can't do it alone. So I need your help to give me additional motivation to keep going.</p>
<p dir="auto">As our hosts in the <a href="https://selfhosted.show/79?t=1418" rel="nofollow">selfhosted.show - In the episode 'The-organization-must-not-be-name is a Hostile Actor'</a> said, this is a massive undertaking of what the team and I are doing. And I would love to someday be able to do this full-time, and I am asking for your help to make that happen.</p>
<p dir="auto">If you feel like this is the right cause and the app is something you are seeing yourself using for a long time, please consider supporting the project with the option below.</p>
<h2 tabindex="-1" dir="auto">Donation</h2>
<ul dir="auto">
<li><a href="https://github.com/sponsors/alextran1502">Monthly donation</a> via GitHub Sponsors</li>
<li><a href="https://github.com/sponsors/alextran1502?frequency=one-time&amp;sponsor=alextran1502">One-time donation</a> via GitHub Sponsors</li>
<li><a href="https://liberapay.com/alex.tran1502/" rel="nofollow">Librepay</a></li>
<li><a href="https://www.buymeacoffee.com/altran1502" rel="nofollow">buymeacoffee</a></li>
<li>Bitcoin: 1FvEp6P6NM8EZEkpGUFAN2LqJ1gxusNxZX</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gandi.net updates pricing, increases rates by up to 1000% (114 pts)]]></title>
            <link>https://chatting.neocities.org/posts/2023-gandi-pricing</link>
            <guid>36673108</guid>
            <pubDate>Mon, 10 Jul 2023 21:20:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chatting.neocities.org/posts/2023-gandi-pricing">https://chatting.neocities.org/posts/2023-gandi-pricing</a>, See on <a href="https://news.ycombinator.com/item?id=36673108">Hacker News</a></p>
<div id="readability-page-1" class="page">

<h5 id="july-10-2023">July 10, 2023</h5>
<hr>
<p>According to <a href="https://chatting.neocities.org/documents/gandi-pricing-email.html">an email</a> sent to its customers in mid-June, Gandi.net, a major European domain name registration and web hosting company, is <a href="https://gandi.link/eur-prices2023">increasing</a> prices across its entire suite of products.</p>
<p>For domain names, the new prices will usually amount to a 25 to 50% increase, depending on the extension. For example, .com domain prices will jump by 34%, .net by 45%, .org by 32%, etc.</p>
<p>A much steeper price hike is awaiting web hosting customers, who will see their costs almost double in some cases. In particular, the Advanced web hosting plan will be raised from €8.25/month to €15.00/month, an 82% surge.</p>
<p>But by far the most egregious is the email service.</p>
<p>Gandi had already started to inform customers on their intention to <a href="https://news.ycombinator.com/item?id=35080777">discontinue</a> their offer of 2 free mailboxes included with every domain name registration, including for users who had pre-paid for multiple years.</p>
<p>Now, Gandi will raise the prices of the Standard mailbox plan from €0.35/month to €3.99/month per mailbox, <b>a 1040% increase</b>. The Premium plan will go from €1.75/month to €5.99/month, a 242% increase.</p>
<p>In its announcement, Gandi appears to justify these price increases by describing the product as “new and improved”, citing increased storage (<i>editor’s note: this does not apply to Premium plan customers</i>) and other nebulous commitments, such as “better anti-spam protection”, and “stronger security”.</p>
<p>The price hikes will come into effect on July 13.</p>
<p><b>It is unclear how with the action of dropping extreme price increases on customers, backed by dubious justifications, and with barely a month’s notice can be reconciled with Gandi’s famous and long-standing <a href="https://www.gandi.net/en/no-bullshit">“No Bullshit” policy</a>.</b></p>
<p>At the time of writing, Gandi has still not updated its website (apart from the <a href="https://www.gandi.net/en/domain/email">email hosting page</a>, which does feature the new prices) to inform potential new customers of the upcoming changes.</p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Stripe is holding 50% for 9 month (112 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36673091</link>
            <guid>36673091</guid>
            <pubDate>Mon, 10 Jul 2023 21:18:53 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36673091">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="36674721"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36674721" href="https://news.ycombinator.com/vote?id=36674721&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>Some staff members from Stripe already replied that they are looking into this. But if you take a look at reviews on Trustpilot this (withholding funds) seems to be a very common complain, which is terrifying.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36674461"><td></td></tr>
                      <tr id="36674703"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36674703" href="https://news.ycombinator.com/vote?id=36674703&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>Every time one of these Stripe posts shows up, I always wonder what'll happen when Edwin retires or moves on.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36674963"><td></td></tr>
                  <tr id="36675493"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36675493" href="https://news.ycombinator.com/vote?id=36675493&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>What is the alternative to stripe? I was going to set it up for a new service in making but with all these posts I'm not so sure.<p>A quick google brings up paddle, is that going to have the same issues?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36675243"><td></td></tr>
            <tr id="36675031"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36675031" href="https://news.ycombinator.com/vote?id=36675031&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>9 month is long. I thought credit card companies only hold for 3 months at the most so not sure why Stripe needs it 3x longer</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36675695"><td></td></tr>
                  <tr id="36675561"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36675561" href="https://news.ycombinator.com/vote?id=36675561&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>Stop using Stripe, this is what you sign up for when you use them. PayPal is not much better. Use Adyen, or a processor that doesn't have to squeeze every dollar out of the transaction like Google Payments.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36675789"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36675789" href="https://news.ycombinator.com/vote?id=36675789&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>This is a regulatory problem not a business problem. Payment processors need to understand a client's risk profile and what business they do in order to prevent money laundering, and in practice, regulators only care about clients that have a certain volume.<p>Paypal, Stripe, and any other major payment processor have developed a strategy of letting users use the service with no restrictions until they hit that volume then they begin doing things like this.</p><p>Ultimately, this is caused by regulation and needs to be fixed by regulation, but it won't because practices like this are extremely lucrative.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36674629"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36674629" href="https://news.ycombinator.com/vote?id=36674629&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>Thank you for submitting your support request through the Stripe back channel support network.  Your ticket will be reviewed by nobody, unless you happen to be extremely lucky and win the Hacker News lottery.  Best of luck!</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36675273"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36675273" href="https://news.ycombinator.com/vote?id=36675273&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>I'm sorry your request has been denied by some schmuck you slighted in 3rd year of uni who works at stripe.<p>Too bad there is no incentive for companies to put in place adequate volume to deal with necessary customer support.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36675487"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36675487" href="https://news.ycombinator.com/vote?id=36675487&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>The shareholders have proclaimed that such a strategy is illegal due to fiduciary duty /s</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="36676060"><td></td></tr>
            <tr id="36675998"><td></td></tr>
            <tr id="36673940"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36673940" href="https://news.ycombinator.com/vote?id=36673940&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>There are a bunch of factors—we look at your refund and chargeback rates, then your revenue and how much you have in your Stripe balance. For macro factors, the card networks may also sense an uptick in refund requests depending on the product/service you're selling.<p>That said, I see your email into our support team and we're taking another look at this now.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36674267"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36674267" href="https://news.ycombinator.com/vote?id=36674267&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>It's a bit horrifying it takes a disgruntled HN post and <i>the pure chance</i> you happened across it. It's also discomforting how comfortable Stripe employees like yourself think holding 50% of someone's earning for any amount of time is acceptable. I really don't care if Stripe is HN's baby, this is awful customer service and gives me great pause in recommending or using Stripe.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36674329"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36674329" href="https://news.ycombinator.com/vote?id=36674329&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>I feel like these posts should be disallowed. Seems like Stripe is using HN as a free help forum, and it's customers have accepted that this is the place to go when you need to talk to a human. I don't think HN should encourage it, and I don't know why we all play along. I've seen a solid handful or two of these posts, and most of the time users aren't looking for discussion, only enough attention to get the Stripe team to act. My opinion only, obviously I sympathize with OP whose just trying to get their money and this seems to be the tried and true way to get a response, but I definitely don't think it adds anything positive to the site other than a recurring warning to other would-be Stripe customers.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36675014"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36675014" href="https://news.ycombinator.com/vote?id=36675014&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>&gt; other than a recurring warning to other would-be Stripe customers<p>that seems like an exceptionally high-value positive in a site with a large number of users who need payment services.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36674498"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36674498" href="https://news.ycombinator.com/vote?id=36674498&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>HN should encourage it, because startup founders read HN. They should think twice risking their business existence, after seeing this benevolent approach.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36674557"><td></td></tr>
                <tr id="36675455"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36675455" href="https://news.ycombinator.com/vote?id=36675455&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>Sunlight only works reasonably well for waterborne pathogens, and only when you've got time.<p>As with disinfecting anything, and especially for societal issues, we need more than sunlight alone.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36674851"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36674851" href="https://news.ycombinator.com/vote?id=36674851&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>Tell that to Edward Snowden, Julian Assange, or one of the billions of working families all over the world that <i>everyone</i> knows is getting f*cked over on a constant basis.<p>Sunlight is not a disinfectant. Better to try gasoline.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="36675244"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36675244" href="https://news.ycombinator.com/vote?id=36675244&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>These types of companies are often frauds (anything involving large amounts charged for travel, or housing of some sort). A bunch of <i>big</i> charges comes in, and then months later there are massive chargebacks and the company in question no longer exists. Stripe is left holding the bag. Their only option with these large charges for services to be delivered in the future is to not release funds until they are quite certain there won't be a charge back.<p>For every instance of what appears to be bad behavior by a payments company, consider the angle of the fraudster. They are really screwing things up for everybody.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36675298"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36675298" href="https://news.ycombinator.com/vote?id=36675298&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>In a working payment system, the scenario where someone initiates a valid dispute nine months after the fact would be extremely rare: it would be limited to a few types of physical card theft where the card owner is unable to report the card missing for some relatively unusual reason. We just don’t have a working payment system, so unfortunately this stuff is incredibly common and everyone has to suffer because of it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36675346"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36675346" href="https://news.ycombinator.com/vote?id=36675346&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>There are genuine cases (travel and lodging are the obvious ones) where people tend to pay for things months in advance. I don't see it going away. If you can come up with a cheap, effective, convenient payment method to cover these things, you are on a winner. I don't think it's possible because the time horizon creates risk, there is no avoiding it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="36674505"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36674505" href="https://news.ycombinator.com/vote?id=36674505&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>The alternative is likely not being able to work with the business at all due to their risk profile and chargeback rates.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36675032"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36675032" href="https://news.ycombinator.com/vote?id=36675032&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>That seems... better? At least you know what you're in for (ie. you can only take crypto or go with a risk-taking payment processor that has an explicit net 120 payout schedule), so you can plan your business around it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="36674159"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36674159" href="https://news.ycombinator.com/vote?id=36674159&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>As always, why bother having real support when you can instead wait for paying customer to bitch publicly on HN?<p>Sickening.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36674289"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36674289" href="https://news.ycombinator.com/vote?id=36674289&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>Sickening is hardly the word for a comms lead of Stripe being the first reply to a post about a customers issue, complete with some informative context and saying they will double check the issue.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36674376"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36674376" href="https://news.ycombinator.com/vote?id=36674376&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>Not the place for many reasons. As an example this discussion should not be publicly indexable whenever a customer Googles this same problem in the future.<p>Basic Marketing and Customer Care Management.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36674378"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36674378" href="https://news.ycombinator.com/vote?id=36674378&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>Are you new here? This website seems to be the only way to get human help from stripe. THAT is sickening.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36674531"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36674531" href="https://news.ycombinator.com/vote?id=36674531&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>What would you say the percentage of the total support requests that Stripe receives per day is compared to issues raised on HN?</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="36675291"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36675291" href="https://news.ycombinator.com/vote?id=36675291&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>When you hold on to someone's money for an extremely long time, do you earn any interest on that money?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36676021"><td></td></tr>
                  <tr id="36674192"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36674192" href="https://news.ycombinator.com/vote?id=36674192&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>Just wanted to say I appreciate seeing the responsiveness here. First comment on the thread even. Always better if it doesn’t reach this point, but it’s nice to know you can get a human’s attention one way or another.<p>Hoping to @edwinwee will share the outcome for other businesses who might need to evaluate Stripe.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36674562"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36674562" href="https://news.ycombinator.com/vote?id=36674562&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>You know what would be better? Maybe a support email? Or this crazy thing called a 1-800-number? The fact that Stripe de facto uses HN as their support forum and that people are lauding them for it is pretty laughable.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36674386"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36674386" href="https://news.ycombinator.com/vote?id=36674386&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><br><div>
                  <p><span>Search this website. Outcome is always the same: something resolved just this once. No procedural changes made. Stripe remains unreachable unless you get enough upvotes on HN</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36674849"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36674849" href="https://news.ycombinator.com/vote?id=36674849&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>The outcome isn't always the same.<p>It's 50/50:</p><p>- The poster was doing something shady and upon further investigation it turns out the situation is warranted</p><p>- The poster was indeed let down by a broken support system and Stripe staff chimes in to save the day
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36675253"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36675253" href="https://news.ycombinator.com/vote?id=36675253&amp;how=up&amp;goto=item%3Fid%3D36673091"></a></center>    </td><td><p><span>The outcome is not always, or perhaps even often, the same.<p>The last time I paid attention to one of these complaints, someone was using stripe to sell cellphone accessories (and yes, at least on my Stripe application, they asked what I was selling.)</p><p>After selling accessories for a while, he then used it to sell a minivan, and had a tantrum when Stripe -- quite reasonably -- said this flags all the flags, and Stripe was going to hold onto the money until they were sure things were kosher.</p><p>Getting dollars before multiple months post transaction basically leaves someone holding a bag of risk, and if you act shady AF, you can't be surprised if Stripe declines to hold that bag of risk for you.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A hash array-mapped trie implementation in C (155 pts)]]></title>
            <link>https://github.com/mkirchner/hamt</link>
            <guid>36672957</guid>
            <pubDate>Mon, 10 Jul 2023 21:06:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mkirchner/hamt">https://github.com/mkirchner/hamt</a>, See on <a href="https://news.ycombinator.com/item?id=36672957">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">libhamt</h2>
<p dir="auto">A hash array-mapped trie (HAMT) implementation in C99. A HAMT is a data
structure that can be used to efficiently implement
<a href="https://en.wikipedia.org/wiki/Persistent_data_structure" rel="nofollow"><em>persistent</em></a> associative arrays (aka maps,
dicts) and sets, see the <a href="#introduction">Introduction</a>. The implementation here
loosely follows Bagwell's 2000 paper<a href="https://lampwww.epfl.ch/papers/idealhashtrees.pdf" rel="nofollow">[1]</a>, with a focus on
code clarity.</p>
<p dir="auto">What prompted the somewhat detailed writeup was the realization that there is
not a lot of in-depth documentation for HAMTs beyond the original Bagwell
paper[<a href="https://lampwww.epfl.ch/papers/idealhashtrees.pdf" rel="nofollow">1</a>]. Some of the more helpful posts are <a href="http://blog.higher-order.net/2009/09/08/understanding-clojures-persistenthashmap-deftwice.html" rel="nofollow">Karl Krukow's
intro to Clojure's <code>PersistentHashMap</code></a>, <a href="https://github.com/chaelim/HAMT">C. S. Lim's
C++ template implementation</a>, <a href="https://blog.acolyer.org/2015/11/27/hamt/" rel="nofollow">Adrian Coyler's morning paper
post</a> and the original <a href="https://michael.steindorfer.name/publications/oopsla15.pdf" rel="nofollow">Steindoerfer/Vinju compressed HAMT
article it summarizes</a>. The rest mostly seems to be
all bits and pieces and this document is an attempt to (partially) improve that
situation.</p>
<p dir="auto"><em>FIXME: Complete docs (removal, persistence, path copying)</em></p>
<h2 tabindex="-1" dir="auto">Quickstart</h2>
<p dir="auto">To build the library and run the tests:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ git clone git@github.com:mkirchner/hamt.git
$ cd hamt
$ make
$ make test"><pre>$ git clone git@github.com:mkirchner/hamt.git
$ <span>cd</span> hamt
$ make
$ make <span>test</span></pre></div>
<p dir="auto">In order to use <code>libhamt</code> in your own projects, copy <code>include/hamt.h</code> and
<code>src/hamt.c</code> in your own source tree and build from there.</p>
<h3 tabindex="-1" dir="auto">Benchmarks</h3>
<p dir="auto">For basic performance comparison with AVL and red-black trees (from <code>libavl</code>)
and the HashTree from GLib, see <a href="https://github.com/mkirchner/hamt-bench">the benchmarking repo</a>.</p>
<h2 tabindex="-1" dir="auto">Introduction</h2>
<p dir="auto">A <em>hash array mapped trie (HAMT)</em> is a data structure that can be used to
implement <a href="https://en.wikipedia.org/wiki/Associative_array" rel="nofollow">associative arrays</a> (aka maps) and
<a href="https://en.wikipedia.org/wiki/Set_(abstract_data_type)" rel="nofollow">sets</a>.</p>
<p dir="auto">Structurally, HAMTs are <a href="https://en.wikipedia.org/wiki/Hash_tree_(persistent_data_structure)" rel="nofollow">hash trees</a> that combine favorable
characteristics of <a href="https://en.wikipedia.org/wiki/Hash_table" rel="nofollow">hash tables</a> and array mapped
<a href="https://en.wikipedia.org/wiki/Trie" rel="nofollow">tries</a>, namely almost hash table-like time complexity
guarantees<a href="https://lampwww.epfl.ch/papers/idealhashtrees.pdf" rel="nofollow">[1]</a> (O(log<sub>32</sub>n)) and economic use of memory.</p>
<p dir="auto">An additional benefit, and a key motivation for the work presented here, is that
augmentation of HAMTs with path copying and garbage collection allows for a
straightforward and efficient implementation of <a href="https://en.wikipedia.org/wiki/Persistent_data_structure" rel="nofollow">persistent</a>
versions of maps and sets.</p>
<p dir="auto">The remaining documentation starts with a description of the <code>libhamt</code> API and
two examples that demonstrate the use of a HAMT as an ephemeral and persistent
data structure, respectively. It then details the implementation: starting from
the foundational data structures and the helper code required for hash
exhaustion and table management, we cover search, insertion, removal, and
iterators. The final implementation section introduces path copying and explains
the changes required to support persistent insert and remove operations. It
closes with an outlook and an appendix.</p>
<h2 tabindex="-1" dir="auto">API</h2>
<h2 tabindex="-1" dir="auto">HAMT lifecycle</h2>
<p dir="auto">The core data type exported in the <code>libhamt</code> interface is <code>struct hamt</code>. In order to
create a <code>struct hamt</code> instance, one must call <code>hamt_create()</code>, which requires a
hash function of type <code>hamt_key_hash_fn</code> to hash keys, a comparison function of
type <code>hamt_cmp_fn</code> to compare keys, and a pointer to a <code>hamt_allocator</code> instance.
<code>hamt_delete()</code> deletes <code>struct hamt</code> instances that were created with <code>hamt_create()</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="/* The libhamt core data structure is a handle to a hash array-mapped trie */

/* Function signature definitions for key comparison and hashing */
typedef int (*hamt_cmp_fn)(const void *lhs, const void *rhs);
typedef uint32_t (*hamt_key_hash_fn)(const void *key, const size_t gen);

/* API functions for lifecycle management */
struct hamt *hamt_create(hamt_key_hash_fn key_hash, hamt_cmp_fn key_cmp, struct hamt_allocator *ator);
void hamt_delete(struct hamt *);"><pre><span>/* The libhamt core data structure is a handle to a hash array-mapped trie */</span>

<span>/* Function signature definitions for key comparison and hashing */</span>
<span>typedef</span> <span>int</span> (<span>*</span><span>hamt_cmp_fn</span>)(<span>const</span> <span>void</span> <span>*</span><span>lhs</span>, <span>const</span> <span>void</span> <span>*</span><span>rhs</span>);
<span>typedef</span> <span>uint32_t</span> (<span>*</span><span>hamt_key_hash_fn</span>)(<span>const</span> <span>void</span> <span>*</span><span>key</span>, <span>const</span> <span>size_t</span> <span>gen</span>);

<span>/* API functions for lifecycle management */</span>
<span>struct</span> <span>hamt</span> <span>*</span><span>hamt_create</span>(<span>hamt_key_hash_fn</span> <span>key_hash</span>, <span>hamt_cmp_fn</span> <span>key_cmp</span>, <span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>);
<span>void</span> <span>hamt_delete</span>(<span>struct</span> <span>hamt</span> <span>*</span>);</pre></div>
<p dir="auto">The <code>hamt_key_hash_fn</code> takes a <code>key</code> and a generation <code>gen</code>. The expectation is
that the supplied hash function returns different hashes for the same key but
different generations. Depending on the choice of hash function this can be
implemented using <code>gen</code> as a seed or modifying a copy of <code>key</code> on the fly.
See the <a href="#examples">examples</a> section for a <code>murmur3</code>-based implementation and
the <a href="#hashing">hashing</a> section for more information on suitable hash functions.</p>
<h3 tabindex="-1" dir="auto">Memory management</h3>
<p dir="auto"><code>libhamt</code> exports its internal memory management API through the <code>hamt_allocator</code>
struct. The struct specifies the functions that the HAMT implementation uses to
allocate, re-allocate and deallocate system memory. The API provides a default
<code>hamt_allocator_default</code> which refers to the standard <code>malloc()</code>, <code>realloc()</code>
and <code>free()</code> functions.</p>
<div dir="auto" data-snippet-clipboard-copy-content="struct hamt_allocator {
    void *(*malloc)(const size_t size);
    void *(*realloc)(void *chunk, const size_t size);
    void (*free)(void *chunk);
};

extern struct hamt_allocator hamt_allocator_default;"><pre><span>struct</span> <span>hamt_allocator</span> {
    <span>void</span> <span>*</span>(<span>*</span><span>malloc</span>)(<span>const</span> <span>size_t</span> <span>size</span>);
    <span>void</span> <span>*</span>(<span>*</span><span>realloc</span>)(<span>void</span> <span>*</span><span>chunk</span>, <span>const</span> <span>size_t</span> <span>size</span>);
    <span>void</span> (<span>*</span><span>free</span>)(<span>void</span> <span>*</span><span>chunk</span>);
};

<span>extern</span> <span>struct</span> <span>hamt_allocator</span> <span>hamt_allocator_default</span>;</pre></div>
<p dir="auto">Exporting the <code>libhamt</code> memory management API enables library clients to make
use of alternate memory management solutions, most notably of garbage collection
solutions (e.g. the <a href="https://www.hboehm.info/gc/" rel="nofollow">Boehm-Demers-Weiser GC</a>) which are required when
using the HAMT as a persistent data structure (see the <a href="#example-2-garbage-collected-persistent-hamts">structural sharing
example</a>).</p>
<h2 tabindex="-1" dir="auto">Query</h2>
<div dir="auto" data-snippet-clipboard-copy-content="size_t hamt_size(const struct hamt *trie);
const void *hamt_get(const struct hamt *trie, void *key);"><pre><span>size_t</span> <span>hamt_size</span>(<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>);
<span>const</span> <span>void</span> <span>*</span><span>hamt_get</span>(<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>, <span>void</span> <span>*</span><span>key</span>);</pre></div>
<p dir="auto">The <code>hamt_size()</code> function returns the size of the HAMT in O(1). Querying the
HAMT (i.e. searching a key) is done with <code>hamt_get()</code> which takes a pointer to a
key and returns a result in O(log<sub>32</sub> n) - or <code>NULL</code> if the key does
not exist in the HAMT.</p>
<h3 tabindex="-1" dir="auto">Iterators</h3>
<p dir="auto">The API also provides key/value pair access through the <code>hamt_iterator</code> struct.</p>
<div dir="auto" data-snippet-clipboard-copy-content="size_t hamt_size(const struct hamt *trie);
const void *hamt_get(const struct hamt *trie, void *key);"><pre><span>size_t</span> <span>hamt_size</span>(<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>);
<span>const</span> <span>void</span> <span>*</span><span>hamt_get</span>(<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>, <span>void</span> <span>*</span><span>key</span>);</pre></div>
<p dir="auto">Iterators are tied to a specific HAMT and are created using the
<code>hamt_it_create()</code> function, passing the HAMT instance the iterator should refer
to. Iterators can be advanced with the <code>hamt_it_next()</code> function and as long as
<code>hamt_it_valid()</code> returns <code>true</code>, the <code>hamt_it_get_key()</code> and
<code>hamt_it_get_value()</code> functions will return the pointers to the current
key/value pair. In order to delete an existing and/or exhausted iterator, call
<code>hamt_it_delete()</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct hamt_iterator_impl *hamt_iterator;

hamt_iterator hamt_it_create(const struct hamt *trie);
void hamt_it_delete(hamt_iterator it);
bool hamt_it_valid(hamt_iterator it);
hamt_iterator hamt_it_next(hamt_iterator it);
const void *hamt_it_get_key(hamt_iterator it);
const void *hamt_it_get_value(hamt_iterator it);"><pre><span>typedef</span> <span>struct</span> <span>hamt_iterator_impl</span> <span>*</span><span>hamt_iterator</span>;

<span>hamt_iterator</span> <span>hamt_it_create</span>(<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>);
<span>void</span> <span>hamt_it_delete</span>(<span>hamt_iterator</span> <span>it</span>);
<span>bool</span> <span>hamt_it_valid</span>(<span>hamt_iterator</span> <span>it</span>);
<span>hamt_iterator</span> <span>hamt_it_next</span>(<span>hamt_iterator</span> <span>it</span>);
<span>const</span> <span>void</span> <span>*</span><span>hamt_it_get_key</span>(<span>hamt_iterator</span> <span>it</span>);
<span>const</span> <span>void</span> <span>*</span><span>hamt_it_get_value</span>(<span>hamt_iterator</span> <span>it</span>);</pre></div>
<p dir="auto">Iterators maintain state about their traversal path and changes to the HAMT
that an iterator refers to implicitly invalidate the iteration (i.e. undefined
behavior).</p>
<p dir="auto">The order in which iterators return the key value pairs is fully defined by
the structure of the trie, which, in turn, is completely defined by the choice
of hash function and (where applicable) seed.</p>
<h2 tabindex="-1" dir="auto">Insert &amp; Remove</h2>
<p dir="auto"><code>libhamt</code> supports ephemeral and
<a href="https://en.wikipedia.org/wiki/Persistent_data_structure" rel="nofollow">persistent</a> (aka not ephemeral) HAMTs through two different interfaces:
<code>hamt_set()</code> and <code>hamt_remove()</code> for ephemeral use, and their <code>p</code>-versions
<code>hamt_pset()</code> and <code>hamt_premove()</code> for persistent use.</p>
<h3 tabindex="-1" dir="auto">Ephemeral modification</h3>
<div dir="auto" data-snippet-clipboard-copy-content="const void *hamt_set(struct hamt *trie, void *key, void *value);
void *hamt_remove(struct hamt *trie, void *key);"><pre><span>const</span> <span>void</span> <span>*</span><span>hamt_set</span>(<span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>, <span>void</span> <span>*</span><span>key</span>, <span>void</span> <span>*</span><span>value</span>);
<span>void</span> <span>*</span><span>hamt_remove</span>(<span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>, <span>void</span> <span>*</span><span>key</span>);</pre></div>
<p dir="auto"><code>hamt_set()</code> takes a pair of <code>key</code> and <code>value</code> pointers and adds the pair to the HAMT,
returning a pointer to the <code>value</code>. If the <code>key</code> already exists, <code>hamt_set()</code>
updates the pointer to the <code>value</code>.</p>
<p dir="auto"><code>hamt_remove()</code> takes a <code>key</code> and removes the key/value pair with the
respective <code>key</code> from the HAMT, returning a pointer to the <code>value</code> that was
just removed. If the <code>key</code> does not exist, <code>hamt_remove()</code> returns <code>NULL</code>.</p>
<h3 tabindex="-1" dir="auto">Persistent HAMTs</h3>
<p dir="auto">The semantics of persistent HAMTs are different from their ephemeral
counterparts: since every modification creates a new version of a HAMT, the
modificiation functions return a new HAMT. Modification of a persistent HAMT
therefore requires a reassignment idiom if the goal is modification only:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const struct hamt *h = hamt_create(...)
...
/* Set a value and drop the reference to the old HAMT; the GC
 * will take care of cleaning up remaining unreachable allocations.
 */
h = hamt_pset(h, some_key, some_value);
..."><pre><span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>h</span> <span>=</span> <span>hamt_create</span>(...)
...
<span>/* Set a value and drop the reference to the old HAMT; the GC</span>
<span> * will take care of cleaning up remaining unreachable allocations.</span>
<span> */</span>
<span>h</span> <span>=</span> <span>hamt_pset</span>(<span>h</span>, <span>some_key</span>, <span>some_value</span>);
...</pre></div>
<p dir="auto">This seems wasteful at first glance but the respective functions implement structural
sharing such that the overhead is limited to <em>~log<sub>32</sub>(N)</em> nodes (where <em>N</em> is the
number of nodes in the graph).</p>
<div dir="auto" data-snippet-clipboard-copy-content="const struct hamt *hamt_pset(const struct hamt *trie, void *key, void *value);
const struct hamt *hamt_premove(const struct hamt *trie, void *key);"><pre><span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>hamt_pset</span>(<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>, <span>void</span> <span>*</span><span>key</span>, <span>void</span> <span>*</span><span>value</span>);
<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>hamt_premove</span>(<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>, <span>void</span> <span>*</span><span>key</span>);</pre></div>
<p dir="auto"><code>hamt_pset()</code> inserts or updates the <code>key</code> with <code>value</code> and returns an opaque
handle to the new HAMT. The new HAMT is guaranteed to contain the new
key/value pair.</p>
<p dir="auto"><code>hamt_premove()</code> attempts to remove the value with the key <code>key</code>. It is <em>not</em>
an error if the key does not exist; the new HAMT is guaranteed to not contain
the key <code>key</code>.</p>
<h2 tabindex="-1" dir="auto">Examples</h2>
<h3 tabindex="-1" dir="auto">Example 1: ephemeral HAMT w/ standard allocation</h3>
<div dir="auto" data-snippet-clipboard-copy-content="#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#include &quot;hamt.h&quot;
#include &quot;murmur3.h&quot;


static uint32_t hash_string(const void *key, const size_t gen)
{
    return murmur3_32((uint8_t *)key, strlen((const char *)key), gen);
}

int main(int argn, char *argv[])
{
    enum { N = 5; };
    struct {
        char *country;
        char *capital;
    } cities[N] = {
        {&quot;Germany&quot;, &quot;Berlin&quot;},
        {&quot;Spain&quot;, &quot;Madrid&quot;},
        {&quot;Italy&quot;, &quot;Rome&quot;},
        {&quot;France&quot;, &quot;Paris&quot;},
        {&quot;Romania&quot;, &quot;Bucharest&quot;}
        /* ... */
    };

    struct hamt *t;

    /* create table */
    t = hamt_create(hash_string, strcmp, &amp;hamt_allocator_default);
    /* load table */
    for (size_t i = 0; i < N; i++) {
        hamt_set(t, cities[i].country, cities[i].capital);
    }

    /* query table */
    for (size_t i = 0; i < N; i++) {
        printf(&quot;%s has capital %s\n&quot;, cities[i].country,
                                      hamt_get(t, cities[i].country));
    }
    /* cleanup */
    hamt_delete(t);
    return 0;
}"><pre><span>#include</span> <span>&lt;stdint.h&gt;</span>
<span>#include</span> <span>&lt;stdio.h&gt;</span>
<span>#include</span> <span>&lt;stdlib.h&gt;</span>
<span>#include</span> <span>&lt;string.h&gt;</span>

<span>#include</span> <span>"hamt.h"</span>
<span>#include</span> <span>"murmur3.h"</span>


<span>static</span> <span>uint32_t</span> <span>hash_string</span>(<span>const</span> <span>void</span> <span>*</span><span>key</span>, <span>const</span> <span>size_t</span> <span>gen</span>)
{
    <span>return</span> <span>murmur3_32</span>((<span>uint8_t</span> <span>*</span>)<span>key</span>, <span>strlen</span>((<span>const</span> <span>char</span> <span>*</span>)<span>key</span>), <span>gen</span>);
}

<span>int</span> <span>main</span>(<span>int</span> <span>argn</span>, <span>char</span> <span>*</span><span>argv</span>[])
{
    <span>enum</span> { <span>N</span> <span>=</span> <span>5</span>; };
    <span>struct</span> {
        <span>char</span> <span>*</span><span>country</span>;
        <span>char</span> <span>*</span><span>capital</span>;
    } <span>cities</span>[<span>N</span>] <span>=</span> {
        {<span>"Germany"</span>, <span>"Berlin"</span>},
        {<span>"Spain"</span>, <span>"Madrid"</span>},
        {<span>"Italy"</span>, <span>"Rome"</span>},
        {<span>"France"</span>, <span>"Paris"</span>},
        {<span>"Romania"</span>, <span>"Bucharest"</span>}
        <span>/* ... */</span>
    };

    <span>struct</span> <span>hamt</span> <span>*</span><span>t</span>;

    <span>/* create table */</span>
    <span>t</span> <span>=</span> <span>hamt_create</span>(<span>hash_string</span>, <span>strcmp</span>, <span>&amp;</span><span>hamt_allocator_default</span>);
    <span>/* load table */</span>
    <span>for</span> (<span>size_t</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>N</span>; <span>i</span><span>++</span>) {
        <span>hamt_set</span>(<span>t</span>, <span>cities</span>[<span>i</span>].<span>country</span>, <span>cities</span>[<span>i</span>].<span>capital</span>);
    }

    <span>/* query table */</span>
    <span>for</span> (<span>size_t</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>N</span>; <span>i</span><span>++</span>) {
        <span>printf</span>(<span>"%s has capital %s\n"</span>, <span>cities</span>[<span>i</span>].<span>country</span>,
                                      <span>hamt_get</span>(<span>t</span>, <span>cities</span>[<span>i</span>].<span>country</span>));
    }
    <span>/* cleanup */</span>
    <span>hamt_delete</span>(<span>t</span>);
    <span>return</span> <span>0</span>;
}</pre></div>
<h3 tabindex="-1" dir="auto">Example 2: Garbage-collected persistent HAMTs</h3>
<p dir="auto">The key to making use of structural sharing is to provide <code>libhamt</code> with a
<code>struct hamt_allocator</code> instance that implements garbage collection.</p>
<p dir="auto">The example below uses the the <a href="https://www.hboehm.info/gc/" rel="nofollow">Boehm-Demers-Weiser</a> GC. For
GC installation, compilation and linking instructions, please refer to the GC
documentation.</p>
<p dir="auto">In brief, the Boehm GC provides a <code>gc.h</code> include file and drop-in replacements
for the standard memory management functions, including <code>malloc</code>, <code>realloc</code>
and <code>free</code>.</p>
<p dir="auto">The following snippet illustrates the required changes:</p>
<div dir="auto" data-snippet-clipboard-copy-content="...
#include &quot;gc.h&quot;  /* Boehm-Demers-Weiser GC */

...

inline void nop(void *_) { return; }

int main(int argc, char *argv[]) {
    ...
    /*
    Set up garbage collection. We set the function pointer for `free` to
    NULL to avoid explicit freeing of memory.
    */
    struct hamt_allocator gc_alloc = {GC_malloc, GC_realloc, nop};
    const struct hamt *t = hamt_create(hash_string, strcmp, &amp;gc_alloc);
    ...
}"><pre>...
<span>#include</span> <span>"gc.h"</span>  <span>/* Boehm-Demers-Weiser GC */</span>

...

<span>inline</span> <span>void</span> <span>nop</span>(<span>void</span> <span>*</span><span>_</span>) { <span>return</span>; }

<span>int</span> <span>main</span>(<span>int</span> <span>argc</span>, <span>char</span> <span>*</span><span>argv</span>[]) {
    ...
    <span>/*</span>
<span>    Set up garbage collection. We set the function pointer for `free` to</span>
<span>    NULL to avoid explicit freeing of memory.</span>
<span>    */</span>
    <span>struct</span> <span>hamt_allocator</span> <span>gc_alloc</span> <span>=</span> {<span>GC_malloc</span>, <span>GC_realloc</span>, <span>nop</span>};
    <span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>t</span> <span>=</span> <span>hamt_create</span>(<span>hash_string</span>, <span>strcmp</span>, <span>&amp;</span><span>gc_alloc</span>);
    ...
}</pre></div>
<p dir="auto">We set the <code>gc_alloc.free</code> function pointer to point to <code>nop()</code>, a
no-operation function. This is necessary to ensure that we rely on the garbage
collector. If we were to provide a pointer to <code>GC_free()</code> (i.e. GC's drop-in
replacement for the <code>free()</code> function), we would still implement explicit
deallocation, just with a different free function.</p>
<h3 tabindex="-1" dir="auto">Example 3: Iterators</h3>
<p dir="auto">The following snipped illustrates how to create, test, exhaust and dispose of
an iterator. We first create the iterator using <code>hamt_it_create()</code>, jump into
a <code>while</code> loop and advance the iterator using <code>hamt_it_next()</code> while the
iterator is valid. In every interation we print the current key/value pair to
<code>stdout</code>. Once we exit the loop, we clean up using <code>hamt_it_delete()</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="    ...
    struct hamt *t = hamt_create(hash_string, strcmp, &amp;hamt_allocator_default);

    /* load table */
    ...

    /* create iterator */
    hamt_iterator it = hamt_it_create(t);
    while (hamt_it_valid(it)) {
        printf(&quot;(%s, %s)\n&quot;, (char *)hamt_it_get_key(it),
                             (char *)hamt_it_get_value(it));
        hamt_it_next(it);
    }
    /* clean up */
    hamt_it_delete(it);

    ...
    hamt_delete(t);
    ..."><pre>    ...
    <span>struct</span> <span>hamt</span> <span>*</span><span>t</span> <span>=</span> <span>hamt_create</span>(<span>hash_string</span>, <span>strcmp</span>, <span>&amp;</span><span>hamt_allocator_default</span>);

    <span>/* load table */</span>
    ...

    <span>/* create iterator */</span>
    <span>hamt_iterator</span> <span>it</span> <span>=</span> <span>hamt_it_create</span>(<span>t</span>);
    <span>while</span> (<span>hamt_it_valid</span>(<span>it</span>)) {
        <span>printf</span>(<span>"(%s, %s)\n"</span>, (<span>char</span> <span>*</span>)<span>hamt_it_get_key</span>(<span>it</span>),
                             (<span>char</span> <span>*</span>)<span>hamt_it_get_value</span>(<span>it</span>));
        <span>hamt_it_next</span>(<span>it</span>);
    }
    <span>/* clean up */</span>
    <span>hamt_it_delete</span>(<span>it</span>);

    ...
    <span>hamt_delete</span>(<span>t</span>);
    ...</pre></div>
<p dir="auto">This concludes the description of the <code>libhamt</code> interface and we now move on
to detailed implementation notes.</p>
<h2 tabindex="-1" dir="auto">Implementation</h2>
<h2 tabindex="-1" dir="auto">Prelude: Setup</h2>
<h3 tabindex="-1" dir="auto">Project structure</h3>
<p dir="auto">The <code>hamt</code> source tree has the following structure:</p>
<div data-snippet-clipboard-copy-content="hamt/
  build/         Out-of-source build destination
  include/       Header files that are part of the interface
  src/           Source and header files
  test/          Test and utility headers &amp; sources
  Makefile"><pre><code>hamt/
  build/         Out-of-source build destination
  include/       Header files that are part of the interface
  src/           Source and header files
  test/          Test and utility headers &amp; sources
  Makefile
</code></pre></div>
<p dir="auto">Sources are organized in three folders: the <code>include</code> folder, for all header
files that are part of the public interface; the <code>src</code> folder, for the
actual implementation and private header files; and the <code>test</code> folder, for all
test code, including headers and sources for testing utilities (e.g. data
loading and benchmarking functions).</p>
<p dir="auto">The build process is governed by a single <code>Makefile</code> in the project root
directory.</p>
<h3 tabindex="-1" dir="auto">Programming Style</h3>
<h3 tabindex="-1" dir="auto">Building the project</h3>
<p dir="auto">To build the library and run the tests:</p>

<h2 tabindex="-1" dir="auto">Design</h2>
<h3 tabindex="-1" dir="auto">Introduction</h3>
<p dir="auto"><strong>Hash tables.</strong> A common and practical answer to efficient value retrieval
from a collection given a key is to "use a <em>hash table</em>".  This is good
advice. <em>Hash tables</em> provide insert, modification, and retrieval in amortized
constant average time, using space linear in the number of elements they
store.  They have been the subject of intensive research and
optimization and are part of <a href="https://www.amazon.com/Algorithms-4th-Robert-Sedgewick/dp/032157351X" rel="nofollow">every</a>
<a href="https://www.amazon.com/Introduction-Algorithms-3rd-MIT-Press/dp/0262033844/ref=zg_bs_491298_1/147-2375898-2942653?pd_rd_i=0262033844&amp;psc=1" rel="nofollow">introductory</a> CS textbook.  Chances are that the
standard library of the languange at hand contains a readily available, tried
and tested implementation.</p>
<p dir="auto">For instance, <code>std::unordered_set</code> and <code>std::unordered_map</code> (and their
<code>*_multiset</code> cousins) are hash table implementations for C++ <sup id="user-content-ac_hash_table_cpp"><a href="#fn_hash_table_cpp">1</a></sup>; for C, multiple
<a href="https://en.wikipedia.org/wiki/C_standard_library" rel="nofollow">libc</a> implementations (e.g. <a href="https://en.wikipedia.org/wiki/Glibc" rel="nofollow">glibc</a>, <a href="https://www.musl-libc.org/" rel="nofollow">musl</a>,
<a href="https://en.wikipedia.org/wiki/C_standard_library#BSD_libc" rel="nofollow">BSD libc</a>) provide POSIX-compliant <code>hsearch</code> facilities,
GNOME's <a href="https://en.wikipedia.org/wiki/GLib" rel="nofollow">GLib</a>
and others provide <a href="https://gitlab.gnome.org/GNOME/glib/-/blob/main/glib/ghash.c" rel="nofollow">hash table</a> implementations<sup id="user-content-ac_hash_table_c"><a href="#fn_hash_table_c">2</a></sup>. Python has the <code>dict</code> type
for associative arrays which <a href="https://stackoverflow.com/a/9022835" rel="nofollow">is implemented as a hash
table</a><sup id="user-content-ac_hash_table_python"><a href="#fn_hash_table_python">3</a></sup>.  Java has
<code>Hashtable</code>, <code>HashMap</code>, and <code>HashSet</code> <sup id="user-content-ac_hash_table_java"><a href="#fn_hash_table_java">4</a></sup> and JavaScript has
<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map" rel="nofollow"><code>Map</code></a>.</p>
<p dir="auto">One property of the classical hash table implementations is that they do not
provide support for <em>persistence</em> (in the sense of <a href="https://en.wikipedia.org/wiki/Persistent_data_structure" rel="nofollow">persistent data
structures</a>, not persistent storage). They are a
<a href="https://github.com/matthiasn/talk-transcripts/blob/master/Hickey_Rich/ValueOfValues.md">place-oriented</a> solution to associative storage and
make destructive modifications to the data structure when the data changes
(note that this is independent of any particular conflict resolution and
capacity maintenance strategies).</p>
<p dir="auto">Persistent associative containers require a different approach.</p>
<p dir="auto"><strong>Persistent data structures.</strong> <em>(Full) persistence</em> is the property of a data
structure to always preserve (all) previous versions if itself under
modification. The property is related to
<a href="https://en.wikipedia.org/wiki/Immutable_object" rel="nofollow">immutability</a>: from the perspective of the client,
every update yields a new copy, making instances practically immutable. This
is a huge conceptual change: if data structures are immutable, functions using
these data structures are pure (i.e. side effect-free). That in turn enables
<a href="https://en.wikipedia.org/wiki/Value_semantics" rel="nofollow">value semantics</a>, <a href="https://en.wikipedia.org/wiki/Referential_transparency" rel="nofollow">referential
transparency</a>, and, consequently, substantial
reduction in programming complexity when dealing with paralellism and
synchronization (see e.g. Rich Hickey's presentations on <a href="https://github.com/matthiasn/talk-transcripts/blob/master/Hickey_Rich/ValueOfValues.md"><em>The Value of
Values</em></a> and <a href="https://github.com/matthiasn/talk-transcripts/blob/master/Hickey_Rich/AreWeThereYet.md"><em>Are We There
Yet?</em></a>).</p>
<p dir="auto">The catch is that classical hash tables set a high bar in terms of time and
space performance characteristics, and persistent data structures need to
approximate that bar.</p>
<p dir="auto"><strong>Efficient persistence.</strong> Persistent associative data structures need to
minimize the memory overhead introduced by value
semantics (i.e. returning copies as opposed to modified originals) and, at
the same time, provide practically average constant time insert, retrieve and
delete capabilities to minimize the performance gap to classical hash tables.</p>
<p dir="auto">It turns out that the data structure of choice to tackle these challenges is a
<em>tree</em>. Trees support efficient <a href="https://en.wikipedia.org/wiki/Persistent_data_structure#Trees" rel="nofollow"><em>structural
sharing</em></a> strategies for efficient memory management
and, if they are <em>balanced</em> and have <em>large branching factors</em>, provide
O(log<sub>k</sub> N) average performance guarantees.</p>
<p dir="auto"><em>Persistent hash array-mapped tries</em> are, in essence, a sophisticated,
practical implementation of such a data structure.</p>
<h3 tabindex="-1" dir="auto">Persistent Hash Array-Mapped Tries</h3>
<p dir="auto">One way to understand hash array-mapped tries is to look at them as an
evolution of <em>k</em>-ary trees (Fig. 1) that follows from a series of real-world
tradeoffs.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mkirchner/hamt/blob/main/doc/img/hamt-trees.png"><img src="https://github.com/mkirchner/hamt/raw/main/doc/img/hamt-trees.png" width="600"></a>
</p>
<p dir="auto"><b>Figure 1:</b> *k*-ary tree, hash tree, and
hash array-mapped trie.</p>
<p dir="auto">In classic <em>k</em>-ary trees Ⓐ,  Internal and leaf nodes have
different types: internal nodes point to <em>n</em> internal or leaf nodes and leaf
nodes hold or point to data (i.e. the keys/value pairs). In their basic form,
<em>n</em>-ary trees (just like binary trees) are not balanced and their performance
characteristics can easily degrade from <em>O(log<sub>k</sub> n)</em> to <em>O(n)</em>
for degenerate input sequences.</p>
<p dir="auto">One approach to balanced trees are explicit implementations of
tree rebalancing (as in e.g. <a href="https://en.wikipedia.org/wiki/Red%E2%80%93black_tree" rel="nofollow">Red-black
trees</a>, <a href="https://en.wikipedia.org/wiki/AVL_tree" rel="nofollow">AVL trees</a>, or
<a href="https://en.wikipedia.org/wiki/B-tree" rel="nofollow">B-trees</a>).</p>
<p dir="auto">Another option is to use a <a href="https://en.wikipedia.org/wiki/Hash_tree_(persistent_data_structure)" rel="nofollow"><em>hash tree</em></a> Ⓑ: like the name
implies, it uses the <em>hash</em> of the key, interpreted as a sequence of <em>b</em>-bit
groups, to detetermine the location of the leaf node that stores the key/value
pair. The group size <em>b</em> determines the branching factor 2<sup><i>b</i></sup>,
i.e. for <em>b</em>=5, every node can have 2<sup>5</sup>=32 child nodes.
Instead of implementing explicit tree rebalancing, hash trees rely on the
distributional properties of a (good) hash function to place nodes uniformly.
While this saves some effort for rebalancing, note that hash trees <em>do</em>
require a strategy to deal with <em>hash exhaustion</em>, a topic covered below.</p>
<p dir="auto">The challenge with vanilla hash trees is that they reserve space for <em>k</em>
children in every internal node. If the tree is sparsely populated this will
cause significant memory overhead and impact performance due to cache misses.</p>
<p dir="auto">For that reason, HAMTs implement <em>array mapping</em> Ⓒ: instead of reserving space
for <em>n</em> pointers to children in each internal node, the parent node stores a
bitmap that indicates which children are present and the actual node only
allocates the memory required to refer to its children. This is an important
optimization that makes trees with a high branching factor more memory
efficient and cache-friendly.</p>
<p dir="auto">In order to implement a <em>persistent</em> map or set, every modification operation
must return a modified copy and maintain the source data structure. And
returning actual copies is prohibitively expensive in time and memory.</p>
<p dir="auto">This, finally, is where HAMTs really shine and the true reason why we build
them in the first place.</p>
<p dir="auto">HAMTs are trees and trees are compatible with
<a href="https://en.wikipedia.org/wiki/Persistent_data_structure" rel="nofollow">structural sharing</a> strategies. Common
techniques are copy-on-write, fat nodes, <a href="https://en.wikipedia.org/wiki/Persistent_data_structure#Techniques_for_preserving_previous_versions" rel="nofollow">path
copying</a>, and there are <a href="https://www.cs.cmu.edu/~sleator/papers/another-persistence.pdf" rel="nofollow">complex
combinations of the previous three</a>. Path copying is
simple, efficient and general and therefore the technique of choice for
<code>libhamt</code>: Instead of returning an actual copy of the tree during an insert,
update or delete operations, we follow the search path to the item in
question, maintaining a path copy with all the nodes along the way, make our
modification along this path and return it to the caller.</p>
<p dir="auto">Note that enabling persistence <em>requires</em> the use of a garbage collection
strategy. Under stanard <code>malloc()</code> memory management, there is no way for
the HAMT nodes to know how many descendants of a HAMT refer to them.</p>
<h3 tabindex="-1" dir="auto">Documentation structure and implementation strategy</h3>
<p dir="auto">In the following we will address these concepts in turn: we first define the
foundational data structure used to build a tree and introduce the concept of
an <em>anchor</em>. We then dive into hash functions and the <em>hash state management</em>
required to make hashing work for trees of arbitrary depths and in the
presence of hash collisions. We then turn to <em>table management</em>,
introducing a set of functions used to create, modify, query and dispose of
mapped arrays.  With these pieces in place, we are ready to implement the
insert/update, query, and delete functions for non-persistent HAMTs. And
lastly, we introduce the concept of path copying and close with the
implementation of persistent insert/update and delete functions for HAMTs.</p>
<h3 tabindex="-1" dir="auto">Foundational data structures</h3>

<p dir="auto"><code>libhamt</code> uses different types to implement internal and leaf nodes.</p>
<p dir="auto">Leaf nodes contain two fields, called <code>value</code> and <code>key</code> (the rationale for the
reverse ordering of the two fields will become evident shortly).</p>
<div dir="auto" data-snippet-clipboard-copy-content="struct {
    void *value;
    void *key;
} kv;"><pre><span>struct</span> {
    <span>void</span> <span>*</span><span>value</span>;
    <span>void</span> <span>*</span><span>key</span>;
} <span>kv</span>;</pre></div>
<p dir="auto">Both fields are
defined as <code>void*</code> pointers to support referring to arbitrary data types via
type casting
<sup id="user-content-ac_cpp_virtual_method_table"><a href="#fn_cpp_virtual_method_table">5</a></sup>.</p>
<p dir="auto"><code>libhamt</code>'s internal nodes are where the magic happens, based on Bagwell's <em><a href="https://lampwww.epfl.ch/papers/idealhashtrees.pdf" rel="nofollow">Ideal Hash
Trees</a></em> paper and according to the design principles
outlined above.</p>
<p dir="auto">With a branching factor <em>k</em>, internal nodes have at most <em>k</em> successors but
can be sparsely populated. To allow for a memory-efficient representation,
internal nodes have a pointer <code>ptr</code> that points to a fixed-size, right-sized
<em>array</em> of child nodes (also known as a <em>table</em>) and a <em>k</em>-bit <code>index</code> bitmap field that
keeps track of the size and occupancy of that array.</p>
<p dir="auto"><code>libhamt</code> uses <em>k</em>=32 and because <code>index</code> is a 32-bit bitmap field, the number
of one-bits in <code>index</code> yields the size of the array that <code>ptr</code> points to (also
known as the <em>population count</em> or <code>popcount()</code> of <code>index</code>).</p>
<p dir="auto">This suggests an initial (incomplete) definition along the following lines:</p>
<div dir="auto" data-snippet-clipboard-copy-content="struct {
    struct T *ptr;  /* incomplete */
    uint32_t index;
} table;"><pre><span>struct</span> {
    <span>struct</span> <span>T</span> <span>*</span><span>ptr</span>;  <span>/* incomplete */</span>
    <span>uint32_t</span> <span>index</span>;
} <span>table</span>;</pre></div>
<p dir="auto">The specification of <code>T</code> must provide the ability for that datatype to point to
internal and external nodes alike, using only a single pointer type.
A solution is to wrap the two types into a <code>union</code> (and then to wrap
the <code>union</code> into a <code>typedef</code> for convenience):</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct hamt_node {
    union {
        struct {
            void *value;
            void *key;
        } kv;
        struct {
            struct hamt_node *ptr;
            uint32_t index;
        } table;
    } as;
} hamt_node;"><pre><span>typedef</span> <span>struct</span> <span>hamt_node</span> {
    <span>union</span> {
        <span>struct</span> {
            <span>void</span> <span>*</span><span>value</span>;
            <span>void</span> <span>*</span><span>key</span>;
        } <span>kv</span>;
        <span>struct</span> {
            <span>struct</span> <span>hamt_node</span> <span>*</span><span>ptr</span>;
            <span>uint32_t</span> <span>index</span>;
        } <span>table</span>;
    } <span>as</span>;
} <span>hamt_node</span>;</pre></div>
<p dir="auto">With this structure, given a pointer <code>hamt_node *p</code> to a <code>hamt_node</code>
instance, <code>p-&gt;as.kv</code> addresses the leaf node, and <code>p-&gt;as.table</code> addresses the
internal node and <code>p-&gt;as.kv.value</code>, <code>p-&gt;as.kv.key</code>, <code>p-&gt;as.table.ptr</code>, and
<code>p-&gt;as.table.index</code> provide access to the respective fields.</p>
<p dir="auto">To maintain sanity, we define the following convenience macros:</p>
<div dir="auto" data-snippet-clipboard-copy-content="#define TABLE(node) node->as.table.ptr
#define INDEX(node) node->as.table.index
#define VALUE(node) node->as.kv.value
#define KEY(node)   node->as.kv.key"><pre><span>#define</span> <span>TABLE</span>(<span>node</span>) node-&gt;as.table.ptr
<span>#define</span> <span>INDEX</span>(<span>node</span>) node-&gt;as.table.index
<span>#define</span> <span>VALUE</span>(<span>node</span>) node-&gt;as.kv.value
<span>#define</span> <span>KEY</span>(<span>node</span>)   node-&gt;as.kv.key</pre></div>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mkirchner/hamt/blob/main/doc/img/hamtnode-table.png"><img src="https://github.com/mkirchner/hamt/raw/main/doc/img/hamtnode-table.png" width="450"></a>
</p>
<p dir="auto"><b>Figure 2:</b>
Memory structure of an internal node. If <code>node</code> is a pointer
to an internal node, <code>TABLE(node)</code> (or, equivalently, <code>
node-&gt;as.table.ptr</code>) points to the first field of the successor table.
</p>
<h3 tabindex="-1" dir="auto">The Anchor</h3>
<p dir="auto">The <code>libhamt</code> codebase makes liberal use of the concept of an <em>anchor</em>.  An
<em>anchor</em> is a <code>hamt_node*</code> pointer to an internal node (i.e.
<code>is_value(VALUE(anchor))</code> evaluates to false). An <code>anchor</code> provides access to
all information relevant to manage the table of child nodes: <code>INDEX(anchor)</code>
returns the bitmap that encodes the array mapping, applying a popcount to the
bitmap gives the size of the table and indexing is implemented using partial
popcounts. Table elements can be accessed through
<code>TABLE(anchor)[i]</code>, where <code>i</code> must be in the valid range.</p>
<h3 tabindex="-1" dir="auto">Pointer tagging</h3>
<p dir="auto">The definition of <code>hamt_node</code> enables the construction of trees with a mix of
internal and leaf nodes. What the definition does not provide, is a way to
determine if a concrete <code>hamt_node*</code> pointer points to an internal or a leaf
node. One solution would be to specify an <code>enum</code> that indicates the type
(i.e. <code>NODE_LEAF</code>, etc.) and to add a <code>type</code> field to <code>struct hamt_node</code>.  While
valid, this would also increase the size of the struct by 50% just to maintain
a single bit of information. Luckily, there is a more memory-efficient
solution: pointer tagging.</p>
<p dir="auto">Since pointers need to be word-aligned, that leaves the lower 3 bits of all
pointers on 64-bit architectures always set to zero. It is possible to make
use of these bits under two conditions: (1) we know we are looking at a
pointer (the bottom three bits for the integer 1 are zero, too); and (2) we
carefully mask the bits in question whenever we actually use the pointer
(since it would point to the wrong location otherwise). The first is not a
problem since we own the code; the second requires diligence and some helper
functions:</p>
<div dir="auto" data-snippet-clipboard-copy-content="#define HAMT_TAG_MASK 0x3
#define HAMT_TAG_VALUE 0x1
#define tagged(__p) (hamt_node *)((uintptr_t)__p | HAMT_TAG_VALUE)
#define untagged(__p) (hamt_node *)((uintptr_t)__p &amp; ~HAMT_TAG_MASK)
#define is_value(__p) (((uintptr_t)__p &amp; HAMT_TAG_MASK) == HAMT_TAG_VALUE)"><pre><span>#define</span> <span>HAMT_TAG_MASK</span> 0x3
<span>#define</span> <span>HAMT_TAG_VALUE</span> 0x1
<span>#define</span> <span>tagged</span>(<span>__p</span>) (hamt_node *)((uintptr_t)__p | HAMT_TAG_VALUE)
<span>#define</span> <span>untagged</span>(<span>__p</span>) (hamt_node *)((uintptr_t)__p &amp; ~HAMT_TAG_MASK)
<span>#define</span> <span>is_value</span>(<span>__p</span>) (((uintptr_t)__p &amp; HAMT_TAG_MASK) == HAMT_TAG_VALUE)</pre></div>
<p dir="auto">In order to mark a leaf node as such, we set <code>key</code> as usual and tag the value
pointer before assigning it to <code>value</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="    p->as.kv.key = key_ptr;
    p->as.kv.value = tagged(value_ptr);"><pre>    <span>p</span><span>-&gt;</span><span>as</span>.<span>kv</span>.<span>key</span> <span>=</span> <span>key_ptr</span>;
    <span>p</span><span>-&gt;</span><span>as</span>.<span>kv</span>.<span>value</span> <span>=</span> <span>tagged</span>(<span>value_ptr</span>);</pre></div>
<p dir="auto">Given a pointer to a leaf (e.g. a search result), we untag <code>value</code> before
returning it:</p>
<div dir="auto" data-snippet-clipboard-copy-content="    ...
    if (status == SEARCH_SUCCESS) {
        return untagged(p->as.kv.value);
    }
    ..."><pre>    ...
    <span>if</span> (<span>status</span> <span>==</span> <span>SEARCH_SUCCESS</span>) {
        <span>return</span> <span>untagged</span>(<span>p</span><span>-&gt;</span><span>as</span>.<span>kv</span>.<span>value</span>);
    }
    ...</pre></div>
<p dir="auto">And, in order to determine what we are looking at, we use <code>is_value</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="    if (is_value(p->as.kv.value)) {
        /* this is a leaf */
        ...
    } else {
        /* this is an internal node */
        ...
    }"><pre>    <span>if</span> (<span>is_value</span>(<span>p</span><span>-&gt;</span><span>as</span>.<span>kv</span>.<span>value</span>)) {
        <span>/* this is a leaf */</span>
        ...
    } <span>else</span> {
        <span>/* this is an internal node */</span>
        ...
    }</pre></div>
<p dir="auto">Pointer tagging is the reason why the <code>value</code> and <code>key</code>
fields in the <code>struct kv</code> struct are ordered the way they are.
The <code>union</code> in <code>hamt_node</code> causes the
memory locations of the <code>struct kv</code> and <code>struct table</code> structs to overlap. Since
the <code>table.index</code> field is <em>not</em> a pointer (and the bottom-three-bits-are-zero
guarantee does not apply), its storage location cannot be used for pointer
tagging, leaving the <code>table.ptr</code> to the task. Putting <code>kv.value</code> first,
aligns the value field with <code>table.ptr</code>. The reverse order would work, but the
<code>kv.key</code> pointer is dereferenced much more often in the code and so it is more
convenient to use <code>kv.value</code>.</p>
<h2 tabindex="-1" dir="auto">Array mapping</h2>
<p dir="auto">The principal idea behing array mapping is to project a sparse bitmap index
onto the index of a dense array, where the size of the array corresponds to
the number of non-zero bits in the bitmap index.</p>
<p dir="auto">Given <code>hamt_node *p</code> is a valid pointer to a node, <code>INDEX(p)</code> corresponds to a
sparse bitmap index. The dense array is located at <code>TABLE(p)</code> and its size is
determined by the <a href="https://en.wikipedia.org/wiki/Hamming_weight" rel="nofollow"><em>population count</em></a> of <code>INDEX(p)</code>.</p>
<p dir="auto">The mapping itself is conceptually trivial: to determine the dense index for
every non-zero bit in the bitmap index, count the number of non-zero bits to
the right of it. In other words, the first set bit goes to index 0, the second
to index 1, and so forth.</p>
<p dir="auto">Efficiently implementing population counting (also known as the hamming
weight) of a bitset is <a href="https://en.wikipedia.org/wiki/Hamming_weight" rel="nofollow">not trivial</a>. <code>libhamt</code> falls back on a
GCC/Clang intrinsic:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static inline int get_popcount(uint32_t n) { return __builtin_popcount(n); }"><pre><span>static</span> <span>inline</span> <span>int</span> <span>get_popcount</span>(<span>uint32_t</span> <span>n</span>) { <span>return</span> <span>__builtin_popcount</span>(<span>n</span>); }</pre></div>
<p dir="auto">With <code>get_popcount()</code> available, determining the position (i.e. dense index)
for a sparse index in a bitmap reduces to calculating the population count of
the bitmap masked off above the sparse index:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static inline int get_pos(uint32_t sparse_index, uint32_t bitmap)
{
    return get_popcount(bitmap &amp; ((1 << sparse_index) - 1));
}"><pre><span>static</span> <span>inline</span> <span>int</span> <span>get_pos</span>(<span>uint32_t</span> <span>sparse_index</span>, <span>uint32_t</span> <span>bitmap</span>)
{
    <span>return</span> <span>get_popcount</span>(<span>bitmap</span> <span>&amp;</span> ((<span>1</span> &lt;&lt; <span>sparse_index</span>) <span>-</span> <span>1</span>));
}</pre></div>
<p dir="auto">Lastly, to determine if a node has a child at a particular index <code>index</code>, we
check if the bit at that index is set in the bitmap:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static inline bool has_index(const hamt_node *anchor, size_t index)
{
    return INDEX(anchor) &amp; (1 << index);
}"><pre><span>static</span> <span>inline</span> <span>bool</span> <span>has_index</span>(<span>const</span> <span>hamt_node</span> <span>*</span><span>anchor</span>, <span>size_t</span> <span>index</span>)
{
    <span>return</span> <span>INDEX</span>(<span>anchor</span>) <span>&amp;</span> (<span>1</span> &lt;&lt; <span>index</span>);
}</pre></div>
<h2 tabindex="-1" dir="auto">Hashing</h2>
<p dir="auto">A <a href="https://en.wikipedia.org/wiki/Hash_function" rel="nofollow"><em>hash function</em></a> is a function that takes data of
arbitrary size and maps it to a fixed-size value (often machine word sizes).
<em>Good</em> hash functions are fast to compute and produce <em>uniform</em> output, they
map their inputs as evenly as possible over the output range.  If it is
practically infeasible to invert the mapping (i.e. determine which hash
corresponds to which input value), the hash function is called a <a href="https://en.wikipedia.org/wiki/Cryptographic_hash_function" rel="nofollow">cryptographic
hash function</a>.</p>
<p dir="auto">For the purpose of implementing a HAMT, cryptographical security is not a
design goal. However, the uniformity of the hash function has direct impact on
the balance of the tree: it is the hash that pre-determines all key positions
in the fully populated tree and it is its distribution properties that
determines the number of collisions (and hence depth extensions) we introduce.</p>
<p dir="auto"><code>libhamt</code> does not force clients to use a particular hash function. The
libary exposes a hash function signature of the form</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef uint32_t (*hamt_key_hash_fn)(const void *key, const size_t gen);"><pre><span>typedef</span> <span>uint32_t</span> (<span>*</span><span>hamt_key_hash_fn</span>)(<span>const</span> <span>void</span> <span>*</span><span>key</span>, <span>const</span> <span>size_t</span> <span>gen</span>);</pre></div>
<p dir="auto">and expects users to provide a suitable function pointer as part of the call to
<code>hamt_create()</code> which, among other parameters, takes a hash function:</p>
<div dir="auto" data-snippet-clipboard-copy-content="/* ... see below for a practical definition of my_keyhash_string */

    struct hamt *t = hamt_create(my_keyhash_string, my_keycmp_string,
                                 &amp;hamt_allocator_default);"><pre><span>/* ... see below for a practical definition of my_keyhash_string */</span>

    <span>struct</span> <span>hamt</span> <span>*</span><span>t</span> <span>=</span> <span>hamt_create</span>(<span>my_keyhash_string</span>, <span>my_keycmp_string</span>,
                                 <span>&amp;</span><span>hamt_allocator_default</span>);</pre></div>
<p dir="auto">There are multiple <a href="https://theoryofcomputing.org/articles/v009a030/v009a030.pdf" rel="nofollow">good, practical choices</a>
for the HAMT.  Per default <code>libhamt</code> includes its <a href="https://github.com/mkirchner/hamt/blob/main/src/murmur3.c">own</a>,
<a href="https://github.com/mkirchner/hamt/blob/62a24e5501d72d5fb505d3c642113015f46904d3/test/test_hamt.c#L92">tested</a> implementation of 32-bit
<a href="https://en.wikipedia.org/wiki/MurmurHash" rel="nofollow">MurmurHash3</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="/* from include/murmur3.h */

uint32_t murmur3_32(const uint8_t *key, size_t len, uint32_t seed);"><pre><span>/* from include/murmur3.h */</span>

<span>uint32_t</span> <span>murmur3_32</span>(<span>const</span> <span>uint8_t</span> <span>*</span><span>key</span>, <span>size_t</span> <span>len</span>, <span>uint32_t</span> <span>seed</span>);</pre></div>
<p dir="auto">This declares the <em>murmur</em> hash function. In its standard form <code>murmur3_32</code>
takes a pointer <code>key</code> to byte-sized objects, a count of <code>len</code> that speficies
the number of bytes to hash and a random seed <code>seed</code>.</p>
<p dir="auto">In order to use murmur3 as a <code>hamt</code> hash function, we need to wrap it into a
helper function:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static uint32_t my_keyhash_string(const void *key, const size_t gen)
{
    uint32_t hash = murmur3_32((uint8_t *)key, strlen((const char *)key), gen);
    return hash;
}"><pre><span>static</span> <span>uint32_t</span> <span>my_keyhash_string</span>(<span>const</span> <span>void</span> <span>*</span><span>key</span>, <span>const</span> <span>size_t</span> <span>gen</span>)
{
    <span>uint32_t</span> <span>hash</span> <span>=</span> <span>murmur3_32</span>((<span>uint8_t</span> <span>*</span>)<span>key</span>, <span>strlen</span>((<span>const</span> <span>char</span> <span>*</span>)<span>key</span>), <span>gen</span>);
    <span>return</span> <span>hash</span>;
}</pre></div>
<p dir="auto">Here, the wrapper makes use of <code>strlen(3)</code>, assuming valid C strings as keys.
Note the use of <code>gen</code> as a seed for the hash (see below for the hash exhaustion
discussion).</p>
<p dir="auto">Here is a full example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="#include &quot;murmur3.h&quot;

/* ... */

static uint32_t my_keyhash_string(const void *key, const size_t gen)
{
    uint32_t hash = murmur3_32((uint8_t *)key, strlen((const char *)key), gen);
    return hash;
}

/* ... */

    struct hamt *t = hamt_create(my_keyhash_string, my_keycmp_string,
                                 &amp;hamt_allocator_default);
"><pre><span>#include</span> <span>"murmur3.h"</span>

<span>/* ... */</span>

<span>static</span> <span>uint32_t</span> <span>my_keyhash_string</span>(<span>const</span> <span>void</span> <span>*</span><span>key</span>, <span>const</span> <span>size_t</span> <span>gen</span>)
{
    <span>uint32_t</span> <span>hash</span> <span>=</span> <span>murmur3_32</span>((<span>uint8_t</span> <span>*</span>)<span>key</span>, <span>strlen</span>((<span>const</span> <span>char</span> <span>*</span>)<span>key</span>), <span>gen</span>);
    <span>return</span> <span>hash</span>;
}

<span>/* ... */</span>

    <span>struct</span> <span>hamt</span> <span>*</span><span>t</span> <span>=</span> <span>hamt_create</span>(<span>my_keyhash_string</span>, <span>my_keycmp_string</span>,
                                 <span>&amp;</span><span>hamt_allocator_default</span>);</pre></div>
<h3 tabindex="-1" dir="auto">Hash exhaustion: hash generations and state management</h3>
<p dir="auto">For a hash trie, the number of elements in the trie is limited by the total number
of hashes that fits into a 32-bit <code>uint32_t</code>, i.e. 2^32-1. Since the HAMT only
uses 30 bits (in 6 chunks of 5 bits), the number of unique keys in the trie is
limited to 2<sup>30</sup>-1 = 1,073,741,823 keys.
At the same time, since every layer of the
tree uses 5 bits of the hash, the trie depth is limited to 32/5 = 6 layers.
Neither the hard limit to the number of elements in the trie,
nor the inability to build a trie beyond depth of 6 are desirable properties.</p>
<p dir="auto">To address both issues, <code>libhamt</code> recalculates the hash with a different seed every
6 layers. This requires a bit of state management and motivates the
existence of the <code>hash_state</code> data type and functions that operate on it:</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct hash_state {
    const void *key;
    hamt_key_hash_fn hash_fn;
    uint32_t hash;
    size_t depth;
    size_t shift;
} hash_state;"><pre><span>typedef</span> <span>struct</span> <span>hash_state</span> {
    <span>const</span> <span>void</span> <span>*</span><span>key</span>;
    <span>hamt_key_hash_fn</span> <span>hash_fn</span>;
    <span>uint32_t</span> <span>hash</span>;
    <span>size_t</span> <span>depth</span>;
    <span>size_t</span> <span>shift</span>;
} <span>hash_state</span>;</pre></div>
<p dir="auto">The struct maintains the pointers <code>key</code> to the key that is being hashed and
<code>hash_fn</code> to the hash function used to calculate the current hash <code>hash</code>. At
the same time, it tracks the current depth <code>depth</code> in the tree (this is the
<em>hash generation</em>) and the bitshift <code>shift</code> of the current 5-bit hash chunk.</p>
<p dir="auto">The interface provides two functions: the means to step from the current 5-bit
hash to the next in <code>hash_next()</code>; and the ability query the current index of a
key at the current trie depth in <code>hash_get_index()</code>.</p>
<p dir="auto"><code>hash_next()</code> takes a pointer to a <code>hash_state</code> instance and steps that instance
from the current to the next chunk. Taking a step involves increasing the
<code>depth</code> and <code>shift</code>, and initiating a rehash if the <code>shift</code> indicates
that the hash has been exhausted:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static inline hash_state *hash_next(hash_state *h)
{
    h->depth += 1;
    h->shift += 5;
    if (h->shift > 25) {
        h->hash = h->hash_fn(h->key, h->depth / 5);
        h->shift = 0;
    }
    return h;
}"><pre><span>static</span> <span>inline</span> <span>hash_state</span> <span>*</span><span>hash_next</span>(<span>hash_state</span> <span>*</span><span>h</span>)
{
    <span>h</span><span>-&gt;</span><span>depth</span> <span>+=</span> <span>1</span>;
    <span>h</span><span>-&gt;</span><span>shift</span> <span>+=</span> <span>5</span>;
    <span>if</span> (<span>h</span><span>-&gt;</span><span>shift</span> <span>&gt;</span> <span>25</span>) {
        <span>h</span><span>-&gt;</span><span>hash</span> <span>=</span> <span>h</span><span>-&gt;</span><span>hash_fn</span>(<span>h</span><span>-&gt;</span><span>key</span>, <span>h</span><span>-&gt;</span><span>depth</span> / <span>5</span>);
        <span>h</span><span>-&gt;</span><span>shift</span> <span>=</span> <span>0</span>;
    }
    <span>return</span> <span>h</span>;
}</pre></div>
<p dir="auto">The index of a hash at its current depth corresponds to the decimal
representation of the current chunk. To determine the current chunk,
we right-shift the hash by <code>h-&gt;shift</code> to right-align the desired
LSB and then mask with <code>0x11111</code> which equals <code>0x1f</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static inline uint32_t hash_get_index(const hash_state *h)
{
    return (h->hash >> h->shift) &amp; 0x1f;
}"><pre><span>static</span> <span>inline</span> <span>uint32_t</span> <span>hash_get_index</span>(<span>const</span> <span>hash_state</span> <span>*</span><span>h</span>)
{
    <span>return</span> (<span>h</span><span>-&gt;</span><span>hash</span> &gt;&gt; <span>h</span><span>-&gt;</span><span>shift</span>) <span>&amp;</span> <span>0x1f</span>;
}</pre></div>
<h2 tabindex="-1" dir="auto">Table management</h2>
<p dir="auto">In order to facilitate memory management for tables (aka the internal nodes),
<code>libhamt</code> defines a set of helper functions. Each of these functions takes a
<code>hamt_allocator</code> and calls the user-supplied allocation, re-allocation and
deallocation functions as appropriate.</p>
<p dir="auto">We start by defining a simple memory abstraction (it would also be correct to use real functions
instead of preprocessor macros for this):</p>
<div dir="auto" data-snippet-clipboard-copy-content="#define mem_alloc(ator, size) (ator)->malloc(size)
#define mem_realloc(ator, ptr, size) (ator)->realloc(ptr, size)
#define mem_free(ator, ptr) (ator)->free(ptr)"><pre><span>#define</span> <span>mem_alloc</span>(<span>ator</span>, <span>size</span>) (ator)-&gt;malloc(size)
<span>#define</span> <span>mem_realloc</span>(<span>ator</span>, <span>ptr</span>, <span>size</span>) (ator)-&gt;realloc(ptr, size)
<span>#define</span> <span>mem_free</span>(<span>ator</span>, <span>ptr</span>) (ator)-&gt;free(ptr)</pre></div>
<p dir="auto">This will make it easier to add optimizations (e.g. table caching) in the
future. On top of these macros, table lifecycle management is accomplished
with a few dedicated allocation and de-allocation functions.</p>
<h3 tabindex="-1" dir="auto">Simple allocation and deallocation</h3>
<p dir="auto"><code>table_allocate()</code> allocates tables with size <code>size</code> and returns a pointer to
the newly allocated table.</p>
<div dir="auto" data-snippet-clipboard-copy-content="hamt_node *table_allocate(struct hamt_allocator *ator, size_t size)
{
    return (hamt_node *)mem_alloc(ator, (size * sizeof(hamt_node)));
}"><pre><span>hamt_node</span> <span>*</span><span>table_allocate</span>(<span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>, <span>size_t</span> <span>size</span>)
{
    <span>return</span> (<span>hamt_node</span> <span>*</span>)<span>mem_alloc</span>(<span>ator</span>, (<span>size</span> <span>*</span> <span>sizeof</span>(<span>hamt_node</span>)));
}</pre></div>
<p dir="auto"><code>table_free()</code> deallocates the allocation referenced by <code>ptr</code>. It also
supports taking a <code>size</code> parameter for future extension (e.g. provide a hint
for allocation pool management) that is currently ignored by the underlying
<code>mem_free()</code> implementation.</p>
<div dir="auto" data-snippet-clipboard-copy-content="void table_free(struct hamt_allocator *ator, hamt_node *ptr, size_t size)
{
    mem_free(ator, ptr);
}
"><pre><span>void</span> <span>table_free</span>(<span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>, <span>hamt_node</span> <span>*</span><span>ptr</span>, <span>size_t</span> <span>size</span>)
{
    <span>mem_free</span>(<span>ator</span>, <span>ptr</span>);
}</pre></div>
<h3 tabindex="-1" dir="auto">Specialized table resize operations</h3>
<p dir="auto">While it is possible to implement table re- and right-sizing with the
two functions introduced above, it makes a lot of sense to provide specialized
functionality for the key allocation/de-allocation use cases: extending,
shrinking and gathering a table.</p>
<p dir="auto"><strong>Table extension.</strong> Since the tables in a HAMT are right-sized to minimize
memory overhead, item insertion must necessarily add an additional row to an
existing table. As illustrated in figure 3, the table extension function takes an anchor for an existing
table, allocates a new table with increased size, copies over the exsiting
entries (leaving a gap at the appropriate position for the new row), assigns
the new key and value to the fields in the new row, updates the anchor with
the new memory location of the table and the new index, and eventually frees the
memory of the old table.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mkirchner/hamt/blob/main/doc/img/table-extend.png"><img src="https://github.com/mkirchner/hamt/raw/main/doc/img/table-extend.png" width="450"></a>
</p>
<p dir="auto"><b>Figure 3:</b>
Extending a table creates a new copy of the existing table with an additional
row for the new node.
</p>
<p dir="auto">Looking at the code, this is implemented in verbatim in the <code>table_extend()</code>
function. <code>table_extend()</code> takes an <code>anchor</code> pointer to a table of
size <code>n_rows</code>, then uses the allocator <code>ator</code> to create a new table of size <code>n_rows + 1</code>
with an empty row at position <code>pos</code> and the bitmap index bit <code>index</code> set. It
uses <code>memcpy()</code> to copy memory ranges into the the appropriate positions in
the new allocation, frees the old table and assignes the new table <code>ptr</code> and
<code>index</code> in the anchor:</p>
<div dir="auto" data-snippet-clipboard-copy-content="hamt_node *table_extend(struct hamt_allocator *ator, hamt_node *anchor,
                       size_t n_rows, uint32_t index, uint32_t pos)
{
    hamt_node *new_table = table_allocate(ator, n_rows + 1);
    if (!new_table)
        return NULL;
    if (n_rows > 0) {
        /* copy over table */
        memcpy(&amp;new_table[0], &amp;TABLE(anchor)[0], pos * sizeof(hamt_node));
        /* note: this works since (n_rows - pos) == 0 for cases
         * where we're adding the new k/v pair at the end and memcpy(a, b, 0)
         * is a nop */
        memcpy(&amp;new_table[pos + 1], &amp;TABLE(anchor)[pos],
               (n_rows - pos) * sizeof(hamt_node));
    }
    table_free(ator, TABLE(anchor), n_rows);
    TABLE(anchor) = new_table;
    INDEX(anchor) |= (1 << index);
    return anchor;
}"><pre><span>hamt_node</span> <span>*</span><span>table_extend</span>(<span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>, <span>hamt_node</span> <span>*</span><span>anchor</span>,
                       <span>size_t</span> <span>n_rows</span>, <span>uint32_t</span> <span>index</span>, <span>uint32_t</span> <span>pos</span>)
{
    <span>hamt_node</span> <span>*</span><span>new_table</span> <span>=</span> <span>table_allocate</span>(<span>ator</span>, <span>n_rows</span> <span>+</span> <span>1</span>);
    <span>if</span> (!<span>new_table</span>)
        <span>return</span> <span>NULL</span>;
    <span>if</span> (<span>n_rows</span> <span>&gt;</span> <span>0</span>) {
        <span>/* copy over table */</span>
        <span>memcpy</span>(<span>&amp;</span><span>new_table</span>[<span>0</span>], <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>0</span>], <span>pos</span> <span>*</span> <span>sizeof</span>(<span>hamt_node</span>));
        <span>/* note: this works since (n_rows - pos) == 0 for cases</span>
<span>         * where we're adding the new k/v pair at the end and memcpy(a, b, 0)</span>
<span>         * is a nop */</span>
        <span>memcpy</span>(<span>&amp;</span><span>new_table</span>[<span>pos</span> <span>+</span> <span>1</span>], <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>pos</span>],
               (<span>n_rows</span> <span>-</span> <span>pos</span>) <span>*</span> <span>sizeof</span>(<span>hamt_node</span>));
    }
    <span>table_free</span>(<span>ator</span>, <span>TABLE</span>(<span>anchor</span>), <span>n_rows</span>);
    <span>TABLE</span>(<span>anchor</span>) <span>=</span> <span>new_table</span>;
    <span>INDEX</span>(<span>anchor</span>) |= (<span>1</span> &lt;&lt; <span>index</span>);
    <span>return</span> <span>anchor</span>;
}</pre></div>
<p dir="auto"><strong>Shrinking a table.</strong> Shrinking a table is the inverse operation of table
extension: since we maintain right-sized tables as an invariant, we need to
adjust table sizes the moment the client deletes a key/value pair from the
HAMT.</p>
<p dir="auto">Figure 4 illustrates the concept: given an anchor, the shrinking function
returns a new table with the specified row removed.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mkirchner/hamt/blob/main/doc/img/table-shrink.png"><img src="https://github.com/mkirchner/hamt/raw/main/doc/img/table-shrink.png" width="450"></a>
</p>
<p dir="auto"><b>Figure 4:</b>
Shrinking a table creates a new copy of the table with the specified row
removed.
</p>
<p dir="auto">In the code, this is what <code>table_shrink()</code> does. In the same way as
<code>table_extend()</code> the function takes a pointer <code>ator</code> to the global allocator,
a pointer <code>anchor</code> to the current anchor, the size of the current tables as
<code>n_rows</code>, and the pair of one-hot bitmap index <code>index</code> and storage array
position <code>pos</code>. And, in analogy to table extension, the function allocation a
right-sized table, copies the data to keep using range copies with <code>memcpy()</code>,
frees up the old table and updates the anchor to reflect the changes.</p>
<div dir="auto" data-snippet-clipboard-copy-content="hamt_node *table_shrink(struct hamt_allocator *ator, hamt_node *anchor,
                       size_t n_rows, uint32_t index, uint32_t pos)
{
    hamt_node *new_table = NULL;
    uint32_t new_index = 0;
    if (n_rows > 0) {
        new_table = table_allocate(ator, n_rows - 1);
        if (!new_table)
            return NULL;
        new_index = INDEX(anchor) &amp; ~(1 << index);
        memcpy(&amp;new_table[0], &amp;TABLE(anchor)[0], pos * sizeof(hamt_node));
        memcpy(&amp;new_table[pos], &amp;TABLE(anchor)[pos + 1],
               (n_rows - pos - 1) * sizeof(hamt_node));
    }
    table_free(ator, TABLE(anchor), n_rows);
    INDEX(anchor) = new_index;
    TABLE(anchor) = new_table;
    return anchor;
}"><pre><span>hamt_node</span> <span>*</span><span>table_shrink</span>(<span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>, <span>hamt_node</span> <span>*</span><span>anchor</span>,
                       <span>size_t</span> <span>n_rows</span>, <span>uint32_t</span> <span>index</span>, <span>uint32_t</span> <span>pos</span>)
{
    <span>hamt_node</span> <span>*</span><span>new_table</span> <span>=</span> <span>NULL</span>;
    <span>uint32_t</span> <span>new_index</span> <span>=</span> <span>0</span>;
    <span>if</span> (<span>n_rows</span> <span>&gt;</span> <span>0</span>) {
        <span>new_table</span> <span>=</span> <span>table_allocate</span>(<span>ator</span>, <span>n_rows</span> <span>-</span> <span>1</span>);
        <span>if</span> (!<span>new_table</span>)
            <span>return</span> <span>NULL</span>;
        <span>new_index</span> <span>=</span> <span>INDEX</span>(<span>anchor</span>) <span>&amp;</span> ~(<span>1</span> &lt;&lt; <span>index</span>);
        <span>memcpy</span>(<span>&amp;</span><span>new_table</span>[<span>0</span>], <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>0</span>], <span>pos</span> <span>*</span> <span>sizeof</span>(<span>hamt_node</span>));
        <span>memcpy</span>(<span>&amp;</span><span>new_table</span>[<span>pos</span>], <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>pos</span> <span>+</span> <span>1</span>],
               (<span>n_rows</span> <span>-</span> <span>pos</span> <span>-</span> <span>1</span>) <span>*</span> <span>sizeof</span>(<span>hamt_node</span>));
    }
    <span>table_free</span>(<span>ator</span>, <span>TABLE</span>(<span>anchor</span>), <span>n_rows</span>);
    <span>INDEX</span>(<span>anchor</span>) <span>=</span> <span>new_index</span>;
    <span>TABLE</span>(<span>anchor</span>) <span>=</span> <span>new_table</span>;
    <span>return</span> <span>anchor</span>;
}</pre></div>
<p dir="auto"><strong>Table gathering.</strong> As we are deleting entries from the HAMT, we may end up
with the table structure shown in Figure 5: a table in which one of the
entries is a single-row table. What we want to do in these cases is to replace
the table entry in <code>TABLE(anchor)[1]</code> with the key/value pair from
<code>TABLE(TABLE(anchor)[1])</code> and <em>gather</em> the one-row table into its parent.
While this comes at additional computational cost upon delete, it maintains
the logarithmic depth properties as the HAMT changes its size.</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mkirchner/hamt/blob/main/doc/img/table-gather.png"><img src="https://github.com/mkirchner/hamt/raw/main/doc/img/table-gather.png" width="450"></a>
</p>
<p dir="auto"><b>Figure 5:</b>
Gathering pulls a one-row-sized table into its parent table (essentially
converting an internal node into a leaf node).
</p>
<p dir="auto">The code is straightforward: we take the allocator <code>alloc</code>, the <code>anchor</code>
pointer, and the position <code>pos</code> of the single-row table inside the parent
table, copy over the key and value from the child table to the parent
(maintaining a temporary handle on the child) and then free the child table:</p>
<div dir="auto" data-snippet-clipboard-copy-content="hamt_node *table_gather(struct hamt_allocator *ator, hamt_node *anchor,
                       uint32_t pos)
{
    int n_rows = get_popcount(INDEX(anchor));
    hamt_node *table = TABLE(anchor);
    KEY(anchor) = table[pos].as.kv.key;
    VALUE(anchor) = table[pos].as.kv.value; /* already tagged */
    table_free(ator, table, n_rows);
    return anchor;
}
"><pre><span>hamt_node</span> <span>*</span><span>table_gather</span>(<span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>, <span>hamt_node</span> <span>*</span><span>anchor</span>,
                       <span>uint32_t</span> <span>pos</span>)
{
    <span>int</span> <span>n_rows</span> <span>=</span> <span>get_popcount</span>(<span>INDEX</span>(<span>anchor</span>));
    <span>hamt_node</span> <span>*</span><span>table</span> <span>=</span> <span>TABLE</span>(<span>anchor</span>);
    <span>KEY</span>(<span>anchor</span>) <span>=</span> <span>table</span>[<span>pos</span>].<span>as</span>.<span>kv</span>.<span>key</span>;
    <span>VALUE</span>(<span>anchor</span>) <span>=</span> <span>table</span>[<span>pos</span>].<span>as</span>.<span>kv</span>.<span>value</span>; <span>/* already tagged */</span>
    <span>table_free</span>(<span>ator</span>, <span>table</span>, <span>n_rows</span>);
    <span>return</span> <span>anchor</span>;
}</pre></div>
<p dir="auto"><strong>Table duplication.</strong> Lastly, table duplication. This will be required for path
copying when we implement persistency and it is so straightforward that there
is no diagram: given an anchor, <code>table_dup()</code> determines the size of the table
that the anchor points to, allocates the required memory and performs a range
copy using <code>memcpy()</code> to duplicate the table contents:</p>
<div dir="auto" data-snippet-clipboard-copy-content="hamt_node *table_dup(struct hamt_allocator *ator, hamt_node *anchor)
{
    int n_rows = get_popcount(INDEX(anchor));
    hamt_node *new_table = table_allocate(ator, n_rows);
    if (new_table) {
        memcpy(&amp;new_table[0], &amp;TABLE(anchor)[0], n_rows * sizeof(hamt_node));
    }
    return new_table;
}"><pre><span>hamt_node</span> <span>*</span><span>table_dup</span>(<span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>, <span>hamt_node</span> <span>*</span><span>anchor</span>)
{
    <span>int</span> <span>n_rows</span> <span>=</span> <span>get_popcount</span>(<span>INDEX</span>(<span>anchor</span>));
    <span>hamt_node</span> <span>*</span><span>new_table</span> <span>=</span> <span>table_allocate</span>(<span>ator</span>, <span>n_rows</span>);
    <span>if</span> (<span>new_table</span>) {
        <span>memcpy</span>(<span>&amp;</span><span>new_table</span>[<span>0</span>], <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>0</span>], <span>n_rows</span> <span>*</span> <span>sizeof</span>(<span>hamt_node</span>));
    }
    <span>return</span> <span>new_table</span>;
}</pre></div>
<h2 tabindex="-1" dir="auto">Putting it all together</h2>
<p dir="auto">The following subsections detail the implementations of search, insertion and
removal of key/value pairs in our HAMT implementation. Note that, while the
implementations shown here have been thoroughly tested and are deemed correct,
they may have been replaced by faster or more capable implementations in the
actual <code>libhamt</code> source. An attempt is being made to keep this section up to
date with the actual implementation but the choice here is in favor of
conceptual clarity and will not necessarily cover every implementation detail.
PRs welcome.</p>
<h3 tabindex="-1" dir="auto">Example data</h3>
<table>
<thead>
<tr>
<th>key</th>
<th>key hash</th>
<th>binary key hash</th>
<th>5-bit ints</th>
</tr>
</thead>
<tbody>
<tr>
<td>"0"</td>
<td>d271c07f</td>
<td><code>11 01001 00111 00011 10000 00011 11111</code></td>
<td>[ 31  3 16  3  7 9 ]</td>
</tr>
<tr>
<td>"2"</td>
<td>0129e217</td>
<td><code>00 00000 10010 10011 11000 10000 10111</code></td>
<td>[ 23 16 24 19 18 0 ]</td>
</tr>
<tr>
<td>"4"</td>
<td>e131cc88</td>
<td><code>11 10000 10011 00011 10011 00100 01000</code></td>
<td>[  8  4 19 3 19 16 ]</td>
</tr>
<tr>
<td>"7"</td>
<td>23ea8628</td>
<td><code>00 10001 11110 10101 00001 10001 01000</code></td>
<td>[  8 17 1 21 30 17 ]</td>
</tr>
<tr>
<td>"8"</td>
<td>bd920017</td>
<td><code>10 11110 11001 00100 00000 00000 10111</code></td>
<td>[ 23 0  0  4 25 30 ]</td>
</tr>
</tbody>
</table>
<h3 tabindex="-1" dir="auto">Search: internal API</h3>
<p dir="auto">Search plays a double role: finding a HAMT entry is a fundamental part of the
HAMT interface (exposed by <code>hamt_get()</code>); and the first step in the insert and remove
functions is finding the anchors to operate on.</p>
<p dir="auto">It is therefore desirable to approach the search implementation from a
more generic perspective such that we do not need to re-invent the
wheel for each of these use cases. We therefore define an internal search
function</p>
<div dir="auto" data-snippet-clipboard-copy-content="static ... search_recursive(...);"><pre><span>static</span> ... <span>search_recursive</span>(...);</pre></div>
<p dir="auto">that is called from internal and the API functions alike. As the
name implies, we implement search in a recursive manner (this is for clarity;
conversion to an iterative solution is straightforward).</p>
<p dir="auto">When we search for a key in the HAMT, there are two fundamental outcomes: the
key is either there, or it is not (note that these are exactly the semantics
of the user-facing <code>hamt_get()</code> function: it either returns a pointer to the
value stored under the key or it returns <code>NULL</code>). However, looking more
closely, searches can fail for two reasons: the search can be unsuccessful
because a key does not exist in the HAMT <em>or</em> it can be unsuccessful because
there is a key value pair that happens to have the same partial hash but a
different key (i.e. there is a hash collission or the hash has not been
sufficiently exhausted to differentiate between the two keys).  And each of
these three cases is meaningful (the latter two corresponding directly to the
two different insertion strategies described below).</p>
<p dir="auto">A good approach here is to define a ternary return value (as opposed to
the usual, binary use-<code>NULL</code>-as-a-failure-indicator approach that is often
prevalent in C code) to allow us to signal each of these cases clearly.</p>
<p dir="auto">We create a suitable three-value <code>enum</code> called <code>search_status</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef enum {
    SEARCH_SUCCESS,
    SEARCH_FAIL_NOTFOUND,
    SEARCH_FAIL_KEYMISMATCH
} search_status;"><pre><span>typedef</span> <span>enum</span> {
    <span>SEARCH_SUCCESS</span>,
    <span>SEARCH_FAIL_NOTFOUND</span>,
    <span>SEARCH_FAIL_KEYMISMATCH</span>
} <span>search_status</span>;</pre></div>
<p dir="auto">where <code>SEARCH_SUCCESS</code> indicates that the key in question was
found, <code>SEARCH_FAIL_NOTFOUND</code> indicates a search failure due to a missing key,
and <code>SEARCH_FAIL_KEYMISMATCH</code> signals a hash conflict.</p>
<p dir="auto">In order to return the result of a search (and not only its status), we
introduce a search result data type that is a bit more heavy-weight:</p>
<div dir="auto" data-snippet-clipboard-copy-content="struct search_result {
    search_status status;
    hamt_node *anchor;
    hamt_node *value;
    hash_state *hash;
};"><pre><span>struct</span> <span>search_result</span> {
    <span>search_status</span> <span>status</span>;
    <span>hamt_node</span> <span>*</span><span>anchor</span>;
    <span>hamt_node</span> <span>*</span><span>value</span>;
    <span>hash_state</span> <span>*</span><span>hash</span>;
};</pre></div>
<p dir="auto">Here, <code>anchor</code> always points to the anchor at which the search was terminated;
if the search was successful, <code>value</code> points to the table row that holds the
key/value pair with matching key; if it was unsuccessful with a key mismatch,
<code>value</code> points to the mismatching key/value pair; and if it was unsuccessful
because the key did not exist, <code>value</code> equals <code>NULL</code>. Depending on the depth
that the search reached, we may have hit hash exhaustion and the hash may have
been recalculated, so we are returning this here, too.</p>
<p dir="auto">Given <code>struct search_result</code>, the return value of <code>search_recursive()</code>
becomes:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static struct search_result search_recursive(...)
{
    // ...
}"><pre><span>static</span> <span>struct</span> <span>search_result</span> <span>search_recursive</span>(...)
{
    <span>// ...</span>
}</pre></div>
<p dir="auto">With these prerequisites out of the way, we can tackle the actual search
algorithm:</p>
<div data-snippet-clipboard-copy-content="    search_recursive(anchor, hash, eq, key, ...):
        if the current 5-bit sub-hash is a valid index in the current table: 
            if the index refers to a key/value pair:
                if the key matches the search key:
                    return SEARCH_SUCCESS
                else:
                    return SEARCH_FAIL_KEYMISMATCH
            else (i.e. it refers to a sub-table):
                search_recursive(sub-table, hash_next(hash), eq, key)
        else:
            return SEARCH_FAIL_NOTFOUND"><pre><code>    search_recursive(anchor, hash, eq, key, ...):
        if the current 5-bit sub-hash is a valid index in the current table: 
            if the index refers to a key/value pair:
                if the key matches the search key:
                    return SEARCH_SUCCESS
                else:
                    return SEARCH_FAIL_KEYMISMATCH
            else (i.e. it refers to a sub-table):
                search_recursive(sub-table, hash_next(hash), eq, key)
        else:
            return SEARCH_FAIL_NOTFOUND
</code></pre></div>
<p dir="auto">The basic idea is to start from the root of the HAMT and then, at every level,
test if the curret sub-hash of the key is present in the current sub-trie. If
not, bail and report failure immediately. If yes, check if the entry refers to
a key/value pair or to another table. If this is true as well, check if the
keys match and return success or failure accordingly. If the entry refers to
a sub-table, repeat the search at the level of the sub-table.</p>
<p dir="auto">With the conceptual approach lined out, let's get into the implementation
details.
We start with deriving the table index for the current search level from the
hash. This is accomplished using
<code>hash_get_index()</code>, which encapsulates the bit-fiddling required to extract
the correct 5-bit hash for the current search level and returns the index as
an unsigned integer.</p>
<div dir="auto" data-snippet-clipboard-copy-content="static search_result search_recursive(hamt_node *anchor,
                                      hash_state *hash,
                                      hamt_cmp_fn cmp_eq,
                                      const void *key, ...)
{
    uint32_t expected_index = hash_get_index(hash);
    ...
}"><pre><span>static</span> <span>search_result</span> <span>search_recursive</span>(<span>hamt_node</span> <span>*</span><span>anchor</span>,
                                      <span>hash_state</span> <span>*</span><span>hash</span>,
                                      <span>hamt_cmp_fn</span> <span>cmp_eq</span>,
                                      <span>const</span> <span>void</span> <span>*</span><span>key</span>, ...)
{
    <span>uint32_t</span> <span>expected_index</span> <span>=</span> <span>hash_get_index</span>(<span>hash</span>);
    ...
}</pre></div>
<p dir="auto">The code then checks if the <code>expected_index</code> exists in the current table:</p>
<div dir="auto" data-snippet-clipboard-copy-content="    ...
    if (has_index(anchor, expected_index)) {
    ...
    }"><pre>    ...
    <span>if</span> (<span>has_index</span>(<span>anchor</span>, <span>expected_index</span>)) {
    ...
    }</pre></div>
<p dir="auto">Here, <code>has_index()</code> is a simple helper function that checks if
the <code>INDEX(anchor)</code> bitfield has the bit set at <code>expected_index</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static inline bool has_expected_index(const hamt_node *anchor, size_t expected_index)
{
    return INDEX(anchor) &amp; (1 << expected_index);
}"><pre><span>static</span> <span>inline</span> <span>bool</span> <span>has_expected_index</span>(<span>const</span> <span>hamt_node</span> <span>*</span><span>anchor</span>, <span>size_t</span> <span>expected_index</span>)
{
    <span>return</span> <span>INDEX</span>(<span>anchor</span>) <span>&amp;</span> (<span>1</span> &lt;&lt; <span>expected_index</span>);
}</pre></div>
<p dir="auto">If <code>has_index()</code> evaluates to false, the key does not exist in the HAMT and we
can immediately fail the search and return the result:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    uint32_t expected_index = hash_get_index(hash);
    if (has_index(anchor, expected_index)) {
        ...
        ... 
        ...
    }
    search_result result = {.status = SEARCH_FAIL_NOTFOUND,
                            .anchor = anchor,
                            .value = NULL,
                            .hash = hash};
    return result;
}"><pre>{
    <span>uint32_t</span> <span>expected_index</span> <span>=</span> <span>hash_get_index</span>(<span>hash</span>);
    <span>if</span> (<span>has_index</span>(<span>anchor</span>, <span>expected_index</span>)) {
        ...
        ... 
        ...
    }
    <span>search_result</span> <span>result</span> <span>=</span> {.<span>status</span> <span>=</span> <span>SEARCH_FAIL_NOTFOUND</span>,
                            .<span>anchor</span> <span>=</span> <span>anchor</span>,
                            .<span>value</span> <span>=</span> <span>NULL</span>,
                            .<span>hash</span> <span>=</span> <span>hash</span>};
    <span>return</span> <span>result</span>;
}</pre></div>
<p dir="auto">If <code>has_index()</code> evaluates to true, we find the array index using
<code>get_pos()</code> (see above), store it into <code>pos</code> and then acquire a pointer to the
<code>next</code> node by addressing <code>pos</code> indices into the <code>anchor</code>'s table.</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    ...
    if (has_index(anchor, expected_index)) {
        /* If yes, get the compact index to address the array */
        int pos = get_pos(expected_index, INDEX(anchor));
        /* Index into the table */
        hamt_node *next = &amp;TABLE(anchor)[pos];
        ...
    }
    ...
}"><pre>{
    ...
    <span>if</span> (<span>has_index</span>(<span>anchor</span>, <span>expected_index</span>)) {
        <span>/* If yes, get the compact index to address the array */</span>
        <span>int</span> <span>pos</span> <span>=</span> <span>get_pos</span>(<span>expected_index</span>, <span>INDEX</span>(<span>anchor</span>));
        <span>/* Index into the table */</span>
        <span>hamt_node</span> <span>*</span><span>next</span> <span>=</span> <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>pos</span>];
        ...
    }
    ...
}</pre></div>
<p dir="auto">If the <code>next</code> node is not a value, we advance the hash state and recurse the
search. If it is, we compare the keys and return success or failure
accordingly:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
        ...
        /* Index into the table */
        hamt_node *next = &amp;TABLE(anchor)[pos];
        /* Are we looking at a value or another level of tables? */
        if (is_value(VALUE(next))) {
            if ((*cmp_eq)(key, KEY(next)) == 0) {
                /* Found: keys match */
                search_result result = {.status = SEARCH_SUCCESS,
                                        .anchor = anchor,
                                        .value = next,
                                        .hash = hash};
                return result;
            }
            /* Not found: same hash but different key */
            search_result result = {.status = SEARCH_FAIL_KEYMISMATCH,
                                    .anchor = anchor,
                                    .value = next,
                                    .hash = hash};
            return result;
        } else {
            /* For table entries, recurse to the next level */
            return search_recursive(next, hash_next(hash), cmp_eq, key);
        }"><pre>{
        ...
        <span>/* Index into the table */</span>
        <span>hamt_node</span> <span>*</span><span>next</span> <span>=</span> <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>pos</span>];
        <span>/* Are we looking at a value or another level of tables? */</span>
        <span>if</span> (<span>is_value</span>(<span>VALUE</span>(<span>next</span>))) {
            <span>if</span> ((<span>*</span><span>cmp_eq</span>)(<span>key</span>, <span>KEY</span>(<span>next</span>)) <span>==</span> <span>0</span>) {
                <span>/* Found: keys match */</span>
                <span>search_result</span> <span>result</span> <span>=</span> {.<span>status</span> <span>=</span> <span>SEARCH_SUCCESS</span>,
                                        .<span>anchor</span> <span>=</span> <span>anchor</span>,
                                        .<span>value</span> <span>=</span> <span>next</span>,
                                        .<span>hash</span> <span>=</span> <span>hash</span>};
                <span>return</span> <span>result</span>;
            }
            <span>/* Not found: same hash but different key */</span>
            <span>search_result</span> <span>result</span> <span>=</span> {.<span>status</span> <span>=</span> <span>SEARCH_FAIL_KEYMISMATCH</span>,
                                    .<span>anchor</span> <span>=</span> <span>anchor</span>,
                                    .<span>value</span> <span>=</span> <span>next</span>,
                                    .<span>hash</span> <span>=</span> <span>hash</span>};
            <span>return</span> <span>result</span>;
        } <span>else</span> {
            <span>/* For table entries, recurse to the next level */</span>
            <span>return</span> <span>search_recursive</span>(<span>next</span>, <span>hash_next</span>(<span>hash</span>), <span>cmp_eq</span>, <span>key</span>);
        }</pre></div>
<p dir="auto">That concludes the implementation of the recursive search function and the
complete implementation looks like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static search_result search_recursive(hamt_node *anchor, hash_state *hash,
                                      hamt_cmp_fn cmp_eq, const void *key)
{
    /* Determine the expected index in table */
    uint32_t expected_index = hash_get_index(hash);
    /* Check if the expected index is set */
    if (has_index(anchor, expected_index)) {
        /* If yes, get the compact index to address the array */
        int pos = get_pos(expected_index, INDEX(anchor));
        /* Index into the table */
        hamt_node *next = &amp;TABLE(anchor)[pos];
        /* Are we looking at a value or another level of tables? */
        if (is_value(VALUE(next))) {
            if ((*cmp_eq)(key, KEY(next)) == 0) {
                /* Found: keys match */
                search_result result = {.status = SEARCH_SUCCESS,
                                        .anchor = anchor,
                                        .value = next,
                                        .hash = hash};
                return result;
            }
            /* Not found: same hash but different key */
            search_result result = {.status = SEARCH_FAIL_KEYMISMATCH,
                                    .anchor = anchor,
                                    .value = next,
                                    .hash = hash};
            return result;
        } else {
            /* For table entries, recurse to the next level */
            return search_recursive(next, hash_next(hash), cmp_eq, key);
        }
    }
    /* Not found: expected index is not set, key does not exist */
    search_result result = {.status = SEARCH_FAIL_NOTFOUND,
                            .anchor = anchor,
                            .value = NULL,
                            .hash = hash};
    return result;
}"><pre><span>static</span> <span>search_result</span> <span>search_recursive</span>(<span>hamt_node</span> <span>*</span><span>anchor</span>, <span>hash_state</span> <span>*</span><span>hash</span>,
                                      <span>hamt_cmp_fn</span> <span>cmp_eq</span>, <span>const</span> <span>void</span> <span>*</span><span>key</span>)
{
    <span>/* Determine the expected index in table */</span>
    <span>uint32_t</span> <span>expected_index</span> <span>=</span> <span>hash_get_index</span>(<span>hash</span>);
    <span>/* Check if the expected index is set */</span>
    <span>if</span> (<span>has_index</span>(<span>anchor</span>, <span>expected_index</span>)) {
        <span>/* If yes, get the compact index to address the array */</span>
        <span>int</span> <span>pos</span> <span>=</span> <span>get_pos</span>(<span>expected_index</span>, <span>INDEX</span>(<span>anchor</span>));
        <span>/* Index into the table */</span>
        <span>hamt_node</span> <span>*</span><span>next</span> <span>=</span> <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>pos</span>];
        <span>/* Are we looking at a value or another level of tables? */</span>
        <span>if</span> (<span>is_value</span>(<span>VALUE</span>(<span>next</span>))) {
            <span>if</span> ((<span>*</span><span>cmp_eq</span>)(<span>key</span>, <span>KEY</span>(<span>next</span>)) <span>==</span> <span>0</span>) {
                <span>/* Found: keys match */</span>
                <span>search_result</span> <span>result</span> <span>=</span> {.<span>status</span> <span>=</span> <span>SEARCH_SUCCESS</span>,
                                        .<span>anchor</span> <span>=</span> <span>anchor</span>,
                                        .<span>value</span> <span>=</span> <span>next</span>,
                                        .<span>hash</span> <span>=</span> <span>hash</span>};
                <span>return</span> <span>result</span>;
            }
            <span>/* Not found: same hash but different key */</span>
            <span>search_result</span> <span>result</span> <span>=</span> {.<span>status</span> <span>=</span> <span>SEARCH_FAIL_KEYMISMATCH</span>,
                                    .<span>anchor</span> <span>=</span> <span>anchor</span>,
                                    .<span>value</span> <span>=</span> <span>next</span>,
                                    .<span>hash</span> <span>=</span> <span>hash</span>};
            <span>return</span> <span>result</span>;
        } <span>else</span> {
            <span>/* For table entries, recurse to the next level */</span>
            <span>return</span> <span>search_recursive</span>(<span>next</span>, <span>hash_next</span>(<span>hash</span>), <span>cmp_eq</span>, <span>key</span>);
        }
    }
    <span>/* Not found: expected index is not set, key does not exist */</span>
    <span>search_result</span> <span>result</span> <span>=</span> {.<span>status</span> <span>=</span> <span>SEARCH_FAIL_NOTFOUND</span>,
                            .<span>anchor</span> <span>=</span> <span>anchor</span>,
                            .<span>value</span> <span>=</span> <span>NULL</span>,
                            .<span>hash</span> <span>=</span> <span>hash</span>};
    <span>return</span> <span>result</span>;
}</pre></div>
<h3 tabindex="-1" dir="auto">Search: external API</h3>
<p dir="auto">The external API for search is <code>hamt_get(trie, key)</code> which takes a <code>trie</code>
and attempts to find (and return) a key/value pair specified by <code>key</code>. Its
implementation uses <code>search_recursive()</code> from above:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const void *hamt_get(const struct hamt *trie, void *key)
{
    hash_state *hash = &amp;(hash_state){.key = key,
                                     .hash_fn = trie->key_hash,
                                     .hash = trie->key_hash(key, 0),
                                     .depth = 0,
                                     .shift = 0};
    search_result sr = search_recursive(trie->root, hash, trie->key_cmp, key,
                                        NULL, trie->ator);
    if (sr.status == SEARCH_SUCCESS) {
        return untagged(sr.VALUE(value));
    }
    return NULL;
}"><pre><span>const</span> <span>void</span> <span>*</span><span>hamt_get</span>(<span>const</span> <span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>, <span>void</span> <span>*</span><span>key</span>)
{
    <span>hash_state</span> <span>*</span><span>hash</span> <span>=</span> <span>&amp;</span>(<span>hash_state</span>){.<span>key</span> <span>=</span> <span>key</span>,
                                     .<span>hash_fn</span> <span>=</span> <span>trie</span><span>-&gt;</span><span>key_hash</span>,
                                     .<span>hash</span> <span>=</span> <span>trie</span><span>-&gt;</span><span>key_hash</span>(<span>key</span>, <span>0</span>),
                                     .<span>depth</span> <span>=</span> <span>0</span>,
                                     .<span>shift</span> <span>=</span> <span>0</span>};
    <span>search_result</span> <span>sr</span> <span>=</span> <span>search_recursive</span>(<span>trie</span><span>-&gt;</span><span>root</span>, <span>hash</span>, <span>trie</span><span>-&gt;</span><span>key_cmp</span>, <span>key</span>,
                                        <span>NULL</span>, <span>trie</span><span>-&gt;</span><span>ator</span>);
    <span>if</span> (<span>sr</span>.<span>status</span> <span>==</span> <span>SEARCH_SUCCESS</span>) {
        <span>return</span> <span>untagged</span>(<span>sr</span>.<span>VALUE</span>(<span>value</span>));
    }
    <span>return</span> <span>NULL</span>;
}</pre></div>
<p dir="auto">In order to use <code>search_recursive()</code>, it is necessary to set up the hash state
management, initializing it with the <code>key</code>, the hashed <code>key</code>, and starting
search from level <code>0</code> (corresponding to a shift of <code>0</code>). If the search is
not successful, the function returns <code>NULL</code>, if it is successful, it passes
a <code>void</code> pointer to the value that corresponds to <code>key</code>. Note the <em>untagging</em>
of the <code>value</code> field since we're using it as a <em>tagged pointer</em> to indicate
field types.</p>
<h3 tabindex="-1" dir="auto">Insert: internal functions</h3>
<p dir="auto"><code>libhamt</code> does not support an explicit insertion function; all insertions into
the HAMT are <em>upserts</em>, i.e. after calling <code>hamt_set()</code> the API guarantees
that the requested key/value pair exists, irrespective of potential previous
entries that may have had the same key but a different value.</p>
<p dir="auto">The internal function that implements this behavior is <code>set()</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static const hamt_node *set(struct hamt *h, hamt_node *anchor, hamt_key_hash_fn hash_fn,
                            hamt_cmp_fn cmp_fn, void *key, void *value)"><pre><span>static</span> <span>const</span> <span>hamt_node</span> <span>*</span><span>set</span>(<span>struct</span> <span>hamt</span> <span>*</span><span>h</span>, <span>hamt_node</span> <span>*</span><span>anchor</span>, <span>hamt_key_hash_fn</span> <span>hash_fn</span>,
                            <span>hamt_cmp_fn</span> <span>cmp_fn</span>, <span>void</span> <span>*</span><span>key</span>, <span>void</span> <span>*</span><span>value</span>)</pre></div>
<p dir="auto"><code>set()</code> takes a HAMT, an anchor in that HAMT, hashing and comparison
functions as well as a key/value pair. After initializing the hash state, the
function makes use of <code>search_recursive</code> to find the specified <code>key</code>. It deals
with three different search outcomes: (1) if the search is successful, the
value of <code>key</code> gets replaced with the new <code>value</code>; (2) if the search is
unsuccessful because the key does not exist, it attempts to insert a new
key/value pair at the appropriate position; and (3) if the search fails due to
a key mismatch (i.e. there is an entry at the expected hash position but its
key does not equal <code>key</code>), it extends the hash trie until the new key/value
pair can be placed correctly. Cases (2) and (3) are covered by the
<code>insert_kv()</code> and <code>insert_table()</code> helper functions, respectively.</p>
<div dir="auto" data-snippet-clipboard-copy-content="static const hamt_node *set(struct hamt *h, hamt_node *anchor, hamt_key_hash_fn hash_fn,
                            hamt_cmp_fn cmp_fn, void *key, void *value)
{
    hash_state *hash = &amp;(hash_state){.key = key,
                                     .hash_fn = hash_fn,
                                     .hash = hash_fn(key, 0),
                                     .depth = 0,
                                     .shift = 0};
    search_result sr =
        search_recursive(anchor, hash, cmp_fn, key, NULL, h->ator);
    const hamt_node *inserted;
    switch (sr.status) {
    case SEARCH_SUCCESS:
        sr.VALUE(value) = tagged(value);
        inserted = sr.value;
        break;
    case SEARCH_FAIL_NOTFOUND:
        if ((inserted = insert_kv(sr.anchor, sr.hash, key, value, h->ator)) !=
            NULL) {
            h->size += 1;
        }
        break;
    case SEARCH_FAIL_KEYMISMATCH:
        if ((inserted = insert_table(sr.value, sr.hash, key, value, h->ator)) !=
            NULL) {
            h->size += 1;
        }
        break;
    }
    return inserted;
}"><pre><span>static</span> <span>const</span> <span>hamt_node</span> <span>*</span><span>set</span>(<span>struct</span> <span>hamt</span> <span>*</span><span>h</span>, <span>hamt_node</span> <span>*</span><span>anchor</span>, <span>hamt_key_hash_fn</span> <span>hash_fn</span>,
                            <span>hamt_cmp_fn</span> <span>cmp_fn</span>, <span>void</span> <span>*</span><span>key</span>, <span>void</span> <span>*</span><span>value</span>)
{
    <span>hash_state</span> <span>*</span><span>hash</span> <span>=</span> <span>&amp;</span>(<span>hash_state</span>){.<span>key</span> <span>=</span> <span>key</span>,
                                     .<span>hash_fn</span> <span>=</span> <span>hash_fn</span>,
                                     .<span>hash</span> <span>=</span> <span>hash_fn</span>(<span>key</span>, <span>0</span>),
                                     .<span>depth</span> <span>=</span> <span>0</span>,
                                     .<span>shift</span> <span>=</span> <span>0</span>};
    <span>search_result</span> <span>sr</span> <span>=</span>
        <span>search_recursive</span>(<span>anchor</span>, <span>hash</span>, <span>cmp_fn</span>, <span>key</span>, <span>NULL</span>, <span>h</span><span>-&gt;</span><span>ator</span>);
    <span>const</span> <span>hamt_node</span> <span>*</span><span>inserted</span>;
    <span>switch</span> (<span>sr</span>.<span>status</span>) {
    <span>case</span> <span>SEARCH_SUCCESS</span>:
        <span>sr</span>.<span>VALUE</span>(<span>value</span>) <span>=</span> <span>tagged</span>(<span>value</span>);
        <span>inserted</span> <span>=</span> <span>sr</span>.<span>value</span>;
        <span>break</span>;
    <span>case</span> <span>SEARCH_FAIL_NOTFOUND</span>:
        <span>if</span> ((<span>inserted</span> <span>=</span> <span>insert_kv</span>(<span>sr</span>.<span>anchor</span>, <span>sr</span>.<span>hash</span>, <span>key</span>, <span>value</span>, <span>h</span><span>-&gt;</span><span>ator</span>)) <span>!=</span>
            <span>NULL</span>) {
            <span>h</span><span>-&gt;</span><span>size</span> <span>+=</span> <span>1</span>;
        }
        <span>break</span>;
    <span>case</span> <span>SEARCH_FAIL_KEYMISMATCH</span>:
        <span>if</span> ((<span>inserted</span> <span>=</span> <span>insert_table</span>(<span>sr</span>.<span>value</span>, <span>sr</span>.<span>hash</span>, <span>key</span>, <span>value</span>, <span>h</span><span>-&gt;</span><span>ator</span>)) <span>!=</span>
            <span>NULL</span>) {
            <span>h</span><span>-&gt;</span><span>size</span> <span>+=</span> <span>1</span>;
        }
        <span>break</span>;
    }
    <span>return</span> <span>inserted</span>;
}</pre></div>
<p dir="auto">If the call to <code>search_recursive()</code> fails with <code>SEARCH_FAIL_NOTFOUND</code>, we know
that there is a free row in the table of <code>sr.anchor</code>. To insert the new
<code>key</code>/<code>value</code> pair, we calculate the position of the <code>key</code> in the current
table: it extracts the 0-31 index position for the current key and stores it
into <code>ix</code>, extends the existing <code>INDEX(anchor)</code> index bitmap to include the
new key by setting the <code>ix</code>-th bit, and then calculates the dense index
position of the new entry via <code>get_pos()</code>. It then uses <code>table_extend()</code> to
extend the table to the correct size and populates the <code>key</code> and <code>value</code>
entries to reflect the new key/value pair. Note the pointer tagging on the
value field to mark it as a key/value row in the table (as opposed to a row
that points to a sub-table).</p>
<div dir="auto" data-snippet-clipboard-copy-content="static const hamt_node *insert_kv(hamt_node *anchor, hash_state *hash,
                                  void *key, void *value,
                                  struct hamt_allocator *ator)
{
    /* calculate position in new table */
    uint32_t ix = hash_get_index(hash);
    uint32_t new_index = INDEX(anchor) | (1 << ix);
    int pos = get_pos(ix, new_index);
    /* extend table */
    size_t n_rows = get_popcount(INDEX(anchor));
    anchor = table_extend(ator, anchor, n_rows, ix, pos);
    if (!anchor)
        return NULL;
    hamt_node *new_table = TABLE(anchor);
    /* set new k/v pair */
    new_table[pos].as.kv.key = key;
    new_table[pos].as.kv.value = tagged(value);
    /* return a pointer to the inserted k/v pair */
    return &amp;new_table[pos];
}"><pre><span>static</span> <span>const</span> <span>hamt_node</span> <span>*</span><span>insert_kv</span>(<span>hamt_node</span> <span>*</span><span>anchor</span>, <span>hash_state</span> <span>*</span><span>hash</span>,
                                  <span>void</span> <span>*</span><span>key</span>, <span>void</span> <span>*</span><span>value</span>,
                                  <span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>)
{
    <span>/* calculate position in new table */</span>
    <span>uint32_t</span> <span>ix</span> <span>=</span> <span>hash_get_index</span>(<span>hash</span>);
    <span>uint32_t</span> <span>new_index</span> <span>=</span> <span>INDEX</span>(<span>anchor</span>) | (<span>1</span> &lt;&lt; <span>ix</span>);
    <span>int</span> <span>pos</span> <span>=</span> <span>get_pos</span>(<span>ix</span>, <span>new_index</span>);
    <span>/* extend table */</span>
    <span>size_t</span> <span>n_rows</span> <span>=</span> <span>get_popcount</span>(<span>INDEX</span>(<span>anchor</span>));
    <span>anchor</span> <span>=</span> <span>table_extend</span>(<span>ator</span>, <span>anchor</span>, <span>n_rows</span>, <span>ix</span>, <span>pos</span>);
    <span>if</span> (!<span>anchor</span>)
        <span>return</span> <span>NULL</span>;
    <span>hamt_node</span> <span>*</span><span>new_table</span> <span>=</span> <span>TABLE</span>(<span>anchor</span>);
    <span>/* set new k/v pair */</span>
    <span>new_table</span>[<span>pos</span>].<span>as</span>.<span>kv</span>.<span>key</span> <span>=</span> <span>key</span>;
    <span>new_table</span>[<span>pos</span>].<span>as</span>.<span>kv</span>.<span>value</span> <span>=</span> <span>tagged</span>(<span>value</span>);
    <span>/* return a pointer to the inserted k/v pair */</span>
    <span>return</span> <span>&amp;</span><span>new_table</span>[<span>pos</span>];
}</pre></div>
<p dir="auto">When the call to <code>search_recursive()</code> in <code>set()</code> fails with
<code>SEARCH_FAIL_KEYMISMATCH</code>, the situation is different: there is another entry
(either a key/value pair or a reference to a sub-table) in the HAMT that
currently occupies a transitionary trie location for <code>key</code>. This is expected
to happen regularly: keys are always inserted with the shortest possible trie
path that resolves hashing conflicts between <em>existing</em> keys. As more and more
entries are added to the HAMT, these paths necessarily must increase in
length. This situation is handled by <code>insert_table()</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="static const hamt_node *insert_table(hamt_node *anchor, hash_state *hash,
                                     void *key, void *value,
                                     struct hamt_allocator *ator)
{
    /* Collect everything we know about the existing value */
    hash_state *x_hash =
        &amp;(hash_state){.key = KEY(anchor),
                      .hash_fn = hash->hash_fn,
                      .hash = hash->hash_fn(KEY(anchor), hash->depth / 5),
                      .depth = hash->depth,
                      .shift = hash->shift};
    void *x_value = VALUE(anchor); /* tagged (!) value ptr */
    /* increase depth until the hashes diverge, building a list
     * of tables along the way */
    hash_state *next_hash = hash_next(hash);
    hash_state *x_next_hash = hash_next(x_hash);
    uint32_t next_index = hash_get_index(next_hash);
    uint32_t x_next_index = hash_get_index(x_next_hash);
    while (x_next_index == next_index) {
        TABLE(anchor) = table_allocate(ator, 1);
        INDEX(anchor) = (1 << next_index);
        next_hash = hash_next(next_hash);
        x_next_hash = hash_next(x_next_hash);
        next_index = hash_get_index(next_hash);
        x_next_index = hash_get_index(x_next_hash);
        anchor = TABLE(anchor);
    }
    /* the hashes are different, let's allocate a table with two
     * entries to store the existing and new values */
    TABLE(anchor) = table_allocate(ator, 2);
    INDEX(anchor) = (1 << next_index) | (1 << x_next_index);
    /* determine the proper position in the allocated table */
    int x_pos = get_pos(x_next_index, INDEX(anchor));
    int pos = get_pos(next_index, INDEX(anchor));
    /* fill in the existing value; no need to tag the value pointer
     * since it is already tagged. */
    TABLE(anchor)[x_pos].as.kv.key = (void *)x_hash->key;
    TABLE(anchor)[x_pos].as.kv.value = x_value;
    /* fill in the new key/value pair, tagging the pointer to the
     * new value to mark it as a value ptr */
    TABLE(anchor)[pos].as.kv.key = key;
    TABLE(anchor)[pos].as.kv.value = tagged(value);

    return &amp;TABLE(anchor)[pos];
}"><pre><span>static</span> <span>const</span> <span>hamt_node</span> <span>*</span><span>insert_table</span>(<span>hamt_node</span> <span>*</span><span>anchor</span>, <span>hash_state</span> <span>*</span><span>hash</span>,
                                     <span>void</span> <span>*</span><span>key</span>, <span>void</span> <span>*</span><span>value</span>,
                                     <span>struct</span> <span>hamt_allocator</span> <span>*</span><span>ator</span>)
{
    <span>/* Collect everything we know about the existing value */</span>
    <span>hash_state</span> <span>*</span><span>x_hash</span> <span>=</span>
        <span>&amp;</span>(<span>hash_state</span>){.<span>key</span> <span>=</span> <span>KEY</span>(<span>anchor</span>),
                      .<span>hash_fn</span> <span>=</span> <span>hash</span><span>-&gt;</span><span>hash_fn</span>,
                      .<span>hash</span> <span>=</span> <span>hash</span><span>-&gt;</span><span>hash_fn</span>(<span>KEY</span>(<span>anchor</span>), <span>hash</span><span>-&gt;</span><span>depth</span> / <span>5</span>),
                      .<span>depth</span> <span>=</span> <span>hash</span><span>-&gt;</span><span>depth</span>,
                      .<span>shift</span> <span>=</span> <span>hash</span><span>-&gt;</span><span>shift</span>};
    <span>void</span> <span>*</span><span>x_value</span> <span>=</span> <span>VALUE</span>(<span>anchor</span>); <span>/* tagged (!) value ptr */</span>
    <span>/* increase depth until the hashes diverge, building a list</span>
<span>     * of tables along the way */</span>
    <span>hash_state</span> <span>*</span><span>next_hash</span> <span>=</span> <span>hash_next</span>(<span>hash</span>);
    <span>hash_state</span> <span>*</span><span>x_next_hash</span> <span>=</span> <span>hash_next</span>(<span>x_hash</span>);
    <span>uint32_t</span> <span>next_index</span> <span>=</span> <span>hash_get_index</span>(<span>next_hash</span>);
    <span>uint32_t</span> <span>x_next_index</span> <span>=</span> <span>hash_get_index</span>(<span>x_next_hash</span>);
    <span>while</span> (<span>x_next_index</span> <span>==</span> <span>next_index</span>) {
        <span>TABLE</span>(<span>anchor</span>) <span>=</span> <span>table_allocate</span>(<span>ator</span>, <span>1</span>);
        <span>INDEX</span>(<span>anchor</span>) <span>=</span> (<span>1</span> &lt;&lt; <span>next_index</span>);
        <span>next_hash</span> <span>=</span> <span>hash_next</span>(<span>next_hash</span>);
        <span>x_next_hash</span> <span>=</span> <span>hash_next</span>(<span>x_next_hash</span>);
        <span>next_index</span> <span>=</span> <span>hash_get_index</span>(<span>next_hash</span>);
        <span>x_next_index</span> <span>=</span> <span>hash_get_index</span>(<span>x_next_hash</span>);
        <span>anchor</span> <span>=</span> <span>TABLE</span>(<span>anchor</span>);
    }
    <span>/* the hashes are different, let's allocate a table with two</span>
<span>     * entries to store the existing and new values */</span>
    <span>TABLE</span>(<span>anchor</span>) <span>=</span> <span>table_allocate</span>(<span>ator</span>, <span>2</span>);
    <span>INDEX</span>(<span>anchor</span>) <span>=</span> (<span>1</span> &lt;&lt; <span>next_index</span>) | (<span>1</span> &lt;&lt; <span>x_next_index</span>);
    <span>/* determine the proper position in the allocated table */</span>
    <span>int</span> <span>x_pos</span> <span>=</span> <span>get_pos</span>(<span>x_next_index</span>, <span>INDEX</span>(<span>anchor</span>));
    <span>int</span> <span>pos</span> <span>=</span> <span>get_pos</span>(<span>next_index</span>, <span>INDEX</span>(<span>anchor</span>));
    <span>/* fill in the existing value; no need to tag the value pointer</span>
<span>     * since it is already tagged. */</span>
    <span>TABLE</span>(<span>anchor</span>)[<span>x_pos</span>].<span>as</span>.<span>kv</span>.<span>key</span> <span>=</span> (<span>void</span> <span>*</span>)<span>x_hash</span><span>-&gt;</span><span>key</span>;
    <span>TABLE</span>(<span>anchor</span>)[<span>x_pos</span>].<span>as</span>.<span>kv</span>.<span>value</span> <span>=</span> <span>x_value</span>;
    <span>/* fill in the new key/value pair, tagging the pointer to the</span>
<span>     * new value to mark it as a value ptr */</span>
    <span>TABLE</span>(<span>anchor</span>)[<span>pos</span>].<span>as</span>.<span>kv</span>.<span>key</span> <span>=</span> <span>key</span>;
    <span>TABLE</span>(<span>anchor</span>)[<span>pos</span>].<span>as</span>.<span>kv</span>.<span>value</span> <span>=</span> <span>tagged</span>(<span>value</span>);

    <span>return</span> <span>&amp;</span><span>TABLE</span>(<span>anchor</span>)[<span>pos</span>];
}</pre></div>
<p dir="auto"><code>insert_table()</code> works in three stages: (1) it initiatlizes the <code>hash_state</code>
for the current anchor; (2) creates a series of single-entry tables until the
hashes of the current and new keys diverge; and (3) finally creates a new
table of size 2 that holds the old entry as well as the new key/value pair.</p>
<h3 tabindex="-1" dir="auto">Insert: external API</h3>
<p dir="auto">The implementation of the external API for inserting and updating values in
the HAMT is straighforward:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const void *hamt_set(struct hamt *trie, void *key, void *value)
{
    const hamt_node *n =
        set(trie, trie->root, trie->key_hash, trie->key_cmp, key, value);
    return VALUE(n);
}"><pre><span>const</span> <span>void</span> <span>*</span><span>hamt_set</span>(<span>struct</span> <span>hamt</span> <span>*</span><span>trie</span>, <span>void</span> <span>*</span><span>key</span>, <span>void</span> <span>*</span><span>value</span>)
{
    <span>const</span> <span>hamt_node</span> <span>*</span><span>n</span> <span>=</span>
        <span>set</span>(<span>trie</span>, <span>trie</span><span>-&gt;</span><span>root</span>, <span>trie</span><span>-&gt;</span><span>key_hash</span>, <span>trie</span><span>-&gt;</span><span>key_cmp</span>, <span>key</span>, <span>value</span>);
    <span>return</span> <span>VALUE</span>(<span>n</span>);
}</pre></div>
<p dir="auto"><code>hamt_set()</code> uses a vanilla call to the internal <code>set()</code> function and returns
a pointer to the value of the new key.</p>
<h3 tabindex="-1" dir="auto">Remove</h3>
<h3 tabindex="-1" dir="auto">Iterators</h3>
<h2 tabindex="-1" dir="auto">Persistent data structures and structural sharing</h2>
<h3 tabindex="-1" dir="auto">Path copying</h3>
<h3 tabindex="-1" dir="auto">Insert</h3>
<h3 tabindex="-1" dir="auto">Remove</h3>
<h2 tabindex="-1" dir="auto">Appendix</h2>
<h2 tabindex="-1" dir="auto">Unit testing</h2>
<p dir="auto">For testing, <code>hamt</code> uses a variant of <a href="http://www.jera.com/techinfo/jtns/jtn002.html" rel="nofollow">John Brewer's <code>minunit</code> testing
framework</a>. Minunit is extremely minimalistic and its
header-only implementation easily fits on a single page:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// test/minunit.h
#ifndef MINUNIT_H
#define MINUNIT_H

#define MU_ASSERT(test, message)                                               \
    do {                                                                       \
        if (!(test))                                                           \
            return message;                                                    \
    } while (0)
#define MU_RUN_TEST(test)                                                      \
    do {                                                                       \
        char *message = test();                                                \
        mu_tests_run++;                                                        \
        if (message)                                                           \
            return message;                                                    \
    } while (0)

#define MU_TEST_CASE(name) static char *name()
#define MU_TEST_SUITE(name) static char *name()

extern int mu_tests_run;

#endif /* !MINUNIT_H */"><pre><span>// test/minunit.h</span>
<span>#ifndef</span> <span>MINUNIT_H</span>
<span>#define</span> <span>MINUNIT_H</span>

<span>#define</span> <span>MU_ASSERT</span>(<span>test</span>, <span>message</span>)                                               \
    do {                                                                       \
        if (!(test))                                                           \
            return message;                                                    \
    } while (0)
<span>#define</span> <span>MU_RUN_TEST</span>(<span>test</span>)                                                      \
    do {                                                                       \
        char *message = test();                                                \
        mu_tests_run++;                                                        \
        if (message)                                                           \
            return message;                                                    \
    } while (0)

<span>#define</span> <span>MU_TEST_CASE</span>(<span>name</span>) static char *name()
<span>#define</span> <span>MU_TEST_SUITE</span>(<span>name</span>) static char *name()

<span>extern</span> <span>int</span> <span>mu_tests_run</span>;

<span>#endif</span> <span>/* !MINUNIT_H */</span></pre></div>
<p dir="auto">With <code>minunit</code>, every unit test is a <code>MU_TEST_CASE</code> We use <code>MU_ASSERT</code> to test
the test invariants.  Test cases are grouped into <code>MU_TEST_SUITE</code>s as
sequential calls to <code>MU_RUN_TEST</code>.  When an assertion fails, the <code>return</code>
statement in <code>MU_ASSERT</code> short-circuts test execution and returns a non-null
pointer to the respective <code>message</code> (generally a static string). This, in turn,
causes <code>MU_RUN_TEST</code> to issue a <code>return</code> call with the string pointer,
short-circuting the remaining test suite. The header also declares a global
variable <code>mu_tests_run</code> that keeps track of the total number of executed
tests.</p>
<p dir="auto">The following listing illustrates the basic structure of unit test
implementations with <code>minunit</code>, check the <a href="https://github.com/mkirchner/hamt/blob/main/test/test_hamt.c">actual tests</a> for
a full listing.</p>
<div dir="auto" data-snippet-clipboard-copy-content="// test/test_hamt.c
#include &quot;minunit.h&quot;
#include &quot;../src/hamt.c&quot;

int mu_tests_run = 0;

MU_TEST_CASE(test_dummy)
{
    /* do something here */
    MU_ASSERT(0 == 0, &quot;Oops X-{&quot;);
    return 0;
}

MU_TEST_SUITE(test_suite)
{
    /* Add tests here */
    MU_RUN_TEST(test_dummy);
    /*
     * ... many more ...
     */
    return 0;
}

int main()
{
    printf(&quot;---=[ Hash array mapped trie tests\n&quot;);
    char *result = test_suite();
    if (result != 0) {
        printf(&quot;%s\n&quot;, result);
    } else {
        printf(&quot;All tests passed.\n&quot;);
    }
    printf(&quot;Tests run: %d\n&quot;, tests_run);
    return result != 0;
}"><pre><span>// test/test_hamt.c</span>
<span>#include</span> <span>"minunit.h"</span>
<span>#include</span> <span>"../src/hamt.c"</span>

<span>int</span> <span>mu_tests_run</span> <span>=</span> <span>0</span>;

<span>MU_TEST_CASE</span>(<span>test_dummy</span>)
{
    <span>/* do something here */</span>
    <span>MU_ASSERT</span>(<span>0</span> <span>==</span> <span>0</span>, <span>"Oops X-{"</span>);
    <span>return</span> <span>0</span>;
}

<span>MU_TEST_SUITE</span>(<span>test_suite</span>)
{
    <span>/* Add tests here */</span>
    <span>MU_RUN_TEST</span>(<span>test_dummy</span>);
    <span>/*</span>
<span>     * ... many more ...</span>
<span>     */</span>
    <span>return</span> <span>0</span>;
}

<span>int</span> <span>main</span>()
{
    <span>printf</span>(<span>"---=[ Hash array mapped trie tests\n"</span>);
    <span>char</span> <span>*</span><span>result</span> <span>=</span> <span>test_suite</span>();
    <span>if</span> (<span>result</span> <span>!=</span> <span>0</span>) {
        <span>printf</span>(<span>"%s\n"</span>, <span>result</span>);
    } <span>else</span> {
        <span>printf</span>(<span>"All tests passed.\n"</span>);
    }
    <span>printf</span>(<span>"Tests run: %d\n"</span>, <span>tests_run</span>);
    <span>return</span> <span>result</span> <span>!=</span> <span>0</span>;
}</pre></div>
<p dir="auto">Note that the test setup <code>include</code>s the <code>hamt.c</code> implementation file. This is a
common trick used in unit testing to gain easy access to testing <code>static</code>
functions that would otherwise be inaccessible since they are local to the
<code>hamt.c</code> compilation unit. This requires some care in
the Makefile setup in order to avoid symbol duplication.</p>
<h2 tabindex="-1" dir="auto">Footnotes</h2>
<p dir="auto"><b id="user-content-fn_hash_table_cpp">[1]</b>
The <code>std::unordered_*</code> methods implement open hashing (aka separate chaining),
with the hash table being an array of buckets, each pointing to the head of a
linked list. This is a deliberate and reasonable compromise for general use;
gaining an order of magnitude of speed improvements for specialized use cases
(e.g. append-only, guaranteed high-quality hash functions) is possible. See
<a href="https://stackoverflow.com/a/31113618" rel="nofollow">this stackoverflow post</a> for a summary of the <a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2003/n1456.html" rel="nofollow">standard
proposal</a>.
<a href="#ac_hash_table_cpp"><g-emoji alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">↩</g-emoji></a></p>
<p dir="auto"><b id="user-content-fn_hash_table_c">[2]</b>
<code>musl</code> provides a <code>hsearch</code> implementation that uses closed hashing with
quadratic probing for conflict resolution. The
<a href="https://git.musl-libc.org/cgit/musl/tree/src/search/hsearch.c" rel="nofollow">documentation</a> states that they use powers of two for
table sizing which seems wrong due to the impact on the modulo (table sizes
should ideally be prime). The GLib <code>GHashTable</code> has surprisingly little
documentation in its implementation details but <a href="https://gitlab.gnome.org/GNOME/glib/-/blob/main/glib/ghash.c" rel="nofollow">appears to be
using</a> a separate chaining approach similar to the C++
solution.
<a href="#ac_hash_table_c"><g-emoji alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">↩</g-emoji></a></p>
<p dir="auto"><b id="user-content-fn_hash_table_python">[3]</b> Python's <code>dict</code> implementation uses
closed hashing (aka open addressing) with pseudo-random probing to mitigate
the poor hashing properties of standard python <code>hash()</code> function for some data
types (from <a href="https://stackoverflow.com/a/9022835" rel="nofollow">here</a>). Python keeps the load factor below
0.66; this avoids gradual performance degradation associated w/ high load
factors in closed hashing but comes at increased memory footprint. The
<a href="https://github.com/python/cpython/blob/main/Objects/dictobject.c">codebase</a> was refactored to split the actual data from the
hash table in 3.6, resulting in better memory efficiency and GC friendliness
(see <a href="https://morepypy.blogspot.com/2015/01/faster-more-memory-efficient-and-more.html" rel="nofollow">here</a> and <a href="https://mail.python.org/pipermail/python-dev/2012-December/123028.html" rel="nofollow">here</a>).
<a href="#ac_hash_table_python"><g-emoji alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">↩</g-emoji></a></p>
<p dir="auto"><b id="user-content-fn_hash_table_java">[4]</b> Java provides <code>Hashtable&lt;K,V&gt;</code> and
<code>HashMap&lt;K,V&gt;</code>, both of which implement <code>Map</code> and <code>Collection</code> interfaces; in
addition, <code>Hashtable</code> is synchronized. The <code>HashSet</code> type internally uses a
<code>HashMap</code>. <code>Hashtable</code> and <code>HashMap</code> implement open hashing
(separate chaining) with a default load factor of 0.75; The OpenJDK
implementation of <code>HashMap</code> converts
between linked list and tree representations in the hash buckets, depending on
bucket size, see <a href="https://github.com/openjdk/jdk17/blob/74007890bb9a3fa3a65683a3f480e399f2b1a0b6/src/java.base/share/classes/java/util/HashMap.java">the source</a>.
<a href="#ac_hash_table_java"><g-emoji alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">↩</g-emoji></a></p>
<p dir="auto"><b id="user-content-fn_cpp_virtual_method_table">[5]</b>
There are alternative approaches to enable (somewhat) typesafe templating in
C, mainly by implementing what basically amounts to virtual method tables
using the C preprocessor. See e.g. <a href="https://stackoverflow.com/questions/10950828/simulation-of-templates-in-c-for-a-queue-data-type/11035347" rel="nofollow">here</a> for a useful stackoverflow
summary or <a href="http://blog.pkh.me/p/20-templating-in-c.html" rel="nofollow">here</a> for a more in-depth treatise.
<a href="#ac_cpp_virtual_method_table"><g-emoji alias="leftwards_arrow_with_hook" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/21a9.png">↩</g-emoji></a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Show HN: The full source code and assets for my custom game engine and game (114 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36672183</link>
            <guid>36672183</guid>
            <pubDate>Mon, 10 Jul 2023 20:04:29 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36672183">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="36679012"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36679012" href="https://news.ycombinator.com/vote?id=36679012&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>More of a general nooby question: what would be a good way to read through code like this? Where to start? How to understand what's going on and when? etc.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36679331"><td></td></tr>
                  <tr id="36674742"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36674742" href="https://news.ycombinator.com/vote?id=36674742&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>You should probably give credit to Casey / Handmade Hero with regards to your windows platform code. Well its kinda scattered around, so probably just credit in general.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36676453"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36676453" href="https://news.ycombinator.com/vote?id=36676453&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>True, Casey/HH was a huge inspiration and the project started from following the first few days of Handmade Hero. I should mention it.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36675541"><td></td></tr>
                <tr id="36677434"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36677434" href="https://news.ycombinator.com/vote?id=36677434&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><p><span>Here 'from scratch' probably means 'from fundamentals', e.g. without a prefab game engine, as opposed to 'taking no inspiration or lessons from anyone else or using any library code written by other people'.<p>Though that would be one hell of a project.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36677483"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36677483" href="https://news.ycombinator.com/vote?id=36677483&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><p><span>I mentioned in a couple of places that I did use other people's file loading libraries (mainly Sean Barrett's excellent stb libs).<p>“If you wish to make an apple pie from scratch, you must first invent the universe” :)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36678074"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36678074" href="https://news.ycombinator.com/vote?id=36678074&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><p><span>&gt; “If you wish to make an apple pie from scratch, you must first invent the universe” :)<p>This is something I posted on Casey's first handmade video, together with some counter arguments to his rant on the shortcomings of c++ and windows. He has since disabled comments.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                                    <tr id="36675969"><td></td></tr>
                <tr id="36676445"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36676445" href="https://news.ycombinator.com/vote?id=36676445&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>The HTTPS certificate expired on my privacy policy page and Google delists the app when that happens. After you renew they require that you resubmit the app and I didn't bother since there were barely any downloads then.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36678562"><td></td></tr>
                        <tr id="36673476"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36673476" href="https://news.ycombinator.com/vote?id=36673476&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>I work in tools for game devs and it's great to see more open-source work like this it's very hard to find good modern open-source game examples.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36676492"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36676492" href="https://news.ycombinator.com/vote?id=36676492&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>Thanks, it's not the cleanest code and for example my immediate mode GUI is nowhere near IMGUI but theres still plenty to look at.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36675685"><td></td></tr>
                <tr id="36676506"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36676506" href="https://news.ycombinator.com/vote?id=36676506&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>Thanks! Finishing such a large project feels great and you learn a lot when you have to implement so much you take for granted when using things other people have built</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36679448"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36679448" href="https://news.ycombinator.com/vote?id=36679448&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>Yes, congratulations on finishing the game! Also, thank you very much for sharing the story of making it. I'm more inspired than ever to keep dev logs for the little side projects I work on. It's important to keep track of progress, reflect on where you were early on, and see how far you've come.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36679689"><td></td></tr>
                              <tr id="36674045"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36674045" href="https://news.ycombinator.com/vote?id=36674045&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><p><span>Any idea what you would have to do to get it running on iOS?<p>Did you release it? If so, how is it doing compared to your expectations? (I looked for it on the Play store but couldn't find it)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36676471"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36676471" href="https://news.ycombinator.com/vote?id=36676471&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><p><span>You would have to write the platform code for iOS similarly to how the Android and Windows is written. The initial idea was to release it on iOS as well but at the end I didn't want to invest in buying a Mac, the license and the time to port it.<p>Edit: As for the release, I think it barely broke a couple of hundred downloads. I didn't do any real marketing and had no real goal of really getting the game out there as it was a learning experience for me first and foremost.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36675869"><td></td></tr>
                  <tr id="36672219"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36672219" href="https://news.ycombinator.com/vote?id=36672219&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>I'm also making a game from scratch and reinventing wheels. What is the processing that you are doing for GLES? I see the GLSL_PREPROCESS in the GLSL.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36672236"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36672236" href="https://news.ycombinator.com/vote?id=36672236&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>I wanted to include common code for all shaders in a simple way, but I was too lazy write a proper parser. So instead I used the C++ preprocessor and macros to include things from common shader code and combine them in the build step.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36674193"><td></td></tr>
                <tr id="36676533"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36676533" href="https://news.ycombinator.com/vote?id=36676533&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>I could have definitely used that and I'm surprised I didn't realise it when researching how to include shader code. Luckily my setup required very little work and with my system I could share snippets between GLES and GL.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="36674356"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36674356" href="https://news.ycombinator.com/vote?id=36674356&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><p><span>if youre wondering why its not on the play store<p>&gt; The game has been since delisted on the Play Store, however you can download the APK here.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36676508"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36676508" href="https://news.ycombinator.com/vote?id=36676508&amp;how=up&amp;goto=item%3Fid%3D36672183"></a></center>    </td><td><br><div>
                  <p><span>Yeah the privacy policy page had an expired certificate and Google delists the app automatically when that happens.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[2048 Bit RSA and the Year 2030 (224 pts)]]></title>
            <link>https://articles.59.ca/doku.php?id=em:20482030</link>
            <guid>36672115</guid>
            <pubDate>Mon, 10 Jul 2023 19:58:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://articles.59.ca/doku.php?id=em:20482030">https://articles.59.ca/doku.php?id=em:20482030</a>, See on <a href="https://news.ycombinator.com/item?id=36672115">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page">
                                                            <!-- wikipage start -->
                    <!-- TOC START -->
<div id="dw__toc">
<h3>Table of Contents</h3>

</div>
<!-- TOC END -->


<p>
In the course of some recent work I developed the impression that 2048 RSA was quite secure. Canada<sup><a href="#fn__1" id="fnt__1">1)</a></sup> (my country of residence) and others
<sup><a href="#fn__2" id="fnt__2">2)</a></sup> are currently strongly suggesting that 2048 bit RSA should be considered potentially insecure after the year 2030 and that the minimum length considered secure should be then be 3072 bits. That is only 7 years from now (2023).
</p>

<h2 id="where_did_the_2030_cutoff_come_from">Where did the 2030 cutoff come from?</h2>
<div>

<p>
I am reasonably certain that the ideas here came from an influential paper released in 2004 by Arjen K. Lenstra<sup><a href="#fn__3" id="fnt__3">3)</a></sup> that showed this year in a table. Here is a simplified version of the table:
</p>
<div><table>
	<thead>
	<tr>
		<th> Modulus Bit Length </th><th> Conservative Year </th><th> Optimistic Year </th>
	</tr>
	</thead>
	<tbody><tr>
		<td> 1024 </td><td> 2006 </td><td> 2006 </td>
	</tr>
	<tr>
		<td> 1280 </td><td> 2014 </td><td> 2017 </td>
	</tr>
	<tr>
		<td> 1536 </td><td> 2020 </td><td> 2025 </td>
	</tr>
	<tr>
		<td> 2048 </td><td> 2030 </td><td> 2040 </td>
	</tr>
	<tr>
		<td> 3072 </td><td> 2046 </td><td> 2065 </td>
	</tr>
	<tr>
		<td> 4096 </td><td> 2060 </td><td> 2085 </td>
	</tr>
	<tr>
		<td> 8192 </td><td> 2100 </td><td> 2142 </td>
	</tr>
</tbody></table></div>

<p>
<em><sub>Common RSA modulus bit-length life spans. Table 4 from: <a href="https://infoscience.epfl.ch/record/164539/files/NPDF-32.pdf" target="_tab" title="https://infoscience.epfl.ch/record/164539/files/NPDF-32.pdf" rel="ugc nofollow noopener">Key Lengths</a></sub></em>
</p>

<p>
So we look at the 2048 bit row, decide we do not feel all that optimistic, and then choose 2030 as the cutoff date. Simple. Straightforward. Definite. Great for long term planning…
</p>

<p>
Lenstra's prediction was largely based on two observations:
</p>
<ul>
<li><p> That the performance of computing technology was doubling every 18 months.</p>
</li>
<li><p> That the increase of factoring efficiency due to software improvement was doubling every 18 months.</p>
</li>
</ul>

<p>
Combining the two observations means that as of 2004, the capability to attack RSA 2048 was doubling every 9 months. Then 2004 to 2030 is 26 years or 312 months or 35 doublings. That is a capability increase of 2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2×2 or 2<sup>35</sup> in exponential notation which works out to a predicted capability increase of 34,359,738,368 times as of 2030. So around 34 billion (34×10<sup>9</sup>) times. This figure can be interpreted as how far 2048 bit RSA encryption was out of reach as of 2004. This is what that increase looks like on a plot:
</p>

<p>
<a href="https://articles.59.ca/lib/exe/detail.php?id=em%3A20482030&amp;media=em:9m0430.svg" target="_tab" title="em:9m0430.svg" rel="noopener"><img src="https://articles.59.ca/lib/exe/fetch.php?media=em:9m0430.svg" loading="lazy" alt=""></a><br>

<em><sub>Doubling every 9 months from 2004 to 2030</sub></em>
</p>

<p>
This sort of curve and this sort of increase is called “exponential” after exponential notation. It is well known that any physical quantity can not increase in this way past a certain point. There is a common and ancient story that illustrates this principle in a way quite appropriate to this discussion:
</p>

<p>
Someone is to be rewarded. As a reward, they ask that the following process be completed and that they should then be rewarded with the resultant number of wheat/rice grains.
</p>

<p>
Place a single grain on the first square of a chessboard. Place double that number of grains (2) on the next empty square. Then double <em>that</em> number of wheat grains (4) on the next empty square. If you fill all 64 squares of the chessboard, doubling each time, you end up with an amount of wheat some thousands of times more than the entire yearly production of wheat on the entire planet. Obviously this process can never be completed. The exponential nature of the increase precludes that<sup><a href="#fn__4" id="fnt__4">4)</a></sup>. An appropriate quote:
</p>
<blockquote><p>
Exponentials can't go on forever, because they will gobble up everything.<br>
</p></blockquote>

<p>
<sub>Carl Sagan, <em>Billions and Billions: Thoughts On Life And Death At the Brink Of The Millennium</em></sub>
</p>

<p>
Having seen that we have a predicted exponential increase of RSA factoring<sup><a href="#fn__5" id="fnt__5">5)</a></sup> capability here, an important task will be to determine if a limit has yet occurred that would prevent that predicted increase.
</p>

</div>

<h2 id="how_did_things_actually_go">How did things actually go?</h2>
<div>

<p>
The best we can do here is to look at the length of the numbers, of the sort relevant to RSA, that have been actually factored. So far, the largest number is 829 bits long<sup><a href="#fn__6" id="fnt__6">6)</a></sup> which was factored in 2020. Late in 2003, a 576 bit number was factored. Here is a plot of those factoring achievements, including the ones between them:
</p>

<p>
<a href="https://articles.59.ca/lib/exe/detail.php?id=em%3A20482030&amp;media=em:factoring.svg" target="_tab" title="em:factoring.svg" rel="noopener"><img src="https://articles.59.ca/lib/exe/fetch.php?media=em:factoring.svg" loading="lazy" alt=""></a>
</p>

<p>
I ignored the impression that the increase in factoring capability seems to have levelled off after 2009 and added a linear extrapolation based on the record factorizations over the entire predicted interval. The result (1024 bits by 2030) was still quite underwhelming. One would expect a more definite trend if the capability was increasing exponentially. The increasing trend is not enough to make us think that 2048 bit RSA would be under threat by 2030.
</p>

<p>
If the actual factoring demonstrations <em>had</em> shown a more definite upward trend then we could assume that the researchers were simply funded at a lower level than, say, some over funded state run signals intelligence agency and that there was the possibility of a genuine threat. But as it is, this is just inconclusive for our purposes. There are any number of reasons that less time and fewer resources might be allocated to these factoring demonstrations. We have to find another way to approach this question.
</p>

</div>

<h2 id="how_are_the_basic_assumptions_holding_up_so_far">How are the basic assumptions holding up so far?</h2>
<p>
Let's consider the two basic assumptions that the prediction was based on:
</p>

<h3 id="factoring_algorithm_performance">Factoring Algorithm Performance</h3>
<div>

<p>
The assumption is that the increase of factoring efficiency due to software improvement will double every 18 months. 
</p>

<p>
It isn't normally possible to predict the rate of software efficiency increase. It is safe to assume that there will be <em>some</em> increase of performance possible for a particular software system, but not how much or when. Software performance improvement, very generally, seems to have three phases:
</p>
<div><table>
	<thead>
	<tr>
		<th>Phase              </th><th> Performance Increase </th><th> Relative Complexity </th>
	</tr>
	</thead>
	<tbody><tr>
		<td>Low hanging fruit  </td><td>Large                 </td><td>Low                  </td>
	</tr>
	<tr>
		<td>High hanging fruit </td><td>Low                   </td><td>High                 </td>
	</tr>
	<tr>
		<td>New algorithm      </td><td>Small to very large   </td><td>—                  </td>
	</tr>
</tbody></table></div>

<p>
So improvement is first easy in the “low hanging fruit” phase and progressively harder in the “high hanging fruit” phase. The cost of this improvement is increased complexity. At some point someone might invent a new algorithm and the cycle can continue. The improvement that comes with a new algorithm can be very large; perhaps even enough to count as exponential if it happens more than once.
</p>

<p>
That is exactly what happened in the case of factoring algorithms in the '80s and '90s<sup><a href="#fn__7" id="fnt__7">7)</a></sup>. The <em>continued fraction factoring algorithm</em> was followed by the <em>quadratic sieve factoring algorithm</em> which was in turn followed by the <em>number field sieve</em> (NFS) algorithm. At the time of Lenstra's paper it seemed that his <em>elliptic curve method</em> might exceed the capability of the NFS algorithm.
</p>

<p>
For the 19 years from 2004 and 2023 and an assumption of 18 month doubling we end up with a predicted increase of algorithm performance of 6,502 times. It is hard to find definite figures on actual algorithm performance increase in this interval as it is mixed in with hardware performance. The CADO-NFS<sup><a href="#fn__8" id="fnt__8">8)</a></sup> system claims a performance increase of 3.2 from version 1.1 to version 2.3.0 at the 512 bit (RSA-155) level. The researchers responsible for the most recent factoring record<sup><a href="#fn__9" id="fnt__9">9)</a></sup> (829 bits) claimed a performance increase of 3-4 times. Even combining these improvements (I am not sure that is appropriate) we end up with a performance increase of 12 times which is well short of the predicted 6,502 times.
</p>

<p>
The elliptic curve method never ended up exceeding the NFS algorithm for factoring at the scale of RSA 2048. The NFS algorithm was never exceeded by any other invented algorithm. As a result the NFS algorithm is still the best available 27 years after its invention. That seems to be the reason for the severe slowdown in algorithm performance increase.
</p>

<p>
The history of computing shows that there is normally a quick burst of progress related to algorithmic performance when some particular task becomes feasible followed by a long period of very gradual improvement. The popular example of sorting is a good example. The mergesort, quicksort and heapsort algorithms were invented in 1945, 1959 and 1964 respectively. They are still in routine use today for sorting large lists of values. In retrospect it very much seems that the same sort of thing happened with respect to factoring algorithms. The complexity here seems to be high (CADO-NFS has hundreds of thousands of lines of code) so we are probably in the “high hanging fruit” phase. At this point there seems to be no reason to expect the predicted exponential increase in factoring algorithm performance (165,140 times) required to threaten 2048 bit RSA by the year 2030.
</p>

</div>

<h3 id="computing_performance">Computing Performance</h3>
<div>

<p>
The prediction is based in the assumption that the available computing performance will double every 18 months.
</p>

<p>
This assumption is popularly known as “Moore's Law”<sup><a href="#fn__10" id="fnt__10">10)</a></sup>. It has become fashionable to speculate that Moore's law is now dead. That is both true and false.
</p>

<p>
It turns out that there are two versions of Moore's law. The 18 month doubling refers to the increase in available computing performance and is appropriate to our discussion here. The original Moore's law refers to the number of <a href="https://en.wikipedia.org/wiki/Transistor" target="_tab" title="https://en.wikipedia.org/wiki/Transistor" rel="noopener">transistors</a> available on a substrate of a particular size and is now generally accepted to mean a doubling every 24 months. The 24 month law related to the number of transistors is still alive (but the rate of examples is decreasing rapidly). The 18 month performance law is the one that is dead. That's the one that the 2030 prediction is based on…
</p>

<p>
If you double the number of transistors on a substrate of a particular size then if the power consumption of those new transistors is half the power consumption of the old transistors then the result will be a substrate that produces the same amount of heat. <a href="https://en.wikipedia.org/wiki/Dennard_scaling" target="_tab" title="https://en.wikipedia.org/wiki/Dennard_scaling" rel="noopener">Dennard scaling</a> explains why this used to be the case. Otherwise, each doubling of transistors would produce more heat in the same area. That heat would quickly become unmanageable. The 18 month performance version of Moore's law depends on Dennard scaling to be practical. But Dennard scaling no longer works and as a result Moore's law 18 is dead.
</p>

<p>
This graph shows an example based on Intel processors:
</p>

<p>
<a href="https://articles.59.ca/lib/exe/detail.php?id=em%3A20482030&amp;media=em:dennard.png" target="_tab" title="em:dennard.png" rel="noopener"><img src="https://articles.59.ca/lib/exe/fetch.php?w=640&amp;tok=b6207e&amp;media=em:dennard.png" loading="lazy" alt="" width="640"></a><br>

<em><sub>From: <a href="https://www.extremetech.com/computing/116561-the-death-of-cpu-scaling-from-one-core-to-many-and-why-were-still-stuck" target="_tab" title="https://www.extremetech.com/computing/116561-the-death-of-cpu-scaling-from-one-core-to-many-and-why-were-still-stuck" rel="ugc nofollow noopener">The death of CPU scaling: From one core to many -- and why we're still stuck</a><br>

… which was from: <a href="http://www.gotw.ca/publications/concurrency-ddj.htm" target="_tab" title="http://www.gotw.ca/publications/concurrency-ddj.htm" rel="ugc nofollow noopener">The Free Lunch Is Over - A Fundamental Turn Toward Concurrency in Software</a></sub></em>
</p>

<p>
Note the logarithmic scale. The rising, roughly straight line for the number of transistors indicates that some sort of exponential increase is occurring. The qualities we are interested in, clock speed and performance per clock, have levelled off. The power consumption has levelled off because of physical limitations associated with removing heat from a substrate. Because of the end of Dennard scaling, performance is limited by the power efficiency of the transistors and the amount of heat that can be removed from the substrate. We can't make the transistors smaller in a way that would make them faster because making transistors smaller greatly decreases their power efficiency. That is mostly because of <a href="https://en.wikipedia.org/wiki/Quantum_tunnelling" target="_tab" title="https://en.wikipedia.org/wiki/Quantum_tunnelling" rel="noopener">quantum tunnelling</a> which turned out to be a fundamental physical limit on the performance of the highest performance computing technology we have.
</p>

<p>
There it is. A physical limit that prevents an exponential increase. In retrospect, it can be seen that this limit was already preventing an exponential increase in 2004 when the prediction was made. Unfortunate timing for the one making the prediction but it makes our task here easier. There was no exponential growth of computing performance from 2004 to 2023. It is very unlikely that such growth will reappear in the 7 years before 2030.
</p>

</div>

<h2 id="where_are_we_now">Where Are We Now?</h2>
<div>

<p>
How do things look for breaking 2048 bit RSA right now in 2023?
</p>

<p>
The best available algorithm known, usable with the most powerful computers we know how to build, is NFS. So we would use the NFS algorithm.
</p>

<p>
How much computing power could be brought to bear? As a exorbitant example we could use the Bitcoin mining network. The Bitcoin mining network is a distributed network devoted to breaking a cryptographic function with a brute force search. So it would seem to be a fairly good example to use with respect to breaking RSA which is the same sort of thing.
</p>

<p>
Bitcoin mining is a process that makes money for the entity running the mining system. This financial incentive has created a situation where the mining network has expanded to what might seem a ridiculous extent. The incentive is very sensitive to the cost of electricity. As a result the mining systems are designed to be as power efficient as humanly possible. The end of Dennard scaling is very relevant here. The troublesome heat starts as expensive electricity. Even with this desperate quest for energy efficiency, it is estimated that the Bitcoin mining network consumed 1/200th (0.5%) of all the electricity generated on the entire planet<sup><a href="#fn__11" id="fnt__11">11)</a></sup>in 2021. This makes the network a good upper limit on what might be done in secret. If some over funded national signals intelligence agency built that much processing power we would be able to tell just by checking their power bill. Electricity consumption at the level of an entire country would be impossible to hide.
</p>

<p>
Let's imagine that we could magically repurpose the processing power of the entire Bitcoin mining network for breaking a single 2048 bit RSA key. This will require us to relate what the network is currently doing to the NFS algorithm. I will use the “apples to apples” relation developed in RFC3766<sup><a href="#fn__12" id="fnt__12">12)</a></sup>. It's based on the situation in 2004 but there does not seem to be a better one available. The operations that the Bitcoin network performs would seem to take roughly the same amount of processing as the operations used as a reference in RFC3766<sup><a href="#fn__13" id="fnt__13">13)</a></sup>. By RFC3766, breaking 2048 bit RSA would require 9.01×10<sup>30</sup> cryptographic operations. The Bitcoin mining network recently achieved a rate of 1.24×10<sup>28</sup> operations/year<sup><a href="#fn__14" id="fnt__14">14)</a></sup>.
</p>

<p>
So using the power of the largest amount of computing ever dedicated to breaking cryptographic operations in history, it would take  9.01×10<sup>30</sup>/1.24×10<sup>28</sup> years to break one RSA key. That works out to 727 years. If we could magically create enough physical hardware to break a RSA key in a year then we would need to come up with 727/200 or 3.6 times the amount of electricity currently generated on the planet to run that hardware.
</p>

<p>
This only compares the amount of computing power required to break 2048 bit RSA vs the amount of computing power embodied by the Bitcoin mining network. The operations that the Bitcoin network do require very little memory/RAM. The NFS algorithm on the other hand requires a tremendous amount of memory. In 2004, R. D. Silverman pushed back against the idea that 1024 bit RSA was at imminent risk of compromise. As part of that argument he pointed out that the NFS algorithm has a phase (matrix reduction) that requires a large amount of computing power tightly coupled to a large amount of memory. On the basis of that observation he predicted that 1024 bit RSA would not be publicly broken before the 2030s. So far that prediction seems on track. In passing he mentioned that 2048 bit RSA would require 10<sup>18</sup> bytes of memory for the irreducible matrix reduction step<sup><a href="#fn__15" id="fnt__15">15)</a></sup>. That's a million terabytes, an unimaginable amount of memory to be found in one place, much less tightly coupled to enough processing power to provide any practical benefit. Speaking of memory, the speed of DRAM has lagged to be something like a hundred times slower than processing and sieving is likely very cache unfriendly. So the idea that the processing power of the Bitcoin network could break a single 2048 bit RSA key in a mere 727 years is hopelessly optimistic.
</p>

<p>
It is clear that 2048 bit RSA is vastly out of the scope of current computing technology running the best available algorithm (NFS)…
</p>

</div>

<h2 id="the_future">The future</h2>
<div>

<p>
It appears that a fundamental algorithmic breakthrough would be a prerequisite to threaten the security of RSA 2048. How likely is such a breakthrough?
</p>

<p>
Factoring, prime numbers and the relationship between the two subjects have fascinated humanity for a very long time. The fundamental idea of sieving as a fast method of finding a lot of primes has been around for thousands of years<sup><a href="#fn__16" id="fnt__16">16)</a></sup> (Number Field <em>Sieve</em>, Quadratic <em>Sieve</em>).
</p>

<p>
When creating advanced factoring algorithms there is a vast amount of historical knowledge available. This is very well trodden ground. I think this reduces the chances of surprising insights and reduces the advantage to those working in secret.
</p>

</div>

<h3 id="quantum_computing">Quantum computing</h3>
<div>

<p>
Quantum computing of the sort intended to break RSA involves a breakthrough in both computing and algorithms. Normally some sort of new computing technology is invented and then algorithms are designed to enable that technology to do something useful. The quantum computing threat to RSA is different. We now have the algorithm (Shor's) but the computer to run it on only exists in our imagination.
</p>

<p>
If someone were to invent such a computer then RSA 2048 would be immediately and trivially breakable. RSA 3072 would also be trivially breakable. The same applies to RSA 4096 and 8192. The threat here is admittedly hypothetical, but this still serves as an example of a situation where routine key length extension is of no real value. By repeatedly increasing the size of keys we are betting that a breakthrough will be exactly strong enough to break the superseded key length and not break the current key length. That seems unlikely.
</p>

<p>
New threats tend to take a different form than old threats…
</p>

</div>

<h2 id="conclusion">Conclusion</h2>
<div>

<p>
The assumptions that the 2030 date for increasing RSA key length were based on turned out to be invalid. A check of current capability confirms this. There seems to be no rational reason to increase RSA key sizes past 2048 starting in the year 2030. We don't have any reason to increase RSA key sizes at <em>any</em> time based on our current understanding.
</p>
<blockquote><p>
Although this type of estimates is the best that can be done at this point, it should be understood that actual factoring capabilities may follow an entirely different pattern. Any prediction more than a few decades away about security levels is wishful thinking. The figures in the tables should be properly interpreted, namely as today’s best estimates that may have to be revised tomorrow. Anyone using factoring based asymmetric cryptosystems should constantly monitor and stay ahead of the developments in the research community.</p></blockquote>

<p>
The proceeding quoted text is found with the previously shown “Common RSA modulus bit-length life spans” table in the original paper. If you don't believe me you should still believe Dr. A. K. Lenstra, the one that came up with the double exponential prediction in the first place. We should enact a continuous process of watchful waiting. Any policy changes should be done as a response to changes in the algorithmic and computing performance landscape.
</p>

<p>
This is important because an aspect like a key length is often deeply embedded in the systems protected by the associated cryptography. Rooting out and changing such aspects is normally very expensive in the sorts of systems where cryptographic protection is provided. Where this is not possible at the software level then wholesale equipment replacement is required. The time and resources expended on pointless key length increases can be more profitably used elsewhere. Longer RSA keys require more processing power to process so that pointless keylength increases waste computing resources and power as well.
</p>

</div>

<h2 id="other_systems">Other systems</h2>
<p>
Up to this point this article was made exclusively about RSA to improve readability. Let's use the ideas developed here to briefly examine some other popular cryptographic systems. I will use the NIST recommendations as the practical example<sup><a href="#fn__17" id="fnt__17">17)</a></sup><sup><a href="#fn__18" id="fnt__18">18)</a></sup>. The period for all of these NIST recommendations is from 2019 to 2030 (11 years). The current keysize recommendation became effective as of 2019 and the next one becomes effective as of 2030.
</p>

<h3 id="discrete_logarithm">Discrete logarithm</h3>
<div>

<p>
Current group size: 2048 bits. Next group size: 3072 bits.
</p>

<p>
This is most often seen as the basis of the <a href="https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange" target="_tab" title="https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange" rel="noopener">Diffie–Hellman key exchange</a> system.
</p>

<p>
The best known algorithm to attack discrete logarithm systems is also a version of NFS. The difficulty vs RSA is slightly harder where the RSA key size is the same as the discrete logarithm group size. So the proceeding discussion about the pointlessness of increasing RSA key sizes directly applies to discrete logarithm systems.
</p>

</div>

<h3 id="elliptic_curve_based">Elliptic curve based</h3>
<div>

<p>
Current key size: 224 bits. Next key size: 256 bits.
</p>

<p>
The rule of thumb for elliptic curves is that 2 extra key bits doubles the difficulty. That's (256-224)/2=16 difficulty doublings over the 11 year period. So an implicit assumption that the capability available for breaking elliptic curves will double every 11*12/16=8.25 months. That's a bit faster than the 9 month double exponential assumption that in turn comes from the assumption that available processing power and algorithmic capability are each doubling every 18 months. We know that that is not true for processing power. I have not been able to find any indication of a current or upcoming breakthrough in software methods for breaking elliptic curves, but I am not really qualified to judge that. So I will have to leave this here. In the absence of any evidence of a algorithmic breakthrough then the increase from 224 bits to 256 bits would be unnecessary.
</p>

<p>
Elliptic curve based systems can use much shorter key lengths than other systems. Pointlessly increasing elliptic curve key length would be tragic. If the current assumptions are maintained that would mean we would see a recommendation for more than 256 bit elliptic curve keys for the 2040 decade.
</p>

</div>

<h3 id="symmetric_encryption">Symmetric encryption</h3>
<div>

<p>
Current key size: 112 bits. Next key size: 128 bits.
</p>

<p>
Some examples of symmetric encryption schemes are: <a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard" target="_tab" title="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard" rel="noopener">AES</a>, <a href="https://en.wikipedia.org/wiki/Salsa20#ChaCha_variant" target="_tab" title="https://en.wikipedia.org/wiki/Salsa20#ChaCha_variant" rel="noopener">ChaCha20</a> and <a href="https://en.wikipedia.org/wiki/Camellia_(cipher)" target="_tab" title="https://en.wikipedia.org/wiki/Camellia_(cipher)" rel="noopener">Camellia</a>.
</p>

<p>
One extra key bit doubles the difficulty here. That's 128-112=16 difficulty doublings over the 11 year period. So an implicit assumption that the capability available for breaking elliptic curves will double every 11*12/16=8.25 months. That's a bit faster than the 9 month double exponential assumption that in turn comes from the assumption that available processing power and algorithmic capability are each doubling every 18 months. We know that that is not true for processing power.
</p>

<p>
The idea that the algorithmic capability against symmetric encryption might be doubling every 18 months is fairly surprising. A regular increase here is not something that is normally assumed. Perhaps there was some sort of “debt” with respect to key length that we are making up for in this time period. It might be good to apply the Bitcoin thought experiment as previously seen in this article as a sort of sanity check.
</p>

<p>
The number of cryptographic operations required to use brute force to break a 112 bit key in a symmetric system is 2<sup>112</sup>=5.19×10<sup>33</sup> operations. We will make the reasonable assumption that one Bitcoin operation would take roughly the same time as one symmetric encryption operation. Then it would take 5.19×10<sup>33</sup>/1.24×10<sup>28</sup>=418,548 years. Reducing this to a year would require 418,548/200=2092 times the current total planetary electricity production<sup><a href="#fn__19" id="fnt__19">19)</a></sup>.
</p>

<p>
It does not seem reasonable to increase minimum symmetric encryption key size past 112 bits after 2030.
</p>

<p>
<a href="https://articles.59.ca/doku.php?id=em:index" title="em:index" data-wiki-id="em:index">Encrypted Messaging index</a><br>

<a href="https://articles.59.ca/doku.php?id=start" title="start" data-wiki-id="start">Home</a>
</p>

</div>


                    <!-- wikipage stop -->
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: What tech newsletters are you currently subscribing to? (111 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36671105</link>
            <guid>36671105</guid>
            <pubDate>Mon, 10 Jul 2023 18:45:30 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36671105">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="36671719"><td></td></tr>
            <tr id="36671454"><td></td></tr>
            <tr id="36671714"><td></td></tr>
            <tr id="36676841"><td></td></tr>
            <tr id="36672214"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36672214" href="https://news.ycombinator.com/vote?id=36672214&amp;how=up&amp;goto=item%3Fid%3D36671105"></a></center>    </td><td><p><span>off topic, how do you find time to follow and read all those newsletters?<p>I tried to follow couple of them, after a while it felt like everyday I am reading some newsletter. At this moment I completely stopped, because messed up my time management skills after having kids. Need to recover
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36672303"><td></td></tr>
            <tr id="36673872"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36673872" href="https://news.ycombinator.com/vote?id=36673872&amp;how=up&amp;goto=item%3Fid%3D36671105"></a></center>    </td><td><p><span>They all get filtered straight in to a folder. No notification when they arrive.<p>Then I just go through it every couple weeks either when im waiting around on my phone or when I find myself wasting time on reddit or hn but dont quite have it in my to jump back in to dev work just yet.</p><p>Between topics not being interesting to me, overlap in covered topics by each newsletter, and overlap with things I've already seen here; the vast majority of content that hits that folder doesn't ever actually get read. I'm yet to find any source of news, tech newsletter or otherwise, that warrants reading on a daily basis. Closest would be the weather.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36672451"><td></td></tr>
                <tr id="36672594"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36672594" href="https://news.ycombinator.com/vote?id=36672594&amp;how=up&amp;goto=item%3Fid%3D36671105"></a></center>    </td><td><br><div>
                  <p><span>Honest question; do people actually go back and read the stuff the intended to read later? I have never been successful with that strategy, and have to bookmark folders to prove it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36673042"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36673042" href="https://news.ycombinator.com/vote?id=36673042&amp;how=up&amp;goto=item%3Fid%3D36671105"></a></center>    </td><td><br><div>
                  <p><span>It’s mostly aspirational for me but I still do it and am happy 10% of the time I actually circle back and read the thing.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36672817"><td></td></tr>
                  <tr id="36672629"><td></td></tr>
                        <tr id="36671555"><td></td></tr>
            <tr id="36672049"><td></td></tr>
            <tr id="36674020"><td></td></tr>
            <tr id="36671899"><td></td></tr>
            <tr id="36672334"><td></td></tr>
                <tr id="36672404"><td></td></tr>
                  <tr id="36671726"><td></td></tr>
                <tr id="36671850"><td></td></tr>
                <tr id="36671985"><td></td></tr>
                        <tr id="36672163"><td></td></tr>
            <tr id="36675534"><td></td></tr>
            <tr id="36671456"><td></td></tr>
                <tr id="36671529"><td></td></tr>
                  <tr id="36676121"><td></td></tr>
            <tr id="36671920"><td></td></tr>
                <tr id="36677004"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36677004" href="https://news.ycombinator.com/vote?id=36677004&amp;how=up&amp;goto=item%3Fid%3D36671105"></a></center>    </td><td><br><div>
                  <p><span>Yep Ben Evans’ newsletter is great. If one only would want to subscribe to one single weekly newsletter about the tech industry, I’d recommend that one.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36673273"><td></td></tr>
            <tr id="36673182"><td></td></tr>
            <tr id="36676356"><td></td></tr>
            <tr id="36671483"><td></td></tr>
            <tr id="36671782"><td></td></tr>
            <tr id="36672410"><td></td></tr>
            <tr id="36671943"><td></td></tr>
            <tr id="36674358"><td></td></tr>
            <tr id="36672504"><td></td></tr>
            <tr id="36672267"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36672267" href="https://news.ycombinator.com/vote?id=36672267&amp;how=up&amp;goto=item%3Fid%3D36671105"></a></center>    </td><td><p><span>Chips and cheese (in depth hardware analysis)<p>Asianometry (just go subscribe, it's brilliant)</p><p>and as others mentioned, the graphics programming newsletter
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36672380"><td></td></tr>
            <tr id="36672413"><td></td></tr>
                <tr id="36672430"><td></td></tr>
                  <tr id="36671476"><td></td></tr>
            <tr id="36672311"><td></td></tr>
            <tr id="36671574"><td></td></tr>
            <tr id="36672193"><td></td></tr>
            <tr id="36671926"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36671926" href="https://news.ycombinator.com/vote?id=36671926&amp;how=up&amp;goto=item%3Fid%3D36671105"></a></center>    </td><td><p><span>The questions is: what are you looking for?<p>- General tech news?</p><p>- Career tips/tricks for devs?</p><p>- Deep dives on functional topics?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36672000"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36672000" href="https://news.ycombinator.com/vote?id=36672000&amp;how=up&amp;goto=item%3Fid%3D36671105"></a></center>    </td><td><br><div>
                  <p><span>Since everything I subscribe to has already been named, I'll ask a question: anyone got any blogs/newsletters/<i>etc.</i> geared toward LISPy topics that they can recommend?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36671940"><td></td></tr>
            <tr id="36671621"><td></td></tr>
            <tr id="36673118"><td></td></tr>
            <tr id="36671730"><td></td></tr>
            <tr id="36672087"><td></td></tr>
            <tr id="36672770"><td></td></tr>
            <tr id="36671800"><td></td></tr>
            <tr id="36671533"><td></td></tr>
            <tr id="36672456"><td></td></tr>
            <tr id="36672927"><td></td></tr>
            <tr id="36675301"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FreeShip Plus in Lazarus – an open-source software for boat and hull design (122 pts)]]></title>
            <link>https://github.com/markmal/freeship-plus-in-lazarus</link>
            <guid>36670328</guid>
            <pubDate>Mon, 10 Jul 2023 17:58:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/markmal/freeship-plus-in-lazarus">https://github.com/markmal/freeship-plus-in-lazarus</a>, See on <a href="https://news.ycombinator.com/item?id=36670328">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content"><pre># freeship-plus-in-lazarus

To download binary packages ( .zip and .deb files) please proceed to Releases page 
<a href="https://github.com/markmal/freeship-plus-in-lazarus/releases">https://github.com/markmal/freeship-plus-in-lazarus/releases</a>

FREE!ship Plus in Lazarus is further development of the FREE!ship Plus (by <a href="http://www.hydronship.net/" rel="nofollow">http://www.hydronship.net</a> ) 
Windows program with free source code FREE!ship v3.x under license GNU GPL. 
This FREE!ship Plus application is migrated into free open source Lazarus / Free Pascal environment 
to promote further development in various platforms and for various platforms (OS and architectures).

FREE!ship Plus is designed for the full parametric analysis of resistance and power prediction for 
a ship and other calculations of hydrodynamics of vessels and underwater vehicles. FREE!ship Plus 
allows the designer to simulate and analyze condition of balance of a complex completely hull - 
rudders - keels - engine - propellers in different regimes and of service conditions of a ship. 
The analyzable system includes a hull, appendages, a propeller and the engine (i.e. resistance, 
power, a thrust and a torque), and also various service conditions (heaving, a wind, a shallow-water 
effect, a regime of tow / pushing, etc...) refer to <a href="http://www.hydronship.net/" rel="nofollow">http://www.hydronship.net</a> for full description.
</pre></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tuition costs have risen 710% since 1983 (1200% since 1980). Inflation: 194% (679 pts)]]></title>
            <link>https://statecraft.beehiiv.com/p/student-loan-debt-forgiveness</link>
            <guid>36669253</guid>
            <pubDate>Mon, 10 Jul 2023 17:03:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://statecraft.beehiiv.com/p/student-loan-debt-forgiveness">https://statecraft.beehiiv.com/p/student-loan-debt-forgiveness</a>, See on <a href="https://news.ycombinator.com/item?id=36669253">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-blocks"><p><i><b>Quick updates before we begin: </b></i></p><p><i><b>New Perks for Subscribers! </b></i><i>Starting today, we’ve added the following perks: </i></p><div><ul><li><p><i>Enabled comment section (leave feedback or suggest a topic!)</i></p><ul><li><p><i>We’ve already received suggestions for AI and climate topics, and those are in the works!</i></p></li></ul></li><li><p><i>Sub-only posts</i></p></li><li><p><i>Access to our full archive</i></p></li><li><p><i>Early access to interactive web and video projects</i></p></li></ul></div><p><i>Lastly, Statecraft is now available on </i><a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL25ld3MuZ29vZ2xlLmNvbS9wdWJsaWNhdGlvbnMvQ0FBcUJ3Z0tNTGFjMFFzdzliZm9Bdz9jZWlkPUNBOmVuJm9jPTMiLCJwb3N0X2lkIjoiODU0YzFlMDgtOTQyMi00ZjZiLWE0ODItNTI3M2Y0YjM5ZDM5IiwicHVibGljYXRpb25faWQiOiJkMWI4MGM5My1lODI4LTRhOTQtOTlkNC1lYzgyYTE0MzAyOGEiLCJ2aXNpdF90b2tlbiI6IjQ3Njc2MDU4LTEwMzctNDY4MC05YjcwLTBjYjIxNjQyN2RiZSIsImlhdCI6MTY4OTAxMjAwMy4zODEsImlzcyI6Im9yY2hpZCJ9.YMZWkJXYt0PLse4X9xGH-asL9J3XH3LbYflfs0C5J1g" target="_blank" rel="noopener noreferrer nofollow"><i>Google News</i></a><i>! </i>🎉<i>&nbsp;</i></p><p> The outstanding student loan debt in the United States is <b>1.78 trillion dollars</b>. For reference, the GDPs of developed countries like South Korea, Australia, and oil-rich Saudi Arabia are less than $1.7T respectively. The explosion in tuition costs and associated debt accumulation is undeniably a looming economic crisis. It’s a crisis that requires a basket of policy solutions to transform higher education from the bottom up - including, yes, cancellation of some federal student loans. But while the fight to cancel student loan debt <a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy53aGl0ZWhvdXNlLmdvdi9icmllZmluZy1yb29tL3N0YXRlbWVudHMtcmVsZWFzZXMvMjAyMi8wOC8yNC9mYWN0LXNoZWV0LXByZXNpZGVudC1iaWRlbi1hbm5vdW5jZXMtc3R1ZGVudC1sb2FuLXJlbGllZi1mb3ItYm9ycm93ZXJzLXdoby1uZWVkLWl0LW1vc3QvP3V0bV9zb3VyY2U9c3RhdGVjcmFmdC5iZWVoaWl2LmNvbSZ1dG1fbWVkaXVtPXJlZmVycmFsJnV0bV9jYW1wYWlnbj13ZS13ZXJlbi10LXJlYWR5LWZvci1hLTcxMC1yaXNlLWluLXR1aXRpb24tY29zdHMiLCJwb3N0X2lkIjoiODU0YzFlMDgtOTQyMi00ZjZiLWE0ODItNTI3M2Y0YjM5ZDM5IiwicHVibGljYXRpb25faWQiOiJkMWI4MGM5My1lODI4LTRhOTQtOTlkNC1lYzgyYTE0MzAyOGEiLCJ2aXNpdF90b2tlbiI6IjQ3Njc2MDU4LTEwMzctNDY4MC05YjcwLTBjYjIxNjQyN2RiZSIsImlhdCI6MTY4OTAxMjAwMy4zODIsImlzcyI6Im9yY2hpZCJ9.TbC4ohDc8GWT4aCRNVhqLlvS81GLNaPsRD2whAgXK1E" target="_blank" rel="noopener noreferrer nofollow">continues</a>, we think it would be especially worthwhile to explore some data and highlight potential solutions for this debt burden. </p><p><h3>📈 How We Got Here</h3></p><p> Tuition cost and fee inflation have outpaced CPI Inflation 4-to-1 over the last 40 years: </p><div><p><img alt="" src="https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/33ae477a-f0ea-4d56-a664-710b346bf8cf/inflation_capture.png"></p><p><small><p>Animated versions of these charts on Threads (or Twitter): @theArmanMadani</p></small></p></div><p> The resulting $1.78T of debt includes $1.65T that is federally owned (93%). 45M Americans have federal student loan debt. Roughly half of all the debt belongs to individuals with graduate degrees while the other half belongs to individuals with undergraduate degrees; though it’s worth noting that there are fewer graduate students overall so the debt held per student is higher for graduates: </p><div><p><img alt="" src="https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/98cda4db-b39d-47e8-98f9-5dc3f95d832e/TotalDebtCapture.jpg"></p><p><small><p>Animated versions of these charts on Threads (or Twitter): @theArmanMadani</p></small></p></div><p><b>Student enrollment in higher ed institutions has </b><a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5zdGF0aXN0YS5jb20vc3RhdGlzdGljcy8xODM5OTUvdXMtY29sbGVnZS1lbnJvbGxtZW50LWFuZC1wcm9qZWN0aW9ucy1pbi1wdWJsaWMtYW5kLXByaXZhdGUtaW5zdGl0dXRpb25zLz91dG1fc291cmNlPXN0YXRlY3JhZnQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Utd2VyZW4tdC1yZWFkeS1mb3ItYS03MTAtcmlzZS1pbi10dWl0aW9uLWNvc3RzIiwicG9zdF9pZCI6Ijg1NGMxZTA4LTk0MjItNGY2Yi1hNDgyLTUyNzNmNGIzOWQzOSIsInB1YmxpY2F0aW9uX2lkIjoiZDFiODBjOTMtZTgyOC00YTk0LTk5ZDQtZWM4MmExNDMwMjhhIiwidmlzaXRfdG9rZW4iOiI0NzY3NjA1OC0xMDM3LTQ2ODAtOWI3MC0wY2IyMTY0MjdkYmUiLCJpYXQiOjE2ODkwMTIwMDMuMzgyLCJpc3MiOiJvcmNoaWQifQ.syv1ZnkHSR_7B0ialU7w22IbaGYPuInRtz-3KPUDXic" target="_blank" rel="noopener noreferrer nofollow"><b>risen consistently since 2000</b></a>. With higher demand for college degrees - including out-of-state demand for state universities - tuition has risen precipitously. Rising enrollment is also associated with a changing <a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5zdGF0aXN0YS5jb20vdG9waWNzLzc3MS9lbXBsb3ltZW50Lz91dG1fc291cmNlPXN0YXRlY3JhZnQuYmVlaGlpdi5jb20mdXRtX21lZGl1bT1yZWZlcnJhbCZ1dG1fY2FtcGFpZ249d2Utd2VyZW4tdC1yZWFkeS1mb3ItYS03MTAtcmlzZS1pbi10dWl0aW9uLWNvc3RzI3RvcGljT3ZlcnZpZXciLCJwb3N0X2lkIjoiODU0YzFlMDgtOTQyMi00ZjZiLWE0ODItNTI3M2Y0YjM5ZDM5IiwicHVibGljYXRpb25faWQiOiJkMWI4MGM5My1lODI4LTRhOTQtOTlkNC1lYzgyYTE0MzAyOGEiLCJ2aXNpdF90b2tlbiI6IjQ3Njc2MDU4LTEwMzctNDY4MC05YjcwLTBjYjIxNjQyN2RiZSIsImlhdCI6MTY4OTAxMjAwMy4zODIsImlzcyI6Im9yY2hpZCJ9.0u-HWi65MIPQbv9rF94ngToxvo3-H3xf_iibpMZwDIU" target="_blank" rel="noopener noreferrer nofollow">US labor profile</a>; for example, manufacturing jobs were eclipsed by “business and professional services” jobs as well as healthcare, education, and retail jobs. One troubling manifestation of the demand for higher degrees can be seen during the 2008-09 Recession as newly unemployed individuals scrambled for retraining. From 2008 through 2009, enrollment in private for-profit colleges rose disproportionately compared to private non-profit and public colleges. Private for-profit colleges have <a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL2VkdWNhdGlvbmRhdGEub3JnL3N0dWRlbnQtbG9hbi1kZWZhdWx0LXJhdGU_dXRtX3NvdXJjZT1zdGF0ZWNyYWZ0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlLXdlcmVuLXQtcmVhZHktZm9yLWEtNzEwLXJpc2UtaW4tdHVpdGlvbi1jb3N0cyIsInBvc3RfaWQiOiI4NTRjMWUwOC05NDIyLTRmNmItYTQ4Mi01MjczZjRiMzlkMzkiLCJwdWJsaWNhdGlvbl9pZCI6ImQxYjgwYzkzLWU4MjgtNGE5NC05OWQ0LWVjODJhMTQzMDI4YSIsInZpc2l0X3Rva2VuIjoiNDc2NzYwNTgtMTAzNy00NjgwLTliNzAtMGNiMjE2NDI3ZGJlIiwiaWF0IjoxNjg5MDEyMDAzLjM4MiwiaXNzIjoib3JjaGlkIn0.-8EXNxtP3mX6u68fZeH1Y6Dbz7SOxEOYlb5VcgJofFE" target="_blank" rel="noopener noreferrer nofollow">extraordinarily high rates of default</a>, near 16% of borrowers default within 3 years. </p><p><img alt="" src="https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/ccaaffe9-1806-4cae-8fad-4bbb1da00da6/image.png"></p><p> During this period of heightened demand, a more obvious inflationary effect took place: <b><a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5uZWEub3JnL2Fkdm9jYXRpbmctZm9yLWNoYW5nZS9uZXctZnJvbS1uZWEvc3RhdGUtZnVuZGluZy1oaWdoZXItZWR1Y2F0aW9uLXN0aWxsLWxhZ2dpbmc_dXRtX3NvdXJjZT1zdGF0ZWNyYWZ0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlLXdlcmVuLXQtcmVhZHktZm9yLWEtNzEwLXJpc2UtaW4tdHVpdGlvbi1jb3N0cyM6fjp0ZXh0PVdoZW4lMjBzdGF0ZSUyMGxhd21ha2VycyUyMHR1cm4lMjB0aGVpcixkb3duJTJDJTIwc3R1ZGVudCUyMHR1aXRpb24lMjBnb2VzJTIwdXAuIiwicG9zdF9pZCI6Ijg1NGMxZTA4LTk0MjItNGY2Yi1hNDgyLTUyNzNmNGIzOWQzOSIsInB1YmxpY2F0aW9uX2lkIjoiZDFiODBjOTMtZTgyOC00YTk0LTk5ZDQtZWM4MmExNDMwMjhhIiwidmlzaXRfdG9rZW4iOiI0NzY3NjA1OC0xMDM3LTQ2ODAtOWI3MC0wY2IyMTY0MjdkYmUiLCJpYXQiOjE2ODkwMTIwMDMuMzgyLCJpc3MiOiJvcmNoaWQifQ.VIfYccx1zSUkzhZU--9xiKvePMPUk1LdVSt0esoPxrI" target="_blank" rel="noopener noreferrer nofollow">state funds were cut</a></b><b> for public colleges on a per-student basis </b>in the run-up and fallout of the Recession. </p><p><h3>💸 What Now</h3></p><p> [WARNING: I include my opinion in this section, I look forward to a lively discussion in the comment section] </p><p> There are short and long-term solutions needed to address the debt problem. </p><p> Short-term, students need recourse. Defaulting on loans creates a cascade of consequences that necessarily harm borrowers earning potential for years (e.g. colleges can withholding proof of attendance, decline in credit score, wage garnishments, etc.) There are loans that are highly likely to default regardless after the COVID moratorium on payments ends. These loans should be forgiven/cancelled. The fallout of default en masse - especially amongst low and middle-income degree holders - would present a much greater cost to the economy than the cost of cancellation. Additionally, controlling what information can be reported to credit bureaus (like medical debt) and regulating what information colleges can withhold would also reduce negative income effects caused by default (e.g. a private for-profit institution cannot withhold proof of attendance in the event of default). </p><p> Long-term, controlling the underlying cost of higher ed itself is an imperative. Federal support for and expansion of programs like <a href="https://flight.beehiiv.net/v2/clicks/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJodHRwczovL3d3dy5jY2Njby5lZHUvQWJvdXQtVXMvQ2hhbmNlbGxvcnMtT2ZmaWNlL0RpdmlzaW9ucy9FZHVjYXRpb25hbC1TZXJ2aWNlcy1hbmQtU3VwcG9ydC9TdHVkZW50LVNlcnZpY2UvV2hhdC13ZS1kby9DYWxpZm9ybmlhLVByb21pc2U_dXRtX3NvdXJjZT1zdGF0ZWNyYWZ0LmJlZWhpaXYuY29tJnV0bV9tZWRpdW09cmVmZXJyYWwmdXRtX2NhbXBhaWduPXdlLXdlcmVuLXQtcmVhZHktZm9yLWEtNzEwLXJpc2UtaW4tdHVpdGlvbi1jb3N0cyIsInBvc3RfaWQiOiI4NTRjMWUwOC05NDIyLTRmNmItYTQ4Mi01MjczZjRiMzlkMzkiLCJwdWJsaWNhdGlvbl9pZCI6ImQxYjgwYzkzLWU4MjgtNGE5NC05OWQ0LWVjODJhMTQzMDI4YSIsInZpc2l0X3Rva2VuIjoiNDc2NzYwNTgtMTAzNy00NjgwLTliNzAtMGNiMjE2NDI3ZGJlIiwiaWF0IjoxNjg5MDEyMDAzLjM4MiwiaXNzIjoib3JjaGlkIn0.46i8nRok0CXsqfgPv_peIiPNn1SIoVbsi1ofW1vypas" target="_blank" rel="noopener noreferrer nofollow">The California Promise</a> - which provides free tuition for California residents who attend CA community colleges - would eliminate the burden for some new graduates. Apportioning more existing or raising new tax revenue to invest in underfunded private colleges (e.g. HBCUs) and public colleges can reduce tuition costs by supplanting/exceeding the aforementioned state funding cuts as well. </p><p> This is a complex issue which requires comprehensive policy solutions. But one of the reasons student debt has received the attention it has is because of the opportunity it presents. An opportunity to extend a stimulus to low/middle income earners in the short-term and transform the higher education system in the long-term. </p><p><i>Thank you for reading this article! If you enjoyed it, consider sharing Statecraft with a friend. You can also connect with us on Threads, Twitter or Instagram. We’re working on our first long-form video content as well, so if you want to see what we produce, head over to YouTube or TikTok. Links below:</i></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Invisible Details of Interaction Design (142 pts)]]></title>
            <link>https://rauno.me/craft/interaction-design</link>
            <guid>36669249</guid>
            <pubDate>Mon, 10 Jul 2023 17:02:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rauno.me/craft/interaction-design">https://rauno.me/craft/interaction-design</a>, See on <a href="https://news.ycombinator.com/item?id=36669249">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-stage=""><p>Design can feel like there's no science to it — only feel and intuition. Even researchers have trouble grounding interaction design practices in science, inherently treating them as a mysterious black box. <sup>1</sup> While from my own experience that's partly true, I have been trying to deconstruct and dig out the <i>why</i> behind great displays of interaction design.</p><p>Searching the Internet for depth on interaction design yields a plethora of recycled content obsessing over user personas, storyboards, and Venn diagrams labeled with "UI" and "UX". Besides a few exceptional talks, actual substance and insight reveal themselves to those willing to fanatically dig for them. Either through studying obscure, long-winded research papers or by maniacally replaying hundreds of slow motion screen recordings.</p><p>Sitting down and just thinking hard does not magically produce valuable discoveries either. The essence of the word "interaction" implies a relationship between a human and an environment. In my experience, great revelations surface from making something — filling your headspace with a problem — and then going for a synthesising daydreaming walk to stir the pot.</p><p>This writing is not a tutorial nor a collection of guidelines. But rather an observation on the invisible details of a few interactions that I use often but rarely think about. Besides recreating interfaces, I found this exercise in reflection to be another great way to build a stronger design intuition and vocabulary.</p><h3 data-heading="true" id="metaphors">Metaphors</h3><p>What even is interaction design? Here's how I think about it through the lens of technology. Interaction design is an artform to make experiences that fluidly respond to human intent. When does a swipe trigger an action? Do gestures retain momentum? What happens if a finger is covering content? How can we predict intent based on context? Executing well on details like these make products feel like a natural extension of ourselves.<!-- --> <sup>2</sup></p><p>But it's not an artform in the same way as painting or composing music. There's a unique human component to interaction design. Why? Because ultimately people are trying to get stuff done using a product. Beauty in form and composition is not enough. There's an inherent satisfaction apparent in striking a holistic balance between form <i>and </i>function.</p><p>Great interaction design rewards learning by reusing metaphors. You can use most touch interfaces with just two gestures: tapping and swiping. For example, on iOS the only gesture you're explicitly taught how to do is swiping up to open:</p><p>Now you've learned swiping which unlocks control over many other parts of the interface. The sliding motion also tells you that the interface is composed of stacked layers, like a deck of cards. Knowing so, you might be enthused to try swiping down on the screen to discover more controls. Conceptually, the interface is further implicitly teaching you that swiping <i>down</i> reveals layers of system functionality. This knowledge compounds as you delve deeper into the Apple ecosystem.</p><p>We can stretch interaction design metaphors even further. Why does swiping horizontally navigate between pages? Because that's how we've intuitively interfaced with books for thousands of years.<!-- --> </p><p>Great interactions are modeled after properties from the real world, like interruptability. This sounds kinda silly because, duh, obviously flipping a page in a book is interruptible. But imagine if it were an animation that you had to wait for!</p><p>Pinching is another gesture that we've come to intuitively associate with zooming. Simply put, zooming is an act of precision — adjusting the amount of detail visible.</p><p>We can think of pinching akin to movements that require intricate motor skills, like picking up tiny objects or working with spices. Naturally, we pinch our fingers for higher precision:</p><p>Notice how pinching on a touch screen works in reverse. You start with your fingers together, and then move them apart for precision. But to grab something, you bring fingers together from afar. This is because the interface needs to first establish an anchor point from where the zooming originates.</p><p><img alt="Two app icons are in focus, both of them have the bottom 10 points duplicated and stretched for an elongated effect." loading="lazy" width="1920" height="1400" decoding="async" data-nimg="1" srcset="https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fpinch-anchor.09bc6fa5.png&amp;w=1920&amp;q=75 1x, https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fpinch-anchor.09bc6fa5.png&amp;w=3840&amp;q=75 2x" src="https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fpinch-anchor.09bc6fa5.png&amp;w=3840&amp;q=75"></p><p>Scientifically or intuitively, there are hundreds of design decisions made by someone obsessesing over the tiniest margins so that when they work, no one has to think about. And many of them tap into our instinctive behaviors.</p><h3 data-heading="true" id="kinetic-physics">Kinetic Physics</h3><p>The lock screen sliding up establishes that, in essence, it's just an overlay that can be dismissed by swiping up, and within that framing so is an app. That means you also now know how to dismiss an application.</p><p>Let's take a look at how dismissing an app morphs into the Dynamic Island. Notice how the gesture retains the momentum and angle at which it was thrown. It's never perfectly centered or consistent in timing.</p><p>This movement builds on our natural sense of physics from the real world, like how swiping a playing card would feel. Although the movement of the playing card exhibits less bounce since it's conceptually lighter and does not magnetically morph into something.<!-- --> </p><h3 data-heading="true" id="swipe-gestures">Swipe Gestures</h3><p>When does a swipe trigger an action? It seems trivial: you press down, move a little, and then finally trigger an action <i>after</i> releasing the finger. After building a few touch interactions myself using SwiftUI, I realised that might not always be the case. Sometimes we expect the action to be triggered <i>whilst</i> <!-- -->swiping.</p><p>Lightweight actions, such as displaying overlays, feel more natural to trigger during the swipe after an arbitrary amount of distance. For example, with a single gesture, I can immediately grok the overlaying surface, understand that it gives me a search input, and then dismiss if it's not what I want. Waiting for the gesture to end would feel like a drag here.</p><p>Here's an example from the MercuryOS SwiftUI prototype I was working on. It feels expected to trigger an action when elements moving during the gesture reach their logical, final position. Notice how the screen is unlocked after both the titles snap to their position, and then locked with a single gesture without releasing the finger. Again, waiting for the gesture to end before unlocking would make the interface feel broken and provide less affordance.</p><p>Now, let's look at examples where triggering an action requires explicit intent. The iOS App Switcher will never dismiss an app before the gesture ends. No matter the distance or the fact that the app is partially off-screen:</p><p>This makes sense to me because dismissing an app is destructive, and it wouldn't feel nice if the app were to dismiss in the middle of the swipe. What if I were change my mind half-way through and accidentally reached the threshold for dismissing? I could potentially lose some important progress in an app! To make sure the interface responds to intent, triggering on gesture end, regardless of distance, feels right here.</p><p>Here's another example where despite swiping an adequate amount of distance for the view to be fully visible and snap, it doesn't until the gesture ends. This makes it lightweight to briefly peek at another screen when scanning for an app, without committing to it, and quickly interrupt the gesture by changing direction.</p><h3 data-heading="true" id="responsive-gestures">Responsive Gestures</h3><p>Truly fluid gestures are immediately responsive. As mentioned above, gestures can have an explicit trigger threshold, but this does not mean simply performing an animation 0 → 1 would feel great.</p><p>For example, a naive implementation for pinching a card would exponentially zoom in after a certain threshold:</p><p>Pinching an adequate amount to animate would not feel exactly broken here. But the interface gives zero affordance or confidence that the card is even pinchable with a lower velocity. Neither does this feel satisfying to perform.</p><p>It feels a lot better by feeling the scale delta applying immediately, and then performing an animation past a given threshold:</p><p>For some reason navigating iOS Settings does not feel as responsive as the App Switcher. A layer slides over from the right which tells me that it can be dismissed by swiping left. But if you happen to mistap, then swiping back immediately does not interrupt the animation. You have to wait for it to end.</p><h3 data-heading="true" id="spatial-consistency">Spatial Consistency</h3><p>The Dynamic Island has this nice interaction where on tap the application slides out under from the Island to cover the screen:</p><p>But if the Island is <i>expanded</i> which conceptually tells the interface my intent is to receive <i>more</i> <!-- -->detail, the application does not slide out from the Island. Instead, it launches from the icon, if its visible. Alternatively, the application slides in from the right:</p><p>I can only assume that by launching Spotify from the icon, it is a lot more clear where the audio is playing from. Say you had three music apps on the same row. Through motion this helps establish a relationship between the audio player and its source.</p><p>Similarly, if the app slides in from the right, it communicates where it is spatially — in the App Switcher. By moving in from the right, not left, it also signifies that the app is now first on the stack of apps in the switcher.</p><p>However, the native Clock app will never open from its icon. It always jumps out from the Island, even when expanded:</p><p>This seems to support the theory above. Because the Island timer module is only specific to one app, and there can't be another with the same Island, there's no need to make it clear where it's from.</p><h3 data-heading="true" id="fluid-morphing">Fluid Morphing</h3><p>We're all familiar with the beautifully fluid, interruptible gestures of iOS to quickly navigate apps. Swiping up morphs the full screen app into its icon:</p><p>A curious detail here is that the icon is intentionally stretched from the bottom to fill the frame as it fludily morphs its shape from a vertical rectangle to a uniform square. It's a tiny bit more obvious what happens when looking at the non-standard GitHub icon:</p><p><img alt="Two app icons are in focus, both of them have the bottom 10 points duplicated and stretched for an elongated effect." loading="lazy" width="1920" height="1080" decoding="async" data-nimg="1" srcset="https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fiphone-close-crop.0334862c.png&amp;w=1920&amp;q=75 1x, https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fiphone-close-crop.0334862c.png&amp;w=3840&amp;q=75 2x" src="https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fiphone-close-crop.0334862c.png&amp;w=3840&amp;q=75"></p><p>This technique does assume that app icons adhere to the guidelines outlined by Apple. The Bluesky icon ignores the recommended safe zone and as a result the bottom ~10pt of the icon is cropped, duplicated, and stretched, resulting in this weird repeating image effect:</p><p><img alt="The Bluesky app icon is in focus. It is an image of a blue sky with clouds. The bottom 10 points are duplicated and stretched, but because its an image, the result is an odd stretching image effect." loading="lazy" width="1920" height="1080" decoding="async" data-nimg="1" srcset="https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fiphone-close-bsky.720d6ba8.png&amp;w=1920&amp;q=75 1x, https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fiphone-close-bsky.720d6ba8.png&amp;w=3840&amp;q=75 2x" src="https://rauno.me/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fiphone-close-bsky.720d6ba8.png&amp;w=3840&amp;q=75"></p><p>In practice this does not feel completely off, but definitely not as great:</p><h3 data-heading="true" id="frequency-novelty">Frequency &amp; Novelty</h3><p>As a designer, I love to animate everything. Object permanence, creating a focal point, and delight are all good reasons for doing so. But it's not so obvious when <i>not</i> to animate something.</p><p>Sometimes we can get away with not animating mouse or keyboard interactions, without it feeling jarring. There is an inherent disconnect between input from peripheral devices and what happens on the screen. Pressing a key feels less visceral, and more mechanical than touching the screen.</p><p>A good example for this would be command menus. It's tempting to throw an opacity and scale fade on the overlay.<!-- --> <!-- -->But if we for a moment consider the interaction frequency being hundreds of times a day, it does start to feel more like cognitive burden after seeing the same animation for the hundredth time. <sup>3</sup></p><p>When so commonly executed, the interaction novelty is also diminished. It doesn't feel like you're doing anything peculiar, deserving of a special flourish.</p><p>A case in point: I was working on a bookmarking tool (<a href="https://bmrks.com/" target="_blank" rel="noopener noreferrer">bmrks.com</a>) and intuitively felt great about animating the active indicator and elements being added and removed from the list:</p><p>After a couple of days they began to feel sluggish. Despite making the motion even snappier, my perceived performance was making me feel like I have to wait too much when interfacing with the keyboard. I removed motion from core interactions and suddenly felt like I was moving much faster:</p><p>Context (right-click) menus on macOS also appear without motion. Used thousands of times a day, with very low novelty and high frequency. Despite being a mouse interaction, it feels right to not animate the menu appearing:</p><p>Interestingly enough, the menu subtly fades out. On closer inspection, the selected item briefly blinks the accent color (pink) to provide assurance that the element was successfully selected. I can only assume that the menu fading out makes this feel more graceful and intentional than abruptly disappearing after the blink:</p><p>Another good example is the App Switcher on macOS which gets a lot of mileage for heavy keyboard users. The overlay never animates which makes moving between apps feel snappy:<!-- --> </p><p>Furthermore, if the time delta between pressing Command and Tab is low enough, the previously active window receives focus immediately without showing the menu:<!-- --> </p><h3 data-heading="true" id="fidgetability">Fidgetability</h3><p>Wonderful interactions don't have to be entirely practical. We've all been in math class, either biting our lips or repetitively clicking a pencil while crunching numbers. Behaviors like this are considered fidgeting. In other words, repetitive movements that apparently help release situational stress, or even enhance concentration. Although there is no scientific research that supports this claim <sup>4</sup>, fidgeting does regardless feel like a part of intentional interaction design.</p><p>Fidgetability could also be an after-thought, or a happy side-effect. However, the AirPods case is uncannily satisfying to play with. Assuming it to be a coincidence would be very generous.</p><p>Apple Pencil is a more obvious candidate to intentionally design to be fidgetable. The tip of the pencil is unscrewable which means it can be replaced eventually. Oddly enough, twisting the tip and rotating the pencil body provides satisfying friction to casually play with while thinking.</p><p>Now here's a crazy one that I would not bet my money on being intentional. Although it is dope.</p><h3 data-heading="true" id="scroll-landmarks">Scroll Landmarks</h3><p>On macOS you can always find the pointer by shaking the mouse. This interaction feels wonderful because it taps into the frustration and natural reaction that people feel when losing track of the pointer.</p><p>Something similar quite often happens to me on mobile when browsing long-form content. I've scrolled down half-way, and while reading I want to quickly recall something from above. But then I feel awkward scrolling back up because I will lose my precious scroll position and reading progress.</p><p>I made a tiny prototype where double tapping the scrollbar will place a landmark for the current scroll position. Now I could freely navigate around the page and double tap the landmark to get back to where I was before.</p><p>This feels familiar to use because the scrollbar is already interactive on touch. If you didn't know, long-pressing the scrollbar would make it draggable which is much faster to scroll quickly.</p><p>This reminds me of an old minimap prototype I made. Inspired by games where you always have a bird's eye view of the surrounding environment. Why not have a similar heads up display for navigating a page?</p><h3 data-heading="true" id="touch-content-visiblity">Touch Content Visiblity</h3><p>On touch interfaces, sometimes a finger might obfuscate what's happening on the screen which makes it hard to perform gestures at pixel-level precision. Commonly, the interface would then render a temporary representation of what's underneath the finger.</p><p>For example, on iOS when pressing down and dragging to move the text caret, a magnifying loupe will appear above the touch point. However, whenever the finger moves downwards and no longer covers the caret, the loupe disappears.</p><p>A similar detail is used for the keyboard. Pressing a key will show an enlarged key which gives you confidence that the interface understood what you meant.</p><p>It doesn't always make sense to mirror the obfuscated region. For example, sliders can be tiny and disappear under the touch of the thumb. It helps to ensure that the dragging gesture does not cancel when moving away from the slider and still pressing down:</p><p>Although seeking video is mostly a visual interaction, there's an unintelligible level of discomfort apparent when interacting with an element that you can't see.</p><p>Here's a more obvious example where it's critical to understand contents of the menu: </p><h3 data-heading="true" id="implicit-input">Implicit Input</h3><p>Forever long we've been peeling back the layers between humans and computing. Touch input elevated the relationship by introducing gestures and haptics. Soon applications will no longer be bound by the constraints of a fixed screen.</p><p>The keyboard, mouse, touch, voice are all explicit inputs. They feel like a natural extension of ourselves<!-- --> <sup>2</sup> when dialed into perfection. But isn't the mother of all inputs no input at all? When an interface makes use of context as input and can infer what you're trying to do without asking, it truly feels magical.</p><p>For example, by looking at the screen, Apple Maps will show the active route navigation without unlocking. Apple Wallet will increase the brightness when presenting a pass for scanning. Spotify will adjust the interface to be more accessible while driving.</p><p>Some custom iOS apps will blur the contents of the app when opening the App Switcher. At first, I figured it was just a performance optimization. But it turns out that it's a deliberate attempt to conceal possibly sensitive data, like medical records or a bank statement.</p><h3 data-heading="true" id="fittss-law">Fitts's Law</h3><p>Fitts's Law states that the time to click on something depends on distance and size. <sup>5</sup> The bigger the target, and the closer it is to where your pointer is, the better.</p><p>Operating systems make use of "magic corners" on the edges of the screen because the target area is infinitely large. <sup>6</sup> For example, on macOS, you can configure what happens when the pointer moves to a corner. You could show the Launchpad from the top-left corner:</p><p>The target size is infinite because the pointer can't overshoot past the corner, so the precision required for this interaction is very low. Reaching for any corner becomes a quick flick of the mouse. This is also why operating systems place commonly used menus, like the Apple menu, in corners.</p><p>Radial menus are an exemplary case of Fitts's Law. They spawn around the pointer making the size and distance towards any target the same for all actions. Over time, muscle memory will kick in and even make it possible to select an action based purely on distance and direction.</p><p>Here's a radial menu you can try:</p><div><p>Hold and rotate from anywhere</p></div><h3 data-heading="true" id="scrolling">Scrolling</h3><p>On most operating systems, you can scroll any scrollable region, even if the window itself is not active. This is great, except when another window scrolls unintentionally.</p><p>With the Magic Mouse I can scroll on a window, then move the pointer over a second window to click or find something, and the scroll events will not register on the second window. This feels great to me.</p><p>However, with any traditional mouse, like the Logitech MX Master 3, the scrolling on the first window is cancelled and hijacked by the second window. And it's really frustrating when this happens daily:</p><p>With the Magic Mouse, scrolling is cancelled explicitly by focusing another window:</p><p>Pointing devices like the Magic Trackpad and Magic Mouse also unlock direct manipulation for desktop computing. Besides the obvious ones like swiping between apps, it's also possible to directly manipulate sliders by scrolling, all with a single interaction:</p><h3 data-heading="true" id="closing-thoughts">Closing Thoughts</h3><p>For me, understanding and articulating why something feels right does not come as intuitively as<!-- --> <i>designing</i> something to feel right. But they are two sides of the same coin. There must be a reason. It can be as simple as a particular spring curve or something more innate, like metaphors. Analyzing and making sense of design details beyond just "it feels nice" helps nurture taste, amplify level of execution, and grow appreciation for how hard the pursuit of excellence is.</p><h3 data-heading="true" id="acknowledgments">Acknowledgments</h3><p>Thanks to <a href="https://twitter.com/pacocoursey" target="_blank" rel="noopener noreferrer">Paco</a>,<!-- --> <a href="https://twitter.com/almonk" target="_blank" rel="noopener noreferrer">Alasdair</a>,<!-- --> <a href="https://twitter.com/emilkowalski_" target="_blank" rel="noopener noreferrer">Emil</a>,<!-- --> <a href="https://twitter.com/thomaspaulmann" target="_blank" rel="noopener noreferrer">Thomas</a> for reading early drafts and their insights and feedback.</p><p>No artificial intelligence was used to generate content for this article.</p><h3 data-heading="true" id="resources">Resources</h3><ol><li><a href="https://summit.sfu.ca/_flysystem/fedora/sfu_migrate/15215/2011_CHI_Understanding_Goodman_vy-edited.pdf" target="_blank" rel="noopener noreferrer">E. Goodman, E. Stolterman, R. Wakkary. Understanding Interaction Design Practices (2011)</a></li><li><a href="https://developer.apple.com/videos/play/wwdc2018/803/" target="_blank" rel="noopener noreferrer">C. Karunamuni, N. Vries, M. Alonso. Designing Fluid Interfaces (2018)</a></li><li><a href="https://brandur.org/interfaces" target="_blank" rel="noopener noreferrer">Brandur. Learning From Terminals to Design the Future of User Interfaces (2017)</a></li><li><a href="https://digscholarship.unco.edu/cgi/viewcontent.cgi?article=1669&amp;context=dissertations" target="_blank" rel="noopener noreferrer">S. L. Kriescher. The Effects of Fidgets on Attention and Learning of College Students (2020)</a></li><li><a href="https://en.wikipedia.org/wiki/Fitts%27s_law" target="_blank" rel="noopener noreferrer">Paul Morris Fitts. The information capacity of the human motor system in controlling the amplitude of movement (1954)</a></li><li><a href="http://www.particletree.com/features/visualizing-fittss-law/" target="_blank" rel="noopener noreferrer">Kevin Hale. Visualizing Fitts's Law (2010)</a></li><li><a href="https://andymatuschak.org/files/papers/Apple%20Human%20Interface%20Guidelines%201987.pdf" target="_blank" rel="noopener noreferrer">Apple Human Interface Guidelines (1987)</a></li><li><a href="https://www.youtube.com/watch?v=76b3c_ssyPQ&amp;ab_channel=Figma" target="_blank" rel="noopener noreferrer">Rasmus Andersson. The curious case of user interfaces (2023)</a></li><li><a href="https://museapp.com/podcast/17-rethink-the-os/" target="_blank" rel="noopener noreferrer">Metamuse. Rethink the OS with Jason Yuan (2020)</a></li><li><a href="https://www.mercuryos.com/" target="_blank" rel="noopener noreferrer">Jason Yuan. MercuryOS (2019)</a></li><li><a href="http://paulgraham.com/greatwork.html#f2n" target="_blank" rel="noopener noreferrer">Paul Graham. How to Do Great Work (2023)</a></li><li><a href="https://brianlovin.com/app-dissection" target="_blank" rel="noopener noreferrer">App Dissection. Brian Lovin</a></li></ol></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lima: A nice way to run Linux VMs on Mac (444 pts)]]></title>
            <link>https://jvns.ca/blog/2023/07/10/lima--a-nice-way-to-run-linux-vms-on-mac/</link>
            <guid>36668964</guid>
            <pubDate>Mon, 10 Jul 2023 16:44:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jvns.ca/blog/2023/07/10/lima--a-nice-way-to-run-linux-vms-on-mac/">https://jvns.ca/blog/2023/07/10/lima--a-nice-way-to-run-linux-vms-on-mac/</a>, See on <a href="https://news.ycombinator.com/item?id=36668964">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
     

<p>Hello! Here’s a new entry in the “cool software julia likes” section.</p>

<p>A little while ago I started using a Mac, and one of my biggest
frustrations with it is that often I need to run Linux-specific software. For
example, the <a href="https://jvns.ca/blog/2021/09/24/new-tool--an-nginx-playground/">nginx playground</a> I
posted about the other day only works on Linux because it uses Linux namespaces (via <code>bubblewrap</code>)
to sandbox nginx. And I’m working on another playground right now that uses bubblewrap too.</p>

<p>This post is very short, it’s just to say that Lima seems nice and much simpler
to get started with than Vagrant.</p>

<h3 id="enter-lima">enter Lima!</h3>

<p>I was complaining about this to a friend, and they mentioned
<a href="https://lima-vm.io/">Lima</a>, which stands for <strong>Li</strong>nux on <strong>Ma</strong>c. I’d heard
of <a href="https://github.com/abiosoft/colima">colima</a> (another way to run Linux
containers on Mac), but I hadn’t realized that Lima also just lets you run VMs.</p>

<p>It was surprisingly simple to set up. I just had to:</p>

<ol>
<li>Install Lima (I did <code>nix-env -iA nixpkgs.lima</code> but you can also install it with <code>brew install lima</code>)</li>
<li>Run <code>limactl start default</code> to start the VM</li>
<li>Run <code>lima</code> to get a shell</li>
</ol>

<p>That’s it! By default it mounts your home directory as read-only inside the VM</p>

<p>There’s a config file in <code>~/.lima/default/lima.yaml</code>, but I haven’t needed to change it yet.</p>

<h3 id="some-nice-things-about-lima">some nice things about Lima</h3>

<p>Some things I appreciate about Lima (as opposed to Vagrant which I’ve used in the past and found kind of frustrating) are:</p>

<ol>
<li>it provides a default config</li>
<li>it automatically downloads a Ubuntu 22.04 image to use in the VM (which is what I would have probably picked anyway)</li>
<li>it mounts my entire home directory inside the VM, which I really like as a default choice (it feels very seamless)</li>
</ol>

<p>I think the paradigm of “I have a single chaotic global Linux VM which I use
for all my projects” might work better for me than super carefully configured
per-project VMs. Though I’m sure that you can have carefully configured
per-project VMs with Lima too if you want, I’m just only using the <code>default</code> VM.</p>

<h3 id="problem-i-don-t-know-how-to-mount-directories-read-write">problem: I don’t know how to mount directories read-write</h3>

<p>I wanted to have my entire home directory mounted read-only, but have some
subdirectories (like <code>~/work/nginx-playground</code>) mounted read-write. I did some
research and here’s what I found:</p>

<ul>
<li>a comment on <a href="https://github.com/lima-vm/lima/issues/873">this github issue</a> says that you can use <a href="https://github.com/lima-vm/lima/blob/master/docs/vmtype.md#vz">mountType: “virtiofs” and vmType: “vz”</a> to mount subdirectories of your home directory read-write</li>
<li>the Lima version packaged in nix 23.05 doesn’t seem to support <code>vmType: vz</code> (though I could be wrong about this)</li>
</ul>

<p>Maybe I’ll figure out how to mount directories read-write later, I’m not too
bothered by working around it for now.</p>

<h3 id="why-not-use-containers">why not use containers?</h3>

<p>I wanted a VM and not a Linux container because:</p>

<ol>
<li>the playground runs on a VM in production, not in a container, and generally
it’s easier to develop in a similar environment to production</li>
<li>all of my playgrounds use Linux namespaces, and I don’t know how to create a
namespace inside a container. Probably you can but I don’t feel like
figuring it out and it seems like an unnecessary distraction.</li>
<li>on Mac you need to run containers inside a Linux VM anyway, so I’d rather
use a VM directly and not introduce another unnecessary layer</li>
</ol>

<h3 id="that-s-all">that’s all!</h3>

<p>Some other notes:</p>

<ul>
<li>It looks like Lima works on Linux too</li>
<li>a bunch of people on Mastodon also said <a href="https://github.com/abiosoft/colima">colima</a> (built on top of Lima) is a nice Docker alternative on Mac for running Linux containers</li>
<li>there’s also an new alternative Linux container runtime for Mac called <a href="https://docs.orbstack.dev/">Orb Stack</a>, which I know nothing about</li>
</ul>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple VisionOS Simulator streaming wirelessly to Meta Quest headset (385 pts)]]></title>
            <link>https://github.com/zhuowei/VisionOSStereoScreenshots/tree/alvr</link>
            <guid>36668732</guid>
            <pubDate>Mon, 10 Jul 2023 16:30:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/zhuowei/VisionOSStereoScreenshots/tree/alvr">https://github.com/zhuowei/VisionOSStereoScreenshots/tree/alvr</a>, See on <a href="https://news.ycombinator.com/item?id=36668732">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">visionOS Simulator to ALVR / Meta Quest wireless streaming</h2>
<p dir="auto">Streams the visionOS Simulator to a Meta Quest wirelessly with <a href="https://github.com/alvr-org/ALVR">ALVR</a> installed.</p>
<p dir="auto">Tested with Xcode 15 beta 2 / macOS 14 beta 2 on Apple Silicon, Meta Quest (original).</p>
<h3 tabindex="-1" dir="auto">Usage</h3>
<ol dir="auto">
<li>
<p dir="auto">First, sideload <a href="https://github.com/alvr-org/ALVR-nightly/releases/tag/v21.0.0-dev00%2Bnightly.2023.07.06">ALVR Nightly 2023.07.06</a> onto your Meta Quest.</p>
<p dir="auto">(This does not currently work with stable ALVR)</p>
</li>
<li>
<p dir="auto">Start the visionOS Simulator from Xcode.</p>
</li>
<li>
<p dir="auto">Download and extract <a href="https://github.com/zhuowei/VisionOSStereoScreenshots/releases">alvr_visionos_streaming.zip</a>.</p>
</li>
<li>
<p dir="auto">Inject the streaming library into the Simulator:</p>
<div data-snippet-clipboard-copy-content="./inject.sh
# this resprings the simulator"><pre><code>./inject.sh
# this resprings the simulator
</code></pre></div>
</li>
<li>
<p dir="auto">Open ALVR on your Meta Quest: if all goes well, the visionOS interface should stream into your headset.</p>
</li>
<li>
<p dir="auto">To configure streaming settings, you can use the ALVR dashboard (./alvr_dashboard). See <a href="https://github.com/alvr-org/ALVR">ALVR's documentation</a> for more info.</p>
</li>
<li>
<p dir="auto">You can't control the Simulator using the Quest's controllers yet (I'm looking into it).</p>
<p dir="auto">For now, use the computer's mouse/keyboard to control the Simulator.</p>
<p dir="auto">You probably want to enable a visible mouse cursor inside the Simulator (Settings -&gt; Accessibility -&gt; Pointer Control)</p>
</li>
</ol>
<h3 tabindex="-1" dir="auto">How it works</h3>
<p dir="auto">This hooks CompositorService APIs inside backboardd so that it renders to our own textures instead of to the simulator screen. We then pass these textures to ALVR's server, which encodes them and streams them to the headset.</p>
<h3 tabindex="-1" dir="auto">What's next</h3>
<ul dir="auto">
<li>Enable passthrough</li>
<li>Hook up Quest controllers / eye gaze?</li>
</ul>
<h3 tabindex="-1" dir="auto">Credits</h3>
<p dir="auto">Thank you so much to <a href="https://mastodon.social/@ShinyQuagsire" rel="nofollow">@ShinyQuagsire</a>: he <a href="https://mastodon.social/@ShinyQuagsire/110670442474420349" rel="nofollow">released</a> the <a href="https://github.com/shinyquagsire23/XRGyroControls_OpenXR">first ever tool</a> for streaming the visionOS Simulator to a Quest headset (via wired Quest Link), and helped me figure out how to port this to work wirelessly using ALVR.</p>
<p dir="auto">Thanks to <a href="https://infosec.exchange/@jjtech" rel="nofollow">@JJTech</a> and <a href="https://mastodon.social/@keithahern" rel="nofollow">@keithahern</a> for figuring out how the visionOS Simulator handles input.</p>
<p dir="auto">Thanks to <a href="https://github.com/alvr-org/ALVR">the ALVR developers</a> for making an amazing cross-platform VR streaming system.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Study: 87% of classic video games are not legally available (476 pts)]]></title>
            <link>https://gamehistory.org/87percent/</link>
            <guid>36668472</guid>
            <pubDate>Mon, 10 Jul 2023 16:15:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gamehistory.org/87percent/">https://gamehistory.org/87percent/</a>, See on <a href="https://news.ycombinator.com/item?id=36668472">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-content" role="main">

	
<article id="post-24002">
	
	<!-- .cover-header -->

	<div id="post-inner">

		
<p>The Video Game History Foundation, in partnership with the Software Preservation Network, has conducted <a href="https://doi.org/10.5281/zenodo.7996492">the first ever study</a> on the commercial availability of classic video games, and the results are bleak. <strong>87% of classic video games released in the United States are critically endangered</strong>.</p>



<p>Imagine if the only way to watch <em>Titanic</em> was to find a used VHS tape, and maintain your own vintage equipment so that you could still watch it. And what if no library, not even the Library of Congress, could do any better — they could keep and digitize that VHS of <em>Titanic</em>, but you’d have to go all the way there to watch it. It sounds crazy, but that’s the reality we live in with video games, a $180 billion industry, while the games and their history disappear. </p>



<div><figure><img decoding="async" width="1024" height="1024" src="https://gamehistory.org/wp-content/uploads/2023/07/8-1-1024x1024.png" alt="" srcset="https://gamehistory.org/wp-content/uploads/2023/07/8-1-1024x1024.png 1024w, https://gamehistory.org/wp-content/uploads/2023/07/8-1-300x300.png 300w, https://gamehistory.org/wp-content/uploads/2023/07/8-1-740x740.png 740w, https://gamehistory.org/wp-content/uploads/2023/07/8-1-768x768.png 768w, https://gamehistory.org/wp-content/uploads/2023/07/8-1.png 1080w" sizes="(max-width: 1024px) 100vw, 1024px"></figure><div>
<p>Just 13% of video game history is being represented in the current marketplace. In fact, no period of video game history defined in this study even cracked <em>20% representation</em>.</p>



<p>Figure 1: Availability rate of historical games, by period, between 1960 and 2009. (n = 1500, ±2.5%, 95% CI) Curious about our methodology? Check out our <a href="https://gamehistory.org/study-explainer">study explainer blog pos</a><a href="https://gamehistory.org/?p=24007">t</a>!</p>
</div></div>



<p>For accessing <em>nearly 9 in 10</em> classic games, there are few options: seek out and maintain vintage collectible games and hardware, travel across the country to visit a library, or… piracy. None of those options are desirable, which means that most video games are inaccessible to all but the most diehard and dedicated fans. That’s pretty grim!</p>



<p>This is where libraries and archives&nbsp;<em>should</em>&nbsp;come in. Anyone&nbsp;<em>should</em>&nbsp;be able to easily explore, research and play classic video games, in the same way that they can read classic novels, listen to classic albums, and watch classic movies. But outdated copyright laws are preventing institutions like ours from doing our jobs.</p>


<div>
<figure><img decoding="async" loading="lazy" src="https://gamehistory.org/wp-content/uploads/2023/07/Historical-Games-Simple.png" alt="" width="454" height="454" srcset="https://gamehistory.org/wp-content/uploads/2023/07/Historical-Games-Simple.png 1080w, https://gamehistory.org/wp-content/uploads/2023/07/Historical-Games-Simple-300x300.png 300w, https://gamehistory.org/wp-content/uploads/2023/07/Historical-Games-Simple-1024x1024.png 1024w, https://gamehistory.org/wp-content/uploads/2023/07/Historical-Games-Simple-740x740.png 740w, https://gamehistory.org/wp-content/uploads/2023/07/Historical-Games-Simple-768x768.png 768w" sizes="(max-width: 454px) 100vw, 454px"><figcaption><em>Availability rate of historical games. (</em>n<em> = 1500, ±2.5%, 95% CI)</em><br><em>Random list of games pulled from MobyGames</em>.</figcaption></figure></div>


<p><strong>Goal of this study:</strong> Get expanded exemptions for libraries and organizations preserving video games, which are currently far more limited than their ability to preserve books, movies, audio, etc.</p>



<p><strong>How this study helps:</strong> The <a href="https://arstechnica.com/gaming/2023/03/why-game-archivists-are-dreading-this-months-3ds-wii-u-eshop-shutdown/">video game industry’s main lobbying group</a> has successfully argued to the US Copyright Office that the industry already does enough to preserve its own history commercially, and that additional protections for preservation institutions would hurt their bottom line. We proved them wrong: the industry has actually only managed to make 13% of its history available, and <a href="https://www.youtube.com/watch?v=HLWY7fCXUwE">it’s unlikely to get better</a>.</p>



<p><em>The argument that these games are commercially available is what’s keeping video games behind in the preservation world.</em></p>











<p>The next <a href="https://www.copyright.gov/1201/">rulemaking proceeding</a> under the Digital Millennium Copyright Act (“DMCA”), Title 17, section 1201, of the&nbsp;<em>United States Code</em> is scheduled for 2024. We’re hopeful that this study will incite change, and that video game preservation will become stronger — before we lose more.</p>



<p><em>Survey of the Video Game Reissue Market in the United States was conducted for the Video Game History Foundation and the Software Preservation Network by VGHF Library Director Phil Salvador, published July 2023.</em></p>



<p><mark>Want to learn more?</mark> Check out our <a href="http://gamehistory.org/study-explainer">explainer blog post</a> for a more in-depth look at this study. A bonus episode of our podcast about this study and its findings will be available soon. You can <a href="https://doi.org/10.5281/zenodo.7996492">read the full study here</a>.</p>



<hr>



<h4>Special thanks to our volunteers:</h4>





		</div><!-- .post-inner -->

	
	<!-- .pagination-single -->

	
</article><!-- .post -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First U.S. ban on sale of cellphone location data might be coming (510 pts)]]></title>
            <link>https://www.wsj.com/articles/first-u-s-ban-on-sale-of-cellphone-location-data-might-be-coming-fbe47e53</link>
            <guid>36667848</guid>
            <pubDate>Mon, 10 Jul 2023 15:28:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/articles/first-u-s-ban-on-sale-of-cellphone-location-data-might-be-coming-fbe47e53">https://www.wsj.com/articles/first-u-s-ban-on-sale-of-cellphone-location-data-might-be-coming-fbe47e53</a>, See on <a href="https://news.ycombinator.com/item?id=36667848">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
This copy is for your personal, non-commercial use only. Distribution and use of this material are governed by
our Subscriber Agreement and by copyright law. For non-personal use or to order multiple copies, please contact
Dow Jones Reprints at 1-800-843-0008 or visit www.djreprints.com.
</p><p>https://www.wsj.com/articles/first-u-s-ban-on-sale-of-cellphone-location-data-might-be-coming-fbe47e53</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Keep Linux Open and Free–We Can’t Afford Not To (190 pts)]]></title>
            <link>https://www.oracle.com/news/announcement/blog/keep-linux-open-and-free-2023-07-10/</link>
            <guid>36667582</guid>
            <pubDate>Mon, 10 Jul 2023 15:11:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.oracle.com/news/announcement/blog/keep-linux-open-and-free-2023-07-10/">https://www.oracle.com/news/announcement/blog/keep-linux-open-and-free-2023-07-10/</a>, See on <a href="https://news.ycombinator.com/item?id=36667582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div data-trackas="rc25"><p><span>Press Release</span></p><p><strong> <!-- -->By Edward Screven, Chief Corporate Architect and Wim Coekaerts, Head of Oracle Linux Development<!-- -->—<!-- -->July 10, 2023<!-- --> </strong></p></div>
<div>
<p>Oracle has been part of the Linux community for 25 years. Our goal has remained the same over all those years: help make Linux the best server operating system for everyone, freely available to all, with high-quality, low-cost support provided to those who need it.</p>

<p>Our Linux engineering team makes significant contributions to the kernel, file systems, and tools. We push all that work back to mainline so that every Linux distribution can include it. We are proud those contributions are part of the reason Linux is now so very capable, benefiting not just Oracle customers, but all users.</p>

<p>In 2006, we launched what is now called Oracle Linux, a RHEL compatible distribution and support offering that is used widely, and powers Oracle’s engineered systems and our cloud infrastructure. We chose to be RHEL compatible because we did not want to fragment the Linux community. Our effort to remain compatible has been enormously successful. In all the years since launch, we have had almost no compatibility bugs filed. Customers and ISVs can switch to Oracle Linux from RHEL without modifying their applications, and we certify Oracle software products on RHEL even though they are built and tested on Oracle Linux only, never on RHEL.</p>

<p>While Oracle and IBM have compatible Linux distributions, we have very different ideas about our responsibilities as open source stewards and about operating under the GPLv2. Oracle has always made Oracle Linux binaries and source freely available to all. We do not have subscription agreements that interfere with a subscriber’s rights to redistribute Oracle Linux. On the other hand, IBM subscription agreements specify that you’re in breach if you use those subscription services to exercise your GPLv2 rights. And now, as of June 21, IBM no longer publicly releases RHEL source code.</p>

<p>Why did IBM make this change? Well, if you read IBM’s <a href="https://www.redhat.com/en/blog/red-hats-commitment-open-source-response-gitcentosorg-changes">blog</a> attempting to explain its rationale, it boils down to this:</p>

<blockquote><q>At Red Hat, thousands of people spend their time writing code to enable new features, fixing bugs, integrating different packages and then supporting that work for a long time … We have to pay the people to do that work.</q></blockquote>

<p>Interesting. IBM doesn’t want to continue publicly releasing RHEL source code because it has to pay its engineers? That seems odd, given that Red Hat as a successful independent open source company chose to publicly release RHEL source and pay its engineers for many years before IBM acquired Red Hat in 2019 for $34 billion.</p>

<p>The blog goes on to mention CentOS. It is no surprise CentOS was top of mind for the author attempting to justify withholding RHEL source. CentOS had been a very popular free RHEL compatible distribution. In December 2020, IBM effectively killed it as a free alternative to RHEL. Two new alternatives to RHEL have sprung up in CentOS’s place: AlmaLinux and Rocky Linux. Now, by withholding RHEL source code, IBM has directly attacked them.</p>

<p>And perhaps that is the real answer to the question of why: eliminate competitors. Fewer competitors means more revenue opportunity for IBM.</p>

<p>As for Oracle, we will continue pursuing our goal for Linux as transparently and openly as we always have while minimizing fragmentation. We will continue to develop and test our software products on Oracle Linux. Oracle Linux will continue to be RHEL compatible to the extent we can make it so. In the past, Oracle’s access to published RHEL source has been important for maintaining that compatibility. From a practical standpoint, we believe Oracle Linux will remain as compatible as it has always been through release 9.2, but after that, there may be a greater chance for a compatibility issue to arise. If an incompatibility does affect a customer or ISV, Oracle will work to remediate the problem.</p>

<p>We want to emphasize to Linux developers, Linux customers, and Linux distributors that Oracle is committed to Linux freedom. Oracle makes the following promise: as long as Oracle distributes Linux, Oracle will make the binaries and source code for that distribution publicly and freely available. Furthermore, Oracle welcomes downstream distributions of every kind, community and commercial. We are happy to work with distributors to ease that process, work together on the content of Oracle Linux, and ensure Oracle software products are certified on your distribution.</p>

<p>By the way, if you are a Linux developer who disagrees with IBM’s actions and you believe in Linux freedom the way we do, we are hiring.</p>

<p>One observation for ISVs: IBM’s actions are not in your best interest. By killing CentOS as a RHEL alternative and attacking AlmaLinux and Rocky Linux, IBM is eliminating one way your customers save money and make a larger share of their wallet available to you. If you don’t yet support your product on Oracle Linux, we would be happy to show you how easy that is. Give your customers more choice.</p>

<p>Finally, to IBM, here’s a big idea for you. You say that you don’t want to pay all those RHEL developers? Here’s how you can save money: just pull from us. Become a downstream distributor of Oracle Linux. We will happily take on the burden.</p>
</div>
<!-- --> <!-- -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Feature Flags: Theory vs. Reality (119 pts)]]></title>
            <link>https://bpapillon.com/post/feature-flags-theory-vs-reality/</link>
            <guid>36667469</guid>
            <pubDate>Mon, 10 Jul 2023 15:02:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bpapillon.com/post/feature-flags-theory-vs-reality/">https://bpapillon.com/post/feature-flags-theory-vs-reality/</a>, See on <a href="https://news.ycombinator.com/item?id=36667469">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        

<figure>
  <img src="https://bpapillon.com/communismtheoryhomer.gif" alt="A discussion about DevOps in 2023">
  <figcaption>A discussion about DevOps in 2023</figcaption>
</figure>

<h2 id="the-rise-of-feature-flags-in-devops"><strong>The Rise of Feature Flags in DevOps</strong></h2>

<p>During the second half of the 2010s, the DevOps movement gained massive momentum throughout the SaaS industry. Riding this wave and buoyed by the marketing efforts of LaunchDarkly, feature toggles rapidly became an essential tool for engineering operations used by practically every SaaS company.</p>

<p>In the DevOps context, feature flags are especially great for continuous deployment. By decoupling release from deployment, they allow marketing and engineering to operate independently. This flexibility enables engineers to continuously deploy features in progress.</p>

<p>However, a few downsides were clear even from the beginning. To name a few:</p>

<ul>
<li><strong>They add complexity to the code</strong>, including additional testing overhead.</li>
<li><strong>It’s difficult to verify</strong> whether all of your changes are actually “behind” a feature flag as intended.</li>
<li><strong>Feature toggle checks aren’t free</strong>; it’s more logic, which means more possible bugs, and the check may take time if a database or external service is involved.</li>
</ul>

<p>Due to these shortcomings, the <a href="https://martinfowler.com/articles/feature-toggles.html">conventional wisdom</a> has always been that feature flags should be short-lived and kept to a minimum:</p>

<blockquote>
<p>Savvy teams view the Feature Toggles in their codebase as inventory which comes with a carrying cost and seek to keep that inventory as low as possible.</p>
</blockquote>

<h2 id="reality-check"><strong>Reality Check!</strong></h2>

<figure>
  <img src="https://bpapillon.com/sideshow-bob-rakes.gif" alt="A software developer interacts with their feature management system">
  <figcaption>A software developer interacts with their feature management system</figcaption>
</figure>

<p>As adoption of the feature toggle pattern has spread throughout the industry, a few common problems can be observed.</p>

<h3 id="1-release-follow-through-and-ever-expanding-complexity"><strong>1. Release follow-through and ever-expanding complexity</strong></h3>

<p>If feature flags are meant to be short-lived, then there must be a process for reviewing them and cleaning them up at the appropriate time. This often takes the form of a manual process, such as filing a ticket for a future sprint.</p>

<p>As a general rule of thumb, any process that relies on humans to remember things is not going to be a reliable one, and for most companies, these processes end up being spotty and inconsistent.</p>

<p>A few common issues I’ve seen:</p>

<ul>
<li><strong>Zombie flags</strong>

<ul>
<li>In our haste to move on to the next big feature, we often forget to rip out the release flag. This leads to higher “carrying cost”, taking many forms: the readability of the code, the number of automated tests that continually execute each branch of the feature flag logic, and so forth. Over time, this makes our code base harder to work on and leads to annoyance, wasted time, and bugs.</li>
</ul></li>
<li><strong>Unfinished business</strong>

<ul>
<li>Perhaps we forget to finish rolling out our flag, or maybe some customer segments never even get the new feature. This leads to unexpected support tickets months later, at a time when everyone in the organization thought that the rollout was long finished.</li>
</ul></li>
<li><strong>Ghost flags</strong>

<ul>
<li>Sometimes, even when we remember to rip out the feature flag, there may be a communication breakdown with other users, who end up futilely toggling a control that has no effect.</li>
</ul></li>
</ul>

<h3 id="2-unintended-or-inappropriate-usage"><strong>2. Unintended or inappropriate usage</strong></h3>

<p>Even with perfect follow-through, a flag meant for release purposes might end up being implemented for long-term feature access. This may seem like a time-saving win, but it results in third-party services becoming load-bearing for cases beyond their intended purpose.</p>

<p>Imagine even a very short outage by the service provider. If we only rely on that service provider for release flags, we probably live with our hardcoded fallback values; but, if we’re using it for something like a permission or entitlement check, there is now essentially an outage of our own.</p>

<p>Many teams understand this danger, and respond by implementing separate systems for short-lived and long-lived flags; the former may use a managed service, while the latter may be a home-grown system that stores its data in the main application database.</p>

<p>Even though we now have a more correct tool for each job, we still end up with a lot of problems in practice:</p>

<ul>
<li><strong>Lack of portability &amp; promotion</strong>

<ul>
<li>We’ve mentioned that release flags can sometimes take on a second life as other types of toggles. With this model, if a release flag starts to make sense as a long-lived flag later, there’s no way to promote it to the more appropriate system without making code changes.</li>
</ul></li>
<li><strong>User confusion &amp; frustration</strong>

<ul>
<li>Across these two systems (short and long-lived), there may be a broad base of user types: engineering, product, customer success, marketing, ops. It’s unlikely that these groups share a consistent mental model for the systems, and people tend to just refer to all of it as “flags”. Many users may be unclear as to why there are two separate systems at all, and feel frustrated that they just have to remember which flag lives in which system.</li>
</ul></li>
<li><strong>Lack of context &amp; readability</strong>

<ul>
<li>Exacerbating the user confusion issue, most flags are exposed as a list of keys (e.g. “new_onboarding_flow”, “widgets_v2”) that provide very little context about what these flags do or how they’re meant to be used.</li>
<li>Without talking to someone, reading code, or separately maintaining documentation, it’s very difficult to know much about the flags, which frustrates non-technical and technical stakholders alike.</li>
</ul></li>
<li><strong>Testing headaches</strong>

<ul>
<li>Developers have to accommodate multiple systems in testing; for example, if you’re using mocks or stubs in your tests to simulate the behavior of these systems, you now need twice as many of these.</li>
</ul></li>
<li><strong>Feature gaps</strong>

<ul>
<li>A long-lived flag service that is homegrown may be able to provide better guarantees with regards to latency and availability; however, the short-lived flag service will almost always be more feature-rich because it’s often provided by a managed service. This reality may incentivize its use over the more reliable system in certain cases.</li>
</ul></li>
</ul>

<h2 id="accepting-reality">Accepting Reality</h2>

<p>After observing the feature flag experiment in the wild for some time, I’ve come to the conclusion that the conventional wisdom to limit the number and age of feature flags in your code base is wise, but unrealistic. The fast-paced and cross-functional nature of modern software development create dynamics that are too hard to overcome with best intentions and best practices.</p>

<p>Furthermore, there’s a disconnect between “feature management” as a term of industry and the actual “feature management” that goes on. This term, and the tools that are sold within this market, generally refer only to DevOps use cases like rollout and experimentation. However, we clearly do a lot more managing of features than this.</p>

<ul>
<li>Every time a new customer signs up, are their features not being managed?</li>
<li>When they upgrade to a more expensive plan?</li>
<li>When a sales negotiation results in a bespoke enterprise plan?</li>
<li>When a customer success rep enables an add-on?</li>
</ul>

<p>These are all feature management, but none of them fall into the definition of “feature management” that our tooling and DevOps culture wants to support.</p>

<p>As engineers, it’s time to change our framing of feature management to better align with the businesses we operate in, but to do this, we need new tools.</p>

<h2 id="taming-complexity">Taming complexity</h2>

<figure>
  <img src="https://bpapillon.com/homer-grill-fail.gif" alt="A software developer evaluates their adherence to feature management best practices">
  <figcaption>A software developer evaluates their adherence to feature management best practices</figcaption>
</figure>

<p>A software developer evaluates their adherence to feature management best practices</p>

<p>If we accept that we as engineers are powerless to contain the spread of feature management, and perhaps that we are holding back our businesses to the extent we try to do so, then we need to stop relying on manual processes for hygiene and maintenance.</p>

<p>Let’s imagine what capabilities we might need a new feature management tool to have in order to accomplish this. A few possibilities:</p>

<ul>
<li>Long-lived and short-lived use cases coexist within the same tool, but are clearly delineated in its interfaces.

<ul>
<li>Short-lived flags might come with additional metadata, such as an expiration date by which we expect the flag should no longer be in use.</li>
</ul></li>
<li>Flags should have an owner, either an individual user or perhaps a user role or group.</li>
<li>Users should be able to add meaning to flags after the fact via metadata and grouping.</li>
<li>If a flag changes purpose, say from a release flag to an entitlement check, we can simply update this in the tool. Such a change would be tracked in an audit log.</li>
<li>Policies can be set; for example, short-lived flags must be removed or graduated within a specified amount of time, or perhaps certain metadata (like expiration date) can be made required for certain types of flags.</li>
<li>Flags can easily be used in relation to one another; for example, one flag might be required in order for another flag to be enabled, or two flags might be incompatible with one another for code reasons. The tool should make it easy to configure such invariants.</li>
</ul>

<p>With capabilities like this in place, the tool could start to automate some of the maintenance processes that are currently manual.</p>

<p>For example:</p>

<ul>
<li>Auditing a codebase for out-of-date flags could be done via a static analysis tool in CI.</li>
<li>We could fail builds or notify engineering or product managers if certain assumptions are not met.</li>
<li>Flag owners could receive notifications when flags they are responsible for are out of compliance, or a ticket could automatically be filed in the ticketing system.</li>
</ul>

<p>If we can automate these processes, then we finally might have a system that holds up to the chaos of the modern software development process and fights back against ever-growing complexity.</p>

<h2 id="onward">Onward!</h2>

<figure>
  <img src="https://bpapillon.com/twirling-towards-freedom.gif" alt="Leaving the old expectations behind as we move to a new framing of feature management">
  <figcaption>Leaving the old expectations behind as we move to a new framing of feature management</figcaption>
</figure>

<p>It’s high time that we take another look at what “feature management” means in our industry.  If we accept a more expansive view that aligns with how our businesses want to be managing features and build the tools needed to support this, we can free ourselves from the need to adhere to best practices that have proved unrealistic.</p>

<p>If we were to have a tool like this that better suited the natural complexity of feature management, then this would be a great start. However, there are more considerations, such as the architecture of such a tool, that I will explore in upcoming posts.</p>

        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Danswer – Open-source question answering across all your docs (166 pts)]]></title>
            <link>https://github.com/danswer-ai/danswer</link>
            <guid>36667374</guid>
            <pubDate>Mon, 10 Jul 2023 14:55:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/danswer-ai/danswer">https://github.com/danswer-ai/danswer</a>, See on <a href="https://news.ycombinator.com/item?id=36667374">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">
<a href="https://www.danswer.ai/" rel="nofollow"> <img width="50%" src="https://github.com/danswer-owners/danswer/raw/1fabd9372d66cd54238847197c33f091a724803b/DanswerWithName.png?raw=true)"></a>
</h2>
<p dir="auto">OpenSource Enterprise Question-Answering</p>
<p dir="auto">
<a href="https://docs.danswer.dev/" rel="nofollow">
    <img src="https://camo.githubusercontent.com/769b1399e79e3be7736d96d113701fd40006e4c9f8e2086ed401496f89785201/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d766965772d626c7565" alt="Documentation" data-canonical-src="https://img.shields.io/badge/docs-view-blue">
</a>
<a href="https://join.slack.com/t/danswer/shared_invite/zt-1u5ycen3o-6SJbWfivLWP5LPyp_jftuw" rel="nofollow">
    <img src="https://camo.githubusercontent.com/d3c199e9bb1afecb780ec71151ae464e07aff40b28c2aef5301d7935d6d1aa09/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736c61636b2d6a6f696e2d626c75652e7376673f6c6f676f3d736c61636b" alt="Slack" data-canonical-src="https://img.shields.io/badge/slack-join-blue.svg?logo=slack">
</a>
<a href="https://discord.gg/TDJ59cGV2X" rel="nofollow">
    <img src="https://camo.githubusercontent.com/936ab21c6224f88d885798b4029607590ef711f69f608fd97256d4913a24d681/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646973636f72642d6a6f696e2d626c75652e7376673f6c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" alt="Discord" data-canonical-src="https://img.shields.io/badge/discord-join-blue.svg?logo=discord&amp;logoColor=white">
</a>
<a href="https://github.com/danswer-ai/danswer/blob/main/README.md">
    <img src="https://camo.githubusercontent.com/1befec5200d934517068ce2e354d48fff5a9f3e762f8e48029e44903d356646c/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d6c6963656e7365266d6573736167653d4d495426636f6c6f723d626c7565" alt="License" data-canonical-src="https://img.shields.io/static/v1?label=license&amp;message=MIT&amp;color=blue">
</a>
</p>
<p dir="auto"><strong><a href="https://www.danswer.ai/" rel="nofollow">Danswer</a></strong> allows you to ask natural language questions against internal documents and get back reliable answers backed by quotes and references from the source material so that you can always trust what you get back. You can connect to a number of common tools such as Slack, GitHub, Confluence, amongst others.</p>
<p dir="auto">Check out our <strong><a href="https://www.youtube.com/watch?v=geNzY1nbCnU" rel="nofollow">Video Demo</a></strong>!</p>
<h2 tabindex="-1" dir="auto">Features <g-emoji alias="woman_dancing" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f483.png">💃</g-emoji></h2>
<ul dir="auto">
<li>Direct QA powered by Generative AI models with answers backed by quotes and source links.</li>
<li>Intelligent Document Retrieval (Semantic Search/Reranking) using the latest LLMs.</li>
<li>An AI Helper backed by a custom Deep Learning model to interpret user intent.</li>
<li>User authentication and document level access management.</li>
<li>Connectors to Slack, GitHub, GoogleDrive, Confluence, local files, and web scraping, with more to come.</li>
<li>Management Dashboard to manage connectors and set up features such as live update fetching.</li>
<li>One line Docker Compose (or Kubernetes) deployment to host Danswer anywhere.</li>
</ul>
<h2 tabindex="-1" dir="auto">Upcoming</h2>
<ul dir="auto">
<li>Chat/Conversation support.</li>
<li>Support custom endpoints for Generative AI models or even self-host options.</li>
<li>Templates to easily build custom connectors.</li>
<li>Personalized search</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A physicist who bets that gravity can’t be quantized (233 pts)]]></title>
            <link>https://www.quantamagazine.org/the-physicist-who-bets-that-gravity-cant-be-quantized-20230710/</link>
            <guid>36667278</guid>
            <pubDate>Mon, 10 Jul 2023 14:48:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/the-physicist-who-bets-that-gravity-cant-be-quantized-20230710/">https://www.quantamagazine.org/the-physicist-who-bets-that-gravity-cant-be-quantized-20230710/</a>, See on <a href="https://news.ycombinator.com/item?id=36667278">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="postBody"><div><h2>The Physicist Who’s Challenging the Quantum Orthodoxy</h2><div><p>For decades, physicists have struggled to develop a quantum theory of gravity. But what if gravity — and space-time — are fundamentally classical?</p></div></div><figure><div><p><img alt="A portrait of Jonathan Oppenheim. He’s in an office and is gazing into the distance, looking thoughtful." src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2023/07/JonathanOppenheim-byPhilippAmmon-Lede-scaled.webp"></p></div><figcaption><div><p>Jonathan Oppenheim, a physicist at University College London, is developing hybrid theories that could unify classical gravity and quantum mechanics.</p><p>Philipp Ammon for <em>Quanta Magazine</em></p></div></figcaption></figure><div><h2>Introduction</h2><div><p>Most physicists expect that when we zoom in on the fabric of reality, the unintuitive weirdness of quantum mechanics persists down to the very smallest scales. But in those settings, quantum mechanics collides with classical gravity in a resolutely incompatible way.</p>
<p>So for almost a century, theorists have tried to create a unified theory by quantizing gravity, or sculpting it according to the rules of quantum mechanics. They still haven’t succeeded.</p>
<p><a href="https://www.ucl.ac.uk/oppenheim/">Jonathan Oppenheim</a>, who runs a program exploring post-quantum alternatives at University College London, suspects that’s because gravity simply can’t be squeezed into a quantum box. Maybe, he argues, our presumption that it must be quantized is wrong. “That view is ingrained,” he said. “But no one knows what the truth is.”</p>
<p>Quantum theories are based on probabilities rather than certainties. For example, when you measure a quantum particle, you can’t predict exactly where you will find it, but you can predict the likelihood that it will be found in a particular place. What’s more, the more certain you are about a particle’s location, the less certain you are about its momentum. Over the 20th century, physicists gradually made sense of electromagnetism and other forces using this framework.<strong>&nbsp;</strong></p>
<p>But when they tried to quantize gravity, they ran into unnatural infinities that had to be sidestepped with clumsy mathematical tricks.</p>
<p><strong>&nbsp;</strong>The problems arise because gravity is a result of space-time itself, rather than something that acts on top of it. So if gravity is quantized, that means space-time is also quantized. But that doesn’t work, because quantum theory only makes sense against a classical space-time background — you can’t add and then evolve quantum states on top of an uncertain foundation.<strong>&nbsp;</strong></p>
</div></div><div><h2>Introduction</h2><div><p>To deal with this deep conceptual conflict, most theorists turned to string theory, which imagines that matter and space-time emerge from tiny, vibrating strings. A smaller faction looked to loop quantum gravity, which replaces the smooth space-time of Einstein’s general relativity with a network of interlocked loops. In both theories, our familiar, classical world somehow emerges from these fundamentally quantum building blocks.<strong>&nbsp;</strong></p>
<p>Oppenheim was originally a string theorist, and string theorists believe in the primacy of quantum mechanics. But he soon became uncomfortable with the elaborate mathematical acrobatics his peers performed to tackle one of the most notorious problems in modern physics: the <a href="https://www.quantamagazine.org/tag/information-paradox">black hole information paradox</a>.<strong>&nbsp;</strong></p>
<p>In 2017, Oppenheim started searching for alternatives that avoided the information paradox by taking both the quantum and the classical worlds as bedrocks. He stumbled across some overlooked <a href="https://www.sciencedirect.com/science/article/abs/pii/037596019390818K">research</a> on quantum-classical <a href="https://arxiv.org/abs/quant-ph/9503023">hybrid theories</a> from the 1990s, which he’s been <a href="https://arxiv.org/abs/1811.03116">extending</a> and <a href="https://arxiv.org/abs/2302.07283">exploring</a> ever since. By studying how the classical and quantum worlds interrelate, Oppenheim hopes to find a deeper theory that is neither quantum nor classical, but some kind of hybrid. “Often we put all our eggs in a few baskets, when there are lots of possibilities,” he said.<strong>&nbsp;</strong></p>
<p>To make his point, Oppenheim recently <a href="https://www.ucl.ac.uk/oppenheim/pub/quantum_vs_classical_bet.pdf">made a bet</a> with <a href="https://physics.berkeley.edu/people/faculty/geoff-penington">Geoff Penington</a> and <a href="https://perimeterinstitute.ca/people/carlo-rovelli">Carlo Rovelli</a> — leaders in their respective fields of string theory and loop quantum gravity. The odds? 5,000-to-1. If Oppenheim’s hunch is correct and space-time isn’t quantized, he stands to win bucketloads of potato chips, colorful plastic <a href="https://twitter.com/postquantum/status/1352292958259785729">bazinga balls</a>, or shots of olive oil, according to his fancy — as long as each item costs at most 20 pence (about 25 cents).</p>
<p>We met in a north London café lined with books, where he calmly unpacked his concerns about the quantum gravity status quo and extolled the surprising beauty of these hybrid alternatives. “They raise all kinds of remarkably subtle questions,” he said. “I’ve really lost my feet trying to understand these systems.” But he perseveres.<strong>&nbsp;</strong></p>
<p>“I want my 5,000 bazinga balls.”</p>
<p>The interview has been condensed and edited for clarity.</p>
<h3><strong>Why are most theorists so sure that space-time is quantized?</strong></h3>
<p>It’s become dogma. All the other fields in nature are quantized. There’s a sense that there’s nothing special about gravity — it’s just a field like any other — and therefore we should quantize it.</p>
</div></div><figure><div><p><img alt="Four images of Oppenheim with his students. In the first, he is studying a chalkboard filled with equations. In the second, a student in a turquoise dress is showing her computer screen to several others while Oppenheim, in the background, writes on a white board. The final two images are of Oppenheim and his students on a lunch outing. It’s a sunny day. We see them ordering from a food stand and then enjoying lunch on a grassy lawn." src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2023/07/JonathanOppenheim-Quatriptych-byPhilippAmmon-scaled.webp"></p></div><figcaption><div><p>Oppenheim and his students, seen here in and around the UCL campus,&nbsp;are developing a new class of hybrid quantum-classical theories in which gravity stays classical. Maybe, Oppenheim argues, gravity is special and the quantum consensus is wrong.</p><p>Philipp Ammon for <em>Quanta Magazine</em></p></div></figcaption></figure><div><h2>Introduction</h2><div><h3><strong>Is gravity special in your view?</strong></h3>
<p>Yes. Physicists define all the other forces in terms of fields evolving in space-time. Gravity alone tells us about the geometry and curvature of space-time itself. None of the other forces describe the universal background geometry that we live in like gravity does.</p>
<p>At the moment, our best theory of quantum mechanics uses this background structure of space-time — which gravity defines. And if you really believe that gravity is quantized, then we lose that background structure.</p>
<h3><strong>What sorts of problems do you run into if gravity is classical and not quantized? </strong></h3>
<p>For a long time, the community believed it was logically impossible for gravity to be classical because coupling a quantum system with a classical system would lead to inconsistencies. In the 1950s, Richard Feynman imagined a situation that illuminated the problem: He began with a massive particle that is in a superposition of two different locations. These locations could be two holes in a metal sheet, as in the famous double-slit experiment. Here, the particle also behaves like a wave. It creates an interference pattern of light and dark stripes on the other side of the slits, which makes it impossible to know which slit it went through. In popular accounts, the particle is sometimes described as going through both slits at once.</p>

<p>But since the particle has mass, it creates a gravitational field that we can measure. And that gravitational field tells us its location. If the gravitational field is classical, we can measure it to infinite precision, infer the particle’s location, and determine which slit it went through. So we then have a paradoxical situation — the interference pattern tells us that we can’t determine which slit the particle went through, but the classical gravitational field lets us do just that.</p>
<p>But if the gravitational field is quantum, there is no paradox — uncertainty creeps in when measuring the gravitational field, and so we still have uncertainty in determining the particle’s location.</p>
<h3><strong>So if gravity behaves classically, you end up knowing too much. And that means that cherished ideas from quantum mechanics, like superposition, break down?</strong></h3>
<p>Yes, the gravitational field knows too much. But there’s a loophole in Feynman’s argument that could allow classical gravity to work.</p>
<h3><strong>What is that loophole?</strong></h3>
<p>As it stands, we only know which path the particle took because it produces a definite gravitational field that bends space-time and allows us to determine the particle’s location.<strong>&nbsp;</strong></p>
<p>But if that interaction between the particle and space-time is random — or unpredictable — then the particle itself doesn’t completely dictate the gravitational field. Which means that measuring the gravitational field will not always determine which slit the particle went through because the gravitational field could be in one of many states. Randomness creeps in, and you no longer have a paradox.</p>
<h3><strong>So why don’t more physicists think gravity is classical?</strong></h3>
<p>Well, it is logically possible to have a theory in which we don’t quantize all the fields. But for a classical theory of gravity to be consistent with everything else being quantized, then gravity has to be fundamentally random. To a lot of physicists that’s unacceptable.</p>
</div></div><figure><div><p><img alt="Oppenheim writing on a blackboard that is stuffed with equations. His back is to the camera." src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2023/07/JonathanOppenheim-Blackboard-byPhilippAmmon-scaled.webp"></p></div><figcaption><div><p>Oppenheim started out as a string theorist, but he eventually grew frustrated with the clumsy mathematical tricks his colleagues employed to get around one of the most notorious conundrums in physics: the black hole information paradox.</p><p>Philipp Ammon for <em>Quanta Magazine</em></p></div></figcaption></figure><div><h2>Introduction</h2><div><h3><strong>Why?</strong></h3>
<p>Physicists spend a lot of time trying to figure out how nature works. So the idea that there is, on a very deep level, something inherently unpredictable is troubling to many.</p>
<p>The outcome of measurements within quantum theory appears to be probabilistic. But many physicists prefer to think that what appears as randomness is just the quantum system and the measuring apparatus interacting with the environment. They don’t see it as some fundamental feature of reality.</p>
<h3><strong>What are you proposing instead?</strong></h3>
<p>My best guess is that the next theory of gravity will be something that is neither completely classical nor completely quantum, but something else entirely.</p>
<p>Physicists are only ever coming up with models that approximate nature. But as an attempt at a closer approximation, my students and I constructed a fully consistent theory in which quantum systems and classical space-time interact. We just had to modify quantum theory slightly and modify classical general relativity slightly to allow for the breakdown of predictability that is required.</p>
<h3><strong>Why did you start working on these hybrid theories?</strong></h3>
<p>I was motivated by the black hole information paradox. When you throw a quantum particle into a black hole and then let that black hole evaporate, you encounter a paradox if you believe that black holes preserve information. Standard quantum theory demands that whatever object you throw into the black hole is radiated back out in some scrambled but recognizable way. But that violates general relativity, which tells us that you can never know about objects that cross the black hole’s event horizon.</p>
<p>But if the black hole evaporation process is indeterministic then there’s no paradox. We never learn what was thrown into the black hole because predictability breaks down. General relativity is safe.</p>
</div></div><figure><div><p><img alt="A portrait of Oppenheim in profile. He is mid-sentence and gesturing with his hands." src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2023/07/JonathanOppenheim-Talking-byPhilippAmmon.webp"></p></div><figcaption><div><p>Recently, Oppenheim made a 5,000-to-1 bet with&nbsp;two colleagues that gravity can’t be quantized. If he wins, he gets to stuff his pockets with 5,000 bags of potato chips or bazinga balls or anything else that suits his fancy — as long as each item costs at most 20 pence (about 25 cents). “I feel I’ve made a pretty safe bet, even if I lose,” Oppenheim said.</p><p>Philipp Ammon for <em>Quanta Magazine</em></p></div></figcaption></figure><div><h2>Introduction</h2><div><h3><strong>So the noisiness in these quantum-classical hybrid theories allows information to be lost?</strong></h3>
<p>Exactly.<strong>&nbsp;</strong></p>
<h3><strong>But information conservation is a key principle in quantum mechanics. Losing this can’t sit easily with many theorists.</strong></h3>
<p>That’s true. There were huge debates about this in recent decades, and almost everybody came to believe that black hole evaporation is deterministic. I’m always puzzled by that.</p>
<h3><strong>Will experiments ever resolve if gravity is quantized or not?</strong></h3>
<p>At some point. We still know almost nothing about gravity on the smallest scales. It hasn’t even been tested to the millimeter scale, let alone to the scale of a proton. But there are some exciting experiments coming online which will do that.</p>
<p>One is <a href="https://arxiv.org/abs/2203.01982">a modern-day version</a> of the “Cavendish experiment,” which calculates the strength of the gravitational attraction between two lead spheres. If there is randomness in the gravitational field, as in these quantum-classical hybrids, then when we try and measure its strength we won’t always get the same answer. The gravitational field will jiggle around. Any theory in which gravity is fundamentally classical has a certain level of gravitational noise.</p>

<h3><strong>How do you know this randomness is intrinsic to the gravitational field and not some noise from the environment?</strong></h3>
<p>You don’t. Gravity is such a weak force that even the best experiments already have a lot of jiggle in them. So you have to eliminate all these other sources of noise as much as possible. What’s exciting is that my students and I showed that if these hybrid theories are true, there must be some minimal amount of gravitational noise. This can be measured by studying gold atoms in a double-slit experiment. These experiments already place bounds on whether gravity is fundamentally classical. We are gradually closing in on the amount of indeterminacy allowed.</p>
<h3><strong>On the flip side of the bet, are there any experiments that would prove that gravity is quantized?</strong></h3>
<p>There are <a href="https://arxiv.org/abs/1707.06050">proposed experiments</a> that look for entanglement mediated by the gravitational field. As entanglement is a quantum phenomenon, that would be a direct test of the quantum nature of gravity. These experiments are very exciting, but probably decades away.</p>
</div></div></div><div><h2>Next article</h2><p>The Lawlessness of Large Numbers</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stéphane Graber has left Canonical (105 pts)]]></title>
            <link>https://stgraber.org/2023/07/10/time-to-move-on/</link>
            <guid>36666920</guid>
            <pubDate>Mon, 10 Jul 2023 14:24:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stgraber.org/2023/07/10/time-to-move-on/">https://stgraber.org/2023/07/10/time-to-move-on/</a>, See on <a href="https://news.ycombinator.com/item?id=36666920">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			    
<h2>Resignation</h2>



<p>After a bit over 12 years working for Canonical, Friday 7th of July was my last day.</p>



<p>It’s a bit of a bittersweet moment leaving a company after you’ve invested so much of your time into it, but I believe that now was the right time for me. As I’ve told colleagues and upper management, Canonical isn’t the company I excitedly joined back in 2011 and it’s not a company that I would want to join today, therefore it shouldn’t be a company that I keep working for either.</p>



<p>I’ll most miss working with the LXD team. Canonical is truly lucky to have such a great team of engineers going above and beyond to support a project like LXD. It’s quite unique to have a small team with such a wide variety of skills ranging from kernel development, to distributed systems, to web frontends and documentation, all working together to make a project like LXD possible.</p>


<div>
<figure><img decoding="async" src="https://discuss.linuxcontainers.org/uploads/default/optimized/2X/9/9a2706de31098d5ac2a35e673b20c14b595cee32_2_375x500.jpeg" alt=""></figure></div>


<h2>LXD</h2>



<p>Following the announcement of my resignation, Canonical decided to pull LXD out of the Linux Containers projects and relocate it to a full in-house project.<br>That’s the news which <a rel="noreferrer noopener" href="https://linuxcontainers.org/lxd/" target="_blank">we announced last week</a>.</p>



<p>I obviously wish that this particular change hadn’t happened, I strongly see value in having a project like LXD be run in a more open, community environment where everyone’s opinion is valued and everyone’s contribution, no matter the size, is welcome. Having the “LXD community experiment” be labeled a failure within Canonical seems unfair to me and to everyone who contributed over the years.</p>



<p>As for my particular involvement in Canonical’s LXD moving forward, I will definitely remain an active user of LXD and will likely still be filing issues and the occasional fix. However, I don’t intend to ever sign <a href="https://ubuntu.com/legal/contributors" data-type="URL" data-id="https://ubuntu.com/legal/contributors">Canonical’s CLA</a>, so should that become a barrier to contribution for the project, I will have to stop contributing to it.</p>



<h2>Ubuntu</h2>



<p>On the Ubuntu front, I’m currently a mostly inactive member of the Ubuntu Release team, Ubuntu Archive team and Ubuntu SRU team. I will be stepping down from all of those as I struggled to find any time to help them out while working for Canonical full time and don’t expect things to improve now.</p>



<p>I will remain an Ubuntu Core Developer and may contribute the occasional bugfix, package updates or new packages here and there. I don’t have any plans to move away from Ubuntu for my own systems.</p>



<h2>Future</h2>



<p>As for what I’ll be doing next. One thing I can share immediately is that I’m not joining another company nor do I have any intention to join another company at this stage.</p>



<p>I’m going to start by working on a number of pet projects that I’ve either neglected or been unable to even start so far. Some of those could lead to a source of revenue, some others will just be for the community’s benefit.</p>



<p>I’m also getting setup for freelance work, so will be able to accept the occasional consultancy or training contract where those make sense for me.</p>



<h2>Conclusion</h2>



<p>It’s a bit of an end of an era for me, a lot has changed over those 12 years both personally and in the industry, so I’m looking forward to have some time to reset and figure out what’s next!</p>
			    			</div><div><p>
		    This entry was posted in <a href="https://stgraber.org/category/planet-ubuntu/" rel="category tag">Planet Ubuntu</a> and tagged <a href="https://stgraber.org/tag/canonical/" rel="tag">canonical</a>. Bookmark the <a href="https://stgraber.org/2023/07/10/time-to-move-on/" title="Permalink to Time to move on" rel="bookmark">permalink</a>.		    		</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We replaced Firecracker with QEMU (364 pts)]]></title>
            <link>https://hocus.dev/blog/qemu-vs-firecracker/</link>
            <guid>36666782</guid>
            <pubDate>Mon, 10 Jul 2023 14:15:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hocus.dev/blog/qemu-vs-firecracker/">https://hocus.dev/blog/qemu-vs-firecracker/</a>, See on <a href="https://news.ycombinator.com/item?id=36666782">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content" itemprop="articleBody"><p><img loading="lazy" alt="Firecracker vs QEMU" src="https://hocus.dev/blog/assets/images/qemu-vs-firecracker-bg-6924bd95888c267d0fcbdbb34b61cea3.png" width="1830" height="583"></p><p>Firecracker, the microVM hypervisor, is renowned for being lightweight, fast, and secure. It's excellent for running short-lived workloads, which is why it's the backbone of AWS Lambda. Our initial prototype for Hocus, a self-hosted alternative to Gitpod and GitHub Codespaces, utilized Firecracker. However, after weeks of testing, we decided to entirely replace it with QEMU. A little-known fact about Firecracker is its lack of support for many modern hypervisor features, such as dynamic RAM management, which is vital for long-lived workloads. In this post, I will explain why Firecracker might not be the best hypervisor choice and when you should avoid it.</p><h2 id="firecracker-optimizes-for-short-lived-workloads">Firecracker Optimizes for Short-Lived Workloads<a href="#firecracker-optimizes-for-short-lived-workloads" aria-label="Direct link to Firecracker Optimizes for Short-Lived Workloads" title="Direct link to Firecracker Optimizes for Short-Lived Workloads">​</a></h2><p>The creators of Firecracker <a href="https://github.com/firecracker-microvm/firecracker/blob/dbd9a84b11a63b5e5bf201e244fe83f0bc76792a/README.md?plain=1#L24" target="_blank" rel="noopener noreferrer">state that</a>:</p><blockquote><p>"Firecracker has a minimalist design. It excludes unnecessary devices and guest-facing functionality to reduce the memory footprint and attack surface area of each microVM."</p></blockquote><p>The term "unnecessary" is intriguing - if this functionality is unnecessary, why was it incorporated into other hypervisors? The definition of "unnecessary" must be understood in the context of what Firecracker was built for. These excluded features are unnecessary for AWS Lambda, which spins up VMs to run short function calls and then shuts them down. If you're running a different kind of workload, like a VM that contains your development environment or a self-hosted GitHub Actions agent, these features cease to be unnecessary. Your VM will run for hours, days, or even months without stopping, unlike the typical Firecracker VM, which runs for seconds or minutes.</p><h2 id="firecracker-not-so-lightweight-after-all">Firecracker, Not So Lightweight After All<a href="#firecracker-not-so-lightweight-after-all" aria-label="Direct link to Firecracker, Not So Lightweight After All" title="Direct link to Firecracker, Not So Lightweight After All">​</a></h2><p>Here are the two most significant features Firecracker lacks:</p><ul><li>Dynamic memory management - Firecracker's RAM footprint starts low, but once a workload inside allocates RAM, Firecracker will never return it to the host system. After running several workloads inside, you end up with an idling VM that consumes 32 GB of RAM on the host, even though it doesn't need any of it.<sup id="fnref-1-1f96f1"><a href="#fn-1-1f96f1">1</a></sup></li><li>Discard operations on storage - if you create a 10 GB file inside a VM and then delete it, the backing space won't be reclaimed on the host. The VM will occupy that disk space until you delete the entire VM drive.<sup id="fnref-2-1f96f1"><a href="#fn-2-1f96f1">2</a></sup></li></ul><p>These deficiencies make Firecracker a memory and disk space hog. The plot below shows the memory usage of the same memory-intensive workload running in QEMU and Firecracker virtual machines.</p><p><img loading="lazy" alt="QEMU vs Firecracker VM Memory Usage" src="https://hocus.dev/blog/assets/images/vm-mem-usage-ae3ffeb0cc6a2df2f662597c653d9bf4.png" title="QEMU vs Firecracker VM Memory Usage" width="1696" height="967"></p><p><em>The workload in Firecracker finishes running around the 200-second mark, and in QEMU around the 250-second mark. It's not a performance difference; it's just when I manually stopped them.</em></p><h2 id="other-features-firecracker-is-missing">Other Features Firecracker Is Missing<a href="#other-features-firecracker-is-missing" aria-label="Direct link to Other Features Firecracker Is Missing" title="Direct link to Other Features Firecracker Is Missing">​</a></h2><ul><li>GPU support - if you need a GPU inside the VM, <a href="https://github.com/firecracker-microvm/firecracker/issues/849" target="_blank" rel="noopener noreferrer">you have to pick a different hypervisor</a>.</li><li>High-performance disk IO - when you connect multiple drives to the VM and run intensive IO operations, you will likely run into a bottleneck. Firecracker uses a virtio-blk implementation that isn’t as memory-hungry as alternatives, but has a <a href="https://lwn.net/Articles/812055/" target="_blank" rel="noopener noreferrer">smaller throughput</a>.<sup id="fnref-3-1f96f1"><a href="#fn-3-1f96f1">3</a></sup></li></ul><h2 id="qemu-is-not-perfect-though">QEMU is Not Perfect Though<a href="#qemu-is-not-perfect-though" aria-label="Direct link to QEMU is Not Perfect Though" title="Direct link to QEMU is Not Perfect Though">​</a></h2><p>The main issue we've had with QEMU is that it has too many options you need to configure. For instance, enabling your VM to return unused RAM to the host requires at least three challenging tasks:</p><ul><li>Knowing that the feature even exists (it's called <a href="https://docs.kernel.org/mm/free_page_reporting.html" target="_blank" rel="noopener noreferrer">free page reporting</a> and you have to specifically enable it in QEMU)</li><li>Understanding that an obscure feature of Linux called <a href="https://www.kernel.org/doc/html/v5.17/vm/damon/index.html" target="_blank" rel="noopener noreferrer">DAMON</a> exists, knowing what it's for<sup id="fnref-4-1f96f1"><a href="#fn-4-1f96f1">4</a></sup>, knowing how to configure it, and compiling a guest Linux kernel that supports it</li><li>Knowing that you need to disable transparent huge pages on the guest, otherwise the VM will never return large amounts of memory</li></ul><p>It took us two months of experimentation, reading through the source code of Firecracker, QEMU, and other hypervisors to develop a reliable QEMU proof of concept. To comprehend DAMON configuration, my co-founder spent days <a href="https://lore.kernel.org/damon/20230504171749.89225-1-sj@kernel.org/T/" target="_blank" rel="noopener noreferrer">running benchmarks and conversing with one of its authors</a>. It's great that we could talk and we are grateful that the author spent the time to help us, but it shows that the technology is not easily accessible yet.</p><h2 id="conclusion">Conclusion<a href="#conclusion" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>QEMU has the features you need to run general-purpose workloads, but configuring it requires a lot of time and patience. If you want to run short-lived, untrusted workloads, Firecracker is a great choice. However, if you just want to run your development environment in a VM, you can use <a href="https://github.com/hocus-dev/hocus" target="_blank" rel="noopener noreferrer">Hocus</a>. We've done all the hard work for you already. It's still in alpha, but you can already check it out on <a href="https://github.com/hocus-dev/hocus" target="_blank" rel="noopener noreferrer">GitHub</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shelf – open-source asset management software (293 pts)]]></title>
            <link>https://github.com/Shelf-nu/shelf.nu</link>
            <guid>36666702</guid>
            <pubDate>Mon, 10 Jul 2023 14:10:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Shelf-nu/shelf.nu">https://github.com/Shelf-nu/shelf.nu</a>, See on <a href="https://news.ycombinator.com/item?id=36666702">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Shelf.nu - Open Source Asset Management Infrastructure for everyone.</h2>
<p dir="auto"><a href="https://twitter.com/ShelfQR" rel="nofollow"><img src="https://camo.githubusercontent.com/529b5acd0d1cbf3a3cfaba6ed41f98eebdba9a4f0b8136b1bcd9ffad15b2ed68/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f68747470732f747769747465722e636f6d2f636c6f7564706f7373652e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f772532302534305368656c665152" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&amp;label=Follow%20%40ShelfQR"></a>
<a href="https://github.com/Shelf-nu/shelf.nu/actions/workflows/deploy.yml"><img src="https://github.com/Shelf-nu/shelf.nu/actions/workflows/deploy.yml/badge.svg?branch=dev" alt="🚀 Deploy"></a></p>
<p dir="auto">Shelf <g-emoji alias="label" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3f7.png">🏷️</g-emoji> Asset Management infrastructure for absolutely everyone (open source).</p>
<p dir="auto">Shelf is a simple and visual asset management and location tracking system that allows people to track their physical assets with ease.</p>
<h2 tabindex="-1" dir="auto">Core Features and Benefits</h2>
<p dir="auto">With Shelf, you can take a picture of any item you own and store it in your own database. From there, you can generate a printable code (QR) that you can tag onto the item, making it easy to identify and locate in the future. Shelf has a handy code printing area where you can add as many QR codes as you can on an A4 sticker paper sheet. You can also add detailed information about the item, including its purchase date, purchase price, warranty information, and more.</p>
<h3 tabindex="-1" dir="auto">Once your assets are online, you will be able to:</h3>
<ul dir="auto">
<li>Generate printable PDFs sheets from assets you select, so you can stick them onto anything</li>
<li>Check the last known location of your assets</li>
<li>Instant Search through your assets database</li>
<li>Use 'lost mode' for emergencies (offer a bounty for a return of an item)</li>
<li>Get notified of assets you are not using</li>
<li>Share your asset vault with other users</li>
</ul>
<h3 tabindex="-1" dir="auto">Use Shelf alone, or as a team. And, these questions will be a thing of the past.</h3>
<ul dir="auto">
<li>Who was the last person that took X,Y or Z?</li>
<li>What gear does X have currently?</li>
<li>Which assets did we appoint to our team member abroad?</li>
<li>What do we have in our storage facility now?</li>
</ul>
<h2 tabindex="-1" dir="auto">Shelf's vision and ambition</h2>
<p dir="auto">To enable and facilitate the tagging of 1 Billion assets by 2023. Shelf therefore allows users to create unlimited assets on their environments. We will fund the growth and further development of the tool by releasing premium features. However, Shelf core will be forever free for individuals.</p>
<hr>
<h3 tabindex="-1" dir="auto">Shelf's current stack</h3>
<p dir="auto">We have decided to give RemixJS a try.</p>
<p dir="auto">For the purpose of shipping asap, we have opted into using a template: <a href="https://github.com/rphlmr/supa-fly-stack">https://github.com/rphlmr/supa-fly-stack</a></p>
<h3 tabindex="-1" dir="auto">Getting started with Shelf</h3>
<h2 tabindex="-1" dir="auto">Remix Supa Fly Stack</h2>
<blockquote>
<p dir="auto">This Readme will be re-written soon</p>
</blockquote>
<div data-snippet-clipboard-copy-content="npx create-remix --template rphlmr/supa-fly-stack"><pre><code>npx create-remix --template rphlmr/supa-fly-stack
</code></pre></div>
<h2 tabindex="-1" dir="auto">What's in the stack</h2>
<ul dir="auto">
<li><a href="https://fly.io/" rel="nofollow">Fly app deployment</a> with <a href="https://www.docker.com/products/docker-desktop/" rel="nofollow">Docker</a></li>
<li>Production-ready <a href="https://supabase.com/" rel="nofollow">Supabase Database</a></li>
<li>Healthcheck endpoint for <a href="https://fly.io/docs/reference/configuration/#services-http_checks" rel="nofollow">Fly backups region fallbacks</a></li>
<li><a href="https://github.com/features/actions">GitHub Actions</a> to deploy on merge to production and staging environments</li>
<li>Email/Password Authentication / Magic Link, with <a href="https://remix.run/docs/en/v1/api/remix#createcookiesessionstorage" rel="nofollow">cookie-based sessions</a></li>
<li>Database ORM with <a href="https://prisma.io/" rel="nofollow">Prisma</a></li>
<li>Forms Schema (client and server sides !) validation with <a href="https://github.com/kiliman/remix-params-helper">Remix Params Helper</a></li>
<li>Styling with <a href="https://tailwindcss.com/" rel="nofollow">Tailwind</a></li>
<li>End-to-end testing with <a href="https://cypress.io/" rel="nofollow">Cypress</a></li>
<li>Local third party request mocking with <a href="https://mswjs.io/" rel="nofollow">MSW</a></li>
<li>Unit testing with <a href="https://vitest.dev/" rel="nofollow">Vitest</a> and <a href="https://testing-library.com/" rel="nofollow">Testing Library</a></li>
<li>Code formatting with <a href="https://prettier.io/" rel="nofollow">Prettier</a></li>
<li>Linting with <a href="https://eslint.org/" rel="nofollow">ESLint</a></li>
<li>Static Types with <a href="https://typescriptlang.org/" rel="nofollow">TypeScript</a></li>
</ul>
<p dir="auto">Not a fan of bits of the stack? Fork it, change it, and use <code>npx create-remix --template your/repo</code>! Make it your own.</p>
<h2 tabindex="-1" dir="auto">Development</h2>
<ul dir="auto">
<li>
<p dir="auto">Create a <a href="https://supabase.com/" rel="nofollow">Supabase Database</a> (free tier gives you 2 databases)</p>
<blockquote>
<p dir="auto"><strong>Note:</strong> Only one for playing around with Supabase or 2 for <code>staging</code> and <code>production</code></p>
</blockquote>
<blockquote>
<p dir="auto"><strong>Note:</strong> Used all your free tiers ? Also works with <a href="https://github.com/supabase/cli">Supabase CLI</a> and local self-hosting</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>Note:</strong> Create a strong database password, but prefer a passphrase, it'll be more easy to use in connection string (no need to escape special char)</p>
<p dir="auto"><em>example : my_strong_passphrase</em></p>
</blockquote>
</li>
<li>
<p dir="auto">Go to <a href="https://app.supabase.io/project/%7BPROJECT%7D/settings/api" rel="nofollow">https://app.supabase.io/project/{PROJECT}/settings/api</a> to find your secrets</p>
</li>
<li>
<p dir="auto">"Project API keys"</p>
</li>
<li>
<p dir="auto">Add your <code>MAPTILER_TOKEN</code>, <code>SUPABASE_URL</code>, <code>SERVER_URL</code>, <code>SUPABASE_SERVICE_ROLE</code> (aka <code>service_role</code> <code>secret</code>), <code>SUPABASE_ANON_PUBLIC</code> (aka <code>anon</code> <code>public</code>) and <code>DATABASE_URL</code> in the <code>.env</code> file</p>
<blockquote>
<p dir="auto"><strong>Note:</strong> <code>SERVER_URL</code> is your localhost on dev. It'll work for magic link login</p>
</blockquote>
</li>
</ul>
<div data-snippet-clipboard-copy-content="DATABASE_URL=&quot;postgres://postgres:{STAGING_POSTGRES_PASSWORD}@db.{STAGING_YOUR_INSTANCE_NAME}.supabase.co:5432/postgres&quot;
SUPABASE_ANON_PUBLIC=&quot;{ANON_PUBLIC}&quot;
SUPABASE_SERVICE_ROLE=&quot;{SERVICE_ROLE}&quot;
SUPABASE_URL=&quot;https://{STAGING_YOUR_INSTANCE_NAME}.supabase.co&quot;
SESSION_SECRET=&quot;super-duper-s3cret&quot;
SERVER_URL=&quot;http://localhost:3000&quot;
MAPTILER_TOKEN=&quot;someToken&quot;
SMTP_HOST=&quot;smtp.yourhost.com&quot;
SMTP_USER=&quot;you@example.com&quot;
SMTP_PWD=&quot;yourSMTPpassword&quot;"><pre lang="en"><code>DATABASE_URL="postgres://postgres:{STAGING_POSTGRES_PASSWORD}@db.{STAGING_YOUR_INSTANCE_NAME}.supabase.co:5432/postgres"
SUPABASE_ANON_PUBLIC="{ANON_PUBLIC}"
SUPABASE_SERVICE_ROLE="{SERVICE_ROLE}"
SUPABASE_URL="https://{STAGING_YOUR_INSTANCE_NAME}.supabase.co"
SESSION_SECRET="super-duper-s3cret"
SERVER_URL="http://localhost:3000"
MAPTILER_TOKEN="someToken"
SMTP_HOST="smtp.yourhost.com"
SMTP_USER="you@example.com"
SMTP_PWD="yourSMTPpassword"
</code></pre></div>
<ul dir="auto">
<li>
<p dir="auto">This step only applies if you've opted out of having the CLI install dependencies for you:</p>

</li>
<li>
<p dir="auto">Initial setup:</p>

</li>
<li>
<p dir="auto">Start dev server:</p>

</li>
</ul>
<p dir="auto">This starts your app in development mode, rebuilding assets on file changes.</p>
<p dir="auto">The database seed script creates a new user with some data you can use to get started:</p>
<ul dir="auto">
<li>Email: <code>hello@supabase.com</code></li>
<li>Password: <code>supabase</code></li>
</ul>
<h3 tabindex="-1" dir="auto">Relevant code:</h3>
<p dir="auto">This is a pretty simple note-taking app, but it's a good example of how you can build a full-stack app with Prisma, Supabase, and Remix. The main functionality is creating users, logging in and out (handling access and refresh tokens + refresh on expiration), and creating and deleting notes.</p>
<ul dir="auto">
<li>auth / session <a href="https://github.com/Shelf-nu/shelf.nu/blob/main/app/modules/auth">./app/modules/auth</a></li>
<li>creating, and deleting notes <a href="https://github.com/Shelf-nu/shelf.nu/blob/main/app/modules/note">./app/modules/note</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Deployment</h2>
<blockquote>
<p dir="auto">Do what you know if you are a Fly.io expert.</p>
</blockquote>
<p dir="auto">This Remix Stack comes with two GitHub Actions that handle automatically deploying your app to production and staging environments.</p>
<p dir="auto">Prior to your first deployment, you'll need to do a few things:</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://fly.io/docs/getting-started/installing-flyctl/" rel="nofollow">Install Fly</a></p>
</li>
<li>
<p dir="auto">Sign up and log in to Fly</p>

<blockquote>
<p dir="auto"><strong>Note:</strong> If you have more than one Fly account, ensure that you are signed into the same account in the Fly CLI as you are in the browser. In your terminal, run <code>fly auth whoami</code> and ensure the email matches the Fly account signed into the browser.</p>
</blockquote>
</li>
<li>
<p dir="auto">Create two apps on Fly, one for staging and one for production:</p>
<div dir="auto" data-snippet-clipboard-copy-content="fly apps create supa-fly-stack-template
fly apps create supa-fly-stack-template-staging  # ** not mandatory if you don't want a staging environnement **"><pre>fly apps create supa-fly-stack-template
fly apps create supa-fly-stack-template-staging  <span><span>#</span> ** not mandatory if you don't want a staging environnement **</span></pre></div>
<blockquote>
<p dir="auto"><strong>Note:</strong> For production app, make sure this name matches the <code>app</code> set in your <code>fly.toml</code> file. Otherwise, you will not be able to deploy.</p>
</blockquote>
<ul dir="auto">
<li>Initialize Empty Git repository.</li>
</ul>

</li>
<li>
<p dir="auto">Create a new <a href="https://repo.new/" rel="nofollow">GitHub Repository</a>, and then add it as the remote for your project. <strong>Do not push your app yet!</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="git remote add origin <ORIGIN_URL>"><pre>git remote add origin <span>&lt;</span>ORIGIN_URL<span>&gt;</span></pre></div>
</li>
<li>
<p dir="auto">Add <code>MAPTILER_TOKEN</code> which is needed for rendering the map which shows the last scanned location. For more info and to get an account and token: <a href="https://www.maptiler.com/" rel="nofollow">https://www.maptiler.com/</a></p>
</li>
<li>
<p dir="auto">Add a <code>FLY_API_TOKEN</code> to your GitHub repo. To do this, go to your user settings on Fly and create a new <a href="https://web.fly.io/user/personal_access_tokens/new" rel="nofollow">token</a>, then add it to <a href="https://docs.github.com/en/actions/security-guides/encrypted-secrets">your repo secrets</a> with the name <code>FLY_API_TOKEN</code>.</p>
</li>
<li>
<p dir="auto">Add a <code>SESSION_SECRET</code>, <code>SUPABASE_URL</code>, <code>SUPABASE_SERVICE_ROLE</code>,<code>SUPABASE_ANON_PUBLIC</code>, <code>SERVER_URL</code> and <code>DATABASE_URL</code> to your fly app secrets</p>
<blockquote>
<p dir="auto"><strong>Note:</strong> To find your <code>SERVER_URL</code>, go to <a href="https://fly.io/apps/supa-fly-stack-template-3a36" rel="nofollow">your fly.io dashboard</a></p>
</blockquote>
<p dir="auto">To do this you can run the following commands:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# production (--app name is resolved from fly.toml)
fly secrets set SESSION_SECRET=$(openssl rand -hex 32)
fly secrets set SUPABASE_URL=&quot;https://{YOUR_INSTANCE_NAME}.supabase.co&quot;
fly secrets set SUPABASE_SERVICE_ROLE=&quot;{SUPABASE_SERVICE_ROLE}&quot;
fly secrets set SUPABASE_ANON_PUBLIC=&quot;{SUPABASE_ANON_PUBLIC}&quot;
fly secrets set DATABASE_URL=&quot;postgres://postgres:{POSTGRES_PASSWORD}@db.{YOUR_INSTANCE_NAME}.supabase.co:5432/postgres&quot;
fly secrets set SERVER_URL=&quot;https://{YOUR_STAGING_SERVEUR_URL}&quot;
fly secrets set MAPTILER_TOKEN=&quot;{YOUR_MAPTILER_TOKEN}&quot;

fly secrets set SMTP_HOST=&quot;smtp.yourhost.com&quot;
fly secrets set SMTP_USER=&quot;you@example.com&quot;
fly secrets set SMTP_PWD=&quot;yourSMTPpassword&quot;


# staging (specify --app name) ** not mandatory if you don't want a staging environnement **
fly secrets set SESSION_SECRET=$(openssl rand -hex 32) --app supa-fly-stack-template-staging
fly secrets set SUPABASE_URL=&quot;https://{YOUR_STAGING_INSTANCE_NAME}.supabase.co&quot; --app supa-fly-stack-template-staging
fly secrets set SUPABASE_SERVICE_ROLE=&quot;{STAGING_SUPABASE_SERVICE_ROLE}&quot; --app supa-fly-stack-template-staging
fly secrets set SUPABASE_ANON_PUBLIC=&quot;{STAGING_SUPABASE_ANON_PUBLIC}&quot; --app supa-fly-stack-template-staging
fly secrets set DATABASE_URL=&quot;postgres://postgres:{STAGING_POSTGRES_PASSWORD}@db.{STAGING_YOUR_INSTANCE_NAME}.supabase.co:5432/postgres&quot; --app supa-fly-stack-template-staging
fly secrets set SERVER_URL=&quot;https://{YOUR_STAGING_SERVEUR_URL}&quot; --app supa-fly-stack-template-staging
"><pre><span><span>#</span> production (--app name is resolved from fly.toml)</span>
fly secrets <span>set</span> SESSION_SECRET=<span><span>$(</span>openssl rand -hex 32<span>)</span></span>
fly secrets <span>set</span> SUPABASE_URL=<span><span>"</span>https://{YOUR_INSTANCE_NAME}.supabase.co<span>"</span></span>
fly secrets <span>set</span> SUPABASE_SERVICE_ROLE=<span><span>"</span>{SUPABASE_SERVICE_ROLE}<span>"</span></span>
fly secrets <span>set</span> SUPABASE_ANON_PUBLIC=<span><span>"</span>{SUPABASE_ANON_PUBLIC}<span>"</span></span>
fly secrets <span>set</span> DATABASE_URL=<span><span>"</span>postgres://postgres:{POSTGRES_PASSWORD}@db.{YOUR_INSTANCE_NAME}.supabase.co:5432/postgres<span>"</span></span>
fly secrets <span>set</span> SERVER_URL=<span><span>"</span>https://{YOUR_STAGING_SERVEUR_URL}<span>"</span></span>
fly secrets <span>set</span> MAPTILER_TOKEN=<span><span>"</span>{YOUR_MAPTILER_TOKEN}<span>"</span></span>

fly secrets <span>set</span> SMTP_HOST=<span><span>"</span>smtp.yourhost.com<span>"</span></span>
fly secrets <span>set</span> SMTP_USER=<span><span>"</span>you@example.com<span>"</span></span>
fly secrets <span>set</span> SMTP_PWD=<span><span>"</span>yourSMTPpassword<span>"</span></span>


<span><span>#</span> staging (specify --app name) ** not mandatory if you don't want a staging environnement **</span>
fly secrets <span>set</span> SESSION_SECRET=<span><span>$(</span>openssl rand -hex 32<span>)</span></span> --app supa-fly-stack-template-staging
fly secrets <span>set</span> SUPABASE_URL=<span><span>"</span>https://{YOUR_STAGING_INSTANCE_NAME}.supabase.co<span>"</span></span> --app supa-fly-stack-template-staging
fly secrets <span>set</span> SUPABASE_SERVICE_ROLE=<span><span>"</span>{STAGING_SUPABASE_SERVICE_ROLE}<span>"</span></span> --app supa-fly-stack-template-staging
fly secrets <span>set</span> SUPABASE_ANON_PUBLIC=<span><span>"</span>{STAGING_SUPABASE_ANON_PUBLIC}<span>"</span></span> --app supa-fly-stack-template-staging
fly secrets <span>set</span> DATABASE_URL=<span><span>"</span>postgres://postgres:{STAGING_POSTGRES_PASSWORD}@db.{STAGING_YOUR_INSTANCE_NAME}.supabase.co:5432/postgres<span>"</span></span> --app supa-fly-stack-template-staging
fly secrets <span>set</span> SERVER_URL=<span><span>"</span>https://{YOUR_STAGING_SERVEUR_URL}<span>"</span></span> --app supa-fly-stack-template-staging
</pre></div>
<p dir="auto">If you don't have openssl installed, you can also use <a href="https://1password.com/generate-password" rel="nofollow">1password</a> to generate a random secret, just replace <code>$(openssl rand -hex 32)</code> with the generated secret.</p>
</li>
</ul>
<p dir="auto">Now that everything is set up you can commit and push your changes to your repo. Every commit to your <code>main</code> branch will trigger a deployment to your production environment, and every commit to your <code>dev</code> branch will trigger a deployment to your staging environment.</p>
<blockquote>
<p dir="auto"><strong>Note:</strong> To deploy manually, just run <code>fly deploy</code> (It'll deploy app defined in fly.toml)</p>
</blockquote>
<h2 tabindex="-1" dir="auto">File Storage</h2>
<p dir="auto">For File storage we use the S3 buckets service provided by supabase. We do this as it makes it easier to manage permissions in relation to our users which are also stored on supabase. To set it up you need to do the following steps:</p>
<h3 tabindex="-1" dir="auto">Profile pictures</h3>
<ol dir="auto">
<li>Create a bucket called <code>profile-pictures</code></li>
<li>Make it a public bucket</li>
<li>Implement a policy for <code>INSERT</code>, <code>UPDATE</code> &amp; <code>DELETE</code>. The policy expression is: <code>((bucket_id = 'profile-pictures'::text) AND ((storage.foldername(name))[1] = (auth.uid())::text))</code> and target roles should be set to <code>authenticated</code></li>
</ol>
<h3 tabindex="-1" dir="auto">Items</h3>
<ol dir="auto">
<li>Create a bucket called <code>items</code></li>
<li>Implement a policy for <code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code> &amp; <code>DELETE</code>. The policy expression is: <code>((bucket_id = 'items'::text) AND ((storage.foldername(name))[1] = (auth.uid())::text))</code> and target roles should be set to <code>authenticated</code></li>
</ol>
<h2 tabindex="-1" dir="auto">GitHub Actions</h2>
<blockquote>
<p dir="auto">DISCLAIMER : Github actions ==&gt; I'm not an expert about that. Read carefully before using it</p>
</blockquote>
<p dir="auto">We use GitHub Actions for continuous integration and deployment. Anything that gets into the <code>main</code> branch will be deployed to production after running tests/build/etc. Anything in the <code>dev</code> branch will be deployed to staging.</p>
<p dir="auto"><g-emoji alias="point_right" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f449.png">👉</g-emoji> <strong>You have to add some env secrets for cypress.</strong> <g-emoji alias="point_left" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f448.png">👈</g-emoji></p>
<p dir="auto">Add a <code>SESSION_SECRET</code>, <code>SUPABASE_URL</code>, <code>SUPABASE_SERVICE_ROLE</code>,<code>SUPABASE_ANON_PUBLIC</code>, <code>SERVER_URL</code> and <code>DATABASE_URL</code> to <a href="https://docs.github.com/en/actions/security-guides/encrypted-secrets">your repo secrets</a></p>
<h2 tabindex="-1" dir="auto">Testing</h2>
<h3 tabindex="-1" dir="auto">Cypress</h3>
<p dir="auto">We use Cypress for our End-to-End tests in this project. You'll find those in the <code>cypress</code> directory. As you make changes, add to an existing file or create a new file in the <code>cypress/e2e</code> directory to test your changes.</p>
<p dir="auto">We use <a href="https://testing-library.com/cypress" rel="nofollow"><code>@testing-library/cypress</code></a> for selecting elements on the page semantically.</p>
<p dir="auto">To run these tests in development, complete your <code>.env</code> and run <code>npm run test:e2e:dev</code> which will start the dev server for the app as well as the Cypress client. Make sure the database is running in docker as described above.</p>
<p dir="auto">We also have a utility to auto-delete the user at the end of your test. Just make sure to add this in each test file:</p>
<div dir="auto" data-snippet-clipboard-copy-content="afterEach(() => {
  cy.cleanupUser();
});"><pre><span>afterEach</span><span>(</span><span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>cy</span><span>.</span><span>cleanupUser</span><span>(</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span></pre></div>
<p dir="auto">That way, we can keep your test db clean and keep your tests isolated from one another.</p>
<h3 tabindex="-1" dir="auto">Vitest</h3>
<p dir="auto">For lower level tests of utilities and individual components, we use <code>vitest</code>. We have DOM-specific assertion helpers via <a href="https://testing-library.com/jest-dom" rel="nofollow"><code>@testing-library/jest-dom</code></a>.</p>
<h3 tabindex="-1" dir="auto">Type Checking</h3>
<p dir="auto">This project uses TypeScript. It's recommended to get TypeScript set up for your editor to get a great in-editor experience with type checking and auto-complete. To run type checking across the whole project, run <code>npm run typecheck</code>.</p>
<h3 tabindex="-1" dir="auto">Linting</h3>
<p dir="auto">This project uses ESLint for linting. That is configured in <code>.eslintrc.js</code>.</p>
<h3 tabindex="-1" dir="auto">Formatting</h3>
<p dir="auto">We use <a href="https://prettier.io/" rel="nofollow">Prettier</a> for auto-formatting in this project. It's recommended to install an editor plugin (like the <a href="https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode" rel="nofollow">VSCode Prettier plugin</a>) to get auto-formatting on save. There's also a <code>npm run format</code> script you can run to format all files in the project.</p>
<h2 tabindex="-1" dir="auto">Start working with Supabase</h2>
<p dir="auto">You are now ready to go further, congrats!</p>
<p dir="auto">To extend your Prisma schema and apply changes on your supabase database :</p>
<ul dir="auto">
<li>
<p dir="auto">Make your changes in <a href="https://github.com/Shelf-nu/shelf.nu/blob/main/app/database/schema.prisma">./app/database/schema.prisma</a></p>
</li>
<li>
<p dir="auto">Prepare your schema migration</p>
<div dir="auto" data-snippet-clipboard-copy-content="npm run db:prepare-migration"><pre>npm run db:prepare-migration</pre></div>
</li>
<li>
<p dir="auto">Check your migration in <a href="https://github.com/Shelf-nu/shelf.nu/blob/main/app/database">./app/database/migrations</a></p>
</li>
<li>
<p dir="auto">Apply this migration to production</p>
<div dir="auto" data-snippet-clipboard-copy-content="npm run db:deploy-migration"><pre>npm run db:deploy-migration</pre></div>
</li>
</ul>
<h2 tabindex="-1" dir="auto">If your token expires in less than 1 hour (3600 seconds in Supabase Dashboard)</h2>
<p dir="auto">If you have a lower token lifetime than me (1 hour), you should take a look at <code>REFRESH_ACCESS_TOKEN_THRESHOLD</code> in <a href="https://github.com/Shelf-nu/shelf.nu/blob/main/app/modules/auth/session.server.ts">./app/modules/auth/session.server.ts</a> and set what you think is the best value for your use case.</p>
<h2 tabindex="-1" dir="auto">Supabase RLS</h2>
<p dir="auto">You may ask "can I use RLS with Remix".</p>
<p dir="auto">The answer is "Yes" but It has a cost.</p>
<p dir="auto">Using Supabase SDK server side to query your database (for those using RLS features) adds an extra delay due to calling a Gotrue rest API instead of directly calling the Postgres database (and this is fine because at first Supabase SDK is for those who don't have/want backend).</p>
<p dir="auto">In my benchmark, it makes my pages twice slower. (~+200ms compared to a direct query with Prisma)</p>
<h2 tabindex="-1" dir="auto">Supabase login with magic link</h2>
<p dir="auto">In order to make the register/login with magic link work, you will need to add some configuration to your Supabase.
You need to add the site url as well as the redirect urls of your local, test and live app that will be used for oauth
To do that navigate to Authentication &gt; URL configiration and add the folowing values:</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://localhost:3000/oauth/callback" rel="nofollow">https://localhost:3000/oauth/callback</a></p>
</li>
<li>
<p dir="auto"><a href="https://localhost:3000/reset-password" rel="nofollow">https://localhost:3000/reset-password</a></p>
</li>
<li>
<p dir="auto"><a href="https://staging-domain.com/oauth/callback" rel="nofollow">https://staging-domain.com/oauth/callback</a></p>
</li>
<li>
<p dir="auto"><a href="https://staging-domain.com/reset-password" rel="nofollow">https://staging-domain.com/reset-password</a></p>
</li>
<li>
<p dir="auto"><a href="https://live-domain.com/oauth/callback" rel="nofollow">https://live-domain.com/oauth/callback</a></p>
</li>
<li>
<p dir="auto"><a href="https://live-domain.com/reset-password" rel="nofollow">https://live-domain.com/reset-password</a></p>
</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hilbert Transform (122 pts)]]></title>
            <link>https://electroagenda.com/en/hilbert-transform/</link>
            <guid>36666260</guid>
            <pubDate>Mon, 10 Jul 2023 13:39:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electroagenda.com/en/hilbert-transform/">https://electroagenda.com/en/hilbert-transform/</a>, See on <a href="https://news.ycombinator.com/item?id=36666260">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

			
<p>The Hilbert transform is a linear operation applied to real signals. In practical terms, the Hilbert transform translates into a phase shift of -90º at the positive frequencies (and +90º at the negative frequencies) that make up the signal. The relevance of the Hilbert transform in telecommunication engineering is due to its contribution in obtaining spectrally efficient signals. For example in the generation of signals with single sideband spectrum or in the generation of the analytical signal.</p>



<p>Next, the basic mathematics of the Hilbert transform and its generic effects on signals in the time and frequency domains are reviewed. Finally, its main applications are discussed. The text is organized with the following table of contents:</p>



<div id="ez-toc-container">

<nav><ul><li><a href="#1_The_Mathematics_of_Hilbert_Transform" title="1. The Mathematics of Hilbert Transform">1. The Mathematics of Hilbert Transform</a><ul><li><a href="#11_Frequency_Domain" title="1.1 Frequency Domain">1.1 Frequency Domain</a></li><li><a href="#12_Time_Domain" title="1.2 Time Domain">1.2 Time Domain</a></li></ul></li><li><a href="#2_Hilbert_Transform_Effects_on_Signals" title="2. Hilbert Transform Effects on Signals">2. Hilbert Transform Effects on Signals</a><ul><li><a href="#21_Baseband_Signals" title="2.1 Baseband Signals">2.1 Baseband Signals</a></li><li><a href="#22_Bandpass_Signals" title="2.2 Bandpass Signals">2.2 Bandpass Signals</a></li></ul></li><li><a href="#3_Applications_of_the_Hilbert_Transform_in_Communications" title="3. Applications of the Hilbert Transform in Communications">3. Applications of the Hilbert Transform in Communications</a><ul><li><a href="#31_Single_Side_Band_Spectrum_SSB" title="3.1 Single Side Band Spectrum (SSB)">3.1 Single Side Band Spectrum (SSB)</a><ul><li><a href="#311_Schematic_Representation" title="3.1.1 Schematic Representation">3.1.1 Schematic Representation</a></li><li><a href="#312_Mathematical_Representation" title="3.1.2 Mathematical Representation">3.1.2 Mathematical Representation</a></li></ul></li><li><a href="#32_Quadrature_Signals" title="3.2 Quadrature Signals">3.2 Quadrature Signals</a><ul><li><a href="#321_Analytic_Signal" title="3.2.1 Analytic Signal">3.2.1 Analytic Signal</a><ul><li><a href="#3211_Mathematical_Representation" title="3.2.1.1 Mathematical Representation">3.2.1.1 Mathematical Representation</a></li><li><a href="#3212_Spectral_Representation" title="3.2.1.2 Spectral Representation">3.2.1.2 Spectral Representation</a></li></ul></li><li><a href="#322_Complex_Envelope" title="3.2.2 Complex Envelope">3.2.2 Complex Envelope</a><ul><li><a href="#3221_Mathematical_Representation" title="3.2.2.1 Mathematical Representation">3.2.2.1 Mathematical Representation</a></li><li><a href="#3222_Spectral_Representation" title="3.2.2.2 Spectral Representation">3.2.2.2 Spectral Representation</a></li></ul></li></ul></li></ul></li><li><a href="#4_Conclusions" title="4. Conclusions">4. Conclusions</a></li></ul></nav></div>




<h2><span id="1_The_Mathematics_of_Hilbert_Transform"></span>1. The Mathematics of Hilbert Transform<span></span></h2>



<p>In this section it is shown that the Hilbert transform  <span data-katex-display="false"> \footnotesize \hat{s}(t) </span> of a real signal <em>s(t)</em> is an operator <span data-katex-display="false"> \footnotesize \mathcal H() </span> equivalent to a linear, noncausal, time invariant filter <a href="http://electroagenda.com/carlson" target="_blank" rel="noreferrer noopener">[1]</a> <a href="http://electroagenda.com/openheim" target="_blank" rel="noreferrer noopener">[2]</a>. The mathematics of the Hilbert transform are explained in both the time and frequency domains. For this purpose, in a generic way, the following notation is used:</p>



<p><span data-katex-display="true">\begin{equation} s(t) \xtofrom[\mathcal{H}^{-1}]{\mathcal{H}} \hat{s}(t) \end{equation}</span> </p>



<p><span data-katex-display="true">\begin{equation} \hat{s}(t) = s(t) * h_{\mathcal{H}}(t)  \end{equation}</span></p>



<p><span data-katex-display="true">\begin{equation} \hat{S}(f) = S(f) · H_{\mathcal{H}}(f)  \end{equation}</span></p>



<p>Where <span data-katex-display="false"> \footnotesize h_{\mathcal{H}}(t) </span> represents the impulse response of the Hilbert transform operator in the time domain, and <span data-katex-display="false"> \footnotesize H_{\mathcal{H}}(f) </span> represents its transfer function in the frequency domain. </p>



<h3><span id="11_Frequency_Domain"></span>1.1 Frequency Domain<span></span></h3>



<p>The analysis in the frequency domain allows a more intuitive interpretation of the Hilbert transform. Given a real signal whose (hermitic) spectrum is <em>S(f)</em>, the transformed spectrum <span data-katex-display="false"> \footnotesize \hat{S}(f) </span> essentially experiences a phase shift of 90°. Specifically, and given the hermiticity of a real signal, this is a phase shift of -90º at positive frequencies and +90º at negative frequencies. Mathematically, therefore, the transfer function of the Hilbert transform is:</p>



<p><span data-katex-display="true">\begin{equation} H_{\mathcal{H}}(f) = -j·sgn(f) = \begin{cases} -j = e^{-j \frac{\pi}{2}} &amp;\text{for } f &gt; 0 \\  0 &amp;\text{for } f = 0 \\ +j= e^{+j \frac{\pi}{2}} &amp;\text{for } f &lt; 0 \end{cases} \end{equation}</span></p>



<p>Thus, the Hilbert transform of the basic trigonometric functions is as follows:</p>



<p><span data-katex-display="true">\begin{equation} \cos(2\pi ft) \xtofrom[\mathcal{H}^{-1}]{\mathcal{H}} \sin(2\pi ft) \xtofrom[\mathcal{H}^{-1}]{\mathcal{H}} -\cos(2\pi ft) \xtofrom[\mathcal{H}^{-1}]{\mathcal{H}} - \sin(2\pi ft) \xtofrom[\mathcal{H}^{-1}]{\mathcal{H}} \cos(2\pi ft) \end{equation}</span> </p>



<p>Note that, in practical terms, the above equation implies that if a signal is represented as a sum of positive frequency components, its Hilbert transform is obtained by adding a phase shift of -90º to these components. </p>



<h3><span id="12_Time_Domain"></span>1.2 Time Domain<span></span></h3>



<p>The impulsional response of the Hilbert transform is:</p>



<p><span data-katex-display="true">\begin{equation} h_{\mathcal{H}}(t) = \cfrac{1}{\pi t}  \end{equation}</span></p>



<p>And, consequently, it is satisfied that its Fourier transform is equal to the transfer function mentioned in the previous section:</p>



<p><span data-katex-display="true">\begin{equation} \cfrac{1}{\pi t} \xtofrom[\mathcal{F}^{-1}]{\mathcal{F}} -j·sgn(f) \end{equation}</span></p>



<p>Proving the above equation directly involves employing the concept of the <a href="https://en.wikipedia.org/wiki/Cauchy_principal_value" target="_blank" rel="noreferrer noopener nofollow">Cauchy Principal Value</a>, which is beyond the scope of this text. Instead, an indirect demonstration is made based on the <a href="https://www.tutorialspoint.com/signals-and-systems-duality-property-of-fourier-transform#" target="_blank" rel="noreferrer noopener nofollow">Duality Property</a> of the Fourier transform, so that it must be satisfied that: </p>



<p><span data-katex-display="true">\begin{equation} -j·sgn(t)  \xtofrom[\mathcal{F}^{-1}]{\mathcal{F}} -\cfrac{1}{\pi f}\end{equation}</span></p>



<p>To prove the above equation, the following concepts must be taken into account:</p>



<ul>
<li>The derivative of the sign function is related to the Dirac delta <em>δ(t)</em> according to the following equation:</li>
</ul>



<p><span data-katex-display="true">\begin{equation} \frac{\partial}{\partial t}sgn(t) = 2\delta(t) \end{equation}</span></p>



<ul>
<li>The Fourier transform of the Dirac delta function is equal to unity so that, from equation (9), it follows:</li>
</ul>



<p><span data-katex-display="true">\begin{equation} {\mathcal{F}} \left[\frac{\partial}{\partial t}sgn(t)\right] = 2 \end{equation}</span></p>



<ul>
<li>The <a href="https://www.tutorialspoint.com/time-differentiation-property-of-fourier-transform" target="_blank" rel="noreferrer noopener nofollow">Fourier transform of the derivative</a> of a generic function <em>g(t)</em> is related to the transform of the function itself such that:</li>
</ul>



<p><span data-katex-display="true">\begin{equation} {\mathcal{F}} \left[\frac{\partial}{\partial t}g(t)\right]= j2\pi f{\mathcal{F}}[g(t)] \end{equation}</span></p>



<p>Applying the generic equation (11) to the function <em>sgn(t)</em> and comparing the result with equation (10), equation (8) follows directly as it was intended to demonstrate. </p>



<h2><span id="2_Hilbert_Transform_Effects_on_Signals"></span>2. Hilbert Transform Effects on Signals<span></span></h2>



<p>This section shows that the effects of the Hilbert transform depend on the spectrum of the original signal. In particular, the cases of baseband and bandpass signals are distinguished. In practice, Hilbert transform applications focus on the spectrum and the spectral efficiency, so the effects on the signal in the time domain may not be relevant.   </p>



<p>For a more generic description of the effect of <a href="https://electroagenda.com/en/frequency-constant-phase-shift-and-distortion/">constant frequency phase shift</a> on real signals, please consult this <a href="https://electroagenda.com/en/frequency-constant-phase-shift-and-distortion/">link</a>.</p>



<h3><span id="21_Baseband_Signals"></span>2.1 Baseband Signals<span></span></h3>



<p>The following image shows the spectrum of a real baseband signal before and after applying the Hilbert transform. The phase shifts of -90º at positive frequencies and 90º at negative frequencies are observed.</p>


<div>
<figure><img decoding="async" loading="lazy" src="https://electroagenda.com/wp-content/uploads/2023/06/Banda_Base_Signal_mod.png" alt="(ES) Espectro de transformada de Hilbert de señal banda base. (EN) Spectrum of Hilbert transform of baseband signal." width="669" height="365" srcset="https://electroagenda.com/wp-content/uploads/2023/06/Banda_Base_Signal_mod.png 892w, https://electroagenda.com/wp-content/uploads/2023/06/Banda_Base_Signal_mod-300x164.png 300w, https://electroagenda.com/wp-content/uploads/2023/06/Banda_Base_Signal_mod-768x419.png 768w" sizes="(max-width: 669px) 100vw, 669px"></figure></div>


<p>Since each of the frequencies that make up the signal suffers a constant phase shift, which is not <a href="https://electroagenda.com/en/the-mathematics-of-linear-distortion/">linear</a> with frequency, the appearance of the resulting time signal is different from that of the original signal. In other words, <a href="https://electroagenda.com/en/transmission-media/phase-distortion-explanation-and-examples/">phase distortion</a> has occurred. The following graph shows an example of a real baseband signal and its Hilbert transform, in the time domain, where the change of aspect can be appreciated:  </p>


<div>
<figure><img decoding="async" loading="lazy" src="https://electroagenda.com/wp-content/uploads/2023/06/phase_cte_baseband_saved_mod_ES.png" alt="(ES) Efecto en el tiempo de la trnasformada de Hilbert para señal banda base. (EN) Hilbert Transform effect on temporal baseband signal." width="743" height="320" srcset="https://electroagenda.com/wp-content/uploads/2023/06/phase_cte_baseband_saved_mod_ES.png 990w, https://electroagenda.com/wp-content/uploads/2023/06/phase_cte_baseband_saved_mod_ES-300x129.png 300w, https://electroagenda.com/wp-content/uploads/2023/06/phase_cte_baseband_saved_mod_ES-768x331.png 768w" sizes="(max-width: 743px) 100vw, 743px"></figure></div>


<h3><span id="22_Bandpass_Signals"></span>2.2 Bandpass Signals<span></span></h3>



<p>In line with the previous example, the application of the Hilbert transform to a real bandpass signal produces the effect on the spectrum shown in the following image:</p>


<div>
<figure><img decoding="async" loading="lazy" src="https://electroagenda.com/wp-content/uploads/2023/06/Pass_Band_Signal_mod-1024x469.png" alt="(ES) Efecto de la  transformada de Hilbert en espectro de señal paso banda. (EN) Hilbert transform effect on pass band spectrum." width="768" height="352" srcset="https://electroagenda.com/wp-content/uploads/2023/06/Pass_Band_Signal_mod-1024x469.png 1024w, https://electroagenda.com/wp-content/uploads/2023/06/Pass_Band_Signal_mod-300x138.png 300w, https://electroagenda.com/wp-content/uploads/2023/06/Pass_Band_Signal_mod-768x352.png 768w, https://electroagenda.com/wp-content/uploads/2023/06/Pass_Band_Signal_mod.png 1032w" sizes="(max-width: 768px) 100vw, 768px"></figure></div>


<p>With a reasoning equivalent to the baseband example, it could be deduced that the resulting signal is different from the original signal, presenting <a href="https://electroagenda.com/en/transmission-media/phase-distortion-explanation-and-examples/">phase distortion</a>. The following graph illustrates this behavior:</p>


<div>
<figure><img decoding="async" loading="lazy" src="https://electroagenda.com/wp-content/uploads/2023/06/phase_cte_wideband_ES.png" alt="(ES) Efecto temporal de la transformada de Hilbert en señal real paso banda. (EN) Temporal effect of Hilbert Transform on pass band real signal." width="732" height="640" srcset="https://electroagenda.com/wp-content/uploads/2023/06/phase_cte_wideband_ES.png 976w, https://electroagenda.com/wp-content/uploads/2023/06/phase_cte_wideband_ES-300x262.png 300w, https://electroagenda.com/wp-content/uploads/2023/06/phase_cte_wideband_ES-768x671.png 768w" sizes="(max-width: 732px) 100vw, 732px"></figure></div>


<p>However, in the case of bandpass signals there is an important clarification. As shown in the image, the envelope of the transformed signal (in black) is equal to the envelope of the original signal. The envelope is the modulating signal, or the signal that is typically intended to be communicated. The effect of the Hilbert transform can be understood as equivalent to a 90º phase shift in the transmitter’s carrier. Therefore, once the receiver locked to the received signal, the demodulated envelope would be the same as the transmitted envelope. In other words, there would be no distortion in the communication.  </p>



<h2><span id="3_Applications_of_the_Hilbert_Transform_in_Communications"></span>3. Applications of the Hilbert Transform in Communications<span></span></h2>



<p>In telecommunications engineering, the Hilbert transform is a fundamental tool for processing and obtaining spectrally efficient signals. <a href="https://electroagenda.com/hilbert" target="_blank" rel="noreferrer noopener">[3]</a>. This section briefly reviews their main applications.</p>



<h3><span id="31_Single_Side_Band_Spectrum_SSB"></span>3.1 Single Side Band Spectrum (SSB)<span></span></h3>



<p>In this case, the Hilbert transform is used to reduce the transmission bandwidth of a bandpass signal. </p>



<h4><span id="311_Schematic_Representation"></span>3.1.1 Schematic Representation<span></span></h4>



<p>The following graph is used to illustrate this strategy: </p>


<div>
<figure><img decoding="async" loading="lazy" src="https://electroagenda.com/wp-content/uploads/2023/07/SSB_thesis_mod-1024x653.png" alt="(ES) Generación de banda lateral única a partir de la transformada de Hilbert. (EN) Single Side Band Generation using Hilbert Transform.  " width="768" height="490" srcset="https://electroagenda.com/wp-content/uploads/2023/07/SSB_thesis_mod-1024x653.png 1024w, https://electroagenda.com/wp-content/uploads/2023/07/SSB_thesis_mod-300x191.png 300w, https://electroagenda.com/wp-content/uploads/2023/07/SSB_thesis_mod-768x490.png 768w, https://electroagenda.com/wp-content/uploads/2023/07/SSB_thesis_mod.png 1109w" sizes="(max-width: 768px) 100vw, 768px"></figure></div>


<p>The starting point is a real baseband signal <em>s(t)</em>. For simplicity, and without loss of generality, the baseband signal in the image consists of a single tone. As seen in the upper branch, when the baseband signal modulates a carrier, frequency components are obtained on both sides of the center frequency <em>ω<sub>c</sub></em> (red and green components in the image). This is known as a double sideband signal (DSB).</p>



<p>However, when adding or subtracting the double-sideband signals obtained by quadrature modulating the original signal <em>s(t)</em> and its Hilbert transform <span data-katex-display="false"> \footnotesize \hat{s}(t) </span>, a single sideband spectrum (SSB) is obtained. Indeed, because the sum of two components with an offset of <em>π</em> radians cancels out, a spectrum can be obtained that only includes frequencies below or above the center frequency (green and red respectively in the image).</p>



<h4><span id="312_Mathematical_Representation"></span>3.1.2 Mathematical Representation<span></span></h4>



<p>It is evident that the above reasoning can be extended to a generic real baseband signal <em>s(t)</em> with a given bandwidth. In this way the Hilbert transform allows to obtain an SSB bandpass signal by the scheme shown in the image. Mathematically:</p>



<p><span data-katex-display="true">\begin{equation} s_{SSB}(t) = s(t)\cos(\omega_ct) \pm \hat{s}(t)\sin(\omega_ct)  \end{equation}</span></p>



<p>The above scheme can be implemented both in an analog form (typically using 90º hybrid couplers and IQ mixers) and with digital signal processing.</p>



<h3><span id="32_Quadrature_Signals"></span>3.2 Quadrature Signals<span></span></h3>



<p>The Hilbert transform is also used in the generation of quadrature signals, i.e. in the complex plane. The advantages of generating and processing these signals are briefly explained below. </p>



<h4><span id="321_Analytic_Signal"></span>3.2.1 Analytic Signal<span></span></h4>



<p>Given a real bandpass signal <em>s(t)</em>, its analytical signal <em>s<sub>a</sub>(t)</em> is complex and incorporates only the positive frequencies of <em>s(t)</em>. In addition <em>s<sub>a</sub><sup><sub>*</sub></sup>(t)</em>, which is also complex, incorporates only the negative frequencies of <em>s(t)</em>. </p>



<h5><span id="3211_Mathematical_Representation"></span>3.2.1.1 Mathematical Representation<span></span></h5>



<p>The analytic signal with the properties described above is obtained by means of the following equation:</p>



<p><span data-katex-display="true">\begin{equation} s_{a}(t) = s(t)+j\hat{s}(t)  \end{equation}</span></p>



<p><span data-katex-display="true">\begin{equation} s_{a}^*(t) = s(t)-j\hat{s}(t)  \end{equation}</span></p>



<p>It is practically immediate to demonstrate that negative frequencies have been eliminated in the generation of the signal <em>s<sub>a</sub>(t)</em>. Mathematically, applying (3) and (4) in (13) gives that:</p>



<p><span data-katex-display="true">\begin{equation} S_a(f) = \begin{cases}S(f)+j[-jS(f)] =2S(f)&amp;\text{for } f &gt;0 \\ S(f)+j0 =S(f)&amp;\text{for } f =0 \\ S(f)+j[jS(f)]=0 &amp;\text{for } f&lt;0  \end{cases}  \end{equation}</span></p>



<p>Similarly:</p>



<p><span data-katex-display="true">\begin{equation} S_a^*(f) = \begin{cases}0&amp;\text{for } f &gt;0 \\ S(f)&amp;\text{for } f =0 \\ 2S(f) &amp;\text{for } f&lt;0  \end{cases}  \end{equation}</span></p>



<h5><span id="3212_Spectral_Representation"></span>3.2.1.2 Spectral Representation<span></span></h5>



<p>Below is an example representing the spectrum of a real bandpass signal and its analytic signal:</p>


<div>
<figure><img decoding="async" loading="lazy" src="https://electroagenda.com/wp-content/uploads/2023/07/Analytic_Pass_Band_Signal-1024x576.png" alt="(ES) Espectro de señal paso banda y de su señal analítica. (EN) Pass band signal spectrum and its analytic signal spectrum. " width="768" height="432" srcset="https://electroagenda.com/wp-content/uploads/2023/07/Analytic_Pass_Band_Signal-1024x576.png 1024w, https://electroagenda.com/wp-content/uploads/2023/07/Analytic_Pass_Band_Signal-300x169.png 300w, https://electroagenda.com/wp-content/uploads/2023/07/Analytic_Pass_Band_Signal-768x432.png 768w, https://electroagenda.com/wp-content/uploads/2023/07/Analytic_Pass_Band_Signal.png 1058w" sizes="(max-width: 768px) 100vw, 768px"></figure></div>


<p>The main advantage of the analytic signal is to eliminate the negative frequencies of a real signal, which can be considered superfluous due to Hermitic symmetry. Switching to complex notation facilitates many mathematical manipulations, especially in modulation and demodulation techniques. After processing the corresponding application, taking the real part of the post-processed analytical signal allows to obtain the real result signal with all its frequencies, positive and negative.</p>



<h4><span id="322_Complex_Envelope"></span>3.2.2 Complex Envelope<span></span></h4>



<p>The complex envelope is obtained from the analytic signal. It represents the baseband signal resulting from moving the analytic signal from its center frequency to DC. </p>



<h5><span id="3221_Mathematical_Representation"></span>3.2.2.1 Mathematical Representation<span></span></h5>



<p>To transfer a bandpass signal from a center frequency, without creating additional replicas, it is necessary to multiply it by a frequency phasor. Assuming that the analytic signal is centered at the carrier frequency <em>ω<sub>c</sub></em>, the complex envelope can be obtained by the following operation:</p>



<p><span data-katex-display="true">\begin{equation} s_{a\downarrow}(t) = s_{a}(t)e^{-j\omega_c t}  \end{equation}</span></p>



<p><span data-katex-display="true">\begin{equation} s_{a\uparrow}(t) = s_{a}^*(t)e^{j\omega_c t}  \end{equation}</span></p>



<h5><span id="3222_Spectral_Representation"></span>3.2.2.2 Spectral Representation<span></span></h5>



<p>Below is an example representing the spectrum of a real bandpass signal and its complex envelope:</p>


<div>
<figure><img decoding="async" loading="lazy" src="https://electroagenda.com/wp-content/uploads/2023/07/Complex_Envelope-1024x576.png" alt="(ES) Espectros de señal paso banda real y su envolvente compleja. (EN) Spectra of pass band real signal and its complex envelope." width="768" height="432" srcset="https://electroagenda.com/wp-content/uploads/2023/07/Complex_Envelope-1024x576.png 1024w, https://electroagenda.com/wp-content/uploads/2023/07/Complex_Envelope-300x169.png 300w, https://electroagenda.com/wp-content/uploads/2023/07/Complex_Envelope-768x432.png 768w, https://electroagenda.com/wp-content/uploads/2023/07/Complex_Envelope.png 1058w" sizes="(max-width: 768px) 100vw, 768px"></figure></div>


<p>A very important advantage over the previous analytic signal is obtained: the signal bandwidth has been reduced, potentially by half. Therefore signal processing can be performed at a lower sampling rate. However, recovering the bandpass signal is more complex because it also requires a frequency shift: </p>



<p><span data-katex-display="true">\begin{equation} s(t) = \real[s_{a}(t)] = \real[s_{a\downarrow}(t)e^{j\omega_c t}]  \end{equation}</span></p>



<p><span data-katex-display="true">\begin{equation} s(t) = \real[s_{a}^*(t)] = \real[s_{a\uparrow}(t)e^{-j\omega_c t}]  \end{equation}</span></p>



<h2><span id="4_Conclusions"></span>4. Conclusions<span></span></h2>



<p>The conclusions of the text are as follows:</p>



<ul>
<li>The Hilbert transform is a linear operator that produces a phase shift of -90º in the (positive) frequencies of a signal.</li>
</ul>



<ul>
<li>The effect of the Hilbert transform in the time domain depends on the signal spectrum. While in baseband cases the appearance of the signal changes completely, in bandpass cases the signal envelope remains unchanged. </li>
</ul>



<ul>
<li>The main application of the Hilbert transform in communications is the generation of spectrally efficient signals: single sideband signal, analytical signal and complex envelope.  </li>
</ul>







<hr>



<p><strong>Bibliography</strong><br><a href="http://www.electroagenda.com/carlson" target="_blank" rel="noreferrer noopener">[1] <em>Communication Systems</em>, A. Bruce Carlson.</a><br><a href="http://www.electroagenda.com/openheim" target="_blank" rel="noreferrer noopener">[2] <em>Signals and Systems</em>, A. V. Openheim.</a><br><a href="https://electroagenda.com/hilbert" target="_blank" rel="noreferrer noopener">[3] <em>Hilbert Transform in Signal Processing</em>, Stephan Hahn.</a><br></p>



<hr>



<p><strong>Subscription</strong><br>If you liked this contribution, please do not hesitate to subscribe to our newsletter:<br></p>


		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firefox Address Bar Tips (911 pts)]]></title>
            <link>https://wiki.tilde.institute/w/firefox-address-bar-tips</link>
            <guid>36666116</guid>
            <pubDate>Mon, 10 Jul 2023 13:30:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wiki.tilde.institute/w/firefox-address-bar-tips">https://wiki.tilde.institute/w/firefox-address-bar-tips</a>, See on <a href="https://news.ycombinator.com/item?id=36666116">Hacker News</a></p>
<div id="readability-page-1" class="page">

<!--
title: firefox address bar tips
author: erxeto
description: Some useful tips for using the firefox address bar more
efficiently
-->



<p>The address bar has become our entry point to the internet these days.
Firefox in its default configuration does some sort of <em>smart</em> guess on
what you type there.  If it resembles a <a href="https://en.wikipedia.org/wiki/URL">URL</a>
then the browser makes that request.  If not, it sends the string you typed
to your default search engine.  It also includes some fuzzy search matches
from your history and all that, which is fine 90% of the time, but
sometimes you need a bit more control over what results it shows you.</p>

<h2>Changing the address bar behaviour</h2>

<p>This is a list of modifiers you can set at the beginning of the search to
tell firefox what do you want to see on the results, a kind of filtering:</p>

<pre><code>^    to search for matches in your browsing history.
*    to search for matches in your bookmarks.
+    to search for matches in pages you've tagged.
%    to search for matches in your currently open tabs.
#    to search for matches in page titles.
$    to search for matches in web addresses (URLs).
?    to search for matches in suggestions.
</code></pre>

<h2>Examples</h2>

<p>So, if you want to search for the word <code>headphones</code> in your bookmarks only,
you can type on the address bar:</p>

<p><code>*headphones</code></p>

<p>And, if you want to include only the results of your browsing history it
would be:</p>

<p><code>^headphones</code></p>

<p><a href="https://wiki.tilde.institute/">back</a></p>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Swap Anything Released – Most Flexible AI Swap (110 pts)]]></title>
            <link>https://imgcreator.zmo.ai/ai-generator</link>
            <guid>36666087</guid>
            <pubDate>Mon, 10 Jul 2023 13:27:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://imgcreator.zmo.ai/ai-generator">https://imgcreator.zmo.ai/ai-generator</a>, See on <a href="https://news.ycombinator.com/item?id=36666087">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__nuxt"><div data-v-2e02c169=""><header data-v-2e02c169=""><p data-v-2e02c169=""><a target="_blank" href="https://www.producthunt.com/posts/kawaai" data-v-2e02c169=""> ✨ Please support us on product hunt, free credits provided there <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="18px" height="18px" fill="currentColor" data-v-2e02c169=""><path d="m10.875 19.3-6.6-6.6q-.15-.15-.213-.325T4 12q0-.2.063-.375t.212-.325l6.6-6.6q.275-.275.688-.287t.712.287q.3.275.313.688T12.3 6.1L7.4 11h11.175q.425 0 .713.288t.287.712q0 .425-.287.713t-.713.287H7.4l4.9 4.9q.275.275.288.7t-.288.7q-.275.3-.7.3t-.725-.3Z"></path></svg></a><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="18px" height="18px" fill="currentColor" data-v-2e02c169=""><path d="M12 2c5.53 0 10 4.47 10 10s-4.47 10-10 10S2 17.53 2 12 6.47 2 12 2m3.59 5L12 10.59 8.41 7 7 8.41 10.59 12 7 15.59 8.41 17 12 13.41 15.59 17 17 15.59 13.41 12 17 8.41 15.59 7Z"></path></svg></p><div data-v-2e02c169=""><nav data-v-2e02c169=""><a href="https://imgcreator.zmo.ai/" data-v-2e02c169=""><svg xmlns="http://www.w3.org/2000/svg" fill="currentColor" viewBox="0 0 942 200" width="145px" height="32px" data-v-2e02c169=""><path fill="currentColor" d="M337 147.5V51.463h18.85V147.5H337Zm32.532 0V51.463h23.562l13.162 62.074 13.325-62.075h23.4V147.5h-16.575V75.675L412.594 147.5h-12.837l-13.65-71.825V147.5h-16.575Zm107.271 1.787c-3.683 0-7.15-.596-10.4-1.787-3.142-1.192-5.742-3.521-7.8-6.988-2.058-3.575-3.088-8.883-3.088-15.925v-52.65c0-5.633 1.084-10.02 3.25-13.162 2.275-3.142 5.417-5.308 9.425-6.5 4.009-1.3 8.667-1.95 13.975-1.95 5.309 0 9.967.65 13.975 1.95 4.117 1.192 7.313 3.358 9.588 6.5 2.383 3.142 3.575 7.53 3.575 13.162v13.488H490.94V70.962c0-2.6-.379-4.55-1.137-5.85-.65-1.408-1.571-2.329-2.763-2.762-1.191-.433-2.654-.65-4.387-.65-1.625 0-3.088.217-4.388.65-1.191.433-2.166 1.354-2.925 2.762-.65 1.3-.975 3.25-.975 5.85v58.175c0 2.492.325 4.442.975 5.85.759 1.3 1.734 2.221 2.925 2.763 1.3.433 2.763.65 4.388.65 2.491 0 4.496-.759 6.012-2.275 1.517-1.517 2.275-3.846 2.275-6.988v-19.175h-7.8V99.237h26.163V147.5h-14.138l-.975-7.475-.65.975a8.068 8.068 0 0 0-.487.812c-1.084 1.842-2.925 3.575-5.525 5.2-2.492 1.517-6.067 2.275-10.725 2.275Zm70.596.163c-5.309 0-10.021-.65-14.138-1.95-4.008-1.192-7.204-3.738-9.587-7.638-2.275-4.008-3.413-9.912-3.413-17.712V71.775c0-5.742 1.138-10.183 3.413-13.325 2.383-3.142 5.633-5.308 9.75-6.5 4.116-1.3 8.829-1.95 14.137-1.95 5.417 0 10.075.65 13.975 1.95 4.008 1.3 7.096 3.52 9.263 6.662 2.166 3.142 3.25 7.53 3.25 13.163V91.6h-18.85V70.962c0-2.6-.325-4.55-.975-5.85-.65-1.408-1.571-2.329-2.763-2.762-1.083-.433-2.383-.65-3.9-.65-1.516 0-2.871.217-4.062.65-1.192.433-2.113 1.354-2.763 2.762-.65 1.3-.975 3.25-.975 5.85v58.175c0 2.492.325 4.442.975 5.85.65 1.3 1.571 2.221 2.763 2.763 1.191.433 2.546.65 4.062.65 1.517 0 2.817-.217 3.9-.65 1.192-.542 2.113-1.463 2.763-2.763.65-1.408.975-3.358.975-5.85v-19.012h18.85v11.862c0 7.909-1.084 13.867-3.25 17.875-2.167 3.9-5.255 6.446-9.263 7.638-3.9 1.3-8.612 1.95-14.137 1.95Zm38.587-1.95V51.463h35.1c6.391 0 11.212 1.462 14.462 4.387 3.25 2.817 4.875 6.77 4.875 11.862v14.95c0 5.092-1.354 9.371-4.062 12.838-2.709 3.466-6.934 5.471-12.675 6.012 3.683.542 6.283 1.788 7.8 3.738 1.625 1.841 3.033 4.875 4.225 9.1l9.1 33.15h-20.963l-6.987-31.85c-.65-2.6-1.408-4.767-2.275-6.5-.758-1.842-1.896-2.763-3.413-2.763h-5.525V147.5h-19.662Zm19.662-54.275h7.8c3.034 0 5.2-.813 6.5-2.438 1.3-1.733 1.95-4.441 1.95-8.125V71.125c0-3.467-.596-5.904-1.787-7.313-1.192-1.408-3.033-2.112-5.525-2.112h-8.938v31.525Zm48.575 54.275V51.463h44.2v13.324h-24.538v25.675h20.8v14.788h-20.8v29.25h24.538v13h-44.2Zm48.747 0 17.225-96.037h26.975l16.9 96.037h-21.125l-2.925-17.713h-13.975l-2.762 17.713H702.97Zm23.4-27.138h13.488L733.195 68.2l-6.825 52.162Zm50.428 27.138V64.787h-14.95V51.462h48.75v13.325h-14.95V147.5h-18.85Zm66.548 1.95c-5.309 0-9.967-.65-13.975-1.95-4.009-1.192-7.15-3.738-9.425-7.638-2.167-4.008-3.25-9.912-3.25-17.712V71.775c0-5.742 1.083-10.183 3.25-13.325 2.275-3.142 5.416-5.308 9.425-6.5 4.116-1.3 8.829-1.95 14.137-1.95 5.2 0 9.804.65 13.813 1.95 4.116 1.3 7.312 3.52 9.587 6.662 2.383 3.142 3.575 7.53 3.575 13.163v50.212c0 7.909-1.192 13.867-3.575 17.875-2.275 3.9-5.471 6.446-9.587 7.638-4.009 1.3-8.667 1.95-13.975 1.95Zm0-11.05c1.625 0 3.033-.271 4.225-.813 1.3-.541 2.275-1.462 2.925-2.762.758-1.409 1.137-3.304 1.137-5.688V70.962c0-2.383-.379-4.225-1.137-5.525-.65-1.408-1.625-2.383-2.925-2.924-1.192-.542-2.6-.813-4.225-.813-1.625 0-3.034.27-4.225.813-1.192.541-2.167 1.516-2.925 2.925-.65 1.3-.975 3.141-.975 5.524v58.175c0 2.384.325 4.279.975 5.688.758 1.3 1.733 2.221 2.925 2.762 1.191.542 2.6.813 4.225.813Zm39.709 9.1V51.463h35.1c6.392 0 11.213 1.462 14.463 4.387 3.25 2.817 4.875 6.77 4.875 11.862v14.95c0 5.092-1.355 9.371-4.063 12.838-2.708 3.466-6.933 5.471-12.675 6.012 3.683.542 6.283 1.788 7.8 3.738 1.625 1.841 3.033 4.875 4.225 9.1l9.1 33.15h-20.962l-6.988-31.85c-.65-2.6-1.408-4.767-2.275-6.5-.758-1.842-1.896-2.763-3.412-2.763h-5.525V147.5h-19.663Zm19.663-54.275h7.8c3.033 0 5.2-.813 6.5-2.438 1.3-1.733 1.95-4.441 1.95-8.125V71.125c0-3.467-.596-5.904-1.788-7.313-1.192-1.408-3.033-2.112-5.525-2.112h-8.937v31.525Z"></path><path fill="url(#a)" d="M89.764 46.52c0 7.42-6.016 13.436-13.437 13.436-7.421 0-13.437-6.016-13.437-13.437 0-7.42 6.016-13.437 13.437-13.437 7.421 0 13.437 6.016 13.437 13.437Z"></path><path fill="url(#b)" d="M89.764 46.52c0 7.42-6.016 13.436-13.437 13.436-7.421 0-13.437-6.016-13.437-13.437 0-7.42 6.016-13.437 13.437-13.437 7.421 0 13.437 6.016 13.437 13.437Z"></path><path fill="url(#c)" d="M158.811 9.515a9.096 9.096 0 1 1-18.193 0 9.096 9.096 0 0 1 18.193 0Z"></path><path fill="url(#d)" d="M291.115 118.46c0 8.334-6.756 15.091-15.091 15.091-8.334 0-15.091-6.757-15.091-15.091 0-8.335 6.757-15.091 15.091-15.091 8.335 0 15.091 6.756 15.091 15.091Z"></path><path fill="url(#e)" d="M.045 133.138c0 35.164 28.507 63.671 63.672 63.671 10.92 0 21.2-2.749 30.181-7.594 11.348-6.12 19.787-11.512 26.875-11.425 19.805.245 29.07 21.5 61.811 21.5 32.533 0 59.349-24.479 63.033-56.023.449-3.845.741-7.13.443-10.336-1.717-18.462-3.17-23.647 14.248-35.35 14.515-9.754 24.605-24.052 24.605-42.173 0-26.487-21.472-47.96-47.96-47.96-25.829.45-30.489 10.283-38.877 19.432-7.755 8.46-16.24 12.383-27.689 10.59-4.864-.761-10.03-1.907-15.504-1.907-17.51 0-33.529 6.384-45.856 16.951-7.509 6.437-10.811 13.644-15.542 16.125-7.18 3.766-15.495.827-29.768.827-35.165 0-63.672 28.507-63.672 63.672Z"></path><defs><linearGradient id="a" x1="10.175" x2="275.197" y1="124.248" y2="71.533" gradientUnits="userSpaceOnUse"><stop stop-color="#4979F3"></stop><stop offset="1" stop-color="#DF9DDD"></stop></linearGradient><linearGradient id="b" x1="10.175" x2="275.197" y1="124.248" y2="71.533" gradientUnits="userSpaceOnUse"><stop stop-color="#4979F3"></stop><stop offset="1" stop-color="#DF9DDD"></stop></linearGradient><linearGradient id="c" x1="10.175" x2="275.197" y1="124.248" y2="71.533" gradientUnits="userSpaceOnUse"><stop stop-color="#4979F3"></stop><stop offset="1" stop-color="#DF9DDD"></stop></linearGradient><linearGradient id="d" x1="10.175" x2="275.197" y1="124.248" y2="71.533" gradientUnits="userSpaceOnUse"><stop stop-color="#4979F3"></stop><stop offset="1" stop-color="#DF9DDD"></stop></linearGradient><linearGradient id="e" x1="10.175" x2="275.197" y1="124.248" y2="71.533" gradientUnits="userSpaceOnUse"><stop stop-color="#4979F3"></stop><stop offset="1" stop-color="#DF9DDD"></stop></linearGradient></defs></svg></a><ul data-v-2e02c169=""><li data-v-2e02c169=""><p> Products </p></li><li data-v-2e02c169=""><a href="https://imgcreator.zmo.ai/community" data-v-2e02c169="">Community</a></li><li data-v-2e02c169=""><a href="https://imgcreator.zmo.ai/helper/content-policy" data-v-2e02c169=""> Help and Content Policy </a></li><li data-v-2e02c169=""><a href="https://discord.gg/SjCECNdrGk" rel="noopener noreferrer" target="_blank" data-v-2e02c169=""><p> Join Discord <br data-v-2e02c169=""> for Free Credits </p></a></li></ul></nav></div></header></div><!--[--><!--[--><div><div><div><svg width="30" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M39.9965 20C40.1965 32.4138 32.4138 40.1885 20 39.9965C7.58616 40.1885 -0.188492 32.4138 0.00347512 20C-0.188492 7.58616 7.58616 -0.188492 20 0.00347512C32.4138 -0.188492 40.1885 7.58616 39.9965 20Z" fill="white"></path><path d="M13.6008 8.00195H29.598C31.1977 8.00195 31.9976 8.80181 31.9976 10.4015V26.3988C31.9976 27.9985 31.1977 28.7983 29.598 28.7983H13.6008C12.001 28.7983 11.2012 27.9985 11.2012 26.3988V10.4015C11.2012 8.80181 12.001 8.00195 13.6008 8.00195Z" fill="#BDEFD1"></path><path d="M10.4015 11.2012H26.3988C27.9985 11.2012 28.7983 12.001 28.7983 13.6008V29.598C28.7983 31.1977 27.9985 31.9976 26.3988 31.9976H10.4015C8.80181 31.9976 8.00195 31.1977 8.00195 29.598V13.6008C8.00195 12.001 8.80181 11.2012 10.4015 11.2012Z" fill="#4ED89D"></path><path d="M23.5999 15.6006H13.2017C12.8835 15.6006 12.5784 15.727 12.3534 15.952C12.1284 16.177 12.002 16.4822 12.002 16.8004C12.002 17.1186 12.1284 17.4238 12.3534 17.6488C12.5784 17.8738 12.8835 18.0002 13.2017 18.0002H17.201V27.1986C17.2011 27.5167 17.3275 27.8219 17.5525 28.0468C17.7775 28.2718 18.0827 28.3981 18.4008 28.3981C18.719 28.3981 19.0241 28.2718 19.2491 28.0468C19.4741 27.8219 19.6006 27.5167 19.6006 27.1986V18.0002H23.5999C23.9181 18.0002 24.2233 17.8738 24.4483 17.6488C24.6733 17.4238 24.7997 17.1186 24.7997 16.8004C24.7997 16.4822 24.6733 16.177 24.4483 15.952C24.2233 15.727 23.9181 15.6006 23.5999 15.6006Z" fill="white"></path></svg><p> Text Input </p></div><div><svg width="30" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M39.9965 20C40.1965 32.4138 32.4138 40.1885 20 39.9965C7.58616 40.1885 -0.188492 32.4138 0.00347512 20C-0.188492 7.58616 7.58616 -0.188492 20 0.00347512C32.4138 -0.188492 40.1885 7.58616 39.9965 20Z" fill="#fff"></path><path d="M13.4231 10H29.5769C31.1923 10 32 10.6154 32 11.8462V24.1538C32 25.3846 31.1923 26 29.5769 26H13.4231C11.8077 26 11 25.3846 11 24.1538V11.8462C11 10.6154 11.8077 10 13.4231 10Z" fill="#4ED8D0"></path><path d="M28.5714 12.6289H8.85714C8.62981 12.6289 8.4118 12.7192 8.25105 12.88C8.09031 13.0407 8 13.2587 8 13.486V24.2523C8.96457 21.5275 11.5634 19.5755 14.6186 19.5755C17.5451 19.5755 20.0529 21.3669 21.1066 23.9126C21.797 23.4043 22.599 23.0684 23.4456 22.9331C24.2922 22.7978 25.1589 22.8669 25.9734 23.1348C26.7879 23.4026 27.5265 23.8614 28.1275 24.4728C28.7286 25.0842 29.1747 25.8306 29.4286 26.6495V13.486C29.4286 13.2587 29.3383 13.0407 29.1775 12.88C29.0168 12.7192 28.7988 12.6289 28.5714 12.6289ZM24.5457 18.8138C24.1712 18.8131 23.8053 18.7015 23.4942 18.493C23.1831 18.2845 22.9408 17.9884 22.7979 17.6422C22.655 17.296 22.6179 16.9152 22.6914 16.548C22.7648 16.1807 22.9455 15.8435 23.2105 15.5789C23.4755 15.3143 23.8131 15.1342 24.1805 15.0614C24.5478 14.9885 24.9285 15.0262 25.2745 15.1697C25.6204 15.3132 25.9161 15.556 26.1241 15.8674C26.3321 16.1788 26.4431 16.545 26.4431 16.9195C26.443 17.1685 26.3937 17.4149 26.2983 17.6449C26.2028 17.8748 26.063 18.0837 25.8868 18.2596C25.7106 18.4355 25.5015 18.575 25.2714 18.6701C25.0413 18.7652 24.7947 18.814 24.5457 18.8138Z" fill="#5275A9"></path><path d="M22.6484 16.9187C22.6484 17.4211 22.8483 17.9029 23.2042 18.2582C23.56 18.6134 24.0426 18.813 24.5459 18.813C25.0491 18.813 25.5317 18.6134 25.8876 18.2582C26.2434 17.9029 26.4433 17.4211 26.4433 16.9187C26.4433 16.4163 26.2434 15.9345 25.8876 15.5792C25.5317 15.224 25.0491 15.0244 24.5459 15.0244C24.0426 15.0244 23.56 15.224 23.2042 15.5792C22.8483 15.9345 22.6484 16.4163 22.6484 16.9187Z" fill="white"></path><path d="M24.2944 22.8652C23.1473 22.8636 22.0301 23.2306 21.1075 23.9121C21.4647 24.7785 21.6452 25.7075 21.6385 26.6446C21.6318 27.5817 21.4379 28.5081 21.0684 29.3692H28.5724C28.7997 29.3692 29.0177 29.2789 29.1785 29.1182C29.3392 28.9574 29.4295 28.7394 29.4295 28.5121V26.649C29.0894 25.5528 28.4076 24.5942 27.4836 23.9134C26.5596 23.2326 25.4421 22.8653 24.2944 22.8652Z" fill="#73CAE5"></path><path d="M8.85714 29.3714H21.0674C21.437 28.5103 21.6309 27.5839 21.6376 26.6468C21.6443 25.7097 21.4638 24.7807 21.1066 23.9143C20.0529 21.3686 17.5451 19.5771 14.6186 19.5771C11.5634 19.5771 8.96457 21.5291 8 24.254V28.5143C8 28.7416 8.09031 28.9596 8.25105 29.1204C8.4118 29.2811 8.62981 29.3714 8.85714 29.3714Z" fill="#95D9EF"></path></svg><p> Image Input </p></div><div><svg width="30" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M39.9965 20C40.1965 32.4138 32.4138 40.1885 20 39.9965C7.58616 40.1885 -0.188492 32.4138 0.00347512 20C-0.188492 7.58616 7.58616 -0.188492 20 0.00347512C32.4138 -0.188492 40.1885 7.58616 39.9965 20Z" fill="white"></path><path d="M18.6214 12.3303C20.9443 12.3683 22.961 14.92 24.3775 18.0111C21.6594 18.8822 18.9304 23.5399 17.3117 25.8541C15.1461 25.7784 12.9733 24.3142 11.117 25.7165C10.8569 25.9132 10.6075 26.1229 10.37 26.3448C10.0686 26.626 9.67006 26.4449 9.78918 26.0093C12.2946 16.8513 15.2386 12.2916 18.6214 12.3303Z" fill="#EECCFF"></path><path d="M24.9907 17.6338C27.1626 17.6338 30.5307 19.8188 33.095 24.1888C33.3083 24.7282 33.2309 25.1112 32.8629 25.3378C29.0611 26.9707 25.7448 26.9707 22.9138 25.3378C21.3047 24.4098 19.4348 25.7578 17.5273 25.5578C19.3964 22.8576 21.8752 17.6338 24.9907 17.6338Z" fill="#EECCFF"></path><path d="M15.8538 14.0208C18.1768 14.0587 20.1934 16.6104 21.6099 19.7016C18.8918 20.5726 16.1628 25.2304 14.5441 27.5446C12.3785 27.4689 10.2057 26.0046 8.34945 27.4069C8.0893 27.6036 7.83996 27.8133 7.60243 28.0352C7.30106 28.3165 6.90249 28.1353 7.0216 27.6997C9.52698 18.5417 12.471 13.9821 15.8538 14.0208Z" fill="#45B2E0"></path><path d="M27.7671 13.5C27.7921 15.0517 26.8193 16.0236 25.2676 15.9996C23.7158 16.0236 22.744 15.0517 22.768 13.5C22.744 11.9483 23.7158 10.9764 25.2676 11.0004C26.8193 10.9764 27.7911 11.9483 27.7671 13.5Z" fill="#EECCFF"></path><path d="M26.7671 16.5C26.7921 18.0517 25.8193 19.0236 24.2676 18.9996C22.7158 19.0236 21.744 18.0517 21.768 16.5C21.744 14.9483 22.7158 13.9764 24.2676 14.0004C25.8193 13.9764 26.7911 14.9483 26.7671 16.5Z" fill="#BC47E4"></path><path d="M22.2231 19.3242C24.395 19.3242 27.7631 21.5092 30.3274 25.8793C30.5408 26.4186 30.4634 26.8016 30.0953 27.0283C26.2935 28.6611 22.9772 28.6611 20.1462 27.0283C18.5371 26.1002 16.6672 27.4482 14.7598 27.2482C16.6288 24.548 19.1076 19.3242 22.2231 19.3242Z" fill="#B45AFC"></path></svg><p> Background AI </p></div><div><svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M29.9974 15C30.1474 24.3104 24.3104 30.1414 15 29.9974C5.68962 30.1414 -0.141369 24.3104 0.00260634 15C-0.141369 5.68962 5.68962 -0.141369 15 0.00260634C24.3104 -0.141369 30.1414 5.68962 29.9974 15Z" fill="white"></path><path d="M6.00399 14.2808C5.87938 11.2012 8.67078 6 15.1269 6C21.5831 6 24.0474 11.2947 24.0474 15.0251C24.0474 17.7085 22.7836 22.5715 16.6723 24.2871C15.4696 24.6242 14.5462 23.2346 15.3094 22.2466C15.5297 21.9607 15.7411 21.6436 15.9402 21.2876C16.5477 20.204 15.4073 18.9902 14.2714 19.4942C12.8762 20.1128 11.1318 20.4476 9.13694 19.8491C6.01622 18.9123 6.00399 14.2808 6.00399 14.2808Z" fill="#8045CF"></path><path d="M8.48273 14.3734C8.48273 14.7821 8.64507 15.174 8.93404 15.463C9.22302 15.752 9.61495 15.9143 10.0236 15.9143C10.4323 15.9143 10.8242 15.752 11.1132 15.463C11.4022 15.174 11.5645 14.7821 11.5645 14.3734C11.5645 13.9647 11.4022 13.5728 11.1132 13.2838C10.8242 12.9949 10.4323 12.8325 10.0236 12.8325C9.61495 12.8325 9.22302 12.9949 8.93404 13.2838C8.64507 13.5728 8.48273 13.9647 8.48273 14.3734Z" fill="white"></path><path d="M11.1485 10.8798C11.1485 11.2197 11.2835 11.5457 11.5239 11.7861C11.7642 12.0264 12.0902 12.1615 12.4302 12.1615C12.7701 12.1615 13.0961 12.0264 13.3364 11.7861C13.5768 11.5457 13.7118 11.2197 13.7118 10.8798C13.7118 10.7115 13.6787 10.5448 13.6143 10.3893C13.5499 10.2338 13.4555 10.0925 13.3364 9.97354C13.2174 9.85452 13.0761 9.76011 12.9206 9.6957C12.7651 9.6313 12.5985 9.59814 12.4302 9.59814C12.2619 9.59814 12.0952 9.6313 11.9397 9.6957C11.7842 9.76011 11.6429 9.85452 11.5239 9.97354C11.4049 10.0925 11.3105 10.2338 11.2461 10.3893C11.1816 10.5448 11.1485 10.7115 11.1485 10.8798Z" fill="white"></path><path d="M15.1169 9.81508C15.1169 9.94847 15.1432 10.0806 15.1942 10.2038C15.2453 10.327 15.3201 10.439 15.4144 10.5333C15.5087 10.6277 15.6207 10.7025 15.7439 10.7535C15.8672 10.8046 15.9993 10.8308 16.1326 10.8308C16.266 10.8308 16.3981 10.8046 16.5214 10.7535C16.6446 10.7025 16.7566 10.6277 16.8509 10.5333C16.9452 10.439 17.02 10.327 17.0711 10.2038C17.1221 10.0806 17.1484 9.94847 17.1484 9.81508C17.1484 9.68169 17.1221 9.5496 17.0711 9.42636C17.02 9.30313 16.9452 9.19115 16.8509 9.09683C16.7566 9.0025 16.6446 8.92768 16.5214 8.87664C16.3981 8.82559 16.266 8.79932 16.1326 8.79932C15.9993 8.79932 15.8672 8.82559 15.7439 8.87664C15.6207 8.92768 15.5087 9.0025 15.4144 9.09683C15.3201 9.19115 15.2453 9.30313 15.1942 9.42636C15.1432 9.5496 15.1169 9.68169 15.1169 9.81508Z" fill="white"></path><path d="M16.1326 17.2705C15.8723 17.2705 15.6142 17.1592 15.4339 16.9434C15.3573 16.8517 15.2995 16.7458 15.2638 16.6317C15.2281 16.5176 15.2153 16.3976 15.226 16.2786C15.2368 16.1595 15.2709 16.0438 15.3264 15.9379C15.382 15.8321 15.4578 15.7382 15.5496 15.6618L24.6025 8.10082C24.9886 7.77929 25.5627 7.83047 25.8842 8.21653C25.9608 8.30824 26.0187 8.41416 26.0543 8.52824C26.09 8.64232 26.1028 8.76231 26.0921 8.88135C26.0813 9.00039 26.0472 9.11615 25.9917 9.22199C25.9362 9.32784 25.8603 9.42169 25.7685 9.49819L16.7156 17.0591C16.5522 17.196 16.3458 17.2709 16.1326 17.2705Z" fill="#AE64EC"></path><path d="M23.3699 11.5025C23.1474 10.9318 22.867 10.37 22.5254 9.83594L15.5497 15.6624C15.1637 15.985 15.1125 16.5591 15.434 16.9441C15.5193 17.0466 15.6261 17.129 15.7469 17.1855C15.8676 17.2421 15.9994 17.2713 16.1327 17.2711C16.3385 17.2711 16.5455 17.2022 16.7157 17.0598L23.3699 11.5025Z" fill="#7462E6"></path></svg><p> Swap Anything </p></div><div><svg width="30" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M39.9965 20C40.1965 32.4138 32.4138 40.1885 20 39.9965C7.58616 40.1885 -0.188492 32.4138 0.00347512 20C-0.188492 7.58616 7.58616 -0.188492 20 0.00347512C32.4138 -0.188492 40.1885 7.58616 39.9965 20Z" fill="white"></path><path d="M33.482 18.8094C30.2979 15.6547 27.1011 12.5129 23.9041 9.37149C23.4092 8.88545 22.8138 8.60177 22.1045 8.59961C21.3579 8.59961 20.7547 8.876 19.972 9.61114C18.9112 10.6072 17.861 11.6144 16.8061 12.6166C14.6962 14.6207 12.5811 16.6194 10.4795 18.6321C9.92411 19.1641 9.85257 19.9627 10.2577 20.5114C10.6644 21.062 11.4709 21.2583 12.1309 20.9258C12.3932 20.7938 12.6269 20.589 12.8435 20.3854C15.3506 18.0282 17.8451 15.6575 20.3579 13.3065C21.6037 12.141 22.4694 12.16 23.7245 13.3238C23.9278 13.5122 24.1178 13.7148 24.3177 13.907C26.6822 16.1805 29.0428 18.4581 31.4153 20.7233C32.1076 21.3841 33.0768 21.3948 33.6492 20.7895C34.1598 20.25 34.1207 19.442 33.482 18.8094Z" fill="#C9DBFF"></path><path d="M30.6462 22.031C28.2736 19.8019 25.9188 17.5538 23.5414 15.3296C22.5852 14.4352 21.4002 14.4594 20.4369 15.3549C18.0698 17.5552 15.7137 19.7679 13.3398 21.9609C13.0761 22.2045 12.9733 22.4434 12.9766 22.7955C12.9928 24.4355 12.9812 26.0758 12.9847 27.716C12.9874 29.0043 13.3875 29.3976 14.6992 29.3992C17.0787 29.4023 19.4581 29.4001 21.8376 29.4001C24.1941 29.4001 26.5504 29.4004 28.9068 29.4C30.5289 29.4 30.9333 28.9892 30.9333 27.3427C30.9333 25.8411 30.9291 24.3396 30.9367 22.838C30.9383 22.526 30.8955 22.2652 30.6462 22.031Z" fill="#C9DBFF"></path><path d="M31.482 19.8094C28.2979 16.6547 25.1011 13.5129 21.9041 10.3715C21.4092 9.88545 20.8138 9.60177 20.1045 9.59961C19.3579 9.59961 18.7547 9.876 17.972 10.6111C16.9112 11.6072 15.861 12.6144 14.8061 13.6166C12.6962 15.6207 10.5811 17.6194 8.47948 19.6321C7.92411 20.1641 7.85257 20.9627 8.2577 21.5114C8.66438 22.062 9.47092 22.2583 10.1309 21.9258C10.3932 21.7938 10.6269 21.589 10.8435 21.3854C13.3506 19.0282 15.8451 16.6575 18.3579 14.3065C19.6037 13.141 20.4694 13.16 21.7245 14.3238C21.9278 14.5122 22.1178 14.7148 22.3177 14.907C24.6822 17.1805 27.0428 19.4581 29.4153 21.7233C30.1076 22.3841 31.0768 22.3948 31.6492 21.7895C32.1598 21.25 32.1207 20.442 31.482 19.8094Z" fill="#1893FE"></path><path d="M28.6462 23.031C26.2736 20.8019 23.9188 18.5538 21.5414 16.3296C20.5852 15.4352 19.4002 15.4594 18.4369 16.3549C16.0698 18.5552 13.7137 20.7679 11.3398 22.9609C11.0761 23.2045 10.9733 23.4434 10.9766 23.7955C10.9928 25.4355 10.9812 27.0758 10.9847 28.716C10.9874 30.0043 11.3875 30.3976 12.6992 30.3992C15.0787 30.4023 17.4581 30.4001 19.8376 30.4001C22.1941 30.4001 24.5504 30.4004 26.9068 30.4C28.5289 30.4 28.9333 29.9892 28.9333 28.3427C28.9333 26.8411 28.9291 25.3396 28.9367 23.838C28.9383 23.526 28.8955 23.2652 28.6462 23.031ZM22.4299 25.7553C21.8249 26.5188 21.0873 27.1493 20.3081 27.7322C20.0993 27.8882 19.9353 27.9372 19.7012 27.757C18.7649 27.0365 17.8527 26.2947 17.2042 25.2865C16.7108 24.5195 16.4567 23.6997 16.7353 22.7899C16.9292 22.1576 17.3078 21.6856 17.9807 21.5225C18.6683 21.3561 19.1858 21.6286 19.6187 22.156C19.7666 22.3362 19.8964 22.8166 20.2459 22.3125C20.7291 21.6158 21.3606 21.2732 22.2082 21.5855C22.9223 21.8488 23.3301 22.6007 23.3268 23.4888C23.3547 24.4263 22.9359 25.1172 22.4299 25.7553Z" fill="#1893FE"></path></svg><p> Room Design </p></div></div><div><div data-v-d396fead=""><div data-v-d396fead=""><p><i data-v-d396fead=""><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" data-v-d396fead=""><path fill="currentColor" d="M771.776 794.88A384 384 0 0 1 128 512h64a320 320 0 0 0 555.712 216.448H654.72a32 32 0 1 1 0-64h149.056a32 32 0 0 1 32 32v148.928a32 32 0 1 1-64 0v-50.56zM276.288 295.616h92.992a32 32 0 0 1 0 64H220.16a32 32 0 0 1-32-32V178.56a32 32 0 0 1 64 0v50.56A384 384 0 0 1 896.128 512h-64a320 320 0 0 0-555.776-216.384z"></path></svg><!--]--></i> Try an example </p></div><div data-v-d396fead=""><form data-v-d396fead=""><!--[--><div data-v-d396fead=""><!--[--><p><span>0 / 400</span></p><!--]--><!--[--><!--]--></div><!--]--></form><div data-v-d396fead=""><p> Prompt Guide </p></div></div><div data-v-d396fead=""><div data-v-d396fead=""><h6 data-v-d396fead="">Category</h6><div data-v-d396fead=""><!--[--><div data-v-d396fead=""><p><img src="https://assets.imgcreator.ai/category/anime.webp" data-v-d396fead=""></p><h6 data-v-d396fead="">Anime</h6></div><div data-v-d396fead=""><div data-v-d396fead=""><p> Pro </p><p><img src="https://assets.imgcreator.ai/category/Realistic_photo.webp" data-v-d396fead=""></p></div><h6 data-v-d396fead="">Realistic photo</h6></div><div data-v-d396fead=""><p><img src="https://assets.imgcreator.ai/category/Free_form.webp" data-v-d396fead=""></p><h6 data-v-d396fead="">Free form</h6></div><div data-v-d396fead=""><div data-v-d396fead=""><p> Pro </p><p><img src="https://assets.imgcreator.ai/category/Illustration.webp" data-v-d396fead=""></p></div><h6 data-v-d396fead="">Vector illustration</h6></div><div data-v-d396fead=""><p><img src="https://assets.imgcreator.ai/category/Art.webp" data-v-d396fead=""></p><h6 data-v-d396fead="">Art</h6></div><div data-v-d396fead=""><p><img src="https://assets.imgcreator.ai/category/Character_text2img.webp" data-v-d396fead=""></p><h6 data-v-d396fead="">Character</h6></div><div data-v-d396fead=""><div data-v-d396fead=""><p> Pro </p><p><img src="https://assets.imgcreator.ai/category/3D_design2.webp" data-v-d396fead=""></p></div><h6 data-v-d396fead="">3D design</h6></div><!--]--></div></div><div data-v-d396fead=""><h6 data-v-d396fead="">Add style</h6><div data-v-d396fead=""><!--[--><div data-v-d396fead=""><p><img src="https://assets.imgcreator.ai/category/AnimeCrystal.webp" data-v-d396fead=""></p><h6 data-v-d396fead="">Anime Vivacity</h6></div><div data-v-d396fead=""><p><img src="https://assets.imgcreator.ai/category/Anime_One.webp" data-v-d396fead=""></p><h6 data-v-d396fead="">Japanese Anime</h6></div><div data-v-d396fead=""><p><img src="https://assets.imgcreator.ai/category/AnimeDrawing.webp" data-v-d396fead=""></p><h6 data-v-d396fead="">Anime Drawing</h6></div><div data-v-d396fead=""><p><img src="https://assets.imgcreator.ai/category/AnimeArtV2.webp" data-v-d396fead=""></p><h6 data-v-d396fead="">Anime Art</h6></div><div data-v-d396fead=""><p><img src="https://assets.imgcreator.ai/category/AnimeD.webp" data-v-d396fead=""></p><h6 data-v-d396fead="">Fairy Girl</h6></div><div data-v-d396fead=""><p><img src="https://assets.imgcreator.ai/category/Anime_Two.webp" data-v-d396fead=""></p><h6 data-v-d396fead="">Anime Classic</h6></div><div data-v-d396fead=""><p><img src="https://assets.imgcreator.ai/category/Noble.webp" data-v-d396fead=""></p><h6 data-v-d396fead="">Gorgeous Girl</h6></div><div data-v-d396fead=""><p><img src="https://assets.imgcreator.ai/category/Soda.webp" data-v-d396fead=""></p><h6 data-v-d396fead="">Bright Girl</h6></div><div data-v-d396fead=""><p><img src="https://assets.imgcreator.ai/category/Makoto_Shinkai.webp" data-v-d396fead=""></p><h6 data-v-d396fead="">Makoto Shinkai</h6></div><div data-v-d396fead=""><p><img src="https://assets.imgcreator.ai/category/Miyazaki.webp" data-v-d396fead=""></p><h6 data-v-d396fead="">Miyazaki</h6></div><div data-v-d396fead=""><p><img src="https://assets.imgcreator.ai/category/Ukiyo-e.webp" data-v-d396fead=""></p><h6 data-v-d396fead="">Ukiyo-e</h6></div><!--]--></div></div></div><div data-v-d396fead=""><h6>Number of images</h6><div><!--[--><p>2 </p><div><p>4 </p><p> Starter </p><!----><!----></div><div><p>6 </p><!----><p> Pro </p><!----></div><div><p>8 </p><!----><!----><p> Boss </p></div><!--]--></div></div><!--[--><div><h6>Output resolution</h6></div><div><h6>Canvas size</h6></div><!--]--></div></div></div><!--]--><!--]--></div></div>]]></description>
        </item>
    </channel>
</rss>